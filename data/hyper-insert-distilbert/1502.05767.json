{"id": "1502.05767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2015", "title": "Automatic differentiation in machine learning: a survey", "abstract": "derivatives, mostly in the matrix form systems of nash gradients and hessians, are ubiquitous in functional machine design learning. automatic differentiation ( agent ad ) is a technique for calculating derivatives modeled efficiently and accurately, established in fields referencing such schools as computational fluid dynamics, molecular nuclear engine engineering, atomic and interdisciplinary atmospheric sciences. despite limiting its advantages and wider use observed in other computational fields, often machine learning practitioners have similarly been little influenced adequately by ad and make perhaps scant contemporary use of directly available calculation tools. we subsequently survey the crucial intersection of ad and machine interaction learning, cover applications where ad systematically has determined the potential desire to make a big impact, organize and report, on the latest recent developments in examining the adoption this technique. presumably we also further aim best to dispel above some misconceptions that we think have impeded maintaining the widespread awareness of ad within the computer machine science learning methodology community.", "histories": [["v1", "Fri, 20 Feb 2015 04:20:47 GMT  (70kb,D)", "http://arxiv.org/abs/1502.05767v1", "28 pages, 5 figures"], ["v2", "Sun, 19 Apr 2015 16:49:13 GMT  (79kb,D)", "http://arxiv.org/abs/1502.05767v2", "29 pages, 5 figures"], ["v3", "Thu, 17 Aug 2017 16:45:07 GMT  (69kb)", "http://arxiv.org/abs/1502.05767v3", "34 pages, 5 figures"]], "COMMENTS": "28 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.SC cs.LG", "authors": ["atilim gunes baydin", "barak a pearlmutter", "alexey", "reyevich radul", "jeffrey mark siskind"], "accepted": false, "id": "1502.05767"}, "pdf": {"name": "1502.05767.pdf", "metadata": {"source": "CRF", "title": "Automatic differentiation in machine learning: a survey", "authors": ["At\u0131l\u0131m G\u00fcne\u015f Baydin", "Barak A. Pearlmutter", "Alexey Andreyevich Radul"], "emails": ["atilimgunes.baydin@nuim.ie;", "barak@cs.nuim.ie", "axofch@gmail.com"], "sections": [{"heading": null, "text": "Keywords Automatic differentiation \u00b7 Optimization \u00b7 Gradient methods \u00b7 Backpropagation"}, {"heading": "1 Introduction", "text": "The computation of derivatives in computer models is addressed by four main methods: 1. manually working out derivatives and coding the result; 2. numerical differentiation (using finite difference approximations); 3. symbolic differentiation (using expression-manipulation in of software such as Maxima, Mathematica, and Maple); and 4. automatic differentiation.\nA. G. Baydin (B) \u00b7 B. A. Pearlmutter Hamilton Institute & Department of Computer Science National University of Ireland Maynooth, Maynooth, Co. Kildare, Ireland E-mail: atilimgunes.baydin@nuim.ie; barak@cs.nuim.ie\nA. A. Radul Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology, Cambridge, MA 02139, United States E-mail: axofch@gmail.com\nar X\niv :1\n50 2.\n05 76\n7v 1\n[ cs\n.S C\n] 2\n0 Fe\nClassically, many methods in machine learning require the evaluation of derivatives and most of the traditional learning algorithms rely on the computation of gradients and Hessians of an objective function (Sra et al, 2011). Examples include the training of artificial neural networks (Widrow and Lehr, 1990), conditional random fields (Vishwanathan et al, 2006), natural language processing (Finkel et al, 2008), and computer vision (Parker, 2010).\nWhen introducing new models, machine learning researchers spend considerable effort on the manual derivation of analytical derivatives and subsequently plug these into standard optimization procedures such as L-BFGS (Zhu et al, 1997) or stochastic gradient descent (Bottou, 1998).\nManual differentiation is evidently time consuming and prone to error. Of the other alternatives, numerical differentiation is simple to implement, but susceptible to round-off and truncation errors that make it inherently unstable (Jerrell, 1997). Symbolic differentiation addresses the weaknesses of both the manual and numerical methods, but often results in complex and cryptic expressions plagued with the problem of \u201cexpression swell\u201d (Corliss, 1988). Furthermore, manual and symbolic methods require the model to be expressed as a closed-form mathematical formula, ruling out algorithmic control flow and severely limiting expressivity.\nWe are concerned with the powerful fourth technique, automatic differentiation1 (AD), which works by systematically applying the chain rule of differential calculus at the elementary operator level. AD allows the accurate evaluation of derivatives in machine precision, with only a small constant factor of overhead and ideal asymptotic efficiency. In contrast with the effort involved in arranging code into closed-form expressions for symbolic differentiation, AD can usually be applied to existing code with minimal change. Because of its generality, AD is an already established tool in applications including realparameter optimization (Walther, 2007), sensitivity analysis (Carmichael and Sandu, 1997), physical modeling (Ekstro\u0308m et al, 2010), and probabilistic inference (Neal, 2011).\nDespite its widespread use in other fields, AD has been underused, if not unknown, by the machine learning community.\nAs it happens, AD and machine learning practice are conceptually very closely interconnected: consider the backpropagation method for training neural networks, which has a colorful history of being rediscovered several times by independent researchers (Widrow and Lehr, 1990). It has been one of the most studied and used training algorithms since the day it became popular mainly through the work of Rumelhart et al (1986). In simplest terms, backpropagation models learning as gradient descent in neural network weight space, looking for the minimum of an error function (Gori and Maggini, 1996). This is accomplished by the backwards propagation of the error values at the output (Fig. 1) utilizing the chain rule to compute the gradient of the error at each point. The resulting algorithm is essentially equivalent to transforming the network evaluation function through the so-called reverse mode AD, which, as we will see, actually generalizes the backpropagation idea. Thus, a modest un-\n1 Also called algorithmic differentiation, and less frequently computational differentiation.\nderstanding of the mathematics underlying backpropagation already provides sufficient background for grasping the AD technique.\nHere we review the AD technique from a machine learning perspective, covering its origins, potential applications in machine learning, and methods of its implementation. It is our hope that the review will be a concise and accessible introduction to the technique for machine learning practitioners. Along the way, we also aim to dispel some misconceptions that we believe have impeded wider appraisal of AD.\nThe article is structured as follows: in Sect. 2 we start by emphasizing how AD actually differs from numerical and symbolic differentiation; Sect. 3 gives an introduction to the technique and its forward and reverse modes of operation; Sect. 4 discusses the role of derivatives in machine learning and examines cases where AD has the potential to have an impact; Sect. 5 covers the implementation approaches and available AD tools; and Sect. 6 offers our conclusions."}, {"heading": "2 What AD is not", "text": "Without proper introduction, the term \u201cautomatic differentiation\u201d has undertones suggesting that it is either a type of symbolic or numerical differentiation. This can be intensified by the dichotomy that the final results from AD are indeed numerical values, while the steps in its computation do depend on algebraic manipulation, giving AD a two-sided nature that is partly symbolic and partly numerical (Griewank, 2003).\nLet us start by stressing how AD is different from, and in some aspects superior to, these two commonly encountered techniques of differentiation.\n2.1 AD is not numerical differentiation\nNumerical differentiation is the finite difference approximation of derivatives using values of the original function evaluated at some sample points (Burden and Faires, 2001) (Fig. 2). In its simplest form, it is based on the standard definition of a derivative. For example, for a function of many variables f :\nRn \u2192 R, we can approximate the gradient \u2207f = (\n\u2202f \u2202x1 , . . . , \u2202f\u2202xn\n) using\n\u2202f(x) \u2202xi \u2248 f(x + hei)\u2212 f(x) h , (1)\nwhere ei is the i-th unit vector and 0 < h 1 is a step size. This has the advantage of being uncomplicated to implement, but the disadvantages of costing O(n) evaluations of f for a gradient in n dimensions and requiring careful consideration in selecting the step size h.\nNumerical approximations of derivatives are inherently ill-conditioned and unstable2, with the exception of complex variable methods that are applicable to a limited set of holomorphic functions (Fornberg, 1981). This is due to the introduction of truncation3 and round-off4 errors, inflicted by the limited precision of computations and the chosen value of the step size h. Truncation error tends to zero as h \u2192 0. However, as h is decreased, round-off error increases and becomes dominant (Fig. 3).\nImproved techniques have been conceived to mitigate this shortcoming of numerical differentiation, such as using a center difference approximation\n\u2202f(x) \u2202xi = f(x + hei)\u2212 f(x\u2212 hei) 2h +O(h2) , (2)\nwhere the first-order errors cancel and effectively move the truncation error from first-order to second-order5 in h. For the one-dimensional case, it is just as costly to compute the forward difference (Eq. 1) and the center difference (Eq. 2), requiring only two evaluations of f . However, with increasing\n2 Using the limit definition of the derivative for finite difference approximation commits both cardinal sins of numerical analysis: \u201cthou shalt not add small numbers to big numbers\u201d, and \u201cthou shalt not subtract numbers which are approximately equal\u201d.\n3 Truncation error is the error of approximation, or inaccuracy, one gets from h not actually being zero. It is proportional to a power of h.\n4 Round-off error is the inaccuracy one gets from valuable low-order bits of the final answer having to compete for machine-word space with high-order bits of f(x + hei) and f(x) (Eq. 1), which the computer has to store just until they cancel in the subtraction at the end. Round-off error is inversely proportional to a power of h.\n5 This does not avoid either of the cardinal sins, and is still highly inaccurate due to truncation.\ndimensionality, a trade-off between accuracy and performance is faced, where computing a Jacobian matrix of an f : Rn \u2192 Rm requires 2mn evaluations.\nOther techniques for improving numerical differentiation, including higherorder finite differences, Richardson extrapolation to the limit (Brezinski and Zaglia, 1991), and differential quadrature methods using weighted sums (Bert and Malik, 1996), also increase rapidly in programming complexity, do not completely eliminate approximation errors, and remain highly susceptible to floating point truncation.\n2.2 AD is not symbolic differentiation\nSymbolic differentiation is the automatic manipulation of expressions for obtaining derivatives (Grabmeier et al, 2003) (Fig. 2). It is carried out by computer algebra packages that implement differentiation rules such as\nd\ndx (f(x) + g(x)) =\nd\ndx f(x) +\nd\ndx g(x) or\nd\ndx (f(x) g(x)) =\n( d\ndx f(x)\n) g(x) + f(x) ( d\ndx g(x)\n) . (3)\nWhen formulae are represented as data structures, symbolically differentiating an expression tree is a perfectly mechanistic process, already considered subject to mechanical automation at the very inception of calculus (Leibniz, 1685). This is realized in modern computer algebra systems such as Mathematica, Maple, and Maxima.\nIn optimization, symbolic differentiation can give valuable insight into the structure of the problem domain and, in some cases, produce analytical solutions of extrema (e.g. ddxf(x) = 0) that can eliminate the calculation of derivatives altogether. On the other hand, symbolic derivatives do not lend themselves to efficient run-time calculation of derivative values, as they can be exponentially larger than the expression whose derivative they represent.\nConsider a function h(x) = f(x)g(x) and the multiplication rule in Eq. 3. Since h is a product, h(x) and ddxh(x) have some common components (namely f(x) and g(x)). Notice also that on the right hand side, f(x) and ddxf(x) appear separately. If we just proceed to symbolically differentiate f(x) and plug its derivative into the appropriate place, we will have nested duplications of any computation that appears in common between f(x) and ddxf(x). Hence, careless symbolic differentiation can easily produce exponentially large symbolic expressions which take correspondingly long to evaluate. This problem is known as expression swell (Table 1).\nWhen we are concerned with the accurate computation of derivative values and not so much with their actual symbolic form, it is in principle possible to simplify computations by storing values of intermediate subexpressions in memory. Moreover, for further efficiency, we can interleave as much as possible the differentiating and simplifying steps.\n4 64x(1 \u2212 x)(1 \u2212 2x)2 (1\u2212 8x+ 8x2)2 128x(1 \u2212 x)(\u22128 + 16x)(1 \u2212 2x)2(1 \u2212 8x + 8x2) + 64(1 \u2212 x)(1\u2212 2x)2(1\u2212 8x+ 8x2)2 \u2212 64x(1\u22122x)2(1\u22128x+8x2)2\u2212 256x(1\u2212 x)(1\u2212 2x)(1\u2212 8x+ 8x2)2 64(1\u221242x+504x2\u22122640x3+ 7040x4 \u2212 9984x5 + 7168x6 \u2212 2048x7)\nThis \u201cinterleaving\u201d idea forms the basis of AD and provides an account of its simplest form: apply symbolic differentiation in elementary operations level and keep intermediate numerical results, in lockstep with the evaluation of the main function."}, {"heading": "3 Preliminaries", "text": "In its most basic description, AD relies on the fact that all computations are ultimately compositions of a finite set of elementary operations for which derivatives are known (Verma, 2000). Combining the derivatives of constituent operations through the chain rule gives the derivative of the overall composition. Usually, these elementary operations include the binary operations + and \u00d7, the unary sign switch \u2212, the reciprocal, and the standard univariate functions such as exp, sin and the like.\nOn the left hand side of Table 2 we see the representation of the computation y = f(x1, x2) = ln(x1) + x1x2 \u2212 sin(x2) as an evaluation trace of elementary operations\u2014also called a Wengert list (Wengert, 1964). We adopt the \u201cthree-part notation\u201d used by Griewank and Walther (2008), where a function f : Rn \u2192 Rm is constructed using intermediate variables vi such that\n\u2013 variables vi\u2212n = xi, i = 1, . . . , n are the input variables, \u2013 variables vi i = 1, . . . , l are the working variables, and \u2013 variables ym\u2212i = vl\u2212i, i = m\u2212 1, . . . , 0 are the output variables.\nA given trace of elementary operations can be also represented using a computational graph (Bauer, 1974), as shown in Fig. 4. Such graphs are useful in visualizing dependency relations between intermediate variables.\nEvaluation traces form the basis of the AD technique. An important point to note here is that any numeric code will eventually be run\u2014or evaluated\u2014 as a trace, with particular input values and the resulting output. Thus, AD\ncan differentiate not only mathematical expressions in the classical sense, but also algorithms making use of control flow statements, loops, and procedure calls. This gives AD an important advantage over symbolic differentiation which can only be applied after arranging code into closed-form mathematical expressions.\n3.1 Forward mode\nForward mode6 is conceptually the most simple type of AD.\nConsider the evaluation trace of the function f(x1, x2) = ln(x1) + x1x2 \u2212 sin(x2) given on the left hand side of Table 2 and in graph form in Fig. 4. For computing the derivative of f with respect to x1, we start by associating with each intermediate variable vi a derivative\nv\u0307i = \u2202vi \u2202x1 .\nApplying the chain rule to each elementary operation in the forward evaluation trace, we generate the corresponding derivative trace, given on the right hand side of Table 2. Evaluating variables vi one by one together with their corresponding v\u0307i values gives us the required derivative in the final variable v\u03075 =\n\u2202y \u2202x1 .\nThis generalizes naturally to computing the Jacobian of a function f : Rn \u2192 Rm with n independent variables xi and m dependent variables yj . In this case, each forward pass of AD is initialized by setting only one of the variables x\u0307i = 1 (in other words, setting x\u0307 = ei, where ei is the i-th unit vector). A run of the code with specific input values x = a would then compute\ny\u0307j = \u2202yj \u2202xi \u2223\u2223\u2223\u2223 x=a , j = 1, . . . ,m ,\n6 Also called tangent linear mode.\ngiving us one column of the Jacobian matrix\nJf =  \u2202y1 \u2202x1 \u00b7 \u00b7 \u00b7 \u2202y1\u2202xn ... . . .\n... \u2202ym \u2202x1 \u00b7 \u00b7 \u00b7 \u2202ym\u2202xn  \u2223\u2223\u2223\u2223\u2223\u2223\u2223 x = a\nevaluated at point a. Thus, the full Jacobian can be computed in n evaluations. Furthermore, forward mode AD provides a very efficient and matrix-free way of computing Jacobian-vector products\nJf r =  \u2202y1 \u2202x1 \u00b7 \u00b7 \u00b7 \u2202y1\u2202xn ... . . .\n... \u2202ym \u2202x1 \u00b7 \u00b7 \u00b7 \u2202ym\u2202xn  r1... rn  , (4) simply by initializing with x\u0307 = r. Thus, we can compute the Jacobian-vector product in just one forward pass. Similarly, for a function f : Rn \u2192 R, we can obtain the directional derivative along a given vector r as a linear combination of the partial derivatives\n\u2207f \u00b7 r\nvia starting the AD computation with the values x\u0307 = r. Forward mode AD is efficient and straightforward for functions f : R \u2192 Rm, as all the derivatives \u2202yi\u2202x can be computed with just one forward pass. Conversely, in the other extreme of f : Rn \u2192 R, forward mode AD would require n evaluations to compute the gradient\n\u2207f = ( \u2202y\n\u2202x1 , . . . ,\n\u2202y\n\u2202xn\n) .\nIn general, for cases f : Rn \u2192 Rm where n m, a superior technique, the reverse mode AD is preferred."}, {"heading": "3.1.1 Dual numbers", "text": "A common way to handle the paired operations in forward mode AD (represented by the left and right hand sides in Table 2) is through the use of dual numbers7, which are defined as formal truncated Taylor series of the form\nx+ x\u2032 .\nDefining arithmetic on dual numbers by 2 = 0 and by interpreting any non-dual number y as y + 0 , we get entities such as\n(x+ x\u2032 ) + (y + y\u2032 ) = (x+ y) + (x\u2032 + y\u2032) ,\n(x+ x\u2032 )(y + y\u2032 ) = (xy) + (xy\u2032 + x\u2032y) ,\nin which the coefficients of conveniently mirror symbolic differentiation rules (e.g. Eq. 3). We can utilize this by setting up a regime where\nf(x+ x\u2032 ) = f(x) + f \u2032(x)x\u2032 (5)\nand using dual numbers as data structures for carrying the derivative together with the undifferentiated value. The chain rule works as expected on this representation. For example, two applications of Eq. 5 give\nf(g(x+ x\u2032 )) = f(g(x) + g\u2032(x)x\u2032 )\n= f(g(x)) + f \u2032(g(x))g\u2032(x)x\u2032 ,\nwhere the coefficient of on the right hand side is exactly the derivative of the composition of f and g. This means that since we implement elementary operations to respect the invariant Eq. 5, all compositions of them will also do so. This, in turn, means that we can extract the derivative of a function of interest by evaluating it in this nonstandard way on an initial input with a coefficient 1 for :\ndf(x)\ndx \u2223\u2223\u2223\u2223 x = epsilon-coefficient(dual-version(f)(x+ 1 )) .\nThis also extends to arbitrary program constructs, since dual numbers, as data types, can be contained in any data structure. As long as no arithmetic is done on the dual number, it will just remain a dual number; and if it is taken out of the data structure and operated on again, then the differentiation will continue. If we look at it from the point of view of the dual number, AD amounts to partially evaluating f with respect to that input to derive a symbolic expression, symbolically differentiating that expression, and collapsing the result (in a particular way) to make sure it is not too big; all interleaved to prevent intermediate expression swells.\nIn practice, a function f coded in a programming language of choice would be fed into an AD tool, which would then augment it with corresponding extra code to handle the dual operations, so that the function and its derivative are\n7 First introduced by Clifford (1873), with important uses in linear algebra and physics.\nsimultaneously computed. This can be implemented through calls to a specific library; in the form of source transformation where a given source code will be automatically modified; or through operator overloading, making the process transparent to the user. We cover the implementation techniques in Sect. 5.\n3.2 Reverse mode\nLike its famous cousin backpropagation, reverse mode8 AD works by propagating derivatives backward from a given output. It does this by supplementing each intermediate variable vi with an adjoint\nv\u0304i = \u2202yj \u2202vi ,\nwhich represents the sensitivity of a considered output yj with respect to changes in vi.\nDerivatives are then computed using what is essentially a two stage process. In the first stage, the original function code is run forward, populating intermediate variables vi and keeping track of the dependencies in the computational graph. In the second stage, derivatives are calculated by propagating adjoints v\u0304i in reverse, from the outputs to the inputs.\nReturning to the example y = f(x1, x2) = ln(x1)+x1x2\u2212sin(x2), in Table 3 we see the adjoint statements on the right hand side, corresponding to each original elementary operation on the left. In simple terms, we are interested in computing the contribution v\u0304i =\n\u2202y \u2202vi of the change in each variable vi on the\nchange in the output y. Taking the variable v0 as an example, we see (Fig. 4) that the only ways it can affect y are through v2 and v3, so its contribution to the change in y is given by\n\u2202y\n\u2202v0 =\n\u2202y\n\u2202v2 \u2202v2 \u2202v0 + \u2202y \u2202v3 \u2202v3 \u2202v0 , or\nv\u03040 = v\u03042 \u2202v2 \u2202v0 + v\u03043 \u2202v3 \u2202v0 .\nIn Table 3, this contribution is computed in two incremental steps\nv\u03040 = v\u03043 \u2202v3 \u2202v0 and v\u03040 = v\u03040 + v\u03042 \u2202v2 \u2202v0 ,\ngrouped with the line in the original trace from which it originates. After the forward sweep on the left hand side, we run the reverse sweep of the adjoints on the right hand side, starting with v\u03045 = y\u0304 = \u2202y \u2202y = 1. In the end we get the derivatives \u2202y\u2202x1 = x\u03041 and \u2202y \u2202x2 = x\u03042 in just one reverse sweep.\n8 Also called adjoint or cotangent linear mode.\nCompared with the straightforward simplicity of the forward mode, reverse mode AD can, at first, appear somewhat \u201cmysterious\u201d (Dennis and Schnabel, 1996). Griewank and Walther (2008) argue that this is in part because of the common acquaintance with the chain rule as a mechanical procedure propagating derivatives forward.\nAn important advantage of the reverse mode is that it is significantly less costly to evaluate than the forward mode for functions with a large number of input variables. In the extreme case of f : Rn \u2192 R, only one application of the reverse mode is sufficient to compute the full gradient \u2207f = ( \u2202y \u2202x1 , . . . , \u2202y\u2202xn ) , compared with the n sweeps of the forward mode needed for the same. In general, for a function f : Rn \u2192 Rm, if we denote the time it takes to evaluate the original function by time(f), the time it takes to calculate the the m \u00d7 n Jacobian by the forward mode is O(n time(f)), whereas the same computation can be done via reverse mode in O(m time(f)). That is to say, reverse mode AD performs better when m n.\nSimilar to the matrix-free computation of Jacobian-vector products with the forward mode (Eq. 4), the reverse mode can be used for computing the transposed Jacobian-vector product\nJ\u1d40f r =  \u2202y1 \u2202x1 \u00b7 \u00b7 \u00b7 \u2202ym\u2202x1 ... . . .\n... \u2202y1 \u2202xn \u00b7 \u00b7 \u00b7 \u2202ym\u2202xn   r1... rm  , by initializing the reverse stage with y\u0304 = r.\nThe advantages of reverse mode AD, however, come with the cost of increased storage requirements growing in proportion to the number of oper-\nations in the evaluated function. It is an active area of research to improve storage requirements in implementations, by methods such as checkpointing strategies and data-flow analysis (Dauvergne and Hascoe\u0308t, 2006).\n3.3 Origins of AD and backpropagation\nIdeas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD9.\nReverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation.\nIncidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation. Within the machine learning community, the method has been reinvented several times, such as by Parker (1985), until it was eventually brought to fame by Rumelhart et al (1986) of the Parallel Distributed Processing (PDP) group. The PDP group became aware of Parker\u2019s work only after their own discovery, and similarly, Werbos\u2019 work was not appreciated until it was found by Parker.\nThis tells us an interesting story of two highly interconnected research communities that have somehow also managed to stay detached during this foundational period."}, {"heading": "4 Derivatives and machine learning", "text": "Let us examine the main uses of derivatives in machine learning and how these can benefit from the use of AD.\nClassically, the main tasks in machine learning where the computation of derivatives is relevant have included optimization, various models of regression analysis (Draper and Smith, 1998), neural networks (Widrow and Lehr, 1990), clustering, computer vision, and parameter estimation.\n9 For a thorough review of the development of AD, we advise readers to refer to Rall (2006). Also see Griewank (2012) for an investigation of the origins of the reverse mode.\n4.1 Gradient methods\nGiven an objective function f : Rn \u2192 R, classical gradient descent has the goal of finding (local) minima w\u2217 = arg minw f(w) via updates \u2206w = \u2212\u03b7\u2207f , where \u03b7 is a step size. Gradient methods make use of the fact that f decreases steepest if one goes in the direction of the negative gradient. Naive gradient descent comes with asymptotic rate of convergence, where the method increasingly \u201czigzags\u201d towards the minimum in a slowing down fashion. Convergence rate is usually improved by adaptive step size techniques that adjust the step size \u03b7 on every iteration (Snyman, 2005).\nAs we have seen, for large n, reverse mode AD provides a highly efficient method for computing gradients10. In Fig. 5 and Table 4, we demonstrate how gradient methods can benefit from AD, looking at the example of Helmholtz free energy function that has been used in AD literature (Griewank, 1989; Griewank and Walther, 2008) for benchmarking gradient calculations.\nSecond-order methods based on Newton\u2019s method make use of both the gradient \u2207f and the Hessian Hf , working via updates \u2206w = \u2212\u03b7H\u22121f \u2207f . Newton\u2019s method converges in fewer iterations, but this comes with the cost of computing Hf in each step (Press et al, 2007). Due to its computational cost, the Hessian is usually replaced by a numerical approximation using updates from gradient evaluations, giving rise to quasi-Newton methods. A highly pop-\n10 See http://gbaydin.github.io/DiffSharp/examples-gradientdescent.html for an example of AD-based gradient descent using the DiffSharp library.\nf(x) = RT\n\u2211n\ni=0 log xi 1\u2212bTx \u2212 xTAx\u221a 8bTx log 1+(1+\n\u221a 2)bTx\n1+(1\u2212 \u221a 2)bTx , where R is the universal gas con-\nstant, T is the absolute temperature, b \u2208 Rn is a vector of constants, A \u2208 Rn\u00d7n is a symmetric matrix of constants, and x \u2208 Rn is the vector of independent variables describing the system. The plots show the evaluation time of f and the gradient \u2207f with numerical differentiation (central difference), forward mode AD, and reverse mode AD, as a function of the number of variables n. Reported times are relative to the evaluation time of f with n = 1. Lower figure shows the data with a logarithmic scale for illustrating the behavior when n < 20. Numerical results are given in Table 4. (Code available online: http://gbaydin.github.io/DiffSharp/examples-helmholtzenergyfunction.html)\nular such method is the BFGS11 algorithm, together with its limited-memory variant L-BFGS (Dennis and Schnabel, 1996).\nAD here provides a way of computing the exact Hessian in an efficient way12. However, in many cases, one does not need the full Hessian but only a Hessian-vector product H r, which can be computed very efficiently using a combination of the forward and reverse modes of AD13. This computes H r with O(n) complexity, even though the H is a n\u00d7n matrix. Moreover, Hessians arising in large-scale applications are typically sparse. This sparsity, along with symmetry, can be readily exploited by AD techniques such as computational graph elimination (Dixon, 1991), partial separability (Gay, 1996), and matrix coloring and compression (Gebremedhin et al, 2009).\nAnother approach for improving the rate of convergence of gradient methods is to use gain adaptation methods such as stochastic meta-descent (SMD) (Schraudolph, 1999), where stochastic sampling is introduced to avoid local minima. An example using SMD with AD Hessian-vector products is given by Vishwanathan et al (2006) on conditional random fields (CRF), a probabilistic method for labeling and segmenting data. Similarly, Schraudolph and Graepel (2003) use Hessian-vector products in their model combining conjugate gradient techniques with stochastic gradient descent.\n4.2 Neural networks\nTraining of neural networks is an optimization problem with respect to a set of weights, which can in principle be addressed via any method including gradient descent, stochastic gradient descent (Zhenzhen and Elhanany, 2007), or BFGS (Apostolopoulou et al, 2009). As we have seen, the highly successful backpropagation algorithm is only a specialized version of reverse mode AD: by applying the reverse mode to any algorithm evaluating a network\u2019s error as a function of its weights, we can readily compute the partial derivatives needed for performing weight updates14.\nThere are instances in neural network literature\u2014albeit few\u2014where explicit reference is made to AD for computing error gradients, such as Eriksson et al (1998) using AD for large-scale feed-forward networks, and the work by Yang et al (2008), where they use AD to train a neural network-based proportional-integral-derivative (PID) controller. Similarly, Rollins (2009) uses reverse mode AD in conjunction with neural networks for the problem of optimal feedback control.\n11 After Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno, who independently discovered the method in the 1970s. 12 See http://gbaydin.github.io/DiffSharp/examples-newtonsmethod.html for an implementation of Newton\u2019s method with the full Hessian. 13 For example, by applying the reverse mode to gradient code already produced through the forward mode. That is, given the function f : Rn \u2192 R, the evaluation point x, and the vector r, first computing the directional derivative \u2207f \u00b7 r through the forward mode via setting x\u0307 = r and then applying the reverse mode on this result to get \u22072f \u00b7 r. 14 See http://gbaydin.github.io/DiffSharp/examples-neuralnetworks.html for an implementation of backpropagation with reverse mode AD.\nBeyond backpropagation, the generality of AD also opens up other possibilities. An example is given for continuous time recurrent neural networks (CTRNN) by Al Seyab and Cao (2008), where they apply AD for the training of CTRNNs predicting dynamic behavior of nonlinear processes in real time. The authors use AD for computing derivatives higher than second-order and report significantly reduced network training time compared with other methods.\n4.3 Computer vision and image processing\nIn image processing, first- and second-order derivatives play an important role in tasks such as edge detection and sharpening (Russ, 2010). However, in most applications, these fundamental operations are applied on discrete functions of integer image coordinates, approximating those derived on a hypothetical continuous image space. As a consequence, derivatives are approximated using numerical differences.\nOn the other hand, in computer vision, many problems are formulated as the minimization of an appropriate energy functional (Bertero et al, 1988; Chambolle, 2000). This minimization is usually accomplished via calculus of variations and the Euler-Lagrange equation. In this area, the first study introducing AD to computer vision is given by Pock et al (2007), where they address the problems of denoising, segmentation, and recovery of information from stereoscopic image pairs, and note the usefulness of AD in identifying sparsity patterns in large Jacobian and Hessian matrices.\nIn another study, Grabner et al (2008) use reverse mode AD for GPUaccelerated medical 2D/3D registration, a task involving the alignment of data from different sources such as X-ray images or computed tomography. The authors report a six-fold increase in speed compared with numerical differentiation using center difference (cf. our benchmark with the Helmholtz function, Fig. 5 and Table 4), demonstrating that the computer vision field is ripe for AD applications.\nBarrett and Siskind (2013) present a use of AD for the task of video event detection. Compared with general computer vision tasks focused on recognizing objects and their properties (which can be thought of as nouns in a narrative), an important aspect of this work is that it aims to recognize and reason about events and actions (i.e. verbs). The method uses Hidden Markov Models (HMMs) and Dalal and Triggs (2005) object detectors, and performs training on a corpus of pre-tracked video by an adaptive step size naive gradient descent algorithm, where gradient computations are done with reverse mode AD. Initially implemented with the R6RS-AD package15 providing forward and reverse mode AD in R6RS Scheme, the gradient code was later ported to C and highly optimized. Even if the final detection code does not di-\n15 https://github.com/qobi/R6RS-AD\nrectly use AD, the authors report16 that AD in this case served as a foundation and a correctness measure for validating subsequent work.\n4.4 Natural language processing\nWithin the natural language processing (NLP) field, statistical models are commonly trained using general purpose or specialized gradient methods and mostly remain expensive to train. Improvements in training time can be realized by using online or distributed training algorithms (Gimpel et al, 2010). An example using stochastic gradient descent for NLP is given by Finkel et al (2008) optimizing conditional random field parsers through an objective function. Related with the work on video event detection in the previous section, Yu and Siskind (2013) report their work on sentence tracking, representing an instance of grounded language learning paired with computer vision, where the system learns word meanings from short video clips paired with descriptive sentences. The method works by using HMMs to represent changes in video frames and meanings of different parts of speech. This work is implemented in C and computes the required gradients using AD through the ADOL-C tool17.\n4.5 Probabilistic programming and Bayesian methods\nProbabilistic programming has been experiencing a recent resurgence thanks to new algorithmic advances for probabilistic inference and new areas of application in machine learning (Goodman, 2013). A probabilistic programming language provides primitive language constructs for random choice and allows the automatic probabilistic inference of distributions specified by programs.\nInference techniques can be static, such as compiling programs to Bayesian networks and using algorithms such as belief propagation for inference; or they can be dynamic, executing programs several times and computing statistics on observed values to infer distributions. Markov chain Monte Carlo (MCMC) methods are typically used for dynamic inference, such as the MetropolisHastings algorithm based on random sampling. Meyer et al (2003) give an example of how AD can be used for speeding up Bayesian posterior inference in MCMC, with an application in stochastic volatility.\nWhen model parameters are continuous, the Hamiltonian\u2014or, hybrid\u2014 Monte Carlo (HMC) algorithm provides improved convergence characteristics avoiding the slow exploration of random sampling, by simulating Hamiltonian dynamics through auxiliary \u201cmomentum variables\u201d (Neal, 1993).\nThe advantages of HMC come at the cost of requiring gradient evaluations of complex probability models. AD is highly suitable here for complementing probabilistic programming, because it relieves the user from the manual\n16 Through personal communication. 17 An implementation of the sentence tracker applied to video search using sentence-\nbased queries can be accessed online: http://upplysingaoflun.ecn.purdue.edu/~qobi/ cccp/sentence-tracker-video-retrieval.html\ncomputation of derivatives for each model. For instance, the probabilistic programming language Stan18 implements automatic Bayesian inference based on HMC and the No-U-Turn sampler (NUTS) (Hoffman and Gelman, 2014) and uses reverse mode AD for the calculation of gradients for both HMC and NUTS. Similarly, Wingate et al (2011) demonstrate the use of AD as a nonstandard interpretation of probabilistic programs enabling efficient inference algorithms.\nAD is particularly promising in this domain because of the dynamic nature of probabilistic programs, that is, dynamically creating or deleting random variables and making it very difficult to formulate closed-form expressions for gradients."}, {"heading": "5 Implementations", "text": "For picking the best tool for a particular application, it is useful to have an understanding of the different ways in which AD can be implemented. Here we cover major implementation strategies and provide a survey of existing tools.\nA principal consideration in any AD implementation is the performance overhead introduced by the AD arithmetic and bookkeeping. In terms of computational complexity, AD ensures that the amount of arithmetic goes up by no more than a small constant. For instance, for the reverse mode, the extra arithmetic corresponds to at most four or five times the arithmetic needed to evaluate the original function19. But, managing this arithmetic can introduce a significant overhead if done carelessly. For instance, naively allocating data structures for holding dual numbers will involve memory access and allocation for every arithmetic operation, which are usually more expensive than arithmetic operations on modern computers. Likewise, using operator overloading may introduce method dispatches with attendant costs, which, compared to raw numerical computation of the original function, can easily amount to a slowdown of an order of magnitude.\nAnother major issue is the possibility of a class of bugs called \u201cperturbation confusion\u201d (Siskind and Pearlmutter, 2005). This essentially means that if two ongoing differentiations affect the same piece of code, the two formal epsilons they introduce (Sect. 3.1.1) need to be kept distinct. It is very easy to have bugs\u2014in particularly performance-oriented AD implementations\u2014that confuse these in various ways. Such situations can also arise when AD is nested, that is, derivatives are computed for functions that internally take derivatives.\nOne should be also cautious about approximated functions and AD. In this case, if you have a procedure approximating an ideal function, AD always gives the derivative of the procedure that was actually programmed, which may not be a good approximation of the derivative of the ideal function that\n18 http://mc-stan.org/ 19 See Griewank and Walther (2008) for a detailed analysis of forward and reverse mode\ncomplexity bounds.\nthe procedure was approximating20. Users of AD implementations must be therefore cautious to approximate the derivative, not differentiate the approximation. This would require explicitly approximating a known derivative, in cases where a mathematical function can only be computed approximately but has a well-defined mathematical derivative.\nIn conjunction with Table 5, we present a review of notable AD implementations21. A thorough taxonomy of implementation techniques was introduced by Juedes (1991), which was later revisited by Bischof et al (2008) and simplified into elemental, operator overloading, compiler-based, and hybrid methods. We adopt a similar classification for briefly presenting the currently popular tools.\n5.1 Elemental libraries\nThese implementations form the most basic category and work by replacing mathematical operations with calls to an AD-enabled library. Methods exposed by the library are then used in function definitions, meaning that the decomposition of any function into elementary operations is done manually at the same time with writing the code.\nThe approach has been utilized since the early days of AD, prototypical examples being the WCOMP and UCOMP packages of Lawson (1971), the APL package of Neidinger (1989), and the work by Hinkins (1994). Likewise, Hill and Rich (1992) formulate their implementation of AD in MATLAB using elemental methods.\nElemental methods still constitute the simplest strategy to implement AD for languages without operator loading.\n5.2 Compilers and source transformation\nThese implementations provide extensions to programming languages that automate the decomposition of equations into AD-enabled elementary operations. They are typically executed as preprocessors22 to transform the input in the extended language into the original language.\nClassical instances of source code transformation include the Fortran preprocessors GRESS (Horwedel et al, 1988) and PADRE2 (Kubo and Iri, 1990), which transform AD-enabled variants of Fortran into standard Fortran 77 before compiling. Similarly, the ADIFOR tool by Bischof et al (1996), given a\n20 As an example, consider ex computed by a piecewise-rational approximation routine. Using AD on this routine would produce an approximated derivative in which each piece of the piecewise formula will get differentiated. Even if this would remain an approximation of the derivative of ex, we know that de x\ndx = ex and the original approximation itself was\nalready a better approximation for the derivative of ex. In modern computers this is not an issue, because ex is a primitive implemented in hardware. 21 Also see the website http://www.autodiff.org/ for a list of tools maintained by the AD community. 22 Preprocessors transform program source code before it is given as an input to a compiler.\nAutomatic differentiation in machine learning: a survey 21 T a b le 5 S u rv ey o f m a jo r A D im p le m en ta ti o n s. L a n g u a g e T o o l T y p e M o d e In st it u ti o n / P ro je c t R e fe re n c e s U R L A M P L A M P L IN T F , R B e ll L a b o ra to ri e s F o u re r e t a l (2 0 0 2 ) h t t p : / / w w w . a m p l . c o m / C , C + + A D IC S T F , R A rg o n n e N a ti o n a l L a b o ra to ry B is ch o f e t a l (1 9 9 7 ) h t t p : / / w w w - n e w . m c s . a n l . g o v / a d i c / d o w n - 2 . h t m A D O L -C O O F , R C o m p u ta ti o n a l In fr a st ru c tu re fo r O p e ra ti o n s R e se a rc h W a lt h e r a n d G ri e w a n k (2 0 1 2 ) h t t p : / / w w w . c o i n - o r . o r g / p r o j e c t s / A D O L - C . x m l C + + C e re s S o lv e r L IB F G o o g le h t t p : / / c e r e s - s o l v e r . o r g / C p p A D O O F , R C o m p u ta ti o n a l In fr a st ru c tu re fo r O p e ra ti o n s R e se a rc h B e ll a n d B u rk e (2 0 0 8 ) h t t p : / / w w w . c o i n - o r . o r g / C p p A D / F A D B A D + + O O F , R T e ch n ic a l U n iv e rs it y o f D e n m a rk B e n d ts e n a n d S ta u n in g (1 9 9 6 ) h t t p : / / w w w . f a d b a d . c o m / f a d b a d . h t m l M x y z p tl k O O F F e rm i N a ti o n a l A c c e le ra to r L a b o ra to ry O st ig u y a n d M ic h e lo tt i (2 0 0 7 ) h t t p s : / / c d c v s . f n a l . g o v / r e d m i n e / p r o j e c t s / f e r m i t o o l s / w i k i / M X Y Z P T L K C # A u to D iff L IB R G e o rg e M a so n U n iv ., D e p a rt m e n t o f C o m p u te r S c ie n c e S h to f e t a l (2 0 1 3 ) h t t p : / / a u t o d i f f . c o d e p l e x . c o m / F # D iff S h a rp L IB F , R N a ti o n a l U n iv e rs it y o f Ir e la n d M a y n o o th h t t p : / / g b a y d i n . g i t h u b . i o / D i f f S h a r p / F o r t r a n A D IF O R S T F , R A rg o n n e N a ti o n a l L a b o ra to ry B is ch o f e t a l (1 9 9 6 ) h t t p : / / w w w . m c s . a n l . g o v / r e s e a r c h / p r o j e c t s / a d i f o r / N A G W a re C O M F , R N u m e ri c a l A lg o ri th m s G ro u p N a u m a n n a n d R ie h m e (2 0 0 5 ) h t t p : / / w w w . n a g . c o . u k / n a g w a r e / R e s e a r c h / a d _ o v e r v i e w . a s p T A M C S T R M a x P la n ck In st it u te fo r M e te o ro lo g y G ie ri n g a n d K a m in sk i (1 9 9 8 ) h t t p : / / a u t o d i f f . c o m / t a m c / F o r t r a n , C / C + + C O S Y IN T F M ic h ig a n S ta te U n iv ., B io m e d ic a l a n d P h y si c a l S c ie n c e s B e rz e t a l (1 9 9 6 ) h t t p : / / w w w . b t . p a . m s u . e d u / i n d e x _ c o s y . h t m T a p e n a d e S T F , R IN R IA S o p h ia -A n ti p o li s H a sc o e\u0308 t a n d P a sc u a l (2 0 1 3 ) h t t p : / / w w w - s o p . i n r i a . f r / t r o p i c s / t a p e n a d e . h t m l H a s k e ll a d O O F , R H a sk e ll p a ck a g e h t t p : / / h a c k a g e . h a s k e l l . o r g / p a c k a g e / a d J a v a D e ri v a L IB F J a v a & C lo ju re li b ra ry h t t p s : / / g i t h u b . c o m / l a m b d e r / D e r i v a M A T L A B A D iM a t S T , O O F , R T e ch n ic a l U n iv e rs it y o f D a rm st a d t, S c ie n ti fi c C o m p u ti n g W il lk o m m a n d V e h re sc h il d (2 0 1 3 ) h t t p : / / a d i m a t . s c . i n f o r m a t i k . t u - d a r m s t a d t . d e / IN T L a b O O F H a m b u rg U n iv e rs it y o f T e ch n o lo g y , In st it u te fo r R e li a b le C o m p u ti n g R u m p (1 9 9 9 ) h t t p : / / w w w . t i 3 . t u - h a r b u r g . d e / r u m p / i n t l a b / T O M L A B / M A D O O F C ra n fi e ld U n iv e rs it y & T o m la b O p ti m iz a ti o n In c . F o rt h (2 0 0 6 ) h t t p : / / t o m l a b . b i z / p r o d u c t s / m a d P y t h o n a d O O R P y th o n p a ck a g e h t t p s : / / p y p i . p y t h o n . o r g / p y p i / a d S c h e m e S c m u ti ls O O F M IT C o m p u te r S c ie n c e a n d A rt ifi c ia l In te ll ig e n c e L a b . S u ss m a n a n d W is d o m (2 0 0 1 ) h t t p : / / g r o u p s . c s a i l . m i t . e d u / m a c / u s e r s / g j s / 6 9 4 6 / r e f m a n . t x t F : F o rw a rd , R : R e v e rs e ; C O M : C o m p il e r, IN T : In te rp re te r, L IB : L ib ra ry , O O : O p e ra to r o v e rl o a d in g , S T : S o u rc e tr a n sf o rm a ti o n\nFortran source code, generates an augmented code in which all specified partial derivatives are computed in addition to the original result. For procedures coded in ANSI C, the ADIC tool (Bischof et al, 1997) implements AD as a source transformation after the specification of dependent and independent variables. A recent and popular tool also utilizing this approach is Tapenade (Pascual and Hascoe\u0308t, 2008; Hascoe\u0308t and Pascual, 2013), implementing forward and reverse mode AD for Fortran and C programs. Tapenade itself is implemented in Java and can be run locally or as an online service23.\nIn addition to language extensions through source code transformation, there are implementations introducing new languages with tightly integrated AD capability through special-purpose compilers or interpreters. Some of the earliest AD tools such as SLANG (Adamson and Winant, 1969) and PROSE (Pfeiffer, 1987) belong to this category. The NAGWare Fortran 95 compiler (Naumann and Riehme, 2005) is a more recent example, where the use of ADrelated extensions triggers automatic generation of derivative code at compile time.\nAs an example of interpreter-based implementation, the algebraic modeling language AMPL (Fourer et al, 2002) enables one to express objectives and constraints in mathematical notation, from which the system deduces active variables and arranges the necessary AD computations. Other examples in this category include the FM/FAD package (Mazourik, 1991), based on the Algol-like DIFALG language, and the object-oriented COSY language (Berz et al, 1996) similar to Pascal.\nThe Stalingrad compiler (Pearlmutter and Siskind, 2008), working on the Scheme-based AD-aware VLAD language, also falls under this category. The newer DVL compiler24 is based on Stalingrad and uses a reimplementation of portions of the VLAD language.\n5.3 Operator overloading\nIn modern programming languages with polymorphic features, operator overloading provides the most straightforward way of implementing AD, exploiting the capability of redefining elementary operation semantics.\nA highly popular tool implemented with operator overloading in C++ is ADOL-C (Walther and Griewank, 2012). ADOL-C requires the use of ADenabled types for variables and works via recording arithmetic operators on variables in data structures called \u201ctapes\u201d, which can subsequently be \u201cplayed back\u201d during reverse mode AD computations. The Mxyzptlk package (Michelotti, 1990) is another example for C++ capable of computing arbitrary-order partial derivatives via forward propagation. The FADBAD++ library (Bendtsen and Stauning, 1996) implements AD for C++ using templates and oper-\n23 http://www-tapenade.inria.fr:8080/tapenade/index.jsp 24 https://github.com/axch/dysvunctional-language\nator overloading. For Python, the ad package25 uses operator overloading to compute first- and second-order derivatives.\nFor functional languages, examples include the AD routines within the Scmutils library26 for Scheme, the ad library27 for Haskell, and the recent DiffSharp library28 for F#."}, {"heading": "6 Conclusions", "text": "Given all its advantages, AD has remained remarkably underused by the machine learning community. We reason that this is mostly because it is poorly understood and frequently confused with the better known symbolic and numerical differentiation methods. In comparison, increasing awareness of AD in fields such as engineering design optimization (Hascoe\u0308t et al, 2003), computational fluid dynamics (Mu\u0308ller and Cusdin, 2005), climatology (Charpentier and Ghemires, 2000), and computational finance (Bischof et al, 2002) provide evidence for its maturity and efficiency, with benchmarks (Giles and Glasserman, 2006; Sambridge et al, 2007; Capriotti, 2011) reporting performance increases of several orders of magnitude.\nMore often than not, machine learning articles tend to present the calculation of analytical derivatives for novel models as an important technical feat, potentially taking up as much space as the main contribution. Needless to say, there are occasions where we are interested in obtaining more than just the numerical value of derivatives. Derivative expressions can be useful for analysis and offer an insight into the problem domain. However, for any non-trivial function of more than a handful of variables, analytic expressions for gradients or Hessians increase rapidly in complexity to render any interpretation unlikely.\nThe dependence on manual or symbolic differentiation impedes expressiveness of models by limiting the set of operations to those for which symbolic derivatives can be computed. Using AD, in contrast, enables us to build models using the full set of algorithmic machinery, knowing that we will be able to compute exact derivatives without handling parts of the model as closed-form expressions.\nAcknowledgements This work was supported in part by Science Foundation Ireland grant 09/IN.1/I2637.\n25 http://pythonhosted.org/ad/ 26 http://groups.csail.mit.edu/mac/users/gjs/6946/refman.txt 27 http://hackage.haskell.org/package/ad 28 http://gbaydin.github.io/DiffSharp/"}], "references": [{"title": "A SLANG simulation of an initially strong shock wave downstream of an infinite area change", "author": ["DS Adamson", "CW Winant"], "venue": "Proceedings of the Conference on Applications of Continuous-System Simulation Languages,", "citeRegEx": "Adamson and Winant,? \\Q1969\\E", "shortCiteRegEx": "Adamson and Winant", "year": 1969}, {"title": "Nonlinear system identification for predictive control using continuous time recurrent neural networks and automatic differentiation", "author": ["RK Al Seyab", "Y Cao"], "venue": "Journal of Process Control 18(6):568\u2013581,", "citeRegEx": "Seyab and Cao,? \\Q2008\\E", "shortCiteRegEx": "Seyab and Cao", "year": 2008}, {"title": "A memoryless BFGS neural network training algorithm", "author": ["MS Apostolopoulou", "DG Sotiropoulos", "IE Livieris", "P Pintelas"], "venue": "IEEE International Conference on Industrial Informatics,", "citeRegEx": "Apostolopoulou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Apostolopoulou et al\\.", "year": 2009}, {"title": "Felzenszwalb-Baum-Welch: Event detection by changing appearance", "author": ["DP Barrett", "JM Siskind"], "venue": null, "citeRegEx": "Barrett and Siskind,? \\Q2013\\E", "shortCiteRegEx": "Barrett and Siskind", "year": 2013}, {"title": "Computational graphs and rounding error", "author": ["FL Bauer"], "venue": "SIAM Journal on Numerical Analysis", "citeRegEx": "Bauer,? \\Q1974\\E", "shortCiteRegEx": "Bauer", "year": 1974}, {"title": "Programs for automatic differentiation for the machine BESM (in russian", "author": ["LM Beda", "LN Korolev", "NV Sukkikh", "TS Frolova"], "venue": null, "citeRegEx": "Beda et al\\.,? \\Q1959\\E", "shortCiteRegEx": "Beda et al\\.", "year": 1959}, {"title": "Algorithmic differentiation of implicit functions and optimal values", "author": ["BM Bell", "JV Burke"], "venue": "J (eds) Advances in Automatic Differentiation, Lecture Notes in Computational Science and Engineering,", "citeRegEx": "Bell and Burke,? \\Q2008\\E", "shortCiteRegEx": "Bell and Burke", "year": 2008}, {"title": "FADBAD, a flexible C++ package for automatic differentiation", "author": ["C Bendtsen", "O Stauning"], "venue": "Technical Report IMM-REP-1996-17, Department of Mathematical Modelling,", "citeRegEx": "Bendtsen and Stauning,? \\Q1996\\E", "shortCiteRegEx": "Bendtsen and Stauning", "year": 1996}, {"title": "Differential quadrature method in computational mechanics: A review", "author": ["CW Bert", "M Malik"], "venue": "Applied Mechanics Reviews 49,", "citeRegEx": "Bert and Malik,? \\Q1996\\E", "shortCiteRegEx": "Bert and Malik", "year": 1996}, {"title": "Ill-posed problems in early vision", "author": ["M Bertero", "T Poggio", "V Torre"], "venue": "Proceedings of the IEEE", "citeRegEx": "Bertero et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Bertero et al\\.", "year": 1988}, {"title": "COSY INFINITY and its applications in nonlinear dynamics", "author": ["M Berz", "K Makino", "K Shamseddine", "GH Hoffst\u00e4tter", "W Wan"], "venue": null, "citeRegEx": "Berz et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berz et al\\.", "year": 1996}, {"title": "ADIC: An extensible automatic differentiation tool for ANSI-C. Software Practice and Experience", "author": ["C Bischof", "L Roh", "A Mauer"], "venue": null, "citeRegEx": "Bischof et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bischof et al\\.", "year": 1997}, {"title": "Automatic differentiation for computational finance", "author": ["CH Bischof", "HM B\u00fccker", "B Lang"], "venue": "Computational Methods in DecisionMaking, Economics and Finance, Applied Optimization,", "citeRegEx": "Bischof et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bischof et al\\.", "year": 2002}, {"title": "On the implementation of automatic differentiation tools. Higher-Order and Symbolic Computation", "author": ["CH Bischof", "PD Hovland", "B Norris"], "venue": null, "citeRegEx": "Bischof et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bischof et al\\.", "year": 2008}, {"title": "Online learning and stochastic approximations. On-line learning in neural networks", "author": ["L Bottou"], "venue": null, "citeRegEx": "Bottou,? \\Q1998\\E", "shortCiteRegEx": "Bottou", "year": 1998}, {"title": "Extrapolation Methods: Theory and Practice", "author": ["C Brezinski", "MR Zaglia"], "venue": null, "citeRegEx": "Brezinski and Zaglia,? \\Q1991\\E", "shortCiteRegEx": "Brezinski and Zaglia", "year": 1991}, {"title": "A steepest ascent method for solving optimum programming problems", "author": ["Bryson AE Jr."], "venue": "Journal of Applied Mechanics", "citeRegEx": "Jr,? \\Q1962\\E", "shortCiteRegEx": "Jr", "year": 1962}, {"title": "Applied optimal control", "author": ["Bryson AE Jr.", "Ho YC"], "venue": null, "citeRegEx": "Jr and YC,? \\Q1969\\E", "shortCiteRegEx": "Jr and YC", "year": 1969}, {"title": "Fast greeks by algorithmic differentiation", "author": ["L Capriotti"], "venue": "Journal of Computational Finance", "citeRegEx": "Capriotti,? \\Q2011\\E", "shortCiteRegEx": "Capriotti", "year": 2011}, {"title": "Sensitivity analysis for atmospheric chemistry models via automatic differentiation", "author": ["GR Carmichael", "A Sandu"], "venue": "Atmospheric Environment", "citeRegEx": "Carmichael and Sandu,? \\Q1997\\E", "shortCiteRegEx": "Carmichael and Sandu", "year": 1997}, {"title": "Inverse problems in image processing and image segmentation: some mathematical and numerical aspects", "author": ["A Chambolle"], "venue": null, "citeRegEx": "Chambolle,? \\Q2000\\E", "shortCiteRegEx": "Chambolle", "year": 2000}, {"title": "Efficient adjoint derivatives: application to the meteorological model meso-nh. Optimization Methods and Software", "author": ["I Charpentier", "M Ghemires"], "venue": null, "citeRegEx": "Charpentier and Ghemires,? \\Q2000\\E", "shortCiteRegEx": "Charpentier and Ghemires", "year": 2000}, {"title": "Application of differentiation arithmetic", "author": ["GC Corliss"], "venue": "Perspectives in Computing,", "citeRegEx": "Corliss,? \\Q1988\\E", "shortCiteRegEx": "Corliss", "year": 1988}, {"title": "Histograms of oriented gradients for human detection", "author": ["N Dalal", "B Triggs"], "venue": "Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),", "citeRegEx": "Dalal and Triggs,? \\Q2005\\E", "shortCiteRegEx": "Dalal and Triggs", "year": 2005}, {"title": "The data-flow equations of checkpointing in reverse automatic differentiation", "author": ["B Dauvergne", "L Hasco\u00ebt"], "venue": "J (eds) Computational Science \u2013 ICCS", "citeRegEx": "Dauvergne and Hasco\u00ebt,? \\Q2006\\E", "shortCiteRegEx": "Dauvergne and Hasco\u00ebt", "year": 2006}, {"title": "Numerical Methods for Unconstrained Optimization and Nonlinear Equations", "author": ["JE Dennis", "RB Schnabel"], "venue": "Classics in Applied Mathematics,", "citeRegEx": "Dennis and Schnabel,? \\Q1996\\E", "shortCiteRegEx": "Dennis and Schnabel", "year": 1996}, {"title": "Use of automatic differentiation for calculating hessians and newton steps. In: Griewank A, Corliss GF (eds) Automatic Differentiation of Algorithms: Theory, Implementation, and Application", "author": ["LC Dixon"], "venue": null, "citeRegEx": "Dixon,? \\Q1991\\E", "shortCiteRegEx": "Dixon", "year": 1991}, {"title": "Arbitrary-order density functional response theory from automatic differentiation", "author": ["U Ekstr\u00f6m", "L Visscher", "R Bast", "AJ Thorvaldsen", "K Ruud"], "venue": "Journal of Chemical Theory and Computation 6:1971\u201380,", "citeRegEx": "Ekstr\u00f6m et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ekstr\u00f6m et al\\.", "year": 2010}, {"title": "Regularization tools for training large feed-forward neural networks using automatic differentiation", "author": ["J Eriksson", "M Gulliksson", "P Lindstr\u00f6m", "P Wedin"], "venue": "Optimization Methods and Software 10(1):49\u201369,", "citeRegEx": "Eriksson et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Eriksson et al\\.", "year": 1998}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["JR Finkel", "A Kleeman", "CD Manning"], "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Finkel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Numerical differentiation of analytic functions", "author": ["B Fornberg"], "venue": "ACM Transactions on Mathematical Software 7(4):512\u201326,", "citeRegEx": "Fornberg,? \\Q1981\\E", "shortCiteRegEx": "Fornberg", "year": 1981}, {"title": "An efficient overloaded implementation of forward mode automatic differentiation in MATLAB", "author": ["SA Forth"], "venue": "ACM Transactions on Mathematical Software", "citeRegEx": "Forth,? \\Q2006\\E", "shortCiteRegEx": "Forth", "year": 2006}, {"title": "AMPL: A Modeling Language for Mathematical Programming", "author": ["R Fourer", "DM Gay", "BW Kernighan"], "venue": null, "citeRegEx": "Fourer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Fourer et al\\.", "year": 2002}, {"title": "Automatically finding and exploiting partially separable structure in nonlinear programming problems", "author": ["DM Gay"], "venue": null, "citeRegEx": "Gay,? \\Q1996\\E", "shortCiteRegEx": "Gay", "year": 1996}, {"title": "Efficient computation of sparse hessians using coloring and automatic differentiation", "author": ["A Gebremedhin", "A Pothen", "A Tarafdar", "A Walther"], "venue": "INFORMS Journal on Computing", "citeRegEx": "Gebremedhin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gebremedhin et al\\.", "year": 2009}, {"title": "Recipes for adjoint code construction", "author": ["R Giering", "T Kaminski"], "venue": "ACM Transactions on Mathematical Software 24:437\u201374,", "citeRegEx": "Giering and Kaminski,? \\Q1998\\E", "shortCiteRegEx": "Giering and Kaminski", "year": 1998}, {"title": "Smoking adjoints: fast monte carlo greeks", "author": ["M Giles", "P Glasserman"], "venue": null, "citeRegEx": "Giles and Glasserman,? \\Q2006\\E", "shortCiteRegEx": "Giles and Glasserman", "year": 2006}, {"title": "Distributed asynchronous online learning for natural language processing", "author": ["K Gimpel", "D Das", "NA Smith"], "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Gimpel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2010}, {"title": "Pontryagin LS (1960) The theory of optimal processes I: The maximum principle", "author": ["VG Goltyanskii", "RV Gamkrelidze"], "venue": "Invest Akad Nauk SSSR Ser Mat", "citeRegEx": "Goltyanskii and Gamkrelidze,? \\Q1960\\E", "shortCiteRegEx": "Goltyanskii and Gamkrelidze", "year": 1960}, {"title": "The principles and practice of probabilistic programming", "author": ["ND Goodman"], "venue": "Proceedings of the 40th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages,", "citeRegEx": "Goodman,? \\Q2013\\E", "shortCiteRegEx": "Goodman", "year": 2013}, {"title": "Optimal convergence of on-line backpropagation", "author": ["M Gori", "M Maggini"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "Gori and Maggini,? \\Q1996\\E", "shortCiteRegEx": "Gori and Maggini", "year": 1996}, {"title": "Computer Algebra Handbook: Foundations", "author": ["J Grabmeier", "E Kaltofen", "VB Weispfenning"], "venue": null, "citeRegEx": "Grabmeier et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Grabmeier et al\\.", "year": 2003}, {"title": "Automatic differentiation for gpu-accelerated", "author": ["T Pock", "T Gross", "B Kainz"], "venue": null, "citeRegEx": "Grabner et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Grabner et al\\.", "year": 2008}, {"title": "The Tapenade Automatic Differentiation tool: Principles", "author": ["L Hasco\u00ebt", "V Pascual"], "venue": null, "citeRegEx": "Hasco\u00ebt and Pascual,? \\Q2013\\E", "shortCiteRegEx": "Hasco\u00ebt and Pascual", "year": 2013}, {"title": "Automatic differentiation for optimum design", "author": ["L Hasco\u00ebt", "M V\u00e1zquez", "A Dervieux"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "A (2014) The no-U-turn sampler: Adaptively setting path lengths", "author": ["Tech. rep", "Lawrence Berkeley Lab", "CA Hoffman MD", "Gelman"], "venue": null, "citeRegEx": "rep. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "rep. et al\\.", "year": 2014}, {"title": "Journal of Machine Learning Research 15:1593\u20131623", "author": ["Monte Carlo"], "venue": "Horwedel JE, Worley BA, Oblow EM, Pin FG", "citeRegEx": "Carlo.,? \\Q1988\\E", "shortCiteRegEx": "Carlo.", "year": 1988}, {"title": "PADRE2, version 1\u2014user\u2019s manual", "author": ["K Kubo", "M Iri"], "venue": "Industrial and Applied Mathematics, Philadelphia,", "citeRegEx": "Kubo and Iri,? \\Q1990\\E", "shortCiteRegEx": "Kubo and Iri", "year": 1990}, {"title": "On the performance of discrete adjoint cfd codes using automatic", "author": ["Laboratory", "Batavia", "IL M\u00fcller JD", "Cusdin P"], "venue": null, "citeRegEx": "Laboratory et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Laboratory et al\\.", "year": 2005}, {"title": "Computing adjoints with the NAGWare Fortran 95 compiler", "author": ["U Naumann", "J Riehme"], "venue": null, "citeRegEx": "Naumann and Riehme,? \\Q2005\\E", "shortCiteRegEx": "Naumann and Riehme", "year": 2005}, {"title": "Probabilistic inference using markov chain monte carlo methods", "author": ["R Neal"], "venue": "Science and Engineering,", "citeRegEx": "Neal,? \\Q1993\\E", "shortCiteRegEx": "Neal", "year": 1993}, {"title": "Analytical differentiation on a digital computer", "author": ["JF Nolan"], "venue": "Master\u2019s thesis Ostiguy JF, Michelotti L", "citeRegEx": "Nolan,? \\Q1953\\E", "shortCiteRegEx": "Nolan", "year": 1953}, {"title": "Reverse-mode AD in a functional framework: Lambda", "author": ["BA Pearlmutter", "JM Siskind"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Algorithmic differentiation: Application to variational", "author": ["T Pock", "M Pock", "H Bischof"], "venue": null, "citeRegEx": "Pock et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pock et al\\.", "year": 2007}, {"title": "Optimization of neural network feedback control systems using automatic", "author": ["E Rollins"], "venue": "Engineering,", "citeRegEx": "Rollins,? \\Q2009\\E", "shortCiteRegEx": "Rollins", "year": 2009}, {"title": "A general framework for parallel", "author": ["DE Rumelhart", "GE Hinton", "JL McClelland"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "The Image Processing Handbook", "author": ["M Sambridge", "P Rickwood", "N Rawlinson", "S Sommacal"], "venue": "JC", "citeRegEx": "Sambridge et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sambridge et al\\.", "year": 2010}, {"title": "Perturbation confusion and referential transparency", "author": ["At\u0131l\u0131m G\u00fcne\u015f Baydin"], "venue": "Siskind JM, Pearlmutter BA", "citeRegEx": "Baydin,? \\Q2005\\E", "shortCiteRegEx": "Baydin", "year": 2005}, {"title": "Optimization for Machine Learning", "author": ["S Sra", "S Nowozin", "SJ Wright"], "venue": "PhD thesis,", "citeRegEx": "Sra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sra et al\\.", "year": 2011}, {"title": "An introduction to automatic differentiation", "author": ["SVN Vishwanathan", "NN Schraudolph", "MW Schmidt", "KP Murphy"], "venue": "Current Science", "citeRegEx": "Vishwanathan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Vishwanathan et al\\.", "year": 2000}, {"title": "Getting started with ADOL-C", "author": ["A Walther", "A Griewank"], "venue": null, "citeRegEx": "Walther and Griewank,? \\Q2012\\E", "shortCiteRegEx": "Walther and Griewank", "year": 2012}, {"title": "Nonstandard interpretations", "author": ["ND Goodman", "A Stuhlm\u00fcller", "JM Siskind"], "venue": null, "citeRegEx": "Wingate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wingate et al\\.", "year": 2011}, {"title": "Application of PID controller based on BP neural", "author": ["W Yang", "Y Zhao", "L Yan", "X Chen"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2008}, {"title": "Grounded language learning from video described with sentences", "author": ["H Yu", "JM Siskind"], "venue": null, "citeRegEx": "Yu and Siskind,? \\Q2013\\E", "shortCiteRegEx": "Yu and Siskind", "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "When introducing new models, machine learning researchers spend considerable effort on the manual derivation of analytical derivatives and subsequently plug these into standard optimization procedures such as L-BFGS (Zhu et al, 1997) or stochastic gradient descent (Bottou, 1998).", "startOffset": 265, "endOffset": 279}, {"referenceID": 22, "context": "Symbolic differentiation addresses the weaknesses of both the manual and numerical methods, but often results in complex and cryptic expressions plagued with the problem of \u201cexpression swell\u201d (Corliss, 1988).", "startOffset": 192, "endOffset": 207}, {"referenceID": 19, "context": "Because of its generality, AD is an already established tool in applications including realparameter optimization (Walther, 2007), sensitivity analysis (Carmichael and Sandu, 1997), physical modeling (Ekstr\u00f6m et al, 2010), and probabilistic inference (Neal, 2011).", "startOffset": 152, "endOffset": 180}, {"referenceID": 40, "context": "In simplest terms, backpropagation models learning as gradient descent in neural network weight space, looking for the minimum of an error function (Gori and Maggini, 1996).", "startOffset": 148, "endOffset": 172}, {"referenceID": 14, "context": "When introducing new models, machine learning researchers spend considerable effort on the manual derivation of analytical derivatives and subsequently plug these into standard optimization procedures such as L-BFGS (Zhu et al, 1997) or stochastic gradient descent (Bottou, 1998). Manual differentiation is evidently time consuming and prone to error. Of the other alternatives, numerical differentiation is simple to implement, but susceptible to round-off and truncation errors that make it inherently unstable (Jerrell, 1997). Symbolic differentiation addresses the weaknesses of both the manual and numerical methods, but often results in complex and cryptic expressions plagued with the problem of \u201cexpression swell\u201d (Corliss, 1988). Furthermore, manual and symbolic methods require the model to be expressed as a closed-form mathematical formula, ruling out algorithmic control flow and severely limiting expressivity. We are concerned with the powerful fourth technique, automatic differentiation (AD), which works by systematically applying the chain rule of differential calculus at the elementary operator level. AD allows the accurate evaluation of derivatives in machine precision, with only a small constant factor of overhead and ideal asymptotic efficiency. In contrast with the effort involved in arranging code into closed-form expressions for symbolic differentiation, AD can usually be applied to existing code with minimal change. Because of its generality, AD is an already established tool in applications including realparameter optimization (Walther, 2007), sensitivity analysis (Carmichael and Sandu, 1997), physical modeling (Ekstr\u00f6m et al, 2010), and probabilistic inference (Neal, 2011). Despite its widespread use in other fields, AD has been underused, if not unknown, by the machine learning community. As it happens, AD and machine learning practice are conceptually very closely interconnected: consider the backpropagation method for training neural networks, which has a colorful history of being rediscovered several times by independent researchers (Widrow and Lehr, 1990). It has been one of the most studied and used training algorithms since the day it became popular mainly through the work of Rumelhart et al (1986). In simplest terms, backpropagation models learning as gradient descent in neural network weight space, looking for the minimum of an error function (Gori and Maggini, 1996).", "startOffset": 266, "endOffset": 2258}, {"referenceID": 30, "context": "Numerical approximations of derivatives are inherently ill-conditioned and unstable, with the exception of complex variable methods that are applicable to a limited set of holomorphic functions (Fornberg, 1981).", "startOffset": 194, "endOffset": 210}, {"referenceID": 15, "context": "Other techniques for improving numerical differentiation, including higherorder finite differences, Richardson extrapolation to the limit (Brezinski and Zaglia, 1991), and differential quadrature methods using weighted sums (Bert and Malik, 1996), also increase rapidly in programming complexity, do not completely eliminate approximation errors, and remain highly susceptible to floating point truncation.", "startOffset": 138, "endOffset": 166}, {"referenceID": 8, "context": "Other techniques for improving numerical differentiation, including higherorder finite differences, Richardson extrapolation to the limit (Brezinski and Zaglia, 1991), and differential quadrature methods using weighted sums (Bert and Malik, 1996), also increase rapidly in programming complexity, do not completely eliminate approximation errors, and remain highly susceptible to floating point truncation.", "startOffset": 224, "endOffset": 246}, {"referenceID": 4, "context": "A given trace of elementary operations can be also represented using a computational graph (Bauer, 1974), as shown in Fig.", "startOffset": 91, "endOffset": 104}, {"referenceID": 25, "context": "Compared with the straightforward simplicity of the forward mode, reverse mode AD can, at first, appear somewhat \u201cmysterious\u201d (Dennis and Schnabel, 1996).", "startOffset": 126, "endOffset": 153}, {"referenceID": 25, "context": "Compared with the straightforward simplicity of the forward mode, reverse mode AD can, at first, appear somewhat \u201cmysterious\u201d (Dennis and Schnabel, 1996). Griewank and Walther (2008) argue that this is in part because of the common acquaintance with the chain rule as a mechanical procedure propagating derivatives forward.", "startOffset": 127, "endOffset": 183}, {"referenceID": 24, "context": "It is an active area of research to improve storage requirements in implementations, by methods such as checkpointing strategies and data-flow analysis (Dauvergne and Hasco\u00ebt, 2006).", "startOffset": 152, "endOffset": 181}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959).", "startOffset": 57, "endOffset": 88}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD.", "startOffset": 58, "endOffset": 206}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD.", "startOffset": 58, "endOffset": 359}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation.", "startOffset": 58, "endOffset": 940}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation.", "startOffset": 58, "endOffset": 961}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation. Incidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation.", "startOffset": 58, "endOffset": 1267}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation. Incidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation.", "startOffset": 58, "endOffset": 1306}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation. Incidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation.", "startOffset": 58, "endOffset": 1324}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation. Incidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation. Within the machine learning community, the method has been reinvented several times, such as by Parker (1985), until it was eventually brought to fame by Rumelhart et al (1986) of the Parallel Distributed Processing (PDP) group.", "startOffset": 58, "endOffset": 1490}, {"referenceID": 51, "context": "Ideas underlying the AD technique date back to the 1950s (Nolan, 1953; Beda et al, 1959). Forward mode AD as a general method for evaluating partial derivatives was essentially discovered by Wengert (1964). It was followed by a period of relatively low activity, until interest in the field was revived by the 1980s mostly through the work of Griewank (1989), also supported by improvements in modern programming languages and the feasibility of an efficient reverse mode AD. Reverse mode AD and backpropagation have an intertwined history. The essence of reverse mode AD, cast in a continuous-time formalism, is the Pontryagin Maximum principle (Rozonoer and Pontryagin, 1959; Goltyanskii et al, 1960). This method was understood in the control theory community (Bryson, 1962; Bryson and Ho, 1969) and cast in more formal terms with discrete-time variables topologically sorted in terms of dependency by Bryson\u2019s PhD student Werbos (1974). Speelpenning (1980) discovered reverse mode AD and gave the first implementation that was actually automatic, in the sense of accepting a specification of a computational process written in a general-purpose programming language and automatically performing the reverse mode transformation. Incidentally, Hecht-Nielsen (1989) cites the work of Bryson and Ho (1969) and Werbos (1974) as the two earliest known instances of backpropagation. Within the machine learning community, the method has been reinvented several times, such as by Parker (1985), until it was eventually brought to fame by Rumelhart et al (1986) of the Parallel Distributed Processing (PDP) group.", "startOffset": 58, "endOffset": 1557}, {"referenceID": 25, "context": "ular such method is the BFGS algorithm, together with its limited-memory variant L-BFGS (Dennis and Schnabel, 1996).", "startOffset": 88, "endOffset": 115}, {"referenceID": 26, "context": "This sparsity, along with symmetry, can be readily exploited by AD techniques such as computational graph elimination (Dixon, 1991), partial separability (Gay, 1996), and matrix coloring and compression (Gebremedhin et al, 2009).", "startOffset": 118, "endOffset": 131}, {"referenceID": 33, "context": "This sparsity, along with symmetry, can be readily exploited by AD techniques such as computational graph elimination (Dixon, 1991), partial separability (Gay, 1996), and matrix coloring and compression (Gebremedhin et al, 2009).", "startOffset": 154, "endOffset": 165}, {"referenceID": 25, "context": "ular such method is the BFGS algorithm, together with its limited-memory variant L-BFGS (Dennis and Schnabel, 1996). AD here provides a way of computing the exact Hessian in an efficient way. However, in many cases, one does not need the full Hessian but only a Hessian-vector product H r, which can be computed very efficiently using a combination of the forward and reverse modes of AD. This computes H r with O(n) complexity, even though the H is a n\u00d7n matrix. Moreover, Hessians arising in large-scale applications are typically sparse. This sparsity, along with symmetry, can be readily exploited by AD techniques such as computational graph elimination (Dixon, 1991), partial separability (Gay, 1996), and matrix coloring and compression (Gebremedhin et al, 2009). Another approach for improving the rate of convergence of gradient methods is to use gain adaptation methods such as stochastic meta-descent (SMD) (Schraudolph, 1999), where stochastic sampling is introduced to avoid local minima. An example using SMD with AD Hessian-vector products is given by Vishwanathan et al (2006) on conditional random fields (CRF), a probabilistic method for labeling and segmenting data.", "startOffset": 89, "endOffset": 1093}, {"referenceID": 25, "context": "ular such method is the BFGS algorithm, together with its limited-memory variant L-BFGS (Dennis and Schnabel, 1996). AD here provides a way of computing the exact Hessian in an efficient way. However, in many cases, one does not need the full Hessian but only a Hessian-vector product H r, which can be computed very efficiently using a combination of the forward and reverse modes of AD. This computes H r with O(n) complexity, even though the H is a n\u00d7n matrix. Moreover, Hessians arising in large-scale applications are typically sparse. This sparsity, along with symmetry, can be readily exploited by AD techniques such as computational graph elimination (Dixon, 1991), partial separability (Gay, 1996), and matrix coloring and compression (Gebremedhin et al, 2009). Another approach for improving the rate of convergence of gradient methods is to use gain adaptation methods such as stochastic meta-descent (SMD) (Schraudolph, 1999), where stochastic sampling is introduced to avoid local minima. An example using SMD with AD Hessian-vector products is given by Vishwanathan et al (2006) on conditional random fields (CRF), a probabilistic method for labeling and segmenting data. Similarly, Schraudolph and Graepel (2003) use Hessian-vector products in their model combining conjugate gradient techniques with stochastic gradient descent.", "startOffset": 89, "endOffset": 1228}, {"referenceID": 54, "context": "Similarly, Rollins (2009) uses reverse mode AD in conjunction with neural networks for the problem of optimal feedback control.", "startOffset": 11, "endOffset": 26}, {"referenceID": 1, "context": "An example is given for continuous time recurrent neural networks (CTRNN) by Al Seyab and Cao (2008), where they apply AD for the training of CTRNNs predicting dynamic behavior of nonlinear processes in real time.", "startOffset": 80, "endOffset": 101}, {"referenceID": 20, "context": "On the other hand, in computer vision, many problems are formulated as the minimization of an appropriate energy functional (Bertero et al, 1988; Chambolle, 2000).", "startOffset": 124, "endOffset": 162}, {"referenceID": 19, "context": "On the other hand, in computer vision, many problems are formulated as the minimization of an appropriate energy functional (Bertero et al, 1988; Chambolle, 2000). This minimization is usually accomplished via calculus of variations and the Euler-Lagrange equation. In this area, the first study introducing AD to computer vision is given by Pock et al (2007), where they address the problems of denoising, segmentation, and recovery of information from stereoscopic image pairs, and note the usefulness of AD in identifying sparsity patterns in large Jacobian and Hessian matrices.", "startOffset": 146, "endOffset": 360}, {"referenceID": 19, "context": "On the other hand, in computer vision, many problems are formulated as the minimization of an appropriate energy functional (Bertero et al, 1988; Chambolle, 2000). This minimization is usually accomplished via calculus of variations and the Euler-Lagrange equation. In this area, the first study introducing AD to computer vision is given by Pock et al (2007), where they address the problems of denoising, segmentation, and recovery of information from stereoscopic image pairs, and note the usefulness of AD in identifying sparsity patterns in large Jacobian and Hessian matrices. In another study, Grabner et al (2008) use reverse mode AD for GPUaccelerated medical 2D/3D registration, a task involving the alignment of data from different sources such as X-ray images or computed tomography.", "startOffset": 146, "endOffset": 622}, {"referenceID": 3, "context": "Barrett and Siskind (2013) present a use of AD for the task of video event detection.", "startOffset": 0, "endOffset": 27}, {"referenceID": 3, "context": "Barrett and Siskind (2013) present a use of AD for the task of video event detection. Compared with general computer vision tasks focused on recognizing objects and their properties (which can be thought of as nouns in a narrative), an important aspect of this work is that it aims to recognize and reason about events and actions (i.e. verbs). The method uses Hidden Markov Models (HMMs) and Dalal and Triggs (2005) object detectors, and performs training on a corpus of pre-tracked video by an adaptive step size naive gradient descent algorithm, where gradient computations are done with reverse mode AD.", "startOffset": 0, "endOffset": 417}, {"referenceID": 44, "context": "Improvements in training time can be realized by using online or distributed training algorithms (Gimpel et al, 2010). An example using stochastic gradient descent for NLP is given by Finkel et al (2008) optimizing conditional random field parsers through an objective function.", "startOffset": 114, "endOffset": 204}, {"referenceID": 44, "context": "Improvements in training time can be realized by using online or distributed training algorithms (Gimpel et al, 2010). An example using stochastic gradient descent for NLP is given by Finkel et al (2008) optimizing conditional random field parsers through an objective function. Related with the work on video event detection in the previous section, Yu and Siskind (2013) report their work on sentence tracking, representing an instance of grounded language learning paired with computer vision, where the system learns word meanings from short video clips paired with descriptive sentences.", "startOffset": 114, "endOffset": 373}, {"referenceID": 39, "context": "Probabilistic programming has been experiencing a recent resurgence thanks to new algorithmic advances for probabilistic inference and new areas of application in machine learning (Goodman, 2013).", "startOffset": 180, "endOffset": 195}, {"referenceID": 50, "context": "When model parameters are continuous, the Hamiltonian\u2014or, hybrid\u2014 Monte Carlo (HMC) algorithm provides improved convergence characteristics avoiding the slow exploration of random sampling, by simulating Hamiltonian dynamics through auxiliary \u201cmomentum variables\u201d (Neal, 1993).", "startOffset": 264, "endOffset": 276}, {"referenceID": 39, "context": "Probabilistic programming has been experiencing a recent resurgence thanks to new algorithmic advances for probabilistic inference and new areas of application in machine learning (Goodman, 2013). A probabilistic programming language provides primitive language constructs for random choice and allows the automatic probabilistic inference of distributions specified by programs. Inference techniques can be static, such as compiling programs to Bayesian networks and using algorithms such as belief propagation for inference; or they can be dynamic, executing programs several times and computing statistics on observed values to infer distributions. Markov chain Monte Carlo (MCMC) methods are typically used for dynamic inference, such as the MetropolisHastings algorithm based on random sampling. Meyer et al (2003) give an example of how AD can be used for speeding up Bayesian posterior inference in MCMC, with an application in stochastic volatility.", "startOffset": 181, "endOffset": 820}, {"referenceID": 47, "context": "Classical instances of source code transformation include the Fortran preprocessors GRESS (Horwedel et al, 1988) and PADRE2 (Kubo and Iri, 1990), which transform AD-enabled variants of Fortran into standard Fortran 77 before compiling.", "startOffset": 124, "endOffset": 144}, {"referenceID": 47, "context": "Classical instances of source code transformation include the Fortran preprocessors GRESS (Horwedel et al, 1988) and PADRE2 (Kubo and Iri, 1990), which transform AD-enabled variants of Fortran into standard Fortran 77 before compiling. Similarly, the ADIFOR tool by Bischof et al (1996), given a", "startOffset": 125, "endOffset": 287}, {"referenceID": 43, "context": "A recent and popular tool also utilizing this approach is Tapenade (Pascual and Hasco\u00ebt, 2008; Hasco\u00ebt and Pascual, 2013), implementing forward and reverse mode AD for Fortran and C programs.", "startOffset": 67, "endOffset": 121}, {"referenceID": 0, "context": "Some of the earliest AD tools such as SLANG (Adamson and Winant, 1969) and PROSE (Pfeiffer, 1987) belong to this category.", "startOffset": 44, "endOffset": 70}, {"referenceID": 49, "context": "The NAGWare Fortran 95 compiler (Naumann and Riehme, 2005) is a more recent example, where the use of ADrelated extensions triggers automatic generation of derivative code at compile time.", "startOffset": 32, "endOffset": 58}, {"referenceID": 60, "context": "A highly popular tool implemented with operator overloading in C++ is ADOL-C (Walther and Griewank, 2012).", "startOffset": 77, "endOffset": 105}, {"referenceID": 7, "context": "The FADBAD++ library (Bendtsen and Stauning, 1996) implements AD for C++ using templates and oper-", "startOffset": 21, "endOffset": 50}, {"referenceID": 21, "context": "In comparison, increasing awareness of AD in fields such as engineering design optimization (Hasco\u00ebt et al, 2003), computational fluid dynamics (M\u00fcller and Cusdin, 2005), climatology (Charpentier and Ghemires, 2000), and computational finance (Bischof et al, 2002) provide evidence for its maturity and efficiency, with benchmarks (Giles and Glasserman, 2006; Sambridge et al, 2007; Capriotti, 2011) reporting performance increases of several orders of magnitude.", "startOffset": 183, "endOffset": 215}, {"referenceID": 36, "context": "In comparison, increasing awareness of AD in fields such as engineering design optimization (Hasco\u00ebt et al, 2003), computational fluid dynamics (M\u00fcller and Cusdin, 2005), climatology (Charpentier and Ghemires, 2000), and computational finance (Bischof et al, 2002) provide evidence for its maturity and efficiency, with benchmarks (Giles and Glasserman, 2006; Sambridge et al, 2007; Capriotti, 2011) reporting performance increases of several orders of magnitude.", "startOffset": 331, "endOffset": 399}, {"referenceID": 18, "context": "In comparison, increasing awareness of AD in fields such as engineering design optimization (Hasco\u00ebt et al, 2003), computational fluid dynamics (M\u00fcller and Cusdin, 2005), climatology (Charpentier and Ghemires, 2000), and computational finance (Bischof et al, 2002) provide evidence for its maturity and efficiency, with benchmarks (Giles and Glasserman, 2006; Sambridge et al, 2007; Capriotti, 2011) reporting performance increases of several orders of magnitude.", "startOffset": 331, "endOffset": 399}], "year": 2015, "abstractText": "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD) is a technique for calculating derivatives efficiently and accurately, established in fields such as computational fluid dynamics, nuclear engineering, and atmospheric sciences. Despite its advantages and use in other fields, machine learning practitioners have been little influenced by AD and make scant use of available tools. We survey the intersection of AD and machine learning, cover applications where AD has the potential to make a big impact, and report on the recent developments in the adoption this technique. We also aim to dispel some misconceptions that we think have impeded the widespread awareness of AD within the machine learning community.", "creator": "LaTeX with hyperref package"}}}