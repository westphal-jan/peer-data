{"id": "1502.07504", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2015", "title": "Rational Kernels for Arabic Stemming and Text Classification", "abstract": "subsequently in this paper, we address equally the problems of maintaining arabic text classification and stemming using transducers and identifying rational kernels. jointly we introduce a new stemming scanning technique based externally on the successful use of arabic patterns ( pattern based stemmer ). patterns are specially modelled properly using appropriate transducers and stemming design is uniquely done without depending rather on adopting any dictionary. consequently using transducers for stemming, documents are alternately transformed into finite probability state compressed transducers. this document representation allows a us to use and explore rational kernels thereby as forming a framework necessary for exploring arabic text textual classification. stemming experiments are mostly conducted upwards on three possible word size collections and classification tracking experiments are annually done modelled on the saudi advertising press agency dataset. results show shown that our current approach, when compared close with ten other approaches, \" is promising ideas specially promising in economic terms terms of combining accuracy, recall and f1.", "histories": [["v1", "Thu, 26 Feb 2015 11:09:59 GMT  (646kb,D)", "http://arxiv.org/abs/1502.07504v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["attia nehar", "djelloul ziadi", "hadda cherroun"], "accepted": false, "id": "1502.07504"}, "pdf": {"name": "1502.07504.pdf", "metadata": {"source": "CRF", "title": "Rational Kernels for Arabic Stemming and Text Classification", "authors": ["Attia Nehar", "Hadda Cherroun"], "emails": ["a.nehar@mail.lagh-univ.dz", "h.cherroun@mail.lagh-univ.dz", "djelloul.ziadi@univ-rouen.fr"], "sections": [{"heading": null, "text": "Arabic Text Classification and stemming using Transducers and Rational Kernels. We introduce a new stemming technique based on the use of Arabic patterns (Pattern Based Stemmer). Patterns are modelled using transducers and stemming is done without depending on any dictionary. Using transducers for stemming, documents are transformed into finite state transducers. This document representation allows us to use and explore rational kernels as a framework for Arabic Text Classification. Stemming experiments are conducted on three word collections and classification experiments are done on the Saudi Press Agency dataset. Results show that our approach, when compared with other approaches, is promising specially in terms of Accuracy, Recall and F1.\nKeywords N-gram \u00b7 Arabic \u00b7 Classification \u00b7 Rational kernels \u00b7 automata \u00b7 Transducers"}, {"heading": "1 Introduction", "text": "Text Classification (TC) is the task of automatically sorting a set of documents into one or more categories from a predefined set [22]. Text classification techniques\nThis work is supported by the MESRS - Algeria under Project 8/U03/7015.\nAttia Nehar \u00b7 Hadda Cherroun Laboratoire d\u2019informatique et Mathe\u0301matiques Universite\u0301 A.T. Laghouat, Alge\u0301rie E-mail: a.nehar,h.cherroun@mail.lagh-univ.dz\nDjelloul Ziadi Laboratoire LITIS - EA 4108, Normandie Universite\u0301, Rouen, France E-mail: djelloul.ziadi@univ-rouen.fr\nare used in many domains, including mail spam filtering, article indexing, Web searching, automated population of hierarchical catalogues of Web resources, even automated essay grading task.\nDue to the complexity of the Arabic language, Arabic Text Classification (ATC) starts receiving great attention. Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23]. In general, we can divide an ATC system into three steps:\n1. Preprocessing step: where punctuation marks, di-\nacritics, stop words and non letters are removed.\n2. Features extraction: a set of features is extracted\nfrom the text, which will represent the text in the next step. For instance, Khreisat [17] used the Ngram technique to extract features from documents. Another work [23], used stemming to extract features. 3. Learning step: many supervised algorithms were\nused to learn systems how to classify Arabic text documents: Support Vector Machines [5,13,18], KNearest Neighbours [14,23], Naive Bayes [5,12] and many others. Most algorithms rely on distance measures over extracted features to decide how much two documents are similar.\nIn the second step, a feature vector is constructed. Several stemming approaches are developed [1]. Khoja and Garside (1999) developed a dictionary based stemmer. It gives good performances, but the dictionary needs to be maintained. The stemmer developed in [2] finds the three-letter roots for Arabic words without depending on any roots dictionary or pattern files. Many Arabic words have the same stem but not the same meaning. Reducing two semantically different words to the same root can induce classification errors. To pre-\nar X\niv :1\n50 2.\n07 50\n4v 1\n[ cs\n.C L\n] 2\n6 Fe\nb 20\nvent this, light stemming is used in TC algorithms [3]. Its main idea is that a lot of words generated from the same root have different meanings. The basis of this light-stemming algorithms consists of several rounds over the text, that attempt to locate and remove the most frequent prefixes and suffixes from the words. This leads to a lot of features due to the light stemming strategy.\nIn the third step, many distance measures could be used to evaluate distance (or dissimilarity) between documents using these feature vectors. The quality of the classification system is related to the used distance measure.\nIn this paper, we study the effect of stemming on ATC. Let\u2019s illustrate this by an example. Given two\nsimple documents d1 =\u201d \u00e9 PY\u00d6\u00cf @ \u00fa \u00af \u00c9 \u00ae\u00a2\u00cb@ \u00fa G . Q K \u00f0 \u00d5\u00ce \u00aa JK \u201d (Child learns and brought up in the school), and d2 =\n\u201d \u00e9J K. Q \u00cb @ \u00f0 \u00d5\u00e6 \u00ca\u00aa J\u00cb @ A J\u00cb A \u00ae\u00a3B P@Y\u00d6\u00cf @ \u00d0Y \u00ae K\u201d (Schools provide education for our children). We compute euclidian distance between them using 3-grams, with and without stemming:\nDistance\nwith 3-grams\nWithout stemming 0.25\nWith stemming 0.18\nIt is clear that distance between d1 and d2 is affected by stemming.\nIn this work, we enhance the stemming technique, introduced by authors in previous paper [20]. Indeed, stemmer introduced in [20] gives a set of possible stems. Our new stemmer chooses the best stem based on a statistical study of characters occurences in the Arabic roots corpus. Hence, a comparaison experiment is conducted to assess performances against standard stemmers. This stemming technique transforms documents into finite state transducers. Then, rational kernels [9] are used as a framework to do ATC [19]. This framework enables the use of different distance measures or kernels.\nThis paper is organized as follows. Section 2 presents,\nin more details, the main stemming techniques. In Section 3, we recall some notions on weighted transducers\nand rational kernels. We present, in Section 4 our new stemming approach, then we explain how to use rational kernels as a framework for ATC. Experiments and results are reported and interpreted in Section 5."}, {"heading": "2 Stemming Techniques", "text": "In the context of ATC, stemming is applied to reduce dimensionality of the feature vectors. Brute stemming (commonly called stemming) transforms each Arabic word in the document, into its root. However, light stemming, reduces word by removing prefixes and suffixes.\nBrute Stemming\nThere are many brute stemming techniques used in the context of ATC. They can be classified into two types: (i) Stemming using a dictionary, where dictionary of Arabic word stems is needed. (ii) Stemming without dictionary, where stems are extracted without depending on any root or pattern files.\nKhoja and Garside stemmer [16] removes the longest suffix and the longest prefix. It then matches the remaining word with verb and noun patterns, to extract the root by means of a dictionary. The stemmer makes use of many linguistic data files such as a list of all diacritic characters, punctuation characters, definite articles and stop words. This stemmer gives good performance but relies on dictionary which needs to be updated. The second technique [2], finds the three-letter roots for Arabic words without depending on any root or pattern files. They extract word roots by assigning weights and ranks to the letters that constitute a word. Consonants were assigned a weight of zero and different weights were assigned to the letters grouped in the\nword ( A\u00eeD K\u00f1\u00d2 J\u00cb A ) where all affixes are formed by combinations of these letters. The algorithm selects the letters with the lowest products (weight \u00d7 rank) as root letters. Weights and ranks are assigned to letters using a little bit information on language [2]. This algorithm, like any other brute stemming algorithm, gives the same stem for two semantically different words.\nLight Stemming\nIn Arabic language, some word variants do not have\nsimilar meanings (like the two words: \u00e9 J. J \u00ba \u00d3 which means library and I. KA\u00bf which means writer). However, these word variants give the same root if a brute stemming is used. Thus, brute stemming can affect the meaning of words. Light stemming [3] aims to enhance the text classification performance while retaining the words meanings. The basis of light-stemming algorithms consists of several rounds over the text, that attempt to locate and remove the most frequent prefixes and suffixes from the word. However, it leads to a lot of features."}, {"heading": "3 Weighted Transducers and Rational Kernels", "text": "Before describing our framework, let\u2019s give in what follows, some preliminaries on Weighted Transducers and Rational Kernels.\nTransducers are finite automata in which each transition is augmented with an output label in addition to the familiar input label. Output labels are concatenated along a path to form an output sequence as with input labels. Weighted transducers are finite-state transducers in which each transition carries some weight in addition to the input and output labels. The weight of a pair of input and output strings (x, y) is obtained by summing the weights of the paths labelled with (x, y). The following definition gives a formal definition of weighted transducers [7,10].\nDefinition 1 A weighted finite-state transducer T over the semiring (K,\u2295,\u2297, 0\u0304, 1\u0304) is an 8-tuple: T = (\u03a3,\u2206,Q, I, F,E, \u03bb, \u03c1) where \u03a3 is a finite input alphabet, \u2206 is a finite output alphabet, Q is a finite set of states, I \u2286 Q the set of initial states, F \u2286 Q the set of final states, E \u2286 Q\u00d7 (\u03a3 \u222a { })\u00d7 (\u2206 \u222a { })\u00d7K\u00d7Q a finite set of transitions, \u03bb : I \u2192 K the initial weight function, and \u03c1 : F \u2192 K the final weight function\nFor a path \u03c0 in a transducer, p[\u03c0] denotes the origin state of that path, n[\u03c0] its destination state and w[\u03c0] gives the sum of the weights of its arcs. The set of paths from the initial states I to the final states F labelled with input string x and output string y is denoted by P (I, x, y, F ). A transducer T is regulated if the output weight associated by T to any pair of input-output strings (x, y) given by: JT K(x, y) = \u2295\n\u03c0\u2208P (I,x,y,F ) \u03bb(p[\u03c0])\u2297 w[\u03c0]\u2297 \u03c1[n[\u03c0]] (1)\nis well-defined in K. JT K(x, y) = 0\u0304 if P (I, x, y, F ) = \u2205. Figure 2 shows an example of a simple transducer, with\nan input string x : \u00c9\u00abA \u00af and an output string y : \u00c9\u00aa \u00af . The only possible path in this transducer is the singular set : P ({0}, x, y, {4}). Regulated weighted transducers are closed under the following operations called rational operations:\n\u2013 the sum (or union) of two weighted transducers T1 and T2 is defined by:\n\u2200(x, y) \u2208 \u03a3\u2217 \u00d7\u03a3\u2217, JT1 \u2295 T2K(x, y) = JT1K(x, y)\u2295 JT2K(x, y) (2)\n\u2013 the product (or concatenation) of two weighted trans-\nducers T1 and T2 is defined by:\n\u2200(x, y) \u2208 \u03a3\u2217 \u00d7\u03a3\u2217, JT1 \u2297 T2K(x, y) =\u2295 x=x1x2,y=y1y2 JT1K(x1, y1)\u2297 JT2K(x2, y2) (3)\n\u2013 The composition of two weighted transducers T1 and\nT2 with matching input and output alphabets \u03a3, is a weighted transducer denoted by T1 \u25e6 T2 when the sum:\nJT1 \u25e6 T2K(x, y) = \u2295\nz in\u03a3\u2217\nJT1K(x, z)\u2297 JT2K(z, y) (4)\nis well-defined in K for all x, y \u2208 \u03a3\u2217\nRational Kernels are a general family of kernels,\nbased on weighted transducers, that extend kernel methods to the analysis of variable-length sequences or more generally weighted automata. Let X and Y be nonempty sets. A function K : X \u00d7 Y \u2192 R is said to be a kernel over X \u00d7 Y . Corinna et al. [9] give a formal definition for rational kernels:\nDefinition 2 A kernel K over \u03a3\u2217\u00d7\u2206\u2217 is said to be rational if there exist a weighted transducer T = (\u03a3,\u2206,Q, I, F,E, \u03bb, \u03c1) over the semiring K and a function \u03d5 : K\u2192 R such that for all x \u2208 \u03a3\u2217 and y \u2208 \u2206\u2217:\nK(x, y) = \u03d5(JT K(x, y)) (5)\nK is then said to be defined by the pair (\u03d5, T )."}, {"heading": "4 Framework for Arabic Stemming and Text Classification", "text": "In the following we explain how to use transducers to do stemming. First, Arabic patterns, prefixes and suffixes are modelled by simple transducers, then, a stemming transducer is constructed using these simple ones by applying rational operations like concatenation, union and composition. Then, we show how to use rational kernels as a framework to do ATC.\nStemming by Transducers\nArabic language differs from other languages syntactically, morphologically and semantically. One of the main characteristic features is that most words are built up from roots by following certain fixed patterns and adding prefixes and suffixes. For instance, the Arabic\nword \u00e9 PY\u00d6\u00cf @ (school) is built from the three-letters root\nor stem PX (learn) and using the pattern \u00c9\u00aa \u00ae\u00d3, then\nprefix \u00cb @ and suffix \u00e8 (which is used to denote female\ngender) are added. This results in the measure \u00e9 \u00ca\u00aa \u00ae\u00d3\n(see Table 1). Notice here that the letter \u00ac denotes the\nfirst letter of the three-letters root, \u00a8 denotes the sec-\nond letter and \u00c8 denotes the third one.\nWe will use measures to construct a transducer which do stemming. Figure 2 shows the example of the mea-\nsure \u00c9\u00abA \u00af. This transducer (Tmeasure1) can be used to extract the three-letters root of any Arabic word matching this measure. This is achieved by composition operation (4). We consider Tword, the transducer which maps any string to itself, i.e., the only possible path is the singleton set P ({0}, word, word, {i}) (Figure 1 shows transducer associated to the Arabic word \u00e9 PY\u00d3 ). The composition of two transducers is also a transducer.\n(Tword \u25e6 Tmeasure1)(word, y) =\u2211 z\u2208\u03a3\u2217 Tword(word, z) \u00b7 Tmeasure1(z, y)\nSince the only possible string matching z is z = word, we conclude that:\n(Tword \u25e6 Tmeasure1)(word, y) = Tword(word,word) \u00b7 Tmeasure1(word, y)\nAs we have Tword(word,word) = 1, so:\n(Tword \u25e6 Tmeasure1)(word, y) = Tmeasure1(word, y)\nIf wordmatches with the measure the output projection will extract the root (or stem) y associated to word.\nIn Arabic language, there are 4 verb prefixes ( @ \u00e0\nH \u00f8 ), 12 noun prefixes (, \u00c8 , \u00c9\u00cb , \u00ac , ,\nH , H. , \u00c8@ , @\n, \u00d1\u00eb , \u00e1\u00eb , \u00e1K , \u00e0\u00f0 , A\u00d3 , A K , A K , \u00e1\u00bb , \u00e1 K , \u00d5\u00bb , \u00d5\u00e7 ' , @\u00f0 , A\u00eb , \u00e0@\n\u00f8 , H@ , @ , H , \u00e8 , \u00e8 , \u00bc , \u00e0 , \u00fa G , \u00fa G , \u00e9 K). When considering the diacritics, there are more than 3000 patterns (in our knowledge). Since we don\u2019t consider diacritics in our approach, patterns are much less (less than 200),\nmuch of them are not used in the context of Modern Standard Arabic. Indeed, the patterns ( , \u00c9 \u00aa \u00af , \u00c9 \u00aa \u00af\n\u00c9\u00aa \u00af , \u00c9 \u00aa \u00af ) will result in only one pattern (\u00c9\u00aa \u00af) after removing diacritics. For illustration, Tables 2,3 shows some examples of noun and verb patterns.\nWe adopt the following process, to construct the stemming transducer, which enable us to include all measures:\n1. Building the transducer of all noun prefixes (resp. verb prefixes); 2. Building the transducer of all noun patterns (resp. verb patterns); 3. Building the transducer of all noun suffixes (resp. verb suffixes); 4. Concatenate noun transducers (resp. verb transducers) obtained in 1, 2 and 3. 5. Sum the two transducers obtained in step 4.\nThe first and third steps are very simple. We construct a transducer for each prefix (resp. suffix) then we do the union of these transducers. The resulting transducer represents the prefixes (resp. suffixes) transducer (see Figure 3 and Figure 4). In the second step, we build all possible noun pattern transducers. Then, the sum of these transducers represents the transducer of all noun patterns. We do the same to build the transducer of all verb patterns (Figure 5). In the forth step, transducers obtained in steps 1, 2 and 3 are concatenated. The final transducer is obtained by the union of transducers built in step 4.\nThe resulting transducer Tstemmer could not be represented graphically because of large number of states (about 400 states). This transducer can stem any wellformed Arabic word, i.e, a word which matches with some Arabic measure. In addition, it can give us a semantic information about the stemmed word. This information can be used to improve the quality of classification system.\nTransducers are created and manipulated using the OpenFst library [4], which is an open source library for constructing, combining, optimizing, and searching weighted finite-state transducers.\nPonderation of Our Stemmer\nThe composition of Tstemmer with any given word transducer Tword gives a transducer which may include many paths, so many possible roots. Indeed, an Arabic word\ncould match with more than one measure at the same time. Lets take the word Q\u00e5 J K @ (win). This Arabic word\nmatches with, at least, two measures: \u00c9\u00aa \u00ae K @ and \u00c9\u00aa J \u00af @\ngiving the stems Q\u00e5 and Q\u00e5 respectively. Thus, the use of Tstemmer leads to a set of one or more possible stems. The correct stem belongs to the set of possible stems. To cope with this situation, stemming transducer must be weighted. Many schemes are possible. We use a bigram window probabilities technique to affect a score to a given stem. The technique is based on a statistical study of letter frequencies in the Arabic roots corpus. This corpus contains more than 10 thousands three letters roots. The score is affected to a given stem by calculating the probability of letter occurrences in different positions. Let s = c1c2c3 a three letters stem. Score(s) is calculated by:\nScore(s) = P1(c1, c2)\u00d7 P2(c2, c3)\nwhere P1(c1, c2) is the probability to have the letter c2 in the second position preceded by c1, and P2(c2, c3) is the probability to have the letter c3 in the third position preceded by c2. Thus we consider the correct stem is the one that has the best score sbest: best = Arg(Max{Score(s) s \u2208 {Possible stems}}).\nRational Kernels for Arabic Text Classification\nOur ATC system is divided into three stages:\n1. preprocessing step. 2. feature extraction: the previous transducer is ap-\nplied on each word of the document resulting from step 1. Then, the transducer resulting from the concatenation of these words stems transducers will represent the document in the next step. 3. learning task: Rational kernels will be used to mea-\nsure distance between documents [9,10], and SVM will be used to do classification.\nConsidering a set of documents, each document consists of a sequence of words: w1w2 . . . wn. Applying our stemming transducer on each word of a document and right concatenate results will transform this document into finite state transducer. These transducers will be packaged into an archive file (far) to be treated by the learning algorithm (Figure 6). OpenKernel, which is a library for creating, combining and using kernels for machine learning applications, will be used to accelerate experiments."}, {"heading": "5 Experimental Results and Discussion", "text": "The next batch reports the main commands of OpenFst and OpenKernel libraries used to implement our classification system.\n1 fstcompose word.fst model.fst result.fst 2 fstconcate doc.fst result.fst doc.fst 3 farcreate data.list data.far 4klngram \u2212order=3 \u2212sigma=29 data.far 3gram.kar 5 svm\u2212train \u2212k openkernel \u2212K 2gram.kar cul.train cul.train.2gram.mdl 6 svm\u2212predict cul.test cul.train.2gram.mdl cul.test.2gram.pred\nTo stem words in the document, we iterate on these words using the OpenFst command [4] fstcompose (line 1), where word.fst is a linear finite state transducer with identical input and output labels, which represents a word, and model.fst is our ponderated stemming transducer . The resulting transducer result.fst represents the best stem. Resulting transducers are right concatenated to a finite state transducer (doc.fst), representing the entire document, using the OpenFst command fstconcate (line 2). The set of finite state transducers (FSTs) is then packaged in a FST archive (Far) using the OpenKernel command farcreate (line 3), where data.list contains the list of all FST documents, one file per line, and data.far is the FST archive (Far).\nVarious types of kernels could be created using OpenKer-\nnel library. 3-gram kernels could be created using the command klngram (line 4), where the first argument \u2013 order specifies the size of the n-grams, and the second argument \u2013sigma specifies the size of the alphabet, epsilon not included (Arabic alphabet size is 28). The first parameter is the FST archive (data.far) and the second parameter (3gram.kar) is the resulting kernel archive.\nOpenKernel library includes a plugin for the LibSVM implementation [8]. This enables us to do training, predicting and scoring on our dataset. Training command creates a model on the training set (line 5), where the first argument -k specifies the kernel format, the second one (-K) specifies the n-gram kernel archive. The first parameter specifies a correctly classified subset of the training set, the second parameter is the resulting model. In this command, cul.train contains a labelled sub set of training documents belonging to Cultural class. Having a model, we can use it to classify documents of the testing dataset with the command svm-predict (line 6), where the first parameter specifies a correctly classified subset of the testing set, the second parameter is the resulting model from the previous command. The last parameter contains the result of prediction using the model.\nStemming Results\nTo check the performances of our stemmer, experiments were performed on three word collections. The first one (Gold1) is a sample taken from the Corpus of Contemporary Arabic [21]. The two others (Gold2 and Gold3) are house built sets. All words of these sets were annotated by hand with the correct root. Roots have been checked by Arabic Language scholars who are experts in the Arabic Language. The three sets are picked randomly from different topics, including politics, culture, sport and news. Table 4 gives an overview of these three collections. We give for each gold, the number of words (# words). Table 5 reports the accuracy of our stemmer on the three sets of words.\nExperiment results show the effectiveness of our approach of stemming. Results on different corpora are stable and the best score is achieved with the greatest corpus (Gold3). Our stemmer results are sandwiched between Khoja and Al-Serhan stemmer results. This can be explained by the fact that Khoja\u2019s stemmer is a dictionary based tool, which makes it language dependent. Al-Serhan stemmer is an unsupervised one. It uses a little bit information about the language. Our stemmer is a semi-supervised tool. It uses a language knowledge -patterns- but only in the construction stage. Patterns are fixed and do not change.\nATC Results\nWe perform experiments on the Saudi Press Agency (SPA) dataset [6] for training and testing the ATC system. As detailed on Table 6, this dataset contains 1,526 text documents belonging to one of the six categories: culture, economic, social, general, politics and sport. As mentioned before, stop words, non Arabic letters, symbols and digits were removed. We have used 80% of documents for training the classifier and 20% for testing. Learning is done using LibSVM implementation [8], included in Openkernel, with three different n-gram kernels (n = 2, 3, 4). Since we want to show the effect of stemming, we report results of the three classifier versions; without stemming (Classifier 1), with Al-Serhan stemmer (Classifier 2) and with our stemmer (Classifier 3), in terms of accuracy, precision, recall and F1. In Figures 7, 9 and 11, we report results in terms of accuracy and precision for the three classifiers with the three kernels (bigrams, 3-grams and 4-grams). Figures 8, 10 and 12 give results in terms of recall and F1 for the same classifiers.\nConcerning the quality of classification, Figure 13 shows that best results were reached with 3-grams kernel for accuracy, recall and F1 measures. This can be\nexplained by the fact that over than 80% of Arabic words are built from 3-letter roots.\nFor the 3-gram kernel, let us measure the effect of stemming on classification. For most classes, stemming enhance results in terms of accuracy, Recall and F1 (see Figures 9 and 10). However, for precision, stemming affects negatively performances (see Figure 9).\nOne can argue the best scores observed by sport class by the fact that it uses a specific vocabulary. Poor results are reported for the General class. This is expected given the used words in this kind of documents which are generic. At last, our classifier surpasses other classifiers in most cases."}, {"heading": "6 Conclusion", "text": "In this paper we introduced a new framework for Arabic word stemming and Text classification. It is based on the use of transducers for stemming, and rational kernels for measuring distance between documents. First, our stemmer uses transducers for modelling Arabic patterns. Second, rational kernels are used to measure distances between documents. Experiments and analysis of this framework in the context of Arabic Text Classification show that stemming improves the quality of classifiers in terms of accuracy, recall and F1. But it lightly decreases the precision. 3-grams based classifiers reached the best results. Like that of Al-Serhan, our approach of stemming do not rely on dictionary, and it gives better results.\nIn future work, other kernels, like word-grams and\ngappy grams, will be investigated.\nFig. 1 Transducer corresponding to the word \u00e9 PY\u00d6\u00cf @ (school).\nFig. 4 Transducer of noun and verb suffixes.\nFig. 5 Transducer of verb patterns.\nFig. 6 Transformation of a text document into a finite state transducer."}], "references": [{"title": "Stemming techniques for Arabic words: A comparative study", "author": ["M. Al-Nashashibi", "D. Neagu", "A. Yaghi"], "venue": "Computer Technology and Development (ICCTD),, pp. 270 \u2013276", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "New Approach For Extracting Arabic Roots", "author": ["H. Al-Serhan", "R.A. Shalabi", "G. Kannan"], "venue": "Proceedings of The 2003 Arab Conf. on Infor. Technology, pp. 42\u201359. Alexandria, Egypt", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "On Arabic Search: Improving the Retrieval Effectiveness Via Light Stemming Approach", "author": ["M. Aljlayl", "O. Frieder"], "venue": "ACM Eleventh Conference on Infor. and Knowledge Management, pp. 340\u2013347", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "OpenFst: A General and Efficient Weighted FiniteState Transducer Library", "author": ["C. Allauzen", "M. Riley", "J. Schalkwyk", "W. Skut", "M. Mohri"], "venue": "Proceedings of the Ninth International Conference on Implementation and Application of Automata, (CIAA 2007), LNCS, vol. 4783, pp. 11\u201323. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated Arabic Text Categorization Using SVM and NB", "author": ["S. Alsaleem"], "venue": "Int. Arab J. e-Technol. 2(2), 124\u2013128", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "KACST Arabic Text Classification Project: Overview and Preliminary Results", "author": ["A. Althubaity", "A. Almuhareb", "S. Alharbi", "A. Al-Rajeh", "M. Khorsheed"], "venue": "Proceedings of The 9th IBIMA conference on Information Management in Modern Organizations", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Transductions and Context-Free Languages", "author": ["J. Berstel"], "venue": "Teubner Studienb\u00fccher, Stuttgart", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1979}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology 2, 1\u201327", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Rational Kernels: Theory and Algorithms", "author": ["C. Cortes", "P. Haffner", "M. Mohri"], "venue": "J. Mach. Learn. Res. 5, 1035\u20131062", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning languages with rational kernels", "author": ["C. Cortes", "L. Kontorovich", "M. Mohri"], "venue": "Proceedings of the 20th annual conference on Learning theory, COLT\u201907, pp. 349\u2013 364. Springer-Verlag, Berlin", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Arabic Text Categorization", "author": ["R.M. Duwairi"], "venue": "Int. Arab J. Inf. Technol. 4(2), 125\u2013132", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic Arabic Document Categorization Based on the Naive Bayes Algorithm", "author": ["M. El Kourdi", "A. Bensaid", "Rachidi", "T.e."], "venue": "Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages, Semitic \u201904, pp. 51\u201358. Association for Computational Linguistics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Arabic Text Classification Using Support Vector Machines", "author": ["T. Gharib", "M. Habib", "Z. Fayed"], "venue": "International Journal of Computers and Their Applications 16(4), 192\u2013199", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Naive Bayesian and k-nearest Neighbour to Categorize Arabic Text Data", "author": ["W. Hadi", "F. Thabtah", "S. ALHawari", "J. Ababneh"], "venue": "Proceedings of the European Simulation and Modelling Conference. Le Havre, France, pp. 196\u2013200", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "A Comparison of Text-classification Techniques Applied to Arabic Text", "author": ["G. Kanaan", "R. Al-Shalabi", "S. Ghwanmeh", "H. Al-Ma\u2019adeed"], "venue": "J. Am. Soc. Inf. Sci. Technol. 60(9), 1836\u20131844", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Stemming arabic text", "author": ["S. Khoja", "R. Garside"], "venue": "Tech. rep., Computing Department, Lancaster University", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "A machine learning approach for Arabic text classification using N-gram frequency statistics", "author": ["L. Khreisat"], "venue": "Journal of Informatrics 3(1), 72\u201377", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Support Vector Machines based Arabic Language Text Classification System: Feature Selection Comparative Study", "author": ["A. Mesleh"], "venue": "T. Sobh (ed.) Advances in Computer and Information Sciences and Engineering, pp. 11\u201316. Springer Netherlands", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Rational kernels for arabic text classification", "author": ["A. Nehar", "D. Ziadi", "H. Cherroun"], "venue": "A.H. Dediu, C. Mart\u0301\u0131nVide, R. Mitkov, B. Truthe (eds.) SLSP, Lecture Notes in Computer Science, vol. 7978, pp. 176\u2013187. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "An Efficient Stemming for Arabic Text Classification", "author": ["A. Nehar", "D. Ziadi", "H. Cherroun", "Y. Guellouma"], "venue": "International Conference on Innovations in Information Technology (IIT), pp. 328 \u2013332", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Open-source Resources and Standards for Arabic Word Structure Analysis", "author": ["M. Sawalha"], "venue": "PhD, University of Leeds, Leeds", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Machine Learning in Automated Text Categorization", "author": ["F. Sebastiani", "C.N.D. Ricerche"], "venue": "ACM Computing Surveys 34, 1\u201347", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "An Intelligent System For Arabic Text Categorization", "author": ["M. Syiam", "Z. Fayed", "M. Habib"], "venue": "International Journal of Intelligent Computing and Information Sciences 6(1), 1\u201319", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 21, "context": "Text Classification (TC) is the task of automatically sorting a set of documents into one or more categories from a predefined set [22].", "startOffset": 131, "endOffset": 135}, {"referenceID": 4, "context": "Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23].", "startOffset": 74, "endOffset": 104}, {"referenceID": 5, "context": "Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23].", "startOffset": 74, "endOffset": 104}, {"referenceID": 10, "context": "Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23].", "startOffset": 74, "endOffset": 104}, {"referenceID": 11, "context": "Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23].", "startOffset": 74, "endOffset": 104}, {"referenceID": 12, "context": "Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23].", "startOffset": 74, "endOffset": 104}, {"referenceID": 13, "context": "Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23].", "startOffset": 74, "endOffset": 104}, {"referenceID": 14, "context": "Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23].", "startOffset": 74, "endOffset": 104}, {"referenceID": 16, "context": "Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23].", "startOffset": 74, "endOffset": 104}, {"referenceID": 17, "context": "Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23].", "startOffset": 74, "endOffset": 104}, {"referenceID": 22, "context": "Many algorithms have been developed to improve performance of ATC systems [5,6,11,12,13,14, 15,17,18,23].", "startOffset": 74, "endOffset": 104}, {"referenceID": 16, "context": "For instance, Khreisat [17] used the Ngram technique to extract features from documents.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "Another work [23], used stemming to extract features.", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "Learning step: many supervised algorithms were used to learn systems how to classify Arabic text documents: Support Vector Machines [5,13,18], KNearest Neighbours [14,23], Naive Bayes [5,12] and many others.", "startOffset": 132, "endOffset": 141}, {"referenceID": 12, "context": "Learning step: many supervised algorithms were used to learn systems how to classify Arabic text documents: Support Vector Machines [5,13,18], KNearest Neighbours [14,23], Naive Bayes [5,12] and many others.", "startOffset": 132, "endOffset": 141}, {"referenceID": 17, "context": "Learning step: many supervised algorithms were used to learn systems how to classify Arabic text documents: Support Vector Machines [5,13,18], KNearest Neighbours [14,23], Naive Bayes [5,12] and many others.", "startOffset": 132, "endOffset": 141}, {"referenceID": 13, "context": "Learning step: many supervised algorithms were used to learn systems how to classify Arabic text documents: Support Vector Machines [5,13,18], KNearest Neighbours [14,23], Naive Bayes [5,12] and many others.", "startOffset": 163, "endOffset": 170}, {"referenceID": 22, "context": "Learning step: many supervised algorithms were used to learn systems how to classify Arabic text documents: Support Vector Machines [5,13,18], KNearest Neighbours [14,23], Naive Bayes [5,12] and many others.", "startOffset": 163, "endOffset": 170}, {"referenceID": 4, "context": "Learning step: many supervised algorithms were used to learn systems how to classify Arabic text documents: Support Vector Machines [5,13,18], KNearest Neighbours [14,23], Naive Bayes [5,12] and many others.", "startOffset": 184, "endOffset": 190}, {"referenceID": 11, "context": "Learning step: many supervised algorithms were used to learn systems how to classify Arabic text documents: Support Vector Machines [5,13,18], KNearest Neighbours [14,23], Naive Bayes [5,12] and many others.", "startOffset": 184, "endOffset": 190}, {"referenceID": 0, "context": "Several stemming approaches are developed [1].", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "The stemmer developed in [2] finds the three-letter roots for Arabic words without depending on any roots dictionary or pattern files.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "vent this, light stemming is used in TC algorithms [3].", "startOffset": 51, "endOffset": 54}, {"referenceID": 19, "context": "In this work, we enhance the stemming technique, introduced by authors in previous paper [20].", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "Indeed, stemmer introduced in [20] gives a set of possible stems.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "Then, rational kernels [9] are used as a framework to do ATC [19].", "startOffset": 23, "endOffset": 26}, {"referenceID": 18, "context": "Then, rational kernels [9] are used as a framework to do ATC [19].", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "Khoja and Garside stemmer [16] removes the longest suffix and the longest prefix.", "startOffset": 26, "endOffset": 30}, {"referenceID": 1, "context": "The second technique [2], finds the three-letter roots for Arabic words without depending on any root or pattern files.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Weights and ranks are assigned to letters using a little bit information on language [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 2, "context": "Light stemming [3] aims to enhance the text classification performance while retaining the words meanings.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "The following definition gives a formal definition of weighted transducers [7,10].", "startOffset": 75, "endOffset": 81}, {"referenceID": 9, "context": "The following definition gives a formal definition of weighted transducers [7,10].", "startOffset": 75, "endOffset": 81}, {"referenceID": 8, "context": "[9] give a formal definition for rational kernels:", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Transducers are created and manipulated using the OpenFst library [4], which is an open source library for constructing, combining, optimizing, and searching weighted finite-state transducers.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "learning task: Rational kernels will be used to measure distance between documents [9,10], and SVM will be used to do classification.", "startOffset": 83, "endOffset": 89}, {"referenceID": 9, "context": "learning task: Rational kernels will be used to measure distance between documents [9,10], and SVM will be used to do classification.", "startOffset": 83, "endOffset": 89}, {"referenceID": 3, "context": "To stem words in the document, we iterate on these words using the OpenFst command [4] fstcompose (line 1), where word.", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "OpenKernel library includes a plugin for the LibSVM implementation [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 20, "context": "The first one (Gold1) is a sample taken from the Corpus of Contemporary Arabic [21].", "startOffset": 79, "endOffset": 83}, {"referenceID": 5, "context": "We perform experiments on the Saudi Press Agency (SPA) dataset [6] for training and testing the ATC system.", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Learning is done using LibSVM implementation [8], included in Openkernel, with three different n-gram kernels (n = 2, 3, 4).", "startOffset": 45, "endOffset": 48}], "year": 2015, "abstractText": "In this paper, we address the problems of Arabic Text Classification and stemming using Transducers and Rational Kernels. We introduce a new stemming technique based on the use of Arabic patterns (Pattern Based Stemmer). Patterns are modelled using transducers and stemming is done without depending on any dictionary. Using transducers for stemming, documents are transformed into finite state transducers. This document representation allows us to use and explore rational kernels as a framework for Arabic Text Classification. Stemming experiments are conducted on three word collections and classification experiments are done on the Saudi Press Agency dataset. Results show that our approach, when compared with other approaches, is promising specially in terms of Accuracy, Recall and F1.", "creator": "LaTeX with hyperref package"}}}