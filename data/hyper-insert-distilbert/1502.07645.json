{"id": "1502.07645", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2015", "title": "Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo", "abstract": "we consider observing the inverse problem of bayesian chance learning on sensitive datasets and present two promising simple but somewhat financially surprising results arising that totally connect bayesian learning \" to \" differential resource privacy :, establishing a cryptographic approach considered to protect individual - level privacy while neither permiting database - higher level utility. specifically, nevertheless we show that that under standard descriptive assumptions, conversely getting zero one single sample recovered from detecting a relative posterior finite distribution regression is more differentially private \" measure for free \". likewise we will also see that standard estimator expectation is primarily statistically consistent, near completely optimal random and computationally tractable whenever the typical bayesian model of genuine interest sampling is most consistent, optimal and partially tractable. more similarly but separately, lately we show that introducing a recent closed line score of computational works that use stochastic gradient for reducing hybrid nonlinear monte carlo ( namely hmc ) sampling also preserve innate differentially parameter privacy whereas with minor or no modifications required of fulfilling the algorithmic procedure mechanism at all, these observations lead to an \" asset anytime \" algorithm for bayesian learning specifically under mutual privacy constraint. we demonstrate that it performs much but better than assuming the state - economy of - the - art analog differential private methods conducted on designing synthetic and real datasets.", "histories": [["v1", "Thu, 26 Feb 2015 17:38:47 GMT  (219kb,D)", "https://arxiv.org/abs/1502.07645v1", null], ["v2", "Sun, 12 Apr 2015 02:53:05 GMT  (223kb,D)", "http://arxiv.org/abs/1502.07645v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yu-xiang wang", "stephen e fienberg", "alexander j smola"], "accepted": true, "id": "1502.07645"}, "pdf": {"name": "1502.07645.pdf", "metadata": {"source": "CRF", "title": "Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo", "authors": ["Yu-Xiang Wang", "Stephen E. Fienberg", "Alex Smola"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 2.\n07 64\n5v 2\n[ st\nat .M\nL ]\nContents"}, {"heading": "1 Introduction 3", "text": ""}, {"heading": "2 Notations and Preliminary 4", "text": "2.1 Differential privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"}, {"heading": "3 Posterior sampling and differential privacy 5", "text": "3.1 Implicitly Preserving Differential Privacy . . . . . . . . . . . . . . . . . . . 6 3.2 Consistency and Near-Optimality . . . . . . . . . . . . . . . . . . . . . . . . 7 3.3 (Efficient) sampling from approximate posterior . . . . . . . . . . . . . . . . 10 3.4 Discussions and comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . 12"}, {"heading": "4 Stochastic Gradient MCMC and ( , \u03b4)-Differential privacy 12", "text": "4.1 Stochastic Gradient Langevin Dynamics . . . . . . . . . . . . . . . . . . . . 13 4.2 Hamiltonian Dynamics, Fisher Scoring and Nose-Hoover Thermostat . . . . 17 4.3 Discussions and caveats. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"}, {"heading": "5 Experiments 20", "text": ""}, {"heading": "6 Related work 22", "text": ""}, {"heading": "7 Conclusion and future work 23", "text": ""}, {"heading": "A Stochastic Gradient Fisher Scoring 23", "text": "A.1 Fisher Scoring and Stochastic Gradient Fisher Scoring . . . . . . . . . . . . 23 A.2 Privacy extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24"}, {"heading": "1 Introduction", "text": "Bayesian models have proven to be one of the most successful classes of tools in machine learning. It stands out as a principled yet conceptually simple pipeline for combining expert knowledge and statistical evidence, modelling with complicated dependency structures and harnessing uncertainty by making probabilistic inferences (Geman & Geman, 1984; Gelman et al., 2014). In the past few decades, the Bayesian approach has been intensively used in modelling speeches (Rabiner, 1989), text documents (Blei et al., 2003), images/videos (Fei-Fei & Perona, 2005), social networks (Airoldi et al., 2009), brain activity (Penny et al., 2011), and is often considered gold standard in many of these application domains. Learning a Bayesisan model typically involves sampling from a posterior distribution, therefore the learning process is inherently randomized.\nDifferential privacy (DP) is a cryptography-inspired notion of privacy (Dwork, 2006; Dwork et al., 2006). It is designed to provide a very strong form of protection of individual user\u2019s private information and at the same time allow data analyses to be conducted with proper utility. Any algorithm that preserves differential privacy must be appropriately randomized too. For instance, one can differential-privately release the average salary of Californian males by adding a Laplace noise proportional to the sensitivity of this figure upon small perturbation of the data sample.\nIn this paper, we connect the two seemingly unrelated concepts by showing that under standard assumptions, the intrinsic randomization in the Bayesian learning can be exploited to obtain a degree of differential privacy. In particular, we show that:\n\u2022 Any algorithm that produces a single sample from the exact (or approximate) posterior distribution of a Bayesian model with bounded log-likelihood is (or ( , \u03b4))differentially private1. By the classic results in asymptotic statistics (Le Cam, 1986; Van der Vaart, 2000), we show that this posterior sample is a consistent estimator whenever the Bayesian model is consistent; and near optimal whenever the asymptotic normality and efficiency of the maximum likelihood estimate holds.\n\u2022 The popular large-scale sampler Stochastic Gradient Langevin Dynamics (Welling & Teh, 2011) and extensions, e.g. Ahn et al. (2012); Chen et al. (2014); Ding et al. (2014) obey ( , \u03b4)-differentially private with no algorithmic changes when the stepsize is chosen to be small. This gives us a procedure that can potentially output many (correlated) samples from an approximate posterior distribution.\nThese simple yet interesting findings make it possible for differential privacy to be explicitly considered when designing Bayesian models, and for Bayesian posterior sampling to be used as a valid DP mechanism. We demonstrate empirically that these methods work as well as\n1Similar observations were made in Mir (2013) and Dimitrakakis et al. (2014) under slightly different regimes and assumptions, and we will review them among other related work in Section 6.\nor better than the state-of-the-art differential private empirical risk minimization (ERM) solvers using objective perturbation (Chaudhuri et al., 2011; Kifer et al., 2012).\nThe results presented in this paper are closely related to a number of previous work, e.g., McSherry & Talwar (2007); Mir (2013); Bassily et al. (2014); Dimitrakakis et al. (2014). Proper comparisons with them would require the knowledge of our results, thus we will defer detailed comparisons to Section 6 near the end of the paper."}, {"heading": "2 Notations and Preliminary", "text": "Throughout the paper, we assume data point x \u2208 X and \u03b8 \u2208 \u0398 is the model. This can be the finite dimensional parameter of a single exponential family model or a collection of these in a graphical model, or a function in a Hilbert space or other infinite dimensional objects if the model is nonparametric. \u03c0(\u03b8) denotes a prior blief of the model parameters and p(x|\u03b8) and `(x|\u03b8) are the likelihood and log-likelihood of observing data point x given model parameter \u03b8. If we observe X = {x1, ...,xn}, the posterior distribution\n\u03c0(\u03b8|X) = \u03c0(\u03b8) \u220fN i=1 p(xi|\u03b8)\u222b \u220fN\ni=1 p(xi|\u03b8)\u03c0(\u03b8)d\u03c0\ndenotes the updated belief conditioned on the observed data. Learning Bayesian models correspond to finding the mean or mode of the posterior distribution, but often, the entire distribution is treated as the output, which provides much richer information than just a point estimator. In particular, we get error bars of the estimators for free (credibility intervals).\nIgnoring the philosophical disputes of Bayesian methods for the moment, practical challenges of Bayesian learning are often computational. As the models get more complicated, often there is not a closed-form expression for the posterior. Instead, we often rely on Markov Chain Monte Carlo methods, e.g., Metropolis-Hastings algorithm (Hastings, 1970) to generate samples. This is often prohibitively expensive when the data is large. One recent approach to scale up Bayesian learning is to combine stochastic gradient estimation as in Robbins & Monro (1951) and Monte Carlo methods that simulates stochastic differential equations, e.g. Neal (2011). These include Stochastic Gradient Langevin dynamics (SGLD) (Welling & Teh, 2011), Stochstic Gradient Fisher scoring (SGFS) (Ahn et al., 2012), Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014) as well as more recent Stochastic Gradient Nose\u0301-Hoover Thermostat (SGNHT) (Ding et al., 2014). We will describe them with more details and show that these series of tools provide differential privacy as a byproduct of using stochastic gradient and requiring the solution to not collapse to a point estimate."}, {"heading": "2.1 Differential privacy", "text": "Now we will talk about what we need to know about differential privacy. Let the space of data be X and data points X,Y \u2208 X n. Define d(X,Y ) to be the edit distance or Hamming distance between data set X and Y , for instance, if X and Y are the same except one data point, then d(X,Y ) = 1. Definition 1. (Differential Privacy) We call a randomized algorithm A ( , \u03b4)-differentially private with domain X n if for all measurable set S \u2282 Range(A) and for all X,Y \u2208 X n such that d(X,Y ) \u2264 1, we have\nP(A(X) \u2208 S) \u2264 exp( )P(A(Y ) \u2208 S) + \u03b4."}, {"heading": "If \u03b4 = 0, then A is the called -differential private.", "text": "This definition naturally prevents linkage attacks and the identification of individual data from adversaries having arbitrary side information and infinite computational power. The promise of differential privacy has been interpreted in statistical testing, Byesian inference and information theory for which we refer readers to Chapter 1 of (Dwork & Roth, 2013).\nThere are several interesting properties of differential privacy that we will exploit here. Firstly, the definition is closed under postprocessing. Lemma 2 (Postprocessing immunity). If A is an ( , \u03b4)-DP algorithm, B\u25e6A is also ( , \u03b4)-DP algorithm for any B.\nThis is natural because otherwise the whole point of differential privacy will be forfeited. Also, the definition automatically allows for cases when the sensitive data are accessed more than once. Lemma 3 (Composition rule). If algorithm A1 is ( 1, \u03b41)-DP, and A2 is ( 2, \u03b42)-DP then (A1 \u2297A2) is ( 1 + 2, \u03b41 + \u03b42)-DP.\nWe will describe more advanced properties of DP as we need in Section 4."}, {"heading": "3 Posterior sampling and differential privacy", "text": "In this section, we make a simple observation that under boundedness condition of a log-likelihood, getting one single sample from the posterior distribution (denoted by \u201cOPS mechanism\u201d from here onwards) preserves a degree of differential privacy for free. Then we will cite classic results in statistics and show that this sample is a consistent estimator in a Frequentist sense and near-optimal in many cases."}, {"heading": "3.1 Implicitly Preserving Differential Privacy", "text": "To begin with, we show that sampling from the posterior distribution is intrinsically differentially private. Theorem 4. If supx\u2208X ,\u03b8\u2208\u0398 |log p(x|\u03b8)| \u2264 B, releasing one sample from the posterior distribution p(\u03b8|Xn) with any prior preserves 4B-differential privacy. Alternatively, if X is a bounded domain (e.g., \u2016x\u2016\u2217 \u2264 R \u2200x \u2208 X ) and log p(x|\u03b8) is an L-Lipschitz function in \u2016 \u00b7 \u2016\u2217 for any \u03b8 \u2208 \u0398, then releasing one sample from the posterior distribution preserves 4LR-differential privacy. Proof. The posterior distribution p(\u03b8|x1, ...,xn) = \u220fn i=1 p(xi|\u03b8)p(\u03b8)\u222b\n\u03b8 \u220fn i=1 p(xi|\u03b8)p(\u03b8)d\u03b8 . For any x1, ...,xn, x \u2032 k,\nThe ratio can be factorized into\np(\u03b8|x1, ..., x\u2032k, ...,xn) p(\u03b8|x1, ...,xk, ...,xn) = p(x\u2032k|\u03b8)\n\u220f i=1:n,i6=k p(xi|\u03b8)p(\u03b8)\u220fn\ni=1 p(xi|\u03b8)p(\u03b8)\ufe38 \ufe37\ufe37 \ufe38 Factor 1\n\u00d7 \u222b \u03b8 \u220fn i=1 p(xi|\u03b8)p(\u03b8)d\u03b8\u222b\n\u03b8 p(x \u2032 k|\u03b8) \u220f i=1:n,i6=k p(xi|\u03b8)p(\u03b8)d\u03b8\ufe38 \ufe37\ufe37 \ufe38\nFactor 2\n.\nIt follows that\nFactor 1 = p(x\u2032k|\u03b8) p(xk|\u03b8) = elog p(x \u2032 k|\u03b8)\u2212log p(xk|\u03b8) \u2264 e2B,\nFactor 2 =\n\u222b \u03b8 \u220f i 6=k p(xi|\u03b8)p(\u03b8)p(xk)d\u03b8\u222b\n\u03b8 p(x \u2032 k|\u03b8) \u220f i 6=k p(xi|\u03b8)p(\u03b8)d\u03b8 =\n\u222b \u03b8 \u220f i 6=k p(xi|\u03b8)p(\u03b8)p(x\u2032k|\u03b8) p(xk) p(x\u2032k)\nd\u03b8\u222b \u03b8 p(x \u2032 k|\u03b8) \u220f i 6=k p(xi|\u03b8)p(\u03b8)d\u03b8\n=\n\u222b \u03b8 \u220f i 6=k p(xi|\u03b8)p(\u03b8)p(x\u2032k|\u03b8)elog p(xk|\u03b8)\u2212log p(x \u2032 k|\u03b8)d\u03b8\u222b\n\u03b8 p(x \u2032 k|\u03b8) \u220f i 6=k p(xi|\u03b8)p(\u03b8)d\u03b8\n\u2264 e2B m(x1, ...,x\n\u2032 k, ...,xn)\nm(x1, ...,x\u2032k, ...,xn) = e2B.\nwhere we use m(X) to denote the marginal distribution. As a result, the whole thing is bounded by e4B.\nAlternatively, we can use the Lipschitz constant and boundedness to get log p(x\u2032k|\u03b8) \u2212 log p(xk|\u03b8) \u2264 L\u2016x\u2032k \u2212 xk\u2016\u2217 \u2264 2LR.\nReaders familiar with differential privacy must have noticed that this is actually an instance of the exponential mechanism (McSherry & Talwar, 2007), a general procedure that preserves privacy while making outputs with higher utility exponentially more likely. If one sets the utility function to be the log-likelihood and the privacy parameter being 4B, then we get exactly the one-posterior sample mechanism. This exponential mechanism point of view provides an an simple extension which allows us to specify by simply scaling the log-likelihood (see Algorithm 1). We will overload the notation OPS to also represent this\nAlgorithm 1 One-Posterior Sample (OPS ) estimator input Data X, log-likelihood function `(\u00b7|\u00b7) satisfying supx,\u03b8 \u2016`(x|\u03b8)\u2016 \u2264 B a prior \u03c0(\u00b7). Privacy loss . 1. Set \u03c1 = min{1, 4B}. 2. Re-define log-likelihood function and the prior `\u2032(\u00b7|\u00b7) := \u03c1`(\u00b7|\u00b7) and \u03c0\u2032(\u00b7) := (\u03c0(\u00b7))\u03c1.\noutput \u03b8\u0302 \u223c P (\u03b8|X) \u221d exp (\u2211N\ni=1 ` \u2032(\u03b8|xi)\n) \u03c0\u2032(\u03b8).\nmechanism where we can specify . The nice thing about this algorithm is that there is almost zero implementation effort to extend all posterior sampling-based Bayesian learning models to have differentially privacy of any specified .\nAssumption on the boundedness. The boundedness on the loss-function (log-likelihood here) is a standard assumption in many DP works (Chaudhuri et al., 2011; Bassily et al., 2014; Song et al., 2013; Kifer et al., 2012). Lipschitz constant L is usually small for continuous distributions (at least when the parameter space \u0398 is bounded). This is a bound on log p(x|\u03b8)) so as long as p(x|\u03b8) does not increase or decrease super exponentially fast at any point, L will be a small constant. R can also be made small by a simple preprocessing step that scales down all data points. In the aforementioned papers that assume L, it is typical that they also assume R = 1 for convenience. So we will do the same. In practice, we can algorithmically remove large data points from the data by some predefined threshold or using the \u201cPropose-Test-Release\u201d framework in (Dwork & Lei, 2009) or perform weighted training where we can assign lower weight to data points with large magnitude. Note that this is a desirable step for the robustness to outliers too. Exponential families (in Hilbert space) are an example, see e.g. Bialek et al. (2001); Hofmann et al. (2008); Wainwright & Jordan (2008)."}, {"heading": "3.2 Consistency and Near-Optimality", "text": "Now we move on to study the consistency of the OPS estimator. In great generality, we will show that the one-posterior sample estimator is consistent whenever the Bayesian model is posterior consistent. Since the consistency in Bayesian methods can have different meanings, we briefly describe two of them according to the nomenclature in Orbanz (2012). Definition 5 (Posterior consistency in the Bayesian Sense). For a prior \u03c0, we say the model is posterior consistent in the Bayesian sense, if \u03b8 \u223c \u03c0(\u03b8), x1, ...,xn \u223c p\u03b8, and the posterior \u03c0(\u03b8|x1, ...,xn) weakly\u2212\u2192 \u03b4\u03b8 a.s. \u03c0.\n\u03b4\u03b8 is the dirac-delta function at \u03b8.\nIn great generality, Doob\u2019s well-known theorem guarantees posterior consistency in the Bayesian sense for a model with any prior under no conditions except identifiability and measurability. A concise statement of Doob\u2019s result can be found in Van der Vaart (2000, Theorem 10.10)).\nAn arguably more reasonable definition is given below. It applies to the case when the statistician who chooses the prior \u03c0 does not know about the true parameter. Definition 6 (Posterior consistency in the Frequentist Sense). For a prior \u03c0, we say the model is posterior consistent in the Frequentist sense, if for every \u03b80 \u2208 \u0398, x1, ...,xn \u223c p\u03b8, the posterior\n\u03c0(\u03b8|x1, ...,xn) weakly\u2212\u2192 \u03b4\u03b80 a.s. p\u03b80 .\nThis type of consistency is much harder to satisfy especially when \u0398 is an infinite dimensional space, in which case the consistency often depends on the specific priors to use. A promising series of results on the consistency for Bayesian nonparametric models can be found in Ghosal (2010)).\nRegardless which definition one favors, the key notion of consistency is that the posterior distribution to concentrates around the true underlying \u03b8 that generates the data. Proposition 7. The one-posterior sample estimator is consistent if and only if the Bayesian model is posterior consistent (in either Definition 5 or 6 ).\nProof. The equivalence follows from the standard equivalence of convergence weakly and convergence in probability when a random variable converges weakly to a point mass.\nHow about the rate of convergence? In the low dimensional setting when \u03b8 \u2208 \u0398 \u2282 Rd and p\u03b8(x) is suitably differentiable and the prior is supported at the neighborhood of the true parameter, then by the Bernstein-von Mises theorem (Le Cam, 1986), the posterior mean is an asymptotically efficient estimator and the posterior distribution converges in L1-distance to a normal distribution with covariance being the inverse Fisher Information. Proposition 8. Under the regularity conditions where Bernstein-von Mises theorem holds, the One-Posterior sample \u03b8\u0302 \u223c \u03c0(\u03b8|x1, ..,xn) obeys\n\u221a n(\u03b8\u0302 \u2212 \u03b80) weakly\u2212\u2192 N (0, 2I\u22121),\ni.e., the One-Posterior sample estimator has an asymptotic relative efficiency of 2.\nProof. Let the One-Posterior sample \u03b8\u0302 \u223c \u03c0(\u03b8|x1, ..,xn). By Bernstein-von Mises theorem \u221a n(\u03b8\u0302 \u2212 \u03b8\u0303) weakly\u2192 N (0, I\u22121). By the asymptotical normality and efficiency of the posterior mean estimator \u221a n(\u03b8\u0303 \u2212 \u03b80)\nweakly\u2192 N (0, I\u22121). The proof is complete by taking the sum of the two asymptotically independent Gaussian vectors (\u03b8\u0303 and \u03b8\u0302 \u2212 \u03b8\u0303 are asymptotically independent).\nThe above proposition suggests that in many interesting classes of parametric Bayesian models, the One-Posterior Sample estimator is asymptotically near optimal. Similar statements can also be obtained for some classes of semi-parametric and nonparametric Bayesian models (Ghosal, 2010), which we leave as future work.\nThe drawback of the above two propositions is that they are only stated for the version of the OPS when = 4B. Using results in De Blasi & Walker (2013) and Kleijn et al. (2012) for Bayesian learning under misspecified models, we can prove consistency, asymptotic normality for any and parameterize the asymptotic relative efficiency of the OPS estimator as a function of . The key idea is that when scaling the log-likelihood and sample from a different distribution, we are essentially fitting a model that may not include the data-generating true distribution. De Blasi & Walker (2013) shows that under mild conditions, when the model is misspecified, the posterior distribution will converge to a point mass \u03b8\u2217 that minimizes the KL-divergence between between the true distribution and the corresponding distribution in the misspecified model. \u03b8\u2217 is essentially MLE and in our case, since we only scaled the distribution, the MLE will remain exactly the same. De Blasi & Walker (2013)\u2019s result is quite general and covers both parametric and nonparametric Bayesian models and whenever their assumptions hold, the OPS estimator is consistent. Using a similar argument and the modifed Bernstein-Von-Mises theorem in Kleijn et al. (2012), we can prove asymptotic normality and near optimality for the subset of problems where regularities of MLE hold. Proposition 9. Under the same assumption as Proposition 8, if we set a different by rescaling the log-likelihood by a factor of 4B , then the the One-Posterior sample estimator obeys\n\u221a n(\u03b8\u0302 \u2212 \u03b80) weakly\u2212\u2192 N ( 0, (1 + 4B )I\u22121 ) ,\nin other word, the estimator has an ARE of (1 + 4B ).\nProof. By scaling the log-likelihood, we are essentially changing the correct model p\u03b8 to a misspecified model (p\u03b8) 4B . Let the true log-likelihood be ` and the misspecified log-likelihood be \u02dc\u0300= 4B `, in addition, define\nV (\u03b8) := E\u03b8\u2207\u02dc\u0300(\u03b8)\u2207\u02dc\u0300(\u03b8)T = 2\n16B2 E\u03b8\u2207`(\u03b8)\u2207`(\u03b8)T =\n2\n16B2 I(\u03b8)\nJ(\u03b8) := \u2212E\u03b8\u22072 \u02dc\u0300(\u03b8) = \u2212 4B E\u03b8\u22072`(\u03b8) = \u2212 4B I(\u03b8).\nThe last equality holds under the standard regularity conditions. By the sandwich formula, the maximum likelihood estimator \u03b8\u0302 under the misspecified model is asymptotically normal:\n\u221a n(\u03b8\u0302 \u2212 \u03b8\u2217) weakly\u2192 N (0, J\u22121V J(\u22121)) = N (0, I\u22121)\nwhere \u03b8\u2217 defines the closest (in terms of KL-divergence) model in the misspecified class of distributions to the true distribution that generates the data. Since the difference is only in scaling, the minimum KL-divergence is obtained at \u03b8\u2217 = \u03b8. Now under the same regularity conditions, we can invoke the modified Bernstein-Von-Mises theorem for misspecified models (Kleijn et al., 2012, Lemma 2.2), which says that the posterior distribution p(\u03b8|Xn) (of the misspecified model) converges in distribution to N (\u03b8\u0302, (nJ)\u22121). In our case, (nJ)\u22121 = 4Bn I\n\u22121. The proof is concluded by noting that the posterior sample is an independent draw.\nWe make a few interesting remarks about the result.\n1. Proposition 9 suggests that for models with bounded log-likelihood, OPS is only a factor of (1+4B/ ) away from being optimal. This is in sharp contrast to most previous statistical analysis of DP methods that are only tight up to a numerical constant\n(and often a logarithmic term). In `2-norm, the convergence rate is O(\n\u221a 1+4B/ \u2016I\u22121\u2016F\u221a\nn ).\nThe bound depends on the dimension through the Frobenius norm which is usually O( \u221a d). The bound can be further sharpened using assumptions on the intrinsic rank, incoherence conditions or the rate of decays in eigenvalues of the Fisher information.\nIn `\u221e-norm, the convergence rate is\n\u221a 1+4B/ \u2016I\u22121\u20162\u221a\nn , which does not depend on the\ndimension of the problem.\n2. Another implication is on statistical inference. Proposition 9 essentially generalizes that classic results in hypothesis testing and confidence intervals, e.g., Wald test, generalized likelihood ratio test, can be directly adopted for the private learning problems, with an appropriate calibration using . We can control the type I error in an asymptotically exact fashion. In addition, the trade-off with and the test power is also explicitly described, so in cases where the power of the tests are wellstudied (Lehmann & Romano, 2006), the same handle can be used to analyze the most-powerful-test under privacy constraints.\n3. Lastly, the results in De Blasi & Walker (2013) and Kleijn et al. (2012) are much more general. It is easy to extend the guarantee for OPS to handle private Bayesian learning in a fully agnostic setting and in non-iid cases. We will leave the formalization of these claims as future directions."}, {"heading": "3.3 (Efficient) sampling from approximate posterior", "text": "The privacy guarantee in Theorem 4 requires sampling from the exact posterior. In practice, however, exact samplers are rare. As Bayesian models get more and more complicated, often the only viable option is to use Markov Chain Monte Carlo (MCMC) samplers which are almost never exact. There are exceptions, e.g., Propp & Wilson (1998) but they only apply to problems with very special structures. A natural question to ask is whether we can\nstill say something meaningful about privacy when the posterior sampling is approximate. It turns out that we can, and the level of approximation in privacy is the same as the level of approximation in the sampling distribution. Proposition 10. If A that sampling from distribution PX preserves -differential privacy, then any approximate sampling procedures A\u2032 that produces a sample from P \u2032X such that \u2016PX \u2212 P \u2032X\u2016L1 \u2264 \u03b4 for any X preserves ( , (1 + e )\u03b4)-differential privacy.\nProof. For any S \u2208 Range(A\u2032), and d(X,X \u2032) \u2264 1\nP ( A\u2032(X) \u2208 S ) = \u222b S dP \u2032X \u2264 \u222b S dPX + \u03b4\n\u2264 e \u222b S dPX\u2032 + \u03b4 \u2264 e \u222b S dPX\u2032\n\u2264 e \u222b S dP \u2032X\u2032 + (1 + e )\u03b4\n= e P ( A\u2032(X \u2032) \u2208 S ) + (1 + e )\u03b4,\nThis is ( , (1 + e )\u03b4)-DP by definition.\nWe are using L1 distance of the distribution because it is a commonly accepted metric to measure the convergence rate MCMC Rosenthal (1995), and Proposition 10 leaves a clean interface for computational analysis in determining the number of iterations needed to attain a specific level of privacy protection.\nA note on computational efficiency. The (unsurprising) bad news is that even approximate sampling from the posterior is NP-Hard in general, see, e.g. Sontag & Roy (2011, Theorem 8). There are however interesting results on when we can (approximately) sample efficiently. Approximation is easy for sampling LDA when \u03b1 > 1 while NP-Hard when \u03b1 < 1. A more general result in Applegate & Kannan (1991) suggests that we can get a sample with arbitrarily close approximation in polynomial time for a class of near log-concave distributions. The log-concavity of the distributions would imply convexity in the log-likelihood, thus, this essentially confirms the computational efficiency of all convex empirical risk minimization problems under differential privacy constraint (see Bassily et al. (2014)).\nThe nice thing is that since we do not modify the form of the sampling algorithm at all, the OPS algorithm is going to be a computationally tractable DP method whenever the Bayesian learning model of interest is proven to be computationally tractable.\nThis observation provides an interesting insight into the problem of computational lower bound of differential private machine learning. Unlike what is conjectured in Dwork et al.\n(2014), our observation seems to suggest that the computational barrier is not specific to differential privacy, but rather the barrier of learning in general. The argument seems to hold at least for some class of problems, where the posterior sample achieves the optimal statistical rate and is at least 4B-DP."}, {"heading": "3.4 Discussions and comparisons", "text": "OPS has a number of advantages over the state-of-the-art differentially private ERM method: objective perturbation (Chaudhuri et al., 2011; Kifer et al., 2012) (ObjPert from here onwards). OPS works with arbitrary bounded loss functions and priors while ObjPert needs a number of restrictive assumptions including twice differentiable loss functions, strongly convexity parameter to be greater than a threshold and so on. These restrictions rule out many commonly used loss functions, e.g., `1-loss, hinge loss, Huber function just to name a few.\nAlso, ObjPert \u2019s privacy guarantee holds only for the exact optimal solution, which is often hard to get in practice. In contrast, OPS works when the sample is drawn from an approximate posterior distribution. From a practical point of view, since OPS stems from the intrinsic privacy protection of Bayesian learning, it requires very little implementation effort to deploy it for practical applications. It also requires the problem to be strong convexity with a minimum strong convexity parameter. When the condition is not satisfied, ObjPert will need to add additional quadratic regularization to make it so, which may bias the problem unnecessarily."}, {"heading": "4 Stochastic Gradient MCMC and ( , \u03b4)-Differential privacy", "text": "Given a fixed privacy budget, we see that the single posterior sample produces an optimal point estimate, but what if we want multiple samples? Can we use the privacy budget in a different way that produces many approximate posterior samples?\nIn this section we will provide an answer to it by looking at a class of Stochastic Gradient MCMC techniques developped over the past few years. We will show that they are also differentially private for free if the parameters are chosen appropriately.\nThe idea is to simply privately release an estimate of the gradient (as in Song et al. (2013); Bassily et al. (2014)) and leverage upon the following two celebrated lemmas in differential privacy in the same way as Bassily et al. (2014) does in deriving the near-optimal ( , \u03b4)-differentially private SGD.\nThe first lemma is the advanced composition which allows us to trade off a small amount of \u03b4 to get a much better bound for the privacy loss due to composition.\nLemma 11 (Advanced composition, c.f.,Theorem 3.20 in (Dwork & Roth, 2013)). For all , \u03b4, \u03b4\u2032 \u2265 0, the class of ( , \u03b4)-DP mechanisms satisfy ( \u2032, k\u03b4 + \u03b4\u2032)-DP under k-fold adaptive composition for:\n\u2032 = \u221a 2k log(1/\u03b4\u2032) + k (e \u2212 1).\nRemark 12. When = c\u221a 2k log(1/\u03b4\u2032)\n< 1 for some constant c < \u221a log(1/\u03b4\u2032), we can simplify\nthe above expression into \u2032 \u2264 2c. To see this, apply the inequality e \u2212 1 \u2264 2 (easily shown via Taylor\u2019s theorem and the assumption that \u2264 1).\nIn addition, we will also make use of the following lemma due to Beimel et al. (2014). Lemma 13 (Privacy for subsampled data. Lemma 4.4 in Beimel et al. (2014).). Over a domain of data sets XN , if an algorithm A is ( , \u03b4) differentially private (with < 1), then for any data set X \u2208 XN , running A on a uniform random \u03b3N -entries of X ensures (2\u03b3 , \u03b4)-DP.\nTo make sense of the above lemma, notice that we are subsampling uniform randomly and the probability of any single data point being sampled is only \u03b3. Thus, if we arbitrarily perturb one of the data points, its impact is evenly spread across all data points thanks to random sampling.\nLet f : X n \u2192 Rd be an arbitrary d-dimensional function. Define the `2 sensitivity of f to be\n\u22062f = sup Y :d(X,Y )\u22641\n\u2016f(X)\u2212 f(Y )\u20162.\nSuppose we want to output f(X) differential privately, \u201cGaussian Mechanism\u201d output f\u0302(X) = f(X) +N (0, \u03c32Id) for some appropriate \u03c3. Theorem 14 (Gaussian Mechanism, c.f. Dwork & Roth (2013)). Let \u2208 (0, 1) be arbitrary.\n\u201cGaussian Mechanism\u201d with \u03c3 \u2265 \u22062f \u221a 2 log(1.25/\u03b4)/ is ( , \u03b4)-differentially private.\nThis will be the main workhorse that we use here."}, {"heading": "4.1 Stochastic Gradient Langevin Dynamics", "text": "SGLD iteratively update the parameters to by running a perturbed version of the minibatch stochastic gradient descent on the negative log-posterior objective function\n\u2212 N\u2211 i=1 log p(xi|\u03b8)\u2212 log \u03c0(\u03b8) =: N\u2211 i=1 `(xi;\u03b8) + r(\u03b8)\nwhere `(xi;\u03b8) and r(\u03b8) are loss-function and regularizer under the empirical risk minimization.\nIf one were to run stochastic gradient descent or any other optimization tools on this, one would eventually a deterministic maximum a posteriori estimator. SGLD avoids this by adding noise in every iteration. At iteration t SGLD first samples uniform randomly \u03c4 data points {xt1 , ...,xt2} and then updates the parameter using\n\u03b8t+1 = \u03b8t \u2212 \u03b7t ( \u2207r(\u03b8) + N\n\u03c4 \u03c4\u2211 i=1 \u2207`(xti|\u03b8)\n) + zt, (1)\nwhere zt \u223c N (0, \u03b7t) and \u03c4 is the mini-batch size.\nFor the ordinary stochastic gradient descent to converge in expectation, the stepsize \u03b7t can be chosen as anything that \u2211\u221e i=1 \u03b7t =\u221e and \u2211\u221e i=1 \u03b7 2 t <\u221e (Robbins & Monro, 1951). Typically, one can chooses stepsize \u03b7t = a(b + t) \u2212\u03b3 with \u03b3 \u2208 (0.5, 1]. In fact, it is shown that for general convex functions and \u00b5-strongly convex functions 1\u221a t and 1\u00b5t can be used\nto obtain the minimax optimal O(1/ \u221a t) and O(1/t) rate of convergence. These results substantiate the first phase of SGLD: a convergent algorithm to the optimal solution. Once it gets closer, however, it transforms into a posterior sampler. According to Welling & Teh (2011) and later formally proven in Sato & Nakagawa (2014), if we choose \u03b7t \u2192 0, the random iterates \u03b8t of SGLD converges in distribution to the p(\u03b8|X). The idea is that as the stepsize gets smaller, the stochastic error from the true gradient due to the random sampling of the minibatch converges to 0 faster than the injected Gaussian noise.\nIn addition, if we use some fixed stepsize lower bound, such that \u03b7t = max{1/(t+ 1), \u03b70} (to alleviate the slow mixing problem of SGLD), the results correspond to a discretization approximation of a stochastic differential equation (Fokker-Planck equation), which obeys the following theorem due to Sato & Nakagawa (2014) (simplified and translated to our notation). Theorem 15 (Weak convergence (Sato & Nakagawa, 2014)). Assume f(\u03b8|X) is differentiable, \u2207f(\u03b8|X) is gradient Lipschitz and bounded 2. Then\u2223\u2223E\u03b8\u223cp(\u03b8|X)[h(\u03b8)]\u2212 E\u03b8\u223cSGLD[h(\u03b8(t))]\u2223\u2223 = O(\u03b7t), for any continuous and polynomial growth function h.\nThis theorem implies that one can approximate the posterior mean (and other estimators) using SGLD. Finite sample properties of SGLD is also studied in (Vollmer et al., 2015).\nNow we will show that with a minor modification to just the \u201cburn-in\u201d phase of SGLD, we will be able to make it differentially private (see Algorithm 2). Theorem 16 (Differentially private Minibatch SGLD). Assume initial \u03b81 is chosen independent of the data, also assume `(x|\u03b8) is L-smooth in \u2016 \u00b7 \u20162 for any x \u2208 X and \u03b8 \u2208 \u0398.\n2We use boundedness to make the presentation simpler. Boundedness trivially implies the linear growth condition in Sato & Nakagawa (2014, Assumption 2)."}, {"heading": "In addition, let , \u03b4, \u03c4, T be chosen such that T \u2265 2N32\u03c4 log(2/\u03b4) . Then Algorithm 2 preserves", "text": "( , \u03b4)-differential privacy. Proof. In every iteration, the only data access is \u2211\ni\u2208S \u2207`(xi|\u03b8) and by the L-Lipschitz condition, the sensitivity of \u2211 i\u2208S \u2207`(xi|\u03b8) is at most 2L. Get the essential noise that is\nadded to \u2211\ni\u2208S \u2207`(xi|\u03b8) by removing the N2\u03b72t \u03c42 factor from the variance \u03c32 in the algorithm,\nand Gaussian mechanism, ensures the privacy loss to be smaller than \u221a N\u221a\n32\u03c4T log(2/\u03b4) with\nprobability > 1\u2212 \u03c4\u03b42NT .\nUsing the same technique in Bassily et al. (2014), we can further exploit the fact that the subset S that we use to compute the stochastic gradient is chosen uniformly randomly. By Lemma 13, the privacy loss for this iteration is in fact\n\u221a N\u221a\n32\u03c4T log(2/\u03b4) \u00b7 2\u03c4 N\n= /2\u221a\n2(NT/\u03c4) log(2/\u03b4) .\nVerify that we can indeed do that as \u221a N\u221a\n32\u03c4T log(2/\u03b4) < 1 from the assumption on T . Note that\nto get T data passes with minibatches of size \u03c4 , we need to go through at most bNT\u03c4 c \u2264 NT \u03c4 iterations. Apply the advanced composition theorem (Remark 12), we get an upper bound of the total privacy loss and failure probability \u03b4 = \u03b42 + \u03c4\u03b4 2NT \u00b7 NT \u03c4 accordingly.\nThe proof is complete by noting that choosing a larger noise level when \u03b7t is bigger can only reduces the privacy loss under the same failure probability.\n\u03b1-Phase transition. For any \u03b1 \u2208 (0, 1), if we choose \u03b7t = \u03b1 2\n128L2 log(2.5NT/(\u03c4\u03b4)) log(2/\u03b4)t ,\nthen whenever t > \u03b1NT/\u03c4 , then we are essentially running SGLD for the last (1\u2212\u03b1)NT/\u03c4 iterations, and we can collect approximate posterior samples from there.\nAlgorithm 2 Differentially Private Stochastic Gradient Langevin Dynamics (DP-SGLD)\nRequire: Data X of size N , Size of minibatch \u03c4 , number of data passes T , privacy parameter , \u03b4, Lipschitz constant L and initial \u03b81. Set t = 1. for t = 1 : bNT/\u03c4c do\n1. Random sample a minibatch S \u2282 [N ] of size \u03c4 . 2. Sample each coordinate of zt iid from N ( 0, 128NTL 2 \u03c4 2 log ( 2.5NT \u03c4\u03b4 ) log(2/\u03b4)\u03b72t \u2228 \u03b7t ) .\n3. Update \u03b8t+1 \u2190 \u03b8t \u2212 \u03b7t ( \u2207r(\u03b8) + N\u03c4 \u2211 i\u2208S \u2207`(xi|\u03b8) ) + zt, 4. Return \u03b8t+1 as a posterior sample (after a pre-defined burn-in period). 5. Increment t\u2190 t+ 1.\nend for\nSmall constant \u03b70. Instead of making \u03b7t to converge to 0 as t increases, we may alternatively use constant \u03b70 after t is larger than a threshold. This is a suggested heuristic in Welling & Teh (2011) and is inline with the analysis in Sato & Nakagawa (2014) and Vollmer et al. (2015).\nChoice of T and \u03c4 By Bassily et al. (2014), it takes at least N data passes to converge in expectation to a point near the minimizer, so taking T = 2N is a good choice. The variance of both random components in our stochastic gradient is smaller when we use larger \u03c4 . Smaller variances would improve the convergence of the stochastic gradient methods and make the SGLD a better approximation to the full Langevin Dynamics. The trade-off is that when \u03c4 is too large, we will use up the allowable T datapasses with just O(T ) iterations and the number of posterior samples we collect from the algorithm will be small.\nOvercoming the large-noise in the \u201cBurn-in\u201d phase When the stepsize \u03b7t is not small enough initially, we need to inject significantly more noise than what SGLD would have to ensure privacy. We can overcome this problem by initializing the SGLD sampler with a valid output of the OPS estimator, modified according to the exponential mechanism so that the privacy loss is calibrated to /2. As the initial point is already in the high probability region of the posterior distribution, we no longer need to \u201cBurn-in\u201d the Monte Carlo sampler so we can simply choose a sufficiently small constant stepsize so that it remains a valid SGLD. This algorithm is summarized in Algorithm 3.\nComparing to OPS The privacy claim of DP-SGLD is very different from OPS . It does not require sampling to be nearly correct to ensure differential privacy. In fact, DP-SGLD privately releases the entire sequence of parameter updates, thus ensures differential privacy even if the internal state of the algorithm gets hacked. However, the quality of the samples is usually worse than OPS due to the random-walk like behavior. The interesting fact, however, is that if we run SGLD indefinitely without worrying about the stronger notion of internal privacy, it leads to a valid posterior sample. We can potentially modify the\nAlgorithm 3 Hybrid Posterior Sampling Algorithm Require: Data X of size N , log-likelihood function `(\u00b7|\u03b8) with Lipschitz constant L in the first argument, assume supx\u2208X \u2016x\u2016, a prior \u03c0. Privacy requirement . 1. Run OPS estimator: Algorithm 1 with /2. Collect sample point \u03b80 2. Run DP-SGLD (Algorithm 2) or other Stochatic Gradient Monte Carlo algorithms and collect samples. output : Return all samples.\nposterior distribution to sample from into the \u201cscaled\u201d version so as to balancing the two ways of getting privacy."}, {"heading": "4.2 Hamiltonian Dynamics, Fisher Scoring and Nose-Hoover Thermostat", "text": "One of the practical drawback of SGLD is its random walk-like behavior which slows down the mixing significantly. In this section, we describe three extensions of SGLD that attempts to resolve the issue by either using auxiliary variables to counter the noise in the stochastic gradient(Chen et al., 2014; Ding et al., 2014), or to exploit second order information so as to use Newton-like updates with large stepsize (Ahn et al., 2012).\nWe note that in all these methods, stochastic gradients are the only form of data access, therefore similar results like what we described for SGLD follow nicely. We briefly describe each method and how to choose their parameters for differential privacy.\nStochastic Gradient Hamiltonian Monte Carlo. According to Neal (2011), Langevin Dynamics is a special limiting case of Hamiltonian Dynamics, where one can simply ignore the \u201cmomentum\u201d auxiliary variable. In its more general form, Hamiltonian Monte Carlo (HMC) is able to generate proposals from distant states and hence enabling more rapid exploration of the state space. Chen et al. (2014) extends the full \u201cleap-frog\u201d method for HMC in Neal (2011) to work with stochastic gradient and add a \u201cfriction\u201d term in the dynamics to \u201cde-bias\u201d the noise in the stochastic gradient.{\n\u03b8t = \u03b8t\u22121 + htrt\u22121 pt = pt\u22121 \u2212 ht\u2207\u0302 \u2212 \u03b7tApt\u22121 +N (0, 2(A\u2212 B\u0302)ht).\n(2)\nwhere B\u0302 is a guessed covariance of the stochastic gradient (the authors recommend restricting B\u0302 to a single number or a diagonal matrix) and A can be arbitrarily chosen as long as A B\u0302. If the stochastic gradient \u2207\u0302 \u223c N (\u2207, B) for some B and B\u0302 = B, then this dynamics is simulating a dynamic system that yields the correct distribution. Note that even if the normal assumption holds and we somehow set B\u0302 = B, we still requires ht to go to 0 to sample from the actual posterior distribution, and as ht converges to 0 the additional noise we artificially inject dominates and we get privacy for free. All we need to do is to set A, B\u0302 and ht so that 2(A\u2212 B\u0302)/ht 128NTL 2 \u03c4 2 log ( 2.5NT \u03c4\u03b4 ) log(2/\u03b4)In. Note that as ht \u2192 0 this quickly becomes true.\nStochastic Gradient Nose\u0301-Hoover Thermostat As we discussed, the key issue about SGHMC is still in choosing B\u0302. Unless B\u0302 is chosen exactly as the covariance of true stochastic\ngradient, it does not sample from the correct distribution even as ht \u2192 0 unless we trivially set B\u0302 = 0. The Stochastic Gradient Nose\u0301-Hoover Thermostat (SGNHT) overcomes the issue by introducing an additional auxiliary variable \u03be, which serves as a thermostat to absorb the unknown noise in the stochastic gradient. The update equations of SGNHT are given below  pt = pt\u22121 \u2212 \u03bet\u22121pt\u22121ht \u2212 \u2207\u0302ht +N (0, 2Aht);\u03b8t = \u03b8t\u22121 + htpt\u22121;\n\u03bet = \u03bet\u22121 + ( 1 np T t pt \u2212 1)ht.\n(3)\nSimilar to the case in SGHMC, appropriately selected discretization parameter ht and the friction term A will imply differential privacy.\nChen et al. (2014); Ding et al. (2014) both described a reformulation that can be interpret as SGD with momentum. This is by setting parameters \u03b7 = h2, a = hA, b\u0302 = hB\u0302 for SGHMC: {\n\u03b8t = \u03b8t\u22121 + vt\u22121 vt = vt\u22121 \u2212 \u03b7t\u2207\u0302 \u2212 av +N (0, 2(a\u2212 b\u0302)\u03b7tI);\n(4)\nand v = ph, \u03b7t = h 2 t , \u03b1 = \u03beh and a = Ah for SGNHT: vt = vt\u22121 \u2212 \u03b1t\u22121vt\u22121 \u2212 \u2207\u0302(\u03b8t\u2212s)\u03b7t +N (0, 2a\u03b7tI);\u03b8t = \u03b8t\u22121 + ut\u22121; \u03b1t = \u03b1t\u22121 + ( 1 nv T t vt \u2212 \u03b7t).\n(5)\nwhere 1\u2212a is the momentum parameter and \u03b7 is the learning rate in the SGD with momentum. Again note that to obtain privacy, we need 2a\u03b7t \u2265 128NTL2 \u03c4 2 log(2NT\u03c4\u03b4 ) log(1/\u03b4).\nNote that as \u03b7t gets smaller, we have the flexibility of choosing a and \u03b7t within a reasonable range.\nStochastic Gradient Fisher Scoring Another extension of SGLD is Stochastic Gradient Fisher Scoring (SGFS), where Ahn et al. (2012) proposes to adaptively interpolate between a preconditioned SGLD (see preconditioning (Girolami & Calderhead, 2011)) and a Markov Chain that samples from a normal approximation of the posterior distribution. For parametric problem where Bernstein-von Mises theorem holds, this may be a good idea. The heuristic used in the SGFS is that the covariance matrix of \u03b8|X, which is also the inverse Fisher information I\u22121N is estimated on the fly. The key features of SGFS is that one can use the stepsize to trade off speed and accuracy, when the stepsize is large, it mixes rapidly to the normal approximation, as the stepsize gets smaller the stationary distribution converges to the true posterior. Further details of SGFS and ideas to privatize it is described in the appendix."}, {"heading": "4.3 Discussions and caveats.", "text": "So far, we have proposed a differentially private Bayesian learning algorithm that is memory efficient, statistically near optimal for a large class of problems, and we can release many intermediate iterates to construct error bars. Given that differential privacy is usually very restrictive, some of these results may appear too good to be true. This is a reasonable suspicion due to the following caveats.\nSmall \u03b7 helps both privacy and accuracy. It is true that as \u03b7 goes to 0, the stationary distribution that these method samples from gets closer to the target distribution. On the other hand, since the variance of the noise we need to add for privacy scales in O(\u03b72) and that for posterior sampling scales like O(\u03b7), privacy and accuracy benefits from the same underlying principle. The caveat is that we also have a budget on how many samples can we collect. Also the smaller the stepsize \u03b7 is, the slower it mixes, as a result, the samples we collect from the monte carlo sampler is going to be more correlated to each other.\nAdaptivity of SGNHT. While SGNHT is able to adaptively adjust the temperature so that the samples that it produces remain \u201cunbiased\u201d in some sense as \u03b7 \u2192 0. The reality is that if the level of noise is too large, either we adjust the stepsize to be too small to search the space at all, or the underlying stochastic differential equation becomes unstable and quickly diverges. As a result, the adaptivity of SGNHT breaks down if the privacy parameter gets to small.\nComputationally efficiency. For a large problem, it is usually the case that we would like to train with only one pass of data or very small number of data passes. However, due to the condition in Lemma 13, our result does not apply to one pass of data unless \u03c4 is chosen to be as large as N . While we can still choose T to be sufficiently large and stop early, but we amount of noise that we add in each iteration will remain the same.\nThe Curse of Numerical constant. The analysis of algorithms often involves larger numerical constants and polylogarithmic terms in the bound. In learning algorithms these are often fine because there are more direct ways to evaluate and compare methods\u2019 performances. In differential privacy however, constants do matter. This is because we need to use these bounds (including constants) to decide how much noise or perturbation we need to inject to ensure a certain degree of privacy. These guarantees are often very conservative, but it is intractable to empirically evaluate the actual of differential privacy due to its \u201cworst\u201d case definition. Our stochastic gradient based differentially private sampler suffers from exactly that. For moderate data size, the product of the constant and logarithmic\nterms can be as large as a few thousands. That is the reason why it does not perform as well as other methods despite the theoretically being optimal in scaling (the optimality result is due to SGD (Bassily et al., 2014))."}, {"heading": "5 Experiments", "text": "Figure 1 is a plain illustration of how these stochastic gradient samplers work using a randomly generated linear regression model (note the its posterior distribution will be normal, as the contour illustrates). On the left, it shows how these methods converge like stochastic gradient descent to the basin of convergence. Then it becomes a posterior sampler. The figure on the right shows that the stochastic gradient thermostat is able to produce more accurate/unbiased result and the impact of differential privacy at the level of = 10 becomes negligible.\nTo evaluate how our proposed methods work in practice, we selected two binary classification datasets: Abalone and Adult, from the first page of UCI Machine Learning Repository and performed privacy constrained logistic regression on them. Specifically, we compared two of our proposed methods, OPS mechanism and hybrid algorithm against the stateof-the-art empirical risk minimization algorithm ObjPert (Chaudhuri et al., 2011; Kifer et al., 2012) under varying level of differential privacy protection. The results are shown in Figure 2. As we can see from the figure, in both problems, OPS significantly improves the classification accuracy over ObjPert . The hybrid algorithm also works reasonably well, given that it collected N samples after initializing it from the output of a run of OPS with privacy parameter /2. For fairness, we used the ( , \u03b4)-DP version of the objective perturbation (Kifer et al., 2012) and similarly we used Gaussian mechanism (rather than\nLaplace mechanism) for output perturbation. All optimization based methods are solved using BFGS algorithm to high numerical accuracy. OPS is implemented using SGNHT and we ran it long enough so that we are confident that it is a valid posterior sample. Minibatch size and number of data passes in the hybrid DP-SGNHT are chosen to be both \u221a N .\nWe note that the plain DP-SGLD and DP-SGNHT without an initialization using OPS does not work nearly as well. In our experiments, it often performs equally or slightly worse than the output perturbation. This is due to the few caveats (especially \u201cthe curse of numerical constant\u201d) we described earlier."}, {"heading": "6 Related work", "text": "We briefly discuss related work here. For the first part, we become aware recently that Mir (2013) and Dimitrakakis et al. (2014) independently developed the idea of using posterior sampling for differential privacy. Mir (2013, Chapter 5) used a probabilistic bound of the log-likelihood to get ( , \u03b4)\u2212DP but focused mostly on conjugate priors where the posterior distribution is in closed-form. Dimitrakakis et al. (2014) used Lipschitz assumption and bounded data points (implies our boundedness assumption) to obtain a generalized notion of differential privacy. Our results are different in that we also studied the statistical and computational properties. Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded.\nRelated to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known. Integrating Williams & McSherry (2010) with our procedure might lead to some further performance boost, but investigating its effect is beyond the scope of the current paper.\nFor the second part, the idea to privately release stochastic gradient has been well-studied. Song et al. (2013); Bassily et al. (2014) explicitly used it for differentially private stochastic gradient descent. And Rajkumar & Agarwal (2012) used it for private multi-party training. Our Theorem 16 is a simple modification of Theorem 2.1 in Bassily et al. (2014). Bassily et al. (2014) also showed that the differential private SGD using Gaussian mechanism with \u03c4 = 1 matches the lower bound up to constant and logarithmic, so we are confident that not many algorithms can do significantly better than Algorithm 2. Our contribution is to point out the interesting algorithmic structures of SGLD and extensions that preserves differential privacy. The method in Song et al. (2013) requires disjoint minibatches in every data pass, and it requires adding significantly more noise in settings when Lemma 13 applies. Song et al. (2013) are however applicable when we are doing only a small number of data passes and for these cases, it gets a much better constant. Rajkumar & Agarwal (2012)\u2019s setting\nis completely different as it injects a fixed amount of noise to the gradient corresponds to each data point exactly once. In this way, it replicates objective perturbation (Chaudhuri et al., 2011) (assuming the method actually finds the optimal solution).\nObjective perturbation is originally proposed in Chaudhuri et al. (2011) and the ( , \u03b4) version that we refer to first appears in Kifer et al. (2012). Comparing to our two mechanisms that attempts to sample from the posterior, their privacy guarantee requires the solution to be exact while ours does not. In comparison, OPS estimator is differentially private allows the distribution it samples from to be approximate, DP-SGLD on the other hand releases all intermediate results and every single iteration is public."}, {"heading": "7 Conclusion and future work", "text": "In this paper, we described two simple but conceptually interesting examples that Bayesian learning can be inherently differentially private. Specifically, we show that getting one sample from the posterior is a special case of exponential mechanism and this sample as an estimator is near-optimal for parametric learning. On the other hand, we illustrate that the algorithmic procedures of stochastic gradient Langevin Dynamics (and variants) that attempts to sample from the posterior also guarantee differential privacy as a byproduct. Preliminary experiments suggests that the One-Posterior-Sample mechanism works very well in practice and it substantially outperforms earlier privacy mechanism in logistic regression. While suffering from a large constant, our second method is also theoretically and practically meaningful in that it provides privacy protection in intermediate steps.\nTo carry the research forward, we think it is important to identify other cases when the existing randomness can be exploited for privacy. Randomized algorithms such as hashing and sketching, dropout and other randomization used in neural networks might be another thing to look at. More on the application end, we hope to explore the one-posterior sample approach in differentially private movie recommendation. Ultimately, the goal is to make differential privacy more practical to the extent that it can truly solve the real-life privacy problems that motivated its very advent."}, {"heading": "A Stochastic Gradient Fisher Scoring", "text": "A.1 Fisher Scoring and Stochastic Gradient Fisher Scoring\nFisher scoring is simply the Newton\u2019s method for solving maximum likelihood estimation problem. The score function S(\u03b8) is the gradient of the log-likelihood. So intuitively, if we solve the equation S(\u03b8) = 0, we can obtain the maximum likelihood estimate. Often\nthis equation is highly non-linear, so we consider the an iterative update for the linearized score function (or a quadratic approximation of the likelihood) by Taylor expand it at the current point \u03b80\nS(\u03b8) \u2248 S(\u03b80) + I(\u03b80)(\u03b8 \u2212 \u03b80) where I(\u03b80) = \u2212 \u2211n i=1\u2207\u2207T `(Zi; \u03b8) is the observed Fisher information evaluated at \u03b80. By the fact that S(\u03b8\u2217) = 0, and plug in the above equation, we get \u03b8\u2217 = \u03b80 + I \u22121(\u03b80)S(\u03b80) Note that this is a fix point iteration and it gives us an iterative update rule to search for \u03b8\u2217 via\n\u03b8k+1 = \u03b8k + I \u22121(\u03b8k)S(\u03b8k).\nRecall that S is the gradient of the score function and I\u22121 is the covariance of the score function and (under mild regularity conditions) the Hessian of the log-likelihood. As a result, this is often the same as Newton iterations.\nAn intuitive idea to avoid passing the entire dataset in every iteration is to simply replacing the gradient (the score function) with stochastic gradient and somehow estimate the Fisher information. Stochastic Gradient Fisher Scoring can be thought of as a Quasi-Newton method.\nA.2 Privacy extension\nBy invoking a more advanced version of the Gaussian Mechanism, we will show that similar privacy guarantee can be obtained for a modified version of SGFS (described in Algorithm 4) while preserving its asymptotic properties. Specifically, under the assumption that IN is given, when \u03b7t is big, it also samples from a normal approximation (with larger variance), when \u03b7t is small, the private algorithm becomes exactly the same as SGFS. Moreover, for a sequence of samples from the posterior, the online estimate in the Fisher Information converges an O(1/N) approximation of true Fisher Information as in Ahn et al. (2012, Theorem 1).\nThe privacy result relies on a more specific smoothness assumption. Assume that for any parameter \u03b8 \u2208 Rd, and X \u2208 XN the ellipsoid E = FBd defined by transforming the unit ball Bd using a linear map F contains the symmetric polytope spanned by {\u00b1\u2207`(x1, \u03b8), ...,\u00b1\u2207`(xN , \u03b8)}. From a differential private point of view, this implies that \u2207\u03b8`(x, \u03b8)\u2019s sensitivity is different towards different direction. Then the non-spherical gaussian mechanism states Lemma 17 (Non-Spherical Gaussian Mechanism). Output \u2211N i=1\u2207`(xi, \u03b8) + Fw where\nw \u223c N (0, (1+ \u221a 1 log(1/\u03b4))2\n2 Id) obeys ( , \u03b4)-DP.\nAlgorithm 4 Differentially Private Stochastic Gradient Fisher Scoring (DP-SGFS)\nRequire: Data X of size N , Size of minibatch \u03c4 , number of data passes T , stepsize \u03b7t for t = 1, ..., bNT/\u03c4c, a public Lipschitz matrix F , and initial \u03b81. Set t = 1, \u03c32 = 32T log(2.5NT/\u03c4\u03b4) log(2/\u03b4)\nN\u03c4 2\nfor t = 1 : bNT/\u03c4c do 1. Random sample a minibatch S \u2282 [N ] of size \u03c4 , compute g\u0304 = 1\u03c4 \u2211 i\u2208S \u2207`(xi|\u03b8).\n2. Sample Zt \u223c N (0, \u03c32 \u2228 1N2\u03b7t Id), Wij \u223c N (0, 49\u2016F\u2016 4\u03c32). 3. Compute private stochastic gradient and sample covariance matrix\ng\u0303 = g\u0304 + FZt, and V = PSd+\n{ 1\n\u03c4 \u2212 1 \u2211 i\u2208S {\u2207`i(\u03b8t)\u2212 g\u0304} {\u2207`i(\u03b8t)\u2212 g\u0304}T +W\n} .\n4. Update the guessed Fisher Information estimate I\u0302t = (1\u2212 \u03bat)I\u0302t\u22121 + \u03batV . 5. Update and return \u03b8t+1 \u2190 \u03b8t + 2\n( (\u03c4+N)N\n\u03c4 I\u0302t + 4FFT \u03b7t\n)\u22121 (\u2207r(\u03b8t) +Ng\u0303) .\n6. Increment t\u2190 t+ 1. end for\nTheorem 18. Let F be that `(x; \u03b8\u2032) \u2264 `\u03b8 + \u2207`(x; \u03b8)T (\u03b8\u2032 \u2212 \u03b8) + 12\u2016F (\u03b8 \u2032 \u2212 \u03b8)\u20162 for any x \u2208 X , \u03b8 \u2208 \u0398. Moreover, let , \u03b4, \u03c4, T be chosen such that T \u2265 2N32\u03c4 log(2/\u03b4) . Then Algorithm 4 guarantees (2 , 2\u03b4)-differential privacy.\nProof. First of all, \u2016F\u20162 is an upper bound for any \u2207`(x|\u03b8), so by applying Lemma 19 on the every set of subsamples in each iteration, by Gaussian mechanism (Lemma 14) and the invariance to post-processing, we know that V is a private release. Then the proof follows by the same line of argument (subsampling and advanced composition) as in Theorem 16 for g\u0303 and V respectively, then the result follows by applying the simple composition theorem.\nLemma 19 (Sensitivity of the sample covariance operator). Let \u2016x\u2016 \u2264 L for any x \u2208 X , n > 4, then\nsup k,x1,...,xn,x\u2032k\n\u2016C\u0302ov(x1, ..., xk, ..., xn)\u2212 C\u0302ov(x1, ..., x\u2032k, ..., xn)\u2016F \u2264 7L2\nn\u2212 1 .\nProof. We prove by taking the difference of two adjacent covariance matrices and bound the residual.\nC\u0302ov(X \u2032) =C\u0302ov(X) + 1 n\u2212 1 (xxT \u2212 x\u2032[x\u2032]T ) + 1 n(n\u2212 1) (xxT + x\u2032[x\u2032]T \u2212 x[x\u2032]T \u2212 x\u2032xT )\n\u2212 1 n\u2212 1 \u00b5(x\u2212 x\u2032)T \u2212 1 n\u2212 1 (x\u2212 x\u2032)\u00b5T .\nNow assume n > 4 and take the upper bound of every term, we get \u22062 (Cov(X)) \u2264 7L 2\nn\u22121 ."}], "references": [{"title": "Bayesian posterior sampling via stochastic gradient fisher scoring", "author": ["S. Ahn", "A. Korattikara", "M. Welling"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML-12)", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Mixed membership stochastic blockmodels", "author": ["E.M. Airoldi", "D.M. Blei", "S.E. Fienberg", "E.P. Xing"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Airoldi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Airoldi et al\\.", "year": 2009}, {"title": "Sampling and integration of near log-concave functions", "author": ["D. Applegate", "R. Kannan"], "venue": "In Proceedings of the twenty-third annual ACM symposium on Theory of computing , (pp. 156\u2013163)", "citeRegEx": "Applegate and Kannan,? \\Q1991\\E", "shortCiteRegEx": "Applegate and Kannan", "year": 1991}, {"title": "Private empirical risk minimization, revisited", "author": ["R. Bassily", "A. Smith", "A. Thakurta"], "venue": null, "citeRegEx": "Bassily et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bassily et al\\.", "year": 2014}, {"title": "Bounds on the sample complexity for private learning and private data release", "author": ["A. Beimel", "H. Brenner", "S.P. Kasiviswanathan", "K. Nissim"], "venue": "Machine learning ,", "citeRegEx": "Beimel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beimel et al\\.", "year": 2014}, {"title": "Predictability, complexity, and learning", "author": ["W. Bialek", "I. Nemenman", "N. Tishby"], "venue": "Neural computation,", "citeRegEx": "Bialek et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bialek et al\\.", "year": 2001}, {"title": "Differentially private empirical risk minimization", "author": ["K. Chaudhuri", "C. Monteleoni", "A.D. Sarwate"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2011}, {"title": "Stochastic Gradient Hamiltonian Monte Carlo", "author": ["T. Chen", "E.B. Fox", "C. Guestrin"], "venue": "In Proceeding of 31th International Conference on Machine Learning (ICML\u201914)", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Bayesian asymptotics with misspecified models", "author": ["P. De Blasi", "S.G. Walker"], "venue": "Statistica Sinica,", "citeRegEx": "Blasi and Walker,? \\Q2013\\E", "shortCiteRegEx": "Blasi and Walker", "year": 2013}, {"title": "Robust and private bayesian inference", "author": ["C. Dimitrakakis", "B. Nelson", "A. Mitrokotsa", "B.I. Rubinstein"], "venue": "In Algorithmic Learning Theory ,", "citeRegEx": "Dimitrakakis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dimitrakakis et al\\.", "year": 2014}, {"title": "Bayesian sampling using stochastic gradient thermostats", "author": ["N. Ding", "Y. Fang", "R. Babbush", "C. Chen", "R.D. Skeel", "H. Neven"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Differential privacy", "author": ["C. Dwork"], "venue": "Proceedings of the 33rd international conference on Automata, Languages and Programming-Volume Part II , (pp. 1\u201312). Springer-Verlag.", "citeRegEx": "Dwork,? 2006", "shortCiteRegEx": "Dwork", "year": 2006}, {"title": "Preserving statistical validity in adaptive data analysis", "author": ["C. Dwork", "V. Feldman", "M. Hardt", "T. Pitassi", "O. Reingold", "A. Roth"], "venue": null, "citeRegEx": "Dwork et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2014}, {"title": "Differential privacy and robust statistics", "author": ["C. Dwork", "J. Lei"], "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing , (pp. 371\u2013380)", "citeRegEx": "Dwork and Lei,? \\Q2009\\E", "shortCiteRegEx": "Dwork and Lei", "year": 2009}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["C. Dwork", "F. McSherry", "K. Nissim", "A. Smith"], "venue": "In Theory of cryptography ,", "citeRegEx": "Dwork et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2006}, {"title": "The algorithmic foundations of differential privacy", "author": ["C. Dwork", "A. Roth"], "venue": "Theoretical Computer Science,", "citeRegEx": "Dwork and Roth,? \\Q2013\\E", "shortCiteRegEx": "Dwork and Roth", "year": 2013}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Fei.Fei and Perona,? \\Q2005\\E", "shortCiteRegEx": "Fei.Fei and Perona", "year": 2005}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. Pattern Analysis and Machine Intelligence", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Transactions on,", "citeRegEx": "Geman and Geman,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman", "year": 1984}, {"title": "The Dirichlet process, related priors and posterior asymptotics, vol", "author": ["S. Ghosal"], "venue": "2. Chapter.", "citeRegEx": "Ghosal,? 2010", "shortCiteRegEx": "Ghosal", "year": 2010}, {"title": "Riemann manifold langevin and hamiltonian monte carlo methods", "author": ["M. Girolami", "B. Calderhead"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Girolami and Calderhead,? \\Q2011\\E", "shortCiteRegEx": "Girolami and Calderhead", "year": 2011}, {"title": "Monte carlo sampling methods using markov chains and their applications", "author": ["W.K. Hastings"], "venue": "Biometrika, 57 (1), 97\u2013109.", "citeRegEx": "Hastings,? 1970", "shortCiteRegEx": "Hastings", "year": 1970}, {"title": "Kernel methods in machine learning", "author": ["T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "The annals of statistics,", "citeRegEx": "Hofmann et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2008}, {"title": "On the\u2019semantics\u2019 of differential privacy: A bayesian formulation", "author": ["S.P. Kasiviswanathan", "A. Smith"], "venue": "Journal of Privacy and Confidentiality ,", "citeRegEx": "Kasiviswanathan and Smith,? \\Q2014\\E", "shortCiteRegEx": "Kasiviswanathan and Smith", "year": 2014}, {"title": "Private convex empirical risk minimization and high-dimensional regression", "author": ["D. Kifer", "A. Smith", "A. Thakurta"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kifer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kifer et al\\.", "year": 2012}, {"title": "The bernstein-von-mises theorem under misspecification", "author": ["B. Kleijn", "A van der Vaart"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Kleijn and Vaart,? \\Q2012\\E", "shortCiteRegEx": "Kleijn and Vaart", "year": 2012}, {"title": "On the Bernstein-von Mises theorem", "author": ["L.M. Le Cam"], "venue": "Department of Statistics, University of California.", "citeRegEx": "Cam,? 1986", "shortCiteRegEx": "Cam", "year": 1986}, {"title": "Testing statistical hypotheses", "author": ["E.L. Lehmann", "J.P. Romano"], "venue": null, "citeRegEx": "Lehmann and Romano,? \\Q2006\\E", "shortCiteRegEx": "Lehmann and Romano", "year": 2006}, {"title": "Mechanism design via differential privacy", "author": ["F. McSherry", "K. Talwar"], "venue": "In Foundations of Computer Science,", "citeRegEx": "McSherry and Talwar,? \\Q2007\\E", "shortCiteRegEx": "McSherry and Talwar", "year": 2007}, {"title": "Differential privacy: an exploration of the privacy-utility landscape", "author": ["D.J. Mir"], "venue": "Ph.D. thesis, Rutgers University.", "citeRegEx": "Mir,? 2013", "shortCiteRegEx": "Mir", "year": 2013}, {"title": "Mcmc using hamiltonian dynamics", "author": ["R. Neal"], "venue": "Handbook of Markov Chain Monte Carlo, 2 .", "citeRegEx": "Neal,? 2011", "shortCiteRegEx": "Neal", "year": 2011}, {"title": "Lecture notes on bayesian nonparametrics", "author": ["P. Orbanz"], "venue": "Journal of Mathematical Psychology , 56 , 1\u201312.", "citeRegEx": "Orbanz,? 2012", "shortCiteRegEx": "Orbanz", "year": 2012}, {"title": "Statistical parametric mapping: the analysis of functional brain images: the analysis of functional brain", "author": ["W.D. Penny", "K.J. Friston", "J.T. Ashburner", "S.J. Kiebel", "T.E. Nichols"], "venue": null, "citeRegEx": "Penny et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Penny et al\\.", "year": 2011}, {"title": "Coupling from the past: a users guide", "author": ["J. Propp", "D. Wilson"], "venue": "Microsurveys in Discrete Probability ,", "citeRegEx": "Propp and Wilson,? \\Q1998\\E", "shortCiteRegEx": "Propp and Wilson", "year": 1998}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE , 77 (2), 257\u2013286.", "citeRegEx": "Rabiner,? 1989", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "A differentially private stochastic gradient descent algorithm for multiparty classification", "author": ["A. Rajkumar", "S. Agarwal"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Rajkumar and Agarwal,? \\Q2012\\E", "shortCiteRegEx": "Rajkumar and Agarwal", "year": 2012}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "The annals of mathematical statistics,", "citeRegEx": "Robbins and Monro,? \\Q1951\\E", "shortCiteRegEx": "Robbins and Monro", "year": 1951}, {"title": "Minorization conditions and convergence rates for markov chain monte carlo", "author": ["J.S. Rosenthal"], "venue": "Journal of the American Statistical Association, 90 (430), 558\u2013566.", "citeRegEx": "Rosenthal,? 1995", "shortCiteRegEx": "Rosenthal", "year": 1995}, {"title": "Approximation analysis of stochastic gradient langevin dynamics by using fokker-planck equation and ito process", "author": ["I. Sato", "H. Nakagawa"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML-14),", "citeRegEx": "Sato and Nakagawa,? \\Q2014\\E", "shortCiteRegEx": "Sato and Nakagawa", "year": 2014}, {"title": "Efficient, differentially private point estimators", "author": ["A. Smith"], "venue": "arXiv preprint arXiv:0809.4794 .", "citeRegEx": "Smith,? 2008", "shortCiteRegEx": "Smith", "year": 2008}, {"title": "Stochastic gradient descent with differentially private updates", "author": ["S. Song", "K. Chaudhuri", "A.D. Sarwate"], "venue": "In IEEE Global Conference on Signal and Information Processing", "citeRegEx": "Song et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Song et al\\.", "year": 2013}, {"title": "Complexity of inference in latent dirichlet allocation. In Advances in neural information processing", "author": ["D. Sontag", "D. Roy"], "venue": null, "citeRegEx": "Sontag and Roy,? \\Q2011\\E", "shortCiteRegEx": "Sontag and Roy", "year": 2011}, {"title": "Asymptotic statistics, vol", "author": ["A.W. Van der Vaart"], "venue": "3. Cambridge university press.", "citeRegEx": "Vaart,? 2000", "shortCiteRegEx": "Vaart", "year": 2000}, {"title": "non-) asymptotic properties of stochastic gradient langevin dynamics", "author": ["S.J. Vollmer", "Zygalakis", "K. C"], "venue": null, "citeRegEx": "Vollmer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vollmer et al\\.", "year": 2015}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends R", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Bayesian learning via stochastic gradient langevin dynamics", "author": ["M. Welling", "Y.W. Teh"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML-11),", "citeRegEx": "Welling and Teh,? \\Q2011\\E", "shortCiteRegEx": "Welling and Teh", "year": 2011}, {"title": "Probabilistic inference and differential privacy", "author": ["O. Williams", "F. McSherry"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Williams and McSherry,? \\Q2010\\E", "shortCiteRegEx": "Williams and McSherry", "year": 2010}, {"title": "Bayesian inference under differential privacy", "author": ["Y. Xiao", "L. Xiong"], "venue": null, "citeRegEx": "Xiao and Xiong,? \\Q2012\\E", "shortCiteRegEx": "Xiao and Xiong", "year": 2012}], "referenceMentions": [{"referenceID": 33, "context": "In the past few decades, the Bayesian approach has been intensively used in modelling speeches (Rabiner, 1989), text documents (Blei et al.", "startOffset": 95, "endOffset": 110}, {"referenceID": 1, "context": ", 2003), images/videos (Fei-Fei & Perona, 2005), social networks (Airoldi et al., 2009), brain activity (Penny et al.", "startOffset": 65, "endOffset": 87}, {"referenceID": 31, "context": ", 2009), brain activity (Penny et al., 2011), and is often considered gold standard in many of these application domains.", "startOffset": 24, "endOffset": 44}, {"referenceID": 11, "context": "Differential privacy (DP) is a cryptography-inspired notion of privacy (Dwork, 2006; Dwork et al., 2006).", "startOffset": 71, "endOffset": 104}, {"referenceID": 14, "context": "Differential privacy (DP) is a cryptography-inspired notion of privacy (Dwork, 2006; Dwork et al., 2006).", "startOffset": 71, "endOffset": 104}, {"referenceID": 0, "context": "Ahn et al. (2012); Chen et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Ahn et al. (2012); Chen et al. (2014); Ding et al.", "startOffset": 0, "endOffset": 38}, {"referenceID": 0, "context": "Ahn et al. (2012); Chen et al. (2014); Ding et al. (2014) obey ( , \u03b4)-differentially private with no algorithmic changes when the stepsize is chosen to be small.", "startOffset": 0, "endOffset": 58}, {"referenceID": 27, "context": "We demonstrate empirically that these methods work as well as Similar observations were made in Mir (2013) and Dimitrakakis et al.", "startOffset": 96, "endOffset": 107}, {"referenceID": 9, "context": "We demonstrate empirically that these methods work as well as Similar observations were made in Mir (2013) and Dimitrakakis et al. (2014) under slightly different regimes and assumptions, and we will review them among other related work in Section 6.", "startOffset": 111, "endOffset": 138}, {"referenceID": 6, "context": "or better than the state-of-the-art differential private empirical risk minimization (ERM) solvers using objective perturbation (Chaudhuri et al., 2011; Kifer et al., 2012).", "startOffset": 128, "endOffset": 172}, {"referenceID": 23, "context": "or better than the state-of-the-art differential private empirical risk minimization (ERM) solvers using objective perturbation (Chaudhuri et al., 2011; Kifer et al., 2012).", "startOffset": 128, "endOffset": 172}, {"referenceID": 26, "context": ", McSherry & Talwar (2007); Mir (2013); Bassily et al.", "startOffset": 28, "endOffset": 39}, {"referenceID": 3, "context": ", McSherry & Talwar (2007); Mir (2013); Bassily et al. (2014); Dimitrakakis et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 3, "context": ", McSherry & Talwar (2007); Mir (2013); Bassily et al. (2014); Dimitrakakis et al. (2014). Proper comparisons with them would require the knowledge of our results, thus we will defer detailed comparisons to Section 6 near the end of the paper.", "startOffset": 40, "endOffset": 90}, {"referenceID": 20, "context": ", Metropolis-Hastings algorithm (Hastings, 1970) to generate samples.", "startOffset": 32, "endOffset": 48}, {"referenceID": 0, "context": "These include Stochastic Gradient Langevin dynamics (SGLD) (Welling & Teh, 2011), Stochstic Gradient Fisher scoring (SGFS) (Ahn et al., 2012), Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al.", "startOffset": 123, "endOffset": 141}, {"referenceID": 7, "context": ", 2012), Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014) as well as more recent Stochastic Gradient Nos\u00e9-Hoover Thermostat (SGNHT) (Ding et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 10, "context": ", 2014) as well as more recent Stochastic Gradient Nos\u00e9-Hoover Thermostat (SGNHT) (Ding et al., 2014).", "startOffset": 82, "endOffset": 101}, {"referenceID": 17, "context": ", Metropolis-Hastings algorithm (Hastings, 1970) to generate samples. This is often prohibitively expensive when the data is large. One recent approach to scale up Bayesian learning is to combine stochastic gradient estimation as in Robbins & Monro (1951) and Monte Carlo methods that simulates stochastic differential equations, e.", "startOffset": 13, "endOffset": 256}, {"referenceID": 17, "context": ", Metropolis-Hastings algorithm (Hastings, 1970) to generate samples. This is often prohibitively expensive when the data is large. One recent approach to scale up Bayesian learning is to combine stochastic gradient estimation as in Robbins & Monro (1951) and Monte Carlo methods that simulates stochastic differential equations, e.g. Neal (2011). These include Stochastic Gradient Langevin dynamics (SGLD) (Welling & Teh, 2011), Stochstic Gradient Fisher scoring (SGFS) (Ahn et al.", "startOffset": 13, "endOffset": 347}, {"referenceID": 6, "context": "The boundedness on the loss-function (log-likelihood here) is a standard assumption in many DP works (Chaudhuri et al., 2011; Bassily et al., 2014; Song et al., 2013; Kifer et al., 2012).", "startOffset": 101, "endOffset": 186}, {"referenceID": 3, "context": "The boundedness on the loss-function (log-likelihood here) is a standard assumption in many DP works (Chaudhuri et al., 2011; Bassily et al., 2014; Song et al., 2013; Kifer et al., 2012).", "startOffset": 101, "endOffset": 186}, {"referenceID": 39, "context": "The boundedness on the loss-function (log-likelihood here) is a standard assumption in many DP works (Chaudhuri et al., 2011; Bassily et al., 2014; Song et al., 2013; Kifer et al., 2012).", "startOffset": 101, "endOffset": 186}, {"referenceID": 23, "context": "The boundedness on the loss-function (log-likelihood here) is a standard assumption in many DP works (Chaudhuri et al., 2011; Bassily et al., 2014; Song et al., 2013; Kifer et al., 2012).", "startOffset": 101, "endOffset": 186}, {"referenceID": 3, "context": ", 2011; Bassily et al., 2014; Song et al., 2013; Kifer et al., 2012). Lipschitz constant L is usually small for continuous distributions (at least when the parameter space \u0398 is bounded). This is a bound on log p(x|\u03b8)) so as long as p(x|\u03b8) does not increase or decrease super exponentially fast at any point, L will be a small constant. R can also be made small by a simple preprocessing step that scales down all data points. In the aforementioned papers that assume L, it is typical that they also assume R = 1 for convenience. So we will do the same. In practice, we can algorithmically remove large data points from the data by some predefined threshold or using the \u201cPropose-Test-Release\u201d framework in (Dwork & Lei, 2009) or perform weighted training where we can assign lower weight to data points with large magnitude. Note that this is a desirable step for the robustness to outliers too. Exponential families (in Hilbert space) are an example, see e.g. Bialek et al. (2001); Hofmann et al.", "startOffset": 8, "endOffset": 982}, {"referenceID": 3, "context": ", 2011; Bassily et al., 2014; Song et al., 2013; Kifer et al., 2012). Lipschitz constant L is usually small for continuous distributions (at least when the parameter space \u0398 is bounded). This is a bound on log p(x|\u03b8)) so as long as p(x|\u03b8) does not increase or decrease super exponentially fast at any point, L will be a small constant. R can also be made small by a simple preprocessing step that scales down all data points. In the aforementioned papers that assume L, it is typical that they also assume R = 1 for convenience. So we will do the same. In practice, we can algorithmically remove large data points from the data by some predefined threshold or using the \u201cPropose-Test-Release\u201d framework in (Dwork & Lei, 2009) or perform weighted training where we can assign lower weight to data points with large magnitude. Note that this is a desirable step for the robustness to outliers too. Exponential families (in Hilbert space) are an example, see e.g. Bialek et al. (2001); Hofmann et al. (2008); Wainwright & Jordan (2008).", "startOffset": 8, "endOffset": 1005}, {"referenceID": 3, "context": ", 2011; Bassily et al., 2014; Song et al., 2013; Kifer et al., 2012). Lipschitz constant L is usually small for continuous distributions (at least when the parameter space \u0398 is bounded). This is a bound on log p(x|\u03b8)) so as long as p(x|\u03b8) does not increase or decrease super exponentially fast at any point, L will be a small constant. R can also be made small by a simple preprocessing step that scales down all data points. In the aforementioned papers that assume L, it is typical that they also assume R = 1 for convenience. So we will do the same. In practice, we can algorithmically remove large data points from the data by some predefined threshold or using the \u201cPropose-Test-Release\u201d framework in (Dwork & Lei, 2009) or perform weighted training where we can assign lower weight to data points with large magnitude. Note that this is a desirable step for the robustness to outliers too. Exponential families (in Hilbert space) are an example, see e.g. Bialek et al. (2001); Hofmann et al. (2008); Wainwright & Jordan (2008).", "startOffset": 8, "endOffset": 1033}, {"referenceID": 30, "context": "Since the consistency in Bayesian methods can have different meanings, we briefly describe two of them according to the nomenclature in Orbanz (2012). Definition 5 (Posterior consistency in the Bayesian Sense).", "startOffset": 136, "endOffset": 150}, {"referenceID": 18, "context": "A promising series of results on the consistency for Bayesian nonparametric models can be found in Ghosal (2010)).", "startOffset": 99, "endOffset": 113}, {"referenceID": 18, "context": "Similar statements can also be obtained for some classes of semi-parametric and nonparametric Bayesian models (Ghosal, 2010), which we leave as future work.", "startOffset": 110, "endOffset": 124}, {"referenceID": 36, "context": "We are using L1 distance of the distribution because it is a commonly accepted metric to measure the convergence rate MCMC Rosenthal (1995), and Proposition 10 leaves a clean interface for computational analysis in determining the number of iterations needed to attain a specific level of privacy protection.", "startOffset": 123, "endOffset": 140}, {"referenceID": 3, "context": "The log-concavity of the distributions would imply convexity in the log-likelihood, thus, this essentially confirms the computational efficiency of all convex empirical risk minimization problems under differential privacy constraint (see Bassily et al. (2014)).", "startOffset": 239, "endOffset": 261}, {"referenceID": 6, "context": "OPS has a number of advantages over the state-of-the-art differentially private ERM method: objective perturbation (Chaudhuri et al., 2011; Kifer et al., 2012) (ObjPert from here onwards).", "startOffset": 115, "endOffset": 159}, {"referenceID": 23, "context": "OPS has a number of advantages over the state-of-the-art differentially private ERM method: objective perturbation (Chaudhuri et al., 2011; Kifer et al., 2012) (ObjPert from here onwards).", "startOffset": 115, "endOffset": 159}, {"referenceID": 38, "context": "The idea is to simply privately release an estimate of the gradient (as in Song et al. (2013); Bassily et al.", "startOffset": 75, "endOffset": 94}, {"referenceID": 3, "context": "(2013); Bassily et al. (2014)) and leverage upon the following two celebrated lemmas in differential privacy in the same way as Bassily et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 3, "context": "(2013); Bassily et al. (2014)) and leverage upon the following two celebrated lemmas in differential privacy in the same way as Bassily et al. (2014) does in deriving the near-optimal ( , \u03b4)-differentially private SGD.", "startOffset": 8, "endOffset": 150}, {"referenceID": 4, "context": "In addition, we will also make use of the following lemma due to Beimel et al. (2014). Lemma 13 (Privacy for subsampled data.", "startOffset": 65, "endOffset": 86}, {"referenceID": 4, "context": "In addition, we will also make use of the following lemma due to Beimel et al. (2014). Lemma 13 (Privacy for subsampled data. Lemma 4.4 in Beimel et al. (2014).).", "startOffset": 65, "endOffset": 160}, {"referenceID": 11, "context": "Dwork & Roth (2013)).", "startOffset": 0, "endOffset": 20}, {"referenceID": 42, "context": "Finite sample properties of SGLD is also studied in (Vollmer et al., 2015).", "startOffset": 52, "endOffset": 74}, {"referenceID": 3, "context": "Using the same technique in Bassily et al. (2014), we can further exploit the fact that the subset S that we use to compute the stochastic gradient is chosen uniformly randomly.", "startOffset": 28, "endOffset": 50}, {"referenceID": 42, "context": "This is a suggested heuristic in Welling & Teh (2011) and is inline with the analysis in Sato & Nakagawa (2014) and Vollmer et al. (2015).", "startOffset": 116, "endOffset": 138}, {"referenceID": 3, "context": "Choice of T and \u03c4 By Bassily et al. (2014), it takes at least N data passes to converge in expectation to a point near the minimizer, so taking T = 2N is a good choice.", "startOffset": 21, "endOffset": 43}, {"referenceID": 7, "context": "In this section, we describe three extensions of SGLD that attempts to resolve the issue by either using auxiliary variables to counter the noise in the stochastic gradient(Chen et al., 2014; Ding et al., 2014), or to exploit second order information so as to use Newton-like updates with large stepsize (Ahn et al.", "startOffset": 172, "endOffset": 210}, {"referenceID": 10, "context": "In this section, we describe three extensions of SGLD that attempts to resolve the issue by either using auxiliary variables to counter the noise in the stochastic gradient(Chen et al., 2014; Ding et al., 2014), or to exploit second order information so as to use Newton-like updates with large stepsize (Ahn et al.", "startOffset": 172, "endOffset": 210}, {"referenceID": 0, "context": ", 2014), or to exploit second order information so as to use Newton-like updates with large stepsize (Ahn et al., 2012).", "startOffset": 101, "endOffset": 119}, {"referenceID": 28, "context": "According to Neal (2011), Langevin Dynamics is a special limiting case of Hamiltonian Dynamics, where one can simply ignore the \u201cmomentum\u201d auxiliary variable.", "startOffset": 13, "endOffset": 25}, {"referenceID": 7, "context": "Chen et al. (2014) extends the full \u201cleap-frog\u201d method for HMC in Neal (2011) to work with stochastic gradient and add a \u201cfriction\u201d term in the dynamics to \u201cde-bias\u201d the noise in the stochastic gradient.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Chen et al. (2014) extends the full \u201cleap-frog\u201d method for HMC in Neal (2011) to work with stochastic gradient and add a \u201cfriction\u201d term in the dynamics to \u201cde-bias\u201d the noise in the stochastic gradient.", "startOffset": 0, "endOffset": 78}, {"referenceID": 7, "context": "Chen et al. (2014); Ding et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 7, "context": "Chen et al. (2014); Ding et al. (2014) both described a reformulation that can be interpret as SGD with momentum.", "startOffset": 0, "endOffset": 39}, {"referenceID": 0, "context": "Stochastic Gradient Fisher Scoring Another extension of SGLD is Stochastic Gradient Fisher Scoring (SGFS), where Ahn et al. (2012) proposes to adaptively interpolate between a preconditioned SGLD (see preconditioning (Girolami & Calderhead, 2011)) and a Markov Chain that samples from a normal approximation of the posterior distribution.", "startOffset": 113, "endOffset": 131}, {"referenceID": 3, "context": "That is the reason why it does not perform as well as other methods despite the theoretically being optimal in scaling (the optimality result is due to SGD (Bassily et al., 2014)).", "startOffset": 156, "endOffset": 178}, {"referenceID": 6, "context": "Specifically, we compared two of our proposed methods, OPS mechanism and hybrid algorithm against the stateof-the-art empirical risk minimization algorithm ObjPert (Chaudhuri et al., 2011; Kifer et al., 2012) under varying level of differential privacy protection.", "startOffset": 164, "endOffset": 208}, {"referenceID": 23, "context": "Specifically, we compared two of our proposed methods, OPS mechanism and hybrid algorithm against the stateof-the-art empirical risk minimization algorithm ObjPert (Chaudhuri et al., 2011; Kifer et al., 2012) under varying level of differential privacy protection.", "startOffset": 164, "endOffset": 208}, {"referenceID": 23, "context": "For fairness, we used the ( , \u03b4)-DP version of the objective perturbation (Kifer et al., 2012) and similarly we used Gaussian mechanism (rather than", "startOffset": 74, "endOffset": 94}, {"referenceID": 26, "context": "For the first part, we become aware recently that Mir (2013) and Dimitrakakis et al.", "startOffset": 50, "endOffset": 61}, {"referenceID": 8, "context": "For the first part, we become aware recently that Mir (2013) and Dimitrakakis et al. (2014) independently developed the idea of using posterior sampling for differential privacy.", "startOffset": 65, "endOffset": 92}, {"referenceID": 8, "context": "For the first part, we become aware recently that Mir (2013) and Dimitrakakis et al. (2014) independently developed the idea of using posterior sampling for differential privacy. Mir (2013, Chapter 5) used a probabilistic bound of the log-likelihood to get ( , \u03b4)\u2212DP but focused mostly on conjugate priors where the posterior distribution is in closed-form. Dimitrakakis et al. (2014) used Lipschitz assumption and bounded data points (implies our boundedness assumption) to obtain a generalized notion of differential privacy.", "startOffset": 65, "endOffset": 385}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS .", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions.", "startOffset": 0, "endOffset": 368}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded.", "startOffset": 0, "endOffset": 612}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.", "startOffset": 0, "endOffset": 828}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.", "startOffset": 0, "endOffset": 938}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known.", "startOffset": 0, "endOffset": 1109}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known. Integrating Williams & McSherry (2010) with our procedure might lead to some further performance boost, but investigating its effect is beyond the scope of the current paper.", "startOffset": 0, "endOffset": 1358}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known. Integrating Williams & McSherry (2010) with our procedure might lead to some further performance boost, but investigating its effect is beyond the scope of the current paper. For the second part, the idea to privately release stochastic gradient has been well-studied. Song et al. (2013); Bassily et al.", "startOffset": 0, "endOffset": 1607}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known. Integrating Williams & McSherry (2010) with our procedure might lead to some further performance boost, but investigating its effect is beyond the scope of the current paper. For the second part, the idea to privately release stochastic gradient has been well-studied. Song et al. (2013); Bassily et al. (2014) explicitly used it for differentially private stochastic gradient descent.", "startOffset": 0, "endOffset": 1630}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known. Integrating Williams & McSherry (2010) with our procedure might lead to some further performance boost, but investigating its effect is beyond the scope of the current paper. For the second part, the idea to privately release stochastic gradient has been well-studied. Song et al. (2013); Bassily et al. (2014) explicitly used it for differentially private stochastic gradient descent. And Rajkumar & Agarwal (2012) used it for private multi-party training.", "startOffset": 0, "endOffset": 1735}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known. Integrating Williams & McSherry (2010) with our procedure might lead to some further performance boost, but investigating its effect is beyond the scope of the current paper. For the second part, the idea to privately release stochastic gradient has been well-studied. Song et al. (2013); Bassily et al. (2014) explicitly used it for differentially private stochastic gradient descent. And Rajkumar & Agarwal (2012) used it for private multi-party training. Our Theorem 16 is a simple modification of Theorem 2.1 in Bassily et al. (2014). Bassily et al.", "startOffset": 0, "endOffset": 1857}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known. Integrating Williams & McSherry (2010) with our procedure might lead to some further performance boost, but investigating its effect is beyond the scope of the current paper. For the second part, the idea to privately release stochastic gradient has been well-studied. Song et al. (2013); Bassily et al. (2014) explicitly used it for differentially private stochastic gradient descent. And Rajkumar & Agarwal (2012) used it for private multi-party training. Our Theorem 16 is a simple modification of Theorem 2.1 in Bassily et al. (2014). Bassily et al. (2014) also showed that the differential private SGD using Gaussian mechanism with \u03c4 = 1 matches the lower bound up to constant and logarithmic, so we are confident that not many algorithms can do significantly better than Algorithm 2.", "startOffset": 0, "endOffset": 1880}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known. Integrating Williams & McSherry (2010) with our procedure might lead to some further performance boost, but investigating its effect is beyond the scope of the current paper. For the second part, the idea to privately release stochastic gradient has been well-studied. Song et al. (2013); Bassily et al. (2014) explicitly used it for differentially private stochastic gradient descent. And Rajkumar & Agarwal (2012) used it for private multi-party training. Our Theorem 16 is a simple modification of Theorem 2.1 in Bassily et al. (2014). Bassily et al. (2014) also showed that the differential private SGD using Gaussian mechanism with \u03c4 = 1 matches the lower bound up to constant and logarithmic, so we are confident that not many algorithms can do significantly better than Algorithm 2. Our contribution is to point out the interesting algorithmic structures of SGLD and extensions that preserves differential privacy. The method in Song et al. (2013) requires disjoint minibatches in every data pass, and it requires adding significantly more noise in settings when Lemma 13 applies.", "startOffset": 0, "endOffset": 2274}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known. Integrating Williams & McSherry (2010) with our procedure might lead to some further performance boost, but investigating its effect is beyond the scope of the current paper. For the second part, the idea to privately release stochastic gradient has been well-studied. Song et al. (2013); Bassily et al. (2014) explicitly used it for differentially private stochastic gradient descent. And Rajkumar & Agarwal (2012) used it for private multi-party training. Our Theorem 16 is a simple modification of Theorem 2.1 in Bassily et al. (2014). Bassily et al. (2014) also showed that the differential private SGD using Gaussian mechanism with \u03c4 = 1 matches the lower bound up to constant and logarithmic, so we are confident that not many algorithms can do significantly better than Algorithm 2. Our contribution is to point out the interesting algorithmic structures of SGLD and extensions that preserves differential privacy. The method in Song et al. (2013) requires disjoint minibatches in every data pass, and it requires adding significantly more noise in settings when Lemma 13 applies. Song et al. (2013) are however applicable when we are doing only a small number of data passes and for these cases, it gets a much better constant.", "startOffset": 0, "endOffset": 2426}, {"referenceID": 3, "context": "Bassily et al. (2014) used exponential mechanism for empirical risk minimization and the procedure is exactly the same as OPS . Our difference is to connect it to Bayesian learning and to provide results on limiting distribution, statistical efficiency and approximate sampling. We are not aware of a similar asymptotic distribution with the exception of Smith (2008), where a different algorithm (the subsample-and-aggregate scheme) is proven to give an estimator that is asymptotically normal and efficient (therefore, stronger than our result) under a different set of assumptions. Specifically, Smith (2008)\u2019s method requires boundedness of the parameter space while ours method can work with potentially unbounded space so long as the log-likelihood is bounded. Related to the general topic, Kasiviswanathan & Smith (2014) explicitly modeled the \u201csemantics\u201d of differential privacy from a Bayesian point of view, Xiao & Xiong (2012) developed a set of tools for performing Bayesian inference under differential privacy, e.g., conditional probability and credibility intervals. Williams & McSherry (2010) studied a related but completely different problem that uses posterior inference as a meta-postprocessing procedure, which aims at \u201cdenoising\u201d the privately obfuscated data when the private mechanism is known. Integrating Williams & McSherry (2010) with our procedure might lead to some further performance boost, but investigating its effect is beyond the scope of the current paper. For the second part, the idea to privately release stochastic gradient has been well-studied. Song et al. (2013); Bassily et al. (2014) explicitly used it for differentially private stochastic gradient descent. And Rajkumar & Agarwal (2012) used it for private multi-party training. Our Theorem 16 is a simple modification of Theorem 2.1 in Bassily et al. (2014). Bassily et al. (2014) also showed that the differential private SGD using Gaussian mechanism with \u03c4 = 1 matches the lower bound up to constant and logarithmic, so we are confident that not many algorithms can do significantly better than Algorithm 2. Our contribution is to point out the interesting algorithmic structures of SGLD and extensions that preserves differential privacy. The method in Song et al. (2013) requires disjoint minibatches in every data pass, and it requires adding significantly more noise in settings when Lemma 13 applies. Song et al. (2013) are however applicable when we are doing only a small number of data passes and for these cases, it gets a much better constant. Rajkumar & Agarwal (2012)\u2019s setting", "startOffset": 0, "endOffset": 2581}, {"referenceID": 6, "context": "In this way, it replicates objective perturbation (Chaudhuri et al., 2011) (assuming the method actually finds the optimal solution).", "startOffset": 50, "endOffset": 74}, {"referenceID": 6, "context": "Objective perturbation is originally proposed in Chaudhuri et al. (2011) and the ( , \u03b4) version that we refer to first appears in Kifer et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 6, "context": "Objective perturbation is originally proposed in Chaudhuri et al. (2011) and the ( , \u03b4) version that we refer to first appears in Kifer et al. (2012). Comparing to our two mechanisms that attempts to sample from the posterior, their privacy guarantee requires the solution to be exact while ours does not.", "startOffset": 49, "endOffset": 150}], "year": 2015, "abstractText": "We consider the problem of Bayesian learning on sensitive datasets and present two simple but somewhat surprising results that connect Bayesian learning to \u201cdifferential privacy\u201d, a cryptographic approach to protect individual-level privacy while permiting database-level utility. Specifically, we show that that under standard assumptions, getting one single sample from a posterior distribution is differentially private \u201cfor free\u201d. We will see that estimator is statistically consistent, near optimal and computationally tractable whenever the Bayesian model of interest is consistent, optimal and tractable. Similarly but separately, we show that a recent line of works that use stochastic gradient for Hybrid Monte Carlo (HMC) sampling also preserve differentially privacy with minor or no modifications of the algorithmic procedure at all, these observations lead to an \u201canytime\u201d algorithm for Bayesian learning under privacy constraint. We demonstrate that it performs much better than the state-of-the-art differential private methods on synthetic and real datasets. 1 ar X iv :1 50 2. 07 64 5v 2 [ st at .M L ] 1 2 A pr 2 01 5", "creator": "LaTeX with hyperref package"}}}