{"id": "1502.07776", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2015", "title": "Efficient Geometric-based Computation of the String Subsequence Kernel", "abstract": "large kernel methods simply are powerful tools in machine learning. certainly they importantly have learned to be computationally efficient. in this paper, we obviously present one a thoroughly novel hierarchical geometric - based constraint approach to compute especially efficiently the string subsequence kernel ( ssk ). here our main alternative idea theory is that designing the ssk computation reduces to range query problem. we started by the construction assumption of a traditional match list $ l ( n s, t ) = \\ { ( i, ( j ) :'s _ { file i } = t _ { string j } \\ } $ 3 where $ + s $ ) and $ s t $ are times the need strings to generally be properly compared ; such simpler match matched list algorithm contains only twice the required data that contribute to the end result. needing to also compute efficiently the ssk, tomorrow we extended compute the layered robust range tree data ladder structure diagram to become a layered range sum matched tree, a range - domain aggregation data distribution structure. the whole process takes $ o ( p | l | \\ log | v l | ) $ d time and $ o ( | l | \\ ] log | l | ) $ l space, between where $ | w l | $ a is the size of the match list implementation and $ n p $ 0 is the length of selecting the ssk. we present empiric evaluations variant of our initial approach \u2014 against scaling the dynamic and facilitate the sparse programming evaluation approaches both on synthetically carefully generated data algorithms and jointly on interactive newswire article archive data. \u2026 such performance experiments thoroughly show the efficiency of our approach for large alphabet jar size except positive for many very often short formats strings. moreover, where compared to always the classical sparse computation dynamic approach, simply the proposed approach markedly outperforms absolutely equality for consistently long format strings.", "histories": [["v1", "Thu, 26 Feb 2015 22:12:22 GMT  (475kb,D)", "http://arxiv.org/abs/1502.07776v1", "24 pages, 11 figures"]], "COMMENTS": "24 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CG", "authors": ["slimane bellaouar", "hadda cherroun", "djelloul ziadi"], "accepted": false, "id": "1502.07776"}, "pdf": {"name": "1502.07776.pdf", "metadata": {"source": "CRF", "title": "Efficient Geometric-based Computation of the String Subsequence Kernel", "authors": ["Slimane Bellaouar", "Hadda Cherroun", "Djelloul Ziadi"], "emails": ["cherroun}@mail.lagh-univ.dz", "djelloul.ziadi@univ-rouen.fr"], "sections": [{"heading": null, "text": "Keywords: string subsequence kernel, computational geometry, layered range tree, range query, range sum"}, {"heading": "1 Introduction", "text": "Kernel methods [4] offer an alternative solution to the limitation of traditional machine learning algorithms, applied solely on linear separable problems. They map data into a high dimensional feature space where we can apply linear learning machines based on algebra, geometry and statistics. Hence, we may discover non-linear relations. Moreover, kernel methods enable other data type processings (biosequences, images, graphs, . . . ).\nStrings are among the important data types. Therefore, machine learning community devotes a great effort of research to string kernels, which are widely used in the fields of bioinformatics and natural language processing. The philosophy of all string kernels can be reduced to different ways to count common ? This work is supported by the MESRS - Algeria under Project 8/U03/7015.\nar X\niv :1\n50 2.\n07 77\n6v 1\n[ cs\n.L G\n] 2\n6 Fe\nb 20\n15\nsubstrings or subsequences that occur in both strings to be compared, say s and t.\nIn the literature, there are two main approaches to improve the computation of the SSK. The first one is based on dynamic programming; Lodhi et al. [6] apply dynamic programming paradigm to the suffix version of the SSK. They achieve a complexity of O(p|s||t|), where p is the length of the SSK. Later, Rousu and Shawe-Taylor [7] propose an improvement to the dynamic programming approach based on the observation that most entries of the dynamic programming matrix (DP) do not really contribute to the result. They use a set of match lists combined with a sum range tree. They achieve a complexity of O(p|L| log min(|s|, |t|)), where L is the set of matches of characters in the two strings. Beyond the dynamic programming paradigm, the trie-based approach [5,7,8] is based on depth first traversal on an implicit trie data structure. The idea is that each node in the trie corresponds to a co-occurrence between strings. But the number of gaps is restricted, so the computation is approximate.\nMotivated by the efficiency of the computation, a key property of kernel methods, in this paper we focus on improving the SSK computation. Our main idea consists to map a machine learning problem on a computational geometry one. Precisely, our geometric-based SSK computation reduces to 2-dimensional range queries on a layered range sum tree (a layered range tree that we have extended to a range-aggregate data structure). We started by the construction of a match list L(s, t) = {(i, j) : si = tj} where s and t are the strings to be compared; such match list contains only the required data that contribute to the result. To compute efficiently the SSK, we constructed a layered range sum tree and applied the corresponding computational geometry algorithms. The overall time complexity is O(p|L| log |L|), where |L| is the size of the match list.\nThe rest of this paper is organized as follows. Section 2 deals with some concept definitions and introduces the layered range tree data structure. In section 3, we recall formally the SSK computation. We also review three efficient computations of the SSK, namely, dynamic programming, trie-based and sparse dynamic programming approaches. Our contribution is addressed in Section 4. Section 5 presents the conducted experiments and discusses the associated results, demonstrating the practicality of our approach for large alphabet sizes. Section 6 presents conclusions and further work."}, {"heading": "2 Preliminaries", "text": "We first deal with concepts of string, substring, subsequence and kernel. We then present the layered range tree data structure."}, {"heading": "2.1 String", "text": "Let \u03a3 be an alphabet of a finite set of symbols. We denote the number of symbols in \u03a3 by |\u03a3|. A string s = s1...s|s| is a finite sequence of symbols of length |s| where si marks the ith element of s. The symbol denotes the empty string. We\nuse \u03a3n to denote the set of all finite strings of length n and \u03a3\u2217 the set of all strings. The notation [s = t] is a boolean function that returns{\n1 if s and t are identical; 0 otherwise."}, {"heading": "2.2 Substring", "text": "For 1 \u2264 i \u2264 j \u2264 |s|, the string s(i : j) denotes the substring sisi+1...sj of s. Accordingly, a string t is a substring of a string s if there are strings u and v such that s = utv (u and v can be empty). The substrings of length n are referred to as n-grams (or n-mers)."}, {"heading": "2.3 Subsequence", "text": "The string t is a subsequence of s if there exists an increasing sequence of indices I = (i1, ..., i|t|) in s, (1 \u2264 i1 < ... < i|t| \u2264 |s|) such that tj = sij , for j = 1, ..., |t|. In the literature, we use t = s(I) if t is a subsequence of s in the positions given by I. The empty string is indexed by the empty tuple. The absolute value |t| denotes the length of the subsequence t which is the number of indices |I|, while l(I) = i|t| \u2212 i1 + 1 refers to the number of characters of s covered by the subsequence t."}, {"heading": "2.4 Kernel methods", "text": "Traditional machine learning and statistic algorithms have been focused on linearly separable problems (i.e. detecting linear relations between data). It is the case where data can be represented by a single row of table. However, real world data analysis requires non linear methods. In this case, the target concept cannot be expressed as simple linear combinations of the given attributes [4]. This was highlighted in 1960 by Minsky and Papert.\nKernel methods were proposed as a solution by embedding the data in a high dimensional feature space where linear learning machines based on algebra, geometry and statistics can be applied. This embedding is called kernel. It arises as a similarity measure (inner product) in a high dimension space so-called feature description.\nThe trick is to be able to compute this inner product directly from the original data space using the kernel function. This can be formally clarified as follows: the kernel function K corresponds to the inner product in a feature space F via a map \u03c6.\n\u03c6 : X \u2192 F x 7\u2192 \u03c6(x)\nK(x, x\u2032) = \u3008\u03c6(x), \u03c6(x\u2032)\u3009."}, {"heading": "2.5 Layered Range Tree", "text": "A Layered Range Tree (LRT) is a spatial data structure that supports orthogonal range queries. It is judicious to describe a 2-dimensional range tree inorder to understand LRT. Consider a set S of points in R2. A range tree is primarily a balanced binary search tree (BBST) built on the x-coordinate of the points of S. Data are stored in the leaves only. Every node v in the BBST is augmented by an associated data structure (Tassoc(v)) whitch is a 1-dimensional range tree, it can be a BBST or a sorted array, of a canonical subset P (v) on y-coordinates. The subset P (v) is the points stored in the leaves of the sub tree rooted at the node v. Figure 1 depicts a 2-dimensional range tree for a set of points S = {(2, 2), (5, 2), (3, 3), (4, 3), (2, 4), (5, 4)}. In the case where two points have the same x or y-coordinate, we have to define a total order by using a lexicographic one. It consists to replace the real number by a composite-number space [2]. The composite number of two reals x and y is denoted by (x|y), so for two points, we have:\n(x|y) < (x\u2032|y\u2032)\u21d4 x < x\u2032 \u2228 (x = x\u2032 \u2227 y < y\u2032). Based on the analysis of computational geometry algorithms, our 2-dimensional\nrange tree for a set S of n points requiresO(n logn) storage and can be constructed in O(n logn) time.\nThe range search problem consists to find all the points of S that satisfy a range query. A useful idea, in terms of efficiency, consists on treating a rectangular range query as a two nested 1-dimensional queries. In other words, let [x1 : x2]\u00d7 [y1 : y2] be a 2-dimensional range query, we first ask for the points with x-coordinates in the given 1-dimensional range query [x1 : x2]. Consequently, we select a\ncollection of O(logn) subtrees. We consider only the canonical subset of the resulted subtrees, which contains, exactly, the points that lies in the x-range [x1 : x2]. At the next step, we will only consider the points that fall in the y-range [y1 : y2]. The total task of a range query can be performed in O(log2 n+ k) time, where k is the number of points that are in the range. We can improve it by enhancing the 2-dimensional range tree with the fractional cascading technique which is described in the following paragraph.\nThe key observation made during the invocation of a rectangular range query is that we have to search the same range [y1 : y2] in the associated structures of O(logn) nodes found while querying the main BBST by the range query [x1 : x2]. Moreover, there exists an inclusion relationship between these associated structures. The goal of the fractional cascading consists on executing the binary search only once and use the result to speed up other searches without expanding the storage by more than a constant factor.\nThe application of the fractional cascading technique introduced by [3] on a range tree creates a new data structure so called layered range tree. The technique consists to add pointers from the entries of an associated data structure Tassoc of some level to the entries of an associated data structure below, say T \u2032assoc as follows: If Tassoc[i] stores a value with the key yi, then we store a pointer to the entry in T \u2032assoc with the smallest key larger than or equal yi. We illustrate such technique in Fig. 2 for the same set represented in Fig. 1. Using this technique, the rectangular search query time becomes O(logn + k), O(logn) for the first binary search and O(k) for browsing the k reported points."}, {"heading": "3 String Subsequence Kernels", "text": "The philosophy of all string kernel approaches can be reduced to different ways to count common substrings or subsequences that occur in the two strings to compare. This philosophy is manifested in two steps:\n\u2013 Project the strings over an alphabet \u03a3 to a high dimension vector space F , where the coordinates are indexed by a subset of the input space. \u2013 Compute the distance (inner product) between strings in F . Such distance reflects their similarity.\nFor the String Subsequence Kernel (SSK) [6], the main idea is to compare strings depending on common subsequences they contain. Hence, the more similar strings are ones that have the more common subsequences. However, a new weighting method is adopted. It reflects the degree of contiguity of the subsequence in the string. In order to measure the distance of non contiguous elements of the subsequence, a gap penalty \u03bb \u2208]0, 1] is introduced. Formally, the mapping function \u03c6p(s) in the feature space F can be defined as follows:\n\u03c6pu(s) = \u2211\nI:u=s(I)\n\u03bbl(I), u \u2208 \u03a3p.\nThe associated kernel can be written as:\nKp(s, t) = \u3008\u03c6p(s), \u03c6p(t)\u3009 = \u2211 u\u2208\u03a3p \u03c6pu(s).\u03c6pu(t)\n= \u2211 u\u2208\u03a3p \u2211 I:u=s(I) \u2211 J:u=t(J) \u03bbl(I)+l(J).\nIn order to clarify the idea of the SSK, we present a widespread example in the literature. Consider the strings bar, bat, car and cat, for a subsequence length p = 2, the mapping to the feature space is as follows:\n\u03c62u ar at ba br bt ca cr ct bar \u03bb2 0 \u03bb2 \u03bb3 0 0 0 0 bat 0 \u03bb2 \u03bb2 0 \u03bb3 0 0 0 car \u03bb2 0 0 0 0 \u03bb2 \u03bb3 0 cat 0 \u03bb2 0 0 0 \u03bb2 0 \u03bb3\nThe unnormalized kernel between bar and bat is K2(bar, bat) = \u03bb4, while the normalized version is obtained by :\nK\u03022(bar, bat) = K2(bar, bat)/ \u221a K2(bar, bar).K2(bat, bat) = \u03bb4/(2\u03bb4+\u03bb6) = 1/(2+\u03bb2).\nA direct implementation of this kernel leads to O(|\u03a3p|) time and space complexity. Since this is the dimension of the feature space. To assist the computation of the SSK a Suffix Kernel is defined through the embedding given by:\n\u03c6p,Su (s) = \u2211\nI\u2208I|s|p :u=s(I)\n\u03bbl(I), u \u2208 \u03a3p,\nwhere Ikp denotes the set of p-tuples of indices I with ip = k. In other words, we consider only the subsequences of length p that the last symbol is identical to the last one of the string s. The associated kernel can be defined as follows:\nKSp (s, t) = \u3008\u03c6p,S(s), \u03c6p,S(t)\u3009 = \u2211 u\u2208\u03a3p \u03c6p,Su (s).\u03c6p,Su (t).\nTo illustrate this kernel counting trick, we take back the precedent example where the mapping is as follows:\n\u03c62,Su ar at ba br bt ca cr ct bar \u03bb2 0 0 \u03bb3 0 0 0 0 bat 0 \u03bb2 0 0 \u03bb3 0 0 0 car \u03bb2 0 0 0 0 0 \u03bb3 0 cat 0 \u03bb2 0 0 0 0 0 \u03bb3\nfor example KS2 (bar, bat) = 0 and KS2 (bat, cat) = \u03bb4. The SSK can be expressed in terms of its suffix version as:\nKp(s, t) = |s|\u2211 i=1 |t|\u2211 j=1 KSp (s(1 : i), t(1 : j)). (1)\nwith KS1 (s, t) = [s|s| = t|t|] \u03bb2."}, {"heading": "3.1 Naive Implementation", "text": "The computation of the similarity of two strings (sa and tb) is conditioned by their final symbols. In the case where a = b, we have to sum kernels of all prefixes of s and t. Hence, a recursion has to be devised:\nKSp (sa, tb) = [a = b] |s|\u2211 i=1 |t|\u2211 j=1 \u03bb2+|s|\u2212i+|t|\u2212jKSp\u22121(s(1 : i), t(1 : j)). (2)\nThis computation leads to a complexity of O(p(|s|2|t|2))."}, {"heading": "3.2 Efficient Implementations", "text": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7]. To describe such approaches, we use two strings s = gatta and t = cata as a running example.\nDynamic Programming Approach. The starting point of the dynamic programming approach is the suffix recursion given by equation (2). From this equation, we can consider a separate dynamic programming table DPp for storing the double sum:\nDPp(k, l) = k\u2211 i=1 l\u2211 j=1 \u03bbk\u2212i+l\u2212j KSp\u22121(s(1 : i), t(1 : j)). (3)\nIt is easy to see that: KSp (sa, tb) = [a = b]\u03bb2 DPp(|s|, |t|)). Computing ordinary DPp for each (k, l) would be inefficient. So we can devise a recursive version of equation (3) with a simple counting device:\nDPp(k, l) = KSp\u22121(s(1 : k), t(1 : l)) + \u03bbDPp(k \u2212 1, l) + \u03bbDPp(k, l \u2212 1)\u2212 \u03bb2DPp(k \u2212 1, l \u2212 1).\nConsequently, using the dynamic programming approach (Algorithm 1), the complexity of the SSK becomes O(p |s||t|).\nTable 1 illustrates the computation of the dynamic programming tables for the running example for p = 1, 2. The evaluation of the kernel is given by the sum of entries of the suffix table KPS:\nK1(s, t) = 6\u03bb2. K2(s, t) = 2\u03bb4 + 2\u03bb5 + \u03bb7.\nTrie-based Approach. This approach is based on search trees known as tries, introduced by E. Fredkin in 1960. The key idea of the trie-based approach is that leaves play the role of the feature space indexed by the set \u03a3p of strings of length p. In the literature, there are variants of trie-based string subsequence kernels. For instance the (p,m)-mismatch string kernel [5] and restricted SSK [8]. In the present section, we try to describe a trie-based SSK presented in [7] that slightly differ from those cited above [5,8]. Figure 3 illustrates the trie data structure for the running example. Each node in the trie corresponds to a co-occurrence between strings. The algorithm maintains for all matches u = s(I) = u1 \u00b7 \u00b7 \u00b7uq, I = i1 \u00b7 \u00b7 \u00b7 iq a list of alive matches As(u, g) as presented in Table 2 that records\nAlgorithm 1: Dynamic SSK computation Input: Strings s and t, the length of the subsequence p, and the decay penalty \u03bb Output: Kernel values Kq(s, t) = K(q) : q = 1, . . . , p 1 m\u2190 length(s) 2 n\u2190 length(t) 3 K(1 : p)\u2190 0\n/* Computation of K1(s, t) */ 4 for i = 1:m do 5 for j = 1:n do 6 if s[i] = t[j] then 7 KPS[i, j]\u2190 \u03bb2 8 K[1]\u2190 K[1] +KPS[i, j]\n/* Computation of Kq(s, t) : q = 2, . . . , p */ 9 for q = 2:p do\n10 for i = 1:m do 11 for j = 1:n do 12 DP [i, j]\u2190 KPS[i, j]+\u03bbDP [i\u22121, j]+\u03bbDP [i, j\u22121]\u2212\u03bb2DP [i\u22121, j\u22121] 13 if s[i] = t[j] then 14 KPS[i, j]\u2190 \u03bb2DP [i\u2212 1, j \u2212 1] 15 K[q]\u2190 K[q] +KPS[i, j]\nthe last index iq where g = l(I)\u2212 |I| is the number of gaps in the match. Notice that in the same list we are able to record many occurrences with different gaps. Alive lists for longer matches uc, c \u2208 \u03a3, can be constructed incrementally by extending the alive list corresponding to u. Similarly, the algorithm is applied to the string t. The process will continue until achieving the depth p. For efficiency reasons, we need to restrict the number of gaps to a given integer gmax, so the computation is approximate. The kernel is evaluated as follows:\nKp(s, t) = \u2211 u\u2208\u03a3p \u03c6pu(s)\u03c6pu(t) = \u2211 gs,gt \u03bbgs+p|Ls(u, gs)| \u00b7 \u03bbgt+p|Lt(u, gt)|.\nGiven that, there are ( p+gmax gmax ) possible combinations to assign p letters and gmax gaps in a window of length p+ gmax, the worst-case time complexity of the algorithm is O( ( p+gmax gmax ) (|s|+ |t|)). The string subsequence kernel for the running example for p = 1 is:\nK1(s, t) = \u03bb0+1|As(\u2019a\u2019, 0)| \u00b7 \u03bb0+1|At(\u2019a\u2019, 0)|+ \u03bb0+1|As(\u2019t\u2019, 0)| \u00b7 \u03bb0+1|At(\u2019t\u2019, 0)| = 4 \u00b7 \u03bb2 + 2 \u00b7 \u03bb2 = 6 \u00b7 \u03bb2.\nSimilar computation is performed for K2 and K3:\nK2(s, t) = (1 \u00b7 \u03bb2+2) \u00b7 (1 \u00b7 \u03bb1+2) + (1 \u00b7 \u03bb0+2 + 1 \u00b7 \u03bb1+2) \u00b7 (1 \u00b7 \u03bb0+2) + (1 \u00b7 \u03bb0+2 + 1 \u00b7 \u03bb1+2) \u00b7 (1 \u00b7 \u03bb0+2) = \u03bb7 + 2 \u00b7 \u03bb5 + 2 \u00b7 \u03bb4.\nand\nK3(s, t) = (2 \u00b7 \u03bb1+3) \u00b7 (1 \u00b7 \u03bb0+3) = 2 \u00b7 \u03bb7.\nSparse Dynamic Programming Approach. It is built on the fact that in many cases, most of the entries of the DP matrix are zero and do not contribute to the result. Rousu and Shawe-Taylor [7] have proposed a solution using the sparse dynamic programming technique to avoid unnecessary computations. To do so, two data structures were proposed: the first one is a range sum tree, which is a B-tree, that replaces the DPp matrix. It is used to return the sum of n values within an interval in O(logn) time. The second one is a set of match lists instead of KSp matrix. Lq(i) = {(j1,KSp (s(1 : i), t(1 : j1)), (j2,KSp (s(1 : i), t(1 : j2)), ...} where KSp (s(1 : i), t(1 : j)) = \u03bbm\u2212i+n\u2212jKSp (s(1 : i), t(1 : j)). This dummy gap weight \u03bbm\u2212i+n\u2212j allows to address the problem of scaling the kernel values as the computation progress. Consequently the recursion (2) becomes:\nKSp (sa, tb) = [a = b]\u03bb2 \u2211 i\u2264|s| \u2211 j\u2264|t| KSp\u22121(s(1 : i), t(1 : j)). (4)\nand the separate dynamic programming table (3) can be expressed as follows:\nDPp(k, l) = \u2211 i\u2264k \u2211 j\u2264l KSp\u22121(s(1 : i), t(1 : j)). (5)\nThereafter, the authors devise a recursive version of (5):\nDPp(k, l) = DPp(k \u2212 1, l) + \u2211 j\u2264l KSp\u22121(s(1 : i), t(1 : j)). (6)\nThis can be interpreted as reducing the evaluation of an orthogonal range query (5) to an evaluation of a simple range query multiple times as much as the number of lines of the KSp matrix.\nTo evaluate efficiently a range query, the authors use a range-sum tree to store a set S = {(j, vj)} \u2282 {1, . . . n} \u00d7R of key-value pairs. A range-sum tree is a binary tree of height h = dlogne where each node in depth d = 0, 1, . . . , h\u2212 1 contains a key j with a sum of values in a sub range [j \u2212 2h\u2212d + 1, j]. The root\nis labeled with 2h, the left child of a node j is j \u2212 j/2 and the right child if it exists is j + j/2. Odd keys label the leaves of the tree.\nTo compute the range sum of values within an interval [1, j] it suffices to browse the path from the node j to the root and sum over the left subtrees as follows:\nRangesum([1, j]) = vj + \u2211\nh\u2208Ancestors(j)/h<j\nvh.\nMoreover to update the value of a node j, we need to update all the values of parents that contain j in their subtree (h \u2208 Ancestors(j)/h > j). These two operations are performed in O(logn)time because we traverse in the worst case the height of the tree.\nFor the sparse dynamic programming algorithm (Algorithm 2) the range-sum tree is used incrementally when computing (6). So that when processing the match list Lp(k) the tree will contain the values vj that satisfy \u2211k i=1 K S p\u22121(s(1 : i), t(1 : j)), 1 \u2264 j \u2264 l. Hence the evaluation of (6) is performed by involving a one dimensional range query:\nRangesum([1, j]) = l\u2211\nj=1 vj\n= k\u2211 i=1 l\u2211 j=1 KSp\u22121(s(1 : i), t(1 : j))\n= DPp(k, l).\nConcerning the cost of computation of this approach, the set of match lists is created on O(m+ n+ |\u03a3|+ |L1|) time and space, while the kernel computation time is O(p|L1| logn), knowing that |L1| \u2265 |L2| \u2265 . . . \u2265 |Lp|.\nTo illustrate the mechanism of the sparse dynamic programming algorithm, Figure 4 depicts the state of the range-sum tree when computing KS2 (s, t). Initially the set of match lists is created as follows:\nL1(1) = () L1(2) = ((2, \u03bb7), (4, \u03bb5)) L1(3) = ((3, \u03bb5)) L1(4) = ((3, \u03bb4)) L1(5) = ((2, \u03bb4), (4, \u03bb7)).\nMeanwhile maintaining the range-sum tree, the algorithm update the set of match lists as presented below:\nL2(1) = () L2(2) = () L2(3) = ((3, \u03bb7)) L2(4) = ((3, \u03bb7)) L2(5) = ((4, \u03bb7 + \u03bb5 + \u03bb4)).\nFinally, summing the values of the updated match list after discarding the dummy weight gives the kernel value K2(s, t):\nK2(s, t) = \u03bb7 \u00b7 \u03bb\u22129+3+3 + \u03bb7 \u00b7 \u03bb\u22129+4+3 + (\u03bb7 + \u03bb5 + \u03bb4) \u00b7 \u03bb\u22129+5+4\n= \u03bb7 + 2\u03bb5 + 2\u03bb4."}, {"heading": "4 Geometric based Approach", "text": "Looking forward to improving the complexity of SSK, our approach is based on two observations. The first one concerns the computation of KSp (s, t) that is required only when s|s| = t|t|. Hence, we have kept only a list of index pairs of these entries rather than the entire suffix table, L(s, t) = {(i, j) : si = tj}.\nIn the rest of the paper, while measuring the complexity of different computations, we will consider, |L|, the size of the match list L(s, t) as the parameter indicating the size of the input data.\nThe complexity of the naive implementation of the list version is O(p|L|2), and it seems not obvious to compute KSp (s, t) efficiently on a list data structure.\nAlgorithm 2: Sparse Dynamic SSK computation Input: Strings s and t,the length of the subsequence p, and the decay penalty \u03bb Output: Kernel value Kp(s, t) = K 1 m\u2190 length(s) 2 n\u2190 length(t) 3 Creation of the set of match lists L1 4 for q = 2:p do 5 Rangesum(1 : n)\u2190 0 (Initialization of the range-sum tree) 6 for i = 1:m do 7 foreach (jh, vh) \u2208 Lq\u22121(i) do 8 S \u2190 Rangesum[1, jh \u2212 1] 9 if S > 0 then\n10 appendlist(Lq(i), (jh, S))\n/* Update of the range-sum tree */ 11 foreach (jh, vh) \u2208 Lq\u22121(i) do 12 update(Rangesum, (jh, vh))\n/* Computation of the kernel value for the final level */ 13 K \u2190 0 14 for i = 1:m do 15 foreach (jh, vh) \u2208 Lp(i) do 16 K \u2190 K + vh\u03bb\u2212m\u2212n+i+jh\nIn order to address this problem, we have made a second observation that the suffix table can be represented as a 2-dimensional space (plane) and the entries where s|i| = t|j| as points in this plane as depicted in Fig. 5. In this case, the match list generated is\nL(s, t) = {A,B,C,D,E, F} = {(2, 2), (2, 4), (3, 3), (4, 3), (5, 2), (5, 4)}.\nWith a view to improving the computation of the SSK, it is easy to perceive from Fig. 5 that the computation of (2) can be interpreted as orthogonal range queries. In this respect, we have used a layered range tree (LRT) in [1]. But the LRT data structure reports all points that lie in a specific range query. However, for the SSK computation we require only the sum of values of the reported points.\nTo achieve this goal, we extend the LRT with the aggregate operations, in particular the summation one. Hence, a novel data structure was created, for instance a Layered Range Sum Tree (LRST). A LRST is a LRT with two substantial extensions to reduce the range sum query time from O(log |L|+ k) to O(log |L|) where k is the number of reported points in the range.\nThe first extension consists to substitute the associated data structures Tassoc in the LRT with new associated data structures T \u2032assoc where each entry i contains a key-value pair (j, psj), psj = \u2211i k=1 vk is the partial sum of Tassoc in the position i. This extension is made to compute the range sum of Tassoc within [i, j] in O(1) time as follows : Rangesum[i, j] = T \u2032assoc[j]\u2212 T \u2032assoc[i\u2212 1].\nThe second extension involves the fractional cascading technique. Let T \u2032assoc1 and T \u2032assoc2 be two sorted arrays that store partial sums of Tassoc1 and Tassoc2 respectively. Suppose that we want to compute the range sum within a query q = [y1, y2] in Tassoc1 and Tassoc2. We start with a binary search with y1 in T \u2032assoc1 to find the smallest key larger than or equal y1. We make also an other binary search with y2 in T \u2032assoc1 to find the largest key smaller than or equal y2. If an entry T \u2032assoc1[i] stores a key yi then we store a pointer to the entry in T \u2032assoc2 with the smallest key larger than or equal to yi, say small pointer, and a second pointer to the entry in T \u2032assoc2 with the largest key smaller than or equal to yi, say large pointer. If there is no such key(s) then the pointer(s) is (are) set to nil.\nIt is easy to see that our extensions does not affect neither the space nor the time complexities of the LRT construction. So according to the analysis of of computational geometry algorithms, our LRST requires O(|L| log |L|) storage and can be constructed in O(|L| log |L|) time. This leads to the following lemma.\nLemma 1. Let s and t be two strings and L(s, t) = {(i, j) : si = tj} the match list associated to the suffix version of the SSK. A Layered range sum tree (LRST) for L(s, t) requires O(|L| log |L|) storage and takes O(|L| log |L|) construction time.\nWe can now exploit these extensions to compute efficiently the range sum inherent to q = [y1, y2] in Tassoc1 and Tassoc2. Let T \u2032assoc1[i1] and T \u2032assoc1[i2] be the results of the binary search with y1 and y2 respectively in T \u2032assoc1. So the Rangesum(y1, y2) = T \u2032assoc1[i2] \u2212 T \u2032assoc1[i1 \u2212 1] in Tassoc1 takes O(log |L|)\ntime. To compute the range sum in Tassoc2 we avoid the binary searches. We consider first the entry T \u2032assoc2[j1] pointed by the small pointer of T \u2032assoc1[i1], it contains the smallest key from T \u2032assoc2 larger than or equal to y1. The second entry is T \u2032assoc2[j2] pointed by the large pointer of T \u2032assoc1[i2], it contains the largest key from T \u2032assoc2 smaller than or equal to y2. Finally the range sum within [y1, y2] in Tassoc2 is given by Rangesum(y1, y2) = T \u2032assoc2[j2]\u2212 T \u2032assoc2[j1 \u2212 1] and it takes O(1) time.\nAlgorithm 3: Geometric SSK computation Input: Strings s and t,the length of the subsequence p, and the decay penalty \u03bb Output: Kernel values Kq(s, t) = K(q) : q = 1, . . . , p 1 m\u2190 length(s) 2 n\u2190 length(t) 3 Creation of the initial match list L\n/* Computation of K1(s, t) */ 4 foreach ((i, j), v) \u2208 L do 5 K[1]\u2190 K[1] + v \u00b7 \u03bbi+j\n/* Computation of Kq(s, t) : q = 2, . . . , p */ 6 for q = 2:p do 7 Building of the LRST corresponding to the match list L 8 foreach ((i, j), v) \u2208 L do /* Preparing the range query for the entry (i, j) */ 9 rq \u2190 [(0| \u2212\u221e) : (i\u2212 1|+\u221e)]\u00d7 [(0| \u2212\u221e) : (j \u2212 1|+\u221e)]\n10 result\u2190 Rangsum[rq] 11 if result > 0 then 12 K[q] = K[q] + result \u00b7 \u03bbi+j 13 appendlist(newL, ((i, j), result))\n14 L\u2190 newL\nFor our geometric approach (Algorithm 3) we will use the LRST to evaluate the SSK. We start by the creation of the match list L(s, t) = {((i, j), K\u0303Sp (s(1 : i), t(1 : j))) : si = tj} where K\u0303Sp (s(1 : i), t(1 : j)) = \u03bb2\u2212i\u2212jKSp (s(1 : i), t(1 : j)). This trick is inspired from [7] to make the range sum results correct. Thus the recursion (2) becomes as follows:\nK\u0303Sp (sa, tb) = [a = b] \u2211 i\u2264|s| \u2211 j\u2264|t| \u03bbi+jK\u0303Sp\u22121(s(1 : i), t(1 : j)). (7)\nIn order to construct efficiently the match list we have to create for each character c \u2208 \u03a3 a list I(c) of occurrence positions (c = si) in the string s. Thereafter, for each character tj \u2208 t we insert key-value pairs ((i, j), K\u0303Sp (s(1 : i), t(1 : j))) in the match list L(s, t) corresponding to I(tj). This process takes O(|s|+ |t|+ |\u03a3|+ |L|) space and O(|s|+|\u03a3|+|L|) time. For example, the match list for our running example is L(s, t) = {((2, 2), \u03bb7), ((2, 4), \u03bb5), ((3, 3), \u03bb5), ((4, 3), \u03bb4), ((5, 2), \u03bb4), ((5, 4), \u03bb2).\nOnce the initial match list created, we start computing the SSK for the subsequence length p = 1. This computation doesn\u2019t require the LRST ; it suffices to traverse the match list and sum over its values. For length subsequence q > 1 we will first create the LRST corresponding to the match list, afterward for each item ((k, l), K\u0303Sp (s(1 : k), t(1 : l))) we invoke the LRST with the query rq = [0, k\u22121]\u00d7 [0, l\u22121]. This latter return the range sum within rq: Rangesum(rq) =\u2211 i<k \u2211 j<l(K\u0303Sp (s(1 : i), t(1 : j))). If Rangesum(rq) is positive then insert the key-value in a new match list for the level q+ 1 and summing the Rangesum(rq) to compute the SSK at the level q. At each iteration, we have to create a new LRST corresponding to the new match list until achieving the request subsequence length p.\nWe recall that in our case, we use composite numbers instead of real numbers, see section (2.5). In such situation, we have to transform the range query [x1 : x2]\u00d7 [y1 : y2] related to a set of points in the plane to the range query [(x1|\u2212\u221e) : (x2|+\u221e)]\u00d7 [(y1| \u2212\u221e) : (y2|+\u221e)] related to the composite space.\nUsing our geometric approach, the range sum query time becomes O(log |L|). For the computation of KSp (s, t) we have to consider |L| entries of the match list. The process iterates p times, therefore, we get a time complexity of O(p|L| log |L|) for evaluating the SSK. This result combined to that of Lemma. 1 lead to the following theorem that summarizes the result of our proposed approach to compute SSK.\nTheorem 2. Let s and t be two strings and L(s, t) = {(i, j) : si = tj} the match list associated to the suffix version of the SSK. A layered range sum tree requires O(|L| log |L|) storage and it can be constructed in O(|L| log |L|) time. With these data structures, the SSK of length p can be computed in O(p|L| log |L|)).\nTo compute K2(s, t), for our running example, we have to invoke the range sum on the LRST at the step p = 2 represented by Fig.6. The SSK computation is performed by summing over all the range sums correponding th the entries of the match list as follows: K2(s, t) = Rangesum[(0| \u2212 \u221e) : (1| +\u221e)] \u00d7 [(0| \u2212 \u221e) : (1| +\u221e)] + Rangesum[(0| \u2212 \u221e) : (1| +\u221e)] \u00d7 [(0| \u2212 \u221e) : (3| +\u221e)] + Rangesum[(0| \u2212\u221e) : (2|+\u221e)]\u00d7 [(0| \u2212\u221e) : (2|+\u221e)] +Rangesum[(0| \u2212\u221e) : (3|+\u221e)]\u00d7 [(0| \u2212\u221e) : (2|+\u221e)] +Rangesum[(0| \u2212\u221e) : (4|+\u221e)]\u00d7 [(0| \u2212\u221e) : (1|+\u221e)] +Rangesum[(0| \u2212\u221e) : (4|+\u221e)]\u00d7 [(0| \u2212\u221e) : (3|+\u221e)].\nTo describe how this can be processed, we deal by the range sum of the query [(0| \u2212 \u221e) : (4| +\u221e)] \u00d7 [(0| \u2212 \u221e) : (3| +\u221e)]. At the associate data structure corresponding to the split node (3|3) of Fig.6 we find the entries (2|2) and (3|4) whose y \u2212 coordinates are the smallest one larger than or equal to (0| \u2212\u221e) and the largest one smaller or equal to (3|+\u221e) respectively. This can be done by binary search. Next, we look for the nodes that are below the split node (3|3) and that are the right child of a node on the search path to (0| \u2212\u221e) where the path go left, or the left child of a node on the search path to (4|+\u221e) where the path go right. The collected nodes are (3|3), (2|2), (4|3) and the result returned form the associated data structures is \u03bb\u22125 + \u03bb\u22124 + \u03bb\u22122. This is done on a constant time by following the small and large pointers form the associated data structure\nof the split node. By the same process we obtain the following results of the invoked range sums: Rangesum[(0| \u2212\u221e) : (1|+\u221e)]\u00d7 [(0| \u2212\u221e) : (1|+\u221e)] = 0 Rangesum[(0| \u2212\u221e) : (1|+\u221e)]\u00d7 [(0| \u2212\u221e) : (3|+\u221e)] = 0 Rangesum[(0| \u2212\u221e) : (2|+\u221e)]\u00d7 [(0| \u2212\u221e) : (2|+\u221e)] = \u03bb\u22122 Rangesum[(0| \u2212\u221e) : (3|+\u221e)]\u00d7 [(0| \u2212\u221e) : (2|+\u221e)] = \u03bb\u22122 Rangesum[(0| \u2212\u221e) : (4|+\u221e)]\u00d7 [(0| \u2212\u221e) : (1|+\u221e)] = 0 After rescaling the returned values by the factor \u03bbi+j we obtain the value of K2(s, t) = \u03bb\u22122 \u00b7 \u03bb3+3 + \u03bb\u22122 \u00b7 \u03bb4+3 + (\u03bb\u22125 + \u03bb\u22124 + \u03bb\u22122) \u00b7 \u03bb5+4 = 2\u03bb4 + 2\u03bb5 + \u03bb7. While invoking the range sums we will prepare the new match list for the next step. In our case the new match list contains the following matchs : {((3, 3), \u03bb\u22122), ((4, 3), \u03bb\u22122), ((5, 2), \u03bb\u22125 + \u03bb\u22124 + \u03bb\u22122)}."}, {"heading": "5 Experimentation", "text": "In this section we describe the experiments that focus on the evaluation of our geometric algorithm against the dynamic and the sparse dynamic ones. Thereafter, these algorithms are referenced as Geometric, Dynamic and Sparse respectively. We have discarded the trie-based algorithm from this comparison because it is an approximate algorithm on the one hand, on the other hand in the preliminary experiments conducted in [7] it was significantly slower than Dynamic and Sparse.\nTo benefit from the empiric evaluation conducted in [7], we tried to keep the same conditions of their experiments. For this reason, we have conducted a series of experiments on both synthetically generated and on newswire article data on Reuter\u2019s news articles.\nWe ran the tests on Intel Core i7 at 2.40 GHZ processor with 16 GB RAM under Windows 8.1 64 bit. We implemented all the tested algorithms in Java. For the LRST implementation, we have extended the LRT implementation available on the page https://github.com/epsilony/."}, {"heading": "5.1 Experiments with synthetic data", "text": "These experiments concern the effects of the string length and the alphabet size on the efficiency of the different approaches and to determine under which conditions our approach outperforms.\nWe randomly generated string pairs with different lengths (2, 4, . . . 8192) over alphabets of different sizes (2, 4, . . . 8192). To simplify the string generation, we considered string symbols as integer in [1, alphabet size]. For convenience of data visualization, we have used the logarithmic scale on all axes. To perform accurate experiments, we have generated multiple pairs for the same string length and alphabet size and for each pair we have took multiple measures of the running time with a subsequence length p = 10 and a decay parameter \u03bb = 0.5. This being\nsaid, Fig. 7 reveals, for our geometric approach, an inverse dependency of the running time with the alphabet size. However, for an alphabet size the running time is proportional to the string length. Figure 8 shows experimental comparison of the performance of the proposed approach against Dynamic. Note that the rate of 100% indicates that the two algorithms deliver the same performances. For the rates less than 100% our approach outperforms, it is the case for strings based on medium and large alphabets excepting those having short length (say alphabet size great than or equal 256, where the string length exceeds 128 characters). For\nshort strings and also for long strings based on small alphabets, Dynamic excels. It remains to present results of the comparison experiment with Sparse which share the same motivations with our approach. Rousu and Shawe-Taylor [7] state that with long strings based on large alphabets their approach is faster. Figure 9 shows that in these conditions our approach dominates. Moreover, our approach is faster than the Sparse one for long strings and for large alphabets absolutely, but gets slower than Sparse for short strings based on small alphabets."}, {"heading": "5.2 Experiments with newswire article data", "text": "Our second experiments use the Reuters-21578 collection to evaluate the speed of Geometric against Dynamic and Sparse on English articles. We created a dataset represented as sequences of syllables by transferring all the XML articles on to text documents. Thereafter, the text documents are preprocessed by removing stop words, punctuation marks, special symbols and finally word syllabifying. We have generated 22260 distinct syllables. As in the first experiment, each syllable alphabet is assigned an integer. To treat the documents randomly, we have shuffled this preliminary dataset.\nFor visualization convenience, while creating document pairs, we have ensured that their lengths are close. Under this condition, we have collected 916 pair documents as final dataset.\nTo compare the candidate algorithms, we have computed the SSK for each document pair of the data set by varing the subsequence length form 2 to 20. Figure 10 and Figure 11 depict the clusters of documents where Geometric is faster than Dynamic and Sparse respectively. A document pair (s, t) is plotted according to the inverse match frequency (X-axis) and the document size (Y-axis).\nThe inverse match frequency is given by: |s||t|/|L|, it plays the role of the alphabet size |\u03a3| inherent to the documents s and t. The document size is calculated as the arithmetic mean of the document pair sizes, it plays the role of the string length. Each cluster is distinguished by a special marker that corresponds to the necessary minimum subsequence length to make Geometric faster than Dynamic or Sparse. For the cluster marked by black diamonds, p \u2264 5 is sufficient. The length 5 < p \u2264 10 is required for the cluster marked by blue filled squares. For the cluster marked by green circles 10 < p \u2264 20 is required and the last cluster marked by plus signs p \u2265 20 is needed. We can distinguish three cases in Fig. 10. The first one arises when the inverse match frequency is weak (small alphabet size), that is to say for dense matrix. In this case, we require important values of the subsequence length (p > 10 for small documents and p > 20 for larger ones) to make Geometric faster than Dynamic. The second case concerns good inverse match frequencies (large alphabet size) corresponding to sparse matrix. In this case, small values of the subsequence length (p \u2264 5) suffice to make Geometric faster than Dynamic. The third case appear for moderate inverse match frequency (medium alphabet size), the values of p that makes Geometric faster than Dynamic depend on the document size. The large document size the large p is required. The results of the comparison between Geometric and Sparse on newswire article data are depicted in Fig. 11. We can discuss 3 cases: The first case emerge when the document size becomes large and also for good inverse match frequency. In this case small values of the subsequence length (p \u2264 5) suffice to make Geometric faster than Sparse. The second case appear for small documents and bad inverse match frequencies. The necessary subsequence length must be important (p > 10 for very small documents and p > 20 for the small ones). The third case concerns modurate inverse frequencies. In this case\nthe value of the subsequence length that makes Geometric faster than Sparse depends on the document size except large sizes which fall in the first case."}, {"heading": "5.3 Discussion of the experiment results", "text": "In step with the results of the two experiments, it is easy to see that the algorithms behave essentially in the same way both on synthetically generated data and newswire article data. These results reveal that our approach outperforms for large alphabet size except for very small strings. Moreover, regarding to the Sparse, Geometric is competitive for long strings.\nWe can argue this as follows: first, the alphabet size and the string length affect substantially the kernel matrix form. Large alphabets can reduce potentially the partially matching subsequences especially on long strings, giving rise to sparse matrix form. Consequently, great number of data stored in the kernel matrix do not contribute to the result. In the other cases, for dense matrix, our approach can be worse than Dynamic by at most Log|L| factor.\nOn the other hand, The complexities of Geometric and Sparse differ only by the factors Log|L| and Log n. The inverse dependency of |L| and |\u03a3| goes in favor of our approach. Also, the comparisons conducted on our datasets give evidence that for long strings |L| << n, remembering that the size of the match list decrease while the SSK execution progress. Moreover, to answer orthogonal range queries, Sparse invoke one dimensional range query multiple times. Whereas, Geometric mark good scores by using orthogonal range queries in conjunction with the fractional cascading and exploit our extension of the LRT data structure to get directly the sum within a range."}, {"heading": "6 Conclusions and further work", "text": "We have presented a novel algorithm that efficiently computes the string subsequence kernel (SSK). Our approach is refined over two phases. We started by the construction of a match list L(s, t) that contains, only, the information that contributes in the result. Thereafter, in order to compute, efficiently, the sum within a range for each entry of the match list, we have extended a layered range tree to be a layered range sum tree. The Whole task takes O(p|L| log |L|) time and O(|L| log |L|) space, where p is the length of the SSK and |L| is the initial size of the match list.\nThe reached result gives evidence of an asymptotic complexity improvement compared to that of a naive implementation of the list version O(p |L|2). The experiments conducted both on synthetic data and newswire article data attest that the dynamic programming approach is faster when the kernel matrix is dense. This case is achieved on long strings based on small alphabets and on short strings. Furthermore, recall that our approach and the sparse dynamic programming one are proposed in the context where the most of the entries of the kernel matrix are zero, i.e. for large-sized alphabets. In such case our approach outperforms. For long strings our approach behave better than the sparse one.\nThis well scaling of the proposed approach with document size and alphabet size could be useful in very tasks of machine learning on long documents as full-length research articles.\nA noteworthy advantage is that our approach can be favorable if we assume that the problem is multi-dimensional. In terms of complexity, this can have influence the storage and the running time, only, by a logarithmic factor. Indeed,\nthe layered range sum tree needs O(|L| logd\u22121 |L|) storage and can compute the sum within a rectangular range in O(logd\u22121 |L|), in a d-dimensional space.\nAt the implementation level, great programming effort is supported by wellstudied and ready to use computational geometry algorithms. Hence, the emphasis is shifted to a variant of string kernel computations that can be easily adapted.\nFinally, it would be very interesting if the LRST can be extended to be a dynamic data structure. This can relieve us to create a new LRST at each evolution of the subsequence length. An other interesting axis consists to combine the LRST with the dynamic programming paradigm. We believe that using rectangular intersection techniques seems to be a good track, though this seems to be a non trivial task."}, {"heading": "1. Bellaouar, S., Cherroun, H., Ziadi, D.: Efficient list-based computation of the string", "text": "subsequence kernel. In: LATA\u201914. pp. 138\u2013148 (2014) 2. Berg, M.d., Cheong, O., Kreveld, M.v., Overmars, M.: Computational Geometry: Algorithms and Applications. Springer-Verlag TELOS, Santa Clara, CA, USA, 3rd ed. edn. (2008) 3. Chazelle, B., Guibas, L.J.: Fractional cascading: I. a data structuring technique. Algorithmica 1(2), 133\u2013162 (1986) 4. Cristianini, N., Shawe-Taylor, J.: An introduction to support Vector Machines: and other kernel-based learning methods. Cambridge University Press, New York, NY, USA (2000) 5. Leslie, C., Eskin, E., Noble, W.: Mismatch String Kernels for SVM Protein Classification. In: Neural Information Processing Systems 15. pp. 1441\u20131448 (2003), http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.4737 6. Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini, N., Watkins, C.: Text classification using string kernels. J. Mach. Learn. Res. 2, 419\u2013444 (Mar 2002), http://dx.doi.org/10.1162/153244302760200687 7. Rousu, J., Shawe-Taylor, J.: Efficient computation of gapped substring kernels on large alphabets. J. Mach. Learn. Res. 6, 1323\u20131344 (Dec 2005), http://dl.acm.org/ citation.cfm?id=1046920.1088717 8. Shawe-Taylor, J., Cristianini, N.: Kernel Methods for Pattern Analysis. Cambridge University Press, New York, NY, USA (2004)"}], "references": [{"title": "Efficient list-based computation of the string subsequence kernel", "author": ["S. Bellaouar", "H. Cherroun", "D. Ziadi"], "venue": "LATA\u201914. pp. 138\u2013148", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Computational Geometry: Algorithms and Applications", "author": ["Berg", "M.d.", "O. Cheong", "Kreveld", "M.v.", "M. Overmars"], "venue": "Springer-Verlag TELOS, Santa Clara, CA, USA, 3rd ed. edn.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Fractional cascading: I", "author": ["B. Chazelle", "L.J. Guibas"], "venue": "a data structuring technique. Algorithmica 1(2), 133\u2013162", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1986}, {"title": "An introduction to support Vector Machines: and other kernel-based learning methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Mismatch String Kernels for SVM Protein Classification", "author": ["C. Leslie", "E. Eskin", "W. Noble"], "venue": "Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Text classification using string kernels", "author": ["H. Lodhi", "C. Saunders", "J. Shawe-Taylor", "N. Cristianini", "C. Watkins"], "venue": "J. Mach. Learn. Res", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Efficient computation of gapped substring kernels on large alphabets", "author": ["J. Rousu", "J. Shawe-Taylor"], "venue": "J. Mach. Learn. Res", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "Kernel methods [4] offer an alternative solution to the limitation of traditional machine learning algorithms, applied solely on linear separable problems.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "[6] apply dynamic programming paradigm to the suffix version of the SSK.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Later, Rousu and Shawe-Taylor [7] propose an improvement to the dynamic programming approach based on the observation that most entries of the dynamic programming matrix (DP) do not really contribute to the result.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "Beyond the dynamic programming paradigm, the trie-based approach [5,7,8] is based on depth first traversal on an implicit trie data structure.", "startOffset": 65, "endOffset": 72}, {"referenceID": 6, "context": "Beyond the dynamic programming paradigm, the trie-based approach [5,7,8] is based on depth first traversal on an implicit trie data structure.", "startOffset": 65, "endOffset": 72}, {"referenceID": 7, "context": "Beyond the dynamic programming paradigm, the trie-based approach [5,7,8] is based on depth first traversal on an implicit trie data structure.", "startOffset": 65, "endOffset": 72}, {"referenceID": 3, "context": "In this case, the target concept cannot be expressed as simple linear combinations of the given attributes [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "It consists to replace the real number by a composite-number space [2].", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "The application of the fractional cascading technique introduced by [3] on a range tree creates a new data structure so called layered range tree.", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "For the String Subsequence Kernel (SSK) [6], the main idea is to compare strings depending on common subsequences they contain.", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7].", "startOffset": 110, "endOffset": 117}, {"referenceID": 6, "context": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7].", "startOffset": 110, "endOffset": 117}, {"referenceID": 7, "context": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7].", "startOffset": 110, "endOffset": 117}, {"referenceID": 6, "context": "We present three methods that compute the SSK efficiently, namely the dynamic programming [6], the trie-based [5,7,8] and the sparse dynamic programming approaches [7].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "For instance the (p,m)-mismatch string kernel [5] and restricted SSK [8].", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "For instance the (p,m)-mismatch string kernel [5] and restricted SSK [8].", "startOffset": 69, "endOffset": 72}, {"referenceID": 6, "context": "In the present section, we try to describe a trie-based SSK presented in [7] that slightly differ from those cited above [5,8].", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "In the present section, we try to describe a trie-based SSK presented in [7] that slightly differ from those cited above [5,8].", "startOffset": 121, "endOffset": 126}, {"referenceID": 7, "context": "In the present section, we try to describe a trie-based SSK presented in [7] that slightly differ from those cited above [5,8].", "startOffset": 121, "endOffset": 126}, {"referenceID": 0, "context": ", p 1 m\u2190 length(s) 2 n\u2190 length(t) 3 K(1 : p)\u2190 0 /* Computation of K1(s, t) */ 4 for i = 1:m do 5 for j = 1:n do 6 if s[i] = t[j] then 7 KPS[i, j]\u2190 \u03bb2 8 K[1]\u2190 K[1] +KPS[i, j]", "startOffset": 153, "endOffset": 156}, {"referenceID": 0, "context": ", p 1 m\u2190 length(s) 2 n\u2190 length(t) 3 K(1 : p)\u2190 0 /* Computation of K1(s, t) */ 4 for i = 1:m do 5 for j = 1:n do 6 if s[i] = t[j] then 7 KPS[i, j]\u2190 \u03bb2 8 K[1]\u2190 K[1] +KPS[i, j]", "startOffset": 159, "endOffset": 162}, {"referenceID": 6, "context": "Rousu and Shawe-Taylor [7] have proposed a solution using the sparse dynamic programming technique to avoid unnecessary computations.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "In this respect, we have used a layered range tree (LRT) in [1].", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": ", p 1 m\u2190 length(s) 2 n\u2190 length(t) 3 Creation of the initial match list L /* Computation of K1(s, t) */ 4 foreach ((i, j), v) \u2208 L do 5 K[1]\u2190 K[1] + v \u00b7 \u03bbi+j", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": ", p 1 m\u2190 length(s) 2 n\u2190 length(t) 3 Creation of the initial match list L /* Computation of K1(s, t) */ 4 foreach ((i, j), v) \u2208 L do 5 K[1]\u2190 K[1] + v \u00b7 \u03bbi+j", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "This trick is inspired from [7] to make the range sum results correct.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "We have discarded the trie-based algorithm from this comparison because it is an approximate algorithm on the one hand, on the other hand in the preliminary experiments conducted in [7] it was significantly slower than Dynamic and Sparse.", "startOffset": 182, "endOffset": 185}, {"referenceID": 6, "context": "To benefit from the empiric evaluation conducted in [7], we tried to keep the same conditions of their experiments.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "Rousu and Shawe-Taylor [7] state that with long strings based on large alphabets their approach is faster.", "startOffset": 23, "endOffset": 26}], "year": 2015, "abstractText": "Kernel methods are powerful tools in machine learning. They have to be computationally efficient. In this paper, we present a novel Geometric-based approach to compute efficiently the string subsequence kernel (SSK). Our main idea is that the SSK computation reduces to range query problem. We started by the construction of a match list L(s, t) = {(i, j) : si = tj} where s and t are the strings to be compared; such match list contains only the required data that contribute to the result. To compute efficiently the SSK, we extended the layered range tree data structure to a layered range sum tree, a range-aggregation data structure. The whole process takes O(p|L| log |L|) time and O(|L| log |L|) space, where |L| is the size of the match list and p is the length of the SSK. We present empiric evaluations of our approach against the dynamic and the sparse programming approaches both on synthetically generated data and on newswire article data. Such experiments show the efficiency of our approach for large alphabet size except for very short strings. Moreover, compared to the sparse dynamic approach, the proposed approach outperforms absolutely for long strings.", "creator": "LaTeX with hyperref package"}}}