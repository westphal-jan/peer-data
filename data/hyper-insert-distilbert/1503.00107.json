{"id": "1503.00107", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2015", "title": "Non-linear Learning for Statistical Machine Translation", "abstract": "unlike modern statistical generic machine element translation ( smt ) systems usually systematically use a straightforward linear similarity combination of different features coupled to formally model effectively the quality of either each unique translation hypothesis. the linear combination assumes ensure that all half the independent features are correlated in a coherent linear diffusion relationship and constrains that each local feature interacts with whilst the rest related features collectively in an linear manner, which might physically limit the expressive power distribution of the primary model and lead to suggesting a inherently under - fit causal model on the current underlying data. in this paper, we similarly propose a non - linear modeling version for challenging the quality dynamics of random translation - hypotheses based based on neural based networks, which allows more complex interaction between features. a learning modeling framework is presented for training employing the predicted non - linear models. rather we also discuss possible heuristics in designing the network structure formulation which may improve the correct non - linear learning tree performance. experimental results show that with the basic features constraint of a hierarchical sparse phrase - frame based machine oriented translation system, so our method holder produce parallel translations curves that must are better than a linear model.", "histories": [["v1", "Sat, 28 Feb 2015 09:53:32 GMT  (25kb)", "http://arxiv.org/abs/1503.00107v1", "submitted to a conference"]], "COMMENTS": "submitted to a conference", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["shujian huang", "huadong chen", "xinyu dai", "jiajun chen"], "accepted": true, "id": "1503.00107"}, "pdf": {"name": "1503.00107.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["email@domain", "email@domain"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 3.\n00 10\n7v 1\n[ cs\n.C L\n] 2\n8 Fe\nb 20\n15"}, {"heading": "1 Introduction", "text": "One of the core problems in the research of statistical machine translation is the modeling of translation hypotheses. Each modeling method defines a score of a target sentence e = e1, e2, ..., ei, ..., eI , given a source sentence f = f1, f2, ..., fj ...fJ , where each ei is the ith target word and fj is the jth source word. The well-known modeling method starts from the Source-Channel model (Brown et al., 1993)(Equation 1). The scoring of e decomposes to the calculation of a translation model and a language model.\nPr(e|f) = Pr(e)Pr(f |e)/Pr(f) (1)\nThe modeling method is extended to log-linear models by Och and Ney (2002), as shown in Equation 2, where hm(e|f) is the mth feature function and \u03bbm is the corresponding weight.\nPr(e|f) = p\u03bbM 1 (e|f)\n= exp[ \u2211M m=1 \u03bbmhm(e|f)]\u2211\ne\u2032 exp[ \u2211M m=1 \u03bbmhm(e \u2032|f)]\n(2)\nBecause the normalization term in Equation 2 is the same for all translation hypotheses of the same source sentence, the score of each hypothesis, denoted by sL, is actually a linear combination of all features, as shown in Equation 3.\nsL(e) =\nM\u2211\nm=1\n\u03bbmhm(e|f) (3)\nThe log-linear models are flexible to incorporate new features and show significant advantage over the traditional source-channel models, thus become the state-of-the-art modeling method and are applied in various translation settings (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006).\nIt is worth noticing that log-linear models try to separate good and bad translation hypotheses using a linear hyper-plane. However, complex interactions between features make it difficult to linearly separate good translation hypotheses from bad ones (Clark et al., 2014).\nTaking features in a typical phrase-based machine translation system (Koehn et al., 2003) as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses. The phrase translation probability feature selects phrases that occurs more frequently in the training corpus, which sometimes are long with lower translation probability, as in translating named entities or id-\nioms; sometimes are short but with high translation probability, as in translating verbs or pronouns. These three features jointly decide the choice of translations. Simply use the weighted sum of their values may not be the best choice for modeling translations.\nAs a result, log-linear models may under-fit the data. This under-fitting may prevents the further improvement of translation quality.\nIn this paper, we propose a non-linear modeling of translation hypotheses based on neural networks. The traditional features of a machine translation system are used as the input to the network. By feeding input features to nodes in a hidden layer, complex interactions among features are modeled, resulting in much stronger expressive power than traditional log-linear models. (Section 3)\nEmploying a neural network as non-linear models for SMT has two issues to be tackled. The first issue is the parameter learning. Loglinear models rely on minimum error rate training (MERT) (Och, 2003) to achieve best performance. When the scoring function become nonlinear, the intersection points of these non-linear functions could not be effectively calculated and enumerated. Thus MERT is no longer suitable for learning the parameters. To solve the problem , we present a framework for effective training including several criteria to transform the training problem into a binary classification task, a unified objective function and an iterative training algorithm. (Section 4)\nThe second issue is the structure of neural network. Single layer neural networks are equivalent to linear models; two-layer networks with sufficient nodes are capable of learning any continuous function (Bishop, 1995). Adding more layers into the network could model complex functions with less nodes, but also brings the problem of vanishing gradient (Erhan et al., 2009). We adapt a two-layer feed-forward neural network to keep the training process efficient. We notice that one major problem that prevent a neural network training reaching a good solution is that there are too many local minimums in the parameter space. Thus we discuss how to constrain the learning of neural networks with our intuition and observations of the features. (Section 5)\nExperiments are conducted to compare various settings and verify the effectiveness of our\nproposed learning framework. Experimental results show that our framework could achieve better translation quality even with the same traditional features as previous linear models. (Section 6)"}, {"heading": "2 Related work", "text": "Many research has been attempting to bring nonlinearity into the training of SMT. These efforts could be roughly divided into the following three categories.\nThe first line of research attempted to reinterpret original features via feature transformation or additional learning. For example, Maskey and Zhou (2012) use a deep belief network to learn representations of the phrase translation and lexical translation probability features. Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder. These work focus on the explicit representation of the features and usually employ extra learning procedure. Our proposed method only take the original feature with no transformation as input. Feature transformation or combination are performed implicitly during the training of the network and integrated with the optimization of translation quality.\nThe second line of research attempted to use non-linear models instead of log-linear models, which is most similar in spirit with our work. Duh and Kirchhoff (2008) used the boosting method to combine several results of MERT and achieved improvement in a re-ranking setting. Liu et al. (2013) proposed an additive neural network which employed a two-layer neural network for embedding-based features. To avoid local minimum, they still rely on a pre-training and posttraining from MERT or PRO. Comparing to these efforts, our proposed method takes a further step that it is integrated with iterative training, instead of re-ranking, and works without the help of any pre-trained linear models.\nThe third line of research attempted to add non-linear features/components into the loglinear learning framework. Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014), translation models (Gao et al., 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target side\nof translation rule as local features. In this paper we focus on enhancing the expressive power of the modeling, which is independent of the research of enhancing translation system with new designed features. We believe additional improvement could be achieved by incorporating more features into our framework."}, {"heading": "3 Non-linear Translation", "text": "The non-linear modeling of translation hypotheses could be used in both phrase-based system and syntax-based systems. In this paper, we take the hierarchical phrase based machine translation system (Chiang, 2005) as an example and introduce how we fit the non-linearity into the system."}, {"heading": "3.1 Decoding", "text": "The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006). For example, in the experiments of this paper, we use a CKY style decoding algorithm following Chiang (2005).\nOur non-linear translation system is different from traditional systems in the way to calculate the score for each hypothesis. Instead of calculating the score as a linear combination, we use neural networks (Section 3.2) to perform a non-linear combination of feature values.\nWe also use the cube-pruning algorithm (Chiang, 2005) to keep the decoding efficient. Although the non-linearity in model scores may cause more search errors in finding the highest scoring hypothesis, in practice it still achieves reasonable results."}, {"heading": "3.2 Two-layer Neural Networks", "text": "We employ a two-layer neural network as the nonlinear model for scoring translation hypotheses. The structure of a typical two-layer feed-forward neural network includes an input layer, a hidden layer, and a output layer (as shown in Figure 1).\nWe use the input layer to accept input features, the hidden layer to combine different input features, the output layer with only one node to output the model score for each translation hypothesis based on the value of hidden nodes. More specifically, the score of hypothesis e, denoted as sN , is\ndefined as:\nsN (e) = \u03c3o(Mo \u00b7\u03c3h(Mh \u00b7h m 1 (e|f)+bh)+bo) (4)\nwhere M , b is the weight matrix, bias vector of the neural nodes, respectively; \u03c3 is the activation function, which is often set to non-linear functions such as the tanh function or sigmoid function; subscript h and o indicates the parameters of hidden layer and output layer, respectively."}, {"heading": "3.3 Features", "text": "We use the standard features of a typical hierarchical phrase based translation system(Chiang, 2005). Adding new features into the framework is left as a future direction. The features as listed as following:\n\u2022 p(\u03b1|\u03b3) and p(\u03b3|\u03b1): conditional probability of translating \u03b1 as \u03b3 and translating \u03b1 as \u03b3, where \u03b1 and \u03b3 is the left and right hand side of a initial phrase (or hierarchical translation rule), respectively;\n\u2022 pw(\u03b1|\u03b3) and pw(\u03b3|\u03b1): lexical probability of translating words in \u03b1 as words in \u03b3 and translating words in \u03b3 as words in \u03b1;\n\u2022 plm: language model probability;\n\u2022 wc: accumulated count of individual words generated during translation;\n\u2022 pc: accumulated count of initial phrases used;\n\u2022 rc: accumulated count of hierarchical rule phrases used;\n\u2022 gc: accumulated count of glue rule used in this hypothesis;\n\u2022 uc: accumulated count of unknown source word;\n\u2022 nc: accumulated count of source phrases that translate into null;"}, {"heading": "4 Non-linear Learning Framework", "text": "Traditional machine translation systems rely on MERT to tune the weight of different features. MERT performs efficient search by enumerating the score function of all the hypotheses and using intersections of these linear functions to form the \u201dupper-envelope\u201d of the model score function (Och, 2003). When the scoring function is non-linear, it is not feasible to find the intersections of these functions. In this section, we discuss alternatives to train the parameter for non-linear models."}, {"heading": "4.1 Training Criteria", "text": "The task of machine translation is a complex problem with structural output space. Decoding algorithms search for the translation hypothesis with the highest score, according to a given scoring function, from an exponentially large set of candidate hypotheses. The purpose of training is to select the scoring function, so that the function score the hypotheses \u201dcorrectly\u201d. The correctness is often introduced by some extrinsic metrics, such as BLEU (Papineni et al., 2002).\nWe denote the scoring function as s(f , e; ~\u03b8), or simply s, which is parametrized by ~\u03b8; denote the set of all candidate hypotheses as C; denote the extrinsic metric as eval(\u00b7). Note that, in linear cases, s is a linear function as in Equation 3, while in the non-linear case described in this paper, s is the scoring function in Equation 4.\nIdeally, the training objective is to select a scoring function s, from all functions S , that scores the correct translation (or references), denoted as e\u0302, higher than any other hypotheses (Equation 5).\ns = {s \u2208 S|s(e\u0302) > s(e)\u2200e \u2208 C} (5)\nIn practice, the candidate set C is exponentially large and hard to enumerate; the correct translation e\u0302 may not even exist in the current search space for various reasons, e.g. unknown source word. As a result, we seek the following three alternatives as approximations to the ideal objective.\nBest v.s. Rest (BR) To score the best hypothesis in the n-best set e\u0303 higher than the rest hypotheses. This objective is very similar to MERT in that it tries to optimize the score\nof e\u0303 and doesn\u2019t concern about the ranking of rest hypothesis. In this case, the n-best set Cnbest is used to approximate C , and e\u0303 to approximate e\u0302.\nBest v.s. Worst (BW) To score the best hypothesis higher than the worst hypothesis in the n-best set. This objective is motivated by the practice of separating the \u201dhope\u201d and \u201dfear\u201d translation hypothesis (Chiang, 2012). We take a simpler strategy which uses the best and worst hypothesis in Cnbest as the \u201dhope\u201d and \u201dfear\u201d hypothesis, respectively, in order to avoid multi-pass decoding.\nPairwise (PW) To score the better hypotheses in sampled hypothesis pairs higher than the worse ones in the same pair. This objective is adapted from the Pairwise Ranking Optimization (PRO) (Hopkins and May, 2011), which tries to ranking all the hypotheses instead of selecting the best one. We use the same sampling strategy as their original paper.\nNote that each of the above criterions transforms the original problem of selecting best hypotheses from an exponential space to a certain pair-wise comparison problem, which could be easily trained as standard binary classifiers."}, {"heading": "4.2 Training Objective", "text": "For the binary classification task, we use a hinge loss following Watanabe (2012). Because the network has a lot of parameters compared with the linear model, we use a L1 norm instead of L2 norm as the regularization term, to favor sparse solutions. We define our training objective function in Equation 6.\nargmin \u03b8\n1\nN\n\u2211\nf\u2208D\n\u2211\n(e1,e2)\u2208T\n\u03b4(f , e1, e2; \u03b8) + \u03bb \u00b7 ||\u03b8||1\nwith\n\u03b4(\u00b7) = max{s(f , e1; \u03b8)\u2212 s(f , e2; \u03b8) + 1, 0}\n(6)\nD is the given training data; (e1, e2) is a training hypothesis-pair, with the assumption that e1 is the one with higher eval(\u00b7) score; N is the total number of hypothesis-pairs in D; T is the set of hypothesis-pairs for each source sentence.\nThe set T is decided by the criterion used for training. For the BR setting, the best hypothesis is\npaired with every other hypothesis in the n-best list (Equation 7); while for the BW setting, it is only paired with the worst hypothesis (Equation 8). The generation of T in PW setting is the same with PRO sampling, we refer the readers to the original paper of Hopkins and May (2011).\nTBR = {(e1, e2)|e1 = arg max e\u2208Cnbest eval(e),\ne2 \u2208 Cnbest and e1 6= e2} (7)\nTBW = {(e1, e2)|e1 = arg max e\u2208Cnbest eval(e),\ne2 = arg min e\u2208Cnbest eval(e)}\n(8)"}, {"heading": "4.3 Training Procedure", "text": "In standard training algorithm for classification, the training instances stays the same in each iteration. In machine translation, decoding algorithms usually return a very different n-best set with different parameters. This is due to the exponentially large size of search space. MERT and PRO extend the current nbest set by merging the n-best set of all previous iterations into a pool (Papineni et al., 2002; Hopkins and May, 2011). In this way, the enlarged n-best set may give a better approximation of the true hypothesis set C and may lead to better and more stable training results.\nWe argue that the training should still focus on hypotheses obtained in current round, because in each iteration the searching for the n-best set is independent of previous iterations. To compromise the above two goals, in our practice, training hypothesis pairs are first generated from the current n-best set, then merged with the pairs generated from all previous iterations. In order to make the model focus more on pairs from current iteration, we assign pairs in previous iterations a small constant weight and assign pairs in current iteration a relatively large constant weight. This is inspired by the AdaBoost algorithm (Schapire, 1999) in weighting instances.\nFollowing the spirit of MERT, we propose a iterative training procedure (Algorithm 1).\nAs shown in Algorithm 1, the training procedure starts by randomly init model parameters \u03b80 (line 1). In ith iteration, the decoding algorithm decodes each sentence f to get the n-best set Cnbest (line 5). Training hypothesis pairs T are extracted from Cnbest according to the training criterion described in Section 4.2 (line 6). New collected pairs\nAlgorithm 1 Iterative Training Algorithm Input: the set of training sentences D, max num-\nber of iteration I 1: \u03b80 \u2190 RandomInit(), 2: for i = 0 to I do 3: Ti \u2190 \u2205; 4: for each f \u2208 D do 5: Cnbest \u2190 NbestDecode(f ; \u03b8i) 6: T \u2190 GeneratePair(Cnbest) 7: Ti \u2190 Ti \u222a T 8: end for 9: Tall \u2190 WeightedCombine(\u222a i\u22121 k=0Tk, Ti)\n10: \u03b8i+1 \u2190 Optimize(Tall, \u03b8i) 11: end for\nTi are combined with pairs from previous iterations before used for training (line 9). \u03b8i+1 is obtained by solving Equation 6 using the Conjugate Sub-Gradient method (Le et al., 2011) (line 10)."}, {"heading": "5 Structure of the Network", "text": "Although neural networks bring strong expressive power to the modeling of translation hypothesis, training a neural network is prone to resulting in local minimum which may affect the training results. We speculate that one reason for these local minimums is the structure of a well-connected network has too many parameters. Take a neural network with k nodes in the input layer and m nodes in the hidden layer as an example. Every node in the hidden layer is connected to each of the k input nodes. This simple structure resulting in at least k \u00d7m parameters.\nIn Section 4.2, we use L1 norm in the objective function in order to get sparser solutions. In this section, we propose some constrained network structures according to our prior knowledge of the features. These structures have much less parameters or simpler structures comparing to original neural networks, thus reduce the possibility of getting stuck in local minimums."}, {"heading": "5.1 Network with two-degree Hidden Layer", "text": "We find the first pitfall of the standard two-layer neural network is that each node in the hidden layer receives input from every input layer node. Features used in SMT are usually manually designed, which has their concrete meanings. For a network of several hidden nodes, combining every features into every hidden node may be redundant\nand not necessary to represent the quality of a hypothesis.\nAs a result, we take a harsh step and constrain the nodes in hidden layer to have a in-degree of two, which means each hidden node only accepts inputs from two input nodes. We do not use any other prior knowledge about features in this setting. So for a network with k nodes in the input layer, the hidden layer should contain C2k = k(k \u2212 1)/2 nodes to accept all combinations from the input layer. We name this network structure as Two-Degree Hidden Layer Network (TDN).\nIt is easy to see that a TDN has C2k \u00d7 2 = k(k \u2212 1) parameters for the hidden layer because of the constrained degree. This is one order of magnitude less than a standard two-layer network with the same number of hidden nodes, which has C2k \u00d7 k = k\n2(k \u2212 1)/2 parameters. Note that we perform a 2-degree combination that looks similar in spirit with those combination of atomic features in large scale discriminative learning for other NLP tasks, such as POS tagging and parsing. However, unlike the practice in these tasks that directly combines values of different features to generate a new feature type, we first linearly combine the value of these features and perform non-linear transformation on these values via an activation function."}, {"heading": "5.2 Network with Grouped Features", "text": "It might be a too strong constraint to require the hidden node have in-degree of 2. In order to relax this constraint, we need more prior knowledge from the features. Our first observation is that there are different types of features. These types are different from each other in terms of value ranges, sources, importance, etc. For example, language model features usually take a very small value of probability, and word count feature takes a integer value and usually has a much higher weight in linear case than other count features.\nThe second observation is that features in the same group are basically of the same type and may not have complex interaction with each other. For example, it is reasonable to combine language model features with word count features in a hidden node. But it may not be necessary to combine the count of initial phrases and the count of unknown words into a hidden node.\nBased on the above two intuitions, we design a new structure of network that has the following\nconstraints: given a disjoint partition of features: G1, G1, Gk, every hidden node takes input from a set of input nodes, where any two nodes in this set come from two different feature groups. We name this network structure as Grouped Network (GN).\nIn practice, we divide the basic features in Section 3.3 into five groups: language model features, translation probability features, lexical probability features, the word count feature, and the rest of count features."}, {"heading": "6 Experiments and Results", "text": ""}, {"heading": "6.1 General Settings", "text": "We conduct experiments on a large scale machine translation tasks. The parallel data comes from LDC, including LDC2002E18, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T10, LDC2007T09, which consists of 8.6 million of sentence pairs. Monolingual data includes Xinhua portion of Gigaword corpus. We use multi-references data MT03 as training data, MT02 as development data, and MT04, MT05 as test data. These data are mainly in the same genre, avoiding the extra consideration of domain adaptation.\nThe Chinese side of the corpora is word segmented using ICTCLAS1. Our translation system is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005). We set the beam size to 20. We train a 5-gram language model on the monolingual data with MKN smoothing(Chen and Goodman, 1998). For each parameter tuning experiments, we ran the same training procedure 3 times and present the average results. The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002). Significant test is performed using bootstrap re-sampling implemented by Clark et al. (2011). We employ a twolayer neural network with 11 input layer nodes,\n1http://ictclas.nlpir.org/\ncorresponding to features listed in Section 3.3 and 1 output layer node. The number of nodes in the hidden layer varies in different settings. The sigmoid function is used as the activation function for each node in the hidden layer. For the output layer we use a linear activation function. We try different \u03bb for the L1 norm from 0.01 to 0.00001 and use the one with best performance on the development set. We solve the optimization problem with ALGLIB package2."}, {"heading": "6.2 Experiments of Training Criteria", "text": "This set experiments evaluates different training criteria discussed in Section 4.1. We generate hypothesis-pair according to BW, BR and PW criteria, respectively, and perform training with these pairs. In the PW criterion, we use the sampling method of PRO (Hopkins and May, 2011) and get the 50 hypothesis pairs for each sentence. We use 20 hidden nodes for all three settings to make a fair comparison.\nThe results are presented in Table 2. The first two rows compare training with and without the weighted combination of hypothesis pairs we discussed in Section 4.3. As the result suggested, with the weighted combination of hypothesis pairs from previous iterations, the performance improves significantly on both test sets.\nAlthough the system performance on the dev set varies, the performance on test sets are almost comparable. This suggest that although the three training criteria are based on different assumptions, their are basically equivalent for training translation systems.\nWe also compares the three training criteria in their number of new instances per iteration and final training accuracy (Table 3). Compared to BR which tries to separate the best hypothesis from the rest hypotheses in the n-best set, and PW which tries to obtain a correct ranking of all hy-\n2http://www.alglib.net/\npotheses, BW only aims at separating the best and worst hypothesis of each iteration, which is a easier task for learning a classifiers. It requires the least training instances and achieves the best performance in training. Note that, the accuracy for each system in Table 3 are the accuracy each system achieves after training stops. They are not calculated on the same set of instances, thus not directly comparable. We use the differences in accuracy as an indicator for the difficulties of the corresponding learning task.\nFor the rest of this paper, we use the BW criterion because it is much simpler compared to sampling method of PRO (Hopkins and May, 2011)."}, {"heading": "6.3 Experiments of Network Structures", "text": "We make several comparisons of the network structures and compare them with a baseline hierarchical phrase-based translation system (HPB) (Table 4).\nWe first compares the neural network with different number of hidden nodes. The systems TLayer20, TLayer30 and TLayer50 are standard two-layer feed forward neural network with 20, 30 and 50 hidden layer nodes3. We can see that training a larger network do lead to an improvement in translation quality. However training a larger network is often time-consuming. We experimented with neural networks with 100 and more hidden nodes (TLayer100 ). But TLayer30 takes 10 times longer in training time for each iter-\n3TLayer20 is the same system as BW in Table 2\nation than TLayer20 and did not finish by the time of submission deadline.\nWe then compared the two network structures proposed in Section 5. The Two-Degree Hidden Layer Network (TDN) already perform comparable to the baseline system. But it constrain all input to the hidden node to be of degree 2, which is likely to be too restrictive. With the grouped feature, we could design networks such as GN, which shows significant improvement over the baseline systems and achieves the best performance among all neural systems. Note that GN is in a much larger scale, but is also sparse in parameters and takes significant less training time than standard neural networks."}, {"heading": "7 Conclusion", "text": "In this paper, we discuss a non-linear framework for modeling translation hypothesis for statistical machine translation system. We also present a learning framework including training criterion and algorithms to integrate our modeling into a state of the art hierarchical phrase based machine translation system. Compared to previous effort in bringing in non-linearity into machine translation, our method uses a single two-layer neural networks and performs training independent with any previous linear training methods (e.g. MERT). Our method also trains its parameters without any pre-training or post-training procedure. Experiment shows that our method could improve the baseline system even with the same feature as input, in a large scale Chinese-English machine translation task.\nIn training neural networks with hidden nodes, we use heuristics to reduce the complexity of network structures and obtain extra advantages over\nstandard networks. It shows that heuristics and intuitions of the data and features are still important to a machine translation system.\nAs future work, it is necessary to integrate more features into our learning framework. It is also interesting to see how the non-linear modeling fit in to more complex learning tasks which involves domain specific learning techniques."}], "references": [{"title": "Decoder integration and expected BLEU training for recurrent neural network language models", "author": ["Auli", "Gao2014] Michael Auli", "Jianfeng Gao"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Auli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2014}, {"title": "Joint language and translation modeling with recurrent neural networks", "author": ["Auli et al.2013] Michael Auli", "Michel Galley", "Chris Quirk", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Auli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2013}, {"title": "Neural Networks for Pattern Recognition", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "The mathematic of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F. Brown", "Stephen Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Chen", "Goodman1998] S.F. Chen", "J.T. Goodman"], "venue": "Technical report,", "citeRegEx": "Chen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1998}, {"title": "A hierarchical phrase-based model for statistical machine translation. In annual meeting of the Association for Computational Linguistics", "author": ["David Chiang"], "venue": null, "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "Hope and fear for discriminative training of statistical translation models", "author": ["David Chiang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Chiang.,? \\Q2012\\E", "shortCiteRegEx": "Chiang.", "year": 2012}, {"title": "Better hypothesis testing for statistical machine translation: Controlling for optimizer instability", "author": ["Chris Dyer", "Alon Lavie", "Noah A. Smith"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Clark et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Locally non-linear learning for statistical machine translation via discretization and structured regularization. Transactions of the Association for Computational Linguistics, 2:393\u2013404", "author": ["Clark et al.2014] Jonathan Clark", "Chris Dyer", "Alon Lavie"], "venue": null, "citeRegEx": "Clark et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M. Schwartz", "John Makhoul"], "venue": null, "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Beyond log-linear models: Boosted minimum error rate training for n-best re-ranking", "author": ["Duh", "Kirchhoff2008] Kevin Duh", "Katrin Kirchhoff"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human", "citeRegEx": "Duh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duh et al\\.", "year": 2008}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Erhan et al.2009] Dumitru Erhan", "Pierre antoine Manzagol", "Yoshua Bengio", "Samy Bengio", "Pascal Vincent"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Gao et al.2014] Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Tuning as ranking", "author": ["Hopkins", "May2011] Mark Hopkins", "Jonathan May"], "venue": null, "citeRegEx": "Hopkins et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hopkins et al\\.", "year": 2011}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In HLT-NAACL", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "On optimization methods for deep learning", "author": ["Le et al.2011] Quoc V. Le", "Jiquan Ngiam", "Adam Coates", "Ahbik Lahiri", "Bobby Prochnow", "Andrew Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Tree-to-string alignment template for statistical machine translation", "author": ["Liu et al.2006] Yang Liu", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of the 44th Annual Meeting of the Association of Computational Linguistics", "citeRegEx": "Liu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "Additive neural networks for statistical machine translation", "author": ["Liu et al.2013] Lemao Liu", "Taro Watanabe", "Eiichiro Sumita", "Tiejun Zhao"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Learning new semi-supervised deep auto-encoder features for statistical machine translation", "author": ["Lu et al.2014] Shixiang Lu", "Zhenbiao Chen", "Bo Xu"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Vol-", "citeRegEx": "Lu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2014}, {"title": "Unsupervised deep belief features for speech translation", "author": ["Maskey", "Zhou2012] Sameer Maskey", "Bowen Zhou"], "venue": "In INTERSPEECH 2012, 13th Annual Conference of the International Speech Communication", "citeRegEx": "Maskey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Maskey et al\\.", "year": 2012}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Och", "Ney2002] Franz Josef Och", "Hermann Ney"], "venue": null, "citeRegEx": "Och et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Och et al\\.", "year": 2002}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL \u201902: Proceedings of the 40th Annual", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A brief introduction to boosting", "author": ["Robert E. Schapire"], "venue": "In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2,", "citeRegEx": "Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Schapire.", "year": 1999}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Yinggong Zhao", "Victoria Fossum", "David Chiang"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language", "citeRegEx": "Vaswani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Optimized online rank learning for machine translation", "author": ["Taro Watanabe"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Watanabe.,? \\Q2012\\E", "shortCiteRegEx": "Watanabe.", "year": 2012}, {"title": "A syntax-based statistical translation model", "author": ["Yamada", "Knight2001] Kenji Yamada", "Kevin Knight"], "venue": "In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Yamada et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 3, "context": "The well-known modeling method starts from the Source-Channel model (Brown et al., 1993)(Equation 1).", "startOffset": 68, "endOffset": 88}, {"referenceID": 21, "context": "Pr(e|f) = Pr(e)Pr(f |e)/Pr(f) (1) The modeling method is extended to log-linear models by Och and Ney (2002), as shown in Equation 2, where hm(e|f) is the mth feature function and \u03bbm is the corresponding weight.", "startOffset": 90, "endOffset": 109}, {"referenceID": 8, "context": "bad ones (Clark et al., 2014).", "startOffset": 9, "endOffset": 29}, {"referenceID": 14, "context": "Taking features in a typical phrase-based machine translation system (Koehn et al., 2003) as an example, the language model feature favors shorter hypotheses; the word penalty feature encourages longer hypotheses.", "startOffset": 69, "endOffset": 89}, {"referenceID": 21, "context": "Loglinear models rely on minimum error rate training (MERT) (Och, 2003) to achieve best performance.", "startOffset": 60, "endOffset": 71}, {"referenceID": 2, "context": "to linear models; two-layer networks with sufficient nodes are capable of learning any continuous function (Bishop, 1995).", "startOffset": 107, "endOffset": 121}, {"referenceID": 11, "context": "ishing gradient (Erhan et al., 2009).", "startOffset": 16, "endOffset": 36}, {"referenceID": 7, "context": "Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "Clark et al. (2014) used discretization to transform realvalued dense features into a set of binary indicator features. Lu et al. (2014) learned new features using a semi-supervised deep auto encoder.", "startOffset": 0, "endOffset": 137}, {"referenceID": 24, "context": "Neural network based models are trained as language models (Vaswani et al., 2013; Auli and Gao, 2014),", "startOffset": 59, "endOffset": 101}, {"referenceID": 12, "context": "translation models (Gao et al., 2014) or joint language and translation models (Auli et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 1, "context": ", 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014).", "startOffset": 49, "endOffset": 89}, {"referenceID": 9, "context": ", 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014).", "startOffset": 49, "endOffset": 89}, {"referenceID": 0, "context": ", 2014) or joint language and translation models (Auli et al., 2013; Devlin et al., 2014). Liu et al. (2013) also introduced word embedding for source and target side", "startOffset": 50, "endOffset": 109}, {"referenceID": 5, "context": "In this paper, we take the hierarchical phrase based machine translation system (Chiang, 2005) as an example and introduce how we fit the non-linearity into the system.", "startOffset": 80, "endOffset": 94}, {"referenceID": 14, "context": "The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006).", "startOffset": 123, "endOffset": 200}, {"referenceID": 5, "context": "The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006).", "startOffset": 123, "endOffset": 200}, {"referenceID": 16, "context": "The basic decoding algorithm could be kept almost the same as traditional phrase-based or syntax-based translation systems (Yamada and Knight, 2001; Koehn et al., 2003; Chiang, 2005; Liu et al., 2006).", "startOffset": 123, "endOffset": 200}, {"referenceID": 5, "context": ", 2003; Chiang, 2005; Liu et al., 2006). For example, in the experiments of this paper, we use a CKY style decoding algorithm following Chiang (2005).", "startOffset": 8, "endOffset": 150}, {"referenceID": 5, "context": "We also use the cube-pruning algorithm (Chiang, 2005) to keep the decoding efficient.", "startOffset": 39, "endOffset": 53}, {"referenceID": 5, "context": "We use the standard features of a typical hierarchical phrase based translation system(Chiang, 2005).", "startOffset": 86, "endOffset": 100}, {"referenceID": 21, "context": "MERT performs efficient search by enumerating the score function of all the hypotheses and using intersections of these linear functions to form the \u201dupper-envelope\u201d of the model score function (Och, 2003).", "startOffset": 194, "endOffset": 205}, {"referenceID": 22, "context": "The correctness is often introduced by some extrinsic metrics, such as BLEU (Papineni et al., 2002).", "startOffset": 76, "endOffset": 99}, {"referenceID": 6, "context": "This objective is motivated by the practice of separating the \u201dhope\u201d and \u201dfear\u201d translation hypothesis (Chiang, 2012).", "startOffset": 103, "endOffset": 117}, {"referenceID": 25, "context": "For the binary classification task, we use a hinge loss following Watanabe (2012). Because the network has a lot of parameters compared with the linear model, we use a L1 norm instead of L2 norm as the regularization term, to favor sparse solutions.", "startOffset": 66, "endOffset": 82}, {"referenceID": 22, "context": "MERT and PRO extend the current nbest set by merging the n-best set of all previous iterations into a pool (Papineni et al., 2002; Hopkins and May, 2011).", "startOffset": 107, "endOffset": 153}, {"referenceID": 23, "context": "This is inspired by the AdaBoost algorithm (Schapire, 1999) in weighting instances.", "startOffset": 43, "endOffset": 59}, {"referenceID": 15, "context": "\u03b8i+1 is obtained by solving Equation 6 using the Conjugate Sub-Gradient method (Le et al., 2011) (line 10).", "startOffset": 79, "endOffset": 96}, {"referenceID": 5, "context": "is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005).", "startOffset": 81, "endOffset": 95}, {"referenceID": 22, "context": "The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002).", "startOffset": 70, "endOffset": 93}, {"referenceID": 5, "context": "is an in-house implementation of the hierarchical phrase-based translation system(Chiang, 2005). We set the beam size to 20. We train a 5-gram language model on the monolingual data with MKN smoothing(Chen and Goodman, 1998). For each parameter tuning experiments, we ran the same training procedure 3 times and present the average results. The translation quality is evaluated use 4-gram case-insensitive BLEU (Papineni et al., 2002). Significant test is performed using bootstrap re-sampling implemented by Clark et al. (2011). We employ a twolayer neural network with 11 input layer nodes,", "startOffset": 82, "endOffset": 529}], "year": 2015, "abstractText": "Modern statistical machine translation (SMT) systems usually use a linear combination of features to model the quality of each translation hypothesis. The linear combination assumes that all the features are in a linear relationship and constrains that each feature interacts with the rest features in an linear manner, which might limit the expressive power of the model and lead to a under-fit model on the current data. In this paper, we propose a nonlinear modeling for the quality of translation hypotheses based on neural networks, which allows more complex interaction between features. A learning framework is presented for training the non-linear models. We also discuss possible heuristics in designing the network structure which may improve the non-linear learning performance. Experimental results show that with the basic features of a hierarchical phrase-based machine translation system, our method produce translations that are better than a linear model.", "creator": "LaTeX with hyperref package"}}}