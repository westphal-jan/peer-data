{"id": "1503.00806", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2015", "title": "An Introduction to Logics of Knowledge and Belief", "abstract": "this chapter provides with an introduction to some related basic concepts inherent of epistemic logic, beyond basic formal languages, since their symbolic semantics, and proof set systems. it also furthermore contains still an electronic overview form of the handbook, and including a brief history of philosophical epistemic finite logic articles and pointers to the literature.", "histories": [["v1", "Tue, 3 Mar 2015 02:20:43 GMT  (1392kb,D)", "http://arxiv.org/abs/1503.00806v1", "FIrst chapter of \"Handbook of Epistemic Logic\", by Hans van Ditmarsch, Joseph Y. Halpern, Wiebe van der Hoek, and Barteld Kooi"]], "COMMENTS": "FIrst chapter of \"Handbook of Epistemic Logic\", by Hans van Ditmarsch, Joseph Y. Halpern, Wiebe van der Hoek, and Barteld Kooi", "reviews": [], "SUBJECTS": "cs.AI cs.LO", "authors": ["hans van ditmarsch", "joseph y halpern", "wiebe van der hoek", "barteld kooi"], "accepted": false, "id": "1503.00806"}, "pdf": {"name": "1503.00806.pdf", "metadata": {"source": "CRF", "title": "An Introduction to Logics of Knowledge and Belief", "authors": ["Hans van Ditmarsch", "Joseph Y. Halpern", "Wiebe van der Hoek", "Barteld Kooi"], "emails": [], "sections": [{"heading": "1.1 Introduction to the Book", "text": "This introductory chapter has four goals:\n1. an informal introduction to some basic concepts of epistemic logic;\n2. basic formal languages, their semantics, and proof systems;\nChapter 1 of the Handbook of Epistemic Logic, H. van Ditmarsch, J.Y. Halpern, W. van der Hoek and B. Kooi (eds), College Publications, 2015, pp. 1\u201351.\nar X\niv :1\n50 3.\n3. an overview of the handbook; and\n4. a brief history of epistemic logic and pointers to the literature.\nIn Section 1.2, we deal with the first two items. We provide examples that should help to connect the informal concepts with the formal definitions. Although the informal meaning of the concepts that we discuss may vary from author to author in this book (and, indeed, from reader to reader), the formal definitions and notation provide a framework for the discussion in the remainder of the book.\nIn Section 1.3, we outline how the basic concepts from this chapter are further developed in subsequent chapters, and how those chapters relate to each other. This chapter, like all others, concludes with a section of notes, which gives all the relevant references and some historical background, and a bibliography."}, {"heading": "1.2 Basic Concepts and Tools", "text": "As the title suggests, this book uses a formal tool, logic, to study the notion of knowledge (\u201cepisteme\u201d in Greek, hence epistemic logic) and belief, and, in a wider sense, the notion of information.\nLogic is the study of reasoning, formalising the way in which certain conclusions can be reached, given certain premises. This can be done by showing that the conclusion can be derived using some deductive system (like the axiom systems we present in Section 1.2.5), or by arguing that the truth of the conclusion must follow from the truth of the premises (truth is the concern of the semantical approach of Section 1.2.2). However, first of all, the premises and conclusions need to be presented in some formal language, which is the topic of Section 1.2.1. Such a language allows us to specify and verify properties of complex systems of interest.\nReasoning about knowledge and belief, which is the focus of this book, has subtleties beyond those that arise in propositional or predicate logic. Take, for instance, the law of excluded middle in classical logic, which says that for any proposition p, either p or \u00acp (the negation of p) must hold; formally, p \u2228 \u00acp is valid. In the language of epistemic logic, we write Kap for \u2018agent a knows that p is the case\u2019. Even this simple addition to the language allows us to ask many more questions. For example, which of the following formulas should be valid, and how are they related? What kind of \u2018situations\u2019 do the formulas describe?\n\u2022 Kap \u2228 \u00acKap\n\u2022 Kap \u2228Ka\u00acp\n\u2022 Ka(p \u2228 \u00acp)\n\u2022 Kap \u2228 \u00acKa\u00acp It turns out that, given the semantics of interest to us, only the first and third formulas above are valid. Moreover as we will see below, Kap logically implies \u00acKa\u00acp, so the last formula is equivalent to \u00acKa\u00acp, and says \u2018agent a considers p possible\u2019. This is incomparable to the second formula, which says agent a knows whether p is true\u2019.\nOne of the appealing features of epistemic logic is that it goes beyond the \u2018factual knowledge\u2019 that the agents have. Knowledge can be about knowledge, so we can write expressions like Ka(Kap \u2192 Kaq) (a knows that if he knows that p, he also knows that q). More interestingly, we can model knowledge about other\u2019s knowledge, which is important when we reason about communication protocols. Suppose Ann knows some fact m (\u2018we meet for dinner the first Sunday of August\u2019). So we have Kam. Now suppose Ann e-mails this message to Bob at Monday 31st of July, and Bob reads it that evening. We then have Kbm\u2227KbKam. Do we have KaKbm? Unless Ann has information that Bob has actually read the message, she cannot assume that he did, so we have (Kam \u2227 \u00acKaKbm \u2227 \u00acKa\u00acKbm).\nWe also have KaKb\u00acKaKbm. To see this, we already noted that \u00acKaKb m, since Bob might not have read the message yet. But if we can deduce that, then Bob can as well (we implicitly assume that all agents can do perfect reasoning), and, moreover, Ann can deduce that. Being a gentleman, Bob should resolve the situation in which \u00acKaKbm holds, which he could try to do by replying to Ann\u2019s message. Suppose that Bob indeed replies on Tuesday morning, and Ann reads this on Tuesday evening. Then, on that evening, we indeed have KaKbKam. But of course, Bob cannot assume Ann read the acknowledgement, so we have \u00acKbKaKbKam. It is obvious that if Ann and Bob do not want any ignorance about knowledge of m, they better pick up the phone and verify m. Using the phone is a good protocol that guarantees Kam\u2227Kbm\u2227KaKbm\u2227KbKam\u2227KaKbKam\u2227 . . . , a notion that we call common knowledge; see Section 1.2.2.\nThe point here is that our formal language helps clarify the effect of a (communication) protocol on the information of the participating agents. This is the focus of Chapter 12. It is important to note that requirements of protocols can involve both knowledge and ignorance: in the above example for instance, where Charlie is a roommate of Bob, a goal (of Bob) for the protocol might be that he knows that Charlie does not know the message (Kb\u00acKcm), while a goal of Charlie might even be KcKb\u00acm. Actually, in the latter case, it may be more reasonable to write KcBb\u00acm: Charlie knows that Bob believes that there is no dinner on Sunday. A temporal progression from Kbm \u2227 \u00acKaKbm to KbKam can be viewed as learning.\nThis raises interesting questions in the study of epistemic protocols: given an initial and final specification of information, can we find a sequence of messages that take us from the former to the latter? Are there optimal such sequences? These questions are addressed in Chapter 5, specifically Sections 5.7 and 5.9.\nHere is an example of a scenario where the question is to derive a sequence of messages from an initial and final specification of information. It is taken from Chapter 12, and it demonstrates that security protocols that aim to ensure that certain agents stay ignorant cannot (and do not) always rely on the fact that some messages are kept secret or hidden.\nAlice and Betty each draw three cards from a pack of seven cards, and Eve (the eavesdropper) gets the remaining card. Can players Alice and Betty learn each other\u2019s cards without revealing that information to Eve? The restriction is that Alice and Betty can make only public announcements that Eve can hear.\nWe assume that (it is common knowledge that) initially, all three agents know the composition of the pack of cards, and each agent knows which cards she holds. At the end of the protocol, we want Alice and Betty to know which cards each of them holds, while Eve should know only which cards she (Eve) holds. Moreover, messages can only be public announcements (these are formally described in Chapter 6), which in this setting just means that Alice and Betty can talk to each other, but it is common knowledge that Eve hears them. Perhaps surprisingly, such a protocol exists, and, hopefully less surprisingly by now, epistemic logic allows us to formulate precise epistemic conditions, and the kind of announcements that should be allowed. For instance, no agent is allowed to lie, and agents can announce only what they know. Dropping the second condition would allow Alice to immediately announce Eve\u2019s card, for instance. Note there is an important distinction here: although Alice knows that there is an announcement that she can make that would bring about the desired state of knowledge (namely, announcing Eve\u2019s card), there is not something that Alice knows that she can announce that would bring about the desired state of knowledge (since does not in fact know Eve\u2019s card). This distinction has be called the de dicto/de re distinction in the literature. The connections between knowledge and strategic ability are the topic of Chapter 11.\nEpistemic reasoning is also important in distributed computing. As argued in Chapter 5, processes or programs in a distributed environment often have only a limited view of the global system initially; they gradually come to know more about the system. Ensuring that each process has the appropriate knowledge needed in order to act is the main issue here.\nThe chapter mentions a number of problems in distributed systems where epistemic tools are helpful, like agreement problems (the dinner example of Ann and Bob above would be a simple example) and the problem of mutual exclusion, where processes sharing a resource must ensure that only one process uses the resource at a time. An instance of the latter is provided in Chapter 8, where epistemic logic is used to specify a correctness property of the Railroad Crossing System. Here, the agents Train, Gate and Controller must ensure, based on the type of signals that they send, that the train is never at the crossing while the gate is \u2018up\u2019. Chapter 8 is on model checking; it provides techniques to automatically verify that such properties (specified in an epistemic temporal language; cf. Chapter 5) hold. Epistemic tools to deal with the problem of mutual exclusion are also discussed in Chapter 11, in the context of dealing with shared file updates.\nReasoning about knowing what others know (about your knowledge) is also typical in strategic situations, where one needs to make a decision based on how others will act (where the others, in turn, are basing their decision on their reasoning about you). This kind of scenario is the focus of game theory. Epistemic game theory studies game theory using notions from epistemic logic. (Epistemic game theory is the subject of Chapter 9 in this book.) Here, we give a simplified example of one of the main ideas. Consider the game in Figure 1.1.\nThis model represents a situation where two players, a and b, take turns, with a starting at the top node A. If a plays l (\u2018left\u2019) in this node, the game ends in node B and the payoff for a is 1 and that for b is 4. If a, however, plays r in A, the game proceeds to node C, where it is b\u2019s turn. Player b has a choice between playing L and R (note that we use upper case to distinguish b\u2019s moves from a\u2019s moves). The game continues until a terminal node is reached. We assume that both players are rational; that is, each prefers a higher outcome for themselves over a lower one. What will a play in the start node A?\nOne way to determine what will happen in this game is to use backward. Consider node E. If that node is reached, given that a is rational (denoted rata), a will play l here, since she prefers the outcome 4 over 3 (which she would get by playing r). Now consider node C. Since b knows that a is rational, he knows that his payoff when playing R at C is 1. Since b is rational, and playing L in C gives him 2, he will play L. The only thing needed to conclude this is (ratb\u2227Kbrata). Finally, consider node A. Player a can reason as we just did, so a knows that she has a choice between the payoff of 2 she would obtain by playing r and the payoff of 1 she would obtain by playing l. Since a is rational, she plays r at A. Summarising, the condition that justifies a playing r at A and b playing L at B is\nrata \u2227Karatb \u2227KaKbrata \u2227 ratb \u2227Kbrata\nThis analysis predicts that the game will end in node D. Although this analysis used only \u2018depth-two\u2019 knowledge (a knows that b knows), to perform a similar analysis for longer variants of this game requires deeper and deeper knowledge of rationality. In fact, in many epistemic analyses in game theory, common knowledge of rationality is assumed. The contribution of epistemic logic to game theory is discussed in more detail in Chapter 9."}, {"heading": "1.2.1 Language", "text": "Most if not all systems presented in this book extend propositional logic. The language of propositional logic assumes a set At of primitive (or atomic) propositions, typically denoted p, q, . . . , possibly with subscripts. They typically refer to statements that are considered basic; that is, they lack logical structure, like \u2018it is raining\u2019, or \u2018the window is closed\u2019. Classical logic then uses Boolean operators, such as \u00ac (\u2018not\u2019), \u2227 (\u2018and\u2019), \u2228, (\u2018or\u2019), \u2192 (\u2018implies\u2019), and\u2194 (\u2018if and only if\u2019), to build more complex formulas. Since all those operators can be defined in terms of \u2227 and \u00ac (see Definition 1.2), the formal definition of the language often uses only these two connectives. Formulas are denoted with Greek letters: \u03d5,\u03c8, \u03b1, . . . . So, for instance,\nwhile (p \u2227 q) is the conjunction of two primitive propositions, the formula (\u03d5\u2227\u03c8) is a conjunction of two arbitrary formulas, each of which may have further structure.\nWhen reasoning about knowledge and belief, we need to be able to refer to the subject, that is, the agent whose knowledge or belief we are talking about. To do this, we assume a finite set Ag of agents. Agents are typically denoted a, b, . . . , i, j, . . . , or, in specific examples, Alice,Bob, . . . . To reason about knowledge, we add operators Ka to the language of classical logic, where Ka\u03d5 denotes \u2018agent a knows (or believes) \u03d5\u2019. We typically let the context determine whether Ka represents knowledge or belief. If it is necessary to reason knowledge and belief simultaneously, we use operators Ka for knowledge and Ba for belief. Logics for reasoning about knowledge are sometimes called epistemic logics, while logics for reasoning about belief are called doxastic logics, from the Greek words for knowledge and belief. The operators Ka and Ba are examples of modal operators. We sometimes use 2 or 2a to denote a generic modal operator, when we want to discuss general properties of modal operators.\nDefinition 1.1 (An Assemblage of Modal Languages) Let At be a set of primitive propositions, Op a set of modal operators, and Ag a set of agent symbols. Then we define the language L(At,Op,Ag) by the following BNF:\n\u03d5 := p | \u00ac\u03d5 | (\u03d5 \u2227 \u03d5) | 2\u03d5,\nwhere p \u2208 At and 2 \u2208 Op. a\nTypically, the set Op depends on Ag. For instance, the language for multi-agent epistemic logic is L(At,Op,Ag), with Op = {Ka | a \u2208 Ag}, that is, we have a knowledge operator for every agent. To study interactions between knowledge and belief, we would have Op = {Ka, Ba | a \u2208 Ag}. The language of propositional logic, which does not involve modal operators, is denoted L(At); propositional formulas are, by definition, formulas in L(At).\nDefinition 1.2 (Abbreviations in the Language) As usual, parentheses are omitted if that does not lead to ambiguity. The following abbreviations are also standard (in the last one, A \u2286 Ag).\ndescription/name definiendum definiens false \u22a5 p \u2227 \u00acp true > \u00ac\u22a5 disjunction \u03d5 \u2228 \u03c8 \u00ac(\u00ac\u03d5 \u2227 \u00ac\u03c8) implication \u03d5\u2192 \u03c8 \u00ac\u03d5 \u2228 \u03c8 dual of K Ma\u03d5 or K\u0302a\u03d5 \u00acKa\u00ac\u03d5 everyone in A knows EA\u03d5 \u2227 a\u2208AKa\u03d5\nNote that Ma\u03d5, which say \u2018agent a does not know \u00ac\u03d5\u2019, can also be read \u2018agent a considers \u03d5 possible\u2019. a\nLet 2 be a modal operator, either one in Op or one defined as an abbreviation. We define the nth iterated application of 2, written 2n, as follows:"}, {"heading": "20\u03d5 = \u03d5 and 2n+1\u03d5 = 22n\u03d5.", "text": "We are typically interested in iterating the EA operator, so that we can talk about \u2018everyone in A knows\u2019, \u2018everyone in A knows that everyone in A knows\u2019, and so on.\nFinally, we define two measures on formulas.\nDefinition 1.3 (Length and modal depth) The length |\u03d5 | and the modal depth d(\u03d5) of a formula \u03d5 are both defined inductively as follows:\n|p | = 1 and d(p) = 0 |\u00ac\u03d5 | = |\u03d5 | +1 and d(\u00ac\u03d5) = d(\u03d5) |(\u03d5 \u2227 \u03c8) | = |\u03d5 | + |\u03c8 | +1 and d(\u03d5 \u2227 \u03c8) = max{d(\u03d5), d(\u03c8)} |2a\u03d5 | = |\u03d5 | +1 and d(2\u03d5) = 1 + d(\u03d5).\nIn the last clause, 2a is a modal operator corresponding to a single agent. Sometimes, if A \u2286 Ag is a group of agents and 2A is a group operator (like EA, DA or CA), |2A\u03d5 | depends not only on \u03d5, but also on the cardinality of A. a\nSo, |2a(q\u22272bp) |= 5 and d(2a(q\u22272bp)) = 2. Likewise, |2aq\u22272bp |= 5 while d(2aq \u22272bp) = 1."}, {"heading": "1.2.2 Semantics", "text": "We now define a way to systematically determine the truth value of a formula. In propositional logic, whether p is true or not \u2018depends on the situation\u2019. The relevant situations are formalised using valuations, where a valuation V : At\u2192 {true, false} determines the truth of primitive propositions. A valuation can be extended so as to determine the truth of all formulas, using a straightforward inductive definition: \u03d5\u2227\u03c8 is true given V iff each of \u03d5 and \u03c8 is true given V , and \u00ac\u03d5 is true given V iff \u03d5 is false given V . The truth conditions of disjunctions, implications, and bi-implications follow directly from these two clauses and Definition 1.2. To model knowledge and belief, we use ideas that go back to Hintikka. We think of an agent a as considering possible\na number of different situations that are consistent with the information that the agent has. Agent a is said to know (or believe) \u03d5, if \u03d5 is true in all the situations that a considers possible. Thus, rather than using a single situation to give meaning to modal formulas, we use a set of such situations; moreover, in each situation, we consider, for each agent, what other situations he or she considers possible. The following example demonstrates how this is done.\nExample 1.1 Bob is invited for a job interview with Alice. They have agreed that it will take place in a coffeehouse downtown at noon, but the traffic is quite unpredictable, so it is not guaranteed that either Alice or Bob will arrive on time. However, the coffeehouse is only a 15-minute walk from the bus stop where Alice plans to go, and a 10-minute walk from the metro station where Bob plans to go. So, 10 minutes before the interview, both Alice and Bob will know whether they themselves will arrive on time. Alice and Bob have never met before. A Kripke model describing this situation is given in Figure 1.2.\nSuppose that at 11:50, both Alice and Bob have just arrived at their respective stations. Taking ta and tb to represent that Alice (resp., Bob) arrive on time, this is a situation (denoted w in Figure 1.2) where both ta and tb are true. Alice knows that ta is true (so in w we have Kata), but she does not know whether tb is true; in particular, Alice considers possible the situation denoted v in Figure 1.2, where ta\u2227\u00actb holds. Similarly, in w, Bob considers it possible that the actual situation is s, where Alice is running late but Bob will make it on time, so that \u00acta \u2227 tb holds. Of course, in s, Alice knows that she is late; that is, Ka\u00acta holds. Since the only situations\nthat Bob considers possible at world w are w and s, he knows that he will be on time (Kbtb), and knows that Alice knows whether or not she is on time (Kb(Kata \u2228Ka\u00acta)). Note that the latter fact follows since Kata holds in world w and Ka\u00acta holds in world s, so Kata \u2228 Ka\u00acta holds in both worlds that Bob considers possible. a\nThis, in a nutshell, explains what the models for epistemic and doxastic look like: they contain a number of situations, typically called states or (possible) worlds, and binary relations on states for each agent, typically called accessibility relations. A pair (v, w) is in the relation for agent a if, in world v, agent a considers state w possible. Finally, in every state, we need to specify which primitive propositions are true.\nDefinition 1.4 (Kripke frame, Kripke model) Given a set At of primitive propositions and a set Ag of agents, a Kripke model is a structure M = \u3008S,RAg, V At), where \u2022 S 6= \u2205 is a set of states, sometimes called the domain of M , and\ndenoted D(M);\n\u2022 RAg is a function, yielding an accessibility relation Ra \u2286 S \u00d7 S for each agent a \u2208 Ag;\n\u2022 V At : S \u2192 (At \u2192 {true, false}) is a function that, for all p \u2208 At and s \u2208 S, determines what the truth value V At(s)(p) of p is in state s (so V At(s) is a propositional valuation for each s \u2208 S).\nWe often suppress explicit reference to the sets At and Ag, and write M = \u3008S,R, V \u3009, without upper indices. Further, we sometimes write sRat or Rast rather than (s, t) \u2208 Ra, and use Ra(s) or Ras to denote the set {t \u2208 S | Rast}. Finally, we sometimes abuse terminology and refer to V as a valuation as well.\nThe class of all Kripke models is denoted K. We use Km to denote the class of Kripke models where |Ag |= m. A Kripke frame F = \u3008S,R\u3009 focuses on the graph underlying a model, without regard for the valuation. a\nMore generally, given a modal logic with a set Op of modal operators, the corresponding Kripke model has the form M = \u3008S,ROp, V At\u3009, where there is a binary relation R2 for every operator 2 \u2208 Op. Op may, for example, consist of a knowledge operator for each agent in some set Ag and a belief operator for each agent in Ag.\nGiven Example 1.1 and Definition 1.4, it should now be clear how the truth of a formula is determined given a model M and a state s. A pair (M, s) is called a pointed model; we sometimes drop the parentheses and write M, s.\nDefinition 1.5 (Truth in a Kripke Model) Given a model M = \u3008S,RAg, V At\u3009, we define what it means for a formula \u03d5 to be true in (M, s), written M, s |= \u03d5, inductively as follows:\nM, s |= p iff V (s)(p) = true for p \u2208 At M, s |= \u03d5 \u2227 \u03c8 iff M, s |= \u03d5 and M, s |= \u03c8 M, s |= \u00ac\u03d5 iff not M, s |= \u03d5 (often written M, s 6|= \u03d5) M, s |= Ka\u03d5 iff M, t |= \u03d5 for all t such that Rast.\nMore generally, if M = \u3008S,ROp, V At\u3009, then for all 2 \u2208 Op:\nM, s |= 2\u03d5 iff (M, t) |= \u03d5 for all t such that R2st.\nRecall that Ma is the dual of Ka; it easily follows from the definitions that\nM, s |= Ma\u03d5 iff there exists some t such that Rast and M, t |= \u03d5.\nWe write M |= \u03d5 if M, s |= \u03d5 for all s \u2208 S. a\nExample 1.2 Consider the model of Figure 1.2. Note that Kap\u2228Ka\u00acp represents the fact that agent a knows whether p is true. Likewise, Map \u2227Ma\u00acp is equivalent to \u00acKa\u00acp \u2227 \u00acKap: agent a is ignorant about p. We have the following (in the final items we write Eab instead of E{a,b}):\n1. (M, s) |= tb: truth of a primitive proposition in s.\n2. M, s |= (\u00acta \u2227Ka\u00acta \u2227 \u00acKb\u00acta) \u2227 (tb \u2227 \u00acKatb \u2227Kbtb): at s, a knows that ta is false, but b does not; similarly, b knows that tb is true, but a does not.\n3. M |= Ka(Kbtb\u2228Kb\u00actb)\u2227Kb(Kata\u2228Ka\u00acta): in all states of M , agent a knows that b knows whether tb is true, and b knows that a knows whether ta is true.\n4. M |= Ka(Mbtb\u2227Mb\u00actb)\u2227Kb(Mata\u2227Ma\u00acta) in all states of M , agent a knows that b does not know whether ta is true, and b knows that a does not know whether tb is true.\n5. M |= Eab((Kata \u2228Ka\u00acta) \u2227 (Matb \u2227Ma\u00actb)): in all states, everyone knows that a knows whether ta is true, but a does not know whether tb is true.\n6. M |= EabEab((Kata\u2228Ka\u00acta)\u2227(Matb\u2227Ma\u00actb)): in all states, everyone knows what we stated in the previous item.\nThis shows that the modelM of Figure 1.2 is not just a model for a situation where a knows ta but not tb and agent b knows tb but not ta; it represents much more information. a\nAs the following example shows, in order to model certain situations, it may be necessary that some propositional valuations occur in more than one state in the model. Example 1.3 Recall the scenario of the interview between Alice and Bob, as presented in Example 1.1. Suppose that we now add the information that in fact Alice will arrive on time, but Bob is not going to be on time. Although Bob does not know Alice, he knows that his friend Carol is an old friend of Alice. Bob calls Carol, leaving a message on her machine to ask her to inform Alice about Bob\u2019s late arrival as soon as she is able to do so. Unfortunately for Bob, Carol does not get his message on time. This situation can be represented in state M,v of the model of Figure 1.3.\nNote that in (M,v), we have \u00acKa\u00actb (Alice does not know that Bob is late), but also Mb(Ka\u00actb) (Bob considers it possible that Alice knows that Bob is late). So, although the propositional valuations in v and v\u2032 are the same, those two states represent different situations: in v agent a is uncertain whether \u00actb holds, while in v\u2032 she knows \u00actb. Also, in M, v, Bob considers it possible that both of them will be late, and that Alice knows this: this is because Rbvu\n\u2032 holds in the model, and M,u\u2032 |= Ka(\u00acta\u2227\u00actb).a We often impose restrictions on the accessibility relation. For example, we may want to require that if, in world v, agent a considers world w possi-\nble, then in w, agent a should consider v possible. This requirement would make Ra symmetric. Similarly, we might require that, in each world w, a considers w itself possible. This would make Ra reflexive. More generally, we are interested in certain subclasses of models (typically characterized by properties of the accessibility relations).\nDefinition 1.6 (Classes of models, validity, satisfiability) Let X be a class of models, that is, X \u2286 K. If M |= \u03d5 for all models M in X , we say that \u03d5 is valid in X , and write X |= \u03d5. For example, for validity in the class of all Kripke models K, we write K |= \u03d5. We write X 6|= \u03d5 when it is not the case that X |= \u03d5. So X 6|= \u03d5 holds if, for some model M \u2208 X and some s \u2208 D(M), we have M, s |= \u00ac\u03d5. If there exists a model M \u2208 X and a state s \u2208 D(M) such that M, s |= \u03d5, we say that \u03d5 is satisfiable in X . a\nWe now define a number of classes of models in terms of properties of the relations Ra in those models. Since they depend only on the accessibility relation, we could have defined them for the underlying frames; indeed, the properties are sometimes called frame properties.\nDefinition 1.7 (Frame properties) Let R be an accessibility relation on a domain of states S.\n1. R is serial if for all s there is a t such that Rst. The class of serial Kripke models, that is, {M = \u3008S,R, V \u3009 | every Ra is serial} is denoted KD.\n2. R is reflexive if for all s, Rss. The class of reflexive Kripke models is denoted KT .\n3. R is transitive if for all s, t, u, if Rst and Rtu then Rsu. The class of transitive Kripke models is denoted K4.\n4. R is Euclidean if for all s, t, and u, if Rst and Rsu then Rtu. The class of Euclidean Kripke models is denoted K5\n5. R is symmetric if for all s, t, if Rst then Rts. The class of symmetric Kripke models is denoted KB\n6. We can combine properties of relations:\n(a) The class of reflexive transitive models is denoted S4. (b) The class of transitive Euclidean models is denoted K45. (c) The class of serial transitive Euclidean models is denoted KD45.\n(d) R is an equivalence relation if R is reflexive, symmetric, and transitive. It not hard to show that R is an equivalence relation if R is reflexive and Euclidean. The class of models where the relations are equivalence relations is denoted S5.\nAs we did for Km, we sometimes use the subscript m to denote the number of agents, so S5m, for instance, is the class of Kripke models with |Ag |= m. a\nOf special interest in this book is the class S5. In this case, the accessibility relations are equivalence classes. This makes sense if we think of Rast holding if s and t are indistinguishable by agent a based on the information that a has received. S5 has typically been used to model knowledge. In an S5 model, write s \u223ca t rather than Rast, to emphasize the fact that Ra is an equivalence relation. When it is clear that M \u2208 S5, when drawing the model, we omit reflexive arrows, and since the relations are symmetric, we connect states by a line, rather than using two-way arrows. Finally, we leave out lines that can be deduced to exist using transitivity. We call this the S5 representation of a Kripke model. Figure 1.4 shows the S5 representation of the Kripke model of Figure 1.3.\nWhen we restrict the classes of models considered, we get some interesting additional valid formulas.\nTheorem 1.1 (Valid Formulas) Parts (c)\u2013(i) below are valid formulas, where \u03b1 is a substitution instance of a propositional tautology (see below), \u03d5 and \u03c8 are arbitrary formulas, and X is one of the classes of models defined in Definition 1.7; parts (a), (b), and (j) show that we can infer some valid formulas from others.\n(a) If X |= \u03d5\u2192 \u03c8 and X |= \u03d5, then X |= \u03c8.\n(b) If X |= \u03d5 then X |= K\u03d5.\n(c) X |= \u03b1.\n(d) X |= K(\u03d5\u2192 \u03c8)\u2192 (K\u03d5\u2192 \u03c8).\n(e) KD |= K\u03d5\u2192M\u03d5.\n(f) T |= K\u03d5\u2192 \u03d5.\n(g) K4 |= K\u03d5\u2192 KK\u03d5.\n(h) K5 |= \u00acK\u03d5\u2192 K\u00acK\u03d5.\n(i) KB |= \u03d5\u2192 KM\u03d5.\n(j) If X \u2286 Y then Y |= \u03d5 implies that X |= \u03d5. a\nSince S5 is the smallest of the classes of models considered in Definition 1.7, it easily follows that all the formulas and inference rules above are valid in S5. To the extent that we view S5 as the class of models appropriate for reasoning about knowledge, Theorem 1.1 can be viewed as describing properties of knowledge. As we shall see, many of these properties apply to the standard interpretation of belief as well.\nParts (a) and (c) emphasise that we represent knowledge in a logical framework: modus ponens is valid as a reasoning rule, and we take all propositional tautologies for granted. In part (c), \u03b1 is a substitution instance of a propositional tautology. For example, since p \u2228 \u00acp and p \u2192 (q \u2192 p) are propositional tautologies, \u03b1 could be Kp \u2228 \u00acKp or K(p \u2228 q) \u2192 (Kr \u2192 K(p \u2228 q)). That is, we can substitute an arbitrary formula (uniformly) for a primitive proposition in a propositional tautology. Part (b) says that agents know all valid formulas, and part (d) says that an agent is able to apply modus ponens to his own knowledge. Part (e) is equivalent to K\u03d5\u2192 \u00acK\u00ac\u03d5; an agent cannot at the same time know a proposition and its negation. Part (f) is even stronger: it says that what an agent knows must be true. Parts (g) and (h) represent what has been called positive and negative introspection, respectively: an agent knows what he knows and what he does not know. Part (i) can be shown to follow from the other valid formulas; it says that if something is true, the agent knows that he considers it possible."}, {"heading": "Notions of Group Knowledge", "text": "So far, all properties that we have encountered are properties of an individual agent\u2019s knowledge. such as EA, defined above. In this section we introduce two other notions of group knowledge, common knowledge CA and distributed knowledge DA, and investigate their properties.\nExample 1.4 (Everyone knows and distributed knowledge) Alice and Betty each has a daughter; their children can each either be at the playground (denoted pa and pb, respectively) or at the library (\u00acpa, and \u00acpb, respectively). Each child has been carefully instructed that, if she ends up being on the playground without the other child, she should call her mother to inform her. Consider the situation described by the model M in Figure 1.5.\nWe have\nM |= ((\u00acpa \u2227 pb)\u2194 Ka(\u00acpa \u2227 pb)) \u2227 ((pa \u2227 \u00acpb)\u2194 Kb(pa \u2227 \u00acpb)).\nThis models the agreement each mother made with her daughter. Now consider the situation at state s. We have M, s |= Ka\u00ac(pa \u2227 \u00acpb), that is, Alice knows that it is not the case that her daughter is alone at the playground (otherwise her daughter would have informed her). What does each agent know at s? If we consider only propositional facts, it is easy to see that Alice knows pa \u2192 pb and Betty knows pb \u2192 pa. What does everyone know at s? The following sequence of equivalences is immediate from the definitions:\nM, s |= E{a,b}\u03d5 iff M, s |= Ka\u03d5 \u2227Kb\u03d5 iff \u2200x(Rasx\u21d2M,x |= \u03d5) and \u2200y(Rbsy \u21d2M,y |= \u03d5) iff \u2200x \u2208 {s, w, t} (M,x |= \u03d5) and \u2200y \u2208 {s, u, t} (M,y |= \u03d5) iff M |= \u03d5.\nThus, in this model, what is known by everyone are just the formulas valid in the model. Of course, this is not true in general.\nNow suppose that Alice and Betty an opportunity to talk to each other. Would they gain any new knowledge? They would indeed. Since M, s |=\nKa(pa \u2192 pb)\u2227Kb(pb \u2192 pa), they would come to know that pa \u2194 pb holds; that is, they would learn that their children are at least together, which is certainly not valid in the model. The knowledge that would emerge if the agents in a group A were allowed to communicate is called distributed knowledge in A, and denoted by the operator DA. In our example, we have M, s |= D{a,b}(pa \u2194 pb), although M, s |= \u00acKa(pa \u2194 pb) \u2227 \u00acKb(pa \u2194 pb). In other words, distributed knowledge is generally stronger than any individual\u2019s knowledge, and we therefore cannot define DA\u03d5 as \u2228 i\u2208AKi\u03d5, the dual of general knowledge that we may have expected; that would be weaker than any individual agent\u2019s knowledge. In terms of the model, what would happen if Alice and Betty could communicate is that Alice could tell Betty that he should not consider state u possible, while Betty could tell Alice that she should not consider state w possible. So, after communication, the only states considered possible by both agents at state s are s and t. This argument suggests that we should interpret DA as the necessity operator (2-type modal operator) of the relation \u22c2 a\u2208ARa. By way of contrast, it follows easily from the definitions that EA can be interpreted as the necessity operator of the relation \u22c3 a\u2208ARa. a\nThe following example illustrates common knowledge.\nExample 1.5 (Common knowledge) This time we have two agents: a sender (s) and a receiver (r). If a message is sent, it is delivered either immediately or with a one-second delay. The sender sends a message at time t0. The receiver does not know that the sender was planning to send the message. What is each agent\u2019s state of knowledge regarding the message?\nTo reason about this, let sz (for z \u2208 Z) denote that the message was sent at time t0+z, and, likewise, let dz denote that the message was delivered at time t = z. Note that we allow z to be negative. To see why, consider the world w0,0 where the message arrives immediately (at time t0). (In general, in the subscript (i, j) of a world wi,j , i denotes the time that the message was sent, and j denotes the time it was received.) In world w0,0, the receiver considers it possible that the message was sent at time t0 \u2212 1. That is, the receiver considers possible the world w\u22121,0 where the message was sent at t0 \u2212 1 and took one second to arrive. In world w\u22121,0, the sender considers possible the world w\u22121,\u22121 where the message was sent at time t0 \u2212 1 and arrived immediately. And in world w\u22121,\u22121, the receiver considers possible a world w\u22122,\u22121 where the message as sent at time t0 \u2212 2. (In general, in world wn,m, the message is sent at time t0 +n and received at time t0 +m.) In addition, in world w0,0, the sender considers possible world w0,1, where the message is received at time t0 + 1. The situation is described in the following model M .\nWriting E for \u2018the sender and receiver both know\u2019, it easily follows that\nM,w0,0 |= s0 \u2227 d0 \u2227 \u00acE\u00acs\u22121 \u2227 \u00acE\u00acd1 \u2227 \u00acE3\u00acs\u22122.\nThe notion of \u03d5 being common knowledge among group A, denoted CA\u03d5, is meant to capture the idea that, for all n, E\nn\u03d5 is true. Thus, \u03d5 is not common among A if someone in A considers it possible that someone in A considers it possible that . . . someone in A considers it possible that \u03d5 is false. This is formalised below, but the reader should already be convinced that in our scenario, even if it is common knowledge among the agents that messages will have either no delay or a one-second delay, it is not common knowledge that the message was sent at or after time t0 \u2212m for any value of m! a Definition 1.8 (Semantics of three notions of group knowledge) Let A \u2286 Ag be a group of agents. Let REA = \u222aa\u2208ARa. As we observed above,\n(M, s) |= EA\u03d5 iff for all t such that REAst, we have (M, t) |= \u03d5.\nSimilarly, taking RDA = \u2229a\u2208ARa, we have\n(M, s) |= DA\u03d5 iff for all t such that RDAst, we have (M, t) |= \u03d5.\nFinally, recall that the transitive closure of a relation R is the smallest relation R+ such that R \u2286 R+, and such that, for all x, y, and z, if R+xy and R+yz then R+xz. We define RCA as R + EA = ( \u22c3 a\u2208ARa) +. Note that, in Figure 1.6, every pair of states is in the relation R+C{r,s} . In general, we have RCAst iff there is some path s = s0, s1, . . . , sn = t from s to t such that n \u2265 1 and, for all i < n, there is some agent a \u2208 A for which Rasisi+1. Define\n(M, s) |= CA\u03d5 iff for all t such that RCAst, (M, t) |= \u03d5.\nIt is almost immediate from the definitions that, for a \u2208 A, we have\nK |= (CA\u03d5\u2192 EA\u03d5) \u2227 (EA\u03d5\u2192 Ka\u03d5) \u2227 (Ka\u03d5\u2192 DA\u03d5). (1.1)\nMoreover, for T (and hence also for S4 and S5), we have\nT |= Da\u03d5\u2192 \u03d5.\nThe relative strengths shown in (1.1) are strict in the sense that none of the converse implications are valid (assuming that A 6= {a}).\nWe conclude this section by defining some languages that are used later in this chapter. Fixing At and Ag, we write LX for the language L(At,Op,Ag), where\nX = K if Op = {Ka | a \u2208 Ag} X = CK if Op = {Ka, CA | a \u2208 Ag, A \u2286 Ag} X = DK if Op = {Ka, DA | a \u2208 Ag, A \u2286 Ag} X = CDK if Op = {Ka, CA, DA | a \u2208 Ag, A \u2286 Ag} X = EK if Op = {Ka, EA | a \u2208 Ag, A \u2286 Ag}."}, {"heading": "Bisimulation", "text": "It may well be that two models (M, s) and (M \u2032, s\u2032) \u2018appear different\u2019, but still satisfy the same formulas. For example, consider the models (M, s), (M \u2032, s\u2032), and (N, s1) in Figure 1.7. As we now show, they satisfy the same formulas. We actually prove something even stronger. We show that all of (M, s), (M, t), (M \u2032, s\u2032), (N, s1), (M, s2), and (N, s3) satisfy the same formulas, as do all of (M,u), (M,w), (M \u2032, w\u2032), (N,w1), and (N,w2). For the purposes of the proof, call the models in the first group green, and the models in the second group red. We now show, by induction on the structure of formulas, that all green models satisfy the same formulas, as do all red models. For primitive propositions, this is immediate. And if two models of the same colour agree on two formulas, they also agree on their negations and their conjunctions. The other formulas we need to consider are knowledge formulas. Informally, the argument is this. Every agent considers, in every pointed model, both green and red models possible. So his knowledge in each pointed model is the same. We now formalise this reasoning.\nDefinition 1.9 (Bisimulation) Given models M = (S,R, V ) and M \u2032 = (S\u2032, R\u2032, V \u2032), a non-empty relation R \u2286 S\u00d7S\u2032 is a bisimulation between M and M \u2032 iff for all s \u2208 S and s\u2032 \u2208 S\u2032 with (s, s\u2032) \u2208 R:\n\u2022 V (s)(p) = V \u2032(s\u2032)(p) for all p \u2208 At;\n\u2022 for all a \u2208 Ag and all t \u2208 S, if Rast, then there is a t\u2032 \u2208 S\u2032 such that R\u2032as \u2032t\u2032 and (t, t\u2032) \u2208 R;\n\u2022 for all a \u2208 Ag and all t\u2032 \u2208 S\u2032, if R\u2032as\u2032t\u2032, then there is a t \u2208 S such that Rast and (t, t \u2032) \u2208 R.\nWe write (M, s)\u2194(M \u2032, s\u2032) iff there is a bisimulation between M and M \u2032 linking s and s\u2032. If so, we call (M, s) and (M \u2032, s\u2032) bisimilar. a\nFigure 1.7 illustrates some bisimilar models. In terms of the models\nof Figure 1.7, we have M, s\u2194M \u2032, s\u2032, M, s\u2194N, s1, etc. We are interested in bisimilarity because, as the following theorem shows, bisimilar models satisfy the same formulas involving the operators Ka and CA.\nTheorem 1.2 (Preservation under bisimulation) Suppose that (M, s)\u2194(M \u2032, s\u2032). Then, for all formulas \u03d5 \u2208 LCK , we have\nM, s |= \u03d5\u21d4M \u2032, s\u2032 |= \u03d5. a\nThe proof of the theorem proceeds by induction on the structure of formulas, much as in our example. We leave the details to the reader.\nNote that Theorem 1.2 does not claim that distributed knowledge is preserved under bisimulation, and indeed, it is not, i.e., Theorem 1.2 does not hold for a language with DA as an operator. Figure 1.8 provides a witness for this. We leave it to the reader to check that although (M, s)\u2194(N, s1) for the two pointed models of Figure 1.8, we nevertheless have (M, s) |= \u00acD{a,b}p and (N, s1) |= D{a,b}p.\nWe can, however, generalise the notion of bisimulation to that of a group bisimulation and \u2018recover\u2019 the preservation theorem, as follows. If A \u2286 Ag,\ns and t are states, then we write RAst if A = {a | Rast}. That is, RAst holds if the set of agents a for which s and t are a-connected is exactly A. (M, s) and (M \u2032, s\u2032) are group bisimilar, written (M, s)\u2194group(M \u2032, s\u2032), if the conditions of Definition 1.9 are met when every occurrence of an individual agent a is replaced by the group A. Obviously, being group bisimilar implies being bisimilar. Note that the models (M, s) and (N, s1) of Figure 1.8 are bisimilar, but not group bisimilar. The proof of Theorem 1.3 is analogous to that of Theorem 1.2.\nTheorem 1.3 (Preservation under bisimulation) Suppose that (M, s)\u2194group(M \u2032, s\u2032). Then, for all formulas \u03d5 \u2208 LCDK , we have\nM, s |= \u03d5\u21d4M \u2032, s\u2032 |= \u03d5. a"}, {"heading": "1.2.3 Expressivity and Succinctness", "text": "If a number of formal languages can be used to model similar phenomena, a natural question to ask is which language is \u2018best\u2019. Of course, the answer depends on how \u2018best\u2019 is measured. In the next section, we compare various languages in terms of the computational complexity of some reasoning problems. Here, we consider the notions of expressivity (what can be expressed in the language?) and succinctness (how economically can one say it?)."}, {"heading": "Expressivity", "text": "To give an example of expressivity and the tools that are used to study it, we start by showing that finiteness of models cannot be expressed in epistemic logic, even if the language includes operators for common knowledge and distributed knowledge.\nTheorem 1.4 There is no formula \u03d5 \u2208 LCDK such that, for all S5-models M = \u3008S,R, V \u3009,\nM |= \u03d5 iff S is finite a\nProof Consider the two models M and M \u2032 of Figure 1.9. Obviously,\nM is finite and M \u2032 is not. Nevertheless, the two models are easily seen to be group bisimilar, so they cannot be distinguished by epistemic formulas. More precisely, for all formulas \u03d5 \u2208 LCDK , we have M, s |= \u03d5 iff M \u2032, s1 |= \u03d5 iff M \u2032, s2 |= \u03d5 iff M \u2032, sn |= \u03d5 for some n \u2208 N, and hence M |= \u03d5 iff M \u2032 |= \u03d5. a\nIt follows immediately from Theorem 1.4 that finiteness cannot be expressed in the language LCDK in a class X of models containing S5.\nWe next prove some results that let us compare the expressivity of two different languages. We first need some definitions.\nDefinition 1.10 Given a class X of models, formulas \u03d51 and \u03d52 are equivalent on X , written \u03d51 \u2261X \u03d52, if, for all (M, s) \u2208 X , we have that M, s |= \u03d51 iff M, s |= \u03d52. Language L2 is at least as expressive as L1 on X , written L1 vX L2 if, for every formula \u03d51 \u2208 L1, there is a formula \u03d52 \u2208 L2 such that \u03d51 \u2261X \u03d52. L1 and L2 are equally expressive on X if L1 vX L2 and L2 vX L1. If L1 vX L2 but L2 6vX L1, then L2 is more expressive than L1 on X , written L1 <X L2.a\nNote that if Y \u2286 X , then L1 vX L2 implies L1 vY L2, while L1 6vY L2 implies L1 6vX L2. Thus, the strongest results that we can show for the classes of models of interest to us are L1 vK L2 and L1 6vS5 L2\nWith these definitions in hand, we can now make precise that common knowledge \u2018really adds\u2019 something to epistemic logic.\nTheorem 1.5 LK vK LCK and LK 6vS5 LCK . a\nProof Since LK \u2286 LCK , it is obvious that LK vK LCK . To show that LCK 6vS5 LK , consider the sets of pointed models M = {(Mn, s1) | n \u2208 N} and N = {(Nn, t1) | n \u2208 N} shown in Figure 1.10. The two models Mn and Nn differ only in (Mn, sn+1) (where p is false) and (Nn, tn+1) (where p is true). In particular, the first n \u2212 1 states of (Mn, s1) and (Nn, t1) are the same. As a consequence, it is easy to show that,\nfor all n \u2208 N and \u03d5 \u2208 LK with d(\u03d5) < n, (Mn, s1) |= \u03d5 iff (Nn, t1) |= \u03d5. (1.2) Clearly M |= C{a,b}\u00acp while N |= \u00acC{a,b}\u00acp. If there were a formula \u03d5 \u2208 LK equivalent to C{a,b}\u00acp, then we would haveM |= \u03d5 while N |= \u00ac\u03d5. Let d = d(\u03d5), and consider the pointed models (Md+1, s1) and (Nd+1, t1). Since the first is a member ofM and the second of N , the pointed models disagree on C{a,b}\u00acp; however, by (1.2), they agree on \u03d5. This is obviously a contradiction, therefore a formula \u03d5 \u2208 L that is equivalent to C{a,b}\u00acp does not exist.\na\nThe next result shows, roughly speaking, that distributed knowledge is not expressible using knowledge and common knowledge, and that common knowledge is not expressible using knowledge and distributed knowledge.\nTheorem 1.6 (a) LK vK LDK and LK 6vS5 LDK ;\n(b) LCK 6vS5 LDK ;\n(c) LDK 6vS5 LCK ;\n(d) LCK vK LCDK and LCDK 6vS5 LCK ;\n(e) LDK vK LCDK and LCDK 6vS5 LDK . a\nProof For part (a), vK holds trivially. We use the models in Figure 1.8 to show that LDK 6vS5 LK . Since (M, s)\u2194(N, s1), the models verify the same L-formulas. However, LDK discriminates them: we have (M, s) |= \u00acD{a,b}p, while (N, s1) |= D{a,b}p. Since (M, s) and (N, s1) also verify the same LCKformulas, part (3) also follows.\nFor part (b), observe that (1.2) is also true for all formulas \u03d5 \u2208 LDK , so the formula C{a,b}\u00acp \u2208 LCK is not equivalent to a formula in LDK .\nPart (c) is proved using exactly the same models and argument as part (a).\nFor part (d), v is obvious. To show that LCDK 6vS5 LDK , we can use the models and argument of part (b). Similarly, for part (e), v is obvious. To show that LCDK 6vS5 LDK , we can use the models and argument of part (a). a\nWe conclude this discussion with a remark about distributed knowledge. We informally described distributed knowledge in a group as the knowledge that would obtain were the agents in that group able to communicate. However, Figure 1.8 shows that this intuition is not quite right. First, observe that both a and b know the same formulas in (M, s) and (N, s1); they even know the same formulas in (M, s) and (N, s1). That is, for all \u03d5 \u2208 LK , we have\n(M, s) |= Ka\u03d5 iff (M, s) |= Kb\u03d5 iff (N, s1) |= Ka\u03d5 iff (N, s1) |= Kb\u03d5\nBut if both agents possess the same knowledge in (N, s1), how can communication help them in any way, that is, how can it be that there is distributed knowledge (of p) that no individual agent has? Similarly, if a has the same knowledge in (M, s) in (N, s1), and so does b, why would communication in one model (N) lead them to know p, while in the other, it does not? Semantically, one could argue that in s1 agent a could \u2018tell\u2019 agent b that t2 \u2018is not possible\u2019, and b could \u2018tell\u2019 a that t1 \u2018is not possible\u2019. But how would verify the same formulas? This observation has led some researchers to require that distributed knowledge be interpreted in what are called bisimulation contracted models (see the notes at the end of the chapter for references). Roughly, a model is bisimulation contracted if it does not contain two points that are bisimilar. Model M of Figure 1.8 is bisimulation contracted, model N is not."}, {"heading": "Succinctness", "text": "Now suppose that two languages L1 and L2 are equally expressive on X , and also that their computational complexity of the reasoning problems for them is equally good, or equally bad. Could we still prefer one language over the other? Representational succinctness may provide an answer here: it may be the case that the description of some properties is much shorter in one language than in the other.\nBut what does \u2018much shorter\u2019 mean? The fact that there is a formula L1 whose length is 100 characters less than the shortest equivalent formula in L2 (with respect to some class X of models) does not by itself make L1 much more succinct that L2.\nWe want to capture the idea that L1 is exponentially more succinct than L2. We cannot do this by looking at just one formula. Rather, we need a sequence of formulas \u03b11, \u03b12, \u03b13, . . . in L1, where the gap in size between \u03b1n and the shortest formula equivalent to \u03b1n in L2 grows exponentially in n. This is formalised in the next definition.\nDefinition 1.11 (Exponentially more succinct) Given a class X of models, L1 is exponentially more succinct than L2 on X if the following conditions hold:\n(a) for every formula \u03b2 \u2208 L2, there is a formula \u03b1 \u2208 L1 such that \u03b1 \u2261X \u03b2 and |\u03b1 |\u2264|\u03b2 |.\n(b) there exist k1, k2 > 0, a sequence \u03b11, \u03b12, . . . of formulas in L1, and a sequence \u03b21, \u03b22, . . . of formulas in L2 such that, for all n, we have:\n(i) |\u03b1n |\u2264 k1n; (ii) |\u03b2n | \u2265 2k2n; (iii) \u03b2n is the shortest formula in L2 that is equivalent to \u03b1n on X .a\nIn words, L1 is exponentially more succinct than L2 if, for every formula \u03b2 \u2208 L2, there is a formula in L1 that is equivalent and no longer than \u03b2, but there is a sequence \u03b11, \u03b12, . . . of formulas in L1 whose length increases at most linearly, but there is no sequence \u03b21, \u03b22, . . . of formulas in L2 such that \u03b2n is the equivalent to \u03b1n and the length of the formulas in the latter sequence is increasing better than exponentially.\nWe give one example of succinctness results here. Consider the language LEK . Of course, EA can be defined using the modal operators Ki for i \u2208 A. But, as we now show, having the modal operators EA in the language makes the language exponentially more succinct.\nTheorem 1.7 The language LEK is exponentially more succinct than LK on X , for all X between K and S5. a\nProof Clearly, for every formula \u03b1 in (L)K , there is an equivalent formula in LEK that is no longer than \u03b1, namely, \u03b1 itself. Now consider the following two sequences of formulas:\n\u03b1n = \u00acEn{a,b}\u00acp\nand\n\u03b21 = \u00ac(Ka\u00acp \u2227Kb\u00acp), and \u03b2n = \u00ac(Ka\u00ac\u03b2n\u22121 \u2227Kb\u00ac\u03b2n\u22121).\nIf we take |EA\u03d5 |=|A | + |\u03d5 |, then it is easy to see that |\u03b1n |= 2n + 3, so |\u03b1n | is increasing linearly in n. On the other hand, since |\u03b2n |> 2 |\u03b2n\u22121 |, we have |\u03b2 |\u2265 2n. It is also immediate from the definition of E{a,b} that \u03b2n is equivalent to \u03b1n for all classes X between K and S5. To complete the proof, we must show that there is no formula shorter than \u03b2n in LK that is equivalent to \u03b1n. This argument is beyond the scope of this book; see the notes for references. a"}, {"heading": "1.2.4 Reasoning problems", "text": "Given the machinery developed so far, we can state some basic reasoning problems in semantic terms. They concern satisfiability and model checking. Most of those problems are typically considered with a specific class of models and a specific language in mind. So let X be some class of models, and let L be a language."}, {"heading": "Decidability Problems", "text": "A decidability problem checks some input for some property, and returns \u2018yes\u2019 or \u2018no\u2019.\nDefinition 1.12 (Satisfiability) The satisfiability problem for X is the following reasoning problem.\nProblem: satisfiability in X , denoted satX . Input: a formula \u03d5 \u2208 L. Question: does there exist a model M \u2208 X and a state s \u2208 D(M) such that M, s |= \u03d5? Output: \u2018yes\u2019 or \u2018no\u2019.\nObviously, there may well be formulas that are satisfiable in some Kripke model (or generally, in a class Y), but not in S5 models. Satisfiability in X is closely related to the problem of validity in X , due to the following equivalence: \u03d5 is valid in X iff \u00ac\u03d5 is not satisfiable in X .\nProblem: validity in X , denoted valX . Input: a formula \u03d5 \u2208 L. Question: is it the case that X |= \u03d5? Output: \u2018yes\u2019 or \u2018no\u2019.\nThe next decision problem is computationally and conceptually simpler than the previous two, since rather than quantifying over a set of models, a specific model is given as input (together with a formula).\nDefinition 1.13 (Model checking) The model checking problem for X is the following reasoning problem:\nProblem: Model checking in X , denoted modcheckX . Input: a formula \u03d5 \u2208 L and a pointed model (M, s) with M \u2208 X and s \u2208 D(M). Question: is it the case that M, s |= \u03d5? Output:: \u2018yes\u2019 or \u2018no\u2019.\nThe field of computational complexity is concerned with the question of how much of a resource is needed to solve a specific problem. The resources of most interest are computation time and space. Computational complexity then asks questions of the following form: if my input were to increase in size, how much more space and/or time would be needed to compute the answer? Phrasing the question this way already assumes that the problem at hand can be solved in finite time using an algorithm, that is, that the problem is decidable. Fortunately, this is the case for the problems of interest to us.\nProposition 1.1 (Decidability of sat and modcheck) If X is one of the model classes defined in Definition 1.7, (M, s) \u2208 X , and \u03d5 is a formula in one of the languages defined in Definition 1.1, then both satX (\u03d5) and modcheckX ((M, s), \u03d5) are decidable. a\nIn order to say anything sensible about the additional resources that an algorithm needs to compute the answer when the input increases in size, we need to define a notion of size for inputs, which in our case are formulas and models. Formulas are by definition finite objects, but models can in principle be infinite (see, for instance, Figure 1.6). The following fact is the\nkey to proving Fact 1.1. For a class of models X , let Fin(X ) \u2286 X be the set of models in X that are finite. Proposition 1.2 (Finite model property) For all classes of models in Definition 1.7 and languages L in Definition 1.1, we have, for all \u03d5 \u2208 L,\nX |= \u03d5 iff Fin(X ) |= \u03d5. a\nFact 1.2 does not say that the models in X and the finite models in X are the same in any meaningful sense; rather, it says that we do not gain valid formulas if we restrict ourselves to finite models. It implies that a formula is satisfiable in a model in X iff it is satisfiable in a finite model in X . It follows that in the languages we have considered so far, \u2018having a finite domain\u2019 is not expressible (for if there were a formula \u03d5 that were true only of models with finite domains, then \u03d5 would be a counterexample to Fact 1.2).\nDefinition 1.14 (Size of Models) For a finite model M = \u3008S,Ag , V At\u3009, the size of M , denoted \u2016M\u2016, is the sum of the number of states (| S |, for which we also write |M |) and the number of pairs in the accessibility relation (|Ra |) for each agent a \u2208 Ag.a\nWe can now strengthen Fact 1.2 as follows.\nProposition 1.3 For all classes of models in Definition 1.7 and languages L in Definition 1.1, we have, for all \u03d5 \u2208 L, \u03d5 is satisfiable in X iff there is a model M \u2208 X such that |D(M) |\u2264 2|\u03d5| and \u03d5 is satisfiable in M . a\nThe idea behind the proof of Proposition 1.3 is that states that \u2018agree\u2019 on all subformulas of \u03d5 can be \u2018identified\u2019. Since there are only |\u03d5 | subformulas of \u03d5, and 2|\u03d5| truth assignments to these formulas, the result follows. Of course, work needs to done to verify this intuition, and to show that an appropriate model can be constructed in the right class X .\nTo reason about the complexity of a computation performed by an algorithm, we distinguish various complexity classes. If a deterministic algorithm can solve a problem in time polynomial in the size of the input, the problem is said to be in P. An example of a problem in P is to decide, given two finite Kripke models M1 and M2, whether there exists a bisimulation between them. Model checking for the basic multi-modal language is also in P; see Proposition 1.4.\nIn a nondeterministic computation, an algorithm is allowed to \u2018guess\u2019 which of a finite number of steps to take next. A nondeterministic algorithm\nfor a decision problem says \u2018yes\u2019 or accepts the input if the algorithm says \u2018yes\u2019 to an appropriate sequence of guesses. So a nondeterministic algorithm can be seen as generating different branches at each computation step, and the answer of the nondeterministic algorithm is \u2018yes\u2019 iff one of the branches results in a \u2018yes\u2019 answer.\nThe class NP is the class of problems that are solvable by a nondeterministic algorithm in polynomial time. Satisfiability of propositional logic is an example of a problem in NP: an algorithm for satisfiability first guesses an appropriate truth assignment to the primitive propositions, and then verifies that the formula is in fact true under this truth assignment.\nA problem that is at least as hard as any problem in NP is called NPhard. An NP-hard problem has the property that any problem in NP can be reduced to it using a polynomial-time reduction. A problem is NP-complete if it is both in NP and NP-hard; satisfiability for propositional logic is well known to be NP-complete. For an arbitrary complexity class C, notions of C-hardness and C-completeness can be similarly defined.\nMany other complexity classes have been defined. We mention a few of them here. An algorithm that runs in space polynomial in the size of the input it is in PSPACE. Clearly if an algorithm needs only polynomial time then it is in polynomial space; that is P \u2286 PSPACE. In fact, we also have NP \u2286 PSPACE. If an algorithm is in NP, we can run it in polynomial space by systematically trying all the possible guesses, erasing the space used after each guess, until we eventually find one that is the \u2018right\u2019 guess. EXPTIME consists of all algorithms that run in time exponential in the size of the input; NEXPTIME is its nondeterministic analogue. We have P \u2286 NP \u2286 PSPACE \u2286 EXPTIME \u2286 NEXPTIME. One of the most important open problems in computer science is the question whether P = NP. The conjecture is that the two classes are different, but this has not yet been proved; it is possible that a polynomial-time algorithm will be found for an NP-hard problem. What is known is that P 6= EXPTIME and NP 6= NEXPTIME.\nThe complement P\u0304 of a problem P is the problem in which all the \u2018yes\u2019 and \u2018no\u2019 answers are reversed. Given a complexity class C, the class co-C is the set of problems for which the complement is in C. For every deterministic class C, we have co-C = C. For nondeterministic classes, a class and its complement are, in general, believed to be incomparable. Consider, for example, the satisfiability problem for propositional logic, which, as we noted above, is NP-complete. Since a formula \u03d5 is valid if and only if \u00ac\u03d5 is not satisfiable, it easily follows that the validity problem for propositional logic is co-NP-complete. The class of NP-complete and co-NP-complete problems are believed to be distinct.\nWe start our summary of complexity results for decision problems in modal logic with model checking.\nProposition 1.4 Model checking formulas in L(At,Op,Ag), with Op = {Ka | a \u2208 Ag}, in finite models is in P. a\nProof We now describe an algorithm that, given a model M = \u3008S,RAg, V At\u3009 and a formula \u03d5 \u2208 L, determines in time polynomial in |\u03d5 | and \u2016M\u2016 whether M, s |= \u03d5. Given \u03d5, order the subformulas \u03d51, . . . \u03d5m of \u03d5 in such a way that, if \u03d5i is a subformula of \u03d5j , then i < j. Note that m \u2264 |\u03d5 |. We claim that\n(*) for every k \u2264 m, we can label each state s in M with either \u03d5j (if \u03d5j if true at s) or \u00ac\u03d5j (otherwise), for every j \u2264 k, in k\u2016M\u2016 steps.\nWe prove (*) by induction on m. If k = 1, \u03d5m must be a primitive proposition, and obviously we need only |M | \u2264 \u2016M\u2016 steps to label all states as required. Now suppose (*) holds for some k < m, and consider the case k + 1. If \u03d5k+1 is a primitive proposition, we reason as before. If \u03d5k+1 is a negation, then it must be \u00ac\u03d5j for some j \u2264 k. Using our assumption, we know that the collection of formulas \u03d51, . . . , \u03d5k can be labeled in M in k\u2016M\u2016 steps. Obviously, if we include \u03d5k+1 = \u00ac\u03d5j in the collection of formulas, we can do the labelling in k more steps: just use the opposite label for \u03d5k+1 as used for \u03d5i. So the collection \u03d51, . . . , \u03d5k+1 can be labelled in M in at (k + 1)\u2016M\u2016 steps, are required. Similarly, if \u03d5k+1 = \u03d5i \u2227 \u03d5j , with i, j \u2264 k, a labelling for the collection \u03d51, . . . , \u03d5k+1 needs only (k + 1)\u2016M\u2016 steps: for the last formula, in each state s of M , the labelling can be completed using the labellings for \u03d5i and \u03d5j . Finally, suppose \u03d5k+1 is of the form Ka\u03d5j with j \u2264 k. In this case, we label a state s with Ka\u03d5j iff each state t with Rast is labelled \u03d5j . Assuming the labels \u03d5j and \u00ac\u03d5j are already in place, this can be done in |Ra(s) |\u2264 \u2016M\u2016 steps. a\nProposition 1.4 should be interpreted with care. While having a polynomial-time procedure seems attractive, we are talking about computation time polynomial in the size of the input. To model an interesting scenario or system often requires \u2018big models\u2019. Even for one agent and n primitive propositions, a model might consist of 2n states. Moreover, the procedure does not check properties of the model either, for instance whether it belongs to a given class X .\nWe now formulate results for satisfiability checking. The results depend on two parameters: the class of models considered (we focus on\nK, T ,S4,KD45 and S5) and the language. Let Ag=1 consist of only one agent, let Ag\u22651 6= \u2205 be an arbitrary set of agents, and let Ag\u22652 be a set of at least two agents. Finally, let Op = {Ka | a \u2208 Ag}. Theorem 1.8 (Satisfiability) The complexity of the satisfiability problem is\n1. NP-complete if X \u2208 {KD45,S5} and L = L(At,Op,Ag=1);\n2. PSPACE-complete if\n(a) X \u2208 {K, T ,S4} and L = L(At,Op,Ag\u22651), or (b) X \u2208 {KD45,S5} and L = L(At,Op,Ag\u22652);\n3. EXPTIME-complete if\n(a) X \u2208 {K, T and L = L(At,Op \u222a {C},Ag\u22651), or (b) X \u2208 {S4,KD45,S5} and L = L(At,Op \u222a {C},Ag\u22652). a\nFrom the results in Theorem 1.8, it follows that the satisfiability problem for logics of knowledge and belief for one agent, S5 and KD45, is exactly as hard as the satisfiability problem for propositional logic. If we do not allow for common knowledge, satisfiability for the general case is PSPACE-complete, and with common knowledge it is EXPTIME-complete. (Of course, common knowledge does not add anything for the case of one agent.)\nFor validity, the consequences of Theorem 1.8 are as follows. We remarked earlier that if satisfiability (in X ) is in some class C, then validity is in co-C. Hence, checking validity for the cases in item 1 is co-NP-complete. Since co-PSPACE = PSPACE, the validity problem for the cases in item 2 is PSPACE-complete, and, finally, since co-EXPTIME = EXPTIME, the validity problem for the cases in item 3 is EXPTIME-complete. What these results on satisfiability and validity mean in practice? Historically, problems that were not in P were viewed as too hard to deal with in practice. However, recently, major advances have been made in finding algorithms that deal well with many NP-complete problems, although no generic approaches have been found for dealing with problems that are co-NP-complete, to say nothing of problems that are PSPACE-complete and beyond. Nevertheless, even for problems in these complexity classes, algorithms with humans in the loop seem to provide useful insights. So, while these complexity results suggest that it is unlikely that we will be able to find tools that do automated satisfiability or validity checking and are guaranteed to always give correct results for the logics that we focus on in this book, this should not be taken to say that we cannot write algorithms for satisfiability, validity,\nor model checking that are useful for the problems of practical interest. Indeed, there is much work focused on just that."}, {"heading": "1.2.5 Axiomatisation", "text": "In the previous section, the formalisation of reasoning was defined around the notion of truth: X |= \u03d5 meant that \u03d5 is true in all models in X . In this section, we discuss a form of reasoning where a conclusion is inferred purely based on its syntactic form. Although there are several ways to do this, in epistemic logic, the most popular way to define deductive inference is by defining a Hilbert-style axiom system. Such systems provide a very simple notion of formal proofs. Some formulas are valid merely because they have a certain syntactic form. These are the axioms of the system. The rules of the system say that one can conclude that some formula is valid due to other formulas being valid. A formal proof or derivation is a list of formulas, where each formula is either an axiom of the system or can be obtained by applying an inference rule of the system to formulas that occur earlier in the list. A proof or derivation of \u03d5 is a derivation whose last formula is \u03d5."}, {"heading": "Basic system", "text": "Our first definition of such a system will make the notion more concrete. We give our definitions for a language where the modal operators are Ka for the agents in some set Ag, although many of the ideas generalise to a setting with arbitrary modal operators.\nDefinition 1.15 (System K) Let L = L(At,Op,Ag), with Op = {Ka | a \u2208 Ag}. The axiom system K consists of the following axioms and rules of inference:\n1 All substitution instances of propositional tautologies. K Ka(\u03d5\u2192 \u03c8)\u2192 (Ka\u03d5\u2192 Ka\u03c8) for all a \u2208 Ag. MP From \u03d5 and \u03d5\u2192 \u03c8 infer \u03c8. Nec From \u03d5 infer Ka\u03d5. a\nHere, formulas in the axioms 1 and K have to be interpreted as axiom schemes: axiom K for instance denotes all formulas {Ka(\u03d5 \u2192 \u03c8) \u2192 (Ka\u03d5\u2192 Ka\u03c8) | \u03d5,\u03c8 \u2208 L}. The rule MP is also called modus ponens; Nec is called necessitation. Note that the notation for axiom K and the axiom system K are the same: the context should make clear which is intended.\nTo see how an axiom system is actually used, we need to define the notion of derivation.\nDefinition 1.16 (Derivation) Given a logical language L, let X be an axiom system with axioms Ax1, . . . , Axn and rules Ru1, . . .Ruk. A derivation of \u03d5 in X is a finite sequence \u03d51, . . . , \u03d5m of formulas such that: (a) \u03d5m = \u03d5, and (b) every \u03d5i in the sequence is either an instance of an axiom or else the result of applying a rule to formulas in the sequence prior to \u03d5i. For the rules MP and Nec, this means the following:"}, {"heading": "MP \u03d5h = \u03d5j \u2192 \u03d5i, for some h, j < i.", "text": "That is, both \u03d5j and \u03d5j \u2192 \u03d5i occur in th sequence before \u03d5i.\nNec \u03d5i = Ka\u03d5j , for some j < i;\nIf there is a derivation for \u03d5 in X we write X ` \u03d5, or `X \u03d5, or, if the system X is clear from the context, we just write ` \u03d5. We then also say that \u03d5 is a theorem of X, or that X proves \u03d5. The sequence \u03d51, . . . , \u03d5m is then also called a proof of \u03d5 in X. a\nExample 1.6 (Derivation in K) We first show that\nK ` Ka(\u03d5 \u2227 \u03c8)\u2192 (Ka\u03d5 \u2227Ka\u03c8). (1.3)\nWe present the proof as a sequence of numbered steps (so that the formula \u03d5i in the derivation is given number i). This allows us to justify each step in the proof by describing which axioms, rules of inference, and previous steps in the proof it follows from. 1. (\u03d5 \u2227 \u03c8)\u2192 \u03d5 1 2. Ka((\u03d5 \u2227 \u03c8)\u2192 \u03d5) Nec, 1 3. Ka((\u03d5 \u2227 \u03c8)\u2192 \u03d5)\u2192 (Ka(\u03d5 \u2227 \u03c8)\u2192 Ka\u03d5) K 4. Ka(\u03d5 \u2227 \u03c8)\u2192 Ka\u03d5 MP, 2, 3 5. (\u03d5 \u2227 \u03c8)\u2192 \u03c8 1 6. Ka((\u03d5 \u2227 \u03c8)\u2192 \u03c8) Nec, 5 7. Ka((\u03d5 \u2227 \u03c8)\u2192 \u03c8)\u2192 (Ka(\u03d5 \u2227 \u03c8)\u2192 Ka\u03c8) K 8. Ka(\u03d5 \u2227 \u03c8)\u2192 Ka\u03c8 MP, 6, 7 9. (Ka(\u03d5 \u2227 \u03c8)\u2192 Ka\u03d5)\u2192\n((Ka(\u03d5 \u2227 \u03c8)\u2192 Ka\u03c8)\u2192 (Ka(\u03d5 \u2227 \u03c8)\u2192 (Ka\u03d5 \u2227Ka\u03c8))) 1 10. (Ka(\u03d5 \u2227 \u03c8)\u2192 Ka\u03c8)\u2192 (Ka(\u03d5 \u2227 \u03c8)\u2192 (Ka\u03d5 \u2227Ka\u03c8)) MP, 4, 9 11. Ka(\u03d5 \u2227 \u03c8)\u2192 (Ka\u03d5 \u2227Ka\u03c8) MP, 8, 10\nLines 1, 5, and 9 are instances of propositional tautologies (this can be checked using a truth table). Note that the tautology on line 9 is of the form (\u03b1 \u2192 \u03b2) \u2192 ((\u03b1 \u2192 \u03b3) \u2192 (\u03b1 \u2192 (\u03b2 \u2227 \u03b3))). A proof like that above may look cumbersome, but it does show what can be done using only the\naxioms and rules of K. It is convenient to give names to properties that are derived, and so build a library of theorems. We have, for instance that K ` KCD, where KCD (\u2018K-over-conjunction-distribution\u2019) is\nKCD Ka(\u03b1 \u2227 \u03b2)\u2192 Ka\u03b1 and Ka(\u03b1 \u2227 \u03b2)\u2192 Ka\u03b2.\nThe proof of this follows steps 1 - 4 and steps 5 - 8, respectively, of the proof above. We can also derive new rules; for example, the following rule: CC (\u2018combine conclusions\u2019) is derivable in K:\nCC from \u03b1\u2192 \u03b2 and \u03b1\u2192 \u03b3 infer \u03b1\u2192 (\u03b2 \u2227 \u03b3).\nThe proof is immediate from the tautology on line 9 above, to which we can, given the assumptions, apply modus ponens twice. We can give a more compact proof of Ka(\u03d5 \u2227 \u03c8)\u2192 (Ka\u03d5 \u2227Ka\u03c8) using this library:\n1. Ka(\u03d5 \u2227 \u03c8)\u2192 Ka\u03d5 KCD 2. Ka(\u03d5 \u2227 \u03c8)\u2192 Ka\u03c8 KCD 3. Ka(\u03d5 \u2227 \u03c8)\u2192 (Ka\u03d5 \u2227Ka\u03c8) CC, 1, 2 a\nFor every class X of models introduced in the previous section, we want to have an inference system X such that derivability in X and validity in X coincide: Definition 1.17 (Soundness and Completeness) Let L be a language, let X be a class of models, and let X be an axiom system. The axiom system is said to be\n1. sound for X and the language L if, for all formulas \u03d5 \u2208 L, X ` \u03d5 implies X |= \u03d5; and\n2. complete for X and the language L if, for all formulas \u03d5 \u2208 L, X |= \u03d5 implies X ` \u03d5.\nWe now provide axioms that characterize some of the subclasses of models that were introduced in Definition 1.7.\nDefinition 1.18 (More axiom systems) Consider the following axioms, which apply for all agents a \u2208 Ag:\nT. Ka\u03d5\u2192 \u03d5 D. Ma> B. \u03d5\u2192 KaMa\u03d5 4. Ka\u03d5\u2192 KaKa\u03d5 5. \u00acKa\u03d5\u2192 Ka\u00acKa\u03d5\nA simple way to denote axiom systems is just to add the axioms that are included together with the name K. Thus, KD is the axiom system that has all the axioms and rules of the system K (1, K, and rules MP and Nec) together with D. Similarly, KD45 extends K by adding the axioms D, 4 and 5. System S4 is the more common way of denoting KT4, while S5 is the more common way of denoting KT45. If it is necessary to make explicit that there are m agents in Ag, we write Km, KDm, and so on. a"}, {"heading": "Using S5 to model knowledge", "text": "The system S5 is an extension of K with the so-called \u2018properties of knowledge\u2019. Likewise, KD45 has been viewed as characterizing the \u2018properties of belief\u2019. The axiom T expresses that knowledge is veridical: whatever one knows, must be true. (It is sometimes called the truth axiom.) The other two axioms specify so-called introspective agents: 4 says that an agent knows what he knows (positive introspection), while 5 says that he knows what he does not know (negative introspection). As a side remark, we mention that axiom 4 is superfluous in S5; it can be deduced from the other axioms.\nAll of these axioms are idealisations, and indeed, logicians do not claim that they hold for all possible interpretations of knowledge. It is only human to claim one day that you know a certain fact, only to find yourself admitting the next day that you were wrong, which undercuts the axiom T. Philosophers use such examples to challenge the notion of knowledge in the first place (see the notes at the end of the chapter for references to the literature on logical properties of knowledge). Positive introspection has also been viewed as problematic. For example, consider a pupil who is asked a question \u03d5 to which he does not know the answer. It may well be that, by asking more questions, the pupil becomes able to answer that \u03d5 is true. Apparently, the pupil knew \u03d5, but was not aware he knew, so did not know that he knew \u03d5.\nThe most debatable among the axioms is that of negative introspection. Quite possibly, a reader of this chapter does not know (yet) what Moore\u2019s paradox is (see Chapter 6), but did she know before picking up this book that she did not know that?\nSuch examples suggest that a reason for ignorance can be lack of awareness. Awareness is the subject of Chapter 3 in this book. Chapter 2 also has an interesting link to negative introspection: this chapter tries to capture what it means to claim \u2018All I know is \u03d5\u2019; in other words, it tries to give an account of \u2018minimal knowledge states\u2019. This is a tricky concept in the presence of axiom 5, since all ignorance immediately leads to knowledge!\nOne might argue that \u2018problematic\u2019 axioms for knowledge should just\nbe omitted, or perhaps weakened, to obtain an appropriate system for knowledge, but what about the basic principles of modal logic: the axiom K and the rule of inference Nec. How acceptable are they for knowledge? As one might expect, we should not take anything for granted. K assumes perfect reasoners, who can infer logical consequences of their knowledge. It implies, for instance, that under some mild assumptions, an agent will know what day of the week July 26, 5018 will be. All that it takes to answer this question is that (1) the agent knows today\u2019s date and what day of the week it is today, (2) she knows the rules for assigning dates, computing leap years, and so on (all of which can be encoded as axioms in an epistemic logic with the appropriate set of primitive propositions). By applying K to this collection of facts, it follows that the agent must know what day of the week it will be on July 26, 5018. Necessitation assumes agents can infer all S5 theorems: agent a, for instance, would know that Kb(Kbq\u2227\u00acKb(p\u2192 \u00acKbq)) is equivalent to (Kbq\u2227Mbp). Since even telling whether a formula is propositionally valid is co-NP-complete, this does not seem so plausible.\nThe idealisations mentioned in this paragraph are often summarised as logical omniscience: our S5 agent would know everything that is logically deducible. Other manifestations of logical omniscience are the equivalence of K(\u03d5 \u2227 \u03c8) and K\u03d5 \u2227 K\u03c8, and the derivable rule in K that allows one to infer K\u03d5 \u2192 K\u03c8 from \u03d5 \u2192 \u03c8 (this says that agents knows all logical consequences of their knowledge).\nThe fact that, in reality, agents are not ideal reasoners, and not logically omniscient, is sometimes a feature exploited by computational systems. Cryptography for instance is useful because artificial or human intruders are, due to their limited capacities, not able to compute the prime factors of a large number in a reasonable amount of time. Knowledge, security, and cryptographic protocols are discussed in Chapter 12\nDespite these problems, the S5 properties are a useful idealisation of knowledge for many applications in distributed computing and economics, and have been shown to give insight into a number of problems. The S5 properties are reasonable for many of the examples that we have already given; here is one more. Suppose that we have two processors, a and b, and that they are involved in computations of three variables, x, y, and z. For simplicity, assume that the variables are Boolean, so that they are either 0 or 1. Processor a can read the value of x and of y, and b can read y and z. To model this, we use, for instance, 010 as the state where x = 0 = z, and y = 1. Given our assumptions regarding what agents can see, we then have x1y1z1 \u223ca x2y2z2 iff x1 = x2 and y1 = y2. This is a simple manifestation of an interpreted system, where the accessibility relation is based on what an agent can see in a state. Such a relation is an equivalence relation. Thus, an\ninterpreted system satisfies all the knowledge axioms. (This is formalised in Theorem 1.9(1) below.)\nWhile T has traditionally been considered an appropriate axiom for knowledge, it has not been considered appropriate for belief. To reason about belief, T is typically replaced by the weaker axiom D: \u00acBa\u22a5, which says that the agent does not believe a contradiction; that is, the agent\u2019s beliefs are consistent. This gives us the axiom system KD45. We can replace D by the following axiom D\u2032 to get an equivalent axiomatisation of belief: D\u2032 : Ka\u03d5\u2192 \u00acKa\u00ac\u03d5. This axioms says that the agent cannot know (or believe) both a fact and its negation. Logical systems that have operators for both knowledge and belief often include the axiom Ka\u03d5 \u2192 Ba\u03d5, saying that knowledge entails belief.\nAxiom systems for group knowledge\nIf we are interested in formalising the knowledge of just one agent a, the language L(At, {Ka},Ag) is arguably too rich. In the logic S51 it can be shown that every formula is equivalent to a depth-one formula, which has no nested occurrences of Ka. This follows from the following equivalences, all of which are valid in S5 as well as being theorems of S5: KK\u03d5\u2194 K\u03d5; K\u00acK\u03d5\u2194 \u00acK\u03d5; K(K\u03d5\u2228\u03c8)\u2194 (K\u03d5\u2228K\u03c8); andK(\u00acK\u03d5\u2228\u03c8)\u2194 \u00acK\u03d5\u2228K\u03c8. From a logical perspective things become more interesting in the multiagent setting.\nWe now consider axiom systems for the notions of group knowledge that were defined earlier. Not surprisingly, we need some additional axioms.\nDefinition 1.19 (Logic of common knowledge) The following axiom and rule capture common knowledge.\nFix. CA\u03d5\u2192 EA(\u03d5 \u2227 CA\u03d5). Ind. From \u03d5\u2192 EA(\u03c8 \u2227 \u03d5) infer \u03d5\u2192 CA\u03c8.\nFor each axiom system X considered earlier, let XC be the result of adding Fix and Ind to X. a\nThe fixed point axiom Fix says that common knowledge can be viewed as the fixed point of an equation: common knowledge of \u03d5 holds if everyone knows both that \u03d5 holds and that \u03d5 is common knowledge. Ind is called the induction rule; it can be used to derive common knowledge \u2018inductively\u2019. If it is the case that \u03d5 is \u2018self-evident\u2019, in the sense that if it is true, then\neveryone knows it, and, in addition, if \u03d5 is true, then everyone knows \u03c8, we can show by induction that if \u03d5 is true, then so is EkA(\u03c8 \u2227 \u03d5) for all k. It follows that CA\u03d5 is true as well. Although common knowledge was defined as an \u2018infinitary\u2019 operator, somewhat surprisingly, these axioms completely characterize it.\nFor distributed knowledge, we consider the following axioms for all A \u2286 Ag:\nW. Ka\u03d5\u2192 DA\u03d5 if a \u2208 A. KD. DA(\u03d5\u2192 \u03c8)\u2192 (DA\u03d5\u2192 DA\u03c8). TD. DA\u03d5\u2192 \u03d5. DD. \u00acDA\u00ac>. BD. \u03d5\u2192 DA\u00acDA\u00ac\u03d5. 4D. DA\u03d5\u2192 DADA\u03d5. 5D. \u00acDA\u03d5\u2192 DA\u00acDA\u03d5.\nThese axioms have to be understood as follows. It may help to think about distributed knowledge in a group A as the knowledge of a wise man, who has been told, by every member of A, what each of them knows. This is captured by axiom W. The other axioms indicate that the wise man has at least the same reasoning abilities as distributed knowledge to the system S5m, we add the axioms W,KD,TD,4D, and 5D to the axiom system. For Km, we add only W and KD."}, {"heading": "Proving Completeness", "text": "We want to prove that the axiom systems that we have defined are sound and complete for the corresponding semantics; that is, that K is sound and complete with respect to K, S5 is sound and complete with respect to S5, and so on. Proving soundness is straightforward: we prove by induction on k that any formula proved using a derivation of length k is valid. Proving completeness is somewhat harder. There are different approaches, but the common one involves to show that if a formula is not derivable, then there is a model in which it is false. There is a special model called the canonical model that simultaneously shows this for all formulas. We now sketch the construction of the canonical model.\nThe states in the canonical model correspond to maximal consistent sets of formulas, a notion that we define next. These sets provide the bridge between the syntactic and semantic approach to validity.\nDefinition 1.20 (Maximal consistent set) A formula \u03d5 is consistent with axiom system X if we cannot derive \u00ac\u03d5 in X. A finite set {\u03d51, . . . , \u03d5n} of formulas is consistent with X if the conjunction\n\u03d51\u2227 . . .\u2227\u03d5n is consistent with X. An infinite set \u0393 of formulas is consistent with X if each finite subset of \u0393 is consistent with X. Given a language L and an axiom system X, a maximal consistent set for X and L is a set \u0393 of formulas in L that is consistent and maximal, in the sense that every strict superset \u0393\u2032 of \u0393 is inconsistent. a\nWe can show that a maximal consistent set \u0393 has the property that, for every formula \u03d5 \u2208 L, exactly one of \u03d5 and \u00ac\u03d5 is in \u0393. If both were in \u0393, then \u0393 would be inconsistent; if neither were in \u0393, then \u0393 would not be maximal. A maximal consistent set is much like a state in a Kripke model, in that every formula is either true or false (but not both) at a state. In fact, as we suggested above, the states in the canonical model can be identified with maximal consistent sets.\nDefinition 1.21 (Canonical model) The canonical model for L and X is the Kripke model M = \u3008S,R, V \u3009 defined as follows:\n\u2022 S is the set of all maximal consistent sets for X and L;\n\u2022 \u0393Ra\u2206 iff \u0393|Ka \u2286 \u2206, where \u0393|Ka = {\u03d5 | Ka\u03d5 \u2208 \u0393};\n\u2022 V (\u0393)(p) = true iff p \u2208 \u0393. a\nThe intuition for the definition of Ra and V is easy to explain. Our goal is to show that the canonical model satisfies what is called the Truth Lemma: a formula \u03d5 is true at a state \u0393 in the canonical model iff \u03d5 \u2208 \u0393. (Here we use the fact that the states in the canonical model are actually sets of formulas\u2014indeed, maximal consistent sets.) We would hope to prove this by induction. The definition of V ensures that the Truth Lemma holds for primitive propositions. The definition of Ra provides a necessary condition for the Truth Lemma to hold for formulas of the form Ka\u03d5. If Ka\u03d5 holds at a state (maximal consistent set) \u0393 in the canonical model, then \u03d5 must hold at all states \u2206 that are accessible from \u0393. This will be the case if \u0393|Ka \u2286 \u2206 for all states \u2206 that are accessible from \u0393 (and the Truth Lemma applies to \u03d5 and \u2206).\nThe Truth Lemma can be shown to hold for the canonical model, as long as we consider a language that does not involve common knowledge or distributed knowledge. (The hard part comes in showing that if \u00acKa\u03d5 holds at a state \u0393, then there is an accessible state \u2206 such that \u00ac\u03d5 \u2208 \u2206. That is, we must show that the Ra relation has \u2018enough\u2019 pairs.) In addition to the Truth Lemma, we can also show that the canonical model for axiom system X is a model in the corresponding class of models; for example, the canonical model for S5 is in S5.\nCompleteness follows relatively easily once these two facts are established. If a formula \u03d5 \u2208 L cannot be derived in X then \u00ac\u03d5 must be consistent with X, and thus can be shown to be an element of a maximal consistent set, say \u0393. \u0393 is a state in the canonical model for X and L. By the Truth Lemma, \u00ac\u03d5 is true at \u0393, so there is a model where \u03d5 is false, proving the completeness of X.\nThis argument fails if the language includes the common knowledge operator. The problem is that with the common knowledge operator in the language, the logic is not compact : there is a set of formulas such that all its finite subsets are satisfiable, yet the whole set is not satisfiable. Consider the set {EnAp | n \u2208 N} \u222a {\u00acCAp}, where A \u2286 Ag is a group with at least two agents. Each finite subset of this set is easily seen to be satisfiable in a model in S5 (and hence in a model in any of the other classes we have considered), but the whole set of formulas is not satisfiable in any Kripke model. Similarly, each finite subset of this set can be shown to be consistent with S5C. Hence, by definition, the whole set is consistent with S5C (and hence all other axiom systems we have considered). This means that this set must be a subset of a maximal consistent set. But, as we have observed, there is no Kripke model where this set of formulas is satisfied.\nThis means that a different proof technique is necessary to prove completeness. Rather than constructing one large canonical model for all formulas, for each formula \u03d5, we construct a finite canonical model tailored to \u03d5. And rather than considering maximal consistent subsets to the set of all formulas in the language, we consider maximal consistent sets of the set of subformulas of \u03d5.\nThe canonical model M\u03d5 = \u3008S\u03d5, R, V \u3009 for \u03d5 and KC is defined as follows:\n\u2022 S\u03d5 is the set of all maximal consistent sets of subformulas of \u03d5 for KC;\n\u2022 \u0393Ra\u2206 iff (\u0393|Ka) \u222a {CA\u03c8 | CA\u03c8 \u2208 \u0393 and a \u2208 A} \u2286 \u2206.\n\u2022 V (\u0393)(p) = true iff p \u2208 \u0393.\nThe intuition for the modification to the definition of Ra is the following: Again, for the Truth Lemma to hold, we must have \u0393|Ka \u2286 \u2206, since if Ka\u03c8 \u2208 \u0393, we want \u03c8 to hold in all states accessible from \u0393. By the fixed point axiom, if CA\u03c8 is true at a state s, so is EACA\u03c8; moreover, if a \u2208 A, then KaCA\u03c8 is also true at s. Thus, if CA\u03c8 is true at \u0393, CA\u03c8 must also be true at all states accessible from \u0393, so we must have {CA\u03c8 | CA\u03c8 \u2208 \u0393 and a \u2208 A} \u2286 \u2206. Again, we can show that the Truth Lemma holds for the canonical model for \u03d5 and KC for subformulas of \u03d5; that is, if \u03c8 is a\nsubformula of \u03d5, then \u03c8 is true at a state \u0393 in the canonical model for \u03d5 and KC iff \u03d5 \u2208 \u0393.\nWe must modify this construction somewhat for axiom systems that contain the axiom 4 and/or 5. For axiom systems that contain 4, we redefineRa so that \u0393Ra\u2206 iff (\u0393 | Ka)\u222a{CA\u03c8 | CA\u03c8 \u2208 \u0393 and a \u2208 A}\u222a{Ka\u03c8 | Ka\u03c8 \u2208 \u0393} \u2286 \u2206. The reason that we want {Ka\u03d5 | Ka\u03d5 \u2208 \u0393} \u2286 \u2206 is that if Ka\u03c8 is true at the state \u0393, so is KaKa\u03c8, so Ka\u03c8 must be true at all worlds accessible from \u0393. An obvious question to ask is why we did not make this requirement in our original canonical model construction. If both Ka\u03c8 and KaKa\u03c8 are subformulas of \u03d5, then the requirement is in fact not necessary. For if Ka\u03c8 \u2208 \u0393, then consistency will guarantee that KaKa\u03c8 is as well, so the requirement that \u0393|Ka \u2286 \u2206 guarantees that Ka\u03c8 \u2208 \u2206. However, if Ka\u03c8 is a subformula of \u03d5 but KaKa\u03c8 is not, this argument fails.\nFor systems that contain 5, there are further subtleties. We illustrate this for the case of S5. In this case, we require that \u0393Ra\u2206 iff {Ka\u03c8 | Ka\u03c8 \u2208 \u0393} = {Ka\u03c8 | Ka\u03c8 \u2208 \u2206} and {CA\u03c8 | CA\u03c8 \u2208 \u0393 and a \u2208 A} = {CA\u03c8 | CA\u03c8 \u2208 \u2206 and a \u2208 A}. Notice that the fact that {Ka\u03c8 | Ka\u03c8 \u2208 \u0393} = {Ka\u03c8 | Ka\u03c8 \u2208 \u2206} implies that \u0393|Ka = \u2206|Ka. We have already argued that having 4 in the system means that we should have {Ka\u03c8 | Ka\u03c8 \u2208 \u0393} \u2286 {Ka\u03c8 | Ka\u03c8 \u2208 \u2206}. For the opposite inclusion, note that if Ka\u03c8 /\u2208 \u0393, then \u00acKa\u03c8 should be true at the state \u0393 in the canonical model, so (by 5) Ka\u00acKa\u03c8 is true at \u0393, and \u00acKa\u03c8 is true at \u2206 if \u0393Ra\u2206. But this means that Ka\u03c8 /\u2208 \u2206 (assuming that the Truth Lemma applies). Similar considerations show that we must have {CA\u03c8 | CA\u03c8 \u2208 \u0393 and a \u2208 A} = {CA\u03c8 | CA\u03c8 \u2208 \u2206 and a \u2208 A}, using the fact that \u00acCA\u03c8 \u2192 EA\u00acCA\u03c8 is provable in S5C.\nGetting a complete axiomatisation for languages involving distributed knowledge requires yet more work; we omit details here.\nWe summarise the main results regarding completeness of epistemic logics in the following theorem. Recall that, for an axiom system X, the axiom system XC is the result of adding the axioms Fix and Ind to X. Similarly, XD is the result of adding the \u2018appropriate\u2019 distributed knowledge axioms to X; specifically, it includes the axiom W, together with every axiom YD for which Y is an axiom of X. So, for example, S5D has the axioms of S5 together with W, KD, TD, 4D, and 5D. Theorem 1.9 If (At,Op,Ag), X is an axiom systems that includes all the axioms and rules of K and some (possibly empty) subset of {T,4,5,D}, and X is the corresponding class of Kripke models, then the following hold:\n1. if Op = {Ka | a \u2208 Ag}, then X is sound and complete for X and L; 2. if Op = {Ka | a \u2208 Ag} \u222a {CA | A \u2286 Ag}, then XC is sound and\ncomplete for X and L;\n3. if Op = {Ka | a \u2208 Ag} \u222a {DA | A \u2286 Ag}, then XD is sound and complete for X and L;\n4. if Op = {Ka | a \u2208 Ag} \u222a {CA | A \u2286 Ag} \u222a {DA | A \u2286 Ag}, then XCD is sound and complete for X and L. a"}, {"heading": "1.3 Overview of the Book", "text": "The book is divided into three parts: informational attitudes, dynamics, and applications. Part I, informational attitudes, considers ways that basic epistemic logic can be extended with other modalities related to knowledge and belief, such as \u201conly knowing\u201d, \u201cawareness\u201d, and probability. There are three chapters in Part I:\nOnly Knowing Chapter 2, on only knowing, is authored by Gerhard Lakemeyer and Hector J. Levesque. What do we mean by \u2018only knowing\u2019? When we say that an agent knows p, we usually mean that the agent knows at least p, but possibly more. In particular, knowing p does not allow us to conclude that q is not known. Contrast this with the situation of a knowledge-based agent, whose knowledge base consists of p, and nothing else. Here we would very much like to conclude that this agent does not know q, but to do so requires us to assume that p is all that the agent knows or, as one can say, the agent only knows p. In this chapter, the logic of only knowing for both single and multiple agents is considered, from both the semantic and proof-theoretic perspective. It is shown that only knowing can be used to capture a certain form of honesty, and that it relates to a form of non-monotonic reasoning.\nAwareness Chapter 3, on logics where knowledge and awareness interact, is authored by Burkhard Schipper. Roughly speaking, an agent is unaware of a formula \u03d5 if \u03d5 is not on his radar screen (as opposed to just having no information about \u03d5, or being uncertain as to the truth of \u03d5). The chapter discusses various approaches to modelling (un)awareness. While the focus is on axiomatisations of structures capable of modelling knowledge and awareness, structures for modelling probabilistic beliefs and awareness, are also discussed, as well as structures for awareness of unawareness.\nEpistemic Probabilistic Logic Chapter 4, authored by Lorenz Demey and Joshua Sack, provides an overview of systems that combine probability theory, which describes quantitative uncertainty, with epistemic logic,\nwhich describes qualitative uncertainty. By combining knowledge and probability, one obtains a very powerful account of information and information flow. Three types of systems are investigated: systems that describe uncertainty of agents at a single moment in time, systems where the uncertainty changes over time, and systems that describe the actions that cause these changes.\nPart II on dynamics of informational attitudes considers aspects of how knowledge and belief change over time. It consists of three chapters:\nKnowledge and Time Chapter 5, on knowledge and time, is authored by Clare Dixon, Cla\u0301udia Nalon, and Ram Ramanujam. It discusses the dynamic aspects of knowledge, which can be characterized by a combination of temporal and epistemic logics. The chapter presents the language and axiomatisation for such a combination, and discusses complexity and expressivity issues. It presents two different proof methods (which apply quite broadly): resolution and tableaux. Levels of knowledge and the relation between knowledge and communication in distributed protocols are also discussed, and an automata-theoretic characterisation of the knowledge of finite-state agents is provided. The chapter concludes with a brief survey on applications.\nDynamic Epistemic Logic Chapter 6, on dynamic epistemic logic, is authored by Lawrence Moss. Dynamic Epistemic Logic (DEL) extends epistemic logic with operators corresponding to epistemic actions. The most basic epistemic action is a public announcement of a given sentence to all agents. In the first part of the chapter, a logic called PAL (public announcement logic), which includes announcement operators, is introduced. Four different axiomatisations for PAL are given and compared. It turns out that PAL without common knowledge is reducible to standard epistemic logic: the announcement operators may be translated away. However, this changes once we include common knowledge operators in the language. The second part of Chapter 6 is devoted to more general epistemic actions, such as private announcements.\nDynamic Logics of Belief Change Chapter 7, on belief change, is authored by Johan van Benthem and Sonja Smets. The chapter gives an overview of current dynamic logics that describe belief update and revision. This involves a combination of ideas from belief revision theory and dynamic epistemic logic. The chapter describes various types of belief change, depending on whether the information received is \u2018hard\u2019 or \u2018soft\u2019. The chapter continues with three topics that naturally complement the setting\nof single steps of belief change: connections with probabilistic approaches to belief change, long-term temporal process structure including links with formal learning theory, and multi-agent scenarios of information flow and belief revision in games and social networks. It ends with a discussion of alternative approaches, further directions, and windows to the broader literature.\nPart III considers applications of epistemic logic in various areas. It consists of five chapters:\nModel Checking Temporal Epistemic Logic Chapter 8, authored by Alessio Lomuscio and Wojciech Penczek, surveys work on model checking systems against temporal-epistemic specifications. The focus is on two approaches to verification: approaches based on ordered binary decision diagrams (OBDDs) and approaches based on translating specifications to propositional logic, and then applying propositional satisfiability checkers (these are called SAT-based approaches). OBDDs provide a compact representation for propositional formulas; they provide powerful techniques for efficient mode checking; SAT-based model checking is the basis for many recent symbolic approach to verification. The chapter also discusses some more advanced techniques for model checking.\nEpistemic Foundations of Game Theory Chapter 9, authored by Giacomo Bonanno, provides an overview of the epistemic approach to game theory. Traditionally, game theory focuses on interaction among intelligent, sophisticated and rational individuals. The epistemic approach attempts to characterize, using epistemic notions, the behavior of rational and intelligent players who know the structure of the game and the preferences of their opponents and who recognize each other\u2019s rationality and reasoning abilities. The focus of the analysis is on the implications of common belief of rationality in strategic-form games and on dynamic games with perfect information.\nBDI Logics Chapter 10, on logics of beliefs, desires, and intentions (BDI), is authored by John-Jules Ch. Meyer, Jan Broersen and Andreas Herzig. Various formalisations of BDI in logic are considered, such as the approach of Cohen and Levesque (recast in dynamic logic), Rao and Georgeff\u2019s influential BDI logic based on the branching-time temporal logic CTL\u2217, the KARO framework, in which action together with knowledge (or belief) is the primary concept on which other agent notions are built, and BDI logics based on STIT (seeing to it that) logics, such as XSTIT.\nKnowledge and Ability Chapter 11, authored by Thomas A\u030agotnes, Valentin Goranko, Wojciech Jamroga and Michael Wooldridge, relates epistemic logics to various logics for strategic abilities. It starts by discussing approaches from philosophy and artificial intelligence to modelling the interaction of agents knowledge and abilities, and then focuses on concurrent game models and the alternating-time temporal logic ATL. The authors discuss how ATL enables reasoning about agents\u2019 coalitional abilities to achieve qualitative objectives in concurrent game models, first assuming complete information and then under incomplete information and uncertainty about the structure of the game model. Finally, extensions of ATL that allow explicit reasoning about the interaction of knowledge and strategic abilities are considered; this leads to the notion of constructive knowledge.\nKnowledge and Security Chapter 12, on knowledge and security, is authored by Riccardo Pucella. A persistent intuition in the field of computer security says that epistemic logic, and more generally epistemic concepts, are relevant to the formalisation of security properties. What grounds this intuition is that much work in the field is based on epistemic concepts. Confidentiality, integrity, authentication, anonymity, non-repudiation, all can be expressed as epistemic properties. This survey illustrates the use of epistemic concepts and epistemic logic to formalise a specific security property, confidentiality. Confidentiality is a prime example of the use of knowledge to make a security property precise. It is explored in two large domains of application: cryptographic protocol analysis and multi-level security systems."}, {"heading": "1.4 Notes", "text": "The seminal work of the philosopher Jaakko Hintikka (1962) is typically taken as the starting point of modern epistemic logic. Two texts on epistemic logic by computer scientists were published in 1995: one by Fagin, Halpern, Moses, and Vardi (1995) and the other by Meyer and van der Hoek (1995). Another influential text on epistemic logic, which focuses more on philosophical aspects, is by Rescher (2005). Formal treatments of the notion of knowledge in artificial intelligence, in particular for reasoning about action, go back to the work of Moore (1977). In the mid-1980s, the conference on Theoretical Aspects of Reasoning About Knowledge (TARK), later renamed to \u201cTheoretical Aspects of Rationality and Knowledge, was started (1986); in the mid-1990s, the Conference on Logic and Foundations of Game and Decision Theory (LOFT) (1996) began. These two conferences\ncontinue to this day, bringing together computer scientists, economists, and philosophers.\nOur chapter is far from the first introduction to epistemic logic. The textbooks by Fagin et al. (1995) and by Meyer and van der Hoek (1995) each come with an introductory chapter; more recent surveys and introductions can be found in the book by van Ditmarsch, van der Hoek, and Kooi (2007, Chapter 2), in a paper on epistemic logic and epistemology by Holliday (2014), in the chapter by Bezhanishvili and van der Hoek (2014), which provides a survey of semantics for epistemic notions, and in online resources (Hendricks and Symons 2014, Wikipedia).\nHalpern (1987) provides an introduction to applications of knowledge in distributed computing; the early chapters of the book by Perea (2012) give an introduction to the use of epistemic logic in game theory. As we already said, more discussion of the examples in Section 1.1 can be found in the relevant chapters. Public announcements are considered in Chapter 6; protocols are studied in Chapter 12 and, to some extent, in Chapter 5; strategic ability is the main topic of Chapter 11; epistemic foundations of game theory are considered in Chapter 9; distributed computing is touched on in Chapter 5, while examples of model checking distributed protocols are given in Chapter 8.\nThe use of Kripke models puts our approach to epistemic logic firmly in the tradition of modal logic, of which Kripke is one of the founders (see Kripke (1963)). Modal logic has become the framework to reason not only about notions as knowledge and belief, but also about agent attitudes such as desires and intentions (Rao and Georgeff, 1991), and about notions like time (Emerson, 1990), action (Harel, 1984), programs (Fischer and Ladner, 1979), reasoning about obligation and permission (von Wright, 1951), and combinations of them. Modern references to modal logic include the textbook by Blackburn, de Rijke, and Venema (2001) and the handbook edited by Blackburn, van Benthem, and Wolter (2006).\nUsing modal logic to formalise knowledge and belief suggests that one has an idealised version of these notions in mind. The discussion in Section 1.2.5 is only the tip of the iceberg. Further discussion of logical omniscience can be found in (Stalnaker, 1991; Sim, 1997) and in (Fagin et al., 1995, Chapter 9). There is a wealth of discussion in the philosophy and psychology literature of the axioms and their reasonableness (Koriat, 1993; Larsson, 2004; Zangwill, 2013). Perhaps the most controversial axiom of knowledge is 5; which was dismissed in the famous claim by Donald Rumsfeld that there are \u2018unknown unknowns\u2019 (see http://en.wikipedia.org/ wiki/There_are_known_knowns). Some approaches for dealing with lack of knowledge using awareness avoid this axiom (and, indeed, all the others); see Chapter 3.\nBroadly speaking, philosophers usually distinguish between the truth of a claim, our belief in it, and the justification for the claim. These are often considered the three key elements of knowledge. Indeed, there are papers that define knowledge as justified true belief. There has been much debate of this definition, going back to Gettier\u2019s (1963) Is justified true belief knowledge?. Halpern, Samet, and Segev (2009) provide a recent perspective on these issues.\nThe notion of common knowledge is often traced back to the philosopher David Lewis\u2019s (1969) independently developed by the sociologist Morris Friedell (1969). Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al. (1995, Chapter 6). Section 9.5 compares the notions of common knowledge with that of common belief.\nDistributed knowledge was discussed first, in an informal way, by Hayek (1945), and then, in a more formal way, by Hilpinen (1977). It was rediscovered and popularized by Halpern and Moses (1990), who originally called it implicit knowledge.\nThe notion of bisimulation is a central notion in modal logic, providing an answer to the question when two models are \u2018the same\u2019 and is discussed in standard modal logic texts (Blackburn et al., 2001, 2006). Bisimulation arises quite often in this book, including in Chapters 5, 6, and 7.\nWe mentioned below Theorem 1.8, when discussing complexity of validity, that some recent advances make NP-complete problems seem more tractable: for this we refer to work by Gomes, Kautz, Sabharwal, and Selman (2008).\nWe end this brief discussion of the background literature by providing the pointers to the technical results mentioned in our chapter. Theorem 1.1 gives some standard valid formulas for several classes of models (see Fagin et al. (1995, Chapter 2.4) for a textbook treatment). Theorem 1.2 is a folk theorem in modal logic: for a proof and discussion, see Blackburn et al. (2006, Chapter 2.3). Proposition 1.3 is proved by Fagin et al. (1995) as Theorem 3.2.2 (for the case X = K) and Theorem 3.2.4 (for X = T ,S4,KD45, and S5). Proposition 1.4 is Proposition 3.2.1 by Fagin et al. (1995). Theorem 1.8 is proved by Halpern and Moses (1992).\nAlthough the first proofs of completeness for multi-agent versions of axiom systems of the form Xm and XCm are by Halpern and Moses (1992), the ideas go back much earlier. In particular, the basic canonical model construction goes back to Makinson (1966) (see Blackburn et al. (2001, Chapter 4) for a discussion), while the idea for completeness of axiom sys-\ntems of the form XC is already in the proof of Kozen and Parikh (1981) for proving completeness of dynamic logic. Completeness for axiom systems of the form XD was proved by Fagin, Halpern, and Vardi (1992) and by van der Hoek and Meyer (1992). A novel proof is provided by Wang (2013, Chapter 3). Theorem 1.6 is part of logical folklore. A proof of Theorem 1.7 was given by French, van der Hoek, Iliev, and Kooi (2013).\nAcknowledgements The authors are indebted to Cla\u0301udia Nalon for a careful reading. Hans van Ditmarsch is also affiliated to IMSc, Chennai, as associated researcher, and he acknowledges support from European Research Council grant EPS 313360. Joseph Y. Halpern was supported in part by NSF grants IIS-0911036 and CCF-1214844, by AFOSR grant FA955009-1-0266, by ARO grants W911NF-09-1-0281 and W911NF-14-1-0017, and by the Multidisciplinary University Research Initiative (MURI) program administered by the AFOSR under grant FA9550-12-1-0040.\n49"}], "references": [{"title": "Agreeing to disagree", "author": ["R.J. Aumann"], "venue": "Annals of Statistics 4 (6), 1236\u20131239.", "citeRegEx": "Aumann,? 1976", "shortCiteRegEx": "Aumann", "year": 1976}, {"title": "Structures for epistemic logic", "author": ["N. Bezhanishvili", "W. van der Hoek"], "venue": "A. Baltag and S. Smets (Eds.), Logical and Informational Dynamics, a volume in honour of Johan van Benthem, pp. 339\u2013381. Springer.", "citeRegEx": "Bezhanishvili and Hoek,? 2014", "shortCiteRegEx": "Bezhanishvili and Hoek", "year": 2014}, {"title": "Modal Logic", "author": ["P. Blackburn", "M. de Rijke", "Y. Venema"], "venue": "Cambridge University Press: Cambridge, England.", "citeRegEx": "Blackburn et al\\.,? 2001", "shortCiteRegEx": "Blackburn et al\\.", "year": 2001}, {"title": "Dynamic Epistemic Logic", "author": ["H. van Ditmarsch", "W. van der Hoek", "B. Kooi"], "venue": null, "citeRegEx": "Ditmarsch et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ditmarsch et al\\.", "year": 2007}, {"title": "Temporal and modal logic", "author": ["E.A. Emerson"], "venue": "J. van Leeuwen (Ed.), Handbook of Theoretical Computer Science Volume B: Formal Models and Semantics, pp. 996\u20131072. Elsevier Science Publishers B.V.: Amsterdam, The Netherlands.", "citeRegEx": "Emerson,? 1990", "shortCiteRegEx": "Emerson", "year": 1990}, {"title": "Reasoning About Knowledge", "author": ["R. Fagin", "J.Y. Halpern", "Y. Moses", "M.Y. Vardi"], "venue": "The MIT Press: Cambridge, MA.", "citeRegEx": "Fagin et al\\.,? 1995", "shortCiteRegEx": "Fagin et al\\.", "year": 1995}, {"title": "What can machines know? on the properties of knowledge in distributed systems", "author": ["R. Fagin", "J.Y. Halpern", "M.Y. Vardi"], "venue": "Journal of the ACM 39 (2), 328\u2013376.", "citeRegEx": "Fagin et al\\.,? 1992", "shortCiteRegEx": "Fagin et al\\.", "year": 1992}, {"title": "Propositional dynamic logic of regular programs", "author": ["M. Fischer", "R. Ladner"], "venue": "Journal of Computer and System Sciences 18, 194\u2013211.", "citeRegEx": "Fischer and Ladner,? 1979", "shortCiteRegEx": "Fischer and Ladner", "year": 1979}, {"title": "On the succinctness of some modal logics", "author": ["T. French", "W. van der Hoek", "P. Iliev", "B. Kooi"], "venue": "Artificial Intelligence 197, 56\u201385.", "citeRegEx": "French et al\\.,? 2013", "shortCiteRegEx": "French et al\\.", "year": 2013}, {"title": "On the structure of shared awareness", "author": ["M. Friedell"], "venue": "Behavioral Science 14 (1), 28\u201339. A working paper with the same title was published in 1967 by the Center for Research on Social Organization, University of Michigan.", "citeRegEx": "Friedell,? 1969", "shortCiteRegEx": "Friedell", "year": 1969}, {"title": "Is justified true belief knowledge? Analysis", "author": ["E. Gettier"], "venue": null, "citeRegEx": "Gettier,? \\Q1963\\E", "shortCiteRegEx": "Gettier", "year": 1963}, {"title": "Satisfiability solvers", "author": ["C.P. Gomes", "H. Kautz", "A. Sabharwal", "B. Selman"], "venue": "Handbook of Knowledge Representation, pp. 89\u2013133.", "citeRegEx": "Gomes et al\\.,? 2008", "shortCiteRegEx": "Gomes et al\\.", "year": 2008}, {"title": "Using reasoning about knowledge to analyze distributed systems", "author": ["J.Y. Halpern"], "venue": "J. F. Traub, B. J. Grosz, B. W. Lampson, and N. J. Nilsson (Eds.), Annual Review of Computer Science, Volume 2, pp. 37\u201368. Palo Alto, Calif.: Annual Reviews Inc.", "citeRegEx": "Halpern,? 1987", "shortCiteRegEx": "Halpern", "year": 1987}, {"title": "Knowledge and common knowledge in a distributed environment", "author": ["J.Y. Halpern", "Y. Moses"], "venue": "Journal of the ACM 37 (3), 549\u2013587. A preliminary version appeared in Proc. 3rd ACM Symposium on Principles of Distributed Computing, 1984.", "citeRegEx": "Halpern and Moses,? 1990", "shortCiteRegEx": "Halpern and Moses", "year": 1990}, {"title": "A guide to completeness and complexity for modal logics of knowledge and belief", "author": ["J.Y. Halpern", "Y. Moses"], "venue": "Artificial Intelligence 54, 319\u2013379.", "citeRegEx": "Halpern and Moses,? 1992", "shortCiteRegEx": "Halpern and Moses", "year": 1992}, {"title": "Defining knowledge in terms of belief: The modal logic perspective", "author": ["J.Y. Halpern", "D. Samet", "E. Segev"], "venue": "The Review of Symbolic Logic 2 (3), 469\u2013487.", "citeRegEx": "Halpern et al\\.,? 2009", "shortCiteRegEx": "Halpern et al\\.", "year": 2009}, {"title": "Dynamic logic", "author": ["D. Harel"], "venue": "D. Gabbay and F. Guenther (Eds.), Handbook of Philosophical Logic Volume II \u2014 Extensions of Classical Logic, pp. 497\u2013604. D. Reidel Publishing Company: Dordrecht, The Netherlands. (Synthese library Volume 164).", "citeRegEx": "Harel,? 1984", "shortCiteRegEx": "Harel", "year": 1984}, {"title": "The use of knowledge in society", "author": ["F. Hayek"], "venue": "American Economic Review 35, 519\u2013530.", "citeRegEx": "Hayek,? 1945", "shortCiteRegEx": "Hayek", "year": 1945}, {"title": "Remarks on personal and impersonal knowledge", "author": ["R. Hilpinen"], "venue": "Canadian Journal of Philosophy 7, 1\u20139.", "citeRegEx": "Hilpinen,? 1977", "shortCiteRegEx": "Hilpinen", "year": 1977}, {"title": "Knowledge and Belief", "author": ["J. Hintikka"], "venue": "Cornell University Press: Ithaca, NY. Reprint: \u2018Knowledge and Belief\u2019, in: Texts in Philosophy, Vol. 1, Kings College Publications, 2005.", "citeRegEx": "Hintikka,? 1962", "shortCiteRegEx": "Hintikka", "year": 1962}, {"title": "Making some issues of implicit knowledge explicit", "author": ["W. van der Hoek", "J.-J"], "venue": "International Journal of Foundations of Computer Science", "citeRegEx": "Hoek and J..J.,? \\Q1992\\E", "shortCiteRegEx": "Hoek and J..J.", "year": 1992}, {"title": "Epistemic logic and epistemology", "author": ["W. Holliday"], "venue": "to appear, preliminary version at http://philosophy.berkeley.edu/file/814/el_episteme.pdf.", "citeRegEx": "Holliday,? 2014", "shortCiteRegEx": "Holliday", "year": 2014}, {"title": "How do we know that we know? the accessibility model of the feeling of knowing", "author": ["A. Koriat"], "venue": "Psychological review 100, 609\u2013639.", "citeRegEx": "Koriat,? 1993", "shortCiteRegEx": "Koriat", "year": 1993}, {"title": "An elementary proof of the completeness of PDL", "author": ["D. Kozen", "R. Parikh"], "venue": "Theoretical Computer Science 14 (1), 113\u2013118.", "citeRegEx": "Kozen and Parikh,? 1981", "shortCiteRegEx": "Kozen and Parikh", "year": 1981}, {"title": "Semantical analysis of modal logic", "author": ["S. Kripke"], "venue": "Zeitschrift f\u00fcr Mathematische Logik und Grundlagen der Mathematik 9, 67\u201396.", "citeRegEx": "Kripke,? 1963", "shortCiteRegEx": "Kripke", "year": 1963}, {"title": "The magic of negative introspection", "author": ["S. Larsson"], "venue": null, "citeRegEx": "Larsson,? \\Q2004\\E", "shortCiteRegEx": "Larsson", "year": 2004}, {"title": "Convention \u2014 A Philosophical Study", "author": ["D. Lewis"], "venue": "Harvard University Press: Cambridge, MA.", "citeRegEx": "Lewis,? 1969", "shortCiteRegEx": "Lewis", "year": 1969}, {"title": "On some completeness theorems in modal logic", "author": ["D. Makinson"], "venue": "Zeitschrift f\u00fcr Mathematische Logik und Grundlagen der Mathematik 12, 379\u2013384.", "citeRegEx": "Makinson,? 1966", "shortCiteRegEx": "Makinson", "year": 1966}, {"title": "Formalization of two puzzles involving knowledge", "author": ["J. McCarthy"], "venue": "V. Lifschitz (Ed.), Formalizing Common Sense : Papers by John McCarthy, Ablex Series in Artificial Intelligence. Norwood, N.J.: Ablex Publishing Corporation. original manuscript dated 1978\u20131981.", "citeRegEx": "McCarthy,? 1990", "shortCiteRegEx": "McCarthy", "year": 1990}, {"title": "Epistemic Logic for AI and Computer Science", "author": ["Meyer", "J.-J.C.", "W. van der Hoek"], "venue": "Cambridge University Press: Cambridge, England.", "citeRegEx": "Meyer et al\\.,? 1995", "shortCiteRegEx": "Meyer et al\\.", "year": 1995}, {"title": "Reasoning about knowledge and action", "author": ["R.C. Moore"], "venue": "Proceedings of the Fifth International Joint Conference on Artificial Intelligence (IJCAI-77), Cambridge, MA.", "citeRegEx": "Moore,? 1977", "shortCiteRegEx": "Moore", "year": 1977}, {"title": "Epistemic Game Theory", "author": ["A. Perea"], "venue": "Cambridge, U.K.: Cambridge University Press.", "citeRegEx": "Perea,? 2012", "shortCiteRegEx": "Perea", "year": 2012}, {"title": "Modeling rational agents within a BDI-architecture", "author": ["A.S. Rao", "M.P. Georgeff", "April"], "venue": "Proceedings of Knowledge Representation and Reasoning (KR&R-91),", "citeRegEx": "Rao et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Rao et al\\.", "year": 1991}, {"title": "Epistemic Logic: A Survey of the Logic of Knowledge", "author": ["N. Rescher"], "venue": "University of Pittsburgh Press.", "citeRegEx": "Rescher,? 2005", "shortCiteRegEx": "Rescher", "year": 2005}, {"title": "Epistemic logic and logical omniscience: A survey", "author": ["K.M. Sim"], "venue": "International Journal of Intelligent Systems 12 (1), 57\u201381.", "citeRegEx": "Sim,? 1997", "shortCiteRegEx": "Sim", "year": 1997}, {"title": "The problem of logical omniscience, I", "author": ["R. Stalnaker"], "venue": "Synthese 89 (3), 425\u2013440.", "citeRegEx": "Stalnaker,? 1991", "shortCiteRegEx": "Stalnaker", "year": 1991}, {"title": "Logical Dynamics of Group Knowledge and Subset Spaces", "author": ["Y. Wang"], "venue": "Ph. D. thesis, University of Bergen.", "citeRegEx": "Wang,? 2013", "shortCiteRegEx": "Wang", "year": 2013}, {"title": "Does knowledge depend on truth", "author": ["N. Zangwill"], "venue": "Acta Analytica", "citeRegEx": "Zangwill,? \\Q2013\\E", "shortCiteRegEx": "Zangwill", "year": 2013}], "referenceMentions": [{"referenceID": 18, "context": "The seminal work of the philosopher Jaakko Hintikka (1962) is typically taken as the starting point of modern epistemic logic.", "startOffset": 43, "endOffset": 59}, {"referenceID": 12, "context": "Two texts on epistemic logic by computer scientists were published in 1995: one by Fagin, Halpern, Moses, and Vardi (1995) and the other by Meyer and van der Hoek (1995).", "startOffset": 90, "endOffset": 123}, {"referenceID": 12, "context": "Two texts on epistemic logic by computer scientists were published in 1995: one by Fagin, Halpern, Moses, and Vardi (1995) and the other by Meyer and van der Hoek (1995). Another influential text on epistemic logic, which focuses more on philosophical aspects, is by Rescher (2005).", "startOffset": 90, "endOffset": 170}, {"referenceID": 12, "context": "Two texts on epistemic logic by computer scientists were published in 1995: one by Fagin, Halpern, Moses, and Vardi (1995) and the other by Meyer and van der Hoek (1995). Another influential text on epistemic logic, which focuses more on philosophical aspects, is by Rescher (2005). Formal treatments of the notion of knowledge in artificial intelligence, in particular for reasoning about action, go back to the work of Moore (1977).", "startOffset": 90, "endOffset": 282}, {"referenceID": 12, "context": "Two texts on epistemic logic by computer scientists were published in 1995: one by Fagin, Halpern, Moses, and Vardi (1995) and the other by Meyer and van der Hoek (1995). Another influential text on epistemic logic, which focuses more on philosophical aspects, is by Rescher (2005). Formal treatments of the notion of knowledge in artificial intelligence, in particular for reasoning about action, go back to the work of Moore (1977). In the mid-1980s, the conference on Theoretical Aspects of Reasoning About Knowledge (TARK), later renamed to \u201cTheoretical Aspects of Rationality and Knowledge, was started (1986); in the mid-1990s, the Conference on Logic and Foundations of Game and Decision Theory (LOFT) (1996) began.", "startOffset": 90, "endOffset": 434}, {"referenceID": 12, "context": "Two texts on epistemic logic by computer scientists were published in 1995: one by Fagin, Halpern, Moses, and Vardi (1995) and the other by Meyer and van der Hoek (1995). Another influential text on epistemic logic, which focuses more on philosophical aspects, is by Rescher (2005). Formal treatments of the notion of knowledge in artificial intelligence, in particular for reasoning about action, go back to the work of Moore (1977). In the mid-1980s, the conference on Theoretical Aspects of Reasoning About Knowledge (TARK), later renamed to \u201cTheoretical Aspects of Rationality and Knowledge, was started (1986); in the mid-1990s, the Conference on Logic and Foundations of Game and Decision Theory (LOFT) (1996) began.", "startOffset": 90, "endOffset": 615}, {"referenceID": 12, "context": "Two texts on epistemic logic by computer scientists were published in 1995: one by Fagin, Halpern, Moses, and Vardi (1995) and the other by Meyer and van der Hoek (1995). Another influential text on epistemic logic, which focuses more on philosophical aspects, is by Rescher (2005). Formal treatments of the notion of knowledge in artificial intelligence, in particular for reasoning about action, go back to the work of Moore (1977). In the mid-1980s, the conference on Theoretical Aspects of Reasoning About Knowledge (TARK), later renamed to \u201cTheoretical Aspects of Rationality and Knowledge, was started (1986); in the mid-1990s, the Conference on Logic and Foundations of Game and Decision Theory (LOFT) (1996) began.", "startOffset": 90, "endOffset": 716}, {"referenceID": 4, "context": "Modal logic has become the framework to reason not only about notions as knowledge and belief, but also about agent attitudes such as desires and intentions (Rao and Georgeff, 1991), and about notions like time (Emerson, 1990), action (Harel, 1984), programs (Fischer and Ladner, 1979), reasoning about obligation and permission (von Wright, 1951), and combinations of them.", "startOffset": 211, "endOffset": 226}, {"referenceID": 16, "context": "Modal logic has become the framework to reason not only about notions as knowledge and belief, but also about agent attitudes such as desires and intentions (Rao and Georgeff, 1991), and about notions like time (Emerson, 1990), action (Harel, 1984), programs (Fischer and Ladner, 1979), reasoning about obligation and permission (von Wright, 1951), and combinations of them.", "startOffset": 235, "endOffset": 248}, {"referenceID": 7, "context": "Modal logic has become the framework to reason not only about notions as knowledge and belief, but also about agent attitudes such as desires and intentions (Rao and Georgeff, 1991), and about notions like time (Emerson, 1990), action (Harel, 1984), programs (Fischer and Ladner, 1979), reasoning about obligation and permission (von Wright, 1951), and combinations of them.", "startOffset": 259, "endOffset": 285}, {"referenceID": 35, "context": "Further discussion of logical omniscience can be found in (Stalnaker, 1991; Sim, 1997) and in (Fagin et al.", "startOffset": 58, "endOffset": 86}, {"referenceID": 34, "context": "Further discussion of logical omniscience can be found in (Stalnaker, 1991; Sim, 1997) and in (Fagin et al.", "startOffset": 58, "endOffset": 86}, {"referenceID": 22, "context": "There is a wealth of discussion in the philosophy and psychology literature of the axioms and their reasonableness (Koriat, 1993; Larsson, 2004; Zangwill, 2013).", "startOffset": 115, "endOffset": 160}, {"referenceID": 25, "context": "There is a wealth of discussion in the philosophy and psychology literature of the axioms and their reasonableness (Koriat, 1993; Larsson, 2004; Zangwill, 2013).", "startOffset": 115, "endOffset": 160}, {"referenceID": 37, "context": "There is a wealth of discussion in the philosophy and psychology literature of the axioms and their reasonableness (Koriat, 1993; Larsson, 2004; Zangwill, 2013).", "startOffset": 115, "endOffset": 160}, {"referenceID": 4, "context": "The textbooks by Fagin et al. (1995) and by Meyer and van der Hoek (1995) each come with an introductory chapter; more recent surveys and introductions can be found in the book by van Ditmarsch, van der Hoek, and Kooi (2007, Chapter 2), in a paper on epistemic logic and epistemology by Holliday (2014), in the chapter by Bezhanishvili and van der Hoek (2014), which provides a survey of semantics for epistemic notions, and in online resources (Hendricks and Symons 2014, Wikipedia).", "startOffset": 17, "endOffset": 37}, {"referenceID": 4, "context": "The textbooks by Fagin et al. (1995) and by Meyer and van der Hoek (1995) each come with an introductory chapter; more recent surveys and introductions can be found in the book by van Ditmarsch, van der Hoek, and Kooi (2007, Chapter 2), in a paper on epistemic logic and epistemology by Holliday (2014), in the chapter by Bezhanishvili and van der Hoek (2014), which provides a survey of semantics for epistemic notions, and in online resources (Hendricks and Symons 2014, Wikipedia).", "startOffset": 17, "endOffset": 74}, {"referenceID": 4, "context": "The textbooks by Fagin et al. (1995) and by Meyer and van der Hoek (1995) each come with an introductory chapter; more recent surveys and introductions can be found in the book by van Ditmarsch, van der Hoek, and Kooi (2007, Chapter 2), in a paper on epistemic logic and epistemology by Holliday (2014), in the chapter by Bezhanishvili and van der Hoek (2014), which provides a survey of semantics for epistemic notions, and in online resources (Hendricks and Symons 2014, Wikipedia).", "startOffset": 17, "endOffset": 303}, {"referenceID": 4, "context": "The textbooks by Fagin et al. (1995) and by Meyer and van der Hoek (1995) each come with an introductory chapter; more recent surveys and introductions can be found in the book by van Ditmarsch, van der Hoek, and Kooi (2007, Chapter 2), in a paper on epistemic logic and epistemology by Holliday (2014), in the chapter by Bezhanishvili and van der Hoek (2014), which provides a survey of semantics for epistemic notions, and in online resources (Hendricks and Symons 2014, Wikipedia).", "startOffset": 17, "endOffset": 360}, {"referenceID": 4, "context": "The textbooks by Fagin et al. (1995) and by Meyer and van der Hoek (1995) each come with an introductory chapter; more recent surveys and introductions can be found in the book by van Ditmarsch, van der Hoek, and Kooi (2007, Chapter 2), in a paper on epistemic logic and epistemology by Holliday (2014), in the chapter by Bezhanishvili and van der Hoek (2014), which provides a survey of semantics for epistemic notions, and in online resources (Hendricks and Symons 2014, Wikipedia). Halpern (1987) provides an introduction to applications of knowledge in distributed computing; the early chapters of the book by Perea (2012) give an introduction to the use of epistemic logic in game theory.", "startOffset": 17, "endOffset": 500}, {"referenceID": 4, "context": "The textbooks by Fagin et al. (1995) and by Meyer and van der Hoek (1995) each come with an introductory chapter; more recent surveys and introductions can be found in the book by van Ditmarsch, van der Hoek, and Kooi (2007, Chapter 2), in a paper on epistemic logic and epistemology by Holliday (2014), in the chapter by Bezhanishvili and van der Hoek (2014), which provides a survey of semantics for epistemic notions, and in online resources (Hendricks and Symons 2014, Wikipedia). Halpern (1987) provides an introduction to applications of knowledge in distributed computing; the early chapters of the book by Perea (2012) give an introduction to the use of epistemic logic in game theory.", "startOffset": 17, "endOffset": 627}, {"referenceID": 4, "context": "The textbooks by Fagin et al. (1995) and by Meyer and van der Hoek (1995) each come with an introductory chapter; more recent surveys and introductions can be found in the book by van Ditmarsch, van der Hoek, and Kooi (2007, Chapter 2), in a paper on epistemic logic and epistemology by Holliday (2014), in the chapter by Bezhanishvili and van der Hoek (2014), which provides a survey of semantics for epistemic notions, and in online resources (Hendricks and Symons 2014, Wikipedia). Halpern (1987) provides an introduction to applications of knowledge in distributed computing; the early chapters of the book by Perea (2012) give an introduction to the use of epistemic logic in game theory. As we already said, more discussion of the examples in Section 1.1 can be found in the relevant chapters. Public announcements are considered in Chapter 6; protocols are studied in Chapter 12 and, to some extent, in Chapter 5; strategic ability is the main topic of Chapter 11; epistemic foundations of game theory are considered in Chapter 9; distributed computing is touched on in Chapter 5, while examples of model checking distributed protocols are given in Chapter 8. The use of Kripke models puts our approach to epistemic logic firmly in the tradition of modal logic, of which Kripke is one of the founders (see Kripke (1963)).", "startOffset": 17, "endOffset": 1327}, {"referenceID": 4, "context": "Modal logic has become the framework to reason not only about notions as knowledge and belief, but also about agent attitudes such as desires and intentions (Rao and Georgeff, 1991), and about notions like time (Emerson, 1990), action (Harel, 1984), programs (Fischer and Ladner, 1979), reasoning about obligation and permission (von Wright, 1951), and combinations of them. Modern references to modal logic include the textbook by Blackburn, de Rijke, and Venema (2001) and the handbook edited by Blackburn, van Benthem, and Wolter (2006).", "startOffset": 212, "endOffset": 471}, {"referenceID": 4, "context": "Modal logic has become the framework to reason not only about notions as knowledge and belief, but also about agent attitudes such as desires and intentions (Rao and Georgeff, 1991), and about notions like time (Emerson, 1990), action (Harel, 1984), programs (Fischer and Ladner, 1979), reasoning about obligation and permission (von Wright, 1951), and combinations of them. Modern references to modal logic include the textbook by Blackburn, de Rijke, and Venema (2001) and the handbook edited by Blackburn, van Benthem, and Wolter (2006). Using modal logic to formalise knowledge and belief suggests that one has an idealised version of these notions in mind.", "startOffset": 212, "endOffset": 540}, {"referenceID": 5, "context": "There has been much debate of this definition, going back to Gettier\u2019s (1963) Is justified true belief knowledge?.", "startOffset": 61, "endOffset": 78}, {"referenceID": 5, "context": "There has been much debate of this definition, going back to Gettier\u2019s (1963) Is justified true belief knowledge?. Halpern, Samet, and Segev (2009) provide a recent perspective on these issues.", "startOffset": 61, "endOffset": 148}, {"referenceID": 5, "context": "There has been much debate of this definition, going back to Gettier\u2019s (1963) Is justified true belief knowledge?. Halpern, Samet, and Segev (2009) provide a recent perspective on these issues. The notion of common knowledge is often traced back to the philosopher David Lewis\u2019s (1969) independently developed by the sociologist Morris Friedell (1969).", "startOffset": 61, "endOffset": 286}, {"referenceID": 5, "context": "The notion of common knowledge is often traced back to the philosopher David Lewis\u2019s (1969) independently developed by the sociologist Morris Friedell (1969). Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence.", "startOffset": 142, "endOffset": 158}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence.", "startOffset": 62, "endOffset": 76}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence.", "startOffset": 62, "endOffset": 100}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al.", "startOffset": 62, "endOffset": 312}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al. (1995, Chapter 6). Section 9.5 compares the notions of common knowledge with that of common belief. Distributed knowledge was discussed first, in an informal way, by Hayek (1945), and then, in a more formal way, by Hilpinen (1977).", "startOffset": 62, "endOffset": 511}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al. (1995, Chapter 6). Section 9.5 compares the notions of common knowledge with that of common belief. Distributed knowledge was discussed first, in an informal way, by Hayek (1945), and then, in a more formal way, by Hilpinen (1977). It was rediscovered and popularized by Halpern and Moses (1990), who originally called it implicit knowledge.", "startOffset": 62, "endOffset": 563}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al. (1995, Chapter 6). Section 9.5 compares the notions of common knowledge with that of common belief. Distributed knowledge was discussed first, in an informal way, by Hayek (1945), and then, in a more formal way, by Hilpinen (1977). It was rediscovered and popularized by Halpern and Moses (1990), who originally called it implicit knowledge.", "startOffset": 62, "endOffset": 628}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al. (1995, Chapter 6). Section 9.5 compares the notions of common knowledge with that of common belief. Distributed knowledge was discussed first, in an informal way, by Hayek (1945), and then, in a more formal way, by Hilpinen (1977). It was rediscovered and popularized by Halpern and Moses (1990), who originally called it implicit knowledge. The notion of bisimulation is a central notion in modal logic, providing an answer to the question when two models are \u2018the same\u2019 and is discussed in standard modal logic texts (Blackburn et al., 2001, 2006). Bisimulation arises quite often in this book, including in Chapters 5, 6, and 7. We mentioned below Theorem 1.8, when discussing complexity of validity, that some recent advances make NP-complete problems seem more tractable: for this we refer to work by Gomes, Kautz, Sabharwal, and Selman (2008). We end this brief discussion of the background literature by providing the pointers to the technical results mentioned in our chapter.", "startOffset": 62, "endOffset": 1181}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al. (1995, Chapter 6). Section 9.5 compares the notions of common knowledge with that of common belief. Distributed knowledge was discussed first, in an informal way, by Hayek (1945), and then, in a more formal way, by Hilpinen (1977). It was rediscovered and popularized by Halpern and Moses (1990), who originally called it implicit knowledge. The notion of bisimulation is a central notion in modal logic, providing an answer to the question when two models are \u2018the same\u2019 and is discussed in standard modal logic texts (Blackburn et al., 2001, 2006). Bisimulation arises quite often in this book, including in Chapters 5, 6, and 7. We mentioned below Theorem 1.8, when discussing complexity of validity, that some recent advances make NP-complete problems seem more tractable: for this we refer to work by Gomes, Kautz, Sabharwal, and Selman (2008). We end this brief discussion of the background literature by providing the pointers to the technical results mentioned in our chapter. Theorem 1.1 gives some standard valid formulas for several classes of models (see Fagin et al. (1995, Chapter 2.4) for a textbook treatment). Theorem 1.2 is a folk theorem in modal logic: for a proof and discussion, see Blackburn et al. (2006, Chapter 2.3). Proposition 1.3 is proved by Fagin et al. (1995) as Theorem 3.", "startOffset": 62, "endOffset": 1624}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al. (1995, Chapter 6). Section 9.5 compares the notions of common knowledge with that of common belief. Distributed knowledge was discussed first, in an informal way, by Hayek (1945), and then, in a more formal way, by Hilpinen (1977). It was rediscovered and popularized by Halpern and Moses (1990), who originally called it implicit knowledge. The notion of bisimulation is a central notion in modal logic, providing an answer to the question when two models are \u2018the same\u2019 and is discussed in standard modal logic texts (Blackburn et al., 2001, 2006). Bisimulation arises quite often in this book, including in Chapters 5, 6, and 7. We mentioned below Theorem 1.8, when discussing complexity of validity, that some recent advances make NP-complete problems seem more tractable: for this we refer to work by Gomes, Kautz, Sabharwal, and Selman (2008). We end this brief discussion of the background literature by providing the pointers to the technical results mentioned in our chapter. Theorem 1.1 gives some standard valid formulas for several classes of models (see Fagin et al. (1995, Chapter 2.4) for a textbook treatment). Theorem 1.2 is a folk theorem in modal logic: for a proof and discussion, see Blackburn et al. (2006, Chapter 2.3). Proposition 1.3 is proved by Fagin et al. (1995) as Theorem 3.2.2 (for the case X = K) and Theorem 3.2.4 (for X = T ,S4,KD45, and S5). Proposition 1.4 is Proposition 3.2.1 by Fagin et al. (1995). Theorem 1.", "startOffset": 62, "endOffset": 1770}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al. (1995, Chapter 6). Section 9.5 compares the notions of common knowledge with that of common belief. Distributed knowledge was discussed first, in an informal way, by Hayek (1945), and then, in a more formal way, by Hilpinen (1977). It was rediscovered and popularized by Halpern and Moses (1990), who originally called it implicit knowledge. The notion of bisimulation is a central notion in modal logic, providing an answer to the question when two models are \u2018the same\u2019 and is discussed in standard modal logic texts (Blackburn et al., 2001, 2006). Bisimulation arises quite often in this book, including in Chapters 5, 6, and 7. We mentioned below Theorem 1.8, when discussing complexity of validity, that some recent advances make NP-complete problems seem more tractable: for this we refer to work by Gomes, Kautz, Sabharwal, and Selman (2008). We end this brief discussion of the background literature by providing the pointers to the technical results mentioned in our chapter. Theorem 1.1 gives some standard valid formulas for several classes of models (see Fagin et al. (1995, Chapter 2.4) for a textbook treatment). Theorem 1.2 is a folk theorem in modal logic: for a proof and discussion, see Blackburn et al. (2006, Chapter 2.3). Proposition 1.3 is proved by Fagin et al. (1995) as Theorem 3.2.2 (for the case X = K) and Theorem 3.2.4 (for X = T ,S4,KD45, and S5). Proposition 1.4 is Proposition 3.2.1 by Fagin et al. (1995). Theorem 1.8 is proved by Halpern and Moses (1992). Although the first proofs of completeness for multi-agent versions of axiom systems of the form Xm and XCm are by Halpern and Moses (1992), the ideas go back much earlier.", "startOffset": 62, "endOffset": 1821}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al. (1995, Chapter 6). Section 9.5 compares the notions of common knowledge with that of common belief. Distributed knowledge was discussed first, in an informal way, by Hayek (1945), and then, in a more formal way, by Hilpinen (1977). It was rediscovered and popularized by Halpern and Moses (1990), who originally called it implicit knowledge. The notion of bisimulation is a central notion in modal logic, providing an answer to the question when two models are \u2018the same\u2019 and is discussed in standard modal logic texts (Blackburn et al., 2001, 2006). Bisimulation arises quite often in this book, including in Chapters 5, 6, and 7. We mentioned below Theorem 1.8, when discussing complexity of validity, that some recent advances make NP-complete problems seem more tractable: for this we refer to work by Gomes, Kautz, Sabharwal, and Selman (2008). We end this brief discussion of the background literature by providing the pointers to the technical results mentioned in our chapter. Theorem 1.1 gives some standard valid formulas for several classes of models (see Fagin et al. (1995, Chapter 2.4) for a textbook treatment). Theorem 1.2 is a folk theorem in modal logic: for a proof and discussion, see Blackburn et al. (2006, Chapter 2.3). Proposition 1.3 is proved by Fagin et al. (1995) as Theorem 3.2.2 (for the case X = K) and Theorem 3.2.4 (for X = T ,S4,KD45, and S5). Proposition 1.4 is Proposition 3.2.1 by Fagin et al. (1995). Theorem 1.8 is proved by Halpern and Moses (1992). Although the first proofs of completeness for multi-agent versions of axiom systems of the form Xm and XCm are by Halpern and Moses (1992), the ideas go back much earlier.", "startOffset": 62, "endOffset": 1961}, {"referenceID": 0, "context": "Work on common knowledge in economics was initiated by Robert Aumann (1976); John McCarthy\u2019s (1990) work involving common knowledge had a significant impact in the field of artificial intelligence. Good starting points for further reading on the topic of common knowledge are by Vanderschraaf and Sillari (2014) and by Fagin et al. (1995, Chapter 6). Section 9.5 compares the notions of common knowledge with that of common belief. Distributed knowledge was discussed first, in an informal way, by Hayek (1945), and then, in a more formal way, by Hilpinen (1977). It was rediscovered and popularized by Halpern and Moses (1990), who originally called it implicit knowledge. The notion of bisimulation is a central notion in modal logic, providing an answer to the question when two models are \u2018the same\u2019 and is discussed in standard modal logic texts (Blackburn et al., 2001, 2006). Bisimulation arises quite often in this book, including in Chapters 5, 6, and 7. We mentioned below Theorem 1.8, when discussing complexity of validity, that some recent advances make NP-complete problems seem more tractable: for this we refer to work by Gomes, Kautz, Sabharwal, and Selman (2008). We end this brief discussion of the background literature by providing the pointers to the technical results mentioned in our chapter. Theorem 1.1 gives some standard valid formulas for several classes of models (see Fagin et al. (1995, Chapter 2.4) for a textbook treatment). Theorem 1.2 is a folk theorem in modal logic: for a proof and discussion, see Blackburn et al. (2006, Chapter 2.3). Proposition 1.3 is proved by Fagin et al. (1995) as Theorem 3.2.2 (for the case X = K) and Theorem 3.2.4 (for X = T ,S4,KD45, and S5). Proposition 1.4 is Proposition 3.2.1 by Fagin et al. (1995). Theorem 1.8 is proved by Halpern and Moses (1992). Although the first proofs of completeness for multi-agent versions of axiom systems of the form Xm and XCm are by Halpern and Moses (1992), the ideas go back much earlier. In particular, the basic canonical model construction goes back to Makinson (1966) (see Blackburn et al.", "startOffset": 62, "endOffset": 2077}, {"referenceID": 22, "context": "tems of the form XC is already in the proof of Kozen and Parikh (1981) for proving completeness of dynamic logic.", "startOffset": 47, "endOffset": 71}, {"referenceID": 12, "context": "Completeness for axiom systems of the form XD was proved by Fagin, Halpern, and Vardi (1992) and by van der Hoek and Meyer (1992).", "startOffset": 67, "endOffset": 93}, {"referenceID": 12, "context": "Completeness for axiom systems of the form XD was proved by Fagin, Halpern, and Vardi (1992) and by van der Hoek and Meyer (1992). A novel proof is provided by Wang (2013, Chapter 3).", "startOffset": 67, "endOffset": 130}, {"referenceID": 12, "context": "Completeness for axiom systems of the form XD was proved by Fagin, Halpern, and Vardi (1992) and by van der Hoek and Meyer (1992). A novel proof is provided by Wang (2013, Chapter 3). Theorem 1.6 is part of logical folklore. A proof of Theorem 1.7 was given by French, van der Hoek, Iliev, and Kooi (2013).", "startOffset": 67, "endOffset": 306}], "year": 2015, "abstractText": "This chapter provides an introduction to some basic concepts of epistemic logic, basic formal languages, their semantics, and proof systems. It also contains an overview of the handbook, and a brief history of epistemic logic and pointers to the literature. 1.", "creator": "LaTeX with hyperref package"}}}