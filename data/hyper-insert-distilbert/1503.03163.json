{"id": "1503.03163", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Mar-2015", "title": "Learning Classifiers from Synthetic Data Using a Multichannel Autoencoder", "abstract": "meanwhile we propose a standard method for using hybrid synthetic data to help assess learning classifiers. synthetic natural data, even is generated based on real data, normally results automatically in causing a significant shift gain from by the usual distribution of common real material data transmitted in feature space. tempting to bridge the gap presented between communicating the real and probable synthetic data, together and jointly learn from between synthetic space and real data, but this paper proposes finding a fully multichannel 3d autoencoder ( mcae ). we show immediately that by suing mcae, it is possible to learn a better feature of representation for vector classification. to evaluate broadly the proposed approach, mostly we conduct performance experiments on two basic types of blended datasets. experimental results centered on seeing two datasets be validate through the efficiency of varying our prospective mcae model parameters and our basic methodology of generating synthetic data.", "histories": [["v1", "Wed, 11 Mar 2015 03:31:53 GMT  (1746kb,D)", "http://arxiv.org/abs/1503.03163v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["xi zhang", "yanwei fu", "i zang", "leonid sigal", "gady agam"], "accepted": false, "id": "1503.03163"}, "pdf": {"name": "1503.03163.pdf", "metadata": {"source": "CRF", "title": "Learning Classifiers from Synthetic Data Using a Multichannel Autoencoder", "authors": ["Xi Zhang", "Yanwei Fu", "Andi Zang", "Leonid Sigal", "Gady Agam"], "emails": ["xzhang22@hawk.iit.edu,", "zang@hawk.iit.edu,", "agam@iit.edu.", "lsigal}@disneyresearch.com"], "sections": [{"heading": null, "text": "F"}, {"heading": "1 INTRODUCTION", "text": "Large and balanced datasets are normally crucial for learning classifiers. In real-world scenarios, however, one always struggles to find adequate amounts of labeled data. Even with the help of crowdsourcing, e.g., Amazon Mechanical Turk (AMT), it is often difficult to collect a large quantity of labeled instances with high quality that is necessary for training a classifier for a real-world problem. In terms of quantity, it has been shown that the amount of available training data, per object class, roughly follows a Zipf distribution [35]. That means a small number of object classes account for most of the available training instances. In terms of quality, some domains, such as the analysis of satellite images (e.g. the comet images from Rosetta), require extensive and detailed expert user annotation [32], [48]. Large volume of LiDAR point cloud data have to be labeled before they can be used to train some classifiers [49]. Such labeling process usually is very time consuming and requires expert-level labeling efforts or expensive equipments. Practically only a very limited portion of the data points can be obtained.\nTo solve the problem of lacking enough training samples, attributes [22], [30], [13] have been introduced to transfer the knowledge held by majority classes to instances in minority classes. Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define. In contrast, rather than using such a \u2018learning to learn\u2019 [38] framework, humans can generalize and associate the similar patterns from images. This ability inspires us to circumvent the problem of lacking enough training data and solve it from a different angle: utilizing the synthetic\n\u2022 Xi Zhang, Andi Zang, and Gady Agam are with the Illinois Institute of Technology Chicago, IL 60616. Email: {xzhang22,zang}@hawk.iit.edu, and agam@iit.edu. \u2022 Yanwei Fu, and Leonid Sigal are with Disney Research, Pittburgh, PA, 15213. Email: {yanwei.fu, lsigal}@disneyresearch.com\ndata (e.g. the synthetic roof edges in Fig. 1) associated with real data (e.g. real roof edges in Fig. 1) in order to learn a better classifier.\nThe idea of associating synthetic data with real data has a long history and is associated with the development of cognitive psychology, artificial intelligence, and computer vision. For example, cognitive psychology studied a case that an infant learns to understand and imitate a facial expression from parents\u2019 examples. In the computing domain, exemplar SVM [26] tries to associate images with training exemplars. Different from these previous works, we create synthetic images to associate them with the real images whilst previous works associate \u2018new\u2019 real data with \u2018old\u2019 real data. By contrast, our approach is a \u2019free lunch\u2019 in the sense that the proposed approach does not need any human annotation of real data, thus we could easily amplify the dataset used in training.\nLearning a classifier from synthetic data is unfortunately extremely challenging due to the following reasons. Firstly, the feature distribution of synthetic data generated will shift away from that of real data. Such distribution shift is termed synthetic gap and illustrated in Fig 2. The synthetic gap is a major obstacle in using synthetic data to help learning classifiers, since synthetic data may fail to simulate the potential useful patterns of real data for training classifiers. To our knowledge, this synthetic gap problem has never been formally\nar X\niv :1\n50 3.\n03 16\n3v 1\n[ cs\n.C V\n] 1\n1 M\nar 2\n01 5\n2 \u221260 \u221240 \u221220 0 20 40 60 \u221260 \u221240 \u221220 0 20 40 60 80 Real Synthetic \u221280 \u221260 \u221240 \u221220 0 20 40 60 \u221260 \u221240 \u221220 0 20 40 60 Real Synthetic\n(a) (b)\nFigure 2. t-SNE visulization of synthetic gap using the data from SRC dataset. (a) synthetic gap of real and synthetic data; (b) MCAE bridges the synthetic gap.\nidentified nor addressed in the literature. Secondly, since practically a small amount of labeled images may be available, it is necessary to jointly learn from synthetic and real data. The learning process must be automatically leveraged between synthetic data and real data.\nTo better learn a classifier from synthetic data, we propose a novel framework \u2013Multichannel Autoencoder (MCAE) which is an extension of sparse autoencoder. The training step of MCAE is a process of bridging the synthetic gap between the real and the synthetic data by learning the mapping from (1) synthetic to real data and (2) real to real data. Critically, such mapping try to keep the real data while enforce MCAE to learn a transfer from the synthetic data to the real data. We thus can generate more synthetic data which will simulate the real data when the learned mapping is applied to them.\nTo facilitate the study on satellite image analysis, we introduce a new benchmark satellite roof classification (SRC) image dataset. The SRC dataset needs expertlevel labeling and has unique challenges, such as satellite image blurring, building shadows, and extremely imbalanced roof class instances. To demonstrate the generality of the proposed approach, we use an additional handwritten digit dataset from the UCI machine learning repository [1]. In both datasets, synthetic data is generated using a parametric model of derived from real data that roughly mimics real data in terms of appearances and basic structure. Experimental results using these datasets demonstrate that better classification results can be obtained by training a classifier using the synthetic data when used by the proposed approach.\nWe thus highlight three contributions in this paper: (1) To the best of our knowledge, this is the first attempt to address the problem of synthetic gap, by solving which we demonstrate that the synthetic data could be used to improve the performance of classifiers. (2) We propose a Multichannel Autoencoder (MCAE) model to bridge the synthetic gap and jointly learn from both real and synthetic data. (3) Also, a novel benchmark dataset \u2013 Satellite Roof Classification (SRC) is introduced to the vision community. Such dataset is of expert-level label annotations as well as great challenges for satellite image analysis."}, {"heading": "2 RELATED WORK", "text": "3D image analysis. Synthetic data has been used for several 3D image analysis applications, but not for helping learn classifiers. A large number of synthetic 3D meshes in [47] were created by a series of mesh editing steps including subdivision, simplification, smooth, adding noise and Poisson reconstruction, in order to automatically evaluate the subjective visual quality of a 3D object. Recently, to circumvent the point labeling difficulty in a building roof classification problem using LiDAR point cloud, [49] explicitly indicated semantic roof points on synthetically created roof point clouds and compute point features from the synthetic point clouds. Techniques such as point cloud resampling, size normalization and mesh erosion are employed to reduce the differences between real roof and synthetic ones in data space.\nGenerating synthetic data. Previous method generate synthetic data in data space using tools including geometrical transformation and degradation models: In [40][41], to help off-line recognition of handwritten text, a perturbation model combined with morphological operation is applied to real data. They showed that when a moderate transformation is added to the real data, the resulting synthetic training set boost the performance. To enhance the quality of degraded document, in [2] degradation models such as brightness degradation, blurring degradation, noise degradation and textureblending degradation were used to create a training dataset for a handwritten text recognition problem. The synthetic minority oversampling technique (SMOTE) [8] and its variants [19][20] are also powerful methods that have shown many success in various applications. However, these previous methods are relatively limited to one particular type of dataset, whilst we propose a more general methodology of generating synthetic data in this paper. We show that our methodology can be used both for SRC and handwritten digits dataset.\nTransfer Learning aims to extract the knowledge from one or more source tasks and applies the knowledge to a target task. Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37]. Transfer learning is categorized to three classes [31]: inductive transfer learning, transductive transfer learning and unsupervised transfer learning. The work in this paper falls into a framework of domain adaptation [4], [44] in the transductive transfer learning. Nonetheless, different from previous domain adaptation tasks of different source and target domains, the synthetic gap is caused by the shifted feature distribution of synthetic data from real data. To solve this problem, our MCAE is developed from the idea of autoencoder.\nAutoencoder is one type of neural network and its output vectors have the same dimensionality as the\n3 input vectors [42]. The hidden representation obtained by training a sparse autoencoder followed by a parameters fine tuning is useful in pre-training a deeper neural network. Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks."}, {"heading": "3 MULTICHANNEL AUTOENCODER (MCAE)", "text": "In this section, we introduce the MCAE model as illustrated in Fig. 3. It can (1) bridge synthetic gap by minimizing the discrepancy between real and synthetic data; and (2) preserve and emphasize the potential useful patterns existed in both real and synthetic data in order to generate the better feature representations used for learning classifiers.\nEssentially, synthetic and real data should have similar patterns, a natural idea of bridging synthetic gap is to learning a mapping from the synthetic data to the real data using an autoencoder, and vice versa. MCAE, hence, provides a more flexible way to learn this mapping due to the specific structure of the MCAE. There are two channels in MCAE, left one and right one. Each channel basically is an SAE, however, two channels share the same hidden layer. With this structure, MCAE basically learns two tasks in the same time. By setting different types of input and out data such as the one in denoising autoencoder [43], MCAE is capable for many applications. In our work, to bridge the gap between synthetic data and real data, we set the task in left channel as one that takes synthetic data as input and real data as reconstruction target, while the task in right channel use real data in both input and reconstrution target. This configuration actually is essentially meaningful that by keeping the reconstruction target identical in two channels, MCAE attempts to transform inputs in two channels towards the same target, thus minimize the discrepancy between two input dataset which are synthetic data and real data in our work."}, {"heading": "3.1 Problem setup", "text": "Our MCAE is built on the sparse autoencoder (SAE). A basic autoencoder is a fully connected neural network with one hidden layer and can be decomposed into two parts: an encoding and a decoding process. Assume an input dataset with n instances X = {xi}ni=1 where xi \u2208 Rm and m is the dimension of each instance. Encoding typically transforms input data to hidden layer representation using an affine mapping squashed by a sigmoid function:\nhe(xi) = f(Wexi + be) (1)\nwhere f(\u00b7) is a sigmoid function and \u03b8e = {We, be}, We \u2208 Rk\u00d7m, be \u2208 Rk is a set of unknown parameters in encoding with k nodes in hidden layer.\n\u03b8e\n\u03b8d L\n\u03b8d R\ntask one task two\nE ncoding\nD ecoding\n(a) (b)\nFigure 3. (a) Illustration of the proposed MCAE model in a stacked autoencoder structure, where black edge between two layers are linked to and shared by two tasks, red and blue links are separately connected to left and right task respectively. (b) A zoom in structure of MCAE.\nWhile in decoding, with parameters \u03b8d = {Wd, bd}, Wd \u2208 Rm\u00d7k, bd \u2208 Rm, autoencoder attempts to reconstruct the input data at the output layer by imposing another affine mapping followed by nonlinearity to hidden representation he(xi):\nhd(xi) = f(Wdhe(xi) + bd) (2)\nIn above equation hd(xi) is viewed as a reconstruction of input xi. Normally, we impose hd(xi) \u2248 xi. Here xi play a role of reconstruction target in this expression and we use notation \u3008i:Xi, t:Xi\u3009 to denote the configuration of input data short for i and reconstruction target short for t in an autoencoder. Xs and Xr indicate synthetic and real data respectively. By minimizing the reconstruction errors of all data instances, we have following objective function:\nJ(\u03b8e, \u03b8d) = 1\nn n\u2211 i=1 (hd(xi)\u2212 xi)2 + \u03bbW (3)\nwhere W = ( \u2211 W 2e + \u2211 W 2d )/2 is a weight decay term added to improve generalization of the autoencoder and \u03bb leverages the importance of this term.\nTo avoid learning identity mapping in autoencoder, a regularization term \u0398 = \u2211k i=1 \u03b4log\n\u03b4 \u03b4\u0302i + (1\u2212 \u03b4)log 1\u2212\u03b4 1\u2212\u03b4\u0302i\nthat penalizes over-activation of the nodes in the hidden layer is added. \u03b4 is a sparsity parameter and is set by users and \u03b4\u0302i = 1k \u2211k i=1 he(xi).\nJ(\u03b8e, \u03b8d) = 1\nn n\u2211 i=1 (x\u0302i \u2212 xi)2 + \u03bbW + \u03c1\u0398 (4)\n\u03c1 controls sparsity of representation in hidden layer. Note that directly applying sparse autoencoder to our problem does not work well. For example, we can train an autoencoder purely by placing synthetic data in input layer and real data in output layer denoted as \u3008i:Xs, t:Xr\u3009 which however can not bridge the synthetic gap in our problem. Such way of reconstruction is only\n4 to complement the missing information in synthetic data from real data but not vice versa1.\nA better representation should be reconstructed by using the information from both real and synthetic data simultaneously. Specifically, we aim at two tasks: one is \u3008i:Xs, t:Xr\u3009L which reconstructs synthetic data towards real data, and the other one is \u3008i:Xr, t:Xr\u3009R which uses identical real data for input and reconstruction target, where \u3008\u00b7\u3009L and \u3008\u00b7\u3009R indicate the left and right channel of MCAE."}, {"heading": "3.2 MCAE model", "text": "We propose a multichannel autoencoder that uses a balance regularization to leverage the learning between two tasks, i.e. \u3008i:Xs, t:Xr\u3009L and \u3008i:Xr, t:Xr\u3009R. The structure of this new autoencoder is shown in Fig. 3. In this new structure, tasks of two channels will share the same parameters \u03b8e in encoding process which will enforce autoencoder to reconstruct common structure in both tasks. However, in decoding process, we divide autoencoder to two separate channels that two tasks will have their own parameters \u03b8Ld and \u03b8 R d . Dividing autoencoder to two channels at decoding layer enable a more flexible control between the two tasks. Thus autoencoder better leverage the common knowledge from the two tasks.\nWith two channels in the MCAE, we target to minimize the reconstruction error of two tasks together while taking into account the balance between two channels. The new objective function of the MCAE is given in the following:\nE = JL(\u03b8e, \u03b8 L d ) + J R(\u03b8e, \u03b8 R d ) + \u03b3\u03a8 (5)\nwhere\n\u03a8 = 1\n2 (JL(\u03b8e, \u03b8 L d )\u2212 JR(\u03b8e, \u03b8Rd ))2 (6)\nis a regularization added to balance the learning rate between two channels. This regularization will have two effects on the MCAE. First, \u03a8 accelerates the speed of optimizing Eq. 9, since minimizing \u03a8 requires both JL(\u03b8e, \u03b8 L d ) and J R(\u03b8e, \u03b8 R d ) are small which in turn cause E decreases faster. Second, \u03a8 penalizes a situation more when difference of learning error between two channels are large, so as to avoid imbalanced learning between two channels.\nThe minimization of Eq. 9 is achieved by back propagation and stochastic gradient descent using QuasiNewton method. Since the regularization term is added to leverage the balance of different tasks, we have to compute the gradient of parameters \u03b8e and \u03b8Ld , \u03b8 R d in MCAE. Please refer to the supplementary material for the detailed computation of gradients.\n1. Please refer to supplemenatry maerial for the validation"}, {"heading": "3.3 The advantages of MCAE over the alternative Configurations", "text": "Our MCAE enforces autoencoder to learn useful class patterns from the two tasks simultaneously. Thus it helps with capturing a common structure of synthetic and real images. Another alternative way is to concatenate the input and target of the two tasks \u3008i:XsXr, t:XrXr\u3009 for autoencoder. We annotate the usage of this autoencoder as Concatenate-Input Autoencoder (CIAE), since this autoencoder learns concatenated tasks at the same time. Such configurations however may result in an unbalanced optimization for these two tasks: the optimization process of one task will take over the process of the other one. It results in a biased reconstructed hidden layer of the autoencoder and thus a limited classification performance. Our experiments also validate this point in Sec. 5."}, {"heading": "4 GENERATING SYNTHETIC DATA", "text": "It is an important and yet less studied topic of how to generate synthetic data. This section discusses the methodology of generating synthetic data used in our experiments. Such synthetic data have some similarities and differences with the augmented data used in deep learning e.g. [21]. Both of synthetic data and augmented data aim at improving the generalisation capacity of classifiers. Nevertheless, the methodology of generating synthetic data brings more deformed patterns than the simply label-preserving transformations used in data augmentation.\nSynthetic data are created to highlight the potential useful pattern existed in real images. We have two stages of generating synthetic data. In the first stage, for each real data used to train MCAE, a synthetic version that best matching appearance of the real data is generated; thus pairs of corresponding real and synthetic data can be used to train the MCAE. In the second stage, more synthetic data could be derived using synthetic data generated in the first stage by both interpolation and extrapolation. To distinguish the set of synthetic data used in these two stages, we use abbreviation Syn I and Syn II to represent them respectively.\nIn the proposed approach, the synthetic data are represented as a parametric model of a set of control points and edges associated to these points in the images. From the control points, the synthetic images could be generated to simulate the real images in terms of having the same structure or a similar appearance. Initially, the control points are selected from a synthetic prototype that generalize all images in the same class. Then the locations of the control points are iteratively optimized until convergence in order to minimize the distance between synthetic images generated by control points and the real image. We annotate the control points and edges associated to them as S = {P,E}, where P = {pi}ni=1 is the set of the control points, and E = {(pi, pj)}, 1 \u2264 i, j \u2264 n is the set of edges connecting control points.\n5 A generalized algorithm of getting the best matching synthetic image is provided in Algorithm 1.\nAlgorithm 1 Get Matching Synthetic Image. Input: \u2022 A real image U . \u2022 A set of control points S = {P,E} with all control points pi \u2208 P set to their initial positions. \u2022 A prototype image V generated using the initial S.\n1: while S is not converged do 2: S = OptimizeControlPoints(U, V,S). 3: Generate V using S. 4: end while 5: Generate synthetic image I using S. 6: return I .\nThe synthetic prototype could be manually design or learning from given data in our work given different tasks. We will show how these two methods produce synthetic data in following two sections respectively."}, {"heading": "4.1 Explicitly Design of the Synthetic Prototype", "text": "The generation of the synthetic prototype and control points in this scenario is inspired by the approach proposed by Zhang et al. [49]. In their work, given enough pre-knowledge about the 3D objects, a synthetic prototype of 3D objects is explicitly designed and built. By adjusting the control points of the prototype, various kinds of 3D objects are generated. In this work, essentially, our data is very similar to theirs in a sense that roof images share a lot of characteristics such as ridge lines, valley lines and intersections between these lines, which make it possible to manually design the synthetic prototypes that characterize these patterns. Based on this observation, a synthetic roof prototype could be generated by setting the control points at the intersections of the ridge or valley lines and drawing segments connecting these control points. 2.\nIn this scenario, the OptimizeControlPoints(U, V,S) function of Alg. 1 turns out to be a process that searches for optimal control point locations which results in a synthetic image minimizing the discrepancy between the real image and the synthetic image. A coordinate descent framework is employed to accelerate the search process. We summarize this method in Alg. 2."}, {"heading": "4.2 Learning Synthetic Prototype from Data", "text": "In hand written digit dataset used in this work, we learn a synthetic prototype from given data. A digit prototype is generated for all images with the same digit. Congealing algorithm proposed in [27] is employed in this step to produce the synthetic prototypes for digits. In congealing, the project transformations are applied to images to minimize a joint entropy. Thus the prototype\n2. In our experiments, classification of the roof images is essentially similar to that of [49]. Our approach recognizes the style of the roofs based on edges extracted from the roof images. For more visualisation results, please refer to our supplementary material.\nAlgorithm 2 OptimizeControlPoints(U, V,S) Case 1 Input: \u2022 A real image U . \u2022 A prototype of the synthetic image S = {P,E}. \u2022 A synthetic image V generated using S.\n1: for pi \u2208 P, 1 \u2264 i \u2264 n do 2: Update S by moving pi by one unit. 3: Generate V using S. 4: if S does not reduce Dist(U, V ) then 5: Cancel the last move of pi. 6: Generate V using S. 7: end if 8: end for 9: return S.\nis considered to be an average image of all images after congealing.\nThen control points are evenly sampled from the boundary detected from the prototype image. The control points needs to be mapped to each digit image in order to generate a synthetic image. To find this mapping we implement an approach that migrates the control points from the prototype images to destination image.\nThis point migration algorithm is based on a series of intermediate images generated in between synthetic prototype and destination image. To generate the intermediate images, we binarize all the images and the distance transformed images[7] of the synthetic prototype and the real image are generated. Given the number of steps, an intermediate image then is generated as a binarized image of linear interpolation between two distance transformed images. In each step, the control points are snapped to the closest boundary pixels of the intermediate image. The algorithm of OptimizeControlPoints(U, V,S) in this situation is given in Algorithm 3, we fix the number of steps to 10 in this algorithm.\nTo generate the SynII dataset, we either interpolate or extrapolate between control points of randomly choose two synthetic images from SynI dataset. The weights used in interpolation and extrapolation is uniformly drawn from 0 to 1."}, {"heading": "5 EXPERIMENTS AND RESULTS", "text": "We validate the proposed MCAE dataset on several applications in this section. This section is organised as follows. First, in Sec 5.1, we introduce a new benchmark dataset \u2013 Satellite Roof Classification (SRC) dataset to vision community. This dataset is of high quality satellite\n6 Algorithm 3 OptimizeControlPoints(U, V,S) Case 2 Input: \u2022 A real image U . \u2022 A prototype of the synthetic image S = {P,E}. \u2022 A synthetic image V .\n1: steps = 10. 2: Compute distance transform image of U, V as U \u2032, V \u2032. 3: for i = 1 to steps do 4: I = (1\u2212 i\nsteps )U \u2032 + i steps V \u2032.\n5: I=Binarize(I). 6: Update S by snapping to the closest boundary pixel on I . 7: end for 8: Set the status of S to be converged. 9: return S.\nroof class labels for the satellite images. We also briefly summarizes the handwritten digits dataset used in our paper. We explain the experimental settings in Sec. 5.2 and discuss the experimental results in Sec. 5.3."}, {"heading": "5.1 Experiment Datasets", "text": ""}, {"heading": "5.1.1 Satellite Roof Classification (SRC) Dataset", "text": "One particular interesting problem of learning classifiers from synthetic data is to analyze satellite images of the Earth. Such problems generally need very high quality (expert-level) labeled data. However, there is no previous dataset for such research purposes. To facilitate the study, a new benchmark Satellite Roof Classification (SRC) Dataset is created and used in our experiments. Given a satellite image, we employ a method described in [48] to crop roof images by registering artificial building footprints with the satellite image. Later, all roof images are aligned using their footprint principal directions using a method proposed in [50] and then are scaled to images with resolution of 128\u00d7256. Two experts are invited to contribute the labels of 6 different roof styles: flat, gable, gambrel, halfhip, hip and pyramid. Example instances of SRC dataset are shown in Fig 5.\nThis dataset is of great challenges for the task of visual analysis. First, qualities of the some satellite images are degraded because of significant image blurring occurred when capturing the satellite images. Second, roofs in these images are covered by various kinds of equipments such as air conditioners chimneys and water tanks, and most of roofs in our dataset are partially occluded by shadows cast by trees and some other stuffs. Such covering and shadows are great obstacles to robust visual analysis algorithms. Furthermore, the class instances of SRC dataset are naturally extreme imbalance, since some particular types of roofs (such as gambrel and pyramid) are far less than the other types in the real world. Such unbalanced distributions of data are compared in in Table 1.\nClassification of the roof styles in the experiments are based on recognizing edges detected from the roof images. We employed the adaptive Otsu edge detection method [29] to extract edges from the roof images. We\nStyles Training # Testing # Total #\nFlat 1232 1748 3080 Gable 1111 1665 2776 Gambrel 156 232 388 Halfhip 268 400 668 Hip 960 1440 2400 Pyramid 133 199 332\nTable 1 The distribution of the roof styles used in the experiments.\ncreate synthetic prototype to characterize primary ridge lines or valley lines in a certain type of roof style. Examples of the synthetic prototypes are shown in Fig. 6. Real roof edge images and matching Syn I images are shown in Fig. 1. To create Syn II images of this dataset, 2000 synthetic images are produced by interpolation and extrapolation between images in Syn I for each roof style."}, {"heading": "5.1.2 Handwritten Digits Dataset", "text": "We also validate our framework on handwritten digits dataset from UCI machine learning repository [1] which totally has 5620 instances. The handwritten digits from 0 to 9 in this dataset are collected from 43 people: 30 contributed to the training set and the other 13 to the test set. In the experiments, the Syn I data are generated using Algorithm 3. The Syn II data of this dataset is generated using interpolation and extrapolation as described in Sec 4."}, {"heading": "5.2 Experimental Settings", "text": "We fix the configuration of MCAE as \u3008i:Xs, t:Xr\u3009L (left channel) and \u3008i:Xr, t:Xr\u3009R (right channel). Specifically, the left channel is the reconstruction process from synthetic data to real data, while the right channel works in the same way as a standard SAE. Our experimental results will show that the representations learned in such way greatly benefit the performance of classifiers we compared.\nIn the experiments3, two different classifiers of utilizing learned representations from MCAE (from Sec. 3) are compared. In the first scenario, MCAE encodes input data to a representation (feature) in the hidden\n3. All codes (including our MCAE and creating synthetic data) will be released once accepted.\n7\nlayer and a SVM using RBF kernel is employed in this case to show the performance of the classification. In the second scenario, MCAE takes the input images and produces the reconstructed images at the output layer. Features, in this case, are images, therefore can be fed to Convolutional Neural Network (CNN) for classification. In our experiments we build a LeNet-5 [24] which is originally created for digit recognition. We show that using the same number of input data, the performance of the CNN prefers to the data produced by the MCAE.\nWe summarize all evaluations and comparisons using F-1 score, which is defined as:\nF1 = 2 \u00b7 Precision \u00b7 Recall\nPrecision + Recall (7)\n5.3 Evaluation4\nMCAE is better than CIAE and SAE. To better evaluate the performance of the proposed MCAE, we compare MCAE with Concatenate-Input Autoencoder (CIAE) [28] and Sparse Autoencoder (SAE) [43]. In these experiments, we evaluate the performance on two classifiers: a CNN using reconstructed images and SVM using encoded hidden layer representation. We present the results of these comparisons in Table 2 and Table 3 for SRC and handwritten digit datasets respectively. It could be observed from these two tables that although the performance of the CIAE is close to MCAE, the proposed MCAE gets a better performance almost in all the comparisons.\nSynthetic data help learning a better classifier. We designed another group of experiments. In these experiments three different configurations of data are either reconstructed and encoded using the proposed MCAE, then used to train a CNN or a SVM in the experiments. All results from these experiments are compared in Table 4 and Table 5 respectively. An interesting thing to notice is that in experiments, using synthetic data can only achieve the same result as using a combination of real\n4. Due to the page limit, please refer to our supplementary material for the additional experimental results.\nData to train autoencoder\nCNN Reconstructed SVM Encoded\nMCAE \u3008i:Syn I, t:Real\u3009 L\n\u3008i:Real, t:Real\u3009R 0.68 0.80\nCIAE \u3008i:Syn I + Real, t:Syn I + Real\u3009 0.68 0.78\nSAE \u3008i:Syn I, t:Syn I\u3009 0.63 0.59 SAE \u3008i:Real, t:Real\u3009 0.62 0.62\nand synthetic data. This result proves that the distribution of the real data in this case is almost overlapping with the distribution of the synthetic data.\nMCAE bridges the synthetic gap. We compare the correlation defined as:\nCorr = Cov(X,Y )\nVar(X)Var(Y ) (8)\nbetween real and Syn I data before and after being reconstructed by the MCAE. The intention of these comparisons is to show that real synthetic images become\nmuch more alike each other in terms of the appearance after being reconstructed by the MCAE. The results are shown in Fig. 8. It is shown that our method almost achieves 100% correlation between real and Syn I when both data are reconstructed by the proposed MCAE. That means the proposed MCAE bridges the synthetic gap between the real data and the synthetic data. The results are shown in Fig. 7. It intuitively shows that our MCAE can help bridge the synthetic gap between real and synthetic data."}, {"heading": "6 CONCLUSION", "text": "In this paper we identify the problem of synthetic gap. By solving this problem, in our experiments, we demonstrate that the synthetic data could be used to improve the performance of classifiers. To better learn classifiers from synthetic data, we have proposed a novel Multichannel autoencoder (MCAE) model. MCAE has multiple channels in its structure and is an extension from standard autoencoder. We show that MCAE not\nonly bridges the synthetic gap between real data and synthetic data, it also jointly learns from both real and synthetic data, thus can provide more robust representation for both data. To facilitate the study on satellite image analysis, we introduce a novel benchmark dataset \u2013 SRC as one dataset used in our experiments. The proposed method has been validated on SRC and handwritten digits datasets."}, {"heading": "6.1 Optimization of MCAE", "text": "With two branches in the MCAE, we target to minimize the reconstruction error of two tasks together while taking into account the balance between two branches.\n10\nThe new objective function of the YMAE is given in the following:\nE = JL(\u03b8e, \u03b8 L d ) + J R(\u03b8e, \u03b8 R d ) + \u03b3\u03a8 (9)\nwhere \u03a8 = 1\n2 (JL(\u03b8e, \u03b8 L d )\u2212 JR(\u03b8e, \u03b8Rd ))2 (10)\nis a regularization added to balance the learning rate between two branches. This regularization will have two effects on the YMAE. First, \u03a8 accelerates the speed of optimizing Eq. 9, since minimizing \u03a8 requires both JL(\u03b8e, \u03b8 L d ) and J R(\u03b8e, \u03b8 R d ) are small which in turn cause E decrease faster. Second, \u03a8 penalize a situation more when difference of learning error between two branches are large, so as to avoid imbalanced learning between two branches.\nThe minimization of Eq. 9 is achieved by back propagation and stochastic gradient descent using QuasiNewton method. In the MCAE, with balance regularization added to the objective, the only difference as opposed to sparse autoencoder is the gradient computation of unknown parameters \u03b8e and \u03b8Ld , \u03b8 R d . We clarify these differences in the following equations:\n\u2207WeE = \u2202JL \u2202We + \u2202JR \u2202We + \u03b3(JL \u2212 JR)( \u2202J L \u2202We \u2212 \u2202J R \u2202We )\n\u2207beE = \u2202JL \u2202be + \u2202JR \u2202be + \u03b3(JL \u2212 JR)(\u2202J L \u2202be \u2212 \u2202J R \u2202be )\n(11)\nand\n\u2207WLd E = \u2202JL \u2202WLd + \u03b3(JL \u2212 JR) \u2202J L \u2202WLd \u2207bLdE = \u2202JL\n\u2202bLd + \u03b3(JL \u2212 JR)\u2202J\nL\n\u2202bLd\n\u2207WRd E = \u2202JR \u2202WRd + \u03b3(JL \u2212 JR)(\u2212 \u2202J R \u2202WRd ) \u2207bRd E = \u2202JR\n\u2202bRd + \u03b3(JL \u2212 JR)(\u2212\u2202J\nR\n\u2202bRd )\n(12)\nThe exact form of gradients of \u03b8e and \u03b8Ld , \u03b8 R d varies according to different sparsity regularization \u0398 used in the framework."}, {"heading": "7 GENERATING SYNTHETIC DATA", "text": "An example in Fig. 9 shows how control points are moved from source image (the one with blue boundary) to destination image (the one with red boundary). It could be observed that most of the control points are moved from the source image to corresponding locations on destination image. In this step, it is not necessary for all control points accurately move to exact corresponding location on the destination image. Our goal is just to use these migrated control points to generate synthetic data which will roughly mimic the real data. Our MCAE later will rectify the difference between synthetic data and real data."}, {"heading": "7.1 Further Validation for MCAE", "text": "Note that directly applying sparse autoencoder to our problem does not work well. For example, we can train an autoencoder purely by placing synthetic data in input layer and real data in output layer which however can not bridge the synthetic gap in our problem. Such way of reconstruction is only to complement the missing information in synthetic data from real data. On the contrary, reconstructed real data using such SAE will add unnecessary information and noisy patterns to reconstructed data.\nTo validate this point, we extend the experiments of two datasets and show that SAE can not bridge the gap of synthetic gap. The results are shown in Fig. 10. The reconstructed data of SAE have lower average divergence than the other methods. That means, SAE performances worse than MCAE in bridging the synthetic gap.\n11"}], "references": [{"title": "Interactive degraded document enhancement and ground truth generation", "author": ["G. Bal", "G. Agam", "O. Frieder", "G. Frieder"], "venue": "Electronic Imaging 2008. International Society for Optics and Photonics,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Autoencoders, unsupervised learning, and deep architectures", "author": ["P. Baldi"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning, Volume 7, page 43,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Mach. Learn., 79(1-2):151\u2013175, May", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning of representations for unsupervised and transfer learning", "author": ["Y. Bengio"], "venue": "Unsupervised and Transfer Learning Challenges in Machine Learning, Volume 7, page 19,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Distance transforms in digital images", "author": ["G. Borgefors"], "venue": "Computer Vision, Graphics, and Image Processing, volume 34, pages 344\u2013371. Elsevier,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1986}, {"title": "Smote: Synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of Artificial Intelligence Research, 16:321\u2013357,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K.Q. Weinberger", "Fei"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Hierarchical ranking of facial attributes", "author": ["A. Datta", "R. Feris", "D. Vaquero"], "venue": "IEEE International Conference on Automatic Face & Gesture Recognition,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse autoencoderbased feature transfer learning for speech emotion recognition", "author": ["J. Deng", "Z. Zhang", "E. Marchi", "B. Schuller"], "venue": "Affective Computing and Intelligent Interaction (ACII), 2013 Humaine Association Conference on, pages 511\u2013516. IEEE,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "High level describable attributes for predicting aesthetics and interestingness", "author": ["S. Dhar", "V. Ordonez", "T.L. Berg"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi", "I. Endres", "D. Hoiem", "D. Forsyth"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Attribute learning for understanding unstructured social activity", "author": ["Y. Fu", "T. Hospedales", "T. Xiang", "S. Gong"], "venue": "European Conference on Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Transductive multi-view embedding for zero-shot recognition and annotation", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "Z. Fu", "S. Gong"], "venue": "European Conference on Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multimodal latent attributes", "author": ["Y. Fu", "T.M. Hospedales", "T. Xiang", "S. Gong"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Transductive multi-label zero-shot learning", "author": ["Y. Fu", "Y. Yang", "T. Hospedales", "T. Xiang", "S. Gong"], "venue": "British Machine Vision Conference,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Borderline-smote: a new over-sampling method in imbalanced data sets learning", "author": ["H. Han", "W.-Y. Wang", "B.-H. Mao"], "venue": "Advances in intelligent computing, pages 878\u2013887. Springer,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Adasyn: Adaptive synthetic sampling approach for imbalanced learning", "author": ["H. He", "Y. Bai", "E.A. Garcia", "S. Li"], "venue": "Neural Networks, 2008. IJCNN 2008.(IEEE World Congress on Computational Intelligence). IEEE International Joint Conference on, pages 1322\u2013 1328. IEEE,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "Recognizing human actions by attributes", "author": ["J. Liu", "B. Kuipers", "S. Savarese"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Ensemble of exemplarsvms for object detection and beyond", "author": ["T. Malisiewicz", "A. Gupta", "A.A. Efros"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning from one example through shared densities on transforms", "author": ["E.G. Miller", "N.E. Matsakis", "P.A. Viola"], "venue": "Computer Vision and Pattern Recognition, 2000. Proceedings. IEEE Conference on, volume 1, pages 464\u2013471. IEEE,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "International Conference on Machine Learning,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "A threshold selection method from gray level histograms", "author": ["N. OTSU"], "venue": "IEEE Trans. Syst. Man Cybern., 9(1):62\u201366,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1979}, {"title": "Zeroshot learning with semantic output codes", "author": ["M. Palatucci", "G. Hinton", "D. Pomerleau", "T.M. Mitchell"], "venue": "Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Data and Knowledge Engineering, 22(10):1345\u20131359,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Remote Sensing Digital Image Analysis \u2013 An introduction", "author": ["J.A. Richards"], "venue": "Springer Berlin Heidelberg,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluating knowledge transfer and zero-shot learning in a large-scale setting", "author": ["M. Rohrbach", "M. Stark", "B. Schiele"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "What helps where \u2013 and why? semantic relatedness for knowledge transfer", "author": ["M. Rohrbach", "M. Stark", "G. Szarvas", "I. Gurevych", "B. Schiele"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 910\u2013917,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to share visual appearance for multiclass object detection", "author": ["R. Salakhutdinov", "A. Torralba", "J. Tenenbaum"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Combining subclassifiers in text categorization: A dst-based solution and a case study", "author": ["K. Sarinnapakorn", "M. Kubat"], "venue": "IEEE Trans. on Knowl. and Data Eng., 19(12):1638\u20131651, Dec.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["R. Socher", "M. Ganjoo", "H. Sridhar", "O. Bastani", "C.D. Manning", "A.Y. Ng"], "venue": "Neural Information Processing Systems,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning To Learn: Introduction", "author": ["S. Thrun"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1996}, {"title": "Visualizing high-dimensional data using t-SNE", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2008}, {"title": "Effects of training set expansion in handwriting recognition using synthetic data", "author": ["T. Varga", "H. Bunke"], "venue": "In 11th Conf. of the International Graphonomics Society. Citeseer,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "Comparing natural and synthetic training data for off-line cursive handwriting recognition", "author": ["T. Varga", "H. Bunke"], "venue": "Frontiers in Handwriting Recognition, 2004. IWFHR-9 2004. Ninth International Workshop on, pages 221\u2013225. IEEE,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2004}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "International Conference on Machine Learning,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research, 11:3371\u20133408,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, ICML \u201909, pages 1113\u20131120, New York, NY, USA,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Action recognition by learning bases of action attributes and parts", "author": ["B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L.J. Guibas", "L. Fei- Fei"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Designing category-level attributes for discriminative visual recognition", "author": ["F.X. Yu", "L. Cao", "R.S. Feris", "J.R. Smith", "S.-F. Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "A learning-based approach for automated quality assessment of computer-rendered images", "author": ["X. Zhang", "G. Agam"], "venue": "IS&T/SPIE Electronic Imaging. International Society for Optics and Photonics,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Alignment of 3d building models with satellite images using extended chamfer matching", "author": ["X. Zhang", "G. Agam", "X. Chen"], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning from synthetic models for roof style classification in point clouds", "author": ["X. Zhang", "A. Zang", "G. Agam", "X. Chen"], "venue": "Proceedings of the 22nd ACM SIGSPATIAL international conference on Advances in geographic information systems. ACM,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and extensible building modeling from airborne lidar data", "author": ["Q.-Y. Zhou", "U. Neumann"], "venue": "Proceedings of the 16th ACM SIGSPATIAL international conference on Advances in geographic information systems, page 7. ACM,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 33, "context": "In terms of quantity, it has been shown that the amount of available training data, per object class, roughly follows a Zipf distribution [35].", "startOffset": 138, "endOffset": 142}, {"referenceID": 30, "context": "the comet images from Rosetta), require extensive and detailed expert user annotation [32], [48].", "startOffset": 86, "endOffset": 90}, {"referenceID": 46, "context": "the comet images from Rosetta), require extensive and detailed expert user annotation [32], [48].", "startOffset": 92, "endOffset": 96}, {"referenceID": 47, "context": "Large volume of LiDAR point cloud data have to be labeled before they can be used to train some classifiers [49].", "startOffset": 108, "endOffset": 112}, {"referenceID": 20, "context": "To solve the problem of lacking enough training samples, attributes [22], [30], [13] have been introduced to transfer the knowledge held by majority classes to instances in minority classes.", "startOffset": 68, "endOffset": 72}, {"referenceID": 28, "context": "To solve the problem of lacking enough training samples, attributes [22], [30], [13] have been introduced to transfer the knowledge held by majority classes to instances in minority classes.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "To solve the problem of lacking enough training samples, attributes [22], [30], [13] have been introduced to transfer the knowledge held by majority classes to instances in minority classes.", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define.", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define.", "startOffset": 74, "endOffset": 78}, {"referenceID": 44, "context": "Nevertheless, for certain tasks, such shared attributes [14], [12], [25], [10], [46] may simply be unavailable or nontrivial to define.", "startOffset": 80, "endOffset": 84}, {"referenceID": 36, "context": "In contrast, rather than using such a \u2018learning to learn\u2019 [38] framework, humans can generalize and associate the similar patterns from images.", "startOffset": 58, "endOffset": 62}, {"referenceID": 24, "context": "In the computing domain, exemplar SVM [26] tries to associate images with training exemplars.", "startOffset": 38, "endOffset": 42}, {"referenceID": 45, "context": "A large number of synthetic 3D meshes in [47] were created by a series of mesh editing steps including subdivision, simplification, smooth, adding noise and Poisson reconstruction, in order to automatically evaluate the subjective visual quality of a 3D object.", "startOffset": 41, "endOffset": 45}, {"referenceID": 47, "context": "Recently, to circumvent the point labeling difficulty in a building roof classification problem using LiDAR point cloud, [49] explicitly indicated semantic roof points on synthetically created roof point clouds and compute point features from the synthetic point clouds.", "startOffset": 121, "endOffset": 125}, {"referenceID": 38, "context": "Previous method generate synthetic data in data space using tools including geometrical transformation and degradation models: In [40][41], to help off-line recognition of handwritten text, a perturbation model combined with morphological operation is applied to real data.", "startOffset": 130, "endOffset": 134}, {"referenceID": 39, "context": "Previous method generate synthetic data in data space using tools including geometrical transformation and degradation models: In [40][41], to help off-line recognition of handwritten text, a perturbation model combined with morphological operation is applied to real data.", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "To enhance the quality of degraded document, in [2] degradation models such as brightness degradation, blurring degradation, noise degradation and textureblending degradation were used to create a training dataset for a handwritten text recognition problem.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "The synthetic minority oversampling technique (SMOTE) [8] and its variants [19][20] are also powerful methods that have shown many success in various applications.", "startOffset": 54, "endOffset": 57}, {"referenceID": 17, "context": "The synthetic minority oversampling technique (SMOTE) [8] and its variants [19][20] are also powerful methods that have shown many success in various applications.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "The synthetic minority oversampling technique (SMOTE) [8] and its variants [19][20] are also powerful methods that have shown many success in various applications.", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 106, "endOffset": 109}, {"referenceID": 34, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 135, "endOffset": 139}, {"referenceID": 21, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 192, "endOffset": 196}, {"referenceID": 20, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 198, "endOffset": 202}, {"referenceID": 14, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 204, "endOffset": 208}, {"referenceID": 43, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 210, "endOffset": 214}, {"referenceID": 13, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 216, "endOffset": 220}, {"referenceID": 15, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 222, "endOffset": 226}, {"referenceID": 31, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 228, "endOffset": 232}, {"referenceID": 32, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 234, "endOffset": 238}, {"referenceID": 35, "context": "Transfer learning has been found helpful in many real world problems, such as in sentiment classification [6], web page classification [36] and zeroshot classification of image and video data [23], [22], [16], [45], [15], [17], [33], [34], [37].", "startOffset": 240, "endOffset": 244}, {"referenceID": 29, "context": "Transfer learning is categorized to three classes [31]: inductive transfer learning, transductive transfer learning and unsupervised transfer learning.", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "The work in this paper falls into a framework of domain adaptation [4], [44] in the transductive transfer learning.", "startOffset": 67, "endOffset": 70}, {"referenceID": 42, "context": "The work in this paper falls into a framework of domain adaptation [4], [44] in the transductive transfer learning.", "startOffset": 72, "endOffset": 76}, {"referenceID": 40, "context": "input vectors [42].", "startOffset": 14, "endOffset": 18}, {"referenceID": 7, "context": "Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks.", "startOffset": 49, "endOffset": 52}, {"referenceID": 16, "context": "Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks.", "startOffset": 54, "endOffset": 58}, {"referenceID": 1, "context": "Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks.", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks.", "startOffset": 177, "endOffset": 180}, {"referenceID": 9, "context": "Recently autoencoder with its different variants [9], [18] also exhibit the success in learning and transferring sharing knowledge among data source from different domains [3], [5], [11], thus benefit other machine learning tasks.", "startOffset": 182, "endOffset": 186}, {"referenceID": 41, "context": "By setting different types of input and out data such as the one in denoising autoencoder [43], MCAE is capable for many applications.", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[49].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Congealing algorithm proposed in [27] is employed in this step to produce the synthetic prototypes for digits.", "startOffset": 33, "endOffset": 37}, {"referenceID": 47, "context": "In our experiments, classification of the roof images is essentially similar to that of [49].", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "To generate the intermediate images, we binarize all the images and the distance transformed images[7] of the synthetic prototype and the real image are generated.", "startOffset": 99, "endOffset": 102}, {"referenceID": 46, "context": "Given a satellite image, we employ a method described in [48] to crop roof images by registering artificial building footprints with the satellite image.", "startOffset": 57, "endOffset": 61}, {"referenceID": 48, "context": "Later, all roof images are aligned using their footprint principal directions using a method proposed in [50] and then are scaled to images with resolution of 128\u00d7256.", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "We employed the adaptive Otsu edge detection method [29] to extract edges from the roof images.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "In our experiments we build a LeNet-5 [24] which is originally created for digit recognition.", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "To better evaluate the performance of the proposed MCAE, we compare MCAE with Concatenate-Input Autoencoder (CIAE) [28] and Sparse Autoencoder (SAE) [43].", "startOffset": 115, "endOffset": 119}, {"referenceID": 41, "context": "To better evaluate the performance of the proposed MCAE, we compare MCAE with Concatenate-Input Autoencoder (CIAE) [28] and Sparse Autoencoder (SAE) [43].", "startOffset": 149, "endOffset": 153}, {"referenceID": 37, "context": "t-SNE [39]visualization of synthetic gap bridged by MCAE.", "startOffset": 6, "endOffset": 10}], "year": 2015, "abstractText": "We propose a method for using synthetic data to help learning classifiers. Synthetic data, even is generated based on real data, normally results in a shift from the distribution of real data in feature space. To bridge the gap between the real and synthetic data, and jointly learn from synthetic and real data, this paper proposes a Multichannel Autoencoder(MCAE). We show that by suing MCAE, it is possible to learn a better feature representation for classification. To evaluate the proposed approach, we conduct experiments on two types of datasets. Experimental results on two datasets validate the efficiency of our MCAE model and our methodology of generating synthetic data.", "creator": "LaTeX with hyperref package"}}}