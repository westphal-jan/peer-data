{"id": "1503.05187", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2015", "title": "An Outlier Detection-based Tree Selection Approach to Extreme Pruning of Random Forests", "abstract": "random forest ( rf ) is an internet ensemble classification technique that really was developed by pierre breiman kobayashi over several a decade ago. accurately compared with other ensemble techniques, it easily has proved easily its dubious accuracy and superiority. many emerging researchers, alone however, believe that there is still room for feedback enhancing and improving, its performance in subjective terms of predictive accuracy. this explains why, first over considering the past multiple decade, globally there have been comparatively many extensions of rf where indeed each further extension employed depicts a variety choice of techniques and differing strategies individually to improve certain aspect ( s ) aspects of supporting rf. since 2000s it has been already proven empirically that numerical ensembles would tend to uniformly yield better results when demonstrated there is a significant diversity among the constituent optimization models, the objective of this paper collection is significant twofolds. secondly first, why it considerably investigates precisely how long an inexpensive unsupervised reinforcement learning technique, namely, local outlier factor ( lof ) can be sufficiently used to identify diverse trees in exploring the matching rf. ( second, trees with the highest lof efficiency scores of are deliberately then used individually to produce an extension of rf capability termed lofb - compatible drf that is much rather smaller in output size anyway than rf, and importantly yet performs reconstruction at at least as good as rf, but mostly each exhibits infinitely higher performance costs in empirical terms of accuracy. the latter refers mostly to a slightly known interaction technique called ensemble pruning. numerical experimental simulation results on computing 10 real datasets largely prove almost the actual superiority point of our recently proposed extension over the above traditional rf. unprecedented pruning capacity levels reaching 99 % have been traditionally achieved after at the time standard of frequency boosting reduces the predictive ability accuracy needed of defining the ensemble. the notably smaller high sampled pruning level makes the induction technique particularly a good candidate for real - time applications.", "histories": [["v1", "Tue, 17 Mar 2015 11:05:31 GMT  (60kb,D)", "http://arxiv.org/abs/1503.05187v1", "21 pages, 4 Figures. arXiv admin note: substantial text overlap witharXiv:1503.04996"]], "COMMENTS": "21 pages, 4 Figures. arXiv admin note: substantial text overlap witharXiv:1503.04996", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["khaled fawagreh", "mohamad medhat gaber", "eyad elyan"], "accepted": false, "id": "1503.05187"}, "pdf": {"name": "1503.05187.pdf", "metadata": {"source": "CRF", "title": "An Outlier Detection-based Tree Selection Approach to Extreme Pruning of Random Forests", "authors": ["Khaled Fawagreh", "Mohamed Medhat Gaber", "Eyad Elyan"], "emails": ["@rgu.ac.uk"], "sections": [{"heading": null, "text": "Random Forest (RF) is an ensemble classification technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance in terms of predictive accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofolds. First, it investigates how an unsupervised learning technique, namely, Local Outlier Factor (LOF) can be used to identify diverse trees in the RF. Second, trees with the highest LOF scores are then used to produce an extension of RF termed LOFB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, but mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 10 real datasets prove the superiority of our proposed extension over the traditional RF. Unprecedented pruning levels reaching as high as 99% have been achieved at the time of boosting the predictive accuracy of the ensemble. The notably high pruning level makes the technique a good candidate for real-time applications.\nKeywords: Random Forest, Local Outlier Factor, Diversity, Clustering, Ensemble Pruning\nPreprint submitted to Applied Soft Computing March 19, 2015\nar X\niv :1\n50 3.\n05 18\n7v 1\n[ cs\n.L G\n] 1"}, {"heading": "1. Introduction", "text": "Ensemble classification is an application of ensemble learning to boost the accuracy of classification. Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3]. Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2]. In such an ensemble, multiple classifiers are used. In its basic mechanism, majority voting is then used to determine the class label for unlabeled instances where each classifier in the ensemble is asked to predict the class label of the instance being considered. Once all the classifiers have been queried, the class that receives the greatest number of votes is returned as the final decision of the ensemble.\nThree widely used ensemble approaches could be identified, namely, boosting, bagging, and stacking. Boosting is an incremental process of building a sequence of classifiers, where each classifier works on the incorrectly classified instances of the previous one in the sequence. AdaBoost [6] is the representative of this class of techniques. However, AdaBoost is proned to overfitting. The other class of ensemble approaches is the Bootstrap Aggregating (Bagging) [7]. Bagging involves building each classifier in the ensemble using a randomly drawn sample of the data with replacement, having each classifier give an equal vote when labeling unlabeled instances. Bagging is known to be more robust than boosting against model overfitting. Random Forest (RF) is the main representative of bagging [8]. Stacking (sometimes called stacked generalization) extends the cross-validation technique that partitions the data set into a held-in data set and a held-out data set; training the models on the held-in data; and then choosing whichever of those trained models performs best on the held-out data. Instead of choosing among the models, stacking combines them, thereby typically getting performance better than any single one of the trained models [9]. Stacking has been successfully used in both supervised learning tasks (regression) [10], and unsupervised learning (density estimation) [11].\nThe ensemble method that is relevant to our work in this paper is RF. RF has been proved to be the state-of-the-art ensemble classification technique. Since RF algorithms typically build between 100 and 500 trees [12], it would be useful to reduce the number of trees participating in majority voting\nand yet achieving better performance both in terms of accuracy and speed. In this paper, we propose an unsupervised learning approach to improve speed and accuracy of RF. For speed, our approach avoids having all trees participate in majority voting as only a small subset of the trees is selected. For accuracy, since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [13] [14] [15], our approach ensures that diverse trees in the ensemble are selected. We adopted Local Outlier Factor for tree diversification. Hence, the method is termed Local Outlier Factor Based Diversified Random Forest (both LOFB-DRF and LOF-DRF are used interchangeably) .\nThis paper is organized as follows. First we discuss related work in Section 2. This is followed by Section 3 where the motivation and an introduction to RF are covered. Section 4 describes the Local Outlier Factor that will be utilized in our proposed extension of RF. Section 5 formalizes our proposed method and corresponding algorithm. Experimental study demonstrating the superiority of the proposed technique over the traditional RF is detailed in Section 6. The paper is then concluded with a summary and pointers to future directions in Section 7."}, {"heading": "2. Related Work", "text": "Several attempts have been made in recent years in order to produce a subset of an ensemble that performs as well as, or better than, the original ensemble. The purpose of ensemble pruning is to search for such a good subset. This is particularly useful for large ensembles that require extra memory usage, computational costs, and occasional decreases in effectiveness. Grigorios et al. [16] recently amalgamated a survey of ensemble pruning techniques where they classified such techniques into four categories: ranking based, clustering based, optimization based, and others. Ranking based methods, that are relevant to us in this paper, are conceptually the simplest. Since using the predictive performance to rank models is too simplistic and does not yield satisfying results [17] [18], ranking based methods employ an evaluation measure to rank models. Kappa statistic measure \u03ba was used in [19] for pruning AdaBoost ensembles. For bagging ensembles, however, kappa has proven to be non-competitive [20]. For bagging ensembles, [21] developed an efficient and effective pruning method based on orientation ordering where the classifiers obtained from bagging are reordered and a subset is selected for aggregation.\nAn interesting issue that remains after ranking the models is to determine the models that will be chosen to form the pruned ensemble. For this, two approaches can be used. The first approach is to use a fixed user-specified amount or percentage of models. A second approach is to dynamically select the size based on the evaluation measure or the predictive performance of ensembles of different sizes. In this paper, the models will be ranked according to their Local Outlier Factor (LOF) values and the models with the top k (where k is a multiple of 5 ranging from 5 to 40) values will be selected to form the pruned ensemble."}, {"heading": "2.1. Diversity Creation Methods", "text": "Because of the vital role diversity plays on the performance of ensembles, it had received a lot of attention from the research community. G. Brown et al. [13] summarized the work done to date in this domain from two main perspectives. The first is a review of the various attempts that were made to provide a formal foundation of diversity. The second, which is more relevant to this paper, is a survey of the various techniques to produce diverse ensembles. For the latter, two types of diversity methods were identified: implicit and explicit. While implicit methods tend to use randomness to generate diverse trajectories in the hypothesis space, explicit methods, on the other hand, choose different paths in the space deterministically. In light of these definitions, bagging and boosting in the previous section are classified as implicit and explicit respectively.\nG. Brown et al. [13] also categorized ensemble diversity techniques into three categories: starting point in hypothesis space, set of accessible hypotheses, and manipulation of training data. Methods in the first category use different starting points in the hypothesis space, therefore, influencing the convergence place within the space. Because of their poor performance of achieving diversity, such methods are used by many authors as a default benchmark for their own methods [5]. Methods in the second category vary the set of hypotheses that are available and accessible by the ensemble. For different ensembles, these methods vary either the training data used or the architecture employed. In the third category, the methods alter the way space is traversed. Occupying any point in the search space, gives a particular hypothesis. The type of the ensemble obtained will be determined by how the space of the possible hypotheses is traversed.\nIn this paper, we propose a new diversity creation method based on unsupervised learning. The method utilizes an existing unsupervised learning\ntechnique that, to the best of our knowledge, has not been used before in the production of pruned ensembles."}, {"heading": "2.2. Diversity Measures", "text": "Regardless of the diversity creation technique used, diversity measures were developed to measure the diversity of a certain technique or perhaps to compare the diversity of two techniques. Tang et al. [15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25]. The goal was not only to show the underlying relationships between them, but also to relate them to the concept of margin, which is one of the contributing factors to the success of ensemble learning algorithms.\nWe suffice to describe the first two measures as the others are outside the scope of this paper. The disagreement measure is used to measure the diversity between two base classifiers hj and hk, and is calculated as follows:\ndisj,k = N10 +N01\nN11 +N10 +N01 +N00\nwhere\n\u2022 N10: means number of training instances that were correctly classified by hj, but are incorrectly classified by hk\n\u2022 N01: means number of training instances that were incorrectly classified by hj, but are correctly classified by hk\n\u2022 N11: means number of training instances that were correctly classified by hj and hk\n\u2022 N00: means number of training instances that were incorrectly classified by hj and hk\nThe higher the disagreement measure, the more diverse the classifiers are. The double fault measure uses a slightly different approach where the diversity between two classifiers is calculated as:\nDFj,k = N00\nN11 +N10 +N01 +N00\nThe above two diversity measures work only for binary classification (AKA binomial) where there are only two possible values (like Yes/No) for the class label, hence, the objects are classified into exactly two groups. They do not work for multiclass (AKA multinomial) classification where the objects are classified into more than two groups."}, {"heading": "3. Preliminaries", "text": ""}, {"heading": "3.1. Motivation", "text": "As mentioned before, RF algorithms tend to build between 100 and 500 trees [12]. Our research aims at producing child RFs that are significantly smaller in size and yet, have accuracy performance that is at least as good as that of the parent RF from which they were derived. The classification speed of each child is guaranteed to be much faster than that of the parent RF because 1) it has much fewer trees and 2) any tree used in the child is also in the parent (i.e., no new trees were introduced in the child)."}, {"heading": "3.2. Random Forest", "text": "RF is an ensemble learning method used for classification and regression. Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation. Using bagging, each decision tree in the ensemble is constructed using a sample with replacement from the training data. Statistically, the sample is likely to have about 64% of instances appearing at least once in the sample. Instances in the sample are referred to as in-bag-instances, and the remaining instances (about 36%), are referred to as out-of-bag instances. Each tree in the ensemble acts as a base classifier to determine the class label of an unlabeled instance. This is done via majority voting where each classifier casts one vote for its predicted class label, then the class label with the most votes is used to classify the instance. Algorithm 1 below depicts the RF algorithm [8] where N is the number of training samples and S is the number of features in data set."}, {"heading": "4. Local Outlier Factor", "text": "The Local Outlier Factor (LOF) algorithm was developed by Breunig et al. [30] to measure the outlierness of an object. The higher the LOF value\nAlgorithm 1 Random Forest Algorithm\n{User Settings} input N , S {Process} Create an empty vector \u2212\u2192 RF for i = 1\u2192 N do Create an empty tree Ti repeat\nSample S out of all features F using Bootstrap sampling Create a vector of the S features \u2212\u2192 FS Find Best Split Feature B( \u2212\u2192 FS) Create A New Node using B( \u2212\u2192 FS) in Ti\nuntil No More Instances To Split On Add Ti to the \u2212\u2192 RF\nend for {Output} A vector of trees \u2212\u2192 RF\nassigned to an object, the more isolated the object is with respect to its neighbors. It is considered a very powerful anomaly detection technique in machine learning and classification. Earlier work on outlier detection was investigated in [31] [32] [33] [34], however, the work was limited by treating an outlier as a binary property to classify an object as an outlier or not, without assigning it a value to measure its outlierness as was done in [30].\nThe LOF can be used as a method to achieve diversity. It was one of 3 strategies used to obtain diversity when constructing an ensemble for the KDDCup 1999 dataset [35]. Schubert et al. [36] proposed methods for measuring similarity and diversity of methods for building advanced outlier detection ensembles using LOF variants and other algorithms.\nFormally, Breunig et al. [30] introduced the concept of reachability distance in order to calculate the LOF. If the distance of object A to the k nearest neighbor is denoted by k-distance(A), where the k nearest neighbors is denoted by Nk(A), the following equation defines the reachability distance (rd):\nrdk(A,B) = max{k\u2212distance(B), d(A,B)} (1)\nwhere d(A,B) is the distance between objects A and B. The local reachability density of object A is then defined by\nlrd(A) =\n\u2211 B\u2208Nk(A) rdk(A,B)\n|Nk(A)| (2)\nUsing the local reachability density of object A as defined in the previous equation, the LOF for object A is given by:\nLOFk(A) =\n\u2211 B\u2208Nk(A) lrd(B) lrd(A)\n|Nk(A)| (3)"}, {"heading": "5. LOF-Based Diverse Random Forest (LOFB-DRF)", "text": "In this section, we propose an extension of RF called LOFB-DRF that spawns a child RF that is 1) much smaller in size than the parent RF and 2) has an accuracy that is at least as good as that of the parent RF. In this extension, we use the LOF discussed in Section 4. As shown in Figure 1, each tree predictions on the training dataset (denoted by the vector C(ti, T )) is assigned an LOF value that indicates the degree of its outlierness. The top k (k=5,10,...,40) trees corresponding to these predictions with the highest weighted LOF values (to be discussed next) are then selected to become members of the resulting LOFB-DRF. In the remainder of this paper, we will refer to the parent/original traditional Random Forest as RF, and refer to the resulting child RF based on our method as LOFB-DRF.\nBased on Figure 1, we formalize the LOFB-DRF algorithm as shown in Algorithm 2 where T is the training set. The constant k refers to the number of trees that will have the highest weighted LOF values as will be discussed later. The domain of this constant is multiple of 5 in the range 5 to 40. This way and as we shall see in the experiments section, we can compare the performance RF with an LOFB-DRF of different sizes.\nIt is important to remember that the size of the resulting LOFB-DRF is determined by the constant k. For example, if k is 5, then the resulting LOFB-DRF will have size 5, and so on."}, {"heading": "5.1. Selection of Trees", "text": "With reference to Algorithm 2, the selection of trees in RF that will become members of LOFB-DRF proceeds as follows. First, predictions of each\nAlgorithm 2 LOFB-DRF Algorithm\n{User Settings} input T , k {Process} Create an empty vector \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 treesPredictions Create an empty vector \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 LOFB \u2212RF Using T, call Algorithm 1 above to create the parent RF for i = 1\u2192 RF.getNumTrees() do \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 treesPredictions \u21d0 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 treesPredictions \u222a C(RF.tree(i), T) end for For each instance in \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 treesPredictions, assign an LOF value Select the top k instances in \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 treesPredictions with highest weighted LOF values Select the corresponding trees from RF and add them to \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 LOFB \u2212DRF {Output} A vector of trees \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 LOFB \u2212DRF\ntree on the training dataset T is computed as a vector and added to the vector \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 treesPredictions. At the conclusion of the for loop, \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 treesPredictions becomes a super vector containing vectors where each vector stores the predictions of each tree. Each instance in \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 treesPredictions is then assigned a normalized LOF value between 0 and 1. This way, each normalized value describes the probability of the instance being an outlier [35]. Then we assign to each instance a weight that is the product of the normalized LOF value and the accuracy rate of the corresponding tree on the training data. Formally, let ci be an instance in the super vector \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 treesPredictions, NormalizedLOF(ci) be the normalized LOF value assigned to this instance, and AccuracyRate(Tree(ci),T) be the accuracy rate of Tree(ci) on the training dataset T where Tree(ci) is the tree that corresponds to the instance ci. The weight assigned to this instance is given by:\nweight = NormalizedLOF (ci)\u00d7 AccuracyRate(Tree(ci), T ) (4)\nThe instances are then sorted in descending order by this weight and the corresponding top k trees are then selected."}, {"heading": "5.2. Diversity Measure", "text": "Here we propose a simple diversity measure to measure the diversity of classifiers that works with binary and multiclass classification. Given two classifiers hj and hk and a training set T of size n. Let C(tl,si) denotes the class label obtained after having tl classify the sample si in the training set T. The diversity between the two classifiers can be measured by:\ndiversityj,k =\nn\u2211 i=1 \u03b4(C(tj, ci), C(tk, ci))\nn (5)\nwhere\n\u03b4(xj, yj) =\n{ 0, if xj = yj\n1, otherwise (6)\nThe higher the number of discrepancies between the two classifiers, the higher the diversity is. For example, assume that we have a training set consisting of 10 training samples T={s1,s2,s3,s4,s5,s6,s7,s8,s9,s10}, and two classifiers t1 and t2. Assume also that there are 3 possible values for the class label {a,b,c}. Let C(t1,T)=<a,a,b,c,c,a,b,c,b,b> and C(t2,T)=<a,a,b,b,a,a,b,c,c,c>. According to 5 above, the diversity between the two classifiers is therefore 4/10 or 40%."}, {"heading": "6. Experiments", "text": "For our experiments, we have used 10 real datasets with varying characteristics from the UCI repository [37]. To use the holdout testing method, each dataset was divided into 2 sets: training and testing. Two thirds (66%) were reserved for training and the rest (34%) for testing. Each dataset consists of input variables (features) and an output variable. The latter refers to the class label whose value will be predicted in each experiment. For the RF in Figure 1, the initial RF to produce the LOFB-DRF had a size of 500 trees, a typical upper limit setting for RF [12].\nThe LOFB-DRF algorithm described above was implemented using the Java programming language utilizing the API of Waikato Environment for Knowledge Analysis (WEKA) [38]. We ran this algorithm 10 times on each dataset where a new RF was created in each run. We then calculated the average of the 10 runs for each resulting LOFB-DRF to produce the average for a variety of metrics including accuracy rate, minimum accuracy rate, maximum accuracy rate, standard deviation, FMeasure, and AUC as shown in Table 5. For the RF, we just calculated the average accuracy rate, FMeasure, and AUC as shown in the last 3 columns of the table."}, {"heading": "6.1. Results", "text": "Table 5 compares the performance of LOFB-DRF and RF on the 10 datasets used in the experiment. To show the superiority of LOFB-DRF, we have highlighted in boldface the average accuracy rate of LOFB-DRF when it is greater than that of RF. With the exception of the audit and vote datasets (last 2 datasets), we find that LOFB-DRF performed at least as good as RF. Interestingly enough, of the 10 datasets, LOFB-DRF, regardless of its size, completely outperformed RF on 3 of the datasets, namely, squash-stored, eucalyptus, and sonar. While LOFB-DRF lost to RF on only 2 datasets\n(audit and vote), the difference was by a very small negligible fraction of less than 1% (in the case of audit), and less than 1.2% (in the case of vote)!"}, {"heading": "6.2. Pruning Level", "text": "In ensemble pruning, a pruning level refers to the reduction ratio between the original ensemble and the pruned one. For example, if the size of the original ensemble is 500 trees and the pruned one is of size 50, then 100%\u2212 50\n500 \u00d7100% = 90% is the pruning level that was achieved in the pruned\nensemble. This means that the pruned ensemble is 90% smaller than the original one. Table 1 shows the pruning levels where the first column shows the maximum possible pruning level for an LOFB-DRF that has outperformed RF, and the second column shows the pruning level of the best performer LOFB-DRF. We can see that with extremely healthy pruning levels ranging from 95% to 99%, our technique outperformed RF. This makes LOFB-DRF a natural choice for real-time applications, where fast classification is an important desideratum. In most cases, 100 times faster classification can be achieved with the 99% pruning level, as shown in the table. In the worst case scenario, only 16.67 times faster classification with 95% pruning level in the squash-unstored dataset. Such estimates are based on the fact that the number of trees traversed in the RF is the dominant factor in the classification response time. This is especially true, given that RF trees are unpruned bushy trees.\nNote that the audit and vote datasets were not listed in the table as the RFs for these datasets (refer to the last 2 datasets in Table 5) outperformed all LOFB-DRFs, however, by a very small amount as shown in Table 2."}, {"heading": "6.3. Analysis", "text": "By showing the number of datasets each was superior on, Figure 2 compares the accuracy rate of RF and LOFB-DRF using different sizes of LOFBDRF. For sizes 10, 15, 20, and 25, the figure clearly shows that LOFB-DRF indeed performed at least as good as RF. As shown in Table 2 below, for the cases (size 5, 30, 35, and 40) where RF outperformed LOFB-DRF, the difference was very small, considering the pruning level that was achieved.\nTable 2: Outperformance Range of RF Over LOFB-DRF\nLOFB-DRF Size 5 30 35 40 Range 0.31% - 4.12% 0.08% - 2.78% 0.05% - 1.45% 0.31% - 3.33%\nPruning Level 99% 94% 93% 92%\n0\n2\n4\n6\n10 20 30 40 Size (Number of Trees)\nNu mb\ner of\nD ata\nse ts\nMethod\nLOF\u2212DRF RF\nFigure 2: Accuracy Rate Comparison of RF & LOFB-DRF"}, {"heading": "6.4. Bias/Variance Analysis", "text": "Bias and variance are measures used to estimate the accuracy of a classifier [39]. The bias measures the difference between the classifier\u2019s predicted class value and the true value of the class label being predicted. The variance, on the other hand, measures the variability of the classifier\u2019s prediction as a result of sensitivity due to fluctuations in the training set. If the prediction is always the same regardless of the training set, it equals zero. However, as the prediction becomes more sensitive to the training set, the variance tends to increase. For a classifier to be accurate, it should maintain a low bias and variance.\nThere is a trade-off between a classifier\u2019s ability to minimize bias and variance. Understanding these two types of measures can help us diagnose classifier results and avoid the mistake of over- or under-fitting. Breiman et al. [40] provided an analysis of complexity and induction in terms of a tradeoff between bias and variance. In this section, we will show that LOFB-DRF can have a bias and variance comparable to and even better than RF. Starting with bias, the first column in Table 3 shows the pruning level of LOFBDRF that performed the best relative to RF, and the second column shows the pruning level of the smallest LOFB-DRF that outperformed RF. As demonstrated in the table, LOFB-DRF has outperformed RF on all datasets. On the other hand, Table 4 shows similar results but variance-wise. Once again, LOFB-DRF has outperformed RF on all datasets. Although looking at bias in isolation of variance (and vice versa) provides only half of the picture, our aim is to demonstrate that with a pruned ensemble, both bias and/or variance can be enhanced. We attribute this to the high diversity our ensemble exhibits.\nWe have also conducted experiments to compare the bias and variance between LOFB-DRFs and Random Forests of identical size. Figure 3 compares the bias and Figure 4 compares the variance. Both figures show that LOFB-DRF in most cases can have bias and variance equal to or better than Random Forest."}, {"heading": "7. Conclusion and Future Directions", "text": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [13] [14] [15]. We adopted the Local Outlier Factor method to select diverse trees in an RF and then used these trees to form a pruned ensemble of the original ensemble. The selection was based on both LOF value and predictive accuracy of each tree. Experimental results have shown the potential of this method with extreme pruning of Random Forests that can outperform the original population of trees with values reaching 99% pruning level. This makes the pruned ensemble a suitable candidate for real-time applications.\nWe have selected trees that correspond to the instances with the top k weighted LOF values. Another interesting variation would be to use a hybrid approach that combines LOF with clustering to boost diversity up. Using this approach, we first create clusters of trees then from each cluster, we select\na representative that corresponds to the instance with the highest weighted LOF value. The current implementation also gives equal importance to the peculiarity of the tree as measured by the LOF score and the predictive accuracy, represented by the percentage of correctly classified instances for the tree. However, tuning this significance can play an important role in enhancing the classifier. From one hand, choosing trees with higher predictive accuracy can lead to model overfitting, and on the other hand, using LOF only can lead to leaving out trees that are most representative of the dataset. Balancing between the two can result in an ensemble that is diverse enough to boost the accuracy.\nTable 5 \u2013 continued from previous page LOFB-DRF Size AVG MIN MAX SD Fmeasure AUC AVG FMeasure AUC 10 81.15 74.71 84.29 3.56 0.71 0.68 15 79.85 77.39 83.14 1.96 0.71 0.67 20 81.42 79.31 83.14 1.24 0.71 0.67 25 80.96 78.93 82.76 1.31 0.71 0.67 30 80.88 78.54 82.76 1.14 0.71 0.67 35 79.81 77.39 81.99 1.40 0.71 0.67 40 81.38 80.08 83.14 0.94 0.71 0.67 car 5 64.17 62.41 67.52 1.33 0.56 0.78 62.26 0.56 0.78 10 63.01 61.56 64.29 0.75 0.56 0.78 15 62.36 60.71 64.29 1.12 0.56 0.78 20 62.35 61.22 63.78 0.82 0.56 0.78 25 62.69 60.88 63.95 0.85 0.56 0.78 30 62.18 61.05 63.10 0.82 0.56 0.78 35 61.96 60.88 63.61 0.72 0.56 0.78 40 61.99 61.05 62.59 0.54 0.55 0.78 sonar 5 12.25 7.04 18.31 3.34 0.26 0.00 0.14 0.29 0.00 10 9.15 0.00 16.90 5.20 0.28 0.00 15 6.34 0.00 14.08 4.47 0.29 0.00 20 3.38 0.00 8.45 2.76 0.29 0.00 25 3.10 0.00 7.04 2.42 0.28 0.00 30 1.83 0.00 4.23 1.27 0.28 0.00 35 3.38 0.00 4.23 1.29 0.28 0.00 40 3.38 0.00 9.86 2.69 0.28 0.00 audit 5 95.63 94.26 96.47 0.72 0.91 0.89 96.31 0.90 0.88 10 95.74 95.00 96.18 0.35 0.90 0.88 15 95.99 95.29 96.47 0.35 0.90 0.88 20 96.06 95.29 96.76 0.39 0.90 0.88 25 96.22 95.88 96.47 0.25 0.91 0.89 30 96.03 95.59 96.47 0.25 0.90 0.88 35 96.26 95.88 96.47 0.18 0.90 0.88 40 96.00 95.59 96.47 0.27 0.90 0.87 vote 5 96.82 95.27 97.97 0.80 0.96 0.98 97.97 0.95 0.97 10 97.09 95.27 97.97 0.86 0.96 0.97 15 97.57 96.62 97.97 0.45 0.95 0.97 20 97.43 96.62 97.97 0.51 0.95 0.97 25 97.57 96.62 97.97 0.45 0.95 0.97 30 97.70 97.30 97.97 0.33 0.95 0.97 35 97.64 96.62 97.97 0.45 0.95 0.97 40 97.64 96.62 97.97 0.45 0.95 0.97"}], "references": [{"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "Circuits and Systems Magazine, IEEE 6 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Ensemble-based classifiers", "author": ["L. Rokach"], "venue": "Artificial Intelligence Review 33 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy", "author": ["L.I. Kuncheva", "C.J. Whitaker"], "venue": "Machine learning 51 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Popular ensemble methods: An empirical study", "author": ["R. Maclin", "D. Opitz"], "venue": "Journal Of Artificial Intelligence Research 11 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Journal of computer and system sciences 55 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine learning 24 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning 45 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural networks 5 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Stacked regressions", "author": ["L. Breiman"], "venue": "Machine learning 24 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Linearly combining density estimators via stacking", "author": ["P. Smyth", "D. Wolpert"], "venue": "Machine Learning 36 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Use R: Data Mining with Rattle and R: the Art of Excavating Data for Knowledge Discovery", "author": ["G. Williams"], "venue": "Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Diversity creation methods: a survey and categorisation", "author": ["G. Brown", "J. Wyatt", "R. Harris", "X. Yao"], "venue": "Information Fusion 6 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Accuracy and diversity in ensembles of text categorisers", "author": ["J.J.G. Adeva", "U. Beresi", "R. Calvo"], "venue": "CLEI Electronic Journal 9 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "An analysis of diversity measures", "author": ["E.K. Tang", "P.N. Suganthan", "X. Yao"], "venue": "Machine Learning 65 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "An ensemble pruning primer", "author": ["G. Tsoumakas", "I. Partalas", "I. Vlahavas"], "venue": "in: Applications of supervised and unsupervised ensemble methods, Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Engineering multiversion neural-net systems", "author": ["D. Partridge", "W.B. Yates"], "venue": "Neural Computation 8 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Ensemble selection for superparent-one-dependence estimators", "author": ["Y. Yang", "K. Korb", "K.M. Ting", "G.I. Webb"], "venue": "in: AI 2005: Advances in Artificial Intelligence, Springer", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "An analysis of ensemble pruning techniques based on ordered aggregation", "author": ["G. Martinez-Muoz", "D. Hern\u00e1ndez-Lobato", "A. Su\u00e1rez"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 31 ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Design of effective neural network ensembles for image classification purposes", "author": ["G. Giacinto", "F. Roli"], "venue": "Image and Vision Computing 19 ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "Statistical methods for rates and proportions", "author": ["J.L. Fleiss", "B. Levin", "M.C. Paik"], "venue": "John Wiley & Sons", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Software diversity: practical statistics for its measurement and exploitation", "author": ["D. Partridge", "W. Krzanowski"], "venue": "Information and software technology 39 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Random decision forests", "author": ["T.K. Ho"], "venue": "in: Document Analysis and Recognition", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1995}, {"title": "The random subspace method for constructing decision forests", "author": ["T.K. Ho"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 20 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Shape quantization and recognition with randomized trees", "author": ["Y. Amit", "D. Geman"], "venue": "Neural computation 9 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "Computing depth contours of bivariate point clouds", "author": ["I. Ruts", "P.J. Rousseeuw"], "venue": "Computational Statistics & Data Analysis 23 ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "On evaluation of outlier rankings and outlier scores", "author": ["E. Schubert", "R. Wojdanowski", "A. Zimek", "H.-P. Kriegel"], "venue": "in: Proceedings of the 12th SIAM International Conference on Data Mining (SDM), Anaheim, CA", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "M", "author": ["K. Bache"], "venue": "Lichman, Uci machine learning repository", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Eibe Frank", "author": ["G.H.B.P.P.R.I.H.W. Mark Hall"], "venue": "The WEKA Data Mining Software: An Update, volume 11, SIGKDD Explorations", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Classification and regression trees", "author": ["B. Leo", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": "Wadsworth International Group ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "Ensemble learning is a supervised machine learning paradigm where multiple models are used to solve the same problem [1] [2] [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 3, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "Since single classifier systems have limited predictive performance [4] [1] [5] [2], ensemble classification was developed to yield better predictive performance [1] [5] [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 4, "context": "AdaBoost [6] is the representative of this class of techniques.", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "The other class of ensemble approaches is the Bootstrap Aggregating (Bagging) [7].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Random Forest (RF) is the main representative of bagging [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Instead of choosing among the models, stacking combines them, thereby typically getting performance better than any single one of the trained models [9].", "startOffset": 149, "endOffset": 152}, {"referenceID": 8, "context": "Stacking has been successfully used in both supervised learning tasks (regression) [10], and unsupervised learning (density estimation) [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "Stacking has been successfully used in both supervised learning tasks (regression) [10], and unsupervised learning (density estimation) [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "Since RF algorithms typically build between 100 and 500 trees [12], it would be useful to reduce the number of trees participating in majority voting", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "For accuracy, since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [13] [14] [15], our approach ensures that diverse trees in the ensemble are selected.", "startOffset": 150, "endOffset": 153}, {"referenceID": 11, "context": "For accuracy, since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [13] [14] [15], our approach ensures that diverse trees in the ensemble are selected.", "startOffset": 154, "endOffset": 158}, {"referenceID": 12, "context": "For accuracy, since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [13] [14] [15], our approach ensures that diverse trees in the ensemble are selected.", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "For accuracy, since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the models [3] [13] [14] [15], our approach ensures that diverse trees in the ensemble are selected.", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "[16] recently amalgamated a survey of ensemble pruning techniques where they classified such techniques into four categories: ranking based, clustering based, optimization based, and others.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Since using the predictive performance to rank models is too simplistic and does not yield satisfying results [17] [18], ranking based methods employ an evaluation measure to rank models.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "Since using the predictive performance to rank models is too simplistic and does not yield satisfying results [17] [18], ranking based methods employ an evaluation measure to rank models.", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "For bagging ensembles, however, kappa has proven to be non-competitive [20].", "startOffset": 71, "endOffset": 75}, {"referenceID": 11, "context": "[13] summarized the work done to date in this domain from two main perspectives.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] also categorized ensemble diversity techniques into three categories: starting point in hypothesis space, set of accessible hypotheses, and manipulation of training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Because of their poor performance of achieving diversity, such methods are used by many authors as a default benchmark for their own methods [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 13, "context": "[15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25].", "startOffset": 122, "endOffset": 126}, {"referenceID": 19, "context": "[15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25].", "startOffset": 168, "endOffset": 172}, {"referenceID": 20, "context": "[15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25].", "startOffset": 196, "endOffset": 200}, {"referenceID": 19, "context": "[15] presented a theoretical analysis on six existing diversity measures: disagreement measure [22], double fault measure [23], KW variance [24], inter-rater agreement [25], generalized diversity [26], and measure of difficulty [25].", "startOffset": 228, "endOffset": 232}, {"referenceID": 10, "context": "As mentioned before, RF algorithms tend to build between 100 and 500 trees [12].", "startOffset": 75, "endOffset": 79}, {"referenceID": 6, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation.", "startOffset": 21, "endOffset": 24}, {"referenceID": 5, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation.", "startOffset": 82, "endOffset": 85}, {"referenceID": 21, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation.", "startOffset": 156, "endOffset": 160}, {"referenceID": 22, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation.", "startOffset": 161, "endOffset": 165}, {"referenceID": 23, "context": "Developed by Breiman [8], the method combines Breiman\u2019s bagging sampling approach [7], and the random selection of features, introduced independently by Ho [27] [28] and Amit and Geman [29], in order to construct a collection of decision trees with controlled variation.", "startOffset": 185, "endOffset": 189}, {"referenceID": 6, "context": "Algorithm 1 below depicts the RF algorithm [8] where N is the number of training samples and S is the number of features in data set.", "startOffset": 43, "endOffset": 46}, {"referenceID": 24, "context": "Earlier work on outlier detection was investigated in [31] [32] [33] [34], however, the work was limited by treating an outlier as a binary property to classify an object as an outlier or not, without assigning it a value to measure its outlierness as was done in [30].", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "[36] proposed methods for measuring similarity and diversity of methods for building advanced outlier detection ensembles using LOF variants and other algorithms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "For our experiments, we have used 10 real datasets with varying characteristics from the UCI repository [37].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "For the RF in Figure 1, the initial RF to produce the LOFB-DRF had a size of 500 trees, a typical upper limit setting for RF [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "The LOFB-DRF algorithm described above was implemented using the Java programming language utilizing the API of Waikato Environment for Knowledge Analysis (WEKA) [38].", "startOffset": 162, "endOffset": 166}, {"referenceID": 28, "context": "[40] provided an analysis of complexity and induction in terms of a tradeoff between bias and variance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [13] [14] [15].", "startOffset": 103, "endOffset": 106}, {"referenceID": 11, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [13] [14] [15].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [13] [14] [15].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "Research conducted in this paper was based on how diversity in ensembles tends to yield better results [3] [13] [14] [15].", "startOffset": 117, "endOffset": 121}], "year": 2015, "abstractText": "Random Forest (RF) is an ensemble classification technique that was developed by Breiman over a decade ago. Compared with other ensemble techniques, it has proved its accuracy and superiority. Many researchers, however, believe that there is still room for enhancing and improving its performance in terms of predictive accuracy. This explains why, over the past decade, there have been many extensions of RF where each extension employed a variety of techniques and strategies to improve certain aspect(s) of RF. Since it has been proven empirically that ensembles tend to yield better results when there is a significant diversity among the constituent models, the objective of this paper is twofolds. First, it investigates how an unsupervised learning technique, namely, Local Outlier Factor (LOF) can be used to identify diverse trees in the RF. Second, trees with the highest LOF scores are then used to produce an extension of RF termed LOFB-DRF that is much smaller in size than RF, and yet performs at least as good as RF, but mostly exhibits higher performance in terms of accuracy. The latter refers to a known technique called ensemble pruning. Experimental results on 10 real datasets prove the superiority of our proposed extension over the traditional RF. Unprecedented pruning levels reaching as high as 99% have been achieved at the time of boosting the predictive accuracy of the ensemble. The notably high pruning level makes the technique a good candidate for real-time applications.", "creator": "LaTeX with hyperref package"}}}