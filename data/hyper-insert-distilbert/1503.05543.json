{"id": "1503.05543", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2015", "title": "Text Segmentation based on Semantic Word Embeddings", "abstract": "whereas we explore employing the use of overlapping semantic word embeddings developed in text segmentation algorithms, including the algorithm c99 matrix segmentation adjustment algorithm and establishing new algorithms inspired by the distributed whole word binary vector sample representation. by developing a general framework techniques for continuously discussing a narrower class of segmentation comparison objectives, moreover we study the effectiveness of greedy targeting versus exact optimization modelling approaches carefully and suggest a new iterative refinement technique for completely improving the performance cost of efficiently greedy strategies. when we compare approximately our sorting results neatly to three known memory benchmarks, using known metrics. we often demonstrate complementary state - control of - affairs the - scale art iteration performance for compare an untrained method with our content weighted vector matrix segmentation ( cvs ) on the choi test set.. finally, we apply the software segmentation procedure to an in - the - wild research dataset consisting successively of existing text extracted from several scholarly blog articles placed in the nonprofit arxiv. org university database.", "histories": [["v1", "Wed, 18 Mar 2015 19:44:06 GMT  (913kb,D)", "http://arxiv.org/abs/1503.05543v1", "10 pages, 4 figures. KDD2015 submission"]], "COMMENTS": "10 pages, 4 figures. KDD2015 submission", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["alexander a alemi", "paul ginsparg"], "accepted": false, "id": "1503.05543"}, "pdf": {"name": "1503.05543.pdf", "metadata": {"source": "CRF", "title": "Text Segmentation based on Semantic Word Embeddings", "authors": ["Alexander A Alemi", "Paul Ginsparg"], "emails": ["aaa244@cornell.edu", "ginsparg@cornell.edu"], "sections": [{"heading": null, "text": "Categories and Subject Descriptors I.2.7 [Natural Language Processing]: Text Analysis\nGeneral Terms Information Retrieval, Clustering, Text\nKeywords Text Segmentation, Text Mining, Word Vectors"}, {"heading": "1. INTRODUCTION", "text": "Segmenting text into naturally coherent sections has many useful applications in information retrieval and automated text summarization, and has received much past attention. An early text segmentation algorithm was the TextTiling method introduced by Hearst [11] in 1997. Text was scanned linearly, with a coherence calculated for each adjacent block, and a heuristic was used to determine the locations of cuts. In addition to linear approaches, there are text segmentation algorithms that optimize some scoring objective. An\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. KDD \u201915 Sydney, Australia Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\nearly algorithm in this class was Choi\u2019s C99 algorithm [3] in 2000, which also introduced a benchmark segmentation dataset used by subsequent work. Instead of looking only at nearest neighbor coherence, the C99 algorithm computes a coherence score between all pairs of elements of text,1 and searches for a text segmentation that optimizes an objective based on that scoring by greedily making a succession of best cuts. Later work by Choi and collaborators [4] used distributed representations of words rather than a bag of words approach, with the representations generated by LSA [8]. In 2001, Utiyama and Ishahara introduced a statistical model for segmentation and optimized a posterior for the segment boundaries. Moving beyond the greedy approaches, in 2004 Fragkou et al. [10] attempted to find the optimal splitting for their own objective using dynamic programming. More recent attempts at segmentation, including Misra et al. [15] and Riedl and Biemann [18], used LDA based topic models to inform the segmentation task. Du et al. consider structured topic models for segmentation [7]. Eisenstein and Barzilay [9] and Dadachev et al. [6] both consider a Bayesian approach to text segmentation. Most similar to our own work, Sakahara et al. [19] consider a segmentation algorithm which does affinity propagation clustering on text representations built from word vectors learned from word2vec [14].\nFor the most part, aside from [19], the non-topic model based segmentation approaches have been based on relatively simple representations of the underlying text. Recent approaches to learning word vectors, including Mikolov et al.\u2019s word2vec [14], Pennington et al.\u2019s GloVe [16] and Levy and Goldberg\u2019s pointwise mutual information [12], have seen remarkable success in solving analogy tasks, machine translation [13], and sentiment analysis [16]. These word vector approaches attempt to learn a log-linear model for wordword co-occurrence statistics, such that the probability of two words (w,w\u2032) appearing near one another is proportional to the exponential of their dot product,\nP (w|w\u2032) = exp(w \u00b7 w \u2032)\u2211\nv exp(v \u00b7 w\u2032) . (1)\nThe method relies on these word-word co-occurrence statistics encoding meaningful semantic and syntactic relationships. Arora et al. [1] have shown how the remarkable performance of these techniques can be understood in terms of relatively mild assumptions about corpora statistics, which\n1By \u2018elements\u2019, we mean the pieces of text combined in order to comprise the segments. In the applications to be considered, the basic elements will be either sentences or words.\nar X\niv :1\n50 3.\n05 54\n3v 1\n[ cs\n.C L\n] 1\n8 M\nar 2\n01 5\nin turn can be recreated with a simple generative model. Here we explore the utility of word vectors for text segmentation, both in the context of existing algorithms such as C99, and when used to construct new segmentation objectives based on a generative model for segment formation. We will first construct a framework for describing a family of segmentation algorithms, then discuss the specific algorithms to be investigated in detail. We then apply our modified algorithms both to the standard Choi test set and to a test set generated from arXiv.org research articles."}, {"heading": "2. TEXT SEGMENTATION", "text": "The segmentation task is to split a text into contiguous coherent sections. We first build a representation of the text, by splitting it into N basic elements, ~Vi (i = 1, . . . , N), each a D-dimensional feature vector Vi\u03b1 (\u03b1 = 1, . . . , D) representing the element. Then we assign a score \u03c3(i, j) to each candidate segment, comprised of the ith through (j \u2212 1)th elements, and finally determine how to split the text into the appropriate number of segments.\nDenote a segmentation of text into K segments as a list of K indices s = (s1, s1, \u00b7 \u00b7 \u00b7 , sK), where the k-th segment includes the elements ~Vi with sk\u22121 \u2264 i < sk, with s0 \u2261 0. For example, the string \u201caaabbcccdd\u201d considered at the character level would be properly split with s = (3, 5, 8, 10) into (\u201caaa\u201d, \u201cbb\u201d, \u201cccc\u201d, \u201cdd\u201d)."}, {"heading": "2.1 Representation", "text": "The text representation thus amounts to turning a plain text document T into an (N \u00d7 D)-dimensional matrix V, with N the number of initial elements to be grouped into coherent segments and D the dimensionality of the element representation. For example, if segmenting at the word level then N would be the number of words in the text, and each word might be represented by a D-dimensional vector, such as those obtained from GloVe [16]. If segmenting instead at the sentence level, then N is the number of sentences in the text and we must decide how to represent each sentence.\nThere are additional preprocessing decisions, for example using a stemming algorithm or removing stop words before forming the representation. Particular preprocessing decisions can have a large effect on the performance of segmentation algorithms, but for discussing scoring functions and splitting methods those decisions can be abstracted into the specification of the N \u00d7D matrix V."}, {"heading": "2.2 Scoring", "text": "Having built an initial representation the text, we next specify the coherence of a segment of text with a scoring function \u03c3(i, j), which acts on the representation V and returns a score for the segment running from i (inclusive) to j (non-inclusive). The score can be a simple scalar or more general object. In addition to the scoring function, we need to specify how to return an aggregrate score for the entire segmentation. This score aggregation function \u2295 can be as simple as adding the scores for the individual segments, or again some more general function. The score S(s) for an overall segmentation is given by aggregating the scores of all of the segments in the segmentation:\nS(s) = \u03c3(0, s1)\u2295 \u03c3(s1, s2)\u2295 \u00b7 \u00b7 \u00b7 \u2295 \u03c3(sK\u22121, sK) . (2)\nFinally, to frame the segmentation problem as a form of optimization, we need to map the aggregated score to a sin-\ngle scalar. The key function (J\u00b7K) returns this single number, so that the cost for the above segmentation is\nC(s) = JS(s)K . (3)\nFor most of the segmentation schemes to be considered, the score function itself returns a scalar, so the score aggregation function \u2295 will be taken as simple addition with the key function the identity, but the generality here allows us to incorporate the C99 segmentation algorithm [3] into the same framework."}, {"heading": "2.3 Splitting", "text": "Having specified the representation of the text and scoring of the candidate segments, we need to prescribe how to choose the final segmentation. In this work, we consider three methods: (1) greedy splitting, which at each step inserts the best available segmentation boundary; (2) dynamic programming based segmentation, which uses dynamic programming to find the optimal segmentation; and (3) an iterative refinement scheme, which starts with the greedy segmentation and then adjusts the boundaries to improve performance.\n2.3.1 Greedy Segmentation The greedy segmentation approach builds up a segmenta-\ntion into K segments by greedily inserting new boundaries at each step to minimize the aggregate score:\ns0 = {N} (4) st+1 = arg min\ni\u2208[1,N) C(st \u222a {i}) (5)\nuntil the desired number of splits is reached. Many published text segmentation algorithms are greedy in nature, including the original C99 algorithm [3].\n2.3.2 Dynamic Programming The greedy segmentation algorithm is not guaranteed to\nfind the optimal splitting, but dynamic programming methods can be used for the text segmentation problem formulated in terms of optimizing a scoring objective. For a detailed account of dynamic programming and segmentation in general, see the thesis by Terzi [20]. Dynamic programming as been applied to text segmentation in Fragkou et al. [10], with much success, but we will also consider here an optimizaton of the the C99 segmentation algorithm using a dynamic programming approach.\nThe goal of the dynamic programming approach is to split the segmentation problem into a series of smaller segmentation problems, by expressing the optimal segmentation of the first n elements of the sequence into k segments in terms of the best choice for the last segmentation boundary. The aggregated score S(n, k) for this optimal segmentation should be minimized with respect to the key function J\u00b7K:\nS(n, 1) = \u03c3(0, n) (6)\nS(n, k) = J\u00b7K\nmin l<n\nS(l, k \u2212 1)\u2295 \u03c3(l, n) . (7)\nWhile the dynamic programming approach yeilds the optimal segmentation for our decomposable score function, it can be costly to compute, especially for long texts. In practice, both the optimal segmentation score and the resulting\nsegmentation can be found in one pass by building up a table of segmentation scores and optimal cut indices one row at a time.\n2.3.3 Iterative Relaxation Inspired by the popular Lloyd algorithm for k-means, we\nattempt to retain the computational benefit of the greedy segmentation approach, but realize additional performance gains by iteratively refining the segmentation. Since text segmentation problems require contiguous blocks of text, a natural scheme for relaxation is to try to move each segment boundary optimally while keeping the edges to either side of it fixed:\nst+1k = J\u00b7K\narg min l\u2208(st\nk\u22121, s t k+1 )\n( \u03c3(0, st1)\u2295 \u00b7 \u00b7 \u00b7\n\u2295 \u03c3(stk\u22121, l)\u2295 \u03c3(l, stk+1)\u2295 \u00b7 \u00b7 \u00b7 \u2295 \u03c3(stK\u22121, stK) ) (8)\n= J\u00b7K\narg min l\u2208(st\nk\u22121, s t k+1 )\nS ( st \u2212 {stk} \u222a {l} ) (9)\nWe will see in practice that by 20 iterations it has typically converged to a fixed point very close to the optimal dynamic programming segmentation."}, {"heading": "3. SCORING FUNCTIONS", "text": "In the experiments to follow, we will test various choices for the representation, scoring function, and splitting method in the above general framework. The segmentation algorithms to be considered fall into three groups:"}, {"heading": "3.1 C99 Segmentation", "text": "Choi\u2019s C99 algorithm [3] was an early text segmentation algorithm with promising results. The feature vector for an element of text is chosen as the pairwise cosine distances with other elements of text, where those elements in turn are represented by a bag of stemmed words vector (after preprocessing to remove stop words):\nAij = \u2211 w fi,wfj,w\u221a\u2211 w f 2 i,w \u2211 w f 2 j,w , (10)\nwith fi,w the frequency of word w in element i. The pairwise cosine distance matrix is noisy for these features, and since only the relative values are meaningful, C99 employs a ranking transformation, replacing each value of the matrix by the fraction of its neighbors with smaller value:\nVij = 1 r2 \u2212 1 \u2211\ni\u2212r/2\u2264l\u2264i+r/2 l6=i\n\u2211 j\u2212r/2\u2264m\u2264j+r/2\nm 6=j\n[Aij > Alm] ,\n(11) where the neighborhood is an r \u00d7 r block around the entry, the square brackets mean 1 if the inequality is satisfied otherwise 0 (and values off the end of the matrix are not counted in the sum, or towards the normalization). Each element of the text in the C99 algorithm is represented by a rank transformed vector of its cosine distances to each other element.\nThe score function describes the average intersentence similarity by taking the overall score to be\nC(s) = \u2211 k \u03b2k\u2211 k \u03b1k , (12)\nwhere \u03b2k = \u2211 sk\u22121\u2264i<sk \u2211 sk\u22121\u2264j<sk Vij is the sum of all ranked cosine similarities in a segment and \u03b1k = (sk+1\u2212sk)2 is the squared length of the segment. This score function is still decomposable, but requires that we define the local score function to return a pair,\n\u03c3(i, j) = ( \u2211 i\u2264k<j \u2211 i\u2264k<j Vij , (j \u2212 i)2 ) , (13)\nwith score aggregation function defined as component addition,\n(\u03b21, \u03b11)\u2295 (\u03b22, \u03b12) = (\u03b21 + \u03b22, \u03b11 + \u03b12) , (14)\nand key function defined as division of the two components,\nJ(\u03b2, \u03b1)K = \u03b2 \u03b1 . (15)\nWhile earlier work with the C99 algorithm considered only a greedy splitting approach, in the experiments that follow we will use our more general framework to explore both optimal dynamic programming and refined iterative versions of C99. Followup work by Choi et al. [4] explored the effect of using combinations of LSA word vectors in eq. (10) in place of the fi,w. Below we will explore the effect of using combinations of word vectors to represent the elements."}, {"heading": "3.2 Average word vector", "text": "To assess the utility of word vectors in segmentation, we first investigate how they can be used to improve the C99 algorithm, and then consider more general scoring functions based on our word vector representation. As the representation of an element, we take\nVik = \u2211 w fiwvwk , (16)\nwith fiw representing the frequency of word w in element i, and vwk representing the k\nth component of the word vector for word w as learned by a word vector training algorithm, such as word2vec [14] or GloVe [16].\nThe length of word vectors varies strongly across the vocabulary and in general correlates with word frequency. In order to mitigate the effect of common words, we will sometimes weight the sum by the inverse document frequency (idf) of the word in the corpus:\nVik = \u2211 w fiw log |D| dfw vwk , (17)\nwhere dfw is the number of documents in which word w appears. We can instead normalize the word vectors before adding them together\nVik = \u2211 w fiwv\u0303wk v\u0303wk = vwk\u221a\u2211 k v 2 wk , (18)\nor both weight by idf and normalize. Segmentation is a form of clustering, so a natural choice for scoring function is the sum of square deviations from the mean of the segment, as used in k-means:\n\u03c3(i, j) = \u2211 l \u2211 k ( Vlk \u2212 \u00b5k(i, j) )2 (19)\nwhere \u00b5k(i, j) = 1\nj \u2212 i j\u22121\u2211 l=i Vlk , (20)\nand which we call the Euclidean score function. Generally, however, cosine similarity is used for word vectors, making angles between words more important than distances. In some experiments, we therefore normalize the word vectors first, so that a euclidean distance score better approximates the cosine distance (recall |v\u0303 \u2212 w\u0303|22 = |v\u0303| 2 2 + |w\u0303| 2 2 \u2212 2v\u0303 \u00b7 w\u0303 = 2(1\u2212 v\u0303 \u00b7 w\u0303) for normalized vectors)."}, {"heading": "3.3 Content Vector Segmentation (CVS)", "text": "Trained word vectors have a remarkable amount of structure. Analogy tasks such as man:woman::king:? can be solved by finding the vector closest to the linear query:\nvwoman \u2212 vman + vking . (21)\nArora et al. [1] constructed a generative model of text that explains how this linear structure arises and can be maintained even in relatively low dimensional vector models. The generative model consists of a content vector which undergoes a random walk from a stationary distribution defined to be the product distribution on each of its components ck, uniform on the interval [\u2212 1\u221a\nD , 1\u221a D ] (with D the dimension-\nality of the word vectors). At each point in time, a word vector is generated by the content vector according to a loglinear model:\nP (w|c) = 1 Zc exp(w \u00b7 c) , Zc = \u2211 v exp(v \u00b7 c) . (22)\nThe slow drift of the content vectors helps to ensure that nearby words obey with high probability a log-linear model for their co-occurence probability:\nlogP (w,w\u2032) = 1\n2d \u2016vw + vw\u2032\u20162 \u2212 2 logZ \u00b1 o(1) , (23)\nfor some fixed Z. To segment text into coherent sections, we will boldly assume that the content vector in each putative segment is constant, and measure the log likelihood that all words in the segment are drawn from the same content vector c. (This is similar in spirit to the probabilistic segmentation technique proposed by Utiyama and Isahara [21].) Assuming the word draws {wi} are independent, we have that the log likelihood\nlogP ({wi}|c) = \u2211 i logP (wi|c) \u221d \u2211 i wi \u00b7 c (24)\nis proportional to the sum of the dot products of the word vectors wi with the content vector c. We use a maximum likelihood estimate for the content vector:\nc = arg max c\nlogP (c|{wi}) (25)\n= arg max c\n( logP ({wi}|c) + logP (c)\u2212 logP ({wi}) ) (26)\n\u221d arg max c\n\u2211 wi \u00b7 c s.t. \u2212\n1\u221a D < ck < 1\u221a D . (27)\nThis determines what we will call the Content Vector Segmentation (CVS) algorithm, based on the score function\n\u03c3(i, j) = \u2211 i\u2264l<j \u2211 k wlkck(i, j) . (28)\nThe score \u03c3(i, j) for a segment (i, j) is the sum of the dot products of the word vectors wlk with the maximum likelihood content vector c(i, j) for the segment, with components\ngiven by\nck(i, j) = sign  \u2211 i\u2264l<j wl,k  1\u221a D . (29)\nThe maximum likelihood content vector thus has components \u00b1 1\u221a\nD , depending on whether the sum of the word\nvector components in the segment is positive or negative. This score function will turn out to generate some of the most accurate segmentation results. Note that CVS is completely untrained with respect to the specific text to be segmented, relying only on a suitable set of word vectors, derived from some corpus in the language of choice. While CVS is most justifiable when working on the word vectors directly, we will also explore the effect of normalizing the word vectors before applying the objective."}, {"heading": "4. EXPERIMENTS", "text": "To explore the efficacy of different segmentation strategies and algorithms, we performed segmentation experiments on two datasets. The first is the Choi dataset [3], a common benchmark used in earlier segmentation work, and the second is a similarly constructed dataset based on articles uploaded to the arXiv, as will be described in Section 4.3. All code and data used for these experiments is available online2."}, {"heading": "4.1 Evaluation", "text": "To evaluate the performance of our algorithms, we use two standard metrics: the Pk metric and the WindowDiff (WD) metric. For text segmentation, near misses should get more credit than far misses. The Pk metric [2], captures the probability for a probe composed of a pair of nearby elements (at constant distance positions (i, i+k)) to be placed in the same segment by both reference and hypothesized segmentations. In particular, the Pk metric counts the number of disagreements on the probe elements:\nPk = 1\nN \u2212 k N\u2212k\u2211 i=1 [ \u03b4hyp(i, i+ k) 6= \u03b4ref(i, i+ k) ] (30)\nk = nearest integer\n1\n2\n# elements\n# segments \u2212 1 ,\nwhere \u03b4(i, j) is equal to 1 or 0 according to whether or not both element i and j are in the same segment in hypothesized and reference segmentations, resp., and the argument of the sum tests agreement of the hypothesis and reference segmentations. (k is taken to be one less than the integer closest to half of the number of elements divided by the number of segments in the reference segmentation.) The total is then divided by the total number of probes. This metric counts the number of disagreements, so lower scores indicate better agreement between the two segmentations. Trivial strategies such as choosing only a single segmentation, or giving each element its own segment, or giving constant boundaries or random boundaries, tend to produce values of around 50% [2].\nThe Pk metric has the disadvantage that it penalizes false positives more severely than false negatives, and can suffer when the distribution of segment sizes varies. Pevzner and\n2github.com/alexalemi/segmentation\nHearst [17] introduced the WindowDiff (WD) metric:\nWD = 1\nN \u2212 k N\u2212k\u2211 i=1 [ bref(i, i+ k) 6= bhyp(i, i+ k) ] , (31)\nwhere b(i, j) counts the number of boundaries between location i and j in the text, and an error is registered if the hypothesis and reference segmentations disagree on the number of boundaries. In practice, the Pk and WD scores are highly correlated, with Pk more prevalent in the literature \u2014 we will provide both for most of the experiments here."}, {"heading": "4.2 Choi Dataset", "text": "The Choi dataset is used to test whether a segmentation algorithm can distinguish natural topic boundaries. It concatenates the first n sentences from ten different documents chosen at random from a 124 document subset of the Brown corpus (the ca**.pos and cj**.pos sets) [3]. The number of sentences n taken from each document is chosen uniformly at random within a range specified by the subset id (i.e., as min\u2013max #sentences). There are four ranges considered: (3\u20135, 6\u20138, 9\u201311, 3\u201311), the first three of which have 100 example documents, and the last 400 documents. The dataset can be obtained from an archived version of the C99 segmentation code release3. An extract from one of the documents in the test set is shown in Fig. 1.\n4.2.1 C99 benchmark We will explore the effect of changing the representation\nand splitting strategy of the C99 algorithm. In order to give fair comparisons we implemented our own version of the C99 algorithm (oC99). The C99 performance depended sensitively on the details of the text preprocessing. Details can be found in Appendix A.\n4.2.2 Effect of word vectors on C99 variant The first experiment explores the ability of word vectors\nto improve the performance of the C99 algorithm. The word vectors were learned by GloVe [16] on a 42 billion word set of the Common Crawl corpus in 300 dimensions4. We emphasize that these word vectors were not trained on the Brown or Choi datasets directly, and instead come from a general corpus of English. These vectors were chosen in order to isolate any improvement due to the word vectors from any confounding effects due to details of the training procedure. The results are summarized in Table 1 below. The upper section cites results from [4], exploring the utility of using LSA word vectors, and showed an improvement of a few percent over their baseline C99 implementation. The middle section shows results from [18], which augmented the C99 method by representing each element with a histogram of topics learned from LDA. Our results are in the lower section, showing how word vectors improve the performance of the algorithm.\nIn each of these last experiments, we turned off the rank transformation, pruned the stop words and punctuation, but did not stem the vocabulary. Word vectors can be incorporated in a few natural ways. Vectors for each word in a\n3 http://web.archive.org/web/20010422042459/http:// www.cs.man.ac.uk/~choif/software/C99-1.2-release. tgz (We thank with Martin Riedl for pointing us to the dataset.) 4Obtainable from http://www-nlp.stanford.edu/data/ glove.42B.300d.txt.gz\nsentence can simply be summed, giving results shown in the oC99tf row. But all words are not created equal, so the sentence representation might be dominated by the vectors for common words. In the oC99tfidf row, the word vectors are weighted by idfi = log\n500 dfi (i.e., the log of the inverse doc-\nument frequency of each word in the Brown corpus, which has 500 documents in total) before summation. We see some improvement from using word vectors, for example the Pk of 14.78% for the oC99tfidf method on the 3\u201311 set, compared to Pk of 15.56% for our baseline C99 implementation. On the shorter 3\u20135 test set, our oC99tfidf method achieves Pk of 10.27% versus the baseline oC99 Pk of 14.22% . To compare to the various topic model based approaches, e.g. [18], we perform spherical k-means clustering on the word vectors [5] and represent each sentence as a histogram of its word clusters (i.e., as a vector in the space of clusters, with components equal to the number of its words in that that cluster). In this case, the word topic representations (oC99k50 and oC99k200 in Table 1) do not perform as well as the C99 variants of [18]. But as was noted in [18], those topic models were trained on cross-validated subsets of the Choi dataset, and benefited from seeing virtually all of the sentences in the test sets already in each training set, so have an unfair advantage that would not necessarily convey to real world applications. Overall, the results in Table 1 illustrate that the word vectors obtained from GloVe can markedly improve existing segmentation algorithms.\n4.2.3 Alternative Scoring frameworks The use of word vectors permits consideration of natural\nscoring functions other than C99-style segmentation scoring. The second experiment examines alternative scoring frameworks using the same GloVe word vectors as in the previous experiment. To test the utility of the scoring functions more directly, for these experiments we used the optimal dynamic programming segmentation. Results are summarized in Table 2, which shows the average Pk and WD scores on the 3\u201311 subset of the Choi dataset. In all cases, we removed stop words and punctuation, did not stem, but after preprocessing removed sentences with fewer than 5 words.\nNote first that the dynamic programming results for our implementation of C99 with tf weights gives Pk = 11.78%, 3% better than the greedy version result of 14.91% reported in Table 1. This demonstrates that the original C99 algorithm and its applications can benefit from a more exact minimization than given by the greedy approach. We con-\nsidered two natural score functions: the Euclidean scoring function (eqn. (20)) which minimizes the sum of the square deviations of each vector in a segment from the average vector of the segment, and the Content Vector scoring (CVS) (eqn. (28) of section 3.3), which uses an approximate log posterior for the words in the segment, as determined from its maximum likelihood content vector. In each case, we consider vectors for each sentence generated both as a strict sum of the words comprising it (tf approach), and as a sum weighted by the log idf (tfidf approach, as in sec. 4.2.2). Additionally, we consider the effect of normalizing the element vectors before starting the score minimization, as indicated by the n column.\nThe CVS score function eqn. (28) performs the best overall, with Pk scores below 6%, indicating an improved segmentation performance using a score function adapted to the choice of representation. While the most principled score function would be the Content score function using tf weighted element vectors without normalization, the normalized tfidf scheme actually performs the best. This is probably due to the uncharacteristically large effect common words have on the element representation, which the log idf weights and the normalization help to mitigate.\nStrictly speaking, the idf weighted schemes cannot claim to be completely untrained, as they benefit from word usage statistics in the Choi test set, but the raw CVS method still demonstrates a marked improvement on the 3\u201311 subset, 5.29% Pk versus the optimal C99 baseline of 11.78% Pk.\n4.2.4 Effect of Splitting Strategy To explore the effect of the splitting strategy and to com-\npare with our overall results on the Choi test set against other published benchmarks, in our third experiment we ran the raw CVS method against all of the Choi test subsets, using all three splitting strategies discussed: greedy, refined, and dynamic programming. These results are summarized in Table 3.\nOverall, our method outperforms all previous untrained methods. As commented regarding Table 1 (toward the end\nof subsection 4.2.2), we have included the results of the topic modeling based approaches M09 [15], R12 [18], and D13 [6] for reference. But due to repeat appearance of the same sentences throughout each section of the Choi dataset, methods that split that dataset into test and training sets have unavoidable access to the entirety of the test set during training, albeit in different order.5 These results can therefore only be compared to other algorithms permitted to make extensive use of the test data during cross-validation training. Only the TT, C99, U00 and raw CVS method can be considered as completely untrained. The C01 method derives its LSA vectors from the Brown corpus, from which the Choi test set is constructed, but that provides only a weak benefit, and the F04 method is additionally trained on a subset of the test set to achieve its best performance, but its use only of idf values provides a similarly weak benefit.\nWe emphasize that the raw CVS method is completely independent of the Choi test set, using word vectors derived from a completely different corpus. In Fig. 2, we reproduce the relevant results from the last column of Table 1 to highlight the performance benefits provided by the semantic word embedding.\nNote also the surprising performance of the refined splitting strategy, with the R-CVS results in Table 3 much lower than the greedy G-CVS results, and moving close to the optimal DP-CVS results, at far lower computational cost. In particular, taking the dynamic programming segmentation as the true segmentation, we can assess the performance of the refined strategy. As seen in Table 4, the refined segmentation very closely approximates the optimal segmentation.\nThis is important in practice since the dynamic programming segmentation is much slower, taking five times longer to compute on the 3\u201311 subset of the Choi test set. The dynamic programming segmentation becomes computationally infeasible to do at the scale of word level segmentation on the arXiv dataset considered in the next section, whereas\n5In [18], it is observed that \u201cThis makes the Choi data set artificially easy for supervised approaches.\u201d See appendix B.\nthe refined segmentation method remains eminently feasible."}, {"heading": "4.3 ArXiv Dataset", "text": "Performance evaluation on the Choi test set implements segmentation at the sentence level, i.e., with segments of composed of sentences as the basic elements. But text sources do not necessarily have well-marked sentence boundaries. The arXiv is a repository of scientific articles which for practical reasons extracts text from PDF documents (typically using pdfminer/pdf2txt.py). That Postscript-based format was originally intended only as a means of formatting text on a page, rather than as a network transmission format encoding syntactic or semantic information. The result is often somewhat corrupted, either due to the handling of mathematical notation, the presence of footers and headers, or even just font encoding issues.\nTo test the segmentation algorithms in a realistic setting, we created a test set similar to the Choi test set, but based on text extracted from PDFs retrieved from the arXiv database. Each test document is composed of a random number of contiguous words, uniformly chosen between 100 and 300, sampled at random from the text obtained from arXiv articles. The text was preprocessed by lowercasing and inserting spaces around every non-alphanumeric character, then splitting on whitespace to tokenize. An example of two of the segments of the first test document is shown in Figure 3 below.\nThis is a much more difficult segmentation task: due to the presence of numbers and many periods in references, there are no clear sentence boundaries on which to initially group the text, and no natural boundaries are suggested in the test set examples. Here segmentation algorithms must work directly at the \u201cword\u201d level, where word can mean a punctuation mark. The presence of garbled mathematical formulae adds to the difficulty of making sense of certain streams of text.\nIn Table 5, we summarize the results of three word vector\npowered approaches, comparing a C99 style algorithm to our content vector based methods, both for unnormalized and normalized word vectors. Since much of the language of the scientific articles is specialized, the word vectors used in this case were obtained from GloVe trained on a corpus of similarly preprocessed texts from 98,392 arXiv articles. (Since the elements are now words rather than sentences, the only issue involves whether or not those word vectors are normalized.) As mentioned, the dynamic programming approach is prohibitively expensive for this dataset.\nWe see that the CVS method performs far better on the test set than the C99 style segmentation using word vectors. The Pk and WD values obtained are not as impressive as those obtained on the Choi test set, but this test set offers a much more challenging segmentation task: it requires the methods to work at the level of words, and as well includes the possibility that natural topic boundaries occur in the test set segments themselves. The segmentations obtained with the CVS method typically appear sensibly split on section boundaries, references and similar formatting boundaries, not known in advance to the algorithm.\nAs a final illustration of the effectiveness of our algorithm at segmenting scientific articles, we\u2019ve applied the best performing algorithm to this article. Fig. 4 shows how the algorithm segments the text roughly along section borders."}, {"heading": "5. CONCLUSION", "text": "We have presented a general framework for describing and developing segmentation algorithms, and compared some existing and new strategies for representation, scoring and splitting. We have demonstrated the utility of semantic word embeddings for segmentation, both in existing algorithms and in new segmentation algorithms. On a real world segmentation task at word level, we\u2019ve demonstrated the ability to generate useful segmentations of scientific articles. In future work, we plan to use this segmentation technique to facilitate retrieval of documents with segments of concentrated content, and to identify documents with localized sections of similar content."}, {"heading": "6. ACKNOWLEDGEMENTS", "text": "This work was supported by NSF IIS-1247696. We thank James P. Sethna for useful discussions and for feedback on the manuscript."}, {"heading": "7. REFERENCES", "text": "[1] S. Arora, Y. Li, T. M. Yingyu Liang, and A. Risteski.\nRandom walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings. 2015, arXiv:1502.03520.\n[2] D. Beeferman, A. Berger, and J. Lafferty. Statistical models for text segmentation. Machine learning, 34(1-3):177\u2013210, 1999.\n[3] F. Y. Choi. Advances in domain independent linear text segmentation. In Proc. of the 1st North American chapter of the Association for Computational Linguistics conference, pages 26\u201333. Association for Computational Linguistics, 2000, arXiv:cs/0003083.\n[4] F. Y. Choi, P. Wiemer-Hastings, and J. Moore. Latent semantic analysis for text segmentation. In In Proceedings of EMNLP. Citeseer, 2001.\n[5] A. Coates and A. Y. Ng. Learning feature representations with k-means. In Neural Networks: Tricks of the Trade, pages 561\u2013580. Springer, 2012.\n[6] B. Dadachev, A. Balinsky, and H. Balinsky. On automatic text segmentation. In Proceedings of the 2014 ACM symposium on Document engineering, pages 73\u201380. ACM, 2014.\n[7] L. Du, W. L. Buntine, and M. Johnson. Topic segmentation with a structured topic model. In HLT-NAACL, pages 190\u2013200. Citeseer, 2013.\n[8] S. T. Dumais. Latent semantic analysis. Ann. Rev. of Information Sci. and Tech., 38(1):188\u2013230, 2004.\n[9] J. Eisenstein and R. Barzilay. Bayesian unsupervised topic segmentation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 334\u2013343. Association for Computational Linguistics, 2008.\n[10] P. Fragkou, V. Petridis, and A. Kehagias. A dynamic programming algorithm for linear text segmentation. Journal of Intelligent Information Systems, 23(2):179\u2013197, 2004.\n[11] M. A. Hearst. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational linguistics, 23(1):33\u201364, 1997.\n[12] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pp. 2177\u20132185, 2014.\n[13] T. Mikolov, Q. V. Le, and I. Sutskever. Exploiting similarities among languages for machine translation. arXiv preprint arXiv:1309.4168, 2013.\n[14] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111\u20133119, 2013.\n[15] H. Misra, F. Yvon, J. M. Jose, and O. Cappe. Text segmentation via topic modeling: an analytical study. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 1553\u20131556. ACM, 2009.\n[16] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12, 2014.\n[17] L. Pevzner and M. A. Hearst. A critique and improvement of an evaluation metric for text segmentation. Comp. Ling., 28(1):19\u201336, 2002.\n[18] M. Riedl and C. Biemann. Text segmentation with topic models. Journal for Language Technology and Computational Linguistics, 27(1):47\u201369, 2012.\n[19] M. Sakahara, S. Okada, and K. Nitta. Domain-independent unsupervised text segmentation for data management. In Data Mining Workshop (ICDMW), 2014 IEEE International Conference on, pages 481\u2013487. IEEE, 2014.\n[20] E. Terzi et al. Problems and algorithms for sequence segmentations. 2006.\n[21] M. Utiyama and H. Isahara. A statistical model for domain-independent text segmentation. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 499\u2013506. Association for Computational Linguistics, 2001.\nAPPENDIX"}, {"heading": "A. DETAILS OF C99 REPRESENTATION", "text": "This set of experiments compare to the results reported in [3]. We implemented our own version of the C99 algorithm (oC99) and tested it on the Choi dataset. We explored the effect of various changes to the representation part of the algorithm, namely the effects of removing stop words, cutting small sentence sizes, stemming the words, and performing the rank transformation on the cosine similarity matrix. For stemming, the implementation of the Porter stemming algorithm from nltk was used. For stopwords, we used the list distributed with the C99 code augmented by a list of punctuation marks. The results are summarized in Table 6.\nWhile we reproduce the results reported in [4] without the rank transformation (C99 in table 6), our results for the rank transformed results (last two lines for oC99) show better performance without stemming. This is likely due to particulars relating to details of the text transformations, such at the precise stemming algorithm and the stopword list. We attempted to match the choices made in [4] as much as possible, but still showed some deviations.\nPerhaps the most telling deviation is the 1.5% swing in results for the last two rows, whose only difference was a change in the tie breaking behavior of the algorithm. In our best result, we minimized the objective at each stage, so in the case of ties would break at the earlier place in the text, whereas for the TBR row, we maximized the negative of the objective, so in the case of ties would break on the rightmost equal value.\nThese relatively large swings in the performance on the Choi dataset suggest that it is most appropriate to compare differences in parameter settings for a particular implementation of an algorithm. Comparing results between different articles to assess performance improvements due to algorithmic changes hence requires careful attention to the implemention details."}, {"heading": "B. OVERFITTING THE CHOI DATASET", "text": "Recall from sec. 4.2 that each sample document in the Choi dataset is composed of 10 segments, and each such segment is the first n sentences from one of a 124 document subset of the Brown corpus (the ca**.pos and cj**.pos sets). This means that each of the four Choi test sets (n = 3-5, 6-8, 9-11, 3-11) necessarily contains multiple repetitions of each sentence. In the 3-5 Choi set, for example, there are 3986 sentences, but only 608 unique sentences, so that each sentence appears on average 6.6 times. In the 3-11 set, with 400 sample documents, there are 28,145 sentences, but only 1353 unique sentences, for an average of 20.8 appearances for each sentence. Furthermore, in all cases there are only 124 unique sentences that can begin a new segment. This redundancy means that a trained method such as LDA will see most or all of the test data during training, and can easily overfit to the observed segmentation boundaries, especially when the number of topics is not much smaller than the number of documents. For example, using standard 10- fold cross validation on an algorithm that simply identifies a segment boundary for any sentence in the test set that began a document in the training set gives better than 99.9% accuracy in segmenting all four parts of the Choi dataset. For this reason, we have not compared to the topic-modeling based segmentation results in Tables 1 and 3."}], "references": [{"title": "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings", "author": ["S. Arora", "Y. Li", "T.M. Yingyu Liang", "A. Risteski"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Statistical models for text segmentation", "author": ["D. Beeferman", "A. Berger", "J. Lafferty"], "venue": "Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Advances in domain independent linear text segmentation", "author": ["F.Y. Choi"], "venue": "In Proc. of the 1st North American chapter of the Association for Computational Linguistics conference,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Latent semantic analysis for text segmentation", "author": ["F.Y. Choi", "P. Wiemer-Hastings", "J. Moore"], "venue": "Proceedings of EMNLP. Citeseer,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Learning feature representations with k-means", "author": ["A. Coates", "A.Y. Ng"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "On automatic text segmentation", "author": ["B. Dadachev", "A. Balinsky", "H. Balinsky"], "venue": "In Proceedings of the 2014 ACM symposium on Document engineering,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Topic segmentation with a structured topic model", "author": ["L. Du", "W.L. Buntine", "M. Johnson"], "venue": "In HLT-NAACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Latent semantic analysis", "author": ["S.T. Dumais"], "venue": "Ann. Rev. of Information Sci. and Tech.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Bayesian unsupervised topic segmentation", "author": ["J. Eisenstein", "R. Barzilay"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "A dynamic programming algorithm for linear text segmentation", "author": ["P. Fragkou", "V. Petridis", "A. Kehagias"], "venue": "Journal of Intelligent Information Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Texttiling: Segmenting text into multi-paragraph subtopic passages", "author": ["M.A. Hearst"], "venue": "Computational linguistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Exploiting similarities among languages for machine translation", "author": ["T. Mikolov", "Q.V. Le", "I. Sutskever"], "venue": "arXiv preprint arXiv:1309.4168,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Text segmentation via topic modeling: an analytical study", "author": ["H. Misra", "F. Yvon", "J.M. Jose", "O. Cappe"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "A critique and improvement of an evaluation metric for text segmentation", "author": ["L. Pevzner", "M.A. Hearst"], "venue": "Comp. Ling.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Text segmentation with topic models", "author": ["M. Riedl", "C. Biemann"], "venue": "Journal for Language Technology and Computational Linguistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Domain-independent unsupervised text segmentation for data management", "author": ["M. Sakahara", "S. Okada", "K. Nitta"], "venue": "In Data Mining Workshop (ICDMW),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Problems and algorithms for sequence segmentations", "author": ["E. Terzi"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "A statistical model for domain-independent text segmentation", "author": ["M. Utiyama", "H. Isahara"], "venue": "In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}], "referenceMentions": [{"referenceID": 13, "context": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation.", "startOffset": 47, "endOffset": 59}, {"referenceID": 15, "context": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation.", "startOffset": 47, "endOffset": 59}, {"referenceID": 11, "context": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation.", "startOffset": 47, "endOffset": 59}, {"referenceID": 2, "context": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation.", "startOffset": 134, "endOffset": 140}, {"referenceID": 3, "context": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation.", "startOffset": 134, "endOffset": 140}, {"referenceID": 17, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 43, "endOffset": 57}, {"referenceID": 14, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 43, "endOffset": 57}, {"referenceID": 2, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 43, "endOffset": 57}, {"referenceID": 3, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 43, "endOffset": 57}, {"referenceID": 1, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 79, "endOffset": 86}, {"referenceID": 16, "context": "We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17].", "startOffset": 79, "endOffset": 86}, {"referenceID": 10, "context": "An early text segmentation algorithm was the TextTiling method introduced by Hearst [11] in 1997.", "startOffset": 84, "endOffset": 88}, {"referenceID": 2, "context": "early algorithm in this class was Choi\u2019s C99 algorithm [3] in 2000, which also introduced a benchmark segmentation dataset used by subsequent work.", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "Later work by Choi and collaborators [4] used distributed representations of words rather than a bag of words approach, with the representations generated by LSA [8].", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "Later work by Choi and collaborators [4] used distributed representations of words rather than a bag of words approach, with the representations generated by LSA [8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "[10] attempted to find the optimal splitting for their own objective using dynamic programming.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] and Riedl and Biemann [18], used LDA based topic models to inform the segmentation task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[15] and Riedl and Biemann [18], used LDA based topic models to inform the segmentation task.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "consider structured topic models for segmentation [7].", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "Eisenstein and Barzilay [9] and Dadachev et al.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "[6] both consider a Bayesian approach to text segmentation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] consider a segmentation algorithm which does affinity propagation clustering on text representations built from word vectors learned from word2vec [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[19] consider a segmentation algorithm which does affinity propagation clustering on text representations built from word vectors learned from word2vec [14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 18, "context": "For the most part, aside from [19], the non-topic model based segmentation approaches have been based on relatively simple representations of the underlying text.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "\u2019s word2vec [14], Pennington et al.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "\u2019s GloVe [16] and Levy and Goldberg\u2019s pointwise mutual information [12], have seen remarkable success in solving analogy tasks, machine translation [13], and sentiment analysis [16].", "startOffset": 9, "endOffset": 13}, {"referenceID": 11, "context": "\u2019s GloVe [16] and Levy and Goldberg\u2019s pointwise mutual information [12], have seen remarkable success in solving analogy tasks, machine translation [13], and sentiment analysis [16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 12, "context": "\u2019s GloVe [16] and Levy and Goldberg\u2019s pointwise mutual information [12], have seen remarkable success in solving analogy tasks, machine translation [13], and sentiment analysis [16].", "startOffset": 148, "endOffset": 152}, {"referenceID": 15, "context": "\u2019s GloVe [16] and Levy and Goldberg\u2019s pointwise mutual information [12], have seen remarkable success in solving analogy tasks, machine translation [13], and sentiment analysis [16].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "[1] have shown how the remarkable performance of these techniques can be understood in terms of relatively mild assumptions about corpora statistics, which", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "For example, if segmenting at the word level then N would be the number of words in the text, and each word might be represented by a D-dimensional vector, such as those obtained from GloVe [16].", "startOffset": 190, "endOffset": 194}, {"referenceID": 2, "context": "For most of the segmentation schemes to be considered, the score function itself returns a scalar, so the score aggregation function \u2295 will be taken as simple addition with the key function the identity, but the generality here allows us to incorporate the C99 segmentation algorithm [3] into the same framework.", "startOffset": 284, "endOffset": 287}, {"referenceID": 2, "context": "Many published text segmentation algorithms are greedy in nature, including the original C99 algorithm [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 19, "context": "For a detailed account of dynamic programming and segmentation in general, see the thesis by Terzi [20].", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "[10], with much success, but we will also consider here an optimizaton of the the C99 segmentation algorithm using a dynamic programming approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Choi\u2019s C99 algorithm [3] was an early text segmentation algorithm with promising results.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "[4] explored the effect of using combinations of LSA word vectors in eq.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "with fiw representing the frequency of word w in element i, and vwk representing the k th component of the word vector for word w as learned by a word vector training algorithm, such as word2vec [14] or GloVe [16].", "startOffset": 195, "endOffset": 199}, {"referenceID": 15, "context": "with fiw representing the frequency of word w in element i, and vwk representing the k th component of the word vector for word w as learned by a word vector training algorithm, such as word2vec [14] or GloVe [16].", "startOffset": 209, "endOffset": 213}, {"referenceID": 0, "context": "[1] constructed a generative model of text that explains how this linear structure arises and can be maintained even in relatively low dimensional vector models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "(This is similar in spirit to the probabilistic segmentation technique proposed by Utiyama and Isahara [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "The first is the Choi dataset [3], a common benchmark used in earlier segmentation work, and the second is a similarly constructed dataset based on articles uploaded to the arXiv, as will be described in Section 4.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "The Pk metric [2], captures the probability for a probe composed of a pair of nearby elements (at constant distance positions (i, i+k)) to be placed in the same segment by both reference and hypothesized segmentations.", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "Trivial strategies such as choosing only a single segmentation, or giving each element its own segment, or giving constant boundaries or random boundaries, tend to produce values of around 50% [2].", "startOffset": 193, "endOffset": 196}, {"referenceID": 16, "context": "Hearst [17] introduced the WindowDiff (WD) metric:", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "pos sets) [3].", "startOffset": 10, "endOffset": 13}, {"referenceID": 15, "context": "The word vectors were learned by GloVe [16] on a 42 billion word set of the Common Crawl corpus in 300 dimensions.", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "The upper section cites results from [4], exploring the utility of using LSA word vectors, and showed an improvement of a few percent over their baseline C99 implementation.", "startOffset": 37, "endOffset": 40}, {"referenceID": 17, "context": "The middle section shows results from [18], which augmented the C99 method by representing each element with a histogram of topics learned from LDA.", "startOffset": 38, "endOffset": 42}, {"referenceID": 17, "context": "[18], we perform spherical k-means clustering on the word vectors [5] and represent each sentence as a histogram of its word clusters (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[18], we perform spherical k-means clustering on the word vectors [5] and represent each sentence as a histogram of its word clusters (i.", "startOffset": 66, "endOffset": 69}, {"referenceID": 17, "context": "In this case, the word topic representations (oC99k50 and oC99k200 in Table 1) do not perform as well as the C99 variants of [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "But as was noted in [18], those topic models were trained on cross-validated subsets of the Choi dataset, and benefited from seeing virtually all of the sentences in the test sets already in each training set, so have an unfair advantage that would not necessarily convey to real world applications.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "Pk WD Algorithm 3\u20135 6\u20138 9\u201311 3\u201311 3\u20135 6\u20138 9\u201311 3\u201311 C99 [4] 12 11 9 9 C99LSA 9 10 7 5 C99 [18] 11.", "startOffset": 56, "endOffset": 59}, {"referenceID": 17, "context": "Pk WD Algorithm 3\u20135 6\u20138 9\u201311 3\u201311 3\u20135 6\u20138 9\u201311 3\u201311 C99 [4] 12 11 9 9 C99LSA 9 10 7 5 C99 [18] 11.", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "C99LSA) shows the few percent improvement over the C99 baseline reported in [4] of using LSA to encode the words.", "startOffset": 76, "endOffset": 79}, {"referenceID": 17, "context": "C99LDA) shows the effect of modifying the C99 algorithm to work on histograms of LDA topics in each sentence, from [18].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "The bottom section shows the effect of using word vectors trained from GloVe [16] in our oC99 implementation of the C99 segmentation algorithm.", "startOffset": 77, "endOffset": 81}, {"referenceID": 2, "context": "Alg 3\u20135 6\u20138 9\u201311 3\u201311 TT [3] 44 43 48 46 C99 [3] 12 9 9 12 C01 [4] 10 7 5 9 U00 [21] 9 7 5 10 F04 [10] 5.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "Alg 3\u20135 6\u20138 9\u201311 3\u201311 TT [3] 44 43 48 46 C99 [3] 12 9 9 12 C01 [4] 10 7 5 9 U00 [21] 9 7 5 10 F04 [10] 5.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "Alg 3\u20135 6\u20138 9\u201311 3\u201311 TT [3] 44 43 48 46 C99 [3] 12 9 9 12 C01 [4] 10 7 5 9 U00 [21] 9 7 5 10 F04 [10] 5.", "startOffset": 63, "endOffset": 66}, {"referenceID": 20, "context": "Alg 3\u20135 6\u20138 9\u201311 3\u201311 TT [3] 44 43 48 46 C99 [3] 12 9 9 12 C01 [4] 10 7 5 9 U00 [21] 9 7 5 10 F04 [10] 5.", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "Alg 3\u20135 6\u20138 9\u201311 3\u201311 TT [3] 44 43 48 46 C99 [3] 12 9 9 12 C01 [4] 10 7 5 9 U00 [21] 9 7 5 10 F04 [10] 5.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "29 M09 [15] 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 17, "context": "3 R12 [18] 1.", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "95 D13 [7] 1.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "2), we have included the results of the topic modeling based approaches M09 [15], R12 [18], and D13 [6] for reference.", "startOffset": 76, "endOffset": 80}, {"referenceID": 17, "context": "2), we have included the results of the topic modeling based approaches M09 [15], R12 [18], and D13 [6] for reference.", "startOffset": 86, "endOffset": 90}, {"referenceID": 5, "context": "2), we have included the results of the topic modeling based approaches M09 [15], R12 [18], and D13 [6] for reference.", "startOffset": 100, "endOffset": 103}, {"referenceID": 17, "context": "In [18], it is observed that \u201cThis makes the Choi data set artificially easy for supervised approaches.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "3\u20135 6\u20138 9\u201311 3\u201311 R-CVS vs DP-CVS [3] 0.", "startOffset": 34, "endOffset": 37}], "year": 2015, "abstractText": "We explore the use of semantic word embeddings [14, 16, 12] in text segmentation algorithms, including the C99 segmentation algorithm [3, 4] and new algorithms inspired by the distributed word vector representation. By developing a general framework for discussing a class of segmentation objectives, we study the effectiveness of greedy versus exact optimization approaches and suggest a new iterative refinement technique for improving the performance of greedy strategies. We compare our results to known benchmarks [18, 15, 3, 4], using known metrics [2, 17]. We demonstrate state-of-the-art performance for an untrained method with our Content Vector Segmentation (CVS) on the Choi test set. Finally, we apply the segmentation procedure to an inthe-wild dataset consisting of text extracted from scholarly articles in the arXiv.org database.", "creator": "LaTeX with hyperref package"}}}