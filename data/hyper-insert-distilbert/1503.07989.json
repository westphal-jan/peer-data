{"id": "1503.07989", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2015", "title": "Discriminative Bayesian Dictionary Learning for Classification", "abstract": "we propose a bayesian scattering approach to learn sorting discriminative dictionaries for sparse representation of data. the specially proposed approach best infers probability loss distributions over the displaced atoms equivalent of a discriminative dictionary using a modified beta process. it also computes sets parameters of hierarchical bernoulli distributions that associate each class labels to mask the criteria learned as dictionary meaning atoms. technically this association signifies the good selection spatial probabilities of capturing the dictionary atoms in illustrating the statistical expansion of class - weight specific query data. ironically furthermore, the non - parametric generic character index of detecting the proposed approach allows recognizing it sufficient to infer the correct size resolution of detecting the dictionary. we now exploit here the aforementioned bernoulli distribution distributions in humans separately learning forming a sparse linear classifier. the classifier version uses the same hierarchical bayesian model as the dictionary, estimates which we present along the path analytical inference, solution equation for gibbs sampling. alternatively for classification, a quantitative test slice instance is first very sparsely encoded over the learned generic dictionary fragments and here the codes are fed smoothly to the homogeneous classifier. we here performed experiments for face filter and slide action model recognition ; and object and virtual scene - category classification using in five public transport datasets computers and compared the empirical results above with state - of - were the - state art discriminative sparse representation query approaches. previous experiments show why that the proposed sampling bayesian variance approach surprisingly consistently outperforms the existing approaches.", "histories": [["v1", "Fri, 27 Mar 2015 08:36:15 GMT  (1156kb,D)", "http://arxiv.org/abs/1503.07989v1", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["naveed akhtar", "faisal shafait", "ajmal mian"], "accepted": false, "id": "1503.07989"}, "pdf": {"name": "1503.07989.pdf", "metadata": {"source": "CRF", "title": "Discriminative Bayesian Dictionary Learning for Classification", "authors": ["Naveed Akhtar", "Faisal Shafait", "Ajmal Mian"], "emails": ["naveed.akhtar@research.uwa.edu.au,", "ajmal.mian}@uwa.edu.au."], "sections": [{"heading": null, "text": "Index Terms\u2014Bayesian sparse representation, Discriminative dictionary learning, Supervised learning, Classification.\nI. INTRODUCTION\nSparse representation encodes a signal as a sparse linear combination of redundant basis vectors. With its inspirational roots in human vision system [16], [17], this technique has been successfully employed in image restoration [18], [19], [20], compressive sensing [21], [22] and morphological component analysis [23]. More recently, sparse representation based approaches have also shown promising results in face recognition and gender classification [9], [8], [10], [13], [24], [25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36]. The success of these approaches comes from the fact that a sample from a class can generally be well represented as a sparse linear combination of the other samples from the same class, in a lower dimensional manifold [8].\nFor classification, a discriminative sparse representation approach first encodes the test instance over a dictionary, i.e.\nN. Akhtar, F. Shafait and A. Mian are with the School of Computer Science and Software Engineering, The University of Western Australia, 35 Stirling Highway Crawley, 6009. WA. naveed.akhtar@research.uwa.edu.au, {faisal.shafait, ajmal.mian}@uwa.edu.au.\na redundant set of basis vectors, known as atoms. Therefore, an effective dictionary is critical for the performance of such approaches. It is possible to use an off-the-shelf basis (e.g. fast Fourier transform [41] or wavelets [42]) as a generic dictionary to represent data from different domains/classes. However, research in the last decade ( [6], [9], [10], [11], [18], [43], [44], [45]) has provided strong evidence in favor of learning dictionaries using the domain/class-specific training data, especially for classification and recognition tasks [10] where class label information of the training data can be exploited in the supervised learning of a dictionary.\nWhereas unsupervised dictionary learning approaches (e.g. K-SVD [6], Method of Optimal Directions [46]) aim at learning faithful signal representations, supervised sparse representation additionally strives for making the dictionaries discriminative. For instance, in Sparse Representation based Classification (SRC) scheme, Wright et al. [8] constructed a discriminative dictionary by directly using the training data as the dictionary atoms. With each atom associated to a particular class, the query is assigned the label of the class whose associated atoms maximally contribute to the sparse representation of the query. Impressive results have been achieved for recognition and classification using SRC, however, the computational complexity of this technique becomes prohibitive for large training data. This has motivated considerable research on learning discriminative dictionaries that would allow sparse representation based classification with much lower computational cost.\nIn order to learn a discriminative dictionary, existing approaches either force subsets of the dictionary atoms to represent data from only specific classes [12], [26], [47] or they associate the complete dictionary to all the classes and constrain their sparse coefficient to be discriminative [7], [9], [28]. A third category of techniques learns exclusive sets of class specific and common dictionary atoms to separate the common and particular features of the data from different classes [11], [54]. Establishing association between the dictionary atoms and the corresponding class labels is a key step of existing methods. However, adaptively building this association is still an open research problem [13]. Moreover, the strategy of assigning different number of dictionary atoms to different classes and adjusting the overall size of the dictionary become critical for the classification accuracy of the existing approaches, as no principled approach is generally provided to predetermine these parameters.\nIn this work, we propose a solution to this problem by approaching the sparse representation based classification from a non-parametric Bayesian perspective. We propose a Bayesian\nar X\niv :1\n50 3.\n07 98\n9v 1\n[ cs\n.C V\n] 2\n7 M\nar 2\n01 5\n2\nsparse representation technique that infers a discriminative dictionary using a Beta Process [56]. Our approach adaptively builds the association between the dictionary atoms and the class labels such that this association signifies the probability of selection of the dictionary atoms in the expansion of classspecific data. Furthermore, the non-parametric character of the approach allows it to automatically infer the correct size of the dictionary. The scheme employed by our approach is shown in Fig. 1. We perform Bayesian inference over a model proposed for discriminative sparse representation of the training data. The inference process learns distributions over the dictionary atoms and sets of Bernoulli distributions associating the dictionary atoms to the labels of the data. The Bernoulli distributions govern the support of the final sparse codes and are later utilized in learning a multi-class linear classifier. The final dictionary is learned by sampling the distributions over the dictionary atoms and the corresponding sparse codes are computed by element-wise product of the support and the inferred weights of the codes. The computed dictionary and the sparse codes also represent the training data faithfully. A query is classified in our approach by first sparsely encoding it over the inferred dictionary and then classifying its sparse code with the learned classifier. In this work, we learn the classifier and the dictionary using the same hierarchical Bayesian model. This allows us to exploit the aforementioned Bernoulli distributions in the accurate estimate of the classifier. We present the proposed Bayesian model along its inference equations for Gibbs sampling. Our approach has been tested on two face-databases [1], [2], an object-database [3], an action-database [5] and a scene-database [4]. The classification results are compared with the state-of-the-art discriminative sparse representation approaches. The proposed approach not only outperforms these approaches in terms of accuracy, its computational efficiency for the classification stage is also comparable to the most efficient existing approaches.\nThis paper is organized as follows. We review the related work in Section II of the paper. In Section III, we formulate the\n3 problem and briefly explain the relevant concepts that clarify the rationale behind our approach. The proposed approach is presented in Section IV, which includes details of the proposed model, the Gibbs sampling process, the classification scheme and the initialization of the proposed approach. Experimental results are reported in Section V and a discussion on the parameter settings is provided in Section VI. We draw conclusions in Section VII."}, {"heading": "II. RELATED WORK", "text": "There are three main categories of the approaches that learn discriminative sparse representation. In the first category, the learned dictionary atoms have direct correspondence to the labels of the classes [26], [47], [12], [48], [35], [49], [36]. Yang et al. [26] proposed an SRC like framework for face recognition, where the atoms of the dictionary are learned from the training data instead of directly using the training data as the dictionary. In order to learn a dictionary that is simultaneously discriminative and reconstructive, Mairal et al. [47] used a discriminative penalty term in the KSVD model [6], achieving state-of-the-art results on texture segmentation. Sprechmann and Sapiro [48] also proposed to learn dictionaries and sparse codes for clustering. In [36], Castrodad and Sapiro computed class-specific dictionaries for actions. The dictionary atoms and their sparse coefficients also exploited the non-negativity of the signals in their approach. Active basis models are learned from the training images of each class and applied to object detection and recognition in [49]. Ramirez et al. [12] have used an incoherence promoting term for the dictionary atoms in their learning model. Encouraging incoherence among the class-specific sub-dictionaries allowed them to represent samples from the same class better than the samples from the other classes. Wang et al. [35] have proposed to learn class-specific dictionaries for modeling individual actions for action recognition. Their model incorporated a similarity constrained term and a dictionary incoherence term for classification. The above mentioned methods mainly associate a dictionary atom directly to a single class. Therefore, a query is generally assigned the label of the class whose associated atoms result in the minimum representational error for the query. The classification stages of the approaches under this category often require the computation of representations of the query over many sub-dictionaries.\nIn the second category of the approaches for discriminative sparse representation, a single dictionary is shared by all the classes, however the representation coefficients are forced to be discriminative ( [9], [28], [7], [29], [30], [45], [31], [50], [33], [51] ). Jiang et al. [9] proposed a dictionary learning model that encourages the sparse representation coefficients of the same class to be similar. This is done by adding a \u2019discriminative sparse-code error\u2019 constraint to a unified objective function that already contains reconstruction error and classification error constraints. A similar approach is taken by Rodriguez and Sapiro [30] where the authors solve for a simultaneous sparse approximation problem [52] while learning the coefficients. It is common to learn dictionaries jointly with a classifier. Pham and Venkatesh [45] and Mairal\net al. [28] proposed to train linear classifiers along the joint dictionaries learned for all the classes. Zhang and Li [7] enhanced the K-SVD algorithm [6] to learn a linear classifier along the dictionary. A task driven dictionary learning framework has also been proposed [31]. Under this framework, different risk functions of the representation coefficients are minimized for different tasks. Broadly speaking, the above mentioned approaches aim at learning a single dictionary together with a classifier. The query is classified by directly feeding its sparse codes over the learned single dictionary to the classifier. Thus, in comparison to the approaches in the first category, the classification stage of these approaches is computationally more efficient. In terms of learning a single dictionary for the complete training data and the classification stage, the proposed approach also falls under this category of discriminative sparse representation techniques.\nThe third category takes a hybrid approach for learning the discriminative sparse representation. In these approaches, the dictionaries are designed to have a set of shared atoms in addition to class-specific atoms. Deng et al. [53] extended the SRC algorithm by appending an intra-class face variation dictionary to the training data. This extension achieves promising results in face recognition with a single training sample per class. Zhou and Fan [54] employ a Fisher-like regularizer on the representation coefficients while learning a hybrid dictionary. Wang and Kong [11] learned a hybrid dictionary to separate the common and particular features of the data. Their approach additionally encouraged the class-specific dictionaries to be incoherent during the optimization process. Shen et al. [55] proposed to learn a multi-level dictionary for hierarchical visual categorization. To some extent, it is possible to reduce the size of the dictionary using the hybrid approach, which also results in reducing the classification time in comparison to the approaches that fall under the first category. However, it is often non-trivial to decide on how to balance between the shared and the class-specific parts of the hybrid dictionary [10], [13]."}, {"heading": "III. PROBLEM FORMULATION AND BACKGROUND", "text": "Let X = [X1, ...,Xc, ...,XC ] \u2208 Rm\u00d7N be the training data comprising N instances from C classes, wherein Xc \u2208 Rm\u00d7Nc represents the data from the cth class and\u2211C c=1Nc = N . The columns of X\nc are indexed in Ic. We denote a dictionary by \u03a6 \u2208 Rm\u00d7|K| with atoms \u03d5k, where k \u2208 K = {1, ...,K} and |.| represents the cardinality of the set. Let A \u2208 R|K|\u00d7N be the sparse code matrix of the data, such that X \u2248 \u03a6A. We can write A = [A1, ...,Ac, ...,AC ], where Ac \u2208 R|K|\u00d7|Ic| is the sub-matrix related to the cth class. The ith column of A is denoted as \u03b1i \u2208 R|K|. To learn a sparse representation of the data, we can solve the following optimization problem:\n< \u03a6,A >= min \u03a6,A ||X\u2212\u03a6A||2F s.t. \u2200i, ||\u03b1i||p \u2264 t, (1)\nwhere t is a predefined constant, ||.||F computes the Frobenius norm and ||.||p denotes the `p-norm of a vector. Generally, p is chosen to be 0 or 1 for sparsity [57]. The non-convex optimization problem of Eq. (1) can be iteratively solved\n4 by fixing one parameter and solving a convex optimization problem for the other parameter in each iteration. The solution to Eq. (1), factors the training data X into two complementary matrices, namely the dictionary and the sparse codes, without considering the class label information of the training data. Nevertheless, we can still exploit this factorization in classification tasks by using the sparse codes of the data as features [9], for which, a classifier can be obtained as\nW = min W N\u2211 i=1 L{hi, f(\u03b1i,W)}+ \u03bb||W||2F , (2)\nwhere W \u2208 RC\u00d7|K| contains the model parameters of the classifier, L is the loss function, hi is the label of the ith training instance xi \u2208 Rm and \u03bb is the regularization parameter.\nIt is usually suboptimal to perform classification based on sparse codes learned by an unsupervised technique. Considering this, existing approaches [7], [45], [29], [28] proposed to jointly optimize a classifier with the dictionary while learning the sparse representation. One intended ramification of this approach is that the label information also gets induced into the dictionary. This happens when the information is utilized in computing the sparse codes of the data, which in turn, are used for computing the dictionary atoms, while solving Eq. (1). This results in improving the discriminative abilities of the learned dictionary. Jiang et al. [9] built further on this concept and encouraged explicit correspondence between the dictionary atoms and the class-labels. More precisely, the following optimization problem is solved by the LabelConsistent K-SVD (LC-KSVD2) algorithm [9]:\n< \u03a6,W,T,A >= min \u03a6,W,T,A \u2225\u2225\u2225\u2225\u2225  X\u221a\u03c5Q\u221a\n\u03baH\n\u2212  \u03a6\u221a\u03c5T\u221a\n\u03baW\nA\u2225\u2225\u2225\u2225\u2225 2\nF\ns.t. \u2200i ||\u03b1i||0 \u2264 t (3)\nwhere \u03c5 and \u03ba are the regularization parameters, the binary matrix H \u2208 RC\u00d7N contains the class label information1, T \u2208 R|K|\u00d7|K| is the transformation between the sparse codes and the discriminative sparse codes Q \u2208 R|K|\u00d7N . Here, for the ith training instance, the ith column of the fixed binary matrix Q has 1 appearing at the kth index only if the kth dictionary atom has the same class label as the training instance. Thus, the discriminative sparse codes form a pre-defined relationship between the dictionary atoms and the class labels. This brings improvement to the discriminative abilities of the dictionary learned by solving Eq. (3).\nIt is worth noting that in Label-Consistent K-SVD algorithm [9], the relationship between class-specific subsets of dictionary atoms and class labels is pre-defined. However, regularization allows flexibility in this association during optimization. We also note that using \u03c5 = 0 in Eq. (3) reduces the optimization problem to the one solved by Discriminative KSVD (D-KSVD) algorithm [7]. Successful results are achievable using the above mentioned techniques for recognition and classification. However, like any discriminative sparse\n1For the ith training instance, the ith column of H has 1 appearing only at the index corresponding to the class label.\nrepresentation approach, these results are obtainable only after careful optimization of the algorithm parameters, including the dictionary size. In Fig. 2, we illustrate the behavior of recognition accuracy under varying dictionary sizes for [7] and [9] for two face databases.\nPaisley and Carin [56] developed a Beta Process for nonparametric factor analysis, which was later used by Zhou et al. [44] in successful image restoration and compressive sensing. Exploiting the non-parametric Bayesian framework, a Beta Process can automatically infer the factor/dictionary size from the training data. With the base measure ~0 and parameters ao > 0 and bo > 0, a Beta Process is denoted by BP(ao, bo, ~0). A draw from this process, i.e. ~ \u223c BP(ao, bo, ~0), can be represented as\n~ = \u2211 k \u03c0k\u03b4\u03d5k(\u03d5), k \u2208 K = {1, ...,K},\n\u03c0k \u223c Beta(\u03c0k|ao/K, bo(K \u2212 1)/K), \u03d5k \u223c ~0, (4)\nwith this a valid measure as K \u2192\u221e. In the above equation, \u03b4\u03d5k(\u03d5) is 1 when \u03d5 = \u03d5k and 0 otherwise. Therefore, ~ can be represented as a set of |K| probabilities, each having an associated vector \u03d5k, drawn i.i.d. from the base measure ~0. Using ~, we can draw a binary vector zi \u2208 {0, 1}|K|, such that the kth component of this vector is drawn zik \u223c Bernoulli(\u03c0k). By independently drawing N such vectors, we may construct a matrix Z \u2208 {0, 1}|K|\u00d7N , where zi is the ith column of this matrix.\nUsing the above mentioned Beta Process, it is possible to factorize X \u2208 Rm\u00d7N as follows:\nX = \u03a6Z + E, (5)\nwhere, \u03a6 \u2208 Rm\u00d7|K| has \u03d5k as its columns and E \u2208 Rm\u00d7N is the error matrix. In Eq. (5), the number of non-zero components in a column of Z is a random number drawn from Poisson(ao/bo) [56]. Thus, sparsity can be imposed on the representation with the help of parameters ao and bo. The components of the kth row of Z are independent draws from Bernoulli(\u03c0k). Let \u03c0 \u2208 R|K| be a vector with \u03c0k\u2208K, as its kth element. This vector governs the probability of selection of the columns of \u03a6 in the expansion of the data. Existence of this physically meaningful latent vector in the Beta Process based matrix factorization plays a central role in the proposed approach for discriminative dictionary learning."}, {"heading": "IV. PROPOSED APPROACH", "text": "We propose a Discriminative Bayesian Dictionary Learning approach for classification. For the cth class, the proposed approach draws |Ic| binary vectors zci \u2208 R|K|, \u2200i \u2208 Ic using a Beta Process. For each class, the vectors are sampled using separate draws with the same base. That is, the matrix factorization is governed by a set of C probability vectors \u03c0c\u2208{1,...,C}, instead of a single vector, however the inferred dictionary is shared by all the classes. An element of the aforementioned set, i.e. \u03c0c \u2208 R|K|, controls the probability of selection of the dictionary atoms for a single class data. This promotes the discriminative abilities of the inferred dictionary.\n5 (a) AR database [1] (b) Extended YaleB database [2]\nFig. 2: Examples of how recognition accuracy is affected with varying dictionary size: \u03ba = 0 for LC-KSVD1 and \u03c5 = 0 for D-KSVD in Eq. (3). All other parameters are kept constant at optimal values reported in [9]. For the AR database, 2000 training instances are used and testing is performed with 600 instances. For the Extended YaleB, half of the database is used for training and the other half is used for testing. The instances are selected uniformly at random."}, {"heading": "A. The Model", "text": "Let \u03b1ci \u2208 R|K| denote the sparse code of the ith training instance of the cth class, i.e. xci \u2208 Rm, over a dictionary \u03a6 \u2208 Rm\u00d7|K|. Mathematically, xci = \u03a6\u03b1ci+ i, where i \u2208 Rm denotes the modeling error. We can directly use the Beta Process discussed in Section III for computing the desired sparse code and the dictionary. However, the model employed by the Beta Process is restrictive, as it only allows the code to be binary. To overcome this restriction, let \u03b1ci = z c i sci , where\ndenotes the Hadamard/element-wise product, zci \u2208 R|K| is the binary vector and sci \u2208 R|K| is a weight vector. We place a standard normal prior N (scik|0, 1/\u03bbcso) on the k\nth component of the weight vector scik, where \u03bb c so denotes the precision of the distribution. In here, as in the following text, we use the subscript \u2018o\u2019 to distinguish the parameters of the prior distributions. The prior distribution over the kth component of the binary vector is Bernoulli(zcik|\u03c0cko). We draw the atoms of the dictionary from a multivariate Gaussian base, i.e. \u03d5k \u223c N (\u03d5k|\u00b5ko ,\u039b \u22121 ko ), where \u00b5ko \u2208 R m is the mean vector and \u039bko \u2208 Rm\u00d7m is the precision matrix for the kth atom of the dictionary. We model the error as zero mean Gaussian in Rm. Thus, we arrive at the following representation model:\nxci = \u03a6\u03b1 c i + i \u2200i \u2208 Ic,\u2200c \u03b1ci = z c i sci zcik \u223c Bernoulli(zcik|\u03c0cko) scik \u223c N (scik|0, 1/\u03bbcso) \u03c0ck \u223c Beta(\u03c0ck|ao/K, bo(K \u2212 1)/K) \u03d5k \u223c N (\u03d5k|\u00b5ko ,\u039b \u22121 ko\n) \u2200k \u2208 K i \u223c N ( i|0,\u039b\u22121 o ) \u2200i \u2208 {1, ..., N} (6)\nNotice, in the above model a conjugate Beta prior is placed over the parameter of the Bernoulli distribution, as mentioned in Section III. Hence, a latent probability vector \u03c0c (with \u03c0ck as its components) is associated with the dictionary atoms for the representation of the data from the cth class. The common dictionary \u03a6 is inferred from C such vectors. In the above model, this fact is notationally expressed by showing the dictionary atoms being sampled from a common set of |K| distributions, while distinguishing the class-specific variables in\nFig. 3: Graphical representation of the proposed discriminative Bayesian dictionary learning model.\nthe other notations with a superscript \u2018c\u2019. We assume the same statistics for the modeling error over the complete training data2. We further place non-informative Gamma hyper-priors over the precision parameters of the normal distributions. That is, \u03bbcs \u223c \u0393(\u03bbcs|co, do) and \u03bb \u223c \u0393(\u03bb |eo, fo), where co, do, eo and fo are the parameters of the respective Gamma distributions. Here, we allow the error to have an isotropic precision, i.e. \u039b = \u03bb Im, where Im denotes the identity matrix in Rm\u00d7m. The graphical representation of the complete model is shown in Fig. 3.\nB. Inference Gibbs sampling is used to perform Bayesian inference over the proposed model3. Starting with the dictionary, below we\n2It is also possible to use different statistics for different classes, however, in practice the assumption of similar noise statistics works well. We adopt the latter to avoid unnecessary complexity.\n3Paisley and Carin [56] derived variational Bayesian algorithm [58] for their model. It was shown by Zhou et al. [44] that Gibbs sampling is an equally effective strategy in data representation using the same model. Since it is easier to relate the Gibbs sampling process to the learning process of conventional optimization based sparse representation (e.g. K-SVD [6]), we derive expressions for the Gibbs sampler for our approach. Due to the conjugacy of the model, these expressions can be derived analytically.\n6 derive analytical expressions for the posterior distributions over the model parameters for the Gibbs sampler. The inference process performs sampling over these posterior distributions. The expressions are derived assuming zero mean Gaussian prior over the dictionary atoms, with isotropic precision. That is, \u00b5ko = 0 and \u039bko = \u03bbkoIm. This simplification leads to faster sampling, without significantly affecting the accuracy of the approach. The sampling process samples the atoms of the dictionary one-by-one from their respective posterior distributions. This process is analogous to the atom-by-atom dictionary update step of K-SVD [6], however the sparse codes remain fixed during our dictionary update.\nSampling \u03d5k: For our model, we can write the following about the posterior distribution over a dictionary atom:\np(\u03d5k|\u2212) \u221d N\u220f i=1 N (xi|\u03a6(zi si), \u03bb\u22121 o Im)N (\u03d5k|0, \u03bb \u22121 ko Im).\nHere, we intentionally dropped the superscript \u2018c\u2019 as the dictionary is updated using the complete training data. Let xi\u03d5k denote the contribution of the dictionary atom \u03d5k to the ith training instance xi:\nxi\u03d5k = xi \u2212\u03a6(zi si) +\u03d5k(zik sik). (7)\nUsing Eq. (7), we can re-write the aforementioned proportionality as p(\u03d5k|\u2212) \u221d N\u220f i=1 N (xi\u03d5k |\u03d5k(ziksik), \u03bb \u22121 o Im)N (\u03d5k|0, \u03bb \u22121 ko Im).\nConsidering the above expression, the posterior distribution over a dictionary atom can be written as\np(\u03d5k|\u2212) = N (\u03d5k|\u00b5k, \u03bb\u22121k Im), (8)\nwhere,\n\u00b5k = \u03bb o \u03bbk N\u2211 i=1 (zik.sik)xi\u03d5k ,\n\u03bbk = \u03bbko + \u03bb o N\u2211 i=1 (zik.sik) 2.\nSampling zcik: Once the dictionary atoms have been sampled, we sample zcik, \u2200i \u2208 Ic, \u2200k \u2208 K. Using the contribution of the kth dictionary atom, the posterior probability distribution over zcik can be expressed as\np(zcik|\u2212) \u221d N (xci\u03d5k |\u03d5k(z c ik.s c ik), \u03bb \u22121 o Im)Bernoulli(z c ik|\u03c0cko).\nHere we are concerned with the cth class only, therefore xci\u03d5k is computed with the cth class data in Eq. (7). With the prior probability of zcik = 1 given by \u03c0 c ko\n, we can write the following about its posterior probability:\np(zcik = 1|\u2212) \u221d \u03c0cko exp ( \u2212\u03bb o\n2 ||xci\u03d5k \u2212\u03d5ks c ik||22\n) .\nIt can be shown that the right hand side of the above proportionality can be written as:\np1 = \u03c0 c ko\u03b61\u03b62,\nwhere, \u03b61 = exp ( \u2212\u03bb os c ik\n2 (||\u03d5k|| 2 2s c ik \u2212 2(xci\u03d5k )\nT\u03d5k) ) and\n\u03b62 = exp ( \u2212\u03bb o2 ||x c i\u03d5k ||22 )\n. Furthermore, since the prior probability of zcik = 0 is given by 1 \u2212 \u03c0cko , we can write the following about its posterior probability:\np(zcik = 0|\u2212) \u221d (1\u2212 \u03c0cko)\u03b62.\nThus, zcik can be sampled from the following normalized Bernoulli distribution:\nBernoulli ( zcik \u2223\u2223\u2223 p1 p1 + (1\u2212 \u03c0cko)\u03b62 ) .\nBy inserting the value of p1 and simplifying, we finally arrive at the following expression for sampling zcik:\nzcik \u223c Bernoulli ( zcik \u2223\u2223\u2223 \u03c0cko\u03b61 1 + \u03c0cko(\u03b61 \u2212 1) ) . (9)\nSampling scik: We can write the following about the posterior distribution over scik:\np(scik|\u2212) \u221d N (xci\u03d5k |\u03d5k(z c ik.s c ik), \u03bb \u22121 o Im)N (s c ik|0, 1/\u03bbcso).\nAgain, notice that we are concerned with the cth class data only. In light of the above expression, scik can be sampled from the following posterior distribution:\np(scik|\u2212) = N (scik|\u00b5cs, 1/\u03bbcs), (10)\nwhere,\n\u00b5cs = \u03bb o \u03bbcs zcik\u03d5 T kx c i\u03d5k , \u03bbcs = \u03bb c so + \u03bb o(z c ik) 2||\u03d5k||22.\nSampling \u03c0ck: Based on our model, we can also write the posterior probability distribution over \u03c0ck as\np(\u03c0ck|\u2212)\u221d \u220f i\u2208Ic Bernoulli(zcik|\u03c0cko)Beta ( \u03c0cko \u2223\u2223\u2223ao K , bo(K \u2212 1) K ) .\nUsing the conjugacy between the distributions, it can be easily shown that the kth component of \u03c0c must be drawn from the following posterior distribution during the sampling process:\np(\u03c0ck|\u2212)=Beta ( \u03c0ck \u2223\u2223\u2223ao K + \u2211 i\u2208Ic zcik, bo(K \u2212 1) K + |Ic| \u2212 \u2211 i\u2208Ic zcik ) (11)\nSampling \u03bbcs: In our model, the components of the weight vectors are drawn from a standard normal distribution. For a given weight vector, common priors are assumed over the precision parameters of these distributions. This allows us to express the likelihood function for \u03bbcs in terms of standard multivariate Gaussian with isotropic precision. Thus, we can write the posterior over \u03bbcs as the following:\np(\u03bbcs|\u2212) \u221d \u220f i\u2208Ic N ( sci \u2223\u2223\u22230, 1 \u03bbcso I|K| ) \u0393(\u03bbcso |co, do).\n7 Using the conjugacy between the Gaussian and Gamma distributions, it can be shown that \u03bbcs must be sampled as follows:\n\u03bbcs \u223c \u0393 ( \u03bbcs \u2223\u2223\u2223 |K|Nc 2 + co, 1 2 \u2211 i\u2208Ic ||sci ||22 + do ) . (12)\nSampling \u03bb : We can write the posterior over \u03bb as\np(\u03bb |\u2212) \u221d N\u220f i=1 N (xi|\u03a6(zi si), \u03bb\u22121 o Im)\u0393(\u03bb o |eo, fo).\nSimilar to \u03bbcs, we can arrive at the following for sampling \u03bb during the inferencing process:\n\u03bb \u223c \u0393( mN\n2 + eo,\n1\n2 N\u2211 i=1 ||xi \u2212\u03a6(zi si)||22 + fo). (13)\nAs a result of Bayesian inference over the model, we obtain sets of posterior distributions over the model parameters. We are particularly interested in two of them. Namely, the set of distributions over the dictionary atoms \u2135 def= {N (\u03d5k|\u00b5k,\u039b \u22121 k ) : k \u2208 K} \u2282 Rm, and the set of probability distributions characterized by the vectors \u03c0c\u2208{1,...,C} \u2208 R|K|. Momentarily, we defer the discussion on the latter. The former is used to compute the desired dictionary \u03a6. This is done by drawing multiple samples from the elements of \u2135 and estimating the corresponding dictionary atoms as respective means of the samples. Indeed, the mean parameters of the elements of \u2135 can also be chosen as the desired dictionary atoms. However, we prefer the former approach for robustness.\nThe proposed model and the sampling process also results in inferring the correct size of the desired dictionary. We present the following Lemmas in this regard:\nLemma 4.1: For K \u2192\u221e, E[\u03be] = aobo ,\u2200c, where \u03be = K\u2211 k=1 zcik.\nProof:4 According to the proposed model, the covariance of a data vector from the cth class can be given by:\nE[(xci )(xci )T] = aoK ao + bo(K \u2212 1) \u039b\u22121ko \u03bbcso + \u039b\u22121 o (14)\nIn Eq. (14), fraction aoao+bo(K\u22121) appears due to the presence of zci in the model and the equation simplifies to E[(xci )(xci )T] = K\n\u039b\u22121ko \u03bbcso + \u039b\u22121 o when we neglect z c i . Here, K signifies the number of dictionary atoms required to represent the data vector. Notice in the equation, that as K \u2192 \u221e, we observe E[(xci )(xci )T]\u2192 aobo \u039b\u22121ko \u03bbcso +\u039b\u22121 o . Thus, in the limit K \u2192\u221e, ao bo corresponds to the expected number of non-zero components\nin zci , given by E[\u03be], where \u03be = K\u2211 k=1 zcik.\nLemma 4.2: Once \u03c0ck = 0 in a given iteration of the sampling process, E[\u03c0ck] \u2248 0 for the later iterations.\nProof: According to Eq. (9), \u2200i \u2208 Ic, zcik = 0 when \u03c0cko = 0. Once this happens, the posterior distribution over \u03c0ck becomes Beta ( \u03c0ck \u2223\u2223\u2223a\u0302, b\u0302), where a\u0302 = aoK and b\u0302 = bo(K\u22121)K + |Ic| (see 4We follow [56] closely in the proof, however, our analysis also takes into account the class labels of the data, whereas no such data discrimination is assumed in [56].\nEq. 11). Thus, the expected value of \u03c0ck for the later iterations can be written as E[\u03c0ck] = a\u0302 a\u0302+b\u0302\n= aoao+bo(K\u22121)+K|Ic| . With 0 < ao, bo < |Ic| K we can see that E[\u03c0ck] \u2248 0.\nIn the Gibbs sampling process, we start with K \u2192 \u221e in our implementation and let 0 < ao, bo < |Ic|. Considering Lemma 4.1, the values of ao and bo are set to ensure that the resulting representation is sparse. We drop the kth dictionary atom during the sampling process if \u03c0ck = 0, for all the classes simultaneously. According to Lemma 4.2, dropping such an atom does not bring significant changes to the final representation. Thus, by removing the redundant dictionary atoms in each sampling iteration, we finally arrive at the correct size of the desired dictionary, i.e. |K|.\nAs mentioned above, with Bayesian inference over the proposed model we also infer a set of probability vectors \u03c0c\u2208{1,...,C}. Each element of this set, i.e. \u03c0c \u2208 R|K|, further characterizes a set of probability distributions =c def= {Bernoulli(\u03c0ck) : k \u2208 K} \u2282 R. Here, Bernoulli(\u03c0ck) is jointly followed by all the kth components of the sparse codes for the cth class. If the kth dictionary atom is commonly used in representing the cth class training data, we must expect a high value of \u03c0ck, and \u03c0 c k \u2192 0 otherwise. In other words, for an arranged dictionary, components of \u03c0c having large values should generally cluster well if the learned dictionary is discriminative. Furthermore, these clusters must appear at different locations in the inferred vectors for different classes. Such clusterings would demonstrate the discriminative character of the inferred dictionary. Fig. 4 verifies this character for the dictionaries inferred under the proposed model. Each row of the figure plots six different probability vectors (i.e. \u03c0c) for different training datasets. A clear clustering of the high value components of the vectors is visible in each plot. Detailed experiments are presented in Section V."}, {"heading": "C. Classification", "text": "Let y \u2208 Rm be a query signal. We follow the common methodology [9], [7] for classification that first encodes y over the inferred dictionary such that y = \u03a6\u03b1\u0302 + , and then computes ` = W\u03b1\u0302, where W \u2208 RC\u00d7|K| contains model parameters of a multi-class linear classifier. The query is assigned the class label corresponding to the largest component of ` \u2208 RC . The main difference between the classification approach of this work and that of the existing techniques is in the learning process of W. Whereas discrimination is induced in \u03a6 by the joint optimization of W and \u03a6 in the existing techniques (see Eq. 3), this is already achieved in the inference process of the proposed approach. Thus, it is possible to optimize a classifier separately from the dictionary learning process without affecting the discriminative abilities of the learned dictionary.\nLet hci \u2208 RC be a binary vector with the only 1 appearing at the cth index, indicating the class of the training instance xci . Let H \u2208 RC\u00d7N be the binary index matrix formed by such vectors for the complete training data X. We aim at computing W such that H = WB + E, where E \u2208 RC\u00d7N denotes the modeling error and B \u2208 R|K|\u00d7N is the coefficient matrix. Notice that, we can directly use the model in Eq. (6)\n8\nto compute W. For that, we can write hci = W\u03b2 c i + i, where \u03b2ci \u2208 R|K| is a column of B. Thus, we infer W under the Bayesian framework using the model proposed in Eq. (6). While learning this matrix, we perform Gibbs sampling such that the probability vectors \u03c0c\u2208{1,...,C} are kept constant to those finally inferred by the dictionary learning stage. That is, wherever required, the value of \u03c0ck is directly used from \u03c0c\u2208{1,...,C} instead of inferring a new value during the sampling process.\nThe reason for using the same \u03c0c\u2208{1,...,C} vectors for inferring W and \u03a6 is straightforward. Since we first sparse code the query over the learned discriminative dictionary, we expect the underlying support of the learned codes to follow some \u03c0c closely. Thus, W can be expected to classify the learned codes better if the discriminative information regarding their support is encoded in it. Notice that, unlike the existing approaches (e.g. [7], [9]) the coupling between W and \u03a6 is kept probabilistic in our approach. We do not assume that the \u2019exact values\u2019 of the sparse codes of the query would match to those of the training sample (and hence W and \u03a6 should be trained jointly), rather, our assumption is that samples from the same class are more likely explainable using similar basis. Therefore, coupling between W and \u03a6 is kept in terms of probabilistic selection of their columns. Our view point also makes Orthogonal Matching Pursuit (OMP) [60] a natural choice for sparse coding the query over the dictionary. This greedy pursuit algorithm efficiently searches for the right basis to represent the data. Therefore, we used OMP in sparse coding the query over the learned dictionary.\nD. Initialization\nFor inferring the dictionary, we need to first initialize \u03a6, zci , s c i and \u03c0 c k. We initialize \u03a6 by randomly selecting the training instances with replacement. We sparsely encode xci over the initial dictionary using OMP [60]. The sparse codes are considered as the initial sci , whereas their support forms the initial vector zci . Computing the initial s c i and z c i with other methods, such as regularized least squares, is equally effective. We set \u03c0ck = 0.5,\u2200c,\u2200k for the initialization. Notice, this means that all the dictionary atoms initially have equal chances of getting selected in the expansion of a training instance from any class. The values of \u03c0ck,\u2200c, \u2200k finally inferred by the dictionary learning process serve as the initial values of these parameters for learning the classifier. Similarly, the vectors zci and sci computed by the dictionary learning stage are used for initializing the corresponding vectors for the classifier. We initialize W using the ridge regression model [61] with the `2-norm regularizer and quadratic loss:\nW = min W ||H\u2212W\u03b1i||2 + \u03bb||W||22, \u2200i \u2208 {1, ..., N}, (15)\nwhere \u03bb is the regularization constant. The computation is done over the complete training data, therefore the superscript \u2018c\u2019 is dropped in the above equation."}, {"heading": "V. EXPERIMENTS", "text": "We have evaluated the proposed approach on two face data sets: the Extended YaleB [2] and the AR database [1], a data set for object categories: Caltech-101 [3], a data set for scene\n9 categorization: Fifteen scene categories [4], and an action data set: UCF sports actions [5]. These data sets are commonly used in the literature for evaluation of sparse representation based classification techniques. We compare the performance of the proposed approach with SRC [8], the two variants of LabelConsistent K-SVD [9] (i.e. LC-KSVD1, LC-KSVD2), the Discriminative K-SVD algorithm (D-KSVD) [7], the Fisher Discrimination Dictionary Learning algorithm (FDDL) [10] and the Dictionary Learning based on separating the Commonalities and the Particularities of the data (DL-COPAR) [11]. In our comparisons, we also include results of unsupervised sparse representation based classification that uses K-SVD [6] as the dictionary learning technique and separately computes a multi-class linear classifier using Eq. (15).\nFor all of the above mentioned methods, except SRC and DKSVD, we acquired the public codes from the original authors. To implement SRC, we used the LASSO [63] solver of the SPAMS toolbox [62]. For D-KSVD, we used the public code provided by Jiang et al. [9] for LC-KSVD2 algorithm and solved Eq. (3) with \u03c5 = 0. The experiments are performed on an Intel Core i7-2600 CPU at 3.4 GHz with 8 GB RAM. We performed our own experiments using the above mentioned methods and the proposed approach using the same data. The parameters of the existing approaches were carefully optimized following the guidelines of the original works. We mention the used parameter values and, where it exists, we note the difference between our values and those used in the original works. In our experiments, these differences were made to favor the existing approaches. Results of the approaches other than those mentioned above, are taken directly from the literature, where the same experimental protocol has been followed.\nFor the proposed approach, the used parameter values were as follows. In all experiments, we chose K = 1.5N for initialization, whereas co, do, eo and fo were all set to 10\u22126. We selected ao = bo = minc |Ic| 2 , whereas \u03bbso and \u03bbko were set to 1 and m, respectively. Furthermore, \u03bb o was set to 10 6 for all the datasets except for Fifteen Scene Categories [4], where we used \u03bb o = 10\n9. In each experiment, the Bayesian inference was performed with 35 Gibbs sampling iterations. We defer further discussion on the selection of the parameter values to Section VI."}, {"heading": "A. Extended YaleB", "text": "Extended YaleB [2] contains 2,414 frontal face images of 38 different people, each having about 64 samples. The images are acquired under varying illumination conditions and the subjects have different facial expressions. This makes the database fairly challenging, see Fig 5a for examples. In our experiments, we used the random face feature descriptor [8], where a cropped 192 \u00d7 168 pixels image was projected onto a 504-dimensional vector. For this, the projection matrix was generated from random samples of standard normal distributions. Following the common settings for this database, we chose one half of the images for training and the remaining samples were used for testing. We performed ten experiments by randomly selecting the samples for training and testing.\n(a) Extended YaleB [2]\n(b) AR database [1]\nFig. 5: Examples from the face databases.\nBased on these experiments, the mean recognition accuracies of different approaches are reported in Table I. The results for Locality-constrained Linear Coding (LLC) [15] is directly taken from [9], where the accuracy is computed using 70 local bases.\nSimilar to Jiang et al. [9], the sparsity threshold for K-SVD, LC-KSVD1, LC-KSVD2 and D-KSVD was set to 30 in our experiments. Larger values of this parameter were found to be ineffective as they mainly resulted in slowing the algorithms without improving the recognition accuracy. Furthermore, as in [9], we used \u03c5 = 4.0 for LC-KSVD1 and LC-KSVD2, whereas \u03ba was set to 2.0 for LC-KSVD2 and D-KSVD in Eq. (3). Keeping these parameter values fixed, we optimized for the number of dictionary atoms for each algorithm. This resulted in selecting 600 atoms for LC-KSVD2, D-KSVD and K-SVD, whereas 500 atoms consistently resulted in the best performance of LC-KSVD1. This value is set to 570 in [9] for all of the four methods. In all techniques that learn dictionaries, we used the complete training data in the learning process. Therefore, all training samples were used as dictionary atoms for SRC. Following [8], we set the residual error tolerance to 0.05 for SRC. Smaller values of this parameter also resulted in very similar accuracies. For FDDL, we followed [10] for the optimized parameter settings. These settings are the same as those reported for AR database in the original work. We refer the reader to the original work for the list of the parameters and their exact values. The results reported in the table are obtained by the Global Classifier (GC) of FDDL, which showed better performance than the Local Classifier (LC). For the parameter settings of DL-COPAR we followed the original work [11]. We fixed 15 atoms for each class and a set of 5 atoms was chosen to learn commonalities of the classes. The reported results are achieved by LC, that performed better than GC in our experiments.\nIt is clear from Table I that our approach outperforms the above mentioned approaches in terms of recognition accuracy, with nearly 23% improvement over the error rate of the second best approach. Furthermore, the time required by the proposed approach for classifying a single test instance is also very low as compared to SRC, FDDL and DL-COPAR. For the pro-\n10\nposed approach, this time is comparable to D-KSVD and LCKSVD. Like these algorithms, the computational efficiency in the classification stage of our approach comes from using the learned multi-class linear classifier to classify the sparse codes of a test instance."}, {"heading": "B. AR Database", "text": "This database contains more than 4,000 color face images of 126 people. There are 26 images per person taken during two different sessions. In comparison to Extended YaleB, the images in AR database have larger variations in terms of facial expressions, disguise and illumination conditions. Samples from AR database are shown in Fig. 5b for illustration. We followed a common evaluation protocol in our experiments for this database, in which we used a subset of 2600 images pertaining to 50 males and 50 female subjects. For each subject, we randomly chose 20 samples for training and the rest for testing. The 165\u00d7120 pixel images were projected onto a 540-dimensional vector with the help of a random projection matrix, as in Section V-A. We report the average recognition accuracy of our experiments in Table II, which also includes the accuracy of LLC [15] reported in [9]. The mean values reported in the table are based on ten experiments.\nIn our experiments, we set the sparsity threshold for K-SVD, LC-KSVD1, LC-KSVD2 and D-KSVD to 50 as compared to 10 and 30 which was used in [7] and [9], respectively. Furthermore, the dictionary size for K-SVD, LC-KSVD2 and D-KSVD was set to 1500 atoms, whereas the dictionary size for LC-KSVD1 was set to 750. These large values (compared to 500 used in [7], [9]) resulted in better accuracies at the expense of more computation. However, the classification time per test instance remained reasonably small. In Table II, we also include the results of LC-KSVD1, LC-KSVD2 and DKSVD using the parameter values proposed in the original works. These results are distinguished with the \u2021 sign. For FDDL and DL-COPAR we used the same parameter settings as in Section V-A. The reported results are for GC and LC for FDDL and DL-COPAR, respectively. For SRC we set the residual error tolerance to 10\u22126. This small value gave the best results.\nFrom Table II, we can see that the proposed approach performs better than the existing approaches in terms of accuracy. The recognition accuracies of SRC and FDDL are fairly close to our approach however, these algorithms require large amount of time for classification. This fact compromises\ntheir practicality. In contrast, the proposed approach shows high recognition accuracy (i.e. 22% reduction in the error rate as compared to SRC) with less than 1.5 ms required for classifying a test instance. The relative difference between the classification time of the proposed approach and the existing approaches remains similar in the experiments below. Therefore, we do not explicitly note these timings for all of the approaches in these experiments."}, {"heading": "C. Caltech-101", "text": "The Caltech-101 database [3] comprises 9, 144 samples from 102 classes. Among these, there are 101 object classes (e.g. minarets, trees, signs) and one \u201cbackground\u201d class. The number of samples per class varies from 31 to 800, and the images within a given class have significant shape variations, as can be seen in Fig. 6. To use the database, first the SIFT descriptors [64] were extracted from 16 \u00d7 16 image patches, which were densely sampled with a 6-pixels step size for the grid. Then, based on the extracted features, spatial pyramid features [38] were extracted with 2l\u00d72l grids, where l = 0, 1, 2. The codebook for the spatial pyramid was trained using k-means with k = 1024. Then, the dimension of a spatial pyramid feature was reduced to 3000 using PCA. Following the common experimental protocol, we selected 5, 10, 15, 20, 25 and 30 instances for training the dictionary and the\n11\nremaining instances were used in testing, in our six different experiments. Each experiment was repeated ten times with random selection of train and test data. The mean accuracies of these experiments are reported in Table III.\nFor this dataset, we set the number of dictionary atoms used by K-SVD, LC-KSVD1, LC-KSVD2 and D-KSVD to the number of training examples available. This resulted in the best performance of these algorithms. The sparsity level was also set to 50 and \u03c5 and \u03ba were set to 0.001. Jiang et al. [9] also suggested the same parameter settings. For SRC, the error tolerance of 10\u22126 gave the best results in our experiments. We used the parameter settings for object categorization given in [10] for FDDL. For DL-COPAR, the selected number of classspecific atoms were kept the same as the number of training instances per class, whereas the number of shared atoms were fixed to 314, as in the original work [11]. For this database GC performed better than LC for DL-COPAR in our experiments.\nFrom Table III, it is clear that the proposed approach consistently outperforms the competing approaches. For some cases the accuracy of LC-KSVD2 is very close to the proposed approach, however with the increasing number of training instances the difference between the results increases in favor of the proposed approach. This is an expected phenomenon since more training samples result in more precise posterior distributions in Bayesian settings. Here, it is also worth mentioning that being Bayesian, the proposed approach is inherently an online technique. This means, in our approach, the computed posterior distributions can be used as prior distributions for further inference if more training data is available. Moreover, our approach is able to handle a batch of large training data more efficiently than LC-KSVD [9] and D-KSVD [7]. This can be verified by comparing the training time of the approaches in Table IV. The timings are given for complete training and testing durations for Caltech-101 database, where we used a batch of 30 images per class for training and the remaining images were used for testing. We note that, like all the other approaches, good initialization (using the procedure presented in Section IV-D) also contributes towards the computational efficiency of our approach. The training time in the table also includes the initialization time for all the approaches. Note that the testing time of the proposed approach is very similar to those of the other approaches in Table IV."}, {"heading": "D. Fifteen Scene Category", "text": "The Fifteen Scene Category dataset [4] has 200 to 400 images per category for fifteen different kinds of scenes. The scenes include images from kitchens, living rooms and country sides etc. In our experiments, we used the Spatial Pyramid Features of the images, which have been made public by Jiang et al. [9]. In this data, each feature descriptor is a 3000-dimensional vector. Using these features, we performed experiments by randomly selecting 100 training instances per class and considering the remaining as the test instances.\nClassification accuracy of the proposed approach is compared with the existing approaches in Table V. The reported mean values are computed over ten experiments. We set the error tolerance for SRC to 10\u22126 and used the parameter settings suggested by Jiang et al. [9] for LC-KSVD1, LCKSVD2 and D-KSVD. Parameters of DL-COPAR were set as suggested in the original work [11] for the same database. The reported results are obtained by LC for DL-COPAR. Again, the proposed approach shows more accurate results than the existing approaches. The accuracy of the proposed approach is 1.66% more than LC-KSVD2 on the used dataset."}, {"heading": "E. UCF Sports Action", "text": "This database comprises video sequences that are collected from different broadcast sports channels (e.g. ESPN and BBC) [5]. The videos contain 10 categories of sports actions that include: kicking, golfing, diving, horse riding, skateboarding, running, swinging, swinging highbar, lifting and walking. Examples from this dataset are shown in Fig. 8. Under the common evaluation protocol we performed fivefold cross validation over the dataset, where four folds are used in training and the remaining one is used for testing. Results, computed as the average of the five experiments, are summarized in Table VI. For D-KSVD, LC-KSVD1 and LC-KSVD2 we followed [9] for the parameter settings. Again, the value of 10\u22126 (along with similar small values) resulted in the best accuracies for SRC.\n12\nIn the Table, the results for some specific action recognition methods are also included, for instance, Qui et al. [33] and action back feature with SVM [40]. These results are taken directly from [13] along the results of DLSI [12], DL-COPAR [11] and FDDL [10]5. Following [40], we also performed leave-one-out cross validation on this database for the proposed approach. Our approach achieves 95.7% accuracy under this protocol, which is 0.7% better than the state-of-the-art results claimed in [40]."}, {"heading": "VI. DISCUSSION", "text": "In our experiments, we chose the values of K, ao and bo in light of the theoretical results presented in Section IV-B. By setting K > N we make sure that K is very large. The results mainly remain insensitive to other similar large values of this parameter. The chosen values of ao and bo ensure that 0 < ao, bo < |Ic|. We used large values for \u03bb o in our experiments as this parameter represents the precision of the white noise distribution in the samples. The datasets used in our experiments are mainly clean in terms of white noise. Therefore, we achieved the best performance with \u03bb o \u2265 106. In the case of noisy data, this parameter value can be adjusted accordingly. For UCF sports action dataset \u03bb o = 10\n9 gave the best results because less number of training samples were available per class. It should be noted that the value of \u03bb increases as a result of Bayesian inference with the availability of more clean training samples. Therefore, we adjusted the precision parameter of the prior distribution to a larger value for UCF dataset. Among the other parameters, co to fo were fixed to 10\u22126. Similar small non-negative values can also be used without affecting the results. This fact can be easily verified by noticing the large values of the other variables involved in equations (12) and (13), where these parameters\n5The results of DL-COPAR [11] and FDDL [10] are taken directly from the literature because the optimized parameter values for these algorithms are not previously reported for this dataset. Our parameter optimization did not outperform the reported accuracies.\nare used. With the above mentioned parameter settings and the initialization procedure presented in Section IV-D, the Gibbs sampling process converges quickly to the desired distributions and the correct number of dictionary atoms, i.e. |K|. In Fig. 9, we plot the value of |K| as a function of Gibbs sampling iterations during dictionary training. Each plot represent a complete training process for one dataset. It can be easily seen that the first 10 iterations of the Gibbs sampling process were enough to infer the correct size of the dictionary. However, It should be mentioned that this fast convergence also owes to the initialization process adopted in this work. In our experiments, while sparse coding a test instance over the learned dictionary, we consistently used the sparsity threshold of 50 for all the datasets except for the UCF [5], for which this parameter was set to 40 because of the smaller dictionary resulting from less training samples. In all the experiments, these values were also kept the same for K-SVD, LC-KSVD1, LC-KSVD2 and D-KSVD for fair comparisons."}, {"heading": "VII. CONCLUSION", "text": "We proposed a non-parametric Bayesian approach for learning discriminative dictionaries for sparse representation of data. The proposed approach employs a Beta process to infer a discriminative dictionary and sets of Bernoulli distributions associating the dictionary atoms to the class labels of the training data. The said association is adaptively built during Bayesian inference and it signifies the selection probabilities of dictionary atoms in the expansion of class-specific data. The inference process also results in computing the correct size of the dictionary. For learning the discriminative dictionary, we presented a hierarchical Bayesian model and the corresponding inference equations for Gibbs sampling. The proposed model is also exploited in learning a linear classifier that finally classifies the sparse codes of a test instance that are learned using the inferred discriminative dictionary. The proposed approach is evaluated for classification using five different databases of human face, human action, scene category and object images. Comparisons with state-of-the-art discriminative sparse representation approaches show that the proposed Bayesian approach consistently outperforms these approaches and has computational efficiency close to the most efficient approach.\nWhereas its effectiveness in terms of accuracy and computation is experimentally proven in this work, there are also other key advantages that make our Bayesian approach to discriminative sparse representation much more appealing than the existing optimization based approaches. Firstly, the Bayesian framework allows us to learn an ensemble of discriminative\n13\ndictionaries in the form of probability distributions instead of the point estimates that are learned by the optimization based approaches. Secondly, it provides a principled approach to estimate the required dictionary size and we can associate the dictionary atoms and the class labels in a physically meaningful manner. Thirdly, the Bayesian framework makes our approach inherently an online technique. Furthermore, the Bayesian framework also provides an opportunity of using domain/class-specific prior knowledge in our approach in a principled manner. This can prove beneficial in many applications. For instance, while classifying the spectral signatures of minerals on pixel and sub-pixel level in remote-sensing hyperspectral images, the relative smoothness of spectral signatures [65] can be incorporated in the inferred discriminative bases. For this purpose, Gaussian Processes [66] can be used as a base measure for the Beta Process. Adapting the proposed approach for remote-sensing hyperspectral image classification is also our future research direction."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported by ARC Grant DP110102399."}], "references": [{"title": "From Few to Many: Illumination Cone Models for Face Recognition under Variable Lighting and Pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Learning Generative Visual Models from Few Training Samples: An Incremental Bayesian Approach Tested on 101 Object Categories", "author": ["L. FeiFei", "R. Fergus", "P. Perona"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshop Generative Model Based Vision,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Beyond bag of features: spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In Proc. IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "A spatio-temporal maximum average correlation height filter for action recognition", "author": ["M. Rodriguez", "J. Ahmed", "M. Shah"], "venue": "In Proc. IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Discriminative K-SVD for Dictionary Learning in Face Recognition", "author": ["Q. Zhang", "B. Li"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Robust Face Recognition via Sparse Representation", "author": ["J. Wright", "M. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Label Consistent K-SVD: Learning a Discriminative Dictionary for Recognition", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol.35,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Sparse Representation Based Fisher Discrimination Dictionary Learning for Image Classification", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": "International Journal of Computer Vision, vol.109,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "A Classification-oriented Dictionary Learning Model: Explicitly Learning the Particularity and Commonality Across Categories", "author": ["D. Wang", "S. Kong"], "venue": "Pattern Recognition, vol.47,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Classification and Clustering via Dictionary Learning with Structured Incoherence and Shared Features", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": "In Proc. IEEE Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Latent Dictionary Learning for Sparse Representation based Classification, In", "author": ["M. Yang", "D. Dai", "L. Shen", "L.V. Gool"], "venue": "Proc. IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Learning Discriminative Dictionary for Group Sparse Representation", "author": ["Y. Sun", "Q. Liu", "J. Tang", "D. Tao"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Locality Constrained Linear Coding for Image Classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T.Huang", "Y. Gong"], "venue": "In Proc. IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Image Denoising Via Sparse and Redundant Representations Over Learned Dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Transactions on Image Processing, vol.15,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Sparse Representation for Color Image Restoration", "author": ["J. Mairal", "M. Elad", "G. Sapiro"], "venue": "IEEE Transactions on Image Processing, vol.17,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Sparse Spatio-spectral Representation for Hyperspectral Image Super Resolution", "author": ["N. Akhtar", "F. Shafait", "A. Mian"], "venue": "Proc. European Conf. on Computer Vision,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Trans. on Information Theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Morphological component analysis: An adaptive thresholding strategy", "author": ["J. Bobin", "J.L. Starck", "J.M. Fadili", "Y. Moudden", "D.L. Donoho"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Gabor feature based sparse representation for face recognition with gabor occlusion dictionary", "author": ["M. Yang", "L. Zhang"], "venue": "In Proc. Computer Vision?ECCV", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Robust sparse coding for face recognition", "author": ["M. Yang", "D. Zhang", "J. Yang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Metaface learning for sparse representation based face recognition", "author": ["M. Yang", "L. Zhang", "J. Yang", "D. Zhang"], "venue": "In Proc. IEEE International Conference on Image Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Sparse representation for signal classification", "author": ["K. Huang", "S. Aviyente"], "venue": "In Advances in Neural Information Processing systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "Supervised translation-invariant sparse coding", "author": ["J. Yang", "K. Yu", "T. Huang"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Sparse Representation for Image Classification: Learning discriminative and reconstructive non-parametric dictionaries", "author": ["F. Rodriguez", "G. Sapiro"], "venue": "Minnesota Univ. Minneapolis,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Task driven dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Sparse dictionary-based representation and recognition of action attributes", "author": ["Q. Qiu", "Z. Jiang", "R. Chellappa"], "venue": "In Proc. IEEE International Conference on Computer Vision pp", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Learning sparse representations for human action recognition", "author": ["G. Tanaya", "R.K. Ward"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Sparse modeling of human actions from motion imagery", "author": ["A. Castrodad", "G. Sapiro"], "venue": "International journal of computer vision,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "SVM-KNN: Discriminative nearest neighbor classification for visual category recognition", "author": ["H. Zhang", "A. Berg", "M. Maire", "J. Malik"], "venue": "In IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In Proc. IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Caltech-256 object category dataset, CIT Technical report", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "Action bank: A high-level representation of activity in video", "author": ["S. Sadanand", "J.J. Corso"], "venue": "In Proc. IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "An algorithm for machine calculation of complex Fourier series", "author": ["J.W. Cooley", "J.W. Tukey"], "venue": "Mathematics of Computation,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1965}, {"title": "A wavelet tour of signal processing, 2nd Edition, Sandiago", "author": ["S. Mallat"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1999}, {"title": "Compression of facial images using the K-SVD algorithm", "author": ["O. Bryt", "M. Elad"], "venue": "Journal of Visual Communication and Image Representation, vol.19,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "Non-parametric Bayesian dictionary learning for sparse image representations, In Advances in neural information processing", "author": ["M. Zhou", "H. Chen", "L. Ren", "G. Sapiro", "L. Carin", "J.W. Paisley"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2009}, {"title": "Joint learning and dictionary construction for pattern recognition", "author": ["D.S. Pham", "S. Venkatesh"], "venue": "In Proc. IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2008}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S.O. Aase", "J.H. Husoy"], "venue": "In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1999}, {"title": "Discriminative learned dictionaries for local image analysis", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "Dictionary learning and sparse coding for unsupervised clustering", "author": ["P. Sprechmann", "G. Sapiro"], "venue": "In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Learning Active Basis Model for Object Detection and Recognition", "author": ["Y.N. Wu", "Z. Si", "H. Gong", "S.C. Zhu"], "venue": "Int. Journal of Computer Vision, vol. 90,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2010}, {"title": "Max-Margin Dictionary Learning for Multiclass Image Categorization", "author": ["X.C. Lian", "Z. Li", "B.L. Lu", "L. Zhang"], "venue": "In Computer Vision (ECCV 2010),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2010}, {"title": "Submodular Dictionary Learning for Sparse Coding", "author": ["Z. Jiang"], "venue": "In Proc. IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2012}, {"title": "Algorithms for simultaneous sparse approximation: part I: Greedy pursuit", "author": ["J.A. Tropp", "A.C. Gilbert", "M.J. Strauss"], "venue": "Signal Process.,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2006}, {"title": "Extended SRC: Undersampled Face Recognition via Intraclass Variant Dictionary", "author": ["W. Deng", "J. Hu", "J. Guo"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2012}, {"title": "Learning inter-related visual dictionary for object recognition", "author": ["N. Zhou", "J.P. Fan"], "venue": "In Proc. IEEE Conf. on Computer Vision and Pattern Recognition", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2012}, {"title": "Multi-level Discriminative Dictionary Learning towards Hierarchical Visual Categorization", "author": ["L. Shen", "S. Wang", "G. Sun", "S. Jiang", "Q. Huang"], "venue": "In Proc. IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2013}, {"title": "Nonparametric factor analysis with beta process prior", "author": ["J. Paisley", "L. Carin"], "venue": "In Proc. Int. Conf. on Machine Learning,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2009}, {"title": "Sparse and redundant representation: From theory to applications in signal and image", "author": ["M. Elad"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2010}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2006}, {"title": "Orthogonal matching pursuit for sparse signal recovery with noise", "author": ["T.T. Cai", "Lei Wang"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2011}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2010}, {"title": "Regression shrinkage and selection via Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 1996}, {"title": "Distinctive Image Features from Scale-Invariant Keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2004}, {"title": "Futuristic greedy approach to sparse unmixing of hypersperctral data", "author": ["N. Akhtar", "F. Shafait", "A. Mian"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "With its inspirational roots in human vision system [16], [17], this technique has been successfully employed in image restoration [18], [19], [20], compressive sensing [21], [22] and morphological component analysis [23].", "startOffset": 131, "endOffset": 135}, {"referenceID": 15, "context": "With its inspirational roots in human vision system [16], [17], this technique has been successfully employed in image restoration [18], [19], [20], compressive sensing [21], [22] and morphological component analysis [23].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "With its inspirational roots in human vision system [16], [17], this technique has been successfully employed in image restoration [18], [19], [20], compressive sensing [21], [22] and morphological component analysis [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "With its inspirational roots in human vision system [16], [17], this technique has been successfully employed in image restoration [18], [19], [20], compressive sensing [21], [22] and morphological component analysis [23].", "startOffset": 175, "endOffset": 179}, {"referenceID": 18, "context": "With its inspirational roots in human vision system [16], [17], this technique has been successfully employed in image restoration [18], [19], [20], compressive sensing [21], [22] and morphological component analysis [23].", "startOffset": 217, "endOffset": 221}, {"referenceID": 7, "context": "More recently, sparse representation based approaches have also shown promising results in face recognition and gender classification [9], [8], [10], [13], [24],", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "More recently, sparse representation based approaches have also shown promising results in face recognition and gender classification [9], [8], [10], [13], [24],", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "More recently, sparse representation based approaches have also shown promising results in face recognition and gender classification [9], [8], [10], [13], [24],", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "More recently, sparse representation based approaches have also shown promising results in face recognition and gender classification [9], [8], [10], [13], [24],", "startOffset": 150, "endOffset": 154}, {"referenceID": 19, "context": "More recently, sparse representation based approaches have also shown promising results in face recognition and gender classification [9], [8], [10], [13], [24],", "startOffset": 156, "endOffset": 160}, {"referenceID": 20, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 6, "endOffset": 10}, {"referenceID": 12, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 57, "endOffset": 61}, {"referenceID": 23, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 63, "endOffset": 67}, {"referenceID": 24, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 69, "endOffset": 73}, {"referenceID": 25, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 121, "endOffset": 124}, {"referenceID": 9, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 132, "endOffset": 136}, {"referenceID": 27, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 166, "endOffset": 170}, {"referenceID": 28, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 172, "endOffset": 176}, {"referenceID": 29, "context": "[25], [26], texture and handwritten digit classification [14], [29], [30], [31], natural image and object classification [9], [11], [32] and human action recognition [33], [34], [35], [36].", "startOffset": 184, "endOffset": 188}, {"referenceID": 6, "context": "sparse linear combination of the other samples from the same class, in a lower dimensional manifold [8].", "startOffset": 100, "endOffset": 103}, {"referenceID": 34, "context": "fast Fourier transform [41] or wavelets [42]) as a generic dictionary to represent data from different domains/classes.", "startOffset": 23, "endOffset": 27}, {"referenceID": 35, "context": "fast Fourier transform [41] or wavelets [42]) as a generic dictionary to represent data from different domains/classes.", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "However, research in the last decade ( [6], [9], [10], [11], [18], [43], [44], [45]) has provided strong evidence in favor of learning dictionaries using the domain/class-specific training data, especially for classification and recognition tasks [10] where class label information of the training data can be exploited in the supervised learning of a dictionary.", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "However, research in the last decade ( [6], [9], [10], [11], [18], [43], [44], [45]) has provided strong evidence in favor of learning dictionaries using the domain/class-specific training data, especially for classification and recognition tasks [10] where class label information of the training data can be exploited in the supervised learning of a dictionary.", "startOffset": 44, "endOffset": 47}, {"referenceID": 8, "context": "However, research in the last decade ( [6], [9], [10], [11], [18], [43], [44], [45]) has provided strong evidence in favor of learning dictionaries using the domain/class-specific training data, especially for classification and recognition tasks [10] where class label information of the training data can be exploited in the supervised learning of a dictionary.", "startOffset": 49, "endOffset": 53}, {"referenceID": 9, "context": "However, research in the last decade ( [6], [9], [10], [11], [18], [43], [44], [45]) has provided strong evidence in favor of learning dictionaries using the domain/class-specific training data, especially for classification and recognition tasks [10] where class label information of the training data can be exploited in the supervised learning of a dictionary.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "However, research in the last decade ( [6], [9], [10], [11], [18], [43], [44], [45]) has provided strong evidence in favor of learning dictionaries using the domain/class-specific training data, especially for classification and recognition tasks [10] where class label information of the training data can be exploited in the supervised learning of a dictionary.", "startOffset": 61, "endOffset": 65}, {"referenceID": 36, "context": "However, research in the last decade ( [6], [9], [10], [11], [18], [43], [44], [45]) has provided strong evidence in favor of learning dictionaries using the domain/class-specific training data, especially for classification and recognition tasks [10] where class label information of the training data can be exploited in the supervised learning of a dictionary.", "startOffset": 67, "endOffset": 71}, {"referenceID": 37, "context": "However, research in the last decade ( [6], [9], [10], [11], [18], [43], [44], [45]) has provided strong evidence in favor of learning dictionaries using the domain/class-specific training data, especially for classification and recognition tasks [10] where class label information of the training data can be exploited in the supervised learning of a dictionary.", "startOffset": 73, "endOffset": 77}, {"referenceID": 38, "context": "However, research in the last decade ( [6], [9], [10], [11], [18], [43], [44], [45]) has provided strong evidence in favor of learning dictionaries using the domain/class-specific training data, especially for classification and recognition tasks [10] where class label information of the training data can be exploited in the supervised learning of a dictionary.", "startOffset": 79, "endOffset": 83}, {"referenceID": 8, "context": "However, research in the last decade ( [6], [9], [10], [11], [18], [43], [44], [45]) has provided strong evidence in favor of learning dictionaries using the domain/class-specific training data, especially for classification and recognition tasks [10] where class label information of the training data can be exploited in the supervised learning of a dictionary.", "startOffset": 247, "endOffset": 251}, {"referenceID": 4, "context": "K-SVD [6], Method of Optimal Directions [46]) aim at learning faithful signal representations, supervised sparse representation additionally strives for making the dictionaries discriminative.", "startOffset": 6, "endOffset": 9}, {"referenceID": 39, "context": "K-SVD [6], Method of Optimal Directions [46]) aim at learning faithful signal representations, supervised sparse representation additionally strives for making the dictionaries discriminative.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "[8] constructed a discriminative dictionary by directly using the training data as the dictionary atoms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "In order to learn a discriminative dictionary, existing approaches either force subsets of the dictionary atoms to represent data from only specific classes [12], [26], [47] or", "startOffset": 157, "endOffset": 161}, {"referenceID": 21, "context": "In order to learn a discriminative dictionary, existing approaches either force subsets of the dictionary atoms to represent data from only specific classes [12], [26], [47] or", "startOffset": 163, "endOffset": 167}, {"referenceID": 40, "context": "In order to learn a discriminative dictionary, existing approaches either force subsets of the dictionary atoms to represent data from only specific classes [12], [26], [47] or", "startOffset": 169, "endOffset": 173}, {"referenceID": 5, "context": "they associate the complete dictionary to all the classes and constrain their sparse coefficient to be discriminative [7], [9], [28].", "startOffset": 118, "endOffset": 121}, {"referenceID": 7, "context": "they associate the complete dictionary to all the classes and constrain their sparse coefficient to be discriminative [7], [9], [28].", "startOffset": 123, "endOffset": 126}, {"referenceID": 9, "context": "A third category of techniques learns exclusive sets of class specific and common dictionary atoms to separate the common and particular features of the data from different classes [11], [54].", "startOffset": 181, "endOffset": 185}, {"referenceID": 47, "context": "A third category of techniques learns exclusive sets of class specific and common dictionary atoms to separate the common and particular features of the data from different classes [11], [54].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "However, adaptively building this association is still an open research problem [13].", "startOffset": 80, "endOffset": 84}, {"referenceID": 49, "context": "sparse representation technique that infers a discriminative dictionary using a Beta Process [56].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "Our approach has been tested on two face-databases [1], [2], an object-database [3], an action-database [5] and a scene-database [4].", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Our approach has been tested on two face-databases [1], [2], an object-database [3], an action-database [5] and a scene-database [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "Our approach has been tested on two face-databases [1], [2], an object-database [3], an action-database [5] and a scene-database [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "Our approach has been tested on two face-databases [1], [2], an object-database [3], an action-database [5] and a scene-database [4].", "startOffset": 129, "endOffset": 132}, {"referenceID": 21, "context": "In the first category, the learned dictionary atoms have direct correspondence to the labels of the classes [26], [47], [12], [48], [35], [49], [36].", "startOffset": 108, "endOffset": 112}, {"referenceID": 40, "context": "In the first category, the learned dictionary atoms have direct correspondence to the labels of the classes [26], [47], [12], [48], [35], [49], [36].", "startOffset": 114, "endOffset": 118}, {"referenceID": 10, "context": "In the first category, the learned dictionary atoms have direct correspondence to the labels of the classes [26], [47], [12], [48], [35], [49], [36].", "startOffset": 120, "endOffset": 124}, {"referenceID": 41, "context": "In the first category, the learned dictionary atoms have direct correspondence to the labels of the classes [26], [47], [12], [48], [35], [49], [36].", "startOffset": 126, "endOffset": 130}, {"referenceID": 42, "context": "In the first category, the learned dictionary atoms have direct correspondence to the labels of the classes [26], [47], [12], [48], [35], [49], [36].", "startOffset": 138, "endOffset": 142}, {"referenceID": 29, "context": "In the first category, the learned dictionary atoms have direct correspondence to the labels of the classes [26], [47], [12], [48], [35], [49], [36].", "startOffset": 144, "endOffset": 148}, {"referenceID": 21, "context": "[26] proposed an SRC like framework for face recognition, where the atoms of the dictionary are learned from the training data instead of directly using the training data as the dictionary.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[47] used a discriminative penalty term in the KSVD model [6], achieving state-of-the-art results on texture segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[47] used a discriminative penalty term in the KSVD model [6], achieving state-of-the-art results on texture segmentation.", "startOffset": 58, "endOffset": 61}, {"referenceID": 41, "context": "Sprechmann and Sapiro [48] also proposed to learn dictionaries and sparse codes for clustering.", "startOffset": 22, "endOffset": 26}, {"referenceID": 29, "context": "In [36], Castrodad and Sapiro computed class-specific dictionaries for actions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 42, "context": "Active basis models are learned from the training images of each class and applied to object detection and recognition in [49].", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "[12] have used an incoherence promoting term for the dictionary atoms in their learning model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "be discriminative ( [9], [28], [7], [29], [30], [45], [31], [50], [33], [51] ).", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "be discriminative ( [9], [28], [7], [29], [30], [45], [31], [50], [33], [51] ).", "startOffset": 31, "endOffset": 34}, {"referenceID": 23, "context": "be discriminative ( [9], [28], [7], [29], [30], [45], [31], [50], [33], [51] ).", "startOffset": 36, "endOffset": 40}, {"referenceID": 24, "context": "be discriminative ( [9], [28], [7], [29], [30], [45], [31], [50], [33], [51] ).", "startOffset": 42, "endOffset": 46}, {"referenceID": 38, "context": "be discriminative ( [9], [28], [7], [29], [30], [45], [31], [50], [33], [51] ).", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "be discriminative ( [9], [28], [7], [29], [30], [45], [31], [50], [33], [51] ).", "startOffset": 54, "endOffset": 58}, {"referenceID": 43, "context": "be discriminative ( [9], [28], [7], [29], [30], [45], [31], [50], [33], [51] ).", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "be discriminative ( [9], [28], [7], [29], [30], [45], [31], [50], [33], [51] ).", "startOffset": 66, "endOffset": 70}, {"referenceID": 44, "context": "be discriminative ( [9], [28], [7], [29], [30], [45], [31], [50], [33], [51] ).", "startOffset": 72, "endOffset": 76}, {"referenceID": 7, "context": "[9] proposed a dictionary learning model that encourages the sparse representation coefficients of the same class to be similar.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "A similar approach is taken by Rodriguez and Sapiro [30] where the authors solve for a simultaneous sparse approximation problem [52] while learning the coefficients.", "startOffset": 52, "endOffset": 56}, {"referenceID": 45, "context": "A similar approach is taken by Rodriguez and Sapiro [30] where the authors solve for a simultaneous sparse approximation problem [52] while learning the coefficients.", "startOffset": 129, "endOffset": 133}, {"referenceID": 38, "context": "Pham and Venkatesh [45] and Mairal et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "Zhang and Li [7] enhanced the K-SVD algorithm [6] to learn a linear classifier along the dictionary.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Zhang and Li [7] enhanced the K-SVD algorithm [6] to learn a linear classifier along the dictionary.", "startOffset": 46, "endOffset": 49}, {"referenceID": 25, "context": "A task driven dictionary learning framework has also been proposed [31].", "startOffset": 67, "endOffset": 71}, {"referenceID": 46, "context": "[53] extended the SRC algorithm by appending an intra-class face variation dictionary to the training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "Zhou and Fan [54] employ a Fisher-like regularizer on the representation coefficients while learning a hybrid dictionary.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "Wang and Kong [11] learned a hybrid dictionary to separate the common and particular features of the data.", "startOffset": 14, "endOffset": 18}, {"referenceID": 48, "context": "[55] proposed to learn a multi-level dictionary for hierarchical visual categorization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "However, it is often non-trivial to decide on how to balance between the shared and the class-specific parts of the hybrid dictionary [10], [13].", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "However, it is often non-trivial to decide on how to balance between the shared and the class-specific parts of the hybrid dictionary [10], [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 50, "context": "Generally, p is chosen to be 0 or 1 for sparsity [57].", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "Nevertheless, we can still exploit this factorization in classification tasks by using the sparse codes of the data as features [9], for which, a classifier can be obtained as", "startOffset": 128, "endOffset": 131}, {"referenceID": 5, "context": "Considering this, existing approaches [7], [45], [29], [28] proposed to jointly optimize a classifier with the dictionary while learning the sparse representation.", "startOffset": 38, "endOffset": 41}, {"referenceID": 38, "context": "Considering this, existing approaches [7], [45], [29], [28] proposed to jointly optimize a classifier with the dictionary while learning the sparse representation.", "startOffset": 43, "endOffset": 47}, {"referenceID": 23, "context": "Considering this, existing approaches [7], [45], [29], [28] proposed to jointly optimize a classifier with the dictionary while learning the sparse representation.", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "[9] built further on this concept and encouraged explicit correspondence between the dictionary atoms and the class-labels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "More precisely, the following optimization problem is solved by the LabelConsistent K-SVD (LC-KSVD2) algorithm [9]:", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "It is worth noting that in Label-Consistent K-SVD algorithm [9], the relationship between class-specific subsets of dictionary atoms and class labels is pre-defined.", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "SVD (D-KSVD) algorithm [7].", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "2, we illustrate the behavior of recognition accuracy under varying dictionary sizes for [7] and [9] for two face databases.", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "2, we illustrate the behavior of recognition accuracy under varying dictionary sizes for [7] and [9] for two face databases.", "startOffset": 97, "endOffset": 100}, {"referenceID": 49, "context": "Paisley and Carin [56] developed a Beta Process for nonparametric factor analysis, which was later used by Zhou et al.", "startOffset": 18, "endOffset": 22}, {"referenceID": 37, "context": "[44] in successful image restoration and compressive sensing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "(5), the number of non-zero components in a column of Z is a random number drawn from Poisson(ao/bo) [56].", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "(a) AR database [1] (b) Extended YaleB database [2]", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "All other parameters are kept constant at optimal values reported in [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 49, "context": "3Paisley and Carin [56] derived variational Bayesian algorithm [58] for their model.", "startOffset": 19, "endOffset": 23}, {"referenceID": 37, "context": "[44] that Gibbs sampling is an equally effective strategy in data representation using the same model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "K-SVD [6]), we derive expressions for the Gibbs sampler for our approach.", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": "This process is analogous to the atom-by-atom dictionary update step of K-SVD [6], however the sparse codes remain fixed during our dictionary update.", "startOffset": 78, "endOffset": 81}, {"referenceID": 49, "context": "4We follow [56] closely in the proof, however, our analysis also takes into account the class labels of the data, whereas no such data discrimination is assumed in [56].", "startOffset": 11, "endOffset": 15}, {"referenceID": 49, "context": "4We follow [56] closely in the proof, however, our analysis also takes into account the class labels of the data, whereas no such data discrimination is assumed in [56].", "startOffset": 164, "endOffset": 168}, {"referenceID": 7, "context": "We follow the common methodology [9], [7] for classification that first encodes y over the inferred dictionary such that y = \u03a6\u03b1\u0302 + , and then computes ` = W\u03b1\u0302, where W \u2208 RC\u00d7|K| contains model parameters of a multi-class linear classifier.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "We follow the common methodology [9], [7] for classification that first encodes y over the inferred dictionary such that y = \u03a6\u03b1\u0302 + , and then computes ` = W\u03b1\u0302, where W \u2208 RC\u00d7|K| contains model parameters of a multi-class linear classifier.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "4: Illustration of the discriminative character of the inferred dictionary: From top, the four rows present results on AR database [1], Extended YaleB [2], Caltech-101 [3] and Fifteen Scene categories [4], respectively.", "startOffset": 151, "endOffset": 154}, {"referenceID": 1, "context": "4: Illustration of the discriminative character of the inferred dictionary: From top, the four rows present results on AR database [1], Extended YaleB [2], Caltech-101 [3] and Fifteen Scene categories [4], respectively.", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "4: Illustration of the discriminative character of the inferred dictionary: From top, the four rows present results on AR database [1], Extended YaleB [2], Caltech-101 [3] and Fifteen Scene categories [4], respectively.", "startOffset": 201, "endOffset": 204}, {"referenceID": 5, "context": "[7], [9]) the coupling between W and \u03a6 is kept probabilistic in our approach.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7], [9]) the coupling between W and \u03a6 is kept probabilistic in our approach.", "startOffset": 5, "endOffset": 8}, {"referenceID": 52, "context": "Our view point also makes Orthogonal Matching Pursuit (OMP) [60] a", "startOffset": 60, "endOffset": 64}, {"referenceID": 52, "context": "We sparsely encode xi over the initial dictionary using OMP [60].", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "We have evaluated the proposed approach on two face data sets: the Extended YaleB [2] and the AR database [1], a data set for object categories: Caltech-101 [3], a data set for scene", "startOffset": 82, "endOffset": 85}, {"referenceID": 1, "context": "We have evaluated the proposed approach on two face data sets: the Extended YaleB [2] and the AR database [1], a data set for object categories: Caltech-101 [3], a data set for scene", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": "categorization: Fifteen scene categories [4], and an action data", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "set: UCF sports actions [5].", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "We compare the performance of the proposed approach with SRC [8], the two variants of LabelConsistent K-SVD [9] (i.", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "We compare the performance of the proposed approach with SRC [8], the two variants of LabelConsistent K-SVD [9] (i.", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "LC-KSVD1, LC-KSVD2), the Discriminative K-SVD algorithm (D-KSVD) [7], the Fisher Discrimination Dictionary Learning algorithm (FDDL) [10] and the Dictionary Learning based on separating the Commonalities and the Particularities of the data (DL-COPAR) [11].", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "LC-KSVD1, LC-KSVD2), the Discriminative K-SVD algorithm (D-KSVD) [7], the Fisher Discrimination Dictionary Learning algorithm (FDDL) [10] and the Dictionary Learning based on separating the Commonalities and the Particularities of the data (DL-COPAR) [11].", "startOffset": 133, "endOffset": 137}, {"referenceID": 9, "context": "LC-KSVD1, LC-KSVD2), the Discriminative K-SVD algorithm (D-KSVD) [7], the Fisher Discrimination Dictionary Learning algorithm (FDDL) [10] and the Dictionary Learning based on separating the Commonalities and the Particularities of the data (DL-COPAR) [11].", "startOffset": 251, "endOffset": 255}, {"referenceID": 4, "context": "In our comparisons, we also include results of unsupervised sparse representation based classification that uses K-SVD [6] as the dictionary learning technique and separately computes a multi-class linear classifier using Eq.", "startOffset": 119, "endOffset": 122}, {"referenceID": 54, "context": "To implement SRC, we used the LASSO [63] solver of the SPAMS toolbox [62].", "startOffset": 36, "endOffset": 40}, {"referenceID": 53, "context": "To implement SRC, we used the LASSO [63] solver of the SPAMS toolbox [62].", "startOffset": 69, "endOffset": 73}, {"referenceID": 7, "context": "[9] for LC-KSVD2 algorithm and solved Eq.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Furthermore, \u03bb o was set to 10 6 for all the datasets except for Fifteen Scene Categories [4],", "startOffset": 90, "endOffset": 93}, {"referenceID": 0, "context": "Extended YaleB [2] contains 2,414 frontal face images of 38 different people, each having about 64 samples.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "In our experiments, we used the random face feature descriptor [8], where a cropped 192 \u00d7 168 pixels image was projected onto a 504-dimensional vector.", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "(a) Extended YaleB [2]", "startOffset": 19, "endOffset": 22}, {"referenceID": 13, "context": "The results for Locality-constrained Linear Coding (LLC) [15] is directly taken from [9], where the accuracy is computed using 70 local bases.", "startOffset": 57, "endOffset": 61}, {"referenceID": 7, "context": "The results for Locality-constrained Linear Coding (LLC) [15] is directly taken from [9], where the accuracy is computed using 70 local bases.", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "[9], the sparsity threshold for K-SVD, LC-KSVD1, LC-KSVD2 and D-KSVD was set to 30 in our experiments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Furthermore, as in [9], we used \u03c5 = 4.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "This value is set to 570 in [9] for all of the four methods.", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "Following [8], we set the residual error tolerance to 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "For FDDL, we followed [10] for the optimized parameter settings.", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "For the parameter settings of DL-COPAR we followed the original work [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 0, "context": "TABLE I: Recognition accuracy with Random-Face features on the Extended YaleB database [2].", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "LLC [15] 90.", "startOffset": 4, "endOffset": 8}, {"referenceID": 4, "context": "7 K-SVD [6] 93.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "37 LC-KSVD1 [9] 93.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "36 D-KSVD [7] 94.", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "38 DL-COPAR [11] 94.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "55 LC-KSVD2 [9] 95.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "FDDL [10] 96.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "SRC [8] 96.", "startOffset": 4, "endOffset": 7}, {"referenceID": 13, "context": "We report the average recognition accuracy of our experiments in Table II, which also includes the accuracy of LLC [15] reported in [9].", "startOffset": 115, "endOffset": 119}, {"referenceID": 7, "context": "We report the average recognition accuracy of our experiments in Table II, which also includes the accuracy of LLC [15] reported in [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 5, "context": "In our experiments, we set the sparsity threshold for K-SVD, LC-KSVD1, LC-KSVD2 and D-KSVD to 50 as compared to 10 and 30 which was used in [7] and [9], respectively.", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "In our experiments, we set the sparsity threshold for K-SVD, LC-KSVD1, LC-KSVD2 and D-KSVD to 50 as compared to 10 and 30 which was used in [7] and [9], respectively.", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "These large values (compared to 500 used in [7], [9]) resulted in better accuracies at the", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "These large values (compared to 500 used in [7], [9]) resulted in better accuracies at the", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": "LLC [15] 88.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "7 DL-COPAR [11] 93.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "80 LC-KSVD1 [9] 93.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "37 K-SVD [6] 94.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "99 LC-KSVD2 [9] 95.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "41 D-KSVD [7] 95.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "38 FDDL [10] 96.", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "03 SRC [8] 96.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "6: Examples from Caltech-101 database [3].", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "The Caltech-101 database [3] comprises 9, 144 samples from 102 classes.", "startOffset": 25, "endOffset": 28}, {"referenceID": 55, "context": "To use the database, first the SIFT descriptors [64] were extracted from 16 \u00d7 16 image patches, which were densely sampled with a 6-pixels step size for the grid.", "startOffset": 48, "endOffset": 52}, {"referenceID": 31, "context": "Then, based on the extracted features, spatial pyramid features [38] were extracted with 2\u00d72 grids, where l = 0, 1, 2.", "startOffset": 64, "endOffset": 68}, {"referenceID": 1, "context": "TABLE III: Classification results using Spatial Pyramid Features on the Caltech-101 dataset [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 30, "context": "[37] 46.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[38] - - 56.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[39] 44.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] 51.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "4 SRC [8] 49.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "9 DL-COPAR [11] 49.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "9 K-SVD [6] 51.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "3 FDDL [10] 52.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "1 D-KSVD [7] 52.", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "1 LC-KSVD1 [9] 53.", "startOffset": 11, "endOffset": 14}, {"referenceID": 7, "context": "5 LC-KSVD2 [9] 53.", "startOffset": 11, "endOffset": 14}, {"referenceID": 7, "context": "[9] also suggested the same parameter settings.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "We used the parameter settings for object categorization given in [10] for FDDL.", "startOffset": 66, "endOffset": 70}, {"referenceID": 9, "context": "For DL-COPAR, the selected number of classspecific atoms were kept the same as the number of training instances per class, whereas the number of shared atoms were fixed to 314, as in the original work [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 7, "context": "Moreover, our approach is able to handle a batch of large training data more efficiently than LC-KSVD [9] and D-KSVD [7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 5, "context": "Moreover, our approach is able to handle a batch of large training data more efficiently than LC-KSVD [9] and D-KSVD [7].", "startOffset": 117, "endOffset": 120}, {"referenceID": 5, "context": "96 D-KSVD [7] 3196 19.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "90 LC-KSVD1 [9] 5434 19.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "65 LC-KSVD2 [9] 5434 19.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "7: Examples images from eight different categories in Fifteen Scene Categories dataset [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "The Fifteen Scene Category dataset [4] has 200 to 400 images per category for fifteen different kinds of scenes.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] for LC-KSVD1, LCKSVD2 and D-KSVD.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Parameters of DL-COPAR were set as suggested in the original work [11] for the same database.", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "ESPN and BBC) [5].", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "For D-KSVD, LC-KSVD1 and LC-KSVD2 we followed [9] for the parameter settings.", "startOffset": 46, "endOffset": 49}, {"referenceID": 2, "context": "TABLE V: Classification accuracy on Fifteen Scene Category dataset [4] using Spatial Pyramid Features.", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "K-SVD [6] 93.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "14 LC-KSVD1 [9] 94.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "17 D-KSVD [7] 96.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "12 SRC [8] 96.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "09 DL-COPAR [11] 96.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "22 LC-KSVD2 [9] 97.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "8: Examples from UCF Sports action dataset [5].", "startOffset": 43, "endOffset": 46}, {"referenceID": 27, "context": "[33] and action back feature with SVM [40].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[33] and action back feature with SVM [40].", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "These results are taken directly from [13] along the results of DLSI [12], DL-COPAR [11] and FDDL [10]5.", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "These results are taken directly from [13] along the results of DLSI [12], DL-COPAR [11] and FDDL [10]5.", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "These results are taken directly from [13] along the results of DLSI [12], DL-COPAR [11] and FDDL [10]5.", "startOffset": 84, "endOffset": 88}, {"referenceID": 8, "context": "These results are taken directly from [13] along the results of DLSI [12], DL-COPAR [11] and FDDL [10]5.", "startOffset": 98, "endOffset": 102}, {"referenceID": 33, "context": "Following [40], we also performed leave-one-out cross validation on this database for the proposed approach.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "7% better than the state-of-the-art results claimed in [40].", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "5The results of DL-COPAR [11] and FDDL [10] are taken directly from the literature because the optimized parameter values for these algorithms are not previously reported for this dataset.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "5The results of DL-COPAR [11] and FDDL [10] are taken directly from the literature because the optimized parameter values for these algorithms are not previously reported for this dataset.", "startOffset": 39, "endOffset": 43}, {"referenceID": 3, "context": "TABLE VI: Classification rates on UCF Sports Action dataset [5]", "startOffset": 60, "endOffset": 63}, {"referenceID": 27, "context": "[33] 83.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "6 LC-KSVD2 [9] 91.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "5 D-KSVD [7] 89.", "startOffset": 9, "endOffset": 12}, {"referenceID": 10, "context": "1 DLSI [12] 92.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "1 LC-KSVD1 [9] 89.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "6 SRC [8] 92.", "startOffset": 6, "endOffset": 9}, {"referenceID": 9, "context": "7 DL-COPAR [11] 90.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "7 FDDL [10] 93.", "startOffset": 7, "endOffset": 11}, {"referenceID": 33, "context": "6 Sadanand [40] 90.", "startOffset": 11, "endOffset": 15}, {"referenceID": 11, "context": "7 LDL [13] 95.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "In our experiments, while sparse coding a test instance over the learned dictionary, we consistently used the sparsity threshold of 50 for all the datasets except for the UCF [5], for which this parameter was set to 40 because of the smaller dictionary resulting from less training samples.", "startOffset": 175, "endOffset": 178}, {"referenceID": 56, "context": "For instance, while classifying the spectral signatures of minerals on pixel and sub-pixel level in remote-sensing hyperspectral images, the relative smoothness of spectral signatures [65] can be incorporated in the inferred discriminative bases.", "startOffset": 184, "endOffset": 188}], "year": 2015, "abstractText": "We propose a Bayesian approach to learn discriminative dictionaries for sparse representation of data. The proposed approach infers probability distributions over the atoms of a discriminative dictionary using a Beta Process. It also computes sets of Bernoulli distributions that associate class labels to the learned dictionary atoms. This association signifies the selection probabilities of the dictionary atoms in the expansion of class-specific data. Furthermore, the non-parametric character of the proposed approach allows it to infer the correct size of the dictionary. We exploit the aforementioned Bernoulli distributions in separately learning a linear classifier. The classifier uses the same hierarchical Bayesian model as the dictionary, which we present along the analytical inference solution for Gibbs sampling. For classification, a test instance is first sparsely encoded over the learned dictionary and the codes are fed to the classifier. We performed experiments for face and action recognition; and object and scene-category classification using five public datasets and compared the results with state-of-the-art discriminative sparse representation approaches. Experiments show that the proposed Bayesian approach consistently outperforms the existing approaches.", "creator": "LaTeX with hyperref package"}}}