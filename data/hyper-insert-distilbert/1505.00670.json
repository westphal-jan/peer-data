{"id": "1505.00670", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2015", "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation", "abstract": "despite recently tremendous progress in computer aid vision, there has not been an attempt today for machine learning collaborating on very high large - size scale novel medical image sequence databases. we present an interleaved synthetic text / mental image deep learning system to extract and inter mine along the semantic interactions of radiology microscopy images synthesized and reports from a national aids research hospital'\u00ab s fluorescent picture archiving and forensic communication system. simultaneous with natural language processing, principally we mine a comprehensive collection of representative ~ 216k weighted two - dimensional key matrix images \u2014 selected indirectly by clinicians for numerical diagnostic reference, and i match the processed images with their biomedical descriptions data in an automated labeling manner. our system now interleaves between unsupervised learning fields and informal supervised learning on particular document - binding and sentence - level text matching collections, to thoroughly generate specialized semantic labels and responses to predict them given an image. given externally an abbreviated image of a patient scan, semantic descriptive topics identified in evaluating radiology activity levels are eventually predicted, and associated linguistic key - words are generated. also, a number of frequent fungal disease indicator types encountered are detected correctly as present or while absent, to provide her more specific physical interpretation of a detailed patient scan. this paper shows the potential of large - data scale learning and prediction in electronic procedure patient records easily available present in roughly most modern worldwide clinical institutions.", "histories": [["v1", "Mon, 4 May 2015 15:05:59 GMT  (28813kb,D)", "http://arxiv.org/abs/1505.00670v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hoo-chang shin", "le lu", "lauren kim", "ari seff", "jianhua yao", "ronald m summers"], "accepted": false, "id": "1505.00670"}, "pdf": {"name": "1505.00670.pdf", "metadata": {"source": "CRF", "title": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation", "authors": ["Hoo-Chang Shin", "Lauren Kim", "Jianhua Yao", "Ronald M. Summers", "Ari Seff", "Le Lu"], "emails": ["hoochang.shin@nih.gov", "le.lu@nih.gov", "lauren.kim2@nih.gov", "ari.seff@nih.gov", "jyao@cc.nih.gov", "rms@nih.gov"], "sections": [{"heading": null, "text": "Keywords: Deep learning, Convolutional Neural Networks, Topic Models, Natural Language Processing, Medical Imaging"}, {"heading": "1. Introduction", "text": "The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al. (2014). Deep CNNs can perform significantly better than traditional shallow learning methods, but usually requires much more training data as was shown by Krizhevsky et al. (2012); Russakovsky et al. (2014). In the medical domain, however, there are no similar large-scale\n\u00a92015 Hoo-Chang Shin, Le Lu, Lauren Kim, Ari Seff, Jianhua Yao, Ronald M. Summers.\nar X\niv :1\n50 5.\n00 67\n0v 1\n[ cs\n.C V\nlabeled image datasets available. On the other hand, large collections of radiology images and reports are stored in many modern hospitals\u2019 Picture Archiving and Communication Systems (PACS). The invaluable semantic diagnostic knowledge inhabiting the mapping between hundreds of thousands of clinician-created high quality text reports and linked image volumes remains largely unexplored. One of our primary goals is to extract and associate radiology images with clinically semantic labels via interleaved text/image data mining and deep learning on a large-scale PACS database (\u223c780K imaging examinations). To the best of our knowledge, this is the first reported work performing automated mining and prediction on a hospital PACS database at a very large scale.\nThe Radiology reports are text documents describing patient history, symptoms, image observations and impressions written by board-certified radiologists. However, the reports do not contain specific image labels to be trained by a machine learning algorithm. Building the ImageNet database (Deng et al. (2009)) was mainly a manual process: harvesting images returned from Google image search engine according to the WordNet (Miller (1995)) ontology hierarchy and pruning falsely tagged images using crowd-sourcing such as Amazon Mechanical Turk (AMT). This does not meet our data collection and labeling needs due to the demanding difficulties of medical annotation tasks and the data privacy reasons. Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories.\nOur work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation. Kulkarni et al. (2013) have spearheaded the efforts of learning the semantic connections between image contents and the sentences describing them, such as image captions. Detecting objects of interest, attributes and prepositions and applying contextual regularization with a conditional random field (CRF) is a feasible approach as shown by Kulkarni et al. (2013), and many useful tools for image annotation using it are available in computer vision.\nBoth deep feed-forward CNNs of Krizhevsky et al. (2012); Simonyan and Zisserman (2014) and recurrent neural networks of Mikolov et al. (2013a,b) were used to model image and text. Also, the CNN parameters pre-trained on ImageNet were used to initialize CNNs and to be adopted for medical image analysis. We show the benefit of this transfer learning and domain adaptation in Section 4.2. The fact that deep learning requires no handcrafted image features is very desirable since significant adaption would be needed to apply conventional image features, e.g., HOG, SIFT to learn the wide variety of medical images. The large-scale datasets of extracted key images and their categorization, vector labels, and describing sentences can be harnessed to alleviate deep learning\u2019s \u201cdata-hungry\u201d challenge in the medical domain. We will make our code and trained deep text/image models publicly available upon acceptance."}, {"heading": "1.1 Related Work", "text": "The ImageCLEF medical image annotation tasks of 2005-2007 by Deselaers and Ney (2008) have 9,000 training and 1,000 testing two-dimensional images, converted to 32 \u00d7 32 pixel thumbnails with 57 labels. Local image descriptors and intensity histograms are used as a bag-of-features approach in that work for this scene-recognition-like problem. Unsupervised LDA based matching from lung disease words (e.g., fibrosis, emphysema) to two-dimensional image blocks from axial CT chest scans of 24 patients is studied by Carrivick et al. (2005). The works of Barnard et al. (2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study.\nThe most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain.\nGraphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.Automatic label mining on large, unlabeled datasets is presented by Ordonez et al. (2011); Jaderberg et al. (2014), however the variety of the label-space is limited to image text annotations. We analyze and mine the medical image semantics on both document and sentence levels, and deep CNNs of Jaderberg et al. (2014); Simonyan and Zisserman (2014) are adapted to learn them from image contents."}, {"heading": "2. Data", "text": "To gain the most comprehensive interpretation of diagnostic semantics, we use all available radiology reports of around 780K imaging examinations, stored in the PACS of National Institutes of Health Clinical Center since the year 2000. Around 216K key twodimensional image slices are studied here, instead of using all three-dimensional image volumes. Within three-dimensional patient scans, most of the imaging information represented is normal anatomy, therefore they are often not the focus of the radiology reports. The two-dimensional \u201ckey images\u201d referenced (Figure 1) by radiologists manually during radiology report writing provide a visual reference to pathologies or other notable findings. Therefore, the two-dimensional key images are more correlated with the diagnostic semantics in the reports than the whole three-dimensional scans, but not all reports have referenced key images (215, 786 images from about 61, 845 unique patients). Table 1 provides some statistics of the extracted database, and Table 2 shows examples of the most frequently occurring words in radiology reports collected. Leveraging our deep learning\nmodels exploited in this paper will make it possible to automatically select key images from three-dimensional patient scans to avoid mis-referencing.\nFinding and extracting key images from radiology reports is done by natural language processing (NLP), i.e, finding a sentence mentioning a referenced image. For example, \u201cThere may be mild fat stranding of the right parapharyngeal soft tissues (series 1001, image 32)\u201d is listed in Figure 1. The NLP steps are sentence tokenization, word/number matching and stemming, and rule-based information extraction (e.g., translating \u201cimage 1013-78\u201d to \u201cimages 1013-1078\u201d). Software package of Bird et al. (2009) was used for basic NLP pipelines. A total of \u223c187K images could be retrieved and matched this way, whereas the rest of \u223c28K key images were extracted according to their reference accession numbers in PACS."}, {"heading": "3. Document Topic Learning with Latent Dirichlet Allocation", "text": "We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics.\nLatent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al. (2012). Furthermore, pLSA can be regarded as a special case of LDA (Girolami and Kaba\u0301n (2003)) and NMF as a semi-equivalent model of pLSA (Gaussier and Goutte (2005); Ding et al. (2006)).\nLDA offers a hierarchy of extracted topics and the number of topics can be chosen by evaluating each model\u2019s perplexity score (Equation 1), which is a common way to measure how well a probabilistic model generalizes by evaluating the log-likelihood of the model on a held-out test set. For an unseen document set Dtest, the perplexity score is defined as in Equation 1, where M is the number of documents in the test set, wd the words in the\nunseen document d, Nd the number of words in document d, with \u03a6 the topic matrix, and \u03b1 the hyper-parameter for topic distribution of the documents.\nperplexity(Dtest) = exp\n{ \u2212 \u2211M\nd=1 log p(wd|\u03a6, \u03b1)\u2211M d=1Nd\n} (1)\nA lower perplexity score generally implies a better fit of the model for a given document set (Blei et al. (2003)).\nBased on the perplexity score evaluated on 80% of the total documents used for training and 20% used for testing, the number of topics chosen is 80 for the document-level model using perplexity scores for model selection (Figure 2). Although the document distribution in the topic space is approximately balanced, the distribution of image counts for the topics is more unbalanced (Figure 3). Specifically, topic #77 (non-primary metastasis spreading across a variety of body parts) contains nearly half of the 216K key images. To address this data bias, sub-topics are obtained for each of the first document-level topics, resulting in 800 topics, where the number of the sub-topics is also chosen based on the average perplexity scores evaluated on each document-level topic. Lastly, to compare the method of using the whole report with using only the sentence directly describing the key images for latent topic mining, a sentence-level LDA topics are obtained based on three sentences only: the sentence mentioning the key-image (Figure 1) and its adjacent sentences as proximal context. The perplexity scores keep decreasing with an increasing number of topics; we choose the topic count to be 1000 as the rate of the perplexity score decrease is very small beyond that point (Figure 2).\nWe observe that LDA-generated image categorization labels are valid, demonstrating good semantic coherence among clinician observers. The lists of key words and sampled images per topic label are subjected to board-certified radiologist\u2019s review and validation. Some examples of document-level topics with their corresponding images and topic key words are shown in Figure 4. Based on radiologists\u2019 review, our LDA topics discover semantics at different levels. There are 73 low-level concepts for example, pathology examination of certain body regions and organs: topic #47 - sinus diseases; #2 - lesions of solid abdominal organs, primarily kidney; #10 - pulmonary diseases; #13 - brain MRI; #19 - renal diseases on mixed imaging modalities; #36 - brain tumors. There are 7 mid- to high-level concepts, such as: topic #77 - non-primary metastasis spreading across a variety of body parts; topic #79 - cases with high diagnosis uncertainty or equivocation; #72 - indeterminate lesions; #74 - instrumentation artifacts limiting interpretation. Low-level topic images tend to be visually more coherent than the higher-level topic images.\nHigh-level topics may be analogous to the high-level visual concepts in natural images as was studied by Kiapour et al. (2014); Ordonez and Berg (2014). About half of the key images are associated with topic #77, implying that the clinicians\u2019 image referencing behavior patterns heavily focuses on metastatic patients. Sub-topics of document-level topic #77 are sub-categories of metastatic disease, for example: #77-0 - abdominal mass; #77-2 - bulky tumor; #77-4 - multifocal metastatic disease; #77-9 - liver tumor. Meanwhile, some of the sub-topics of document-level #77 do not seem very focused. Many of the sentencelevel topics have valid semantics too, e.g. \u2018renal imaging\u2019, \u2018musculoskeletal imaging\u2019, \u2018chest port catheter\u2019, \u2018chest imaging with disease or pathology\u2019, and \u2018degenerative disease in bone\u2019.\nText/Image Mining on a Large-Scale Radiology Database\n250\n300\n350\n400\n450\n500\nx\nit\ny\n10 20 30 40 50 60 70 80 90 100\n840\n860\n880\n900\n920\n940\n960\n980 1000 1020 perplexity scores for document-level topic model\nperplexity\nnumber of topics\np e rp\nle x it y\nWe also obtained LDA topics on the reports having associated images only, resulting in 20 topics according to perplexity score. However, these did not add any more meaningful semantics in addition to the already obtained topics in three levels, so that we did not include the topics. For more details and the image-topic associations, refer to Figures 4, 5, and the supplementary material. Even though LDA labels are computed with text information only, we next investigate the plausibility of mapping images to the topic labels of different levels via deep CNN models."}, {"heading": "4. Image to Document Topic Mapping with Deep Convolutional Neural", "text": "Networks\nFor each level of topics discussed in Section 3, we train deep CNNs to map the images into document categories using the Caffe framework of Jia et al. (2014). We split our whole key image dataset as follows: 85% used as the training dataset, 5% as the crossvalidation (CV), and 10% as the test dataset. If a topic has too few images to be divided into training/CV/test for deep CNN learning, then that topic is neglected for the CNN training. These cases are normally topics of rare imaging protocols, for example: topic #5\n- Abdominal ultrasound; topic #28& #49 - DEXA scans of different usages. In total, 60 topics were used for the document-level topic mapping, 385 for the document-level sub-topic mapping, and 717 for the sentence-level mapping. Systematic diagrams showing how each level of semantic topics are learned, assigned to images, and trained to map from images to topics are shown in Figure 6."}, {"heading": "4.1 Implementation", "text": "All our CNN network settings are similar or the same as the ImageNet Challenge \u201cAlexNet\u201d (Krizhevsky et al. (2012)), and \u201cVGG-16 & 19\u201d (Simonyan and Zisserman (2014)) models. For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al. (2012) has about 60 million parameters (650,000 neurons) and consists of five convolutional layers (1st, 2nd and 5th followed by max-pooling layers), and three fully-connected (FC) layers with a final classification layer. The VGG variations of CNN models by Simonyan and Zisserman (2014) are significantly deeper by having 16\u223c19 convolutional layers and 133\u223c144 million parameters. The top-1 error rates on ImageNet dataset of these models are AlexNet:15.3% (Krizhevsky et al. (2012)); VGG-16:7.4%; and VGG-19:7.3% (Simonyan and Zisserman (2014)), respectively.\nFor image to topic mapping, we change the numbers of output nodes in the last softmax classification layer, i.e., 60, 385 and 717 for the document-level, document-level sub-topics, and sentence-level respectively. The networks for first-level semantic labels are fine-tuned\nafter,which,evidence,were,pulmonary,made,adrenal,prior,pelvic,without,cysts,spleen,mass,disease, multiple,isovue-300,obtained,areas,consistent,nodules,changes,pleural,lesions,following,abdominal, that,hilar,axillary].\nfrom the pre-trained ImageNet models, where the networks for the lower-level semantic labels are fine-tuned from the models of the higher-level semantic labels."}, {"heading": "4.2 Transfer Learning and Domain Adaptation", "text": "We found that transfer learning from the ImageNet pre-trained CNN parameters on natural images to our medical image modalities (mostly CT, MRI) significantly helps the image classification performance. Additionally, transfer learning from a CNN trained for a more related task (e.g. from CNN trained on image-to-document-level-topic models to train CNN for image-to-document-level-sub-topic model) was found to be more effective than from a CNN trained for a less related task (e.g. from CNN trained on ImageNet to train CNN for image-to-document-level-sub-topic model). Examples of classification accuracy traces during training using CNNs from random initialization, transfer learning from CNN trained on ImageNet, and transfer learning from higher level image-to-topic model to lower level image-to-topic models are shown in Figure 7. Similar findings that deep CNN features can be generalized across different image modalities have been reported by Gupta et al. (2014, 2013), but are empirically verified with only much smaller datasets than ours. Our key\nimage dataset is about one fifth the size of ImageNet (Russakovsky et al. (2014)) and is the largest annotated medical image dataset to date.\nFrom Figure 7 we can see that: (1) CNN testing accuracy increases from \u223c0% to 50+% quickly in about 1600 iterations due to the unbalanced data distribution among classes in document-level; (2) A more complex, deeper CNN model (VGG-Net) performs better than the model which already is a good benchmark (AlexNet), but only when starting from a good initialization (i.e., pre-training via ImageNet models); (3) Fine-tuning from a more closely related task CNN model is even better than fine-tuning from less related task model (alexnet tp80 h2 start tp80h1 > alexnet tp80 h2 start imagenet).\nWith these findings, we train our CNN models with transfer-learning by default for the remaining parts of our study. All the CNN layers except the newly modified ones are initialized with the weights of a previously trained related model, and trained with a new task with low learning rate of 0.001. The modified layers with new number of classes are initialized randomly, and their learning rates are set with higher learning rate of 0.01. All the key images are resampled to a spatial resolution of 256 \u00d7 256 pixels. Then we follow the approach of Simonyan and Zisserman (2014) to crop the input images from 256 \u00d7 256 to 227\u00d7 227 for training."}, {"heading": "4.3 Classification Results and Discussion", "text": "We would expect that the level of difficulties for learning and classifying the images into the LDA-induced topics will be different for each semantic level. Low-level semantic classes can have key images of axial/sagittal/coronal slices with position variations and across MRI/CT modalities. Mid- to high-level concepts all demonstrate much larger within-class variations in their visual appearance since they are diseases occurring within different organs and are only coherent at high level semantics. Table 3 provides the top-1 and top-5 testing in classification accuracies for each level of topic models using AlexNet (Krizhevsky et al. (2012)), and VGG-16&19 (Simonyan and Zisserman (2014)) based deep CNN models.\nAll top-5 accuracy scores are significantly higher than top-1 values, e.g. increasing from 0.658 to 0.946 using VGG-19, or 0.607 to 0.929 via AlexNet in document-level. This indicates that the classification errors or fusions are not uniformly distributed among other false classes. Latent \u201cblocky subspace of classes\u201d may exist in our discovered label space, where several topic classes form a tightly correlated subgroup. The confusion matrices in Figure 8 verify this finding.\nIt is shown that the deeper models (VGG-16&19) perform consistently better than the shallower 8-layer model (AlexNet) in classification accuracy, especially for document-level sub-topics. While the images of some topic categories and some body parts are easily distinguishable as shown in Figure 4, the visual differences in abdominal parts are rather subtle as in Figure 5. Distinguishing the subtleties and high-level concept categories in the images could benefit from a more complex model so that the model can handle these subtleties.\nIt is also noticeable that deeper models require significantly more computational resource and time to train than the shallower model. Table 4 shows the memory consumption and time required to train the CNN models for the image-to-sentence-level-topic model with up to 70,000 iterations using the NVidia Tesla K40 GPU. However, comparing VGG-16 and VGG19, three additional convolutional layers seem to have contributed to raise the top-5 accuracies by a small amount (\u223c2%), which is coherent with the results reported by Simonyan and Zisserman (2014) for object recognition task on the ImageNet dataset.\nCompared with the ImageNet 2014 results, top-1 error rates are moderately higher (34% versus 30%) and top-5 test errors (6% \u223c 8%) are comparable. In summary, our quantitative results are very encouraging, but there also exist some uncertainties in annotations because labels stem from an unsupervised learning algorithm. Multi-level semantic concepts show good image learnability by deep CNN models which sheds light on the feasibility of automatically parsing very large-scale radiology image databases."}, {"heading": "5. Generating Image-to-Text Description", "text": "The image-to-topic mapping in Section 4 is a promising first step towards automated interpretation of medical images in large scale. However, it is too expensive and time consuming for radiologists to examine all of the 1880 (80 + 800 + 1000) topics generated with their keywords and images. In addition, key words in the topics can help to understand the semantic contents of a given image with more semantic meaning. We therefore propose to generate relevant key-word text descriptions similar to Kulkarni et al. (2013), using deep language/image CNN models."}, {"heading": "5.1 Word-to-Vector Modeling and Removing Word-Level Ambiguity", "text": "In radiology reports, there exist many recurring word morphisms in text identification, e.g., [mr, mri, t1-/t2-weighted] (natural language expressions for imaging modalities of magnetic resonance imaging (MRI)), [cyst, cystic, cysts], [tumor, tumour, tumors, metastasis, metastatic], etc. We train a deep word-to-vector model of Mikolov et al. (2013c,b,a) to address this word-level labeling space ambiguity, while also transforming the words to vectors. A total of 1.2 billion words from our radiology reports as well as from biomedical research articles obtained from OpenI (openi: http://openi.nlm.nih.gov) are used. Words with similar meaning are mapped or projected to closer locations in the vector space than dissimilar ones. An example visualization of the word vectors on a two-dimensional space using principal component analysis is shown in Figure 9.\nA skip-gram model of Mikolov et al. (2013a,b) is employed with the mapping vector dimension of R256\u00d71 per word, trained using the hierarchical softmax cost function, slidingwindow size of 10 and frequent words sub-sampled in frequency of 0.01. It is found that combining an additional, more diverse set of related documents such as OpenI biomedical research articles, is helpful for the model to learn a better vector representation while keep-\ning all the hyper-parameters the same. Similar findings on unsupervised feature learning models, that robust faetures can be learned from a slightly noisy and diverse set of input, were reported by Vincent et al. (2010, 2008); Shin et al. (2013). Some examples of query words and their corresponding closest words in terms of cosine similarity for the word-tovector models (Mikolov et al. (2013c)), trained on radiology reports only (total of ~1 billion words) and with additional OpenI articles (total of 1.2 billion words) are shown in Figure 10."}, {"heading": "5.2 Image-to-Description Relation Mining and Matching", "text": "The sentence referring to a key image and its adjacent sentences may contain a variety of words, but we are mostly interested in the disease-related terms which are highly correlated to diagnostic semantics. To obtain only the disease-related terms, we exploit the human disease terms and their synonyms from the Disease-Ontology (DO; Schriml et al. (2012)), a collection of 8,707 unique disease-related terms. While the sentences referring to an image and their adjacent sentences have 50.08 words on average, the number of disease-related terms in the three consecutive sentences is 5.17 on average with a standard deviation of 2.5. Therefore we chose to use bi-grams for the image descriptions, to achieve a good trade-off between the medium level complexity without neglecting too many text-image pairs. Some statistics about the number of words in the documents are shown in Table 5.\nBi-gram disease terms are extracted so that we can train a deep CNN model in Section 5.3 to predict the vector/word- level image representation of R256\u00d72. If multiple bi-grams can be extracted per image from the sentence referring the image and the two adjacent\nones, the image is trained as many times as the number of bi-grams with different target vectors (R256\u00d72). If a disease term cannot form a bi-gram, then the term is ignored, and the process is illustrated in Figure 11. This is a challenging weakly annotated learning problem using referring sentences for labels. The bi-grams of DO disease-related terms in the vector representation of R256\u00d72 are somewhat analogous to the work of Kulkarni et al. (2013) detecting multiple objects of interest and describing their spatial configurations in the image caption. A deep regression CNN model is employed here, to map an image to a continuous output word-vector space from an image. The resulting bi-gram vector can be matched against a reference disease-related vocabulary in the word-vector space using cosine similarity."}, {"heading": "5.3 Image-to-Words Deep CNN Regression", "text": "It has been shown by Sutskever et al. (2014) that deep recurrent neural networks (RNN) can learn the language representation for machine translation. To learn the image-to-text representation, we map the images to the vectors of word sequences describing the image. This can be formulated as a regression CNN, replacing the softmax cost in Section 4 with the cross-entropy cost function for the last output layer of VGG-19 CNN model (Simonyan and Zisserman (2014)):\nE = \u2212 1 n N\u2211 n=1 [g(z)ng\u0302(z\u0304n) + (1\u2212 g(zn)) log(1\u2212 g(z\u0302n))], (2)\nwhere zn or z\u0302n is any uni-element of the target word vectors Zn or optimized output vectors Z\u0302n, g(x) is the sigmoid function (g(x) = 1/(1 + e\nx)), and n is the number of samples in the database.\nWe adopt the CNN model of Simonyan and Zisserman (2014) for the image-to-text representation since it works consistently better than the other relatively simpler model of Krizhevsky et al. (2012) in our image-to-topic mapping tasks. We fine-tune the parameters of the CNNs for predicting the topic-level labels in Section 4 with the modified cost function, to model the image-to-text representation instead of classifying images into categories. The newly modified output layer has 512 nodes for bi-grams as 256 nodes for each word in a bi-gram."}, {"heading": "5.4 Key-Word Generation from Images and Discussion", "text": "For any key image in testing, first we predict its topics of three levels (document-level, document-level sub-topics, sentence-level) using the three deep CNN models of Simonyan and Zisserman (2014) in Section 4. Top 50 key-words in each LDA document-topics are mapped into the word-to-vector space of multivariate variables in R256\u00d71 (Section 5.1). Then, the image is mapped to a R256\u00d72 output vector using the bi-gram CNN model in Section 5.3. Lastly, we match each of the 50 topic key-word vectors of R256\u00d71 against the first and second half of the R256\u00d72 output vector using cosine similarity. The closest key-words at three levels of topics (with the highest cosine similarity against either of the bi-gram words) are kept per image.\nThe rate of predicted disease-related words matching the actual words in the report sentences (recall-at-K, K=1 (R@1 score)) was 0.56. Two examples of key-word generation are shown in Figure 12, with three key-words from three categorization levels per image. We only report R@1 score on disease-related words compared to the previous works of Karpathy et al. (2014); Frome et al. (2013), where they report from R@1 up to R@20 on the entire image caption words (e.g. R@1=0.16 on Flickr30K dataset by Karpathy et al. (2014)). As we used NLP to parse and extract image-describing sentences from the whole radiology reports, our ground-truth image-to-text associations are much noisier than the\ncaption dataset used by Frome et al. (2013); Karpathy et al. (2014). Also for that reason, our generated image-to-text associations are not as exact as the generated descriptions by Frome et al. (2013); Karpathy et al. (2014)."}, {"heading": "5.4.1 Discussion", "text": "Generating key-words for images by CNN regression shows good feasibility for automated interpretation of patient images. The generated key-words describe what to expect from the given image, although sometimes unrelated words can be generated too. Finding and understanding the relations between the generated words will be the next step to explore, for example via more thorough text mining using sophisticated NLP parsing as by Li et al. (2011) and combining them with the specific frequent disease prediction in the next section."}, {"heading": "6. Predicting Presence or Absence of Frequent Disease Types", "text": "While the key-words generation in Section 5 can aid the interpretation of a patient scan, the generated key-words, e.g. \u201cspine\u201d, \u201clung\u201d, are not very specific to a disease in an image. Nonetheless, one of the ultimate goal for large-scale radiology image/text analysis would be to automatically diagnose disease from a patient scan. In order to achieve the goal of automated disease detection, we added an additional pipeline of mining disease words rather than disease-related words using radiology semantics, and predicting these in an image using CNNs with softmax cost-function."}, {"heading": "6.1 Mining Presence/Absence of Frequent Disease Terms", "text": "The disease names in Disease Ontology (DO) contains not only disease terms but also nondisease terms as well describing a disease. Some examples of disease names in DO containing non-disease terms are \u201cocclusion of gallbladder\u201d (DOID: 9714), \u201cacute diarrhea\u201d (DOID: 0050140), \u201cstrawberry gallbladder\u201d (DOID: 10254), and \u201cexocrine pancreatic insufficiency\u201d (DOID: 13316). Nonetheless, it is rare that \u201cocclusion of gallbladder\u201d or \u201cexocrine pancreatic insufficiency\u201d is described in radiology reports exactly that way, making it difficult to mine specific disease terms with presence or absence.\nThe Unified Medical Language System (UMLS) of Lindberg et al. (1993); Humphreys et al. (1998) integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and inter-operable biomedical information systems and services, including electronic health records. It is a compendium of many controlled vocabularies in the biomedical sciences, created in 1986 and maintained by the National Library of Medicine.\nThe Metathesaurus (Schuyler et al. (1993)) forms the base of the UMLS and comprises over 1 million biomedical concepts and 5 million concept names, where all of them are collected from the over 100 incorporated controlled vocabularies and classification systems. The Metathesaurus is organized by concept, where each concept has specific attributes defining its meaning and is linked to the corresponding concept names. The Metathesaurus has 133 semantic types that provide a consistent categorization of all concepts represented in it. Among the 133 semantic types we chose to focus on \u201cT033: finding\u201d and \u201cT047: disease or syndrome\u201d, as they seemed most relevant to be disease specific. Examples of some other semantic types we did not focus on this study are: \u201cT017: anatomical structure\u201d, \u201cT074: medical device\u201d, and \u201cT184: sign or symptom\u201d.\nRadLex (Langlotz (2006)) is a unified language to organize and retrieve radiology imaging reports and medical records. While the Metathesaurus has a vast resource of biomedical concepts, we also used RadLex to confine our disease-term-mining more specifically to radiology related terms. The mined words are one word terms appearing in the \u201cT033: finding\u201d and \u201cT047: disease or syndrome\u201d of the UMLS Metathesaurus appearing also in RadLex (RadLex is not a subset of Metathesaurus).\nWe are not only interested in disease terms associated with image, but also whether the disease mentioned is present or absent. After detecting semantic terms of \u201cT033: finding\u201d and \u201cT047: disease or syndrome\u201d, we used the assertion/negation detection algorithm of Chapman et al. (2001, 2013) to detect presence and absence of disease terms. The algorithm of Chapman et al. (2001, 2013) locates trigger terms which can indicate a clinical condition as negated or possible and determines which text falls within the scope of the trigger terms. The number of occurrences \u201cT033: finding\u201d and \u201cT047: disease or syndrome\u201d detected as assertion or negations in radiology reports are shown in Figure 13.\nWhile the assertion/negation detection of \u201cT047: disease or syndrome\u201d seemed specific enough, the detection of \u201cT033: finding\u201d was not. For example, it seemed difficult to derive any specific disease information from 43,219 occurrences of possible \u201cunchanged\u201d and 422 occurrences of negated \u201cunchanged\u201d. Some other similar examples are: 10,236 occurrences of possible \u201cfinding\u201d and 1,129 occurrences of negated \u201cfinding\u201d; 3,781 occurrences of possible \u201ct2\u201d (a MRI image modality) and 661 occurrences of negated \u201ct2\u201d. We therefore decided\nto focus on \u201cT047: disease or syndrome\u201d terms only, and further ignored the terms which occurred less then 10 times in the whole radiology reports. The total number of \u201cT047: disease or syndrome\u201d terms for detecting their presence are 59, and the total number of the terms for detecting their absence are 18."}, {"heading": "6.2 Predicting Disease in Images using CNN", "text": "Similarly to the object detection task in the ImageNet challenge, we match and detect disease terms found in the sentence of radiology reports referring the image using CNN and softmax cost function. Softmax models the probability of an instance being the class j of N total classes with normalized exponential. It is a generalization of the logistic function summarizing an N dimensional vector z of real values in the range of 0 to 1:\n\u03c3(z)j = ezj\u2211N k=1 e zk . (3)\nSoftmax is often implemented at the final layer of the neural networks used for classification, and is a standard among the CNN based approaches in ImageNet object recognition challenge.\nIn addition to assigning disease terms to images, we also assign negated disease terms as absence of the diseases in the images. The total number of labels is 77 (59 present, 18 absent). If more than one disease term is mentioned for a image, we simply assigned the terms multiple times for an image. Some statistics on the number of assertion/negation occurrences per image are shown in Table 6.\nAs we found in Section 4.2 that transfer learning from the most related model is helpful, we fine-tune the image-to-topic CNN model for the disease prediction model. For this task we fine-tuned from the image to sentence-level-topic (h3) model in Section 4, as the image-tosentence-level-topic seems to be most closely related to the image-to-disease-specific-terms model. Similarly to Section 4, 85% of image-label pairs were used for training, 5% for cross-validation, and 10% for testing."}, {"heading": "6.3 Prediction Result and Discussion", "text": "With the CNN trained to model image to disease presence/absence prediction, the top1 test accuracy achieved is 0.71, and top-5 accuracy is 0.88. We combine this with the previous image-to-topic mapping and key-word generation (Section 5.4) to generate the final output for comprehensive image interpretation. Some examples of test cases where top-1 probability output matches the originally assigned disease labels are shown in Figure 14. It is noticeable that specific disease words are detected with high probability when there is one disease word per image, and with somewhat low top-1 probability for one disease word and the other words within the top-5 probabilities (Figure 14 (b) \u2013 \u201c ... infection abcess\u201d).\nWe can also notice that automatic label assignment to images can sometimes be challenging. In Figure 14 (d) \u201ccyst\u201d is assigned as the correct label based on the original statement \u201c... possibly due to cyst ...\u201d, but it would be unclear whether cyst will be present on the image (and cyst is not visibly apparent). It applies similarly to Figure 14 (e) where the presence of \u201costephyte\u201d is not clear from the referring sentence but was assigned as the correct label (and osteophyte is not visibly apparent on the image). In Figure 14 (f) \u201cno cyst\u201d was labeled and predicted correctly, but it would be less clear what to derive from the prediction result indicating an absence of a disease then a presence.\nSome examples of test cases where top-1 probability does not match the originally assigned labels are shown in Figure 15. Four ((a),(c),(e),(f)) of the six examples contain the originally assigned label in the top-5 probability predictions, which is coherent with the relatively high (88%) top-5 prediction accuracy.\nHere again, Figure 15 (a) was automatically labeled as \u201ccyst\u201d, but cyst is not clearly visible on the image where the original statement \u201c... too small to definitely characterize cyst ...\u201d supports this. The example of Figure 15 (b) shows a failed case of assertion/negation algorithm, where \u201ccyst\u201d is detected as negated based on the statement \u201c... small cyst\u201d. Nonetheless, true label (\u201ccyst\u201d) is detected as its top-1 probability. For Figure 15 (c) \u201ccyst\u201d is predicted where the true label assigned was \u201cabscess\u201d, however cyst and abscess are sometime visible similar. Similarly to Figure 14 (d) it is unclear to find emphysema from the statement like \u201c ... possibly due to emphysema\u201d (and emphysema is not visibly present), but it would be challenging to correctly interpretate such statement for label assignment. Figure 15 (e) shows a disease which can be bronchiectasis, however it is also unclear from the image. Nonetheless, bronchiectasis is predicted with the second highest probability. Bronchiectasis is visible on Figure 15 (f), and it was predicted with second highest probability too."}, {"heading": "6.3.1 Discussion", "text": "Automated mining of disease specific terms enables us to predict disease more specifically with promising result. However, compared to image-to-topic modeling in Section 4 where image labeling was based on topic modeling and loose coupling of image-to-keyword pairs, by matching the images to more specific disease words we lose about 90% of the images for the analysis due to nonspecific original statements. The proportion of the cases where radiologists indicate a disease as strongly positive or negative is often much less then the cases where they describe a finding rather vaguely. By mining and assigning the semantic label \u201cT033: finding\u201d will yield us more image to specific-disease-label pairs. However, it is probably less specific to model an image with a generic term as \u201cmass\u201d (which is a more vague indication of a specific disease such as \u201ccyst\u201d or \u201ctumor\u201d) and detecting it than modeling and detecting an image with a more specific term as \u201ccyst\u201d (similarly to \u201cfinding\u201d or \u201cunchanged\u201d).\nIt is a compromise between whether to go for big data and loose labels, or to go for smaller data and more accurate labels. The key-word generation from the rather loose labeling scheme enables us to use most of the available 216K images. While the generated key-words can help understand the contents of the image, sometimes they are not specific and can also be irrelevant. More specific mining and assignment of specific disease labels to image could provide more accurate and precise disease prediction, however only about 10% of the total images are matched by this scheme. Another alternative is to obtain annotation by radiologists to be even more specific, but the amount of data available will be even smaller due to the time and cost limitations.\nUtilizing bigger data will enable us to make a more generalizable model, but labeling will become more challenging as the amount of data gets bigger and becomes more heterogeneous. The compromise between the amount of data and the quality of labels seems to be a recurring dilemma probably in the majority of the automated mining in big data applications. More advanced NLP processing and comprehensive analysis of hospital discharge summaries, progress notes, and patient histories might address the need to get more specific information relating to an image even when the original image descriptions are not very specific."}, {"heading": "7. Conclusion", "text": "It has been unclear how to extend the significant success in image classification using deep convolutional neural networks from computer vision to medical imaging. What are the clinically relevant image labels to be defined, how to annotate the huge amount of medical images required by deep learning models, and to what extent and scale the deep CNN architecture is generalizable in medical image analysis are the open questions.\nIn this paper, we present an interleaved text/image deep mining system to extract the semantic interactions of radiology reports and diagnostic key images at a very large, unprecedented scale in the medical domain. Images are classified into different levels of topics according to their associated documents, and a neural language model is learned to assign disease terms to predict what is in the image. In addition, we mined and matched specific disease terms for more specific automated image labeling, and demonstrated promising results.\nTo the best of our knowledge, this is the first study performing a large-scale image/text analysis on a hospital picture archiving and communication system database. Our reportextracted key image database is the largest one ever reported and is highly representative of the huge collection of radiology diagnostic semantics over the last decade. Exploring effective deep learning models on this database opens new ways to parse and understand large-scale radiology image informatics.\nWe hope that this study will inspire and encourage other institutions in mining other large unannotated clinical databases, to achieve the goal of establishing a central training resource and performance benchmark for large-scale medical image research, similar to the ImageNet of Deng et al. (2009) for computer vision."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the Intramural Research Program of the National Institutes of Health Clinical Center, and in part by a grant from the KRIBB Research Initiative Program (Korean Visiting Scientist Training Award), Korea Research Institute of Bioscience and Biotechnology, Republic of Korea. This study utilized the high-performance computational capabilities of the Biowulf Linux cluster at the National Institutes of Health, Bethesda, MD (http://biowulf.nih.gov). We thank NVIDIA for the K40 GPU donation."}], "references": [{"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Natural language processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Modeling annotated data", "author": ["D. Blei", "M. Jordan"], "venue": "In ACM SIGIR,", "citeRegEx": "Blei and Jordan.,? \\Q2003\\E", "shortCiteRegEx": "Blei and Jordan.", "year": 2003}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Unsupervised learning in radiology using novel latent variable models", "author": ["Luke Carrivick", "Sanjay Prabhu", "Paul Goddard", "Jonathan Rossiter"], "venue": "In CVPR,", "citeRegEx": "Carrivick et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Carrivick et al\\.", "year": 2005}, {"title": "A simple algorithm for identifying negated findings and diseases in discharge summaries", "author": ["Wendy W Chapman", "Will Bridewell", "Paul Hanbury", "Gregory F Cooper", "Bruce G Buchanan"], "venue": "Journal of biomedical informatics,", "citeRegEx": "Chapman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chapman et al\\.", "year": 2001}, {"title": "Extending the negex lexicon for multiple languages", "author": ["Wendy W Chapman", "Dieter Hilert", "Sumithra Velupillai", "Maria Kvist", "Maria Skeppstedt", "Brian E Chapman", "Michael Conway", "Melissa Tharp", "Danielle L Mowery", "Louise Deleger"], "venue": "Studies in health technology and informatics,", "citeRegEx": "Chapman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chapman et al\\.", "year": 2013}, {"title": "Imagenet: A largescale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Deformations, patches, and discriminative models for automatic annotation of medical radiographs", "author": ["Thomas Deselaers", "Hermann Ney"], "venue": "PRL,", "citeRegEx": "Deselaers and Ney.,? \\Q2008\\E", "shortCiteRegEx": "Deselaers and Ney.", "year": 2008}, {"title": "Nonnegative matrix factorization and probabilistic latent semantic indexing: Equivalence chi-square statistic, and a hybrid method", "author": ["Chris Ding", "Tao Li", "Wei Peng"], "venue": "In Proceedings of the national conference on artificial intelligence,", "citeRegEx": "Ding et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ding et al\\.", "year": 1999}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Gregory Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Relation between plsa and nmf and implications", "author": ["Eric Gaussier", "Cyril Goutte"], "venue": "In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Gaussier and Goutte.,? \\Q2005\\E", "shortCiteRegEx": "Gaussier and Goutte.", "year": 2005}, {"title": "On an equivalence between plsi and lda", "author": ["Mark Girolami", "Ata Kab\u00e1n"], "venue": "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,", "citeRegEx": "Girolami and Kab\u00e1n.,? \\Q2003\\E", "shortCiteRegEx": "Girolami and Kab\u00e1n.", "year": 2003}, {"title": "Natural image bases to represent neuroimaging data", "author": ["Ashish Gupta", "Murat Ayhan", "Anthony Maida"], "venue": "In ICML,", "citeRegEx": "Gupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2013}, {"title": "Learning rich features from rgb-d images for object detection and segmentation", "author": ["S. Gupta", "R. Girshick", "P. Arbelez", "J. Malik"], "venue": "In ECCV,", "citeRegEx": "Gupta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2014}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "The unified medical language system an informatics research collaboration", "author": ["Betsy L Humphreys", "Donald AB Lindberg", "Harold M Schoolman", "G Octo Barnett"], "venue": "Journal of the American Medical Informatics Association,", "citeRegEx": "Humphreys et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Humphreys et al\\.", "year": 1998}, {"title": "Deep features for text spotting", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "In ECCV,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Deep fragment embeddings for bidirectional image sentence mapping", "author": ["Andrej Karpathy", "Armand Joulin", "Fei Fei F Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Hipster wars: Discovering elements of fashion styles", "author": ["H. Kiapour", "K. Yamaguchi", "A. Berg", "T. Berg"], "venue": "In ECCV,", "citeRegEx": "Kiapour et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiapour et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Computer Science Department,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A. Berg", "T. Berg"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Kulkarni et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2013}, {"title": "Attribute-based classification for zero-shot visual object categorization", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Lampert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2014}, {"title": "Radlex: A new method for indexing online educational materials 1. Radiographics", "author": ["Curtis P Langlotz"], "venue": null, "citeRegEx": "Langlotz.,? \\Q2006\\E", "shortCiteRegEx": "Langlotz.", "year": 2006}, {"title": "Learning methods for generic object recognition with invariance to pose and lighting", "author": ["Yann LeCun", "Fu Jie Huang", "Leon Bottou"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "LeCun et al\\.,? \\Q2004\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2004}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Daniel D Lee", "H Sebastian Seung"], "venue": null, "citeRegEx": "Lee and Seung.,? \\Q1999\\E", "shortCiteRegEx": "Lee and Seung.", "year": 1999}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T. Berg", "A. Berg", "Y. Choi"], "venue": "In ACM CoNLL,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "The unified medical language system", "author": ["Donald A Lindberg", "Betsy L Humphreys", "Alexa T McCray"], "venue": "Methods of information in Medicine,", "citeRegEx": "Lindberg et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Lindberg et al\\.", "year": 1993}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Learning high-level judgments of urban perception", "author": ["V. Ordonez", "T. Berg"], "venue": "In ECCV,", "citeRegEx": "Ordonez and Berg.,? \\Q2014\\E", "shortCiteRegEx": "Ordonez and Berg.", "year": 2014}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["Vicente Ordonez", "Girish Kulkarni", "Tamara L Berg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Ordonez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ordonez et al\\.", "year": 2011}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier"], "venue": "In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk,", "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "arXiv preprint arXiv:1409.0575,", "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Multi-attribute spaces: Calibration for attribute fusion and similarity search", "author": ["W. Scheirer", "N. Kumar", "P. Belhumeur", "T. Boult"], "venue": "In CVPR,", "citeRegEx": "Scheirer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Scheirer et al\\.", "year": 2012}, {"title": "Disease ontology: a backbone for disease semantic integration", "author": ["Lynn Marie Schriml", "Cesar Arze", "Suvarna Nadendla", "Yu-Wei Wayne Chang", "Mark Mazaitis", "Victor Felix", "Gang Feng", "Warren Alden Kibbe"], "venue": "Nucleic acids research,", "citeRegEx": "Schriml et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schriml et al\\.", "year": 2012}, {"title": "The umls metathesaurus: representing different views of biomedical concepts", "author": ["Peri L Schuyler", "William T Hole", "Mark S Tuttle", "David D Sherertz"], "venue": "Bulletin of the Medical Library Association,", "citeRegEx": "Schuyler et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Schuyler et al\\.", "year": 1993}, {"title": "Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4d patient data", "author": ["Hoo-Chang Shin", "Matthew R Orton", "David J Collins", "Simon J Doran", "Martin O Leach"], "venue": "Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shin et al\\.,? \\Q1930\\E", "shortCiteRegEx": "Shin et al\\.", "year": 1930}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Richard Socher", "Milind Ganjoo", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Exploring topic coherence over many models and many topics", "author": ["Keith Stevens", "Philip Kegelmeyer", "David Andrzejewski", "David Buttler"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Stevens et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stevens et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories.", "startOffset": 70, "endOffset": 89}, {"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al.", "startOffset": 70, "endOffset": 414}, {"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al.", "startOffset": 70, "endOffset": 445}, {"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al. (2014). Deep CNNs can perform significantly better than traditional shallow learning methods, but usually requires much more training data as was shown by Krizhevsky et al.", "startOffset": 70, "endOffset": 468}, {"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al. (2014). Deep CNNs can perform significantly better than traditional shallow learning methods, but usually requires much more training data as was shown by Krizhevsky et al. (2012); Russakovsky et al.", "startOffset": 70, "endOffset": 641}, {"referenceID": 7, "context": "Introduction The ImageNet Large Scale Visual Recognition Challenge by Deng et al. (2009) provides more than one million labeled images of 1,000 object categories. The accessibility of a huge amount of well-annotated image data in computer vision rekindles deep convolutional neural networks (CNNs) as a premier learning tool to solve the visual object class recognition tasks, as shown by Krizhevsky et al. (2012); Simonyan and Zisserman (2014); Szegedy et al. (2014). Deep CNNs can perform significantly better than traditional shallow learning methods, but usually requires much more training data as was shown by Krizhevsky et al. (2012); Russakovsky et al. (2014). In the medical domain, however, there are no similar large-scale", "startOffset": 70, "endOffset": 668}, {"referenceID": 6, "context": "Building the ImageNet database (Deng et al. (2009)) was mainly a manual process: harvesting images returned from Google image search engine according to the WordNet (Miller (1995)) ontology hierarchy and pruning falsely tagged images using crowd-sourcing such as Amazon Mechanical Turk (AMT).", "startOffset": 32, "endOffset": 51}, {"referenceID": 6, "context": "Building the ImageNet database (Deng et al. (2009)) was mainly a manual process: harvesting images returned from Google image search engine according to the WordNet (Miller (1995)) ontology hierarchy and pruning falsely tagged images using crowd-sourcing such as Amazon Mechanical Turk (AMT).", "startOffset": 32, "endOffset": 180}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels.", "startOffset": 140, "endOffset": 159}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al.", "startOffset": 140, "endOffset": 650}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al.", "startOffset": 140, "endOffset": 677}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation.", "startOffset": 140, "endOffset": 813}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation. Kulkarni et al. (2013) have spearheaded the efforts of learning the semantic connections between image contents and the sentences describing them, such as image captions.", "startOffset": 140, "endOffset": 953}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation. Kulkarni et al. (2013) have spearheaded the efforts of learning the semantic connections between image contents and the sentences describing them, such as image captions. Detecting objects of interest, attributes and prepositions and applying contextual regularization with a conditional random field (CRF) is a feasible approach as shown by Kulkarni et al. (2013), and many useful tools for image annotation using it are available in computer vision.", "startOffset": 140, "endOffset": 1295}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation. Kulkarni et al. (2013) have spearheaded the efforts of learning the semantic connections between image contents and the sentences describing them, such as image captions. Detecting objects of interest, attributes and prepositions and applying contextual regularization with a conditional random field (CRF) is a feasible approach as shown by Kulkarni et al. (2013), and many useful tools for image annotation using it are available in computer vision. Both deep feed-forward CNNs of Krizhevsky et al. (2012); Simonyan and Zisserman (2014) and recurrent neural networks of Mikolov et al.", "startOffset": 140, "endOffset": 1438}, {"referenceID": 3, "context": "Thus we first propose to mine categorical semantic labels using non-parametric topic modeling method \u2013 latent Dirichlet Allocation (LDA) by Blei et al. (2003) \u2013 to provide a semantic interpretation of a patient image in three levels. While this provides a first-level interpretation of a patient image, labeling based on categorization can be nonspecific. To alleviate the issue of non-specificity, we further mined specific disease words in the reports mentioning the images. Feed-forward CNNs were then used to train and predict the presence/absence of the specific disease categories. Our work has been inspired by the works of Deng et al. (2009); Russakovsky et al. (2014) building very large-scale image databases and the works establishing semantic connections of texts and images by Kulkarni et al. (2013). Please note, that there has not yet been much comparable development on large-scale medical imaging interpretation. Kulkarni et al. (2013) have spearheaded the efforts of learning the semantic connections between image contents and the sentences describing them, such as image captions. Detecting objects of interest, attributes and prepositions and applying contextual regularization with a conditional random field (CRF) is a feasible approach as shown by Kulkarni et al. (2013), and many useful tools for image annotation using it are available in computer vision. Both deep feed-forward CNNs of Krizhevsky et al. (2012); Simonyan and Zisserman (2014) and recurrent neural networks of Mikolov et al.", "startOffset": 140, "endOffset": 1469}, {"referenceID": 5, "context": "1 Related Work The ImageCLEF medical image annotation tasks of 2005-2007 by Deselaers and Ney (2008) have 9,000 training and 1,000 testing two-dimensional images, converted to 32 \u00d7 32 pixel thumbnails with 57 labels.", "startOffset": 76, "endOffset": 101}, {"referenceID": 3, "context": ", fibrosis, emphysema) to two-dimensional image blocks from axial CT chest scans of 24 patients is studied by Carrivick et al. (2005). The works of Barnard et al.", "startOffset": 110, "endOffset": 134}, {"referenceID": 3, "context": ", fibrosis, emphysema) to two-dimensional image blocks from axial CT chest scans of 24 patients is studied by Carrivick et al. (2005). The works of Barnard et al. (2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study.", "startOffset": 110, "endOffset": 170}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study.", "startOffset": 8, "endOffset": 31}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al.", "startOffset": 8, "endOffset": 210}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al.", "startOffset": 8, "endOffset": 231}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al.", "startOffset": 8, "endOffset": 424}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds.", "startOffset": 8, "endOffset": 467}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al.", "startOffset": 8, "endOffset": 647}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al.", "startOffset": 8, "endOffset": 669}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al.", "startOffset": 8, "endOffset": 712}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)).", "startOffset": 8, "endOffset": 733}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al.", "startOffset": 8, "endOffset": 893}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al.", "startOffset": 8, "endOffset": 926}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al.", "startOffset": 8, "endOffset": 959}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain.", "startOffset": 8, "endOffset": 987}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al.", "startOffset": 8, "endOffset": 1146}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al.", "startOffset": 8, "endOffset": 1170}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.", "startOffset": 8, "endOffset": 1219}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.Automatic label mining on large, unlabeled datasets is presented by Ordonez et al. (2011); Jaderberg et al.", "startOffset": 8, "endOffset": 1343}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.Automatic label mining on large, unlabeled datasets is presented by Ordonez et al. (2011); Jaderberg et al. (2014), however the variety of the label-space is limited to image text annotations.", "startOffset": 8, "endOffset": 1368}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.Automatic label mining on large, unlabeled datasets is presented by Ordonez et al. (2011); Jaderberg et al. (2014), however the variety of the label-space is limited to image text annotations. We analyze and mine the medical image semantics on both document and sentence levels, and deep CNNs of Jaderberg et al. (2014); Simonyan and Zisserman (2014) are adapted to learn them from image contents.", "startOffset": 8, "endOffset": 1573}, {"referenceID": 2, "context": "(2003); Blei and Jordan (2003) using generative models of combining words and images under a very limited word/image vocabulary has also motivated this study. The most related works are by Socher et al. (2013); Frome et al. (2013) where they first map words into vector space using recurrent neural networks and then project images into the label-associated word-vector embeddings by minimizing the L2 (Socher et al. (2013)) or hinge rank losses (Frome et al. (2013)) between the visual and label manifolds. The language model is trained on the texts of Wikipedia and tested on label-associated images from the CIFAR (Krizhevsky and Hinton (2009); Socher et al. (2013)) and ImageNet datasets (Deng et al. (2009); Frome et al. (2013)). Image-to-language correspondence was learned from ImageNet dataset and reasonably high quality image description datasets (Pascal1K (Rashtchian et al. (2010)), Flickr8K (Hodosh et al. (2013)), Flickr30K (Young et al. (2014))) by Karpathy et al. (2014), where such caption datasets are not available in the medical domain. Graphical models have been employed to predict image attributes by Lampert et al. (2014); Scheirer et al. (2012), or to describe images by Kulkarni et al. (2013) using manually annotated datasets.Automatic label mining on large, unlabeled datasets is presented by Ordonez et al. (2011); Jaderberg et al. (2014), however the variety of the label-space is limited to image text annotations. We analyze and mine the medical image semantics on both document and sentence levels, and deep CNNs of Jaderberg et al. (2014); Simonyan and Zisserman (2014) are adapted to learn them from image contents.", "startOffset": 8, "endOffset": 1604}, {"referenceID": 1, "context": "Software package of Bird et al. (2009) was used for basic NLP pipelines.", "startOffset": 20, "endOffset": 39}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS.", "startOffset": 153, "endOffset": 172}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al.", "startOffset": 153, "endOffset": 267}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies.", "startOffset": 153, "endOffset": 294}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles.", "startOffset": 153, "endOffset": 860}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999).", "startOffset": 153, "endOffset": 1089}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al.", "startOffset": 153, "endOffset": 1157}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al. (2012). Furthermore, pLSA can be regarded as a special case of LDA (Girolami and Kab\u00e1n (2003)) and NMF as a semi-equivalent model of pLSA (Gaussier and Goutte (2005); Ding et al.", "startOffset": 153, "endOffset": 1384}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al. (2012). Furthermore, pLSA can be regarded as a special case of LDA (Girolami and Kab\u00e1n (2003)) and NMF as a semi-equivalent model of pLSA (Gaussier and Goutte (2005); Ding et al.", "startOffset": 153, "endOffset": 1471}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al. (2012). Furthermore, pLSA can be regarded as a special case of LDA (Girolami and Kab\u00e1n (2003)) and NMF as a semi-equivalent model of pLSA (Gaussier and Goutte (2005); Ding et al.", "startOffset": 153, "endOffset": 1543}, {"referenceID": 3, "context": "Document Topic Learning with Latent Dirichlet Allocation We propose to mine image categorization labels using non-parametric topic-modeling algorithm of Blei et al. (2003) on the ~780K radiology text reports in PACS. Unlike the images of ImageNet (Deng et al. (2009); Russakovsky et al. (2014)) which often have a dominant object appearing in the center, our key images are mostly CT and MRI slices showing several organs usually with pathologies. There are high amounts of intrinsic ambiguity in defining and assigning a semantic label set to images, even for experienced clinicians. Our hypothesis is that the large collection of sub-million radiology reports statistically defines the categories meaningful for topic-mining and visual correspondence learning for these topics. Latent Dirichlet Allocation (LDA) was originally proposed by Blei et al. (2003) to find latent topic models for a collection of text documents such as newspaper articles. There are some other popular methods for document topic modeling, such as Probabilistic Latent Semantic Analysis (pLSA) by Hofmann (1999) and Non-negative Matrix Factorization (NMF) by Lee and Seung (1999). We choose LDA for extracting latent topic labels among radiology report documents, because LDA is shown to be more flexible yet learns more coherent topics over large sets of documents, as was studied by Stevens et al. (2012). Furthermore, pLSA can be regarded as a special case of LDA (Girolami and Kab\u00e1n (2003)) and NMF as a semi-equivalent model of pLSA (Gaussier and Goutte (2005); Ding et al. (2006)).", "startOffset": 153, "endOffset": 1563}, {"referenceID": 3, "context": "A lower perplexity score generally implies a better fit of the model for a given document set (Blei et al. (2003)).", "startOffset": 95, "endOffset": 114}, {"referenceID": 3, "context": "A lower perplexity score generally implies a better fit of the model for a given document set (Blei et al. (2003)). Based on the perplexity score evaluated on 80% of the total documents used for training and 20% used for testing, the number of topics chosen is 80 for the document-level model using perplexity scores for model selection (Figure 2). Although the document distribution in the topic space is approximately balanced, the distribution of image counts for the topics is more unbalanced (Figure 3). Specifically, topic #77 (non-primary metastasis spreading across a variety of body parts) contains nearly half of the 216K key images. To address this data bias, sub-topics are obtained for each of the first document-level topics, resulting in 800 topics, where the number of the sub-topics is also chosen based on the average perplexity scores evaluated on each document-level topic. Lastly, to compare the method of using the whole report with using only the sentence directly describing the key images for latent topic mining, a sentence-level LDA topics are obtained based on three sentences only: the sentence mentioning the key-image (Figure 1) and its adjacent sentences as proximal context. The perplexity scores keep decreasing with an increasing number of topics; we choose the topic count to be 1000 as the rate of the perplexity score decrease is very small beyond that point (Figure 2). We observe that LDA-generated image categorization labels are valid, demonstrating good semantic coherence among clinician observers. The lists of key words and sampled images per topic label are subjected to board-certified radiologist\u2019s review and validation. Some examples of document-level topics with their corresponding images and topic key words are shown in Figure 4. Based on radiologists\u2019 review, our LDA topics discover semantics at different levels. There are 73 low-level concepts for example, pathology examination of certain body regions and organs: topic #47 - sinus diseases; #2 - lesions of solid abdominal organs, primarily kidney; #10 - pulmonary diseases; #13 - brain MRI; #19 - renal diseases on mixed imaging modalities; #36 - brain tumors. There are 7 mid- to high-level concepts, such as: topic #77 - non-primary metastasis spreading across a variety of body parts; topic #79 - cases with high diagnosis uncertainty or equivocation; #72 - indeterminate lesions; #74 - instrumentation artifacts limiting interpretation. Low-level topic images tend to be visually more coherent than the higher-level topic images. High-level topics may be analogous to the high-level visual concepts in natural images as was studied by Kiapour et al. (2014); Ordonez and Berg (2014).", "startOffset": 95, "endOffset": 2673}, {"referenceID": 3, "context": "A lower perplexity score generally implies a better fit of the model for a given document set (Blei et al. (2003)). Based on the perplexity score evaluated on 80% of the total documents used for training and 20% used for testing, the number of topics chosen is 80 for the document-level model using perplexity scores for model selection (Figure 2). Although the document distribution in the topic space is approximately balanced, the distribution of image counts for the topics is more unbalanced (Figure 3). Specifically, topic #77 (non-primary metastasis spreading across a variety of body parts) contains nearly half of the 216K key images. To address this data bias, sub-topics are obtained for each of the first document-level topics, resulting in 800 topics, where the number of the sub-topics is also chosen based on the average perplexity scores evaluated on each document-level topic. Lastly, to compare the method of using the whole report with using only the sentence directly describing the key images for latent topic mining, a sentence-level LDA topics are obtained based on three sentences only: the sentence mentioning the key-image (Figure 1) and its adjacent sentences as proximal context. The perplexity scores keep decreasing with an increasing number of topics; we choose the topic count to be 1000 as the rate of the perplexity score decrease is very small beyond that point (Figure 2). We observe that LDA-generated image categorization labels are valid, demonstrating good semantic coherence among clinician observers. The lists of key words and sampled images per topic label are subjected to board-certified radiologist\u2019s review and validation. Some examples of document-level topics with their corresponding images and topic key words are shown in Figure 4. Based on radiologists\u2019 review, our LDA topics discover semantics at different levels. There are 73 low-level concepts for example, pathology examination of certain body regions and organs: topic #47 - sinus diseases; #2 - lesions of solid abdominal organs, primarily kidney; #10 - pulmonary diseases; #13 - brain MRI; #19 - renal diseases on mixed imaging modalities; #36 - brain tumors. There are 7 mid- to high-level concepts, such as: topic #77 - non-primary metastasis spreading across a variety of body parts; topic #79 - cases with high diagnosis uncertainty or equivocation; #72 - indeterminate lesions; #74 - instrumentation artifacts limiting interpretation. Low-level topic images tend to be visually more coherent than the higher-level topic images. High-level topics may be analogous to the high-level visual concepts in natural images as was studied by Kiapour et al. (2014); Ordonez and Berg (2014). About half of the key images are associated with topic #77, implying that the clinicians\u2019 image referencing behavior patterns heavily focuses on metastatic patients.", "startOffset": 95, "endOffset": 2698}, {"referenceID": 19, "context": "Image to Document Topic Mapping with Deep Convolutional Neural Networks For each level of topics discussed in Section 3, we train deep CNNs to map the images into document categories using the Caffe framework of Jia et al. (2014). We split our whole key image dataset as follows: 85% used as the training dataset, 5% as the crossvalidation (CV), and 10% as the test dataset.", "startOffset": 212, "endOffset": 230}, {"referenceID": 22, "context": "1 Implementation All our CNN network settings are similar or the same as the ImageNet Challenge \u201cAlexNet\u201d (Krizhevsky et al. (2012)), and \u201cVGG-16 & 19\u201d (Simonyan and Zisserman (2014)) models.", "startOffset": 107, "endOffset": 132}, {"referenceID": 22, "context": "1 Implementation All our CNN network settings are similar or the same as the ImageNet Challenge \u201cAlexNet\u201d (Krizhevsky et al. (2012)), and \u201cVGG-16 & 19\u201d (Simonyan and Zisserman (2014)) models.", "startOffset": 107, "endOffset": 183}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al.", "startOffset": 53, "endOffset": 71}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al.", "startOffset": 53, "endOffset": 148}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al. (2012) has about 60 million parameters (650,000 neurons) and consists of five convolutional layers (1st, 2nd and 5th followed by max-pooling layers), and three fully-connected (FC) layers with a final classification layer.", "startOffset": 53, "endOffset": 194}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al. (2012) has about 60 million parameters (650,000 neurons) and consists of five convolutional layers (1st, 2nd and 5th followed by max-pooling layers), and three fully-connected (FC) layers with a final classification layer. The VGG variations of CNN models by Simonyan and Zisserman (2014) are significantly deeper by having 16\u223c19 convolutional layers and 133\u223c144 million parameters.", "startOffset": 53, "endOffset": 476}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al. (2012) has about 60 million parameters (650,000 neurons) and consists of five convolutional layers (1st, 2nd and 5th followed by max-pooling layers), and three fully-connected (FC) layers with a final classification layer. The VGG variations of CNN models by Simonyan and Zisserman (2014) are significantly deeper by having 16\u223c19 convolutional layers and 133\u223c144 million parameters. The top-1 error rates on ImageNet dataset of these models are AlexNet:15.3% (Krizhevsky et al. (2012)); VGG-16:7.", "startOffset": 53, "endOffset": 672}, {"referenceID": 19, "context": "For \u201cAlexNet\u201d we used the Caffe reference network of Jia et al. (2014), which is a slight modification to the \u201cAlexNet\u201d by Krizhevsky et al. (2012). The AlexNet model byKrizhevsky et al. (2012) has about 60 million parameters (650,000 neurons) and consists of five convolutional layers (1st, 2nd and 5th followed by max-pooling layers), and three fully-connected (FC) layers with a final classification layer. The VGG variations of CNN models by Simonyan and Zisserman (2014) are significantly deeper by having 16\u223c19 convolutional layers and 133\u223c144 million parameters. The top-1 error rates on ImageNet dataset of these models are AlexNet:15.3% (Krizhevsky et al. (2012)); VGG-16:7.4%; and VGG-19:7.3% (Simonyan and Zisserman (2014)), respectively.", "startOffset": 53, "endOffset": 734}, {"referenceID": 40, "context": "image dataset is about one fifth the size of ImageNet (Russakovsky et al. (2014)) and is the largest annotated medical image dataset to date.", "startOffset": 55, "endOffset": 81}, {"referenceID": 45, "context": "Then we follow the approach of Simonyan and Zisserman (2014) to crop the input images from 256 \u00d7 256 to 227\u00d7 227 for training.", "startOffset": 31, "endOffset": 61}, {"referenceID": 23, "context": "Table 3 provides the top-1 and top-5 testing in classification accuracies for each level of topic models using AlexNet (Krizhevsky et al. (2012)), and VGG-16&19 (Simonyan and Zisserman (2014)) based deep CNN models.", "startOffset": 120, "endOffset": 145}, {"referenceID": 23, "context": "Table 3 provides the top-1 and top-5 testing in classification accuracies for each level of topic models using AlexNet (Krizhevsky et al. (2012)), and VGG-16&19 (Simonyan and Zisserman (2014)) based deep CNN models.", "startOffset": 120, "endOffset": 192}, {"referenceID": 23, "context": "Table 3: Top-1, top-5 test classification accuracies for image to document-level topics, document-level sub-topics (document-level-h2) and sentence-level topics, using AlexNet (Krizhevsky et al. (2012)), and VGG-16&19 (Simonyan and Zisserman (2014)) deep CNN models.", "startOffset": 177, "endOffset": 202}, {"referenceID": 23, "context": "Table 3: Top-1, top-5 test classification accuracies for image to document-level topics, document-level sub-topics (document-level-h2) and sentence-level topics, using AlexNet (Krizhevsky et al. (2012)), and VGG-16&19 (Simonyan and Zisserman (2014)) deep CNN models.", "startOffset": 177, "endOffset": 249}, {"referenceID": 19, "context": "Table 4: Training times for the CNN models used to reach 70,000 iterations,and their memory consumption, using Caffe framework (Jia et al. (2014)) on NVidia Tesla K40 GPU.", "startOffset": 128, "endOffset": 146}, {"referenceID": 45, "context": "However, comparing VGG-16 and VGG19, three additional convolutional layers seem to have contributed to raise the top-5 accuracies by a small amount (\u223c2%), which is coherent with the results reported by Simonyan and Zisserman (2014) for object recognition task on the ImageNet dataset.", "startOffset": 202, "endOffset": 232}, {"referenceID": 45, "context": "Figure 8: Confusion matrices of (a) document-level topic, (b) document-level sub-topic (documentlevel-h2), and (c) sentence- level classification Simonyan and Zisserman (2014) ((b) and (c) can be viewed best in electronic version of this document).", "startOffset": 146, "endOffset": 176}, {"referenceID": 24, "context": "We therefore propose to generate relevant key-word text descriptions similar to Kulkarni et al. (2013), using deep language/image CNN models.", "startOffset": 80, "endOffset": 103}, {"referenceID": 40, "context": "(2010, 2008); Shin et al. (2013). Some examples of query words and their corresponding closest words in terms of cosine similarity for the word-tovector models (Mikolov et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 31, "context": "Some examples of query words and their corresponding closest words in terms of cosine similarity for the word-tovector models (Mikolov et al. (2013c)), trained on radiology reports only (total of ~1 billion words) and with additional OpenI articles (total of 1.", "startOffset": 127, "endOffset": 150}, {"referenceID": 42, "context": "To obtain only the disease-related terms, we exploit the human disease terms and their synonyms from the Disease-Ontology (DO; Schriml et al. (2012)), a collection of 8,707 unique disease-related terms.", "startOffset": 127, "endOffset": 149}, {"referenceID": 24, "context": "The bi-grams of DO disease-related terms in the vector representation of R256\u00d72 are somewhat analogous to the work of Kulkarni et al. (2013) detecting multiple objects of interest and describing their spatial configurations in the image caption.", "startOffset": 118, "endOffset": 141}, {"referenceID": 47, "context": "3 Image-to-Words Deep CNN Regression It has been shown by Sutskever et al. (2014) that deep recurrent neural networks (RNN) can learn the language representation for machine translation.", "startOffset": 58, "endOffset": 82}, {"referenceID": 45, "context": "This can be formulated as a regression CNN, replacing the softmax cost in Section 4 with the cross-entropy cost function for the last output layer of VGG-19 CNN model (Simonyan and Zisserman (2014)):", "startOffset": 168, "endOffset": 198}, {"referenceID": 44, "context": "We adopt the CNN model of Simonyan and Zisserman (2014) for the image-to-text representation since it works consistently better than the other relatively simpler model of Krizhevsky et al.", "startOffset": 26, "endOffset": 56}, {"referenceID": 23, "context": "We adopt the CNN model of Simonyan and Zisserman (2014) for the image-to-text representation since it works consistently better than the other relatively simpler model of Krizhevsky et al. (2012) in our image-to-topic mapping tasks.", "startOffset": 171, "endOffset": 196}, {"referenceID": 45, "context": "4 Key-Word Generation from Images and Discussion For any key image in testing, first we predict its topics of three levels (document-level, document-level sub-topics, sentence-level) using the three deep CNN models of Simonyan and Zisserman (2014) in Section 4.", "startOffset": 218, "endOffset": 248}, {"referenceID": 42, "context": "Bi-grams are selected from the image\u2019s reference sentences containing disease-related terms from the disease ontology (DO; Schriml et al. (2012)).", "startOffset": 123, "endOffset": 145}, {"referenceID": 19, "context": "We only report R@1 score on disease-related words compared to the previous works of Karpathy et al. (2014); Frome et al.", "startOffset": 84, "endOffset": 107}, {"referenceID": 10, "context": "(2014); Frome et al. (2013), where they report from R@1 up to R@20 on the entire image caption words (e.", "startOffset": 8, "endOffset": 28}, {"referenceID": 10, "context": "(2014); Frome et al. (2013), where they report from R@1 up to R@20 on the entire image caption words (e.g. R@1=0.16 on Flickr30K dataset by Karpathy et al. (2014)).", "startOffset": 8, "endOffset": 163}, {"referenceID": 10, "context": "caption dataset used by Frome et al. (2013); Karpathy et al.", "startOffset": 24, "endOffset": 44}, {"referenceID": 10, "context": "caption dataset used by Frome et al. (2013); Karpathy et al. (2014). Also for that reason, our generated image-to-text associations are not as exact as the generated descriptions by Frome et al.", "startOffset": 24, "endOffset": 68}, {"referenceID": 10, "context": "caption dataset used by Frome et al. (2013); Karpathy et al. (2014). Also for that reason, our generated image-to-text associations are not as exact as the generated descriptions by Frome et al. (2013); Karpathy et al.", "startOffset": 24, "endOffset": 202}, {"referenceID": 10, "context": "caption dataset used by Frome et al. (2013); Karpathy et al. (2014). Also for that reason, our generated image-to-text associations are not as exact as the generated descriptions by Frome et al. (2013); Karpathy et al. (2014).", "startOffset": 24, "endOffset": 226}, {"referenceID": 29, "context": "Finding and understanding the relations between the generated words will be the next step to explore, for example via more thorough text mining using sophisticated NLP parsing as by Li et al. (2011) and combining them with the specific frequent disease prediction in the next section.", "startOffset": 182, "endOffset": 199}, {"referenceID": 26, "context": "The Unified Medical Language System (UMLS) of Lindberg et al. (1993); Humphreys et al.", "startOffset": 46, "endOffset": 69}, {"referenceID": 15, "context": "(1993); Humphreys et al. (1998) integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and inter-operable biomedical information systems and services, including electronic health records.", "startOffset": 8, "endOffset": 32}, {"referenceID": 15, "context": "(1993); Humphreys et al. (1998) integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and inter-operable biomedical information systems and services, including electronic health records. It is a compendium of many controlled vocabularies in the biomedical sciences, created in 1986 and maintained by the National Library of Medicine. The Metathesaurus (Schuyler et al. (1993)) forms the base of the UMLS and comprises over 1 million biomedical concepts and 5 million concept names, where all of them are collected from the over 100 incorporated controlled vocabularies and classification systems.", "startOffset": 8, "endOffset": 466}, {"referenceID": 15, "context": "(1993); Humphreys et al. (1998) integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and inter-operable biomedical information systems and services, including electronic health records. It is a compendium of many controlled vocabularies in the biomedical sciences, created in 1986 and maintained by the National Library of Medicine. The Metathesaurus (Schuyler et al. (1993)) forms the base of the UMLS and comprises over 1 million biomedical concepts and 5 million concept names, where all of them are collected from the over 100 incorporated controlled vocabularies and classification systems. The Metathesaurus is organized by concept, where each concept has specific attributes defining its meaning and is linked to the corresponding concept names. The Metathesaurus has 133 semantic types that provide a consistent categorization of all concepts represented in it. Among the 133 semantic types we chose to focus on \u201cT033: finding\u201d and \u201cT047: disease or syndrome\u201d, as they seemed most relevant to be disease specific. Examples of some other semantic types we did not focus on this study are: \u201cT017: anatomical structure\u201d, \u201cT074: medical device\u201d, and \u201cT184: sign or symptom\u201d. RadLex (Langlotz (2006)) is a unified language to organize and retrieve radiology imaging reports and medical records.", "startOffset": 8, "endOffset": 1294}, {"referenceID": 7, "context": "We hope that this study will inspire and encourage other institutions in mining other large unannotated clinical databases, to achieve the goal of establishing a central training resource and performance benchmark for large-scale medical image research, similar to the ImageNet of Deng et al. (2009) for computer vision.", "startOffset": 281, "endOffset": 300}], "year": 2015, "abstractText": "Despite tremendous progress in computer vision, there has not been an attempt for machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital\u2019s Picture Archiving and Communication System. With natural language processing, we mine a collection of representative \u223c216K two-dimensional key images selected by clinicians for diagnostic reference, and match the images with their descriptions in an automated manner. Our system interleaves between unsupervised learning and supervised learning on documentand sentence-level text collections, to generate semantic labels and to predict them given an image. Given an image of a patient scan, semantic topics in radiology levels are predicted, and associated key-words are generated. Also, a number of frequent disease types are detected as present or absent, to provide more specific interpretation of a patient scan. This shows the potential of largescale learning and prediction in electronic patient records available in most modern clinical institutions.", "creator": "LaTeX with hyperref package"}}}