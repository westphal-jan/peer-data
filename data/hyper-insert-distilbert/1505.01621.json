{"id": "1505.01621", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2015", "title": "Blind Compressive Sensing Framework for Collaborative Filtering", "abstract": "existing works based specifically on successful latent factor analysis models have focused on representing the attribute rating control matrix termed as a product category of user and the item versus latent factor matrices, both characteristics being dense. latent ( factor ) vectors correctly define the degree to which a trait is possessed by an adversary item or the specific affinity characteristics of various user characteristics towards that chosen trait. a weakly dense robust user matrix is a reasonable aggregation assumption shown as, each assessed user will usually like / dislike a trait object to certain extent.'however, matching any correlated item will possess in only a few of namely the attributes and affects never all. hence, the item matrix should ideally have a sparse composite structure rather broader than a dense one as so formulated out in earlier phylogenetic works. therefore we simultaneously propose to factor the specific ratings matrix into just a dense user matrix and compute a sparse item sample matrix which leads us better to the blind function compressed alternative sensing ( inverse bcs ) framework. we derive an efficient selection algorithm for solving the classical bcs comparison problem based essentially on majorization minimization ( mm ) technique. our proposed approach used is first able to achieve substantially significantly higher search accuracy and implement shorter compute run times as analyzed compared to existing sparse approaches.", "histories": [["v1", "Thu, 7 May 2015 08:19:05 GMT  (301kb)", "http://arxiv.org/abs/1505.01621v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["anupriya gogna", "angshul majumdar"], "accepted": false, "id": "1505.01621"}, "pdf": {"name": "1505.01621.pdf", "metadata": {"source": "CRF", "title": "Blind Compressive Sensing Framework for Collaborative Filtering", "authors": ["Anupriya Gogna", "Angshul Majumdar"], "emails": ["anupriyag@iiitd.ac.in", "angshul@iiitd.ac.in"], "sections": [{"heading": null, "text": "representing the rating matrix as a product of user and item latent factor matrices, both being dense. Latent (factor) vectors define the degree to which a trait is possessed by an item or the affinity of user towards that trait. A dense user matrix is a reasonable assumption as each user will like/dislike a trait to certain extent. However, any item will possess only a few of the attributes and never all. Hence, the item matrix should ideally have a sparse structure rather than a dense one as formulated in earlier works. Therefore we propose to factor the ratings matrix into a dense user matrix and a sparse item matrix which leads us to the Blind Compressed Sensing (BCS) framework. We derive an efficient algorithm for solving the BCS problem based on Majorization Minimization (MM) technique. Our proposed approach is able to achieve significantly higher accuracy and shorter run times as compared to existing approaches."}, {"heading": "Keywords", "text": "Blind compressive sensing, collaborative filtering, latent factor model, matrix factorization, recommender system."}, {"heading": "1. INTRODUCTION", "text": "With an exponential rise in online resources and retailers the users are inundated with choices. The difficulty in evaluating huge number of options and selecting a few becomes a daunting task. This is what forms the ground for increased interest in design of efficient recommender systems (RS) in recent times. RS suggests a few items of relevance to the customers from the repository based on their past behaviour or choices. They make use of either explicit (ratings given by users) data or information extracted from implicit sources (browsing history or buying pattern) to make appropriate suggestions. Collaborative filtering (CF) [1] is the most popular approach for design of RS. These techniques basically rest on the assumption that if two users rate a few items similarly, they will exhibit similar choice for other items as well. This belief can be exploited in two ways - memory based approach and latent factor model. Memory based models [2] [3] are heuristic methods which use the available rating data to find users similar to the target user and a weighted average of ratings by similar users is used as to predict rating for the target user. On the other hand (latent factor) model based techniques [4] fundamentally represent the available information in the form of a lower dimensional structure under the proposition that the overall rating matrix can be represented by a relatively small number of independent variables or factors (Latent factor model). In latent factor models a user is modelled as a vector describing his/her affinity to the all the latent factors and items are modelled as\nvectors defined by the degree to which they possess each of the latent factor. They have been shown to outperform their memory based counterparts, in terms of Quality of Prediction (QoP) and coverage [5]. Most of the current research is centred on latent factor models. They mostly focus on improving the prediction accuracy by incorporating additional constraints or information into the existing framework. However, the basic model remains the same; involving factorization of rating matrix into two matrices one representing users and other items (1) - both being dense (dense solution promoted by Frobenius norm constraint).\n 2 2 2min ( ) 2 , Y A UV U V F F U V    (1)\nwhere, U and V represent the user and item latent factor matrix respectively,  is the regularization parameter, Y is the available set of ratings and A is a linear operator. Traditionally two methods Stochastic Gradient Descent (SGD) (http://sifter.org/ simon/ journal/20061211.html) and Alternating Least Squares (ALS) [6] are commonly employed to solve the above problem.\nOur work focusses on changing the basic structure of the latent factor model itself while retaining the primary concept that a limited number of (latent) factors affect the overall rating structure. We propose to factor the rating matrix into a dense user latent factor matrix and a sparse item factor matrix, by replacing\nFrobenius norm constraint on V by a sparsity promoting 1l constraint (2).\n 2 2min ( ) 2 1, Y A UV U vec V Fu vU V     (2)\nwhere, u  and v  are the regularization parameters.\nThis stems from the reasoning that no item will ever possess all the factors and hence its latent factor vector will have zeros for attributes not possessed by the items. The remaining paper is organized as follows. In section 2, we discuss our problem formulation and proposed algorithm. Section 3 contains the experimental setup, results and comparisons with existing CF techniques. Paper ends with conclusion and future direction in section 4."}, {"heading": "2. PROPOSED FORMULATION AND ALGORITHM", "text": "In this section, we discuss our novel proposition for latent factor model based formulation for design of an effectual recommender system."}, {"heading": "2.1 Problem Formulation", "text": "Consider a rating matrix MXNR in which a value ,m n indicates the rating (explicit) given by mth user (out of M total users) on the nth item (out of N total items). As previously discussed, in latent factor approach each user/item can be modelled as a vector defining its affinity/degree of possession of a latent factor. For example, a book may be characterized in terms of features such as drama, comic, and sci-fi and a user vector will have values based on his liking/disliking of these features. Preference of a user for a particular item can hence be formulated as the interaction between their individual latent factor vectors as in (3)\n, , U V m n m n   (3)\nwhere, Vn is the latent factor vector of item n and Um is the m th\nuser\u2019s latent factor vector.\nHowever, the actual (explicit) ratings are not just a result of this interaction. They are plagued by certain inherent biases of users and items. For example, a critical user, giving lesser valued ratings, has an inherent negative bias, whereas a popular or award winning book/movie will tend to be given higher ratings by all users generally, thereby afflicting it with a positive bias. Thus, the actual rating can be modelled by modifying (3) by incorporating the bias terms (or baseline estimates) with the interaction part.\n, = +b +b + U ,V m n g m n m n  (4)\nwhere, g is the global mean, bm is the bias of m th user and bn is\nthe bias of nth item. +b +b g m n  forms the baseline part.\nIn our formulation, we estimate the baseline offline from the available dataset using method outlined in [7] by solving the following convex optimization framework.\n  2\n2 2min ,* 2 b b b b m n m n g m n\nb\n       (5)\nwhere,  is the regularization parameter. Once baseline is estimated, we extract the interaction part from the available ratings as - - - , , Z b b m n m n g m n  .\nTraditionally, Z is completely recovered from its sub sampled observation using matrix factorization approach outlined in (1). Equation (1) aims to recover the rating (interaction) matrix assuming that both user and item latent factor matrices have a dense structure or none of the terms in the latent factor vectors are zero for either users or items.\nHowever, such an assumption is not entirely correct. Consider a RS which recommends movies to users. Each user can be described as a vector of values indicating his preference for a particular trait such as violence, musical, rom-com etc. Usually all users will have a certain affinity either for or against each of these traits. Thus the latent factor vector for any user will be dense, having non zero values throughout. However, the same reasoning cannot be applied to the items. Each item, say a movie in above scenario will either possess a trait or not. For example, a movie with a comic storyline or a children movie, will not have any presence of violence in it. Each item will thus be profiled by a latent factor vector which is sparse. There will be zeros in\naltogether. Thus the item matrix (V) will have sparse structure with several zeros in each column unlike the assumption made in previous models.\nCarrying forward this logic, we put forth a new formulation for matrix factorization model. We aim to predict the missing ratings by formulating an optimization problem which promotes recovery of a dense user factor matrix (U) and a sparse item factor matrix (V) as in (2), repeated here for convenience.\n 2 2min ( ) 2 1,V Y A U V U vec V Fu vU      (6)\nOur formulation follows the Blind Compressive Sensing framework [8] [9] given in (7).\n  2 2\u02c6 \u02c6min . .\n1\u02c6 2, 1\nl M c y c s t const\ni i Fc i\n  \n         \n(7)\nwhere, the task is to estimate the sparse representation (c) and\nsparsifying basis ( \u0302 ) from a given set of observations (y). M is the sub sampling operator and  is the regularization parameter.\nCompressive sensing framework focusses on estimating the sparse representation of a signal assuming that the sparsifiying basis is known a priori. BCS proposes an augmentation of CS theory, which assumes that the sparsifying basis is not known and the same is estimated along with the sparse coefficients only. The\nconstraint on (bounded Frobenius norm) along with 1l penalty term (on c) ensures uniqueness of the solution. Our formulation is similar to unconstrained form of (7) with the sparse coefficient set (c) equivalent to (V) and the dictionary being equivalent to (U).\nBefore deriving the algorithm for solving our problem, we will briefly justify the choice of BCS for solving our problem. According to the CS convention, A is the sensing matrix and U is the sparsifying basis / dictionary. For our problem A is a Dirac / sampling operator; A is the canonical basis. In order to satisfy the incoherence criterion demanded by CS the dictionary U should be incoherent with A, i.e. it should be incoherent with the canonical basis. In other words, the elements in U should be of small magnitude in order to ensure the coherence to be small. As the columns of A are sparse (canonical basis), the rows of U should be dense, i.e. should have small valued coefficients throughout. The 2l penalty on the dictionary (U) enforces a minimum energy\nsolution; it is well known that the minimum energy solution is dense with small values everywhere. This guarantees that the dictionary is incoherent with the sensing matrix A. Therefore, by design the BCS formulation yields a theoretically sound recovery framework for our problem."}, {"heading": "2.2 Algorithm Design", "text": "To efficiently solve our formulation and recover the complete rating matrix, we propose an algorithm following the Majorization Minimization (MM) approach [11]. The use of MM approach enables us to break complex optimization problem into simpler and more efficiently solvable steps.\nFirst we briefly discuss the MM approach to depict its advantage in reducing computational complexity. Consider an\nunderdetermined system of equation, y Ax\nwhere, , l p A R where p l    is a fat matrix. As A is not full rank,\nwe can compute x by least square minimization as   1 T Tx A A A y   which involves computation of inverse of\nvery large, such as in recommender systems, the size of TA A becomes prohibitively large to efficiently compute its inverse within reasonable resource requirements. Use of MM approach eliminates the need to compute such inverses and reduces the computational burden significantly.\nMM approach essentially involves replacing the minimization of an existing function, F(x), by minimization of another function, G(x), which is easier to minimize. G(x) should be majorizer of F(x) i.e.\n1. ( ) ( )G x F x x \n2. ( ) ( ) G x F x at x x k  \nFor the function 2\n( ) 2\nF x y Ax  , we can take G(x) s.t. at kth\niteration it is given by (8)\n    2( ) 2 T TG x y Ax x x I A A x x k k k       (8)\nwhere, max ( )Teig A A  .\nAfter some mathematical manipulation it can be shown that,\n( )G x can be minimized iteratively using following steps.\n 1 Tz x A y Axk k   (9)\n2\n2 min z z x (10)\nNow, we consider our problem formulation (6). We use ADMM (Alternate Direction Method of Multipliers), as the variables U and V are separable, to split (6) into 2 simpler sub problems.\nSub problem 1\n  2 2\nmin 2 Y A UV U FuU   (11)\nSub problem 2\n    2\nmin 2 1 Y A UV vec V vV   (12)\nWe use majorization minimization, as described before, for solving both the sub problems.\nFirstly, considering sub problem 1 (11) we can solve it in the following algorithmic steps using MM approach.\n  \n \n1\n, max\nTZ U V A Y A U V k k k k\nTWhere eig A A\n\n\n  \n\n(13)\n2 2 min\n2 Z UV U FuU   (14)\nEquation (14) of the above algorithm can be recast in following form\nw , T TU N D here D VV I and N ZV u      (15)\nEquation (15) is just a linear system of equation which can be solved iteratively using any gradient descent algorithm.\n2nd sub problem can also be similarly reorganized to give\n  11 1 TW U V A Y A U V k k k      (16)\n2 min\n2 1 W UV V vV   (17)\nFor solving (17), we use iterative soft thresholding as follows\n \n   \n \n, 2\n1 , T\n, ( )max 0,| | -\n1.01 max\nvV soft T\nTWhere V U W UV\nand soft t s sign t t s\nTand eig U U\n\n\n\n\n     \n \n  \n\n \n(18)\nBoth the sub problems are alternately solved till a desired number of iterations are complete or objective function converges. The complete algorithm is outlined in fig. 1."}, {"heading": "3. EXPERIMENTAL SETUP AND RSULTS", "text": "We conducted experiment on the movielens 100K and 1M datasets (http://grouplens.org/datasets/movielens/). The 100K dataset consist of 100,000 ratings (rating value ranging from 1-5) given by 943 users on 1682 movies. 1M dataset consists of ratings on 3952 movies by 6040 users. Both the datasets are divided into test and training data to perform cross validation. We conducted three, five and ten fold cross validation to vary the ratio of sizes of test and training data. For \u2018n\u2019 fold cross validation the entire dataset is split into n blocks. \u2018n-1\u2019 blocks are combined to form the training data and the nth set is taken as the test set. The simulations are carried out on system with i7-3770S CPU @3.10GHz with 8GB RAM.\nFor our algorithm (BCS-CF), the value of regularization parameter, in (5) is taken to be 1e-3. The value of regularization parameters for (6) is kept at\nu   1e3 and v   1e-1 for the 100K\ndataset and u   1e4 and v   1e-1 for the 1M dataset. The values are achieved using L-curve technique [12]. The number of latent\n  \n: , 0 0\n;\n. ,\n_ ( ) - _ ( -1\n1\n1\n1\n1\n) 1 - 7\nInitialization U V rand\nSet regularization parameters\nSet maximum no of iterations N\nwhile k N or\nTZ U V A Y A U V k k k k\nT TU Solve U\no\nZV VV I k u\nTW\nbj func k obj fu\nU V A Y k k\nnc k e\n\n\n\n  \n              \n \n\n\n\n\n  \n  1 ,\n1\n21\nA U V k k\nT vV Soft V U W UV\nend while\nk\n\n\n \n      \n \nfactors (rank) of user/item matrices is taken to be 50. The values are selected after extensive experimentation.\nThe results of our formulation is compared against the results obtained using SGD formulation proposed in [7], Accelerated Proximal Gradient based matrix completion (APG) algorithm [13], Matrix completion using Split bregman (MCSB) [14] and optSpace [15]. We also compare our results against the BCS algorithm (BCSJ) put forth in [9] to show the efficiency of our algorithm. The algorithms are compared terms of Quality of prediction, measured in terms of Mean Absolute Error (MAE) (19) and on the basis of run times.\n\u02c6 , ,\n, m n m n\nm n MAE\n \n \n(19)\nwhere, ,m n\n and \u02c6\n,m n  are the actual and predicted ratings and\n is the cardinality of the rating matrix .\nTable 1. MAE for 100K dataset"}, {"heading": "Algorithm MAE-3 Fold MAE-5 Fold MAE-10 Fold", "text": "BCS-CF 0.7417 0.7215 0.7140\nMC-SB 0.7454 0.7323 0.7279\nAPG 0.8573 0.8847 0.9187\nOptSpace 0.7629 0.7450 0.7323\nBCS-J 0.7527 0.7430 0.7414\nSGD 0.8002 0.7432 0.7312\nTable 1. MAE for 1M dataset"}, {"heading": "Algorithm MAE-3 Fold MAE-5 Fold MAE-10 Fold", "text": "BCS-CF 0.6835 0.6762 0.6712\nMC-SB 0.6999 0.6943 0.6897\nAPG 1.0178 0.9782 0.9352\nOptSpace 0.6907 0.6886 0.6844\nBCS-J 0.6967 0.6917 0.6858\nSGD 0.6988 0.6936 0.6907\nTable 1 gives the MAE values of all the algorithms for 3, 5 and 10 fold cross validation. The results shown are averaged values for 100 independent runs of each algorithms on each test-train pair. It is evident from all the three sets of data that our algorithm perform better than both the other techniques compared against. Our algorithm gives an improvement in recovery accuracy of around 3% (for 10 fold validation) to 8 % (for 3 fold cross validation) over SGD. Thus, it is clear that performance of SGD deteriorates as the size of available training set reduces (10 fold to 3 fold) but our algorithm consistently performs well. As compared to other state of the art matrix completion approaches (APG, optSpace and MCSB) our algorithm gives an improvement of atleast 2%.\nTable 2 gives the MAE values on 1M dataset for all the three methods. Even for the 1M dataset, our algorithm gives improved results over others compared against (around 3 % improvement in MAE).\nTable 3 gives the average run times of all the algorithms for both\n100K and 1M datasets for 5-fold validation. The run times for other cross validation schemes are also in close vicinity. From entries in table 3, it is clear that SGD is very slow (40 times slower) as compared to our formulation. This is mainly because of the large number of iterations involved in the former. The BCS-J algorithm also is slower than ours by around 5 times. Our formulation is around 4 times faster than the fastest of the algorithms compared against i.e. optSpace. As the size of dataset increases (100K to 1M), the time required also increases and thus our algorithm is better suited for large recommender systems viza-viz others. A faster algorithm is not only more appropriate for online computations, it also reduces the burden on the system if recalculations need to be made when a new user/item is added. Thus updation can be carried out more promptly."}, {"heading": "4. CONCLUSION", "text": "In this work, we propose a novel formulation based on latent factor model for collaborative filtering. The basic formulation remains the same, i.e. the ratings matrix can be factored into two matrices \u2013 user latent factor matrix and item latent factor matrix. However all prior studies assumed that both these matrices are dense. We argue that this is not the case; the user latent factor matrix is dense but the item latent factor matrix is supposed to be sparse. This is because all users are likely to have an affinity for all the different factors, but it is not possible for the items to possess all factors simultaneously.\nWe show that our proposed formulation naturally fits into the recently proposed blind compressive sensing framework. BCS is a recent framework and there are no efficient algorithms for solving the BCS problem. In this paper we propose a majorization minimization algorithm to solve the BCS problem. Use of MM approach greatly reduces the computational complexity and hence execution times of our algorithm. Our claim is braced by comparison of run times of other methods viz-a-viz ours. A fast algorithm aids in easier updation of model with addition of new users or items along with faster online computations.\nIn this work we have experimented on movie datasets. The proposed formulation yields significant improvement over existing matrix factorization models. In the future, we would like to see if our proposal enjoys the same success for other recommendation systems (books, garments etc.) as well."}, {"heading": "5. REFERENCES", "text": "[1] Xiaoyuan Su and TaghiMKhoshgoftaar, \u201cA survey of\ncollaborative filtering techniques,\u201d Advances in artificial intelligence, vol. 2009, pp. 4, 2009.\n[2] Buhwan Jeong, Jaewook Lee, and Hyunbo Cho,\n\u201cImproving memory-based collaborative filtering via similarity updating and prediction modulation,\u201d Information Sciences, vol. 180, no. 5, pp. 602\u2013612, 2010.\n[3] Badrul Sarwar, George Karypis, Joseph Konstan, and\nJohn Riedl, \u201cItem-based collaborative filtering recommendation algorithms,\u201d in Proceedings of the 10th international conference on World Wide Web. ACM, 2001, pp. 285\u2013295.\n[4] Thomas Hofmann, \u201cLatent semantic models for\ncollaborative filtering,\u201d ACM Transactions on Information Systems (TOIS), vol. 22, no. 1, pp. 89\u2013115, 2004.\n[5] Gediminas Adomavicius and Alexander Tuzhilin,\n\u201cToward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions,\u201d Knowledge and Data Engineering, IEEE Transactions on, vol. 17, no. 6, pp. 734\u2013749, 2005.\n[6] Robert M Bell and Yehuda Koren, \u201cScalable\ncollaborative filtering with jointly derived neighborhood interpolation weights,\u201d in Data Mining, 2007. ICDM 2007. Seventh IEEE International Conference on. IEEE, 2007, pp. 43\u201352.\n[7] Yehuda Koren, Robert Bell, and Chris Volinsky,\n\u201cMatrix factorization techniques for recommender systems,\u201d Computer, vol. 42, no. 8, pp. 30\u201337, 2009.\n[8] Sivan Gleichman and Yonina C Eldar, \u201cBlind\ncompressed sensing,\u201dInformation Theory, IEEE Transactions on, vol. 57, no. 10, pp. 6958\u20136975, 2011.\n[9] Sajan Goud Lingala and Mathews Jacob, \u201cBlind\ncompressive sensing dynamic mri,\u201d Medical Imaging, IEEE Transactions on, vol. 32, no. 6, pp. 1132\u20131145, 2013.\n[10] Emmanuel Candes and Justin Romberg, \u201cSparsity and\nincoherence in compressive sampling,\u201d Inverse problems, vol. 23, no. 3, pp. 969, 2007.\n[11] M\u00b4ario AT Figueiredo, Jos\u00b4e M Bioucas-Dias, and\nRobert D Nowak, \u201cMajorization\u2013minimization algorithms for wavelet based image restoration,\u201d Image Processing, IEEE Transactions on, vol. 16, no. 12, pp. 2980\u20132991, 2007.\n[12] Charles L Lawson and Richard J Hanson, Solving least\nsquares problems, vol. 161, SIAM, 1974.\n[13] Kim-Chuan Toh and Sangwoon Yun, \u201cAn accelerated\nproximal gradient algorithm for nuclear norm regularized linear least squares problems,\u201d Pacific Journal of Optimization, vol. 6, no. 615-640, pp. 15, 2010.\nMajumdar, \u201cMatrix recovery using split bregman,\u201d arXiv preprint arXiv:1312.6872, 2013.\n[15] Raghunandan H Keshavan, Andrea Montanari, and\nSewoong Oh, \u201cMatrix completion from a few entries,\u201d Information Theory, IEEE Transactions on, vol. 56, no. 6, pp. 2980\u20132998, 2010."}], "references": [{"title": "A survey of collaborative filtering techniques", "author": ["Xiaoyuan Su", "TaghiMKhoshgoftaar"], "venue": "Advances in artificial intelligence, vol. 2009, pp. 4, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving memory-based collaborative filtering via similarity updating and prediction modulation", "author": ["Buhwan Jeong", "Jaewook Lee", "Hyunbo Cho"], "venue": "Information Sciences, vol. 180, no. 5, pp. 602\u2013612, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["Badrul Sarwar", "George Karypis", "Joseph Konstan", "John Riedl"], "venue": "Proceedings of the 10th international conference on World Wide Web. ACM, 2001, pp. 285\u2013295.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Latent semantic models for collaborative filtering", "author": ["Thomas Hofmann"], "venue": "ACM Transactions on Information Systems (TOIS), vol. 22, no. 1, pp. 89\u2013115, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["Gediminas Adomavicius", "Alexander Tuzhilin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 17, no. 6, pp. 734\u2013749, 2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["Robert M Bell", "Yehuda Koren"], "venue": "Data Mining, 2007. ICDM 2007. Seventh IEEE International Conference on. IEEE, 2007, pp. 43\u201352.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": "Computer, vol. 42, no. 8, pp. 30\u201337, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Blind compressed sensing,\u201dInformation", "author": ["Sivan Gleichman", "Yonina C Eldar"], "venue": "Theory, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Blind compressive sensing dynamic mri", "author": ["Sajan Goud Lingala", "Mathews Jacob"], "venue": "Medical Imaging, IEEE Transactions on, vol. 32, no. 6, pp. 1132\u20131145, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparsity and incoherence in compressive sampling", "author": ["Emmanuel Candes", "Justin Romberg"], "venue": "Inverse problems, vol. 23, no. 3, pp. 969, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Majorization\u2013minimization algorithms for wavelet based image restoration", "author": ["M \u0301ario AT Figueiredo", "Jos \u0301e M Bioucas-Dias", "Robert D Nowak"], "venue": "Image Processing, IEEE Transactions on, vol. 16, no. 12, pp. 2980\u20132991, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems", "author": ["Kim-Chuan Toh", "Sangwoon Yun"], "venue": "Pacific Journal of Optimization, vol. 6, no. 615-640, pp. 15, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Matrix recovery using split bregman", "author": ["Anupriya Gogna", "Ankita Shukla", "Angshul Majumdar"], "venue": "arXiv preprint arXiv:1312.6872, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix completion from a few entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "Information Theory, IEEE Transactions on, vol. 56, no. 6, pp. 2980\u20132998, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Collaborative filtering (CF) [1] is the most popular approach for design of RS.", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "Memory based models [2] [3] are heuristic methods which use the available rating data to find users similar to the target user and a weighted average of ratings by similar users is used as to predict rating for the target user.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "Memory based models [2] [3] are heuristic methods which use the available rating data to find users similar to the target user and a weighted average of ratings by similar users is used as to predict rating for the target user.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "On the other hand (latent factor) model based techniques [4] fundamentally represent the available information in the form of a lower dimensional structure under the proposition that the overall rating matrix can be represented by a relatively small number of independent variables or factors (Latent factor model).", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "They have been shown to outperform their memory based counterparts, in terms of Quality of Prediction (QoP) and coverage [5].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "html) and Alternating Least Squares (ALS) [6] are commonly employed to solve the above problem.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "In our formulation, we estimate the baseline offline from the available dataset using method outlined in [7] by solving the following convex optimization framework.", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "Our formulation follows the Blind Compressive Sensing framework [8] [9] given in (7).", "startOffset": 64, "endOffset": 67}, {"referenceID": 8, "context": "Our formulation follows the Blind Compressive Sensing framework [8] [9] given in (7).", "startOffset": 68, "endOffset": 71}, {"referenceID": 10, "context": "2 Algorithm Design To efficiently solve our formulation and recover the complete rating matrix, we propose an algorithm following the Majorization Minimization (MM) approach [11].", "startOffset": 174, "endOffset": 178}, {"referenceID": 6, "context": "The results of our formulation is compared against the results obtained using SGD formulation proposed in [7], Accelerated Proximal Gradient based matrix completion (APG) algorithm [13], Matrix completion using Split bregman (MCSB) [14] and optSpace [15].", "startOffset": 106, "endOffset": 109}, {"referenceID": 11, "context": "The results of our formulation is compared against the results obtained using SGD formulation proposed in [7], Accelerated Proximal Gradient based matrix completion (APG) algorithm [13], Matrix completion using Split bregman (MCSB) [14] and optSpace [15].", "startOffset": 181, "endOffset": 185}, {"referenceID": 12, "context": "The results of our formulation is compared against the results obtained using SGD formulation proposed in [7], Accelerated Proximal Gradient based matrix completion (APG) algorithm [13], Matrix completion using Split bregman (MCSB) [14] and optSpace [15].", "startOffset": 232, "endOffset": 236}, {"referenceID": 13, "context": "The results of our formulation is compared against the results obtained using SGD formulation proposed in [7], Accelerated Proximal Gradient based matrix completion (APG) algorithm [13], Matrix completion using Split bregman (MCSB) [14] and optSpace [15].", "startOffset": 250, "endOffset": 254}, {"referenceID": 8, "context": "We also compare our results against the BCS algorithm (BCSJ) put forth in [9] to show the efficiency of our algorithm.", "startOffset": 74, "endOffset": 77}], "year": 2015, "abstractText": "Existing works based on latent factor models have focused on representing the rating matrix as a product of user and item latent factor matrices, both being dense. Latent (factor) vectors define the degree to which a trait is possessed by an item or the affinity of user towards that trait. A dense user matrix is a reasonable assumption as each user will like/dislike a trait to certain extent. However, any item will possess only a few of the attributes and never all. Hence, the item matrix should ideally have a sparse structure rather than a dense one as formulated in earlier works. Therefore we propose to factor the ratings matrix into a dense user matrix and a sparse item matrix which leads us to the Blind Compressed Sensing (BCS) framework. We derive an efficient algorithm for solving the BCS problem based on Majorization Minimization (MM) technique. Our proposed approach is able to achieve significantly higher accuracy and shorter run times as compared to existing approaches.", "creator": "Microsoft\u00ae Word 2013"}}}