{"id": "1505.04073", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2015", "title": "Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices", "abstract": "multi - iteration task dual feature learning ( mtfl ) which is a recently powerful technique seen in empirical boosting yields the cognitive predictive performance by learning multiple intimately related tensor classification / regression / statistical clustering tasks running simultaneously. however, solving the mtfl problem remains purely challenging models when the only feature of dimension involved is substantially extremely large. in this paper, we propose having a novel elementary screening regression rule - - - that np is merely based on the standard dual projection onto convex sets ( dpc ) - - - to quickly instantly identify presumably the stable inactive return features - - - models that have zero coefficients in the null solution vectors across all tasks. one of namely the appealing features here of dpc is discovery that : clearly it almost is quite safe modeling in the sense that the highly detected inactive features are guaranteed to have zero coefficients in dragging the overlapping solution vectors vectors together across all tasks. thus, by removing the inactive features efficiently from covering the training procedure phase, also we may have substantial savings in both the limited computational application cost and memory usage consumed without sacrificing expected accuracy. to the best of our empirical knowledge, analyzing it theoretically is actually the first screening replacement rule proof that is reasonably applicable to sparse constraint models with multiple data matrices. still a therefore key challenge in deriving dpc is to solve a nonconvex problem. we show that nowadays we can solve for virtually the global optimum representation efficiently via a properly tailored chosen parametrization required of being the constraint selection set. precisely moreover, dpc optimization has very low low computational calculation cost and can smoothly be integrated with any valid existing solvers. importantly we have evaluated the proposed explicit dpc segregation rule on both synthetic dimension and real convex data sets. combining the experiments indicate even that dpc is very relatively effective in initially identifying merely the inactive replacement features - - - especially possibly for every high ordered dimensional data - - - which leads to a speedup error up to several lower orders points of order magnitude.", "histories": [["v1", "Fri, 15 May 2015 14:31:09 GMT  (77kb,D)", "http://arxiv.org/abs/1505.04073v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jie wang", "jieping ye"], "accepted": true, "id": "1505.04073"}, "pdf": {"name": "1505.04073.pdf", "metadata": {"source": "CRF", "title": "Safe Screening for Multi-Task Feature Learning with Multiple Data Matrices", "authors": ["Jie Wang", "Jieping Ye"], "emails": [], "sections": [{"heading": null, "text": "Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule\u2014that is based on the dual projection onto convex sets (DPC)\u2014to quickly identify the inactive features\u2014that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features\u2014especially for high dimensional data\u2014which leads to a speedup up to several orders of magnitude."}, {"heading": "1 Introduction", "text": "Empirical studies have shown that learning multiple related tasks (MTL) simultaneously often provides superior predictive performance relative to learning each tasks independently (Ando and Zhang, 2005, Argyriou et al., 2008, Bakker and Heskes, 2003, Evgeniou et al., 2005, Zhang et al., 2006, Chen et al., 2013). This observation also has solid theoretical foundations (Ando and Zhang, 2005, Baxter, 2000, Ben-David and Schuller, 2003, Caruana, 1997), especially when the training sample size is small for each task. One popular MTL method especially for high-dimensional data is multi-task feature learning (MTFL), which uses the group Lasso penalty to ensure that all tasks select a common set of features (Argyriou et al., 2007). MTFL has found great success in many real-world applications including but not limited to: breast cancer classification (Zhang et al., 2010), disease progression prediction (Zhou et al., 2012), gene data analysis (Kim and Xing, 2009), and neural semantic basis discovery (Liu et al., 2009a). A major issue in MTFL\u2014that is of great practical importance\u2014is to develop efficient solvers (Liu et al., 2009b, Sra, 2012, Wang et al., 2013a). However, it remains challenging to apply the MTFL models to large-scale problems.\nar X\niv :1\n50 5.\n04 07\n3v 1\n[ cs\n.L G\n] 1\n5 M\nay 2\nThe idea of screening has been shown to be very effective in scaling the data and improving the efficiency of many popular sparse models, e.g., Lasso (El Ghaoui et al., 2012, Wang et al., 2013b, Wang et al., Xiang et al., 2011, Tibshirani et al., 2012), nonnegative Lasso Wang and Ye (2014), group Lasso (Wang et al., 2013b, Wang et al., Tibshirani et al., 2012), mixed-norm regression (Wang et al., 2013a), `1-regularized logistic regression (Wang et al., 2014b), sparse-group Lasso (Wang and Ye, 2014), support vector machine (SVM) (Ogawa et al., 2013, Wang et al., 2014a), and least absolute deviations (LAD) (Wang et al., 2014a). Essentially, screening aims to quickly identify the zero components in the solution vectors such that the corresponding features\u2014called inactive features (e.g., Lasso)\u2014or data samples\u2014called non-support vectors (e.g., SVM)\u2014can be removed from the optimization. Therefore, the size of the data matrix and the number of variables to be computed can be significantly reduced, which may lead to substantial savings in the computational cost and memory usage without sacrificing accuracy. Compared to the solvers without screening, the speedup gained by the screening methods can be several orders of magnitude.\nHowever, we note that all the existing screening methods are only applicable to sparse models with a single data matrix. Therefore, motivated by the challenges posed by large-scale data and the promising performance of existing screening methods, we propose a novel framework for developing effective and efficient screening rules for a popular MTFL model via the dual projection onto convex sets (DPC). The framework of DPC extends the state-of-the-art screening rule, called EDPP (Wang et al.), for the standard Lasso problem (Tibshirani, 1996)\u2014that assumes a single data matrix\u2014to a popular MTFL model\u2014that involves multiple data matrices across different tasks. To the best of our knowledge, DPC is the first screening rule that is applicable to sparse models with multiple data matrices.\nThe DPC screening rule detects the inactive features by maximizing a convex function over a convex set containing the dual optimal solution, which is a nonconvex problem. To find the region containing the dual optimal solution, we show that the corresponding dual problem can be formulated as a projection problem\u2014which admits many desirable geometric properties\u2014by utilizing the bilinearity of the inner product. Then, by a carefully chosen parameterization of the constraint set, we transform the nonconvex problem to a quadratic programming problem over one quadratic constraint (QP1QC) (Gay, 1981), which can be solved for the global optimum efficiently. Experiments on both synthetic and real data sets indicate that the speedup gained by DPC can be orders of magnitude. Moreover, DPC shows better performance as the feature dimension increases, which makes it a very competitive candidate for the applications of very high-dimensional data.\nWe organize the rest of this paper as follows. In Section 2, we briefly review some basics of a popular MTFL model. Then, we derive the dual problem in Section 3. Based on an indepth analysis of the geometric properties of the dual problem and the dual feasible set, we present the proposed DPC screening rule in Section 4. In Section 5, we evaluate the DPC rule on both synthetic and real data sets. We conclude this paper in Section 6. Please refer to the supplement for proofs not included in the main text.\nNotation: Denote the `2 norm by \u2016 \u00b7 \u2016. For x \u2208 Rn, let its ith component be xi, and the diagonal matrix with the entries of x on the main diagonal be diag(x). For a set of positive integers {Nt : t = 1, . . . , T, \u2211T t=1Nt = N}, we denote the tth subvector of x \u2208 RN by xt such that x = (xT1 , . . . ,x T T ) T , where xt \u2208 RNt for t = 1, . . . , T . For vectors x,y \u2208 Rn, we use \u3008x, y\u3009 and xTy interchangeably to denote the inner product. For a matrix M \u2208 Rm\u00d7n, let mi, mj , and mij be its i\nth row, jth column and (i, j)th entry, respectively. We define the (2, 1)-norm of M by \u2016M\u20162,1 = \u2211m i=1 \u2016mi\u2016. For two matrices A,B \u2208 Rm\u00d7n, we define their inner product by\n\u3008A,B\u3009 = tr(ATB). Let I be the identity matrix. For a convex function f(\u00b7), let \u2202f(\u00b7) be its subdifferential. For a vector x and a convex set C, the projection operator is:\nPC(x) := argminy\u2208C 1 2\u2016y \u2212 x\u2016."}, {"heading": "2 Basics", "text": "In this section, we briefly review some basics of a popular MTFL model and mention several equivalent formulations.\nSuppose that we have T learning tasks {(Xt,yt) : t = 1, . . . , T}, where Xt \u2208 RNt\u00d7d is the data matrix of the tth task with Nt samples and d features, and yt \u2208 RNt is the corresponding response vector. A widely used MTFL model (Argyriou et al., 2007) takes the form of\nmin W\u2208Rd\u00d7T \u2211T t=1 1 2\u2016yt \u2212Xtwt\u2016 2 + \u03bb\u2016W\u20162,1, (1)\nwhere wt \u2208 Rd is the weight vector of the tth task and W = (w1, . . . ,wT ). Because the \u2016\u00b7\u20162,1-norm induces sparsity on the rows of W , the weight vectors across all tasks share the same sparse pattern. We note that the model in (1) is equivalent to several other popular MTFL models.\nThe first example introduces a positive weight parameter \u03c1t for t = 1, . . . , T to each term in the loss function:\nmin W\u2208Rd\u00d7T \u2211T t=1 1 2\u03c1t \u2016yt \u2212Xtwt\u20162 + \u03bb\u2016W\u20162,1,\nwhich reduces to (1) by setting y\u0303t = yt\u221a \u03c1t and X\u0303t = Xt\u221a \u03c1t .\nThe second example introduces another regularizer to (1):\nmin W\u2208Rd\u00d7T \u2211T t=1 1 2\u2016yt \u2212Xtwt\u2016 2 + \u03bb\u2016W\u20162,1 + \u03c1\u2016W\u20162F ,\nwhere \u03c1 is a positive parameter and \u2016 \u00b7 \u2016F is the Frobenius norm. Let I \u2208 Rd\u00d7d be the identity matrix and 0 be the d-dimensional vector with all zero entries. By letting\nX\u0304t = (X T t , \u221a 2\u03c1tI) T , y\u0304t = (y T t ,0 T )T , t = 1, . . . , T,\nwe can also simplify the above MTFL model to (1). In this paper, we focus on developing the DPC screening rule for the MTFL model in (1)."}, {"heading": "3 The Dual Problem", "text": "In this section, we show that we can formulate the dual problem of the MTFL model in (1) as a projection problem by utilizing the bilinearity of the inner product.\nWe first introduce a new set of variables:\nzt = yt \u2212Xtwt, t = 1, . . . , T. (2)\nThen, the MTFL model in (1) can be written as\nmin W,z \u2211T t=1 1 2\u2016zt\u2016 2 + \u03bb\u2016W\u20162,1, (3)\ns.t. zt = yt \u2212Xtwt, t = 1, . . . , T.\nLet \u03bb\u03b8 \u2208 RN be the vector of Lagrangian multipliers. Then, the Lagrangian of (1) is\nL(W, z; \u03b8) = \u2211T\nt=1\n1 2\u2016zt\u2016 2 + \u03bb\u2016W\u20162,1 (4) + \u03bb \u2211T\nt=1 \u3008\u03b8t,yt \u2212Xtwt \u2212 zt\u3009.\nTo get the dual problem, we need to minimize L(W, z; \u03b8) over W and z. We can see that\n0 = \u2207z L(W, z; \u03b8)\u21d2 argminz L(W, z; \u03b8) = \u03bb\u03b8. (5)\nFor notational convenience, let f(W ) = \u03bb\u2016W\u20162,1 \u2212 \u03bb \u2211T\nt=1 \u3008\u03b8t, Xtwt\u3009.\nThus, to minimize L(W, z; \u03b8) with respect to W , it is equivalent to minimize f(W ), i.e.,\n{W\u0302 : 0 \u2208 \u2202W L(W\u0302 , z; \u03b8)} = {W\u0302 : 0 \u2208 \u2202 f(W\u0302 )}.\nBy the bilinearity of the inner product, we can decouple f(W ) into a set of independent subproblems. Indeed, we can rewrite the second term of f(W ) as\u2211T\nt=1 \u3008\u03b8t, Xtwt\u3009 = \u2211T t=1 \u3008XTt \u03b8t,wt\u3009 = \u3008M,W \u3009, (6)\nwhere M = (XT1 \u03b81, . . . , X T T \u03b8T ). Eq. (6) expresses \u3008M,W \u3009 by the sum of the inner products of the corresponding columns. By the bilinearity of the inner product, we can also express \u3008M,W \u3009 by the sum of the inner products of the corresponding rows:\u2211T\nt=1 \u3008\u03b8t, Xtwt\u3009 = \u3008M,W \u3009 = \u2211d `=1 \u3008m`,w`\u3009. (7)\nDenote the jth column of Xt by x (t) j . We can see that\nm` = (\u3008x(1)` , \u03b81\u3009, \u3008x (2) ` , \u03b82\u3009, . . . , \u3008x (T ) ` , \u03b8T \u3009). (8)\nMoreover, as \u2016W\u20162,1 = \u2211d `=1 \u2016w`\u2016, Eqs. (7) implies that:\nf(W ) = \u03bb \u2211d\n`=1 f (`)(w`),\nwhere f (`)(w`) = \u2016w`\u2016 \u2212 \u3008m`,w`\u3009. Thus, to minimize f(W ), we can minimize each f (`)(w`) separately. The subdifferential counterpart of the Fermat\u2019s rule (Bauschke and Combettes, 2011), i.e., 0 \u2208 \u2202f (`)(w\u0302`), yields:\nm` \u2208 { w\u0302`/\u2016w\u0302`\u2016, if w\u0302` 6= 0, {u \u2208 Rd : \u2016u\u2016 \u2264 1}, if w\u0302` = 0,\n(9)\nwhere w\u0302` is the minimizer of f (`)(\u00b7). We note that Eq. (9) implies \u2016m`\u2016 \u2264 1. If this is not the case, then f `(\u00b7) is not lower bounded (see the supplements for discussions), i.e., minw` f `(w`) = \u2212\u221e. Thus, by Eqs. (5) and (9), the dual function is\nq(\u03b8) = minW,z L(W, z; \u03b8) (10)\n=\n{ \u2212\u03bb22 \u2016\u03b8\u2016\n2 + \u03bb\u3008\u03b8,y\u3009, \u2016m`\u2016 \u2264 1, \u2200 ` \u2208 {1, . . . , d}, \u2212\u221e, otherwise.\nMaximizing q(\u03b8) yields the dual problem of (1) as follows:\nmax \u03b8\n1 2\u2016y\u2016 2 \u2212 \u03bb22 \u2225\u2225y \u03bb \u2212 \u03b8 \u2225\u22252 , (11) s.t. \u2211T t=1 \u3008x(t)` , \u03b8t\u3009 2 \u2264 1, ` = 1, . . . , d.\nIt is evident that the problem in (11) is equivalent to\nmin \u03b8\n1 2 \u2225\u2225y \u03bb \u2212 \u03b8 \u2225\u22252 , (12) s.t. \u2211T t=1 \u3008x(t)` , \u03b8t\u3009 2 \u2264 1, ` = 1, . . . , d.\nIn view of (12), it is indeed a projection problem. Let F be the feasible set of (12). Then, the optimal solution of (12), denoted by \u03b8\u2217(\u03bb), is the projection of y/\u03bb onto F , namely,\n\u03b8\u2217(\u03bb) = PF (y \u03bb ) . (13)"}, {"heading": "4 The DPC Rule", "text": "In this section, we present the proposed DPC screening rule for the MTFL model in (1). Inspired by the Karush-Kuhn-Tucker (KKT) conditions (Gu\u0308ler, 2010), in Section 4.1, we first present the general guidelines. The most challenging part lies in two folds: 1) we need to estimate the dual optimal solution as accurately as possible; 2) we need to solve a nonconvex optimization problem. In Section 4.2, we give an accurate estimation of the dual optimal solution based on the geometric properties of the projection operators. Then, in Section 4.3, we show that we can efficiently solve for the global optimum to the nonconvex problem. We present the DPC rule for the MTFL model (1) in Section 4.4."}, {"heading": "4.1 Guidelines for Developing DPC", "text": "We present the general guidelines to develop screening rules for the MTFL model (1) via the KKT conditions.\nLet W \u2217(\u03bb) = (w\u22171(\u03bb), . . . ,w \u2217 T (\u03bb)) be the optimal solution (1). By Eqs. (2), (5) and (9), the\nKKT conditions are:\nyt = Xtw \u2217 t (\u03bb) + \u03bb\u03b8 \u2217 t (\u03bb), t = 1, . . . , T, (14)\ng`(\u03b8 \u2217(\u03bb)) \u2208 { 1, if (w`)\u2217(\u03bb) 6= 0, [\u22121, 1], if (w`)\u2217(\u03bb) = 0, ` = 1, . . . , d. (15)\nwhere (w`)\u2217(\u03bb) is the `th row of W \u2217(\u03bb), and\ng`(\u03b8) = \u2211T\nt=1 \u3008x(t)` , \u03b8t\u3009 2, ` = 1, . . . , d. (16)\nFor ` = 1, . . . , d, Eq. (15) yields\ng`(\u03b8 \u2217(\u03bb)) < 1\u21d2 (w`)\u2217(\u03bb) = 0. (R)\nThe rule in (R) provides a method to identify the rows in W \u2217(\u03bb) that have only zero entries. However, (R) is not applicable to real applications, as it assumes knowledge of \u03b8\u2217(\u03bb), and solving the dual problem (12) could be as expensive as solving the primal problem (1). Inspired by SAFE (El Ghaoui et al., 2012), we can first estimate a set \u0398 that contains \u03b8\u2217(\u03bb), and then relax (R) as follows:\nmax\u03b8\u2208\u0398 g`(\u03b8) < 1\u21d2 (w`)\u2217(\u03bb) = 0, ` = 1, . . . , d. (R\u2217)\nTherefore, to develop a screening rule for the MTFL model in (1), (R\u2217) implies that: 1) we need to estimate a region \u0398\u2014that turns out to be a ball (please refer to Section 4.2)\u2014containing \u03b8\u2217(\u03bb); 2) we need to solve the maximization problem\u2014that turns out to be nonconvex (please refer to Section 4.3)\u2014on the left hand side of (R\u2217)."}, {"heading": "4.2 Estimation of the Dual Optimal Solution", "text": "Based on the geometric properties of the dual problem (12) that is a projection problem, we first derive the closed form solutions of the primal and dual problems for specific values of \u03bb in Section 4.2.1, and then give an accurate estimation of \u03b8\u2217(\u03bb) for the general cases in Section 4.2.2."}, {"heading": "4.2.1 Closed form solutions", "text": "The primal and dual optimal solutions W \u2217(\u03bb) and \u03b8\u2217(\u03bb) are generally unknown. However, when the value of \u03bb is sufficiently large, we expect that W \u2217(\u03bb) = 0, and \u03b8\u2217(\u03bb) = y\u03bb by Eq. (14). The following theorem confirms this.\nTheorem 1. For the MTFL model in (1), let\n\u03bbmax = max `=1,...,d \u221a\u2211T t=1 \u3008x(t)` ,y\u30092. (17)\nThen, the following statements are equivalent: y \u03bb \u2208 F \u21d4 \u03b8 \u2217(\u03bb) = y\u03bb \u21d4W \u2217(\u03bb) = 0\u21d4 \u03bb \u2265 \u03bbmax.\nRemark 1. Theorem 1 indicates that: both the primal and dual optimal solutions of the MTFL model (1) admit closed form solutions for \u03bb \u2265 \u03bbmax. Thus, we will focus on the cases with \u03bb \u2208 (0, \u03bbmax) in the rest of this paper."}, {"heading": "4.2.2 The general cases", "text": "Theorem 1 gives a closed form solution of \u03b8\u2217(\u03bb) for \u03bb \u2265 \u03bbmax. Therefore, we can estimate \u03b8\u2217(\u03bb) with \u03bb < \u03bbmax in terms of a known \u03b8\n\u2217(\u03bb0). Specifically, we can simply set \u03bb0 = \u03bbmax and utilize the result \u03b8\u2217(\u03bbmax) = y/\u03bbmax. To make this paper self-contained, we first review some geometric properties of projection operators.\nTheorem 2. (Ruszczyn\u0301ski, 2006) Let C be a nonempty closed convex set. Then, for any point u\u0304, we have\nu = PC(u)\u21d4 u\u2212 u \u2208 NC(u),\nwhere NC(u) = {v : \u3008v,u\u2032 \u2212 u\u3009 \u2264 0, \u2200u\u2032 \u2208 C} is called the normal cone to C at u \u2208 C.\nAnother useful property of the projection operator in estimating \u03b8\u2217(\u03bb) is the so-called firmly nonexpansiveness.\nTheorem 3. (Bauschke and Combettes, 2011) Let C be a nonempty closed convex subset of a Hilbert space H. The projection operator with respect to C is firmly nonexpansive, namely, for any u1,u2 \u2208 H,\n\u2016PC(u1)\u2212 PC(u2)\u20162 + \u2016(I \u2212 PC)(u1)\u2212 (I \u2212 PC)(u2)\u20162\n\u2264 \u2016u1 \u2212 u2\u20162. (18)\nThe firmly nonexpansiveness of projection operators leads to the following useful result.\nCorollary 4. Let C be a nonempty closed convex subset of a Hilbert space H and 0 \u2208 C. For any u \u2208 H, we have:\n1. \u2016PC(u)\u20162 + \u2016u\u2212 PC(u)\u20162 \u2264 \u2016u\u20162. 2. \u3008u,u\u2212 PC(u)\u3009 \u2265 0.\nRemark 2. Part 1 of Corollary 4 indicates that: if a closed convex set C contains the origin, then, for any point u, the norm of its projection with respect to C is upper bounded by the norm of \u2016u\u2016. The second part is a useful consequence of the first part and plays a crucial role in the estimation of the dual optimal solution (see Theorem 5).\nWe are now ready to present an accurate estimation of the dual optimal solution \u03b8\u2217(\u03bb).\nTheorem 5. For the MTFL model in (1), suppose that \u03b8\u2217(\u03bb0) is known with \u03bb0 \u2208 (0, \u03bbmax]. Let g` be given by Eq. (16) for ` = 1, . . . , d, and\n`\u2217 \u2208 { argmax`=1,...,d g`(y) } . (19)\nFor any \u03bb \u2208 (0, \u03bb0), we define\nn(\u03bb0) =  y \u03bb0 \u2212 \u03b8\u2217(\u03bb0), if \u03bb0 \u2208 (0, \u03bbmax), \u2207g`\u2217 ( y \u03bbmax ) , if \u03bb0 = \u03bbmax.\n(20)\nr(\u03bb, \u03bb0) = y \u03bb \u2212 \u03b8 \u2217(\u03bb0), (21)\nr\u22a5(\u03bb, \u03bb0) = r(\u03bb, \u03bb0)\u2212 \u3008n(\u03bb0), r(\u03bb, \u03bb0)\u3009 \u2016n(\u03bb0)\u20162 n(\u03bb0). (22)\nThen, the following holds: 1. n(\u03bb) \u2208 NF (\u03b8\u2217(\u03bb)), 2. \u3008y,n(\u03bb0)\u3009 \u2265 0, 3. \u3008r(\u03bb, \u03bb0),n(\u03bb0)\u3009 \u2265 0, 4. \u2225\u2225\u03b8\u2217(\u03bb)\u2212 (\u03b8\u2217(\u03bb0) + 12r\u22a5(\u03bb, \u03bb0))\u2225\u2225 \u2264 12\u2016r\u22a5(\u03bb, \u03bb0)\u2016.\nConsider Theorem 5. Part 1 characterizes \u03b8\u2217(\u03bb) via the normal cone. Parts 2 and 3 illustrate key geometric identities that lead to the accurate estimation of \u03b8\u2217(\u03bb) in part 4 (see supplement for details).\nRemark 3. The estimation of the dual optimal solution in DPC and EDPP (Wang et al.)\u2014that is for Lasso\u2014are both based on the geometric properties of the projection operators. Thus, the formulas of the estimation in Theorem 5 are similar to that of EDPP. However, we note that the estimations in DPC and EDPP are determined by the completely different geometric structures of the corresponding dual feasible sets. Problem (12) implies that the dual feasible set of the MTFL model (1) is much more complicated than that of Lasso\u2014which is a polytope (the intersection of a set of closed half spaces). Therefore, the estimation of the dual optimal solution in DPC is much more challenging than that of EDPP, e.g., we need to find a vector in the normal cone to the dual feasible set at y/\u03bbmax [see n(\u03bbmax)].\nFor notational convenience, let\no(\u03bb, \u03bb0) = \u03b8 \u2217(\u03bb0) +\n1 2 r\u22a5(\u03bb, \u03bb0). (23)\nTheorem 5 implies that \u03b8\u2217(\u03bb) lies in the ball:\n\u0398(\u03bb, \u03bb0) = { \u03b8 : \u2016\u03b8 \u2212 o(\u03bb, \u03bb0)\u2016 \u2264 1\n2 \u2016r\u22a5(\u03bb, \u03bb0)\u2016\n} . (24)"}, {"heading": "4.3 Solving the Nonconvex Problem", "text": "In this section, we solve the optimization problem in (R\u2217) with \u0398 given by \u0398(\u03bb, \u03bb0) [see Eq. (24)], namely,\ns`(\u03bb, \u03bb0) = max \u03b8\u2208\u0398(\u03bb,\u03bb0)\n{ g`(\u03b8) = \u2211T t=1 \u3008x(t)` , \u03b8t\u3009 2 } . (25)\nAlthough g`(\u00b7) and \u0398(\u03bb, \u03bb0) are convex, problem (25) is nonconvex, as it is a maximization problem. However, we can efficiently solve for the global optimal solutions to (25) by transforming it to a QP1PC via a parametrization of the constraint set. We first cite the following result.\nTheorem 6. (Gay, 1981) Let H be a symmetric matrix and D be a positive definite matrix. Consider\nmin \u2016Du\u2016\u2264\u2206\n\u03c8(u) = 1\n2 uTHu + qTu, (26)\nwhere \u2206 > 0. Then, u\u2217 minimizes \u03c8(u) over the constraint set if and only if there exists \u03b1\u2217 \u2265 0\u2014 that is unique\u2014such that (H + \u03b1\u2217DTD)u\u2217 is positive semidefinite,\n(H + \u03b1\u2217DTD)u\u2217 = \u2212q, (27) \u2016Du\u2217\u2016 = \u2206, if\u03b1\u2217 > 0. (28)\nWe are now ready to solve for s`(\u03bb, \u03bb0).\nTheorem 7. Let o = o(\u03bb, \u03bb0) and u \u2217 be the optimal solution of problem (26) with \u2206 = 12\u2016r \u22a5(\u03bb, \u03bb0)\u2016, D = I,\nH =\u2212 diag(2\u2016x\u2016(1)` , . . . , 2\u2016x\u2016 (T ) ` ), q =\u2212 (\n2\u2016x(1)` \u2016|\u3008x (1) ` ,o1\u3009|, . . . , 2\u2016x (T ) ` \u2016|\u3008x (T ) ` ,oT \u3009|\n)T ,\nnamely, there exists a \u03b1\u2217 \u2265 0 such that \u03b1\u2217 and u\u2217 solve Eqs. (27) and (28). Let\n\u03c1` = max t=1,...,T\n\u2016x(t)` \u2016, I` = { t\u2217 : \u2016x(t\u2217)` \u2016 = \u03c1` } .\nThen, the following hold: 1. \u03b1\u2217 is unique, and \u03b1\u2217 \u2265 2\u03c1`. 2. We define u\u0304 \u2208 RT by\nu\u0304t = { \u2212qt/(htt + 2\u03c1`), if t /\u2208 I`, 0, otherwise.\nThen, we have\n\u03b1\u2217 \u2208\n{ 2\u03c1`, if \u2016u\u0304\u2016 \u2264 \u2206, and \u3008x (t\u2217) ` ,ot\u2217\u3009 = 0, for t\u2217 \u2208 I`,\n(2\u03c1`,\u221e), otherwise.\n3. Let V = {v \u2208 RT : vt = 0 for t /\u2208 I`, \u2016u\u0304 + v\u2016 = \u2206}. Then, we have\nu\u2217 \u2208 { u\u0304 + v, v \u2208 V, if \u03b1\u2217 = 2\u03c1`, \u2212(H + \u03b1\u2217I)\u22121q, otherwise.\n4. The maximum value of problem (25) is given by\ns`(\u03bb, \u03bb0) = \u2211T\nt=1 \u3008x(t)` ,ot\u3009\n2 + \u03b1\u2217\n2 \u22062 \u2212 1 2 qTu\u2217.\nProof. We first transform problem (25) to a QP1PC by a parameterization of \u0398(\u03bb, \u03bb0):\n\u0398(\u03bb, \u03bb0)\n=   o1 + u1\u03b81...\noT + uT \u03b8T\n : \u2016u\u2016 \u2264 r, \u2016\u03b8t\u2016 \u2264 1, , t = 1, . . . , T  ,\nwhere u = (u1, . . . , uT ) T . We define\nh`(u, \u03b8) = g`   o1 + u1\u03b81...\noT + uT \u03b8T\n  .\nThus, problem (25) becomes\ns`(\u03bb, \u03bb0) = max \u2016u\u2016\u2264\u2206\n{ max\n{\u03b8:\u2016\u03b8t\u2016\u22641,t=1,...,T} h`(u, \u03b8)\n} .\nBy the Cauchy-Schwartz inequality, for a fixed u, we have\n\u03c6(u) = max {\u03b8:\u2016\u03b8t\u2016\u22641,t=1,...,T} h`(u, \u03b8)\n= \u2211T\nt=1 u2t \u2016x (t) ` \u2016 2 + 2|ut|\u2016x(t)` \u2016|\u3008x (t) ` ,ot\u3009|+ \u3008x (t) ` ,ot\u3009 2.\nLet \u2212\u03c8(u) = \u2211T\nt=1 u 2 t \u2016x (t) ` \u2016 2 + 2ut\u2016x(t)` \u2016|\u3008x (t) ` ,ot\u3009|. We can see that\nmax\u2016u\u2016\u2264r \u03c6(u) = max\u2016u\u2016\u2264r \u2212\u03c8(u) + \u2211T\nt=1 \u3008x(t)` ,ot\u3009 2.\nThus, problem (25) becomes\ns`(\u03bb, \u03bb0) = \u2212min\u2016u\u2016\u2264r \u03c8(u) + \u2211T\nt=1 \u3008x(t)` ,ot\u3009 2.\nTherefore, to solve (25), it suffices to solve problem (26) with \u2206, D, H, and q as in the theorem. The statement follows immediately from Theorem 6.\nRemark 4. To develop the DPC rule, (R\u2217) implies that we only need the maximum value of problem (25). Thus, Theorem 6 does not show the global optimal solutions. However, in view of the proof, we can easily compute the global optimal solutions in terms of \u03b1\u2217 and u\u2217.\nComputing \u03b1\u2217 and u\u2217 Consider Theorem 7. If \u2016u\u0304\u2016 \u2264 \u2206 and \u3008x(t\u2217)` ,ot\u2217\u3009 = 0 for t\u2217 \u2208 I`, then \u03b1\u2217 and u\u2217 admit closed form solutions. Otherwise, \u03b1\u2217 is strictly larger than 2\u03c1`, which implies that H + \u03b1\u2217I is positive definite and invertible. If this is the case, we apply Newton\u2019s method (Gay, 1981) to find \u03b1\u2217 as follows. Let\n\u03d5(\u03b1) = \u2016(H + \u03b1I)\u22121q\u2016\u22121 \u2212\u2206\u22121.\nBecause \u03d5(\u00b7) is strictly increasing on (2\u03c1`,\u221e), \u03b1\u2217 is the unique root of \u03d5(\u00b7) on (2\u03c1`,\u221e). Let \u03b10 = 2\u03c1`. Then, the k th iteration of Newton\u2019s method to solve \u03d5(\u03b1\u2217) = 0 is:\nuk =\u2212 (H + \u03b1k\u22121I)\u22121q, (29) \u03b1k =\u03b1k\u22121 + \u2016uk\u20162 \u2016uk\u2016 \u2212\u2206\n\u2206uTk (H + \u03b1k\u22121I) \u22121uk\n. (30)\nAs pointed out by More\u0301 and Sorensen (1983), Newton\u2019s method is very efficient to find \u03b1\u2217 as \u03d5(\u03b1) is almost linear on (2\u03c1`,\u221e). Our experiments indicates that five iterations usually leads to an accuracy higher than 10\u221215."}, {"heading": "4.4 The Proposed DPC Rule", "text": "As implied by R\u2217, we present the proposed screening rule, DPC, for the MTFL model (1) in the following theorem.\nTheorem 8. For the MTFL model (1), suppose that \u03b8\u2217(\u03bb0) is known with \u03bb0 \u2208 (0, \u03bbmax]. Then, we have\ns`(\u03bb, \u03bb0) < 1\u21d2 (w`)\u2217(\u03bb) = 0, \u03bb \u2208 (0, \u03bb0),\nwhere s`(\u03bb, \u03bb0) is given by Theorem 7.\nIn real applications, the optimal parameter value of \u03bb is generally unknown. Commonly used approaches to determine an appropriate value of \u03bb, such as cross validation and stability selection, need to solve the MTFL model over a grid of tuning parameter values \u03bb1 > \u03bb2 > . . . > \u03bbK, which is very time consuming. Inspired by the ideas of Strong Rule (Tibshirani et al., 2012) and SAFE (El Ghaoui et al., 2012), we develop the sequential version of DPC. Specifically, suppose that the optimal solution W \u2217(\u03bbk) is known. Then, we apply DPC to identify the inactive features of MTFL model (1) at \u03bbk+1 via W \u2217(\u03bbk). We repeat this process until all W \u2217(\u03bbk), k = 1, . . . ,K are computed.\nCorollary 9. DPC For the MTFL model (1), suppose that we are given a sequence of parameter values \u03bbmax = \u03bb0 > \u03bb1 > . . . > \u03bbK. Then, for any k = 1, 2, . . . ,K\u2212 1, if W \u2217(\u03bbk) is known, we have\ns`(\u03bbk+1, \u03bbk) < 1\u21d2 (w`)\u2217(\u03bbk+1) = 0,\nwhere s`(\u03bb, \u03bb0) is given by Theorem 7.\nWe omit the proof of Corollary 9 since it is a direct application of Theorem 8."}, {"heading": "5 Experiments", "text": "We evaluate DPC on both synthetic and real data sets. To measure the performance of DPC, we report the rejection ratio, namely, the ratio of the number of inactive features identified by DPC to the actual number of inactive features. We also report the speedup, i.e., the ratio of the running time of solver without screening to the running time of solver with DPC. The solver is from the SLEP package (Liu et al., 2009c). For each data set, we solve the MTFL model in (1) along a sequence of 100 tuning parameter values of \u03bb equally spaced on the logarithmic scale of \u03bb/\u03bbmax from 1.0 to 0.01. We only evaluate DPC since no existing screening rule is applicable for the MTFL model in (1)."}, {"heading": "5.1 Synthetic Studies", "text": "We perform experiments on two synthetic data sets, called Synthetic 1 and Synthetic 2, that are commonly used in the literature (Tibshirani et al., 2012, Zou and Hastie, 2005). Both synthetic 1 and Synthetic 2 have 50 tasks. Each task contains 50 samples. For t = 1, . . . , 50, the true model is\nyt = Xtw \u2217 t + 0.01 , \u223c N(0, 1).\nFor Synthetic 1, the entries of each data matrix Xt are i.i.d. standard Gaussian with pairwise correlation zero, i.e., corr ( x\n(t) i ,x (t) j ) = 0. For Synthetic 2, the entries of each data matrix Xt\nare drawn from i.i.d. standard Gaussian with pairwise correlation 0.5|i\u2212j|, i.e., corr ( x\n(t) i ,x (t) j\n) =\n0.5|i\u2212j|. To construct w\u2217t , we first randomly select 10% of the features. Then, the corresponding components of w\u2217t are populated from a standard Gaussian, and the remaining ones are set to 0. For both Synthetic 1 and Synthetic 2, we set the feature dimension to 10000, 20000, and 50000, respectively. For each setting, we run 20 trials and report the average performance in Fig. 1 and Table 1.\nFig. 1 shows the rejection ratios of DPC on Synthetic 1 and Synthetic 2. For all the six settings, the rejection ratios of DPC are higher than 90%, even for small parameter values. This demonstrates one of the advantages of DPC, as previous empirical studies (El Ghaoui et al., 2012, Tibshirani et al., 2012, Wang et al.) indicate that the capability of screening rules in identifying inactive features usually decreases as the parameter value decreases. Moreover, Fig. 1 also shows that as the feature dimension increases, the rejection ratios of DPC become higher\u2014that is very close to 1. This implies that the potential capability of DPC in identifying the inactive features on high-dimensional data sets would be even more significant.\nTable 1 presents the running time of the solver with and without DPC. The speedup is very significant, which is up to 60 times. Take Synthetic 1 for example. When the feature dimension is 50000, the solver without DPC takes about 40.68 hours to solve problem (1) at 100 paramater values. In contrast, combined with DPC, the solver only takes less than one hour to solve the same 100 problems\u2014which leads to a speedup about 60 times. Table 1 also shows that the computational cost of DPC is very low\u2014which is negligible compared to that of the solver without screening. Moreover, as the rejection ratios of DPC increases with feature dimension growth (see Fig. 1),\nTable 1 shows that the speedup by DPC increases as well."}, {"heading": "5.2 Experiments on Real Data Sets", "text": "We perform experiments on three real data sets: 1) the TDT2 text data set (Cai et al., 2009); 2) the animal data set (Lampert et al., 2009); 3) the Alzheimers Disease Neuroimaging Initiative (ADNI) data set (http://adni.loni.usc.edu/).\nThe Animal Data Set The data set consists of 30475 images of 50 animals classes. By following the experiment settings in Kang et al. (2011), we choose 20 animal classes in the data set: antelope, grizzly-bear, killer-whale, beaver, Dalmatian, Persiancat, horse, german- shepherd, blue-whale, Siamese-cat, skunk, ox, tiger, hippopotamus, leopard, moose, spidermonkey, humpbackwhale, elephant, and gorilla. We construct 20 tasks, where each of them is a classification task of one type of animal against all the others. For the tth task, we first randomly select 30 samples from the tth class as the positive samples; and then we randomly select 30 samples from all the other classes as the negative samples. We make use of all the seven sets of features kindly provided by Lampert et al. (2009): color histogram features, local self-similarity features, PyramidHOG (PHOG) features, SIFT features, colorSIFT features, SURF features, and DECAF features. Thus, each image is represented by a 15036-dimensional vectors. Hence, the data matrix Xt of the t\nth task is of 60\u00d7 15036, where t = 1, . . . , 20.\nThe TDT2 Data Set The original data set contains 9394 documents of 30 categories. Each document is represented by a 36771-dimensional vector. Similar to the Animal data set, we construct 30 tasks, each of which is a classification task of one category against all the others (Amit et al., 2007). Also, for the tth task, we first randomly select 50 samples from the tth category as the positive samples, and then we randomly select 50 samples from all the other categories as the negative samples. Moreover, we remove the features that have only zero entries, thus leaving us 24262 features. Hence, the data matrix Xt of the t\nth task is of 100\u00d7 24262, where t = 1, . . . , 30. The ADNI Data Set The data set consists of 747 patients with 504095 single nucleotide polymorphisms (SNPs), and the volume of 93 brain regions for each patient. We first randomly select 20 brain regions. Then, for each region, we randomly select 50 patients, and utilize the corresponding SNPs data as the data matrix and the volumes of that brain region as the response. Thus, we have 20 tasks, each of which is a regression task. The data matrix Xt of the t\nth task is of 50\u00d7 504095, where t = 1, . . . , 20.\nFig. 2 shows the rejection ratios of DPC\u2014that are above 90%\u2014on the aforementioned three real data sets. In particular, the rejection ratios of DPC on the ADNI data set are higher than 99% at the 100 parameter values. Table 1 shows that the resulting speedup is very significant\u2014that is up to 270 times. We note that the feature dimension of the ADNI data set is more than half million. Without screening, Table 1 shows that the solver takes about seven days (approximately one week) to compute the MTFL model (1) at 100 parameter values. However, integrated with the DPC screening rule, the solver computes the 100 solutions in about half an hour. The experiments again indicate that DPC provides better performance (in terms of rejection ratios and speedup) for higher dimensional data sets."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a novel screening method for the MTFL model in (1), called DPC. The DPC screening rule is based on an indepth analysis of the geometric properties of the dual problem and the dual feasible set. To the best of our knowledge, DPC is the first screening rule that is applicable to sparse models with multiple data matrices. DPC is safe in the sense that the identified features by DPC are guaranteed to have zero coefficients in the solution vectors across all tasks. Experiments on synthetic and real data sets demonstrate that DPC is very effective in identifying the inactive features, which leads to a substantial savings in computational cost and memory usage without sacrificing accuracy. Moreover, DPC is more effective as the feature dimension increases, which makes DPC a very competitive candidate for the applications of very high-dimensional data. We plan to extend DPC to more general MTFL models, e.g., the MTFL models with multiple regularizers."}, {"heading": "A Discussions regarding to the Dual Problem of (1)", "text": "Although Eq. (9) implies that \u2016m`\u2016 \u2264 1, this might not be the case. Thus, we need to consider the following two cases.\n(i) If Eq. (9) holds, we can see that \u3008m`,w`\u3009 = \u2016w`\u2016 and thus\nmin w`\nf (`)(w`) = 0. (31)\nTherefore, we have\nmin W f(W ) = 0. (32)\n(ii) If Eq. (9) does not hold, i.e., \u2016m`\u2016 > 1, we would have\ninf w`\nf (`)(w`) = \u2212\u221e, (33)\nand thus\nmin W\nf(W ) = \u2212\u221e. (34)\nTo see this, we define w(`)(t) = t m `\n\u2016m`\u2016 and thus\n\u3008m`,w(`)(t)\u3009 = t\u2016m`\u2016.\nThen, we have\nf (`)(w`(t)) = t(1\u2212 \u2016m`\u2016). (35)\nBecause \u2016m`\u2016 > 1, the above equation yields\ninf w` f (`)(w`) \u2264 lim t\u2192\u221e f (`) 2 (w `(t)) = \u2212\u221e. (36)\nThe above discussion implies that\nmin W f(W ) = { 0, if \u2016m`\u2016 \u2264 1, ` = 1, . . . , d, \u2212\u221e, otherwise.\n(37)"}, {"heading": "B Proof of Theorem 1", "text": "Proof. For notational convenience, let\n1. y\n\u03bb \u2208 F ;\n2. \u03b8\u2217(\u03bb) = y\n\u03bb ;\n3. W \u2217(\u03bb) = 0;\n4. \u03bb \u2265 \u03bbmax.\nEq. (13) implies that 1 is equivalent to 2. (2 \u21d4 3) Suppose that 2 holds. Eq. (14) implies that Xtw\u2217t (\u03bb) = 0 for t = 1, . . . , T . Denote the objective function of the MTFL model (1) by f(W ). We claim that W \u2217(\u03bb) must be zero. To see this, let W \u2217 (\u03bb) 6= 0 be another optimal solution of (1) and thus Xtw\u0304\u2217t (\u03bb) = 0 for t = 1, . . . , T . However, it is evident that f(W \u2217(\u03bb)) < f(W \u2217 (\u03bb)). This leads to a contradiction. Thus, the optimal solution W \u2217(\u03bb) is zero and we have proved 2 \u21d2 3. The converse direction, i.e., 2 \u21d0 3 is a direct consequence of Eq. (14).\n(1 \u21d4 4) It is evident that 1 holds if and only if y/\u03bb is a feasible solution of problem (12), namely, all constraints in (12) holds at y/\u03bb. By plugging y/\u03bb into the constraints in (12), we can see that the feasibility of y/\u03bb is equivalent to 4. Thus, we can see that 1 is equivalent to 4. This completes the proof."}, {"heading": "C Proof of Corollary 4", "text": "Proof. 1. To show part 1, we only need to set u1 = u and u2 = 0, and then plug them into the inequality (18) [note that PC(0) = 0 since 0 \u2208 C].\n2. Part 1 implies that \u2016PC(u)\u2016 \u2264 \u2016u\u2016. Thus, we have\n\u2016u\u20162 \u2265 \u2016u\u2016\u2016PC(u)\u2016 \u2265 \u3008u,PC(u)\u3009,\nwhich is equivalent to the statement in part 2. The proof is completed."}, {"heading": "D Proof of Theorem 5", "text": "We first cite some useful properties of the projection operators.\nLemma 10. (Ruszczyn\u0301ski, 2006, Bauschke and Combettes, 2011) Let C be a nonempty closed convex set of a Hilbert space and u \u2208 C. Then\n1. NC(u) = {v : PC(u + v) = u}.\n2. PC(u + v) = u, \u2200v \u2208 NC(u).\n3. Let u /\u2208 C and u = PC(u). Then, PC(u + t(u\u2212 u)) = u for all t \u2265 0.\nWe are now ready to prove Theorem 5\nProof.\n(i) For \u03bb \u2208 (0, \u03bbmax), Theorem 1 implies that y/\u03bb /\u2208 F . Thus, the statement holds for \u03bb \u2208 (0, \u03bbmax) by Theorem 2 and Eq. (13) [let u\u0304 = y/\u03bb and u = \u03b8\u2217(\u03bb)]. To show the statement holds at \u03bbmax, Theorem 2 indicates that we need to show\u2329\n\u2207g`\u2217 ( y\n\u03bbmax\n) , \u03b8 \u2212 y\n\u03bbmax\n\u232a \u2264 0, \u2200\u03b8 \u2208 F . (38)\nBecause g`\u2217(\u00b7) is convex, we have (Ruszczyn\u0301ski, 2006) g`\u2217(\u03b8)\u2212 g`\u2217 ( y\n\u03bbmax\n) \u2265 \u2329 \u2207g`\u2217 ( y\n\u03bbmax\n) , \u03b8 \u2212 y\n\u03bbmax\n\u232a . (39)\nNote that, g` is the constraint function of the dual problem in (12). Thus, for any dual feasible solution \u03b8 \u2208 F , it is evident that g`\u2217(\u03b8) \u2264 1. Moreover, Eq. (17) implies that g`\u2217(y/\u03bbmax) = 1. Therefore, the left hand of the inequality (39) must be non-positive, which yields inequality (38). Thus, the statement holds.\n(ii) A direct application of part 2 of Corollary 4 yields\u2329 y\n\u03bb0 ,n(\u03bb0)\n\u232a = \u2329 y\n\u03bb0 ,\ny\n\u03bb0 \u2212 \u03b8\u2217(\u03bb0)\n\u232a \u2265 0, \u2200\u03bb0 \u2208 (0, \u03bbmax).\nWhen \u03bb0 = \u03bbmax, by noting that n(\u03bbmax) = \u2207g`\u2217( y \u03bbmax ), we have\n\u2329 y\n\u03bbmax ,n(\u03bbmax)\n\u232a = T\u2211 t=1 2 \u2329 x (t) `\u2217 , y \u03bbmax \u232a2 \u2265 0.\nThus, the statement holds.\n(iii) By Eq. (21), we have \u3008r(\u03bb, \u03bb0),n(\u03bb0)\u3009 = ( 1\n\u03bb \u2212 1 \u03bb0\n) \u3008y,n(\u03bb0)\u3009+ \u2329 y\n\u03bb0 \u2212 \u03b8\u2217(\u03bb0),n(\u03bb0)\n\u232a . (40)\nBy Eqs. (20) and (13), the second term on the right hand side of Eq. (40) is nonnegative for all \u03bb0 \u2208 (0, \u03bbmax]. The fact that 0 \u2208 F yields \u2329\n0\u2212 y \u03bbmax ,n(\u03bbmax)\n\u232a \u2264 0.\nThus, the first term on the right hand side of Eq. (40) is nonnegative for \u03bb0 = \u03bbmax. For \u03bb0 \u2208 (0, \u03bbmax), part 2 of Corollary 4, Eqs. (13) and (20) imply that\u2329\ny\n\u03bb0 ,\ny\n\u03bb0 \u2212 PF\n( y\n\u03bb0\n)\u232a = \u2329 y\n\u03bb0 ,n(\u03bb0)\n\u232a \u2265 0.\nThus, the first term on the right hand side of Eq. (40) is nonnegative for \u03bb0 \u2208 (0, \u03bbmax). As a result, the inner product \u3008r(\u03bb, \u03bb0),n(\u03bb0)\u3009 is nonnegative.\n(iv) We define\n\u03b8(t) = \u03b8\u2217(\u03bb0) + tn(\u03bb0). (41)\nPart 1 of Lemma 10 implies that\nPF (\u03b8(t)) = \u03b8 \u2217(\u03bb0), \u2200 t \u2265 0. (42)\nThe nonexpansiveness of the projection operators yields [let u1 = y/\u03bb and u2 = \u03b8(t) and plug them into (18)]\u2225\u2225\u2225PF (y\n\u03bb\n) \u2212 PF (\u03b8(t)) \u2225\u2225\u22252 + \u2016(PF \u2212 Id)(y \u03bb ) \u2212 (PF \u2212 Id)(\u03b8(t))\u20162 \u2264 \u2225\u2225\u2225y \u03bb \u2212 \u03b8(t) \u2225\u2225\u22252 , \u2200 t \u2265 0. By Eqs. (13), (42) and (21), the above inequality reduces to\n\u2016\u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0)\u20162 + \u2016\u03b8\u2217(\u03bb)\u2212 \u03b8\u2217(\u03bb0)\u2212 (r(\u03bb, \u03bb0)\u2212 tn(\u03bb0))\u20162 \u2264 \u2016r(\u03bb, \u03bb0)\u2212 tn(\u03bb0)\u20162, \u2200 t \u2265 0. (43)\nLet us consider\nmin t\u22650\nr(t) = \u2016r(\u03bb, \u03bb0)\u2212 tn(\u03bb0)\u20162. (44)\nBecause r(t) is a quadratic function of t, we can see that\nmin t\u22650 r(t) = { \u2016r(\u03bb, \u03bb0)\u20162, if \u3008r(\u03bb, \u03bb0),n(\u03bb0)\u3009 < 0, \u2016r\u22a5(\u03bb, \u03bb0)\u20162, if \u3008r(\u03bb, \u03bb0),n(\u03bb0)\u3009 \u2265 0.\nBecause of part 3, we have\nmin t\u22650\nr(t) = \u2016r\u22a5(\u03bb, \u03bb0)\u20162 (45)\nargmin t\u22650 r(t) = \u3008r(\u03bb, \u03bb0),n(\u03bb0)\u3009 \u2016n(\u03bb0)\u20162\n(46)\nPlugging Eqs. (45) and (46) into (43) yields the statement, which completes the proof.\nThe proof is complete."}], "references": [{"title": "Uncovering shared structures in multiclass classification", "author": ["Y. Amit", "M. Fink", "N. Srebro", "S. Ullman"], "venue": "In Proceedings of the 24th Annual International Conference on Machine Learning,", "citeRegEx": "Amit et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amit et al\\.", "year": 2007}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R. Ando", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ando and Zhang.,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Argyriou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2007}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "Argyriou et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2008}, {"title": "Task clustering and gating for bayesian multictask learning", "author": ["B. Bakker", "T. Heskes"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bakker and Heskes.,? \\Q2003\\E", "shortCiteRegEx": "Bakker and Heskes.", "year": 2003}, {"title": "Convex Analysis and Monotone Operator Theory in Hilbert Spaces", "author": ["H.H. Bauschke", "P.L. Combettes"], "venue": null, "citeRegEx": "Bauschke and Combettes.,? \\Q2011\\E", "shortCiteRegEx": "Bauschke and Combettes.", "year": 2011}, {"title": "A model for inductive bias learning", "author": ["J. Baxter"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Baxter.,? \\Q2000\\E", "shortCiteRegEx": "Baxter.", "year": 2000}, {"title": "Exploiting task relatedness for multiple task learning", "author": ["S. Ben-David", "R. Schuller"], "venue": "In Proceedings of Computational Learning Theory,", "citeRegEx": "Ben.David and Schuller.,? \\Q2003\\E", "shortCiteRegEx": "Ben.David and Schuller.", "year": 2003}, {"title": "Probabilistic dyadic data analysis with local and global consistency", "author": ["Deng Cai", "Xuanhui Wang", "Xiaofei He"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Cai et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2009}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "Caruana.,? \\Q1997\\E", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "A convex formulation for learning shared structures from multiple tasks", "author": ["J. Chen", "L. Tang", "J. Liu", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Safe feature elimination in sparse supervised learning", "author": ["L. El Ghaoui", "V. Viallon", "T. Rabbani"], "venue": "Pacific Journal of Optimization,", "citeRegEx": "Ghaoui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ghaoui et al\\.", "year": 2012}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Evgeniou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2005}, {"title": "Computing optimal locally constrained steps", "author": ["D. Gay"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "Gay.,? \\Q1981\\E", "shortCiteRegEx": "Gay.", "year": 1981}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "In Proceedings of the 28th Annual International Conference on Machine Learning,", "citeRegEx": "Kang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2011}, {"title": "Tree-guided group lasso for multi-task regression with structured sparsity", "author": ["S. Kim", "E. Xing"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Kim and Xing.,? \\Q2009\\E", "shortCiteRegEx": "Kim and Xing.", "year": 2009}, {"title": "Learning to detect unseen object classes by betweenclass atttribute transfer", "author": ["C. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Lampert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lampert et al\\.", "year": 2009}, {"title": "Bsparsity coordinate descent procedures for the multi-task with applications to neural semantic basis discovery", "author": ["H. Liu", "M. Palatucci", "J. Zhang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Multi-task feature learning with efficient `2,1-norm minimization", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "In The 25th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "SLEP: Sparse Learning with Efficient Projections", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Computing a trust region step", "author": ["J. Mor\u00e9", "D. Sorensen"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "Mor\u00e9 and Sorensen.,? \\Q1983\\E", "shortCiteRegEx": "Mor\u00e9 and Sorensen.", "year": 1983}, {"title": "Safe screening of non-support vectors in pathwise SVM computation", "author": ["K. Ogawa", "Y. Suzuki", "I. Takeuchi"], "venue": "In Proceedings of the 30th Annual International Conference on Machine Learning,", "citeRegEx": "Ogawa et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ogawa et al\\.", "year": 2013}, {"title": "Nonlinear Optimization", "author": ["A. Ruszczy\u0144ski"], "venue": null, "citeRegEx": "Ruszczy\u0144ski.,? \\Q2006\\E", "shortCiteRegEx": "Ruszczy\u0144ski.", "year": 2006}, {"title": "Fast projections onto mixed-norm balls with applications", "author": ["S. Sra"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Sra.,? \\Q2012\\E", "shortCiteRegEx": "Sra.", "year": 2012}, {"title": "Regression shringkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Strong rules for discarding predictors in lasso-type problems", "author": ["R. Tibshirani", "J. Bien", "J. Friedman", "T. Hastie", "N. Simon", "J. Taylor"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Tibshirani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tibshirani et al\\.", "year": 2012}, {"title": "Two-layer feature reduction for sparse-group lasso via decomposition of convex sets", "author": ["J. Wang", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang and Ye.,? \\Q2014\\E", "shortCiteRegEx": "Wang and Ye.", "year": 2014}, {"title": "Efficient mixed-norm regularization: Algorithms and safe screening methods", "author": ["J. Wang", "J. Liu", "J. Ye"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Lasso screening rules via dual polytope projection", "author": ["J. Wang", "J. Zhou", "P. Wonka", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Scaling SVM and least absolute deviations via exact data reduction", "author": ["J. Wang", "P. Wonka", "J. Ye"], "venue": "In Proceedings of the 31th Annual International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "A safe screening rule for sparse logistic regression", "author": ["J. Wang", "J. Zhou", "J. Liu", "P. Wonka", "J. Ye"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Learning sparse representation of high dimensional data on large scale dictionaries", "author": ["Z.J. Xiang", "H. Xu", "P.J. Ramadge"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Xiang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xiang et al\\.", "year": 2011}, {"title": "Learning multiple related tasks using latent independent component analysis", "author": ["J. Zhang", "Z. Ghahramani", "Y. Yang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2006}, {"title": "Probabilistic multi-task feature selection", "author": ["Y. Zhang", "D. Yeung", "Q. Xu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}, {"title": "Modeling disease progression via fused sparse group lasso", "author": ["J. Zhou", "J. Liu", "V. Narayan", "J. Ye"], "venue": "In International Conference On Knowledge Discovery and Data Mining,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Zou and Hastie.,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie.", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "One popular MTL method especially for high-dimensional data is multi-task feature learning (MTFL), which uses the group Lasso penalty to ensure that all tasks select a common set of features (Argyriou et al., 2007).", "startOffset": 191, "endOffset": 214}, {"referenceID": 33, "context": "MTFL has found great success in many real-world applications including but not limited to: breast cancer classification (Zhang et al., 2010), disease progression prediction (Zhou et al.", "startOffset": 120, "endOffset": 140}, {"referenceID": 34, "context": ", 2010), disease progression prediction (Zhou et al., 2012), gene data analysis (Kim and Xing, 2009), and neural semantic basis discovery (Liu et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 15, "context": ", 2012), gene data analysis (Kim and Xing, 2009), and neural semantic basis discovery (Liu et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 26, "context": ", 2014b), sparse-group Lasso (Wang and Ye, 2014), support vector machine (SVM) (Ogawa et al.", "startOffset": 29, "endOffset": 48}, {"referenceID": 24, "context": "), for the standard Lasso problem (Tibshirani, 1996)\u2014that assumes a single data matrix\u2014to a popular MTFL model\u2014that involves multiple data matrices across different tasks.", "startOffset": 34, "endOffset": 52}, {"referenceID": 13, "context": "Then, by a carefully chosen parameterization of the constraint set, we transform the nonconvex problem to a quadratic programming problem over one quadratic constraint (QP1QC) (Gay, 1981), which can be solved for the global optimum efficiently.", "startOffset": 176, "endOffset": 187}, {"referenceID": 11, "context": ", Lasso (El Ghaoui et al., 2012, Wang et al., 2013b, Wang et al., Xiang et al., 2011, Tibshirani et al., 2012), nonnegative Lasso Wang and Ye (2014), group Lasso (Wang et al.", "startOffset": 12, "endOffset": 149}, {"referenceID": 2, "context": "A widely used MTFL model (Argyriou et al., 2007) takes the form of", "startOffset": 25, "endOffset": 48}, {"referenceID": 5, "context": "The subdifferential counterpart of the Fermat\u2019s rule (Bauschke and Combettes, 2011), i.", "startOffset": 53, "endOffset": 83}, {"referenceID": 22, "context": "(Ruszczy\u0144ski, 2006) Let C be a nonempty closed convex set.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "(Bauschke and Combettes, 2011) Let C be a nonempty closed convex subset of a Hilbert space H.", "startOffset": 0, "endOffset": 30}, {"referenceID": 13, "context": "(Gay, 1981) Let H be a symmetric matrix and D be a positive definite matrix.", "startOffset": 0, "endOffset": 11}, {"referenceID": 13, "context": "If this is the case, we apply Newton\u2019s method (Gay, 1981) to find \u03b1\u2217 as follows.", "startOffset": 46, "endOffset": 57}, {"referenceID": 20, "context": "As pointed out by Mor\u00e9 and Sorensen (1983), Newton\u2019s method is very efficient to find \u03b1\u2217 as \u03c6(\u03b1) is almost linear on (2\u03c1`,\u221e).", "startOffset": 18, "endOffset": 43}, {"referenceID": 25, "context": "Inspired by the ideas of Strong Rule (Tibshirani et al., 2012) and SAFE (El Ghaoui et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 8, "context": "2 Experiments on Real Data Sets We perform experiments on three real data sets: 1) the TDT2 text data set (Cai et al., 2009); 2) the animal data set (Lampert et al.", "startOffset": 106, "endOffset": 124}, {"referenceID": 16, "context": ", 2009); 2) the animal data set (Lampert et al., 2009); 3) the Alzheimers Disease Neuroimaging Initiative (ADNI) data set (http://adni.", "startOffset": 32, "endOffset": 54}, {"referenceID": 0, "context": "Similar to the Animal data set, we construct 30 tasks, each of which is a classification task of one category against all the others (Amit et al., 2007).", "startOffset": 133, "endOffset": 152}, {"referenceID": 13, "context": "By following the experiment settings in Kang et al. (2011), we choose 20 animal classes in the data set: antelope, grizzly-bear, killer-whale, beaver, Dalmatian, Persiancat, horse, german- shepherd, blue-whale, Siamese-cat, skunk, ox, tiger, hippopotamus, leopard, moose, spidermonkey, humpbackwhale, elephant, and gorilla.", "startOffset": 40, "endOffset": 59}, {"referenceID": 13, "context": "By following the experiment settings in Kang et al. (2011), we choose 20 animal classes in the data set: antelope, grizzly-bear, killer-whale, beaver, Dalmatian, Persiancat, horse, german- shepherd, blue-whale, Siamese-cat, skunk, ox, tiger, hippopotamus, leopard, moose, spidermonkey, humpbackwhale, elephant, and gorilla. We construct 20 tasks, where each of them is a classification task of one type of animal against all the others. For the tth task, we first randomly select 30 samples from the tth class as the positive samples; and then we randomly select 30 samples from all the other classes as the negative samples. We make use of all the seven sets of features kindly provided by Lampert et al. (2009): color histogram features, local self-similarity features, PyramidHOG (PHOG) features, SIFT features, colorSIFT features, SURF features, and DECAF features.", "startOffset": 40, "endOffset": 713}, {"referenceID": 22, "context": "Because g`\u2217(\u00b7) is convex, we have (Ruszczy\u0144ski, 2006) g`\u2217(\u03b8)\u2212 g`\u2217 ( y \u03bbmax ) \u2265 \u3008 \u2207g`\u2217 ( y \u03bbmax ) , \u03b8 \u2212 y \u03bbmax \u3009 .", "startOffset": 34, "endOffset": 53}], "year": 2015, "abstractText": "Multi-task feature learning (MTFL) is a powerful technique in boosting the predictive performance by learning multiple related classification/regression/clustering tasks simultaneously. However, solving the MTFL problem remains challenging when the feature dimension is extremely large. In this paper, we propose a novel screening rule\u2014that is based on the dual projection onto convex sets (DPC)\u2014to quickly identify the inactive features\u2014that have zero coefficients in the solution vectors across all tasks. One of the appealing features of DPC is that: it is safe in the sense that the detected inactive features are guaranteed to have zero coefficients in the solution vectors across all tasks. Thus, by removing the inactive features from the training phase, we may have substantial savings in the computational cost and memory usage without sacrificing accuracy. To the best of our knowledge, it is the first screening rule that is applicable to sparse models with multiple data matrices. A key challenge in deriving DPC is to solve a nonconvex problem. We show that we can solve for the global optimum efficiently via a properly chosen parametrization of the constraint set. Moreover, DPC has very low computational cost and can be integrated with any existing solvers. We have evaluated the proposed DPC rule on both synthetic and real data sets. The experiments indicate that DPC is very effective in identifying the inactive features\u2014especially for high dimensional data\u2014which leads to a speedup up to several orders of magnitude.", "creator": "LaTeX with hyperref package"}}}