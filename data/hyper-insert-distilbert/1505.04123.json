{"id": "1505.04123", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2015", "title": "Margins, Kernels and Non-linear Smoothed Perceptrons", "abstract": "we always focus on the problem techniques of finding consistently a non - infinite linear classification function that once lies in a reproducing numerical kernel hilbert space ( rkhs ) both come from the primal point of supreme view ( finding himself a coherent perfect separator when false one exists ) onwards and ignoring the primitive dual point of partial view ( rarely giving half a certificate resolution of true non - contradiction existence ), those with extreme special focus on generalizations of almost two classical computing schemes - the perceptron ( virtually primal ) and optimal von - neumann ( complete dual ) algorithms.", "histories": [["v1", "Fri, 15 May 2015 16:54:58 GMT  (21kb)", "http://arxiv.org/abs/1505.04123v1", "17 pages, published in the proceedings of the International Conference on Machine Learning, 2014"]], "COMMENTS": "17 pages, published in the proceedings of the International Conference on Machine Learning, 2014", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NA math.OC", "authors": ["aaditya ramdas", "javier pe\u00f1a"], "accepted": true, "id": "1505.04123"}, "pdf": {"name": "1505.04123.pdf", "metadata": {"source": "CRF", "title": "Margins, Kernels and Non-linear Smoothed Perceptrons", "authors": ["Aaditya Ramdas"], "emails": ["aramdas@cs.cmu.edu", "jfp@andrew.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 5.\n04 12\n3v 1\n[ cs\n.L G\n] 1\n5 M\nWe cast our problem as one of maximizing the regularized normalized hard-margin (\u03c1) in an RKHS and rephrase it in terms of a Mahalanobis dot-product/semi-norm associated with the kernel\u2019s (normalized and signed) Gram matrix. We derive an accelerated smoothed algorithm with a convergence rate of \u221a log n\n\u03c1 given\nn separable points, which is strikingly similar to the classical kernelized Perceptron algorithm whose rate is 1\n\u03c12 . When no such classifier exists, we prove a version of Gordan\u2019s separation theorem for RKHSs, and give a reinterpretation of negative margins. This allows us to give guarantees for a primal-dual algorithm that halts in min{ \u221a n |\u03c1| , \u221a n \u01eb } iterations with a perfect separator in the RKHS if the primal is feasible or a dual \u01eb-certificate of near-infeasibility."}, {"heading": "1 Introduction", "text": "We are interested in the problem of finding a non-linear separator for a given set of n points x1, ..., xn \u2208 Rd with labels y1, ..., yn \u2208 {\u00b11}. Finding a linear separator can be stated as the problem of finding a unit vector w \u2208 Rd (if one exists) such that for all i\nyi(w \u22a4xi) \u2265 0 i.e. sign(w\u22a4xi) = yi. (1)\nThis is called the primal problem. In the more interesting non-linear setting, we will be searching for functions f in a Reproducing Kernel Hilbert Space (RKHS) FK associated with kernel K (to be defined later) such that for all i yif(xi) \u2265 0. (2) We say that problems (1), (2) have an unnormalized margin \u03c1 > 0, if there exists a unit vector w, such that for all i,\nyi(w \u22a4xi) \u2265 \u03c1 or yif(xi) \u2265 \u03c1.\nTrue to the paper\u2019s title, margins of non-linear separators in an RKHS will be a central concept, and we will derive interesting smoothed accelerated variants of the Perceptron algorithm that have convergence rates (for the aforementioned primal and a dual problem introduced later) that are inversely proportional to the RKHS-margin as opposed to inverse squared margin for the Perceptron.\nThe linear setting is well known by the name of linear feasibility problems - we are asking if there exists any vector w which makes an acute angle with all the vectors yixi, i.e.\n(XY )\u22a4w > 0n, (3)\nwhere Y := diag(y), X := [x1, ..., xn]. This can be seen as finding a vector w inside the dual cone of cone{yixi}.\nWhen normalized, as we will see in the next section, the margin is a well-studied notion of conditioning for these problems. It can be thought of as the width of the feasibility cone as in Freund & Vera (1999), a radius of well-posedness as in Cheung & Cucker (2001), and its inverse can be seen as a special case of a condition number defined by Renegar (1995) for these systems."}, {"heading": "1.1 Related Work", "text": "In this paper we focus on the famous Perceptron algorithm from Rosenblatt (1958) and the less-famous VonNeumann algorithm from Dantzig (1992) that we introduce in later sections. As mentioned by Epelman & Freund (2000), in a technical report by the same name, Nesterov pointed out in a note to the authors that the latter is a special case of the now-popular Frank-Wolfe algorithm.\nOur work builds on Soheili & Pen\u0303a (2012, 2013b) from the field of optimization - we generalize the setting to learning functions in RKHSs, extend the algorithms, simplify proofs, and simultaneously bring new perspectives to it. There is extensive literature around the Perceptron algorithm in the learning community; we restrict ourselves to discussing only a few directly related papers, in order to point out the several differences from existing work.\nWe provide a general unified proof in the Appendix which borrows ideas from accelerated smoothing methods developed by Nesterov (2005) - while this algorithm and others by Nemirovski (2004), Saha et al. (2011) can achieve similar rates for the same problem, those algorithms do not possess the simplicity of the Perceptron or Von-Neumann algorithms and our variants, and also don\u2019t look at the infeasible setting or primal-dual algorithms.\nAccelerated smoothing techniques have also been seen in the learning literature like in Tseng (2008) and many others. However, most of these deal with convex-concave problems where both sets involved are the probability simplex (as in game theory, boosting, etc), while we deal with hard margins where one of the sets is a unit \u21132 ball. Hence, their algorithms/results are not extendable to ours trivially. This work is also connected to the idea of \u01eb-coresets by Clarkson (2010), though we will not explore that angle.\nA related algorithm is called the Winnow by Littlestone (1991) - this works on the \u21131 margin and is a saddle point problem over two simplices. One can ask whether such accelerated smoothed versions exist for the Winnow. The answer is in the affirmative - however such algorithms look completely different from the Winnow, while in our setting the new algorithms retain the simplicity of the Perceptron."}, {"heading": "1.2 Paper Outline", "text": "Sec.2 will introduce the Perceptron and Normalized Perceptron algorithm and their convergence guarantees for linear separability, with specific emphasis on the unnormalized and normalized margins. Sec.3 will then introduce RKHSs and the Normalized Kernel Perceptron algorithm, which we interpret as a subgradient algorithm for a regularized normalized hard-margin loss function.\nSec.4 describes the Smoothed Normalized Kernel Perceptron algorithm that works with a smooth approximation to the original loss function, and outlines the argument for its faster convergence rate. Sec.5 discusses the non-separable case and the Von-Neumann algorithm, and we prove a version of Gordan\u2019s theorem in RKHSs.\nWe finally give an algorithm in Sec.6 which terminates with a separator if one exists, and with a dual certificate of near-infeasibility otherwise, in time inversely proportional to the margin. Sec.7 has a discussion and some open problems."}, {"heading": "2 Linear Feasibility Problems", "text": ""}, {"heading": "2.1 Perceptron", "text": "The classical perceptron algorithm can be stated in many ways, one is in the following form\nAlgorithm 1 Perceptron Initialize w0 = 0 for k = 0, 1, 2, 3, ... do\nif sign(w\u22a4k xi) 6= yi for some i then wk+1 := wk + yixi else Halt: Return wk as solution\nend if end for\nIt comes with the following classic guarantee as proved by Block (1962) and Novikoff (1962): If there exists a unit vector u \u2208 Rd such that Y X\u22a4u \u2265 \u03c1 > 0, then a perfect separator will be found in maxi \u2016xi\u2016 2 2\n\u03c12\niterations/mistakes. The algorithm works when updated with any arbitrary point (xi, yi) that is misclassified; it has the same guarantees when w is updated with the point that is misclassified by the largest amount, argmini yiw\u22a4xi. Alternately, one can define the probability distribution over examples\np(w) = arg min p\u2208\u2206n\n\u3008Y X\u22a4w, p\u3009, (4)\nwhere \u2206n is the n-dimensional probability simplex. Intuitively, p picks the examples that have the lowest margin when classified byw. One can also normalize the updates so that we can maintain a probability distribution over examples used for updates from the start, as seen below:\nAlgorithm 2 Normalized Perceptron Initialize w0 = 0, p0 = 0 for k = 0, 1, 2, 3, ... do\nif Y X\u22a4wk > 0 then Exit, with wk as solution else \u03b8k := 1 k+1\nwk+1 := (1\u2212 \u03b8k)wk + \u03b8kXY p(wk) end if\nend for\nRemark. Normalized Perceptron has the same guarantees as perceptron - the Perceptron can perform its update online on any misclassified point, while the Normalized Perceptron performs updates on the most misclassified point(s), and yet there does not seem to be any change in performance. However, we will soon see that the ability to see all the examples at once gives us much more power."}, {"heading": "2.2 Normalized Margins", "text": "If we normalize the data points by the \u21132 norm, the resulting mistake bound of the perceptron algorithm is slightly different. Let X2 represent the matrix with columns xi/\u2016xi\u20162. Define the unnormalized and\nnormalized margins as\n\u03c1 := sup \u2016w\u20162=1 inf p\u2208\u2206n\n\u3008Y X\u22a4w, p\u3009,\n\u03c12 := sup \u2016w\u20162=1 inf p\u2208\u2206n\n\u3008Y X\u22a42 w, p\u3009.\nRemark. Note that we have sup\u2016w\u20162=1 in the definition, this is equivalent to sup\u2016w\u20162\u22641 iff \u03c12 > 0. Normalized Perceptron has the following guarantee on X2: If \u03c12 > 0, then it finds a perfect separator in\n1 \u03c122 iterations.\nRemark. Consider the max-margin separator u\u2217 for X (which is also a valid perfect separator for X2). Then\n\u03c1 maxi \u2016xi\u20162 = mini\n( yix \u22a4 i u \u2217\nmaxi \u2016xi\u20162\n)\n\u2264 min i\n( yix \u22a4 i u \u2217\n\u2016xi\u20162\n)\n\u2264 sup \u2016u\u20162=1 min i\n( yix \u22a4 i u\n\u2016xi\u20162\n)\n= \u03c12.\nHence, it is always better to normalize the data as pointed out in Graepel et al. (2001). This idea extends to RKHSs, motivating the normalized Gram matrix considered later.\nExample Consider a simple example in R2+. Assume that + points are located along the line 6x2 = 8x1, and the \u2212 points along 8x2 = 6x1, for 1/r \u2264 \u2016x\u20162 \u2264 r, where r > 1. The max-margin linear separator will be x1 = x2. If all the data were normalized to have unit Euclidean norm, then all the + points would all be at (0.6, 0.8) and all the \u2212 points at (0.8, 0.6), giving us a normalized margin of \u03c12 \u2248 0.14. Unnormalized, the margin is \u03c1 \u2248 0.14/r and maxi \u2016xi\u20162 = r. Hence, in terms of bounds, we get a discrepancy of r4, which can be arbitrarily large.\nWinnow The question arises as to which norm we should normalize by. There is a now classic algorithm in machine learning, called Winnow by Littlestone (1991) or Multiplicate Weights. It works on a slight transformation of the problem where we only need to search for u \u2208 Rd+. It comes with some very wellknown guarantees - If there exists a u \u2208 Rd+ such that Y X\u22a4u \u2265 \u03c1 > 0, then feasibility is guaranteed in \u2016u\u201621maxi \u2016ai\u20162\u221e logn/\u03c12 iterations. The appropriate notion of normalized margin here is\n\u03c11 := max w\u2208\u2206d min p\u2208\u2206n\n\u3008Y X\u22a4\u221ew, p\u3009,\nwhere X\u221e is a matrix with columns xi/\u2016xi\u2016\u221e. Then, the appropriate iteration bound is logn/\u03c121. We will return to this \u21131-margin in the discussion section. In the next section, we will normalize by using the kernel appropriately."}, {"heading": "3 Kernels and RKHSs", "text": "The theory of Reproducing Kernel Hilbert Spaces (RKHSs) has a rich history, and for a detailed introduction, refer to Scho\u0308lkopf & Smola (2002). Let K : Rd \u00d7 Rd \u2192 R be a symmetric positive definite kernel, giving rise to a Reproducing Kernel Hilbert Space FK with an associated feature mapping at each point x \u2208 Rd called \u03c6x : Rd \u2192 FK where \u03c6x(.) = K(x, .) i.e. \u03c6x(y) = K(x, y). FK has an associated inner product \u3008\u03c6u, \u03c6v\u3009K = K(u, v). For any f \u2208 FK , we have f(x) = \u3008f, \u03c6x\u3009K .\nDefine the normalized feature map\n\u03c6\u0303x = \u03c6x \u221a\nK(x, x) \u2208 FK and \u03c6\u0303X := [\u03c6\u0303xi ]n1 .\nFor any function f \u2208 FK , we use the following notation\nY f\u0303(X) := \u3008f, Y \u03c6\u0303X\u3009K = [yi\u3008f, \u03c6\u0303xi\u3009K ]n1 = [\nyif(xi)\u221a K(xi,xi)\n]n\n1 .\nWe analogously define the normalized margin here to be\n\u03c1K := sup \u2016f\u2016K=1 inf p\u2208\u2206n\n\u2329 Y f\u0303(X), p \u232a . (5)\nConsider the following regularized empirical loss function\nL(f) =\n{\nsup p\u2208\u2206n\n\u2329 \u2212Y f\u0303(X), p \u232a\n}\n+ 12\u2016f\u2016 2 K. (6)\nDenoting t := \u2016f\u2016K > 0 and writing f = t (\nf \u2016f\u2016K\n)\n= tf\u0304 , let us calculate the minimum value of this\nfunction\ninf f\u2208FK L(f) = inf t>0 inf \u2016f\u0304\u2016K=1 sup p\u2208\u2206n\n\u3008\u2212\u3008tf\u0304 , Y \u03c6\u0303X\u3009K , p\u3009+ t 2\n2\n= inf t>0\n{ \u2212t\u03c1K + 12 t 2 }\n= \u2212 12\u03c1 2 K when t = \u03c1K > 0. (7)\nSince maxp\u2208\u2206n \u2329 \u2212Y f\u0303(X), p \u232a\nis some empirical loss function on the data and 12\u2016f\u20162K is an increasing function of \u2016f\u2016K , the Representer Theorem (Scho\u0308lkopf et al. 2001) implies that the minimizer of the above function lies in the span of \u03c6xis (also the span of the yi\u03c6\u0303xis). Explicitly,\narg min f\u2208FK L(f) =\nn \u2211\ni=1\n\u03b1iyi\u03c6\u0303xi = \u3008Y \u03c6\u0303X , \u03b1\u3009. (8)\nSubstituting this back into Eq.(6), we can define\nL(\u03b1) :=\n{\nsup p\u2208\u2206n\n\u3008\u2212\u03b1, p\u3009 G\n}\n+ 12\u2016\u03b1\u2016 2 G, (9)\nwhere G is a normalized signed Gram matrix with Gii = 1,\nGji = Gij := yiyjK(xi,xj)\u221a K(xi,xi)K(xj ,xj) = \u3008yi\u03c6\u0303xi , yj \u03c6\u0303xj \u3009K ,\nand \u3008p, \u03b1\u3009 G := p\u22a4G\u03b1, \u2016\u03b1\u2016G :=\n\u221a \u03b1\u22a4G\u03b1. One can verify that G is a PSD matrix and the G-norm \u2016.\u2016G is a\nsemi-norm, whose properties are of great importance to us."}, {"heading": "3.1 Some Interesting and Useful Lemmas", "text": "The first lemma justifies our algorithms\u2019 exit condition.\nLemma 1. L(\u03b1) < 0 implies G\u03b1 > 0 and there exists a perfect classifier iff G\u03b1 > 0. Proof. L(\u03b1) < 0 \u21d2 supp\u2208\u2206n \u3008\u2212G\u03b1, p\u3009 < 0 \u21d4 G\u03b1 > 0. G\u03b1 > 0 \u21d2 f\u03b1 := \u3008\u03b1, Y \u03c6\u0303X\u3009 is perfect since\nyjf\u03b1(xj) \u221a\nK(xj , xj) =\nn \u2211\ni=1\n\u03b1i yiyjK(xi, xj) \u221a\nK(xi, xi)K(xj , xj)\n= Gj\u03b1 > 0.\nIf a perfect classifier exists, then \u03c1K > 0 by definition and\nL(f\u2217) = L(\u03b1\u2217) = \u2212 12\u03c1 2 K < 0 \u21d2 G\u03b1 > 0,\nwhere f\u2217, \u03b1\u2217 are the optimizers of L(f), L(\u03b1).\nThe second lemma bounds the G-norm of vectors.\nLemma 2. For any \u03b1 \u2208 Rn, \u2016\u03b1\u2016G \u2264 \u2016\u03b1\u20161 \u2264 \u221a n\u2016\u03b1\u20162. Proof. Using the triangle inequality of norms, we get\n\u221a \u03b1\u22a4G\u03b1 = \u221a \u2329 \u3008\u03b1, Y \u03c6\u0303X\u3009, \u3008\u03b1, Y \u03c6\u0303X\u3009 \u232a\nK\n= \u2016 \u2211\ni\n\u03b1iyi\u03c6\u0303xi\u2016K \u2264 \u2211\ni\n\u2016\u03b1iyi\u03c6\u0303xi\u2016K\n\u2264 \u2211\ni\n|\u03b1i| \u2225 \u2225 \u2225 \u2225\n\u2225\nyi \u03c6xi \u221a\nK(xi, xi)\n\u2225 \u2225 \u2225 \u2225 \u2225\nK\n= \u2211\ni\n|\u03b1i|,\nwhere we used \u3008\u03c6xi , \u03c6xi\u3009K = K(xi, xi).\nThe third lemma gives a new perspective on the margin.\nLemma 3. When \u03c1K > 0, f maximizes the margin iff \u03c1Kf optimizes L(f). Hence, the margin is equivalently\n\u03c1K = sup \u2016\u03b1\u2016G=1 inf p\u2208\u2206n\n\u3008\u03b1, p\u3009 G \u2264 \u2016p\u2016G for all p \u2208 \u2206n.\nProof. Let f\u03c1 be any function with \u2016f\u03c1\u2016K = 1 that achieves the max-margin \u03c1K > 0. Then, it is easy to plug \u03c1Kf\u03c1 into Eq. (6) and verify that L(\u03c1Kf\u03c1) = \u2212 12\u03c12K and hence \u03c1Kf\u03c1 minimizes L(f).\nSimilarly, let fL be any function that minimizes L(f), i.e. achieves the value L(fL) = \u2212 12\u03c12K . Defining t := \u2016fL\u2016K , and examining Eq. (7), we see that L(fL) cannot achieve the value \u2212 12\u03c12K unless t = \u03c1K and supp\u2208\u2206n \u2329 \u2212Y f\u0303L(X), p \u232a\n= \u2212\u03c12K which means that fL/\u03c1K must achieve the max-margin. Hence considering only f = \u2211\ni \u03b1iyi\u03c6\u0303xi is acceptable for both. Plugging this into Eq. (5) gives the equality and\n\u03c1K = inf p\u2208\u2206n sup \u2016\u03b1\u2016G=1\n\u3008\u03b1, p\u3009 G \u2264 sup \u2016\u03b1\u2016G=1 \u3008\u03b1, p\u3009 G\n\u2264 \u2016p\u2016G by applying Cauchy-Schwartz\n(can also be seen by going back to function space)."}, {"heading": "4 Smoothed Normalized Kernel Perceptron", "text": "Define the distribution over the worst-classified points\np(f) := arg min p\u2208\u2206n\n\u2329 Y f\u0303(X), p \u232a\nor p(\u03b1) := arg min p\u2208\u2206n \u3008\u03b1, p\u3009 G . (10)\nImplicitly fk+1 = (1\u2212 \u03b8k)fk + \u03b8k\u3008Y \u03c6\u0303X , p(fk)\u3009 = fk \u2212 \u03b8k ( fk \u2212 \u3008Y \u03c6\u0303X , p(fk)\u3009 )\n= fk \u2212 \u03b8k\u2202L(fk)\nand hence the Normalized Kernel Perceptron (NKP) is a subgradient algorithm to minimize L(f) from Eq. (6).\nRemark. Lemma 3 yields deep insights. Since NKP can get arbitrarily close to the minimizer of strongly convex L(f), it also gets arbitrarily close to a margin maximizer. It is known that it finds a perfect classifier\nAlgorithm 3 Normalized Kernel Perceptron (NKP) Set \u03b10 := 0 for k = 0, 1, 2, 3, ... do\nif G\u03b1k > 0n then Exit, with \u03b1k as solution else \u03b8k := 1 k+1\n\u03b1k+1 := (1\u2212 \u03b8k)\u03b1k + \u03b8kp(\u03b1k) end if\nend for\nin 1/\u03c12K iterations - we now additionally infer that it will continue to improve to find an approximate maxmargin classifier. While both classical and normalized Perceptrons find perfect classifiers in the same time, the latter is guaranteed to improve.\nRemark. \u03b1k+1 is always a probability distribution. Curiously, a guarantee that the solution will lie in \u2206n is not made by the Representer Theorem in Eq. (8) - any \u03b1 \u2208 Rn could satisfy Lemma 1. However, since NKP is a subgradient method for minimizing Eq. (6), we know that we will approach the optimum while only choosing \u03b1 \u2208 \u2206n.\nDefine the smooth minimizer analogous to Eq. (10) as\np\u00b5(\u03b1) := arg min p\u2208\u2206n\n{\n\u3008\u03b1, p\u3009 G + \u00b5d(p)\n}\n(11)\n= e\u2212G\u03b1/\u00b5\n\u2016e\u2212G\u03b1/\u00b5\u20161 ,\nwhere d(p) := \u2211\ni\npi log pi + log n (12)\nis 1-strongly convex with respect to the \u21131-norm (Nesterov 2005). Define a smoothened loss function as in\nAlgorithm 4 Smoothed Normalized Kernel Perceptron Set \u03b10 = 1n/n, \u00b50 := 2, p0 := p\u00b50(\u03b10) for k = 0, 1, 2, 3, ... do\nif G\u03b1k > 0n then Halt: \u03b1k is solution to Eq. (8) else \u03b8k := 2 k+3\n\u03b1k+1 := (1\u2212 \u03b8k)(\u03b1k + \u03b8kpk) + \u03b82kp\u00b5k(\u03b1k) \u00b5k+1 = (1\u2212 \u03b8k)\u00b5k pk+1 := (1\u2212 \u03b8k)pk + \u03b8kp\u00b5k+1(\u03b1k+1)\nend if end for\nEq. (9)\nL\u00b5(\u03b1) = sup p\u2208\u2206n\n{\n\u2212 \u3008\u03b1, p\u3009 G \u2212 \u00b5d(p)\n}\n+ 12\u2016\u03b1\u2016 2 G.\nNote that the maximizer above is precisely p\u00b5(\u03b1).\nLemma 4 (Lower Bound). At any step k, we have\nL\u00b5k(\u03b1k) \u2265 L(\u03b1k)\u2212 \u00b5k logn.\nProof. First note that supp\u2208\u2206n d(p) = logn. Also,\nsup p\u2208\u2206n\n{\n\u2212 \u3008\u03b1, p\u3009 G \u2212 \u00b5d(p)\n}\n\u2265 sup p\u2208\u2206n\n{\n\u2212 \u3008\u03b1, p\u3009 G\n}\n\u2212 sup p\u2208\u2206n\n{ \u00b5d(p) } .\nCombining these two facts gives us the result.\nLemma 5 (Upper Bound). In any round k, SNKP satisfies\nL\u00b5k(\u03b1k) \u2264 \u2212 12\u2016pk\u2016 2 G.\nProof. We provide a concise, self-contained and unified proof by induction in the Appendix for Lemma 5 and Lemma 8, borrowing ideas from Nesterov\u2019s excessive gap technique (Nesterov 2005) for smooth minimization of structured non-smooth functions.\nFinally, we combine the above lemmas to get the following theorem about the performance of SNKP.\nTheorem 1. The SNKP algorithm finds a perfect classifier f \u2208 FK when one exists in O (\u221a\nlogn \u03c1K\n)\niterations.\nProof. Lemma 4 gives us for any round k,\nL\u00b5k(\u03b1k) \u2265 L(\u03b1k)\u2212 \u00b5k logn.\nFrom Lemmas 3, 5 we get L\u00b5k(\u03b1k) \u2264 \u2212 12p \u22a4 k Gpk \u2264 \u2212 12\u03c1 2 K .\nCombining the two equations, we get that\nL(\u03b1k) \u2264 \u00b5k logn\u2212 12\u03c1 2 K .\nNoting that \u00b5k = 4(k+1)(k+2) < 4 (k+1)2 , we see that L(\u03b1k) < 0 (and hence we solve the problem by Lemma 1) after at most k = 2 \u221a 2 logn/\u03c1K steps."}, {"heading": "5 Infeasible Problems", "text": "What happens when the points are not separable by any function f \u2208 FK? We would like an algorithm that terminates with a solution when there is one, and terminates with a certificate of non-separability if there isn\u2019t one. The idea is based on theorems of the alternative like Farkas\u2019 Lemma, specifically a version of Gordan\u2019s theorem (Chvatal 1983):\nLemma 6 (Gordan\u2019s Thm). Exactly one of the following two statements can be true\n1. Either there exists a w \u2208 Rd such that for all i,\nyi(w \u22a4xi) > 0,\n2. Or, there exists a p \u2208 \u2206n such that \u2016XY p\u20162 = 0, (13)\nor equivalently \u2211\ni piyixi = 0.\nAlgorithm 5 Normalized Von-Neumann (NVN) Initialize p0 = 1n/n,w0 = XY p0 for k = 0, 1, 2, 3, ... do\nif \u2016XY pk\u20162 \u2264 \u01eb then Exit and return pk as an \u01eb-solution to (13) else j := argmini yix \u22a4 i wk\n\u03b8k := argmin\u03bb\u2208[0,1] \u2016(1\u2212 \u03bb)wk + \u03bbyjxj\u20162 pk+1 := (1\u2212 \u03b8k)pk + \u03b8kej wk+1 := XY pk+1 = (1 \u2212 \u03b8k)wk + \u03b8kyjxj\nend if end for\nAs mentioned in the introduction, the primal problem can be interpreted as finding a vector in the interior of the dual cone of cone{yixi}, which is infeasible the dual cone is flat i.e. if cone{yixi} is not pointed, which happens when the origin is in the convex combination of yixis.\nWe will generalize the following algorithm for linear feasibility problems, that can be dated back to VonNeumann, who mentioned it in a private communication with Dantzig, who later studied it himself (Dantzig 1992).\nThis algorithm comes with a guarantee: If the problem (3) is infeasible, then the above algorithm will terminate with an \u01eb-approximate solution to (13) in 1/\u01eb2 iterations.\nEpelman & Freund (2000) proved an incomparable bound - Normalized Von-Neumann (NVN) can com-\npute an \u01eb-solution to (13) in O (\n1 \u03c122 log ( 1 \u01eb )\n) and can also find a solution to the primal (using wk) in O (\n1 \u03c122\n)\nwhen it is feasible. We derive a smoothed variant of NVN in the next section, after we prove some crucial lemmas in RKHSs."}, {"heading": "5.1 A Separation Theorem for RKHSs", "text": "While finite dimensional Euclidean spaces come with strong separation guarantees that come under various names like the separating hyperplane theorem, Gordan\u2019s theorem, Farkas\u2019 lemma, etc, the story isn\u2019t always the same for infinite dimensional function spaces which can often be tricky to deal with. We will prove an appropriate version of such a theorem that will be useful in our setting.\nWhat follows is an interesting version of the Hahn-Banach separation theorem, which looks a lot like Gordan\u2019s theorem in finite dimensional spaces. The conditions to note here are that either G\u03b1 > 0 or \u2016p\u2016G = 0.\nTheorem 2. Exactly one of the following has a solution: 1. Either \u2203f \u2208 FK such that for all i,\nyif(xi) \u221a\nK(xi, xi) = \u3008f, yi\u03c6\u0303xi\u3009K > 0 i.e. G\u03b1 > 0,\n2. Or \u2203p \u2208 \u2206n such that \u2211\ni\npiyi\u03c6\u0303xi = 0 \u2208 FK i.e. \u2016p\u2016G = 0. (14)\nProof. Consider the following set\nQ =\n{\n(f, t) =\n(\n\u2211\ni\npiyi\u03c6\u0303xi , \u2211\ni\npi\n) : p \u2208 \u2206n }\n= conv [ (y1\u03c6\u0303x1 , 1), ..., (yn\u03c6\u0303xn , 1) ]\n\u2286 FK \u00d7 R.\nIf (2) does not hold, then it implies that (0, 1) /\u2208 Q. Since Q is closed and convex, we can find a separating hyperplane between Q and (0, 1), or in other words there exists (f, t) \u2208 FK \u00d7 R such that\n\u2329 (f, t), (g, s) \u232a \u2265 0 \u2200(g, s) \u2208 Q\nand \u2329 (f, t), (0, 1) \u232a < 0.\nThe second condition immediately yields t < 0. The first condition, when applied to (g, s) = (yi\u03c6\u0303xi , 1) \u2208 Q yields\n\u3008f, yi\u03c6\u0303xi\u3009K + t \u2265 0\n\u21d4 yif(xi)\u221a K(xi, xi) > 0\nsince t < 0, which shows that (1) holds. It is also immediate that if (2) holds, then (1) cannot.\nNote that G is positive semi-definite - infeasibility requires both that it is not positive definite, and also that the witness to p\u22a4Gp = 0 must be a probability vector. Similarly, while it suffices that G\u03b1 > 0 for some \u03b1 \u2208 Rn, but coincidentally in our case \u03b1 will also lie in the probability simplex."}, {"heading": "5.2 The infeasible margin \u03c1K", "text": "Note that constraining \u2016f\u2016K = 1 (or \u2016\u03b1\u2016G = 1) in Eq. (5) and Lemma 3 allows \u03c1K to be negative in the infeasible case. If it was \u2264, then \u03c1K would have been non-negative because f = 0 (ie \u03b1 = 0) is always allowed.\nSo what is \u03c1K when the problem is infeasible? Let\nconv(Y \u03c6\u0303X) := { \u2211\ni\npiyi\u03c6\u0303xi |p \u2208 \u2206n } \u2282 FK\nbe the convex hull of the yi\u03c6\u0303xis.\nTheorem 3. When the primal is infeasible, the margin1 is\n|\u03c1K | = \u03b4max := sup { \u03b4 \u2223 \u2223 \u2016f\u2016K \u2264 \u03b4 \u21d2 f \u2208 conv(Y \u03c6\u0303X) }\nProof. (1) For inequality \u2265. Choose any \u03b4 such that f \u2208 conv(Y \u03c6\u0303X) for any \u2016f\u2016K \u2264 \u03b4. Given an arbitrary f \u2032 \u2208 FK with \u2016f \u2032\u2016K = 1, put f\u0303 := \u2212\u03b4f \u2032.\n1We thank a reviewer for pointing out that by this definition, \u03c1K might always be 0 for infinite dimensional RKHSs because there are always directions perpendicular to the finite-dimensional hull - we conjecture the definition can be altered to restrict attention to the relative interior of the hull, making it non-zero.\nBy our assumption on \u03b4, we have f\u0303 \u2208 conv(Y \u03c6\u0303X) implying there exists a p\u0303 \u2208 \u2206n such that f\u0303 = \u3008Y \u03c6\u0303X , p\u0303\u3009 . Also\n\u2329 f \u2032, \u3008Y \u03c6\u0303X , p\u0303\u3009 \u232a K = \u3008f \u2032, f\u0303\u3009K = \u2212\u03b4\u2016f \u2032\u20162K = \u2212\u03b4.\nSince this holds for a particular p\u0303, we can infer\ninf p\u2208\u2206n\n\u2329 f \u2032, \u3008Y \u03c6\u0303X , p\u0303\u3009 \u232a\nK \u2264 \u2212\u03b4.\nSince this holds for any f \u2032 with \u2016f \u2032\u2016G = 1, we have\nsup \u2016f\u2016K=1 inf p\u2208\u2206n\n\u2329 f \u2032, \u3008Y \u03c6\u0303X , p\u0303\u3009 \u232a\nK \u2264 \u2212\u03b4 i.e. |\u03c1K | \u2265 \u03b4.\n(2) For inequality \u2264. It suffices to show \u2016f\u2016K \u2264 |\u03c1K | \u21d2 f \u2208 conv(Y \u03c6\u0303X). We will prove the contrapositive f /\u2208 conv(Y \u03c6\u0303X) \u21d2 \u2016f\u2016K > |\u03c1K |.\nSince\u2206n is compact and convex, conv(Y \u03c6\u0303X) \u2282 FK is closed and convex. Therefore if f /\u2208 conv(Y \u03c6\u0303X), then there exists g \u2208 FK with \u2016g\u2016K = 1 that separates f and conv(Y \u03c6\u0303X), i.e. for all p \u2208 \u2206n,\n\u3008g, f\u3009K < 0 and \u3008g, \u3008Y \u03c6\u0303X , p\u3009\u3009K \u2265 0 i.e. \u3008g, f\u3009K < inf\np\u2208\u2206n \u3008g, \u3008Y \u03c6\u0303X , p\u3009\u3009K\n\u2264 sup \u2016f\u2016K=1 inf p\u2208\u2206n \u3008f, \u3008Y \u03c6\u0303X , p\u3009\u3009K = \u03c1K .\nSince \u03c1K < 0 |\u03c1K | < |\u3008f, g\u3009K | \u2264 \u2016f\u2016K\u2016g\u2016K = \u2016f\u2016K ."}, {"heading": "6 Kernelized Primal-Dual Algorithms", "text": "The preceding theorems allow us to write a variant of the Normalized VonNeumann algorithm from the previous section that is smoothed and works for RKHSs. Define\nW := { p \u2208 \u2206n \u2223 \u2223 \u2223 \u2211\ni\npiyi\u03c6\u0303xi = 0 } = { p \u2208 \u2206n \u2223 \u2223 \u2223 \u2016p\u2016G = 0 }\nas the set of witnesses to the infeasibility of the primal. The following lemma bounds the distance of any point in the simplex from the witness set by its \u2016.\u2016G norm.\nLemma 7. For all q \u2208 \u2206n, the distance to the witness set\ndist(q,W ) := min w\u2208W\n\u2016q \u2212 w\u20162 \u2264 min { \u221a 2, \u221a 2\u2016q\u2016G |\u03c1K | } ."}, {"heading": "As a consequence, \u2016p\u2016G = 0 iff p \u2208 W .", "text": "Proof. This is trivial for p \u2208 W . For arbitrary p \u2208 \u2206n\\W , let p\u0303 := \u2212 |\u03c1K |p\u2016p\u2016G so that \u2016\u3008Y \u03c6\u0303X , p\u0303\u3009\u2016K = \u2016p\u0303\u2016G \u2264 |\u03c1K |.\nHence by Theorem 3, there exists \u03b1 \u2208 \u2206n such that\n\u3008Y \u03c6\u0303X , \u03b1\u3009 = \u3008Y \u03c6\u0303X , p\u0303\u3009.\nLet \u03b2 = \u03bb\u03b1+ (1 \u2212 \u03bb)p where \u03bb = \u2016p\u2016G\u2016p\u2016G+|\u03c1K | . Then\n\u3008Y \u03c6\u0303X , \u03b2\u3009 = 1 \u2016p\u2016G + |\u03c1K | \u2329 Y \u03c6\u0303X , \u2016p\u2016G\u03b1+ |\u03c1K |p \u232a\n= 1\n\u2016p\u2016G + |\u03c1K | \u3008Y \u03c6\u0303X , \u2016p\u2016Gp\u0303+ |\u03c1K |p\u3009\n= 0,\nso \u03b2 \u2208 W (by definition of what it means to be in W ) and\n\u2016p\u2212 \u03b2\u20162 = \u03bb\u2016p\u2212 \u03b1\u20162 \u2264 \u03bb \u221a 2 \u2264 min\n{\n\u221a 2, \u221a 2\u2016q\u2016G |\u03c1K | } .\nWe take min with \u221a 2 because \u03c1K might be 0.\nHence for the primal or dual problem, points with small G-norm are revealing - either Lemma 3 shows that the margin \u03c1K \u2264 \u2016p\u2016G will be small, or if it is infeasible then the above lemma shows that it is close to the witness set.\nWe need a small alteration to the smoothing entropy prox-function that we used earlier. We will now use\ndq(p) = 1 2\u2016p\u2212 q\u2016 2 2\nfor some given q \u2208 \u2206n, which is strongly convex with respect to the \u21132 norm. This allows us to define\npq\u00b5(\u03b1) = arg min p\u2208\u2206n \u3008G\u03b1, p\u3009+ \u00b5 2 \u2016p\u2212 q\u201622,\nLq\u00b5(\u03b1) = sup p\u2208\u2206n\n{ \u2212 \u3008\u03b1, p\u3009G \u2212 \u00b5dq(p) } + 12\u2016\u03b1\u2016 2 G,\nwhich can easily be found by sorting the entries of q \u2212 G\u03b1\u00b5 .\nAlgorithm 6 Smoothed Normalized Kernel Perceptron-VonNeumann (SNKPVN(q, \u03b4)) Input q \u2208 \u2206n, accuracy \u03b4 > 0 Set \u03b10 = q, \u00b50 := 2n, p0 := pq\u00b50(\u03b10) for k = 0, 1, 2, 3, ... do\nif G\u03b1k > 0n then Halt: \u03b1k is solution to Eq. (8) else if \u2016pk\u2016G < \u03b4 then Return pk else \u03b8k := 2 k+3\n\u03b1k+1 := (1\u2212 \u03b8k)(\u03b1k + \u03b8kpk) + \u03b82k pq\u00b5k(\u03b1k) \u00b5k+1 = (1\u2212 \u03b8k)\u00b5k pk+1 := (1\u2212 \u03b8k)pk + \u03b8k pq\u00b5k+1(\u03b1k+1)\nend if end for\nWhen the primal is feasible, SNKPVN is similar to SNKP.\nLemma 8 (When \u03c1K > 0 and \u03b4 < \u03c1K). For any q \u2208 \u2206n,\n\u2212 12\u2016pk\u2016 2 G \u2265 Lq\u00b5k(\u03b1k) \u2265 L(\u03b1k)\u2212 \u00b5k.\nHence SNKPVN finds a separator f in O (\u221a\nn \u03c1K\n)\niterations.\nProof. We give a unified proof for the first inequality and Lemma 5 in the Appendix. The second inequality mimics Lemma 4. The final statement mimics Theorem 1.\nThe following lemma captures the near-infeasible case.\nLemma 9 (When \u03c1K < 0 or \u03b4 > \u03c1K). For any q \u2208 \u2206n,\n\u2212 12\u2016pk\u2016 2 G \u2265 Lq\u00b5k(\u03b1k) \u2265 \u2212 1 2\u00b5kdist(q,W ) 2.\nHence SNKPVN finds a \u03b4-solution in at most O ( min {\u221a\nn \u03b4 , \u221a n\u2016q\u2016G \u03b4|\u03c1K | }) iterations.\nProof. The first inequality is the same as in the above Lemma 8, and is proved in the Appendix.\nLq\u00b5k(\u03b1k) = sup p\u2208\u2206n\n{ \u2212 \u3008\u03b1, p\u3009G \u2212 \u00b5kdq(p) } + 12\u2016\u03b1\u2016 2 G\n\u2265 sup p\u2208W\n{ \u2212 \u3008\u03b1, p\u3009G \u2212 \u00b5kdq(p) }\n= sup p\u2208W\n{\n\u2212 12\u00b5k\u2016p\u2212 q\u2016 2 2\n}\n= \u2212 12\u00b5kdist(q,W ) 2 \u2265 \u2212\u00b5k min {\n2, \u2016q\u20162G |\u03c1K |2\n}\nusing Lemma 7.\nSince \u00b5k = 4n(k+1)(k+2) \u2264 4n(k+1)2 we get\n\u2016pk\u2016G \u2264 2 \u221a n\n(k + 1) min\n{\u221a 2, \u2016q\u2016G \u03c1K } .\nHence \u2016p\u2016G \u2264 \u03b4 after 2 \u221a n \u03b4 min {\u221a 2, \u2016q\u2016G\u03c1K } steps.\nUsing SNKPVN as a subroutine gives our final algorithm.\nAlgorithm 7 Iterated Smoothed Normalized Kernel Perceptron-VonNeumann (ISNKPVN(\u03b3, \u01eb)) Input constant \u03b3 > 1, accuracy \u01eb > 0 Set q0 := 1n/n for t = 0, 1, 2, 3, ... do \u03b4t := \u2016qt\u2016G/\u03b3 qt+1 := SNKPVN(qt, \u03b4t) if \u03b4t < \u01eb then\nHalt; qt+1 is a solution to Eq. (14) end if\nend for\nTheorem 4. Algorithm ISNKPVN satisfies\n1. If the primal (2) is feasible and \u01eb < \u03c1K , then each call to SNKPVN halts in at most 2 \u221a 2n\n\u03c1K iterations.\nAlgorithm ISNKPVN finds a solution in at most log(1/\u03c1K)log(\u03b3) outer loops, bounding the total iterations by\nO\n(\u221a n\n\u03c1K log\n(\n1\n\u03c1K\n))\n.\n2. If the dual (14) is feasible or \u01eb > \u03c1K , then each call to SNKPVN halts in at most O ( min {\u221a n \u01eb , \u221a n |\u03c1K | })\nsteps. Algorithm ISNKPVN finds an \u01eb-solution in at most log(1/\u01eb)log(\u03b3) outer loops, bounding the total iterations by\nO\n(\nmin\n{\u221a n\n\u01eb ,\n\u221a n\n|\u03c1K |\n}\nlog\n(\n1\n\u01eb\n))\n.\nProof. First note that if ISNKPVN has not halted, then we know that after t outer iterations, qt+1 has small G-norm:\n\u2016qt+1\u2016G \u2264 \u03b4t \u2264 \u2016q0\u2016G \u03b3t+1 . (15)\nThe first inequality holds because of the inner loop return condition, the second because of the update for \u03b4t.\n1. Lemma 3 shows that for all p we have \u03c1K \u2264 \u2016p\u2016G, so the inner loop will halt with a solution to the primal as soon as \u03b4t \u2264 \u03c1K (so that \u2016p\u2016G < \u03b4t \u2264 \u03c1K cannot be satisfied for the inner loop to return). From Eq. (15), this will definitely happen when \u2016q0\u2016G\u03b3t+1 \u2264 \u03c1K , ie within T = log(\u2016qo\u2016G/\u03c1K) log(\u03b3) iterations.\nBy Lemma 8, each iteration runs for at most 2 \u221a 2n\n\u03c1K steps.\n2. We halt with an \u01eb-solution when \u03b4t < \u01eb, which definitely happens when \u2016q0\u2016G \u03b3t+1 < \u01eb, ie within\nT = log(\u2016qo\u2016G/\u01eb)log(\u03b3) iterations. Since \u2016qt\u2016G \u03b4t = \u03b3, by Lemma 9, each iteration runs for at most O ( min {\u221a\nn \u01eb ,\n\u221a n |\u03c1K | }) steps."}, {"heading": "7 Discussion", "text": "The SNK-Perceptron algorithm presented in this paper has a convergence rate of \u221a logn \u03c1K and the Iterated SNK-Perceptron-Von-Neumann algorithm has a min {\u221a\nn \u01eb ,\n\u221a n |\u03c1K | } dependence on the number of points. Note\nthat both of these are independent of the underlying dimensionality of the problem. We conjecture that it is possible to reduce this dependence to \u221a logn for the primal-dual algorithm also, without paying a price in terms of the dependence on margin 1/\u03c1 (or the dependence on \u01eb). It is possible that tighter dependence on n is possible if we try other smoothing functions instead of the \u21132 norm used in the last section. Specifically, it might be tempting to smooth with the \u2016.\u2016G semi-norm and define:\npq\u00b5(\u03b1) = arg min p\u2208\u2206n\n\u3008\u03b1, p\u3009G + \u00b5\n2 \u2016p\u2212 q\u20162G\nOne can actually see that the proofs in the Appendix go through with no dimension dependence on n at all! However, it is not possible to solve this in closed form - taking \u03b1 = q and \u00b5 = 1 reduces the problem to asking\npq(q) = arg min p\u2208\u2206n 1 2\u2016p\u2016 2 G\nwhich is an oracle for our problem as seen by equation (14) - the solution\u2019s G-norm is 0 iff the problem is infeasible.\nIn the bigger picture, there are several interesting open questions. The ellipsoid algorithm for solving linear feasibility problems has a logarithmic dependence on 1/\u01eb, and a polynomial dependence on dimension.\nRecent algorithms involving repeated rescaling of the space like the one by Dunagan & Vempala (2008) have logarithmic dependence on 1/\u03c1 and polynomial in dimension. While both these algorithms are polytime under the real number model of computation of Blum et al. (1998), it is unknown whether there is any algorithm that can achieve a polylogarithmic dependence on the margin/accuracy, and a polylogarithmic dependence on dimension. This is strongly related to the open question of whether it is possible to learn a decision list polynomially in its binary description length.\nOne can nevertheless ask whether rescaled smoothed perceptron methods like Dunagan & Vempala (2008) can be lifted to RKHSs, and whether using an iterated smoothed kernel perceptron would yield faster rates. The recent work Soheili & Pen\u0303a (2013a) is a challenge to generalize - the proofs relying on geometry involve arguing about volumes of balls of functions in an RKHS - we conjecture that it is possible to do, but we leave it for a later work."}, {"heading": "Acknowledgements", "text": "We thank Negar Soheili and Avrim Blum for insightful discussions."}, {"heading": "A Unified Proof By Induction of Lemma 5, 8: L\u00b5k(\u03b1k) \u2264 \u221212\u2016pk\u20162G", "text": "Let d(p) be 1-strongly convex with respect to the #-norm, ie d(q)\u2212 d(p)\u2212 \u3008\u2207d(p), q\u2212 p\u3009 \u2265 12\u2016q\u2212 p\u20162#for any p, q \u2208 \u2206n. Let the #-norm be lower bounded by the G-norm as \u2016p\u20162G \u2264 \u03bb#\u2016p\u20162#. For d(p) = \u2211\ni pi log pi + logn, # is the 1-norm, \u03bb# = 1 and p \u2217 = 1nn . For d(p) = 1 2\u2016q \u2212 p\u201622, # is the 2-norm,\n\u03bb# = n and p\u2217 = q. Choose \u00b50 = 2\u03bb#. Let the smoothed minimizer be defined by p\u00b5(\u03b1) := argminp\u2208\u2206n\u3008G\u03b1, p\u3009+\u00b5d(p), and p\u2217 := argminp\u2208\u2206n d(p). The optimality condition of p\u00b5(\u03b1) and p\u2217 (the gradient is perpendicular to any feasible direction) is that for any r \u2208 \u2206n,\n\u3008G\u03b1+ \u00b5\u2207d(p\u00b5(\u03b1)), r \u2212 p\u3009 = 0 (16) \u3008\u2207d(p\u2217), r \u2212 p\u3009 = 0 \u21d2 d(p0) \u2265 12\u2016p0 \u2212 p \u2217\u20162#. (17)\nFor k = 0 : \u2212 12\u2016p0\u2016 2 G = \u2212 12\u2016p0 \u2212 p \u2217\u20162G \u2212 \u3008p\u2217, p0 \u2212 p\u2217\u3009G \u2212 12\u2016p \u2217\u20162G writing p0 = (p0 \u2212 p\u2217) + p\u2217\n\u2265 \u2212\u03bb#2 \u2016p0 \u2212 p \u2217\u20162# \u2212 \u3008p\u2217, p0\u3009G + 12\u2016p \u2217\u20162G using \u2016p\u20162G \u2264 \u03bb#\u2016p\u20162# \u2265 \u2212\u00b50d(p0)\u2212 \u3008\u03b10, p0\u3009G + 12\u2016\u03b10\u2016 2 G adding \u2212 \u03bb# 2 \u2016p0 \u2212 p\u2217\u201621, using Eq. (2) = L\u00b50(\u03b10).\nAssume it holds upto k. We drop index k, and write x+ for xk+1. Let p\u0302 = (1 \u2212 \u03b8)p + \u03b8p\u00b5(\u03b1) so \u03b1+ = (1\u2212 \u03b8)\u03b1 + \u03b8p\u0302. (3)\nL\u00b5+(\u03b1+) = 1 2\u2016\u03b1+\u2016 2 G \u2212\n\u2329 \u03b1+, p\u00b5+(\u03b1+) \u232a G \u2212 \u00b5+d(p\u00b5+(\u03b1+))\n= 12 \u2225 \u2225(1 \u2212 \u03b8)\u03b1+ \u03b8p\u0302 \u2225 \u2225 2 G \u2212 \u03b8 \u2329 p\u0302, p\u00b5+(\u03b1+) \u232a G \u2212 (1\u2212 \u03b8)\n[\n\u2329 \u03b1, p\u00b5+(\u03b1+) \u232a\nG + \u00b5d(p\u00b5+(\u03b1+))\n]\nusing Eq. (3)\n\u2264 (1\u2212 \u03b8) [\n1 2\u2016\u03b1\u2016 2 G \u2212\n\u2329 \u03b1, p\u00b5+(\u03b1+) \u232a G \u2212 \u00b5d(p\u00b5+(\u03b1+))\n]\n1\n+ \u03b8\n[\n\u2212 12\u2016p\u0302\u2016 2 G \u2212\n\u2329 p\u0302, p\u00b5+(\u03b1+)\u2212 p\u0302 \u232a\nG\n]\n,\nwhere we used the convexity of \u2016.\u20162G. Recall p+ = (1 \u2212 \u03b8)p + \u03b8p\u00b5+(\u03b1+), so that p+ \u2212 p\u0302 = \u03b8(p\u00b5+(\u03b1+) \u2212 p\u00b5(\u03b1)). (4)\n[ . ]\n1 =\n[\n1 2\u2016\u03b1\u2016 2 G \u2212\n\u2329 \u03b1, p\u00b5(\u03b1) \u232a\nG \u2212 \u00b5d(p\u00b5(\u03b1))\n] \u2212 \u2329 \u03b1, p\u00b5+(\u03b1+)\u2212 p\u00b5(\u03b1) \u232a G \u2212 \u00b5 [ d(p\u00b5+(\u03b1+))\u2212 d(p\u00b5(\u03b1)) ]\n= L\u00b5(\u03b1) \u2212 \u00b5 [ d(p\u00b5+(\u03b1+))\u2212 d(p\u00b5(\u03b1)) \u2212 \u2329 \u2207d(p\u00b5(\u03b1)), p\u00b5+ (\u03b1+)\u2212 p\u00b5(\u03b1) \u232a] using Eq. (1)\n\u2264 \u2212 12\u2016p\u2016 2 G \u2212 \u00b52 \u2016p\u00b5+(\u03b1+)\u2212 p\u00b5(\u03b1)\u2016 2 # using strong convexity of d(p) \u2264 \u2212 12\u2016p\u0302+ (p\u2212 p\u0302)\u20162G \u2212 \u00b5 2\u03bb# \u2016p\u00b5+(\u03b1+)\u2212 p\u00b5(\u03b1)\u20162G using \u2016p\u20162G \u2264 \u03bb#\u2016p\u20162#\n\u2264 \u2212 12\u2016p\u0302\u2016 2 G \u2212\n\u2329 p\u0302, p\u2212 p\u0302 \u232a G \u2212 \u00b5\n2\u03bb#\u03b82 \u2016p+ \u2212 p\u0302\u20162G using Eq. (4) and dropping a \u2212 12\u2016p\u2212 p\u0302\u20162G term.\nUsing (1\u2212 \u03b8)(p\u2212 p\u0302) = \u2212\u03b8(p\u00b5(\u03b1)\u2212 p\u0302) and substituting back,\nL\u00b5+(\u03b1+) \u2264 (1\u2212 \u03b8) [ \u2212 12\u2016p\u0302\u2016 2 G + \u03b8 1\u2212\u03b8 \u2329 p\u0302, p\u00b5(\u03b1)\u2212 p\u0302 \u232a G \u2212 \u00b5\n2\u03bb#\u03b82 \u2016p+ \u2212 p\u0302\u20162G\n]\n+ \u03b8\n[\n\u2212 12\u2016p\u0302\u2016 2 G \u2212\n\u2329 p\u0302, p\u00b5+(\u03b1+)\u2212 p\u0302 \u232a\nG\n]\n= \u2212 12\u2016p\u0302\u2016 2 G \u2212 \u03b8\n\u2329 p\u0302, p\u00b5+(\u03b1+)\u2212 p\u00b5(\u03b1) \u232a G \u2212 \u00b5(1\u2212 \u03b8)\n2\u03bb#\u03b82 \u2016p+ \u2212 p\u0302\u20162G\n\u2264 \u2212 12\u2016p\u0302\u2016 2 G \u2212\n\u2329 p\u0302, p+ \u2212 p\u0302 \u232a\nG \u2212 12\u2016p+ \u2212 p\u0302\u2016 2 G using Eq. (4) and\n\u03b82\n1\u2212\u03b8 = 4 (k+1)(k+3) \u2264 4(k+1)(k+2) = \u00b5 \u03bb#\n= \u2212 12\u2016p+\u2016 2 G.\nThis wraps up our unified proof for both settings."}], "references": [{"title": "The perceptron: A model for brain functioning", "author": ["H. Block"], "venue": "i\u2019, Reviews of Modern Physics", "citeRegEx": "Block,? \\Q1962\\E", "shortCiteRegEx": "Block", "year": 1962}, {"title": "Complexity and real computation, Springer", "author": ["L. Blum", "F. Cucker", "M. Shub", "S. Smale"], "venue": null, "citeRegEx": "Blum et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1998}, {"title": "A new condition number for linear programming", "author": ["D. Cheung", "F. Cucker"], "venue": "Mathematical programming", "citeRegEx": "Cheung and Cucker,? \\Q2001\\E", "shortCiteRegEx": "Cheung and Cucker", "year": 2001}, {"title": "Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm", "author": ["K.L. Clarkson"], "venue": "ACM Transactions on Algorithms", "citeRegEx": "Clarkson,? \\Q2010\\E", "shortCiteRegEx": "Clarkson", "year": 2010}, {"title": "A simple polynomial-time rescaling algorithm for solving linear programs", "author": ["J. Dunagan", "S. Vempala"], "venue": "Mathematical Programming", "citeRegEx": "Dunagan and Vempala,? \\Q2008\\E", "shortCiteRegEx": "Dunagan and Vempala", "year": 2008}, {"title": "Condition number complexity of an elementary algorithm for computing a reliable solution of a conic linear system", "author": ["M. Epelman", "R.M. Freund"], "venue": "Mathematical Programming", "citeRegEx": "Epelman and Freund,? \\Q2000\\E", "shortCiteRegEx": "Epelman and Freund", "year": 2000}, {"title": "Condition-based complexity of convex optimization in conic linear form via the ellipsoid algorithm", "author": ["R.M. Freund", "J.R. Vera"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Freund and Vera,? \\Q1999\\E", "shortCiteRegEx": "Freund and Vera", "year": 1999}, {"title": "From margin to sparsity\u2019, Advances in neural information processing systems", "author": ["T. Graepel", "R. Herbrich", "R.C. Williamson"], "venue": null, "citeRegEx": "Graepel et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Graepel et al\\.", "year": 2001}, {"title": "Redundant noisy attributes, attribute errors, and linear-threshold learning using winnow, in \u2018Proceedings of the fourth annual workshop on Computational learning theory", "author": ["N. Littlestone"], "venue": null, "citeRegEx": "Littlestone,? \\Q1991\\E", "shortCiteRegEx": "Littlestone", "year": 1991}, {"title": "Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "author": ["A. Nemirovski"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Nemirovski,? \\Q2004\\E", "shortCiteRegEx": "Nemirovski", "year": 2004}, {"title": "Excessive gap technique in nonsmooth convex minimization", "author": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Nesterov,? \\Q2005\\E", "shortCiteRegEx": "Nesterov", "year": 2005}, {"title": "On convergence proofs for perceptrons, Technical report", "author": ["A.B. Novikoff"], "venue": null, "citeRegEx": "Novikoff,? \\Q1962\\E", "shortCiteRegEx": "Novikoff", "year": 1962}, {"title": "Incorporating condition measures into the complexity theory of linear programming", "author": ["J. Renegar"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Renegar,? \\Q1995\\E", "shortCiteRegEx": "Renegar", "year": 1995}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain.", "author": ["F. Rosenblatt"], "venue": "Psychological review", "citeRegEx": "Rosenblatt,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt", "year": 1958}, {"title": "New approximation algorithms for minimum enclosing convex shapes, in \u2018Proceedings of the Twenty-Second", "author": ["A. Saha", "S. Vishwanathan", "X. Zhang"], "venue": "Annual ACM-SIAM Symposium on Discrete Algorithms\u2019,", "citeRegEx": "Saha et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saha et al\\.", "year": 2011}, {"title": "A generalized representer theorem, in \u2018Computational learning theory", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "Learning with kernels", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "A smooth perceptron algorithm", "author": ["N. Soheili", "J. Pe\u00f1a"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Soheili and Pe\u00f1a,? \\Q2012\\E", "shortCiteRegEx": "Soheili and Pe\u00f1a", "year": 2012}, {"title": "2013a), \u2018A deterministic rescaled perceptron algorithm", "author": ["N. Soheili", "J. Pe\u00f1a"], "venue": null, "citeRegEx": "Soheili and Pe\u00f1a,? \\Q2013\\E", "shortCiteRegEx": "Soheili and Pe\u00f1a", "year": 2013}, {"title": "2013b), A primal\u2013dual smooth perceptron\u2013von Neumann algorithm, in \u2018Discrete Geometry and Optimization", "author": ["N. Soheili", "J. Pe\u00f1a"], "venue": null, "citeRegEx": "Soheili and Pe\u00f1a,? \\Q2013\\E", "shortCiteRegEx": "Soheili and Pe\u00f1a", "year": 2013}, {"title": "On accelerated proximal gradient methods for convex-concave optimization", "author": ["P. Tseng"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Tseng,? \\Q2008\\E", "shortCiteRegEx": "Tseng", "year": 2008}], "referenceMentions": [{"referenceID": 8, "context": "It can be thought of as the width of the feasibility cone as in Freund & Vera (1999), a radius of well-posedness as in Cheung & Cucker (2001), and its inverse can be seen as a special case of a condition number defined by Renegar (1995) for these systems.", "startOffset": 222, "endOffset": 237}, {"referenceID": 8, "context": "It can be thought of as the width of the feasibility cone as in Freund & Vera (1999), a radius of well-posedness as in Cheung & Cucker (2001), and its inverse can be seen as a special case of a condition number defined by Renegar (1995) for these systems. 1.1 Related Work In this paper we focus on the famous Perceptron algorithm from Rosenblatt (1958) and the less-famous VonNeumann algorithm from Dantzig (1992) that we introduce in later sections.", "startOffset": 222, "endOffset": 354}, {"referenceID": 8, "context": "It can be thought of as the width of the feasibility cone as in Freund & Vera (1999), a radius of well-posedness as in Cheung & Cucker (2001), and its inverse can be seen as a special case of a condition number defined by Renegar (1995) for these systems. 1.1 Related Work In this paper we focus on the famous Perceptron algorithm from Rosenblatt (1958) and the less-famous VonNeumann algorithm from Dantzig (1992) that we introduce in later sections.", "startOffset": 222, "endOffset": 415}, {"referenceID": 8, "context": "It can be thought of as the width of the feasibility cone as in Freund & Vera (1999), a radius of well-posedness as in Cheung & Cucker (2001), and its inverse can be seen as a special case of a condition number defined by Renegar (1995) for these systems. 1.1 Related Work In this paper we focus on the famous Perceptron algorithm from Rosenblatt (1958) and the less-famous VonNeumann algorithm from Dantzig (1992) that we introduce in later sections. As mentioned by Epelman & Freund (2000), in a technical report by the same name, Nesterov pointed out in a note to the authors that the latter is a special case of the now-popular Frank-Wolfe algorithm.", "startOffset": 222, "endOffset": 492}, {"referenceID": 7, "context": "As mentioned by Epelman & Freund (2000), in a technical report by the same name, Nesterov pointed out in a note to the authors that the latter is a special case of the now-popular Frank-Wolfe algorithm. Our work builds on Soheili & Pe\u00f1a (2012, 2013b) from the field of optimization - we generalize the setting to learning functions in RKHSs, extend the algorithms, simplify proofs, and simultaneously bring new perspectives to it. There is extensive literature around the Perceptron algorithm in the learning community; we restrict ourselves to discussing only a few directly related papers, in order to point out the several differences from existing work. We provide a general unified proof in the Appendix which borrows ideas from accelerated smoothing methods developed by Nesterov (2005) - while this algorithm and others by Nemirovski (2004), Saha et al.", "startOffset": 81, "endOffset": 793}, {"referenceID": 7, "context": "We provide a general unified proof in the Appendix which borrows ideas from accelerated smoothing methods developed by Nesterov (2005) - while this algorithm and others by Nemirovski (2004), Saha et al.", "startOffset": 172, "endOffset": 190}, {"referenceID": 7, "context": "We provide a general unified proof in the Appendix which borrows ideas from accelerated smoothing methods developed by Nesterov (2005) - while this algorithm and others by Nemirovski (2004), Saha et al. (2011) can achieve similar rates for the same problem, those algorithms do not possess the simplicity of the Perceptron or Von-Neumann algorithms and our variants, and also don\u2019t look at the infeasible setting or primal-dual algorithms.", "startOffset": 172, "endOffset": 210}, {"referenceID": 7, "context": "We provide a general unified proof in the Appendix which borrows ideas from accelerated smoothing methods developed by Nesterov (2005) - while this algorithm and others by Nemirovski (2004), Saha et al. (2011) can achieve similar rates for the same problem, those algorithms do not possess the simplicity of the Perceptron or Von-Neumann algorithms and our variants, and also don\u2019t look at the infeasible setting or primal-dual algorithms. Accelerated smoothing techniques have also been seen in the learning literature like in Tseng (2008) and many others.", "startOffset": 172, "endOffset": 541}, {"referenceID": 3, "context": "This work is also connected to the idea of \u01eb-coresets by Clarkson (2010), though we will not explore that angle.", "startOffset": 57, "endOffset": 73}, {"referenceID": 3, "context": "This work is also connected to the idea of \u01eb-coresets by Clarkson (2010), though we will not explore that angle. A related algorithm is called the Winnow by Littlestone (1991) - this works on the l1 margin and is a saddle point problem over two simplices.", "startOffset": 57, "endOffset": 176}, {"referenceID": 0, "context": "do if sign(w\u22a4 k xi) 6= yi for some i then wk+1 := wk + yixi else Halt: Return wk as solution end if end for It comes with the following classic guarantee as proved by Block (1962) and Novikoff (1962): If there exists a unit vector u \u2208 R such that Y X\u22a4u \u2265 \u03c1 > 0, then a perfect separator will be found in maxi \u2016xi\u2016 2 2 \u03c12 iterations/mistakes.", "startOffset": 167, "endOffset": 180}, {"referenceID": 0, "context": "do if sign(w\u22a4 k xi) 6= yi for some i then wk+1 := wk + yixi else Halt: Return wk as solution end if end for It comes with the following classic guarantee as proved by Block (1962) and Novikoff (1962): If there exists a unit vector u \u2208 R such that Y X\u22a4u \u2265 \u03c1 > 0, then a perfect separator will be found in maxi \u2016xi\u2016 2 2 \u03c12 iterations/mistakes.", "startOffset": 167, "endOffset": 200}, {"referenceID": 7, "context": "Hence, it is always better to normalize the data as pointed out in Graepel et al. (2001). This idea extends to RKHSs, motivating the normalized Gram matrix considered later.", "startOffset": 67, "endOffset": 89}, {"referenceID": 7, "context": "Hence, it is always better to normalize the data as pointed out in Graepel et al. (2001). This idea extends to RKHSs, motivating the normalized Gram matrix considered later. Example Consider a simple example in R+. Assume that + points are located along the line 6x2 = 8x1, and the \u2212 points along 8x2 = 6x1, for 1/r \u2264 \u2016x\u20162 \u2264 r, where r > 1. The max-margin linear separator will be x1 = x2. If all the data were normalized to have unit Euclidean norm, then all the + points would all be at (0.6, 0.8) and all the \u2212 points at (0.8, 0.6), giving us a normalized margin of \u03c12 \u2248 0.14. Unnormalized, the margin is \u03c1 \u2248 0.14/r and maxi \u2016xi\u20162 = r. Hence, in terms of bounds, we get a discrepancy of r, which can be arbitrarily large. Winnow The question arises as to which norm we should normalize by. There is a now classic algorithm in machine learning, called Winnow by Littlestone (1991) or Multiplicate Weights.", "startOffset": 67, "endOffset": 883}, {"referenceID": 7, "context": "Hence, it is always better to normalize the data as pointed out in Graepel et al. (2001). This idea extends to RKHSs, motivating the normalized Gram matrix considered later. Example Consider a simple example in R+. Assume that + points are located along the line 6x2 = 8x1, and the \u2212 points along 8x2 = 6x1, for 1/r \u2264 \u2016x\u20162 \u2264 r, where r > 1. The max-margin linear separator will be x1 = x2. If all the data were normalized to have unit Euclidean norm, then all the + points would all be at (0.6, 0.8) and all the \u2212 points at (0.8, 0.6), giving us a normalized margin of \u03c12 \u2248 0.14. Unnormalized, the margin is \u03c1 \u2248 0.14/r and maxi \u2016xi\u20162 = r. Hence, in terms of bounds, we get a discrepancy of r, which can be arbitrarily large. Winnow The question arises as to which norm we should normalize by. There is a now classic algorithm in machine learning, called Winnow by Littlestone (1991) or Multiplicate Weights. It works on a slight transformation of the problem where we only need to search for u \u2208 R+. It comes with some very wellknown guarantees - If there exists a u \u2208 R+ such that Y X\u22a4u \u2265 \u03c1 > 0, then feasibility is guaranteed in \u2016u\u20161maxi \u2016ai\u2016\u221e logn/\u03c1 iterations. The appropriate notion of normalized margin here is \u03c11 := max w\u2208\u2206d min p\u2208\u2206n \u3008Y X\u22a4 \u221ew, p\u3009, where X\u221e is a matrix with columns xi/\u2016xi\u2016\u221e. Then, the appropriate iteration bound is logn/\u03c11. We will return to this l1-margin in the discussion section. In the next section, we will normalize by using the kernel appropriately. 3 Kernels and RKHSs The theory of Reproducing Kernel Hilbert Spaces (RKHSs) has a rich history, and for a detailed introduction, refer to Sch\u00f6lkopf & Smola (2002). Let K : R \u00d7 R \u2192 R be a symmetric positive definite kernel, giving rise to a Reproducing Kernel Hilbert Space FK with an associated feature mapping at each point x \u2208 R called \u03c6x : R \u2192 FK where \u03c6x(.", "startOffset": 67, "endOffset": 1646}, {"referenceID": 15, "context": "is some empirical loss function on the data and 12\u2016f\u2016K is an increasing function of \u2016f\u2016K , the Representer Theorem (Sch\u00f6lkopf et al. 2001) implies that the minimizer of the above function lies in the span of \u03c6xis (also the span of the yi\u03c6\u0303xis).", "startOffset": 115, "endOffset": 138}, {"referenceID": 10, "context": "i pi log pi + log n (12) is 1-strongly convex with respect to the l1-norm (Nesterov 2005).", "startOffset": 74, "endOffset": 89}, {"referenceID": 10, "context": "We provide a concise, self-contained and unified proof by induction in the Appendix for Lemma 5 and Lemma 8, borrowing ideas from Nesterov\u2019s excessive gap technique (Nesterov 2005) for smooth minimization of structured non-smooth functions.", "startOffset": 165, "endOffset": 180}], "year": 2015, "abstractText": "We focus on the problem of finding a non-linear classification function that lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of view (finding a perfect separator when one exists) and the dual point of view (giving a certificate of non-existence), with special focus on generalizations of two classical schemes the Perceptron (primal) and Von-Neumann (dual) algorithms. We cast our problem as one of maximizing the regularized normalized hard-margin (\u03c1) in an RKHS and rephrase it in terms of a Mahalanobis dot-product/semi-norm associated with the kernel\u2019s (normalized and signed) Gram matrix. We derive an accelerated smoothed algorithm with a convergence rate of \u221a log n \u03c1 given n separable points, which is strikingly similar to the classical kernelized Perceptron algorithm whose rate is 1 \u03c12 . When no such classifier exists, we prove a version of Gordan\u2019s separation theorem for RKHSs, and give a reinterpretation of negative margins. This allows us to give guarantees for a primal-dual algorithm that halts in min{ \u221a n |\u03c1| , \u221a n \u01eb } iterations with a perfect separator in the RKHS if the primal is feasible or a dual \u01eb-certificate of near-infeasibility.", "creator": "LaTeX with hyperref package"}}}