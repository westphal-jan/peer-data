{"id": "1506.00278", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2015", "title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering", "abstract": "in this 2000 paper, we introduce presented a new dataset collection consisting of 360, 92 001 focused geometric natural language structured descriptions tailored for 10, 738 images. during this automated dataset, the visual madlibs dataset, usually is collected using automatically produced pdf fill - a in - type the - blank visual templates designed to gather targeted speech descriptions about : people and objects, define their altered appearances, bodily activities, structural and psychological interactions, as well ultimately as inferences about the general scene meaning or represent its broader discourse context. we provide it several analyses as of the modified visual madlibs dataset and demonstrate as its applicability to realize two prominent new description generation automation tasks : focused description generation, and multiple - choice phase question - matrix answering tests for 3d images. experiments researched using joint - embedding and developing deep process learning simulation methods further show incredibly promising results on calculating these tasks.", "histories": [["v1", "Sun, 31 May 2015 19:39:44 GMT  (5935kb,D)", "http://arxiv.org/abs/1506.00278v1", "10 pages; 8 figures; 4 tables"]], "COMMENTS": "10 pages; 8 figures; 4 tables", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["licheng yu", "eunbyung park", "alexander c berg", "tamara l berg"], "accepted": false, "id": "1506.00278"}, "pdf": {"name": "1506.00278.pdf", "metadata": {"source": "CRF", "title": "Visual Madlibs: Fill in the blank Image Generation and Question Answering", "authors": ["Licheng Yu", "Eunbyung Park", "Alexander C. Berg"], "emails": ["tlberg}@cs.unc.edu"], "sections": [{"heading": "1. Introduction", "text": "Much of everyday language and discourse concerns the visual world around us, making understanding the relationship between the physical world and language describing that world an important challenge problem for AI. Understanding this complex and subtle relationship will have broad applicability toward inferring human-like understanding for images, producing natural human robot interactions, and for tasks like natural language grounding in NLP. In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8]. However, most of these methods and existing datasets have focused on only one type of description, a generic description for the entire image.\nIn this paper, we collect a new dataset of focused, targeted, descriptions, the Visual Madlibs dataset, as illustrated in Figure 1. To collect this dataset, we introduce automatically produced fill-in-the-blank templates designed to collect a range of different descriptions for visual content in an image. For example, a user might be presented with an\nimage and a fill-in-the-blank template such as \u201cThe frisbee is [blank]\u201d and asked to fill in the [blank] with a description of the appearance of frisbee. Alternatively, they could be asked to fill in the [blank] with a description of what the person is doing with the frisbee. Fill-in-the-blank questions can be targeted to collect descriptions about people and objects, their appearances, activities, and interactions, as well as descriptions of the general scene or the broader emotional, spatial, or temporal context of an image. Using these templates, we collect a large collection of 360,001\nar X\niv :1\n50 6.\n00 27\n8v 1\n[ cs\n.C V\n] 3\n1 M\nay 2\ntargeted descriptions for 10,738 images. Fig. 2 shows some Madlibs description samples.\nWith this new dataset, we can develop methods to generate more focused descriptions. Instead of asking an algorithm to \u201cdescribe the image\u201d we can now ask for more focused descriptions such as \u201cdescribe the person\u201d, \u201cdescribe what the person is doing,\u201c or \u201cdescribe the relationship between the person and the frisbee.\u201d We can also ask questions about aspects of an image that are somewhat beyond the scope of the directly depicted content. For example, \u201cdescribe what might have happened just before this picture was taken.\u201d or \u201cdescribe how this image makes you feel.\u201d These types of descriptions reach toward high-level goals of producing human-like visual interpretations for images.\nIn addition to focused description generation, we also introduce a multiple-choice question-answering task for images. In this task, the computer is provided with an image and a partial description such as \u201cThe person is [blank]\u201d. A set of possible answers is also provided, one answer that was written about the image in question, and several ad-\nditional answers written about other images. The computer is evaluated on how well it can select the correct choice. In this way, we can evaluate performance of description generation on a concrete task, making evaluation more straightforward. Varying the difficulty of the negative answers\u2014adjusting how similar they are to the correct answer\u2014provides a nuanced measurement of performance.\nFor both the generation and question-answering tasks, we study and evaluate a recent state of the art approach for image description generation [32], as well as a simple joint-embedding method learned on deep representations. The evaluation also includes extensive analysis of the Visual Madlibs dataset and comparisons to the existing MS COCO dataset of natural language descriptions for images.\nIn summary, our contributions are: 1) A new description collection strategy, Visual Madlibs, for constructing fill-in-the-blank templates to collect targeted natural language descriptions. 2) A new Visual Madlibs Dataset consisting of 360,001 targeted descriptions, spanning 12 different types of templates,\nfor 10,738 images, as well as analysis of the dataset and comparisons to existing MS COCO descriptions. 3) Evaluation of a generation method and a simple joint embedding method for targeted description generation. 4) Definition and evaluation of generation and jointembedding methods on a new task, multiple-choice fill-inthe-blank question answering for images.\nThe rest of our paper is organized as follows. First, we review related work (Sec 2). Then, we describe our strategy for automatically generating fill-in-the-blank templates and introduce our Visual Madlibs dataset (Sec 3). Next we outline the multiple-choice question answering and targeted generation tasks (Sec 4) and provide several analyses of our dataset (Sec 5). Finally, we provide experiments evaluating description generation and joint-embedding methods on the proposed tasks (Sec 6) and conclude (Sec 7)."}, {"heading": "2. Related work", "text": "Description Generation: Recently, there has been an explosion of interest in methods for producing natural language descriptions for images or video. Early work in this area generally explored two complementary directions. The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates. The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].\nWith the advancement of deep learning for content estimation, there have been many exciting recent attempts to generate image descriptions using neural network based approaches. Some methods first detect words or phrases using Convolutional Neural Network (CNN) features, then generate and re-rank candidate sentences [9, 20]. Other approaches take a more end-to-end approach to generate output descriptions directly from images. Kiros et al. [16] learn a joint image-sentence embedding using visual CNNs and Long Short Term Memory (LSTM) networks. Similarly, several other methods have made use of CNN features and LSTM or recurrent neural networks (RNN) for generation with a variety of different architectures [32, 15, 5]. These new methods have shown great promise for image description generation under some measures (e.g. BLEU1) achieving near-human performance levels. We look at related, but more focused description generation tasks.\nDescription Datasets: Along with the development of image captioning algorithms there have been a number of datasets collected for this task. One of the first datasets collected for this problem was the UIUC Pascal Sentence data set [10] which contains 1,000 images with 5 sentences per image written by workers on Amazon Mechanical Turk. As the description problem gained popularity larger and richer\ndatasets were collected, including the Flickr8K [28] and Flickr30K [34] datasets, containing 8,000 and 30,000 images respectively. In an alternative approach, the SBU Captioned photo dataset [27] contains 1 million images with existing captions collected from Flickr. This dataset is larger, but the text tends to contain more contextual information since captions were written by the photo owners. Most recently, Microsoft released the MS COCO [21] dataset. MS COCO contains 120,000 images depicting 80 common object classes, with object segmentations and 5 turker written descriptions per image. These datasets have been one of the driving forces in improving methods for description generation, but are currently limited to a single description about the general content of an image. We make use of MS COCO data, extending the types of descriptions associated with images.\nQuestion-answering Natural language questionanswering has been a long standing goal of NLP, with commercial companies like Ask-Jeeves or Google playing a significant role in developing effective methods. Recently, embedding and deep learning methods have shown great promise for question-answering [30, 3, 4]. Lin et al. [22] take an interesting multi-modal approach to questionanswering. A multiple-choice text-based question is first constructed from 3 sentences written about an image; 2 of the sentences are used as the question, and 1 is used as the positive answer, mixed with several negative answers from sentences written about other images. The authors develop ranking methods to answer these questions and show that generating abstract images for each potential answer can improve results. Note, here the algorithms are not provided with an image as part of the question. Some recent work has started to look at the problem of question-answering for images. Malinowski et al. [23] combine computer vision and NLP in a Bayesian framework, but restrict their method to scene based questions. Geman et al. [12] design a visual Turing test to test image understanding using a series of binary questions about image content. We design more general question-answering tasks that allow us to ask a variety of different types of natural language questions about images."}, {"heading": "3. Designing and collecting Visual Madlibs", "text": "The goal of Visual Madlibs is to study targeted natural language descriptions of image content that go beyond describing which objects are in the image, and beyond generic descriptions of the whole image. The experiments in this paper begin with a dataset of images where the presence of some objects have already been labeled1. The prompts for the Madlibs-style fill-in-the-blank questions are automatically generated based on image content, in a\n1More generally, acquiring such labels could be included as part of collecting Madlibs.\nmanner designed to elicit more detailed descriptions of the objects, their interactions, and the broader context of the scene shown in each image.\nVisual Madlibs: Image+Instruction+Prompts+Blank A single fill-in-the-blank question consists of a prompt and a blank, e.g., Person A is [blank] the car. The implicit question is, \u201cWhat goes in the blank?\u201d This is presented to a person along with an image and instructions, e.g., Describe the relationship between the indicated person and object. The same image and prompt may be used with different instructions to collect a variety of description types. Instantiating Questions While the general form of the questions for the Visual Madlibs were chosen by hand, see Table 1, most of the questions are instantiated depending on a subset of the objects present in an image. For instance, if an image contained two people and a dog, questions about each person (question types 9-11 in Table 1), the dog (types 6-8), relationships between the two people and the dog (type 12), could be instantiated. For each possible instantiation, the wording of the questions might alter slightly to maintain grammatical consistency. In addition to these types of questions that depend on the objects present in the image, other questions (types 1-5) can be instantiated for an image regardless of the objects present.\nNotice in particular the questions about the temporal context \u2013 what might have happened before or what might happen after the image was taken. People can make inferences beyond the specific content depicted in an image. Sometimes these inferences will be consistent between people (e.g., when what will happen next is obvious), and other times these descriptions may be less consistent. We can use the variability of returned responses to select images for which these inferences are reliable.\nAsking questions about every object and all pairs of objects quickly becomes unwieldy as the number of objects increases. To combat this, we choose a subset of objects\npresent to use in instantiating questions. Such selection could be driven by a number of factors. The experiments in this paper consider comparisons to existing, general, descriptions of images, so we instantiate questions about the objects mentioned in those existing natural language descriptions. Whether an object is mentioned in an image description can be viewed as an indication of the object\u2019s importance [2]."}, {"heading": "3.1. Data Collection", "text": "To collect the Visual Madlibs Dataset we use a subset of 10,738 human-centric images from MS COCO, that make up about a quarter of the validation data [21], and instantiate fill-in-the-blank templates as described above. The MS COCO images are annotated with a list of objects present in the images, segmentations for the locations of those objects, and 5 general natural language descriptions of the image. To select the subset of images for collecting Madlibs, we start with the 19,338 images with a person labeled. We then look at the five descriptions for each and perform a dependency parse [7], only keeping those images where a word referring to a person (woman, man, etc. E.g., in Fig. 3, guys, men) is the head noun for part of the parse. This leaves 14,150 images. We then filter out the images whose descriptions do not include a synonym for any of the 79 non-person object categories labeled in the MS COCO dataset. This leaves 10,738 human-centric images with at least one other object from the MS COCO data set mentioned in the general image descriptions.\nBefore final instantiation of the fill-in-the blank templates, we need to resolve a potential ambiguity regarding which objects are referred to in the descriptions. There could be several different people or different instances of an object type labeled in an image. It is not immediately obvious which ones are described in the sentences. To address this assignment problem, we estimate the quantity of each described person/object in the sentence by parsing the determinant (two men and a frisbee in Fig. 3), the conjunction (a man and a woman), and the singular/plural form (dog,\ndogs). We compare this number with the number of annotated instances for each category, and consider two possible cases: 1) there are fewer annotated instances than the sentences describe, 2) there are more annotated instances than the sentences describe. It is easy to address the first case, just construct templates for all of the labeled instances. For the second case, we sort the area of each segmented instance, and pick the largest ones up to the parsed number for instantiation. Using this procedure, we obtain 26,148 labeled object or person instances in the 10,738 images.\nEach Visual Madlib is answered by 3 workers on Amazon\u2019s Mechanical Turk. To date, we have collected 360,001 answers to Madlib questions. Some example Madlibs answers are shown in Fig. 2,"}, {"heading": "4. Tasks: Multiple-choice question answering", "text": "and targeted generation\nWe design two tasks to evaluate targeted natural language description for images. The first task is to automatically generate natural language descriptions of images to fill in the blank for one of the Madlibs questions. This allows for producing targeted descriptions such as: a description specifically focused on the appearance of an object, or a description about the relationship between two objects. The input to this task is an image, instructions, and a Madlibs prompt. As has been discussed at length in the community working on description generation for images, it can be difficult to evaluate free form generation. Our second task tries to address this issue by developing a new targeted multiple-choice question answering task for images. Here the input is again an image, instruction, and a prompt, but instead of a free form text answer, there are a fixed set of multiple-choice answers to fill in the blank. The possible multiple-choice answers are sampled from the Madlibs responses, one that was written for the particular image/instruction/prompt as the correct answer, and distractors chosen from either similar images or random images depending on the level of difficulty desired. This ability to choose distractors to adjust the difficulty of the question as well as the relative ease of evaluating multiple choice answers are attractive aspects of this new task.\nIn our experiments we randomly select 20% of the 10,738 images to use as our test set for evaluating these tasks. For the multiple-choice questions we form two sets of answers for each, with one set designed to be more difficult than the other. We first establish the easy task distractor answers by randomly choosing three descriptions (of the same question type) from other images [22]. The hard task is designed more delicately. Instead of randomly choosing from the other images, we now only look for those containing the same objects as our question image, and then arbitrarily pick three of their descriptions. Sometimes, the descriptions sampled from \u201csimilar\u201d images could also be good answers for our questions (later we experiment with using Turkers to select less ambiguous multiple-choice questions from this set). For the targeted generation task, for question types 1-5, algorithms generate descriptions given the image, instructions, and prompt. For the other question types whose prompts are related to some specific person or object, we additionally provide the algorithm with the location of each person/object mentioned in the prompt. We also experiment with estimating these locations using object detectors."}, {"heading": "5. Analyzing the Visual Madlibs Dataset", "text": "We begin by conducting quantitative analyses of the responses collected in the Visual Madlibs Dataset in Sec. 5.1. A main goal is understanding what additional information is provided by the targeted descriptions in the Visual Madlibs Dataset vs general image descriptions. The MS COCO dataset [21] collects general image descriptions following a similar methodology to previous efforts for collecting general image descriptions, e.g. [28, 34]. So, we provide further analyses comparing the Visual Madlibs to the MS COCO descriptions collected for the same images in Sec. 5.2"}, {"heading": "5.1. Quantifying Visual Madlibs responses", "text": "We analyze the length, structure, and consistency of the Visual Madlibs responses. First, the average length of each type of description is shown in the far right column of Table 1. Note that descriptions of people tend to be longer than descriptions of other objects in the dataset2.\nSecond, we use the phrase chunking [6] to analyze which phrasal structures are commonly used to fill in the blanks for different questions. Fig. 4, top row, shows relative frequencies for the top-5 most frequent templates used for several question types. Object attributes are usually described briefly with a simple adjectival phrase. On the other hand, people use more words and a wider variety of structure to describe possible future events. Except for future and past descriptions, the distribution of structures is generally concentrated on a few likely choices for each question type.\n2Also note that the length of the prompts varies slightly depending on the object names used to instantiate the Madlib, hence the fractional values in the mean length of the prompts shown in gray.\nPr : N\nP VP\nN P\nPr : N\nP VP\nPr : N\nP VP\nP P\nNP\nPr : N\nP VP\nA DV\nP\nPr : N\nP VP\nP RT\nN P\n0\n20%\n40%\n60%\n80%\n100% One or two seconds after this\npicture was taken, ___ .\nPr:= NP PP NP VP O ___ O\nImage's future\nPr : V\nP AD\nJP\nPr : V\nP NP\nPr : V\nP\nPr : V\nP PP\nN P\nPr : V\nP AD\nVP 0\n20%\n40%\n60%\n80%\n100%\nThe object(s) is/are [blank] .\nPr:= NP ___ O\nObject's attribute\nPr : V\nP\nPr : V\nP PP Pr : V P NP P P Pr : V P NP N P\nPr : V\nP PP\nN P\nPP 0\n20%\n40%\n60%\n80%\n100%\nPeople could ___ the object(s) .\nPr:= NP ___ NP O\nObject's affordance\nPr : V\nP NP\nPr : V\nP\nPr V\nP PP\nN P\nPr : V\nP NP\nP P\nNP\nPr : V\nP AD\nVP 0\n20%\n40%\n60%\n80%\n100%\nThe person/people is/are ___ .\nPr:= NP ___ O\nPerson's activity\n0 0.2 0.4 0.6 0.8 1.0 0\n5%\n10%\n15%\n20%\n25% Image's future\n0 0.2 0.4 0.6 0.8 1.0 0\n5%\n10%\n15%\n20%\n25% Object's attribute\n0 0.2 0.4 0.6 0.8 1.0 0\n5%\n10%\n15%\n20%\n25% Object's affordance\n0 0.2 0.4 0.6 0.8 1.0 0\n5%\n10%\n15%\n20%\n25% Person's activity\nFigure 4: First row shows top-5 most frequent phrase templates for image\u2019s future, object\u2019s attribute, object\u2019s affordance and person\u2019s activity. Second row shows the histograms of similarity between answers.\nThird, we analyze how consistent the Mechanical Turk workers\u2019 answers are for each type of question. To compute a measure of similarity between a pair of responses we use the cosine similarity between representations of each response. A response is represented by the mean of the Word2Vec [25] vectors for each word in the response, following [22, 20]. Word2Vec is a 300 dimensional embedding representation for words that encodes the distributional context of words learned over very large word corpora. This measure takes into account the actual words used in a response, as opposed to the previous analyses of parse structure. Each Visual Madlibs question is answered by three workers, providing 3 pairs for which similarity is computed. Fig. 4, bottom row, shows a histogram of all pairwise similarities for several question types. Generally the similarities have a normal-like distribution with an extra peak around 1 indicating the fraction of responses that agree almost perfectly. Once again, descriptions of the future and past are least likely to be (near) identical, while object attributes and affordances are often very consistent."}, {"heading": "5.2. Visual Madlibs vs general descriptions", "text": "We compare the targeted descriptions in the Visual Madlibs Dataset to the general image descriptions in MS COCO. First, we analyze the words used in Visual Madlibs compared to MS COCO descriptions of the same images. For each image, we extract the unique set of words from all descriptions of that image from both datasets, and compute the coverage of each set with respect to the other. We find that on average (across images) 22.45% of the Madlibs\u2019s words are also present in MSCOCO descriptions, while 52.38% of the COCO words are also present in Madlibs.\nSecond, we compare how Madlibs and MS COCO answers describe the people and objects in images. We ob-\nserve that the Madlibs questions types, Table 1, cover much of the information in MS COCO descriptions [20]. As one way to see this, we run the StanfordNLP parser3 on both datasets. For attributes of people, we use the parsing template shown in Fig. 5(a) to analyze the structures being used.\n3http://nlp.stanford.edu/software/lex-parser. shtml\nThe refer name indicates whether the person was mentioned in the description. Note that the Madlibs descriptions always have one reference to a person in the prompt (The person is [blank].). Therefore, for Madlibs, we report the presence of additional references to the person (e.g., the person is a man). The general attribute directly describes the appearance of the person or object (e.g., old or small); the affiliate object indicates whether additional objects are used to describe the targeted person (e.g. with a bag, coat, or glasses) and the affiliate attribute are appearance characteristics of those secondary objects (e.g., red coat). The templates for object\u2019s attribute and verbs are more straightforward as shown in Fig. 5(b)(c). The table in Fig. 5 shows the frequency of each parse component. Overall, more of the potential descriptive elements in these constructions are used in response to the Madlibs prompts than in the general descriptions found in MS COCO.\nWe also break down the overlap between Visual Madlibs and MS COCO descriptions over different parsing templates for descriptions about people and object (Fig. 6). Yellow bars show how often words for each parse type in MSCOCO descriptions were also found in the same parse type in the Visual Madlibs answers, and green bars measure the reverse direction. Observations indicate that Madlibs provides more coverage in its descriptions than MS COCO for all templates except for person\u2019s refer name. One possible reason is that the prompts already indicates \u201cthe person\u201d or \u201cpeople\u201d explicitly, so workers need not add an additional reference to the person in their descriptions.\nExtrinsic comparison of Visual Madlibs Data and general descriptions: Here we provide an extrinsic analysis of the information available in the general descriptions compared to Visual Madlibs. We perform this analysis by using either: a) the MS COCO descriptions for an image, or b) Visual Madlibs responses from other Turkers for an image, to select answers for our multiple-choice evaluation task. Specifically, we use one of the human provided descriptions, either from Madlibs or from MS COCO, and select the multiple-choice answer that is most similar to that description. Similarity is measured as cosine similarity between the mean Word2Vec vectors for the words a description compared to the Word2Vec vectors of the multiplechoice answers. In addition to comparing how well the Madlibs or MS COCO descriptions can select the correct multiple-choice answer, we also use the descriptions automatically produced by a recent natural language generation system (CNN+LSTM [32], implementation from [15]) trained on MS COCO dataset. This allows us to make one possible measurement of how close current automatically generated image descriptions are to our Madlibs descriptions. Fig. 7 shows the accuracies resulting from using Madlibs, MSCOCO, or CNN+LSTM [32] to select the correct multiple-choice answer.\nAlthough this approach is quite simple, it allows us we make two interesting observations. First, Madlibs outperforms MS COCO on all types of multiple-choice questions. If Madlibs and MS COCO descriptions provided the same information, we would expect their performance to be comparable. Presumably the performance increase for Madlibs is due to the coverage of targeted descriptions compared to MS COCO\u2019s sentences that describe the overall image content more generally. Second, the automatically generated descriptions from the pre-trained CNN+LSTM perform much worse than the actual MS COCO descriptions, despite doing quite well on general image description generation (The BLEU-1 score of CNN+LSTM, 0.67, is near human agreement 0.69 on MS COCO [32])."}, {"heading": "6. Experiments", "text": "In this section we evaluate a series of methods on the Visual Madlibs Dataset for the targeted natural language generation and multiple-choice question answering tasks, introduced in Sec. 4. As methods, we evaluate simple jointembedding methods \u2013 canonical correlation analysis (CCA) and normalized CCA (nCCA) [14] \u2013 as well as a recent deep-learning based method for image description generation \u2013 CNN+LSTM [32]. We train these models on 80% of the images in the MadLibs collection and evaluate their performance on the remaining 20%.\nIn our experiments we extract image features using the VGG Convolutional Neural Network (CNN) [29]. This model has been trained on the ILSVRC-2012 dataset to rec-\nognize images depicting 1000 object classes, and generates a 4,096 dimensional image representation. On the sentence\nside, we average the Word2Vec of all words in a sentence to obtain a 300 dimensional representation.\nCCA is an approach for finding a joint embedding between two multi-dimensional variables, in our case image and text vector representations. In an attempt to increase the flexibility of the feature selection and for improving computational efficiency, Gong et al. [14] proposed a scalable approximation scheme of explicit kernel mapping followed by dimension reduction and linear CCA. In the projected latent space, the similarity is measured by the eigenvalueweighted normalized correlation. This method, nCCA, provides high-quality retrieval results, improving over the original CCA performance significantly [14].\nWe train CCA and nCCA models for each question type separately using the training portion of the Visual Madlibs Dataset. These models allow us to map from an image representation, to the joint-embedding space, to vectors in the Word2Vec space, and vice versa. For targeted generation, we map an image to the joint-embedding space and then choose the answer from the training set text that is closest to this embedded point. In order to answer a multiple-choice question we embed each multiple choice answer, and then select the answer who\u2019s embedding is closest to image.\nFollowing the recent \u201cShow and Tell\u201d description generation technique [32] (using an implementation from [15]), we train a CNN+LSTM model for each question type on the Visual Madlibs training set. This approach has demonstrated state of the art performance on generating general natural language descriptions for images. These models directly learn a mapping from an image to a sequence of words which we can use to evaluate the targeted generation task. Note that we input the words from the prompt, e.g., The chair is, and then let the CNN+LSTM system generate the remaining words of the description4. For the multiple choice task, we compute cosine similarity between Word2Vec representations of the generated description and each question answer and select the most similar answer."}, {"heading": "6.1. Discussion of results", "text": "Table 2 shows accuracies of each algorithm on the easy and hard versions of the multiple-choice task. Fig. 8, shows example correct and wrong answer choices. There are several interesting observations we can make. First, training nCCA on all types of question together, labeled as nCCA(all), is helpful for the easy variant of the task, however it is less useful on the \u201cfine-grained\u201d hard version of the task. Second, extracting visual features from the bounding box of the relevant person/object yields higher accuracy for predicting attributes, but not for other questions. Based on this finding, we try answering the attribute question using automatic detection methods. The detectors are trained on\n4The missing entries for questions 7 and 12 are due to this priming failing for a fraction of the questions.\nImageNet using R-CNN [13], covering 42 MS COCO categories. We observe similar performance between groundtruth and detected bounding boxes in Table 3.\nAs an additional experiment we ask humans to answer the multiple choice task, with 5 Turkers answering each question. We use their results to filter out a subset of the hard multiple-choice questions where at least 3 Turkers choose the correct answer. Results of the methods on this subset are shown in Table 2 bottom set of rows. These results show the same pattern as on the unfiltered set, with slightly higher accuracy.\nTable 4 shows BLEU-1 and BLEU-2 scores for targeted generation. Although the CNN+LSTM models we trained on Madlibs were not quite as accurate as nCCA for selecting the correct multiple-choice answer, they did result in better, sometimes much better, accuracy (as measured by BLEU scores) for targeted generation."}, {"heading": "7. Conclusions", "text": "We have introduced a new fill-in-the blank strategy for targeted natural language descriptions and used this to collect a Visual Madlibs dataset. Our analyses show that these descriptions are usually more detailed than generic whole image descriptions. We also introduce a targeted natural language description generation task, and a multiplechoice question answering task, then train and evaluate joint-embedding and generation models. Data produced by this paper will be publicly released upon acceptance."}, {"heading": "Acknowledgement", "text": "We thank the vision and language community for feedback regarding this dataset, especially Julia Hockenmaier, Kate Saenko, and Jason Corso. This research is supported by NSF Awards #1417991, 1405822, 144234, and 1452851, and Microsoft Research."}], "references": [{"title": "Generating image descriptions using dependency relational patterns", "author": ["A. Aker", "R. Gaizauskas"], "venue": "ACL", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Understanding and predicting importance in images", "author": ["A.C. Berg", "T.L. Berg", "H.D. III", "J. Dodge", "A. Goyal", "X. Han", "A. Mensch", "M. Mitchell", "A. Sood", "K. Stratos", "K. Yamaguchi"], "venue": "CVPR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Question answering with subgraph embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1406.3676", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Open question answering with weakly supervised embedding models", "author": ["A. Bordes", "J. Weston", "N. Usunier"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1411.5654", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["M.-C. De Marneffe", "B. MacCartney", "C.D. Manning"], "venue": "Generating typed dependency parses from phrase structure parses. In Proceedings of LREC", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "arXiv preprint arXiv:1411.4389", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt"], "venue": "From captions to visual concepts and back. arXiv preprint arXiv:1411.4952", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "In ECCV", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Topic models for image annotation and text illustration", "author": ["Y. Feng", "M. Lapata"], "venue": "ACL", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A multi-view embedding space for modeling internet images", "author": ["Y. Gong", "Q. Ke", "M. Isard", "S. Lazebnik"], "venue": "tags, and their semantics. IJCV", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "arXiv preprint arXiv:1412.2306", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating natural-language video descriptions using text-mined knowledge", "author": ["N. Krishnamoorthy", "G. Malkarnenkar", "R.J. Mooney", "K. Saenko", "S. Guadarrama"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["G. Kulkarni", "V. Premraj", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "ACL", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Phrase-based image captioning", "author": ["R. Lebret", "P.O. Pinheiro", "R. Collobert"], "venue": "CoRR, abs/1502.03671", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft COCO: common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "CoRR, abs/1405.0312", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Don\u2019t just listen", "author": ["X. Lin", "D. Parikh"], "venue": "use your imagination: Leveraging visual common sense for non-visual tasks. arXiv preprint arXiv:1502.06108", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain-independent captioning of domainspecific images", "author": ["R. Mason"], "venue": "HLT-NAACL", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "EACL", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez", "G. Kulkarni", "T.L. Berg"], "venue": "NIPS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk, pages 139\u2013147. Association for Computational Linguistics", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Weakly supervised memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "arXiv preprint arXiv:1503.08895", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "arXiv preprint arXiv:1412.4729", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "arXiv preprint arXiv:1411.4555", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Corpus-guided sentence generation of natural images", "author": ["Y. Yang", "C.L. Teo", "H. Daum\u00e9 III", "Y. Aloimonos"], "venue": "EMNLP", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 8, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 14, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 31, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 15, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 19, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 212, "endOffset": 234}, {"referenceID": 30, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 245, "endOffset": 252}, {"referenceID": 7, "context": "In computer vision, along with improvements in deep learning based visual recognition, there has been an explosion of recent interest in methods to automatically generate natural language descriptions for images [5, 9, 15, 32, 16, 20] or videos [31, 8].", "startOffset": 245, "endOffset": 252}, {"referenceID": 31, "context": "For both the generation and question-answering tasks, we study and evaluate a recent state of the art approach for image description generation [32], as well as a simple joint-embedding method learned on deep representations.", "startOffset": 144, "endOffset": 148}, {"referenceID": 17, "context": "The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates.", "startOffset": 170, "endOffset": 186}, {"referenceID": 32, "context": "The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates.", "startOffset": 170, "endOffset": 186}, {"referenceID": 25, "context": "The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates.", "startOffset": 170, "endOffset": 186}, {"referenceID": 9, "context": "The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates.", "startOffset": 170, "endOffset": 186}, {"referenceID": 16, "context": "The first type of approach focused on detecting content elements such as objects, attributes, activities, or spatial relationships and then composing captions for images [18, 33, 26, 10] or videos [17] using linguistically inspired templates.", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].", "startOffset": 115, "endOffset": 122}, {"referenceID": 0, "context": "The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].", "startOffset": 115, "endOffset": 122}, {"referenceID": 26, "context": "The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].", "startOffset": 165, "endOffset": 177}, {"referenceID": 18, "context": "The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].", "startOffset": 165, "endOffset": 177}, {"referenceID": 23, "context": "The second type of approach explored methods to make use of existing text either directly associated with an image [11, 1] or retrieved from visually similar images [27, 19, 24].", "startOffset": 165, "endOffset": 177}, {"referenceID": 8, "context": "Some methods first detect words or phrases using Convolutional Neural Network (CNN) features, then generate and re-rank candidate sentences [9, 20].", "startOffset": 140, "endOffset": 147}, {"referenceID": 19, "context": "Some methods first detect words or phrases using Convolutional Neural Network (CNN) features, then generate and re-rank candidate sentences [9, 20].", "startOffset": 140, "endOffset": 147}, {"referenceID": 15, "context": "[16] learn a joint image-sentence embedding using visual CNNs and Long Short Term Memory (LSTM) networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Similarly, several other methods have made use of CNN features and LSTM or recurrent neural networks (RNN) for generation with a variety of different architectures [32, 15, 5].", "startOffset": 164, "endOffset": 175}, {"referenceID": 14, "context": "Similarly, several other methods have made use of CNN features and LSTM or recurrent neural networks (RNN) for generation with a variety of different architectures [32, 15, 5].", "startOffset": 164, "endOffset": 175}, {"referenceID": 4, "context": "Similarly, several other methods have made use of CNN features and LSTM or recurrent neural networks (RNN) for generation with a variety of different architectures [32, 15, 5].", "startOffset": 164, "endOffset": 175}, {"referenceID": 9, "context": "One of the first datasets collected for this problem was the UIUC Pascal Sentence data set [10] which contains 1,000 images with 5 sentences per image written by workers on Amazon Mechanical Turk.", "startOffset": 91, "endOffset": 95}, {"referenceID": 27, "context": "As the description problem gained popularity larger and richer datasets were collected, including the Flickr8K [28] and Flickr30K [34] datasets, containing 8,000 and 30,000 images respectively.", "startOffset": 111, "endOffset": 115}, {"referenceID": 33, "context": "As the description problem gained popularity larger and richer datasets were collected, including the Flickr8K [28] and Flickr30K [34] datasets, containing 8,000 and 30,000 images respectively.", "startOffset": 130, "endOffset": 134}, {"referenceID": 26, "context": "In an alternative approach, the SBU Captioned photo dataset [27] contains 1 million images with existing captions collected from Flickr.", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "Most recently, Microsoft released the MS COCO [21] dataset.", "startOffset": 46, "endOffset": 50}, {"referenceID": 29, "context": "Recently, embedding and deep learning methods have shown great promise for question-answering [30, 3, 4].", "startOffset": 94, "endOffset": 104}, {"referenceID": 2, "context": "Recently, embedding and deep learning methods have shown great promise for question-answering [30, 3, 4].", "startOffset": 94, "endOffset": 104}, {"referenceID": 3, "context": "Recently, embedding and deep learning methods have shown great promise for question-answering [30, 3, 4].", "startOffset": 94, "endOffset": 104}, {"referenceID": 21, "context": "[22] take an interesting multi-modal approach to questionanswering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] combine computer vision and NLP in a Bayesian framework, but restrict their method to scene based questions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] design a visual Turing test to test image understanding using a series of binary questions about image content.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Whether an object is mentioned in an image description can be viewed as an indication of the object\u2019s importance [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 20, "context": "To collect the Visual Madlibs Dataset we use a subset of 10,738 human-centric images from MS COCO, that make up about a quarter of the validation data [21], and instantiate fill-in-the-blank templates as described above.", "startOffset": 151, "endOffset": 155}, {"referenceID": 6, "context": "We then look at the five descriptions for each and perform a dependency parse [7], only keeping those images where a word referring to a person (woman, man, etc.", "startOffset": 78, "endOffset": 81}, {"referenceID": 21, "context": "We first establish the easy task distractor answers by randomly choosing three descriptions (of the same question type) from other images [22].", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "The MS COCO dataset [21] collects general image descriptions following a similar methodology to previous efforts for collecting general image descriptions, e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "[28, 34].", "startOffset": 0, "endOffset": 8}, {"referenceID": 33, "context": "[28, 34].", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "Second, we use the phrase chunking [6] to analyze which phrasal structures are commonly used to fill in the blanks for different questions.", "startOffset": 35, "endOffset": 38}, {"referenceID": 24, "context": "A response is represented by the mean of the Word2Vec [25] vectors for each word in the response, following [22, 20].", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "A response is represented by the mean of the Word2Vec [25] vectors for each word in the response, following [22, 20].", "startOffset": 108, "endOffset": 116}, {"referenceID": 19, "context": "A response is represented by the mean of the Word2Vec [25] vectors for each word in the response, following [22, 20].", "startOffset": 108, "endOffset": 116}, {"referenceID": 19, "context": "serve that the Madlibs questions types, Table 1, cover much of the information in MS COCO descriptions [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 31, "context": "In addition to comparing how well the Madlibs or MS COCO descriptions can select the correct multiple-choice answer, we also use the descriptions automatically produced by a recent natural language generation system (CNN+LSTM [32], implementation from [15]) trained on MS COCO dataset.", "startOffset": 226, "endOffset": 230}, {"referenceID": 14, "context": "In addition to comparing how well the Madlibs or MS COCO descriptions can select the correct multiple-choice answer, we also use the descriptions automatically produced by a recent natural language generation system (CNN+LSTM [32], implementation from [15]) trained on MS COCO dataset.", "startOffset": 252, "endOffset": 256}, {"referenceID": 31, "context": "7 shows the accuracies resulting from using Madlibs, MSCOCO, or CNN+LSTM [32] to select the correct multiple-choice answer.", "startOffset": 73, "endOffset": 77}, {"referenceID": 31, "context": "Figure 7: The accuracy of Madlibs, MS COCO and CNN+LSTM [32](trained on MS COCO) used as references to answer the Madlibs hard multiple-choice questions.", "startOffset": 56, "endOffset": 60}, {"referenceID": 31, "context": "69 on MS COCO [32]).", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "As methods, we evaluate simple jointembedding methods \u2013 canonical correlation analysis (CCA) and normalized CCA (nCCA) [14] \u2013 as well as a recent deep-learning based method for image description generation \u2013 CNN+LSTM [32].", "startOffset": 119, "endOffset": 123}, {"referenceID": 31, "context": "As methods, we evaluate simple jointembedding methods \u2013 canonical correlation analysis (CCA) and normalized CCA (nCCA) [14] \u2013 as well as a recent deep-learning based method for image description generation \u2013 CNN+LSTM [32].", "startOffset": 217, "endOffset": 221}, {"referenceID": 28, "context": "In our experiments we extract image features using the VGG Convolutional Neural Network (CNN) [29].", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "[14] proposed a scalable approximation scheme of explicit kernel mapping followed by dimension reduction and linear CCA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "This method, nCCA, provides high-quality retrieval results, improving over the original CCA performance significantly [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 31, "context": "Following the recent \u201cShow and Tell\u201d description generation technique [32] (using an implementation from [15]), we train a CNN+LSTM model for each question type on the Visual Madlibs training set.", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "Following the recent \u201cShow and Tell\u201d description generation technique [32] (using an implementation from [15]), we train a CNN+LSTM model for each question type on the Visual Madlibs training set.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "ImageNet using R-CNN [13], covering 42 MS COCO categories.", "startOffset": 21, "endOffset": 25}], "year": 2015, "abstractText": "In this paper, we introduce a new dataset consisting of 360,001 focused natural language descriptions for 10,738 images. This dataset, the Visual Madlibs dataset, is collected using automatically produced fill-in-the-blank templates designed to gather targeted descriptions about: people and objects, their appearances, activities, and interactions, as well as inferences about the general scene or its broader context. We provide several analyses of the Visual Madlibs dataset and demonstrate its applicability to two new description generation tasks: focused description generation, and multiple-choice question-answering for images. Experiments using joint-embedding and deep learning methods show promising results on these tasks.", "creator": "LaTeX with hyperref package"}}}