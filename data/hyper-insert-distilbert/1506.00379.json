{"id": "1506.00379", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2015", "title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "abstract": "representation learning technique of knowledge matrix bases ( kbs ) aims out to embed both entities internally and relations protocols into a uniquely low - security dimensional application space. most yet existing development methods only potentially consider direct relations in representation learning. we tentatively argue that multiple - step relation evaluation paths also contain numerous rich key inference patterns interacting between larger entities, and propose a path - source based representation learning model. this common model considers classical relation paths arising as translations vectors between all entities for representation theory learning, and addresses illustrates two key challenges : ( 1 ) not since not consistently all relation paths specification are yet reliable, typically we consistently design that a path - constraint resource similarity allocation algorithm to successfully measure directly the reliability of relation paths. ( 2 ) whether we represent privileged relation paths via detailed semantic composition of structured relation embeddings. experimental results on real - world datasets show arguments that, as favorably compared with consensus baselines, our model achieves three significant and consistent accuracy improvements placed on knowledge base completion principles and ensures relation extraction from document text.", "histories": [["v1", "Mon, 1 Jun 2015 08:22:49 GMT  (164kb,D)", "https://arxiv.org/abs/1506.00379v1", "10 pages"], ["v2", "Sat, 15 Aug 2015 09:28:49 GMT  (166kb,D)", "http://arxiv.org/abs/1506.00379v2", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yankai lin", "zhiyuan liu", "huan-bo luan", "maosong sun", "siwei rao", "song liu"], "accepted": true, "id": "1506.00379"}, "pdf": {"name": "1506.00379.pdf", "metadata": {"source": "CRF", "title": "Modeling Relation Paths for Representation Learning of Knowledge Bases", "authors": ["Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu"], "emails": ["(liuzy@tsinghua.edu.cn)"], "sections": [{"heading": "1 Introduction", "text": "People have recently built many large-scale knowledge bases (KBs) such as Freebase, DBpedia and YAGO. These KBs consist of facts about the real world, mostly in the form of triples, e.g., (Steve Jobs, FounderOf, Apple Inc.). KBs are important resources for many applications such as question answering and Web search. Although typical KBs are large in size, usually containing thousands of relation types, millions of entities and billions of facts (triples), they are far from\n\u2217Corresponding author: Z. Liu (liuzy@tsinghua.edu.cn)\ncomplete. Hence, many efforts have been invested in relation extraction to enrich KBs.\nRecent studies reveal that, neural-based representation learning methods are scalable and effective to encode relational knowledge with lowdimensional representations of both entities and relations, which can be further used to extract unknown relational facts. TransE (Bordes et al., 2013) is a typical method in the neural-based approach, which learns vectors (i.e., embeddings) for both entities and relations. The basic idea behind TransE is that, the relationship between two entities corresponds to a translation between the embeddings of the entities, that is, h + r \u2248 t when the triple (h, r, t) holds. Since TransE has issues when modeling 1-to-N, N-to-1 and N-to-N relations, various methods such as TransH (Wang et al., 2014) and TransR (Lin et al., 2015) are proposed to assign an entity with different representations when involved in various relations.\nDespite their success in modeling relational facts, TransE and its extensions only take direct relations between entities into consideration. It is known that there are also substantial multiple-step relation paths between entities indicating their semantic relationships. The relation paths reflect complicated inference patterns among relations in KBs. For example, the relation path h BornInCity\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 e1\nCityInState\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 e2\nStateInCountry\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 t indicates the relation Nationality between h and t, i.e., (h, Nationality, t).\nIn this paper, we aim at extending TransE to model relation paths for representation learning of KBs, and propose path-based TransE (PTransE). In PTransE, in addition to direct connected relational facts, we also build triples from KBs using entity pairs connected with relation paths. As shown in Figure 1, TransE only considers direct relations between entities, e.g., h r\u2212\u2192 t, builds a triple (h, r, t), and optimizes the objec-\nar X\niv :1\n50 6.\n00 37\n9v 2\n[ cs\n.C L\n] 1\n5 A\nug 2\n01 5\ntive h + r = t. PTransE generalizes TransE by regarding multiple-step relation paths as connections between entities. Take the 2-step path h r1\u2212\u2192 e1 r2\u2212\u2192 t for example as shown in Figure 1. Besides building triples (h, r1, e1) and (e1, r2, t) for learning as in TransE, PTransE also builds a triple (h, r1 \u25e6 r2, t), and optimizes the objective h+ (r1 \u25e6 r2) = t, where \u25e6 is an operation to join the relations r1 and r2 together into a unified relation path representation.\nAs compared with TransE, PTransE takes rich relation paths in KBs for learning. There are two critical challenges that make PTransE nontrivial to learn from relation paths:\nRelation Path Reliability. Not all relation paths are meaningful and reliable for learning. For example, there is often a relation path h Friend\u2212\u2212\u2212\u2212\u2212\u2192 e1\nProfession\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 t, but actually it does not indicate any semantic relationship between h and t. Hence, it is inappropriate to consider all relation paths in our model. In experiments, we find that those relation paths that lead to lots of possible tail entities are mostly unreliable for the entity pair. In this paper, we propose a path-constraint resource allocation algorithm to measure the reliability of relation paths. Afterwards, we select the reliable relation paths for representation learning.\nRelation Path Representation. In order to take relation paths into consideration, relation paths should also be represented in a low-dimensional space. It is straightforward that the semantic meaning of a relation path depends on all relations in this path. Given a relation path p = (r1, . . . , rl) , we will define and learn a binary operation function (\u25e6) to obtain the path embedding p by recursively composing multiple relations, i.e., p = r1 \u25e6 . . . \u25e6 rl.\nWith relation path selection and representation, PTransE learns entity and relation embeddings by\nregarding relation paths as translations between the corresponding entities. In experiments, we select a typical KB, Freebase, to build datasets and carry out evaluation on three tasks, including entity prediction, relation prediction and relation extraction from text. Experimental results show that, PTransE significantly outperforms TransE and other baseline methods on all three tasks."}, {"heading": "2 Our Model", "text": "In this section, we introduce path-based TransE (PTransE) that learns representations of entities and relations considering relation paths. In TransE and PTransE, we have entity set E and relation set R, and learn to encode both entities and relations in Rk. Given a KB represented by a set of triples S = {(h, r, t)} with each triple composed of two entities h, t \u2208 E and their relation r \u2208 R. Our model is expected to return a low energy score when the relation holds, and a high one otherwise."}, {"heading": "2.1 TransE and PTransE", "text": "For each triple (h, r, t), TransE regards the relation as a translation vector r between two entity vectors h and t. The energy function is defined as\nE(h, r, t) = ||h+ r\u2212 t||, (1)\nwhich is expected to get a low score when (h, r, t) holds, and high otherwise.\nTransE only learns from direct relations between entities but ignores multiple-step relation paths, which also contain rich inference patterns between entities. PTransE take relation paths into consideration for representation learning.\nSuppose there are multiple relation paths P (h, t) = {p1, . . . , pN} connecting two entities h and t, where relation path p = (r1, . . . , rl) indicates h r1\u2212\u2192 . . . rl\u2212\u2192 t. For each triple (h, r, t), the energy function is defined as\nG(h, r, t) = E(h, r, t) + E(h, P, t), (2)\nwhereE(h, r, t) models correlations between relations and entities with direct relation triples, as defined in Equation (1). E(h, P, t) models the inference correlations between relations with multiplestep relation path triples, which is defined as\nE(h, P, t) = 1\nZ \u2211 p\u2208P (h,t) R(p|h, t)E(h, p, t), (3)\nwhere R(p|h, t) indicates the reliability of the relation path p given the entity pair (h, t), Z =\n\u2211 p\u2208P (h,t)R(p|h, t) is a normalization factor, and E(h, p, t) is the energy function of the triple (h, p, t).\nFor the energy of each triple (h, p, t), the component R(p|h, t) concerns about relation path reliability, and E(h, p, t) concerns about relation path representation. We introduce the two components in detail as follows."}, {"heading": "2.2 Relation Path Reliability", "text": "We propose a path-constraint resource allocation (PCRA) algorithm to measure the reliability of a relation path. Resource allocation over networks was originally proposed for personalized recommendation (Zhou et al., 2007), and has been successfully used in information retrieval for measuring relatedness between two objects (Lu\u0308 and Zhou, 2011). Here we extend it to PCRA to measure the reliability of relation paths. The basic idea is, we assume that a certain amount of resource is associated with the head entity h, and will flow following the given path p. We use the resource amount that eventually flows to the tail entity t to measure the reliability of the path p as a meaningful connection between h and t.\nFormally, for a path triple (h, p, t), we compute the resource amount flowing from h to t given the path p = (r1, . . . , rl) as follows. Starting from h and following the relation path p, we can write the flowing path as S0 r1\u2212\u2192 S1 r2\u2212\u2192 . . . rl\u2212\u2192 Sl, where S0 = h and t \u2208 Sl. For any entity m \u2208 Si, we denote its direct predecessors along relation ri in Si\u22121 as Si\u22121(\u00b7,m). The resource flowing to m is defined as\nRp(m) = \u2211\nn\u2208Si\u22121(\u00b7,m)\n1\n|Si(n, \u00b7)| Rp(n), (4)\nwhere Si(n, \u00b7) is the direct successors of n \u2208 Si\u22121 following the relation ri, andRp(n) is the resource obtained from the entity n.\nFor each relation path p, we set the initial resource in h as Rp(h) = 1. By performing resource allocation recursively from h through the path p, the tail entity t eventually obtains the resource Rp(t) which indicates how much information of the head entity h can be well translated. We use Rp(t) to measure the reliability of the path p given (h, t), i.e., R(p|h, t) = Rp(t)."}, {"heading": "2.3 Relation Path Representation", "text": "Besides relation path reliability, we also need to define energy functionE(h, p, t) for the path triple\n(h, p, t) in Equation (2). Similar with the energy function of TransE in Equation (1), we will also represent the relation path p in the embedding space.\nThe semantic meaning of a relation path considerably relies on its involved relations. It is thus reasonable for us to build path embeddings via semantic composition of relation embeddings. As illustrated in Figure 2, the path embedding p is composed by the embeddings of BorninCity, CityInState and StateInCountry.\nFormally, for a path p = (r1, . . . , rl), we define a composition operation \u25e6 and obtain path embedding as p = r1 \u25e6 . . .\u25e6rl. In this paper, we consider three types of composition operation:\nAddition (ADD). The addition operation obtains the vector of a path by summing up the vectors of all relations, which is formalized as\np = r1 + . . .+ rl. (5)\nMultiplication (MUL). The multiplication operation obtains the vector of a path as the cumulative product of the vectors of all relations, which is formalized as\np = r1 \u00b7 . . . \u00b7 rl. (6)\nBoth addition and multiplication operations are simple and have been extensively investigated in semantic composition of phrases and sentences (Mitchell and Lapata, 2008).\nRecurrent Neural Network (RNN). RNN is a recent neural-based model for semantic composition (Mikolov et al., 2010). The composition operation is realized using a matrix W:\nci = f(W [ci\u22121; ri]), (7)\nwhere f is a non-linearity or identical function, and [a; b] represents the concatenation of two vec-\ntors. By setting c1 = r1 and recursively performing RNN following the relation path, we will finally obtain p = cn. RNN has also been used for representation learning of relation paths in KBs (Neelakantan et al., 2015).\nFor a multiple-step relation path triple (h, p, t), we could have followed TransE and define the energy function as E(h, p, t) = ||h + p \u2212 t||. However, since we have minimized ||h + r \u2212 t|| with the direct relation triple (h, r, t) to make sure r \u2248 t\u2212h, we may directly define the energy function of (h, p, t) as\nE(h, p, t) = ||p\u2212(t\u2212h)|| = ||p\u2212r|| = E(p, r), (8) which is expected to be a low score when the multiple-relation path p is consistent with the direct relation r, and high otherwise, without using entity embeddings."}, {"heading": "2.4 Objective Formalization", "text": "We formalize the optimization objective of PTransE as L(S) = \u2211\n(h,r,t)\u2208S\n[ L(h, r, t)+ 1\nZ \u2211 p\u2208P (h,t) R(p|h, t)L(p, r) ] .\n(9)\nFollowing TransE, L(h, r, t) and L(p, r) are margin-based loss functions with respect to the triple (h, r, t) and the pair (p, r):\nL(h, r, t) = \u2211\n(h\u2032,r\u2032,t\u2032)\u2208S\u2212 [\u03b3 + E(h, r, t)\u2212 E(h\u2032, r\u2032, t\u2032)]+,\n(10)\nand\nL(p, r) = \u2211\n(h,r\u2032,t)\u2208S\u2212 [\u03b3 + E(p, r)\u2212 E(p, r\u2032)]+, (11)\nwhere [x]+ = max(0, x) returns the maximum between 0 and x, \u03b3 is the margin, S is the set of valid triples existing in a KB and S\u2212 is the set of invalid triples. The objective will favor lower scores for valid triples as compared with invalid triples.\nThe invalid triple set with respect to (h, r, t) is defined as\nS\u2212 = {(h\u2032, r, t)}\u222a{(h, r\u2032, t)}\u222a{(h, r, t\u2032)}. (12)\nThat is, the set of invalid triples is composed of the original valid triple (h, r, t) with one of three components replaced."}, {"heading": "2.5 Optimization and Implementation Details", "text": "For optimization, we employ stochastic gradient descent (SGD) to minimize the loss function. We randomly select a valid triple from the training set iteratively for learning. In the implementation, we also enforce constraints on the norms of the embeddings h, r, t. That is, we set\n\u2016h\u20162 \u2264 1, \u2016r\u20162 \u2264 1, \u2016t\u20162 \u2264 1. \u2200h, r, t. (13)\nThere are also some implementation details that will significantly influence the performance of representation learning, which are introduced as follows.\nReverse Relation Addition. In some cases, we are interested in the reverse version of a relation, which may not be presented in KBs. For example, according to the relation path e1\nBornInCity\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 e2\nCityOfCountry\u2190\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 e3 we expect to infer the fact that (e1,Nationality, e3). In this paper, however, we only consider the relation paths following one direction. Hence, we add reverse relations for each relation in KBs. That is, for each triple (h, r, t) we build another (t, r\u22121, h). In this way, our method can consider the above-mentioned path as e1 BornInCity\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 e2 CityOfCountry\u22121\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 e3 for learning. Path Selection Limitation. There are usually large amount of relations and facts about each entity pair. It will be impractical to enumerate all possible relation paths between head and tail entities. For example, if each entity refers to more than 100 relations on average, which is common in Freebase, there will be billions of 4-step paths. Even for 2-step or 3-step paths, it will be timeconsuming to consider all of them without limitation. For computational efficiency, in this paper we restrict the length of paths to at most 3-steps and consider those relation paths with the reliability score larger than 0.01."}, {"heading": "2.6 Complexity Analysis", "text": "We denote Ne as the number of entities, Nr as the number of relations and K as the vector dimension. The model parameter size of PTransE is (NeK + NrK), which is the same as TransE. PTransE follows the optimization procedure introduced by (Bordes et al., 2013) to solve Equation (9). We denote S as the number of triples for learning, P as the expected number of relation paths between two entities, and L as the expected\nlength of relation paths. For each iteration in optimization, the complexity of TransE is O(SK) and the complexity of PTransE isO(SKPL) for ADD and MUL, and O(SK2PL) for RNN."}, {"heading": "3 Experiments and Analysis", "text": ""}, {"heading": "3.1 Data Sets and Experimental Setting", "text": "We evaluate our method on a typical large-scale KB Freebase (Bollacker et al., 2008). In this paper, we adopt two datasets extracted from Freebase, i.e., FB15K and FB40K. The statistics of the datasets are listed in Table 1.\nWe evaluate the performance of PTransE and other baselines by predicting whether testing triples hold. We consider two scenarios: (1) Knowledge base completion, aiming to predict the missing entities or relations in given triples only based on existing KBs. (2) Relation extraction from texts, aiming to extract relations between entities based on information from both plain texts and KBs."}, {"heading": "3.2 Knowledge Base Completion", "text": "The task of knowledge base completion is to complete the triple (h, r, t) when one of h, t, r is missing. The task has been used for evaluation in (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013). We conduct the evaluation on FB15K, which has 483, 142 relational triples and 1, 345 relation types, among which there are rich inference and reasoning patterns.\nIn the testing phase, for each testing triple (h, r, t), we define the following score function for prediction\nS(h, r, t) = G(h, r, t) +G(t, r\u22121, h), (14)\nand the score function G(h, r, t) is further defined as\nG(h, r, t) =||h+ r\u2212 t||+ 1\nZ \u2211 p\u2208P (h,t) Pr(r|p)R(p|h, t)||p\u2212 r||.\n(15)\nThe score function is similar to the energy function defined in Section 2.1. The difference is that,\nhere we consider the reliability of a path p is also related to the inference strength given r, which is quantified as Pr(r|p) = Pr(r, p)/Pr(p) obtained from the training data.\nWe divide the stage into two sub-tasks, i.e., entity prediction and relation prediction."}, {"heading": "3.2.1 Entity Prediction", "text": "In the sub-task of entity prediction, we follow the setting in (Bordes et al., 2013). For each testing triple with missing head or tail entity, various methods are asked to compute the scores of G(h, r, t) for all candidate entities and rank them in descending order.\nWe use two measures as our evaluation metrics: the mean of correct entity ranks and the proportion of valid entities ranked in top-10 (Hits@10). As mentioned in (Bordes et al., 2013), the measures are desirable but flawed when an invalid triple ends up being valid in KBs. For example, when the testing triple is (Obama, PresidentOf, USA) with the head entity Obama is missing, the head entity Lincoln may be regarded invalid for prediction, but in fact it is valid in KBs. The evaluation metrics will under-estimate those methods that rank these triples high. Hence, we can filter out all these valid triples in KBs before ranking. The first evaluation setting was named as \u201cRaw\u201d and the second one as \u201cFilter\u201d.\nFor comparison, we select all methods in (Bordes et al., 2013; Wang et al., 2014) as our baselines and use their reported results directly since the evaluation dataset is identical.\nIdeally, PTransE has to find all possible relation paths between the given entity and each candidate entity. However, it is time consuming and impractical, because we need to iterate all candidate entities in |E| for each testing triple. Here we adopt a re-ranking method: we first rank all candidate entities according to the scores from TransE, and then re-rank top-500 candidates according to the scores from PTransE.\nFor PTransE, we find the best hyperparameters according to the mean rank in validation set. The optimal configurations of PTransE we used are \u03bb = 0.001, \u03b3 = 1, k = 100 and taking L1 as dissimilarity. For training, we limit the number of epochs over all the training triples to 500.\nEvaluation results of entity prediction are shown in Table 2. The baselines include RESCAL (Nickel et al., 2011), SE (Bordes et al., 2011), SME (linear) (Bordes et al., 2012), SME (bilinear)\n(Bordes et al., 2012), LFM (Jenatton et al., 2012), TransE (Bordes et al., 2013) (original version and our implementation considering reverse relations), TransH (Wang et al., 2014), and TransR (Lin et al., 2015).\nFor PTransE, we consider three composition operations for relation path representation: addition (ADD), multiplication (MUL) and recurrent neural networks (RNN). We also consider relation paths with at most 2-steps and 3-steps. With the same configurations of PTransE, our TransE implementation achieves much better performance than that reported in (Bordes et al., 2013).\nFrom Table 2 we observe that: (1) PTransE significantly and consistently outperforms other baselines including TransE. It indicates that relation paths provide a good supplement for representation learning of KBs, which have been successfully encoded by PTransE. For example, since both George W. Bush and Abraham Lincoln were presidents of the United States, they exhibit similar embeddings in TransE. This may lead to confusion for TransE to predict the spouse of Laura Bush. In contrast, since PTransE models relation paths, it can take advantage of the relation paths between George W. Bush and Laura Bush, and leads to more accurate prediction. (2) For PTransE, the addition operation outperforms other composition operations in both Mean Rank and Hits@10. The reason is that, the addition operation is compatible with the learning objectives of both TransE and PTransE. Take h r1\u2212\u2192 e1\nr2\u2212\u2192 t for example. The optimization objectives of two direct relations h + r1 = e1 and e1 + r2 = t can easily derive the path objective h + r1 + r2 = t. (3) PTransE of considering relation paths with at most 2-step and 3-step achieve comparable results.\nThis indicates that it may be unnecessary to consider those relation paths that are too long.\nAs defined in (Bordes et al., 2013), relations in KBs can be divided into various types according to their mapping properties such as 1-to-1, 1-toN, N-to-1 and N-to-N. Here we demonstrate the performance of PTransE and some baselines with respect to different types of relations in Table 3. It is observed that, on all mapping types of relations, PTransE consistently achieves significant improvement as compared with TransE."}, {"heading": "3.2.2 Relation Prediction", "text": "Relation prediction aims to predict relations given two entities. We also use FB15K for evaluation. In this sub-task, we can use the score function of PTransE to rank candidate relations instead of reranking like in entity prediction.\nSince our implementation of TransE has achieved the best performance among all baselines for entity prediction, here we only compare PTransE with TransE due to limited space. Evaluation results are shown in Table 4, where we report Hits@1 instead of Hits@10 for comparison, because Hits@10 for both TransE and PTransE exceeds 95%. In this table, we report the performance of TransE without reverse relations (TransE), with reverse relations (+Rev) and considering relation paths for testing like that in PTransE (+Rev+Path). We report the performance of PTransE with only considering relation paths (- TransE), only considering the part in Equation (1) (-Path) and considering both (PTransE).\nThe optimal configurations of PTransE for relation prediction are identical to those for entity prediction: \u03bb = 0.001, \u03b3 = 1, k = 100 and taking L1 as dissimilarity.\nFrom Table 4 we observe that: (1) PTransE outperforms TransE+Rev+Path significantly for relation prediction by reducing 41.8% prediction errors. (2) Even for TransE itself, considering relation paths for testing can reduce 17.3% errors as compared with TransE+Rev. It indicates that encoding relation paths will benefit for predicting relations. (3) PTransE with only considering relation paths (PTransE-TransE) gets surprisingly high mean rank. The reason is that, not all entity pairs in testing triples have relation paths, which will lead to random guess and the expectation of rank of these entity pairs is |R|/2. Meanwhile, Hits@1 of PTransE-TransE is relatively reasonable, which indicates the worth of modeling rela-\ntion paths. As compared with TransE, the inferior of PTransE-TransE also indicates that entity representations are informative and crucial for relation prediction."}, {"heading": "3.3 Relation Extraction from Text", "text": "Relation extraction from text aims to extract relational facts from plain text to enrich existing KBs. Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these methods reason new facts only based on plain text. TransE was used to enrich a text-based model and achieved a significant improvement (Weston et al., 2013), and so do TransH (Wang et al., 2014) and TransR (Lin et al., 2015). In this task, we explore the effectiveness of PTransE for relation extraction from text.\nWe use New York Times corpus (NYT) released by (Riedel et al., 2010) as training and testing data. NYT aligns Freebase with the articles in New York Times, and extracts sentence-level features such\nas part-of-speech tags, dependency tree paths for each mention. There are 53 relations (including non-relation denoted as NA) and 121, 034 training mentions. We use FB40K as the KB, consisting all entities mentioned in NYT and 1, 336 relations.\nIn the experiments, we implemented the textbased model Sm2r presented in (Weston et al., 2013). We combine the ranking scores from the text-based model with those from KB representations to rank testing triples, and generate precision-recall curves for both TransE and PTransE. For learning of TransE and PTransE, we set the dimensions of entities/relations embeddings k = 50, the learning rate \u03bb = 0.001, the margin \u03b3 = 1.0 and dissimilarity metric as L1. We also compare with MIMLRE (Surdeanu et al., 2012) which is the state-of-art method using distant supervision. The evaluation curves are shown in Figure 3.\nFrom Figure 3 we can observe that, by combining with the text-based model Sm2r, the precision of PTransE significantly outperforms TransE especially for the top-ranked triples. This indicates that encoding relation paths is also useful for relation extraction from text.\nNote that TransE used here does not consider reverse relations and relation paths because the performance does not change much. We analyze the reason as follows. In the task of knowledge base completion, each testing triple has at least one valid relation. In contrast, many testing triples in this task correspond to non-relation (NA), and there are usually several relation paths between two entities in these non-relation triples. TransE does not encode relation paths during the training phase like PTransE, which results in worse performance for predicting non-relation when considering relation paths in the testing phase, and compensates the improvement on those triples that do have relations. This indicates it is non-trivial to encode relation paths, and also confirms the effectiveness of PTransE."}, {"heading": "3.4 Case Study of Relation Inference", "text": "We have shown that PTransE can achieve high performance for knowledge base completion and relation extraction from text. In this section, we present some examples of relation inference according to relation paths.\nAccording to the learning results of PTransE, we can find new facts from KBs. As shown in Figure 4, two entities Forrest Gump and English are connected by three relation paths, which give us more confidence to predict the relation between the two entities to LanguageOfFilm."}, {"heading": "4 Related Work", "text": "Recent years have witnessed great advances of modeling multi-relational data such as social networks and KBs. Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) . Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency. All existing representation learning methods of knowledge bases only use direct relations between entities, ignoring rich information in relation paths.\nRelation paths have already been widely considered in social networks and recommender systems. Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006). Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012). PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013). (Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations. We note that, these methods focus on modeling relation paths for relation extraction without considering any information of entities. In contrast, by successfully integrating the merits of modeling entities and relation paths, PTransE can learn superior representations of both entities and relations for knowledge graph completion and relation extraction as shown in our experiments."}, {"heading": "5 Conclusion and Future Work", "text": "This paper presents PTransE, a novel representation learning method for KBs, which encodes relation paths to embed both entities and relations\nin a low-dimensional space. To take advantages of relation paths, we propose path-constraint resource allocation to measure relation path reliability, and employ semantic composition of relations to represent paths for optimization. We evaluate PTransE on knowledge base completion and relation extraction from text. Experimental results show that PTransE achieves consistent and significant improvements as compared with TransE and other baselines.\nIn future, we will explore the following research directions: (1) This paper only considers the inference patterns between direct relations and relation paths between two entities for learning. There are much complicated patterns among relations. For example, the inference form Queen(e) Inference=====\u21d2 Female(e) cannot be handled by PTransE. We may take advantages of first-order logic to encode these inference patterns for representation learning. (2) There are some extensions for TransE, e.g., TransH and TransR. It is non-trivial for them to adopt the idea of PTransE, and we will explore to extend PTransE to these models to better deal with complicated scenarios of KBs."}, {"heading": "6 Acknowledgments", "text": "Zhiyuan Liu and Maosong Sun are supported by the 973 Program (No. 2014CB340501) and the National Natural Science Foundation of China (NSFC No. 61133012) and Tsinghua-Samsung Joint Lab. Huanbo Luan is supported by the National Natural Science Foundation of China (NSFC No. 61303075). We sincerely thank Yansong Feng for insightful discussions, and thank all anonymous reviewers for their constructive comments."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of KDD,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Jason Weston", "Ronan Collobert", "Yoshua Bengio"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Joint learning of words and meaning representations for opentext semantic parsing", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Bordes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Bordes et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "A semantic matching energy function for learning with multirelational data", "author": ["Xavier Glorot", "Jason Weston", "Yoshua Bengio"], "venue": "Machine Learning,", "citeRegEx": "Bordes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Learning new facts from knowledge bases with neural tensor networks and semantic word vectors", "author": ["Chen et al.2013] Danqi Chen", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of ICLR", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Improving learning and inference in a large knowledge-base using latent syntactic cues", "author": ["Gardner et al.2013] Matt Gardner", "Partha Pratim Talukdar", "Bryan Kisiel", "Tom M Mitchell"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Gardner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2013}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of ACL-HLT,", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "A latent factor model for highly multirelational data", "author": ["Nicolas L Roux", "Antoine Bordes", "Guillaume R Obozinski"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Jenatton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["Kemp et al.2006] Charles Kemp", "Joshua B Tenenbaum", "Thomas L Griffiths", "Takeshi Yamada", "Naonori Ueda"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Kemp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["Lao", "Cohen2010] Ni Lao", "William W Cohen"], "venue": "Machine learning,", "citeRegEx": "Lao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2010}, {"title": "Random walk inference and learning in a large scale knowledge base", "author": ["Lao et al.2011] Ni Lao", "Tom Mitchell", "William W Cohen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Lao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2011}, {"title": "Reading the web with learned syntactic-semantic inference rules", "author": ["Lao et al.2012] Ni Lao", "Amarnag Subramanya", "Fernando Pereira", "William W Cohen"], "venue": "In Proceedings of EMNLP-CoNLL,", "citeRegEx": "Lao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lao et al\\.", "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin et al.2015] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Link prediction in complex networks: A survey", "author": ["L\u00fc", "Zhou2011] Linyuan L\u00fc", "Tao Zhou"], "venue": "Physica A: Statistical Mechanics and its Applications,", "citeRegEx": "L\u00fc et al\\.,? \\Q2011\\E", "shortCiteRegEx": "L\u00fc et al\\.", "year": 2011}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Nonparametric latent feature models for link prediction", "author": ["Miller et al.2009] Kurt Miller", "Michael I Jordan", "Thomas L Griffiths"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Miller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2009}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata"], "venue": "In Proceedings of ACL,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Compositional vector space models for knowledge base inference", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": "In 2015 AAAI Spring Symposium Series", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of ICML,", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of WWW,", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Proceedings of ECML-PKDD,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Relational learning via collective matrix factorization", "author": ["Singh", "Gordon2008] Ajit P Singh", "Geoffrey J Gordon"], "venue": "In Proceedings of KDD,", "citeRegEx": "Singh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2008}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Modelling relational data using bayesian clustered tensor factorization", "author": ["Joshua B Tenenbaum", "Ruslan Salakhutdinov"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Fast random walk with restart and its applications", "author": ["Tong et al.2006] Hanghang Tong", "Christos Faloutsos", "Jia-Yu Pan"], "venue": "In Proceedings of ICDM,", "citeRegEx": "Tong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tong et al\\.", "year": 2006}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Connecting language and knowledge bases with embedding models for relation extraction", "author": ["Weston et al.2013] Jason Weston", "Antoine Bordes", "Oksana Yakhnenko", "Nicolas Usunier"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Weston et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Bipartite network projection and personal recommendation", "author": ["Zhou et al.2007] Tao Zhou", "Jie Ren", "Mat\u00fa\u0161 Medo", "Yi-Cheng Zhang"], "venue": "Physical Review E,", "citeRegEx": "Zhou et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2007}, {"title": "Max-margin nonparametric latent feature models for link prediction", "author": ["Jun Zhu"], "venue": "In Proceedings of ICML,", "citeRegEx": "Zhu.,? \\Q2012\\E", "shortCiteRegEx": "Zhu.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "TransE (Bordes et al., 2013) is a typical method in the neural-based approach, which learns vectors (i.", "startOffset": 7, "endOffset": 28}, {"referenceID": 28, "context": "Since TransE has issues when modeling 1-to-N, N-to-1 and N-to-N relations, various methods such as TransH (Wang et al., 2014) and TransR (Lin et al.", "startOffset": 106, "endOffset": 125}, {"referenceID": 13, "context": ", 2014) and TransR (Lin et al., 2015) are proposed to assign an entity with different representations when involved in various relations.", "startOffset": 19, "endOffset": 37}, {"referenceID": 30, "context": "Resource allocation over networks was originally proposed for personalized recommendation (Zhou et al., 2007), and has been successfully used in information retrieval for measuring relatedness between two objects (L\u00fc and Zhou, 2011).", "startOffset": 90, "endOffset": 109}, {"referenceID": 15, "context": "RNN is a recent neural-based model for semantic composition (Mikolov et al., 2010).", "startOffset": 60, "endOffset": 82}, {"referenceID": 19, "context": "RNN has also been used for representation learning of relation paths in KBs (Neelakantan et al., 2015).", "startOffset": 76, "endOffset": 102}, {"referenceID": 3, "context": "PTransE follows the optimization procedure introduced by (Bordes et al., 2013) to solve Equation (9).", "startOffset": 57, "endOffset": 78}, {"referenceID": 0, "context": "We evaluate our method on a typical large-scale KB Freebase (Bollacker et al., 2008).", "startOffset": 60, "endOffset": 84}, {"referenceID": 1, "context": "The task has been used for evaluation in (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013).", "startOffset": 41, "endOffset": 104}, {"referenceID": 2, "context": "The task has been used for evaluation in (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013).", "startOffset": 41, "endOffset": 104}, {"referenceID": 3, "context": "The task has been used for evaluation in (Bordes et al., 2011; Bordes et al., 2012; Bordes et al., 2013).", "startOffset": 41, "endOffset": 104}, {"referenceID": 3, "context": "In the sub-task of entity prediction, we follow the setting in (Bordes et al., 2013).", "startOffset": 63, "endOffset": 84}, {"referenceID": 3, "context": "As mentioned in (Bordes et al., 2013), the measures are desirable but flawed when an invalid triple ends up being valid in KBs.", "startOffset": 16, "endOffset": 37}, {"referenceID": 3, "context": "For comparison, we select all methods in (Bordes et al., 2013; Wang et al., 2014) as our baselines and use their reported results directly since the evaluation dataset is identical.", "startOffset": 41, "endOffset": 81}, {"referenceID": 28, "context": "For comparison, we select all methods in (Bordes et al., 2013; Wang et al., 2014) as our baselines and use their reported results directly since the evaluation dataset is identical.", "startOffset": 41, "endOffset": 81}, {"referenceID": 20, "context": "The baselines include RESCAL (Nickel et al., 2011), SE (Bordes et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 1, "context": ", 2011), SE (Bordes et al., 2011), SME (linear) (Bordes et al.", "startOffset": 12, "endOffset": 33}, {"referenceID": 2, "context": ", 2011), SME (linear) (Bordes et al., 2012), SME (bilinear)", "startOffset": 22, "endOffset": 43}, {"referenceID": 2, "context": "(Bordes et al., 2012), LFM (Jenatton et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": ", 2012), LFM (Jenatton et al., 2012), TransE (Bordes et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 3, "context": ", 2012), TransE (Bordes et al., 2013) (original version and our implementation considering reverse relations), TransH (Wang et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 28, "context": ", 2013) (original version and our implementation considering reverse relations), TransH (Wang et al., 2014), and TransR (Lin et al.", "startOffset": 88, "endOffset": 107}, {"referenceID": 13, "context": ", 2014), and TransR (Lin et al., 2015).", "startOffset": 20, "endOffset": 38}, {"referenceID": 3, "context": "With the same configurations of PTransE, our TransE implementation achieves much better performance than that reported in (Bordes et al., 2013).", "startOffset": 122, "endOffset": 143}, {"referenceID": 3, "context": "As defined in (Bordes et al., 2013), relations in KBs can be divided into various types according to their mapping properties such as 1-to-1, 1-toN, N-to-1 and N-to-N.", "startOffset": 14, "endOffset": 35}, {"referenceID": 17, "context": "Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).", "startOffset": 183, "endOffset": 270}, {"referenceID": 22, "context": "Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).", "startOffset": 183, "endOffset": 270}, {"referenceID": 7, "context": "Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).", "startOffset": 183, "endOffset": 270}, {"referenceID": 25, "context": "Many works regard large-scale KBs as distant supervision to annotate sentences as training instances and build relation classifiers according to features extracted from the sentences (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).", "startOffset": 183, "endOffset": 270}, {"referenceID": 29, "context": "TransE was used to enrich a text-based model and achieved a significant improvement (Weston et al., 2013), and so do TransH (Wang et al.", "startOffset": 84, "endOffset": 105}, {"referenceID": 28, "context": ", 2013), and so do TransH (Wang et al., 2014) and TransR (Lin et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 13, "context": ", 2014) and TransR (Lin et al., 2015).", "startOffset": 19, "endOffset": 37}, {"referenceID": 22, "context": "We use New York Times corpus (NYT) released by (Riedel et al., 2010) as training and testing data.", "startOffset": 47, "endOffset": 68}, {"referenceID": 29, "context": "In the experiments, we implemented the textbased model Sm2r presented in (Weston et al., 2013).", "startOffset": 73, "endOffset": 94}, {"referenceID": 25, "context": "We also compare with MIMLRE (Surdeanu et al., 2012) which is the state-of-art method using distant supervision.", "startOffset": 28, "endOffset": 51}, {"referenceID": 9, "context": "Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al.", "startOffset": 197, "endOffset": 272}, {"referenceID": 16, "context": "Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al.", "startOffset": 197, "endOffset": 272}, {"referenceID": 26, "context": "Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al.", "startOffset": 197, "endOffset": 272}, {"referenceID": 31, "context": "Many works cope with relational learning as a multi-relational representation learning problem, encoding both entities and relations in a low-dimensional latent space, based on Bayesian clustering (Kemp et al., 2006; Miller et al., 2009; Sutskever et al., 2009; Zhu, 2012), energy-based models (Bordes et al.", "startOffset": 197, "endOffset": 272}, {"referenceID": 1, "context": ", 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al.", "startOffset": 40, "endOffset": 143}, {"referenceID": 5, "context": ", 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al.", "startOffset": 40, "endOffset": 143}, {"referenceID": 24, "context": ", 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al.", "startOffset": 40, "endOffset": 143}, {"referenceID": 3, "context": ", 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al.", "startOffset": 40, "endOffset": 143}, {"referenceID": 4, "context": ", 2009; Zhu, 2012), energy-based models (Bordes et al., 2011; Chen et al., 2013; Socher et al., 2013; Bordes et al., 2013; Bordes et al., 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al.", "startOffset": 40, "endOffset": 143}, {"referenceID": 20, "context": ", 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) .", "startOffset": 30, "endOffset": 96}, {"referenceID": 21, "context": ", 2014), matrix factorization (Singh and Gordon, 2008; Nickel et al., 2011; Nickel et al., 2012) .", "startOffset": 30, "endOffset": 96}, {"referenceID": 3, "context": "Among existing representation models, TransE (Bordes et al., 2013) regards a relation as translation between head and tail entities for optimization, which achieves a good trade-off between prediction accuracy and computational efficiency.", "startOffset": 45, "endOffset": 66}, {"referenceID": 27, "context": "Most of these works regard each relation and path as discrete symbols, and deal with them using graph-based algorithms, such as random walks with restart (Tong et al., 2006).", "startOffset": 154, "endOffset": 173}, {"referenceID": 12, "context": "Relation paths have also been used for inference on large-scale KBs, such as Path Ranking algorithm (PRA) (Lao and Cohen, 2010), which has been adopted for expert finding (Lao and Cohen, 2010) and information retrieval (Lao et al., 2012).", "startOffset": 219, "endOffset": 237}, {"referenceID": 11, "context": "PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013).", "startOffset": 69, "endOffset": 109}, {"referenceID": 6, "context": "PRA has also been used for relation extraction based on KB structure (Lao et al., 2011; Gardner et al., 2013).", "startOffset": 69, "endOffset": 109}, {"referenceID": 19, "context": "(Neelakantan et al., 2015) further learns a recurrent neural network (RNN) to represent unseen relation paths according to involved relations.", "startOffset": 0, "endOffset": 26}], "year": 2015, "abstractText": "Representation learning of knowledge bases aims to embed both entities and relations into a low-dimensional space. Most existing methods only consider direct relations in representation learning. We argue that multiple-step relation paths also contain rich inference patterns between entities, and propose a path-based representation learning model. This model considers relation paths as translations between entities for representation learning, and addresses two key challenges: (1) Since not all relation paths are reliable, we design a path-constraint resource allocation algorithm to measure the reliability of relation paths. (2) We represent relation paths via semantic composition of relation embeddings. Experimental results on real-world datasets show that, as compared with baselines, our model achieves significant and consistent improvements on knowledge base completion and relation extraction from text. The source code of this paper can be obtained from https://github.com/mrlyk423/ relation_extraction.", "creator": "LaTeX with hyperref package"}}}