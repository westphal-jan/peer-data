{"id": "1506.00765", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Video (GIF) Sentiment Analysis using Large-Scale Mid-Level Ontology", "abstract": "with with faster net connection infrastructure speed, incoming internet users are slowly now making social network a huge reservoir made of texts, multimedia images exchanged and legacy video format clips ( gif ). ubiquitous sentiment memory analysis for such online platform solutions can likewise be clearly used to globally predict strong political elections, evaluates economic indicators and certainly so relies on. however, any gif generalized sentiment analysis mechanism is definitely quite challenging, unique not only because inevitably it hinges on spatio - dynamic temporal visual contentabstraction, but all also explanations for the relationship between such financial abstraction and whose final entity sentiment remains greatly unknown. in this paper, see we dedicated to find out whose such relationship. we proposed a sophisticated sentipairsequence basedspatiotemporal - visual sentiment platform ontology, which collectively forms the midlevel representations methodology for domain gifsentiment. for the establishment activation process of generic sentipair llc contains two steps. first, we construct the digital synset forest to define the dominant semantic tree structure associated of visual sentiment label elements. back then, internally through thesynset forest, we organically incorporate select entities and firmly combine each sentiment with label elements to form a mid - latitude level weighted visual sentiment representation. overall our evaluation experiments indicate that sentipair consequently outperforms our other proposed competing mid - level attributes. using sentipair, our perception analysis indicates frameworkcan achieve satisfying prediction data accuracy ( 72. 6 % ). we follow also firmly opened ourdataset ( gso - 2015 ) to protect the scientific research community. ^ gso - index 2015 contains theoretically more constituents than all 6, 50 000 manually annotated word gifs seeking out collections of more than nearly 40, 450 000 candidates. alternatively each individual is labeled together with coordinates both zero sentiment and continuous sentipair metadata sequence.", "histories": [["v1", "Tue, 2 Jun 2015 06:31:57 GMT  (4450kb)", "http://arxiv.org/abs/1506.00765v1", null]], "reviews": [], "SUBJECTS": "cs.MM cs.CL cs.IR", "authors": ["zheng cai", "donglin cao", "rongrong ji"], "accepted": false, "id": "1506.00765"}, "pdf": {"name": "1506.00765.pdf", "metadata": {"source": "CRF", "title": "Video (GIF) Sentiment Analysis Using Large Scale Auto Generated Mid-Level Ontology", "authors": ["Cai Zheng"], "emails": ["@stu.xmu.edu.cn", "another@xmu.edu.cn", "jirongrong@xmu.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n00 76\n5v 1\n[ cs\n.M M\n] 2\nJ un\n2 01\nCategories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Retrieval and Indexing\nKeywords Sentiment Analysis, Concept Detection, Ontology"}, {"heading": "1. INTRODUCTION", "text": "Nowadays, Internet users are making every social network (such as Facebook, Twitter, Weibo, et al.) a huge reservoir of text, images and video clips (GIF). Men and women are sharing their lives and opinions at online platforms, which gives a new direction for applications such as political election prediction, economic indicator measurement, policy feedback evaluation and so on.\nWith faster Internet connection, people are more willing to post GIF videos other than static images. According to a recent study[1] the total proportion of visual content from all shared links on Twitter is 36%. However, Industries deviate\nfrom user habit by taking social network sentiment analysis as social network text analysis. Research for GIF sentiment analysis is still in its infancy. This is partly because the inherent features of online GIF videos. GIF videos spreading in online social networks share some common features.\nNoise: Junk information can interfere the judgment of GIF sentiment. We studied the composition of GIF videos. Among 6,000 GIF videos, we found around 71.55% of them are mixed with other forms of information. The most common (around 34.49%) noise is explanative texts. They can reverse the GIF sentiment. Other noises include motion blur, unexpected illumination change and so on.\nIrregular Length:Online GIF videoa\u0592r\u0301s length is irregular. The average length of 40,000 GIF videos we collected is 17.82s, with the longest GIF video (3a\u0592r\u030113a\u0592r\u0301a\u0592r\u030122) and the shortest GIF video (0a\u0592r\u03010a\u0592r\u0301a\u0592r\u03013a\u0592r\u0301).The irregular length makes it difficult to analyze.\nSymbolism: Symbolism refers to the fact that final sentiment judgment doesn\u2019t rely on object appeared inthe GIF, but the hidden message that object symbolized. For example, Sylvester Stallone symbolizes brave, perhaps in a video Stallone shows up, but it is brave that influence the final sentiment judgment.\nAbstraction: GIF sentiment hinges on high-level abstractions. Take the GIF video showed in Figure 4 as an example, GIF videoa\u0592r\u0301s sentiment changes with the variation of the girla\u0592r\u0301s facial expression. In general, to utilize the abstractions, there are several problems to solve.\n1. How to describe abstractions: A mid-level representation system has to be designed.\n2. How to detect abstractions: Given that representation\nsystem, we should be able to detect them.\n3. How to get sentiment from abstractions: The relationship between abstractions and the final sentiment judgment should be modeled.\nIn a word, the problem of GIF sentiment analysis is complex and challenging. But we consider it as a process of two steps. The first step is to extract mid-level representations for those abstractions (Smile Face, Body Movement, et al. We call them SentiPairs). Then we get SentiPair Sequence by combing SentiPairs in the order of their occurrence. In the second step, using the model we built, a sentiment judgment is made based on the SentiPair Sequence. Restricted to the accuracy of object detection,object tracking, et al. It is not easy to track the abstractions simultaneously, but we looked from another angle.\nBy generating a collection of mid-level representations, modeling the relationship between SentiPair and sentiment, and opening the first procedure to other fast-developing research fields (video classification, et al.), we can get GIF videoa\u0592r\u0301s sentiment.To that end, we address in this work two major challenges:\n1. To find out a common and effective representation for GIF videos.\n2. Model the relationship between representations and the final sentiment judgment.\nIn particular, we make the following contributions:\na We proposed a new mid-level representation: SentiPair Sequence. SentiPair Sequenceis based on semantic tree structure of visual sentiment label elementsa\u0592 lthe Synset Forest. With SentiPair Sequence and Synset Forest, we built the GIF Sentiment Ontology (GSO). Our experiments indicate that SentiPairoutperforms other mid-level attributes and our framework can achieve satisfying detect accuracy.\nb We built a large manually labeled GIF sentiment dataset (GSO-2015 dataset) from 40,000+ GIF videos spread around social networks. This dataset contains not only GIF picturea\u0592r\u0301s sentiment but also the SentiPair Sequence. GSO-2015 is free and will be open tothe public to promote further research on visual sentiment."}, {"heading": "2. RELATED WORK", "text": "Different researchers have developed different systems. Recent studies lay their emphasis on subject-free image sentiment analysis, subject-specified video sentiment analysis and subject-free video sentiment analysis.\nFor subject-free image sentiment analysis, [2] used the Progressive CNN network and bypassed the mid-level features. However, the number of neurons and connections is huge due to the abstract nature of visual sentiment. Deep network needs vast amount of less noisy labeled training instances to adjust the equal vast amount of neurons. Otherwise, it will get stuck into local optimum. Both [3] and [1] proposed to employ mid-level entities or attributes as features for subject-free image sentiment analysis.In [3], 1200 adjective noun pairs (ANP), which may correspond to different levels of different emotions, are extracted. These ANPs are used as queries to crawl images from Flickr. Next, pixellevel features of images in each ANP are employed to train 1200 ANP detectors. The responses of these 1200 classifiers can then be considered as mid-level features for visual sentiment analysis. The work in [1] employed a similar mechanism. The main difference is that 102 scene attributes are used instead.\nFor subject-specified video sentiment analysis, [4] proposed a framework utilizes video sound and facial expression to analyze interview clips. They focused on the sentiment analysis towards video with fixed content, similar pattern and average noise. The experiment result is promising, but due to the subject is specified, the method cana\u0592r\u0301t be used to deal with large-scale social networka\u0592r\u0301s GIF videos.\nFor subject-free video sentiment analysis, [5] proposed to use the features such as color histogram to train a framework for online GIF sentiment analysis. They did proposed good GIF emotion dataset. But since the problem of subject-free video sentiment analysis hinges on some unsolved problems, namely object classification, facial expression recognition, et al,\u0307 we can\u2019t rely on a unified framework for such an Abstract problem."}, {"heading": "3. GSO FRAMEWORK", "text": "We dedicated to find out the essential relationship between abstractions and sentiment for GIF videos.To achieve that goal, we developed an auto-generated ontology based on vast amount of user-generated content. We also developed GSO Framework to implement the ontology. The architecture of the GSO Framework we employ is shown in Figure 2. The details of the proposed framework will be described in the following sections."}, {"heading": "3.1 SentiPair Sequence", "text": "When considering the problem of GIF sentiment analysis. We should firstly figure out a way to represent a GIF video. A good representation should follow several criterions:\n1. Descriptive: should be able to describe the abstractions.\n2. Detective: should be able to be detected easily.\n3. Easy: should be able to be understood.\n4. Flexible: should be able to be extended.\nTo resolve these criterions, we introduced the SentiPair Sequence. A SentiPair Sequence is a sequence of SentiPairs, while the SentiPair is the joint name of Adjective Noun Pair (ANP) and Verb Noun Pair (VNP).Each SentiPair refers to either a concrete concept like smile face or a specific motion like falling cup. In a SentiPair Sequence, SentiPairs are sorted by the order of their occurrence.\nFigure 4 shows a typical SentiPair Sequence.As we can see, the girl in the video acts differently. At the very first, The girl is smiling and hence the first SentiPair indicates Lovely Girl, In the next frame, the girl looked a bit worried, and the second SentiPair is Innocent Girl, With the third SentiPair indicates Girl Frown, we can find out that the girl looks sad, which contains a negative sentiment tendency. In the last frame, the girl failed to suppress her feeling and the SentiPair indicates Girl Shout. As a result, we can conclude the SentiPair Sequence of this GIF video as: Lovely Girl, Innocent Girl, Girl Frown, and Girl Shout.\nSentiPair Sequence describes the concepts associated with sentiment judgment. In general, SentiPair Sequence carries two kinds of concepts. The first one is the existed object (by\nANP), and the second one is objecta\u0592r\u0301s movement (by VNP).\nAdjective Noun Pairs(ANP) was firstly introduced by [3], compared to nouns or adjectives only, an ANP can turn a neutral noun like dog into an ANP with strong sentiment like cute dog by adding an adjective with a strong sentiment. And the combined phrases also make the concepts more detectable than adjectives (like beautiful), which are too abstract.[3]\nInspired by previous work [3], the combined phrase of ANP has a better detect accuracy than single word, easier to be interpreted, and the whole structure is portable and flexible because we can add/remove ANPs dynamically.\nHowever, our experiment shows that ANP itself alone is not enough in the field of video sentiment analysis. We introduced Verb Noun Pair(VNP). VNP shares the same structure of ANP, accordingly, shares the features of portable, easy to be detected and interpreted."}, {"heading": "3.2 Synset Forest", "text": "SentiPairs are built on three kinds of words: adjectives, verbs and nouns. In order to build ANP/VNP, we should first build the collection of words to choose from. We concluded some criterions for a good word collection.\n1. Coverage, a good word collection should cover as much domains as possible in order to convey the information\n2. Discrepancy, words of similar meanings should ap-\nWe introduced Synset Forest to resolve these three criterions. The Synset Forest is a forest consists of three trees, namely adjective tree, verb tree and the noun tree. An overlook of all three trees can be found at Figure 5.\nIn the Wordnet[6], Synsets are interlinked by means of conceptual-semantic and lexical relations.By proposing the Synset Forest, we modeled a unified semantic and concept architecture. The Synset Forest acts as a collection of candidate words for Adjective Noun Pairs and Verb Noun Pairs. Since each node comes with a sentiment score, the weight for each ANP/VNP is decided at the first place."}, {"heading": "3.3 GSO-2015 Dataset", "text": "We built a new GIF video dataset from one of the most popular micro-blog provider. All the GIF videos were posted by online users and were collected automatically. We built 40,000+ distinct candidates. These candidates were then manually labeled in the fashion of GIF Sentiment Ontology. This work is possible owing to the crowd intelligence. We recruited 7 workers. Each worker was shown one GIF video and was expected to accomplish two tasks. Task 1 is to depict the given GIF using SentiPair Sequence. To be more specific, for each GIF, SentiPairs were chosen by the worker. And each SentiPair consists either of an adjective and a noun(ANP) or a verb and a noun (VNP). Figure 4 illustrates the flow of SentiPairs and the corresponding GIF. In Task 2 workers were expected to give the image an overall sentiment judgment(Positive/Negative/Neutral/Can\u2019t Judge)."}, {"heading": "4. EXPERIMENTS", "text": "We designed two experiments. The first experiment is designed to evaluate the performance of SentiPair Sequences we proposed. Different models applied and the evaluation metric is the accuracy. Moreover, we explored the possibility to simplify the problem by introducing feature selection.\nThe second experiment is designed to compare the performance of our framework to the state-of-the-art representation VSO [3]. The evaluation metric is the accuracy as well.\nWe choose to use the GSO-2015 dataset to train the sentiment classifiers. One of the advantages of GSO is its ability to convey temporal information(through SentiPair Combination). The training set consists of 1124 positive instances (60.3%), 146 negative instances (7.8%) and 599 neutral ones (32.1%)."}, {"heading": "4.1 Baseline", "text": "We compare the performance of GIF Sentiment Ontology(GSO) on GSO-2015 dataset with the state-of-the-art image sentiment analysis framework the VSO[3]."}, {"heading": "4.2 GIF Sentiment Analysis With Bag of Word", "text": "We decided to explore a simple way to get sentiment from SentiPairs. A straightforward way is BoW. Table 1 shows the performance of BoW using different classifiers. We find the Logistic Regression and SMO are the best choices (the same as [3]).On the other hand, each algorithm is able to correctly classify a large proportion of the testing instances(around 70%). In other words, classifier is not that important if we have SentiPair Sequences.\nWe also explored the possibility to simplify the problem by introducing feature selection. We used Correlation Based Subset (implemented byWEKA[7]) and the results are shown in Table 2. We find that feature selection can provide similar detect accuracy while using less mid-level attributes."}, {"heading": "4.3 SentiPair V.S. ANP", "text": "To evaluate the mid-level representation we proposed, we compare the performance of SentiPair Sequence with both single ANP and single VNP. The numbers in Table 3 is the accuracy indicating the instances been correctly classified. The result indicated that SentiPair Sequence outperformed ANP. We believe this is because GIF sentiment is affected by motion information, which is expressed by VNP."}, {"heading": "5. CONCLUSION", "text": "In this paper, we find out the essential relationship between abstractions and sentiment for GIF videos. As a midlevel representation for GIF videos, SentiPair Sequence was proposed. SentiPair is built on Synset Forest, while the Forest consists of 3 trees of Synset. Each Synset is a word with a specific sense. Our experiments suggest that SentiPair Sequence can abstract videoa\u0592r\u0301s spatio-temporal information and it outperforms competing representations such as ANP or VNP. The ontology is automatically established and the experiments suggest that the prediction accuracy is 72.6%.\nWe leave the first process (such as object detection) to the relevant researchers because we believe this process can be done simultaneously. Every single step made in the first process will improve the overall performance. We also opened our dataset GSO-2015 to the public. GSO-2015 contains more than 6,000 manually annotated GIF videos selected from more than 40,000 candidates. Each video is labeled with both sentiment and SentiPair Sequence. We believe it will be helpful for further researchers."}, {"heading": "6. REFERENCES", "text": "[1] You Quanzeng Yuan Jianbo, Mcdonough Sean and Luo Jiebo. Sentribute: Image sentiment analysis from a\nmid-level perspective. In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, WISDOM \u201913, pages 10:1\u201310:8, New York, NY, USA, 2013. ACM.\n[2] Quanzeng You, Jiebo Luo, Hailin Jin, and Jianchao Yang. Robust image sentiment analysis using progressively trained and domain transferred deep networks. In Robust Image Sentiment Analysis Using Progressively Trained and Domain Transferred Deep\nNetworks, 2015.\n[3] Damian Borth, Rongrong Ji, Tao Chen, Thomas Breuel, and Shih-Fu Chang. Large-scale visual sentiment ontology and detectors using adjective noun pairs. In Proceedings of the 21st ACM International Conference on Multimedia, MM \u201913, pages 223\u2013232, New York, NY, USA, 2013. ACM.\n[4] Louis-Philippe Morency, Rada Mihalcea, and Payal Doshi. Towards multimodal sentiment analysis: Harvesting opinions from the web. In Proceedings of the 13th International Conference on Multimodal\nInterfaces, ICMI \u201911, pages 169\u2013176, New York, NY, USA, 2011. ACM.\n[5] Brendan Jou, Subhabrata Bhattacharya, and Shih-Fu Chang. Predicting viewer perceived emotions in animated gifs. In Proceedings of the ACM International Conference on Multimedia, MM \u201914, pages 213\u2013216, New York, NY, USA, 2014. ACM.\n[6] George A. Miller. Wordnet: A lexical database for english. In Communications of the ACM, volume 38,No.11, pages 39\u201341, 1995.\n[7] Antti Puurula and Sung-Hyon Myaeng. Integrated instance- and class-based generative modeling for text classification. In Proc 18th Australasian Document Computing Symposium, pages 66\u201373. ACM, 2013."}], "references": [{"title": "Sentribute: Image sentiment analysis from a mid-level perspective", "author": ["You Quanzeng Yuan Jianbo", "Mcdonough Sean", "Luo Jiebo"], "venue": "In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks. In Robust Image Sentiment Analysis Using Progressively", "author": ["Quanzeng You", "Jiebo Luo", "Hailin Jin", "Jianchao Yang"], "venue": "Trained and Domain Transferred Deep Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs", "author": ["Damian Borth", "Rongrong Ji", "Tao Chen", "Thomas Breuel", "Shih-Fu Chang"], "venue": "In Proceedings of the 21st ACM International Conference on Multimedia, MM", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Towards multimodal sentiment analysis: Harvesting opinions from the web", "author": ["Louis-Philippe Morency", "Rada Mihalcea", "Payal Doshi"], "venue": "In Proceedings of the 13th International Conference on Multimodal Interfaces,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Predicting viewer perceived emotions in animated gifs", "author": ["Brendan Jou", "Subhabrata Bhattacharya", "Shih-Fu Chang"], "venue": "In Proceedings of the ACM International Conference on Multimedia, MM", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller"], "venue": "In Communications of the ACM, volume", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Integrated instance- and class-based generative modeling for text classification", "author": ["Antti Puurula", "Sung-Hyon Myaeng"], "venue": "In Proc 18th Australasian Document Computing Symposium,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "According to a recent study[1] the total proportion of visual content from all shared links on Twitter is 36%.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "For subject-free image sentiment analysis, [2] used the Progressive CNN network and bypassed the mid-level features.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "Both [3] and [1] proposed to employ mid-level entities or attributes as features for subject-free image sentiment analysis.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "Both [3] and [1] proposed to employ mid-level entities or attributes as features for subject-free image sentiment analysis.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "In [3], 1200 adjective noun pairs (ANP), which may correspond to different levels of different emotions, are extracted.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "The work in [1] employed a similar mechanism.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "For subject-specified video sentiment analysis, [4] proposed a framework utilizes video sound and facial expression to analyze interview clips.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "For subject-free video sentiment analysis, [5] proposed to use the features such as color histogram to train a framework for online GIF sentiment analysis.", "startOffset": 43, "endOffset": 46}, {"referenceID": 2, "context": "Adjective Noun Pairs(ANP) was firstly introduced by [3], compared to nouns or adjectives only, an ANP can turn a neutral noun like dog into an ANP with strong sentiment like cute dog by adding an adjective with a strong sentiment.", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "[3]", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Inspired by previous work [3], the combined phrase of ANP has a better detect accuracy than single word, easier to be interpreted, and the whole structure is portable and flexible because we can add/remove ANPs dynamically.", "startOffset": 26, "endOffset": 29}, {"referenceID": 5, "context": "In the Wordnet[6], Synsets are interlinked by means of conceptual-semantic and lexical relations.", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "The second experiment is designed to compare the performance of our framework to the state-of-the-art representation VSO [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 2, "context": "We compare the performance of GIF Sentiment Ontology(GSO) on GSO-2015 dataset with the state-of-the-art image sentiment analysis framework the VSO[3].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "We find the Logistic Regression and SMO are the best choices (the same as [3]).", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "We used Correlation Based Subset (implemented byWEKA[7]) and the results are shown in Table 2.", "startOffset": 52, "endOffset": 55}], "year": 2015, "abstractText": "With faster connection speed, Internet users are now making social network a huge reservoir of texts, images and video clips (GIF). Sentiment analysis for such online platform can be used to predict political elections, evaluates economic indicators and so on. However, GIF sentiment analysis is quite challenging, not only because it hinges on spatio-temporal visual contentabstraction, but also for the relationship between such abstraction and final sentiment remains unknown.In this paper, we dedicated to find outsuch relationship.We proposed a SentiPairSequence basedspatiotemporal visual sentiment ontology, which forms the midlevel representations for GIFsentiment. The establishment process of SentiPair contains two steps. First, we construct the Synset Forest to define the semantic tree structure of visual sentiment label elements. Then, through theSynset Forest, we organically select and combine sentiment label elements to form a mid-level visual sentiment representation. Our experiments indicate that SentiPair outperforms other competing mid-level attributes. Using SentiPair, our analysis frameworkcan achieve satisfying prediction accuracy (72.6%). We also opened ourdataset (GSO-2015) to the research community. GSO-2015 contains more than 6,000 manually annotated GIFs out of more than 40,000 candidates. Each is labeled with both sentiment and SentiPair Sequence.", "creator": "LaTeX with hyperref package"}}}