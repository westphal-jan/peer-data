{"id": "1506.01056", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2015", "title": "Performing Bayesian Risk Aggregation using Discrete Approximation Algorithms with Graph Factorization", "abstract": "risk variable aggregation algorithm is especially a popular method used to estimate the inverse sum output of a typical collection of financial assets beneficial or adverse events, where usually each asset or event composition is initially modelled as a general random variable. engineering applications, in mainly the financial property services industry, include vehicle insurance, operational risk, numerical stress control testing, and sensitivity dependence analysis, but likewise the problem is widely sometimes encountered alone in many other application simulation domains. this thesis has contributed two algorithms to perform weak bayesian risk element aggregation when same model exhibit strong hybrid dependency and high | dimensional inter - dependency. the fowler first algorithm operates on a subset of the simpler general problem, with specifically an emphasis rather on convolution problems, differ in noting the presence of continuous weights and finite discrete variables ( perhaps so just called hybrid models ) and combining the the second algorithm sets offer a rather universal method for general purpose inference over much a wider other classes of bayesian network models.", "histories": [["v1", "Tue, 2 Jun 2015 20:53:26 GMT  (3166kb)", "http://arxiv.org/abs/1506.01056v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["peng lin"], "accepted": false, "id": "1506.01056"}, "pdf": {"name": "1506.01056.pdf", "metadata": {"source": "CRF", "title": "Performing Bayesian Risk Aggregation using Discrete Approximation Algorithms with Graph Factorization", "authors": ["Peng Lin"], "emails": [], "sections": [{"heading": null, "text": "QUEEN MARY, UNIVERSITY OF LONDON\nPerforming Bayesian Risk Aggregation using Discrete\nApproximation Algorithms with Graph Factorization\nPeng Lin\nDecember 2014\nSubmitted in partial fulfilment of the requirements of the degree of Doctor of Philosophy\nDeclaration of originality\nI, Peng Lin, confirm that the research included within this thesis is my own work or that where it has been carried out in collaboration with, or supported by others, that this is duly acknowledged below and my contribution indicated. Previously published material is also acknowledged below.\nI attest that I have exercised reasonable care to ensure that the work is original, and does not to the best of my knowledge break any UK law, infringe any third party\u2019s copyright or other Intellectual Property Right, or contain any confidential material.\nI accept that the College has the right to use plagiarism detection software to check the electronic version of the thesis.\nI confirm that this thesis has not been previously submitted for the award of a degree by this or any other university.\nThe copyright of this thesis rests with the author and no quotation from it or information derived from it may be published without the prior written consent of the author.\nSignature:\nDate: December 31 st , 2014\nTo my parents"}, {"heading": "Acknowledgement", "text": "Firstly, I would like to thank my principal supervisor, Prof. Martin Neil. Without his\nenthusiasm and the effort spent on research discussions I would not have produced\nthis work. He provided me with sound advice, rigorous standards and also free space\nfor independent thinking.\nI would also like to thank my second supervisor, Prof. Norman Fenton. He was a\nconstant support and ever responsive, even when busy.\nMy parents are far away from the UK, but they are close to my research and they\nhave followed every step of my progression. Without this constant support and care\nthis research would not have been possible.\nI thank those academics with whom I had stimulating and challenging research\ndiscussions: Dr. William Marsh (Queen Mary) and Dr. Talya Meltzer (Hebrew\nUniversity). Dr. Anthony Constantinou and Dr. Barbaros Yet have conducted proof\nreading for this thesis. I thank all members in the Risk and Information Management\ngroup of Queen Mary.\nI thank Yihan Tao for keeping me alive in good health. I warmly thank my friends\nand colleagues at Queen Mary, whose friendship helped sustain me during the\ndifficulties, encountered my PhD studies and helped lighten the load.\nFinally I thank Prof. Neil Alford (Imperial College), who recommended me to\npursue a PhD in Queen Mary.\nAbstract\nRisk aggregation is a popular method used to estimate the sum of a collection of financial assets or events, where each asset or event is modelled as a random variable. Applications, in the financial services industry, include insurance, operational risk, stress testing, and sensitivity analysis, but the problem is widely encountered in many other application domains.\nThis thesis has contributed two algorithms to perform Bayesian risk aggregation when model exhibit hybrid dependency and high dimensional inter-dependency. The first algorithm operates on a subset of the general problem, with an emphasis on convolution problems, in the presence of continuous and discrete variables (so called hybrid models) and the second algorithm offer a universal method for general purpose inference over much wider classes of Bayesian Network models.\nThe first algorithm is called the Bayesian Factorization and Elimination (BFE) algorithm which performs convolution on the hybrid models required to aggregate risk in the presence of causal dependencies. This algorithm exploits a number of advances from the field of Bayesian Networks, covering methods to approximate statistical and conditionally deterministic functions to factorize multivariate distributions for efficient computation. This algorithm aims to support the representation of Bayesian \u201cviews\u201d in an explicit causal dependent structure, whilst providing the computational framework for evaluating convolution models. Such causal models would involve discrete explanatory (regime switching) variables, hybrid mixtures of dependent discrete and continuous variables, and high dimensional inter-dependent continuous variables.\nThe second algorithm developed is called Dynamic Discretized Belief Propagation (DDBP). It combines a dynamic discretization approach, to approximate continuous variables, with a new Triplet Region Construction (TRC) algorithm to perform inference on high dimensional hybrid models. The TRC algorithm is an optimized region graph approach based on graph factorization and Generalized Belief Propagation (GBP), which reduces the model complexity from exponential to polynomial. Proofs and experiments show that the algorithm converges, meets the\nrequirements for a balanced, maximum entropy normal region graph and does not restrain the model to any particular parameterization. DDBP offers a general purpose solution to inference in hybrid Bayesian Networks of any size regardless of dimensionality, provided the model is binary factorizable, which may be inconvenient to solve by traditional algorithms. Experiments show that it produces comparably accurate result with exact values.\nTable of Contents\nDeclaration of originality ............................................................................................. 2\nAcknowledgement........................................................................................................ 4\nAbstract ........................................................................................................................ 5\nGlossary of Abbreviations ......................................................................................... 10\nList of Figures ............................................................................................................ 12\nList of Tables.............................................................................................................. 15\nList of Algorithms ...................................................................................................... 16\n1. Introduction ......................................................................................................... 17\n1.1. Motivation ................................................................................................ 17\n1.2. Research Hypotheses ............................................................................... 20\n1.3. Structure of the Thesis ............................................................................. 21\n1.4. Publications .............................................................................................. 22\n2. Belief Networks and Graph Factorization .......................................................... 23\n2.1. Bayesian Probability ................................................................................ 23\n2.2. Factorization and Bayesian Belief Networks........................................... 24\n2.3. Causation and BN Structuring ................................................................. 28\n2.4. Decomposing Node Probability Tables ................................................... 30\n2.5. Converting a sparse graph into a DCCD ................................................. 35\n3. Inference in Belief Networks .............................................................................. 37\n3.1. Exact Inference ........................................................................................ 37\n3.2. Approximate Inference ............................................................................ 40\n3.3. Dynamically Discretized Inference Algorithm ........................................ 44\n3.3.1 Dynamic Discretization ................................................................ 44\n3.3.2 Dynamically Discretized Junction Tree Algorithm ...................... 46\n3.4. Generalized Belief Propagation ............................................................... 48\n3.4.1 Converting a BN to a Markov Network........................................ 49\n3.4.2 Factor Graphs ................................................................................ 50\n3.4.3 Belief Propagation on Factor Graph ............................................. 51\n3.4.4 Region Free Energy and Region Graph ........................................ 53\n3.4.5 GBP Message Passing .................................................................. 59\n4. BFE Risk Aggregation ........................................................................................ 61\n4.1. Risk Aggregation and BNs ...................................................................... 61\n4.2. n-fold Convolution BF Process ................................................................ 63\n4.3. Bayesian Factorization and Elimination (BFE) ....................................... 65\n4.3.1 Log Based Aggregation (LBA)..................................................... 66\n4.3.2 Variable Elimination (VE) ............................................................ 67\n4.3.3 Compound Density Factorization (CDF) ...................................... 72\n4.3.4 The BFE Convolution Algorithm with Example .......................... 74\n4.4. Deconvolution using the BFE Algorithm ................................................ 76\n4.4.1 Deconvolution ............................................................................... 76\n4.4.2 Reconstructing the Frequency Variable ........................................ 80\n4.4.3 The BFE Deconvolution Algorithm with Examples..................... 81\n4.5. Experiments ............................................................................................. 85\n4.6. Summary .................................................................................................. 92\n5. Inference for High Dimensional Models............................................................. 94\n5.1. Conditional Gaussian DCCD Model ....................................................... 95\n5.2. The Triplet Region Construction Algorithm ........................................... 98\n5.2.1 Identifying the TRC regions ......................................................... 98\n5.2.2 Constructing the TRC region graph ............................................ 102\n5.2.3 Proof that TRC region graph is MaxEnt-Normal and has correct counting numbers ..................................................................................... 109\n5.2.4 TRC complexity .......................................................................... 110\n5.2.5 Relationship with the Join Graph approach ................................ 111\n5.3. DDBP algorithm .................................................................................... 114\n5.4. Experiments ........................................................................................... 116\n5.4.1 Experiment 1: Inference for a 5 BFG model with all binary variables using TRC ................................................................................. 116\n5.4.2 Experiment 2: Inference for a 8 BFG with evidence using TRC\n118\n5.4.3 Experiment 3: Inference for 20 dimensional CG-DCCD model using DDBP ............................................................................................. 119\n5.4.4 Experiment 4: Inference for 10 dimensional CG-DCCD model with observations using DDBP ................................................................ 120\n5.4.5 Experiment 5: Pair correlation test for 15 dimensional CG-DCCD model 121\n5.4.6 Experiment 6: Inference in a linear model using DDBP ............ 122\n5.4.7 Experiment 7: Aggregation of inter-dependent random variables\n125\n5.5. Summary ................................................................................................ 128\n6. Conclusions and Future Work ........................................................................... 130\nBibliography ............................................................................................................. 131\nAppendix .................................................................................................................. 137\nPart A: Proof of Compound Density Factorization ................................. 137\nPart B: Proof of BFE deconvolution is equivalent to deconvolution in the full model .................................................................................................. 137\nPart C: Proof of the TRC region graph for n full-BFG is maxent-normal with correct counting numbers................................................................. 139\nPart D: Parameterization for experiments in section 5.4tion for experiments in section 5.4 ........................................................................ 143\nGlossary of Abbreviations\nBF Binary Factorization BFE Bayesian Factorization and Elimination BFG Binary Factorized Graph BM Bethe Method BN Bayesian Network BP Belief Propagation CDF Compound Density Factorization CG-DCCD Conditional Gaussian-Densely Connected Chain DAG CI Conditional Independence CPD Conditional Probability Distribution CTE Cluster Tree Elimination CVM Cluster Variation Method DAG Directed Acyclic Graph DCCD Densely Connected Chain DAG DD Dynamic Discretization DDBP Dynamic Discretized Belief Propagation DDJT Dynamic Discretized Junction Tree EDBP Edge Deletion Belief Propagation EP Expectation Propagation FFT Fast Fourier Transform FG Factor Graph GBP Generalized Belief Propagation IJGP Iterative Join Graph Propagation JGM Junction Graph Method KL Kullback-Leibler LBA Log Based Aggregation LBP Loopy Belief Propagation MC Monte Carlo MC-BU Mini-Clustering Belief Updating\nMCMC Markov Chain Monte Carlo MGD Multivariate Gaussian Distribution MN NBP Markov Network Non-parametric Belief Propagation NPT PBP Node Probability Table Particle Belief Propagation RG Region Graph SP Survey Propagation TLSBP Truncated Loop Series Belief Propagation TRC Triplet Region Construction VE Variable Elimination VI Variational Inference\nList of Figures\nFigure 2.1 BN with four variables distribution .................................................. 25\nFigure 2.2 n dimensional DCCD ...................................................................... 28\nFigure 2.3 Dependence connections and its associated chain rule probability .. 29\nFigure 2.4 Binary factorization mechanics: ....................................................... 31\nFigure 2.5 Binary factorization of 4 dimensional DCCD model ....................... 31\nFigure 2.6 Binary factorization of 5 dimensional DCCD model ....................... 32\nFigure 2.7 Counter example of complete BFG .................................................. 34\nFigure 2.8 conversion of a sparse 5 dimensional BN to its full BFG ................ 36\nFigure 3.1 BN model where X is conditionally deterministic sum of normal distribution parents ..................................................................................... 48\nFigure 3.2 DDJT approximation of posterior marginal with relative entropy error convergence plot................................................................................ 48\nFigure 3.3 Convert a 4 full BFG G to moral graph MG ................................ 50\nFigure 3.4 FG representation for the joint distribution: ..................................... 51\nFigure 3.5 Messages passing in the FG in Figure 3.4 ........................................ 52\nFigure 3.6 Message updating in a FG with dashed lines are message passing directions .................................................................................................... 52\nFigure 3.7 Factor graph containing cycles ......................................................... 53\nFigure 3.8 Generate region, RGG , graph for a BN, G ....................................... 56\nFigure 4.1 BN models of n-fold convolution of i.i.d. severity variables ( 1G ) and of common cause version ( 2G ) with accompanying binary factorized versions ( 1'G and 2'G ) ............................................................................. 64\nFigure 4.2 Architecture of BN algorithms ........................................................ 65\nFigure 4.3 1G and 2G BNs binary factorized for aggregation ......................... 66\nFigure 4.4 VE process applied to part of BN 1G ............................................... 69\nFigure 4.5 (a) Simple common cause model binary factorization and VE process, (b) VE process applied to part of BN G ................................................... 71\nFigure 4.6 Multiple common cause model binary factorization and VE process .................................................................................................................... 71\nFigure 4.7 Compound density factorization ...................................................... 72\nFigure 4.8 Frequency and severity distribution ................................................ 75\nFigure 4.9 Compound density factorization for first three terms in the example .................................................................................................................... 76\nFigure 4.10 (a) Convolution and (b) Deconvolution .......................................... 78\nFigure 4.11 Binary factored common cause N-fold BN, A , reduced by applying the VE algorithm to G and then 'G .......................................................... 79\nFigure 4.12 (a) Reconstruct N (b) Deconvoluting N ....................................... 82\nFigure 4.13 (a) N-fold common cause model, (b) Binary factorized form of (a) with marginal distributions superimposed on the BN graph ...................... 84\nFigure 4.14 (a) N-fold model deconvolution on full model; (b) N-fold model deconvolution using BFE algorithm .......................................................... 85\nFigure 4.15 Marginal distributions for overlaid on BN graph containing query nodes for experiment 4.1 ............................................................................ 88\nFigure 4.16 Marginal distributions for overlaid on BN graph containing query nodes for experiment 4.2 ............................................................................ 89\nFigure 4.17 (a) Common cause dependent severity; (b) 16-fold convolution of dependent severity ...................................................................................... 90\nFigure 4.18 Compound densities (a) MC; (b) BFE ............................................ 90\nFigure 4.19 (a) Common Cause N-fold convolution using BFE algorithm; (b) Nfold model deconvolution using BFE algorithm ........................................ 91\nFigure 5.1 Six dimensional CG-DCCD to 6 full BFG .................................... 98\nFigure 5.2 Topology of a 5 dimensional BFG, with solid circle a primary triplet, dashed line a moral edge and dashed circle an interaction triplet 100\nFigure 5.3 Identifying primary triplets using moral graph of a 5 full-BFG .. 101\nFigure 5.4 Identifying interaction triplets (with irrelevant edges removed) .... 102\nFigure 5.5 Valid region graph generated by CVM .......................................... 103\nFigure 5.6 Two level initial region graph for 5 BFG model with region counting number listed aside .................................................................... 103\nFigure 5.7 Connectivity after removing all cognate intersection regions except root intersection region 1 2X X\nR .................................................................. 105\nFigure 5.8 5 , 6 and 7 full-BFG with only original variables, and counting numbers (resulting from InitRG ) for each cognate intersection (edge) listed .................................................................................................................. 105\nFigure 5.9 TRCRG generated for a 5 BFG ...................................................... 107\nFigure 5.10 Generated Join Graph (with CPD listed aside each cluster) for the original BN, G , using the IJGP join graph structuring algorithm ........... 111\nFigure 5.11 Generated TRC region graph for the original BN G in Figure 5.10 .................................................................................................................. 112\nFigure 5.12 Convert TRC region graph into join graph ................................... 113\nFigure 5.13 Join graph for TRC region graph in Figure 5.10 .......................... 114\nFigure 5.14 TRC result for a 5 BFG with all binary variables ..................... 117\nFigure 5.15 a 8 BFG with all binary variables ............................................. 118\nFigure 5.16 20 dimensional CG-DCCD model ............................................... 119\nFigure 5.17 15 BFG model (Intermediate variables are labelled as \u201cnew\u201d which are added during the full-BF procedure. Nodes 1X to 15X are the original variables.).................................................................................... 121\nFigure 5.18 (a) Bayesian linear regression plate model .................................. 123\nFigure 5.19 Converted 10 BFG model from Figure 5.18 (b) with observations .................................................................................................................. 124\nFigure 5.20 (a): 7 full-BFG model ................................................................ 127\nList of Tables\nTable 4.1 The NPT of 0N .................................................................................. 83\nTable 4.2 Results of convolution with basic parameterisation .......................... 87\nTable 4.3 Results of convolution with multi-modal severity distribution ......... 88\nTable 4.4 Severity NPT ...................................................................................... 89\nTable 4.5 Common cause N-fold convolution density ....................................... 90\nTable 5.1 TRC using binary variables ............................................................. 117\nTable 5.2 TRC using binary variables ............................................................. 118\nTable 5.3 statistics for 20 dimensional CG-DCCD model (DDBP done with DD iterations 35, maximum GBP iterations 60) ............................................. 120\nTable 5.4 10 dimensional CG-DCCD model with observation 10 10X   (DDBP performed with DD iterations 30, maximum GBP iterations 180) .................................................................................................................. 120\nTable 5.5 pair correlation test........................................................................... 121\nTable 5.6 data observations used in this experiment ....................................... 124\nTable 5.7 results comparing DDJT, OLS and DDBP ...................................... 125\nTable 5.8 results of the sum of 7 dimensional CG-DCCD model ................... 128\nList of Algorithms\nAlgorithm 1 DDJT algorithm ............................................................................. 47\nAlgorithm 2 BFE convolution algorithm ........................................................... 74\nAlgorithm 3 BFE deconvolution algorithm ....................................................... 82\nAlgorithm 4 Cognate intersection pruning ....................................................... 106\nAlgorithm 5 TRC algorithm ............................................................................. 108\nAlgorithm 6 DDBP algorithm .......................................................................... 115"}, {"heading": "1. Introduction", "text": ""}, {"heading": "1.1. Motivation", "text": "Risk aggregation is a popular method used to estimate the sum of a collection of financial assets or events, where each asset or event is modelled as a random variable. Existing techniques make a number of assumptions about these random variables. Firstly, they are almost always continuous. Secondly, should they be dependent, these dependencies are best represented by correlation functions, such as copulas (Politou & Giudici, 2009) , where marginal distribution functions are linked by some dependence structure. These statistical methods have tended to model associations between variables as a purely phenomenological artefact extant in historical statistical data. Recent experience, at least since the beginning of the current financial crisis in 2007, has amply demonstrated the inability of these assumptions to handle non-linear effects or \u201cshocks\u201d on financial assets and events, resulting in models that are inadequate for prediction, stress testing and model comprehension (IMF, 2009), (Laeven & Valencia, 2008).\nIt has been extensively argued that modelling dependence as correlation is insufficient, since it ignores any views that the analyst may, quite properly, hold about those causal influences that help generate and explain the statistical data observed (Meucci, 2008), (Rebonato, 2010). Such causal influences are commonplace and permeate all levels of economic and financial discourse. For example, does a dramatic fall in equity prices cause an increase in equity implied volatilities or is it an increase in implied volatility that causes a fall in equity prices? The answer is trivial in this case, since a fall in equity prices is well known to affect implied volatility, but correlation alone contains no information about the direction of causation. To incorporate causation we need to involve the analyst or expert and \u201cfold into\u201d the model views of how discrete events interact and the effects of this interaction on the aggregation of risk. This approach extends the methodological boundaries last pushed back by the celebrated Black\u2013Litterman model (Black & Litterman, 1991). In that approach a risk manager\u2019s role is as an active participant in\nthe risk modelling, and the role of the model is to accommodate their subjective \u201cviews\u201d, expressed as Bayesian priors of expectations and variances of asset returns. This thesis aims to represent these Bayesian \u201cviews\u201d in an explicit causal structure, whilst providing the computational framework for solutions. Such causal models would involve discrete explanatory (regime switching) variables, hybrid dependent variables, and high dimensional inter-dependent variables. A causal risk aggregation model might incorporate expert derived views about macro-economic, behavioural, operational or strategic factors that might influence the assets or events under \u201cnormal\u201d or \u201cabnormal\u201d conditions. Applications of the approach include insurance, stress testing, operational risk and sensitivity analysis, but the problem is widely encountered in many other application domains.\nAt its heart risk aggregation requires the sum of n random variables. In practice this involves the use of two well-known mathematical operations: n-fold convolution (for a fixed value of n ) and N-fold convolution (Heckman & Meyers, 1983), defined as the compound sum of a frequency distribution, N , and a severity distribution, S , where the number of constant n-fold convolutions is determined stochastically. Currently numerical tools such as Panjer\u2019s recursion (Panjer, 1981), Fast Fourier transforms (Heckman & Meyers, 1983) and Monte Carlo (MC) simulation (Meyers, 1980) perform risk aggregation numerically using parameters derived from historical data to estimate the distributions for both S and N . These approaches produce acceptable results, but they have not been designed to cope with the new modelling challenges outlined above. In the context of modelling general dependencies among severity variables, a popular approach is to use copulas, both to model the dependent variables and to perform risk aggregation.\nOne aim of this thesis is to show how we can carry out a stochastic risk aggregation (N-fold convolution) in a causal Bayesian framework, in such a way that subjective views about inter-dependencies can be explicitly modelled and numerically evaluated i.e. where discrete and continuous variables are inter-dependent and may influence N and S in complex, non-linear ways. Conventional approaches, such as the copula based risk aggregation framework (Bruneton, 2011), cannot perform deconvolution because the model is usually not identifiable since it involves an inverse transform of the aggregated model, resulting in multiple solutions. The\nalgorithms described in this thesis support deconvolution (illustrated in chapter 4) because the discrete nature of the approximation is more tractable.\nWe see this as the first of many financial modelling problems that are amenable to this new approach.\nThis thesis describes the development in two parts. The first part involves Chapters 2 and 4, which describe a Bayesian Factorization and Elimination (BFE) algorithm that performs convolution on the hybrid models required to aggregate risk in the presence of causal dependencies. This algorithm exploits a number of advances from the field of Bayesian Networks (BNs) (Pearl, 1988) (F. V. Jensen & Nielsen, 2009), covering methods to approximate statistical and conditionally deterministic functions and to factorize multivariate distributions for efficient computation.\nThe second part is covered in Chapters 3 and 5, where the aim is to solve the problem on general inference on high dimensional models of any form, including non-Gaussian and discrete cases, and which can be extended to many classes of hybrid Bayesian Networks. In this way risk aggregation on high dimensional models can be carried out, but, as a natural by product, so can approximate inference on many other more general problems. A discrete approximation inference algorithm called Dynamically Discretized Belief Propagation (DDBP) is presented to perform inference on models exhibiting high dimensionality; a major computation barrier existed for current discrete approximation algorithms, such as the Junction Tree (JT) algorithm. DDBP uses graph factorization, provided the model is binary factorizable, and uses Generalized Belief Propagation (GBP) (Yedidia, Freeman, & Weiss, 2005) for discrete approximation. It significantly reduces the computational complexity from (worst case) exponential to polynomial compared with Junction Tree based approaches. The DDBP algorithm contains a sub algorithm called Triplet Region Construction (TRC), derived from GBP (Yedidia et al., 2005) that builds an optimal region graph satisfying the maxent-normal (Yedidia et al., 2005) property and guaranteeing a balanced propagation structure. The TRC algorithm solves several related problems, e.g. how to determine the graph topology, interaction strength and ensures the counting number of the region graph is balanced. When coupled with Dynamic Discretization (DD) the TRC algorithm produces approximate results on a\nwide class of BN models and experiments demonstrate that such models converge and are accurate.\nThe BFE and DDBP algorithms are major contributions of this research. They both rely on factorizing the graph structure in principled ways and both use discrete approximation methods, using dynamic discretization (DD), to approximate continuous variables. The thesis compares the results produced from these against conventional numerical algorithm, such as Markov Chain Monte Carlo (MCMC) (Gamerman & Lopes, 2006), and with analytical closed form solutions. The algorithms developed by this research, however, have the advantages to be more flexible, extendable and also just as accurate as the existing state of the art."}, {"heading": "1.2. Research Hypotheses", "text": "The main research objective of this thesis is to perform a stochastic risk aggregation using a causal Bayesian framework, typically such models could contain independent random variables, hybrid dependent random variables and high dimensional inter-dependent random variables. So this research is carried out to answer the following four research hypotheses.\n First, can we perform a Bayesian stochastic risk aggregation using discrete\napproximation approach to accurately address the compound density when severity variables are independent?\n Second, can we perform a Bayesian stochastic risk aggregation using discrete\napproximation approach to accurately address the compound density when severity variables are hybrid and dependent?\n Third, can we accurately de-convolute the causal explanatory variables by\nusing discrete approximation approach for the Bayesian stochastic risk aggregation model?\n Fourth, can we perform a Bayesian stochastic risk aggregation using discrete\napproximation approach to accurately address the compound density when severity variables exhibiting high dimensionality?\nThis thesis will describe the solutions for all these hypotheses, supported by experiments and proofs. The reader should be aware however that the research results demonstrated answer hypothetical questions beyond those outlined above and as such the thesis exceeds the ambitions of the original research agenda."}, {"heading": "1.3. Structure of the Thesis", "text": "Chapter 2 reviews the background of Bayesian Network inference with an emphasis on factorization, the central orientation of this thesis. It covers Bayes\u2019 theorem, Bayesian networks and other supporting structures (such as Markov Graphs). The graph factorization approach is presented, which is then used for the BFE and DDBP algorithms. It also illustrates how deterministic conditional functions are factorized for continuous variables.\nChapter 3 covers the inference approaches for Bayesian networks. It reviews the current popular inference approaches, e.g. JT, VI, MCMC and GBP.\nChapter 4 introduces the BFE algorithm for N-fold convolution models. The severity models considered are independent and hybrid dependent random variables. This chapter presents the factorization and variable elimination techniques, which are central components of BFE algorithm. Next the de-convolution of the Bayesian risk aggregation model is illustrated. This chapter focuses on research hypotheses one, two and three.\nChapter 5 develops an optimal region graph construction algorithm called triplet region construction (TRC), which is a sub algorithm for DDBP. It covers the mechanics of constructing an optimized region graph derived from GBP algorithm, which is automatic and satisfies maxent-normal property. It then develops the DDBP algorithm by combining TRC and dynamic discretization. The experiments in this chapter illustrate the Bayesian risk aggregation on high dimensional models, and also on general models. This chapter focuses on research hypothesis four.\nChapter 6 concludes the thesis, discusses the overall value of this research and highlights several improvements to the BFE and DDBP algorithms to achieve better performance."}, {"heading": "1.4. Publications", "text": "Until the submission of this thesis:\n1. Lin, P., Neil, M., & Fenton, N. (2014). Risk aggregation in the presence of\ndiscrete causally connected random variables. Annals of Actuarial Science, 8 (2), pp 298-319 doi:10.1017/S1748499514000098\n2. Lin, P., Neil, M., & Fenton, N. (2014). Inference for high dimensional Bayesian\nnetwork models using dynamically discretized belief propagation, submitted to Information Sciences."}, {"heading": "2. Belief Networks and Graph", "text": "Factorization\nThis chapter provides an overview of the fundamental probability concepts central to this thesis. We will discuss Bayesian probability and (touch on) causality. Bayesian networks and other network structures will be covered in more detail and approaches to graph factorization will be given particular emphasis. The concepts presented in this chapter are used to describe the algorithms presented in the following chapters of this thesis."}, {"heading": "2.1. Bayesian Probability", "text": "Bayesian probability (Cox, 1961) (Finetti, 1970) provides a way to reason coherently toward the uncertainty we face in the real world. It reflects personal knowledge and any belief about uncertainty is assumed to be provisional on experience gained to date (the prior), is then updated by new experience and data (the likelihood) to provide a new personal disposition about the uncertainty (the posterior). Probabilities are quantitative measures of this uncertainty on a unit scale and subject to the axioms of probability theory (Devore, 2011).\nHowever, probability is not only about numbers, but also about the structure of reasoning (Pearl, 1988), which can be used to reason causally (i.e. from cause to effect or vice versa). Indeed in recent decades Bayesian probability has gained in popularity because Bayes\u2019 theorem supports reasoning about causal propositions of how the variables are related along with the beliefs (probabilities) between these variables.\nBayesian probability was initially proposed by Thomas Bayes in 18 th century, and pioneered and popularised by Laplace (Stigler, 1986). It can be expressed as Equation 2.1.\n( , ) ( | ) ( ) ( | )\n( ) ( )\np E p E p p E\np E p E\n       (2.1)\nThe last term of Equation 2.1 is known as Bayes\u2019 rule (Fenton & Neil, 2012), and is constantly used in this thesis. Here variable E is an observable data point/evidence and  is a model parameter/hypothesis. ( , )p E is the joint probability of  and E ,\nwhich represent the problem domain containing these two variables. By the chain rule of probability (Russell & Norvig, 2010) the last term of Equation 2.1 factorizes the joint probability into conditional probability (the likelihood) and the prior. If we can organise this factorize appropriately then the number of parameters might be substantially reduced (will discuss in section 2.2). Such factorization might reflect the causal nature of the problem or may be subject to mathematical rules governing the decomposition lying behind the parameterization.\nIn Bayesian interpretation of probability, probability is a measure of belief, and\n( )p  is the probability of some hypotheses or parameters,  , referred to as the\nprior. It is the belief about  without knowing any information about the evidence,\nE . ( | )p E  is the conditional belief about the evidence, E , given information about\nthe hypotheses or parameters,  , and is referred to as the likelihood. ( )p E is used as\na normalizing factor which is the probability of observing the evidence over all hypotheses. ( )p E is a marginal probability obtained by the marginalization\noperation over the joint probability ( , )p E , and in the discrete form can be\nexpressed as Equation 2.2.\n( ) ( , ) ( | ) ( )p E p E p E p         (2.2)"}, {"heading": "2.2. Factorization and Bayesian Belief Networks", "text": "Direct computation of joint probability is inefficient or even intractable. The complexity problem of the computation of joint probability can be overcome by exploiting conditional independence (Dawid, 1979) (Pearl, 1988).\nFollowing the chain rule of probability, we can factorize the joint probability\n1 2( , ,..., )np X X X into Equation 2.3.\n1 2 1 2 1 1 1\n1 1\n1\n( , ,..., ) ( ) ( | )... ( | ,..., )\n( | ,..., )\nn n n\nn\ni i\ni\np X X X p X p X X p X X X\np X X X\n\n\n\n  \n (2.3)\nFactorization Equation 2.3 does not imply any benefit for parameter reduction, however, domain knowledge usually allow us to identify a subset of parent variables,\n1 2 1{ } { , ,..., }i ipa X X X X  , such that given { }ipa X , variable iX is conditional\nindependent of all variables in 1 2{ , ,..., }\\ { }n iX X X pa X . The parameterization might be therefore being expressed as Equation 2.4, thus reducing dimensionality.\n1 1( | ,..., ) ( | { })i i i ip X X X p X pa X  (2.4)\nAnd further we can modularise the joint probability distribution as the product of\nthese local child variables, iX , conditioned on parents, ( )ipa X , as shown in Equation 2.5.\n1 2\n1\n( , ,..., ) ( | { }) n\nn i i\ni p X X X p X pa X   (2.5)\nIn this way we use independence and dependence assumptions to reduce the parameter complexity. We can consider a BN (defined below) as simple a set of conditional independence assumptions (Barber, 2012) and can be used to simplify the representation/factorization of joint distributions. In the remainder of this section, we give a recap of the properties of BNs.\nA BN is a graphical probabilistic model that represents a set of random variables and the conditional independence assumptions between those variables. A BN can be represented by a directed acyclic graph (DAG), with a directed arc pointing from a parent variable to child variable, and the i th node in the graph corresponding to the\nfactor ( | { })i ip X pa X (Barber, 2012).\nA Bayesian Network (BN) consists of two main elements:\n1. Qualitative: This is given by a DAG G , with nodes representing random\nvariables (parent or child), which can be discrete or continuous, and may or may not be observable, and directed arcs (from parent to child) representing causal or influential relationships between variables. See Figure 2.1 for an example BN.\n2. Quantitative: A probability distribution associated with each node iX . For a\nnode with parents this is a conditional probability distribution (CPD),\n | { }i ip X pa X that defines the probabilistic relationship of node given its\nrespective parents { }ipa X . For each node iX without parents, called root\nnodes, this is their marginal probability distribution ( )ip X . If iX is discrete, the CPD can be represented as a Node Probability Table (NPT) (Fenton &\nNeil, 2012),  | { }i ip X pa X , which lists the probability that iX takes, on\neach of its different values, for each combination of values of its parents\n{ }ipa X . For continuous variables, the CPDs represent conditional\nprobability density functions.\nTogether, the qualitative and quantitative parts of the BN encode all relevant information contained in a full joint probability model. The conditional independence (CI) assertions about the variables, represented by the absence of arcs, allow factorization of the underlying joint probability distribution in a compact way as a product of CPDs. More detailed information on BNs can be obtained from (F. V. Jensen & Nielsen, 2009) (Fenton & Neil, 2012) (Koller & Friedman, 2009) (Pearl, 1993) (S. L. Lauritzen, 1996).\nAny joint probability distribution factorized by the chain rule can be represented as a complete graph, where each pair of nodes is connected by a directed edge. A variation of BNs that is complete graph is called DCCD and is defined:\nDefinition: A DAG, G , factorized by the chain rule is defined as a Densely Connected Chain DAG (DCCD) if each pair of variables are connected by a\ndirected edge, and each variable iX is directly connected to all of its parents\n{ }ipa X . The DCCD representation encodes no independence constraints.\nA DAG G that is not a DCCD, is referred to as a sparse graph. A DCCD can be discrete, continuous or hybrid (appearing discrete and continuous variables simultaneously) model. DCCDs will be used to illustrate the algorithms presented in the rest of this thesis.\nHowever, CI assumption is problem tailored and model specific, which is not a general method for reducing the NPT complexity, i.e. not applicable to DCCD models. We will introduce NPT decomposition approach in section 2.4.\nThe Markov assumption states that each random variable, X , is independent of its non-descendents given its parents, { }pa X (Koller & Friedman, 2009). This can be\nwritten as ( ; ( )| { })Ind X NonDesc X pa X . A DAG, G , is an I-map of a distribution\np if all Markov assumptions implied by G are satisfied by p , assuming G and p\nboth use the same set of random variables (Koller & Friedman, 2009).\nA DCCD G suffices to encode all Markov assumptions for any distribution p , so a\nDCCD, G , is an I-map for arbitrary distribution p (Barber, 2012), which is\nsufficient to represent arbitrary p if p can be factorized in the form of Equation 2.5.\nSo any BN model can be viewed as a DCCD with some edges being removed under the CI assumptions. Conversely, any non-DCCD model can be converted to a DCCD model by adding appropriate edges, and then CPDs need to be changed accordingly with some parameters set to zero to imply the CI assumptions. So theoretically any BN models can be represented by DCCD models. Any inference algorithm that\nworks on the worst case (the DCCD case) BN will work for any BN (this is the case in Chapter 5 for inference task).\nAn arbitrary dimensional BN model can be represented as:\nFigure 2.2 shows an n dimensional DCCD, with the dimensionality n can be arbitrary. The DCCD models are general form of any BN model, where the difference is only on dimensions. So in the following sections/chapters, we will use DCCD models to illustrate concepts and develops. In chapter 5, the TRC algorithm is particularly designed for inference on DCCD models."}, {"heading": "2.3. Causation and BN Structuring", "text": "The graphical structure and CI assumptions of a BN can be used to encode causal information into the model. This offers a unique mechanism to fuse information from data, say in the form of correlations but also in the form of subjective probabilities and reflecting, in the structure, statements about how the world might operate. We refer the details of discussions on causation in BNs to (Pearl, 2000) (Fenton & Neil, 2012). In this thesis, risk aggregation with discrete causal variables is presented in Chapter 4, where it demonstrates how causation dependencies are captured along with the aggregation. The causation relationships between random variables e.g. cause to consequence or vice versa are quantified by Bayesian inference. In this way, we can propagate/infer the observed data from either cause or consequence variable to other variables. The prior distributions are revised into posterior distributions after accounting for the newly observed data. Revising prior belief into posterior after observing new relevant evidence is believed to be consistent with human reasoning. The causal explanations can be modelled explicitly by three dependence connections in BBN as shown by the three following conditional independence assumptions:\nA: serial connection, ( , , ) ( ) ( | ) ( | )p u m v p u p m u p v m  \nB: diverging connection, ( , , ) ( ) ( | ) ( | )p u m v p m p m u p m v  \nC: converging connection, ( , , ) ( ) ( ) ( | , )p u m v p u p v p m u v  \nTypically to scale up a BN it suffices to use the above three kinds of dependence connections shown in Figure 2.3. Formally, we introduce the DAG concepts of dconnection and d-separation (\u201cd\u201d stands for directional) that are central to determine CI in any BN with structure given by the DAG. Let P be a trial, (that is, a collection of edges which is like a path, but each of whose edges may have any direction) from node u to v . Then P is said to be d-separated by a set of nodes Z , if and only if one of the following holds.\n1. P contains a serial connection, u m v  , such that the middle node m\nis in Z .\n2. P contains a diverging connection, u m v  , such that the middle node\nm is in Z .\n3. P contains a converging connection, u m v  , such that the middle\nnode m is not in Z and no descendent of m is in Z .\nThus u and v are said to be d-separated by Z if all trails between them are dseparated. If u and v are not d-separated they are said to be d-connected. In a serial connection of Figure 2.3, if we instantiate m then u and v are d-separated given m (blocked), entering evidence at u does not affect v and vice versa. The same phenomenon applies for a diverging connection case. In the case of a converging connection, entering evidence on u does not affect v and vice versa since they are d-separated. If the certainty of m changes then u and v become dependent. The notations we introduced here will be applied to chapter 4 and 5.\nA BN B is often first developed by creating a causal DAG G , such that B satisfies the Markov assumption with respect to G . One then ascertain the CPDs of each\nvariable given its parents in G . If in the case where the variables are discrete, if we define the joint distribution of B to be the product of these CPDs, then B is a Bayesian network with respect to G .\nIn the case where the variables are approximated by piece-wise constant partitions (discrete approximation of a continuous variable), the product of these CPDs can be significantly computational inefficient. The next section discusses, from a computation point of view, how the size of NPTs can be reduced, which is a crucial mechanism used to develop efficient numerical algorithms in Chapter 4 and 5."}, {"heading": "2.4. Decomposing Node Probability Tables", "text": "The dimension of an NPT is simply the enumeration of all combinations of child and parent variable discrete states where the complexity is exponential in the number of variables involved.\nThis section explains an approach to NPT decomposition called Binary Factorization (BF) (Neil, Chen, & Fenton, 2012), which is able to factorize conditional expressions used in the definition of an NPT in such a way that it guarantees that each conditional continuous variable in the BN has no more than two parent variables. In this way the size of the NPT is reduced from an exponential to the sum of NPTs of polynomial size.\nConditional expressions that can be binary factorized in this way include conditionally deterministic expressions and statistical distributions whose parameters are defined as functions of parent variables.\nNPT decomposition is not only important in reducing the complexity of the NPTs but also facilitates the development of the risk aggregation algorithm (Chapter 4) and inference algorithms (Chapter 5) in this thesis.\n(a) NPT table for 4 1 2 3( | , , )p X X X X\n(b) NPT table for 4 1 3( | , )p X E X after binary factorization\nConsider the conditionally deterministic function for a variable, 4X , and its parents:\n4 1 2 3X X X X   , as shown in Figure 2.4 (a). If all four variables are continuous,\nand each variable is approximated by 25 piece-wise constant partitions, the overall NPT size for a single CPD 4 1 2 3( | , , )p X X X X is 425 . To reduce the computation complexity, we use BF to factorize the conditional expressions defined for each conditional NPT, so the resulting NPTs defined on continuous conditioned variables will only contain three variables. In this way the maximum NPT size for its CPD is 325 , as shown in Figure 2.4 (b) and the size of the conditional NPTs is now 3( 1) mm n n  where m is the number of parent variables and n is the number of\nstates.\nIf all four variables in Figure 2.1 are continuous and the associated CPDs are conditional deterministic functions, we can perform BF process and obtain its BF formatted version 'G in Figure 2.5.\nIn Figure 2.5, the BF process factorizes the conditional deterministic expression for Figure 2.1:\n4 1 1 2 2 3 3X c X c X c X  \ninto 4 1 3 3X E c X  , where 1 1 1 2 2E c X c X  is an intermediate variable added to substitute the other two parents. By using the BF process, each continuous variable has only two parents. This produces a maximal discretized NPT of size 3m ( m is the average number of piece-wise constant partitions for each variable) rather than 4m for the case we considered. Note that the BF process will not work on a CPD with three or more parameters that cannot be decomposed, such as a hyper Geometric distribution.\nSimilarly, we can transform a 5 dimensional DCCD model to its binary factorized form shown by Figure 2.6.\nIn the following sections, we will assume all DCCD models have already undergone a BF transform and inference is carried out on the Binary Factorized Graph (BFG) rather than the original BN. We have implicitly assumed all these DCCD models are BF decomposable, which means CPDs in the DCCD are either factorizable, e.g. for\ncontinuous variable case, or can be represented in a BFG form, i.e. discrete variables case.\nThe BFG makes our discrete inference feasible to perform DCCD models but under the BF decomposable assumption. Beside that we will guarantee the DCCD to BFG model is I-equivalent, which means network structures\u2019 encode the same independent statements. So that the BFG is sufficient to represent any BF decomposable distribution p implied by DCCD. Such a BFG is unique and can be\nobtained by extending the BF process with an additional property. We define a fullBFG, 'G :\nDefinition: A full-BFG, 'G , is a BFG where for any i j k  where 2k  it\nis possible to find paths from iX to jX and iX to kX through intermediate\nvariables tE in 'G , where the set of intermediate variables are disjoint in the two paths 1 .\nFollowing conversion to a full-BFG each original variable iX G now has at most\ntwo parents in 'G , and each new intermediate variable tE in 'G has one and only one child in 'G . The disjoint property is important because it guarantees that the resulting full-BFG is an I-map of the original joint distribution, p .\nTo create a full-BFG we perform binary factorization on all nodes in the order of the\nnode parents numbering in the original BN G . So, if a node has three parents 1X ,\n2X , 3X then the binary factorization will be in the order 1 2( , )X X and 3X rather\nthan 1X and 2 3( , )X X or 1 3( , )X X and 2X . In Figure 2.6 1 2 3, ,X X X have no more than two parents in their CPD expressions and so need no factorizations, and the\npaths 1 4X X and 1 5X X , pass through the disjoint sets of intermediate variables.\nA BFG 'G that does not satisfy the above constraints will not be I-equivalent to its DCCD, and hence it will be insufficient to represent an arbitrary (BF decomposable) distribution, p . Example 2.2 illustrates this point using a counter example.\n1 We require 2k  since root nodes 1X and 2X are directly connected to 3X .\nExample 2.2\nConsider the full-BFG in Figure 2.7 which we present again in Figure 2.7 as 'G\n(with the graph reorganized). If 1 1c d and 2 2c d , then the CPD expressions\nfor 1E and 2E become identical. We could therefore create a binary factorized graph ''G in which these two intermediate variables are merged into one\nvariable, 1E (Figure 2.7). But then the disjoint sets defined by the full-BFG no\nlonger hold because the paths 1 4X X and 1 5X X have a shared\nintermediate variable, 1E . The graph ''G is not I-equivalent with its DCCD for an arbitrary distribution p since it cannot represent the distribution p when\n1 1c d and/or 2 2c d .\nIn the case of ''G it is easy to see that if, for each intermediate variable, we add edges connecting its parent variables to its child variables and then remove the intermediate variables and all connecting edges, the resulting graph is not a DCCD. For 'G applying the same process results in a DCCD.\nExcept notations, we will assume all models discussed in the following sections are full-BFGs. So in general, the BF transform of an n -dimensional DCCD model\nresults in a n dimensional full BFG model, where the number of nodes n , can be determined by the sum of n original variables and the inserted number of intermediate variables, as in Equation 2.6.\n2( 2)( 3) 3 6\n2 2 n\nn n n n n        (2.6)\nThe complete BF notation introduced in this section will be applied to chapter 5 for developing TRC algorithm."}, {"heading": "2.5. Converting a sparse graph into a DCCD", "text": "A continuous sparse graph model, G , can be converted to a DCCD, 'G , while preserving associated CPDs by:\n1. Ordering all original variables from parent to child, with children allocated\nhigher valued labels than their parents;\n2. Adding edges for each variable to all of its higher labelled descendant\nvariables (Each variable is then connected via a path to its descendants);\n3. Assigning each original variable the new CPD defined on each variable and\nits ancestors, by blocking those unrelated ancestors {ancestors\\parents} by setting zeros, as weights, to the new conditionally deterministic expression, with respect to the variables in {ancestors\\parents}.\nIf a CPD is conditionally deterministic: 5 1 2 4 1 2 3 40X X X X X X X X        ,\nthe expression for 5X is defined on three parents, but it also can be defined on all of\nits four ancestors to satisfy a DCCD, with a block on one ancestor, in this case 3X , as encoded in the DCCD with a zero. Once we have converted the sparse graph to a DCCD we can then convert it to a full-BFG.\nFor discrete and hybrid BNs, the conversion is feasible, but is subject to ongoing research.\nExample 2.3\nHere we transform a sparse BN, G , into a DCCD and then a 5 full-BFG.\nWe consider a sparse original BN G in Figure 2.8. The variables are labelled in\nparent to child order. We start from lowest labelled variable, 1X , so it adds\nedges to all its higher labelled variables, 2 3 4, ,X X X (dashed lines in DCCD in"}, {"heading": "3. Inference in Belief Networks", "text": "This chapter discusses the popular exact and approximation inference of BN algorithms. Section 3.1 deals with exact inference, 3.2 with approximate inference and 3.3 and 3.4 are particularly focused on developments related to the TRC and DDBP algorithms."}, {"heading": "3.1. Exact Inference", "text": "There are different types of inference that can be performed in BNs, such as marginal inference, max-product inference and inference through finding the most probable states (Barber, 2012). In this thesis we focus our discussion solely on marginal inference. Marginal inference is concerned with the computation of the distribution of a subset of variables, with possible conditioning on another subset.\nConsider a 5 dimensional DCCD, with joint distribution:\n1 1 2 2 3 3 4 4 5 5( , , , , )P X x X x X x X x X x     and evidence 1X e . We can\nperform marginal inference to query a subset joint distribution 2 2 1( , )P X x X e  as equation 3.1.\n3 4 5\n2 2 1 1 2 2 3 3 4 4 5 5\n, , ( , ) ( , , , , ) x x x P X x X e P X e X x X x X x X x       \n(3.1)\nThe right term in equation 3.1 can be expressed by the chain rule of CPDs implied by the DAG G in equation 3.2.\n3 4 5\n2 2 1 1 2 2 1 3 3 1 2 2\n, ,\n4 4 1 2 2 3 3 5 5 1 2 2 3 3 4 4\n( , ) ( ) ( | ) ( | , )\n( | , , ) ( | , , , )\nx x x\nP X x X e P X e P X x X e P X x X e X x\nP X x X e X x X x P X x X e X x X x X x\n           \n         \n\n(3.2)\nEquation 3.2 is a discrete form of marginalization over a set of variables\n3 4 5{ , , }X X X . Rather than proceeding the marginalization over all three variables at\nthe same time, a more efficient way is to push the summation over each variable in\nthe set 3 4 5{ , , }X X X .\n3\n4\n5\n1 2 3 45\n2 2 1 1 2 2 1 3 3 1 2 2\n4 4 1 2 2 3 3\n5 5 1 2 2 3 3 4 4\n( , , , )\n( , ) ( ) ( | ) ( | , )\n( | , , )\n( | , , , )\nX\nx\nx\nx\nX X X X\nP X x X e P X e P X x X e P X x X e X x\nP X x X e X x X x\nP X x X e X x X x X x\n\n           \n    \n    \n\n\n\n(3.3)\nIn Equation 3.3 variable 5X only appears in one term of factors, so it can be\nmarginalized out individually to obtain the remaining factor 5 1 2 3 4 ( , , , )X X X X X . Then 4X is marginalized out and we repeat the process by marginalizing one variable out at a time. This procedure is called variable elimination (VE) (Barber, 2012), because each time a single variable is eliminated from the joint distribution.\nWe can view the VE process as passing a message to a neighbouring node on a tree (singly connected graph). One can calculate a univariate marginal of any tree by starting at a leaf node of the tree, eliminating the variable there and then obtain a subtree of the original tree to perform next VE process. If an original BN graph is not singly connected we can group a set of nodes into a single clique, by which we can obtain a join/junction tree to perform massage passing. To convert an original BN into junction tree, involves the use of tree-decomposition operations. Tree decomposition is the basis for many well-known algorithms, i.e. junction tree clustering (JT) (Jensen, Lauritzen, & Olesen, 1990) (Shafer & Shenoy, 1990) (Lauritzen & Spiegelhalter, 1988) (Jensen & Nielsen, 2009) and cluster-tree elimination (CTE) (Kask et al, 2005). JT is an exact inference algorithm. The decomposition is achieved by using a triangulation algorithm by embedding the BN\u2019s moral graph into a chordal graph, and using its maximal cliques as nodes in the junction tree. JT and other tree decomposition algorithms are all based on local computation where we compute the joint distribution by calculating the marginal for one or more clusters in the tree and then using message passing to update the other clusters in the same tree.\nThe JT algorithm consists of the following steps (Barber, 2012):\n1. Moralization: add edges for parent nodes, this is only required for DAG.\n2. Triangulation: this process produces a graph for which there exists a VE\norder that introduces no extra links in the graph, where every \u2018square\u2019 (loop of length 4) must have a \u2018triangle\u2019 with edge added to satisfy this criterion.\n3. Junction Tree: Form a junction tree from the triangulated graph, removing\nany unnecessary links in a loop on the clique graph. There is exactly one path between each pair of cliques. This is called singly connected.\n4. The resulting junction tree\u2019s cliques are all connected by\nintersections/separator, which are common variable sets between neighbouring cliques. This is also called the running intersection property.\n5. Factor Assignment: factors for each clique are assigned by multiplying all\nits associated NPTs induced by the variables contained. Separators are assigned by uniform factors.\n6. Message Propagation: perform absorption (sending messages to neighbours)\nand distribution operations (receiving messages from neighbours) to pass update messages to all cliques and separators in the junction tree. The resulting marginal for each variable in different cliques are now consistent.\nThe efficiency of the JT architecture depends on the size of the cliques in the associated tree and the inference for both exact and approximate is NP-hard (Cooper & Herskovits, 1992). Although JT based algorithms are intended to produce junction trees with minimum clusters size for many models the complexity of JT algorithm grows exponentially with clique size as the number of discrete variables and states increases, thus making the computation of the marginal distribution very costly or even impossible.\nConsider the full-BFG 'G in Figure 2.7, and compare it with the original BN, G . The largest factor size of a single NPT is reduced from 5m to 3m , where m is the number of partitions. However, the largest clique size is 5m because of the triangulation performed during JT construction. The BF procedure has only reduced the computation complexity for the NPTs but not of the cliques, i.e. to compute the\nmarginal for 1X from the joint distribution the minimal clique (containing variables\n1X , 2X , 3X , 1E and 2E ) size is still five. Besides that, to find the most efficient\nand optimal triangulation is NP-hard, thus an alternative clustering algorithm is required. Section 3.4 introduces Generalized Belief Propagation (GBP), which offers the potential to be a credible alternative.\nWe will use the VE process in Chapter 4 for developing BFE algorithm. The JT algorithm will be discussed again in the context of dynamic discretization (Neil, Tailor, & Marquez, 2007) in Section 3.3."}, {"heading": "3.2. Approximate Inference", "text": "The most well-known approaches to approximate inference in BNs include Markov Chain Monte Carlo (MCMC) samplers, Variational Inference (VI), Dynamic Discretization with Junction Tree (DDJT), (Generalized) Belief Propagation (G/BP) and its variants, and BP-based continuous domain algorithms: expectation propagation (EP), Non-parametric Belief Propagation (NBP), Particle Belief Propagation (PBP). This section briefly reviews MCMC, VI, DDJT, BP/GBP, EP, NBP and PBP. GBP will be discussed in further in section 3.4.\n1. MCMC: MCMC is a stochastic simulation based method. There are a large\nnumber of specifically designed MCMC samplers (Gamerman & Lopes, 2006) that are widely applied to Bayesian inference. MCMC samples from probability distributions based on constructing a Markov chain that has the desired distribution as its equilibrium distribution and then the target distribution is obtained from the sample states of the chain. MCMC is not restricted in the number of model variables and so scales up very well, but there are still some open problems to be solved when using MCMC. For example, handling multimodal distributions 2 is difficult and guaranteeing convergence for arbitrary models can be a major problem. When designing an MCMC algorithm there is always a balance to be found between exploiting information to adjust the parameters and searching for new regions of the sample space (Ceperley et al., 2012). When the models are complex and\n2 For example, multi-modal distributions with multiple peaks and narrow variance around each peak are difficult to handle using MCMC.\nhybrid the necessary MCMC algorithms have to be specially tailored, thus involving significant labour and testing.\n2. VI: Analytical approximation methods have been developed to achieve much\nhigher computational efficiency compared to simulation based approaches such as MCMC. The mainstream methods include \u2018variational\u2019 approaches (Beal, 2003) and the mean field algorithm [17], a simplified variational algorithm (Jordan, et al, 1998), which involves choosing a family of distributions over the latent (as opposed to observable) variables with their own variational parameters. Then, it searches for the parameters settings that make the chosen parameter distributions closer to the true posterior of interest. Recent work on variational inference (Wang & Blei, 2013), has explored a generic algorithm for non-conjugate models, involving extending variational inference to a broader range of models, but despite this the analytical effort can be prohibitive.\n3. DDJT: The Dynamically Discretized Junction Tree (DDJT) algorithm (Neil et\nal., 2007) is a combination of the JT algorithm and Dynamic Discretization (DD), where JT performs inference over a set of discrete variables and DD transforms the continuous variables in the model using discrete approximations. The advantage of using DDJT over the other approaches is that it can easily perform discrete inference on a hybrid BN. The approximation error is only introduced by discretization and DD performance is much superior to static discretization. DDJT performance is robust and accurate; it has been applied in numerous domains: systems reliability modelling (Neil, et al, 2008) (Marquez, Neil, & Fenton, 2010) , operational risk modelling in finance (Neil & Fenton, 2008) and others (Fenton & Neil, 2010) (Neil & Marquez, 2012) (Fenton & Neil, 2012). However, the curse of dimensionality is a major barrier in DDJT since the Node Probability Tables (NPTs) needed in JT is exponential to the number of parent node states. Likewise, the triangulation operation in the JT algorithm means that the cluster sizes (the basic data structures in JT) also grow exponentially.\n4. BP: Belief Propagation (BP) (Pearl, 1988) (Kschischang, Frey, & Loeliger,\n2001), also known as sum-product message passing, is a message passing\nalgorithm for performing inference on undirected joint tree/graph models. Unlike most sampling schemes, BP does not suffer from high variance and is often much more efficient (Welling, 2004). When the graphical model represents a BN whose corresponding factor graph 3 forms a tree, the inference is equivalent to the exact inferences obtained by the JT algorithm (Yedidia et al., 2005). If the factor graph contains loops it is an approximation. Performing BP iteratively on networks that contain cycles yields loopy belief propagation (LBP, also called iterative BP) (Murphy, Weiss, & Jordan, 1999). However the convergence of LBP is not guaranteed, although LBP has achieved success on coding networks (Mateescu et al, 2010). Survey Propagation (SP) is a version of LBP, which is an improvement of BP and has been successfully applied to NP-complete problems like satisfiability (Braunstein, M\u00e9zard, & Zecchina, 2005), but SP does not guarantee convergence either. Exact inference, i.e. JT, maybe introducing large clusters during belief updating, Mini-Clustering belief updating (MC-BU) algorithms (Kask, 2001) (Mateescu et al, 2002) (Dechter & Rish, 2003) partition large clusters into bounded (user specified) mini-clusters, and use max/mean operators in all of its mini-clusters to derive a strict upper bound of the joint belief. The performance of MC-BU algorithm is still limited on specific networks (Mateescu et al., 2010). The Iterative Join Graph Propagation (IJGP) algorithm (Mateescu et al., 2010) combines the idea of mini-clustering and iterative BP and produce a join graph decomposition based on bounded mini-cluster size. BP is carried out on the resulting cyclic join graph. IJGP introduces cycles in the join graph, so it also uses edge separation algorithms to avoid over counting each variable. It enables trade-offs between accuracy and complexity based on user defined mini-cluster size. Mateescu (Mateescu et al., 2010) shows that IJGP surpasses other join graph based BP algorithms, i.e. Edge Deletion Belief Propagation (EDBP) (Choi & Darwiche, 2006), Truncated Loop Series Belief Propagation (TLSBP) (G\u00f3mez, Mooij, & Kappen, 2007), and surpasses or is equal to iterative BP and MC-BU for specific networks.\n3 A factor graph is a bipartite graph representing the factorization of a function. So in the case of a BN the function is the associated joint probability distribution function.\n5. EP, NBP and PBP: Expectation propagation (EP) (T. P. Minka, 2001), non-\nparametric belief propagation (NBP) (Sudderth, Ihler, Freeman, & Willsky, 2003) and particle belief propagation (PBP) (Ihler & McAllester, 2009), characterize messages in the continuous domain, where additional approximations are developed using BP decompositions. EP uses an iterative approach that leverages the factorization structure of the target distribution p ,\nwhere large joint factors are factorized into more compact factors, where the resulting joint distribution p is tractable and one sets free parameters that\nminimize the Kullback-Leibler distance, ( || )KL p p (Barber, 2012). EP is\nlimited to exponential families. NBP and PBP are both MCMC sampling based algorithms. NBP uses Gaussian mixtures to represent BP messages and it needs to smooth the sample set when taking products of messages, which is a further approximation. PBP sidesteps the shared collection of samples used in NBP by using separate message sampling strategies, which shown improved performance over NBP (Ihler & McAllester, 2009). Because they are both sampling based algorithms, performance is sensitive to the choice of proposed sampling distribution and sampling strategy and, in practice, the iterative message sampling procedure for NBP and PBP can make it difficult to achieve convergence (Ihler & McAllester, 2009).\n6. GBP: Generalized Belief Propagation (GBP) (Yedidia et al., 2005), is also an\nimprovement on BP and provides the generalized form for all message passing algorithms. GBP perform message passing on a region graph, which is a directed graph produced using flexible clustering schemes. There are connections between the region graph and join graph formalisms (Mateescu et al., 2010); if the edge directions in a region graph are removed it can be viewed as join graph with a particular clustering scheme. GBP is best explained with the context of statistical physics, where the simplest version of GBP is called Bethe approximation. A more general version is called Kikuchi (Kikuchi, 1951) approximation, where variables are grouped into clusters. GBP algorithms are applications of Kikuchi approximation. Yedidia (Yedidia et al., 2005) shows that GBP is flexible and can achieve accurate results provided that it converges. The most widely applied GBP algorithm would be Cluster Variation Method (CVM). There are numerous lines of GBP based\nalgorithms, i.e. join graph based algorithms (i.e. IJGP) can be converted to region graph based algorithms (i.e. CVM). However, the GBP algorithm has rarely been applied to factorized models, the exception being some research on its application to continuous Gaussian linear models (Bickson et al., 2008) (Shental et al., 2008), where the emphasis is on analytical solutions to the Gaussian case only. In practice general purpose inference using GBP has to use heuristics to convert a factor graph into a region graph, where a good construction of such a region graph can be difficult to find and is nongeneralizable."}, {"heading": "3.3. Dynamically Discretized Inference Algorithm", "text": "This section introduces dynamic discretization (Neil et al., 2007), alongside the Junction Tree algorithm, to perform approximate inference on hybrid models. Different with conventional approximation inferences, dynamic discretized inference algorithm uses exact inference JT algorithm. The approximation error is introduced by discretization, which is more accurate than static discretization methods."}, {"heading": "3.3.1 Dynamic Discretization", "text": "If continuous variables are represented by mixtures of constant uniform distributions, the overall BN variables can be represented universally using only discrete variables. Conventional static discretization has historically been used to approximate the domain of the continuous variables in a BN using predefined, fixed piecewise constant partitions. This approximation will be valid when the posterior high density region remains in the specified domain during inference. However the analyst has no advanced knowledge about which areas of the domain require the greater number of intervals thus resulting in an inaccurate posterior estimate. Dynamic Discretization (DD) is an alternative discretization approach that searches for the high density region during inference. It dynamically adds more intervals where they are needed, and removes intervals where they are not (by merging or deletion). The algorithm iteratively discretizes the target variables by the convergence of relative entropy error threshold (Kozlov & Koller, 1997).\nKozlov (Kozlov & Koller, 1997) proposed a non-uniform dynamic discretization algorithm for hybrid networks. The discretization of continuous variables is done at the level of join tree cliques, where they use a recursive binary split partition (BSP) tree data structure to decompose the multidimensional hierarchical space generated by the clique. Leaves of the BSP tree store the average constant value of the function over a sub-region. The discretization interleaves the join tree propagation algorithm, and iteratively improves accuracy by calculating the weighted KL distance between multivariate distributions and splitting regions that improve this. The approach is accurate since it is performed over multidimensional cliques directly, rather than subsets of them, but it can be slow because the number of samples needed to determine where the next optimal split should take place is determined by the size of the clique. Likewise, implementing the algorithm is challenging since the algorithms for manipulating BSP tree are more complex than those needed for plain junction trees. Compared to Kozlov\u2019s approach, DD discretizes each variable separately based on the marginal distribution and uses the standard junction tree message passing algorithm. This is less accurate than Kozlov\u2019s approach, demands more space, but converges to the same result and is faster.\nIn addition to the above splitting strategies for representing CPD tables, Friedmen (Friedman & Goldszmidt, 1996) describes a method that uses local structures in CPD tables during the learning process and this can reduce the tabular size for learning. The local structures used employ knowledge about context specific independence, described by either asymmetrical tables or decision trees (Friedman & Goldszmidt, 1996). This structure reduces the number of parameters required in CPDs. In contrast with DD, Friedman\u2019s approach limited to discrete case only and is rarely implemented.\nIn outline the DD algorithm is introduced below:\nSuppose X is a continuous random node in a BN. The range of X is denoted by X , and the probability density function of X , with support X , is denoted by Xf . The idea of discretization is to approximate Xf as follows:\n1. Partition X into a set of interval { }X jw  ,\n2. Define a locally constant function xf on the partitioning intervals.\nWe estimate the relative entropy error induced by the discretized function using an upper bound of the KL metric between two density functions f and g :\n dxxg xf xfgfD )( )( log)()||( (3.4)\nUnder the KL metric the optimal value for the discretized function xf is given by the mean of the function xf in each of the intervals of the discretized domain. The discretization task reduces then to finding an optimal partition set \u02c6 x .\nDD searches X for the most accurate specification of the high-density regions given the model and the evidence, calculating a sequence of discretization intervals\nin X iteratively. At each stage in the iterative process, a candidate discretization,\n x jw  , is tested to determine whether the relative entropy error of the resulting\ndiscretized probability density xf is below a given threshold, defined according to some stopping rule. After each variable in the model is discretized, the underlying inference algorithm, e.g. Junction Tree, is performed to calculate the joint posterior distributions for all variables in the model. This produces a new posterior probability density for all variables and these are then re-discretized in the next iteration. This process continues until a stopping rule is triggered, i.e. where the KL distance reaches some target."}, {"heading": "3.3.2 Dynamically Discretized Junction Tree Algorithm", "text": "DD offers the potential to be constructed with any inference engine, e.g. JT algorithm, or other types of inference algorithms. The discretization strategy is determined by the relative entropy error resulted from inference, and then new factors can be generated for NPT tables in order to perform the next propagation. The NPT generation is determined by discretized partitions and deterministic or statistical functions, where the child and its parents\u2019 partitions are combined to form an NPT, either using a set of uniform mixtures, in the case of conditionally deterministic functions, or directly from a statistical distribution.\nFor example for a conditionally deterministic distribution, X Y Z  , the NPT for\n( | , )p X Y Z can be determined by (Neil et al., 2007):\n( | [ , ], [ , ])\n(min( , , , ), max( , , , ))\nu v u v\nu u u v v u v v u u u v v u v v\np X Y y y Z z z\nUniform y z y z y z y z y z y z y z y z\n \n (3.5)\nThe new generated NPT is converted to factors for the JT tree to perform next iteration. We summarize the dynamic discretized junction tree (DDJT) algorithm as (Neil et al., 2007):\nAlgorithm 1 Dynamic Discretized Junction Tree (DDJT) Input: original BN G Output: original BN G with marginals 1. Build a Junction Tree to determine the cliques, R , and separators 2. Assign the clique\u2019s potentials/factors )( kR by multiplying the NPTs/factors\n( | { })P X pa X for all variables into the relevant cliques\n3. Assign uniform potentials/factors for all separators 4. Initialize each clique\u2019s discretization (0) k\u03a8 , by its support k\u03a9 5. Do Compute the approximate NPTs,   0\n( | { })P X pa X , on  1l X   for all variables\nX and initialize the clique\u2019s factors\n6. Query the BN from node E X e 7. for 1 to max_num_itel  repeat 8. Create a new discretization ( )l k\u03a8 for clique domain k\u03a9 9. Perform absorption and distribution of messages on the JT 10. Compute the new discretized potential )()1( k l R 11. Compute the approximate total relative entropy error 12. end for 13. until convergence of the posterior marginal for each clique by stable entropy error stopping rule and low entropy error stopping rule 14. Extract marginal for each node from the relevant clique 15. return G\nAlgorithm 1 DDJT algorithm\nExample 3.1 Convolution X Y Z  Consider the example BN model in Figure 3.1\nFigure 3.2 illustrates the results from applying the DDJT iteratively to approximate the posterior marginal of X . Figure 3.2 left shows the result after two iterations with a relative entropy error for X of approximately 0.01, and after 16 iterations (Figure 3.2 right) the model converges to an acceptable marginal distribution with a relative entropy error for X of 0.001. After 25 iterations the marginal distribution of X is approximated with a mean 5 and variance 51.\nThis thesis discusses the DDBP algorithm in chapter 5, where the JT inference engine is swapped out and replaced by the GBP inference engine."}, {"heading": "3.4. Generalized Belief Propagation", "text": "Generalized Belief Propagation (GBP) is a generalization of all message passing algorithms, including JT, and allows flexible clustering, where cluster size can be adjusted. In all standard BP algorithms (Kschischang et al., 2001), messages are sent from one node to a neighbour node in a factor graph. Propagation is exact when the factor graph has no cycles, but for factor graph containing cycles we can only\nperform approximate propagation. Although the BP algorithms are well defined when factor graph have cycles, convergence is sometimes not achieved (Yedidia et al., 2005).\n(Yedidia et al., 2005) generalized the BP algorithm and demonstrated that BP convergence is equivalent to stationary points of the Bethe approximation of the free energy of a factor graph. They demonstrated that the Generalized BP (GBP) algorithm obtained from the region based free energy approximation, improved the Bethe approximation and achieved better accuracy than ordinary BP. These gains are achieved by constructing a region graph (with factor graph nodes being clustered) as an alternative to a factor graph. The region based free energy corresponds to the difference of variational average energy of region beliefs and region entropies and when the region beliefs are the same as the joint probability distribution the free energy is minimized. Furthermore, they also demonstrated that a valid construction of the corresponding region graph can be specified by the Bethe Method (BM), Junction Graph Method (JGM) and the Cluster Variation Method (CVM). As we have noted in section 3.2, the join graph based algorithm, i.e. IJGP can be converted to CVM and vice versa. These connections provide some justification for the development of the TRC algorithm in this thesis, as described in Chapter 5."}, {"heading": "3.4.1 Converting a BN to a Markov Network", "text": "GBP is defined on an undirected graph and this requires the conversion of a BN to a\nMarkov Network (MN). For a set of variables 1{ ,..., }nX XX , a MN is an\nundirected graph G , and is defined as a product of factors, ( )c c X , on subsets of the\nvariables c X X :\n1\n1\n1 ( ,..., ) ( )\nC\nn c c\nc\np X X Z     X , (3.6)\nwhere 1,...,c C are the maximal clusters of G, and where Z is a normalization\nconstant.\nThe conversion from BN to MN involves two steps:\n1. Convert BN parameterization to MN parameterization:\n{ } { }( | { }) ( , { })i ii i X pa X i ip X pa X X pa X .\n2. Connect all parent nodes that have the same child node, and convert all\ndirected edges into undirected edges.\nThe converted MN graph is also called a moral graph, since the parents are connected.\nFigure 3.3 shows the results of a conversion of a BN G to its corresponding moral graph. The CPD ( | { })i ip X pa X in G for each variable is re-parameterized to\n{ } { }( , { })i iX pa X i iX pa X ."}, {"heading": "3.4.2 Factor Graphs", "text": "Both BNs and MNs can be represented by a unifying representation called a Factor Graph (FG) (Kschischang et al., 2001). FGs explicitly express the factorization structure of the corresponding probability distribution. Standard BP performs message passing on factor graphs.\nAn FG is a particular type of graphical model with applications in Bayesian inference that enables efficient computation of marginal distributions through the sum-product algorithm 4 (Koller & Friedman, 2009) , (Kschischang et al., 2001). It is a bipartite graph representing the factorization of a function. Given a factorization of\na function 1 2 1\n( , ,..., ) ( ) m\nn j j\nj g X X X f S   , where 1 2{ , ,..., }j nS X X X , the\ncorresponding factor graph ( , , )G X F E consists of variable vertices\n4 The sum-product algorithm defined as to compute marginal we need to distribute the sum over variable states over the product of factors.\n1 2{ , ,..., }nX X X X , factor vertices 1 2{ , ,..., }mF f f f , and edges E . The edges\ndepend on the factorization as follows: there is an undirected edge between factor\nvertex jf and variable vertex kX when k jX S . The function is tacitly assumed to\nbe real valued. FGs can be combined with message passing algorithms to efficiently\ncompute certain characteristic of the function 1 2( , ,..., )ng X X X , such as the marginal distribution.\nIn Figure 2.5 'G , the joint distribution of BN representation is factorized by the FG\nin Figure 3.4, where 1 2 3 1 4{ , , , , }X X X E X are variable nodes, 1 1 5 3 1 4{ ,..., }f X f X E X are factor nodes.\n1 2 3 1 4 1 1 2 1 2 3 1 3 4 4 1 2 1 5 3 1 4\n1 ( , , , , ) ( ) ( , ) ( , , ) ( , , ) ( , , )P X X X E X f X f X X f X X X f X X E f X E X\nZ "}, {"heading": "3.4.3 Belief Propagation on Factor Graph", "text": "For a given FG with variables 1,..., NX X , the joint probability mass function is\n1 1 1 ( ,..., ) ( ) ( )N N a a\na\nP X x X x p f Z     x x (3.7)\nWhere x is he set },...,{ 1 Nxx . Generally we are interested in computation of\nestimating marginal  S pp SS xx xx \\ )()( .\nDepicted in Figure 3.5, the message passing in a FG involves two steps:\n1. Messages )( iia xm  from factors to variables: what values does factor a like\nvariable iX to take on.\n2. Messages )( iai xm  from variables to factors: what values iX likes based on\ninformation from all but a .\nRefer to information theory, we define variable and factor nodes\u2019 Beliefs (Yedidia et al., 2005):\n1.    )( )()( iNa iiaii xmxb , are based on all pieces of information coming into iX ,\nthe product is independent pieces of information.\n2.    )( )()()( aNi iaiaaaa xnfb xx , are based on product of local factors and\nmessages coming from variables.\nThe message updating rule is defined in the following three steps and illustrated in Figure 3.6.\n1. The belief of every variable is \\ ( ) ( ) i i i a a x b x b  ax x , 2. The product of all the messages coming from variable i \u2019s neighbour factor\nnodes that are used for calculating the local beliefs:\n'\n\\ \\( ) ( ) ( ) ' ( )\\\n' '\n\\' ( )\\ ( )\\\n( ) ( ) ( ) ( ) ( )\n( ) ( ) ( )\na i a i\na i\na i i a a j a j a a a j j\nx xa N i j N a j N a a N j a\na i i a a a j j\nxa N i a j N a i\nm x f n x f m x\nm x f m x\n  \n   \n \n \n \n\n    \n \nx x\nx\nx x\nx\n(3.8)\n3. Messages for each factor to variable nodes are computed by marginalization\nconstraint:\n\\ ( )\\ ( ) ( ) ( ) a i a i i a a j a j x j N a i m x f n x      x x (3.9)\nWhen BP is performed on the factor graph messages are sent between factor nodes and variable nodes. The BP convergence is equivalent to stationary points of the Bethe approximation (Yedidia et al., 2005) of the free energy of this FG."}, {"heading": "3.4.4 Region Free Energy and Region Graph", "text": "In all standard BP algorithms, messages are sent from one node to a neighbour node in a graphic representation. Propagation is exact when the graphic model has no cycles, but for models with cycles we can only perform propagation in an approximate manner.\nFigure 3.7 is identical to Figure 3.5 but illustrates the cycles contained in the factor graph. The standard BP performance on this graph is an approximation.\nYedidia, et al (Yedidia et al., 2005) generalized the BP algorithm and demonstrated that BP convergence is equivalent to constructing the free energy of a system. Rather than propagate messages among nodes in a FG GBP operates on a Region Graph (RG) which is a graphical formalism for generating free energy approximations.\nThere are several ways to define the regions in a graph to support message exchange. Yedidia, et al has shown that a valid construction of the corresponding RG can be specified by several clustering algorithms: Bethe Method (BM), Junction Graph Method (JGM) and the Cluster Variation Method (CVM).\nA RG is constructed as follows. Let I be the set of indices for the factor and variable nodes in a factor graph. A RG is a labelled, directed graph ( , , )G V E L in\nwhich each vertex v V (corresponding to a region) is labelled with a subset of I . We denote the label of vertex v by ( )l v L . A directed edge Ee may exist\ndirected from vertex pv to vertex cv if ( )cl v is a subset of ( )pl v . If such an edge\nexists, we say that cv is a child of pv , that pv is a parent of cv . If there exists a\ndirected path from vertex av to vertex dv , we say that av is an ancestor of dv , and\ndv is a descendant of av . For a graph G to qualify as a RG, we require the following region graph constraints (Yedidia et al., 2005):\n1. Regions can be organized as a directed acyclic graph, 1 2R R only if\n2 1R R , where all factors a and all variables i are included.\n2. The marginalization constraint for region beliefs: 1 1 2 2\n1 2\n( ) ( ) R R R R R R x x b x b x   \n(where 2 2 ( )R Rb x are probabilities/beliefs).\n3. The subgraph of all regions containing each factor, a , or variable, i , is\nconnected so that the region graph gives consistent beliefs about them.\n4. ' ' ( ) 1R R ancestors R A R c c     , where 'Rc is degree of freedom (number of parent\nregions) and Rc corresponds to counting number of each region.\n5. For every Ii (whether it is the index of a factor node or a variable node),\nthe sub graph ( ) ( ( ), ( ), ( ))G i V i E i L i formed by just those vertices whose\nlabels include i is a connected graph that satisfied the condition:    )( 1 iVv vc .\nMaintaining the counting number at one is important to ensure that variables are not under or over counted during inference. Note that BP produces exact results if the resulting region graph forms a tree and satisfies all of the constraints above.\nThe BM is always an exemplar of the JGM and is only a special case of the CVM if the factor graph does not contain any pairs of factor nodes that share more than one variable node. CVM is more flexible and easy to use than other methods and we will use CVM as reference algorithm for the TRC algorithm in later chapters. Informally, the steps in the CVM algorithm are (see Example 3.2):\n1. Define the first level regions, 0R , such that every factor node, a , and every\nvariable node, i , in our factor graph is included in at least one region 0R R .\nThere must be no region 0R R that is a sub-region of other regions in 0R .\n2. Construct second level regions 1R by forming all possible intersections\nbetween regions in 0R , but discard from 1R any region that is a sub-region of\nother regions in 1R .\n3. If possible, repeat step 2 for 0R U 1R to form 2R , resulting in a final set of\nregions, 0RR U 1R U ... kR .\nExample 3.2 shows the construction of a region graph using CVM.\nExample 3.2\nConsider the original BN in Figure 3.8, G . The conversion of this BN to its moral graph involves changing all directed edges to undirected edges and\nadding an undirected edge between each pair of parents (dashed lines in MG ,\nFigure 3.8). All CPDs in G then need to be re-parameterized to factors in MG .\nFor example, the CPD 1 1 2( | , )p E X X is re-parameterized to 1 1 2( , , )E X X , and\nother smaller factors like 1( )X , 2( )X can be multiplied together to give\n1 1 2( , , )E X X . The resulting factors are then 1 1 2( , , )E X X , 2 3 4( , , )E X X and\n1 2 5( , , )E E X . we can construct the region graph based on its moral graph MG , and use these factors we obtained above. These are shown in Figure 3.8.\nX2X1 X3 X4\nX5\nG GM\nE2E1\nX2X1 X3 X4\nX5\nE2E1\nGRG\nE1X1X2 E2X3X4E1E2X5\nE1 E2 -1 -1\n1 1 1\nFigure 3.8 Generate region, RGG , graph for a BN, G\nFigure 3.8 illustrates the RG using the CVM construction method. The CVM algorithm requires that all of the factors and variables appear in the first level regions.\nThe first level regions derived from MG are: 1 1 2E X X , 2 3 4E X X and 1 2 5E E X . The\nresulting region graph, also show in Figure 3.8, is RGG , which is a two-level acyclic graph. Moreover, it is singly connected 5 so it is equivalent to a JT.\nIf the region graph contains cycles, the approximation is not guaranteed and sometimes it can fail to converge at all, so it is preferable to construct a region graph that is a tree.\nThe choice of which region graph to use with GBP is an open research question, since it is not well understood which region graph topologies result in good approximations and which do not. Welling et al. (Welling, Minka, & Teh, 2005) (Welling, 2004) (Gelfand & Welling, 2012) discuss ways to structure region graphs based on graphic topology and offer guidance based on structural information criteria, a sequential approach where new regions are added bottom-up to the region graph, and tree-robustness. In practice, most region design is guided by constructing good approximations to the free energy of the problem.\nThere is very little research on how to ensure the interaction strength (refers to what and how many regions should be chosen) between regions is approximated, in terms of maintaining the interdependencies of the joint distribution; instead most research concerns structural information about the graph. Welling (Welling, 2004) has mentioned the impact of interaction strength by adding extra candidate regions to the\n5 There is only one path between pair of clusters.\nexisting region graph, where approximation can be improved by testing and choosing additional external regions from a candidate pool. We will discuss the region interaction strength in chapter 5 when develop the TRC algorithm. Below we briefly discuss one basic criterion, called Maxent-Normal constraint with reference (Yedidia et al., 2005) to free energy in statistical physics, to evaluate an already built region graph, shown below.\nIn the Section 3.4.3 in Equation 3.7 we have shown that the probability for state x is\n1 ( ) ( )a a\na\np f Z  x x . An energy term in statistical physics is defined as:\n( ) log ( )a a a E f x x . So by substituting ( )E x we produce Boltzmann\u2019s Law,\n( )/1( ) , ( ) E Tp e Z T  xx where ( )/( ) E T\nS\nZ T e\n  x x\nis a partition function with S , the\nspace of all possible states x of the system and the temperature, T (Boltzmann, 1884).\nUsing Gibbs free energy (Yedidia et al., 2005):\n( ) ( ) ( ) ( ) ( ) ( ) S F b b E H b U b H b      x x x (3.10)\nwhere ( )U b is the variational average energy, and ( ) ( ) log ( ) S H b b b    x x x the variational entropy. The belief ( )b x is an estimate probability and we need to attain\nTEe TZ b /)( )(\n1 )( xx  when )()( xx pb  , and substitute ( ) ( ) log ( )\nS H b b b    x x x we\nget HFTZbF  )(log)( , where HF is the Helmholtz free energy that can be used to recover our target function ( )Z T to calculate ( )p x . Thus we obtain the following:\n( ) ( || )HF b F KL b p  (3.11)\nSince ( || )KL b p is non-negative, ( ) HF b F , and with ( ) HF b F only when )()( xx pb  . So the task is to minimize the free energy ( )F b . When minimising\n( )F b computation of ( )H b is expensive, so a better solution is to approximate\n( )H b as a function of marginal beliefs. In the region graph representation, we\nusually replace b by a family of marginal region beliefs and introduce a set of\nconstraints (Yedidia et al., 2005) (i.e. ensuring global consistency by connections and local consistency) on these beliefs.\nWe can derive the similar procedure based on region beliefs )( RRb x to approximate the true probability ( )R Rp x . The free energy for each region is:\n( ) ( ) ( ) ( ) log ( )\n( ) ( )\nR R\nR R R R R R R R R R\nR R R R\nF b b E b b\nU b H b\n \n \n  x x x x x x (3.12)\nThe region based free energy is:\n  R RRR R RRRR bHcbUcbF )()(})({ (3.13)\nWhere Rc corresponds to counting number of each region. The constrained region free energy ( )R RF b must be minimized. The relationship between minimizing a system free energy and maximizing the system entropy at the system equilibrium is well known in the theory of thermodynamics (Callen, 2006), which proves that for a closed system with fixed internal energy the entropy is maximized at equilibrium (fixed point of GBP), and the free energy (i.e. Gibbs) is minimized at equilibrium given fixed entropy.\nSo we are most interested in the accuracy of the constrained region based entropy near its maximum (Yedidia et al., 2005). The maximum of the true entropy occurs when the joint probability distribution is uniform. Yedidia (Yedidia et al., 2005) introduced a similar property to hold for constrained region based entropies,\nMaxent-normal: a constrained region based free energy approximation is maxentnormal if it is valid and the corresponding constrained region based entropy\n({ })R RH b achieves its maximum when all the beliefs ( )R Rb x are uniform.\nWe aim to maximize the overall region entropy ({ }) ( )R R R R R R H b c H b when all beliefs are uniform. As pointed out by Yedidia (Yedidia et al., 2005), if the region based approximation is not maxent-normal one cannot expect a good result, because it will always produce the wrong answer even when there is no energy term. We will use the maxent-normal property as a requirement to our TRC algorithm in Chapter 5."}, {"heading": "3.4.5 GBP Message Passing", "text": "There are several ways of message passing in GBP: parent to child, child to parent and two way message passing. All message passing algorithms are derived from belief equations. Each algorithm has its advantages and disadvantages but here the two way message algorithm is used as the basis for the TRC algorithm, since TRC guarantees to provide a region graph to be a DAG. Therefore here our discussion will focus on two way message passing. The two way message algorithm is particularly elegant when each region and its sub-regions forms a tree, and all factors appear in the first level of region graph (Yedidia et al., 2005).\nIn the two way message algorithm the belief equation in a region is a product of local factors and messages arriving from all the connected regions, whether they are parents or children, as in Equation 3.14.\n( ) ( ) ( ) ( ) ( ) ( )R R R R C R C P R P C c R P p R b f n m      x x x x (3.14)\nThe two way message algorithm uses the region counting numbers Rc and parent\nnumbers of region R as input parameters to compute a message parameter R , then it defines a set of pseudo-messages for all regions R and their parents P and children C , as pseudo-messages is computed prior to real message updating. Then the real messages are a mixture of these pseudo messages.\nThe set of pseudo-messages for all regions R and their parents P and children C :\n0 ' ' ( )\\ ( ) ( ) ( ) ( ) ( )R P R R R P R R C R C P p R P C c R n f m n       x x x x (3.15)\nAnd\n0 ' ' \\ ( ) ' ( )\\ ( ) ( ) ( ) ( ) R C R C C R R P R R C R C P p R C c R C m f m n         x x x x x x (3.16)\nWhere  ( ) ( ) R\nr\nc\nR R a A a af f x x . The real message used in the two directions of a\nlink is the mixture of these pseudo-messages:\n    10 0( ) ( ) ( )R RR P R R P R P R Rn n m      x x x (3.17)\nAnd\n   10 0( ) ( ) ( )R RP R R R P R P R Rm n m     x x x (3.18)\nFor simplicity we omit the parameter R \u2019s definition equations here, which can be referred to (Yedidia et al., 2005).\nOnce the RG is produced the two way message passing is performed using a depth first search algorithm for the updating order of region edges. When each propagation has completed the old messages are replaced by new messages, and the region belief is calculated by the product of these messages with local factors. Each propagation may be composed of multiple iterations updating the region edges."}, {"heading": "4. BFE Risk Aggregation", "text": "This chapter covers Bayesian risk aggregation algorithms for hybrid models. Section 4.1 provides an overview of popular methods for risk aggregation. Section 4.2 illustrates the n-fold convolution using BNs. The BFE risk aggregation algorithm is described in Section 4.3 showing how it builds and extends on the standard BN algorithms. Section 4.4 presents a version of BFE that performs deconvolution and Section 4.5 presents experimental results showing the performance of BFE. Section 4.6 concludes the chapter."}, {"heading": "4.1. Risk Aggregation and BNs", "text": "An encyclopaedic overview of the current state of the art in risk aggregation is presented in (McNeil, Frey, & Embrechts, 2010). The general aggregation formula for fixed, n , assets, is:\n0 1 ... nT S S S    (4.1)\nwhere T is the sum of n asset valuations and each iS is from the same common\ncontinuous distribution xf , which can be thought of as a return (severity)\ndistribution S . This is called an n-fold convolution. If ~ xS f and if we have a variable number of assets, N , then Equation 4.1 can be rewritten as an N-fold convolution:\n*\n0 ( ) ( ) ( )jT j\nf x f x P N j \n\n  (4.2)\nwhere * *( 1)\n0\n( ) ( ) ( )j jf x f x y f dy\n\n  is a recursive n-fold convolution on S . We can\ntherefore rewrite Equation 4.2 in a discrete form: ( ) jP N j a  , for 0,1,...,j L ,\nwhere L is the length of discretized frequency N . The following expressions hold:\n0 0 0 1 1 1( ) ( ) ( ) ... ( )L L lP T t a P T t a P T t a P T t        (4.3)\n0 0 1 0 1 0 1, ,..., ...L LT S T S S T S S S       (4.4)\nwhere each jT is a constant n-fold convolution. The Equation 4.3 represents a\nmixture distribution where the mixture components consist of mutually exclusive variables, themselves composed using the conditionally deterministic functions stated in Equation 4.4.\nFor the sake of clarity in insurance, and similar, applications N is interpreted as a frequency distribution, and S is defined as a severity (loss) distribution whereas, for some other financial applications, the interpretation of the parameters differs: we could, equally validly, interpret, N , as a count of assets and S as the financial return from each asset.\nGeneral numerical solutions to computing the aggregate distribution include Panjer\u2019s recursion (Panjer, 1981), Fast Fourier transform (Heckman & Meyers, 1983). and Monte Carlo (MC) simulation (Meyers, 1980).\nIn this chapter severity variables can depend on discrete explanatory variables with dependencies expressed via conditioning in Bayesian networks. This contrasts with the classic approach for dependency modelling among severity variables using copulas. Rather than use dependency and conditioning the copula approach models the dependency structure independently with marginal functions, which supports the construction of high dimensional models.\nIn the context of copula based risk aggregation Bruneton (Bruneton, 2011) proposes the use of hierarchical aggregation using copulas. Also, Arbenz (Arbenz & Canestraro, 2012) proposes hierarchical risk aggregation based on tree dependence modelling using step-wise low dimensional copulas, and also gives a sample reordering algorithm for numerical approximation. Brechmann (Brechmann, 2014) suggests hierarchical Kendall copulas to achieve flexible building blocks, where risk aggregation is supported by the Kendall function. These approaches capture the joint dependencies from a hierarchical structure and exploit use of small building blocks. In contrast to correlation modelling, our work assumes causality and dependency, where joint dependency is decomposed by conditional dependencies using the Bayesian network framework.\nBNs have already been employed to address financial problems. For example, in (Cowell, Verrall, & Yoon, 2007) BNs were used for overall loss distribution and making predictions for insurance; in (Neil & Fenton, 2008) BNs were used for modelling operational risk in financial institutes, while the work in (Politou & Giudici, 2009) combines Monte Carlo simulation, graphic models and copula functions to build operational risk models for a bank. Likewise, (Rebonato, 2010) discusses a coherent stress testing approach using BNs.\nWe have chosen to use BNs because the latest algorithms can model causal dependencies between discrete and continuous variables during inference, to produce approximate posterior marginal distributions for the variables of interest. Also, by virtue of Bayes\u2019 Theorem they are agnostic about causal direction and can perform inference from cause to effect and vice versa (or convolution to de-convolution, as is the case here). Until very recently BN tools were unable to properly handle nonGaussian continuous variables, and so such variables had to be discretized manually, with inevitable loss of accuracy. A solution to this problem is DDJT algorithm described in section 3.3.2. The result of inference is a set of queries on the BN in the form of univariate or multivariate posterior marginal distributions. This allows the approximate solution of classical Bayesian statistical problems, involving continuous variables, as well as hybrid problems involving both discrete and continuous variables, without any restriction on distribution family or any requirement for conjugacy.\nWe have used AgenaRisk (AgenaRisk, 2014), a BN package and extended it to incorporate the new BFE algorithm and carry out the experiments described in Section 4.5.\n4.2. n-fold Convolution BF Process\nThe cost of using off-the-shelf BN algorithms to calculate N-fold convolution can be computationally expensive. The conditional probability density expression of node\nT is defined by all of its parent nodes by Equation 4.1: 0 1 ...n nT S S S    .\nIf each node has a node state of size m and the total number of parents is n , then the NPT for T has a total size of 1nm  given the intervals computed under DD. To help reduce the NPT size we employ binary factorization to factorize the BN graph according to the statistical and deterministic functions declared in it.\nTo illustrate the BF process, we consider constant n-fold convolution models for both the independent and common cause case, as represented by BNs 1G and 2G respectively in Figure 4.1. This is just Equation 2.1.\nAfter employing binary factorization, the BNs 1G and 2G are transformed into 1'G and 2'G respectively as shown in Figure 4.1. In Figure 4.1 1G shows the n-fold convolution when severities are independent and identically distributed. 2G denotes the n-fold convolution when severities are dependent on a discrete common cause random vector, C .\nBF ensures that, in the transformed BN, each variable\u2019s NPT expression involves has a maximum of two continuous parent variables in the transformed BN. This produces a maximal discretized NPT of size 3m . Consider the two BNs, 1G and 2G , shown in Figure 3. In each we can factorize the variable T by creating new variables\n1 2 1{ , ,..., }nT T T  where each is binary factorized by pair sum blocks (Equation 4.5):\n1 0 1{ }T S S  , 2 1 2{ }T T S  ,\u2026, 1{ }n n nT T S  (4.5)\nTheoretical equivalence of 1G and 1'G with the resulting BN models 2G and 2'G is given in (Neil et al., 2012)."}, {"heading": "4.3. Bayesian Factorization and Elimination (BFE)", "text": "To solve the N-fold convolution problem using off-the-shelf BN technology is not possible because we cannot compute 1G and 2G effectively from the conditional dependency structures defined in Figure 4.1. This is because, even with binary factorization, either the model size is prohibitively large (in the case of 1G ) or the junction trees clique sizes would be exponential in size (as with 2G ). Therefore, the original contribution of this thesis is to produce an iterative factorized approach to the computation that scales up to arbitrary sized models. This approach is called Bayesian Factorization and Elimination (BFE). This algorithm performs convolution on the hybrid models required to aggregate risk in the presence (or absence) of causal dependencies. This algorithm exploits a number of advances from the field of BNs already described in chapter 2. We refer to these advances as the BN engine and they are shown in the overall algorithm architecture in Figure 4.2.\nThe BFE algorithm contains three separate steps, each performing specific optimisations:\n1. Log Based Aggregation (LBA): this algorithm computes Equation 4.4, the n-\nfold convolution, in a log based pattern that can be more efficient than aggregation by straight summation.\n2. Variable Elimination (VE): variables are iteratively eliminated during LBA\nprocess, by which we can achieve greater computation efficiency for calculating arbitrary constant n-fold convolutions.\n3. Compound Density Factorization (CDF): the compound sum Equation 4.3\ncan be factorized by this algorithm in order to reduce large node probability tables into smaller ones. CDF is similar to binary factorization except that in CDF we introduce one more intermediate variable (a Boolean node) for weighting the compound density combination at each step in the aggregation process."}, {"heading": "4.3.1 Log Based Aggregation (LBA)", "text": "In Equation 4.3 each , 1,...,iT i n is the sum of its parent variables 1iT  and iS , the\naggregation process simply involves repeated summations of the same variable iS .\nAs binary factorization proceeds intermediate variables jF are created to aggregate\nevery two parents, creating a hierarchy until the total aggregate, T , is computed. An example, in the presence and absence of common cause vector is shown in Figure 4.3 (for convenience we have assumed the hierarchy has depth three and the other\nlevel contains intermediate variables labelled jF ). The computational efficiency of\nthis process is ( )O n .\nThis approach to aggregation is computationally expensive since all the variables are entered and computed in the BN explicitly. Log based aggregation simply computes and subsequently reuses prior computed results recursively, so that in each subsequent step we can reuse results from previous steps, without creating the whole\nBN. Thus rather than create and compute the BN as a whole, we create and reuse BN fragments and then remove (prune) those fragments of the BN we do not need. For instance, to sum four i.i.d. variables we would sum two variables and then add the result of this summation to itself to get the total; thus achieving the aggregate total in\ntwo rather than three steps. The resulting process is 2(log )O n , hence the name logbased aggregation."}, {"heading": "4.3.2 Variable Elimination (VE)", "text": "The aim of Variable Elimination (VE) is to remove nodes from a BN, G , that do not belong to a query set, Q , containing only the variables of interest, by a process of\nmarginalization. For simple uncorrelated aggregations this process is simple and obvious but in the presence of common causes it requires some care, since the nodes being eliminated will not be leaf nodes. Here we use variable elimination to reduce the number of variables we handle but add additional steps to exploit repeated structure in the binary factorized model. We do not need, therefore, to explicitly manipulate the whole BN, nor do we create a large junction tree or use the junction tree algorithm over this large tree, because we are not interested in setting arbitrary query variables or conditioning evidence. Instead we iterate through the binary factored model, progressively creating subsets of the aggregation hierarchy that can be reused recursively, eliminating nodes and reusing parts as we go (assuming i.i.d. severity variables).\nWe first consider a full binary factorized BN and use this to identify variables that can be eliminated and query sets necessary during VE. In the simple case for an nfold convolution for independent i.i.d. severity variables, Figure 4.1 1'G denotes the\nbinary factorized form of the computation of 0\nn\nn j\nj T S   after introducing the\nintermediate binary factored variables 1 2 1{ , ,..., }nT T T  . The marginal distribution for\nnT has the form:\n0 1 1\n0 1 1\n0 0 1 1 1 1 2 2 1 1\n( ..., , ,..., )\n1 1 1 1 2 2 1 1\n( ..., , ,..., )\n1 1 0 0 1 1 0 0\n( ) ( , ,..., , , ,..., , )\n( | , ) ( | , )...\n( | , ) ( )\nn n\nn n\nn n n n n n n n\ns s t t\nn n n n n n n n n n n n\ns s t t\nP T t P S s S s S s T t T t T t T t\nP T t T t S s P T t T t S s\nP T t S s S s P S s P\n\n\n \n       \n        \n      \n   \n\n\n1 1( )... ( )n nS s P S s \n(4.6)\n(Exploiting the conditional independence relations in Figure 4.1)\nNotice that every pair of parent variables iT and 1iS  is independent in this model\nand we can marginalize out each pair of iT and 1iS  from the model separately. Equation 4.6 can be alternatively expressed as predefined \u2018query blocks\u2019:\n1\n1 2 0 1\n1 1\n,\n2 2 1 1 2 2 1 1 0 0 1 1 0 0 1 1 2 2\n, ,\n( ) ( | , )\n... ( | , ) ( | , ) ( ) ( ) ( ) ...\n( )\nn n\nn n n n n n n n\nt s\nt s s s\nn n\nP T t P T t T t S s\nP T t T t S s P T t S s S s P S s P S s P S s\nP S s\n\n      \n        \n \n               \n\n \n(4.7)\nSo using Equation 4.7 we can recursively marginalize out, i.e. eliminate or prune,\neach pair of parents iT and 1iS  from the model. For example, the elimination order\nin Equation 4.7 could be: 0 1 1 2 1{ , },{ , }...{ , }n nS S T S T S . The marginal distribution of\nnT , i.e. the final query set, is then yielded at the last elimination step.\nIn order to illustrate the recursive BN graph operations, required during VE, consider Figure 4.1 and BN 1G . The first few steps involved are shown in Figure 4.4. We\nstart by taking the first pair of severity variables 0S and 1S and calculate the sum,\n0F , shown as the graph 1K . Once we have computed 1K we can reuse this\ncalculation in graph 2K , for severities 2S and 3S (of course if the severities are i.i.d\nwe can simply reuse the result at 0F and substitute this for 1F ). We are now\ninterested in using the marginal distributions of 0F and 1F in the next step so these\nare added to the query set and the nodes 0S and 1S are eliminated, thus reducing\ngraph 1K to 1L . Similarly for graph 2K we eliminate 2S and 3S to get a graph 2L .\nNext we can reuse the original structure in 1K and substitute 1K \u2019s leaf nodes with\n0F and 1F , and then compute 0A . The resulting 0A now becomes the query set and\nwe eliminate 0F and 1F to achieve graph 3L . At each stage we reuse the same graph\nstructures and expressions for graphs 1 2 3{ , , }K K K and 1 2 3{ , , }L L L . We can proceed through the binary factorized BN, computing the marginal distributions for the query set, removing elimination sets and repeating the process until we exhaust the variable list.\nHowever, in the case where common cause dependencies are present in the BN, as illustrated by 2G shown in Figure 4.1, additional care is needed during VE. Here the elimination set does not simply consist of leaf nodes that can be eliminated directly since we have a common parent node, C , that we want to preserve in the query set at each step. To help highlight how the VE process operates in the presence of common cause variables consider BN 'G and compute the posterior\nmarginal distribution for 2T . The marginal distribution for 2T has the form (4.8):\n0 1 2 1\n2 1\n0\n2 2 2 2 1 1 2 2 1 1 0 0 1 1 0 0\n, , , ,\n1 1 2 2\n2 2 1 1 2 2 2 2\n, ,\n1 1 0 0 1 1 0 0 1 1\n( ) ( | , ) ( | , ) ( | )\n( | ) ( | ) ( )\n( | , ) ( | ) ( )\n( | , ) ( | ) ( | )\nc s s s t\nc s t\ns\nP T t P T t T t S s P T t S s S s P S s C c\nP S s C c P S s C c P C c\nP T t T t S s P S s C c P C c\nP T t S s S s P S s C c P S s C c\n         \n     \n      \n       \n\n\n1,s         \n(4.8)\nWe first want to eliminate 0S and 1S by marginalizing them:\n0 1\n1 1 1 1 0 0 1 1 0 0 1 1\n, ( | ) ( | , ) ( | ) ( | ) s s P T t C c P T t S s S s P S s C c P S s C c         \n(4.9)\nThe marginal of 2T can now be expressed along with C , 1T and 3S alone:\n2 1\n2 2 2 2 1 1 2 2 2 2 1 1\n, , ( ) ( | , ) ( | ) ( | ) ( ) c s t P T t P T t T t S s P S s C c P T t C c P C c         \nNext we eliminate 2S and 1T :\n1 2\n2 2 2 2 1 1 2 2 2 2 1 1\n, ( | ) ( | , ) ( | ) ( | ) t s P T t C c P T t T t S s P S s C c P T t C c         \n(4.10)\nIn general, by variable elimination, we obtain the conditional distribution for each\nvariable 1nT  (the sum of n severity variables) with the form:\n2 1\n1 1 1 1 2 2 1 1\n,\n2 2 1 1\n( | ) ( | , )\n( | ) ( | )\nn n\nn n n n n n n n\nt s\nn n n n\nP T t C c P T t T t S s\nP T t C c P S s C c\n \n       \n   \n     \n    \n (4.11)\nSince Equation 4.11 specifies the conditional distribution for variable 1 |nT C ,\ntherefore the posterior marginal distribution for the target n-fold convolution 1nT  , the aggregate total, is obtained by marginalizing out C .\nIn order to explain the VE algorithm, in terms of graph manipulation, in the common cause case we step through a 3-fold convolution. Figure 4.5 (a) depicts a 3-fold convolution model, binary factorized (from G to 'G ) and then subject to VE, resulting in reduced the BN V . The VE steps are shown in Figure 4.5 (b), which,\nalthough operating on subsets of G , result in the same graph i.e. 2L V .\nTo calculate the arbitrary n-fold convolution in the multiple common cause case it is essential to maintain the structure connecting the common causes in 'G in every elimination step so that when variables are eliminated any dependencies on common cause variables are correctly maintained. Consequently the elimination task involves\ngenerating the marginal for variable jT conditional on the set 0 1, ,..., mC C CC . This\nmore general case is shown in Figure 4.6, with multiple common cause variables\n0 1, ,..., mC C C , and dependent severity variables, iS . It is easy to see how the scheme can be generalised to any configuration of common causes (e.g. including parent nodes of the common causes)."}, {"heading": "4.3.3 Compound Density Factorization (CDF)", "text": "The compound density factorization (CDF) involves the approximate inference in the context of mixture models. Similar approaches, such as Minka (Minka & Winn, 2008) proposed the \u201cGates\u201d representation of mixture models in factor graph, and discussed to apply a variety of approximate inference approaches (i.e. EP, message passing, Gibbs sampling) with gate models. Our CDF algorithm uses DDJT as the approximate inference for mixture models, and is an implementation version of cut set conditioning (Pearl, 1988).\nRecall the compound density expression for an N-fold convolution, as given in\nEquation 4.3, where 0\n, 0... ( ) j\nj i\ni T S j L length of N    is an i-fold convolution with\nS itself and ( )ja P N j  is the weighting assigned to the corresponding jT .\nUnfortunately, the compound density expression for ( )P T is very space inefficient\nand to address this we need to factorize it. Given each component in the mixture is mutually exclusive, i.e. for a given value of N the aggregate total is equal to one,\nand only one iT , variable, this factorization is straightforward. However, we cannot use a binary factorization for Equation 4.3, therefore we factorize the compound density expression into pair block densities and combine each block density incrementally.\nEquation 4.3 is factorized as shown in Figure 4.7, where additional Boolean variables, jE (with only two states True and False ) 6 , are introduced to assign\nweightings proportional to ja , to each pair of block nodes, for example,\n6 \u201cTrue\u201d and \u201cFalse\u201d are used for convenience; any binary labelling would do equally well.\n0 1 0 2 2{ , },{ , },...,{ , }j jT T F T F T . Factor variables, jF , are created to calculate the\nweighted aggregate for each step, up to the length of the N-fold convolution, L .\nThe node probability table for 1jE  is defined by the following:\n0 1 1\n1\n0 1\n... ( )\n...\nj\nj\nj\na a a P E True\na a a\n\n\n    \n   (4.12)\nThe conditionally deterministic expression for variable 1jF  (called a partitioned\nnode in BN parlance) is defined by:\n2 1\n1\n1\nif if j j j\nj j\nF E True F\nT E False\n \n\n\n  \n (4.13)\nSince 0T and 1T are mutually exclusive, the marginal distribution for variable 0F is:\n0 0 0 0 0 0 1 1 0 0 0 1 1 1( ) ( ) ( ) ( ) ( ) ( ) ( )P F f P E True P T t P E False P T t a P T t a P T t          \nwhich is identical to the first two terms in the original compound density expression,\nEquation 4.3. Similarly, the marginal for variable jF becomes:\n1 1 1 2 2 1( ) ( ) ( ) ( ) ( )j j j j j j j jP F f P E True P F f P E False P T t           \n(4.14)\nAfter applying the CDF method to Equation 4.3 we have the marginal for 1jF  as\nshown by Equation 4.14, which yields the compound density, ( )P T , for the N-fold\nconvolution. Therefore by using the CDF method we can compute the compound density (Equation 4.3) more efficiently. The proof is given in the Appendix, proof A.\nThe CDF method is a general way of factorizing a compound density. It takes as input any n-fold convolution, regardless of the causal structure governing the severity variables. Note that the CDF method can be made more efficient by applying variable elimination (VE) to remove leaf nodes. Likewise we can execute the algorithm recursively reuse the same BN fragment ( | , , )P F F T E ."}, {"heading": "4.3.4 The BFE Convolution Algorithm with Example", "text": "The BFE convolution algorithm is formalised, as pseudo code:\nAlgorithm 2 BFE convolution algorithm Input: S : Severity variable, N : Frequency variable, C : vector of common causes (optional) Output: Compound density T Main: 1. Compute the probability density function of N , with sample space Z by:\n( ) ( ) ({ : ( ) }) , 0 1N j jf x P N x P z Z N z x a j , ,...,length(Z)      \n2. for 0j  to (length of Z ) do\n3. for 0i  to jz do\n4. Compute jz -fold convolution\n0\nj\nj\nz\nz i\ni T S   by BF and LBA algorithms\n5. Eliminate nodes (out of query set) by VE algorithm"}, {"heading": "6. end for", "text": ""}, {"heading": "7. While 2j  do", "text": "8. Apply CDF algorithm to factorize (4.3) by probability density of N ,\nCompute 1 1 2 1( ) ( ) ( ) ( )jj j j j zF P E True P F P E False P T      \n9. Eliminate nodes iS , 2jF  and jzT by VE algorithm"}, {"heading": "10. end while", "text": ""}, {"heading": "11. end for", "text": ""}, {"heading": "12. return", "text": "1( )jP F  {marginal distribution of T }\nAlgorithm 2 BFE convolution algorithm\nExample 4.1 Consider a simple example model that aggregates events with marginal frequency ~ (0.5)N Geometric and with marginal severity distribution\n~ (1)S Exponential , shown in Figure 4.8.\nFinally we build and execute a BN parameterised with the relevant values. Figure 4.9 shows the partial factorization steps for the first three terms in the compound density. By keeping factorization using the CDF algorithm we yield the compound density T with mean 1 and variance 3.1 in AgenaRisk, while the analytically derived mean and variance are 1 and 3 respectively.\nWe have factorized the conditional deterministic function ( )P T by CDF algorithm,\nwhereby frequency N is factorized by 0 ,..., jE E as generating Boolean expressions.\nThis factorization is reversible as we can recover the frequency N by combining\n0 ,..., jE E , where deconvolution of N is achieved."}, {"heading": "4.4. Deconvolution using the BFE Algorithm", "text": ""}, {"heading": "4.4.1 Deconvolution", "text": "Where we are interested in the posterior marginal distribution of the causes conditional on these consequences we can perform deconvolution, in effect reversing the direction of inference involved in convolution. This is of interest in sensitivity analysis, where we might be interested in identifying which causal variables has the largest, differential, impact on some summary statistic of the aggregated total, such as the mean loss or the conditional Value At Risk (cVAR), derived from\n0( | )P C T t .\nOne established solution for deconvolution involves inverse filtering using Fourier Transforms, whereby the severity, S , is obtained by inverse transformation from its characteristic function. However, it is first necessary that the density function for S possess an inverse, and should this not exist or if the convolution algebra admits zero divisors, this, unfortunately, results in an infinite number of solutions (Idier, 2010). Alternative analytical estimation methods, i.e. maximum likelihood, and numerical evaluation involving Fourier transforms or simulation based sampling methods, can be attempted but none of them is known to have been applied to N-fold deconvolution in hybrid models containing discrete causal variables.\nBN based inference offers an alternative, natural, way of solving deconvolution because it offers both predictive (cause to consequence) reasoning and diagnostic (consequence to cause) reasoning. This process is a backwards inference, whereby evidence is entered into the BN on a consequence node and then the model is updated to determine the posterior probabilities of all parent and antecedent variables in the model. A \u201cbackwards\u201d version of the BFE algorithm offers a solution for answering deconvolution problems, in a general way without making any assumptions about the form of the density function of S . The approach again uses a discretized form for all continuous variables in the hybrid BN, thus ensuring that the frequency distribution, N , is tractable. Note that, computationally, the deconvolution process is a natural use of DDJT algorithm.\nExample 4.2 convolution and deconvolution:\nTo illustrate how backwards propagation works, and by extension deconvolution, let us consider a simple BN with parent variable distributions\n2~ ( 5, 5)X Normal    , 2~ ( 10, 10)Y Normal    and likelihood\nfunction for a child variable ( | , ) ( )P Z X Y P Z X Y   . Figure 4.10 (a) shows\nthe prior convolution effects of the backwards inference calculation, as marginal distributions superimposed on the BN graph. The exact posterior marginal for Z is 2~ ( 15, 15)Z Normal    . Our approximation produces a\nmean of 14.99 and variance 16.28 (DD performed with 25 iterations using AgenaRisk).\n(a) (b)\nT0 TT1\nN\nC\nT\nT2\nC\nN\nA\nS0 S2S1\nC\nT\nG\u2019\nT0 T1 T2\nG\n0 0 0 1 1 1 2 2 2( ) ( ) ( ) ( ), ( ), 0,1,2iP T t a P T t a P T t a P T t a P N i i         \nGiven evidence 0T t the deconvolution of C is achieved by:\n0 0\n0\n0\n, ,\n( , ) ( | )\n( )\n( | { }) ( ) ( | { }) ( | ) ( ) i i i i i i i s t n\nP C c T t P C c T t\nP T t\nP T t pa T P N n P T t pa T P S s C c P C c\n    \n\n      \n(4.16)\nwhere { }pa T denotes the parents of T .\nSo, once the convolution model has eliminated all irrelevant variables, in this\ncase , , , ji z j j S T E F we would be left with the query set, which here is { , }Q C T ."}, {"heading": "4.4.2 Reconstructing the Frequency Variable", "text": "If we are also interested in including the frequency variable, N , in our query set we\nmust be careful to cache variables jE , 2jF  and jzT during convolution. Recall that\nthe prior distribution for N was decomposed into the jE during compound density\nfactorization, therefore we need some way of updating this prior using the new\nposterior probabilities generated on the Boolean variables, jE , during\ndeconvolution. To perform deconvolution on N it is first necessarily to reconstruct\nN from the jE variables that together composed the original N .\nReconstruction involves composing all Boolean variables, jE , into the frequency\nvariable N , in a way that the updating of jE can directly result in generating a new\nposterior distribution of N . The model is established by connecting all jE nodes to\nN , where the new NPT for N has the form of combining all its parents. However,\nit turns out this NPT is exponential ( 12 j ) in size. To avoid the drawback we use an alternative, factorized, approach that can reconstruct the NPT incrementally.\nAs before, we reconstruct N using binary factorization where the conditioning is conducted efficiently using incremental steps. Here the intermediate variables\nproduced during binary factorization, , ( 0,..., 1)kN k j  , are created efficiently by ensuring their NPTs are of minimal size.\nThe routine for constructing the NPTs for , ( 0,..., 1)kN k j  from the jE \u2019s is:\n1. Order parents jE and 1jE  from higher index to lower index for kN \u2019s NPT\n(since jE is Boolean variable with only two states, one concatenating all\n1jE  \u2019s states and another state is single state that 1jE  does not contain. In\nthis example 1E should be placed on top of 0E in the NPT table, as it is easier for comparing the common sets)\n2. As we have already generated the NPT map of jE , 1jE  and kN . Next we\nspecify the NPT entry with unit value (\u201c1\u201d) at kN  , when jE and 1jE \nhas common sets  (In this example, e.g. 1E and 0E have common sets \"0\"  and \"1\"  )\n3. Specify NPT entry with value (\u201c1\u201d) at kN  , when jE and 1jE  has no\ncommon sets and jE  ( jE has one state  that 1jE  does not contain, so\nunder this case kN only needs to be consistent with jE as the changes on\n1jE  won\u2019t affect the probability ( )kP N  , E.g. in this example it is when\n1 \"2\"E   )\n4. Specify NPT entry with value (\u201c0\u201d) at all other entries.\nWe repeat this routine for all , ( 0,..., 1)kN k j  until we have exhausted all jE \u2019s,\nproducing a fully reconstructed N . Once we\u2019ve built the reconstructed structure\n( kN ) for N , in fact the updates of jE \u2019s probabilities are directly mapping to kN ,\nand so deconvolution of N is retrieved."}, {"heading": "4.4.3 The BFE Deconvolution Algorithm with Examples", "text": "The BFE deconvolution algorithm, for N-fold deconvolution, is formalised, as pseudo code:\nAlgorithm 3 BFE deconvolution algorithm Input: S : Severity variable, N : Frequency variable, C : vector of common causes\nand 0T t\nOutput: posterior marginal of query set members i.e. 0( | )P T tC , 0( | )P N T t Main: 1. do convolution BFE algorithm to produce final query set 2. if N is in query set\n3. reconstruct N from jE"}, {"heading": "4. end if", "text": "5. set evidence 0t on T and perform inference 6. return posterior marginal distributions for query set\nAlgorithm 3 BFE deconvolution algorithm\nExample 4.3 deconvolutes frequency distribution:\nConsider a simplified example for deconvoluting N , suppose frequency distribution N is discretized as {0.1, 0.2, 0.3, 0.4} with discrete states\n{0,1, 2, 3} and ~ (1)S Exponential . Figure 4.12 (a) shows these incremental\nsteps for example 4.3. In this example there are three parents ( 0 1 2, ,E E E ) to N .\nThe incremental composition steps of jE have introduced two intermediate\nvariables 0N and 1N , and we expect the frequency N to be reconstructed at\nthe end of the incremental step, which is variable 1N . Key to this process is\nhow to build the NPT for each kN .\ngenerating a zero compound sum at 2F .\nThe reconstruction algorithm is applicable to cases that N has discrete parent cause\nvariables as well, where jE \u2019s NPTs are generated directly from N \u2019s parents, and\nthe deconvolution is performed by BFE deconvolution algorithm.\nExample 4.4 de-convolutes common cause variables:\nConsider another example for deconvoluting common cause variable as shown in the BN in Figure 4.13. The model has this form: Frequency distribution N is discretized as {0.2, 0.3, 0.5} with discrete values {1, 2, }N .\nThe common cause variable, C , has labelled values and probabilities { 0.8, 0.19, 0.01}Normal High Extreme   . There are three i-fold variables,\none for each of the frequency states: _1_T fold , _ 2 _T fold and\n_ _T N fold , which are all dependent on C . The compound density is given\nby variable T which satisfies:\nWe might be interested in the scenario where 3000T  and wish to determine what the posterior probability is for the common cause given this observations. Figure 4.14 (a) shows the full BN, with evidence entered and Figure 4.14 (b) is the corresponding BN generated by BFE.\nWe can reconstruct the frequency N from the jE in Figure 4.14 (b), to obtain\nthe posterior distribution for ~{0.11, 0.166, 0.724}N . The two models (a) and\n(b) produced identical posterior marginal for the query set { , }Q C N under\nBFE deconvolution.\nA proof that our deconvolution algorithm produces the same result as a complete model is given in the Appendix \u2013 Proof B."}, {"heading": "4.5. Experiments", "text": "We report on a number of experiments using the BFE algorithm in order to determine whether it can be applied to a spectrum of risk aggregation problem archetypes. Where possible the results are compared to analytical results, FFT, Panjer\u2019s approach and Monte Carlo (MC) simulation 7 . Throughout experiments 1-3, we use MC as a numerical reference point to determine whether the BFE algorithm\u2019s is sufficiently accurate since Panjer and FFT are not convenient or possible to calculate in some of the experiments. The following experiments, with accompanying rationale, were carried out:\n7 In the experiments, we define the MC process as: first we draw samples\ni n from frequency distribution, and\nthen draw i n samples from severity distribution and sum them. Finally we normalize all the samples to yield the compound sum.\n1. Experiment 1: Convolution with basic parameterisation. This is a\nstraightforward N-fold convolution, included here for easy comparison with prior art. The severity variables are assumed i.i.d. 2. Experiment 2: Convolution with multi-modal (mixtures of) severity\ndistribution. We believe this to be a particularly difficult case for those methods that are more reliant on particular analytical assumptions. Practically, multi-modal distributions are of interest in cases where we might have extreme outcomes, such as sharp regime shifts in asset valuations. 3. Experiment 3: Convolution with discrete common causes variables. This is\nthe key experiment in the paper since these causes will be co-dependent and the severity distribution will depend on their values (and hence will be a conditional mixture). 4. Experiment 4: Deconvolution with discrete common causes. This is the\ninverse of experiment 3 where we seek to estimate the posterior marginal for the common causes conditioned on some observed total aggregated value.\nThe computing environment settings for the experiments are as follows. Operation system: Windows XP Professional, Intel i5 @ 3.30GHz, 4.0GB RAM. AgenaRisk was used to implement the BFE algorithm, which was written in java (not generally recognised as a high performance language for numerical calculations), where typically the DD settings were for 65 iterations for severity variables and 25 iterations for the frequency variable. The reference algorithms were written in R (R, 2013) using the actuarial add-on package, actuar. For FFT the settings used were range 0:95M  and span 0.1h  . Panjer settings: R, range 0:95M  and span 0.1h  . A sample size of 2.0E+5 was used as the settings in R for the Monte Carlo simulation. Comparing the speed and memory requirements of each reference algorithm can be deceptive given that solving each problem involves both the human analyst and the computer. Some algorithms require non-trivial front loaded investment of analytical effort to design and configure the exact form of the algorithm in advance of any computation (for instance the frequency variable has to be in Panjer\u2019s class for use in the Panjer algorithm (Embrechts & Frei, 2009). Others are more general purpose and simply involve the user declaring the model and setting some parameters with no\nmathematical preparation stage. Given this we do not compare simply choose the algorithm on computational speed but instead bear in mind the commensurate or compensating benefits of ease of use, personal productivity and ease of presentation to, and validation by, an end user.\nIn each experiment we compare the following summary statistics, for all aggregated distributions, for each algorithm: Mean, Standard deviation, 95 th percentile and 99 th percentile. We also give an indication of analytical preparation time needed { , , }High Medium Low (of course this would include effort to formulate the model by\nhand and write any bespoke code needed to solve, or approximate it).\nExperiment 4.1: Convolution with basic parameterisation\nConsider the simple example model by that the frequency is ~ (50)N Possion with\nseverity ~ (1)S Exponential distributions is computed.\nTable 4.2 shows the accuracy of each algorithm. Clearly BFE is as accurate as other approaches. However the preparation and analysis time is commensurate with using Monte Carlo simulation.\nThe corresponding marginal distribution for the query node set { , , }T N S is shown\nin Figure 4.15.\nExperiment 4.2: Convolution with multi-modal severity distribution\nHere we set the event frequency as ~ (50)N Poisson but the severity distribution is\na mixture distribution, ~ SS f :\n(0.2) (5,1.5) (0.3) (25, 2) (0.4) (50, 3) (0.1) (100, 2)Sf Gamma Normal Normal Gamma   \nIn a hybrid BN a mixture distribution is modelled by conditioning the severity variable on one or more partitioning discrete variables, C . Assuming that that\nseverity variables, jS , are i.i.d. we can calculate the compound density using BFE.\nThe characteristic function of a mixture distribution is inconvenient to define (with continuous and discrete components). The analytical and programming effort needed to solve each multi-modal severity distribution for Panjer is high, so here we compare with MC only.\nThe corresponding marginal distribution for the query node set { , , }T N S is shown\nin Figure 4.16.\nExperiment 4.3: Convolution with discrete common causes variables\nLoss distributions from operational risk can vary in different circumstances, e.g. exhibiting co-dependences among causes. Suppose in some cases that losses are caused by daily operations and these losses are drawn from a mixture of truncated Normal distributions, whereas extreme or some unexpected losses are distributed in a more severe distribution. We model this behaviour by a hierarchical common cause\ncombination 0 4,...,C C .\nThe severity variable S is conditioning on common cause variable, 0 1 2, ,C C C . And\nthese common cause variables are conditioned on higher common causes 3C and 4C . Severity NPT is shown in Table 4.4. The frequency distribution of losses is modelled as ~ (50)N Poisson .\nIn Figure 4.17 (a) the model severities with dependencies by common cause\nvariables 0 4,...,C C is introduced. Figure 4.17 (b) depicts a 16-fold convolution of dependent severities using the variable elimination method. For any given frequency distribution, N , we can apply the BFE convolution algorithm to calculate the common cause N-fold convolution.\nFigure 4.18 illustrates the output compound densities for the compared algorithms. Table 4.5 shows the results for the two approaches are almost identical on summary statistics except the small difference on standard deviation. BFE has offered a unified approach to construct and compute such a model conveniently.\nExperiment 4.4: deconvolution with discrete common causes variables\nWe reuse the convolution model from experiment 4.3 as the input model for deconvolution (Figure 4.19).\nIn Figure 4.19, the intermediate variables in this example are shown for reference despite them being eliminated during the convolution process.\nFigure 4.19 (b) sets an observation on total aggregation node _AggS N . After performing deconvolution we queried the posterior marginal of common causes and diagnose that the most likely common cause is 0C , which is in its \u201cLow\u201d state with certainty. This is easily explained by the fact that from the severity NPT, shown in\nTable 4.4, it is only when state of 0C is \u201cLow\u201d that a value of 6000 can be at all probable. This deconvolution is currently only supported by BEF since the information cannot be back retrieved by other approaches.\nDeconvolution is obviously useful in carrying out a sensitivity of the model results, allowing the analyst to quickly check model assumptions and identify which causal factors have the largest consequential effect on the result. This is difficult to do manually or informally in the presence of non-linear interactions. Also, without \u201cbackwards\u201d deconvolution we can only compute such sensitivities \u201cforwards\u201d one casual variable at a time and this is computationally much more expensive. For example, the forwards calculation of T from ten Boolean common cause variables would require 102 calculations versus 40 in the backwards case (assuming T was discretized into 40 states)."}, {"heading": "4.6. Summary", "text": "This chapter has reviewed historical, popular, methods for performing risk aggregation and compared them with a new method called Bayesian Factorization and Elimination (BFE). The method exploits a number of advances from the field of Bayesian Networks, covering methods to approximate statistical and conditionally deterministic functions and to factorize multivariate distributions for efficient computation. The objective for BFE was for it to perform aggregation for classes of problems that the existing methods cannot solve (namely hybrid situations involving common causes) while performing at least as well on conventional aggregation problems. The experiments show that our objectives were achieved. For more difficult hybrid problems the experimental results show that BFE provides a more general solution than is possible or convenient to produce with the previous methods. For example, BFE outperforms the Panjer and FFT in hybrid cases. MC sampling techniques are tailored to specific cases; thus a general solution may be difficult. MCMC, however, may be generally applicable, such as a Metropolis-Hasting sampler (Metropolis, et al, 1953) (Hastings, 1970) or Gelman\u2019s Stan toolbox (Matthew, Carpenter, & Gelman, 2012), but a general MCMC sampler may still perform poorly on a problem without bespoke design or parameter adjustment. BFE does not require specific adjustment on experimental settings except where certain problems may require an increase in discretization resolution. MC is also not suitable for constructing a deconvolution model from the convolution model. In contrast BFE\nprovides a single unified procedure for performing Bayesian convolution, and also a convenient way to perform deconvolution or model reconstruction.\nThe BFE approach can be easily extended to perform deconvolution for the purposes of stress testing and sensitivity analysis in a way that competing methods cannot currently offer. The BFE deconvolution method reported here provides a low resolution result, which is likely good enough for the purposes of model checking and sensitivity analysis. However, we are investigating an alternative high resolution approach whereby variables are discretized efficiently during the deconvolution process, thus providing more accurate posterior results.\nWith regard to the research hypotheses outlines in Chapter 1, Section 1.2 the research results presented here positively satisfy the first three hypotheses."}, {"heading": "5. Inference for High Dimensional Models", "text": "To perform Bayesian risk aggregation when there are many aggregated variables is very challenging. This includes the representation challenges and inference challenges. One general approach to model general dependencies is by using copulas, i.e. multivariate distributions are converted into local copula parameterizations, and there exists a variety of construction mechanics using copula building blocks. Under the Bayesian framework, however, the representation requires further decomposition of the existing functions into conditional forms. One way to model an arbitrary multivariate distributions using BN maybe by seeking a combination of copula and BN, e.g. copula Bayesian Network (CBN) (Elidan, 2010), where the joint multivariate distribution can be decomposed by copula functions, and in further can be factorized into conditional forms.\nIf a BN representation problem can be resolved, i.e. via CBN or general decomposition approach, a DCCD structure will be sufficient with respect to the graphical representation, as discussed before, the DCCD is the unique structure for representing an arbitrary distribution. One of the classic representations of DCCD models are the conditional Gaussian (CG) models, which have been used to factorize high dimensional multivariate Gaussian distributions (MGD) (McNeil et al., 2010) into CG forms. In this way an MGD model can be always represented by CG-DCCD model (discussed in section 5.1).\nThe inference is very challenging for a DCCD model: the analytical solutions are usually intractable. The stochastic simulation based algorithms are flexible to use, but is problem tailored and the convergence for arbitrary models is not guranteed. The standard discrete approximation approach, i.e. DDJT, can easily become computational intractable along with the rapid growth of the number of discrete space caused by high dimensions. GBP is flexible to use and can achieve accurate result if converged. However, the GBP algorithm has rarely applied to DCCD models, although there exists some research for Gaussian belief propagation solving continuous Gaussian linear models (Bickson et al., 2008) (Shental et al., 2008), these works are analytical and particularly focus on Gaussian models. General purpose\ninference for GBP in discrete forms requires a sophisticated clustering procedure to convert a factor graph into region graph, where a good construction of such region graph is difficult to find.\nThis chapter targets the inference problem of DCCD models, explores discrete approximation extending the use of discretization and approximation inference.\nIt develops a triplet region construction (TRC) algorithm based on the clustering method in GBP in discrete form, by constructing an optimal region graph and also satisfies the maxent-normal constraint (Yedidia et al., 2005). The TRC algorithm uses binary factorization algorithm to reduce the computation complexity introduced by deterministic function of parent-child NPTs in DCCD models. The size of the factors generated for region graph is then reduced. Then belief propagation is performed on TRC, compared to JT, the efficiency is achieved by replacing JT based cliques into smaller triplet clusters.\nIt then combines TRC based belief propagation with dynamic discretization to propose a dynamically discretized belief propagation (DDBP) algorithm. In such a way, inference can be carried out for general purpose, with continuous variables being discretized. We use conditional Gaussian DCCD models to show the accuracy of our algorithm, since such model has a simple analytical solution and can be used to validate the algorithms. The result of our experiment is close to the analytical solution. All experiments are implemented onto Bayesian software AgenaRisk (AgenaRisk, 2014)."}, {"heading": "5.1. Conditional Gaussian DCCD Model", "text": "In practice a BN with continuous CPDs (i.e. conditional Gaussian, continuous conditional non-Gaussian) are easy to convert to DCCD models, since the CPDs are deterministic likelihood functions that are easy to modify (when edges are added) using arithmetic operations. Usually discrete and hybrid models will present some difficulty because their CPDs are not conditionally deterministic, although it is feasible to use methods to ensure they can be factorized.\nLet us focus on the common special case where we have a complete graph all of whose nodes correspond to Gaussian distributions. It is well known that Conditional\nGaussian (CG) models can be used to factorize MGDs. So an MGD model can be always represented by a DCCD model for a BN structure. Inference for such a model can be approximate or may use exact methods (S. L. Lauritzen, 1992) (this is not restricted to continuous but is also applicable to hybrid CG models). Our experiments in Section 5.4 are carried out using MGD models, as they have an analytical solution and so is a good basis for comparison (but note that the DDBP algorithm is designed to be general purpose and is not exclusively designed for use on MGD models).\nTo show how we decompose a complete MGD model consider an MGD vector\n1{ ,..., }nX XX , where each univariate component is a Gaussian distribution and the\npairwise correlations are encoded in a correlation matrix. Given such a vector X ,\nthere always exists a partition { , } 1 2X X X , where vector 1{ ,..., }kX X1X and\n1{ ,..., }k nX X2X , if the mean vector and covariance matrix for 1X and 2X are\nrespectively:\n       1 2 \u03bc \u03bc \u03bc ,        11 12 21 22 \u03a3 \u03a3 \u03a3 \u03a3 \u03a3\nThen ~ ( , )N1 1 11X \u03bc \u03a3 and ~ ( , )N2 2 22X \u03bc \u03a3 are also MGD vectors. Assuming that\neach \u03a3 is positive definite, the conditional distributions of 2X given 1X may also\nbe shown to be multivariate Gaussian, i.e., | ~ ( , )N 2 1 2|1 22|1 X X \u03bc \u03a3 , as in Equation 5.1,\nwhere\n  -1 2|1 2 21 11 1 1 \u03bc \u03bc \u03a3 \u03a3 (X -\u03bc ) and -1 22|1 22 21 11 12 \u03a3 = \u03a3 -\u03a3 \u03a3 \u03a3 (5.1)\nis the conditional mean vector and covariance matrix respectively. More generally, the partition can be sequenced arbitrarily. Each univariate variable can be conditioned on its antecedent variables, thus resulting in the conditional representation of a multivariate distribution.\nThe MGD vector X can be decomposed to a CG-DCCD where there are\ninterdependences between every pair of variables in 1{ ,..., }nX X (i.e. the covariance\nbetween the pairs). An arbitrary variable jX is conditioned on all its antecedents\n1 1,..., jX X  but not directly conditioned on its descendants. We can then apply\nEquation 5.1 to express the decomposition of general multivariate Gaussian models.\nX2 X1\nE2E1X3\nE4X4\nX5\nG\u2019G\nX1\nX2\nX4\nX5\nX3\nX6\nX6\nE3\nE5\nE6\nFigure 5.1 Six dimensional CG-DCCD to 6 full BFG"}, {"heading": "5.2. The Triplet Region Construction Algorithm", "text": "This section develops the Triplet Region Construction (TRC) algorithm along with the associated subsidiary algorithms introduced thus far. It automatically constructs\nan optimized region graph using a n dimensional full-BFG as input.\nWe have already reviewed Generalized Belief Propagation (GBP) and discussed a particular approach called the Cluster Variation Method (CVM) in chapter 3, which produces, as output, an object called a region graph, which can be used for inference.\nIn section 5.2.1 we outline the desirable properties of this region graph that we need to preserve in our new TRC algorithm. This involves identifying two levels of regions containing primary triplets in the first region and interaction triplets in the second region. These interaction triplets are then pruned to ensure that balance and the Maximum Entropy Normal property are maintained. Section 5.2.2 offers full definitions of all algorithmic steps. Proofs and demonstrations are provided throughout."}, {"heading": "5.2.1 Identifying the TRC regions", "text": "In Chapter 3 we have introduced the region graph choice and interaction strength that are key components for good approximations. To identify TRC regions, our algorithm is similar to Welling\u2019s (Welling, 2004), where we partition the full-BFG into triplets to model interactions. The partitions chosen need to be sufficient to guarantee messages are passed between neighbouring triplets in order to minimize\ncomputational payload. However the extent to which the model sufficiently accounts for the strength of interactions can only be determined empirically.\nFrom (Yedidia et al., 2005) and Welling et al. (Welling et al., 2005) (Welling, 2004) (Gelfand & Welling, 2012) we identify three properties necessary for guaranteeing the best approximation under GBP:\nProperty 1: Acyclic \u2013 the region graph is acyclic and contains two levels.\nProperty 2: Balanced \u2013 the region graph contains variables that are counted exactly once (i.e. the counting number for each variable is one).\nProperty 3: Maximum Entropy Normal \u2013 the maxent-normal property is met when the region based entropy is maximised when all beliefs are uniform.\nFrom free energy theory (Yedidia et al., 2005), the free energy of the region graph achieves its minimum when the beliefs, b , derived from the region graph is equal to the joint probability distribution, p , or, equivalently, the entropy of the region graph\nachieves its maximum when all region beliefs are uniform (i.e. have the same noninformative value). This maxent-normal property is a necessary, but not sufficient, condition for a good approximation.\nAn acyclic region graph is preferable because it helps ensure convergence of messages (Yedidia et al., 2005). In our case an increase in model dimensionality would lead to an exponential increase in the amount of message passing, making message scheduling difficult and sensitive to message order. Many popular message passing algorithms, such as the tree reweighted max-product algorithm (Kolmogorov, 2006) and loopy BP (Murphy et al., 1999), do not guarantee convergence (Meltzer, Globerson, & Weiss, 2009).\nThe full-BFG graph 'G topology has the following advantages in inference:\n1. Homogeneous: The two level acyclic region graph structure is maintained\nirrespective of the number of model dimensions.\n2. Uniform factor size: except for the root NPT 1( )p X and single parent NPT\n2 1( | )p X X , all other NPTs are defined on triplets (a group of three variables containing a child with two parents, or parent with two children). For each\ntriplet there is always an associated factor in the factor graph and the NPTs can be multiplied into the triplet.\nThe region graph can be constructed from the following components (Figure 5.2 shows these superimposed on an undirected full-BFG):\n Primary triplet: a triplet with an NPT defined by the BFG, 'G , i.e. a child\nvariable and its two parents (original and intermediate).\n Moral edge: an undirected edge connecting the parent nodes of each primary\ntriplet; this links an original variable and an intermediate variable.\n Interaction triplet: a triplet used to interact with the primary triplet through a\nmoral edge.\nHowever, for these primary triplets to communicate they need to send messages via the interaction triplets, and the number and composition of these triplets determines the strength of interaction (and the amount of dependence the approximation supports).\nThe only open decision then is the choice of interaction triplets and to determine the interaction triplets we can exploit the following properties:\n1. Each moral edge always connects an original variable, iX , and an\nintermediate variable, tE , where the intermediate variable is always a common member in two primary triplets.\n2. A candidate interaction triplet is always composed from the two variables\nconnecting a moral edge (original and intermediate) and one original parent\nvariables, iX , of either of these two variables.\nWhen presented by a choice of interaction triplets to select we retain the one that\nensures the counting number for all of the original variables, iX , is balanced, and discard the others. This guarantees the \u201cBalanced\u201d property, and the resulting region graph is therefore balanced.\nAn example of the process is given in Example 5.2.\nExample 5.2\nConsider the moral graph of a 5 BFG 'G in Figure 5.3, with edge directions maintained to help identify the primary triplets. The moral edges are represented as dashed lines. All primary triplets and moral edges are listed in\nthe table placed aside 'G in Figure 5.3. The root primary triplet is 1 2 3X X X .\nWe can identify all of the interaction triplets in Figure 5.4 ''G as follows. Let us\nfirst consider moral edge 4 3( , )X E . The parent variables associated with\nvariables sharing this moral edge are 3X , 1E and 2E . The triplets we could\nproduce by combining the parents with the moral variables, 4 3( , )X E , are\n3 4 3X X E , 3 1 3X E E and 3 3 2X E E . However, triplets 3 1 3X E E and 3 3 2X E E would be\ninvalid since they do not contain any original parent variables of 4 3( , )X E .\nTherefore the interaction triplet for moral edge 4 3( , )X E is 3 4 3X X E .\nNext let us consider the variables associated with moral edge 3 1( , )X E and the\ncandidate triplets can be either 2 3 1X X E or 1 3 1X X E . Finally, for moral edge\n3 2( , )X E the candidate triplets would be 2 3 2X X E or 1 3 2X X E .\nTo balance the number of times 2X and 1X appear in interaction triplets, we\nchoose 2 3 1X X E for moral edge 3 1( , )X E and then should choose 1 3 2X X E for\nmoral edge 3 2( , )X E , or vice versa. This ensures the counting numbers for all original variables balance."}, {"heading": "5.2.2 Constructing the TRC region graph", "text": "We aim to build an acyclic two-level region graph. A valid region graph for a 5 full-BFG model using CVM can be built using all primary triplets and all interaction triplets as first level regions, with the shared interactions between these as second level regions and finally a third level region containing variables shared amongst level two regions, as shown in Figure 5.5. Notice that Figure 5.5 has three levels, where the third level contains only marginal variables receiving message passes from\nparent regions spanning the width of the graph. We have found that inference in a 5 full-BFG model using the build of a three levels region graph can achieve\nconvergence, but that higher dimensional models often fail to converge (for example,\nin Section 5.4 experiment 2 we report a 8 BFG model with some strong correlation factors that failed to converge by using CVM with three levels).\nTriplet Region Construction (TRC) starts with a similar approach to CVM. Initially\nwe can use CVM to construct a two level region graph InitRG that is acyclic. The first level regions are the largest regions that contain all factors and variables, and are exclusively determined by primary triplets and interaction triplets. The second level regions are simply the intersections of the first level regions.\nFor example, we construct InitRG for a 5 full BFG 'G in Figure 5.6.\nInitRG is not yet valid since a valid region graph requires each factor and each\nvariable to be counted exactly once. We therefore present an optimization procedure\nthat will guarantee the counting number by manipulating InitRG .\nFrom InitRG the second level intersection regions can be classed into two types:\n Hybrid intersection: contains an original variable, iX , and an intermediate\nvariable, tE .\n Cognate intersection: contains two original variables, iX and jX , except\n1 2X X .\n Root intersection: is a cognate intersection 1 2X X that is connected to the\nprimary root region ( 1 2 3X X X ) and contains the root variable in the full-BFG,\n1X .\nAll hybrid intersections have identical counting number -1, and this is invariant with the number of dimensions. This is important since it helps to satisfy the maxentnormal property and so it would be preferable if all intersections are hybrid intersections. If these hybrid intersections are sufficient to cover all interactions among the first level regions, the region graph construction can be optimized by removing all cognate intersections.\nWe can test the region graph connectivity of the first level regions, by removing all\ncognate intersections (if we also removed the root intersection region, 1 2X X R , the first level region 1R would become disconnected from the rest of the region graph, hence it is not defined as a cognate intersection).\nR\nFigure 5.7 shows the result of removing all cognate intersections from Figure 5.6, demonstrating that it is sufficient to connect all first level regions using hybrid intersections and the root intersection alone. This suggests that we can optimize the region graph but to do so we need to guarantee the counting numbers. Clearly, the cognate intersections are represented as edges in the full-BFG and, given the index ordering of the original variables, , 1,...,iX i n , there is a clear pattern to the counting numbers. The absolute value of counting numbers decreases by one along the path of edges in the original full-BFG from lower to higher indexed original variables. Formally, the counting numbers, X Xi j\nRc , for regions 1i iX XR  , 1 2i iX XR   and\n2i iX X R  satisfy: 1 2 1 2X X X X X Xi i i i i i R R Rc c c       . This is illustrated in Figure 5.8, where the\ncounting number for each cognate intersection (edge) is shown. For example, the\ncounting numbers, X Xi j Rc , for regions 1 3X XR , 2 3X XR and 1 2X XR satisfy:\n1 2 1 3 2 3X X X X X X R R Rc c c  .\nWe propose the cognate intersection pruning algorithm, to prune cognate\nintersections from InitRG , which results in a new region graph TRCRG , and guarantees optimum counting numbers and satisfies maxent-normality.\nBased on the pattern of counting numbers we prune each cognate intersection i jX X\n( i j ) by removing iX when the counting number does not equal one, 1iXc  . This will result in a region graph whose variable nodes have the counting number one, and is maxent-normal (with proof in Appendix C).\nAlgorithm 4 Cognate Intersection Pruning\nInput: Initial region graph InitRG , full-BFG 'G\nOutput: TRC region graph TRCRG\n1. do each original variable iX in 'G , 1 i N  where N includes all variables\n2. calculate the counting number i i\ni\nX R\nR R c c   \n3. if 1 iX c  4. for each cognate intersection jI in InitRG , with variables 0X and 1X\n5. if 0 1{ }X pa X and 1 2j X XI R 6. remove 0X from jI in InitRG\n7. end if 8. end for 9. end if\n10. until ( 1 iX c  for all variables) 11. TRC InitRG RG\n12. return TRCRG\nAlgorithm 4 Cognate intersection pruning\nAfter applying the cognate intersection pruning algorithm we have transformed the\ninitial region graph InitRG into our target TRCRG . Figure 5.9 shows the TRCRG generated by the cognate intersection pruning algorithm from the initial region graph\nInitRG : regions 12R , 15R and 19R have all been pruned from InitRG .\nIn general, if n is the number of original variables, the number of cognate intersections pruned is ( 2)n .\nWe formalize the TRC algorithm, by summarizing all the previous algorithms and principles for constructing an optimal region graph for BFG, as shown in Algorithm 5.\nOur TRC algorithm is top down and comparable, in terms of resulting region graph structure, to bottom-up region pursuing algorithms, such as (Welling, 2004). The cognate intersection pruning algorithm is actually generating a local equivalent structure to that which would be produced by adding outer regions 8 to existing region graph shown in (Welling, 2004), resulting in a valid region graph. From this point of view, unlike CVM, TRC does not require that a second level region not be a sub-region of any other second level regions, but nevertheless TRC still satisfies the region graph constraints. In section 5.2.5 we provide a more intuitive justification of TRC given its connections with join graph approaches.\nAlgorithm 5 Triplet Region Construction (TRC) Input: d full-BFG 'G (with 4d  ) 9\nOutput: TRCRG\n1. for each variable iX in 'G , 1 i N \n8 An outer region is defined to be a region with no parents. 9 We require the minimum dimension 4d  because a 4 full-BFG does not build a valid region graph under the TRC algorithm. In the case of 4d  we instead multiply all factors into one single region and perform BP on it.\n2. if size of { } 2ipa X \n3. add iX and { }ipa X to primary triplet set 0F\n4. find 0 1{ } { , }ipa X P P where 0 1( , )P P is a moral edge\n5. find parent common set 0 1 0{ , } { }P P pa P  \u2229 1{ }pa P\n6. if size of 0 1{ , } 1P P  {choice must be between root variable 1X and\nsingle parent variable 2X }\n7. for the size of interaction triplet set 0U ,\n8. calculate the number of interaction triplets 1X n containing 1X and\nthe number of interaction triplets 2X n containing 2X\n9. if 1 2X X n n return the parent variable 0 2C X"}, {"heading": "10. else return 0 1C X", "text": ""}, {"heading": "11. end if", "text": ""}, {"heading": "12. end for", "text": "13. else return the only parent node 0 1 0{ , }P P C "}, {"heading": "14. end if", "text": "15. if 0C null add 0 1{ , }P P and 0C to a new interaction triplet, and update 0U"}, {"heading": "16. end if", "text": ""}, {"heading": "17. end if", "text": ""}, {"heading": "18. end for", "text": "19. compose first level of InitRG by triplet sets 0F and 0U\n20. compose second level of InitRG by intersections of the first level regions 21. call Algorithm 4 cognate intersection pruning\n22. return TRCRG\nAlgorithm 5 TRC algorithm"}, {"heading": "5.2.3 Proof that TRC region graph is MaxEnt-Normal and", "text": "has correct counting numbers\nHere we prove that TRCRG for an example 5 , full-BFG model, is maxent-normal and has the correct counting number for each variable. The more detailed general\nproof of this, for any TRCRG , n full-BFG is given in Appendix C.\nWe use the same approach as Yedidia, who gives a proof that the Bethe approximation is maxent-normal, and, given this, the entropy of the region graph,\nRGH , can be written as Equation 5.2 (Yedidia et al., 2005).\n1 1\n( ) ( ) N M\nRG i a\ni a H H b I b      (5.2)\nwhere N is the number of variables, iX , and M is the number of factors, a , ( aX\nare the variables defined by the factor a ). ( ) ( ) ln ( ) i i i i i i X H b b X b X  is the sum of\nentropies from all variables iX in the region graph, and\n( ) ( ) ( ) ln ( ) ( ) a a a a a a i X i N a I b b X b X H b     is the entropy for a region containing factor\na , minus the entropies of all variables contained in factor a . RGH is maximal,\nequalling 1\n( ) N\ni\ni H b   , when all beliefs, ( )i ib X and ( )a ab X , are uniform, and under\nthese circumstances the mutual information, ( )aI b , equals zero.\nFor the region graph in Figure 5.9 Equation 5.2 can be expanded, giving Equation 5.3.\n1 2 3 1 2 1 1 2 2\n1\n3 1 4 3 2 3 4 3 5 3 3 4 2 1 3 3 1 2\n( ) ({ , },{ }) ({ , }, ) ({ , }, )\n({ , },{ }) ({ , }, ) ({ , }, ) ({ , }, ) ({ , }, ) ({ }, , )\nN\nRG i\ni\nH H b I X X X I X X E I X E X\nI X E X I X E E I X E X I X E X I X E X I X X E\n\n    \n   \n\n\n(5.3)\nwhere each I term is the mutual information for a level 1 triplet region (all three variables in I term), the level 2 interaction regions it is connected to (two or single variables with bracket {...}) and the single variables belonging to the level 1 region\nbut which are absent from the connected level 2 regions. For instance\n1 2 1 1 2 11 2 1 ({ , }, ) ( ) ( ) ( )X X E X X EI X X E H b H b H b    . Therefore, for Figure 5.9\n1\n( ) N\nRG i\ni H H b   because all of the I terms have value zero, and RGH maximised\nwhen the beliefs are uniform it is maxent-normal.\nNext we turn to the counting number of TRCRG for a 5 full-BFG model. Here the\nregion based entropy RGH is defined by the sum of the entropies from all regions, weighted by their counting numbers, as shown by Equation 5.4.\n1 2\n1 1\n( ) L L\nRG t t i i\nt i H H c R c R      (5.4)\nWe can write RGH for the region graph in Figure 5.9 as the sum of all region entropies, weighted by their counting numbers, as in Equation 5.5.\n1 2 3 1 2 1 1 2 2\n3 1 4 3 2 3 4 3 5 3 3 4 2 1 3 3 1 2 1 2 3 1 2 3 1 4 3 2 4 3\n({ , , }) ({ , , }) ({ , , })\n({ , , }) ({ , , }) ({ , , }) ({ , , }) ({ , , }) ({ , , }) 2 ({ , }) ({ }) ({ , }) ({ , }) ({ }) ({ , }) ({ , }\nRGH H X X X H X X E H X E X H X E X H X E E H X E X H X E X H X E X H X X E H X X H X H X E H X E H X H X E H X E                \n3 3 2 1 3\n)\n({ , }) ({ , }) ({ })H X E H X E H X\n\n \n(5.5)\nEach region one triplet\u2019s entropy is cancelled by its connected second level regions entropies and this should result in a counting number of one for each variable. For\nexample, the number of positively weighted occurrences for 1X in Equation 5.5 is\nfour and the number of negatively weighted occurrences for 1X is three, giving a counting number of one."}, {"heading": "5.2.4 TRC complexity", "text": "Here we compare the complexity of TRC with the Junction Tree algorithm. The space complexity for inference in a DCCD model using JT is, of course, exponential, being ( )nO m , where m is the maximum number of discrete states for each variable\nand n the number of variables. Crucially, we now show that, in contrast the space complexity for the TRC algorithm is polynomial.\nFrom our proof (Appendix C) the number of region edges, RGE , updated during\nmessage passing in TRCRG , is the absolute value of counting number plus one, via all intersections. These intersections are composed of all intersections with counting number -1 and those with counting number from (3 )n down to -2, so we can\ncalculate the number of region edges as given by Equation 5.6.\n2( 2) 1\n1\n2\n(| | 1)\n[ 2 ( 2)( 3) / 2] [ 2] [2( 2) 2 4)] ( 2)(5 11) / 2, {intersections}, 3\nj\nn\nRG R\nj\nj\nE c\nn n n n n n n n R n\n \n\n \n               \n\n(5.6)\nThe number of messages being updated is polynomial in Equation 5.6, so the complexity of TRC is polynomial."}, {"heading": "5.2.5 Relationship with the Join Graph approach", "text": "The proof in section 5.2.3 is based on information theory which may be less intuitive, when justifying the cognate intersection pruning algorithm, in terms of verifying the TRC region graph\u2019s effectiveness. This section converts the TRC region graph into a join graph to intuitively support the proof.\nExample 5.3\nWe borrow the example BN, G , used in (Mateescu et al., 2010) with variables renamed for consistency purpose in this thesis and shown in Figure 5.10.\nExcept for the four primary triplets 10 that are obviously included in the first\nlevel of the region graph, there are two interaction triplets, 2 1 2X E E and\n2 3 1X X E are added to incorporate two moral edges 2 2( , )X E and 3 1( , )X E .\nAfter converting the TRC region graph to a join graph, we obtain TRCJG in\nFigure 5.12. and can verify that TRCJG is equivalent with JG in Figure 5.10, with the small clusters ( 1X and 1 1X E ) being absorbed into the triplet 1 2 1X X E . The cognate intersection pruning algorithm pruned the intersection 2 3X X into 3X , so variable 2X will not be propagated cyclically in TRCJG , given that TRCJG is a cyclic graph. (Mateescu et al., 2010) has proven that this pruning will not change the CI assumptions captured by the join graph, which is also used in IJGP algorithm for avoid over-counting problems on cyclic join graph.\nThe cognate intersection algorithm not only avoids over-counting in the TRC region graph but also generates an optimal region graph equivalent to a join graph produced using IJGP.\nFigure 5.13 illustrates the join graph generated for the TRC region graph in Figure 5.9. Two dashed circle lines mark out the loops in this join graph, where the original un-pruned intersections are listed aside each pruned intersection. We can verify that the cognate intersection pruning algorithm avoids the cyclic propagation of the pruned variables in the join graph TRCJG .\nThe TRC algorithm has similar characteristics with join graph based approach, i.e. it guarantees convergence in finite time. Differed from IJGP, TRC algorithm operates on an acyclic graph (and its join graph has no cyclic propagation of variables), which does not need to worry about cyclic graph based problems. If we convert the CVM three-level region graph in Figure 5.5 into a join graph, we will obtain a join graph with cyclic propagation of the third level region variables. If it converges it tends to\nbe very accurate, i.e. for 5 BFG it may converge for some instances and the result is accurate, but in higher dimensional cases the CVM three-level region graph fails to converge, since by converting it to join graph it contains many cyclic propagations of the third level region variables."}, {"heading": "5.3. DDBP algorithm", "text": "The TRC algorithm is sufficient for inference on full-BFG models containing discrete variables. However, to perform inference on high dimensional continuous DCCD models, we must first discretize the continuous variables. We use DD to carry out dynamic discretization during belief propagation and name the combined method Dynamically Discretized Belief Propagation (DDBP). DDBP can be viewed as a replacing JT in the DDJT algorithm with TRC.\nWe formalize the DDBP algorithm as show in Algorithm 6.\nAlgorithm 6 Dynamically Discretized Belief Propagation (DDBP) Input: original BN G (BF decomposable and dimension 4d  ) Output: original BN G with marginals\n1. reorder all nodes in G from ancestors to descendants with new label iX 2. convert G into a DCCD G by adding edges from ancestors to descendants\n3. reassign all CPDs for 1 1( | ,..., )i if X X X  by blocking unrelated ancestors 4. transform G into binary factorized version complete-BFG 'G\n5. call Algorithm 2 Triplet Region Construction to generate TRCRG\n6. assign each region\u2019s factor ( )kR by multiplying the NPTs ( | { })i ip X pa X for all\nvariables into the relevant primary triplets\n7. assign uniform factors for all interaction triplets and separators 8. initialize each region\u2019s discretization (0) k\u03a8 , by its support k\u03a9 9. do compute the approximate NPTs,  0\n( | { })i ip X pa X , on  1\ni\nl X   for each\nvariable iX and initialize each region\u2019s factor\n10. query the BN from node E X e 11. while total relative entropy error > target threshold 12. create a new discretization ( )l k\u03a8 for region domain k\u03a9 13. define updating order by depth first search algorithm 14. perform two way messaging 15. check BP convergence by low discrepancy threshold 16. compute the new discretized potential ( 1) ( )l kR  17. compute the approximate total relative entropy error"}, {"heading": "18. end for", "text": "19. until convergence of the posterior marginal for each region by stable entropy\nerror stopping rule and low entropy error stopping rule\n20. extract marginal for each node from the relevant region and copy to G 21. return G\nAlgorithm 6 DDBP algorithm\nBoth DD and TRC based belief propagation have their stopping rules. The TRC stopping rule is a low discrepancy rule, which states the discrepancy of region beliefs generated from the previous and current rounds of propagation must be below\na certain threshold. For example, if the old beliefs are generated by the first M updates, then the current beliefs are generated by 2M updates, where M is the number of connections between the first and second levels in the region graph. The convergence check determines the discrepancy between the old beliefs and current beliefs (the convergence metric is determined by monitoring each individual probability between old and current beliefs). If the discrepancy fails to fall below a certain threshold, e.g. 1.0E-6, the message updates continue. In our tests TRC guarantees convergence in finite time. For DD the stopping rule is similar in that the entropy error also has to fall below a specified threshold. DDBP features an inner loop of BP iterations and DD outer loop iterations. DDBP converges under the convergence of both BP and DD."}, {"heading": "5.4. Experiments", "text": "We report seven experiments to evaluate the TRC algorithm (Experiments 1-2) and DDBP algorithm (Experiments 3-7). The parameterisation for the BN models used is listed in Appendix B. We compare our results to analytical results and MCMC approximations only for the purpose of illustration and validation of our algorithm. Other related algorithms, such as EP and PBP are not compared, because off-theshelf implementations of these algorithms (for the types of empirical problems investigated here) are not available, and would involve considerable analytical and programming effort.\nThe DDBP algorithm was written in Java and used libraries available on the AgenaRisk product (AgenaRisk, 2014) using Java JDK 6. This also allowed comparison with DDJT since this is already available in AgenaRisk."}, {"heading": "5.4.1 Experiment 1: Inference for a 5 BFG model with all", "text": "binary variables using TRC\nWe first test TRC alone without involving continuous variables, using a 5 BFG in Figure 5.14 with all binary variables.\nThe parameter settings for the NPTs in this experiment are listed in Appendix D. Table 5.1 shows TRC produced good approximations compared to exact results produced under JT. We have tested higher dimensional full-BFG models with more discrete states rather than just using binary state variables. The results are accurate. We have also compared TRC with Bethe method, CVM (three levels), and basic mean field approach (with default message orders in the package) using the fastInf package (Jaimovich, et al, 2014) and the TRC result is better than those produced by these competing approaches."}, {"heading": "5.4.2 Experiment 2: Inference for a 8 BFG with evidence", "text": "using TRC\nThe model contains binary (Boolean) variables only, with evidence set on variable\n8X False . NPT settings are listed in Appendix B. In Figure 5.15, variables 3X ,\n27E and 71E have identical NPT settings, so these variables will have very close\nmarginal distributions.\nWe compare TRC with the JT results in Table 5.2. The maximum KL distance in Table 5.2 is 3.55828e-08 which confirms the performance of TRC. Also, the\nstrongly correlated variables 2X , 27E and 71E are approximated very well."}, {"heading": "5.4.3 Experiment 3: Inference for 20 dimensional CG-", "text": "DCCD model using DDBP\nWe test DDBP using the 20-dimensional conditional Gaussian model shown in Figure 5.16. Parameter settings for the original BN model is shown in the Appendix D (20 dimensions). Results for mean and standard deviation statistics are listed in Table 5.3:\nCompared with the exact result the accuracy for the mean statistics is high for each variable and a small discrepancy (less than 1%) is produced on standard deviation (SD) statistics. The results suggest a stable approximation without degradation, despite increased dimensionality. The region graph generated by CVM with three levels fails to converge for this model."}, {"heading": "5.4.4 Experiment 4: Inference for 10 dimensional CG-", "text": "DCCD model with observations using DDBP\nThis experiment tests the accuracy of DDBP when the model contains an observation. Here we have a 10 dimensional CG-DCCD model with observation for variable\n10 10X   . The parameter settings for the original ten dimensional BN model are\nshown in Appendix B. We compare the result with that achieved using MCMC with a single chain and 1.0E6 updates, as well as the exact solution. Table 5.4 shows that the results for both DDBP and MCMC methods are very similar.\n11\nThis is the maximum iteration settings for BP, though it may converge prior to the limit."}, {"heading": "5.4.5 Experiment 5: Pair correlation test for 15 dimensional", "text": "CG-DCCD model\nHere we use empirical methods to determine how well the DDBP approximates the pair wise covariance/correlation structure in an MGD model and focus on how well the Pearson correlation coefficient,  , is approximated.\nThe parameter settings for the original BN model are shown in Appendix D. The DDBP model was run with 30 DD iterations and a maximum 240 GBP iterations.\nWe use combination pairs of variables 1 6 11, ,X X X and 15X for this experiment (there are 2 15 105C  combinations in total). Figure 5.17 shows the 15 BFG model corresponding to the 15 dimensional CG-DCCD model.\nThe results are shown in Table 5.5 alongside the exact correlation.\nWe use this weak correlation model for testing the accuracy of DDBP. Note that for efficiency considerations this correlation test is only based on a discretized static\nmodel produced by DD, to approximate the joint distribution. Then to produce the conditional distribution for each pair in Table 5.5, we fixed the state of one variable and enter evidence on each of its discrete states to query the posterior distribution of the other variables during propagation. The accuracy of the results is lower but computation time is faster. Despite this the result is still a very close approximation to the exact correlation coefficient,  ."}, {"heading": "5.4.6 Experiment 6: Inference in a linear model using", "text": "DDBP\nThis experiment applies DDBP inference to a linear model, which is a sparse model and not a DCCD. Therefore the model is first converted, from a sparse model, to a DCCD by adding edges and assigning zero weights. This experiment shows how to use DDBP to perform inference on general (initially non-DCCD) models.\nWe use a Bayesian linear regression model (notation is borrowed from (Bishop, 2006)) in Figure 5.18 (a), with the model specification:\n( , )y  t x w , where 0 1 1( , ) ... ( ) T D Dy w w x w x     x w w x ,\nThe error term  is a zero mean Gaussian random variable with precision,  . For\nconvenience we define 0 ( ) 1 x . The model in Figure 5.18 (b) is defined as:\n2\n0 ~ ( 0, 1000)w Normal    , regression coefficient\n2\n1 ~ ( 0, 1000)w Normal    , bias parameter\n~ (1,10)Gamma , prior for the precision parameter over t\nvar ~1/  , noise parameter, inverse of beta\n1~ ( , )Normal  t \u03bc , 1,..., Nt t , target variables\n2~ ( 0, 1000)Normal   x , 1,..., Nx x , explanatory variables\n( )n n x , basis functions\n~ ( )T n\u03bc w x , mean parameter over t\nThe likelihood: 1\n1\n( | , , ) ( | ( ), ) N T\nn n\nn\np N t   \n\nt x w w x , we assume noise is known\nso we set 2( 0, 0.25)Normal    .\nFor a simple illustration Figure 5.18 (b) shows the model structure for the plate model used in DDBP for the case 3n  in (the algorithm is independent of the size of n ).\n(b) Regression model when 3n  for (a)\nThe original model in Figure 5.18 (b) is a sparse continuous model, so it can be converted to a 10 dimensional DCCD, and then subject to the BF process to produce\na 10 full-BFG shown in Figure 5.19. Note that two variables, 2t and 3t , have exact copies in the form of intermediate variables (this only happens when the original model is not DCCD) and given that these variables are observed the intermediate variables are set with the same observation values\n(x,t) = {(-0.1225, -8.0444), (-0.2369, -8.1457), (0.531, -4.9519)}\nTable 5.6 data observations used in this experiment\nWe use the data in Table 5.6 and compare the experimental results for two models in Figure 5.18 (b) and Figure 5.19 using DDJT and DDBP, with 50 DD iterations on each, as well as against the Ordinary Least Square (OLS) solution.\nThe results are shown in Table 5.7.\nThe results for DDJT and DDBP are very close, although the approximation of variable var is relatively poor. We believe that this is due to the discretization including a long tail and the bins\u2019 range in the tail could be slightly different. (This is supported by that fact that the median for var is 0.11 for both DDJT and DDBP)."}, {"heading": "5.4.7 Experiment 7: Aggregation of inter-dependent", "text": "random variables\nFinal experiment illustrates the Bayesian risk aggregation of inter-dependent random variables. This is particularly an application of high dimensional models we have performed using DDBP algorithm.\nThe sum of d dependent random variables is used in finance to calculate an overall\ncapital charge in order to calculate the risk position 1 2 ...d dS X X X    derived from a portfolio of d random valuations.\nSuppose we have a multivariate vector X that represents some kind of portfolio of risks held. ' 1( ,..., )dX XX is defined on some probability space. If the distribution function of X is FX , the aggregation of the risks portfolio is the sum of 1,..., dX X , that takes the form:\n1 1\n( ) [ ... ] ( ,..., ),d d s X X s dF x x s        X , (5.7)\nwhere }:{)( 1  \n d\ni\ni\nd sxxs .\nNumerical evaluation of Equation 5.7 is a rather onerous task; one often has to rely on the bespoke design of MC methods. An alternative to this is the analytic numerical algorithm, AEP, (Arbenz, Embrechts, & Puccetti, 2011), which uses hyper-cubes to iteratively approximate the target distribution by calculating a\nsimplex with probability masses. This algorithm is theoretically promising but in practice can only cope with five or fewer dimensions because of its space complexity.\nOur algorithm has the advantages that it has avoided the space constraints as it reduced the computation complexity from exponential to polynomial, and if the joint distribution can be expressed by a conditional form, it retains its generalization for any conditionally deterministic operations not limited to Equation 5.7.\nConsider the sum of a seven dimensional CG-DCCD model. For a seven dimensional model we produce the full-BF model shown in Figure 5.20 (a). Figure 5.20 (b) shows the full-BF transformed BN when we add the sum of seven\ndimensions to Figure 5.20 (a). Here intermediate variables 1 6,...,E E are the\nincremental sum of the original variables 1 7,...,X X , where 5E yields the total aggregation. The structure of Figure 5.20 (b) simply includes the addition of an\nindividual path 1 6,...,E E along 1X to 7X to Figure 5.20 (a). This operation makes no difference to the structure itself and results in a DCCD, as we would expect since Equation 5.7 is BF decomposable. Likewise, more general arithmetical operations defined on the vector X are just as applicable and would not change the topology of the full-BFG model. If we rearrange the labelled variables\u2019 positions in Figure 5.20\n(b) it is equivalent to a 8 full-BFG, as shown in Figure 5.20 (c).\n(a)\n(b): full-BFG transformed model for the sum of 7 dimensional CG-DCCD model\n(c): Equivalent 8 full-BFG structure with (b)\nThe results of the approximation using a CG-DCCD compared to the exact values are shown in Table 5.8."}, {"heading": "5.5. Summary", "text": "This chapter developed an inference approach which called DDBP by combining Dynamic Discretization and Belief Propagation algorithms to perform efficient inference for DCCD models. It overcomes the computational limitations of discrete approximation algorithms, such as DDJT, by limiting cluster size whilst ensuring the approximation retains desired properties (i.e. maxent-normal) in the triplet heuristics. The new DDBP approach uses an architecture comprising a series of algorithms, which take a BN and convert it to region graph where inference is done using Generalized Belief Propagation (GBP). This is done in two steps. Firstly we convert a BN to a DCCD and then into a full-Binary Factored Graph (full-BFG), to ensure it is I-equivalent to the original joint probability distribution. Next we take the fullBFG and use a Triplet Region Construction (TRC) algorithm to produce an optimized region graph upon which inference can take place. For discrete variables this is done directly on the region graph but when the model contains continuous variables we employ DD to approximate these variables.\nThe TRC algorithm we have developed is a variant of the CVM algorithm, designed to have three desirable properties for inference: a) it is acyclic and two-level, is b) balanced, in that the counting numbers are all one and c) it is maximum entropy normal. These properties mean that the algorithm is likely to converge quickly and produce accurate results. Unlike the JT algorithm, whose complexity is exponential in the worst case, the complexity of TRC is polynomial in all cases where the original joint distribution is BF-decomposable. TRC algorithm has close connections with join graph based approach which means it guarantees to converge in finite time.\nWith DDBP we can deal with arbitrary continuous variables and are not restricted to Gaussian families. Experiments undertaken to evaluate the algorithm include inference on two discrete binary full-BFG models and 10, 15 and 20 dimensional MGD models, with and without observations, a linear regression model and finally a large aggregation model. Results show that our algorithm converges and achieves comparably accurate results compared to exact solutions and competing approximation methods.\nThe DDBP algorithm offers a solution to performing Bayesian risk aggregation of inter-dependent random variables, i.e. illustrated in section 5.4.7. This positively answers research hypotheses 4 in Chapter 1, Section 1.2.\nNote that to connect the work described in this Chapter to Chapter 4, it is easy to see that each n-fold convolution output of inter-dependent random variables can then be used to perform stochastic N-fold convolution using BFE."}, {"heading": "6. Conclusions and Future Work", "text": "This thesis has addressed a general Bayesian risk aggregation framework, using a family of algorithms: BFE, TRC and DDBP. Together these algorithms positively satisfy the four hypotheses set out in Chapter 1 and, in sum, they provide general purpose approximate algorithms that provide accurate results.\nMany popular financial methods can be accommodated to our approach and can now be extended to include causal risk factors. With regard to the BFE algorithm, ongoing and future research is focused on more complex situations involving both copulas and common cause variables. The challenge here is to decompose these models into lower dimensional distributions where complexity is minimized by factorization. One final area of interest includes optimization of the results such that we might choose a set of actions in the model that maximize returns for minimum risk: deconvolution looks promising here.\nThis research proposed a new inference approach, DDBP, which is general purpose and improves the current discrete inference approaches for many classes of Bayesian network, and not simply those encountered in the area of financial applications. Further work to the DDBP and TRC algorithms include optimizing the efficiency and resolution of the prototype implementations of the algorithms. For example the current DD splitting strategy is based on univariate densities, whilst a finer-grained resolution can be achieved by splitting on the joint distribution (Langseth, Neil, & Marquez, 2013), i.e. on the clusters at the cost of efficiency. Likewise further work would demand greater focus on converting discrete and hybrid BNs to DCCD-BFGs using appropriate factorization methods. The search for methods to model arbitrary distributions with general dependencies, such as pair copulas, which can be represented using our factorization and discretization architecture, is ongoing so that inference may be carried out for arbitrary models. A thorough comparison with other related algorithms, such as EP and PBP, will be conducted in future research."}], "references": [{"title": "AgenaRisk", "author": ["AgenaRisk."], "venue": "Retrieved from http://www.agenarisk.com/ Arbenz, P., & Canestraro, D. (2012). Estimating Copulas for Insurance from Scarce Observations, Expert Opinion and Prior Information: A Bayesian Approach. ASTIN Bulletin, 42(01), 271\u2013290. doi:10.2143/AST.42.1.2160743 Arbenz, P., Embrechts, P., & Puccetti, G. (2011). The AEP algorithm for the fast", "citeRegEx": "AgenaRisk.,? 2014", "shortCiteRegEx": "AgenaRisk.", "year": 2014}, {"title": "May). variational algorithms for approximate bayesian inference. University College London, The Gatsby Computational Neuroscience Unit", "author": ["M.J. Beal"], "venue": null, "citeRegEx": "Beal,? \\Q2003\\E", "shortCiteRegEx": "Beal", "year": 2003}, {"title": "Challenges and Advances in High Dimensional and High Complexity Monte Carlo Computation and Theory. Presented at the BIRS workshop, Canada", "author": ["D. Ceperley", "Y. Chen", "R. Craiu", "Meng", "X.-L", "A. Mira", "J. Rosentha"], "venue": null, "citeRegEx": "Ceperley et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ceperley et al\\.", "year": 2012}, {"title": "Algebra of Probable Inference", "author": ["T. R"], "venue": "With Bayesian Networks. Journal of Risk and Insurance,", "citeRegEx": "R.,? \\Q2007\\E", "shortCiteRegEx": "R.", "year": 2007}, {"title": "Mini-buckets: A General Scheme for Bounded Inference", "author": ["R. Dechter", "I. Rish"], "venue": "J. ACM,", "citeRegEx": "Dechter and Rish,? \\Q2003\\E", "shortCiteRegEx": "Dechter and Rish", "year": 2003}, {"title": "Copula Bayesian Networks", "author": ["G. Elidan"], "venue": "Embrechts, P., & Frei, M. (2009). Panjer recursion versus FFT for compound distributions. Mathematical Methods of Operations Research, 69(3), 497\u2013508. doi:10.1007/s00186-008-0249-2 Fenton, N., & Neil, M. (2010). Comparing risks of alternative medical diagnosis", "citeRegEx": "Elidan,? 2010", "shortCiteRegEx": "Elidan", "year": 2010}, {"title": "Learning Bayesian Networks With Local Structure", "author": ["N. Friedman", "M. Goldszmidt"], "venue": null, "citeRegEx": "Friedman and Goldszmidt,? \\Q1996\\E", "shortCiteRegEx": "Friedman and Goldszmidt", "year": 1996}, {"title": "The calculation of aggregate loss distributions (pp. 22\u201361)", "author": ["W.K. Hastings"], "venue": "Monte Carlo Sampling Methods Using Markov Chains and Their Applications. Biometrika,", "citeRegEx": "Hastings,? \\Q1970\\E", "shortCiteRegEx": "Hastings", "year": 1970}, {"title": "Bayesian Approach to Inverse Problems", "author": ["J. Society. Idier"], "venue": "John Wiley & Sons. Ihler, A. T., & McAllester, D. A. (2009). Particle Belief Propagation. In AISTATS (pp. 256\u2013263). IMF. (2009). IMF Global Financial Stability Report -- Navigating the Financial", "citeRegEx": "Idier,? 2010", "shortCiteRegEx": "Idier", "year": 2010}, {"title": "Bayesian updating in causal probabilistic networks by local computations", "author": ["Ahead. Washington", "A. DC. Jaimovich", "O. Meshi", "I. Mcgraw", "G. Elidan", "N. Friedman"], "venue": "Computational Statistics Quaterly,", "citeRegEx": "Washington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Washington et al\\.", "year": 2014}, {"title": "Bayesian Networks and Decision Graphs", "author": ["F.V. 269\u2013282. Jensen", "T.D. Nielsen"], "venue": null, "citeRegEx": "Jensen and Nielsen,? \\Q2009\\E", "shortCiteRegEx": "Jensen and Nielsen", "year": 2009}, {"title": "Approximation algorithms for graphical models", "author": ["K. Irvine. Kask", "R. Dechter", "J. Larrosa", "A. Dechter"], "venue": "Information and Computer Science,", "citeRegEx": "Kask et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kask et al\\.", "year": 2001}, {"title": "A Theory of Cooperative Phenomena", "author": ["R. Kikuchi"], "venue": "Physical Review,", "citeRegEx": "Kikuchi,? \\Q1951\\E", "shortCiteRegEx": "Kikuchi", "year": 1951}, {"title": "Convergent Tree-Reweighted Message Passing for Energy Minimization", "author": ["V. Kolmogorov"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(10), 1568\u20131583. doi:10.1109/TPAMI.2006.200 Kozlov, A. V., & Koller, D. (1997). Nonuniform Dynamic Discretization in Hybrid Networks. In Proceedings of the Thirteenth Conference on Uncertainty in Artificial", "citeRegEx": "Kolmogorov,? 2006", "shortCiteRegEx": "Kolmogorov", "year": 2006}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B.J. Frey", "Loeliger", "H.-A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Kschischang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 2001}, {"title": "Fast approximate inference in hybrid Bayesian networks using dynamic discretisation", "author": ["H. Langseth", "M. Neil", "D. Marquez"], "venue": "(SSRN Scholarly Paper No. ID 1278435)", "citeRegEx": "Langseth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Langseth et al\\.", "year": 2013}, {"title": "Propagation of Probabilities, Means and Variances in Mixed Graphical Association Models", "author": ["S.L. Lauritzen"], "venue": "Journal of the American Statistical Association, 87, 1098\u20131108. Lauritzen, S. L. (1996). Graphical Models. Oxford University Press. Lauritzen, S., & Spiegelhalter, D. (1988). Local Computations with Probabilities on", "citeRegEx": "Lauritzen,? 1992", "shortCiteRegEx": "Lauritzen", "year": 1992}, {"title": "Tree approximation for belief updating", "author": ["R. Mateescu", "R. Dechter", "K. Kask"], "venue": "Proceedings of the 18th National Conference on Artificial Intelligence", "citeRegEx": "Mateescu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Mateescu et al\\.", "year": 2002}, {"title": "Stan, scalable software for Bayesian modeling", "author": ["H. Matthew", "B. Carpenter", "A. Gelman"], "venue": "In Proceedings of the NIPS Workshop on Probabilistic Programming", "citeRegEx": "Matthew et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Matthew et al\\.", "year": 2012}, {"title": "Convergent message passing algorithms - a unifying view (pp. 393\u2013401)", "author": ["T. Meltzer", "A. Globerson", "Y. Weiss"], "venue": "Presented at the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Meltzer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meltzer et al\\.", "year": 2009}, {"title": "Expectation Propagation for Approximate Bayesian Inference", "author": ["T.P. Minka"], "venue": "In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (pp. 362\u2013369)", "citeRegEx": "Minka,? \\Q2001\\E", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Using Bayesian networks to model the operational risk to information technology infrastructure in financial institutions", "author": ["M. doi:10.1109/TKDE.2011.87 Neil", "N. Fenton"], "venue": "Journal of Financial Transformation,", "citeRegEx": "Neil and Fenton,? \\Q2008\\E", "shortCiteRegEx": "Neil and Fenton", "year": 2008}, {"title": "Modelling dependable systems using hybrid Bayesian networks", "author": ["M. Neil", "M. Tailor", "D. Marquez", "N. Fenton", "P. Hearty"], "venue": "Reliability Engineering & System Safety,", "citeRegEx": "Neil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Neil et al\\.", "year": 2008}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann. Pearl, J. (1993). [Bayesian Analysis in Expert Systems]: Comment: Graphical Models, Causality and Intervention. Statistical Science, 8(3), 266\u2013269. Pearl, J. (2000). Causality: Models, Reasoning, and Inference. Cambridge University", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Modelling Operational Risk Losses with Graphical Models and Copula Functions", "author": ["D. Press. Politou", "P. Giudici"], "venue": "Methodology and Computing in Applied Probability,", "citeRegEx": "Politou and Giudici,? \\Q2009\\E", "shortCiteRegEx": "Politou and Giudici", "year": 2009}, {"title": "The R Project for Statistical Computing. Retrieved 23 December 2013, from http://www.r-project.org/ Rebonato, R", "author": [], "venue": null, "citeRegEx": "136,? \\Q2013\\E", "shortCiteRegEx": "136", "year": 2013}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S.J. Russell", "P. Norvig"], "venue": "Probability propagation. Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "Russell and Norvig,? \\Q2010\\E", "shortCiteRegEx": "Russell and Norvig", "year": 2010}, {"title": "Nonparametric belief propagation", "author": ["E.B. Sudderth", "A.T. Ihler", "W.T. Freeman", "A.S. Willsky"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Sudderth et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sudderth et al\\.", "year": 2003}, {"title": "On the Choice of Regions for Generalized Belief Propagation", "author": ["M. Welling"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Welling,? \\Q2004\\E", "shortCiteRegEx": "Welling", "year": 2004}], "referenceMentions": [{"referenceID": 23, "context": "This algorithm exploits a number of advances from the field of Bayesian Networks (BNs) (Pearl, 1988) (F.", "startOffset": 87, "endOffset": 100}, {"referenceID": 3, "context": "Risk aggregation in the presence of discrete causally connected random variables. Annals of Actuarial Science, 8 (2), pp 298-319 doi:10.1017/S1748499514000098 2. Lin, P., Neil, M., & Fenton, N. (2014). Inference for high dimensional Bayesian network models using dynamically discretized belief propagation, submitted to Information Sciences.", "startOffset": 0, "endOffset": 201}, {"referenceID": 23, "context": "However, probability is not only about numbers, but also about the structure of reasoning (Pearl, 1988), which can be used to reason causally (i.", "startOffset": 90, "endOffset": 103}, {"referenceID": 23, "context": "The complexity problem of the computation of joint probability can be overcome by exploiting conditional independence (Dawid, 1979) (Pearl, 1988).", "startOffset": 132, "endOffset": 145}, {"referenceID": 2, "context": "When designing an MCMC algorithm there is always a balance to be found between exploiting information to adjust the parameters and searching for new regions of the sample space (Ceperley et al., 2012).", "startOffset": 177, "endOffset": 200}, {"referenceID": 1, "context": "The mainstream methods include \u2018variational\u2019 approaches (Beal, 2003) and the mean field algorithm [17], a simplified variational algorithm (Jordan, et al, 1998), which involves choosing a family of distributions over the latent (as opposed to observable) variables with their own variational parameters.", "startOffset": 56, "endOffset": 68}, {"referenceID": 23, "context": "BP: Belief Propagation (BP) (Pearl, 1988) (Kschischang, Frey, & Loeliger, 2001), also known as sum-product message passing, is a message passing", "startOffset": 28, "endOffset": 41}, {"referenceID": 28, "context": "Unlike most sampling schemes, BP does not suffer from high variance and is often much more efficient (Welling, 2004).", "startOffset": 101, "endOffset": 116}, {"referenceID": 12, "context": "A more general version is called Kikuchi (Kikuchi, 1951) approximation, where variables are grouped into clusters.", "startOffset": 41, "endOffset": 56}, {"referenceID": 14, "context": "In all standard BP algorithms (Kschischang et al., 2001), messages are sent from one node to a neighbour node in a factor graph.", "startOffset": 30, "endOffset": 56}, {"referenceID": 14, "context": "Both BNs and MNs can be represented by a unifying representation called a Factor Graph (FG) (Kschischang et al., 2001).", "startOffset": 92, "endOffset": 118}, {"referenceID": 14, "context": "An FG is a particular type of graphical model with applications in Bayesian inference that enables efficient computation of marginal distributions through the sum-product algorithm 4 (Koller & Friedman, 2009) , (Kschischang et al., 2001).", "startOffset": 211, "endOffset": 237}, {"referenceID": 28, "context": "(Welling, Minka, & Teh, 2005) (Welling, 2004) (Gelfand & Welling, 2012) discuss ways to structure region graphs based on graphic topology and offer guidance based on structural information criteria, a sequential approach where new regions are added bottom-up to the region graph, and tree-robustness.", "startOffset": 30, "endOffset": 45}, {"referenceID": 28, "context": "Welling (Welling, 2004) has mentioned the impact of interaction strength by adding extra candidate regions to the", "startOffset": 8, "endOffset": 23}, {"referenceID": 0, "context": "We have used AgenaRisk (AgenaRisk, 2014), a BN package and extended it to incorporate the new BFE algorithm and carry out the experiments described in Section 4.", "startOffset": 23, "endOffset": 40}, {"referenceID": 23, "context": "Our CDF algorithm uses DDJT as the approximate inference for mixture models, and is an implementation version of cut set conditioning (Pearl, 1988).", "startOffset": 134, "endOffset": 147}, {"referenceID": 8, "context": "However, it is first necessary that the density function for S possess an inverse, and should this not exist or if the convolution algebra admits zero divisors, this, unfortunately, results in an infinite number of solutions (Idier, 2010).", "startOffset": 225, "endOffset": 238}, {"referenceID": 7, "context": "MCMC, however, may be generally applicable, such as a Metropolis-Hasting sampler (Metropolis, et al, 1953) (Hastings, 1970) or Gelman\u2019s Stan toolbox (Matthew, Carpenter, & Gelman, 2012), but a general MCMC sampler may still perform poorly on a problem without bespoke design or parameter adjustment.", "startOffset": 107, "endOffset": 123}, {"referenceID": 5, "context": "copula Bayesian Network (CBN) (Elidan, 2010), where the joint multivariate distribution can be decomposed by copula functions, and in further can be factorized into conditional forms.", "startOffset": 30, "endOffset": 44}, {"referenceID": 0, "context": "All experiments are implemented onto Bayesian software AgenaRisk (AgenaRisk, 2014).", "startOffset": 65, "endOffset": 82}, {"referenceID": 28, "context": "To identify TRC regions, our algorithm is similar to Welling\u2019s (Welling, 2004), where we partition the full-BFG into triplets to model interactions.", "startOffset": 63, "endOffset": 78}, {"referenceID": 28, "context": ", 2005) (Welling, 2004) (Gelfand & Welling, 2012) we identify three properties necessary for guaranteeing the best approximation under GBP: Property 1: Acyclic \u2013 the region graph is acyclic and contains two levels.", "startOffset": 8, "endOffset": 23}, {"referenceID": 13, "context": "Many popular message passing algorithms, such as the tree reweighted max-product algorithm (Kolmogorov, 2006) and loopy BP (Murphy et al.", "startOffset": 91, "endOffset": 109}, {"referenceID": 28, "context": "Our TRC algorithm is top down and comparable, in terms of resulting region graph structure, to bottom-up region pursuing algorithms, such as (Welling, 2004).", "startOffset": 141, "endOffset": 156}, {"referenceID": 28, "context": "The cognate intersection pruning algorithm is actually generating a local equivalent structure to that which would be produced by adding outer regions 8 to existing region graph shown in (Welling, 2004), resulting in a valid region graph.", "startOffset": 187, "endOffset": 202}, {"referenceID": 0, "context": "The DDBP algorithm was written in Java and used libraries available on the AgenaRisk product (AgenaRisk, 2014) using Java JDK 6.", "startOffset": 93, "endOffset": 110}], "year": 2015, "abstractText": "Risk aggregation is a popular method used to estimate the sum of a collection of financial assets or events, where each asset or event is modelled as a random variable. Applications, in the financial services industry, include insurance, operational risk, stress testing, and sensitivity analysis, but the problem is widely encountered in many other application domains. This thesis has contributed two algorithms to perform Bayesian risk aggregation when model exhibit hybrid dependency and high dimensional inter-dependency. The first algorithm operates on a subset of the general problem, with an emphasis on convolution problems, in the presence of continuous and discrete variables (so called hybrid models) and the second algorithm offer a universal method for general purpose inference over much wider classes of Bayesian Network models. The first algorithm is called the Bayesian Factorization and Elimination (BFE) algorithm which performs convolution on the hybrid models required to aggregate risk in the presence of causal dependencies. This algorithm exploits a number of advances from the field of Bayesian Networks, covering methods to approximate statistical and conditionally deterministic functions to factorize multivariate distributions for efficient computation. This algorithm aims to support the representation of Bayesian \u201cviews\u201d in an explicit causal dependent structure, whilst providing the computational framework for evaluating convolution models. Such causal models would involve discrete explanatory (regime switching) variables, hybrid mixtures of dependent discrete and continuous variables, and high dimensional inter-dependent continuous variables. The second algorithm developed is called Dynamic Discretized Belief Propagation (DDBP). It combines a dynamic discretization approach, to approximate continuous variables, with a new Triplet Region Construction (TRC) algorithm to perform inference on high dimensional hybrid models. The TRC algorithm is an optimized region graph approach based on graph factorization and Generalized Belief Propagation (GBP), which reduces the model complexity from exponential to polynomial. Proofs and experiments show that the algorithm converges, meets the", "creator": "Microsoft\u00ae Word 2010"}}}