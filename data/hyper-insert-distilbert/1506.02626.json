{"id": "1506.02626", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2015", "title": "Learning both Weights and Connections for Efficient Neural Networks", "abstract": "neural stem networks operations are both itself computationally intensive and therefore memory intensive, making them sometimes difficult to permanently deploy on ordinary embedded systems. also, conventional fibre networks must fix temporarily the architecture layer before training starts ; supposedly as a result, robust training devices cannot regularly improve aboard the architecture. looking to address these limitations, here we describe a redundant method seeks to reduce precisely the storage and configuration computation required continuously by integrating neural networks by inducing an eleventh order amount of eighty magnitude without simply affecting half their accuracy, by learning only the highly important connections. our method prunes redundant connections using effectively a three - legged step method. namely first, we simply train properly the network to learn appropriately which connections measurements are universally important. \u2022 next, we prune the structurally unimportant embedded connections. furthermore finally, we completely retrain the network to fine tune the weights demanded of the remaining connections. on the imagenet pattern dataset, our method reduced the number of parameters of alexnet by a factor error of 9x, drawing from + 61 million to frequency 6. 11 7 million, without incurring accuracy echo loss. as similar experiments sampled with vgg16 algorithm found results that the original network architecture as more a whole can be reduced 6. 8x just by pruning the fully - connected layers, again each with no noise loss matter of original accuracy.", "histories": [["v1", "Mon, 8 Jun 2015 19:28:43 GMT  (2968kb,D)", "http://arxiv.org/abs/1506.02626v1", null], ["v2", "Wed, 29 Jul 2015 22:27:31 GMT  (1484kb,D)", "http://arxiv.org/abs/1506.02626v2", null], ["v3", "Fri, 30 Oct 2015 23:29:27 GMT  (1075kb,D)", "http://arxiv.org/abs/1506.02626v3", "Published as a conference paper at NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["song han", "jeff pool", "john tran", "william j dally"], "accepted": true, "id": "1506.02626"}, "pdf": {"name": "1506.02626.pdf", "metadata": {"source": "CRF", "title": "Learning both Weights and Connections for Efficient Neural Networks", "authors": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "emails": ["dally}@stanford.edu", "johntran}@nvidia.com"], "sections": [{"heading": "1 Introduction", "text": "Neural networks have become ubiquitous in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3]. We consider convolutional neural networks used for computer vision tasks which have grown over time. In 1998 Lecun classified handwritten digits with less than 1M parameters [4], while in 2012, Krizhevsky et al. [1] won the ImageNet competition with 60M parameters. Deepface classified human faces with 120M parameters [5], and Coates et al. [6] scaled up a network to 10B parameters.\nWhile these large neural networks are very powerful, their size consumes considerable storage, memory bandwidth, and computational resources. For embedded mobile applications, these resource demands become prohibitive. Figure 1 shows the energy cost of basic arithmetic and memory operations in a 45nm CMOS process [7]. From this data we see the energy per connection is dominated by memory access and ranges from 3.5pJ for 32b coefficients in on-chip SRAM to 640pJ for 32b coefficients in off-chip LPDDR2 DRAM. Large networks do not fit in on-chip storage and hence require the more costly DRAM accesses. Running a 1G connection neural network, for example, at 20Hz would require (20Hz)(1G)(640pJ) = 12.8W just for DRAM access - well beyond the power envelope of a typical mobile device. Our goal in pruning networks is to reduce the energy required to run such large networks so they can be run in real time on mobile devices.\nTo achieve this goal, we present a method to prune network connections in a manner that preserves the original accuracy. After an initial training phase, we remove all connections whose weight is lower than a threshold. This pruning converts a dense, fully-connected layer to a sparse layer. This first phase learns the topology of the networks \u2014 learning which connections are important and removing the unimportant connections. We then retrain the sparse network so the remaining connections can compensate for the connections that have been removed. The phases of pruning and retraining may\nar X\niv :1\n50 6.\n02 62\n6v 1\n[ cs\n.N E\n] 8\nJ un\n2 01\nbe repeated iteratively to further reduce network complexity. In effect, this training process learns the network connectivity in addition to the weights - much as in the mammalian brain [8] [9], where synapses are created in the first few months of a child\u2019s development, followed by gradual pruning of little-used connections, falling to typical adult values."}, {"heading": "2 Related Work", "text": "Neural networks are typically over-parameterized, and there is significant redundancy for deep learning models [10]. This results in a waste of both computation and memory usage. There have been various proposals to remove the redundancy: Vanhoucke et al. [11] explored a fixed-point implementation with 8-bit integer (vs 32-bit floating point) activations. Denton et al. [12] exploited the linear structure of the neural network by finding an appropriate low-rank approximation of the parameters and keeping the accuracy within 1% of the original model. With similar accuracy loss, Gong et al. [13] compressed deep convnets using vector quantization, which reduced storage but added one level of indirection in memory reference due to accessing the codebook. These approximation and quantization techniques are orthogonal to network pruning, and they can potentially be used together to obtain further gains.\nThere have been other attempts to reduce the number of parameters of neural networks by replacing the fully connected layer with global average pooling. The Network in Network architecture [14] and GoogLenet [15] achieves state-of-the-art results on several benchmarks by adopting this idea. However, transfer learning, i.e. reusing features learned on the ImageNet dataset and applying them to new tasks by only fine-tuning the fully connected layers, is more difficult with this approach. This problem is noted by Szegedy et al [15] and motivates them to add a linear layer on the top of their networks to enable transfer learning.\nNetwork pruning has been used both to reduce network complexity and to reduce over-fitting. An early approach to pruning was biased weight decay [16]. Optimal Brain Damage [17] and Optimal Brain Surgeon [18] prune networks to reduce the number of connections based on the Hessian of the loss function and suggest that such pruning is more accurate than magnitude-based pruning such as weight decay. However, calculating the second derivative is costly for today\u2019s large scale neural networks.\nDropout [19] and DropConnect [20] zeros out activations and connections in the network to reduce over-fitting rather than to improve efficiency. A similar approach was originally described in [21]. Dropout differs from our method both in motivation and in that it zeros connections or activations during training rather than pruning them: during subsequent testing these connections or activations still take on non-zero values. Thus a dense layer remains a dense layer, and there\u2019s no parameter saving at deployment time.\nHashedNets [22] is a recent technique to reduce model sizes by using a hash function to randomly group connection weights into hash buckets, so that all connections within the same hash bucket share a single parameter value. This technique may benefit from pruning. As pointed out in Shi et al. [23] and Weinberger et al. [24], sparsity will minimize hash collision making feature hashing even more effective. HashedNets may be used together with pruning to give even better parameter savings."}, {"heading": "3 Learning Connections in Addition to Weights", "text": "Our pruning method employs a three-step process, as illustrated in Figure 2, which begins by learning the connectivity via normal network training. Unlike conventional training, however, we are not learning the final values of the weights, but rather we are learning which connections are important.\nThe second step is to prune the low-weight connections. All connections with weights below a threshold are removed from the network \u2014 converting a dense network into a sparse network, as shown in Figure 3.\nThe final step retrains the network to learn the final weights for the remaining sparse connections. This step is critical, however, if the pruned network is used without retraining, accuracy is significantly impacted."}, {"heading": "3.1 Regularization", "text": "Choosing the correct regularization impacts the performance of pruning and retraining. L1 regularization penalizes non-zero parameters resulting in more parameters near zero. This gives better accuracy after pruning, but before retraining. However, the remaining connections are not as good as with L2 regularization, resulting in lower accuracy after retraining."}, {"heading": "3.2 Dropout and Capacity Control", "text": "Dropout is widely used to prevent over-fitting, and this also applies to retraining. During retraining, however, the dropout ratio must be adjusted to account for the change in model capacity. Hinton\u2019s dropout [19] can be regarded as a \u201dsoft dropout,\u201d since each parameter is dropped with a probability, not definitely dropped out. Pruning can be regarded as a \u201dhard dropout,\u201d where parameters are dropped forever after pruning and have no chance to come back. As the parameters get sparse, the classifier will select the most informative predictors and thus have much less prediction variance, which reduces over-fitting. As pruning already reduced model capacity, the retraining dropout ratio should be smaller.\nQuantitatively, let Ci be the number of connections in layer i, Cio for the original network, Cir for the network after retraining, Ni be the number of neurons in layer i. Since dropout works on neurons, and the Ci has a square relationship with the Ni, according to equation 1 thus the dropout ratio after pruning the parameters should follow equation 2, where Do represent the original dropout rate, Dr represent the dropout rate during retraining.\nCi = NiNi\u22121 (1) Dr = Do \u221a Cir Cio\n(2)"}, {"heading": "3.3 Local Pruning and Parameter Co-adaptation", "text": "During retraining, it is better to retain the weights from the initial training phase for the connections that survived pruning than it is to re-initialize the pruned layers. [25] shows that CNNs contain fragile co-adapted features: gradient descent is able to find a good solution when the network is initially trained, but not after re-initializing some layers and retraining them. So if we re-initialize the pruned layer, we have to retrain the whole network; if we retrain just the pruned layers, we need to retrain while retaining the surviving parameters.\nRetraining the pruned layers starting with retained weights requires less computation because we don\u2019t have to back propagate through the entire network. Also, neural networks are prone to suffer the vanishing gradient problem [26] as the networks get deeper, which makes pruning errors harder to recover for deep networks. To prevent this, we fix the parameters for part of the network and only retrain a shallow network by reusing the surviving parameters, which already co-adapted well with the un-pruned layers during initial training."}, {"heading": "3.4 Iterative Pruning", "text": "Learning the right connections is an iterative process. Pruning followed by a retraining is one iteration, after many such iterations the minimum number connections could be found. Without loss of accuracy, this method can boost pruning rate from 5\u00d7 to 9\u00d7 on AlexNet compared with single-step aggressive pruning. Each iteration is a greedy search in that we find the best connections. We also experimented with probabilistically pruning parameters based on their absolute value, but this gave worse results."}, {"heading": "3.5 Pruning Neurons", "text": "After pruning connections, neurons with zero input connections or zero output connections may be safely pruned. This pruning is furthered by removing all connections to or from a pruned neuron. The retraining phase automatically arrives at the result where dead neurons will have both zero input connections and zero output connections. This occurs due to gradient descent and regularization. A neuron that has zero input connections(or zero output connections) will have no contribution to the final loss, leading the gradient to be zero for its output connection(or input connection), respectively. Only the regularization term will push the weights to zero. Thus the dead neurons will be automatically removed during retraining."}, {"heading": "4 Experiments", "text": "We implemented network pruning on Caffe [27]. Caffe was modified to add a mask for all the weight tensors, which disregards pruned parameters during network operation. The pruning threshold is chosen as a quality parameter multiplied by the standard deviation of a layer\u2019s weights. We carried out the experiments on Nvidia TitanX and GTX980 GPUs.\nWe pruned 4 networks: 2 on MNIST and 2 on ImageNet datasets. The network parameters and accuracy 1 before and after pruning are shown in Table 1."}, {"heading": "4.1 LeNet on MNIST", "text": "We first experimented on MNIST dataset with LeNet-300-100 and LeNet-5 network [4]. LeNet-300100 is a fully connected network with two hidden layers, with 300 and 100 neurons each, which achieves 1.6% error rate on Mnist. LeNet-5 is a convolutional network that has two convolutional layers and two fully connected layers, which achieves 0.8% error rate on Mnist. After pruning, the network is retrained with 1/10 of the original network\u2019s original learning rate. Table 1 shows pruning\n1Reference accuracy values were generated without data augmentation\nachieves 12\u00d7 parameter savings on these networks. For each layer of the network the table shows (left to right) the original number of weights, the number of floating point operations to compute that layer\u2019s activations, the average percentage of activations that are non-zero, the percentage of weights remaining after pruning, and the percentage of operations remaining after pruning and exploiting sparse activation. The bottom line gives the same data for the overall network.\nAn interesting byproduct of network pruning is that it detects visual attention regions. Figure 4 shows the sparsity pattern of the first fully connected layer of LeNet-300-100, which is of size 784 \u2217 300 (the image size is 28 by 28 and the hidden layer size is 300). It has a banded structure with 28 bands, each band is of size 28, corresponding to the 28\u00d7 28 input pixels. The colored regions of the figure, which means non-zero parameters, correspond to the center of the image. Because digits are written in the center of the image, these are the important parameters. The graph is sparse on the left and right as well, corresponding to the less important regions on the top and bottom of the image. After pruning, the neural network finds the center of the image more important, and the connections to the peripheral regions are more heavily pruned."}, {"heading": "4.2 AlexNet on ImageNet", "text": "We further examine the performance of pruning on the ImageNet ILSVRC-2012 dataset, which has 1.2M training examples and 50k validation examples. We use the AlexNet Caffe model as the reference model, which has 61 million parameters across 5 convolutional layers and 3 fully connected layers. The AlexNet Caffe model achieved a top-1 accuracy of 57.2% and a top-5 accuracy of 80.3%. The original AlexNet took 450K iterations to train [27]. After pruning, the whole network is retrained with 1/100 of the original network\u2019s initial learning rate. 900K and 700K iterations were required to retrain the fully connected and convolutional layers, respectively. Table 1 shows that AlexNet can be\npruned to 1/9 of its original size without impacting accuracy and computation can be reduced by 3\u00d7. The pruning is most effective on the fully-connected layers.\nWe experimented with L1 and L2 regularization before and after retraining, together with iterative pruning to give the five accuracy v.s parameter trade off curves shown in Figure 5. The green and dotted purple lines show the importance of retraining the network after pruning. The purple line shows the accuracy of the network after pruning but before retraining. Without retraining we can still prune the network considerably: half of AlexNet could be pruned without impacting the accuracy much. However, accuracy begins dropping much sooner \u2014 with 1/3 of the original connections, rather than with 1/10 of the original connections. Without retraining we are only able to reduce the number of connections by 2\u00d7 before losing accuracy, while with retraining we are ably to reduce connections by 9\u00d7.\nL1 regularization gives better accuracy than L2 directly after pruning (dotted blue and purple lines) since it pushes more parameters closer to zero. However, comparing the yellow and green lines shows that L2 outperforms L1 after retraining, since there is no benefit to further pushing values towards zero. One extension is to use L1 regularization for pruning and then L2 for retraining, but this did not beat simply using L2 for both phases. Parameters from one mode do not adapt well to the other.\nThe biggest gain comes from iterative pruning (solid red line with solid circles). Here we take the pruned and retrained network (solid green line with circles) and prune and retrain it again. The leftmost dot on this curve corresponds to the point on the green line at 80% (5\u00d7 pruning) pruned to 8\u00d7. There\u2019s no accuracy loss at 9\u00d7. Not until 10\u00d7 does the accuracy begin to drop. Two points achieve slightly better accuracy than the original model after pruning and retraining2. We believe this small accuracy improvement is due to pruning reducing the capacity of the network and hence reducing overfitting.\nBoth fully connected layers and convolutional layers can be pruned. Fully connected layers occupy the majority of the parameters and could be pruned as much as (11\u00d7) compared to convolution layers (3\u00d7). The details of pruning each layer are shown in Table 5. Network pruning not only saves parameter\u2019s memory access but also saves floating point operations, given appropriate hardware support. If either the weight or the activation is zero, the multiply and add operations can be saved. An estimation of computation savings assumes independent distribution of non-zero weights and non-zero activations. The FLOP column is derived by multiplying the non-zero parameter ratio of the current layer with the non-zero activation ratio of the previous layer, since the input of this layer is the output of the previous layer. Although fully connected layer has more parameters and parameter savings, the convolutional layers are more computationally intensive. Thus, the FLOP savings is determined by the convolutional layers, which is 3\u00d7.\nFigure 6 shows the sensitivity of each layer to network pruning. The figure shows how accuracy suffers as parameters are pruned on a layer-by-layer basis. The convolutional layers (on the left) are more sensitive to pruning than the fully connected layers (on the right). The first convolutional layer, which interacts with the input image directly, is most sensitive to pruning. We suspect this sensitivity is due to the input layer having only 3 channels and thus less redundancy than the other convolutional layers. We used these results to find each layer\u2019s threshold. The smallest threshold was applied to the most sensitive layer, which is the first convolutional layer, etc.\nFigure 7 shows histograms of weight distribution before (left) and after (right) pruning. The weight is from the first fully connected layer of AlexNet. The two panels have different y-axis scales. The original distribution of weights is centered on zero with tails dropping off quickly. Almost all parameters are between [\u22120.015, 0.015]. After pruning the large center region is removed. The network parameters adjust themselves during the retraining phase. The result is that the parameters form a bimodal distribution and become more spread across the x-axis, between [\u22120.025, 0.025]."}, {"heading": "4.3 VGG16 on ImageNet", "text": "With promising results on AlexNet, we also looked at a larger, more recent network, VGG16 [30], on the same ILSVRC-2012 dataset. VGG16 has far more convolutional layers but still only three fully-\n2We verified that the Caffe model is fully trained.\nconnected layers. Following a similar methodology, we aggressively pruned these fully-connected layers to realize a significant reduction in the number of weights, shown in Table 6. Specifically, we used three iterations of pruning on the pre-trained network from Caffe.\nThe VGG16 results are, like those for AlexNet, very promising. The network as a whole has been reduced to 14.6% of its original size, and we have not yet had a chance to prune the convolutional layers, which are 6\u00d7 larger than those in AlexNet. That is, all the gains so far comes solely from pruning the fully-connected layers. In particular, note that the two larger fully-connected layers can each be pruned to less than 4% of their original size. This reduction is critical for real time image processing, where there is little reuse of these layers across images (unlike batch processing). The reduced layers will fit in an on-chip SRAM and have modest bandwidth requirements. Without the reduction the bandwidth requirements are prohibitive."}, {"heading": "5 Conclusion", "text": "We have presented a method to improve the efficiency of neural networks without affecting accuracy by finding the right connections. Our method, motivated in part by how learning works in the mammalian brain, operates by learning which connections are important, pruning the unimportant connections, and then retraining the remaining sparse network. We highlight our experiments on AlexNet on ImageNet, showing that both fully connected layer and convolutional layer can be pruned, reducing the number of connections by 9\u00d7 without loss of accuracy, or by 12\u00d7 while incurring only 0.3% accuracy loss. We show similar results for VGG16 and LeNet networks. This leads to smaller memory capacity and bandwidth requirements for real-time image processing."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep learning with cots hpc systems", "author": ["Adam Coates", "Brody Huval", "Tao Wang", "David Wu", "Bryan Catanzaro", "Ng Andrew"], "venue": "In 30th ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Neuronal mechanisms of developmental plasticity in the cat\u2019s visual system", "author": ["JP Rauschecker"], "venue": "Human neurobiology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}, {"title": "Peter huttenlocher (1931-2013)", "author": ["Christopher A Walsh"], "venue": "Nature, 502(7470):172\u2013172,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Improving the speed of neural networks on cpus", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Comparing biases for minimal network construction with back-propagation", "author": ["Stephen Jos\u00e9 Hanson", "Lorien Y Pratt"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1989}, {"title": "Optimal brain damage", "author": ["Yann Le Cun", "John S. Denker", "Sara A. Solla"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G Stork"], "venue": "Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann L Cun", "Rob Fergus"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Improving generalization of neural networks through pruning", "author": ["Hans Henrik Thodberg"], "venue": "International Journal of Neural Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1991}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "arXiv preprint arXiv:1504.04788,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Hash kernels for structured data", "author": ["Qinfeng Shi", "James Petterson", "Gideon Dror", "John Langford", "Alex Smola", "SVN Vishwanathan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "How transferable are features in deep neural networks", "author": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Deep fried convnets", "author": ["Zichao Yang", "Marcin Moczulski", "Misha Denil", "Nando de Freitas", "Alex Smola", "Le Song", "Ziyu Wang"], "venue": "arXiv preprint arXiv:1412.7149,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Memory bounded deep convolutional networks", "author": ["Maxwell D Collins", "Pushmeet Kohli"], "venue": "arXiv preprint arXiv:1412.1442,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Neural networks have become ubiquitous in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Neural networks have become ubiquitous in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "Neural networks have become ubiquitous in applications ranging from computer vision [1] to speech recognition [2] and natural language processing [3].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "In 1998 Lecun classified handwritten digits with less than 1M parameters [4], while in 2012, Krizhevsky et al.", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "[1] won the ImageNet competition with 60M parameters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Deepface classified human faces with 120M parameters [5], and Coates et al.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "[6] scaled up a network to 10B parameters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "In effect, this training process learns the network connectivity in addition to the weights - much as in the mammalian brain [8] [9], where synapses are created in the first few months of a child\u2019s development, followed by gradual pruning of little-used connections, falling to typical adult values.", "startOffset": 125, "endOffset": 128}, {"referenceID": 7, "context": "In effect, this training process learns the network connectivity in addition to the weights - much as in the mammalian brain [8] [9], where synapses are created in the first few months of a child\u2019s development, followed by gradual pruning of little-used connections, falling to typical adult values.", "startOffset": 129, "endOffset": 132}, {"referenceID": 8, "context": "Neural networks are typically over-parameterized, and there is significant redundancy for deep learning models [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "[11] explored a fixed-point implementation with 8-bit integer (vs 32-bit floating point) activations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] exploited the linear structure of the neural network by finding an appropriate low-rank approximation of the parameters and keeping the accuracy within 1% of the original model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] compressed deep convnets using vector quantization, which reduced storage but added one level of indirection in memory reference due to accessing the codebook.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "The Network in Network architecture [14] and GoogLenet [15] achieves state-of-the-art results on several benchmarks by adopting this idea.", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "This problem is noted by Szegedy et al [15] and motivates them to add a linear layer on the top of their networks to enable transfer learning.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "An early approach to pruning was biased weight decay [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Optimal Brain Damage [17] and Optimal Brain Surgeon [18] prune networks to reduce the number of connections based on the Hessian of the loss function and suggest that such pruning is more accurate than magnitude-based pruning such as weight decay.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "Optimal Brain Damage [17] and Optimal Brain Surgeon [18] prune networks to reduce the number of connections based on the Hessian of the loss function and suggest that such pruning is more accurate than magnitude-based pruning such as weight decay.", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "Dropout [19] and DropConnect [20] zeros out activations and connections in the network to reduce over-fitting rather than to improve efficiency.", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "Dropout [19] and DropConnect [20] zeros out activations and connections in the network to reduce over-fitting rather than to improve efficiency.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "A similar approach was originally described in [21].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "HashedNets [22] is a recent technique to reduce model sizes by using a hash function to randomly group connection weights into hash buckets, so that all connections within the same hash bucket share a single parameter value.", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "[23] and Weinberger et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24], sparsity will minimize hash collision making feature hashing even more effective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Hinton\u2019s dropout [19] can be regarded as a \u201dsoft dropout,\u201d since each parameter is dropped with a probability, not definitely dropped out.", "startOffset": 17, "endOffset": 21}, {"referenceID": 22, "context": "[25] shows that CNNs contain fragile co-adapted features: gradient descent is able to find a good solution when the network is initially trained, but not after re-initializing some layers and retraining them.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Also, neural networks are prone to suffer the vanishing gradient problem [26] as the networks get deeper, which makes pruning errors harder to recover for deep networks.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "We implemented network pruning on Caffe [27].", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "We first experimented on MNIST dataset with LeNet-300-100 and LeNet-5 network [4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 25, "context": "Network Top-1 Error Top-5 Error Parameters Compression Rate Fastfood-32-AD [28] 41.", "startOffset": 75, "endOffset": 79}, {"referenceID": 25, "context": "8M 2\u00d7 Fastfood-16-AD [28] 42.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "7\u00d7 Collins & Kohli [29] 44.", "startOffset": 19, "endOffset": 23}, {"referenceID": 26, "context": "[29] reduced the parameters by 4\u00d7 and with inferior accuracy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Deep Fried Convnets [28] worked on fully connected layers only and reduced the parameters by less than 4\u00d7.", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "[12] exploited linear structure of convnets and reduced the parameters of each layer separately, where model compression on a single layer incurred 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "The original AlexNet took 450K iterations to train [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 27, "context": "With promising results on AlexNet, we also looked at a larger, more recent network, VGG16 [30], on the same ILSVRC-2012 dataset.", "startOffset": 90, "endOffset": 94}], "year": 2015, "abstractText": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy, by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9\u00d7, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG16 found that the network as a whole can be reduced 6.8\u00d7 just by pruning the fully-connected layers, again with no loss of accuracy.", "creator": "LaTeX with hyperref package"}}}