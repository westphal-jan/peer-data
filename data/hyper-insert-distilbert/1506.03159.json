{"id": "1506.03159", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2015", "title": "Copula variational inference", "abstract": "we develop immediately a general estimation methodology notation for variational inference which preserves arbitrary dependency among the many latent candidate variables. this is done by augmenting the entire families of derived distributions used in arithmetic mean - directional field inversion and structured approximation with copulas. numerical copulas typically allow one to separately to model partially the insertion dependency given a factorization of the variational information distribution, and can guarantee better us utilizing better compared approximations sufficient to determine the posterior bias as measured when by kl denotes divergence. we truly show immediately that hybrid inference generated on the augmented distribution is highly scalable using symmetric stochastic optimization. { furthermore, the natural addition curve of a copula identity is nearly generic and can be typically applied straightforwardly coupled to any inference procedure measured using the original modified mean - field derivative or mixed structured calculation approach. this vastly reduces bias, introduces sensitivity to local optima, enhance sensitivity directly to hyperparameters, and significantly helped helps characterize and interpret the dependency among multiple the latent variables.", "histories": [["v1", "Wed, 10 Jun 2015 04:14:22 GMT  (573kb,D)", "http://arxiv.org/abs/1506.03159v1", null], ["v2", "Sat, 31 Oct 2015 06:52:07 GMT  (998kb,D)", "http://arxiv.org/abs/1506.03159v2", "Appears in Neural Information Processing Systems, 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.CO stat.ME", "authors": ["dustin tran", "david m blei", "edoardo m airoldi"], "accepted": true, "id": "1506.03159"}, "pdf": {"name": "1506.03159.pdf", "metadata": {"source": "CRF", "title": "Variational inference with copula augmentation", "authors": ["Dustin Tran", "David M. Blei", "Edoardo M. Airoldi"], "emails": [], "sections": [{"heading": null, "text": "We develop a general methodology for variational inference which preserves dependency among the latent variables. This is done by augmenting the families of distributions used in mean-field and structured approximation with copulas. Copulas allow one to separately model the dependency given a factorization of the variational distribution, and can guarantee us better approximations to the posterior as measured by KL divergence. We show that inference on the augmented distribution is highly scalable using stochastic optimization. Furthermore, the addition of a copula is generic and can be applied straightforwardly to any inference procedure using the original meanfield or structured approach. This reduces bias, sensitivity to local optima, sensitivity to hyperparameters, and significantly helps characterize and interpret the dependency among the latent variables.\nKeywords: Bayesian inference, variational inference, copulas, stochastic optimiza-\ntion, network analysis\nar X\niv :1\n50 6.\n03 15\n9v 1\n[ st\nat .M\nL ]\nContents"}, {"heading": "1 Introduction 3", "text": "1.1 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"}, {"heading": "2 Background 4", "text": "2.1 Variational inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 Copulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 Vine copulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"}, {"heading": "3 Copula variational inference 8", "text": "3.1 Sampling from the copula-augmented variational distribution . . . . . . . . . 10\n3.2 Calculating the gradient of the copula-augmented variational distribution . . 11"}, {"heading": "4 Experiments 13", "text": "4.1 Mixture of Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4.2 Latent space model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15"}, {"heading": "5 Conclusion 16", "text": "A Choosing the tree structure and pair copula families 18"}, {"heading": "1 Introduction", "text": "Variational inference is a computationally efficient approach for approximating posterior distributions, and has thus seen wide applicability for scaling complex statistical models (Wainwright and Jordan, 2008). Nevertheless, in order to make the computation tractable one makes either a mean-field independence assumption or slightly relaxes this by preserving some of the original structure among the latent variables (Saul and Jordan, 1995). We propose a general methodology for variational inference which augments the variational distribution with a copula in order to preserve dependency. We list our contributions as follows:\nA generalization of the original procedure in variational inference. The algo-\nrithm we consider generalizes variational inference for mean-field and structured factorizations\u2014 corresponding to running only one step of the algorithm. Furthermore, the algorithm follows an EM-like procedure that monotonically decreases the KL divergence to the posterior at each step, as it alternates between fitting the mean-field parameters and the copula parameters. See Figure 1 which demonstrates the algorithm on a toy example of fitting a bivariate Gaussian posterior.\nImproving generic inference. The addition of a copula family and the proposed procedure can be added to any current mean-field or structured approach for variational inference. As it does not require knowledge of the model, it falls plainly into the framework of modelling dependency in black box variational inference (Ranganath et al., 2014). That is, from the practitioner\u2019s perspective, one need only write down a function to evaluate the model log-likelihood, and the rest of the algorithm\u2019s calculations, such as sampling from the augmented variational distribution and calculating gradients, can easily be placed in a library from which the mean-field and copula parameters are estimated.\nState-of-the-art variational approximations. We demonstrate our method for performing inference on the standard example of Gaussian mixture models, and show that it consistently estimates the parameters and reduces sensitivity to local optima. We also examine the implications of the method for expressing dependency in variational inference on the latent space model. This widens the feasibility of variational inference, which has previously been restricted to either generic inference on simple models\u2014where dependency does not make a significant difference\u2014or writing model-specific variational updates."}, {"heading": "1.1 Related work", "text": "Preserving structure in variational inference has first been studied by Saul and Jordan (1995) in the case of probabilistic neural networks. It has more recently been studied to work with more scalable solutions on particular classes of models (Mimno et al., 2012; Salimans and Knowles, 2013; Hoffman and Blei, 2015). Our work differs from these lines in that we automatically learn the dependency structure during inference, and thus we do not require explicit knowledge of the model. In fact, our augmentation strategy works more broadly to any posterior distribution and any factorized variational family, and thus it generalizes these approaches. The only previous augmentation strategy is in the case of Giordano and Broderick (2015), who apply a calculation in order to correct the estimated posterior covariance from the mean-field. The method assumes the use of exponential families and mean-field, convergence at the true mean-field parameters, and that a linear perturbation of the posterior offers a sufficient covariance correction. Our approach relaxes all of these assumptions respectively by specifying no restrictions on the class of posterior distributions or variational family, by refitting the mean-field and copula parameters conditioned on each other\u2019s estimates, and using a variety of bivariate copula families to model separate dependencies."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Variational inference", "text": "Let x be a set of observations, z be latent variables, and \u03bb be the free parameters of a variational distribution q(z | \u03bb). We aim to find the best approximation of the posterior p(z | x) using the variational distribution q(z | \u03bb), where distance is measured in KL divergence. This is equivalent (Wainwright and Jordan, 2008) to maximizing the quantity\nL (\u03bb) \u2261 Eq [ log p(x, z)\nq(z | \u03bb) ] = Eq[log p(x, z)]\ufe38 \ufe37\ufe37 \ufe38\nenergy\n\u2212Eq[log q(z | \u03bb)]\ufe38 \ufe37\ufe37 \ufe38 entropy\n(1)\nL(\u03bb) is referred to as the Evidence Lower BOund (ELBO) (Blei et al., 2003), or the variational free energy. For simpler computation, the standard assumption is to choose a factorizable family of distributions for the mean-field approximation\nq(z | \u03bb) = d\u220f i=1 qi(zi | \u03bbi) (2)\nwhere z = {z1, . . . , zd}. This is quite a strong independence assumption, and more sophisticated approaches restore several dependencies that are crucial for posterior inference, known as structured variational inference (Saul and Jordan, 1995). In our work, we restore dependencies using copulas, which is flexible to the choice of factorization. If one were to explicitly model particular dependencies due to information about the posterior, then our procedure is to augment the variational distribution with a copula that introduces dependencies between all other pairs of random variables (if they should exist). We next review copulas."}, {"heading": "2.2 Copulas", "text": "Consider the factorization of a d-dimensional random variable z = {z1, . . . , zd} with density q(z) as\nq(z) = [ d\u220f i=1 q(zi) ] c(Q(z1), . . . , Q(zd)) (3)\nwhere Q(zi) is the marginal cumulative distribution function (CDF) of the random variable zi and c is a joint distribution.1 We say the distribution c is the copula of z (Sklar, 1959), which is the joint multivariate density of Q(z1), . . . , Q(zd) with uniform marginal distributions; this is because Q(zi) is the probability integral transform (PIT) of zi, i.e., Q(zi) \u223c U [0, 1]. Such a factorization into a product of marginal densities and a copula always exists and integrates to one (Nelsen, 2006).\nIntuitively, the copula captures the information about the multivariate random variable after eliminating the marginal information, i.e., by applying the probability integral transform on each variable. Therefore c captures only and all of the dependencies among the zi\u2019s. For example, the bivariate Gaussian copula is defined as\ncGauss(u1, u2; \u03c1) \u2261 \u03a6\u03c1(\u03a6\u22121(u1),\u03a6\u22121(u2)) (4)\nwhere \u03a6\u22121 denotes the inverse CDF of a standard normal and \u03a6\u03c1 is the bivariate Gaussian distribution with zero mean and Pearson correlation coefficient \u03c1. In the case of factorizing a bivariate Gaussian density q(z1, z2), the argument \u03a6 \u22121(\u03a6(zi)) cancels, and so the Gaussian copula with parameter \u03c1 directly models the correlation between z1 and z2.\nLearning the copula is difficult, as it requires specifying a family of distributions that is computationally tractable and also expresses a broad range of dependencies. Much historic work has focused on the two-dimensional case, leading to copula families such as the Student-t, Clayton, Gumbel, Frank, and Joe copulas (Nelsen, 2006). However, their multivariate extensions lack the flexibility of accurately modelling dependencies in higher dimensions (Genest et al., 2009). A successful approach in recent literature has been through the construction of a set of conditional bivariate copulas called a vine (Joe, 1996; Bedford and Cooke, 2001, 2002; Kurowicka and Cooke, 2006)."}, {"heading": "2.2.1 Vine copulas", "text": "A vine V specifies a factorization of a copula density c(u1, . . . , ud) into a product of conditional bivariate copulas, also known as pair copulas. This makes it easier to specify a high-dimensional copula, as one need only express the dependence for each pair of random variables conditioned on a subset of the others.\n1We overload the notation for the marginal CDF Q to depend on the names of the argument, and we occasionally use Qi(zi) when more clarity is necessary. This is analogous to the standard convention of overloading the probability density function q(\u00b7).\nSee Figure 2 for an example of a vine which factorizes a 5-dimensional copula into the product of 10 pair copulas. The first tree T1 has nodes 1, 2, 3, 4, 5 representing the random variables u1, u2, u3, u4, u5 respectively. An edge corresponds to a bivariate copula, e.g., 1, 5 symbolizes c(u1, u5). Edges in T1 collapse into nodes in the next tree T2, and edges in T2 correspond to conditional bivariate copulas, e.g., 1, 2|3 symbolizes c(u1, u2|u3). This propagates further until the last nested tree T4, where 4, 5|123 symbolizes c(u4, u5|u1, u2, u3). Each pair copula can be of a different family with its own set of parameters, and thus the vine structure specifies a complete factorization of the multivariate copula:\nc(u1, u2, u3, u4, u5) = c(u1, u5)c(u1, u3)c(u2, u3)c(u3, u4)\u00b7\nc(u1, u2|u3)c(u3, u5|u1)c(u1, u4|u3)\u00b7 c(u2, u4|u1, u3)c(u2, u5|u1, u3)\u00b7 c(u4, u5|u1, u2, u3)\n(5)\nMore formally, a vine is a nested set of trees V = {T1, . . . , Td\u22121} with the following properties:2\n2What we define as vines are also known as regular vines in the literature. They are a generalization of other types of pair copula constructions such as the canonical vines and drawable vines (Dissmann et al., 2012).\n1. Tree Tj = {Nj, Ej} has d+ 1\u2212 j nodes and d\u2212 j edges.\n2. Edges in the jth tree Ej are the nodes in the (j + 1) th tree Nj+1.\n3. Two nodes in tree Tj+1 are joined by an edge only if the corresponding edges in tree\nTj share a node.\nEach edge e in the nested set of trees {T1, . . . , Td\u22121} specifies a different pair copula, and the product of all edges comprise of a factorization of the copula density. Since there are a total of d(d\u2212 1)/2 edges, V factorizes c(u1, . . . , ud) as the product of d(d\u2212 1)/2 pair copulas.\nThe form for each pair copula is specified as follows. Each edge has variable indices C(e), D(e) \u2282 {1, . . . , d} known as the conditioned set and conditioning set respectively. For any edge e(i, k) \u2208 Tj with conditioned set C(e) = {i, k} and conditioning set D(e), we define cik|D(e) to be the bivariate copula density for ui and uk given the value of the conditioning variables {uj : j \u2208 D(e)}:\ncik|D(e) \u2261 c(Qi|D(e), Qk|D(e)|uj : j \u2208 D(e)) (6)\nHere, Qi|D(e) \u2261 Q(ui|uj : j \u2208 D(e)) is the conditional CDF of ui given the value of the conditioning variables {uj : j \u2208 D(e)}. We explain how to obtain Qi|D(e) when its calculation is first required in Section 3.1. Then the vine V specifies the following factorization for the copula density:\nc(u1, . . . , ud; \u03b7) = d\u22121\u220f j=1 \u220f e(i,k)\u2208Ej cik|D(e) (7)\nThis rigorizes the notation in the vine factorization of Figure 2. We also highlight that c depends on \u03b7, the set of all pair copula parameters. Thus the vine construction provides us with the flexibility to model dependencies in high dimensions using a decomposition of pair copulas which are easier to estimate. Moreover, we shall see that the construction allows us to efficiently take stochastic gradients by taking individual (and thus easy) gradients on each pair copula."}, {"heading": "3 Copula variational inference", "text": "We now introduce our method for performing accurate and scalable variational inference. Consider the mean-field approximation augmented with a copula, in which it is easy to extend to any structured factorization as we shall note later. Let \u03bb be the original parameters,\ni.e., of the mean-field, and \u03b7 be the augmented parameters, i.e., of the copula. Then the factorization of the variational distribution is\nq(z | \u03bb, \u03b7) = [ d\u220f i=1 q(zi | \u03bb) ] \ufe38 \ufe37\ufe37 \ufe38\nmean-field\nc(Q(z1 | \u03bb), . . . , Q(zd | \u03bb); \u03b7)\ufe38 \ufe37\ufe37 \ufe38 copula\n(8)\nThe induced (augmented) ELBO that we aim to maximize is\nL (\u03bb, \u03b7) = Eq[log p(x, z)]\u2212 Eq[log q(z | \u03bb, \u03b7)] (9)\nwhich has the gradient (Ranganath et al., 2014)\n\u2207{\u03bb,\u03b7}L = Eq[\u2207{\u03bb,\u03b7} log q(z | \u03bb, \u03b7) \u00b7 (log p(x, z)\u2212 log q(z | \u03bb, \u03b7))] (10)\nCopula variational inference (CVI) alternates between two steps: 1. fix the copula parameters \u03b7 and solve for the mean-field parameters \u03bb; and 2. fix the mean-field parameters \u03bb and solve for the copula parameters \u03b7. Thus solving for the mean-field approximation is the special case of running only the first step once, where we initialize the copula c to be uniform. We employ stochastic optimization for each estimation procedure, and set the learning rate \u03c1t \u2208 R for t = 1, 2, . . . to satisfy the standard assumption for the convergence of stochastic approximations (Robbins and Monro, 1951), i.e., \u2211\u221e t=1 \u03c1t =\u221e, \u2211\u221e t=1 \u03c1 2 t <\u221e. A summary of the algorithm is outlined in Algorithm 1.\nThis alternating set of optimizations falls under the class of minorize-maximization (MM) iterative methods, which includes many procedures such as the alternating least squares algorithm, the iterative procedure for the generalized method of moments (Hansen et al., 1996), and the EM algorithm (Dempster et al., 1977). In particular, it shares the property that each step monotonically increases the objective function, and therefore CVI is guaranteed in each step to better approximate the posterior distribution.\nFor simplicity of describing the method, we assume that the tree structure and copula families are given and remain fixed throughout the sequence of maximizations. One can fit them however, and in our experiments we automatically learn the tree structure and pair copula families using sequential tree selection (Dissmann et al., 2012) and Bayesian model selection (Gruber and Czado, 2015) among a choice of 16 bivariate copula families, c.f., the supplement. In preliminary experiments, we\u2019ve found that re-selection of the tree structure and copula families do not change significantly in future iterations.\nAlgorithm 1 Copula variational inference (CVI)\nInput: data x, joint distribution p, variational family q Initialize \u03bb randomly, \u03b7 so that c is uniform repeat\n// Fix \u03b7, maximize over \u03bb Initialize t = 1 repeat z \u223c q(z | \u03bb, \u03b7) \u03bb = \u03bb+ \u03c1t(\u2207\u03bb log q(z | \u03bb, \u03b7) \u00b7 (log p(x, z)\u2212 log q(z | \u03bb, \u03b7))) t = t+ 1 until change of \u03bb is less than a small 1 // Fix \u03bb, maximize over \u03b7 Initialize t = 1 repeat z \u223c q(z | \u03bb, \u03b7) \u03b7 = \u03b7 + \u03c1t(\u2207\u03b7 log q(z | \u03bb, \u03b7) \u00b7 (log p(x, z)\u2212 log q(z | \u03bb, \u03b7))) t = t+ 1\nuntil change of \u03b7 is less than a small 2 until change of \u03bb and \u03b7 is less than a small 1 and 2 respectively\nCVI is similar in input requirements to black box variational inference (Ranganath et al., 2014). Thus it is as generic, in that the user need only specify the joint probability p(x, z) in order to perform inference. More interestingly, copula variational inference relaxes the restriction that the variational family must be a mean-field, as it works for more arbitrary factorizations: by the vine construction, one chooses the pair copulas corresponding to any dependencies already in the factorization to be the independence copula, which adds uniform mass everywhere and thus adds no additional expressivity.\nThe augmentation of the variational family q with a copula makes it naturally more difficult to both sample from q and calculate the gradient \u2207 log q. We address these two challenges in the following sections."}, {"heading": "3.1 Sampling from the copula-augmented variational distribution", "text": "We sample from the copula-augmented distribution by repeatedly doing inverse transform sampling (Devroye, 1986), also known as inverse CDF, on the individual pair copulas and finally the marginals. This can be computed efficiently in worst-case complexity of O(md2), where m is the number of desired samples and d is the dimension of the distribution. More specifically, the sampling procedure is as follows:\n1. Generate u = (u1, . . . , ud) where each ui \u223c U(0, 1).\n2. Calculate v = (v1, . . . , vd) which follows a joint uniform distribution with dependencies\ngiven by the copula:\nv1 = u1 (11) v2 = Q \u22121 2|1(u2 | v1) (12) v3 = Q \u22121 3|12(u3 | v1, v2) (13)\n...\nvd = Q \u22121 d|12\u00b7\u00b7\u00b7d\u22121(ud | v1, v2, . . . , vd\u22121) (14)\nExplicit calculations of the inverse of the conditional CDFs Q\u22121i|12\u00b7\u00b7\u00b7i\u22121 can be found in Kurowicka and Cooke (2007). In general, one calculates the conditional CDF Qi|D(e) used in the pair copula cik|D(e) at tree Tj by\nQi|D(e) = \u2202\n\u2202Qk|D(e)\\k Cik|D(e)\\k =\n\u2202\n\u2202Qk|D(e)\\k C(Qi|D(e)\\k, Qk|D(e)\\k|uj : j \u2208 D(e)\\k) (15)\nwhere Cik|D(e)\\k, which is the CDF of the pair copula cik|D(e)\\k, and Qi|D(e)\\k, Qk|D(e)\\k are obtained recursively from the previous tree Tj\u22121. This is because D(e)\\k is the conditioning set for the pair copulas and conditional CDFs of the previous tree. In order to calculate all conditional CDFs required for the sampling, the procedure loops through the d(d\u2212 1)/2 pair copulas and thus has worst-case complexity of O(d2).\n3. Calculate z = (Q\u221211 (v1), . . . , Q \u22121 d (vd)), which is a sample from the copula-augmented\ndistribution q(z | \u03bb, \u03b7)."}, {"heading": "3.2 Calculating the gradient of the copula-augmented variational", "text": "distribution\nOne can efficiently take gradients by separating the gradient of the log mean-field from the gradient of the log copula:\n\u2207{\u03bb,\u03b7} log q(z | \u03bb, \u03b7)\n=\n[ \u2207\u03bb \u2211d\ni=1 log q(zi | \u03bbi) +\u2207\u03bb log c(Q(z1 | \u03bb), . . . , Q(zd | \u03bb); \u03b7) \u2207\u03b7 log c(Q(z1 | \u03bb), . . . , Q(zd | \u03bb); \u03b7)\n] (16)\nTherefore, for current mean-field implementations the only change is twofold: the addition of the gradient of the log-copula at the mean-field parameters, and the augmented parameter space which uses the gradient of the log copula at the copula parameters. Analogous results follow for structured approaches, where the gradients evaluated for the pre-specified independence copulas are zero.\nWe show how to take gradients on a vine at each set of parameters. Let the mean-field parameters \u03bb = (\u03bb1, . . . , \u03bbd), where \u03bbi denotes the set of parameters for the i th marginal distribution q(zi | \u03bbi). The gradient of log q(z | \u03bb, \u03b7) at \u03bbi can be further simplified as\n\u2207\u03bbi log q(z | \u03bb, \u03b7)\n= \u2207\u03bbi log q(zi | \u03bbi) +\u2207Q(zi|\u03bbi) log c(Q(z1 | \u03bb1), . . . , Q(zd | \u03bbd); \u03b7)\u2207\u03bbiQ(zi | \u03bbi) (17)\n= \u2207\u03bbi log q(zi | \u03bbi) +\u2207\u03bbiQ(zi | \u03bbi) d\u22121\u2211 j=1 \u2211 e(k,`)\u2208Ej : i\u2208C(e) \u2207Q(zi|\u03bbi) log ck`|D(e) (18)\nThe summation in Equation 18 is over all pair copulas which contain Q(zi | \u03bbi) in its conditioning set C. We note that any non-zero gradients of the pair copulas are gradients with respect to one of the copula\u2019s two arguments.\nThus gradient evaluation at the mean-field parameters \u03bb can be efficiently computed in O(d2) complexity, and we also show that the same holds for the copula parameters \u03b7. Let \u03b7 = (\u03b71, . . . , \u03b7d(d\u22121)/2), where \u03b7i denotes the set of parameters for the i th pair copula. To calculate the gradient of log q(z | \u03bb, \u03b7) at \u03b7i, one sums over all pair copulas whose conditioning set C or conditioned set D contains the ith pair copula:\n\u2207\u03b7i log c(Q(z1 | \u03bb), . . . , Q(zd | \u03bb); \u03b7) = d\u22121\u2211 j=1 \u2211 e(k,`)\u2208Ej :\ne\u03b7i\u2208{C(e),D(e)}\n\u2207\u03b7i log ck`|D(e) (19)\nNote that Equation 19 requires gradients of the pair copulas with respect to copula parameters, and Equation 18 requires gradients of the pair copulas with respect to their arguments. This means one can evaluate gradients for arbitrary pair copula families independent of the marginal CDFs Q(\u00b7), and for convenience one can employ automatic differentiation tools for doing so (Schepsmeier et al., 2015; Stan Development Team, 2014). Hence the copula augmentation can easily be incorporated into any current variational inference algorithm."}, {"heading": "4 Experiments", "text": "In order to demonstrate the efficacy of the method, we run benchmarks on both synthetic data and real data sets using the Gaussian mixture model. We choose this model as it is the classical example which stresses the difficulty of modelling dependency. We also demonstrate the potential of CVI in that it broadens the feasibility of variational inference to more complicated classes of models, such as the latent space model; in such cases the dependency in the latent variables is crucial and the mean-field provides arbitrarily bad estimates.\nBecause of the flexibility of the CVI framework laid out in Algorithm 1, we make several modifications in order to perform better in practice. In particular, we use a mini-batch setting of m = 1024, in which we generate m samples from the variational distribution and take the average of the stochastic gradients in order to reduce the variance at each iteration. We also use ADAM (Kingma and Ba, 2015), which is an adaptive learning rate that combines ideas from both AdaGrad and RMSprop. In practice, we\u2019ve found that ADAM works best among other competitive learning rate schedules."}, {"heading": "4.1 Mixture of Gaussians", "text": "We follow the set of tasks in Giordano and Broderick (2015), which is to estimate the posterior covariance for Gaussian mixture models. Consider an unknown probability vector \u03c0 of length K and a set of P -dimensional multivariate normals N (\u00b5k,\u039b\u22121k ) for k = 1, . . . , K, with unknown mean \u00b5k and P \u00d7 P precision matrix \u039bk. The joint probability for a mixture of Gaussian distributions is given by\np(x, z, \u00b5,\u039b\u22121, \u03c0) = p(\u03c0) K\u220f k=1 p(\u00b5k,\u039b \u22121 k ) N\u220f n=1 p(xn | zn, \u00b5zn ,\u039b\u22121zn )p(zn | \u03c0) (20)\nin which one specifies a Dirichlet prior on \u03c0 and a normal-Wishart prior on \u00b5k and \u039bk. We apply the mean-field approximation (MF) which assigns independent factors on \u00b5, \u03c0,\u039b, and z, and perform CVI over the copula-augmented distribution for this factorization, i.e., which has non-independence pair copulas over the variables (\u00b5k,\u039bk) and over z. We also compare our results to linear response variational Bayes (LRVB) (Giordano and Broderick, 2015), which is a correction technique for covariance estimation using a linear perturbation argument on the posterior.\nFigure 3 displays estimates for the standard deviation of \u039b for 100 simulations, and plots them against the ground truth using 500 effective Gibb samples. We note that estimates for \u00b5 and \u03c0 indicate the same pattern. The second plot displays all off-diagonal covariance estimates.\nUpon initializing at the true mean-field parameters, both CVI and LRVB achieve consistent estimates of the posterior variance, whereas MF underestimates the variance, which is a well-known problem for MF when the components overlap (Wainwright and Jordan, 2008). We also note that because the MF estimates are correctly specified, CVI converges to the truth upon one step of fitting the copula.\nWe highlight that CVI is more robust than LRVB, as LRVB is sensitive to local optima and more generally any problems known for MF. This is because LRVB conditions on the MF parameters in order to estimate the posterior covariance. On the other hand, CVI re-adjusts the MF and copula parameters as it alternates between fitting the two. To demonstrate this, we use the MNIST data set of handwritten digits and perform unsupervised classification for separating 0s and 1s, where the classification is given by the membership assignment. This provides a set of 12,665 training examples and 2,115 test examples. CVI reports an test set error rate of 0.06, whereas depending on the local convergence of the mean-field parameters,\nLRVB ranges between 0.06 and 0.32."}, {"heading": "4.2 Latent space model", "text": "We demonstrate the potential of CVI by performing inference on the latent space model (Hoff et al., 2001), which is a Bernoulli latent factor model for relational data. We simulate a 100,000 node network with latent node attributes from a K = 10 dimensional normal distribution zn \u223c N(\u00b5,\u039b\u22121). For each pair (i, j) we draw a Bernoulli random variable with probability logit(p) = \u03b8 \u2212 |zi \u2212 zj|, where \u03b8 is an unknown value. We apply independent factors on \u00b5,\u039b, \u03b8 and z, and augment the mean-field with non-independence copulas over (\u00b5,\u039b) and z.\nTable 1 examines the predictive likelihood of held out data on CVI compared to both mean-field and LRVB. We ran the minibatch setting of m = 1024 for stochastic gradient descent in parallel on 16 cores and obtained convergence of CVI in two hours, where each step of CVI took 12 minutes on average. MF took roughly 15 minutes. The covariance correction for LRVB took an additional 23 minutes given the MF estimates, and similarly, the second iteration of CVI, in which one fits the copula after performing MF, required an additional 17 minutes. Thus the copula estimation without refitting already dominates LRVB in both runtime and accuracy. We note however that as LRVB requires the inversion of a O(NK3) \u00d7 O(NK3) matrix, one could better scale the method and achieve faster estimates than CVI if one were to apply stochastic approximations for the inversion. On the other hand, CVI always outperforms LRVB in the approximation, and drastically so upon convergence. As a runtime comparison to the best estimate from CVI, we also attempted Hamiltonian Monte Carlo; however, it did not converge within the five hours we allowed it to run."}, {"heading": "5 Conclusion", "text": "We augment the variational distribution with a copula in order to preserve dependency among the latent variables of the posterior. We then propose copula variational inference (CVI), which is a principled and scalable algorithm for performing inference on the copulaaugmented distribution. In CVI, the mean-field and copula estimation proceed in an alternating and highly scalable manner using stochastic optimization. It is a generalization of both mean-field and structured variational inference, which means it can be easily added onto any current software. CVI significantly reduces the bias and better estimates the posterior variance, and is more accurate than other forms of variational approximations. Thus CVI broadens the capabilities of variational inference for achieving scalable estimates while minimizing bias."}, {"heading": "Acknowledgements", "text": "We are grateful to Robin Gong and Luke Bornn, who helped lay the foundation for the methodology. We also thank Alp Kucukelbir for helpful discussions."}, {"heading": "A Choosing the tree structure and pair copula families", "text": "We assume that the vine structure and pair copula families are specified in order to perform CVI, in the same way one must specify the mean-field family for black box variational inference (Ranganath et al., 2014). In general however, given a factorization of the variational distribution, one can determine the tree structure and pair copula families based on synthetic data of the latent variables z.\nDuring tree selection, enumerating and calculating all possibilities is computationally intractable, as the number of possible vines on d variables grows factorially: there exist d!/2 \u00b7 2( d\u22122 2 ) many choices (Morales-Na\u0301poles et al., 2010). The most common approach in practice is to sequentially select the maximum spanning tree starting from the initial tree T1, where the weights of an edge are assigned by absolute values of the Kendall\u2019s \u03c4 correlation on each pair of random variables. Intuitively, the tree structures are selected as to model the strongest pairwise dependencies. This procedure of sequential tree selection follows Dissmann et al. (2012).\nIn order to select a family of distributions for each conditional bivariate copula in the vine, one may employ Bayesian model selection, i.e., choose among a set of families which maximizes the marginal likelihood. We note that both the sequential tree selection and model selection are implemented in the VineCopula package in R (Schepsmeier et al., 2015), which makes it easy for users to learn the structure and families for the copula-augmented variational distribution.\nWe also list below the 16 bivariate copula families used in our experiments.\nFamily Parameter \u03b8(\u03c4) Independent \u2014 \u2014 Gaussian \u03b8 \u2208 [\u22121, 1] sin (\u03c0 2 \u03c4 )\nStudent-t \u03b8 \u2208 [\u22121, 1] Clayton \u03b8 \u2208 (0,\u221e) 2\u03c4/(1\u2212 \u03c4) Gumbel \u03b8 \u2208 [1,\u221e) 1/(1\u2212 \u03c4) Frank \u03b8 \u2208 (0,\u221e) No closed form Joe \u03b8 \u2208 (1,\u221e)\nTable 2: The 16 bivariate copula families, with their parameter domains and expressed in terms of Kendall\u2019s \u03c4 correlations, that we consider in experiments. We include rotated versions (90\u25e6, 180\u25e6, and 270\u25e6) of the Clayton, Gumbel, and Joe copulas."}], "references": [{"title": "Probabilistic density decomposition for conditionally dependent random variables modeled by vines", "author": ["Tim Bedford", "Roger M. Cooke"], "venue": "Annals of mathematics and Artificial Intelligence,", "citeRegEx": "Bedford and Cooke.,? \\Q2001\\E", "shortCiteRegEx": "Bedford and Cooke.", "year": 2001}, {"title": "Vines - a new graphical model for dependent random variables", "author": ["Tim Bedford", "Roger M. Cooke"], "venue": "Annals of Statistics,", "citeRegEx": "Bedford and Cooke.,? \\Q2002\\E", "shortCiteRegEx": "Bedford and Cooke.", "year": 2002}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["Arthur P. Dempster", "Nan M. Laird", "Donald B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Non-Uniform Random Variate Generation", "author": ["Luc Devroye"], "venue": null, "citeRegEx": "Devroye.,? \\Q1986\\E", "shortCiteRegEx": "Devroye.", "year": 1986}, {"title": "Selecting and estimating regular vine copulae and application to financial returns", "author": ["Jeffrey Dissmann", "Eike Christian Brechmann", "Claudia Czado", "Dorota Kurowicka"], "venue": "arXiv preprint arXiv:1202.2002,", "citeRegEx": "Dissmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dissmann et al\\.", "year": 2012}, {"title": "Editorial to the special issue on modeling and measurement of multivariate risk in insurance and finance", "author": ["Christian Genest", "Hans U. Gerber", "Marc J. Goovaerts", "Roger Laeven"], "venue": "Insurance: Mathematics and Economics,", "citeRegEx": "Genest et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Genest et al\\.", "year": 2009}, {"title": "Covariance matrices and influence scores for mean field variational bayes", "author": ["Ryan Giordano", "Tamara Broderick"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Giordano and Broderick.,? \\Q2015\\E", "shortCiteRegEx": "Giordano and Broderick.", "year": 2015}, {"title": "Sequential bayesian model selection of regular vine copulas", "author": ["Lutz Gruber", "Claudia Czado"], "venue": "International Society for Bayesian Analysis,", "citeRegEx": "Gruber and Czado.,? \\Q2015\\E", "shortCiteRegEx": "Gruber and Czado.", "year": 2015}, {"title": "Finite-sample properties of some alternative gmm estimators", "author": ["Lars Hansen", "John Heaton", "Amir Yaron"], "venue": "Journal of Business & Economic Statistics,", "citeRegEx": "Hansen et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 1996}, {"title": "Latent space approaches to social network analysis", "author": ["Peter D. Hoff", "Adrian E. Raftery", "Mark S. Handcock"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hoff et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hoff et al\\.", "year": 2001}, {"title": "Structured Stochastic Variational Inference", "author": ["Matthew D. Hoffman", "David M. Blei"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Hoffman and Blei.,? \\Q2015\\E", "shortCiteRegEx": "Hoffman and Blei.", "year": 2015}, {"title": "Families of m-variate distributions with given margins and m(m\u2212 1)/2 bivariate dependence parameters, pages 120\u2013141", "author": ["Harry Joe"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "Joe.,? \\Q1996\\E", "shortCiteRegEx": "Joe.", "year": 1996}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Uncertainty Analysis with High Dimensional Dependence Modelling", "author": ["Dorota Kurowicka", "Roger M. Cooke"], "venue": null, "citeRegEx": "Kurowicka and Cooke.,? \\Q2006\\E", "shortCiteRegEx": "Kurowicka and Cooke.", "year": 2006}, {"title": "Sampling algorithms for generating joint uniform distributions using the vine-copula method", "author": ["Dorota Kurowicka", "Roger M. Cooke"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Kurowicka and Cooke.,? \\Q2007\\E", "shortCiteRegEx": "Kurowicka and Cooke.", "year": 2007}, {"title": "Sparse stochastic inference for latent dirichlet allocation", "author": ["David M. Mimno", "Matthew D. Hoffman", "David M. Blei"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Mimno et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mimno et al\\.", "year": 2012}, {"title": "Black box variational inference", "author": ["New York", "Inc", "2006. Rajesh Ranganath", "Sean Gerrish", "David M. Blei"], "venue": null, "citeRegEx": "York et al\\.,? \\Q2006\\E", "shortCiteRegEx": "York et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 16, "context": "It has more recently been studied to work with more scalable solutions on particular classes of models (Mimno et al., 2012; Salimans and Knowles, 2013; Hoffman and Blei, 2015).", "startOffset": 103, "endOffset": 175}, {"referenceID": 11, "context": "It has more recently been studied to work with more scalable solutions on particular classes of models (Mimno et al., 2012; Salimans and Knowles, 2013; Hoffman and Blei, 2015).", "startOffset": 103, "endOffset": 175}, {"referenceID": 7, "context": "The only previous augmentation strategy is in the case of Giordano and Broderick (2015), who apply a calculation in order to correct the estimated posterior covariance from the mean-field.", "startOffset": 58, "endOffset": 88}, {"referenceID": 2, "context": "L(\u03bb) is referred to as the Evidence Lower BOund (ELBO) (Blei et al., 2003), or the variational free energy.", "startOffset": 55, "endOffset": 74}, {"referenceID": 6, "context": "However, their multivariate extensions lack the flexibility of accurately modelling dependencies in higher dimensions (Genest et al., 2009).", "startOffset": 118, "endOffset": 139}, {"referenceID": 12, "context": "A successful approach in recent literature has been through the construction of a set of conditional bivariate copulas called a vine (Joe, 1996; Bedford and Cooke, 2001, 2002; Kurowicka and Cooke, 2006).", "startOffset": 133, "endOffset": 202}, {"referenceID": 14, "context": "A successful approach in recent literature has been through the construction of a set of conditional bivariate copulas called a vine (Joe, 1996; Bedford and Cooke, 2001, 2002; Kurowicka and Cooke, 2006).", "startOffset": 133, "endOffset": 202}, {"referenceID": 5, "context": "They are a generalization of other types of pair copula constructions such as the canonical vines and drawable vines (Dissmann et al., 2012).", "startOffset": 117, "endOffset": 140}, {"referenceID": 9, "context": "This alternating set of optimizations falls under the class of minorize-maximization (MM) iterative methods, which includes many procedures such as the alternating least squares algorithm, the iterative procedure for the generalized method of moments (Hansen et al., 1996), and the EM algorithm (Dempster et al.", "startOffset": 251, "endOffset": 272}, {"referenceID": 3, "context": ", 1996), and the EM algorithm (Dempster et al., 1977).", "startOffset": 30, "endOffset": 53}, {"referenceID": 5, "context": "One can fit them however, and in our experiments we automatically learn the tree structure and pair copula families using sequential tree selection (Dissmann et al., 2012) and Bayesian model selection (Gruber and Czado, 2015) among a choice of 16 bivariate copula families, c.", "startOffset": 148, "endOffset": 171}, {"referenceID": 8, "context": ", 2012) and Bayesian model selection (Gruber and Czado, 2015) among a choice of 16 bivariate copula families, c.", "startOffset": 37, "endOffset": 61}, {"referenceID": 4, "context": "We sample from the copula-augmented distribution by repeatedly doing inverse transform sampling (Devroye, 1986), also known as inverse CDF, on the individual pair copulas and finally the marginals.", "startOffset": 96, "endOffset": 111}, {"referenceID": 14, "context": "Explicit calculations of the inverse of the conditional CDFs Q\u22121 i|12\u00b7\u00b7\u00b7i\u22121 can be found in Kurowicka and Cooke (2007). In general, one calculates the conditional CDF Qi|D(e) used in the pair copula cik|D(e) at tree Tj by Qi|D(e) = \u2202 \u2202Qk|D(e)\\k Cik|D(e)\\k = \u2202 \u2202Qk|D(e)\\k C(Qi|D(e)\\k, Qk|D(e)\\k|uj : j \u2208 D(e)\\k) (15) where Cik|D(e)\\k, which is the CDF of the pair copula cik|D(e)\\k, and Qi|D(e)\\k, Qk|D(e)\\k are obtained recursively from the previous tree Tj\u22121.", "startOffset": 92, "endOffset": 119}, {"referenceID": 13, "context": "We also use ADAM (Kingma and Ba, 2015), which is an adaptive learning rate that combines ideas from both AdaGrad and RMSprop.", "startOffset": 17, "endOffset": 38}, {"referenceID": 7, "context": "We follow the set of tasks in Giordano and Broderick (2015), which is to estimate the posterior covariance for Gaussian mixture models.", "startOffset": 30, "endOffset": 60}, {"referenceID": 7, "context": "We also compare our results to linear response variational Bayes (LRVB) (Giordano and Broderick, 2015), which is a correction technique for covariance estimation using a linear perturbation argument on the posterior.", "startOffset": 72, "endOffset": 102}, {"referenceID": 7, "context": "Following Giordano and Broderick (2015), we set N = 10, 000 samples, K = 2 components, and P = 2 dimensional Gaussian distributions.", "startOffset": 10, "endOffset": 40}, {"referenceID": 10, "context": "We demonstrate the potential of CVI by performing inference on the latent space model (Hoff et al., 2001), which is a Bernoulli latent factor model for relational data.", "startOffset": 86, "endOffset": 105}], "year": 2017, "abstractText": "We develop a general methodology for variational inference which preserves dependency among the latent variables. This is done by augmenting the families of distributions used in mean-field and structured approximation with copulas. Copulas allow one to separately model the dependency given a factorization of the variational distribution, and can guarantee us better approximations to the posterior as measured by KL divergence. We show that inference on the augmented distribution is highly scalable using stochastic optimization. Furthermore, the addition of a copula is generic and can be applied straightforwardly to any inference procedure using the original meanfield or structured approach. This reduces bias, sensitivity to local optima, sensitivity to hyperparameters, and significantly helps characterize and interpret the dependency among the latent variables.", "creator": "LaTeX with hyperref package"}}}