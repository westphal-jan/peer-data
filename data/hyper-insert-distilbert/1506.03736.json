{"id": "1506.03736", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2015", "title": "GAP Safe screening rules for sparse multi-task and multi-class models", "abstract": "solving high objective dimensional regression formulation benefits from sparsity after promoting distance regularizations. screening rules leverage prevents the known numerical sparsity of rendering the generic solution by ignoring some variables already in analyzing the expected optimization, hence speeding hurry up solvers. when running the procedure is proven ideally not to discard boundary features parameter wrongly the rules are said itself to ultimately be \\ emph { \\ safe }. in this paper beginning we derive new safe rules relevant for smoothing generalized linear random models regularized with $ \\ ell _ x 1 $ and $ \\ lambda ell _ > 1 / \\ ell _ 2 $ norms. the rules are precisely based on duality gap computations above and spherical lattice safe regions whose diameters converge steadily to absolute zero. completing this allows to carefully discard safely more variables, which in name particular those for for low regularization possible parameters. since the gap safe rule algorithms can rapidly cope precisely with half any iterative variable solver equations and here we already illustrate its interesting performance on coordinate descent for classical multi - sectional task lasso, algebraic binary and multinomial logistic differential regression, demonstrating relatively significant speed build ups on all tested datasets with respect to your previous cluster safe rules.", "histories": [["v1", "Thu, 11 Jun 2015 16:25:36 GMT  (504kb,D)", "https://arxiv.org/abs/1506.03736v1", null], ["v2", "Wed, 18 Nov 2015 10:07:20 GMT  (534kb,D)", "http://arxiv.org/abs/1506.03736v2", "in Proceedings of the 29-th Conference on Neural Information Processing Systems (NIPS), 2015"]], "reviews": [], "SUBJECTS": "stat.ML cs.LG math.OC stat.CO", "authors": ["eug\u00e8ne ndiaye", "olivier fercoq", "alexandre gramfort", "joseph salmon"], "accepted": true, "id": "1506.03736"}, "pdf": {"name": "1506.03736.pdf", "metadata": {"source": "CRF", "title": "GAP Safe screening rules for sparse multi-task and multi-class models", "authors": ["Eug\u00e8ne Ndiaye", "Olivier Fercoq", "Alexandre Gramfort", "Joseph Salmon"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The computational burden of solving high dimensional regularized regression problem has lead to a vast literature in the last couple of decades to accelerate the algorithmic solvers. With the increasing popularity of `1-type regularization ranging from the Lasso [18] or group-Lasso [24] to regularized logistic regression and multi-task learning, many algorithmic methods have emerged to solve the associated optimization problems. Although for the simple `1 regularized least square a specific algorithm (e.g., the LARS [8]) can be considered, for more general formulations, penalties, and possibly larger dimension, coordinate descent has proved to be a surprisingly efficient strategy [12].\nOur main objective in this work is to propose a technique that can speed-up any solver for such learning problems, and that is particularly well suited for coordinate descent method, thanks to active set strategies.\nThe safe rules introduced by [9] for generalized `1 regularized problems, is a set of rules that allows to eliminate features whose associated coefficients are proved to be zero at the optimum. Relaxing the safe rule, one can obtain some more speed-up at the price of possible mistakes. Such heuristic strategies, called strong rules [19] reduce the computational cost using an active set strategy, but require difficult post-precessing to check for features possibly wrongly discarded. Another road to speed-up screening method has been the introduction of sequential safe rules [21, 23, 22]. The idea is to improve the screening thanks to the computations done for a previous regularization parameter. This scenario is particularly relevant in machine learning, where one computes solutions over a grid of regularization parameters, so as to select the best one (e.g., to perform cross-validation). Nevertheless, such strategies suffer from the same problem as strong rules, since relevant features can be wrongly disregarded: sequential rules usually rely on theoretical quantities that are not known by the solver, but only approximated. Especially, for such rules to work one needs the exact dual optimal solution from the previous regularization parameter.\nRecently, the introduction of safe dynamic rules [6, 5] has opened a promising venue by letting the screening to be done not only at the beginning of the algorithm, but all along the iterations. Following a\nar X\niv :1\n50 6.\n03 73\n6v 2\n[ st\nat .M\nL ]\n1 8\nN ov\nmethod introduced for the Lasso [11], we generalize this dynamical safe rule, called GAP Safe rules (because it relies on duality gap computation) to a large class of learning problems with the following benefits:\n\u2022 a unified and flexible framework for a wider family of problems, \u2022 easy to insert in existing solvers, \u2022 proved to be safe, \u2022 more efficient that previous safe rules, \u2022 achieves fast true active set identification.\nWe introduce our general GAP Safe framework in Section 2. We then specialize it to important machine learning use cases in Section 3. In Section 4 we apply our GAP Safe rules to a multi-task Lasso problem, relevant for brain imaging with magnetoencephalography data, as well as to multinomial logistic regression regularized with `1{`2 norm for joint feature selection."}, {"heading": "2 GAP Safe rules", "text": ""}, {"heading": "2.1 Model and notations", "text": "We denote by rds the set t1, . . . , du for any integer d P N, and by QJ the transpose of a matrix Q. Our observation matrix is Y P Rn\u02c6q where n represents the number of samples, and q the number of tasks or classes. The design matrix X \u201c rxp1q, . . . , xppqs \u201c rx1, . . . , xnsJ P Rn\u02c6p has p explanatory variables (or features) column-wise, and n observations row-wise. The standard `2 norm is written } \u00a8 }2, the `1 norm } \u00a8 }1, the `8 norm } \u00a8 }8. The `2 unit ball is denoted by B2 (or simply B) and we write Bpc, rq the `2 ball with center c and radius r. For a matrix B P Rp\u02c6q, we denote by }B}22 \u201c \u0159p j\u201c1 \u0159q k\u201c1 B2j,k the Frobenius norm, and by x\u00a8, \u00a8y the associated inner product. We consider the general optimization problem of minimizing a separable function with a group-Lasso regularization. The parameter to recover is a matrix B P Rp\u02c6q, and for any j in Rp,Bj,: is the j-th row of B, while for any k in Rq, B:,k is the k-th column. We would like to find\npBp\u03bbq P arg min BPRp\u02c6q\nn\u00ff i\u201c1 fipxJi Bq ` \u03bb\u2126pBq\nloooooooooooomoooooooooooon P\u03bbpBq\n, (1)\nwhere fi : R1\u02c6q \u00de\u00d1 R is a convex function with 1{\u03b3-Lipschitz gradient. So F : B \u00d1 \u0159ni\u201c1 fipxJi Bq is also convex with Lipschitz gradient. The function \u2126 : Rp\u02c6q \u00de\u00d1 R` is the `1{`2 norm \u2126pBq \u201c \u0159pj\u201c1 }Bj,:}2 promoting a few lines of B to be non-zero at a time. The \u03bb parameter is a non-negative constant controlling the trade-off between data fitting and regularization.\nSome elements of convex analysis used in the following are introduced here. For a convex function f : Rd \u00d1 r\u00b48,`8s the Fenchel-Legendre transform1 of f , is the function f\u02da : Rd \u00d1 r\u00b48,`8s defined by f\u02dapuq \u201c supzPRdxz, uy \u00b4 fpzq. The sub-differential of a function f at a point x is denoted by Bfpxq. The dual norm of \u2126 is the `8{`2 norm and reads \u2126\u02dapBq \u201c maxjPrps }Bj,:}2. Remark 1. For the ease of reading, all groups are weighted with equal strength, but extension of our results to non-equal weights as proposed in the original group-Lasso [24] paper would be straightforward."}, {"heading": "2.2 Basic properties", "text": "First we recall the associated Fermat\u2019s condition and a dual formulation of the optimization problem:\n1this is also often referred to as the (convex) conjugate of a function\nTheorem 1. Fermat\u2019s condition (see [3, Proposition 26.1] for a more general result) For any convex function f : Rn \u00d1 R:\nx P arg min xPRn fpxq \u00f4 0 P Bfpxq. (2)\nTheorem 2 ([9]). A dual formulation of (1) is given by\np\u0398p\u03bbq \u201c arg max \u0398P\u2206X\n\u00b4 n\u00ff\ni\u201c1 fi\u030a p\u00b4\u03bb\u0398i,:q\nlooooooooomooooooooon D\u03bbp\u0398q\n. (3)\nwhere \u2206X \u201c t\u0398 P Rn\u02c6q : @j P rps, }xpjqJ\u0398}2 \u010f 1u \u201c t\u0398 P Rn\u02c6q : \u2126\u02dapXJ\u0398q \u010f 1u. The primal and dual solutions are linked by @i P rns, p\u0398p\u03bbqi,: \u201c \u00b4\u2207fipxJi pBp\u03bbqq{\u03bb. (4) Furthermore, Fermat\u2019s condition reads:\n@j P rps, xpjqJp\u0398p\u03bbq P $ & % \" B\u0302\u03bbj,; }B\u0302\u03bbj,;}2 * , if pBp\u03bbqj,: \u2030 0,\nB2, if pBp\u03bbqj,: \u201c 0. (5)\nRemark 2. Contrarily to the primal, the dual problem has a unique solution under our assumption on fi. Indeed, the dual function is strongly concave, hence strictly concave. Remark 3. For any \u0398 P Rn\u02c6q let us introduce Gp\u0398q \u201c r\u2207f1p\u03981,:qJ, . . . ,\u2207fnp\u0398n,:qJs P Rn\u02c6q. Then the primal/dual link can be written p\u0398p\u03bbq \u201c \u00b4GpXpBp\u03bbqq{\u03bb ."}, {"heading": "2.3 Critical parameter: \u03bbmax", "text": "For \u03bb large enough the solution of the primal problem is simply 0. Thanks to the Fermat\u2019s rule (2), 0 is optimal if and only if \u00b4\u2207F p0q{\u03bb P B\u2126p0q. Thanks to the property of the dual norm \u2126\u02da, this is equivalent to \u2126\u02dap\u2207F p0q{\u03bbq \u010f 1 where \u2126\u02da is the dual norm of \u2126. Since \u2207F p0q \u201c XJGp0q, 0 is a primal solution of P\u03bb if and only if \u03bb \u011b \u03bbmax :\u201c maxjPrps }xpjqJGp0q}2 \u201c \u2126\u02dapXJGp0qq.\nThis development shows that for \u03bb \u011b \u03bbmax, Problem (1) is trivial. So from now on, we will only focus on the case where \u03bb \u010f \u03bbmax."}, {"heading": "2.4 Screening rules description", "text": "Safe screening rules rely on a simple consequence of the Fermat\u2019s condition: }xpjqJp\u0398p\u03bbq}2 \u0103 1 \u00f1 pBp\u03bbqj,: \u201c 0 . (6) Stated in such a way, this relation is useless because p\u0398p\u03bbq is unknown (unless \u03bb \u0105 \u03bbmax). However, it is often possible to construct a set R \u0102 Rn\u02c6q, called a safe region, containing it. Then, note that\nmax \u0398PR }x pjqJ\u0398}2 \u0103 1 \u00f1 pBp\u03bbqj,: \u201c 0 . (7) The so called safe screening rules consist in removing the variable j from the problem whenever the previous test is satisfied, since pBp\u03bbqj,: is then guaranteed to be zero. This property leads to considerable speed-up in practice especially with active sets strategies, see for instance [11] for the Lasso case. A natural goal is to find safe regions as narrow as possible: smaller safe regions can only increase the number of screened out variables. However, complex regions could lead to a computational burden limiting the benefit of screening. Hence, we focus on constructing R satisfying the trade-off: \u2022 R is as small as possible and contains p\u0398p\u03bbq. \u2022 Computing max\u0398PR }xpjqJ\u0398}2 is cheap."}, {"heading": "2.5 Spheres as safe regions", "text": "Various shapes have been considered in practice for the set R such as balls (referred to as spheres) [9], domes [11] or more refined sets (see [23] for a survey). Here we consider the so-called \u201csphere regions\u201d choosing a ball R \u201c Bpc, rq as a safe region. One can easily obtain a control on max\u0398PBpc,rq }xpjqJ\u0398}2 by extending the computation of the support function of a ball [11, Eq. (9)] to the matrix case: max\n\u0398PBpc,rq }xpjqJ\u0398}2 \u010f\n}xpjqJc}2 ` r}xpjq}2 . Note that here the center c is a matrix in Rp\u02c6q. We can now state the safe sphere test:\nSphere test: If }xpjqJc}2 ` r}xpjq}2 \u0103 1, then pBp\u03bbqj,: \u201c 0. (8)"}, {"heading": "2.6 GAP Safe rule description", "text": "In this section we derive a GAP Safe screening rule extending the one introduced in [11]. For this, we rely on the strong convexity of the dual objective function and on weak duality.\nFinding a radius: Remember that @i P rns, fi is differentiable with a 1{\u03b3-Lipschitz gradient. As a consequence, @i P rns, fi\u030a is \u03b3-strongly convex [14, Theorem 4.2.2, p. 83] and so D\u03bb is \u03b3\u03bb2-strongly concave:\n@p\u03981,\u03982q P Rn\u02c6q \u02c6 Rn\u02c6q, D\u03bbp\u03982q \u010f D\u03bbp\u03981q ` x\u2207D\u03bbp\u03981q,\u03982 \u00b4\u03981y \u00b4 \u03b3\u03bb 2\n2 }\u03981 \u00b4\u03982}2.\nSpecifying the previous inequality for \u03981 \u201c p\u0398p\u03bbq,\u03982 \u201c \u0398 P \u2206X , one has\nD\u03bbp\u0398q \u010f D\u03bbpp\u0398p\u03bbqq ` x\u2207D\u03bbpp\u0398p\u03bbqq,\u0398\u00b4 p\u0398p\u03bbqy \u00b4 \u03b3\u03bb 2\n2 }p\u0398p\u03bbq \u00b4\u0398}2.\nBy definition, p\u0398p\u03bbq maximizes D\u03bb on \u2206X , so we have: x\u2207D\u03bbpp\u0398p\u03bbqq,\u0398\u00b4 p\u0398p\u03bbqy \u010f 0. This implies\nD\u03bbp\u0398q \u010f D\u03bbpp\u0398p\u03bbqq \u00b4 \u03b3\u03bb 2\n2 }p\u0398p\u03bbq \u00b4\u0398}2."}, {"heading": "By weak duality @B P Rp\u02c6q, D\u03bbpp\u0398p\u03bbqq \u010f P\u03bbpBq, so : @B P Rp\u02c6q,@\u0398 P \u2206X , D\u03bbp\u0398q \u010f P\u03bbpBq\u00b4 \u03b3\u03bb22 }p\u0398p\u03bbq\u00b4\u0398}2,", "text": "and we deduce the following theorem:\nTheorem 3.\n@B P Rp\u02c6q,@\u0398 P \u2206X , \u2225\u2225\u2225p\u0398p\u03bbq \u00b4\u0398 \u2225\u2225\u2225 2 \u010f d 2pP\u03bbpBq \u00b4D\u03bbp\u0398qq \u03b3\u03bb2 \u201c: r\u0302\u03bbpB,\u0398q. (9)\nProvided one knows a dual feasible point \u0398 P \u2206X and a B P Rp\u02c6q , it is possible to construct a safe sphere with radius r\u0302\u03bbpB,\u0398q centered on \u0398. We now only need to build a (relevant) dual point to center such a ball. Results from Section 2.3, ensure that \u00b4Gp0q{\u03bbmax P \u2206X , but it leads to a static rule, a introduced in [9]. We need a dynamic center to improve the screening as the solver proceeds.\nFinding a center: Remember that p\u0398p\u03bbq \u201c \u00b4GpXpBp\u03bbqq{\u03bb. Now assume that one has a converging algorithm for the primal problem, i.e., Bk \u00d1 pBp\u03bbq. Hence, a natural choice for creating a dual feasible point \u0398k is to choose it proportional to \u00b4GpXBkq, for instance by setting:\n\u0398k \u201c # Rk \u03bb , if \u2126\u02dapXJRkq \u010f \u03bb,\nRk \u2126\u02dapXJRkq , otherwise.\nwhere Rk \u201c \u00b4GpXBkq . (10)\nA refined method consists in solving the one dimensional problem: arg max\u0398P\u2206XXSpanpRkqD\u03bbp\u0398q. In the Lasso and group-Lasso case [5, 6, 11] such a step is simply a projection on the intersection of a line and the (polytope) dual set and can be computed efficiently. However for logistic regression the computation is more involved, so we have opted for the simpler solution in Equation (10). This still provides converging safe rules (see Proposition 1).\nDynamic GAP Safe rule summarized\nWe can now state our dynamical GAP Safe rule at the k-th step of an iterative solver:\n1. Compute Bk, and then obtain \u0398k and r\u0302\u03bbpBk,\u0398kq using (10). 2. If }xpjqJ\u0398k}2 ` r\u0302\u03bbpBk,\u0398kq}xpjq}2 \u0103 1, then set pBp\u03bbqj,: \u201c 0 and remove xpjq from X. Dynamic safe screening rules are more efficient than existing methods in practice because they can increase the ability of screening as the algorithm proceeds. Since one has sharper and sharper dual regions available along the iterations, support identification is improved. Provided one relies on a primal converging algorithm, one can show that the dual sequence we propose is converging too.\nThe convergence of the primal is unaltered by our GAP Safe rule: screening out unnecessary coefficients of Bk can only decrease its distance with its original limits. Moreover, a practical consequence is that one can observe surprising situations where lowering the tolerance of the solver can reduce the computation time. This can happen for sequential setups.\nProposition 1. Let Bk be the current estimate of pBp\u03bbq and \u0398k defined in Eq. (10) be the current estimate of p\u0398p\u03bbq. Then limk\u00d1`8 Bk \u201c pBp\u03bbq implies limk\u00d1`8\u0398k \u201c p\u0398p\u03bbq.\nNote that if the primal sequence is converging to the optimal, our dual sequence is also converging. But we know that the radius of our safe sphere is p2pP\u03bbpBkq \u00b4D\u03bbp\u0398kqq{p\u03b3\u03bb2qq1{2. By strong duality, this radius converges to 0, hence we have certified that our GAP Safe regions sequence Bp\u0398k, r\u0302\u03bbpBk,\u0398kqq is a converging safe rules (in the sense introduced in [11, Definition 1]).\nRemark 4. The active set obtained by our GAP Safe rule (i.e., the indexes of non screened-out variables) converges to the equicorrelation set [20] E\u03bb :\u201c tj P p : }xpjqJp\u0398p\u03bbq}2 \u201c 1u, allowing us to early identify relevant features (see Proposition 2 in the supplementary material for more details)."}, {"heading": "3 Special cases of interest", "text": "We now specialize our results to relevant supervised learning problems, see also Table 1."}, {"heading": "3.1 Lasso", "text": "In the Lasso case q \u201c 1, the parameter is a vector: B \u201c \u03b2 P Rp, F p\u03b2q \u201c 1{2}y \u00b4X\u03b2}22 \u201c \u0159n i\u201c1pyi \u00b4 xJi \u03b2q2, meaning that fipzq \u201c pyi \u00b4 zq2{2 and \u2126p\u03b2q \u201c }\u03b2}1.\n3.2 `1{`2 multi-task regression In the multi-task Lasso, which is a special case of group-Lasso, we assume that the observation is Y P Rn\u02c6q, F pBq \u201c 12}Y \u00b4XB}22 \u201c 12 \u0159n i\u201c1 }Yi,: \u00b4 xJi B}22 (i.e., fipzq \u201c }Yi,: \u00b4 z}2{2) and \u2126pBq \u201c \u0159p j\u201c1 }Bj,:}2. In signal processing, this model is also referred to as Multiple Measurement Vector (MMV) problem. It allows to jointly select the same features for multiple regression tasks [1, 2].\nRemark 5. Our framework could encompass easily the case of non-overlapping groups with various size and weights presented in [6]. Since our aim is mostly for multi-task and multinomial applications, we have rather presented a matrix formulation."}, {"heading": "3.3 `1 regularized logistic regression", "text": "Here, we consider the formulation given in [7, Chapter 3] for the two classes logistic regression. In such a context, one observes for each i P rns a class label ci P t1, 2u. This information can be recast as yi \u201c 1tci\u201c1u, and it is then customary to minimize (1) where\nF p\u03b2q \u201c n\u00ff\ni\u201c1\n`\u00b4yixJi \u03b2 ` log ` 1` exp `xJi \u03b2 \u02d8\u02d8\u02d8 , (11)\nwith B \u201c \u03b2 P Rp (i.e., q \u201c 1), fipzq \u201c \u00b4yiz ` logp1 ` exppzqq and the penalty is simply the `1 norm: \u2126p\u03b2q \u201c }\u03b2}1. Let us introduce Nh, the (binary) negative entropy function defined by 2:\nNhpxq \u201c # x logpxq ` p1\u00b4 xq logp1\u00b4 xq, if x P r0, 1s , `8, otherwise . (12)\nThen, one can easily check that fi\u030a pziq \u201c Nhpzi ` yiq and \u03b3 \u201c 4.\n3.4 `1{`2 multinomial logistic regression We adapt the formulation given in [7, Chapter 3] for the multinomial regression. In such a context, one observes for each i P rns a class label ci P t1, . . . , qu. This information can be recast into a matrix Y P Rn\u02c6q filled by 0\u2019s and 1\u2019s: Yi,k \u201c 1tci\u201cku. In the same spirit as the multi-task Lasso, a matrix B P Rp\u02c6q is formed by q vectors encoding the hyperplanes for the linear classification. The multinomial `1{`2 regularized regression reads:\nF pBq \u201c n\u00ff\ni\u201c1\n\u02dc q\u00ff\nk\u201c1 \u00b4Yi,kxJi B:,k ` log\n\u02dc q\u00ff\nk\u201c1 exp\n` xJi B:,k\n\u02d8 \u00b8\u00b8\n, (13)\nwith fipzq \u201c \u0159qk\u201c1\u00b4Yi,kzk ` log p \u0159q k\u201c1 exp pzkqq to recover the formulation as in (1). Let us introduce NH, the negative entropy function defined by (still with the convention 0 logp0q \u201c 0)\nNHpxq \u201c #\u0159q i\u201c1 xi logpxiq, if x P \u03a3q \u201c tx P Rq` : \u0159q i\u201c1 xi \u201c 1u,\n`8, otherwise. (14)\nAgain, one can easily check that fi\u030a pzq \u201c NHpz ` Yi,:q and \u03b3 \u201c 1. Remark 6. For multinomial logistic regression, D\u03bb implicitly encodes the additional constraint \u0398 P domD\u03bb \u201c t\u03981 : @i P rns,\u00b4\u03bb\u03981i,: ` Yi,: P \u03a3qu where \u03a3q is the q dimensional simplex, see (14). As 0 and Rk{\u03bb both belong to this set, any convex combination of them, such as \u0398k defined in (10), satisfies this additional constraint.\nRemark 7. The intercept has been neglected in our models for simplicity. Our GAP Safe framework can also handle such a feature at the cost of more technical details (by adapting the results from [15] for instance). However, in practice, the intercept can be handled in the present formulation by adding a constant column to the design matrix X. The intercept is then regularized. However, if the constant is set high enough, regularization is small and experiments show that it has little to no impact for high-dimensional problems. This is the strategy used by the Liblinear package [10].\n2with the convention 0 logp0q \u201c 0"}, {"heading": "4 Experiments", "text": "In this section we present results obtained with the GAP Safe rule. Results are on high dimensional data, both dense and sparse. Implementation have been done in Python and Cython for low critical parts. They are based on the multi-task Lasso implementation of Scikit-Learn [17] and coordinate descent logistic regression solver in the Lightning software [4]. In all experiments, the coordinate descent algorithm used follows the pseudo code from [11] with a screening step every 10 iterations.\nNote that we have not performed comparison with the sequential screening rule commonly acknowledge as the state-of-the-art \u201csafe\u201d screening rule (such as th EDDP+ [21]), since we can show that this kind of rule is not safe. Indeed, the stopping criterion is based on dual gap accuracy, and comparisons would be unfair since such methods sometimes do not converge to the prescribed accuracy. This is backed-up by a counter example given in the supplementary material. Nevertheless, modifications of such rules, inspired by our GAP Safe rules, can make them safe. However the obtained sequential rules are still outperformed by our dynamic strategies (see Figure 2 for an illustration).\n4.1 `1{`2 multi-task regression To demonstrate the benefit of the GAP Safe screening rule for a multi-task Lasso problem we used neuroimaging data. Electroencephalography (EEG) and magnetoencephalography (MEG) are brain imaging modalities that allow to identify active brain regions. The problem to solve is a multi-task regression problem with squared loss where every task corresponds to a time instant. Using a multi-task Lasso one can constrain the recovered sources to be identical during a short time interval [13]. This corresponds to a temporal stationary assumption. In this experiment we used a joint MEG/EEG data with 301 MEG and 59 EEG sensors leading to n \u201c 360. The number of possible sources is p \u201c 22, 494 and the number of time instants q \u201c 20. With a 1 kHz sampling rate it is equivalent to say that the sources stay the same for 20 ms.\nResults are presented in Figure 1. The GAP Safe rule is compared with the dynamic safe rule from [6]. The experimental setup consists in estimating the solutions of the multi-task Lasso problem for 100 values of \u03bb on a logarithmic grid from \u03bbmax to \u03bbmax{103. For the experiments on the left a fixed number of iterations from 2 to 211 is allowed for each \u03bb. The fraction of active variables is reported. Figure 1 illustrates that the GAP Safe rule screens out much more variables than the compared method, as well as the converging nature of our safe regions. Indeed, the more iterations performed the more the rule allows to screen variables. On\nthe right, computation time confirms the effective speed-up. Our rule significantly improves the computation time for all duality gap tolerance from 10\u00b42 to 10\u00b48, especially when accurate estimates are required, e.g., for feature selection."}, {"heading": "4.2 `1 binary logistic regression", "text": "Results on the Leukemia dataset are reported in Figure 2. We compare the dynamic strategy of GAP Safe to a sequential and non dynamic rule such as Slores [22]. We do not compare to the actual Slores rule as it requires the previous dual optimal solution, which is not available. Slores is indeed not a safe method (see Section B in the supplementary materials). Nevertheless one can observe that dynamic strategies outperform pure sequential one, see Section C in the supplementary material).\n4.3 `1{`2 multinomial logistic regression We also applied GAP Safe to an `1{`2 multinomial logistic regression problem on a sparse dataset. Data are bag of words features extracted from the News20 dataset (TF-IDF removing English stop words and words occurring only once or more than 95% of the time). One can observe on Figure 3 the dynamic screening and its benefit as more iterations are performed. GAP Safe leads to a significant speedup: to get a duality gap smaller than 10\u00b42 on the 100 values of \u03bb, we needed 1,353 s without screening and only 485 s when GAP Safe was activated."}, {"heading": "5 Conclusion", "text": "This contribution detailed new safe rules for accelerating algorithms solving generalized linear models regularized with `1 and `1{`2 norms. The rules proposed are safe, easy to implement, dynamic and converging, allowing to discard significantly more variables than alternative safe rules. The positive impact in terms of computation time was observed on all tested datasets and demonstrated here on a high dimensional regression task using brain imaging data as well as binary and multiclass classification problems on dense and sparse data. Extensions to other generalized linear model, e.g., Poisson regression, are expected to reach the same conclusion. Future work could investigate optimal screening frequency, determining when the screening has correctly detected the support."}, {"heading": "Acknowledgment", "text": "We acknowledge the support from Chair Machine Learning for Big Data at Te\u0301le\u0301com ParisTech and from the Orange/Te\u0301le\u0301com ParisTech think tank phi-TAB. This work benefited from the support of the \u201dFMJH Program Gaspard Monge in optimization and operation research\u201d, and from the support to this program from EDF."}, {"heading": "A Proofs", "text": "A.1 Proof of variable identification\nProposition 2. There exists k0 P N such that for all k \u011b k0, an index j P rps is screened out by the GAP Safe rule if and only if j P E\u03bb :\u201c tj P p : }xpjqJp\u0398p\u03bbq}2 \u201c 1u. Proof. For simplicity we use the notation Rk \u201c Bp\u0398k, r\u0302\u03bbpBk,\u0398kqq for the safe region at step k. Define maxjRE\u03bb |xpjqJp\u0398p\u03bbq| \u201c t \u0103 1. Fix \u0105 0 such that \u0103 p1\u00b4 tq{pmaxjRE\u03bb }xpjq}q. As \u0398k is converging to p\u0398p\u03bbq, and limk\u00d18 r\u0302\u03bbpBk,\u0398kq \u201c 0, there exists k0 P N such that @k \u011b k0,@\u0398 P Rk, }\u0398 \u00b4 p\u0398p\u03bbq} \u010f . Hence, for any j R E\u03bb and any \u0398 P Rk, |xpjqJp\u0398\u00b4 p\u0398p\u03bbqq| \u010f pmaxjRE\u03bb }xpjq}q}\u0398\u00b4 p\u0398p\u03bbq} \u010f pmaxjRE\u03bb }xpjq}q . Using the triangle inequality, one gets\n|xpjqJ\u0398| \u010fpmax jRE\u03bb }xpjq}q `max jRE\u03bb |xpjqJp\u0398p\u03bbq| \u010fpmax\njRE\u03bb }xpjq}q ` t \u0103 1,\nprovided that \u0103 p1\u00b4 tq{pmaxjRE\u03bb }xpjq}q. Hence, for all k \u011b k0, j R E\u03bb implies that j is screened out by the GAP Safe rule thanks to the last inequality. For the reverse inclusion take j P E\u03bb, i.e., |xpjqJp\u0398p\u03bbq| \u201c 1. Since by construction of our GAP Safe screening rule @k P N, p\u0398p\u03bbq P Rk, then j P tj1 P rps : max\u0398PRk |xpj\n1qJ\u0398| \u011b 1u. This means that the variable j can not be eliminated by our safe rule, and we have shown that in the limit we have exactly identified the equicorrelation set.\nA.2 Proof that the GAP Safe rule is converging (Proposition 1)\nProof. We consider two cases. First let us assume that \u03b8k \u201c Rk{\u2126\u02dapXJGpXBkqq\n\u2225\u2225\u2225\u0398k \u00b4 p\u0398p\u03bbq \u2225\u2225\u2225\n2 \u201c\n\u2225\u2225\u2225\u2225 \u00b4GpXBkq\n\u2126\u02dapXJGpXBkqq ` 1 \u03bb GpXpBp\u03bbqq \u2225\u2225\u2225\u2225 2\n\u010f \u2225\u2225\u2225\u2225 GpXBkq \u03bb \u00b4 GpXBkq \u2126\u02dapXJGpXBkqq \u2225\u2225\u2225\u2225\n2\n` \u2225\u2225\u2225\u2225\u2225 GpXpBp\u03bbqq \u00b4GpXBkq \u03bb \u2225\u2225\u2225\u2225\u2225 2\n\u010f \u02c7\u030c \u02c7\u030c 1 \u03bb \u00b4 1 \u2126\u02dapXJGpXBkqq \u02c7\u030c \u02c7\u030c \u2016GpXBkq\u20162 ` \u2225\u2225\u2225\u2225\u2225 GpXpBp\u03bbqq \u00b4GpXBkq \u03bb \u2225\u2225\u2225\u2225\u2225 2\nThe second term converges to zero whenever Bk \u00d1 pBp\u03bbq since G is continuous (it is \u03b3-Lipschitz). For the first term, note that \u2126\u02dapXJGpXBkqq \u00d1 \u2126\u02dapXJGpXpBp\u03bbqqq \u201c \u03bb\u2126\u02dapXJp\u0398p\u03bbqq \u201c \u03bb (thanks to the primal/dual link, and that p\u0398p\u03bbq is dual feasible). Then, as G is a Lipschitz function and all norms are equivalent in a finite dimension space, the right hand side converges to zero in the previous inequality, and the results stated follows.\nIn the second case \u0398k \u201c Rk{\u03bb, so \u2225\u2225\u2225\u0398k \u00b4 p\u0398p\u03bbq \u2225\u2225\u2225 2 \u201c \u2225\u2225\u2225\u00b4GpXBkq`GpX pB p\u03bbqq \u03bb \u2225\u2225\u2225 2 and the proof proceeds as in\nthe first case."}, {"heading": "B EDPP is not safe", "text": "In the two last sections, we present a study on the EDDP method [21], a screening rule that relies on the dual optimal point obtained for the previous \u03bb in the path. Note that the same conclusion would hold true for generalization of the sequential approach given in [22], as well as for any other screening rule that needs exact dual solution at one step. To simplify the reading we use the vectorial (with no capital letters) notation used earlier. In the remainder we consider \u03bb0 \u201c \u03bbmax and a non-increasing sequence of T \u00b41 tuning parameters p\u03bbtqtPrT\u00b41s in p0, \u03bbmaxq. In practice, we choose the common grid [7][2.12.1]): \u03bbt \u201c \u03bb010\u00b4\u03b4t{pT\u00b41q. Wang et al. [21] proposed a sequential screening rule based on properties of the projection onto a convex set. Their rule is based on the exact knowledge of the true optimal solution for the previous parameter. Such a rule can be used to compute \u03b8\u0302p\u03bb1q since \u03b8\u0302p\u03bb0q \u201c y{\u03bb0 p\u201c y{\u03bbmaxq is known. However for t \u0105 1, \u03b8\u0302p\u03bbtq is only known approximately and the rules introduced in [21] are not safe anymore: some active groups may be wrongly disregarded if one does not use the exact value of \u03b8\u0302p\u03bbtq. We first first recall the property they proved. Then, we give a counter-example that shows that the rule is indeed not safe. In Section C, we propose to modify their rule in order to make it safe in all cases. Recall that in this case q \u201c 1, the parameters are vectors: B \u201c \u03b2 P Rp and \u0398 \u201c \u03b8 P Rn.\nProposition 3 ([21, Theorem 19]). Assume that \u03bbt\u00b41 \u0103 \u03bbmax, then the dual optimal solution of the groupLasso with parameter \u03bbt, satisfies\n\u03b8\u0302p\u03bbtq P B`\u03b8\u0302p\u03bbt\u00b41q ` 1 2 vKp\u03bbt\u00b41, \u03bbtq, 1 2 \u2225\u2225vKp\u03bbt\u00b41, \u03bbtq \u2225\u2225 2 \u02d8 (15)\nwhere\nvKp\u03bbt\u00b41, \u03bbtq \u201c y \u03bbt \u00b4 \u03b8\u0302p\u03bbt\u00b41q \u00b4 \u03b1r\u03b8\u0302p\u03bbt\u00b41qsp y \u03bbt\u00b41 \u00b4 \u03b8\u0302p\u03bbt\u00b41qq\nand\n\u03b1r\u03b8\u0302p\u03bbt\u00b41qs :\u201c arg min \u03b1PR`\n\u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b8\u0302p\u03bbt\u00b41q \u00b4 \u03b1p y \u03bbt\u00b41 \u00b4 \u03b8\u0302p\u03bbt\u00b41qq \u2225\u2225\u2225\u2225 2\n\u201c x y\u03bbt\u00b41 \u00b4 \u03b8\u0302p\u03bbt\u00b41q, y\u03bbt \u00b4 \u03b8\u0302p\u03bbt\u00b41qy\n\u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u0302p\u03bbt\u00b41q\u201622 . (16)\nNote that the rule proposed by [21] (as pointed out in [6]) relies on the exact knowledge of a dual optimal solution for a previously solved Lasso problem. This is impossible to obtain in practice and even if it is possible to find accurate solutions, the search for high accuracy may hinder the benefits of the screening when it was not actually needed. Using inaccurate solutions may lead to discarding variables that should have been active and so the screened optimization algorithm will not converge to a solution of the original problem.\nWe illustrate this issue on Figure 4. Knowing an approximation \u03b2 to the optimal primal point, returned by the optimization algorithm at the previous regularization parameter \u03bbt\u00b41, we need to choose an approximation \u03b8 to the optimal dual point to run EDPP.\n\u2022 If we choose to approximate the dual optimal point by \u03b8 \u201c 1\u03bbt\u00b41 py\u00b4X\u03b2q (blue curve with diamonds), then the result is catastrophic. Indeed, at \u03bb1, \u03b2 \u201c 0 is a valid -solution for \u201c 10\u00b41.5 and the screening rule tries to perform a division by 0 when computing \u03b1r\u03b8s.\n\u2022 If we choose to approximate the dual optimal point by 1maxp\u03bbt\u00b41,\u2016XJpy\u00b4X\u03b2q\u20168q py\u00b4X\u03b2q, we have a better behavior (purple curve with triangles) but we may still have an algorithm which does not converge to an -solution. Here, for the 13th Lasso problem a variable is erroneously removed and the problem can only be solved to accuracy 0.03515 \u0105 10\u00b41.5 \u00ab 0.03162. This may look like a small issue but when the stopping criterion is based on the duality gap, this causes the algorithm to continue until the maximum number of iterations is reached."}, {"heading": "C Making EDDP screening rule safe", "text": "C.1 The simpler screening rule\nIn the present paper, we give computable guarantees on the distance between the current dual feasible point and the solution of the problem. We show here how we can combine our result with Wang et al. \u2019s in order to make their screening rule work even with approximate solutions to the previous Lasso problem.\nFor simplicity, we first consider the initial version of Wang et al. \u2019s sphere test:\n\u03b8\u0302p\u03bbtq P B`\u03b8\u0302p\u03bbt\u00b41q,\u2225\u2225vKp\u03bbt\u00b41, \u03bbtq \u2225\u2225\n2\n\u02d8 , (17)\nproved in [21, Theorem 7]. As we do not know \u03b8\u0302p\u03bbt\u00b41q, we cannot readily use this ball. However, we can modify it to make it a safe screening rules as follows:\nProposition 4. Assume that \u03bbt\u00b41 \u0103 \u03bbmax, \u03b8 P \u2206X is a dual feasible point and r\u03bbt\u00b41 \u0105 0 is a radius satisfying \u03b8\u0302p\u03bbt\u00b41q P Bp\u03b8, r\u03bbt\u00b41q, then\n\u03b8\u0302p\u03bbtq P B \u00b4 \u03b8, r\u03bbt\u00b41p1` |1\u00b4 \u03b1r\u03b8s|q ` \u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b8 \u00b4 \u03b1r\u03b8sp y \u03bbt\u00b41 \u00b4 \u03b8q \u2225\u2225\u2225\u2225 2 \u00af , (18)\nwhere\n\u03b1r\u03b8s :\u201c arg min \u03b1PR`\n\u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b8 \u00b4 \u03b1p y \u03bbt\u00b41 \u00b4 \u03b8q \u2225\u2225\u2225\u2225 2 \u201c \u02dcx y\u03bbt\u00b41 \u00b4 \u03b8, y\u03bbt \u00b4 \u03b8y \u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622 \u00b8 ` , (19)\nand for any t P R, ptq` \u201c maxp0, tq. Proof. Start first by noting that (17) implies\n\u03b8\u0302p\u03bbtq P \u010f \u03b81PBp\u03b8,r\u03bbt\u00b41 q B \u00b4 \u03b81, min \u03b1PR` \u2225\u2225\u2225\u2225 y \u03bbt \u00b4 \u03b81 \u00b4 \u03b1p y \u03bbt\u00b41 \u00b4 \u03b81q \u2225\u2225\u2225\u2225 2 \u00af .\nLet us denote\nH \u201c max \u03b81PBp\u03b8,r\u03bbt\u00b41 q min \u03b1PR`\n\u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b81 \u00b4 \u03b1p y \u03bbt\u00b41 \u00b4 \u03b81q \u2225\u2225\u2225\u2225 2 ,\nthen \u03b8\u0302p\u03bbtq P Bp\u03b8, r\u03bbt\u00b41 ` Hq. We now need to upper bound H. A simple choice is to take \u03b1 to be \u03b1r\u03b8s defined in Eq. (19) The motivation for such a choice is because it is optimal when r\u03bbt\u00b41 \u201c 0. This provides the following bound on H:\nH \u010f max \u03b81PBp\u03b8,r\u03bbt\u00b41 q\n\u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b81 \u00b4 \u03b1r\u03b8sp y \u03bbt\u00b41 \u00b4 \u03b81q \u2225\u2225\u2225\u2225 2 ,\n\u201c \u2225\u2225\u2225\u2225\u2225\u2225 y \u03bbt \u00b4 \u03b8 \u00b4 \u03b1r\u03b8sp y \u03bbt\u00b41 \u00b4 \u03b8q ` r\u03bbt\u00b41p\u03b1r\u03b8s \u00b4 1q y \u03bbt \u00b4 \u03b8 \u00b4 \u03b1r\u03b8sp y\u03bbt\u00b41 \u00b4 \u03b8q\u2225\u2225\u2225 y\u03bbt \u00b4 \u03b8 \u00b4 \u03b1r\u03b8sp y\u03bbt\u00b41 \u00b4 \u03b8q \u2225\u2225\u2225 \u2225\u2225\u2225\u2225\u2225\u2225 2 , \u010f r\u03bbt\u00b41 |\u03b1r\u03b8s \u00b4 1| ` \u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b8 \u00b4 \u03b1r\u03b8s.p y \u03bbt\u00b41 \u00b4 \u03b8q\n\u2225\u2225\u2225\u2225 . (20)\nHence, after some simplifications:\n\u03b8\u0302p\u03bbtq P B \u00b4 \u03b8, r\u03bbt\u00b41p1` |1\u00b4 \u03b1r\u03b8s|q ` \u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b8 \u00b4 \u03b1r\u03b8sp y \u03bbt\u00b41 \u00b4 \u03b8q \u2225\u2225\u2225\u2225 2 \u00af .\nRemark 8. In the case that \u2016y{\u03bbt\u00b41\u2016 \u010f \u2016y{\u03bbt\u00b41 \u00b4 \u03b8\u2016 \u010f 1 then with the definition of \u03b1r\u03b8s and the CauchySchwartz inequality one has that 1` |\u03b1r\u03b8s \u00b4 1| \u010f \u03bbt\u00b41\u03bbt . This means that the multiplicative ratio in front of r\u03bbt\u00b41 is \u03bbt\u00b41{\u03bbt. In [11, Proposition 3], the bound obtained would only lead to the smaller ratio: a \u03bbt\u00b41{\u03bbt. Remark 9. From the proof of Theorem 7 in [21], it holds that for \u03bb \u0103 \u03bbmax then \u2225\u2225\u2225\u03b8\u0302p\u03bbq\n\u2225\u2225\u2225 \u010f \u2016y\u2016 \u03bb \u00f4 \u03b8\u0302p\u03bbq P B \u02c6 0, \u2016y\u2016 \u03bb \u02d9 . (21)\nC.2 The complete screening rule (EDDP+) Let us now consider the EDDP+ screening rule [21] relying on the property (15): \u03b8\u0302p\u03bbtq P B`\u03b8\u0302p\u03bbt\u00b41q ` 1 2v Kp\u03bbt\u00b41, \u03bbtq, 12 \u2225\u2225vKp\u03bbt\u00b41, \u03bbtq \u2225\u2225 2 \u02d8 . Using the same technique as for Proposition 4, we can strengthen our previous proposition with the following result.\nProposition 5. Assume that \u03bbt\u00b41 \u0103 \u03bbmax, \u03b8 P \u2206X is a dual feasible point and r\u03bbt\u00b41 \u0105 0 is a radius satisfying \u03b8\u0302p\u03bbt\u00b41q P Bp\u03b8, r\u03bbt\u00b41q. Define \u03b1r\u03b8s as in (19),\nr\u03bbt \u201c |1\u00b4 \u03b1r\u03b8s|` 1` \u03b1r\u03b8s\n2 r\u03bbt\u00b41 `\n1\n2\n\u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b8 \u00b4 \u03b1r\u03b8sp y \u03bbt\u00b41 \u00b4 \u03b8q \u2225\u2225\u2225\u2225 2\n` \u2225\u2225\u2225 y\u03bbt \u00b4 y\u03bbt\u00b41 \u2225\u2225\u2225 2 r\u03bbt\u00b41\n2\u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622 \u00b4 3 \u2225\u2225\u2225\u2225 y \u03bbt\u00b41 \u00b4 \u03b8 \u2225\u2225\u2225\u2225 2 ` 2r\u03bbt\u00b41 \u00af\nand vKp\u03b8, \u03bbt\u00b41, \u03bbtq \u201c y\n\u03bbt \u00b4 \u03b8 \u00b4 \u03b1r\u03b8sp y \u03bbt\u00b41 \u00b4 \u03b8q. (22)\nThen \u03b8\u0302p\u03bbtq P B \u00b4 \u03b8 ` 12vKp\u03b8, \u03bbt\u00b41, \u03bbtq, r\u03bbt \u00af .\nProof. As before, we do not know exactly \u03b8\u0302p\u03bbt\u00b41q but we know that denoting\nvKp\u03b81, \u03bbt\u00b41, \u03bbtq \u201c y \u03bbt \u00b4 \u03b81 \u00b4 \u03b1r\u03b81sp y \u03bbt\u00b41 \u00b4 \u03b81q (23)\nwith\n\u03b1r\u03b81s \u201c \u02dcx y\u03bbt\u00b41 \u00b4 \u03b81, y\u03bbt \u00b4 \u03b81y\n\u2016 y\u03bbt\u00b41 \u00b4 \u03b81\u201622\n\u00b8\n` , (24)\nwe have\n\u03b8\u0302p\u03bbtq P \u010f \u03b81PBp\u03b8,r\u03bbt\u00b41 q B \u00b4 \u03b81 ` 1 2 vKp\u03b81, \u03bbt\u00b41, \u03bbtq, 1 2 \u2225\u2225vKp\u03b81, \u03bbt\u00b41, \u03bbtq \u2225\u2225 2 \u00af .\nOur goal is to find a ball centered at \u03b8` 12vKp\u03b8, \u03bbt\u00b41, \u03bbtq that contains all these balls, thus containing \u03b8\u0302p\u03bbtq. First, reminding (20)\n\u2225\u2225vKp\u03b81, \u03bbt\u00b41, \u03bbtq \u2225\u2225\n2 \u201c min \u03b1PR`\n\u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b81 \u00b4 \u03b1p y \u03bbt\u00b41 \u00b4 \u03b81q \u2225\u2225\u2225\u2225 2\n\u010f max \u03b81PBp\u03b8,r\u03bbt\u00b41 q min \u03b1PR`\n\u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b81 \u00b4 \u03b1p y \u03bbt\u00b41 \u00b4 \u03b81q \u2225\u2225\u2225\u2225 2\n\u010f r\u03bbt\u00b41 |1\u00b4 \u03b1r\u03b8s|` \u2225\u2225\u2225\u2225 y\n\u03bbt \u00b4 \u03b8 \u00b4 \u03b1r\u03b8sp y \u03bbt\u00b41 \u00b4 \u03b8q \u2225\u2225\u2225\u2225 2 .\nWe continue as\n\u03b81 ` 1 2 vKp\u03b81,\u03bbt\u00b41, \u03bbtq \u00b4 \u03b8 \u00b4 1 2 vKp\u03b8, \u03bbt\u00b41, \u03bbtq\n\u201c p\u03b81 \u00b4 \u03b8q ` 1 2 \u00b4 y \u03bbt \u00b4 \u03b81 \u00b4 \u03b1r\u03b81sp y \u03bbt\u00b41 \u00b4 \u03b81q \u00b4 y \u03bbt ` \u03b8 ` \u03b1r\u03b8sp y \u03bbt\u00b41 \u00b4 \u03b8q \u00af \u201c 1 2 \u00b4 \u03b81 \u00b4 \u03b8 \u00b4 p\u03b1r\u03b81s \u00b4 \u03b1r\u03b8sqp y \u03bbt\u00b41 \u00b4 \u03b81q ` \u03b1r\u03b8sp\u03b81 \u00b4 \u03b8q \u00af .\nTaking the norm on both sides of the previous display,\n\u2225\u2225\u2225\u2225\u03b81 ` 1 2 vKp\u03b81, \u03bbt\u00b41, \u03bbtq \u00b4 \u03b8 \u00b4 1 2 vKp\u03b8, \u03bbt\u00b41, \u03bbtq \u2225\u2225\u2225\u2225 2 \u010f 1` \u03b1r\u03b8s 2 \u2225\u2225\u03b81 \u00b4 \u03b8\u2225\u2225 2 ` |\u03b1r\u03b8 1s \u00b4 \u03b1r\u03b8s| 2 \u2225\u2225\u2225\u2225 y \u03bbt\u00b41 \u00b4 \u03b81 \u2225\u2225\u2225\u2225 2 .\nNow, reminding that x \u00de\u00d1 pxq` is a 1-Lipschitz function, \u2223\u2223\u03b1r\u03b81s \u00b4 \u03b1r\u03b8s\u2223\u2223 \u010f\n\u2223\u2223\u2223\u2223\u2223 x y\u03bbt\u00b41 \u00b4 \u03b81, y\u03bbt \u00b4 \u03b81y \u2016 y\u03bbt\u00b41 \u00b4 \u03b81\u201622 \u00b4 x y\u03bbt\u00b41 \u00b4 \u03b8, y\u03bbt \u00b4 \u03b8y \u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622 \u2223\u2223\u2223\u2223\u2223\n\u201c \u2223\u2223\u2223\u2223\u2223 x y\u03bbt\u00b41 \u00b4 \u03b81, y\u03bbt \u00b4 y\u03bbt\u00b41 y \u2016 y\u03bbt\u00b41 \u00b4 \u03b81\u201622 ` 1\u00b4 x y\u03bbt\u00b41 \u00b4 \u03b8, y\u03bbt \u00b4 y\u03bbt\u00b41 y \u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622 \u00b4 1 \u2223\u2223\u2223\u2223\u2223 \u201c \u2223\u2223\u2223\u2223\u2223 x\u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622p y\u03bbt\u00b41 \u00b4 \u03b81q \u00b4 \u2016 y\u03bbt\u00b41 \u00b4 \u03b81\u201622p y\u03bbt\u00b41 \u00b4 \u03b8q, y\u03bbt \u00b4 y\u03bbt\u00b41 y\n\u2016 y\u03bbt\u00b41 \u00b4 \u03b81\u201622\u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622\n\u2223\u2223\u2223\u2223\u2223\n\u010f \u2225\u2225\u2225 y\u03bbt \u00b4 y\u03bbt\u00b41 \u2225\u2225\u2225 2\n\u2016 y\u03bbt\u00b41 \u00b4 \u03b81\u201622\u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622 \u00b4 \u2016 y \u03bbt\u00b41 \u00b4 \u03b81\u20162 \u2223\u2223\u2223\u2223\u2016 y \u03bbt\u00b41 \u00b4 \u03b8\u201622 \u00b4 \u2016 y \u03bbt\u00b41 \u00b4 \u03b81\u201622 \u2223\u2223\u2223\u2223` \u2225\u2225\u03b8 \u00b4 \u03b81\u2225\u2225 2 \u2016 y \u03bbt\u00b41 \u00b4 \u03b81\u201622 \u00af\n\u010f \u2225\u2225\u2225 y\u03bbt \u00b4 y\u03bbt\u00b41 \u2225\u2225\u2225 2\n\u2016 y\u03bbt\u00b41 \u00b4 \u03b81\u20162\u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622 \u00b4 2\u2016 y \u03bbt\u00b41 \u00b4 \u03b8 1 ` \u03b8 2 \u20162\u2016\u03b8 \u00b4 \u03b81\u20162 ` \u2225\u2225\u03b8 \u00b4 \u03b81\u2225\u2225 2 \u2016 y \u03bbt\u00b41 \u00b4 \u03b81\u20162 \u00af\n\u010f \u2225\u2225\u2225 y\u03bbt \u00b4 y\u03bbt\u00b41 \u2225\u2225\u2225 2 \u2016\u03b8 \u00b4 \u03b81\u20162\n\u2016 y\u03bbt\u00b41 \u00b4 \u03b81\u20162\u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622 \u00b4 2\u2016 y \u03bbt\u00b41 \u00b4 \u03b8\u20162 ` \u2225\u2225\u03b8 \u00b4 \u03b81\u2225\u2225 2 ` \u2016 y \u03bbt\u00b41 \u00b4 \u03b8\u20162 ` \u2225\u2225\u03b8 \u00b4 \u03b81\u2225\u2225 2 \u00af . (25)\nwhere the second inequality comes from the triangle inequality and the Cauchy-Schwartz Inequality, and the\nthird is obtained by factorizing the difference of squares. Plugging this in the former, we get:\n\u2225\u2225\u2225\u03b81 ` 1 2 vKp\u03b81,\u03bbt\u00b41, \u03bbtq \u00b4 \u03b8 \u00b4 1 2 vKp\u03b8, \u03bbt\u00b41, \u03bbtq \u2225\u2225\u2225 2\n\u010f 1` \u03b1r\u03b8s 2 \u2225\u2225\u03b81 \u00b4 \u03b8\u2225\u2225 2 ` 1 2\n\u2225\u2225\u2225 y\u03bbt \u00b4 y\u03bbt\u00b41 \u2225\u2225\u2225 2 \u2016\u03b8 \u00b4 \u03b81\u20162\n\u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622 \u00b4 3 \u2225\u2225\u2225\u2225 y \u03bbt\u00b41 \u00b4 \u03b8 \u2225\u2225\u2225\u2225 2 ` 2 \u2225\u2225\u03b8 \u00b4 \u03b81\u2225\u2225 2 \u00af .\nOne could check that there exists \u03b81 P Bp\u03b8, r\u03bbt\u00b41q satisfying \u03b8\u0302p\u03bbtq P B ` \u03b81` 12vKp\u03b81, \u03bbt\u00b41, \u03bbtq, 12 \u2225\u2225vKp\u03b81, \u03bbt\u00b41, \u03bbtq \u2225\u2225 2 \u02d8 and so combining the last inequality with (25)\n\u2225\u2225\u2225\u2225\u03b8\u0302p\u03bbtq \u00b4 \u03b8 \u00b4 1 2 vKp\u03b8, \u03bbt\u00b41, \u03bbtq \u2225\u2225\u2225\u2225 2 \u010f \u2225\u2225\u2225\u2225\u03b8\u0302p\u03bbtq \u00b4 \u03b81 \u00b4 1 2 vKp\u03b81, \u03bbt\u00b41, \u03bbtq \u2225\u2225\u2225\u2225 2\n` \u2225\u2225\u2225\u03b81 ` 1\n2 vKp\u03b81, \u03bbt\u00b41, \u03bbtq \u00b4 \u03b8 \u00b4 1 2 vKp\u03b8, \u03bbt\u00b41, \u03bbtq \u2225\u2225\u2225 2\n\u010f |1\u00b4 \u03b1r\u03b8s|` 1` \u03b1r\u03b8s 2 r\u03bbt\u00b41 ` 1 2 \u2225\u2225\u2225\u2225 y \u03bbt \u00b4 \u03b8 \u00b4 \u03b1r\u03b8sp y \u03bbt\u00b41 \u00b4 \u03b8q \u2225\u2225\u2225\u2225 2\n` \u2225\u2225\u2225 y\u03bbt \u00b4 y\u03bbt\u00b41 \u2225\u2225\u2225 2 r\u03bbt\u00b41\n2\u2016 y\u03bbt\u00b41 \u00b4 \u03b8\u201622 \u00b4 3 \u2225\u2225\u2225\u2225 y \u03bbt\u00b41 \u00b4 \u03b8 \u2225\u2225\u2225\u2225 2 ` 2r\u03bbt\u00b41 \u00af"}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "<lb>High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage<lb>the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up<lb>solvers. When the procedure is proven not to discard features wrongly the rules are said to be safe. In<lb>this paper we derive new safe rules for generalized linear models regularized with `1 and<lb>`1{`2 norms.<lb>The rules are based on duality gap computations and spherical safe regions whose diameters converge to<lb>zero. This allows to discard safely more variables, in particular for low regularization parameters. The<lb>GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent<lb>for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on<lb>all tested datasets with respect to previous safe rules.", "creator": "LaTeX with hyperref package"}}}