{"id": "1506.04002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2015", "title": "Knowledge Representation in Learning Classifier Systems: A Review", "abstract": "rendering knowledge representation tool is lastly a recent key component valuable to the success of all rule based systems approaches including embedded learning classifier response systems ( lcss ). this component brings insight into regarding how to partition back the shared problem face space then what in turn critically seeks prominent role in communicating generalization capacity of the classification system performance as for a whole. recently, developing knowledge page representation component theory has received great deal of attention arising within data mining communities nationally due to its deep impacts concentrated on specific rule diagrams based detection systems particularly in pure terms of efficiency and efficacy. fundamentally the current work is an attempt to structurally find a comprehensive and confusing yet elaborate view tuning into evaluating the existing traditional knowledge sphere representation techniques, in one lcs domain in general and targeting xcs in that specific. to achieve each the objective objectives, suitable knowledge map representation techniques here are grouped informally into different categories based approximately on the classification approach in modelling which they are continually incorporated. in each category, the underlying graphical rule representation schema and the format variation of specified classifier condition taken to support the corresponding representation are presented. furthermore, a precise explanation agrees on the way of that applicable each technique partitions together the problem space dimension along geography with the extensive experimental results is provided. seeming to essentially have an elaborated view on the functionality of each technique, a carefully comparative analysis of existing techniques on some conventional network problems directly is provided. [ we truly expect this quarterly survey to be of our interest to unite the different lcs researchers and practitioners since it provides a guideline for in choosing a proper knowledge space representation technique formulation for a given problem medium and also opens up new streams of academic research proceeding on this topic.", "histories": [["v1", "Fri, 12 Jun 2015 12:29:31 GMT  (1101kb)", "http://arxiv.org/abs/1506.04002v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["farzaneh shoeleh", "mahshid majd", "ali hamzeh", "sattar hashemi"], "accepted": false, "id": "1506.04002"}, "pdf": {"name": "1506.04002.pdf", "metadata": {"source": "CRF", "title": "Knowledge Representation in Learning Classifier Systems: A Survey", "authors": ["Farzaneh Shoeleh", "Mahshid Majd", "Sattar Hashemi"], "emails": ["shoeleh@cse.shirazu.ac.ir", "majd@cse.shirazu.ac.ir", "ali@cse.shirazu.ac.ir", "hashemi@shirazu.ac.ir"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n04 00\n2v 1\n[ cs\n.N E\n] 1\n2 Ju\ning learning classifier systems (LCSs). This component brings insight into how to partition the problem space what in turn seeks prominent role in generalization capacity of the system as a whole. Recently, knowledge representation component has received great deal of attention within data mining communities due to its impacts on rule based systems in terms of efficiency and efficacy. The current work is an attempt to find a comprehensive and yet elaborate view into the existing knowledge representation techniques in LCS domain in general and XCS in specific.\nTo achieve the objectives, knowledge representation techniques are grouped into different categories based on the classification approach in which they are incorporated. In each category, the underlying rule representation schema and the format of classifier condition to support the corresponding representation are presented. Furthermore, a precise explanation on the way that each technique partitions the problem space along with the extensive experimental results is provided. To have an elaborated view on the functionality of each technique, a comparative analysis of existing techniques on some conventional problems is provided. We expect\nF. Shoeleh Computer Science and Engineering Dept. Shiraz University, Shiraz, Iran. E-mail: shoeleh@cse.shirazu.ac.ir\nM. Majd E-mail: majd@cse.shirazu.ac.ir\nA. Hamzeh E-mail: ali@cse.shirazu.ac.ir\nS. Hashemi E-mail: s hashemi@shirazu.ac.ir\nthis survey to be of interest to the LCS researchers and practitioners since it provides a guideline for choosing a proper knowledge representation technique for a given problem and also opens up new streams of research on this topic.\nKeywords Learning Classifier Systems \u00b7 XCS \u00b7 Knowledge Representation \u00b7 Rule Based Systems"}, {"heading": "1 Introduction", "text": "The first framework of Learning Classifier System (LCS) labeled \u201dcognitive system\u201d was introduced more than 30 years ago by John H. Holland (Holand, 1976). LCSs were originally inspired by the general principles of Darwinian evolution and cognitive learning. The LCS framework was reformed to use reinforcement learning techniques such as Q-learning (Sutton and Barto, 1998) in order to ensure appropriate reward estimation and propagation. LCS is also known as rule-based evolutionary online learning system. It is a heuristic method in which a population of production systems are consisted and adapted by using genetic algorithm and reinforcement learning techniques. Each production system can cover small region of environment and represent some portions of the overall solution. Therefore, a LCS system is able to solve a problem by using the best evolved production systems in its population.\nAccording to the solution encoding methodology, LCS has been addressed from two different points of view: the Pittsburgh Classifier System (Smith, 1980), usually referring to the models which have been inspired by the work of Smith and De Jong at the University of Pittsburg (Smith, 1980; De Jong, 1988; Smith, 1983), and Michigan Classifiers Systems that are usually the models which have been inspired by Holland\u2019s work\n2 Farzaneh Shoeleh et al.\nat the University of Michigan (Holand, 1976; Holand, 1995). In Pittsburg approach, each population member is a production system and GA selects the best one as a complete solution of a given problem. In Michigan approach, the population members are individual rules, and the whole population forms the solution of the given problem. A rewarding mechanism is needed to reward and penalize bad rules.\nThe research in learning classifier systems sprang up in the 1980s. Early classifier systems were known as modeling tools. But they lost this initial characterization with rise of the abilities in the area of machine learning, especially in classification problems and the interest in reinforcement learning and autonomous agents. The year 1995 marked as a milestone in LCS researches due to Wilson\u2019s flavor of Holland\u2019s recipe. Wilson revised the main structure of Holland\u2019s LCS by simplifying it and changing its learning mechanism to use reinforcement learning techniques introduced in (Sutton and Barto, 1998). Nowadays the most popular Michigan system is an evolution of zeroth level classifier system, ZCS (Wilson, 1994), called accuracy based learning classifier system, XCS (Wilson, 1995), which can be considered as a milestone in classifier system research. XCS was the first LCS wherein the fitness of each rule is defined based on its accuracy of the payoff prediction instead of the receiving reward itself. Also, it has another main feature which uses the action set to define the environment niches. In Lanzi\u2019s view, the effectiveness of XCS as a machine learning paradigm is that \u201dXCS was the first classifier system to be both general enough to allow applications to several domains and simple enough to allow duplication of the presented results\u201d (Lanzi, 2008).\nFrom 1995 to now, the applicability of XCS has been extended to a wide range of applications including computational economics (Schulenburg and Ross, 1999; Wong and Schulenburg, 2007), classification and data mining (Bull et al., 2008), autonomous robotics (Patel and Dorigo, 1994; Studley, 2005; Studley and Bull, 2005), power distribution network (Vargas et al., 2004), traffic light control (Bull et al., 2004), function approximation tasks (Wilson, 2002) and many more (Bull, 2004; Lanzi et al., 2000; Bull et al., 2008). So, the current LCS researches are very diverse and pervasive and done better than ever.\nThere are many valuable researches to show the effectiveness and the power of LCS in many domains. Some of these researches provided a description of the overall advances of the LCS field as a survey study. For example, in (Lanzi, 2008), the author tried to answer this question \u201dWhat has happened to learning classifier systems in the last decade?\u201d and examined the\ncurrent state of learning classifier systems research. Indeed, the aim of this study is to emphasize recent developments and the state-of-the-art Learning Classifier Systems. Bull et al. brought together the works on the use of LCS for data mining problems (Bull et al., 2008). LCSs have proved its efficiency at solving online and offline classification tasks. Bacardit et al. gave a summary of past, present and future LCS researches in (Bacardit et al., 2008). They tried to take a look back at the LCS realm and discuss which challenges and opportunities are laying ahead for successful system applications in various domains. As LCS can be seen as a learning system which is able to solve reinforcement learning problems, a study (Sigaud and Wilson, 2007) was done which focuses more on the sequential decision domains than on automatic classification. In (Urbanowicz and Moore, 2009), after introducing a description of basic LCS framework, a historical review of major advancements and a roadmap of algorithmic components are provided. It also emphasizes the differences between alternative LCS implementation and their problem domains. The main aim of such studies is to provide accessible and comprehensible principles and different backgrounds for researches interested in developing their own LCS to achieve a specific goal.\nTo the best of our knowledge, most of the existing surveys have tried to investigate the general behavior of LCS in terms of its common issues, basic components and major applications. These studies can be considered as appropriate resources for who are interested in LCS research field providing that they give a general perspective of the current research opportunities and challenges in LCSs area. But they might not be efficient enough to show the progress of LCSs regarding to the improvement of their main components to handle new issues and challenges. In other words, according to the large number of studies dedicated to each of the main components of LCSs, surveys individually focusing on each one would be of great use provided that they show how such component can be modified to improve the performance of a LCS like XCS in solving a given problem. Consequently, this survey attempts to provide a detailed description of LCS concentrating on one of its major components known as knowledge representation. . Recently, knowledge representation component has received great deal of attention within data mining communities due to its impacts on rule based systems in terms of efficiency and efficacy. It is worth mentioning that it is the first step to identify the environmental properties and it supports an interaction between the environment and the learning system. The current work is an attempt to elaborate view into the existing knowledge representation in LCS domain in gen-\nKnowledge Representation in Learning Classifier Systems: A Survey 3\neral and XCS in specific. The knowledge representation techniques are grouped into different categories based on the classification approach in which they are incorporated. In each category, the underlying rule representation schema, the format of classifier condition and a precise explanation on how it can partition the problem space along with the experimental results are provided. Additionally, to have an elaborated view on the functionality of each technique, a comparative analysis of existing techniques on some conventional problems is provided. It is worth mentioning that the main focus of current study is not explicitly on presenting an alternative knowledge representation scheme, rather it focuses on providing a guideline to choose a proper representation technique for a given problem which is nowadays human need. Undoubtedly, addressing the none-trivial recent issue is of great importance. We hope that this survey facilitates a better understanding of the different streams of research on this topic and provides a guideline for choosing a proper knowledge representation technique for a given problem. Furthermore, it can be of interest to the LCS researchers and practitioners and also opens up new streams of research on this topic. The rest of this paper is organized as follows; at first in Section 2, we briefly introduce learning classifier systems and provide a description of XCS in detail. Section 3 describes the knowledge representation component and the various works that have been done to enhance this component. We group knowledge representation techniques into different categories based on the classification approach in which they are incorporated. In the subsections of Section 3, the underlying rule representation schema, the format of classifier condition and a precise explanation on how it can partition the problem space along with the experimental results are provided. In Section 4, to elucidate the functionalities of each technique a comparative analysis of existing techniques on some conventional problems is provided in general perspective. The last section includes summary and conclusion remarks."}, {"heading": "2 Learning Classifier Systems in Brief", "text": "Learning Classifier Systems belong to a family of machine learning techniques which combines genetic algorithm (GA) with the power of the reinforcement learning paradigm to solve a given problem. In Holland\u2019s recipe, as the inventor of LCS, it has four main components with specific goals: (1) a population of classifiers known as knowledge representation component, which represents the current system knowledge; (2) a performance component, which provides a proper input for system and an applicable output to apply to the\nlearning environment; (3) a reinforcement (or credit assignment) component, which is responsible to distribute the incoming reward among the classifiers that are accountable for it; (4) a rule discovery component, which creates new rules through a covering mechanism and evolves the existing ones by using an evolutionary algorithm, usually a GA. Most of the developed models which had been proposed in the realm of Michigan style LCS were extended and improved the original idea of Holland\u2019s LCS, but kept these entire four main components. It is notable that the effectiveness of LCS to solve a problem depends on the proper interaction between these four components, therefore the compatibility between them must be considered in developing a new extension of LCS.\nIn each learning step of a Michigan style LCS, the\nperformance component perceives the environmental state (or input) through its detectors and performs the selected action through its effectors. Then, the environment eventually pays a reward to the system regarding the selected action\u2019s efficacy. Like other reinforcement learning techniques, LCS tries to suggest a solution which can maximize the amount of received reward through, a set of condition-action-prediction rules called classifiers which are produced and evolved to represent the current solution. On the other hand, the reinforcement component acts on the population to estimate the action values in each subproblem. The discovery component produces new rules for unseen subproblems and usually uses genetic algorithm to evolve the current solution. In the next section, a brief review on XCS (a milestone of LCS) and its main components is provided.\n2.1 XCS in brief\nThe year 1995 is marked a milestone in LCS researches due to the appearance of the most successful and popular Michigan style LCS named XCS. Recent analysis and researches (Wong and Schulenburg, 2007; Bull et al., 2008; Studley and Bull, 2005; Vargas et al., 2004) have shown that XCS has an especial architecture that makes it a competitive learning system in solving complex problems. Its architecture ingredients provide some promising properties such as its online learning capability, its noise robustness, the generality in the learning mechanism, and its continuous adaptation.\nXCS contains a population of classifiers which is called [P]. This population can be empty in the beginning of the experiment, or be filled randomly. Each classifier in [P] is made up of different parts. These parts are: a condition usually from the alphabet {0, 1, #},\n4 Farzaneh Shoeleh et al.\nan action which is usually an integer, and a set of associated parameters. These parameters are (1) a payoff prediction Pj , which estimates the payoff that the system will receive when its action is applied to the corresponding environment; (2) the prediction error \u01ebj , (3) the fitness Fj , and some other parameters such as exp, num and etc.\nWhen XCS receives an environmental state, it forms the related match set [M]. This set includes those classifiers whose condition parts match the current environmental state. If no classifier matches, the covering operator will create a predefined number of classifiers which match the current input and insert them into the population and into [M]. If the covering operator causes the size of the population to grow over a predefined threshold N, some other classifiers will be eliminated from the population regarding their fitness and experience (exp parameter).\nThen, for each action ak, which is proposed by classifiers in [M ], the system computes a fitness weighted\naveragePk using this equation: Pk =\n\u2211 cl\u2208[M|ak]\nclk.F\u00d7clk.P\u2211 cl\u2208[M] clk.F\nwhere [M |ak] is the subset of [M ] contains classifiers which propose the action ak.This value is used as the bid of the corresponding action to win the current phase. Then, XCS chooses an action from those proposed in [M ] regarding its Explore/Exploit strategy. Finally, an action set [A] is formed which consists of a subset of [M ] with the same action as the chosen one.\nAfter that, the selected action is applied to the environment and a reward R is received from the environment which may be zero. Then, the parameters of the involved classifiers are updated. In the sequential environments, the update procedure occurs for classifiers in the previous action set which is called hereafter [A]\u22121. It is also notable that all updates are done using a Q-Learning update regime. To do so, the payoff P is calculated as follows: P = r\u22121 + \u03b3maxa pa where r\u22121 is the previous environmental reward which is 0 or 1000 for classification problem, \u03b3 is the discount factor and pa is the predicted payoff for the applied action in the current trial. Then, classifier predictions in [A]\u22121 are updated as follows: Pj = Pj + \u03b2(R \u2212 Pj) where \u03b2 is the learning rate in the Widrow-Hoff update rule. Next, the prediction errors are re-estimated using this equation:\u01ebj = \u01ebj + \u03b2(|R\u2212 Pj |). Then the accuracy for the corresponding classifier is calculated as follows: kj = 0.1(\n\u01ebj \u01eb0 )\u2212\u03bd for \u01ebj > \u01eb0, else 1.0. The parameter \u01eb0\nis termed the error threshold and \u03bd is a positive integer and both of them are initiated at the beginning of the experiments. Then, the relative accuracy of each classifier is calculated as: k\u2032j = kj\u2211 j kj . And at last, this relative accuracy is used to update the classifier fitness:\nFj = Fj + \u03b2(|k \u2032 j \u2212 Fj |). These updated values are used in another important component of XCS: The discovery component. On a predefined period based on the parameter \u03b8ga, a GA is applied to [A] \u22121. As usual, applying the GA consists of three phases: the selection, the crossover and the mutation. In the selection phase, two classifiers are selected with the proportionate selection operator regarding their fitness. The crossover operator is applied to the two selected parents at the rate of \u03c7. Then, at the rate of \u00b5, each allele of the generated offspring is mutated. The resulting offspring are inserted into the population. If the size of the population grows over a predefined threshold N, two classifiers will be deleted from the population considering their fitness.\nTo improve the generalization capability of XCS, another concept is utilized: the subsumption deletion. When a classifier is added to the population by GA, it is checked against all classifiers in the population to find a sufficiently experienced and accurate classifier which covers the newly inserted classifier. If the covering classifier is found, then the new classifier will be eliminated from the population and the numerosity parameter (num) of the covering classifier will be added by one. This process is called GA subsumption. A same process also occurs in the action set creation phase. In this phase, all classifiers in [A] are matched against accurate and sufficiently experienced classifiers in [A]. If a classifier is covered by another classifier, it will be eliminated from the population and the numerosity parameter of the covering classifier will be increased by one. Figure 1 gives an overall picture of XCS system, which is shown in interaction with an environment. Above XCS description is an abbreviate one, for more details; see the original XCS paper (Wilson, 1995) and (Butz and Wilson, 2001)."}, {"heading": "3 Knowledge Representation", "text": "The first and main step to solve a problem is finding the proper realization of its definition and input space. In the other words, the solver system must be able to model the regularities of the problem space to make decision regarding its experiences. In rule based systems, their rule set is the one responsible for such modeling where each rule represents some portion of the problem space and makes the best decision in corresponding subspace. Consequently, a rule based system is able to solve a problem efficiently if the rule set can cover the whole problem space properly and also each rule makes effective decision. Therefore, it can be concluded that rule representation, i.e. knowledge representation,\nKnowledge Representation in Learning Classifier Systems: A Survey 5\nhas an essential role in rule based systems. In particular, a rule based classifier like XCS can achieve all the three main objectives of classification task which are high accuracy, comprehensibility and compactness, provided that it has a proper rule representation.\nThe first and main step to solve a problem is finding the proper realization of its definition and input space. In the other words, the solver system must be able to model the regularities of the problem space to make decision regarding its experiences. In rule based systems, their rule set is the one responsible for such modeling where each rule represents some portion of the problem space and makes the best decision in corresponding subspace. Consequently, a rule based system is able to solve a problem efficiently if the rule set can cover the whole problem space properly and also each rule makes effective decision. Therefore, it can be concluded that rule representation, i.e. knowledge representation, has an essential role in rule based systems. In particular, a rule based classifier like XCS can achieve all the three main objectives of classification task which are high accuracy, comprehensibility and compactness, provided that it has a proper rule representation.\nAs mentioned earlier, knowledge representation is an essential component of XCS whose elements are certain number of classifiers. Each classifier has two main parts; a condition which represents the regularities of the problem regarding effective generalization, and, an action part that shows what decision being made in corresponding condition. These parts can be adapted\nfor a particular purpose, without modifying the main structure of the system.\nIn the problems with binary inputs, the first and\nmost commonly used syntax for classifier condition termed \u201dTernary\u201d can be used as done in (Wilson, 1995; Holland and Reitman,1978; Schuurmans and Schaeffer, 1989). In this syntax, the condition part is simply represented by a fixed length bit string defined over the alphabet {0, 1, #} where the \u2019don\u2019t care\u2019 symbol (#) matches both one and zero. It is mentioned in (Schuurmans and Schaeffer, 1989) that \u201dTernary representation can hardly model the relation among problem and there is an avoidable bias in system generalization capabilities\u201d, it has three noticeable advantages; first, it is easy to analyze and the obtained rule set is well understood; second, it is proper for textual and categorical information; and third, any data are eventually changed into binary in a computer system. However, since most of real world data sets contain real valued, nominal or mixed attributed data, researchers encourage to proposed powerful representation which are applicable to the complex problems with such data sets.\nMany representations have been developed to handle real valued data, such as, disjunctions of intervals (Wilson, 2000a; Wilson, 2000b; Stone and Bull, 2003; Dam et al., 2005a), ellipsoidal (Butz, 2005; Butz et al., 2006; Butz et al., 2008) and convex hulls (Lanzi and Wilson, 2006). Other general purpose representations have also been developed, such as, first order logic (Mellor, 2005; Mellor, 2006), fuzzy logic (ValenzuelaRendo\u0301n, 1991; Bonarini and Matteucci, 2007; Casillas et al., 2007; Orriols-Puig et al., 2008a), GP-like conditions (Lanzi and Perrucci, 1999; Lanzi, 2001; Lanzi, 2003; Wilson 2008; Preen and Bull, 2009), messy coding (Lanzi, 1999), tile coding (Lanzi et al. 2006) and neural network (Bull and O\u2019Hara, 2002; Hurst and Bull, 2004; O\u2019Hara and Bull, 2005; Howard et al., 2008; Dam et al., 2008) based representation. Table 1 shows the synopsis of different kinds of knowledge representation emerged in XCS. The next subsections arranged in such a way that they detail knowledge representation schemes provided in Table 1 respectively.\nFurther on, we group the knowledge representation techniques into different categories described in each subsection based on the classification approach in which they are incorporated. In each category, we present the following issues: the underlying rule representation schema and the format of classifier condition to support the corresponding representation, a precise explanation bringing insight into how to partition the problem space and the extensive experimental results reported in the original papers.\n6 Farzaneh Shoeleh et al.\n3.1 Interval Based Representation\nThe traditional ternary representation has been substituted with interval-based representation to manage continuous-valued inputs. It has been shown that this modified XCS can be effectively applied to real-valued problems (Bernado\u0301-Mansilla et al., 2001;Wilson, 2000b; Wyatt, 2004; Bull et al., 2008). In the interval-based representation condition part of each classifier is de-\ntermined using intervals defined over each dimension. To define these intervals, four representation techniques have been introduced as follows:\nCenter Spread Representation (CSR): In (Wilson, 2000a), Wilson modified XCS and introduced CSR to handle real-valued inputs. In CSR, for the solution space of range [pmin, qmax), an interval predicate [pi, qi)p is represented as a tuple (ci, si)g where ci indicates the center of the interval and si is a radius around ci.\nKnowledge Representation in Learning Classifier Systems: A Survey 7\nBoth ci and si are real valued numbers. So, a classifier matches input x if and only if ci \u2212 si < xi < ci + si for all dimension of x.\nStone and Bull (Stone and Bull, 2003) analyzed the behavior of classifiers with this representation in XCS. They argued that using CSR causes the incomplete and many to one genotype to phenotype mapping. As an example, the possible values of i\u2019th dimension vary between [0,10), the interval [7,10) in phenotype space can be mapped by (9,2),(8.5,1.5), (10,3), and many more.\nOnce nonlinear genotype to phenotype mapping has performed and mutation and crossover operators have applied, a truncation operator must be applied to classifiers to keep them in range. This operator introduces a bias in the distribution of intervals phonotype. Through this bias, the frequencies of [pmin, qi)p and [pi, qmax)p intervals increase as qi increases or pi decreases, respectively. Therefore, the [pmin, qmax)p interval would have much greater chance to be appeared in classifier conditions.\nLower-Upper Bound Representation or MinMax Representation (MMR): Wilson lately proposed another representation named MMR (Wilson, 2000b) for integer-valued inputs. It can also be used for real-valued inputs (Wilson, 2002). Here, an interval predicate [pi, qi)p is encoded as a tuple [li, ui)g where li = pi and ui = qi. In real valued problems li,ui \u2208 R and in integer-valued problems li, ui \u2208 Z. A classifier matches input x if and only if li < xi < ui for all dimension of x.\nStone and Bull (Stone and Bull, 2003) stated that this representation has complete and one to one genotype to phenotype mapping. Furthermore, this method overcomes the bias generated by truncation in CSR. In comparison with CSR, in MMR, the frequency of intervals of the form [pmin, qi)p and [pi, qmax)p is constant for all pi and qi and [pmin, qmax)p interval has same frequency as that of any other interval. But, another issue arises; the tuples have an ordering restriction (li < ui) and genetic operators might produce infeasible intervals.\nUnordered-Bound Representation (UBR): In order to overcome the ordering issue of the previous method, Stone and Bull (Stone and Bull, 2003) proposed a new approach named unordered-bound representation (UBR) which is similar to MMR without ordering restriction. Thus, an interval predicate [pi, qi)p can be defined in the form of [li, ui)g or [li, ui)g where li 6= ui and li = pi and ui = qi. In other words, the genotype to phenotype mapping is normally complete and two to one except where qi = pi which is one to one. The mutation operator provides strong specialization pressure with UBR, unlike MMR and CSR, where\nthe mutation operator yields a low level of specialization pressure.\nAlthough UBR is able to obviate the problems of previous approaches, it raises new issues. The semantic of the genotypes is not permanent; that is, the gene presenting lower bound of interval in one generation may present the upper bound in the next generation. This issue is inconsistent with building block hypothesis which is the keystone in the architecture of GA and is known as one of the important necessities to run it (Goldberg, 1989).\nMin-Percentage Representation (MPR): In (Dam\net al., 2005a), Dam et al. purposed to overcome the problem of UBR by presenting new approach named Min-Percentage Representation which maintains the semantic of the genotype all over evolutionary run. In MPR, an interval predicate [pi, qi)p can be encoded as a tuple [mi, peri)g where mi = pi and peri = qi\u2212pi\npmax\u2212pi .\nSo a classifier matches input x if and only if mi < xi < mi + peri \u2217 (pmax \u2212mi) for all dimension of x.\nIn (Dam et al., 2005a), MRP and UBR were compared in 6-Real-Multiplexer and Checkerboard problems. Results indicated that the two techniques have equivalent performance and similar behavior in both problems. But, MRP is unlikely to change the semantic of the genotypes.\nAs illustrated in Figure 2, XCS with interval based representation uses hyper rectangles to partition the problem space. In (Butz et al., 2006) Butz showed, that the interval based condition makes difficulties to solve problems with oblique decision boundaries. He also showed that the learning takes longer time and the compactness of the model is not efficient when dealing with oblique boundaries. For example, XCSF (Wilson, 2002), one of the successful extensions of XCS, needs\n8 Farzaneh Shoeleh et al.\nan enormous number of classifiers with hyper rectangle conditions to accurately approximate a nonlinear surface such as a circle. Interval based representation is well-suited in modeling axis-parallel boundaries, but in dealing with oblique boundaries, it is more advantageous to use a representation which partitions the problem space into complex regions.\n3.2 Ellipsoidal Based Representation\nIn (Butz, 2005), Butz suggested a new structure of classifier condition, based on defining hyper sphere and hyper ellipsoid shapes. There, three structures were proposed; first, the condition part of the classifiers represents a hyper sphere in the problem space. As hyper sphere has common radius in all dimensions, the condition part consists of a center and a deviation, that is, C = {\u2212\u2192m,\u03c3} = {m1,m2, ...,mn, \u03c3}. Second, the condition part represents an axis-parallel hyper ellipsoid which has different deviation in each dimension, that is, C = {\u2212\u2192m,\u2212\u2192\u03c3 } = {m1,m2, ...,mn, \u03c31, \u03c32, ..., \u03c3n}. Third, the condition part is redefined to present a general hyper ellipsoidal structure which is axis-independent unlike the previous one. So, it is defined by a center point in addition to elements of matrix \u03a3 named transformation matrix which indicates fully Mahalanobis distance metric of a hyper ellipsoid. This matrix shows stretch and rotation of the represented hyper ellipsoid. The condition part of a classifier is defined as follows: C = {\u2212\u2192m,\u03a3} = {m1,m2, ...,mn, \u03c31,1, \u03c31,2, ..., \u03c3(n,n\u22121), \u03c3(n,n)} where \u2212\u2192m shows the center of represented hyper ellipsoid and \u03a3 shows the transformation matrix of the condition. This structure is a general form of the first and second ones; hyper sphere is a hyper ellipsoid where the diagonal entries of \u03a3 are initialized with common value and all other entries are set to zero, and axisparallel hyper ellipsoid has a diagonal matrix as \u03a3. In the general form, angular orientation and stretch of the represented hyper ellipsoid are implicitly encoded in \u03a3. Due to the redundancy of this encoding; the mutation and crossover operators may not act beneficially (Butz et al., 2008). This would decrease the reproductive opportunities of the successful classifiers and lead the evolutionary progress to slow down. To solve this problem, Butz et al. (Butz et al., 2006; Butz et al., 2008) investigated another condition representation which is able to explicitly codify the rotation of the desired hyper ellipsoid in its structure. So, the condition part of each classifier is expressed by three vectors, C = {\u2212\u2192m,\u2212\u2192\u03c3 ,\u2212\u2192\u03b3 }; a vector \u2212\u2192m that indicates the center point, a vector \u2212\u2192\u03c3 = (\u03c31, \u03c32, ..., \u03c3n) T which represent the stretch, and a vector \u2212\u2192\u03b3 with size of (n2 ) to point out the orientation angles of the corresponding hyper ellipsoid. Figure\n3 highlights how a classifier with mentioned representations can cover a partition of the input space.\nIn all these structures, activation of each classifier in forming [M ] is determined by a Gaussian kernel function, applied to the distance between the current input and the center point. So, to find whether a classifier can match the current input x or not, at first, the activation of the classifier cl.ac is computed using Formula 1, 2 or 3 according to its structure. Then, the current input will be matched with a classifier if the activation of such classifier is greater than a threshold \u03b8m. Formula 1, 2 or 3 are used when the condition of the classifier represents a hyper sphere (C = {\u2212\u2192m,\u03c3)}, an\nKnowledge Representation in Learning Classifier Systems: A Survey 9\naxis-parallel hyper ellipsoid (C = {\u2212\u2192m,\u2212\u2192\u03c3 }) , or a general hyper ellipsoid (C = {\u2212\u2192m,\u03a3}) respectively.\ncl.ac = exp(\u2212 \u2016x\u2212m\u2016\n2\n(2\u03c32) ). (1)\ncl.ac = exp(\u2212 \u2211\n(\ni = 1)n (xi \u2212mi)\n2\n(2\u03c32) ). (2)\ncl.ac = exp(\u2212 (\u2212\u2192x \u2212\u2212\u2192m)T\u03a3T\u03a3(\u2212\u2192x \u2212\u2212\u2192m)\n2 ). (3)\nPromising results of these condition representation approaches reported in (Butz, 2005; Butz et al., 2006; Butz et al., 2008) showed an improvement in performance of XCS (or XCSF) in continues space problems. Especially, it is more profitable to use a more general condition structure while the dimensional dependencies of the problem are unknown. Although these representations can appropriately model oblique boundaries, but it must be noted that it might be not well-suited where decision boundaries of problem are axis-parallel for which case interval based representation can be applied effectively.\n3.3 Convex Hull Based Representation\nAnother approach that can be utilized to represent the condition of a classifier in the real-valued problems is using the convex hull concept as proposed in (Lanzi and Wilson, 2006). In this approach, condition part of each classifier comprises a set of points in the problem space that identifies a convex hull. In other words, each classifier depicts a convex region of the problem space and matches all problem instances which lie inside this region, as shown in Figure 4. As all the other geometric shapes can be approximated through a convex hull with sufficient number of points, the convex hull based representation is a general form of both previous representations. Besides, the convex hull representation has a fine ability to identify more complex regions due to its asymmetric shape. Condition part of the classifiers can represent a convex hull in two manners; first, using a set of points. Second, each classifier can define a convex hull directly by presenting angles and radius of the convex hull in its condition part, instead of using a set of points. In both cases, number of convex hull vertices can be fixed or variable. In (Lanzi and Wilson, 2006), the authors discussed the influence of variable sized conditions and concluded that \u2019 represent arbitrarily complex convex regions but increase the complexity of the genetic search and also introduce the bloating phenomena that are typical with variable size representations \u2019.\nIn (Lanzi and Wilson, 2006), the performance of XCSF based on convex hull representation was compared to the XCSF with interval-based representation. Results showed the fast convergence of system when the problem space is partitioned by convex regions. Also it was shown that XCSF with variable sized condition converges faster than a version of XCSF where condition part of classifiers consists of 10 or 15 points, but compared to 3 and 5 points, it converges slower.\n3.4 Fuzzy Logic Based Representation\nIn the rule based classifier systems, the comprehensibility and interpretability are two must considerable features (Hayes-Roth, 1985). Besides, fuzzy logic is one of the best known mechanisms providing such properties. There are number of approaches to use fuzzy logic and fuzzy set theory (Zadeh, 1965; Zadeh, 1973) as a technique for representing rules in Michigan-style LCSs, such as (Valenzuela-Rendo\u0301n, 1991; Bonarini and Matteucci, 2007; Casillas et al., 2007; Orriols-Puig et al., 2008a). The main goal behind such efforts is combining the generalization capabilities of LCS with the fine interpretability of fuzzy rules to achieve an online learning system with more accurate, general and well understandable rule set. In the following, first, we briefly describe important works in this area. Second, a comprehensive description of notable approaches which try to embed fuzzy logic in knowledge representation component of LCS is provided.\n10 Farzaneh Shoeleh et al.\nEarly attempts to integrate fuzzy logic and learning classifier systems were proposed in (Valenzuela-Rendo\u0301n, 1991; Nomura et al., 1998; Parodi and Bonelli, 1993). Valenzuela-Rendo\u0301n in (Valenzuela-Rendo\u0301n, 1991) introduced the first proposal of Learning Fuzzy-Classifier Systems (LFCSs), which is a Michigan style LCS and consists of fixed-size fuzzy-rule set and a fuzzy message list. Afterwards, several researchers have used the idea of fuzzy logic or fuzzy set theory into LCS to have an online fuzzy rule based system which can be used in many tasks (Nomura et al., 1998; Parodi and Bonelli, 1993; Furuhashi et al., 1994; Nakaoka et al., 1994; Velasco, 1998; Ishibuchi et al., 1999). Many of proposed systems were applied to the reinforcement learning and control tasks. As the initial LFCS framework was not completely coincident with reinforcement learning architecture, Nomura et al. (Nomura et al., 1998) improved the LFCS structure to be a true reinforcement learning technique. In (Parodi and Bonelli, 1993), Parodi and Bonelli defined an LFCS which can automatically learn the fuzzy relations, membership function and weights. Velasco (Velasco, 1998) designed a new extension of LFCS especially designed for fuzzy process controls and Ishibuchi et al. (Ishibuchi et al., 1999) proposed an LFCS for pattern classification.\nThe research on this topic had been continued by the works of Bonarini (Bonarini et al., 2000; Bonarini, 1998; Bonarini, 2000; Bonarini and Matteucci, 2007). He addressed the classic \u201dCompetition versus Cooperation\u201d problem in genetic fuzzy systems, (Bonarini, 1996; Bonarini and Trianni, 2001). He proposed a Michigan style LCS named ELF where the rule set is divided into subpopulations. To produce the correct action, the classifiers of these subpopulations cooperated whilst the classifiers in each subpopulation competed with each other. The behavior of ELF in overcoming some of issues of strength-based LCSs was verified by applying it to several reinforcement learning problems such as the coordination of autonomous agents. In (Bonarini, 2000; Bonarini et al., 2007), a general framework of learning classifier systems were introduced and later this framework was extended particularly for XCS called FIXCS in (Bonarini and Matteucci, 2007). In (Bonarini, 2000), the different components of this framework have been analyzed to be consistent with fuzzy models. In addition, some features are introduced for the sake of classifying LFCS proposals presented in the literature.\nRecently, in (Casillas et al., 2004; Casillas et al., 2007) Casillas et al. also used fuzzy logic in knowledge representation component of XCS to express rules in fuzzy format. There, the theoretical issues of applying fuzzy model in accuracy based learning classifier systems have been investigated. The proposed ap-\nproach was named Fuzzy-XCS which is the first successful accuracy based fuzzy rule based system with generalization ability. Soon after, the Orriols-Puig proposal (Orriols-Puig et al., 2008a; Orriols-Puig et al., 2008b), named Fuzzy-UCS, extended this approach to be applied in supervised learning classifier system, i.e. UCS which is a derivation of XCS introduced in 2003 (Bernado\u0301-Mansilla and Garrell, 2003) for classification task in data mining. In following, a comprehensive description of well known fuzzy representation which is successfully used in LCS realms to produce accuracy based fuzzy rule based system such as Fuzzy-XCS and Fuzzy-UCS.\nThe main idea of using fuzzy logic in XCS as the knowledge representation tool is to represent the labels associated to fuzzy sets in the rule\u2019s structures and offer LCS a mechanism to evolve them. This mechanism must be consistent with fuzzy rules and it must also be able to learn a rule set in order to implement an input to output mapping where both input and output can be either real valued or nominal. In common manner, the rule set consists of fuzzy rules which are defined in disjunctive normal form (DNF) with the following structure:\nIF X1 is 1 and ... and Xn is n THEN Y is B. (4)\nWhere each input variable Xi is described through a set of linguistic terms, i = {Ai1V...V Ail}, represented by a disjunction (T-conorm operator) of l linguistic terms. Meanwhile, each output variable is represented by a usual linguistic variable. For example, in (Casillas et al., 2007) to use this representation in classifiers of Fuzzy-XCS, a binary coding schema for the condition part and an integer coding schema for the action part were proposed. The number of bits considered in the condition part is equal to the total number of defined linguistic terms for all dimensions. For example, consider a problem with two dimensions where five linguistic terms are defined for both the first and second dimensions. So, the condition part of each classifier should have ten bits to codify these linguistic terms. The value of each bit indicates the appearance of the corresponding linguistic term. For each output variable, there is a gene in action part which denotes the index of utilized linguistic term. Below, there is an example to show how a rule can be codified by this representation:\nTerms for each input/output variable:\nvS [very small], S [small], M [medium], L [large], vL [very large]\nFuzzy rule:\nIF X1 is {M,L} and X2 is S THEN Y1 is M and Y2 is L\nKnowledge Representation in Learning Classifier Systems: A Survey 11\nRepresentation:\n[ 0011001000 \u2016 23 ]\nFigure 5 shows what portion of input space can be covered by a fuzzy classifier. As it is shown, each dimension is partitioned according to its defining linguistic terms and the covering region is denoted by a dashed rectangle.\nRules with fuzzy structure entail several features making it useful as claimed in (Casillas et al., 2007):\n\u2013 It offers an approach to handle mixed attribute. \u2013 It can handle missing values due to the natural ca-\npability of fuzzy logic in supporting the absence of some input variables. \u2013 This structure leads to have more compact descrip-\ntion and the evolved rule set consists of fuzzy rules with different generalization degrees.\n\u2013 It is robust to noisy input. \u2013 It can produce the output classes with a certainty\nfactor in classification problems.\nPromising results are obtained by evaluating the system on some function approximation problems and a realistic robot simulation online learning and supervised learning problems. Results show that it can increase the compactness of evolved rule set which is accurate, general and co-adapted However, using fixed and\npredefined fuzzy sets cause a limited number of possible rules. So, they are not flexible enough to fit the data very well. In order to overcome this issue, system must have an ability to modify the predefined fuzzy sets. In (Marin-Bla\u0301zquez et al., 2007; Marin-Bla\u0301zquez and Shen, 2008; Marin-Bla\u0301zquez and Martnez Prez, 2008), linguistic hedges are employed to produce a new fuzzy set by modifying the original fuzzy set in interpretable manner where the original fuzzy set remains unaltered. By using linguistic hedges, the model has more freedom in domain knowledge representation and extraction. The research has demonstrated that better granularity of linguistic fuzzy modeling are achieved by using linguistic hedges as a technique to modify fuzzy sets and the inclusion of hedges causes improvement in the accuracy of the resulting system.\n3.5 First Order Logic Based Representation\nMellor proposal in (Mellor, 2005;Mellor, 2006) extended XCS to a new model named FOXCS where rules are represented based on the first-order logic. First-order logic is useful to improve the expressive power of XCS due to its ability to present the complex relationships among attributes of a task domain. The modified classifiers are in form of Horn clauses which consist of three parts; an action part, a condition part and a background part, that is action \u2190 condition,Background. Condition part consists of a number of variables. Here, variables have a generalization role in XCS similar to the # symbol in the ternary representation. However, variables can also be placed in action part. But, as a classifier should be defined in Horn clause form, its action part must consist of just one variable, i.e. atom. The background part can be empty.\nThe main advantages of using the first order logic are; 1) it is facilitated to represent relational concepts which relate variables of action part to those of condition, like (A \u2190 ABB) where A and B are variables of given problem. Obviously, the variable A which takes place in action part can also appear in condition part. To illustrate this property, there is an example for a Blocks World tasks in Figure 6. 2) Another advantage is that rules can contain background knowledge which is a feature of many inductive logic programming (ILP) and rational reinforcement learning (RRL) systems, and can be helpful to solve these tasks effectively. 3) Mellor [Mellor, 2008] claimed that FOXCS as a RRL system is general, model free and \u201dtabula rasa\u201d system. It is general due to no restriction in problem framework\n12 Farzaneh Shoeleh et al.\nand \u201dtabula rasa\u201d1 because the initial policy can be left unspecified. The Results reported in (Mellor, 2005; Mel-\nlor, 2006; Mellor, 2008) show that the extended XCS can reach near optimal solution in the Poker and Blocks World problems which are not solvable by traditional XCS. Experimental results demonstrated that FOXCS can learn the optimal or near optimal policies where accuracy is comparable to the accuracy of many ILP algorithms in solving standard RRL tasks.\n3.6 Messy Code Representation\nIn (Lanzi, 1999), Lanzi introduced the messy version of XCS classifier system named XCSm in which the condition part of each classifier is defined by messy coding. The messy representation was firstly introduced by Goldberg in Messy Genetic Algorithm (Goldberg et al., 1989). In this representation, each gene is defined by a pair: (Gene Number, Allele Value). As it is not necessary to have an allele for each possible gene in a chromosome, messy chromosome can be underspecified. In XCSm, a variable sized set of messy genes composes the classifier condition in which Gene Number presents the tested sensor and Allele Value is a fixed length bit string that specifies the sensed input. For example, consider a maze environment with eight sensors. The food, obstacle and empty cells are codified as \u201d11\u201d,\u201d10\u201d, and\n1 Tabula rasa is the epistemological thesis that individuals are born without built-in mental content and that their knowledge comes from experience and perception.\n\u201d00\u201d alternatively. Any input state with a goal or obstacle in its south position is matched by a messy gene like (S,\u201d1#\u201d). Figure 7 shows an example of such classifier in Maze7 problem introduced in (Lanzi, 1998). As shown in Figure 7(a), the corresponding classifier can cover states marked by star in their center. With comparison to Ternary representation, the main advantageous of this representation is the independency of this representation\u2019s bits from the position of input sensors bits. Experimental results in (Lanzi, 1999) showed that\nXCSm cannot reach the optimal solution. By analyzing the final population of classifiers, it is found that most of the evolved classifiers are over-general one. In order to cope with under-specification and over-specification of classifier condition, Lanzi developed XCSm by using high covering probability, extending mutation operator. He also suggested a new matching operator in which a condition matches current input if all its messy genes can match this input. As experiments notified, this new enhanced XCSm can learn an optimal solution.\n3.7 GP-like Representation\nThe representation methods which represent a tree form as GP (Koza, 1992), an expression as BNF2 grammar (Chomsky, 1956), and the other similar structures are known as GP-like representation. Firstly, using this representation in XCS was exploited by Lanzi in (Lanzi and Perrucci, 1999; Lanzi, 2001a). There, a system named XCSL was proposed that the condition parts of its classifiers have a general purpose representation, namely lisp s-expressions. The conditions were defined by the BNF grammar as shown in Figure 8. The non-terminal symbol <expression>in BNF form must be defined in\n2 Backus Normal Form or Backus-Naur Form\nKnowledge Representation in Learning Classifier Systems: A Survey 13\norder to use this lisp like representation for a given problem.\nTo evaluate a classifier in the matching phase, the input data of XCSL must be represented as a string of attribute-value pairs and the terminal symbols presented in the classifier must be replaced with their actual values of corresponding attributes. Since the classifiers in XCSL have GP like structure, the genetic operators work as they do in traditional Genetic Programming (Koza, 1992).\nAs shown in (Lanzi, 2001b), XCSL tends to produce many overgeneral classifiers in population, so the learning performance of XCSL would be influenced in some cases. Besides, due to the bloat phenomenon (Langton et al., 1996; Soule et al., 1996), which is common in any GP like system, it is almost impossible to analyze what kind of classification model is developed by XCSL. To overcome the former problem, a final condensation phase was added into the system to extract a minimal subset of classifiers as the final solution. It extracts a compact solution from many overlapping classifiers instead of extracting all best classifiers from the population. The promising results suggested that XCS with symbolic based representation might be an interesting approach to extract useful knowledge from data. In (Lanzi, 2003), an extension to XCS was proposed by adding a stack-based representation into the classifiers\u2019 condition which was a linear program expression in Reverse Polish Notation (RPN) form. So, each condition consists of a sequence of tokens; each token can be a constant, a function or a variable which can be assigned to the corresponding input attribute value. There, a stack-based genetic programming, introduced in (Perkis, 1994), is used to mutate and recombine the classifiers. The reported results are quite interesting. Since genetic operators do not take into account any information about the structure of the operators defined in RPN and their arity, syntactically incorrect conditions can be easily generated. Consequently, the search space of the feasible solution would be even more highly redundant in comparison with XCSL.\nWilson has explored the use of gene expression programming (GEP) within XCS (Wilson, 2008) named GEP-XCS. The main aim of using GEP translation in\nXCS was not only to fit the environmental regularities better than rectilinear but also to produce a well understood rule set. The condition part of each classifier in GEP-XCS is expressed with an expression tree which can be evaluated by initializing the tree\u2019s terminals with the corresponding input attributes values. So, in the process of forming [M ], the obtained value of applying the presented expression in each classifier on the current input is compared with a predetermined threshold. If the obtained value exceeds this threshold, corresponding classifier will be added to the [M ].\nThe most important feature of using GEP translation is that every chromosome generated in GA cycle is syntactically valid. In addition, the size of expression tree in GEP representation is limited by the fixed chromosome size therefore the bloat phenomenon which usually happens in GP does not occur. GEP has further important properties such as combining several genes into a single chromosome and the concurrent evolution ability which are disregarded in GEP-XCS. Figure 9 illustrates how a GP-like classifier can partition the problem space. In this example, the classifier presents an expression tree form shown in Figure 9 (b). According to the different types, namely using GP or GEP, to encode the phenotype of the classifier, the expression tree can present in two different ways shown in Figure 9 (b). As mentioned, the condition part of classifier in GEP based representation must have fixed size; here it is assumed that the length of condition part of each classifier must be fifteen. Hence, since the corresponding tree form only needs seven genes to be encoded, in GEP based representation the additional genes are initialized with terminal variables. But, in the GP representation the classifier can have variable length so the length of classifier condition part can be equal to the number of nodes of the expression tree. Adding GEP to XCS as a condition representation has three main advantages; (1) each classifier has functional condition, with fine ability to fit environmental regularities, (2) the applied genetic operators can be quite simple because they are dealing with linear chromosomes instead of expression trees. Also, there is no need to verify the validation of produced offspring because genetic operators applying on GEP chromosomes always produce valid chromosomes and (3) the most attractive property which makes GEP based representation more desirable than other GP-like representations, is fixed size of expression tree. The reported results showed that using such representation can achieve good performance and gives greater insight into the environment\u2019s regularities but it leads fairly slower evolution and the evolved rule set was not compact.\n14 Farzaneh Shoeleh et al.\nRecently, an investigation (Preen and Bull, 2009) was done in using a discrete dynamical system representation within XCS which is termed Dynamical Genetic Programming (DGP). DGP-XCS used a graph based representation for the condition of each classifier, wherein each node is a Boolean function. In other words, each classifier condition presents a Random Boolean Network (RBN) with N nodes which is equal to sum of the inputs I and the outputs O plus one (N = I+O+1). The first connection of each input node is assigned to the corresponding locus of such input and other connections are set at random. The condition part of each classifier presents a RBN with its true table and the connections of each node. This new system is tested on two most common benchmarks, the multiplexer and the maze navigation problem. According to the obtained results, it can be concluded that it is possible to design an ensemble of RBN by using XCS which is able to solve a computational task under RL scheme.\n3.8 Neural Networks Based Representation\nLarry Bull and Toby O\u2019Hara proposed an accuracy based neural classifier named X-NCS and a Neuro-fuzzy classifier system called X-NFCS in (Bull and O\u2019Hara, 2002).\nIn X-NCS, both condition and action parts of each classifier present a single and full connected neural network like multilayer perceptron (MLP) (Bishop, 1995). In the condition part of each classifier, the weights of such small neural networks are concatenated together and can be evolved under GA mechanism in XCS. All the neural networks presented in all classifiers have the same number of nodes in their hidden layers. Besides the number of output nodes is equal to sum of the number of possible actions and one extra output node named \u2019not match-set member\u2019. Since all rules can see the input state, this extra output node is considered to signify whether the corresponding classifier is a member of the [M ] or not. Each classifier whose \u2019not match-set member\u2019 node does not have the highest output value can be a member of the [M ]. The winner action of each classifier is the one whose corresponding output node has the highest activation.\nX-NCS had been tested on two tasks; a 6-bit multiplexer which is a single step task and to solve a maze named Woods2 which is a multi step problem. The obtained results showed that X-NCS could solve both tasks. In addition, X-NCS components and parameters were modified to work as a function approximator. Its performance had been examined with root-meansquare. As expected, the results showed that the most accurate solution is achieved when one classifier can cover the whole input space. To extend X-NCS to Neurofuzzy version (X-NFCS), radial basis function (RBF) is used due to its similarity to the fuzzy rule based system (Jang and Sun, 1995). The discovery component in X-NFCS evolves RBFs as it does in X-NCS for MLPs.\nOne of the main advantages of this scheme, namely using neural network based representation, is its ability to be applied on problems with continuous action space. The promising results showed that both X-NCS and XNFCS can be used in more complex tasks including both continuous and discrete action space and also it is applicable to both single step and multi step problems. To address the slow convergence in X-NCS, in (O\u2019Hara and Bull, 2005) a hybrid technique was proposed to speed up the learning using Memetic Algorithm (MA) in XCS which augments the GA search with a local search (Moscato, 1989).\nIn (Howard et al., 2008), the principle of constructivism and self-adaptation was explored within XCS and XCSF. There, those systems were applied on wellknown maze problems and the reported results showed using self adaptive operators, neural constructivism, and prediction computation could be an improvement in the realm of XCS. These systems can optimally perform in complex and noisy environments. Overall, the experiments of these mentioned papers and the similar\nKnowledge Representation in Learning Classifier Systems: A Survey 15\nones showed that X-NCS is able to model the environment in simple robotics applications (Hurst and Bull, 2004) and also can solve several maze problems (Bull and O\u2019Hara, 2002; O\u2019Hara and Bull, 2005; Howard et al., 2008). It must be mentioned that the complete replacement of a rule by a neural network makes X-NCS to lose its main advantage as a rule based systems, that is, the potential ability to produce a well understood rule set. To overcome this issue, a new system named NLCS was proposed in (Dam et al., 2008). It is an extension of UCS which is driven for classification task in data mining problems. In (Dam et al., 2008), neural network is just used to identify the proper action of classifiers. The aim of such usage is to produce smaller evolved rule set while it still maintains or even improves the predictive accuracy. So, each classifier consists of two parts: the condition part which can be encoded by any existing representation method such as interval predicates (which is also used in (Dam et al., 2008)) like messy code and GP-like, and the action part which contains a neural network. For better understanding, Figure 10 shows the structure of a classifier in a twodimension problem and how such classifiers will partition the problem space. As results showed (Dam et\nal., 2008), defining neural network in action part allows XCS to produce more general classifiers because their condition part can cover large area of the problem space with more general hyper rectangle. So, NLCS can achieve better performance in classification problems with less number of evolved rules rather than UCS.\n3.9 Tile Coding Based Representation\nLanzi et al. had extended XCSF with tile coding prediction (Lanzi et al. 2006). Tile coding is one of the most common and successful methods to tackle complex environment in reinforcement learning realm (Sutton and Barto, 1998). In tile coding, the problem space is mapped into a set of overlapping tilings; each tiling partitions the input space into a set of nonoverlapping hyper-rectangles named tiles. Classifiers in XCSF with tile coding prediction have two additional parameters; the number of tilings and their resolution. The extended system can adapt these parameters through GA. Also the usual linear prediction function in XCSF is replaced with a tile coding approximator. So, the weight vector of each classifier contains the parameters related to each tile. Figure 11 shows how a classifier with two tilings partitions the input space and which tiles (states) could match the specified input. The new\nextended XCSF was tested on three multistep problems taken from the reinforcement learning literature: the 2D Gridworld (Boyan and Moore, 1995), the puddle world (Boyan and Moore, 1995), and the mountain car (Sutton, 1996). Indeed, such XCSF evolves an ensemble of tile coding approximators, each one on the problem subspace, instead of the typical monolithic approximator. The reported results showed that XCSF with tile coding can always reach an optimal solution and converge faster than XCSF with linear approximation."}, {"heading": "4 Relative Strength and Weakness of Knowledge Representation Techniques", "text": "Each of knowledge representation techniques discussed in previous section its unique strengths but also weaknesses. For solving a given problem effectively by a rule based system such as LCS and XCS in particular, it is important to know which knowledge representation technique is the well suited for identifying the prob-\n16 Farzaneh Shoeleh et al.\nlem regularities. In other words, the first step in solving a problem is identifying the problem area to choose a proper representation which can cover whole problem space completely and embrace its complexity effectively. In this section, we present a few simple problem settings and analyze the relative strength and weakness of each discussed technique in handling such problems. Let us consider the following categories of problems. It is worth mentioning that we name each category based on the decision boundaries shapes and the complexity of the problem regularities.\nTheoretical Problems: the main aim of this kind of problems is analyzing the behavior of the learning system in solving a problem without facing the complexity of environments. In other words, in these problems researchers try to choose a simple environment with specific properties. Hence, simple techniques for knowledge representation such as ternary for bit string inputs, and, interval based representation for real inputs could be a good choice.\nOrthogonal Problems: in these problems the decision boundaries or regularities which are interesting to be modeled are parallel with the main axises. Figure 12 shows checkerboard problem, a sample problem of this problem category.\nMost of discussed knowledge representation techniques will handle this kind of problems. But interval based representation is well suited because it is a simple and popular technique and also it might be easier to apply compact rule set methods on the evolved rule set. In other words, since classifiers with interval based representation partition the problem space by defining axis-parallel hyper-rectangles of different sizes, this representation has an implicit ability to cover axis-parallel boundaries. But it is important to note that those problems with not only axis-parallel regularities but also continuous action can be handled using other representations such as neuro-fuzzy and neural network based representation. On the other way round, interval based\nrepresentation will be able to handle such problems with continuous action if another independent method is used to produce a continuous action.\nOblique Problems: this category contains problems with oblique decision boundaries. Figure 13 shows several specimens of the recent type.\nIf we have enough information about the regularities of problems, it is better to use techniques which can intuitively fit the boundaries properly. To elucidate, consider Cycloid and Tao data sets which have oblique cycloidal boundaries. A proper choice for such boundaries might be ellipsoidal based representation due to its natural cycloidal shapes. On the contrary, convex hull based representation can model the cornered decision boundaries of Pentagram data set adequately due to providing asymmetric shape and having higher flexibility in covering complex areas.\nHeterogeneous Problems:This category contains\nthe problems in which have complex regularities or in other world have heterogeneous decision boundaries. To handle such problems successfully as if the decision boundaries can be modeled more precisely, obviously one has to choose a technique among more complex representation with ability to cover complex problem spaces. Since knowledge representation techniques such as GP-like and neural network based representation are general purpose and able to represent arbitrary regularities; they are also the methods of choice when the\nKnowledge Representation in Learning Classifier Systems: A Survey 17\n18 Farzaneh Shoeleh et al.\nMessy Code Representation (XCSm) \u2013 Variable length representa-\ntion. \u2013 Well-suited in spars prob-\nlems.\n\u2013 Not suited for real valued problems.\n\u2013 Multi step problems [Lanzi, 1999]\nGP-like Representation (XCSL, XCSF-GEP, DGPXCS)\n\u2013 Applicable to real valued and nominal problems. \u2013 Able to represent arbitrary regularities. \u2013 Offer greater transparency. \u2013 Have the ability to ignore\nunneeded inputs or add ones that become relevant.\n\u2013 Applicable to real valued and nominal problems. \u2013 Able to represent arbitrary regularities. \u2013 Offer greater transparency. \u2013 Have the ability to ignore\nunneeded inputs or add ones that become relevant.\n\u2013 Function Approximation [Wilson,2008;] \u2013 Multi step problems [Preen and Bull, 2009; Lanzi and Perrucci, 1999; , Lanzi, 2003; Cielecki and Unold, 2007] \u2013 Data Mining [Lanzi, 2001a] \u2013 Learning Context-Free Lan-\nguage [Unold, 2005; Unold and Cielecki, 2005; Unold, 2007]\nNeural Networks Based Representation (X-NCS, NCS, NLCS, XCSFNN, X-NFCS)\n\u2013 Applicable to real valued environments. \u2013 Able to represent arbitrary regularities. \u2013 Able to apply on problem with continuous action space. \u2013 Able to produce more general classifier.\n\u2013 Low interpretability \u2013 Every classifier must accept\ninputs from all variables, whereas this might not be necessary for some regularities.\n\u2013 Function Approximation [Bull and O\u2019Hara, 2002; Loiacono and Lanzi, 2006] \u2013 Multiple Domains [[Bull and O\u2019Hara, 2002; O\u2019Hara and Bull, 2005] \u2013 Robotics [Hurst and Bull, 2004] \u2013 Classification [Dam et al., 2008]\nTile Coding Based Representation (XCSF-RTC)\n\u2013 Applicable to real valued environments. \u2013 XCSF with tile coding prediction converge faster than XCSF with linear approximation.\n\u2013 Hard to evolved the number of tiling and their resolutions.\n\u2013 Reinforcement Problems [Lanzi et al., 2006]\nproblem has not only complex regularities but also unknown dimensional independencies.\nProblems with mixed attributes: Hitherto, real valued problems and bit string ones are discussed independently while mixed attributed problems are more common in real word applications. A straightforward solution is considering the problem as two subproblems: one is real valued problem with real valued attribute and the other is nominal problem only containing nominal attributes. These subproblems can be solved individually by two different representation techniques like interval based and messy code representation. Since not all mixed attributed problems can be divided into such subproblems, a representation which can handle both kind of attribute indiscriminately would be useful in such problems. GP-like and fuzzy based representations are some examples of those representation techniques.\nIn addition to what has already been mentioned, each representation technique has its own remarkable properties which provide an appropriate solution for particular problems. Table 2 summarized the existing knowledge representation approaches, some of their main advantageous, disadvantages, and the problem domain(s) for which the representation was designed and/or tested.\nWe expect that this table to be a roadmap for choosing the best technique among wide varieties of existing knowledge representation approaches proposed so far."}, {"heading": "5 Summary and Conclusion", "text": "In this study, we focus on the progress of XCS as a rule based learning system with a special consideration into the improvement of one of its most important components known as knowledge representation. As this component seeks prominent role in generalization capacity of the system and impacts on rule based system in terms of efficiency and efficacy, the past decade has seen a growing interest in proposing different knowledge representation techniques in LCS domain in general and XCS in specific. Here, after reviewing some basic information, we survey different knowledge representation techniques proposed in Michigan LCS realm and grouping them into different categories based on the classification approach in which they are incorporated. In each category, the underlying rule representation schema and the format of classifier condition to support the corresponding representation are presented. Furthermore, a precise explanation on the way that each technique\nKnowledge Representation in Learning Classifier Systems: A Survey 19\npartitions the problem space along with the extensive experimental results is provided. The main properties of knowledge representation technique, investigated in this paper, can be used as an illumination guideline to determine an appropriate knowledge representation technique for the domain in question. To shed light on these properties, a comparative analysis on some conventional problems is provided in general perspective.\nWe hope that current review facilitates a better understanding of the different streams of research on this topic and underlies proper usage of LCS in many applications. In addition, since knowledge representation is a typical component of any rule based system like XCS, the current research can be a roadmap in other rule based systems to represent the problem regularities properly. Furthermore, we anticipate this survey to be interest to the LCS researchers and practitioners since it provides a guideline for choosing a proper knowledge representation technique for a given problem and also opens up new streams of research on this topic."}], "references": [{"title": "Learning Classifier Systems: Looking Back and Glimpsing Ahead", "author": ["J Bacardit", "E Bernad\u00f3-Mansilla", "MV Butz"], "venue": "In Learn Clasif Sys From Found to Appl,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Accuracy-based learning classifier systems: Models, analysis and applications to classification tasks", "author": ["E Bernad\u00f3-Mansilla", "M Garrell J"], "venue": "Evol Comput", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "XCS and GALE: a comparative study of two learning classifier systems with six other learning algorithms on classification tasks", "author": ["E Bernad\u00f3-Mansilla", "X Llor\u00e1", "JM Garrell"], "venue": "In Fourth Inter Workshop on Learn Clasif Sys", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Neural Networks for Pattern Recognition", "author": ["CM Bishop"], "venue": "Proc of EUFIT \u201993 ELITE Found,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Some methodological issues about designing autonomous agents which learn their behaviors: the ELF experience", "author": ["A Bonarini"], "venue": "R. Trappl (ed.) Cybern and Sys Res", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Evolutionary learning of fuzzy rules: Competition and cooperation", "author": ["A Bonarini"], "venue": "Fuzzy Model: Paradig and Pract,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Reinforcement distribution for fuzzy classifiers: A methodology to extend crisp algorithms", "author": ["A Bonarini"], "venue": "IEEE Intern Conf on Evol Comp,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "An introduction to learning fuzzy classifier systems", "author": ["A Bonarini"], "venue": "In Learn Clasif Sys From Found to Appl,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Fuzzy and crisp representations of real valued input for learning classifier systems", "author": ["A Bonarini", "C Bonacina", "M Matteucci"], "venue": "In Learn Clasif Sys From Found to Appl,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "FIXCS: a Fuzzy Implementation of XCS", "author": ["A Bonarini", "M Matteucci"], "venue": "FUZZ-IEEE", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Learning Fuzzy Classifier Systems: Architecture and Exploration Issues", "author": ["A Bonarini", "M Matteucci", "M Restelli"], "venue": "Intern J on Artif Intell Tools", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Learning fuzzy classifier systems for multi-agent coordination", "author": ["A Bonarini", "V Trianni"], "venue": "Info Sci", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Generalization in reinforcement learning: Safely approximating the value function", "author": ["JA Boyan", "AW Moore"], "venue": "Adv in Neural Info Proc Sys,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "Applications of learning classifier systems. Stud in fuzziness and soft comp", "author": ["L Bull"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Learning classifier systems in data mining", "author": ["L Bull", "B Bernad\u00f3-Mansilla", "JH Holmes"], "venue": "Stud in Comp Intell,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Accuracy-based neuro and neuro-fuzzy classifier systems", "author": ["L Bull", "T O\u2019Hara"], "venue": "In GECCO", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Towards distributed adaptive control for road traffic junction signals using learning classifier systems", "author": ["L Bull", "A Sha\u2019Aban", "A Tomlinson", "J Addison", "B Heydecker"], "venue": "Appl of Learn Clasif Sys,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Kernel-based, ellipsoidal conditions in the real-valued XCS classifier system", "author": ["MV Butz"], "venue": "Proc of the 2005 conf on GECCO,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Hyper-ellipsoidal conditions in XCS: rotation, linear approximation, and solution structure", "author": ["MV Butz", "P Lanzi", "SW Wilson"], "venue": "In Proc of the 8th GECCO, NY,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Function Approximation with XCS: Hyperellipsoidal Conditions, Recursive Least Squares, and Compaction", "author": ["MV Butz", "P Lanzi", "SW Wilson"], "venue": "IEEE Trans on Evol Comp", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "An Algorithmic Description of XCS", "author": ["MV Butz", "SW Wilson"], "venue": "Adv in Learn Clasif Sys,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Fuzzy xcs: An accuracy based fuzzy classifier system, in Proc of the XII Cong Esp sobre Tecnologiay Logica Fuzzy", "author": ["J Casillas", "V Carse", "L Bull"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Reinforcemen learning by an accuracy-based fuzzy classifier system with realvalued output", "author": ["J Casillas", "V Carse", "L Bull"], "venue": "Proc I inter Workshop on Genet Fuzzy Sys,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Fuzzy-XCS: A Michigan Genetic Fuzzy System", "author": ["J Casillas", "B Carse", "L Bull"], "venue": "IEEE Trans on Fuzzy Sys", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "GCS with Real-Valued Input, Bio-insp Model of Cognetiv Task, pp 488-497", "author": ["L Cielecki", "O Unold"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Be real! XCS with continuous-valued inputs", "author": ["H Dam", "H Abbass", "C Lokan"], "venue": "In Proc of the 2005 workshops on GECCO, NY,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Neural-Based Learning Classifier Systems", "author": ["H Dam", "H Abbass", "C Lokan", "X Yao"], "venue": "Transactions on Knowledge and Data Engineering,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Can Evolutionary Computation Handle Large Dataset", "author": ["H Dam", "K Shafi", "H Abbass"], "venue": "Tech Rep AI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Learning with genetic algorithms: An overview", "author": ["KA De Jong"], "venue": "Mach Learn,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1988}, {"title": "Suppression of excess fuzziness using multiple fuzzy classifier systems", "author": ["T Furuhashi", "K Nakaoka", "Y Uchikawa"], "venue": "In Proc of the 3th IEEE Intern Conf on Fuzzy Sys,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1994}, {"title": "Genetic Algorithms in Search, Optimization, and Machine Learning, Addision-Wesley Publishing Company, INC", "author": ["DE Goldberg"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1989}, {"title": "Messy genetic algorithms. Motivation, analysis and first results", "author": ["DE Goldberg", "B Korb", "K Deb"], "venue": "Complex Sys,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1989}, {"title": "Learning classifier system ensemble for data mining", "author": ["Y Gao", "J Huang", "H Rong", "D Gu"], "venue": "Proc of the 2005 workshops on GECCO,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2005}, {"title": "LCSE: Learning Classifier System Ensemble for Incremental Medical Instances", "author": ["Y Gao", "J Huang", "H Rong", "D Gu"], "venue": "Learn Clasif Sys,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Ensemble Learning Classifier System and Compact Ruleset", "author": ["Y Gao", "L Wu", "J Huang"], "venue": "Simu Evol and Learn,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "An Evolutionary Function Approximation Approach to Compute Prediction in XCSF", "author": ["A. Hamzeh", "Rahmani A"], "venue": "Mach learn : ECML,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2005}, {"title": "A New Architecture of XCS to Approximate Real-Valued Functions Based on High Order Polynomials Using Variable-Length GA", "author": ["A Hamzeh", "A Rahmani"], "venue": "In III Intern Conf on Nat Comp", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Rule-based systems", "author": ["F Hayes-Roth"], "venue": "Commun of the ACM", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1985}, {"title": "Escaping brittleness: the possibilities of general-purpose learning algorithms applied to parallel rule-based systems. Mach learn an artif intell approach", "author": ["JH Holand"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1986}, {"title": "Cognitive systems based on adaptive algorithms Reprinted in: Evolutionary computation. The fossil record", "author": ["JH Holland", "JS Reitman"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1978}, {"title": "Self-adaptive constructivism in Neural XCS and XCSF", "author": ["DG Howard", "L Bull", "PL Lanzi"], "venue": "Proc of GECCO", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "A self-adaptive neural learning classifier system with constructivism for mobile robot control. Parallel Problem Solving from Nature ", "author": ["J Hurst", "L Bull"], "venue": "PPSN VIII,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "Performance evaluation of fuzzy classifier systems for multidimensional pattern classification problems", "author": ["H Ishibuchi", "T Nakashima", "T Murata"], "venue": "IEEE Trans on Sys, Man, and Cybern-Part B: Cybernetics,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1999}, {"title": "Neuro-fuzzy Modeling and Control", "author": ["JS Jang", "CT Sun"], "venue": "Proc of the IEEE", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1995}, {"title": "Real-valued negative selection algorithm with variable-sized detectors", "author": ["Z Ji", "D Dasgupta"], "venue": "GECCO", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2004}, {"title": "Genetic Programming: on the Programming of Computers by Means of Natural Selection", "author": ["R Koza J"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1992}, {"title": "The evolution of size and the shape", "author": ["WB Langton", "T Soule", "R Poli", "JA Foster"], "venue": "Adv in Genet Program", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1996}, {"title": "Adding Memory to XCS", "author": ["PL Lanzi"], "venue": "Proc of the IEEE Conf on Evol Comp,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1998}, {"title": "Extending the representation of classifier conditions part I: from binary to messy coding", "author": ["PL Lanzi"], "venue": "Proc of GECCO", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1999}, {"title": "Mining interesting knowledge from data with the XCS classifier system", "author": ["PL Lanzi"], "venue": "Proc of GECCO", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "Generalization in the XCS Classifier Systems with Symbolic Representation", "author": ["PL Lanzi"], "venue": "Technical Report 01, Dipartimento di Elettronicae Informazione", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2001}, {"title": "XCS with stack-based genetic programming", "author": ["PL Lanzi"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2003}, {"title": "Learning classifier systems: then and now", "author": ["PL Lanzi"], "venue": "Evol Intell", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2008}, {"title": "Classifier prediction based on tile coding", "author": ["PL Lanzi", "D Loiacono", "SW Wilson", "DE Goldberg"], "venue": "GECCO 2006, ACM Press,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2006}, {"title": "Classifier systems that compute action mappings", "author": ["L Lanzi P", "D Loiacono"], "venue": "In Proc of GECCO", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2007}, {"title": "Extending the representation of classifier conditions part II: From messy coding to S-expressions", "author": ["PL Lanzi", "A Perrucci"], "venue": "Proc of GECCO", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1999}, {"title": "Using convex hulls to represent classifier conditions", "author": ["L Lanzi P", "SW Wilson"], "venue": "In Proc of GECCO", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2006}, {"title": "Inducing partially-defined instances with evolutionary algorithms", "author": ["X Llor\u00e1", "JMG Guiu"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2001}, {"title": "Towards better than human capability in diagnosing prostate cancer using infrared spectroscopic imaging", "author": ["X Llor\u00e1", "R Reddy", "B Matesic", "R Bhargava"], "venue": "In Proc of GECCO", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2007}, {"title": "Evolving Neural Networks for Classifier Prediction with XCSF. In ECAI Neural Networks for Classifier Prediction with XCS, pp 36-40", "author": ["D Loiacono", "P Lanzi"], "venue": null, "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2006}, {"title": "Intrusion detection using a linguistic hedged fuzzy-XCS classifier system", "author": ["JG Marin-Bl\u00e1zquez", "G Martnez-Prez"], "venue": "Soft Comp", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2008}, {"title": "A Linguistic Fuzzy-XCS classifier system", "author": ["JG Marin-Bl\u00e1zquez", "G Martinez Perez", "M Gil Perez"], "venue": "In Fuzzy Sys Conf, IEEE Int,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2007}, {"title": "A fuzzy-XCS classifier system with linguistic hedges", "author": ["JG Marin-Bl\u00e1zquez", "Q Shen"], "venue": "Comp Intell Res", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2008}, {"title": "A first order logic classifier system", "author": ["D Mellor"], "venue": "Proc of GECCO", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2005}, {"title": "Extending XCS with Representation in First Order Logic", "author": ["D Mellor"], "venue": "Proc IWLCS", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2006}, {"title": "A Learning Classifier System Approach to Relational Reinforcement Learning", "author": ["D Mellor"], "venue": "Learn Clasif Sys,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2008}, {"title": "On Evolution, Search, Optimization, Genetic Algorithms and Martial Arts: Towards Memetic Algorithms", "author": ["P Moscato"], "venue": "Caltech Concur Comp Program", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1989}, {"title": "A study on apportionment of credits of fuzzy classifier system for knowledge acquisition in large scale systems", "author": ["K Nakaoka", "T Furuhashi", "Y Uchikawa"], "venue": "In Proc of 3th IEEE Int Conf on Fuzzy Syst,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 1994}, {"title": "Reinforcement learning in the fuzzy classifier system", "author": ["Y Nomura", "K Ikebukuro", "K Yokoyama", "T Takeuchi", "Y Arikawa", "S Ohno", "I Karube", "M Valenzuela-Rend\u00f3n"], "venue": "Expert Sys with Appl", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 1998}, {"title": "A Memetic Accuracy-based Neural Learning Classifier System", "author": ["T O\u2019Hara", "L Bull"], "venue": "Proc of the IEEE Congr on Evol Comp,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2005}, {"title": "Fuzzy-UCS: preliminary results", "author": ["A Orriols-Puig", "J Casillas", "E Bernad\u00f3-Mansilla"], "venue": "Proc of GECCO", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2007}, {"title": "Fuzzy-UCS: a Michigan-style learning fuzzy-classifier system for supervised learning", "author": ["A Orriols-Puig", "J Casillas", "E Bernad\u00f3-Mansilla"], "venue": "IEEE Trans on Evol Comp", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2008}, {"title": "Approximate versus linguistic representation in fuzzyUCS", "author": ["A Orriols-Puig", "J Casillas", "E Bernad\u00f3-Mansilla"], "venue": "Proc of the 2008 Int Workshop on Hybrid Artif Intell Sys HAIS\u201908,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2008}, {"title": "A new approach to fuzzy classifier systems", "author": ["A Parodi", "P Bonelli"], "venue": "Proc of the 5th Int Conf on Genet Algorithms,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1993}, {"title": "Adaptive Learning of a Robot Arm", "author": ["MJ Patel", "M Dorigo"], "venue": "Evol Comp, AISB workshop selected papers, LNCS 865,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 1994}, {"title": "Stack-based genetic programming", "author": ["T Perkis"], "venue": "Proc of the 1994 IEEE World Congr on Comp Intell", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 1994}, {"title": "Discrete dynamical genetic programming in XCS", "author": ["R Preen", "L Bull"], "venue": "Proc of GECCO", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2009}, {"title": "Representational difficulties with classifier systems", "author": ["D Schuurmans", "J Schaeffer"], "venue": "Proc of the 3rd Int Conf on Genet Algorithms", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 1989}, {"title": "Learning Classifier Systems: A Survey", "author": ["O Sigaud", "SW Wilson"], "venue": "Soft Comp - A Fusion of Foundations, Methodologies and Applications", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2007}, {"title": "A learning system based on genetic adaptive algorithms", "author": ["SF Smith"], "venue": "Doctoral thesis,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 1980}, {"title": "Flexible learning of problem solving heuristics through adaptive search", "author": ["SF Smith"], "venue": "Int Joint Conf on Artif Intell,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 1983}, {"title": "Code growth in genetic programming", "author": ["T Soule", "JA Foster", "J Dickinson"], "venue": "Genet Program Proc First Annual Conf ,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 1996}, {"title": "For real! XCS with continuousvalued inputs", "author": ["C Stone", "L Bull"], "venue": "Evol Comp,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2003}, {"title": "Learning classifier systems for multiobjective robot control", "author": ["M Studley"], "venue": "Ph.D. thesis, Faculty of Computing, Engineering and Mathematics University of theWest of England. Learning Classifier Systems Group Technical Report UWELCSG06-005", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2005}, {"title": "X-tcs: accuracy-based learning classifier system robotics", "author": ["M Studley", "L Bull"], "venue": "Proc of IEEE Congr on Evol Comp,", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2005}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["RS Sutton"], "venue": "Neural Info Processing Sys,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 1996}, {"title": "Reinforcement learning-an introduction", "author": ["RS Sutton", "AG Barto"], "venue": "Arch of Rchives of Control Sci", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 1998}, {"title": "Grammar-Based Classifier System for Recognition of Promoter Regions", "author": ["O Unold"], "venue": "Adapt and Nat Comput Algorithms,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2007}, {"title": "Paradigms, EXIT, Warsaw, pp 273-286", "author": ["Unold O", "Cielecki L (2005) Grammar-based Classifier System. Issues in Intell Sys"], "venue": "Urbanowicz RJ, Moore JH", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2009}, {"title": "The fuzzy classifier system: A classifier system for continuously varying variables", "author": ["M Valenzuela-Rend\u00f3n"], "venue": "In 4th ICGA,", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 1991}, {"title": "Application of learning classifier systems to the on-line reconfiguration of electric power distribution networks", "author": ["P Vargas", "C Filho", "FV Zuben"], "venue": "Appl of Learn Classif Systems. Studies in fuzziness and soft computing,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2004}, {"title": "891-903", "author": ["Velasco J (1998) Genetic-based on-line learning for fuzzy process control. Int J of Intell Sys", "13"], "venue": "Walter D, Mohan C", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2000}, {"title": "ZCS: a zeroth level classifier system", "author": ["SW Wilson"], "venue": "Evol Comp", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 1994}, {"title": "Classifier Fitness Based on Accuracy", "author": ["SW Wilson"], "venue": "Evol Comp", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 1995}, {"title": "2000a) Get real! XCS with continuousvalued inputs", "author": ["SW Wilson"], "venue": "Learn Clasif Sys, From Found to Appl,", "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2000}, {"title": "Mining oblique data with XCS", "author": ["SW Wilson"], "venue": "Proc of the Third Int Workshop (IWLCS-2000),", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2000}, {"title": "Compact Rulesets from XCSI", "author": ["SW Wilson"], "venue": "Revised Papers from the 4th Int Workshop on Adv in Learn Clasif Sys,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2001}, {"title": "Classifiers that approximate functions", "author": ["SW Wilson"], "venue": "Natural Comp", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2002}, {"title": "Classifier Conditions Using Gene Expression Programming", "author": ["SW Wilson"], "venue": "IWLCS 2007,", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2008}, {"title": "Portfolio allocation using XCS experts in technical analysis, market conditions and options market", "author": ["SY Wong", "S Schulenburg"], "venue": null, "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2007}, {"title": "Applying the XCS Learning Classifier System to Continuous-Valued Data-mining Problems. Technical Report UWELCSG05-001, Learning Classifier Systems Group, University of the West", "author": ["D Wyatt"], "venue": null, "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2004}, {"title": "Outline of a new approach to the analysis of complex systems and decision processes", "author": ["L Zadeh"], "venue": "IEEE Trans on Sys, Man, and Cybern", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 1973}], "referenceMentions": [], "year": 2015, "abstractText": "Knowledge representation is a key component to the success of all rule based systems including learning classifier systems (LCSs). This component brings insight into how to partition the problem space what in turn seeks prominent role in generalization capacity of the system as a whole. Recently, knowledge representation component has received great deal of attention within data mining communities due to its impacts on rule based systems in terms of efficiency and efficacy. The current work is an attempt to find a comprehensive and yet elaborate view into the existing knowledge representation techniques in LCS domain in general and XCS in specific. To achieve the objectives, knowledge representation techniques are grouped into different categories based on the classification approach in which they are incorporated. In each category, the underlying rule representation schema and the format of classifier condition to support the corresponding representation are presented. Furthermore, a precise explanation on the way that each technique partitions the problem space along with the extensive experimental results is provided. To have an elaborated view on the functionality of each technique, a comparative analysis of existing techniques on some conventional problems is provided. We expect F. Shoeleh Computer Science and Engineering Dept. Shiraz University, Shiraz, Iran. E-mail: shoeleh@cse.shirazu.ac.ir M. Majd E-mail: majd@cse.shirazu.ac.ir A. Hamzeh E-mail: ali@cse.shirazu.ac.ir S. Hashemi E-mail: s hashemi@shirazu.ac.ir this survey to be of interest to the LCS researchers and practitioners since it provides a guideline for choosing a proper knowledge representation technique for a given problem and also opens up new streams of research on this topic.", "creator": "LaTeX with hyperref package"}}}