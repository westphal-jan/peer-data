{"id": "1506.05070", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "Reservoir Characterization: A Machine Learning Approach", "abstract": "reservoir characterization ( rc ) accounting can be defined as the act of building through a reservoir measurement model that incorporates all the characteristics of the reservoir that are pertinent to manifest its ability to simultaneous store refined hydrocarbons smoothly and essentially also to simultaneously produce them. it modeling is a difficult problem due owing to non - repeating linear and heterogeneous reservoir subsurface properties and associated with a surprising number scales of generally complex tasks such as data table fusion, deep data interval mining, formulation techniques of the knowledge base, and handling techniques of finding the uncertainty. this present simulation work here describes the development paradigm of imaging algorithms to obtain visually the functional relationships between predictor seismic attributes defined and target lithological critical properties. single seismic attributes are sometimes available nightly over a study area with lower vertical aspect resolution. conversely, well referenced logs and lithological defining properties are still available appearing only at chosen specific well locations ranging in a reservoir study facility area comparable with unexpectedly high vertical resolution. sand fraction, which represents drops per unit sand drop volume within the rock, generally has a balanced brightness distribution between almost zero solution to unity. the reference thesis addresses finally the three issues critical of handling the unstable information content yielding mismatch variance between predictor and predictable target structural variables and below proposes regularization analyses of correlated target trait property composition prior to later building a prediction model. in this thesis, two artificial data neural functional network ( et ann ) based modelling frameworks are proposed to visually model estimated sand interval fraction from coordinating multiple parallel seismic attributes without differences and with each well tops critical information respectively. the performances of the frameworks are quantified in static terms of correlation coefficient, integral root mean through square error, absolute error error mean, etc.", "histories": [["v1", "Mon, 15 Jun 2015 16:20:23 GMT  (2323kb)", "http://arxiv.org/abs/1506.05070v1", "Supervisors: Prof. Aurobinda Routray and Prof. William K. Mohanty"], ["v2", "Wed, 22 Jul 2015 16:09:33 GMT  (3178kb)", "http://arxiv.org/abs/1506.05070v2", "Supervisors: Prof. Aurobinda Routray and Prof. William K. Mohanty"]], "COMMENTS": "Supervisors: Prof. Aurobinda Routray and Prof. William K. Mohanty", "reviews": [], "SUBJECTS": "cs.CE cs.LG", "authors": ["soumi chaki"], "accepted": false, "id": "1506.05070"}, "pdf": {"name": "1506.05070.pdf", "metadata": {"source": "CRF", "title": "RESERVOIR CHARACTERIZATION: A MACHINE LEARNING APPROACH", "authors": ["Soumi Chaki", "William K. Mohanty"], "emails": [], "sections": [{"heading": null, "text": "RESERVOIR CHARACTERIZATION: A MACHINE\nLEARNING APPROACH\nThesis submitted to the\nIndian Institute of Technology, Kharagpur\nfor award of the degree\nof\nMaster of Science (by Research) by\nSoumi Chaki\nUnder the guidance of\nProf. Aurobinda Routray\nProf. William K. Mohanty\nDEPARTMENT OF ELECTRICAL ENGINEERING\nINDIAN INSTITUTE OF TECHNOLOGY, KHARAGPUR\nApril 2015\n\u00a9 2015 Soumi Chaki. All rights reserved.\nCERTIFICATE OF APPROVAL\nCertified that the thesis entitled Reservoir Characterization: A Machine Learning\nApproach submitted by Soumi Chaki, to the Indian Institute of Technology,\nKharagpur, for the award of the degree of Master of Science (by Research) has been\naccepted by the external examiners and that the student has successfully defended the\nthesis in the viva-voce examination held today.\nSignature:\nName:\n(Member of the DAC)\nSignature:\nName:\n(Member of the DAC)\nSignature:\nName:\n(Member of the DAC)\nSignature:\nName:\n(Superviser)\nSignature:\nName:\n(Joint Superviser)\nSignature:\nName:\n(External Examiner)\nSignature:\nName:\n(Chairman)\nCERTIFICATE\nThis is to certify that this thesis entitled Reservoir Characterization: A Machine Learning Approach submitted by Soumi Chaki, to the Indian Institute of Technology, Kharagpur, is a record of bona fide research work carried out under our supervision and is worthy of consideration for award of the degree of Master of Science (by Research) of the Institute.\n__________________________ __________________________\nPlace: IIT Kharagpur\nDate:\nSupervisor\nAurobinda Routray\nDepartment of Electrical Engineering\nIndian Institute of Technology, Kharagpur\nKharagpur, West Bengal\nIndia - 721302\nDepartment of Electrical Engineering\nIndian Institute of Technology, Kharagpur\nKharagpur, India 721302.\nSupervisor\nWilliam K. Mohanty\nDepartment of Geology and Geophysics\nIndian Institute of Technology, Kharagpur\nKharagpur, West Bengal\nIndia - 721302\nDECLARATION\nI certify that\na. The work contained in the thesis is original and has been done by myself under the\ngeneral supervision of my supervisor(s).\nb. The work has not been submitted to any other Institute for any degree or diploma.\nc. I have followed the guidelines provided by the Institute in writing the thesis.\nd. I have conformed to the norms and guidelines given in the Ethical Code of Conduct of\nthe Institute.\ne. Whenever I have used materials (data, theoretical analysis, and text) from other sources,\nI have given due credit to them by citing them in the text of the thesis and giving their\ndetails in the references.\nf. Whenever I have quoted written materials from other sources, I have put them under\nquotation marks and given due credit to the sources by citing them and giving required\ndetails in the references.\nSoumi Chaki\nACKNOWLEDGEMENTS\nThis thesis marks the end of a long and eventful journey during which there were many\npeople whom I would like to acknowledge for their encouragement and support. I am grateful\nto my supervisors, Prof. Aurobinda Routray, and Prof. William K. Mohanty, for their\nconstant support throughout the research work. Their advice and constructive criticism have\nhelped the thesis take its present shape.\nI would like to thank the members of the DAC, Prof. A. K. Deb, Prof. P. K. Dutta, Prof.\nM. Jenamani for their suggestions and comments during the presentations and throughout\nthe tenure of the work. I am thankful to Head, Department of Electrical Engineering, IIT\nKharagpur, for providing me the facilities to carry out my research work. I am also grateful\nto all other faculty members of the Department of Electrical Engineering, IIT Kharagpur for\nthe help received during my research. I also extend my thanks to the staffs of Central Library,\nIIT Kharagpur, for their assistance in locating books in a library which is one of the largest\nof its type.\nI would like to thank Geodata Processing and Interpretation Centre (GEOPIC),\nDehradun, Oil and Natural Gas Corporation Limited (ONGC) for collaborating with us for\nthe research project. My special thanks to Mrs. Puja Prakash, Mr. S. K. Das and Mr. P.K.\nChaudhuri. I would like to acknowledge the help from Mr. Akhilesh Kumar Verma from\nDepartment of Geology and Geophysics, IIT Kharagpur.\nI would like to appreciate the help and support of all my friends and colleagues of Real\nTime Embedded System Lab., Systems and Information Lab., Department of Electrical\nEngineering and Centre for Railway Research Lab.\nFinally, I would like to thank my family who always encouraged me for higher studies\nand made me believe that knowledge is more powerful than wealth. My deepest gratitude\ngoes to my parents for their unflagging love and support throughout my life.\nPlace: IIT Kharagpur\nDate: Soumi Chaki\nAbstract\n\u2018Reservoir Characterization (RC)\u2019 can be defined as the act of building a reservoir model that incorporates all the characteristics of the reservoir that are pertinent to its ability to store hydrocarbons and also to produce them. It is a difficult problem due to non-linear and heterogeneous subsurface properties and associated with a number of complex tasks such as data fusion, data mining, formulation of the knowledge base, and handling of the uncertainty.\nThis present work describes the development of algorithms to obtain the functional relationships between predictor seismic attributes and target lithological properties. Seismic attributes are available over a study area with lower vertical resolution. Conversely, well logs and lithological properties are available only at specific well locations in a study area with high vertical resolution. If a functional relationship can be calibrated between seismic signals and lithological properties at available well locations, then distribution of these properties across the study area can be predicted from available seismic information. Depending on the distribution of the lithological properties, a dataset can be classified into two categories \u2013 balanced and imbalanced. Sand fraction, which represents per unit sand volume within the rock, has a balanced distribution between zero to unity. On the other hand, water saturation, oil saturation etc. has an imbalanced distribution skewed at one and zero respectively. The investigation about the sand fraction (balanced distribution) variation over the study area has been attempted as a prediction problem; whereas, the distribution of water saturation (balanced distribution) has been approached as a classification (Class low/ Class high) problem in this work. The thesis addresses the issues of handling the information content mismatch between predictor and target variables and proposes regularization of target property prior to building a prediction model. In this thesis, two Artificial Neural Network (ANN) based frameworks are proposed to model sand fraction from multiple seismic attributes without and with well tops information respectively. The performances of the frameworks are quantified in terms of Correlation Coefficient (CC), Root Mean Square Error (RMSE), Absolute Error Mean (AEM), etc.\nAfter successful completion of sand fraction prediction, a one-class classification framework based on Support Vector Data Description (SVDD) is proposed to classify water saturation from well logs. The designed framework is modified to include seismic variables as predictor attributes to obtain the variation of water saturation over the study area. In other words, the class labels (Class low/Class high) of water saturation belonging to a well location can be predicted from seismic attributes by the modified classification based framework. The proposed frameworks have outperformed other supervised classification algorithms in terms of g-metric means and program execution time (in seconds).\nKeywords: Information content, entropy, Normalized Mutual Information (NMI), Artificial Neural Network (ANN), Support Vector Data Description (SVDD), regularization, wavelets, Empirical Mode Decomposition (EMD), Fourier Transform (FT), Wavelet Decomposition (WD), Sand Fraction (SF), g-metric means, Root Mean Square Error (RMSE), Absolute Error Mean (AEM), Correlation Coefficient (CC), Scatter Index (SI), well tops, sand fraction, water saturation.\nContents Title Page\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026 i\nCertificate of Approval\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026..\u2026\u2026\u2026\u2026\u2026\u2026\u2026 iii\nCertificate by the Supervisor \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026. v\nDeclaration \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026............................... vii\nAcknowledgement \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.\u2026....... ix\nAbstract \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026....\u2026..... xi\nList of Symbols \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.\u2026\u2026\u2026..\u2026..... xv\nList of Abbreviations \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. xvii\nList of Figures \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026......... xix\nList of Tables \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026... xxi\nChapter 1. Introduction ............................................................................................... 1\n1.1 Literature Review ............................................................................................ 1 1.2 Research Issues ............................................................................................... 5 1.3 Objectives ....................................................................................................... 5 1.4 Contributions of the Thesis ............................................................................. 6 1.5 Organization of the Thesis............................................................................... 6\nChapter 2. Pre-processing ........................................................................................... 7\n2.1 Data Description ............................................................................................. 7 2.1.1 Study Area .............................................................................................. 7 2.1.2 Preparation of Data .................................................................................. 8 2.2 Pre-Processing ................................................................................................ 8 2.2.1 Signal Reconstruction .............................................................................. 8 2.2.2 Data Regularization and Re-sampling .................................................... 10\n2.3 Conclusion .................................................................................................... 18 Chapter 3. Prediction of Sand Fraction .................................................................... 21\n3.1 Artificial Neural Network (ANN) .................................................................. 22 3.2 Prediction of Sand Fraction from Seismic Attributes without Well Tops Information .................................................................................................................. 27 3.2.1 Pre-processing ....................................................................................... 29 3.2.2 Model Building and Validation .............................................................. 29 3.2.3 Post Processing...................................................................................... 32 3.2.4 Discussion ............................................................................................. 33\n3.3 Prediction of Sand Fraction from Seismic Attributes with Well Tops Guidance \u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026.. 34\n3.3.1 Methodology based on MANN Concept ................................................ 34 3.3.2 The Proposed Framework ...................................................................... 39\n3.3.2.1 Pre-processing .................................................................................. 39 3.3.2.2 Model Building and Validation ......................................................... 40 3.3.2.3 Volumetric Prediction ....................................................................... 43 3.3.2.4 Post Processing ................................................................................. 46\n3.3.3 Discussion ............................................................................................ 47 3.4 Conclusion ................................................................................................... 47\nChapter 4. Classification of Water Saturation.......................................................... 49\n4.1 Support Vector Data Description (SVDD) .................................................... 49 4.2 Development of a Framework to Classify Water Saturation from Well Logs 50\n4.2.1 Data Description ................................................................................... 50 4.2.2 Proposed Classification Framework ...................................................... 51\n4.2.2.1 Data Preparation ............................................................................... 52 4.2.2.2 Preliminary Analysis ........................................................................ 53 4.2.2.3 Training and Testing ........................................................................ 54 4.2.3 Discussion ............................................................................................ 56 4.3 Development of a Framework to Classify Water Saturation from Seismic Attributes. .................................................................................................................... 57 4.3.1 Data Description ................................................................................... 57 4.3.2 Proposed Classification Framework ...................................................... 57\n4.3.2.1 Data Preparation ............................................................................... 58 4.3.2.2 Preliminary Analysis ........................................................................ 60 4.3.2.3 Training and Testing ........................................................................ 60 4.3.2.4 Volumetric Classification and Visualization ..................................... 63\n4.4 Conclusion ................................................................................................... 64 Chapter 5. Conclusion and Future Scope ................................................................. 65\n5.1 Dissemination out of this Work .................................................................... 66 5.2 Future Scope ................................................................................................ 66\nReferences ....................................................................................................................... 67\nList of Symbols\nSymbol Description\nX A variable\nX Mean of the X\n X Standard deviation of the X\n( )H X Entropy\n( )ip x Probability of X having the thi value ix in the dataset ( | )H X A The conditional entropy of X after A has been observed\n( ; )I X A Mutual information between X and A\n( ; )NMI X A Normalized mutual information\n( )x t A signal\n( )X k Fourier Transform of ( )x t\n( )t a mother wavelet\n( )t scaling function\n( )la k the approximate coefficients at level l\n( )ld k the detailed coefficients at level l\nw Window size of a spatial filter\nPacc sensitivity\nNacc specificity g G-metric means\n( , )i jK x x Kernel function\nL Lagrangian function\nList of Abbreviations\nAbbreviation Description SF Sand Fraction\nMI Mutual Information NMI Normalized Mutual Information ANN Artificial Neural Network\nFT Fourier Transform WD Wavelet Decomposition db4 Daubechies 4 wavelet\nEMD Empirical Mode Decomposition IMF Intrinsic Mode Functions\nRMSE Root Mean Square Error AEM Absolute Error Mean CC Correlation Coefficient SI Scatter Index\nSVM Support Vector Machines SVDD Support Vector Data Description PSD Power Spectral Density BPNN Back Propagation Neural Network MANN Modular Artificial Neural Network\nSCG Scaled Conjugate Gradient GR Gamma Ray content\nRHOB Bulk Density RT Deep Resistivity DT P-sonic RM Medium Resistivity RS Shallow Resistivity\nNPHI Neutron Porosity SP Spontaneous Potential AI Acoustic Impedance\nAPI American Petroleum Institute IDWT Inverse Discrete Wavelet Transform EEG Electroencephalography SOM Self-Organizing Map\nList of Figures\nFig. 2-1: Seismic and sand fraction signals along the well A .............................................. 9 Fig. 2-2: Regularization based on FT to reconstruct sand fraction signal along Well A ..... 12 Fig. 2-3: Demonstration of wavelet decomposition of a signal for level 3 ......................... 14 Fig. 2-4: Regularization based on WD to reconstruct the sand fraction along Well A ........ 15 Fig. 2-5: Regularization based on EMD to reconstruct sand fraction signal along Well A 18 Fig. 3-1: Structure of an ANN with single hidden layer n-input single output ................... 23 Fig. 3-2: Training framework of an ANN ......................................................................... 23 Fig. 3-3: The prediction framework to benchmark regularization stage ............................. 28 Fig. 3-4: Visualization of amplitude at inline 136 ............................................................. 31\nFig. 4-12: Bar plot describing comparative performance analysis in terms of g-metric means ................................................................................................................................. 62 Fig. 4-13: Bar plot describing comparative performance analysis in terms of program\nexecution time .......................................................................................................... 63 Fig. 4-14: Seismic impedance variation at a particular inline ............................................ 63 Fig. 4-15: Water saturation level variation at a particular inline ........................................ 64\nList of Tables\nTable 2-1: Entropy (in bit) of PSD of signals for Well A .................................................. 13 Table 2-2: NMI among predictors and target sand fraction for Well A .............................. 13 Table 3-1: Statistics of validation performance (Two predictors) ...................................... 31 Table 3-2: Statistics of validation performance (Three predictors) .................................... 31 Table 4-1: Performance comparison of classifiers in terms of g-metric mean .................... 55 Table 4-2: Performance comparison of classifiers in terms of program execution time ..... 55 Table 4-3: G-metric mean comparison among the proposed framework and other classifiers ................................................................................................................................. 62 Table 4-4 : Program execution time comparison among the proposed framework and other\nclassifiers .................................................................................................................. 62\nIntroduction\n1\nChapter 1. Introduction The act of building a reservoir model that incorporates all the characteristics of the reservoir to store hydrocarbons and also to produce them is termed as \u2018Reservoir Characterization (RC)\u2019 [1]. The non-linear and heterogeneous physical properties of the subsurface make reservoir characterization a difficult task. The initial step of this characterization is prediction of reservoir characteristics (such as sand fraction, shale fraction, porosity, permeability, fluid saturation, water saturation etc.) or class variations of these properties from well logs and seismic attributes. Prediction of petrophysical properties is associated with a number of complex tasks such as data fusion (i.e. integration of data from various sources), data mining (i.e. information retrieval after analysing those data), formulation of the knowledge base, and handling of the uncertainty. The applications of advanced statistical, machine learning and pattern recognition techniques to such problems have received considerable interest among the researchers in oil-gas sector [2], [3]. The objective of these types of studies is to identify potential zone for drilling a new well [4]. The fundamental characteristics of a reservoir system are typically distributed spatially in a non-uniform and non-linear manner. Extraction of lithological information from available datasets is an important step in the reservoir characterization process. Since there is no direct measurement for the lithological parameters, they are to be computed from other geophysical logs [5] or seismic attributes [6]. This process also requires repeated intervention of the experts for fine tuning the prediction results. Standard regression methods are not suitable for this problem due to the high degree of the unknown nonlinearity. The problem is further complicated because of uncertainties associated with lithological units. In this context, Artificial Neural Network (ANN) and its variants with Fuzzy Logic are considered to be useful tools to establish a mapping between lithological and well log properties [7]\u2013[10].\nFurther, it is important to characterize how 3D seismic information is related to production, lithology, geology, and well log data. It is suggested that the use of 3D seismic data along with well logs can provide better insights while extrapolating reservoir properties away from the existing wells [11], [12]. ANN [13], Adaptive Neuro-Fuzzy Inference System (ANFIS) [14], Support Vector Machine (SVM) [15], type-2 Fuzzy Logic system [16], and hybrid systems [17], [18] are some the efficient machine learning tools used in the field of reservoir characterization. Now a days, one of the challenging problems for the petroleum industry is to enhance oil recovery from naturally occurring complex reservoir systems. Therefore, it is important to identify the patterns of the characteristic distributions of the pertinent reservoir parameters in the subsurface."}, {"heading": "1.1 Literature Review", "text": "Hydrocarbons migrate from source rock through porous medium to reach reservoir rock for temporary preservation [19]. Finally, the mobile hydrocarbons get seized in the cap rocks. As such, the identification of hydrocarbon\u2013enriched\u2013formations by characterization of each layer\nIntroduction\n2\nin the borehole is of enormous necessity to the explorers. Recognition of a potential hydrocarbon\u2013enriched zone in a prospective oil exploration field can be carried out using well logs which categorize layers into different sections such as dry, water containing, and hydrocarbon bearing layers. The lithological properties in the neighborhood of a borehole can be known from well logs, whereas these remain unknown and difficult to estimate away from the wells. In such cases, available seismic attributes can be used as a guidance to predict lithological information at all traces of the area of interest [11]. Well logs and seismic attributes are integrated at available well locations to design a reservoir model with the least uncertainty. However, mapping between lithological properties and seismic attributes is governed with nonlinear relationship and mismatch in information content. Ahmadi et al. [20] explored that nonlinear problems can be approached using state-of-art computer\u2013based methods like expert systems [21], multiple regression, neural networks [22], Neuro-fuzzy Systems [23] etc. ANN is widely used to model single or multiple target properties from predictor variables in different research domains. It has been found from literature that ANN is a natural choice of researchers and engineers because of its prediction and generalization capability. For example, it is used in climatological studies [24], ocean engineering [25], telecommunications [26], text recognition [27], financial time series [28], reservoir characterization [4], [10], [29]\u2013 [32], etc. A diverse dataset containing information assembled from multiple domains can be used for learning and validation of ANN. However, it has some inherent limitations. Firstly, performance of ANN is dependent on the selection of network structure and associated parameters. Secondly, training a complex, multilayered network is a time intensive process. Furthermore, a complex network trained with relatively smaller number of learning patterns may lead to overfitting, and thus, generalization capability is compromised. It is equally important to assess the possibility of modeling the target property from predictor variables using ANN or any other nonlinear modeling approach. Sometimes the model performance can be improved by applying suitable filtering techniques to the predictor/target variables in the pre-processing stage. Several studies have contributed on the performance analysis of ANN along with other machine learning algorithms to model a target variable from single or multiple predictors with respect to RC problem; however, the following aspects still remain unexplored such as:\n Design of appropriate pre-processing stages for effectiveness of machine learning algorithms\n Proper choice of structure and methods associated with selected machine learning algorithms (here, ANN model parameters- e.g. activation function type, number of hidden layers etc.)\n Suitable post-processing methods for the predicted output Modelling of petrophysical characteristics from well logs and seismic data plays a crucial role in petroleum exploration. Two major challenges are faced while interpreting and integrating different kinds of datasets (mainly, well logs and seismic data); 1) nonlinear and diverse nature of reservoir variables associated with the subsurface systems, and 2) absence of\nIntroduction\n3\nany direct relationship between seismic and well log signals from a theoretical perspective. Similarly, calibration of a functional relationship between a reservoir characteristic and predictor seismic attributes is an intricate task. Linear multiple regression and neural networks are popular among statistical techniques for reservoir modelling from well logs and seismic attributes [4], [33]. Lately, several computation intensive Artificial Intelligence methods such as ANN, neuro-fuzzy, Self-Organizing Map (SOM), committee machine and Learning Vector Quantization (LVQ) have attained recognition as the potential tools to solve nonlinear and complex problems in the domain of reservoir characterization [4], [5], [11], [29], [31], [34]\u2013 [38]. Despite the difference in theory and computation, most of the modelling algorithms are applied for the same purpose. On the contrary, a single technique can serve as a potential tool for solving different problems. For instance, ANN has been applied in several areas of science and technology for different objectives such as prediction, classification, etc. Moreover, different categories of neural network architectures are capable to solve nonlinear problems; however, as complexity of the problem increases due to enhancement in the number of inputs or the complex nature of the predictor variables, the performance of the network decreases rapidly.\nSeveral researchers have claimed that the application of modular networks and hybrid networks show better performance compared to a single algorithm [39]\u2013[44]. In particular, due to the heterogeneous nature of the subsurface system (or reservoir variables), application of a single network for complete depth range of a well may not be sufficient to achieve adequate prediction accuracy. In this context, module based networks, so-called modular artificial neural networks (MANN), are well suited to solve complex nonlinear problems. Moreover, the concept of modularity is applied in many fields to divide a complex problem into a set of relatively easier sub-problems; then, the smaller sub-problems are solved by modules; finally, the obtained results are combined to achieve the solution of the main problem [40], [45]\u2013[47]. The module-wise division is carried out based on different clusters and classes of the dataset. This modularity concept is implemented individually or along with another machine learning algorithm to solve different types of problem. Lithological information extraction from an integrated dataset is a major step in the reservoir modelling.\nMANN has been previously used in reservoir characterization domain. Fung et al., [29] used to predict petrophysical properties from a set of well logs. The smaller networks are constructed corresponding to different classes obtained from a trained LVQ network. However, seismic attributes are not considered in the study by Fung et al., [29]. A similar work has been carried out to predict permeability from multiple well logs such as spectral gamma ray, electrical resistivity, water saturation, total porosity etc. recorded from four closely spaced boreholes using modular neural network [37]. The dataset is divided into three sets\u2013 70%, 15%, and 15% for training, validation, and testing respectively. Despite improvement in the prediction results compared to a single network, this study suffers some inherent limitations. Firstly, seismic attributes are not considered as predictor variables in this study. Secondly, blind prediction of the modeled lithological property (permeability) is not carried out. Thirdly,\nIntroduction\n4\nselection of the number of networks and number of hidden layer neurons are not guided by any particular theory. The best network is finalized by trail-and-error framework. These limitations can be addressed to design a framework to model petrophysical properties from seismic attributes from a data set of diverse lithological nature along the depth.\nIn oil exploration, different lithological classes, clusters, zones of interest in terms of well tops and horizons are identified from the preliminary analysis of well logs and it is integrated with seismic data of the same region. In the past, many classifications and clustering techniques have been used to make different classes of data depending on their variability [29], [48]\u2013[50]. Recently, the concept of chaotic time series data analysis namely dynamic programming [51], synchronization methods [52], [53] are applied to assess similarity between the pattern of the well log data, and which lead towards the identification of similar zones among the wells. Generally, zonation of the logs is carried out manually by experienced geoscientists. Above nonlinear approaches are aimed to provide information of similar patches in the log data or similar zones in different wells, and hence have potential application in the reservoir characterization. In the present study, well tops are identified from a combination of well logs and accordingly different zones are marked on the log data. In the literature, it has been claimed that modular (multi-nets) systems have the advantage of being easier to understand or modify as per requirement. Geoscientists\u2019 guided zone-wise division of a well log can assist in target evaluation after training several models using zone wise divided training patterns yielding improved prediction accuracy [29], [50]. Hence, a study can be carried out on module wise prediction of a reservoir property to achieve improved performance.\nThese modelling of reservoir characteristics from seismic data and well logs are carried out using state-of-art nonlinear approaches such as ANN, Fuzzy Logic (FL), Genetic Algorithm (GA), etc. Some applications of these methods in the field of petroleum reservoir modelling are discussed in [4], [54]\u2013[56]. However, it has been observed that the accuracy in reservoir modelling can be improved using classification-based approaches [57]. The prediction of class labels of petrophysical properties from well logs and seismic attributes can be beneficial for reservoir studies. Depending on the distribution of the lithological properties, a dataset can be classified into two categories \u2013 balanced and imbalanced. For example, sand fraction i.e. per unit sand volume within the rock varies from zero to unity. On the other hand, water saturation, oil saturation etc. have imbalanced distributions skewed at one and zero respectively. Water saturation, which represents the fraction of formation water presents in the pore space, is an important reservoir characteristic. Now, it is a complex task whose performance depends on the available subsurface information. Supervised classifiers are generally selected over unsupervised clustering algorithms due to the complex nature of the problem. Nevertheless, the requirement of a complete and representative training dataset is must for accurate learning of these supervised classifiers. Supervised algorithms can be useful in case of classifying a balanced dataset. However, in case of an imbalanced dataset, the aforementioned constraints of the training dataset do not get satisfied. Moreover, the underrepresented training dataset may have several class distribution skews. Lately, the learning problems from imbalance datasets\nIntroduction\n5\nhave received interests from researchers due to existence of such dataset in \u201creal-world applications\u201d [58]\u2013[61]. Kernel-based methods have gained acceptance in classification of imbalanced dataset over other supervised classification methods, especially in remote sensing fields [62]\u2013[64]. Support vector data description (SVDD) is a latest kernel-based algorithm which has attracted attention from researchers of different fields for its ability in learning without any a priori knowledge on distribution of dataset [65]\u2013[67]. However, in the domain of reservoir characterization, the classification of an imbalanced dataset using one-class classification approach has not been attempted. Therefore, the study related to imbalanced dataset classification can be addressed in the scope of this thesis."}, {"heading": "1.2 Research Issues", "text": "From the above cited literature review, the issues pertaining to the accurate reservoir characterization are as follows:  Integration of dataset: Preparation of the master dataset combining information\nacquired from different sources prior to modelling and classification of lithological properties. For example, well logs and seismic attributes are collected by different methods with different sampling rates, resolutions. The problem of non-unique sampling of well log and seismic data, different scales of seismic, well logs and other reservoir data should also be adequately handled by developing generalized methodologies that may be independent of the target reservoir characteristic.  Thin reservoir units: Area with thin\u2013bedded stacked reservoirs (sand/shale units) makes it hard to identify the changes in the sand/shale fraction.\n Poor data quality: Data acquired from a study area with poor data quality or a limited number of well controls and seismic coverage is not helpful to carry out reservoir characterization. It is difficult to design prediction or classification based models using a poor data set. Pre-processing methodologies are to be fine-tuned to accommodate this fact. Uncertainties associated with acquired dataset also contribute to poor performance of a designed model.\n Information content: In case of designing a machine learning model to predict lithological properties from seismic inputs, a key challenge is the information content of the predictor variables. If the information content of the predictor variables is less than that of the petrophysical properties then a trade-off between the amount of information required and actual amount of retrieval possible, has to be carried out according to the information theory."}, {"heading": "1.3 Objectives", "text": "The primary objective of this work is the development of algorithms to obtain the functional relationships between predictor seismic attributes and target lithological properties (e.g. sand fraction, water saturation etc.). This can further be divided into:  Integration of the seismic and borehole datasets\nIntroduction\n6\n Addressing the issue related to the mismatch of the information content of the seismic attributes and lithological properties\n Prediction of the variation of the lithological properties from seismic attributes  Prediction of class labels of the lithological properties from seismic attributes"}, {"heading": "1.4 Contributions of the Thesis", "text": "In this thesis, sand fraction and water saturation are used as target variables to develop prediction and classification based frameworks from well logs and seismic attributes.\nThe major contributions of this thesis are as follows:  Development of a pre-processing scheme to improve the prediction capability of machine\nlearning algorithms by information filtering for prediction of a lithological property (e.g. sand fraction) from seismic attributes\n Development of a complete framework to carry out well tops guided prediction of sand fraction from seismic attributes\n Development of a classification framework to classify water saturation from well logs  Modification of the aforementioned classification framework to classify water saturation\nfrom seismic attributes"}, {"heading": "1.5 Organization of the Thesis", "text": "The thesis is organized as follows. Chapter 2 discusses a pre-processing scheme involving a regularization stage to improve the mapping between predictor seismic attributes and target lithological properties. Chapter 3 presents two frameworks to model sand fraction from multiple seismic attributes with and without well tops information respectively. Chapter 4 describes a one-class classification based framework to classify water saturation from well logs. In other words, the class labels variation of water saturation (Class low/ Class high) has been predicted from well logs. Then, a modification of the framework involving seismic attributes as predictor variables has been discussed. Chapter 5 concludes the thesis with the discussion and the future possibilities of research.\nPre-processing\n7\nChapter 2. Pre-processing This thesis describes the development of algorithms to obtain the functional relationships between predictor seismic attributes and target lithological properties. Seismic attributes are available over a study area with lower vertical resolution. Conversely, well logs and lithological properties are available only at specific well locations in a study area with high vertical resolution. If a functional relationship can be calibrated between seismic signals and lithological properties at available well locations, then distribution of these properties across the study area can be predicted from available seismic information. The thesis addresses the issues of handling the information content mismatch between predictor and target variables and proposes regularization of target property prior to building a prediction model in the pre-processing stage. In case of building a machine learning model for classification, prediction related problems, the pre-processing stage plays a crucial effect on the performance of the model. This stage is also very important to interpret the nature of the working dataset.\nThis chapter presents a pre-processing scheme to improve the prediction of a lithological property from multiple seismic attributes using machine learning and information filtering. The pre-processing framework includes signal reconstruction, data normalization, and target signal regularization. The main contribution of this research work is attributed to the aforementioned regularization scheme. The available data of lithological properties belong to the high-resolution well logs and has far more information content than the low-resolution seismic attributes. Therefore, regularization schemes based on Fourier Transform (FT), Wavelet Decomposition (WD) and Empirical Mode Decomposition (EMD) have been proposed to shape the high-resolution target lithological property for effective machine learning. The processed dataset by the proposed scheme will be used to for prediction of a lithological property from several seismic attributes."}, {"heading": "2.1 Data Description", "text": "This section describes the study area and discusses about the preparations of the dataset."}, {"heading": "2.1.1 Study Area", "text": "In this study, the working dataset has been acquired from a western onshore hydrocarbon field in India. Structurally, the field is located as a broad nosing feature; thus, housing the hydrocarbons between two major synclines. The hydrocarbon is present within a series of vertically stacked sandstone reservoirs individually separated by intervening shale. The average thickness of the sand layer is in the order of 5-6 m. The imaging of the seismic data and mapping between seismic attributes and well logs are difficult due to discrete sand deposition and lesser thickness of sand (only 5-6 m) in the larger depth of the subsurface (around 3000 m). The basin that contains the hydrocarbon field is an intra-cratonic basin, spread along the western periphery of central India. It is surrounded by the Aravalli range and Deccan craton and Saurashtra craton in the east-west direction. Extensive lava flow covered\nPre-processing\n8\nthe basin by lava flow during the period of cretaceous age above the formerly deposited Mesozoic sediments. Deccan Trap acted as the basement for deposition of a huge thickness of Tertiary-Quaternary sediments."}, {"heading": "2.1.2 Preparation of Data", "text": "A spatial database containing seismic attributes and well logs has been acquired from the study area in SEG-Y and Log ASCII Standard (LAS) format respectively. The SEG-Y file format is one of numerous standards developed by the Society of Exploration Geophysicists (SEG) for storing geophysical data [68]. On the other hand, the Canadian Well Logging Society presented LAS format to standardize the organization of digital log curves information in 1989 [69]. As the workflow is developed on MATLAB platform, the .sgy data files are converted in .mat format (MATLAB software compatible format) for MATLAB compatibility. The database contains seismic attributes over the study area and borehole dataset at four well locations in the study area. These four wells will be hereafter referred as A, B, C, and D in terms of inlines and crosslines (xlines). The depth of each well is around 3000 meter from the ground, whereas the zone of interest varies from around 2720 meter to 2975 meter under surface. The borehole dataset contains basic logs such as gamma ray, resistivity, density along with derived geo-scientific logs such as sand fraction, permeability, porosity, water saturation, etc. These well logs are treated as one-dimensional signals for further processing in this study. On the other hand, the seismic dataset contains impedance, instantaneous frequency and seismic amplitude across the volume. The difference between the maximum displacement of a seismic wave and from the null point (point of no displacement) is defined as the seismic amplitude. It is recorded over a study area by converting the mechanical energy due to the motion of seismic wave through the depositional layers in subsurface into electrical energy by a geophone. The product of density and seismic velocity through different types of rock layers represents the seismic impedance. The third predictor variable i.e. the instantaneous frequency is defined as the rate of change of the phase of seismic amplitude signal [1], [70]. Sand fraction (SF) represents per unit sand volume within the rock [1]. In this study, the sand fraction is used as target lithological property.\nOther geophysical techniques such as Ground Penetrating Radar (GPR) [71] is a nearsurface technique mainly used in archaeological researches. Similarly, remote sensing imaginary dataset is used in detection and classification of objects on the surface or in the oceans, atmosphere. However, the range of penetration depths for these techniques are very small compared to 3000 meter i.e. depth of the wells. Therefore, these datasets cannot be used instead of seismic dataset for prediction of lithological parameters in deep subsurface."}, {"heading": "2.2 Pre-Processing", "text": "Pre-processing plays a crucial role on the performance tuning of a machine learning algorithm. In this chapter, an efficient pre-processing approach is proposed as part of the adopted methodology to obtain a functional relationship between seismic attributes and sand fraction. 2.2.1 Signal Reconstruction The borehole data are recorded at specific well locations along the depth with a high vertical\nPre-processing\n9\nresolution. The seismic data are acquired in the time domain where depth is measured in milliseconds two-way travel-time instead of meters. The time required for the sound wave to reach the reflector from the source and return to the receiver after hitting the reflector is termed as two-way travel time. In case of shallow reflectors, high frequencies are reflected, whereas, the lower frequency content of the sound signal penetrates the ground further down. The velocity and wavelength increase with the depth unlike the frequency. Thus, the seismic resolution reduces with increasing depth under subsurface [72]. For this particular dataset, the seismic data are collected spatially in the time domain with a sampling interval of two milliseconds. First, the well logs are converted from the depth domain to the time domain at 0.15 milliseconds sampling interval using the given velocity profile resulting from wellseismic-tie. The sampling intervals of both kinds of data are different. The seismic attributes and the sand fraction can be integrated either by upsampling the seismic signal or downsampling the latter. As a band-limited signal can be reconstructed from its samples based on Nyquist-Shannon theorem, and downsampling reduces the size of the dataset, the first option i.e. upsampling the seismic signal is opted. Hence, the band-limited seismic attributes are reconstructed at each time instant corresponding to the well logs by a sinc interpolator while adhering to the Nyquist\u2013Shannon sampling theorem [73].\nFig. 2-1 represents three seismic attributes- (a) seismic impedance, (b) amplitude, (c) instantaneous frequency and the target lithological property- (d) sand fraction along the Well A. The red dots on the seismic attributes represent original values at time interval of two milliseconds and the green curves represent reconstructed signals at the time instants marked on the well logs. Fig. 2-1 (d) demonstrates a blue high-frequency curve representing sand fraction along the same well (Well A).\nPre-processing\n10\nDuring the experimentation with the current dataset, we propose different normalization schemes for predictor and target variables. The predictor variables are normalized using the Zscore normalization. The values of attribute X are normalized using the mean and standard deviation of the X. The normalized value is obtained following the equation:\n_ X X valnormalized val    \n(2-1)\nwhere X and  X represent the mean and standard deviation of the attribute X. Then, the target variable is normalized using the min-max normalization that performs a linear transformation on original data. The relationships among the original data are preserved in this normalization. For this particular dataset, the range of normalized target variables is selected as [0.1, 0.9]. The normalized dataset is used in the regularization stage."}, {"heading": "2.2.2 Data Regularization and Re-sampling", "text": "It can be observed from Fig. 2-1 that the frequencies present in seismic signals are much lower compared to that of the sand fraction. In other words, the sand fraction carries much higher information as compared to the seismic attributes. According to laws of information theory, a higher information-carrying signal cannot be modelled using single or multiple lower information-carrying predictor signals. Only a part of the target variable that is dependent on the predictor variables can be modelled. Thus, the necessity of information filtering through regularization is established. In this chapter, three different signal processing approaches are selected and implemented in order to filter the target signal. The parameters belonging to this stage are tuned following the changes in entropy before and after filtering along with visual inspection of the output signal with respect to that of the original target signal. The entropy has been computed from the Power Spectral Density (PSD) of the signal. The average amount of information gained from a measurement that specifies X is defined to be the entropy ( )H X of a system. It can be formally defined as\n2( ) ( ) log ( )i i i H X p x p x  (2-2)\nwhere ( )ip x is the probability of X having the thi value ix in the dataset.\nThis is known as Shannon entropy [74], [75]. If A is another random variable described on the same dataset then the mutual information between the two can be expressed as\n( ; ) ( ) ( | )I X A H X H X A  (2-3)\nwhere, ( | )H X A is the conditional entropy of X after Ahas been observed. A reservoir property (here sand fraction) can be represented by X and seismic attribute e.g. seismic impedance can be represented by A . The statistical property of ( ; )I X A can be interpreted as the reduction in the uncertainty of the reservoir property, due to observing the attribute A . The statistical property\nPre-processing\n11\nof ( ; )I X A can be interpreted as the reduction in the uncertainty of the reservoir property, due to observing the attribute A . In [76], Normalized Mutual Information (NMI) is defined as the mutual information normalized by minimum entropy of both the variables.\n( ; ) ( ; ) / min( ( ), ( ))NMI X A I X A H X H A (2-4) In this study, the NMI computed between predictor and target signal has been used to adjust the parameters of the information filtering algorithms.\nFourier Transform (FT) based Regularization\nThe first regularization approach is based on FT (Algorithm 2-1). Here, the spectrums of target and predictor variables are compared, and higher frequency components of the target signal are truncated. Then, the target signal is reconstructed using Inverse Fourier Transform (IFT). Comparing the FT of the sand fraction (Fig. 2-2(a)) with that of the seismic impedance (Fig. 2-2 (b)), the presence of higher order frequencies is evident in the former. It can be observed from Fig. 2-2 (b) that the spectrum of band-limited seismic impedance diminishes beyond frequency range (-0.2: +0.2 hertz). Then, the part of the sand fraction spectrum belonging to slightly a wider frequency range (green curve, Fig. 2-2 (a)) is reconstructed to obtain regularized target. The wider range of frequencies is chosen with the assumption that Neural Networks as nonlinear predictors are capable of mapping input signals of lower frequencies to output signals with higher frequencies. Of course it needs an entirely different research to find the prediction capability of a given nonlinear mapping process. The original and regularized target signals are presented in Fig. 2-2(c) by the blue and red curves respectively. As shown in Table 2-1, the information content of the original sand fraction is higher as compared to that of the seismic predictor variables which makes it difficult to model the target (sand fraction) from predictor attributes. The regularization process decreases the information content in the sand fraction as seen in Table 2-1. The dependency between predictor and target variables in terms of NMI (Table 2-2) also improves as a result of regularization.\nAlgorithm 2-1 : SF Regularization based on FT Task : Regularizing target sand fraction based on FT Input : Predictor signal ( )x t and target signal ( )y t\na) The target ( )y t and predictor signal ( )x t are extracted from raw dataset.\nb) Compute Fourier Transform of ( )x t : ( 1)( 1) 1 ( ) ( )   \n N\nj k N\nj X k x j\nwhere, 2 /i NN e   is the thN root of unity Similarly, FT of target ( )y t is computed as:\n( 1)( 1) 1 ( ) ( )    \nN j k N j Y k y j , where, 2 /i NN e   is the thN root of unity\nc) Compare the spectrums of target and predictor signals\nPre-processing\n12\nd) Select the bandwidth parameter max Hz e) The part of the target spectrum exceeding max Hz is truncated to zero. Modified Target : mod ( )Y k f) Construct regularized target signal ( )ry t by carrying out IFT of the truncated spectrum:\nyr (t )  1 N\nYmod k 1\nN\n (k )N( j 1)(k 1) g) Calculate entropies of predictors (seismic attributes here) as well as original and\nregularized target signals (sand fraction here). h) If entropy of regularized target is comparable with that of the predictor signal and\nthe regularization result is satisfactory, then regularization is completed else go to step d).\nOutput : Regularized target signal ( )ry t\n(a) (b)\n-1 -0.5 0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\nFrequency (Hz)\n|Y (f\n)|\nActual SF Spectrum Filtered SF Spectrum\n-1 -0.5 0 0.5 1 0\n0.2\n0.4\n0.6\n0.8\nFrequency (Hz)\n|Z (f\n)|\nSeismic impedance spectrum\n2220 2240 2260 2280 2300 2320 2340 2360 2380 2400\n0.2\n0.4\n0.6\n0.8\nTime (ms)\nSa nd\nfr ac\nti on\nPre-processing\n13\nTable 2-1: Entropy (in bit) of PSD of signals for Well A\nPredictor Variable Seismic Impedance Inst. Amplitude Inst.\nFrequency Original Signal 0.12 0.11 0.09\nR eg\nul ar\niz ed\nSi\ngn al\nFT 0.16 0.16 0.14 WD 0.15 0.14 0.11\nEMD 0.14 0.14 0.12\nWavelet Decomposition (WD) based Regularization\nA time-frequency representation of a one-dimensional non-stationary signal is obtained using wavelet analysis. Recent literatures reveal that the wavelet decomposition is used in different fields of research. For example, it is applied in Electroencephalography (EEG) signal analysis for artefact removal to detect the effect of sleep deprivation [77], study of geomagnetic signals [78]\u2013[80] etc. Wavelets are small wave like oscillating functions that are localized in time and frequency [77], [81]\u2013[83]. A finite energy time domain signal can be decomposed and expressed in terms of scaled and shifted versions of a mother wavelet ( )t and a corresponding scaling function ( )t in the discrete domain. The scaled and shifted form of the mother wavelet\n, ( )l k t and the corresponding scaling function , ( )l k t are arithmetically represented as\n/2 , ( ) 2 (2 ), , l l l k t t k l k R    (2-5)\n/2 , ( ) 2 (2 ), , l l l k t t k l k R    (2-6)\nThe original signal ( )X t is first decomposed into high-frequency and low-frequency components using high pass and low pass filters. After each filtering step, the output time series is down-sampled by two. The low-frequency part approximates the signal while the highfrequency part denotes residuals between original and approximate signal. At successive levels, the approximate component is further decomposed using the same set of high-pass and\nPre-processing\n14\nlow-pass filters. A signal ( )X t can be expressed mathematically in terms of the above wavelet\n, ( )l k t and corresponding scaling function , ( )l k t at level l as\n, ,( ) ( ) ( ) ( ) ( )l l k l l k k k X t a k t d k t    (2-7)\nwhere ( )la k and ( )ld k are the approximate and detailed coefficients at level l . These coefficients are computed using filter bank approach as in [84].\nFig. 2-3 describes the steps of WD for three levels. Here, a signal is decomposed into approximate and detailed coefficients using low pass ( )H k and high pass ( )G k filters respectively. After decomposition, the coefficients can be modified. In case of signal reconstruction, the modified approximate and detailed coefficients are up-sampled by two and then convolved with respective synthesis filters and then the resulting pair is summed. Finally, modified signal is acquired following l level synthesis.\nThe performance of wavelet analysis is dependent on the mother wavelet selection and decomposition level. The Daubechies family of wavelets has a compact support with relatively more number of vanishing moments [81]. Therefore, in most of the cases different variants of Daubechies family wavelets are used for signal analysis. The initial wavelet selections can be modified if necessary. Algorithm 2-2 describes the steps associated with WD based regularization.\nAlgorithm 2-2: SF Regularization based on WD Task : Regularizing target sand fraction based on WD Input : Predictor signal ( )x t and target signal ( )y t\na) Same as Algorithm 2-1 b) Select the wavelet type and number of decomposition levels.\nPre-processing\n15\nc) Apply the procedure as in Fig.5 to the target signal. d) Decide: detailed coefficients to be truncated for regularization by looking at the\nseismic amplitude and its decomposition result e) The selected detailed coefficients are made zero. f) The regularized target signal is reconstructed from the modified coefficients. g) Calculate entropies of predictors as well as original and regularized target signals. h) If entropy of regularized target is comparable with that of the predictor signal and\nthe regularization result is satisfactory, then regularization is completed, else go to step d).\nOutput : Regularized target signal ( )ry t\nFor this study, fourth order Daubechies wavelet (db4) with six levels of decomposition has been chosen. Fig. 2-4 (a)-(b) represent the results of WD\u2013based regularization of target sand fraction with predefined wavelet type, and decomposition level. The first three detailed coefficients of the original sand fraction signal are demonstrated in Fig. 2-4 (a).\nAfter the decomposition, the detailed coefficients of initial levels of the original target signal are made zero and the regularized signal is constructed by performing Inverse Discrete Wavelet Transform (IDWT) from the modified coefficients. For Well A, the first five detailed coefficients are truncated and the regularized target is reconstructed from the approximate and\nPre-processing\n16\ndetailed coefficients of the sixth level by IDWT. The regularization result for Well A is presented in Fig. 2-4 (b), where the blue and red curves represent the original and regularized target sand fraction signals respectively.\nTable 2-1 and Table 2-2 reveal the changes in information content of the original and regularized sand fraction by WD and increase of the dependency between target and predictor variables as a result of regularization.\nEmpirical Mode Decomposition (EMD) based Regularization\nSeismic and well log signals are non-stationary signals. Reports suggest that in most of the cases the frequency analysis of signals are carried out in selected windows with respect to a given orthogonal basis [85]\u2013[87]. The disadvantage of basis decomposition techniques is the mismatch between signal trend and constant basis functions. These necessitate a new decomposition method, namely EMD.\nAlgorithm 2-3: SF Regularization based on EMD Task : Regularizing target sand fraction based on EMD Input : Predictor signal ( )x t and target signal ( )y t\na) Same as Algorithm 2-1 b) Initialize: 0 ( ) ( )r t x t , 1i  c) Extract the thi IMF: i. Initialize: 0( ) ( ) ih t r t , 1j \nii. Extract the local minima and maxima of 1( )jh t\niii. Create upper envelope max ( )e t and lower envelope min ( )e t of 1( )jh t by interpolating local maxima and minima iv. Calculate mean envelope: max min1 ( ) ( )( )\n2j e t e tm t  \nv. 1 1( ) ( ) ( )  j j jh t h t m t\nvi. If ( )jh t is an IMF, then, ( ) ( )i jimf t h t else go to the step-(ii). with 1j j \nd) 1( ) ( ) ( ) i i ir t r t imf t e) If ( )ir t has at least two extrema,\nThen, go to c) with 1i i \nElse, 1\n( ) ( ) ( ) n\ni n i x t imf t r t    is decomposed into n numbers of IMFs and residue\nsignal. f) EMD of target signal ( )y t is carried out following steps b)-e)\nPre-processing\n17\n1 ( ) ( ) ( )\np\ni p i y t imf t r t   \ng) The number of intrinsic mode functions (IMF) and distribution of IMFs are observed for target ( )y t and predictor ( )x t : p n\nh) Decide 1p : number of IMF truncated from the EMD of ( )y t for regularization where,\n1 p p\ni) Construct regularized target signal ( )ry t : 1\n1 ( ) ( ) ( )   \np\nr i p i\ny t imf t r t\nj) Calculate entropies of predictors as well as original and regularized target signals. k) If entropy of regularized target is comparable with that of the predictor signal and the\nregularization result is satisfactory, then regularization is completed else go to step h). Output : Regularized target signal ( )ry t\nEMD is an algorithmic decomposition method which decomposes the input signal into a set of Intrinsic Mode Functions (IMFs) and a residue signal [88]. There are two properties associated with IMFs such that (1) the numbers of zero\u2013crossings and extrema present in IMFs are same, and (2) IMFs are symmetric with respect to the local mean [88]. In other words, EMD detects and extracts the highest frequency component in the signal [81], [89]\u2013[95]; such that, in step (k+1), the extracted IMF contains lower frequency component compared to that extracted in step k. Moreover, being an adaptive data-driven method, EMD decomposes an input signal into a variable number of components. Thus, EMD overcomes the inherent limitation of deciding a priori the number of decomposition levels as in WD. Algorithm 2-3 describes the detailed steps associated with EMD based regularization of target SF.\nIn Fig. 2-5 (a), the first three IMFs of the sand fraction log along the Well A are plotted. The comparison between EMD results of the sand fraction and seismic impedance reveals that the number of IMFs obtained is higher in case of target signal (sand fraction) than its predictor counterpart (seismic-impedance). The first IMF component is suppressed and the other IMFs are used to reconstruct the regularized sand fraction. The superimposed plots of actual (blue curve) and regularized sand fraction (red curve) signals are presented in Fig. 2-5 (b).\nThe user decides the regularization result is satisfactory or not based on visual inspection of original and regularized target variables. The regularized target is smoother compared to the original signal; nevertheless, the trend of the original signal is preserved even after information filtering based on either of the three proposed regularization methods.\nTable 2-1 represents the entropies of the predictor and target attributes for Well A. The improvement in mutual dependency between the predictor and target variables in terms of NMI is evident from Table 2-2.\nPre-processing\n18"}, {"heading": "2.3 Conclusion", "text": "Three regularization approaches based on FT, WD, EMD are proposed and implemented to shape the target sand fraction in order to improve the predictability of the target from predictor seismic attributes. The improvement of information content and dependency between target and predictor variables are apparent from the entropy and NMI respectively. The advantages and disadvantages of the three approaches are now briefly discussed. To start with, FT is a linear transform where the frequency spectrum of a time-domain signal is obtained. Comparing the frequency spectrum of the predictor seismic impedance and target SF, the regularization parameter is selected. Depending on the selected regularization parameter the result of FT based regularization changes. However, seismic signals are non-stationary signals. Therefore, the next two approaches based on WD and EMD are opted to cross-validate the performance of FT based regularization. The selection of the wavelet and the decomposition level is important for the decomposition of the original target signal. Then, the detailed coefficients to be truncated are decided. The regularized target signal is constructed from the modified coefficients. The variable parameters i.e. wavelet type, decomposition level, and detailed\nPre-processing\n19\ncoefficients to be truncated are finalized empirically based on the regularization result. However, in case of EMD, number of selection parameters are less. Only, the number of IMFs to be used for reconstruction of the target is decided. In all the three cases, it is maintained that the regularized target signal should preserve the trend of the original signal. In case of FT based approach, the regularization parameter is decided based on the FT results of both the predictor as well as the target signal. However, in the case WD, the predictor signal is regularized to observe that the frequency content of the individual coefficients are less compared to the target signal case with the selected decomposition level and wavelet type. Similarly, the number of decomposed IMFs of the seismic attributes are less compared to the SF counterpart in case of EMD. However, in both cases (WD and EMD), the number of detailed coefficients and IMFs are decided irrespective of the decomposition results of the predictor seismic signals.\nIn the next chapter, the regularized sand fraction is modelled using ANN based prediction frameworks. Therefore, the three regularization approaches would cross-validate each other in terms of evaluators over the network performance using original SF as target signal in Section 3.2.\nPre-processing\n20\nPrediction of Sand Fraction\n21\nChapter 3. Prediction of Sand Fraction Sand fraction (SF), which represents per unit sand volume within the rock, is an important lithological property. Higher values of the sand fraction represent increased probability of porosity in a layer, and also indicate a higher probability of presence of hydrocarbon in case of a porous medium. Hence, the sand fraction is an important attribute to be modelled in reservoir characterization.\nThis chapter describes two different frameworks for prediction of a lithological property (sand fraction) from three predictor seismic attributes using two different datasets. The first section of this chapter describes the theory associated with ANN in brief. The next section of this chapter deals with the first set of data combining sand fraction logs at four well locations and seismic attributes logs at the corresponding well locations. The next section discusses the works with another set of data including seismic attributes from a western onshore hydrocarbon field of India and borehole data along with borehole information. In case of the second dataset, the borehole data are acquired from eight wells in the study area. Additionally, well tops at the eight well locations and horizon information over the area of interest are provided with seismic attributes and well logs.\nTo start with, a framework is designed to benchmark the proposed pre-processing stage as in Chapter 2 through a complete framework consisting of three stages (pre-processing, model building and validation, and finally, post-processing). The same dataset as in Chapter 2 is used in this work. An ANN with conjugate-gradient learning algorithm is used to model the sand fraction. The input data sets are segregated into training, testing and validation sets. The test results are primarily used to change the network structure and activation functions. Once the network passes the testing phase with an acceptable performance in terms of standard performance indicators, the validation phase follows. In the validation stage, the prediction is tested against unseen data. The network that yielded satisfactory performance in the validation stage is used to predict the target lithological property using seismic attributes as predictor variables throughout the given volume. Finally, a post-processing scheme using 3-D spatial filtering is carried out for smoothing the sand fraction in the volume.\nSection 3.3 describes the prediction framework to predict sand fraction from the same seismic predictor attributes using the concept of MANN. At first, the acquired dataset is integrated and normalized. Then, well log analysis and segmentation of the total depth range into three different units (zones) separated by well tops are carried out. Then, three different networks are trained corresponding to three different zones using combined dataset of seven wells and then the trained networks are validated using the remaining test well data. The target property of the test well is predicted using three different tuned networks corresponding to three zones; and then the estimated values obtained from three different networks are concatenated to represent the predicted log along the complete depth range of the test well. It has been observed that the application of multiple\nPrediction of Sand Fraction\n22\nsimpler networks instead of a single one improves the prediction accuracy in terms of performance evaluators\u2013 correlation coefficient, root mean square error, absolute error mean and program execution time. Then, volumetric prediction of reservoir properties is carried out using calibrated network parameters. This stage is followed by post-processing to improve visualization. Thus, a complete framework, that includes pre-processing, model building and validation, volumetric prediction, and post-processing, is designed for successful mapping between seismic attributes and a reservoir characteristic. The proposed framework has performed better than a single ANN with reduced prediction error, program execution time and improved correlation coefficient as a result of the application of the MANN concept."}, {"heading": "3.1 Artificial Neural Network (ANN)", "text": "Computers have become immensely powerful over the recent years; therefore, it has become easier to emulate a simple task carried out by a human being. For example, recognition of a particular letter can be carried out with the guidance of a teacher (supervised learning). On the other hand, it can be carried out without the help from a teacher; a child can learn a particular letter by correcting his mistakes several times (unsupervised learning). These types of simple tasks such as character recognition, differentiation between different objects are carried out by humans very easily. Similarly, these tasks can be addressed with the help of a high performance computer [96].\nAn ANN is an information\u2013processing system that emulates certain performance characteristics of a biological neuron. An ANN can be characterized by three properties such as:\n ANN architecture (i.e. connections between neurons)  Deciding the weights (by training/ learning)  Activation functions\nANN is used in different research domains such as signal processing, control, pattern recognition, speech recognition etc. to solve different classification and prediction problems.\nMultilayer ANN is a network, which processes one or several hidden layers joining input and hidden layers. Fig. 3-1 represents a typical example of a n-input single output multilayer ANN with a single hidden layer in it. The nodes corresponding to the hidden layer are connected to input and output nodes through weights and biases are connected to each node in the hidden and output layers. As shown in the figure, hyperbolic tangent sigmoid transfer function is used in the hidden layer nodes to facilitate the learning process [97]. In contrast, log-sigmoid transfer function is used in the output node. The hyperbolic tangent sigmoid transfer function and log sigmoid transfer function (used in output layer node) have the following forms respectively:\n2\n2 1tan ( ) 1\nx\nx esig x e\n\n\n \n (3-1)\n1log ( ) 1 x sig x e  \n(3-2)\nPrediction of Sand Fraction\n23\nThe training procedure of an ANN is demonstrated in Fig. 3-2. At first, the dataset is analysed and partitioned into training and testing sets followed by dataset normalization. Then, the network\nPrediction of Sand Fraction\n24\nstructure (number of hidden layers, nodes, activation functions) are selected and weights and biases are initialized. The training starts with selected learning algorithm. The weights and biases are modified and performance evaluators (such as RMSE, CC, and AEM etc.) are calculated. The training process continues until the satisfactory values of the performance evaluators are achieved. The back-propagation learning of a multilayer perceptron is carried out in two phases. The synoptic weights of the network are constant and the input signal is propagated through the hidden layers to the output layer. The changes only take place in the activation potentials and output of the neurons [13]. In contrast, an error signal is computed as the difference between network output and actual (desired) target (response). The error signal is propagated through layers in backward direction from output layer to input layer in the backward phase. The training procedure is described in details as follows.\nAssume,\n1{ ( ), ( )} N nx n d n  (3-3)\nto be the training samples to be used for training of a back propagation multilayer perceptron. The input vector ( )x n is applied to the input layer nodes and the desired output vector ( )d n is present at the output node to compute the error between desired and actual output. The stages associated with the back-propagation algorithm can be summarized as follows:\n1. Initialization: The synaptic weights and thresholds are selected from a uniform distribution with zero mean. The variance of the distribution is selected such that the standard deviation of the induced local fields of the neurons lie at the transition between the linear and standard sections of the sigmoid activation function. 2. Presentations of training patterns: The forward (step 3) and backward propagation (step 4) are carried out for each sample of the training datasets. 3. Forward propagation: The forward signal is propagated from input layer to output layer through one or multiple hidden layers. The induced local field for neuron j in layer l can be computed from the output of neuron i in the previous layer ( 1)l  at iteration n i.e.\n1( )liy n  and the synaptic weight ( )ljiw n that is connected from neuron i in ( 1)l  layer and\nis expressed as ( ) ( ) ( 1)( ) ( ) ( )l l lj ji i\ni v n w n y n (3-4)\nIn case of output layer (here, l L and L is the network depth), the output of neuron j is written as\n( )( ) ( )L jjy n o n (3-5) Therefore, the error is computed as\n( ) ( ) ( )j j je n d n o n  (3-6)\nPrediction of Sand Fraction\n25\nwhere, ( )jd n is the j th element of the desired response vector ( )d n .\n4. Backward computation: In this step, the local gradients of the network are computed as:\n( ) ( ) ( )'( ) ( ) ( ( ))l L Ljj j jn e n v n  (3-7) for neuron j in output layer L and\n( ) ( ) ( 1) ( 1)'( ) ( ( )) ( ) ( )l l l ljj j k kj k n v n n w n     (3-8) for neuron j in output layer l\nwhere, the prime in ' ( )j  represents the differentiation with respect to the argument.\nThe synaptic weights of the network in layer l according to the generalized delta rule ( ) ( ) ( ) ( ) ( 1)( 1) ( ) [ ( 1)] ( ) ( )l l l l lji ji ji j iw n w n w n n y n        (3-9)\nwhere,  and  are the learning-rate parameter and the momentum constant respectively.\n5. Iterations: The forward and backward propagations are carried out until the selected stopping criterion is reached. The training examples are randomized in each epoch and the momentum and learning-rate parameter are modified.\nThe supervised learning of a multilayer ANN can be viewed as a problem of numerical optimization. The error surface of a multilayer ANN is a nonlinear function of weight vector w . Assume that, the error energy averaged over the training samples or the empirical risk be avg .\nUsing (3-6), avg can be computed as\n2\n1\n1 ( ) 2\nN\navg j n j C e n N      (3-10) where, the set C contains all the neurons in the output layer. The second derivative of the cost function avg with respect to the weight vector w is called the Hessian matrix and denoted by H so that,\n2\n2 avgH w   \n(3-11)\nThe Hessian matrix is considered as positive definite unless mentioned. There are several algorithms to train an ANN. In case of conjugate gradient methods, the computational complexity and memory usage are large because of calculation and storage of the Hessian matrix at each stage. The indefiniteness of H by a scalar parameter k in case of the scaled conjugate gradient (SCG). The other parameters kr and  kp represent the search direction and the steepest descent direction respectively. In this work, SCG algorithm is selected over other supervised algorithms to train the ANN. The steps associated with SCG can be presented as follows [98]:\nPrediction of Sand Fraction\n26\n1. Selection of parameters: Weight vector 1w and 40 10   , 410 10   , 1 0  . Assume,   111 '( ), 1 and success = truep r E w k    2. If success = true , then compute the second-order information:\n/k kp    ( '( ) '( )) /k k kk kks E w p E w   \nT kk kp s  \n(3-12)\n3. Modify k :\n  2( )kk k k kp      (3-13)\n4. If 0k  , then make the Hessian matrix positive definite\n \n\n\n2\n2\n2( / )k k k k\nk k k k\nkk\np\np\n \n\n \n \n   \n\n(3-14)\n5. Evaluate step size: \n/\nT kk k\nk k k\np r     \n (3-15)\n6. Compute the comparison parameter:    22 [ ( ) ( )] /k kk k k kkE w E w p     (3-16)\n7. If 0k  , then error can be reduced:   1k k k kw w p  \n1 1'( )k kr E w    0, success = truek \n(3-17)\nIf mod 0k N  then  11 kkp r    (3-18)\nelse:\n \n2 1 1\n11\n( ) /k T k kk k\nk kk k\nr r r\np r p\n \n\n \n\n \n \n    (3-19)\nIf 0.75k , modify the scale parameter, 0.25k k  (3-20)\nElse\nPrediction of Sand Fraction\n27\n , success = falsek k  (3-21) 8. If 0.25k , increase the scale parameter\n 2( (1 ) / )k k k k kp     (3-22)\n9. Check: If the steepest descent direction 0,kr  then  1kw  as the desired minimum. else 1k k  and goto Step 2.\nThe value of the parameter is kept small indicating that it is not critical for the performance of SCG. The fact that SCG does not involve any user dependent parameter that is critical for its performance is an advantage of this algorithm."}, {"heading": "3.2 Prediction of Sand Fraction from Seismic Attributes without Well Tops Information", "text": "The integrated dataset of seismic attributes and sand fraction at four well locations, which is prepared following the pre-processing stage as in Chapter 2, is used in this study. The predictor attributes used in this study are three seismic variables such as seismic amplitude, seismic impedance, and instantaneous frequency and the target variable is sand fraction. The available borehole data along with the 3-D seismic attributes have been used to benchmark the proposed pre-processing stage using a methodology (Fig. 3-3) which consists of three stages, i.e., preprocessing, training and post-processing. The available sand fraction belongs to the high-resolution borehole data and has far more information content than the low-resolution seismic attributes. Therefore, three alternative regularization schemes based on FT, WD and EMD as described in Chapter 2 have been used in the pre-processing stage to shape the high-resolution sand fraction data for effective machine learning.\nPrediction of Sand Fraction\n28\nPrediction of Sand Fraction\n29"}, {"heading": "3.2.1 Pre-processing", "text": "Pre-processing plays a pivotal role in the performance fine-tuning of a machine learning algorithm. The detailed pre-processing scheme consisting of seismic signal reconstruction and target lithological properties regularization by three alternative signal processing approaches based on FT, WD, and EMD respectively is described in details in Chapter 2. The pre-processed dataset is used here for model building and validation."}, {"heading": "3.2.2 Model Building and Validation", "text": "The task of model building and validation stage is implemented to evaluate the network performances using original and regularized sand fraction as target variable, respectively. In each case, the seismic inputs and sand fraction values are normalized using the data from all four wells taken together. The target variable is normalized within the range of output activation function with some offset from the limiting value of the activation function. Otherwise, the back propagation algorithm tends to drive the free network parameters to infinity. As a result, the learning process will slow down [13]. Hence, the target variables are normalized between 0.1 and 0.9 to avoid any overlap with the saturation region of the log-sigmoid function. Then, the training dataset is created by aggregating 70% sample patterns from each of the wells. The training patterns are scrambled to remove any trend along the depths. The remaining 30% samples from each of the four wells are combined and scrambled again. Then, the samples are divided into two parts to create the testing and the validation datasets. First, the network is trained using training patterns with initial parameter values. Then, the network structure and activation functions are tuned using testing patterns.\nThe testing phase is important for evaluating the generalization capability of the trained network [28]. The network that performs satisfactorily in terms of performance evaluators is then chosen to enter the validation stage. The performance of the trained networks is evaluated using four parameters \u2013CC, RMSE, AEM, and SI. The scatter index represents the ratio of RMSE to mean of in situ observations [24]. These statistical characteristics are defined as\nCorrelation coefficient (CC): 2 2 1 1\n( )( ) / ) ( ) N N i i i i i i i i i i CC X X Y Y X X Y Y        \nRoot mean square error: 2 1\n( ) / N i i i RMSE X Y N   \nAbsolute error mean: 1 1\n1 1N N i i i\ni i AEM X Y e N N     \nScatter index: /SI RMSE Y where, Xi and Yi ( i = 1, 2, 3,\u2026,N) represent modelled and observed values, respectively, X and Y are their corresponding mean values. Total number of data points are N and ei denotes absolute error.\nPrediction of Sand Fraction\n30\nThe statistical analysis of the errors involved in the model is important for proper understanding of the performance. The initial network structure is decided intuitively depending on the nature of the problem and the amount of available training patterns. In this study several runs of training, testing and validation of neural network structures with varying number of neurons, layers, activation functions and learning methods have been carried out to decide the best structure as well as the most effective learning algorithm. Different network structures for each instance are experimented and systematically changed keeping the improvement direction in view. Finally in the hidden layer, hyperbolic tangent sigmoid transfer function has been used. The tangent sigmoid transfer function is an automatic choice for researchers to use in the hidden layer to achieve the bidirectional swing [97], [99]. The activation function used in the output layer is log-sigmoid which is non-symmetric. It is reported that the learning rate of the network is faster when the network is anti-symmetric [13].\nFinally, a network with a single hidden layer is trained using the Scaled Conjugate Gradient (SCG) Backpropagation Algorithm [98]. The advantage of this algorithm is that it does not contain any user-dependent parameters. Moreover, it is faster than other second order algorithms as it avoids the time-consuming line search per learning iteration by using a step size scaling mechanism. The number of nodes in the input layer is same as the number of predictor attributes to be used to model the ANN. For example, in case of predicting sand fraction from three predictor attributes\u2013 namely, seismic impendence, instantaneous frequency and seismic amplitude, the number of input and output nodes will be three and one respectively.\nIt can be observed from Table 3-1 and Table 3-2 that the network performance is superior in case of regularization based on WD with two predictor variables. On the other hand, with three predictors FT based regularization outperformed the other two regularization approaches in terms of performance evaluators. However, for all cases, the performance is improved while using the regularized sand fraction as target instead of the original log. It can be envisaged that a user can select any of the three proposed regularization techniques that best suit with the working dataset.\nPrediction of Sand Fraction\n31\nPrediction of Sand Fraction\n32\nThe reported results reveal that the prediction accuracy increases with the inclusion of all available seismic attributes as predictors and regularized sand fraction value as target. The prediction in the entire volume along each inline and cross line is done using the trained network finalized during the validation step. Fig. 3-4 represents the variation of the seismic amplitude at a specific inline (inline 136 containing Well D). The sand fraction variation at the same inline (inline 136) as in Fig. 3-5 is obtained by prediction of the sand fraction over the study area from available seismic attributes using the validated network parameters. The network used for prediction over the study area has been calibrated using EMD\u2013regularized\u2013sand fraction as target. The reported results in Table 3-1and Table 3-2 are better than the performances of multilayer perceptron, SVM, and Co-Active Neuro-Fuzzy Inference System (CANFIS) for permeability modelling in terms of CC as reported in [100]. Thus, it can be said that improved performance is achieved for testing with unseen data using the framework described in Section 3.2.\nIt can be envisaged from the predictor seismic attributes as in Fig. 3-4 that the variation of the predicted sand fraction over the study area is smooth. However, the sand fraction across the study area as shown in Fig. 3-5 changes abruptly. Therefore, need of the post\u2013processing stage is established to obtain a smooth sand fraction variation across the volume."}, {"heading": "3.2.3 Post Processing", "text": "In reality, the sand fraction across the volume cannot change abruptly. The transition should be smoother and more or less agree to the patterns of seismic data. To incorporate this rationale, the predicted values are filtered through a 3-D median filter.\nMedian filter [101], [102] is a popular order-statistics filter where the value of a pixel is replaced by the median of a neighbourhood centred at that particular pixel. Selection of window value is crucial for the degree of smoothing.\nFig. 3-6 represents a pixel , and eight points surrounding it at location ( j, k ) . For a two dimensional with window size (2 1) (2 1)M N   , the complete pixel vector around the centre\npixel ,j kx is , 1, , ,{ , ,..., ,..., }j M k N j M k N j k j M k Nx x x x       . The pixel at centre location ( j, k ) is replaced by median value of the pixel vector. If 1, 1M N  , the matrix will be filtered using a 3 3 window. In case of 3-D median filter having a, the median of the pixel vector within 3 3 3  window size can be evaluate after carrying out six 3 3 \u2019partial-sort operation\u2019 [103].\nPrediction of Sand Fraction\n33\nThe predicted sand fraction in the volume is used as input to the post-processing operation. Every element in the volume is considered as a pixel and is smoothened using 3-D median filter with respect to its neighbourhood within a 3x3x3 window size. The missing values along the boundaries are ignored.\nThe sand fraction value along inline 136 is extracted from the smoothened sand fraction 3-D volume. Fig. 3-7 represents the result of median filtering along inline 136. The effect of localizing different levels of sand fraction values can be observed by comparing Fig. 3-5 and Fig. 3-7.\nFig. 3-7: Result of median filtering on sand fraction at inline 136\nThus, the complete framework including pre-processing, learning and validation, and finally post-processing, successfully carries out mapping between seismic attributes and the sand fraction."}, {"heading": "3.2.4 Discussion", "text": "Chapter 2 brings out an elegant regularization step in pre-processing to enhance the learning capability of ANN to carry out mapping between seismic and lithological property (sand fraction) successfully. Then, the improvement in mapping between seismic attributes and target sand fraction with regularization step is established from the performance analysis Section 3.2. Another contribution of this study is the improvement in the predicted sand fraction over the volume using 3-D spatial filtering.\nCross Line\nTi m\ne( m\ns)\n100 200 300 400 500 600 700\n2150\n2200\n2250\n2300\n2350\n2400\n2450\n0\n0.2\n0.4\n0.6\n0.8\nPrediction of Sand Fraction\n34"}, {"heading": "3.3 Prediction of Sand Fraction from Seismic Attributes with Well Tops Guidance", "text": "Section 3.3 proposes a complete framework consisting pre-processing, modelling, and postprocessing stages to carry out well tops guided prediction of a reservoir property (sand fraction) from three seismic attributes (seismic impedance, amplitude, and instantaneous frequency) using MANN algorithm.\nWe can encapsulate the work done in this section as a motivated outcome of the concepts of modularity and synchronization together. The idea of well tops guided division of the dataset is evolved from synchronization or similarity. The similarity between well logs sections belonging to a certain horizon is more compared to that of the similarity between multiple complete length logs. Then, the modularity concept enables to divide a complex problem into a set of relatively simple sub-problems. The borehole data are available at specific well locations; whereas seismic attributes are acquired over the area of interest. If a functional relationship can be established between seismic and well log signals (petrophysical properties), then, the variation of the reservoir characteristic over the area can be predicted from the seismic attributes itself. As the predictor and target signals are from two different domains, therefore, a single ANN structure may not be able to successfully represent the mapping function between these two types of signals, which is the current research problem in this domain [37]. In this section, we have attempted to devise a complete framework, with the objective of overcoming the limitations of the previous studies.\nIn this study, two well tops (namely Top1 and Top2) are identified after analysing well logs and seismic data. In the process of mapping sand fraction from seismic attributes, first, seismic attributes are extracted from 3D seismic cube at eight well locations. Next, integration of seismic and borehole data are carried out using time-depth relationship information at the available well locations. The pre-processed master dataset is then divided into three zones based on the two well tops such as 1st available patterns to Top 1, Top 1 to Top 2, and Top 2 to last available data pattern. In the model building and validation stage, first, three networks have been designed for three different zones separately. Sand fraction and three seismic attributes corresponding to seven wells are used for training and testing, and patterns corresponding to the remaining well are used for blind prediction. The satisfactory performance in blind testing encourages to carry out volumetric prediction of sand fraction using the three trained models. Then, results evaluated by three different networks (zone-wise) are merged to form a volumetric cube containing the estimated sand fraction values across the study area along the entire depth range. After model building and validation, a post-processing is carried out to improve the visualization quality."}, {"heading": "3.3.1 Methodology based on MANN Concept", "text": "In recent years, ANN is widely used to solve nonlinear modeling problems in the fields of science and technology such as computer science, electronics, mathematics, geosciences, medicine, physics, etc., [104]. ANN and its hybrid approaches have also proven to be useful in the nonlinear mapping of reservoir properties from well logs and seismic data [104], [105]. ANN performs\nPrediction of Sand Fraction\n35\nsatisfactorily in non-linear data mapping, pattern recognition and classification problems; however, the execution speed is slow in cases involving large dataset. Therefore, there is a significant scope of improvement in order to accelerate the training process by modifying the basic algorithm while not compromising with the prediction accuracy. Hence, MANN, which is a special category of ANN based on data categorization, is introduced as a potential tool for machine learning with efficient estimation capability and high speed [29], [50], [105]\u2013[107]. The concept of modularity is derived from the principle of divide and conquers. Here, a complex computational task is subdivided into smaller and simpler subtasks. Each local computational model performs an explicit, interpretable and relevant job according to the mechanics of the problem involved. Finally, the output of the model will be the combination of individual results of dedicated local computational systems. In this approach, module wise networks are trained and tested, and the outputs of all modules are integrated to achieve complete sequence of the target variable.\nDescription of Dataset\nThe well logs and seismic data used in this section are acquired from a hydrocarbon field located at the western onshore of India. The borehole dataset includes basic logs such as gamma ray, resistivity, density and other derived logs such as sand fraction value, permeability, porosity, water saturation, etc. Conversely, the seismic dataset includes different attributes, i.e., seismic impedance, instantaneous frequency, seismic envelope, seismic sweetness, etc. This section involves seismic impedance, amplitude and instantaneous frequency to model sand fraction using an integrated dataset of seismic and sand fraction signals available at eight well locations. The lithological properties along the depth range of a well vary in a non-linear and heterogeneous fashion. The variations of lithological properties along depth for well 4 and well 5 can be observed in Fig. 3-8. Two well tops namely Top 1 (red line) and Top 2 (green line) are shown in the figure. Integrated dataset corresponding to seven wells are used for learning of the three zone-wise prediction models and data from the eighth well is used for validation purpose.\nPresent study discusses the application of MANN concept for prediction of a reservoir property from seismic attributes. Well tops represent abrupt changes in the log data that corresponds to the changes in lithology denoting the corresponding zone boundaries. In this case, two well tops (Top 1 and Top2) are marked on the logs of petrophysical properties by expert geologists which in turn segments a log into three zones: Zone1: starting of log to Top 1, Zone 2: between Top 1 and Top 2, Zone 3: Top 2 to end of the log. Previous studies [53] reported that the similar zones in a well log reveals similar characteristic. Based on this hypothesis, the number of modules is decided as three same as the number of zones. Therefore, the master dataset combining seismic and borehole data is first divided into three zones (Z-1, Z-2 and Z-3) based on well tops guidance. Fig. 3-9 represents a schematic diagram depicting application of MANN concept for the present study.\nPrediction of Sand Fraction\n36\nPrediction of Sand Fraction\nPrediction of Sand Fraction\n38\ne) Data normalization based on min-max normalization OR z-score normalization\nf) for i = 1 to n (i: number of zones)\ng) Selection of network structure (number of neuron, training algorithm)\nh) Initialization of weights and biases, maximum epoch: itermax, min error: errormin\ni) for epoch=1: itermax\nj) Modify weights and biases following selected training algorithm\nk) Calculate RMSE\nl) if RMSE  errormin || epoch=itermax break, else epoch= epoch+1, end\nm) end for\nn) Test the network using testing patterns of well k\no) if testing is satisfactory go to step p) else go to step g)\np) Freeze the network structure : MANNi (i = 1 to n, here n=3)\nq) Save the network structures and parameters for minimum error\nr) end for\nOutput: Three sets of calibrated network parameters (weights and biases) w.r.t. three zones\nDataset from seven wells are used for training of the network, and then trained network is used to blindly model sand fraction for remaining one well. Separate training is carried out for each of the three modular networks keeping the learning algorithm and transfer functions same for all three networks. The optimal model is obtained by minimizing the RMSE between network output and target using selected state-of-art learning algorithm for each case. The testing of each model is carried out by using the zone-wise divided testing patterns corresponding to eighth well that is not included in the training set. Proposed MANN approach for the present study is described in Algorithm 3-1. Thus, three mapping functions are obtained using MANN approach [29] corresponding to three zones (Z-1, Z-2 and Z-3). These three trained networks are further used to obtain predicted sand fraction log for the whole study area. As indicated in Fig. 3-9 the predicted sand fraction logs from each modular network are concatenated to obtain the complete log profile. The obtained input-target relationships are used to estimate the lithological properties over the whole study area from seismic attributes.\nPrediction of Sand Fraction\n39"}, {"heading": "3.3.2 The Proposed Framework", "text": "Seismic data are collected over a large study area, whereas well logs are available at specific well locations in the same region. Furthermore, the vertical resolution of seismic attributes is inferior compared to that of the well logs due to larger sampling interval. In general, the seismic data are helpful to model a reservoir; however, it is difficult to estimate the vertical distribution of reservoir properties with the help of seismic signals [11], [12]. Therefore, information of both - seismic and well logs is necessary to characterize a reservoir property with high-resolution in both vertical and horizontal directions. For example, sand/shale fraction, porosity, permeability and saturation are important petrophysical properties used in the interpretation of hydrocarbon reserves in details. Therefore, modeling of any such petrophysical characteristic has crucial importance in this research domain.\nIn the present study, sand fraction is estimated from three seismic signals (seismic amplitude, impedance and instantaneous frequency) using MANN concept. A framework, which includes preprocessing, modeling and validation, volumetric prediction, and post processing, to carry out sand fraction modeling is described.\nThe proposed workflow is implemented on a 64 bit MATLAB platform installed in the Intel(R) core (TM) i5 CPU @3.10 GHz computing system having 8.00 GB RAM. First, a combined dataset of seven wells is used to train three different neural networks according to depth wise zones (Z-1, Z-2, and Z-3) (refer to Fig. 3-9 and Fig. 3-10). Then, the trained networks are validated using the dataset of the remaining well. The three predicted log sections for the test well corresponding to each zone (Z-1, Z-2, and Z-3) are merged to obtain complete sand fraction log."}, {"heading": "3.3.2.1 Pre-processing", "text": "This step involves integration and normalization of target (sand fraction) and predictor variables (seismic amplitude, impedance and instantaneous frequency), followed by data partition into training and testing set.\nIntegration of Seismic and Well Log Signals Integration of signals from different domains with the help of heuristic knowledge from human experts plays a major role in reservoir characterization [2]. Therefore, the first task in preprocessing is integration of seismic (which is in the time domain) and well log signals (which is in the depth domain) at each available well location. First, we extract the seismic attributes at eight available well locations. Then, data points in well log signals carrying missing values are excluded. It is followed by conversion of logs from the depth to the time domain using suitable velocity profile resulting from well-to-seismic tie. Then, the mismatch in sampling intervals of these two data sources (seismic and well logs) is addressed. Specifically, band-limited seismic attributes are sampled at an interval of two milliseconds, whereas the sampling interval of well logs is ~0.15 milliseconds for this particular dataset. Since, the sampling intervals of both the data are different,\nPrediction of Sand Fraction\n40\nwe apply Nyquist\u2013Shannon sampling theorem [108], which states that a band-limited signal can be completely reconstructed from the samples, to reconstruct seismic attributes at each time instant corresponding to the well logs using cubic spline interpolation method [109]. Due to the removal of missing values from logs, the dataset is not uniform anymore. Finally, the dataset uniformly resampled at an interval of 0.10 milliseconds.\nData Normalization Data normalization plays a crucial role for tuning the performance of machine learning algorithms. The predictors and target variables are normalized using the Z-score and min-max normalization, respectively.\nData Partition A common approach in machine learning algorithms is to divide a dataset into training and testing sets for learning and validation, respectively. In this study, a combined dataset of seismic and well log signals corresponding to seven boreholes is used for training the networks whereas data of the remaining eighth well is used for testing the networks.\nIn this study, well tops guided zone wise prediction is carried out using the concept of MANN. Fig. 3-10 depicts a workflow for integration and division of the dataset into three separate zones (Z-1, Z-2, and Z-3) for further modeling of reservoir properties."}, {"heading": "3.3.2.2 Model Building and Validation", "text": "The learning starts after completion of pre-processing of the working dataset. Three networks corresponding to each of the three depth zones (Z-1, Z-2, and Z-3) are trained and tested. Each of the networks has three predictor variables corresponding to the presence of three input nodes in the network structure and a single output node representing target sand fraction. In this study, we opted for a single hidden layer for all cases. Selection of activation functions and training algorithm plays a crucial role in training of the network. In the present study, hyperbolic tangent sigmoid is used in the hidden layer [97], [99]; however, for the output layer log-sigmoid transfer function is used. In these type of iterative processes, the connecting weights are updated using the back propagation till the global minimum error is achieved. Conjugate gradient method is an advanced and effective method for error minimization [13]. Here, scaled-conjugate-gradient-backpropagation is selected over several other learning algorithms for its speed and simplicity [13] in training the networks. Number of neurons in the hidden layer and epochs are initialized with small values and gradually increased keeping the improvement of fitting between target and predicted sand fraction in consideration. However, the number of hidden layer neurons cannot be indefinitely increased; the possibility of overfitting has been avoided by keeping the maximum number of trainable parameters at least fifteen times lower than the number of available training patterns [13].\nPrediction of Sand Fraction\n41\nThree separate networks are designed and trained for the three depth zones, and finally the trained networks are used for blind prediction. The performance of the trained networks is quantified in terms of four performance evaluators namely CC, RMSE, AEM and program execution time. It is important to carry out statistical analysis of the errors involved in the model. The calibrated networks, which performed well in blind prediction in terms of the four aforementioned performance evaluators, are saved and used in the next step, i.e., volumetric prediction.\nFig. 3-11 represents superimposed plots of target and network predicted sand fraction values for Zone 1, 2, and 3, respectively for well 6 only. Close observation of Fig. 3-11 reveals that the predicted logs follow the target ones with acceptable correlation coefficients (0.8058 for Z-1; 0.7699 for Z-2; and 0.8841 for Z-3). These high values of correlation coefficients indicate good prediction by the proposed framework. Similar results are obtained for other wells as well.\nThe correlation coefficients obtained by blind testing using three networks corresponds to three zones (Z-1, Z-2, and Z-3), and their average are compared with blind prediction coefficient using a single ANN for the overall depth range. Fig. 3-12\u2013Fig. 3-15 present the results of performance comparison of the proposed workflow with an ANN in terms of performance evaluators for well 2, 4, and 6.\nPrediction of Sand Fraction\n42\nPrediction of Sand Fraction\n43\nFor example, in case of Well 4, first, the dataset is segregated into three sections following the well tops guided zonation. Then, three sets of training patterns are generated combining the samples belong to Well 1\u2013 3, 5, \u2013 8 for each of the segregated dataset in the previous stage. Three networks are initialized and trained using the training patterns corresponding to three zones (Z1, Z2, and Z3). Next, the calibrated networks are validated using testing patterns belong to Well 4 for each zone separately. In case of CC, RMSE, and AEM, average performance of the proposed framework is expressed by carrying out mean of the respective measures belong to three individual models. Fig. 3-12 and Fig. 3-14\u2013Fig. 3-15 demonstrate CC, RMSE, and AEM respectively by the three individual models (Model 1, Model 2, and Model 3 for Z-1, Z-2, and Z3 respectively), their average performance, and the single ANN model used for the overall depth range. Contrarily, the total program execution time is resultant of summation of the three individual models. Fig. 3-13 depicts individual and total program execution times in seconds taken by the three models workflow along with that of the single ANN associated with whole depth range. As smaller networks deal with simpler structures and smaller dataset, acceptable accuracy is achieved with a reduced execution time in case of MANN approach."}, {"heading": "3.3.2.3 Volumetric Prediction", "text": "This step is essential for visualization of reservoir characteristic at the boreholes and away from it after prediction from seismic attributes is carried out. As no direct relationship between seismic and well logs is evident in theory, which might be inherent, it is a challenging task to estimate lithological properties across the study area from seismic signals. Therefore, it would be beneficial for the geoscientists if a mapping between seismic and reservoir properties could be carried out by deriving a relationship between these two types of data from integrated dataset of seismic and lithological parameters at available well locations using MANN concept. The horizon or well tops information of the study area is available. Therefore, the dataset containing predictor attributes throughout the study area are segregated into three parts according to well top information. Then, for each zone, predicted sand fraction log is generated from seismic signals using tuned network\nPrediction of Sand Fraction\n44\nparameters corresponding to a particular zone. Thus, a set of three logs is available for each particular trace point. These three logs can be concatenated accordingly to obtain the complete sand fraction log at a particular trace point. Hence, sand fraction logs are predicted for the study area from seismic input using tuned networks. Visualization at a specific in-line is demonstrated after predicting the sand fraction from three seismic attributes (seismic impedance, amplitude, and instantaneous frequency) over the area. In parallel, input attributes are also visualized across the in-line. It can be observed from the figures presented in the results section that predicted sand fraction requires post-processing step to improve the visualization quality.\nFig. 3-16 describes the variation of input seismic attributes and predicted sand fraction at an inline corresponds to well 6. It can be observed from Fig. 3-16 (a)\u2013(c) that the input attributes change smoothly throughout the study area. On the other hand, networks predicted sand fraction variation is not smooth.\nPrediction of Sand Fraction\n45\nPrediction of Sand Fraction\n46"}, {"heading": "3.3.2.4 Post Processing", "text": "In this study, the predicted sand fraction is smoothened using moving average filter [73]. The necessity of the post-processing step is established by comparing the variation of seismic attributes and estimated sand fraction.\nAlgorithm 3-2: Moving Average Filter Task : Reduce noise in predicted sand fraction volume\nInput : Predicted sand volume matrix X , window size w\na) Initialize: w\n,j kx - pixel value at ( , )j k , ,wj kI be a window of size w w centered at ( , )j k\nb) Compute ,, mov w j ki \u2013average of the pixel values in , w j kI\nc) Replace ,j kx by ,, mov w j ki , thus obtain filtX\nd) If result is satisfactory, then stop, else go to step a).\nOutput : Filtered sand fraction matrix filtX\nEvery matrix element in predicted sand fraction volume is considered as a pixel and smoothened using moving average filter respective to neighborhood of pixels within selected window size following Algorithm 3-2. In specific cases, where some of the neighborhood cell values are missing for a particular element, those missing values are replaced by NaN (not a number). For example, edge of the input matrix is filtered following above procedure. Here, the window size of the moving average filter used to smooth the predicted sand fraction is selected as 3\u00d73 empirically.\nThis uneven variation in predicted sand fraction necessitates inclusion of a post-processing algorithm. We opt for a moving average filter based algorithm with a 3\u00d73 window size. Implementation of the filtering technique on predicted sand fraction reduces noise in it. Comparing Fig. 3-16 (d) with Fig. 3-17, it can be observed that the variation of the latter is smoother than former. Thus, a realistic presentation of sand fraction variation over an area is obtained.\nPrediction of Sand Fraction\n47"}, {"heading": "3.3.3 Discussion", "text": "The objective of the present study is to establish well tops guided prediction of a reservoir property using MANN concept over a single network while working on a large and complex dataset. Section 3.3 has presented the performance of the proposed workflow along with ANN by blind estimation of the sand fraction from the three seismic attributes (seismic impedance, amplitude, and instantaneous frequency). It is evident from the presented results that the proposed workflow has outperformed ANN in terms of higher correlation coefficient, reduced error measures and low program execution time by successfully calibrating a functional relationship between seismic and well log signals corresponding to each zone. Thus, MANN concept can be selected over ANN in case of complex large dataset. The post-processing on predicted sand fraction improves the visualization realistically.\nThe contributions of this study are consolidated as follows:\n Fusion of two concepts \u2013 similarity between logs belonging to same horizon and MANN\n Inclusion of seismic data as predictor variables  The selection of number of modules based on well-top information  Blind prediction  The proposed workflow is established to produce better performance with reduced\nprogram execution time  Enhanced visualization by post-processing\nNext phase of research may be focused on estimation of 3D geo-cellular model for other characteristics involving seismic and well log signals at available well control points. Here, number of modules are decided based on well tops guided instead of trial-and-error methods."}, {"heading": "3.4 Conclusion", "text": "The target property in this chapter i.e. sand fraction is distributed from zero to unity. The distribution of SF is not skewed at any particular point. Thus, the dataset is balanced from the\nPrediction of Sand Fraction\n48\nperspective of the SF. The performance analysis carried out in Section 3.2 has established the regularization scheme proposed in Chapter 2. The three alternative approaches based on FT, WD, and EMD have cross-validated each other in terms of multiple performance indicators. Each of the three schemes has yielded better result compared to the case where the original SF was used as the target. Therefore, user may select either of the three schemes. The regularized SF is modelled from seismic attributes using ANN. The selection of the ANN structure and the initialization of parameters are crucial job to attain acceptable prediction performance. The performances are quantified in terms of multiple evaluators. With the increase in number of predictor variables, hidden layers, and neurons in each hidden layer, the structure of ANN will become more complex which in turn would increase the difficulty to train the network. The availability of enough number of training patterns is required for learning of the ANN. In case of large number of predictor attributes and smaller amount of training patterns, the dimensionality of the dataset need to be reduced in the pre-processing stage itself. Principal Component Analysis (PCA), forward sequential selection approach can be opted for dimensionality reduction. However, for this work, dimensionality reduction was not required owing to the presence of large training datasets.\nIn case of MANN, the problem is divided into multiple sub-problems. For each case, individual modules are trained using smaller datasets which in turn reduces the complexity in learning. Therefore, with the availability of well tops information, the complete dataset is divided into three zone wise datasets. Then, three individual ANN models are trained and tested separately.\nOther petrophysical properties such as porosity, permeability, shale fraction, etc. can be modelled from seismic attributes using the frameworks proposed in this chapter.\nClassification of Water Saturation\n49\nChapter 4. Classification of Water Saturation Evaluation of hydrocarbon reservoir requires classification of petrophysical properties from available dataset. However, characterization of reservoir attributes is difficult due to the nonlinear and heterogeneous nature of the subsurface physical properties. In this context, present study proposes a generalized one-class classification framework based on Support Vector Data Description (SVDD) to classify a reservoir characteristic\u2013 water saturation into two classes (Class high and Class low) from four logs namely gamma ray, neutron porosity, bulk density, and P-sonic using an imbalanced dataset. The comparison is carried out among the proposed framework and different supervised classification algorithms in terms of g-metric means and execution time. Experimental results show that the proposed framework has outperformed other classifiers in terms of these performance evaluators. Then, the proposed framework is modified and seismic attributes are used as predictor variables. The modified framework has predicted class labels of water saturation (Class low/Class high) from seismic information over the study area.\nThis chapter is designed as follows. First, the theory of SVDD is described in brief. Then, the framework to classify water saturation from well logs is presented. Finally, the modified framework to classify the water saturation from seismic attributes is described."}, {"heading": "4.1 Support Vector Data Description (SVDD)", "text": "Large dataset can be characterized using data description techniques. Significant efforts have been made for the classification of real world datasets. SVDD, an extension of SVM, is widely used approach for the data classifications [65], [66].\nIn general, data are described by defining a closed boundary around the data. This closed boundary is defined by hypersphere, ( , )F R a where \u2018a\u2019 represents centre and \u2018 R \u2019 is the radius. Volume of the hypersphere should be minimized for the data description [65]\u2013[67], [110]. Outlier\nin the data can be characterized by defining slacks variables i \u2265 0. In this case, the minimization term of error function is given by\n22 2( , ) i i i i F R a R C x a R       (4-1) where,\n2 2 , for alli ix a R i    (4-2)\nKernel function    ( ) .,i j i jxK x xx    is used for smooth data description. Then the SVDD function can be represented as\n, ( , ) ( , ) for all :0i i j i i i j i i i i j L K x x K x x C         (4-3)\nClassification of Water Saturation\n50\nOptimization of (4-3) gives the data description which can be obtained by several algorithms available in the literature, and Lagrange multipliers should satisfy the normalization constraint\n1i i   . The values of \u03b1 is can be found out by minimizing L. We have used a Gaussian kernel  , i jqx xi jK x x e  (4-4) to represent the dot product     .i jx x  as discussed in [65], [66], [111]. In order to calculate the radius we have to look for the support vectors. Firstly, 2( )R x in terms of the kernel function for each of the point is found out. Then, we get\n        , 2 , 2 , ,i i j i i j i j i R x K x x K x x K x x     \n(4-5)\nNow the support vectors are those data objects which lie on the surface of the hypersphere i.e.,\nfor which iC  . The contours are formed by the data points along the cluster boundaries. For the purpose of our work, we take the radius of the circle R to be the maximum of values ( )R x for the support vectors. Any data point lying beyond R is considered to be an outlier. In one-class classification using SVDD, the minority class patterns are used as the target in the training phase to construct the hypersphere. Once the hypersphere is constructed, the classifier is evaluated by using majority class patterns as testing dataset. For imbalanced dataset, the improvement in oneclass classifier performance compared to its two-class counterpart is apparent [112], [113]."}, {"heading": "4.2 Development of a Framework to Classify Water Saturation from Well Logs", "text": "The first important contribution of this chapter is to propose a generalized framework based on SVDD [65], [66] to characterize the water saturation from input well logs. Next, a comparative analysis is presented to demonstrate the effectiveness of the proposed classification method over other classifiers (discriminant[110], [114], naive Bayes [110], [115], support vector machine based classifier [116], [117]). The rest of Section 4.2 is structured as follows: first, the data used in this study is described; after that the proposed classification framework is described. Then, a brief description of performance evaluators used in this work is given. After that, experimental results are reported. Finally, we conclude this chapter with a discussion and future scope."}, {"heading": "4.2.1 Data Description", "text": "The well logs used in this work are acquired from four closely spaced boreholes located in an onshore hydrocarbon field of India. Henceforward, these aforementioned wells are to be referred as A, B, C, and D, respectively. The borehole data contains several logs such as gamma ray content (GR), bulk density (RHOB), P-sonic (DT), neutron porosity (NPHI), spontaneous potential (SP), acoustic impedance (AI) and different resistivity logs such as deep resistivity (RT), medium resistivity (RM) and shallow resistivity (RS) logs. Reservoir characteristics, e.g., sand fraction,\nClassification of Water Saturation\n51\nporosity, water saturation, oil saturation etc. are derived from these log properties. Literature study reveals that GR, RHOB, DT, NPHI, SP among different logs are to be used as predictor variables to model or classify lithological properties. After selection of relevant features among available logs, we have used GR, RHOB, DT, and NPHI logs as input attributes to classify water saturation level. The rock properties of subsurface formations can be interpreted from these variables. The gamma radiation of different formations along the depth is represented by gamma ray log in American Petroleum Institute (API) unit. The density log is recorded in grams per cubic centimetre unit. It varies according to mineralogy and porosity values. Travel time of P-waves versus depth is recorded as P-sonic log in microsecond per feet. The fourth predictor variable i.e. neutron porosity log is attuned to read the true porosity and represented in per unit. In this work, the target variable is water saturation, which is an important characteristic in the petroleum industry representing the fraction of formation water present in the pore space."}, {"heading": "4.2.2 Proposed Classification Framework", "text": "In the recent years, SVDD and other kernel-based algorithms have been reported as popular techniques adapted for classification of imbalanced dataset in the field of hyperspectral image processing, outlier detection, document classification etc. In this work, an attempt has been made to construct an SVDD based framework to classify reservoir properties using an imbalanced geological dataset. The proposed generalized framework, which includes three steps namely- 1)\nClassification of Water Saturation\n52\ndata preparation, 2) preliminary analysis, and 3) training and testing, is represented in Fig. 4-1. These steps are briefly discussed in this section."}, {"heading": "4.2.2.1 Data Preparation", "text": "Well log data from four wells located in the western onshore hydrocarbon field of India are used in the present study. The procedure of data preparation is started with data acquisition as shown in Fig. 4-1. The log files contain a number of missing data values. These patterns are removed to make a data file of valid values only. Then we uniformly re-sample the data.\nThis stage is the starting point of the proposed framework. Well logs are selected and preprocessed. Fig. 4-2 represents plots of gamma ray, neutron porosity, bulk density, and resistivity logs along depth for well A. Similarly, Fig. 4-3 represents P-sonic, acoustic impedance, and water saturation logs along depth for the same well. Designing a classifier is required to classify water saturation log from available log variables. The selection of the input variables is carried out using Relief algorithm as discussed in the following section.\nClassification of Water Saturation\n53"}, {"heading": "4.2.2.2 Preliminary Analysis", "text": "Feature selection plays a crucial role in tuning the performance of pattern classifiers. In the preprocessing stage, several number of \u201ccandidate features\u201d are extracted from raw dataset. Then relevant features are selected using different algorithms i.e. mutual information, Relief algorithm, and its variants. Here, we use Relief algorithm [118], which identifies statistically relevant features and performs well in case of noisy dataset, to select input attributes before training the classifier. Designing a classifier with several inputs prolongs the training time along with unnecessary proliferation in the model complexity. Moreover, the generalization capability of a model enhances while using only relevant features as inputs.\nNext, we classify the water saturation into two classes, namely- Class high and Class low using a user-defined threshold. Two factors guide the choice of threshold value. Firstly, saturation values belonging to the Class high must be as close to one as possible while in Class low it must be as close to zero as possible. This is done by observing the histogram of the saturation values. Secondly, the high computational complexity of the SVDD classifier has compelled us to set the threshold in a manner so as to have reasonable small number of patterns at least in one-class to have the classifier trained within reasonable time. This threshold value is modified depending on the training speed of the SVDD algorithm. After completion of the preliminary analysis, training and testing of SVDD based one-class classifier is started. Besides, selection of the threshold level is confirmed by expert geologists.\nFirst, several attributes are extracted from the raw dataset. Then, four relevant attributes are selected from the six \u201ccandidate attributes\u201d using Relief algorithm. The result of the Relief algorithm is represented in Fig. 4-4. It can be observed from the figure that GR, NPHI, RHOB, and DT logs are more relevant features related to water saturation in terms of predictor importance weight compared to RT and AI logs.\nAfter selection of the appropriate input attributes the next task is to classify water saturation into two classes using a user-defined threshold value. We consider two criteria as discussed in an\nClassification of Water Saturation\n54\nearlier section for the selection of the threshold level to classify the water saturation values into two classes. For this particular problem, we choose 0.7 as the threshold value after verifying the constraints related to computational speed of SVDD algorithm and experienced geoscientists\u2019 view. Patterns with a saturation level greater than or equal to 0.7 are called Class high and the other patterns are called Class low. We have 3% of the whole data set in the Class low set. It can be observed from Fig. 4-5 that the distribution of water saturation values is skewed at one. Specifically, 97% of the total available patterns belong to Class high that is associated with higher values of water saturation. Therefore, Class low and Class high can be termed as minority and majority classes respectively."}, {"heading": "4.2.2.3 Training and Testing", "text": "The training and testing steps associated with the one-class classifier are shown in the lower part of Fig. 4-1. In this problem, the available patterns are significantly large in case of Class high compared to Class low. In other words, Class high and Class low can be invariably denoted as majority and minority classes. After training the SVDD using combined minority class patterns of remaining three wells, we test the performance of the classifier using majority class patterns of these three wells along with all the patterns (majority and minority) of the test well. The results reported in this article corresponds to the blind testing of the individual well when classifier learning is carried out using a kernel function and an initial C value. For example, in case of blind prediction of well C, the SVDD hypersphere is constructed using patterns belong to minority class from combined dataset of remaining three wells using Gaussian kernel of width parameter of 2.0, and C = 0.008 as initial parameter setting.\nThe input attributes (GR, RHOB, DT, and NPHI) of the training patterns are used to construct the SVDD hypersphere. Classification accuracy of SVDD is improved by adjusting few parameters: type of the kernel function and associated parameters, and radius of the hypersphere C. The kernel functions such as Gaussian, higher order polynomial (with order 2\u201310), radial basis function, exponential radial basis function, kernel parameters, are experimented with values of C varying from 0 to 1. The classifier uses a Lagrangian function which is minimized using constrained optimization. It divides the patterns into two classes as true data that resides inside the hypersphere and outliers that reside outside the boundary of the hypersphere. The points which make the boundary of the hypersphere are called support vectors. In this work, we include these support vectors in the outlier class. The trained parameters are saved and applied to the majority class to test the classifier performance.\nThe performance of the proposed framework using one-class classifier based on SVDD is evaluated upon the accuracy of both positive and negative classes. Instead of employing confusion matrix, which is generally used to measure performance of classifier, here we use g-metric means [119]. This performance evaluator is often used in case of imbalanced dataset. G-metric means can be represented as\nClassification of Water Saturation\nP Ng= acc *acc (4-6)\nwhere, Pacc and Nacc represent sensitivity and specificity, respectively. Sensitivity indicates the accuracy on the positive instances i.e. (true positives/ (true positives + false negatives)) and similarly, specificity denotes the accuracy on the negative instances i.e. (true negatives/ (true negatives + false positives)).\nProgram execution time is also recorded to compare the performance of proposed framework with respect to other classifiers.\nAfter completion of the training and testing stage, the classification performance achieved using this proposed framework is compared to other classifiers namely discriminant, naive Bayes, and SVM based classifier. SVM, naive base, and discriminant classifiers are optimized after initialization with appropriate parameter values using the same predictor variables. From the test output, the patterns classified as outliers and support vectors are considered to be majority class components; and data vectors are specified as minority class components. Then, comparison is carried out among these supervised classifiers depending on the blind testing result of each of the wells.\nTable 4-1 and Table 4-2 represent comparison result of the proposed framework with other supervised classifiers.\nClassification of Water Saturation\n56\nIt is evident from the Table 4-1 and Table 4-2 that the proposed classifier workflow outperformed other supervised classifiers in terms of g-metric means and program execution time.\nFig. 4-6 and Fig. 4-7 represent the result of performance comparison of the supervised classifiers in terms of g-metric means and program execution time respectively. Therefore, it can be inferred from the results that the proposed workflow based on SVDD can be used as a powerful tool to classify imbalanced dataset in reservoir characterization domain."}, {"heading": "4.2.3 Discussion", "text": "In this work, a complete framework based on SVDD is proposed to classify water saturation from well logs using an imbalanced geological dataset. Comparative analysis reported in this section has shown that the proposed methodology has outperformed existing classifier algorithms in terms of performance evaluators (g-metric means and program execution time). This work can be extended with the inclusion of seismic attributes as inputs to the classifier based model. Integration of the seismic and limited number of available borehole data will help to produce 3D volume representing high and low water saturation values throughout a study area.\nClassification of Water Saturation\n57"}, {"heading": "4.3 Development of a Framework to Classify Water Saturation from Seismic Attributes", "text": "Water saturation is an important property in reservoir engineering domain. Thus, satisfactory classification of water saturation from seismic attributes is beneficial for reservoir characterization. However, diverse and non-linear nature of the subsurface attributes makes the classification task difficult. Section 3.2 has proposed a generalized SVDD based novel classification framework to classify water saturation into two classes (Class high and Class low) from four well logs. In this section, the aforementioned framework is modified to use three seismic attributes such as seismic impedance, amplitude envelope, and seismic sweetness as predictor variables. Like previous section, g-metric means and program execution time are used to quantify the performance of the modified framework along with the established supervised classifiers. The documented results imply that the proposed framework is superior to the existing classifiers. The present study is envisioned to contribute in further reservoir modelling. The contributions of the present study are as follows:\n A complete classification framework integrating seismic and well log signals  Blind prediction  Comparison with other classifiers  Water saturation level map over the area"}, {"heading": "4.3.1 Data Description", "text": "The dataset corresponding to the four wells (Well A, Well B, Well C, and Well D) as in Section 4.2 is used in this section. As an extension of the work carried out in Section 4.2, seismic attributes corresponding to the study area are included as predictor variables instead of well logs to achieve an area map of water saturation level. There are five seismic attributes acquired from the same study area such as seismic impedance, amplitude, instantaneous frequency, amplitude envelope and seismic sweetness. However, seismic impedance, amplitude envelope, and seismic sweetness are selected over amplitude and instantaneous frequency by Relief algorithm."}, {"heading": "4.3.2 Proposed Classification Framework", "text": "A classification framework is designed to classify water saturation from seismic attributes using an imbalanced geological dataset in this section. There are four steps included in the workflow namely\u2013 data preparation, preliminary analysis, training and testing, volumetric classification and visualization of water saturation level map as demonstrated in Fig. 4-8. The steps in the proposed framework are designed by modifying the work done in Section 4.2 and briefly described in this section.\nThe research work carried out in this study are performed on a 64 bit MATLAB platform installed on a Intel(R) Core(TM) i5CPU @3.20 GHz workstation having 16 GB RAM. The following sections describe the experimental results achieved in every step of the proposed framework.\nClassification of Water Saturation\n58"}, {"heading": "4.3.2.1 Data Preparation", "text": "Seismic attributes along with water saturation log corresponding to four well locations are used in this study. As shown in the figure (Fig. 4-8), the procedure is started with data acquisition and integration of seismic and borehole data. First, the well logs are converted into the time domain from the depth domain using the time-depth relationships available at the four well locations. Then, seismic attributes at the four well locations are extracted from seismic volume. It is found that the sampling intervals of these dataset (seismic and well logs) are different. For example, the seismic patterns are sampled at an interval of two milliseconds, whereas the sampling interval of well logs is 0.15 milliseconds. Hence, we interpolate the band limited seismic signals at 0.15 milliseconds sampling interval corresponds to that of the well logs. Thus, the combined dataset of seismic attributes and water saturation is prepared to be used in the preliminary analysis stage.\nFig. 4-9 and Fig. 4-10 represent available five seismic attributes- (Fig. 4-9(a)) seismic impedance, (Fig. 4-9(b)) amplitude, (Fig. 4-9(c)) instantaneous frequency, (Fig. 4-10(a)) seismic amplitude envelope, (Fig. 4-10(b)) seismic sweetness and (Fig. 4-10(c)) water saturation along the\nClassification of Water Saturation\n59\nWell A. The red dots on the seismic attributes represent original values at time interval of two milliseconds and the green curves represent reconstructed signals along the time interval of well log data. The blue curve in Fig. 4-10(c) represents water saturation along the Well A. It can be observed that water saturation distribution is biased towards maximum water saturation value (i.e. one).\nClassification of Water Saturation\n60"}, {"heading": "4.3.2.2 Preliminary Analysis", "text": "The performance of classifiers is dependent on the selection of relevant features. First, a number of \u201ccandidate features\u201d are extracted from the raw dataset. Then, different algorithms i.e. mutual information, Relief algorithm [118], and its variants are used to identify relevant features among available features before starting to train the classifier. In this chapter, Relief algorithm selects statistically relevant features from a noisy dataset. Inclusion of unnecessary inputs in model elongates training time along with an increase in the model complexity. In contrary, application of relevant features as predictor variables enhances the generalization capability of a model [120]. The result of Relief algorithm is represented in Fig. 4-11. Fig. 4-11 reveals that seismic impedance, seismic amplitude envelope, and seismic sweetness are more relevant features with respect to water saturation in terms of predictor importance weight compared to amplitude and instantaneous frequency.\nThen, the water saturation is classified into two classes, namely- Class high and Class low using a user-defined threshold. The selection of the threshold level is governed by two constraints (as in Section 4.2). We selected the initial threshold level as 0.7 as in Section 4.2."}, {"heading": "4.3.2.3 Training and Testing", "text": "The lower part of Fig. 4-8 represents the training and testing steps associated with the classifier. For the working dataset, the number of available samples belongs to Class high is significantly large which in turn makes it majority class. Conversely, Class low is minority class due to the presence of small amount of samples belonging to this category in the working dataset. The division of training and testing pattern is carried out as in Section 4.2. The minority class (Class low) patterns belong to integrated dataset of three wells are used to train the classifier. The tuned\nClassification of Water Saturation\n61\nclassifier parameters are validated using the Class low patterns of test well and the combined majority class (Class high) samples of all the wells.\nThe input attributes (seismic impedance, amplitude envelope, and seismic sweetness) of training patterns are used to construct the SVDD hypersphere. Classification accuracy of SVDD is improved by adjusting multiple parameters such as the kernel function and associated parameters, and radius of the hypersphere C. We have experimented with different kernel functions such as Gaussian, higher order polynomial (with order of 2\u201310), radial basis function, and exponential radial basis function along with associated kernel parameters with C values varying from 0 to 1. The task of the classifier is to minimize the Lagrangian function by constrained optimization as mentioned earlier in Section III. The data samples are categorized into three categories: true data (inside the hypersphere), outliers (outside the hypersphere), and support vectors (at the hypersphere periphery) by this optimization. As in [120], the support vectors are encompassed in the outlier category. The tuned parameters are tested using the majority class samples.\nTo establish the modified framework over existing classifier algorithms (e.g. ANN, and SVM based classifier), a comparison has been carried out. In all cases, the predictor attributes, and performance evaluators are same as the proposed framework. The division of training-testing samples and associated classification parameters are varied depending on respective classifiers. These classifiers are optimized with appropriate parameter values related to respective algorithms. The predictor variables are same (seismic impedance, seismic amplitude envelope, and seismic sweetness) as that of the proposed framework. The difference lies in the creation of training and testing data set. For these classifiers, the learning is carried out using the integrated dataset of three wells. The samples corresponding to the remaining fourth well are used to test the trained classifiers. Thus, majority and minority class components are collectively used to train the network instead of using only minority class patterns.\nThe performance of the modified framework (as in Fig. 4-8) is quantified using g-metric means [119], [120] and program execution time. G-metric means is associated with the accuracy of both positive and negative classes and often used in case of imbalanced dataset. Table 4-3 and Table 4-4 represent the comparison results of the proposed framework with other three classifiers in terms of g-metric mean and program execution time. It can be observed from Table 4-3 that the gmetric mean values in case of ANN based classifier are very poor. Then, the blind testing performance improves while using kernel-based algorithm SVM based classifier. Finally, our framework has yielded better performance compared to both\u2013 ANN and SVM based classifiers. As the number of patterns belongs to the minority class is insignificant compared to that of the majority class; hence, trained classifiers can detect the majority class testing patterns correctly. However, the minority class test patterns are also wrongfully classified in Class high (majority class). Hence, g-metric mean is poor. On the other hand, our framework is based on one-class\nClassification of Water Saturation\n62\nclassification. Therefore, it can detect minority class patterns in testing dataset yielding better gmetric means within reduced program execution time.\nThe results in Table 4-3 and Table 4-4 are pictorially represented in Fig. 4-12 and Fig. 4-13 respectively. Fig. 4-12 and Fig. 4-13 reveal that the proposed framework has attained better performance compared to other classifiers with higher speed.\nClassification of Water Saturation\n63"}, {"heading": "4.3.2.4 Volumetric Classification and Visualization", "text": "The trained parameters which yield acceptable results in the blind testing are saved. Then, the water saturation level in the study area can be estimated from seismic attributes. The saved SVDD parameters classify the water saturation level in Class high or Class low at any location in the study area using seismic attributes of the area. After the classification over the area, the variation of water saturation level is visualized at any selected part of the study area.\nFig. 4-14 represents the variation of seismic impedance, at a particular inline over the study area. The tuned classifier which was saved while blind prediction of well A is further used to classify water saturation over the study area from predictor seismic signals. Fig. 4-15 represents the distribution of water saturation level classified in two categories: Class high and Class low over the area at the same inline. Inside the study area, blue represents Class low and red colour represents Class high samples. It can be observed from Fig. 4-15 that the presence of Class high patterns is significant over that of the Class low samples throughout the area.\nFig. 4-14: Seismic impedance variation at a particular inline\nCross Line\nTi m\ne( m\ns)\n100 200 300 400 500 600\n2150\n2200\n2250\n2300\n2350\n2400\n2450 6\n7\n8\n9\n10 x 10\n6\nClassification of Water Saturation\n64\nSection 4.3 has proposed a classification framework to classify water saturation levels from the seismic attributes using a small imbalanced dataset. In other words, the class labels (Class low/Class high) variation of water saturation can be predicted from the seismic attributes using the modified framework presented in Section 4.3. The area map representing high and low water saturation level is created using the proposed framework."}, {"heading": "4.4 Conclusion", "text": "In this chapter, water saturation is classified from well logs and seismic attributes respectively using a dataset of four wells. Application of the SVDD to solve the class labels prediction problem using integrated dataset of seismic and borehole data in reservoir characterization field is the contribution of this chapter. Water saturation varies from zero to unity with a skew at unity. The class labels variation of other lithological properties having similar skewed nature can be predicted using the proposed frameworks from well logs and seismic attributes respectively. Although the frameworks have outperformed existing supervised classifiers in terms of performance evaluators, there is a scope of improvement in the selection of parameters associated with the SVDD algorithm. The SVDD parameters are selected empirically keeping the improvement in classification in view. In future, efforts can be made to automate the selection procedure using some evolutionary algorithms such as genetic algorithm, particle swarm optimization etc. The execution time taken by the SVDD algorithm is dependent on the number of training patterns available in the minor class. Thus the selection of the user defined classification threshold is carried out keeping this factor in mind along with experienced geophysicist\u2019s opinion. Apart from geological dataset, this framework can be implemented in binary class problems wherever number of training patterns pertaining to a particular class is very less compared to the other class.\nConclusion and Future Scope\n65\nChapter 5. Conclusion and Future Scope In the present study, a novel pre-processing scheme is proposed to improve the prediction capability of machine learning algorithms by information filtering for prediction of a lithological property from seismic attributes in Chapter 2. As a result of this pre-processing scheme, the mutual dependency between predictor and seismic attributes are increased in the expense of the decrease in information content of the target property. The proposed scheme is implemented using seismic impedance, amplitude, and instantaneous frequency to model sand fraction using ANN. The issues associated with the data dimension and size of the training network and complexity associated with the selection of the ANN structure and parameters are discussed briefly. The selection of the network structures and initialization of network parameters are carried out empirically. In future research scope, selection of network structure and parameters can be automated using evolutionary algorithms. Then, in case of the working dataset with small number of training patterns and large dimension, inclusion of a dimensionality deduction algorithm in the pre-processing stage. However, for this study, the available training patterns are large enough compared to the data dimension. Here, post-processing schemes based on different spatial filters with selected window size are implemented to improve the visualization across the volume. Introduction of model based filtering based on variation of predictor attributes across the volume is a probable direction of research. Lithological properties having similar distributions as sand fraction i.e. varying between the minimum and maximum values can be modelled using the frameworks proposed in Chapter 3. For example, shale fraction, porosity, permeability, etc. can be predicted from seismic attributes using the generalized frameworks as in Chapter 3 depending on the availability of the well tops information.\nWater saturation has an imbalanced distribution skewed at unity. Therefore, instead of predicting exact values of water saturation, class labels detection can serve the purpose of a reservoir engineer as the layers having low water saturation is of importance. In Chapter 4, the variation of the class labels (Class low/Class high) of water saturation is predicted using well logs and seismic data using a one-class classification framework based on SVDD. The performances of the proposed frameworks have been compared with other supervised algorithms. The selection of the SVDD parameters are crucial for obtaining good performance. It has been carried out empirically here. In future, the parameters can be automated using different evolutionary algorithms. The class labels of other characteristics having skewed distribution such as oil saturation can be modelled using the frameworks designed here.\nOn the whole this thesis discussed about creation of synthetic logs of lithological properties from seismic attributes of a study area after calibrating a functional relationships between the predictors and target using an integrated dataset of these two types of data at available well control points.\nConclusion and Future Scope\n66"}, {"heading": "5.1 Dissemination out of this Work", "text": "Journal Papers\n1. S. Chaki, A. Routray, and W. K. Mohanty, \u201cA novel pre-processing scheme to improve the prediction of sand fraction from seismic attributes using neural networks,\u201d IEEE J. Sel. Topics Appl. Earth Observations and Remote Sens., 10.1109/JSTARS.2015.2404808 (Accepted). 2. S. Chaki, A. K. Verma, A. Routray, W. K. Mohanty, and M. Jenamani, \u201cWell tops guided prediction of reservoir properties using modular neural network concept: A case study from western onshore, India,\u201d J. Pet. Sci. Eng., vol. 123, pp. 155-163, Nov. 2014. 3. A. K. Verma, S. Chaki, A. Routray, W. K. Mohanty, M. Jenamani, \u201cQuantification of sand fraction from seismic attributes using Neuro-Fuzzy approach,\u201d J. Appl. Geophysics, vol. 111, pp. 141-155, Dec. 2014.\nConference Papers 1. S. Chaki, A. K. Verma, A. Routray, M. Jenamani, W. K. Mohanty, P. K. Chaudhuri, and\nS. K. Das, \u201cPrediction of porosity and sand fraction from well log data using ANN and ANFIS\u202f: a comparative study,\u201d in 10th Biennial Int. Conf. Expo. SPG, Kochi, India, 2013. 2. S. Chaki, A. K. Verma, A. Routray, W. K. Mohanty, and M. Jenamani, \u201cA one-class classification framework using SVDD\u202f: application to an imbalanced geological dataset,\u201d in Proc. IEEE Students\u2019 Technology Symp. (TechSym), Kharagpur, India, 2014, pp. 76\u201381. 3. S. Chaki, A. K. Verma, A. Routray, W. K. Mohanty, and M. Jenamani, \u201cA novel framework based on SVDD to classify water saturation from seismic attributes,\u201d in Fourth Int. Conf. Emerging Applicat. Inform. Technology (EAIT), Kolkata, India, 2014, pp. 64\u2013 69."}, {"heading": "5.2 Future Scope", "text": "The following appears to be promising area for future research\n As initial parameters selection of machine learning algorithms crucial for achieving acceptable performance, it would be interesting to automate the initialization parameters by an appropriate metaheuristic algorithm\n Model based post\u2013processing instead of spatial filtering on predicted lithological properties  Uncertainty quantification associated with Reservoir characterization: Some interesting\nworks have been reported in [121], [122] about application of uncertainty quantification in reservoir characterization. These publications can be used as a guiding point to explore uncertainty analysis related to modeling and data acquisition process of reservoir characterization.\n67"}], "references": [{"title": "Soft computing-based computational intelligent for reservoir characterization", "author": ["M. Nikravesh"], "venue": "Expert Syst. with Appl., vol. 26, no. 1, pp. 19\u201338, Jan. 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Soft computing : tools for intelligent reservoir characterization (IRESC) and optimum well placement (OWP)", "author": ["M. Nikravesh", "R.D. Adams", "R.A. Levey"], "venue": "J. Pet. Sci. Eng., vol. 29, pp. 239\u2013262, 2001.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Past, present and future intelligent reservoir characterization trends", "author": ["M. Nikravesh", "F. Aminzadeh"], "venue": "J. Pet. Sci. Engi., vol. 31, no. 2\u20134, pp. 67\u201379, Nov. 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Neural network prediction of porosity and permeability of heterogeneous gas sand reservoirs using NMR and conventional logs", "author": ["G.M. Hamada", "M.A. Elshafei"], "venue": "NAFTA, vol. 61, no. 10, pp. 451\u2013460, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Application of seismic attributes and neural network for sand probability prediction \u2014 A case study in the North Malay Basin", "author": ["J. Hou", "T. Takahashi1", "A. Katoh1", "S. Jaroonsitha", "K.P. Chumsena", "K. Nakayama"], "venue": "Bull. Geol. Soc. Malaysia, vol. 54, pp. 115\u2013121, 2008.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Determination of lithology from well logs using a neural network", "author": ["S.J. Rogers", "J.H. Fang", "C.L. Karr", "D.A. Stanley"], "venue": "Am. Assoc. Pet. Geol. Bull., vol. 76, pp. 731\u2013739, 1992.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1992}, {"title": "Application of artificial neural networks for reservoir characterization with limited data", "author": ["K. Aminian", "S. Ameri"], "venue": "J. Pet. Sci. Eng., vol. 49, no. 3\u20134, pp. 212\u2013222, Dec. 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural Fuzzy System Development for the Prediction of Permeability From Wireline Data Based on Fuzzy Clustering", "author": ["H. Kaydani", "A. Mohebbi", "A. Baghaie"], "venue": "Pet. Sci. Eng., vol. 30, no. 19, pp. 2036\u20132045, Jul. 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Reservoir properties determination using fuzzy logic and neural networks from well data in offshore Korea", "author": ["J.-S. Lim"], "venue": "J. Pet. Sci. Engi., vol. 49, no. 3\u20134, pp. 182\u2013192, Dec. 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Seismic inversion for reservoir properties combining statistical rock physics and geostatistics: A review", "author": ["M. Bosch", "T. Mukerji", "E.F. Gonzalez"], "venue": "Geophysics, vol. 75, no. 5, pp. 75A165\u201375A176, Sep. 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Porosity from seismic data: A geostatistical approach", "author": ["P.M. Doyen"], "venue": "Geophysics, vol. 53, no. 10, pp. 1263\u2013 1276, 1998.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1998}, {"title": "Neural networks: a comprehensive foundation", "author": ["S. Haykin"], "venue": "New Jersey, USA: Prentice hall,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "ANFIS: Adaptive-network-based fuzzy inference system", "author": ["J. Jang"], "venue": "IEEE Trans. Syst. Man Cybern., vol. 23, no. 3, pp. 665\u2013685, 1993.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1993}, {"title": "Modeling the permeability of carbonate reservoir using type-2 fuzzy logic systems", "author": ["S.O. Olatunji", "A. Selamat", "A. Abdulraheem"], "venue": "Comput. Ind., vol. 62, no. 2, pp. 147\u2013163, Feb. 2011.  68", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Hybrid computational models for the characterization of oil and gas reservoirs", "author": ["T. Helmy", "A. Fatai", "K. Faisal"], "venue": "Expert Syst. Appl., vol. 37, no. 7, pp. 5353\u20135363, Jul. 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Fuzzy logic-driven and SVM-driven hybrid computational intelligence models applied to oil and gas reservoir characterization", "author": ["F. Anifowose", "A. Abdulraheem"], "venue": "J. Nat. Gas Sci. Eng., vol. 3, no. 3, pp. 505\u2013517, Jul. 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Lithologic Characterization of a Reservoir Using Continuous-Wavelet Transforms", "author": ["G. \u00c1lvarez", "B. Sans\u00f3", "R.J. Michelena", "J.R. Jim\u00e9nez"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 41, no. 1, pp. 59\u201365, Jan. 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Reservoir permeability prediction by neural networks combined with hybrid genetic algorithm and particle swarm optimization", "author": ["M. Ali Ahmadi", "S. Zendehboudi", "A. Lohi", "A. Elkamel", "I. Chatzis"], "venue": "Geophys. Prospect., vol. 61, no. 3, pp. 582\u2013598, May 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "A neuro-fuzzy GA-BP method of seismic reservoir fuzzy rules extraction", "author": ["S. Yu", "X. Guo", "K. Zhu", "J. Du"], "venue": "Expert Syst. with Appl., vol. 37, no. 3, pp. 2037\u20132042, Mar. 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Use of multiattribute transforms to predict log properties from seismic data", "author": ["D.P. Hampson", "J.S. Schuelke", "J. a. Quirein"], "venue": "Geophysics, vol. 66, no. 1, pp. 220\u2013236, Jan. 2001.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Confidence bounds of petrophysical predictions from conventional neural networks", "author": ["P.M. Wong", "A.G. Bruce", "T.T.D. Gedeon"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 40, no. 6, pp. 1440\u20131444, 2002.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "A neural network approach to improve the vertical resolution of atmospheric temperature profiles from geostationary satellites", "author": ["N. Sharma", "M.M. Ali"], "venue": "IEEE Geosci. Remote Sens. Lett., vol. 10, no. 1, pp. 34\u2013 37, Jan. 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "A neural network approach to estimate tropical cyclone heat potential in the Indian ocean", "author": ["M.M. Ali", "P.S.V. Jagadeesh", "I.-I. Lin", "J.-Y. Hsu"], "venue": "IEEE Geosci. Remote Sens. Lett., vol. 9, no. 6, pp. 1114\u20131117, Nov. 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Macrocell path-loss prediction using artificial neural networks", "author": ["E. Ostlin", "H.-J. Zepernick", "H. Suzuki"], "venue": "IEEE Trans. Veh. Technol., vol. 59, no. 6, pp. 2735\u20132747, Jul. 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving offline handwritten text recognition with hybrid HMM/ANN models", "author": ["S. Espa\u00f1a-Boquera", "M.J. Castro-Bleda", "J. Gorbe-Moya", "F. Zamora-Martinez"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 4, pp. 767\u2013779, Apr. 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Designing a neural network for forecasting financial and economic time series", "author": ["I. Kaastra", "M. Boyd"], "venue": "Neurocomputing, vol. 10, pp. 215\u2013236, 1996.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1996}, {"title": "Modular artificial neural network for prediction of petrophysical properties from well log data", "author": ["H. Eren", "C.C. Fung", "K.W. Wong"], "venue": "IEEE Trans. Instrum. Meas., vol. 46, no. 6, pp. 1295\u20131299, Dec. 1997.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1997}, {"title": "An improved technique in porosity prediction: a neural network approach", "author": ["P.M. Wong", "T.D. Gedeon", "I.J. Taggart"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 33, no. 4, pp. 971\u2013980, Jul. 1995.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1995}, {"title": "Intelligent reservoir characterization ( IRESC)", "author": ["M. Nikravesh", "M. Hassibi"], "venue": "Proc. IEEE Int. Conference on Ind. Informatics, 2003, pp. 369\u2013373.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Well tops guided prediction of reservoir properties using modular neural network concept: a case study from western onshore, India", "author": ["S. Chaki", "A.K. Verma", "A. Routray", "W.K. Mohanty", "M. Jenamani"], "venue": "J. Pet. Sci. Eng., vol. 123, pp. 155 \u2013 163, Jul. 2014.  69", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Inversion of traveltime data under a statistical model for seismic velocities and layer interfaces", "author": ["M. Bosch", "P. Barton", "S.C. Singh", "I. Trinks"], "venue": "Geophysics, vol. 70, no. 4, pp. R33\u2013R43, 2005.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Data mining and fusion with integrated neuro-fuzzy agents rock properties and seismic attenuation", "author": ["M. Nikravesh", "B. Novak", "F. Aminzadeh"], "venue": "Proc. the Fourth Joint Conference on Information Sciences, 1998.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1998}, {"title": "A new approach to improve neural networks\u2019 algorithm in permeability prediction of petroleum reservoirs using supervised committee machine neural network (SCMNN)", "author": ["S. Karimpouli", "N. Fathianpour", "J. Roohi"], "venue": "J. Pet. Sci. Engi., vol. 73, no. 3\u20134, pp. 227\u2013232, Sep. 2010.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparative evaluation of back-propagation neural network learning algorithms and empirical correlations for prediction of oil PVT properties in Iran oilfields", "author": ["J. Asadisaghandi", "P. Tahmasebi"], "venue": "J. Pet. Sci. Eng., vol. 78, no. 2, pp. 464\u2013475, Aug. 2011.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "A fast and independent architecture of artificial neural network for permeability prediction", "author": ["P. Tahmasebi", "A. Hezarkhani"], "venue": "J. Pet. Sci. Eng., vol. 86\u201387, pp. 118\u2013126, May 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Identification of well logs with significant impact on prediction of oil and gas reservoirs permeability using statistical analysis of RSE values", "author": ["A. Majdi", "M. Beiki", "A. Pirayehgar", "G. Hosseinyar"], "venue": "J. Pet. Sci. Eng., vol. 75, no. 1\u20132, pp. 91\u201399, Dec. 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "Modular neural networks: a survey", "author": ["G. Auda", "M. Kamel"], "venue": "Int. J. Neural Syst., vol. 9, no. 2, pp. 129\u201351, Apr. 1999.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1999}, {"title": "Design of structural modular neural networks with genetic algorithm", "author": ["N. Jiang", "Z. Zhao", "L. Ren"], "venue": "Adv. Eng. Softw., vol. 34, no. 1, pp. 17\u201324, Jan. 2003.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "Development of artificial neural network models for predicting water saturation and fluid distribution", "author": ["N. Al-Bulushi", "P.R. King", "M.J. Blunt", "M. Kraaijveld"], "venue": "J. Pet. Sci. Eng., vol. 68, no. 3\u20134, pp. 197\u2013208, Oct. 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "A new hybrid artificial neural networks for rainfall\u2013 runoff process modeling", "author": ["S. Asadi", "J. Shahrabi", "P. Abbaszadeh", "S. Tabanmehr"], "venue": "Neurocomputing, vol. 121, pp. 470\u2013480, Dec. 2013.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "An artificial neural network model for predicting the recovery performance of surfactant polymer floods", "author": ["M.M. Al-Dousari", "A. a. Garrouch"], "venue": "J. Pet. Sci. Eng., vol. 109, pp. 51\u201362, Sep. 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "A new approach combining Karhunen-Loeve decomposition and artificial neural network for estimating tight gas sand permeability", "author": ["N. Smaoui", "A.A. Garrouch"], "venue": "J. Pet. Sci. Eng., vol. 18, pp. 101\u2013112, 1997.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1997}, {"title": "Interval type-2 fuzzy logic and modular neural networks for face recognition applications", "author": ["O. Mendoza", "P. Mel\u00edn", "O. Castillo"], "venue": "Appl. Soft Comput., vol. 9, pp. 1377\u20131387, Sep. 2009.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "A modular neural network architecture with concept", "author": ["Y. Ding", "Q. Feng", "T. Wang", "X. Fu"], "venue": "Neurocomputing, vol. 125, pp. 3\u20136, Feb. 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimization of modular granular neural networks using hierarchical genetic algorithms for human recognition using the ear biometric measure", "author": ["D. S\u00e1nchez", "P. Melin"], "venue": "Eng. Appl. Artif. Intell., vol. 27, pp. 41\u2013 56, Jan. 2014.  70", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2014}, {"title": "Petrophysical data prediction from seismic attributes using committee fuzzy inference system", "author": ["A. Kadkhodaie-Ilkhchi", "M.R. Rezaee", "H. Rahimpour-Bonab", "A. Chehrazi"], "venue": "Comput. Geosci., vol. 35, no. 12, pp. 2314\u2013 2330, Dec. 2009.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2009}, {"title": "Detecting stratigraphic discontinuities using wavelet and S- transform analysis of well log data", "author": ["A.K. Verma", "B.A. Cheadle", "W.K. Mohanty", "A. Routray", "L. Mansinha"], "venue": "GeoConvention 2012: Vision, 2012, pp. 1\u20138.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2012}, {"title": "Determination of facies from well logs using modular neural networks", "author": ["A. Bhatt", "H.B. Helle"], "venue": "Pet. Geosci., vol. 8, no. 3, pp. 217\u2013228, Sep. 2002.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2002}, {"title": "Application of dynamic programming to the correlation of paleoclimate records", "author": ["L.E. Lisiecki", "P. a. Lisiecki"], "venue": "Paleoceanography, vol. 17, no. 4, pp. 1\u201312, Dec. 2002.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2002}, {"title": "Visibility graph analysis of geophysical time series: potentials and possible pitfalls", "author": ["R.V. Donner", "J.F. Donges"], "venue": "Acta Geophys., vol. 60, no. 3, pp. 589\u2013623, Apr. 2012.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "Assessment of similarity between well logs using synchronization measures", "author": ["A.K. Verma", "A. Routray", "W.K. Mohanty"], "venue": "IEEE Geosci. Remote Sens. Lett., pp. 1\u20135, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Practical application of fuzzy logic and neural networks to fractured reservoir characterization", "author": ["A. Ouenes"], "venue": "Comput. Geosci., vol. 26, no. 8, pp. 953\u2013962, Oct. 2000.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2000}, {"title": "Prediction of porosity and sand fraction from well log data using ANN and ANFIS : a comparative study", "author": ["S. Chaki", "A.K. Verma", "A. Routray", "M. Jenamani", "W.K. Mohanty", "P.K. Chaudhuri", "S.K. Das"], "venue": "10th Biennial International Conference & Exposition of SPG, 2013.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2013}, {"title": "Quantifying sand fraction from seismic attributes using modular artificial neural network", "author": ["A.K. Verma", "S. Chaki", "A. Routray", "W.K. Mohanty", "M. Jenamani", "P.K. Chaudhuri", "S.K. Das"], "venue": "10th Biennial International Conference & Exposition of SPG, 2013.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2013}, {"title": "A methodological approach for reservoir heterogeneity characterization using artificial neural networks", "author": ["S. Mohaghegh", "R. Arefi", "S. Ameri", "M.H. Hefner"], "venue": "SPE Annual Technical Conference and Exhibition, SPE 28394, 1994, pp. 1\u20135.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning from imbalanced data", "author": ["H. He", "E. a. Garcia"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 21, no. 9, pp. 1263\u20131284, Sep. 2009.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2009}, {"title": "Kernel-based methods for hyperspectral image classification", "author": ["G. Camps-valls", "L. Bruzzone", "S. Member"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 43, no. 6, pp. 1\u201312, 2005.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2005}, {"title": "A semilabeled-sample-driven bagging technique for ill-posed classification problems", "author": ["M. Chi", "L. Bruzzone"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 2, no. 1, pp. 69\u201373, 2005.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2005}, {"title": "A novel transductive SVM for semisupervised classification of remote-sensing images", "author": ["L. Bruzzone", "M. Chi", "M. Marconcini"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 44, no. 11, pp. 3363\u20133373, 2006.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2006}, {"title": "Partially supervised oil-slick detection by SAR imagery using kernel expansion", "author": ["G. Mercier", "F. Girard-Ardhuin"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 44, no. 10, pp. 2839\u20132846, Oct. 2006.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2006}, {"title": "One-class classification, Concept-learning in the absence of counter-examples", "author": ["D.M. . Tax"], "venue": "Delft Univ. Technol., Delft, The Netherlands, 2001.  71", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2001}, {"title": "A Support Vector Domain Description approach to supervised classification of remote sensing images", "author": ["J. Mu\u00f1oz-mar\u00ed", "L. Bruzzone", "G. Camps-valls"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 45, no. 8, pp. 2683\u20132692, 2007.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2007}, {"title": "Support vector domain description", "author": ["D.M. . Tax", "R.P. . Duin"], "venue": "Pattern Recognit. Lett., vol. 20, no. 11\u2013 13, pp. 1191\u20131199, Nov. 1999.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1999}, {"title": "Support Vector Data Description", "author": ["D. Fisher", "D.M. . Tax", "R.P. . Duin"], "venue": "Mach. Learn., vol. 54, no. 1, pp. 45\u201366, 2004.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2004}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2004}, {"title": "Report on recommended standard for digital tape formats", "author": ["K.M. Barry", "D.A. Cavers", "C.W. Kneale"], "venue": "Geophysics, vol. 40, no. 2, pp. 344\u2013352, 1975.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1975}, {"title": "Kansas Geological Survey", "author": ["J.R. Victorine"], "venue": "http://www.kgs.ku.edu/stratigraphic/PROFILE/HELP/Help- PC-SaveLASFile.html, 2011. .", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2011}, {"title": "Complex seismic trace analysis", "author": ["M.T. Taner", "F. Koehler", "R.E. Sheriff"], "venue": "Geophysics, vol. 44, no. 6, pp. 1041\u20131063, Jun. 1979.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 1979}, {"title": "Ground-penetrating radar for landscape archaeology: Method and applications", "author": ["L.B. Conyers"], "venue": "Seeing the Unseen. Geophysics and Landscape Archaeology, 2009, pp. 245 \u2013 255.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2009}, {"title": "Seismic resolution and frequency fitering", "author": ["B. Rafaelsen"], "venue": "Univ. Tromso Lecture Series, Tromso, Norway, 2006.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2006}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": null, "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1991}, {"title": "Statistical Mechanics and Information-Theoretic Perspectives on Complexity in the Earth System", "author": ["G. Balasis", "R. Donner", "S. Potirakis", "J. Runge", "C. Papadimitriou", "I. Daglis", "K. Eftaxias", "J. Kurths"], "venue": "Entropy, vol. 15, no. 11, pp. 4844\u20134888, Nov. 2013.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2013}, {"title": "Normalized mutual information feature selection", "author": ["P. a Est\u00e9vez", "M. Tesmer", "C. a Perez", "J.M. Zurada"], "venue": "IEEE Trans. Neural Netw., vol. 20, no. 2, pp. 189\u2013201, Feb. 2009.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2009}, {"title": "Effect of sleep deprivation on estimated distributed sources for scalp EEG signals : A case study on human drivers", "author": ["A. Chaudhuri", "A. Routray", "S. Kar"], "venue": "IEEE 4th Int. Conf. Intelligent Human Comput. Interaction, 2012, pp. 1 \u2013 6.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2012}, {"title": "Magnetospheric ULF wave studies in the frame of Swarm mission: a time-frequency analysis tool for automated detection of pulsations in magnetic and electric field observations", "author": ["G. Balasis", "I.A. Daglis", "M. Georgiou", "C. Papadimitriou", "R. Haagmans"], "venue": "Earth, Planets Sp., vol. 65, no. 11, pp. 1385\u20131398, Nov. 2013.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2013}, {"title": "ULF wave activity during the 2003 Halloween superstorm: multipoint observations from CHAMP, Cluster and Geotail missions", "author": ["G. Balasis", "I. a. Daglis", "E. Zesta", "C. Papadimitriou", "M. Georgiou", "R. Haagmans", "K. Tsinganos"], "venue": "Ann. Geophys., vol. 30, no. 12, pp. 1751\u20131768, Dec. 2012.  72", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "Wavelet-based multiscale analysis of geomagnetic disturbance", "author": ["N. Zaourar", "M. Hamoudi", "M. Mandea", "G. Balasis", "M. Holschneider"], "venue": "Earth, Planets Sp., vol. 65, no. 12, pp. 1525\u20131540, Dec. 2013.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2013}, {"title": "A Wavelet Tour of Signal Processing, 2nd Ed", "author": ["S. Mallat"], "venue": null, "citeRegEx": "81", "shortCiteRegEx": "81", "year": 1999}, {"title": "The wavelet transform, time-frequency localization and signal analysis", "author": ["I. Daubechies"], "venue": "IEEE Trans. Inf. Theory, vol. 36, no. 5, pp. 961\u20131005, Sep. 1990.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 1990}, {"title": "Analysis of electroencephalograph signals for detecting fatigue in human drivers", "author": ["S. Kar"], "venue": "Indian Institute of Technology Kharagpur, India, Kharagpur, India, 2011.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2011}, {"title": "Wavelets and Signal Processing", "author": ["O. Rioul", "M. Vetterli"], "venue": "IEEE Signal Processing Mag., pp. 14\u201338, 1991.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 1991}, {"title": "Time-frequency analysis theory and applications", "author": ["L. Cohen"], "venue": null, "citeRegEx": "85", "shortCiteRegEx": "85", "year": 1995}, {"title": "Empirical mode decomposition based time-frequency attributes", "author": ["I. Magrin-chagnolleau", "R.G. Baraniuk"], "venue": "69th Annual International Meeting, SEG, Expanded Abstracts, 1999, pp. 1949\u20131953.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 1999}, {"title": "The empirical mode decomposition and the Hilbert spectrum for non-linear and non-stationary time series analysis", "author": ["N.E. Huang", "Z. Shen", "S.R. Long", "M.C. Wu", "H.H. Shih", "Q. Zheng", "N.-C. Yen", "C.C. Tung", "H.H. Liu"], "venue": "Proc. R. Soc. London. Ser. A Math. Phys. Eng. Sci., vol. 454, no. 1971, pp. 903\u2013995, 1998.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 1971}, {"title": "Empirical Mode Decomposition vs . Wavelet Decomposition for the Extraction of Respiratory Signal from Single-Channel ECG : A Comparison", "author": ["D. Labate", "F. La Foresta", "G. Occhiuto", "F.C. Morabito", "A. Lay-ekuakille", "P. Vergallo"], "venue": "IEEE Sensors J., vol. 13, no. 7, pp. 2666\u20132674, Jul. 2013.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic artifact rejection from multichannel scalp EEG by wavelet ICA", "author": ["N. Mammone", "F. La Foresta", "F.C. Morabito", "S. Member"], "venue": "IEEE Sensors J., vol. 12, no. 3, pp. 533\u2013542, Mar. 2012.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2012}, {"title": "Wavelet Methods for Time Series Analysis", "author": ["D.B. Perciva", "A.T. Walden"], "venue": null, "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2000}, {"title": "Filter Bank Property of Multivariate Empirical Mode Decomposition", "author": ["N. ur Rehman", "D.P. Mandic"], "venue": "IEEE Trans. Signal Process., vol. 59, no. 5, pp. 2421\u20132426, May 2011.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2011}, {"title": "Multivariate empirical mode decomposition", "author": ["N. Rehman", "D.P. Mandic"], "venue": "Proc. R. Soc. London. Ser. A Math. Phys. Eng. Sci., vol. 466, no. 2117, pp. 1291\u20131302, 2010.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2010}, {"title": "A Fast Empirical Mode Decomposition Technique for Nonstationary Nonlinear Time Series", "author": ["C.D. Blakely"], "venue": null, "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2005}, {"title": "Fundamentals of neural neworks : architectures, algorithms, and applications", "author": ["L. Fausett"], "venue": "New Jersey, USA: Prentice hall,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 1994}, {"title": "Why Tanh : chossing a sigmoidal function", "author": ["B.L. Kalman", "S.C. Kwasny"], "venue": "Int. Joint Conf. on Neural Networks, 1992, vol. 4, pp. 578\u2013581.  73", "citeRegEx": "97", "shortCiteRegEx": null, "year": 1992}, {"title": "A scaled conjugate gradient algorithm for fast supervised learning", "author": ["M.F. Moller"], "venue": "Neural Networks, vol. 6, pp. 525\u2013533, 1993.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 1993}, {"title": "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function", "author": ["M. Leshno", "V.Y. Lin", "A. Pinkus", "S. Schocken"], "venue": "Neural Networks, vol. 6, pp. 861\u2013867, 1993.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 1993}, {"title": "Prediction of permeability in a tight gas reservoir by using three soft computing approaches: A comparative study", "author": ["S. Baziar", "M. Tadayoni", "M. Nabi-Bidhendi", "M. Khalili"], "venue": "J Nat. Gas Sci. Eng., vol. 21, pp. 718\u2013724, Nov. 2014.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2014}, {"title": "A Fast Two-Dimensional Median Filtering Algorithm", "author": ["T.S. Huang", "G.J. Yang", "G.Y. Tang"], "venue": "IEEE Trans. Acoust., Speech, Signal Process., vol. ASSP-27, no. 1, pp. 13\u201318, Feb. 1979.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 1979}, {"title": "Digital image processing, Second", "author": ["R. Gonzalez", "R. Woods"], "venue": "New Jersey, USA: Prentice hall,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2002}, {"title": "High-performance 3D median filter architecture for medical image despeckling", "author": ["M. Jiang", "D. Crookes"], "venue": "Electron. Lett., vol. 42, no. 24, pp. 1379\u20131380, 2006.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural networks as an intelligence amplification tool : a review of applications", "author": ["M.M. Poulton"], "venue": "Geophysics, vol. 67, no. 3, pp. 979\u2013993, 2002.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2002}, {"title": "Modular neural network approach for short term flood forecasting a comparative study", "author": ["R.P. Deshmukh", "A.A. Ghatol"], "venue": "Int. J. Adv. Comput. Sci. Appl., vol. 1, no. 5, pp. 81\u201387, 2010.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient classification for multiclass problems using modular neural networks", "author": ["R. Anand", "K. Mehrotra", "C.K. Mohan", "S. Ranka"], "venue": "IEEE Trans. Neural Netw., vol. 6, no. 1, pp. 117\u2013124, Jan. 1995.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 1995}, {"title": "Task decomposition and module combination based on class relations: a modular neural network for pattern classification", "author": ["B.L. Lu", "M. Ito"], "venue": "IEEE Trans. Neural Netw., vol. 10, no. 5, pp. 1244\u20131256, Jan. 1999.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 1999}, {"title": "Communication in the presence of noise", "author": ["C.E. Shannon"], "venue": "Proc. IRE National Convention, 1949, pp. 10\u2013 21.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 1949}, {"title": "Cubic Spline Interpolation", "author": ["C.O. Neill"], "venue": "Acta Math. Hungarica, vol. 107, no. May, pp. 493\u2013507, 2002.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2002}, {"title": "Support Vector Clustering", "author": ["A. Ben-HUr", "D. Horn", "H.T. Siegelmann", "V. Vapnik"], "venue": "J. Mach. Learn. Res., vol. 2, pp. 125\u2013137, 2001.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2001}, {"title": "Parameter Optimization of Kernel-based One-class Classifier on Imbalance Learning", "author": ["L. Zhuang", "H. Dai"], "venue": "J. Comput., vol. 1, no. 7, pp. 32\u201340, 2006.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2006}, {"title": "Parameter Optimization of Kernel-based One-class Classifier on Imbalance Text Learning", "author": ["L. Zhuang", "H. Dai"], "venue": "Lect. Notes Artif. Intell. Springer, vol. 4009, pp. 434\u2013443, 2006.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2006}, {"title": "Determination of lithology from well logs by statistical analysis", "author": ["J.M. Busch", "W.G. Fortney", "L.N. Berry"], "venue": "SPE Form Eval., vol. 2, pp. 412\u2013418, 1987.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 1987}, {"title": "Mclachlan, Discriminant Analysis and Statistical Pattern Recognition", "author": ["J. G"], "venue": null, "citeRegEx": "115", "shortCiteRegEx": "115", "year": 1992}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B.E. Boser", "I.M. Guyon", "V.N. Vapnik"], "venue": "Proc. of the fifth annual workshop on Computational learning theory, 1992, pp. 144\u2013152.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 1992}, {"title": "Support vector method for function approximation, regression estimation, and signal processing", "author": ["V. Vapnik", "S.E. Golowich", "A. Smola"], "venue": "Advances i. Cambridge,: MA: MIT Press,", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 1997}, {"title": "A practical approach to feature selection", "author": ["K. Kira", "L.A. Rendell"], "venue": "Proc. Ninth Int. Workshop on Machine Learning, 1992, pp. 249\u2013256.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 1992}, {"title": "Addressing the curse of imbalanced training sets: one-sided selection", "author": ["M. Kubat", "S. Matwin"], "venue": "Proc. 14th Int. Conference on Machine Learning, 1997, pp. 179\u2013186.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 1997}, {"title": "A one-class classification framework using SVDD : application to an imbalanced geological dataset", "author": ["S. Chaki", "A.K. Verma", "A. Routray", "W.K. Mohanty", "M. Jenamani"], "venue": "Proc. IEEE Students\u2019 Technology Symp. (TechSym), 2014, pp. 76\u201381.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2014}, {"title": "Uncertainty analysis in upscaling well log data by Markov chain Monte Carlo method", "author": ["K. Hwang"], "venue": "MS Thesis, Dept. Geophysics, Texas A&M University, Texas, USA, 2009.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2009}, {"title": "Statistical rock physics: combining rock physics, information theory, and geostatistics to reduce uncertainty in seismic reservoir characterization", "author": ["T. Mukerji", "P. Avseth", "G. Mavko", "I. Takahashi", "E.F. Gonz\u00e1lez"], "venue": "Lead. Edge, vol. 20, no. 3, pp. 313\u2013319, Mar. 2001.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "The applications of advanced statistical, machine learning and pattern recognition techniques to such problems have received considerable interest among the researchers in oil-gas sector [2], [3].", "startOffset": 187, "endOffset": 190}, {"referenceID": 1, "context": "The applications of advanced statistical, machine learning and pattern recognition techniques to such problems have received considerable interest among the researchers in oil-gas sector [2], [3].", "startOffset": 192, "endOffset": 195}, {"referenceID": 2, "context": "The objective of these types of studies is to identify potential zone for drilling a new well [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "Since there is no direct measurement for the lithological parameters, they are to be computed from other geophysical logs [5] or seismic attributes [6].", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "Since there is no direct measurement for the lithological parameters, they are to be computed from other geophysical logs [5] or seismic attributes [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "In this context, Artificial Neural Network (ANN) and its variants with Fuzzy Logic are considered to be useful tools to establish a mapping between lithological and well log properties [7]\u2013[10].", "startOffset": 185, "endOffset": 188}, {"referenceID": 8, "context": "In this context, Artificial Neural Network (ANN) and its variants with Fuzzy Logic are considered to be useful tools to establish a mapping between lithological and well log properties [7]\u2013[10].", "startOffset": 189, "endOffset": 193}, {"referenceID": 9, "context": "It is suggested that the use of 3D seismic data along with well logs can provide better insights while extrapolating reservoir properties away from the existing wells [11], [12].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "It is suggested that the use of 3D seismic data along with well logs can provide better insights while extrapolating reservoir properties away from the existing wells [11], [12].", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "ANN [13], Adaptive Neuro-Fuzzy Inference System (ANFIS) [14], Support Vector Machine (SVM) [15], type-2 Fuzzy Logic system [16], and hybrid systems [17], [18] are some the efficient machine learning tools used in the field of reservoir characterization.", "startOffset": 4, "endOffset": 8}, {"referenceID": 12, "context": "ANN [13], Adaptive Neuro-Fuzzy Inference System (ANFIS) [14], Support Vector Machine (SVM) [15], type-2 Fuzzy Logic system [16], and hybrid systems [17], [18] are some the efficient machine learning tools used in the field of reservoir characterization.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "ANN [13], Adaptive Neuro-Fuzzy Inference System (ANFIS) [14], Support Vector Machine (SVM) [15], type-2 Fuzzy Logic system [16], and hybrid systems [17], [18] are some the efficient machine learning tools used in the field of reservoir characterization.", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": "ANN [13], Adaptive Neuro-Fuzzy Inference System (ANFIS) [14], Support Vector Machine (SVM) [15], type-2 Fuzzy Logic system [16], and hybrid systems [17], [18] are some the efficient machine learning tools used in the field of reservoir characterization.", "startOffset": 148, "endOffset": 152}, {"referenceID": 15, "context": "ANN [13], Adaptive Neuro-Fuzzy Inference System (ANFIS) [14], Support Vector Machine (SVM) [15], type-2 Fuzzy Logic system [16], and hybrid systems [17], [18] are some the efficient machine learning tools used in the field of reservoir characterization.", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "1 Literature Review Hydrocarbons migrate from source rock through porous medium to reach reservoir rock for temporary preservation [19].", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "In such cases, available seismic attributes can be used as a guidance to predict lithological information at all traces of the area of interest [11].", "startOffset": 144, "endOffset": 148}, {"referenceID": 17, "context": "[20] explored that nonlinear problems can be approached using state-of-art computer\u2013based methods like expert systems [21], multiple regression, neural networks [22], Neuro-fuzzy Systems [23] etc.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] explored that nonlinear problems can be approached using state-of-art computer\u2013based methods like expert systems [21], multiple regression, neural networks [22], Neuro-fuzzy Systems [23] etc.", "startOffset": 118, "endOffset": 122}, {"referenceID": 19, "context": "[20] explored that nonlinear problems can be approached using state-of-art computer\u2013based methods like expert systems [21], multiple regression, neural networks [22], Neuro-fuzzy Systems [23] etc.", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "[20] explored that nonlinear problems can be approached using state-of-art computer\u2013based methods like expert systems [21], multiple regression, neural networks [22], Neuro-fuzzy Systems [23] etc.", "startOffset": 187, "endOffset": 191}, {"referenceID": 21, "context": "For example, it is used in climatological studies [24], ocean engineering [25], telecommunications [26], text recognition [27], financial time series [28], reservoir characterization [4], [10], [29]\u2013 [32], etc.", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "For example, it is used in climatological studies [24], ocean engineering [25], telecommunications [26], text recognition [27], financial time series [28], reservoir characterization [4], [10], [29]\u2013 [32], etc.", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "For example, it is used in climatological studies [24], ocean engineering [25], telecommunications [26], text recognition [27], financial time series [28], reservoir characterization [4], [10], [29]\u2013 [32], etc.", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "For example, it is used in climatological studies [24], ocean engineering [25], telecommunications [26], text recognition [27], financial time series [28], reservoir characterization [4], [10], [29]\u2013 [32], etc.", "startOffset": 122, "endOffset": 126}, {"referenceID": 25, "context": "For example, it is used in climatological studies [24], ocean engineering [25], telecommunications [26], text recognition [27], financial time series [28], reservoir characterization [4], [10], [29]\u2013 [32], etc.", "startOffset": 150, "endOffset": 154}, {"referenceID": 2, "context": "For example, it is used in climatological studies [24], ocean engineering [25], telecommunications [26], text recognition [27], financial time series [28], reservoir characterization [4], [10], [29]\u2013 [32], etc.", "startOffset": 183, "endOffset": 186}, {"referenceID": 8, "context": "For example, it is used in climatological studies [24], ocean engineering [25], telecommunications [26], text recognition [27], financial time series [28], reservoir characterization [4], [10], [29]\u2013 [32], etc.", "startOffset": 188, "endOffset": 192}, {"referenceID": 26, "context": "For example, it is used in climatological studies [24], ocean engineering [25], telecommunications [26], text recognition [27], financial time series [28], reservoir characterization [4], [10], [29]\u2013 [32], etc.", "startOffset": 194, "endOffset": 198}, {"referenceID": 29, "context": "For example, it is used in climatological studies [24], ocean engineering [25], telecommunications [26], text recognition [27], financial time series [28], reservoir characterization [4], [10], [29]\u2013 [32], etc.", "startOffset": 200, "endOffset": 204}, {"referenceID": 2, "context": "Linear multiple regression and neural networks are popular among statistical techniques for reservoir modelling from well logs and seismic attributes [4], [33].", "startOffset": 150, "endOffset": 153}, {"referenceID": 30, "context": "Linear multiple regression and neural networks are popular among statistical techniques for reservoir modelling from well logs and seismic attributes [4], [33].", "startOffset": 155, "endOffset": 159}, {"referenceID": 2, "context": "Lately, several computation intensive Artificial Intelligence methods such as ANN, neuro-fuzzy, Self-Organizing Map (SOM), committee machine and Learning Vector Quantization (LVQ) have attained recognition as the potential tools to solve nonlinear and complex problems in the domain of reservoir characterization [4], [5], [11], [29], [31], [34]\u2013 [38].", "startOffset": 313, "endOffset": 316}, {"referenceID": 3, "context": "Lately, several computation intensive Artificial Intelligence methods such as ANN, neuro-fuzzy, Self-Organizing Map (SOM), committee machine and Learning Vector Quantization (LVQ) have attained recognition as the potential tools to solve nonlinear and complex problems in the domain of reservoir characterization [4], [5], [11], [29], [31], [34]\u2013 [38].", "startOffset": 318, "endOffset": 321}, {"referenceID": 9, "context": "Lately, several computation intensive Artificial Intelligence methods such as ANN, neuro-fuzzy, Self-Organizing Map (SOM), committee machine and Learning Vector Quantization (LVQ) have attained recognition as the potential tools to solve nonlinear and complex problems in the domain of reservoir characterization [4], [5], [11], [29], [31], [34]\u2013 [38].", "startOffset": 323, "endOffset": 327}, {"referenceID": 26, "context": "Lately, several computation intensive Artificial Intelligence methods such as ANN, neuro-fuzzy, Self-Organizing Map (SOM), committee machine and Learning Vector Quantization (LVQ) have attained recognition as the potential tools to solve nonlinear and complex problems in the domain of reservoir characterization [4], [5], [11], [29], [31], [34]\u2013 [38].", "startOffset": 329, "endOffset": 333}, {"referenceID": 28, "context": "Lately, several computation intensive Artificial Intelligence methods such as ANN, neuro-fuzzy, Self-Organizing Map (SOM), committee machine and Learning Vector Quantization (LVQ) have attained recognition as the potential tools to solve nonlinear and complex problems in the domain of reservoir characterization [4], [5], [11], [29], [31], [34]\u2013 [38].", "startOffset": 335, "endOffset": 339}, {"referenceID": 31, "context": "Lately, several computation intensive Artificial Intelligence methods such as ANN, neuro-fuzzy, Self-Organizing Map (SOM), committee machine and Learning Vector Quantization (LVQ) have attained recognition as the potential tools to solve nonlinear and complex problems in the domain of reservoir characterization [4], [5], [11], [29], [31], [34]\u2013 [38].", "startOffset": 341, "endOffset": 345}, {"referenceID": 35, "context": "Lately, several computation intensive Artificial Intelligence methods such as ANN, neuro-fuzzy, Self-Organizing Map (SOM), committee machine and Learning Vector Quantization (LVQ) have attained recognition as the potential tools to solve nonlinear and complex problems in the domain of reservoir characterization [4], [5], [11], [29], [31], [34]\u2013 [38].", "startOffset": 347, "endOffset": 351}, {"referenceID": 36, "context": "Several researchers have claimed that the application of modular networks and hybrid networks show better performance compared to a single algorithm [39]\u2013[44].", "startOffset": 149, "endOffset": 153}, {"referenceID": 41, "context": "Several researchers have claimed that the application of modular networks and hybrid networks show better performance compared to a single algorithm [39]\u2013[44].", "startOffset": 154, "endOffset": 158}, {"referenceID": 37, "context": "Moreover, the concept of modularity is applied in many fields to divide a complex problem into a set of relatively easier sub-problems; then, the smaller sub-problems are solved by modules; finally, the obtained results are combined to achieve the solution of the main problem [40], [45]\u2013[47].", "startOffset": 277, "endOffset": 281}, {"referenceID": 42, "context": "Moreover, the concept of modularity is applied in many fields to divide a complex problem into a set of relatively easier sub-problems; then, the smaller sub-problems are solved by modules; finally, the obtained results are combined to achieve the solution of the main problem [40], [45]\u2013[47].", "startOffset": 283, "endOffset": 287}, {"referenceID": 44, "context": "Moreover, the concept of modularity is applied in many fields to divide a complex problem into a set of relatively easier sub-problems; then, the smaller sub-problems are solved by modules; finally, the obtained results are combined to achieve the solution of the main problem [40], [45]\u2013[47].", "startOffset": 288, "endOffset": 292}, {"referenceID": 26, "context": ", [29] used to predict petrophysical properties from a set of well logs.", "startOffset": 2, "endOffset": 6}, {"referenceID": 26, "context": ", [29].", "startOffset": 2, "endOffset": 6}, {"referenceID": 34, "context": "recorded from four closely spaced boreholes using modular neural network [37].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "In the past, many classifications and clustering techniques have been used to make different classes of data depending on their variability [29], [48]\u2013[50].", "startOffset": 140, "endOffset": 144}, {"referenceID": 45, "context": "In the past, many classifications and clustering techniques have been used to make different classes of data depending on their variability [29], [48]\u2013[50].", "startOffset": 146, "endOffset": 150}, {"referenceID": 47, "context": "In the past, many classifications and clustering techniques have been used to make different classes of data depending on their variability [29], [48]\u2013[50].", "startOffset": 151, "endOffset": 155}, {"referenceID": 48, "context": "Recently, the concept of chaotic time series data analysis namely dynamic programming [51], synchronization methods [52], [53] are applied to assess similarity between the pattern of the well log data, and which lead towards the identification of similar zones among the wells.", "startOffset": 86, "endOffset": 90}, {"referenceID": 49, "context": "Recently, the concept of chaotic time series data analysis namely dynamic programming [51], synchronization methods [52], [53] are applied to assess similarity between the pattern of the well log data, and which lead towards the identification of similar zones among the wells.", "startOffset": 116, "endOffset": 120}, {"referenceID": 50, "context": "Recently, the concept of chaotic time series data analysis namely dynamic programming [51], synchronization methods [52], [53] are applied to assess similarity between the pattern of the well log data, and which lead towards the identification of similar zones among the wells.", "startOffset": 122, "endOffset": 126}, {"referenceID": 26, "context": "Geoscientists\u2019 guided zone-wise division of a well log can assist in target evaluation after training several models using zone wise divided training patterns yielding improved prediction accuracy [29], [50].", "startOffset": 197, "endOffset": 201}, {"referenceID": 47, "context": "Geoscientists\u2019 guided zone-wise division of a well log can assist in target evaluation after training several models using zone wise divided training patterns yielding improved prediction accuracy [29], [50].", "startOffset": 203, "endOffset": 207}, {"referenceID": 2, "context": "Some applications of these methods in the field of petroleum reservoir modelling are discussed in [4], [54]\u2013[56].", "startOffset": 98, "endOffset": 101}, {"referenceID": 51, "context": "Some applications of these methods in the field of petroleum reservoir modelling are discussed in [4], [54]\u2013[56].", "startOffset": 103, "endOffset": 107}, {"referenceID": 53, "context": "Some applications of these methods in the field of petroleum reservoir modelling are discussed in [4], [54]\u2013[56].", "startOffset": 108, "endOffset": 112}, {"referenceID": 54, "context": "However, it has been observed that the accuracy in reservoir modelling can be improved using classification-based approaches [57].", "startOffset": 125, "endOffset": 129}, {"referenceID": 55, "context": "5 have received interests from researchers due to existence of such dataset in \u201creal-world applications\u201d [58]\u2013[61].", "startOffset": 105, "endOffset": 109}, {"referenceID": 58, "context": "5 have received interests from researchers due to existence of such dataset in \u201creal-world applications\u201d [58]\u2013[61].", "startOffset": 110, "endOffset": 114}, {"referenceID": 59, "context": "Kernel-based methods have gained acceptance in classification of imbalanced dataset over other supervised classification methods, especially in remote sensing fields [62]\u2013[64].", "startOffset": 166, "endOffset": 170}, {"referenceID": 61, "context": "Kernel-based methods have gained acceptance in classification of imbalanced dataset over other supervised classification methods, especially in remote sensing fields [62]\u2013[64].", "startOffset": 171, "endOffset": 175}, {"referenceID": 62, "context": "Support vector data description (SVDD) is a latest kernel-based algorithm which has attracted attention from researchers of different fields for its ability in learning without any a priori knowledge on distribution of dataset [65]\u2013[67].", "startOffset": 227, "endOffset": 231}, {"referenceID": 64, "context": "Support vector data description (SVDD) is a latest kernel-based algorithm which has attracted attention from researchers of different fields for its ability in learning without any a priori knowledge on distribution of dataset [65]\u2013[67].", "startOffset": 232, "endOffset": 236}, {"referenceID": 65, "context": "The SEG-Y file format is one of numerous standards developed by the Society of Exploration Geophysicists (SEG) for storing geophysical data [68].", "startOffset": 140, "endOffset": 144}, {"referenceID": 66, "context": "On the other hand, the Canadian Well Logging Society presented LAS format to standardize the organization of digital log curves information in 1989 [69].", "startOffset": 148, "endOffset": 152}, {"referenceID": 67, "context": "the instantaneous frequency is defined as the rate of change of the phase of seismic amplitude signal [1], [70].", "startOffset": 107, "endOffset": 111}, {"referenceID": 68, "context": "Other geophysical techniques such as Ground Penetrating Radar (GPR) [71] is a nearsurface technique mainly used in archaeological researches.", "startOffset": 68, "endOffset": 72}, {"referenceID": 69, "context": "Thus, the seismic resolution reduces with increasing depth under subsurface [72].", "startOffset": 76, "endOffset": 80}, {"referenceID": 70, "context": "This is known as Shannon entropy [74], [75].", "startOffset": 33, "endOffset": 37}, {"referenceID": 71, "context": "This is known as Shannon entropy [74], [75].", "startOffset": 39, "endOffset": 43}, {"referenceID": 72, "context": "In [76], Normalized Mutual Information (NMI) is defined as the mutual information normalized by minimum entropy of both the variables.", "startOffset": 3, "endOffset": 7}, {"referenceID": 73, "context": "For example, it is applied in Electroencephalography (EEG) signal analysis for artefact removal to detect the effect of sleep deprivation [77], study of geomagnetic signals [78]\u2013[80] etc.", "startOffset": 138, "endOffset": 142}, {"referenceID": 74, "context": "For example, it is applied in Electroencephalography (EEG) signal analysis for artefact removal to detect the effect of sleep deprivation [77], study of geomagnetic signals [78]\u2013[80] etc.", "startOffset": 173, "endOffset": 177}, {"referenceID": 76, "context": "For example, it is applied in Electroencephalography (EEG) signal analysis for artefact removal to detect the effect of sleep deprivation [77], study of geomagnetic signals [78]\u2013[80] etc.", "startOffset": 178, "endOffset": 182}, {"referenceID": 73, "context": "Wavelets are small wave like oscillating functions that are localized in time and frequency [77], [81]\u2013[83].", "startOffset": 92, "endOffset": 96}, {"referenceID": 77, "context": "Wavelets are small wave like oscillating functions that are localized in time and frequency [77], [81]\u2013[83].", "startOffset": 98, "endOffset": 102}, {"referenceID": 79, "context": "Wavelets are small wave like oscillating functions that are localized in time and frequency [77], [81]\u2013[83].", "startOffset": 103, "endOffset": 107}, {"referenceID": 80, "context": "These coefficients are computed using filter bank approach as in [84].", "startOffset": 65, "endOffset": 69}, {"referenceID": 77, "context": "The Daubechies family of wavelets has a compact support with relatively more number of vanishing moments [81].", "startOffset": 105, "endOffset": 109}, {"referenceID": 81, "context": "Reports suggest that in most of the cases the frequency analysis of signals are carried out in selected windows with respect to a given orthogonal basis [85]\u2013[87].", "startOffset": 153, "endOffset": 157}, {"referenceID": 82, "context": "Output : Regularized target signal ( ) r y t EMD is an algorithmic decomposition method which decomposes the input signal into a set of Intrinsic Mode Functions (IMFs) and a residue signal [88].", "startOffset": 189, "endOffset": 193}, {"referenceID": 82, "context": "There are two properties associated with IMFs such that (1) the numbers of zero\u2013crossings and extrema present in IMFs are same, and (2) IMFs are symmetric with respect to the local mean [88].", "startOffset": 186, "endOffset": 190}, {"referenceID": 77, "context": "In other words, EMD detects and extracts the highest frequency component in the signal [81], [89]\u2013[95]; such that, in step (k+1), the extracted IMF contains lower frequency component compared to that extracted in step k.", "startOffset": 87, "endOffset": 91}, {"referenceID": 83, "context": "In other words, EMD detects and extracts the highest frequency component in the signal [81], [89]\u2013[95]; such that, in step (k+1), the extracted IMF contains lower frequency component compared to that extracted in step k.", "startOffset": 93, "endOffset": 97}, {"referenceID": 89, "context": "In other words, EMD detects and extracts the highest frequency component in the signal [81], [89]\u2013[95]; such that, in step (k+1), the extracted IMF contains lower frequency component compared to that extracted in step k.", "startOffset": 98, "endOffset": 102}, {"referenceID": 90, "context": "Similarly, these tasks can be addressed with the help of a high performance computer [96].", "startOffset": 85, "endOffset": 89}, {"referenceID": 91, "context": "As shown in the figure, hyperbolic tangent sigmoid transfer function is used in the hidden layer nodes to facilitate the learning process [97].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "The changes only take place in the activation potentials and output of the neurons [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 92, "context": "The steps associated with SCG can be presented as follows [98]:", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "As a result, the learning process will slow down [13].", "startOffset": 49, "endOffset": 53}, {"referenceID": 25, "context": "The testing phase is important for evaluating the generalization capability of the trained network [28].", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "The scatter index represents the ratio of RMSE to mean of in situ observations [24].", "startOffset": 79, "endOffset": 83}, {"referenceID": 91, "context": "The tangent sigmoid transfer function is an automatic choice for researchers to use in the hidden layer to achieve the bidirectional swing [97], [99].", "startOffset": 139, "endOffset": 143}, {"referenceID": 93, "context": "The tangent sigmoid transfer function is an automatic choice for researchers to use in the hidden layer to achieve the bidirectional swing [97], [99].", "startOffset": 145, "endOffset": 149}, {"referenceID": 11, "context": "It is reported that the learning rate of the network is faster when the network is anti-symmetric [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 92, "context": "Finally, a network with a single hidden layer is trained using the Scaled Conjugate Gradient (SCG) Backpropagation Algorithm [98].", "startOffset": 125, "endOffset": 129}, {"referenceID": 94, "context": "The reported results in Table 3-1and Table 3-2 are better than the performances of multilayer perceptron, SVM, and Co-Active Neuro-Fuzzy Inference System (CANFIS) for permeability modelling in terms of CC as reported in [100].", "startOffset": 220, "endOffset": 225}, {"referenceID": 95, "context": "Median filter [101], [102] is a popular order-statistics filter where the value of a pixel is replaced by the median of a neighbourhood centred at that particular pixel.", "startOffset": 14, "endOffset": 19}, {"referenceID": 96, "context": "Median filter [101], [102] is a popular order-statistics filter where the value of a pixel is replaced by the median of a neighbourhood centred at that particular pixel.", "startOffset": 21, "endOffset": 26}, {"referenceID": 97, "context": "In case of 3-D median filter having a, the median of the pixel vector within 3 3 3 \uf0b4 \uf0b4 window size can be evaluate after carrying out six 3 3 \uf0b4 \u2019partial-sort operation\u2019 [103].", "startOffset": 169, "endOffset": 174}, {"referenceID": 34, "context": "As the predictor and target signals are from two different domains, therefore, a single ANN structure may not be able to successfully represent the mapping function between these two types of signals, which is the current research problem in this domain [37].", "startOffset": 254, "endOffset": 258}, {"referenceID": 98, "context": ", [104].", "startOffset": 2, "endOffset": 7}, {"referenceID": 98, "context": "ANN and its hybrid approaches have also proven to be useful in the nonlinear mapping of reservoir properties from well logs and seismic data [104], [105].", "startOffset": 141, "endOffset": 146}, {"referenceID": 99, "context": "ANN and its hybrid approaches have also proven to be useful in the nonlinear mapping of reservoir properties from well logs and seismic data [104], [105].", "startOffset": 148, "endOffset": 153}, {"referenceID": 26, "context": "Hence, MANN, which is a special category of ANN based on data categorization, is introduced as a potential tool for machine learning with efficient estimation capability and high speed [29], [50], [105]\u2013[107].", "startOffset": 185, "endOffset": 189}, {"referenceID": 47, "context": "Hence, MANN, which is a special category of ANN based on data categorization, is introduced as a potential tool for machine learning with efficient estimation capability and high speed [29], [50], [105]\u2013[107].", "startOffset": 191, "endOffset": 195}, {"referenceID": 99, "context": "Hence, MANN, which is a special category of ANN based on data categorization, is introduced as a potential tool for machine learning with efficient estimation capability and high speed [29], [50], [105]\u2013[107].", "startOffset": 197, "endOffset": 202}, {"referenceID": 101, "context": "Hence, MANN, which is a special category of ANN based on data categorization, is introduced as a potential tool for machine learning with efficient estimation capability and high speed [29], [50], [105]\u2013[107].", "startOffset": 203, "endOffset": 208}, {"referenceID": 50, "context": "Previous studies [53] reported that the similar zones in a well log reveals similar characteristic.", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "Thus, three mapping functions are obtained using MANN approach [29] corresponding to three zones (Z-1, Z-2 and Z-3).", "startOffset": 63, "endOffset": 67}, {"referenceID": 9, "context": "In general, the seismic data are helpful to model a reservoir; however, it is difficult to estimate the vertical distribution of reservoir properties with the help of seismic signals [11], [12].", "startOffset": 183, "endOffset": 187}, {"referenceID": 10, "context": "In general, the seismic data are helpful to model a reservoir; however, it is difficult to estimate the vertical distribution of reservoir properties with the help of seismic signals [11], [12].", "startOffset": 189, "endOffset": 193}, {"referenceID": 0, "context": "Integration of Seismic and Well Log Signals Integration of signals from different domains with the help of heuristic knowledge from human experts plays a major role in reservoir characterization [2].", "startOffset": 195, "endOffset": 198}, {"referenceID": 102, "context": "40 we apply Nyquist\u2013Shannon sampling theorem [108], which states that a band-limited signal can be completely reconstructed from the samples, to reconstruct seismic attributes at each time instant corresponding to the well logs using cubic spline interpolation method [109].", "startOffset": 45, "endOffset": 50}, {"referenceID": 103, "context": "40 we apply Nyquist\u2013Shannon sampling theorem [108], which states that a band-limited signal can be completely reconstructed from the samples, to reconstruct seismic attributes at each time instant corresponding to the well logs using cubic spline interpolation method [109].", "startOffset": 268, "endOffset": 273}, {"referenceID": 91, "context": "In the present study, hyperbolic tangent sigmoid is used in the hidden layer [97], [99]; however, for the output layer log-sigmoid transfer function is used.", "startOffset": 77, "endOffset": 81}, {"referenceID": 93, "context": "In the present study, hyperbolic tangent sigmoid is used in the hidden layer [97], [99]; however, for the output layer log-sigmoid transfer function is used.", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "Conjugate gradient method is an advanced and effective method for error minimization [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 11, "context": "Here, scaled-conjugate-gradient-backpropagation is selected over several other learning algorithms for its speed and simplicity [13] in training the networks.", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "However, the number of hidden layer neurons cannot be indefinitely increased; the possibility of overfitting has been avoided by keeping the maximum number of trainable parameters at least fifteen times lower than the number of available training patterns [13].", "startOffset": 256, "endOffset": 260}, {"referenceID": 62, "context": "SVDD, an extension of SVM, is widely used approach for the data classifications [65], [66].", "startOffset": 80, "endOffset": 84}, {"referenceID": 63, "context": "SVDD, an extension of SVM, is widely used approach for the data classifications [65], [66].", "startOffset": 86, "endOffset": 90}, {"referenceID": 62, "context": "Volume of the hypersphere should be minimized for the data description [65]\u2013[67], [110].", "startOffset": 71, "endOffset": 75}, {"referenceID": 64, "context": "Volume of the hypersphere should be minimized for the data description [65]\u2013[67], [110].", "startOffset": 76, "endOffset": 80}, {"referenceID": 62, "context": "i j x x \uf066 \uf066 as discussed in [65], [66], [111].", "startOffset": 28, "endOffset": 32}, {"referenceID": 63, "context": "i j x x \uf066 \uf066 as discussed in [65], [66], [111].", "startOffset": 34, "endOffset": 38}, {"referenceID": 104, "context": "i j x x \uf066 \uf066 as discussed in [65], [66], [111].", "startOffset": 40, "endOffset": 45}, {"referenceID": 105, "context": "For imbalanced dataset, the improvement in oneclass classifier performance compared to its two-class counterpart is apparent [112], [113].", "startOffset": 125, "endOffset": 130}, {"referenceID": 106, "context": "For imbalanced dataset, the improvement in oneclass classifier performance compared to its two-class counterpart is apparent [112], [113].", "startOffset": 132, "endOffset": 137}, {"referenceID": 62, "context": "2 Development of a Framework to Classify Water Saturation from Well Logs The first important contribution of this chapter is to propose a generalized framework based on SVDD [65], [66] to characterize the water saturation from input well logs.", "startOffset": 174, "endOffset": 178}, {"referenceID": 63, "context": "2 Development of a Framework to Classify Water Saturation from Well Logs The first important contribution of this chapter is to propose a generalized framework based on SVDD [65], [66] to characterize the water saturation from input well logs.", "startOffset": 180, "endOffset": 184}, {"referenceID": 107, "context": "Next, a comparative analysis is presented to demonstrate the effectiveness of the proposed classification method over other classifiers (discriminant[110], [114], naive Bayes [110], [115], support vector machine based classifier [116], [117]).", "startOffset": 156, "endOffset": 161}, {"referenceID": 108, "context": "Next, a comparative analysis is presented to demonstrate the effectiveness of the proposed classification method over other classifiers (discriminant[110], [114], naive Bayes [110], [115], support vector machine based classifier [116], [117]).", "startOffset": 182, "endOffset": 187}, {"referenceID": 109, "context": "Next, a comparative analysis is presented to demonstrate the effectiveness of the proposed classification method over other classifiers (discriminant[110], [114], naive Bayes [110], [115], support vector machine based classifier [116], [117]).", "startOffset": 229, "endOffset": 234}, {"referenceID": 110, "context": "Next, a comparative analysis is presented to demonstrate the effectiveness of the proposed classification method over other classifiers (discriminant[110], [114], naive Bayes [110], [115], support vector machine based classifier [116], [117]).", "startOffset": 236, "endOffset": 241}, {"referenceID": 111, "context": "Here, we use Relief algorithm [118], which identifies statistically relevant features and performs well in case of noisy dataset, to select input attributes before training the classifier.", "startOffset": 30, "endOffset": 35}, {"referenceID": 112, "context": "Instead of employing confusion matrix, which is generally used to measure performance of classifier, here we use g-metric means [119].", "startOffset": 128, "endOffset": 133}, {"referenceID": 111, "context": "mutual information, Relief algorithm [118], and its variants are used to identify relevant features among available features before starting to train the classifier.", "startOffset": 37, "endOffset": 42}, {"referenceID": 113, "context": "In contrary, application of relevant features as predictor variables enhances the generalization capability of a model [120].", "startOffset": 119, "endOffset": 124}, {"referenceID": 113, "context": "As in [120], the support vectors are encompassed in the outlier category.", "startOffset": 6, "endOffset": 11}, {"referenceID": 112, "context": "4-8) is quantified using g-metric means [119], [120] and program execution time.", "startOffset": 40, "endOffset": 45}, {"referenceID": 113, "context": "4-8) is quantified using g-metric means [119], [120] and program execution time.", "startOffset": 47, "endOffset": 52}, {"referenceID": 114, "context": "\uf0b7 As initial parameters selection of machine learning algorithms crucial for achieving acceptable performance, it would be interesting to automate the initialization parameters by an appropriate metaheuristic algorithm \uf0b7 Model based post\u2013processing instead of spatial filtering on predicted lithological properties \uf0b7 Uncertainty quantification associated with Reservoir characterization: Some interesting works have been reported in [121], [122] about application of uncertainty quantification in reservoir characterization.", "startOffset": 433, "endOffset": 438}, {"referenceID": 115, "context": "\uf0b7 As initial parameters selection of machine learning algorithms crucial for achieving acceptable performance, it would be interesting to automate the initialization parameters by an appropriate metaheuristic algorithm \uf0b7 Model based post\u2013processing instead of spatial filtering on predicted lithological properties \uf0b7 Uncertainty quantification associated with Reservoir characterization: Some interesting works have been reported in [121], [122] about application of uncertainty quantification in reservoir characterization.", "startOffset": 440, "endOffset": 445}], "year": 2015, "abstractText": "\u2018Reservoir Characterization (RC)\u2019 can be defined as the act of building a reservoir model that incorporates all the characteristics of the reservoir that are pertinent to its ability to store hydrocarbons and also to produce them. It is a difficult problem due to non-linear and heterogeneous subsurface properties and associated with a number of complex tasks such as data fusion, data mining, formulation of the knowledge base, and handling of the uncertainty. This present work describes the development of algorithms to obtain the functional relationships between predictor seismic attributes and target lithological properties. Seismic attributes are available over a study area with lower vertical resolution. Conversely, well logs and lithological properties are available only at specific well locations in a study area with high vertical resolution. If a functional relationship can be calibrated between seismic signals and lithological properties at available well locations, then distribution of these properties across the study area can be predicted from available seismic information. Depending on the distribution of the lithological properties, a dataset can be classified into two categories \u2013 balanced and imbalanced. Sand fraction, which represents per unit sand volume within the rock, has a balanced distribution between zero to unity. On the other hand, water saturation, oil saturation etc. has an imbalanced distribution skewed at one and zero respectively. The investigation about the sand fraction (balanced distribution) variation over the study area has been attempted as a prediction problem; whereas, the distribution of water saturation (balanced distribution) has been approached as a classification (Class low/ Class high) problem in this work. The thesis addresses the issues of handling the information content mismatch between predictor and target variables and proposes regularization of target property prior to building a prediction model. In this thesis, two Artificial Neural Network (ANN) based frameworks are proposed to model sand fraction from multiple seismic attributes without and with well tops information respectively. The performances of the frameworks are quantified in terms of Correlation Coefficient (CC), Root Mean Square Error (RMSE), Absolute Error Mean (AEM), etc. After successful completion of sand fraction prediction, a one-class classification framework based on Support Vector Data Description (SVDD) is proposed to classify water saturation from well logs. The designed framework is modified to include seismic variables as predictor attributes to obtain the variation of water saturation over the study area. In other words, the class labels (Class low/Class high) of water saturation belonging to a well location can be predicted from seismic attributes by the modified classification based framework. The proposed frameworks have outperformed other supervised classification algorithms in terms of g-metric means and program execution time (in seconds).", "creator": null}}}