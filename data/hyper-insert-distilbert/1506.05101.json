{"id": "1506.05101", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2015", "title": "Big Data Analytics in Bioinformatics: A Machine Learning Perspective", "abstract": "bioinformatics research is characterized essentially by voluminous modeling and incremental neural datasets and complex functional data flow analytics analytic methods. considered the preferred machine learning technology methods both used in biological bioinformatics biology are iterative dynamic and parallel. these methods can be efficiently scaled to handle big gene data using the distributed network and parallel biomedical computing technologies.", "histories": [["v1", "Mon, 15 Jun 2015 11:32:00 GMT  (1422kb,D)", "http://arxiv.org/abs/1506.05101v1", "20 pages survey paper on Big data analytics in Bioinformatics"]], "COMMENTS": "20 pages survey paper on Big data analytics in Bioinformatics", "reviews": [], "SUBJECTS": "cs.CE cs.LG", "authors": ["hirak kashyap", "hasin afzal ahmed", "nazrul hoque", "swarup roy", "dhruba kumar bhattacharyya"], "accepted": false, "id": "1506.05101"}, "pdf": {"name": "1506.05101.pdf", "metadata": {"source": "META", "title": "Big Data Analytics in Bioinformatics: A Machine Learning Perspective", "authors": ["Hirak Kashyap", "Hasin Afzal Ahmed", "Nazrul Hoque", "Swarup Roy", "Dhruba Kumar Bhattacharyya"], "emails": ["dkb}@tezu.ernet.in"], "sections": [{"heading": null, "text": "Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing.\nHowever, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.\nIndex Terms\u2014Big data, Bioinformatics, Machine learning, MapReduce, Clustering, Gene regulatory network\nF"}, {"heading": "1 INTRODUCTION", "text": "A S we enter into the information age, data are beinggenerated by variety of sources other than people and servers, such as sensors embedded into phones and wearable devices, video surveillance cameras, MRI scanners, and set-top boxes. Considering the annual growth of data generation, the digital universe - data we generate annually - will reach 44 zettabytes, or 44 trillion gigabytes by the year 2020, which is ten times the size of the digital universe in 2013 [2]. The fast transition into the information age has been fueled by the digitization of all of our devices and communication technology. Yesteryears technologies, such as analog telephony and film cameras, have been digitized. The advent of the Internet, followed by the WWW boom digitized our mailing systems, televisions, banking systems, and retailing, leading to storage and transmission of voluminous data. High performance technologies are used in scientific research, such as fast data capturing tools and very high resolution satellite data recording.\nApart from digitization of services and enterprises, a new trend has emerged recently to network all the man-made things around us, such as cars, home appliances, weapons, traffic lights, and power meters. These things communicate with each other to share data captured through various sensors, in order to take intelligent operational decisions by themselves. This network has been termed as the Internet of Things (IoT) [3]. The first networked appliance, a coke vending machine, was deployed\n\u2022 H. Kashyap, H. A. Ahmed, N Hoque, and D. K. Bhattacharyya are with Department of Computer Science and Engineering, Tezpur University, India - 784028. E-mail: {hirak, hasin, nhoq, dkb}@tezu.ernet.in \u2022 S. Roy is with Department of Information Technology, North Eastern Hill University, Shillong-22, India. E-mail: swarup@nehu.ac.in\nManuscript received June 15, 2015\nat the Computer Science department of Carnegie Mellon University in the year 19901. The IoT is growing fast and machine-to-machine connections will reach 1.2 billion in 2017, up from only 200 million in 2012 [4].\nHowever, it should be noted that not all data, that we generate, are useful for descriptive or predictive analysis. Only a part of the data in the digital universe is useful, when tagged, termed as target-rich data. Metadata are more target-rich than the data itself. According to Turner et al. [2], approximately all of the target rich data were general IT data in the year 2014; however, by the year 2020, IoT data will occupy more than 20% of the target-rich data lake. Figure 1 shows the forecast made in the IDC report [2] regarding the size of the digital universe and the target-rich portion of it by the year 2020.\nThe significance of these data are paramount as they embed the real life scenarios, such as environmental changes, cyber attacks, consumer drifts, and forthcoming epidemics,\n1. www.cs.cmu.edu/ coke/\nar X\niv :1\n50 6.\n05 10\n1v 1\n[ cs\n.C E\n] 1\n5 Ju\nn 20\n15\nand also because they are being generated and shared in real time. Consequently, these data are being heavily used for decision making and intelligent control.\nDue to this high availability of information intensive data stream and the advances in high performance computing technologies, big data analytics have emerged to perform real time descriptive and predictive analysis on massive amount of data, in order to formulate intelligent informed decisions. Big data refers to a high volume of heterogeneous data formed by continuous or discontinuous information stream. In the literature, big data has been characterized as either 3Vs or 4Vs [5], [6]. The 3Vs refer to Volume, Velocity, and Variety; whereas the 4th V in the later definition refers to Veracity, i.e., reliability of the accumulated data. Additionally, there are two very important characteristics of big data that are not covered by this traditional definition. First, big data are incremental, i.e., new data are dynamically added to the big data lake from time to time. Second, big data are geographically distributed. These characteristics separate big data from traditional databases or data-warehouses."}, {"heading": "1.1 Big data in bioinformatics", "text": "The volume of data is growing fast in bioinformatics research. Big data sources are no longer limited to particle physics experiments or search-engine logs and indexes. With digitization of all processes and availability of high throughput devices at lower costs, data volume is rising everywhere, including in bioinformatics research. For instance, the size of a single sequenced human genome is approximately 200 gigabytes [7]. This trend in rising data volume is also supported by decreasing computing cost and increasing analytics throughput with growing big data technologies. Biologists no longer use traditional laboratories to discover a novel biomarker for a disease, rather they rely on huge and continuously growing genomic data made available by various research groups. Technologies for capturing bio data are becoming cheaper and more effective, such as automated genome sequencers, giving rise to this new era of big data in bioinformatics.\nThe data size in bioinformatics is increasing dramatically in the recent years. The European Bioinformatics Institute (EBI), one of the largest biology-data repositories, had approximately 40 petabytes of data about genes, proteins, and small molecules in 2014, in comparsion to 18 petabytes in 2013 [8]. Their total storage size is doubling every year. Figure 2 shows the increasing trend in their genome and expression data store.\nEBI has installed a cluster, the Hinxton data centre cluster, with 17,000 cores and 74 terabytes of RAM, to process their data. Its computing power is increased in almost every month. More importantly, EBI is not the only organization involved in massive bio-data store. There are many other organizations, who are storing and processing huge collections of biological databases and distributing them around the world, such as National Center for Biotechnology Information (NCBI), USA and National Institute of Genetics, Japan.\nAvailability of high volume of data is helpful for more accurate analytics, particularly in a highly sensitive field of\nresearch like bioinformatics. However, the big data challenges here are much different from other well known big data problems, such as particle physics data captured at CERN or high resolution satellite data received at NRSC/ISRO open data archive2. The difference comes mainly in two aspects. First, bioinformatics data are highly heterogeneous in nature. Many analytics problems in bioinformatics require multiple heterogeneous and independent databases for inference and validation. Moreover, bioinformatics data are generated by many uncontrolled organizations and consequently, the same types of data are represented in different forms by their sources. Second, bioinformatics data, massive and growing in terms of dimension and number of instances, is geographically distributed all over the world. While part of these data may be transferred over the Internet, the remaining are not transferable due to their size (and hence inefficient), cost, privacy, and other ethical issues [9]. This sometimes forces to perform part of the analysis remotely and share the results. Therefore, big data problems in bioinformatics are not only characterized by volume, velocity, and variety, but also by geographically distributed data.\nIn order to tackle these challenges of big data in bioinformatics, cloud computing technologies have been used, with a lot of success. The best policy is to use cloud for both data store as well as for computation [9]. In fact, this policy helps to handle the big data challenges imposed by bioinformatics research over massive, growing and remotely distributed data. BGI, formerly known as Beijing Genomics Institute, one of the world\u2019s premiere genome sequencing centers, has installed a cloud-based analysis workflow called Gaea, using Hadoop framework. Gaea can\n2. bhuvan.nrsc.gov.in\nbe used to perform large-scale genome analysis in parallel across hundreds of cloud-based computers. Another notable cloud-based genome analytics solution is provided by Bina Technologies3, a Stanford University and UC Berkeley spinoff, in terms of a hardware component, called Bina box, to do the pre-processing on genome data and a cloud-based component to perform analytics on the pre-processed data. Bina box also reduces the size of genome data for their efficient transfer to the cloud component. This solution claims to improve the throughput of genome analytics by orders of magnitude higher than the traditional approaches [10]."}, {"heading": "1.2 Types of big data in bioinformatics", "text": "There are primarily five types of data that are massive in size and used heavily in bioinformatics research: i) gene expression data, ii) DNA, RNA, and protein sequence data, iii) protein-protein interaction (PPI) data, iv) pathway data, and v) gene ontology (GO). Although, other types of data such as human disease network and disease gene association network are also used, and highly important for many research directions including disease diagnosis.\nIn gene expression analysis, the expression levels of thousands of genes are analyzed over different conditions, such as separate developmental stages of treatments or diseases. Microarray-based gene expression profiling is usually used to record the expression levels for analysis. There are three types of microarray data, namely gene-sample, gene-time, and gene-sample-time. Gene expression profiles over sample space record the expression levels for varying external conditions, whereas over time space, they record the expression levels at different instances of time. Gene expression analysis can identify genes that are affected from pathogens or viruses, by comparing the expression values from infected and uninfected cells. The analysis results may be used to suggest biomarkers for disease diagnosis and prevention, among others. There are many public sources for microrarray databases, such as ArrayExpress4 from EBI, Gene Expression Omnibus5 from NCBI, and Stanford Microarray Database6.\nIn sequence analysis, DNA, RNA or peptide sequences are processed using various analytical methods to understand their features, functions, structures, and evolution. DNA sequencing is used in study of genomes and proteins and their associations with diseases and phenotypes and identification of potential drugs, evolutionary biology, identification of micro species present in a sample environment, forensic identification, etc. Sequence analysis methodologies include sequence alignment and biological database search, among others. Although RNA sequencing is mainly used as an alternative for microarrays, it can be used for additional purposes also, such as mutation identification, identification of post-transcriptional mechanisms, detection of viruses and exogenous RNAs, and identification of Polyadenylation. Sequence analysis is more effective than microarray analysis, since sequence data embed richer information. However, it requires more sophisticated analytic tools and computing\n3. www.bina.com 4. www.ebi.ac.uk/arrayexpress 5. www.ncbi.nlm.nih.gov/geo 6. smd.princeton.edu\ninfrastructures, in order to deal with massive amount of sequence data [11]. Important sequence databases include DNA Data Bank of Japan7, RDP8, and miRBase9.\nPPIs provide crucial information regarding all biological processes. Therefore, forming and analyzing PPI networks can give a proper understanding of protein functions. PPIs are intrinsic to the interactomics of living cell. Therefore, anomalous PPIs are the basis of various diseases, such as Alzheimer\u2019s disease and cancer. PPIs have been studied in different fields of research, such as bioinformatics, biochemistry, quantum chemistry, and molecular dynamics, thus giving rise to high volume of heterogeneous data regarding the interactions. Important PPI repositories are DIP10, STRING11, and BioGRID12, among others\nPathway analysis is useful for understanding molecular basis of a disease. Additionally, pathway analysis identifies genes and proteins associated with the etiology of a disease, predicts drug targets, and helps to conduct targeted literature searches. Further, it helps to integrate diverse biological information and assign functions to genes. The most notable pathway data sources are KEGG [12], Reactome [13], and Pathway Commons [14].\nThe GO database13 provides dynamic, structured, and species-independent gene ontologies for associated biological processes, cellular components, and molecular functions. The GO database uses controlled vocabularies to facilitate querying at different levels. A vast number of tools uses the GO database for bioinformatics research. Most of these tools are third-party based, however the GO project itself maintains certain tools, such as AmiGO, DAG-Edit, and OBO-Edit. The GO-based tool chain is so huge that there exist tools, such as SerbGO [15], to search the appropriate GO tools for a particular bioinformatics problem. The GO database has highly been used for various purposes, such as to build ontologies for anatomies, to validate semisupervised and unsupervised analytics results from data, and to develop timelines for model organisms, human diseases and plant growth environments."}, {"heading": "1.3 Big data problems in bioinformatics", "text": "The solutions for cloud-based large-scale big data analytics, such as Bina box for genome analysis, are very recent. There are several other big data problems in the domain of bioinformatics that are yet to be explored. Considering the recent big data boom in bioinformatics, as discussed above, there is an urgent need to address many of these problems. In this paper, we categorize the big data analytics problems in bioinformatics into seven categories. They are discussed below."}, {"heading": "1.3.1 Microarray data analysis", "text": "The size and number of microarray datasets are growing rapidly, mainly due to decreasing cost and widespread use\n7. www.ddbj.nig.ac.jp 8. rdp.cme.msu.edu 9. www.mirbase.org 10. dip.doe-mbi.ucla.edu 11. string.embl.de 12. thebiogrid.org 13. www.geneontology.org\nof microarray experiments. Moreover, microarray experiments are also been performed for gene-sample-time space, in order to capture the changes in expression values over time or over different stages of a disease. Big data technologies are required for fast construction of co-expression and regulatory networks using voluminous microarray.\nAs gene expression data are being captured at different progression stages of a disease over time, there has been an opportunity to identify the genes that are affected by the disease, in order to identify biomarkers for the disease. Computationally, the addition of the third dimension, time, makes the analytics much higher in complexity than the traditional analysis of gene complexes."}, {"heading": "1.3.2 Gene-gene network analysis", "text": "Gene regulatory networks (GRN) alterations underlie many anomalous conditions, such as cancer. Inferring GRN and their alterations from high-throughput microarray data is a fundamental but challenging task. With the rapid growth of high throughput sequencing technologies, system biologist are now able to infer gigabytes of data. In many cases, movement of such large volume of data is not feasible. Integration of large multiple GRNs from different sources help in reconstruction of a unified GRN. Reconstruction of GRNs locally and then their integration through cloud infrastructure may help system biologists to better analyze a diseased network.\nAdditionally, the inference can be translated to genomic medicine. Although there exist many GRN inference mechanisms, their relative strength are unknown, due to the lack of large-scale validation. To find the most effective inference mechanism to identify the abnormal networks and to prioritize the target proteins for druggabilty are demanding issues and need to be addressed using fast, reliable, and scalable architectures.\nGene co-expression network analysis estimates the correlation among different gene-gene networks obtained from gene-expression analysis. Differential co-expression analysis finds the changes incurred by the gene complexes over time or over different stages of a disease. This helps in finding the relations between gene complexes and traits of interest. Gene complexes of different species can also be studied to find genotypic similarities. Gene co-expression network analysis is a complex and highly iterative problem and requires large-scale data analytics systems."}, {"heading": "1.3.3 PPI data analysis", "text": "PPI complexes and changes in them inhibit high information content about various diseases. PPI networks are being studied in various domains of life sciences with production of voluminous data. The volume, velocity, and variety of data make PPI complex analytics is a genuine big data problem. It demands for an efficient and scalable architecture to provide fast and accurate PPI complex generation, validation, and rank-aggregation."}, {"heading": "1.3.4 Sequence analysis", "text": "With the increasing volume (in order of petabytes) of DNA data deluge originated from thousands of sources, the present DNA sequencing tools have been found inadequate.\nSo, development of a high throughput and compact architecture for DNA sequence analysis with renewed focus for big data management is a bioinformatics problem with high demand in the recent days.\nRNA sequencing technology has emerged as a strong successor to the microarray technology, due to its more accurate and quantitative gene expression measurements. However, RNA sequence data also contain additional information, which are often overlooked, and require complex machine learning techniques to be extracted. Big data technologies can be used to identify mutations, allele-specific expressions, and exogenous RNA contents, such as viruses, from RNA sequence data using sophisticated machine learning methods.\nThe next generation genome sequencing provides information on the complete genome of an individual, in orders of magnitude bigger in size than microarray based methods for genetic assessment. Large scale methods are needed to study the specific changes in genome sequences due to a particular disease and to compare with the existing results of the same or different related diseases."}, {"heading": "1.3.5 Evolutionary research", "text": "The recent advances in molecular biological technologies have become a prominent source for the generation of big data. Huge amount data has been generated by various projects at microbial level, such as whole genome sequencing, microarrays, and metabolomics. Bioinformatics has emerged as a significant platform for analysis and archival of this wealth of information. An important big data problem in bioinformatics has been the study of functional trends of adaptation and evolution using microbial research, by investigating primitive organisms."}, {"heading": "1.3.6 Pathway analysis", "text": "Pathway analysis associates genetic products with phenotypes of interest, in order to predict gene function, identify biomarkers and traits, and classify patients and samples. The genetic, genomic, metabolomics, and proteomic data has increased rapidly and big data technologies are required to perform association analysis on huge volumes of these data."}, {"heading": "1.3.7 Disease network analysis", "text": "Large disease networks have been formulated for many species, including human. These networks are continuously growing and new networks are being added by different sources in their own format. The multi-objective associations among diseases in heterogeneous networks are useful for understanding the relations among diseases across networks. Traditional network analytics techniques would not perform well over unstructured and heterogeneous data without compromising information quality, and intelligent and efficient analytics are required. Big data technologies are required to effectively deep mine the associations among heterogeneous disease networks.\nComplex networks of molecular phenotypes characterize causal or predictive genes or mechanisms for diseaseassociated traits. Ability for fast processing of these data allows researchers to analyze more datasets, that were\nnot possible to analyze before. Although large collections of these data can be analyzed with existing technologies, techniques for data integration are still inefficient. Optimal integration methods are required to analyze multiple, heterogeneous omics databases.\nIn addition to that, new high-throughput methods collect personalized phenotypes of huge number of individuals. Large scale machine learning tools are needed to recognize and visualize complex data patterns for the purpose of disease genesis analysis and diagnosis.\nAlthough some of these bioinformatics problems existed before the big data era, their complexity and efficiency have significantly scaled up with the emerge of big data. On the other hand, the other problems have been made possible by the availability of massive amount of data. In either case, sophisticated big data analytics technologies are of urgent need to handle these large scale problems."}, {"heading": "1.4 Techniques for big data Analytics", "text": "Supervised, unsupervised, and hybrid machine learning approaches are the most widely used tools for descriptive and predictive analytics on big data. Apart from that, various techniques from mathematics have been used in big data analytics. The problem of big data volume can be somewhat minimized by dimensionality reduction. Linear mapping methods, such as principal component analysis (PCA) and singular value decomposition, as well as non-linear mapping methods, such as Sammon\u2019s mapping, kernel principal component analysis, and laplacian eigenmaps, have been widely used for dimensionality reduction.\nAnother important tool used in big data analytics is mathematical optimization. Subfields of optimization, such as constraint satisfaction programming, dynamic programming, and heuristics & metaheuristics are widely used in AI and machine learning problems. Other important optimization methods include multi-objective and multi-modal optimization methods, such as pareto optimization [16] and evolutionary algorithms [17], respectively.\nStatistics is considered as a counterpart to machine learning; differentiated by data model versus algorithmic model respectively. The two fields have subsumed ideas from each other. Statistical concepts, such as expectation-maximization and PCA, are widely adopted in machine learning problems. Similarly, machine learning techniques, such as probably approximately correct learning are used in applied statistics. However, both of these tools have been heavily used for big data analytics.\nBig data analytics has a close proximity to data mining approaches. Mining big data is more challenging than traditional data mining due to massive data volume. The common practice is to extend the existing data mining algorithms to cope with massive datasets, by executing on samples of big data and then merging the sample results. This kind of clustering algorithms include CLARA (Clustering LARge Applications) [18] and BIRCH (Balanced Iterative Reducing using Cluster Hierarchies) [19]. Researchers have also emphasized on the reduction of computational complexity of data mining algorithms. For example, spectral regression discriminant analysis significantly reduces the time and space complexity by simplifying discriminant\nanalysis to a set of regularized least squares problems [20]. Similarly, Shi et al. [21] reduce the space complexity of non-linear discriminant analysis from O(n2) to O(n), to minimize computation and storage problem on large-scale datasets.\nNevertheless, time and space complexity of most of the machine learning and statistical methods are very high to be effective for real time analysis on large-scale dataset. In the recent years, distributed and parallel computing technologies have emerged as the prime solution to largescale computing problems, due to their scalability, performance, and reliability. Therefore, efforts have been made to perform big data analytics using distributed computing, under strict performance and reliability constraints. Consequently, distributed data analytics algorithms have been proposed in the literature. Mining of distributed data in itself has emerged as a new paradigm of data analytics. It should be noted that, to be effective, the nodes should perform the computations independently, i.e., without constantly sharing intermediate data with peer nodes. Park and Kargupta [22] discuss the distributed algorithms for classifier learning, association rule mining, and clustering. Rana et al. propose a component-based system, designated as PaDDMAS, for developing distributed data mining applications [23]. Similar systems for distributed machine learning methods are proposed, such as MLbase [24]. Further, cloud computing infrastructure-based systems are also proposed for performing distributed machine learning, such as the Distributed GraphLab [25] framework that emphasizes on consistency and fault-tolerance in distributed analytics.\nThe main driving force for big data analytics has been the industry researches for massive-scale commercial applications. Although cluster and grid computing have existed for long, they are designed specifically for particular applications and require high cost and expertise. Therefore, the technologies for big data analytics did not evolve significantly in that period. When cloud computing infrastructure and distributed processing platforms, such as MapReduce [26], and their open source implementations became widely available, the research on big data analytics escalated. Iterative graph processing systems, for solving large scale practical computing problems, have also been proposed. The proprietary graph processing architecture developed at Google, known as Pregel [27], addresses distributed processing of large scale real-life graphs. An opensource counterpart of Pregel is Apache Giraph14, which provides additional features, such as edge oriented input and out-of-core computation.\nMoreover, the rising data volume has contributed to the increasing demand for big data analytics. In the recent years, distributed file system technologies, such as HDFS [28] and QFS [29], as well as NoSQL databases for unstructured data, such as MongoDB15 and CouchDB16 have been widely used for big data analytics.\nMachine learning libraries have been developed for big data analytics. The most notable machine learning library for big data analytics is Apache Mahout [30], which contain\n14. giraph.apache.org 15. www.mongodb.org 16. couchdb.apache.org\nimplementations of various machine learning techniques, such as classifiers, clustering, and recommender systems, which are scalable to large scale datasets. MLlib17 is a similar library to perform machine learning on big data on the Apache Spark platform, a MapReduce variant for iterative and fast computations on big data. However, these libraries still lack many important machine learning methods and more contributions are needed from the community."}, {"heading": "1.5 Contributions", "text": "This paper provides an in depth study on the sources and types of big data in bioinformatics, the existing machine learning and big data techniques to analyze them, and the limitations and future research. The contributions of this paper are listed below.\n1) Two additional characteristics to the traditional definition of big data are introduced. Accordingly, big data are also incremental and geographically distributed. 2) The problems in bioinformatics, that face the challenges of huge, ever growing, and heterogeneous datasets, are categorized into seven classes. Research issues in each of these problem categories are identified. 3) State of the art big data technologies are classified into three classes based on their overall system architectures. The generic architectures for each of the classes are introduced. 4) Machine learning methods for large scale data analytics are presented. The limitations of the traditional methods and their incremental versions for fast, scalable, and accurate big data solutions are discussed. 5) Big data tools and machine learning techniques available for each category of the bioinformatics problems and the scope for future contributions are discussed."}, {"heading": "1.6 Organization of the paper", "text": "The paper is organized as follows. In Section 2, generic architectures for the existing big data analytics computational models are presented. The traditional as well as the big data oriented machine learning methods, along with discussion on their capabilities and limitations are discussed in Section 3. In Section 4, the issues and challenges associated with big data analytics are discussed. Existing big data tools for bioinformatics are presented in Section 5 and our conclusions on the study are presented in Section 6."}, {"heading": "2 ARCHITECTURES FOR BIG DATA ANALYTICS", "text": "Big data analytics systems have been proposed with several architectures. However, many of them share common computational models. Based on our study, we classify big data solutions into three major architectures. Each of them has their own advantages as well as limitations, and their suitability depends on the nature and requirements of the algorithm to be implemented. These are discussed below.\n17. spark.apache.org/mllib"}, {"heading": "2.1 MapReduce architecture", "text": "MapReduce is a data-parallel architecture, originally developed by Google [31]. Parallelism is achieved by multiple machines or nodes performing the same task on different data. Apache Hadoop18 is a highly used open-source implementation of MapReduce. A MapReduce daemon runs on the nodes all the time. There is one master node that performs the configuration and control throughout the execution of the problem. The other nodes are called the worker nodes and perform actual computation on data. The master node also splits the data, assigns them to worker nodes, and puts them into the global memory as (key, value) pairs. Figure 3 depicts the basic architecture of MapReduce, where Wi\u2019s are the worker nodes.\nMapReduce works in rounds, each consisting of two phases, namely map and reduce phases. A node can be used in both map and reduce phases. Each phase consists of three states: input, computation, and output. There is one synchronization barrier between any two consecutive phases. During synchronization, local memory of a node is cleared and written onto the global memory. The master node can read/write onto the global memory and communicate with the other nodes during all time. However, the worker nodes can read/write onto the global memory only during synchronization. In Figure 3, this has been distinguished using thick and thin arrows.\nDuring the map phase, the problem is data-distributed among the worker nodes and the partial results generated by the worker nodes are stored in the global memory. During the reduce phase, the partial results are combined to obtain the overall result, to be stored in the global memory. If the intermediate results need to be further processed, the phases are repeated again.\nMapReduce architecture performs well when the data size is huge and the problem in hand is embarrassingly parallel. The architecture provides fault-tolerance by re-doing the computation (done by the failing node) for the phase on another node. However, the architecture has limitations for problems involving high computational dependencies among data. Moreover, the architecture cannot be used to express iterative computations and becomes inefficient with high I/O overhead.\n18. hadoop.apache.org\nEfforts have been made to mitigate the limitations of the MapReduce architecture and improve its performance. Twister [32] optimizes iterative computations on the MapReduce architecture by using in-memory computations, rather than writing onto the distributed memory after each phase. However, Twister has fault-tolerant issues due to inmemory processing. Apache Spark19 extends Hadoop by using Resilient Distributed Database (RDD) [33] to allow in-memory processing as well as fault-tolerance by reconstructing a faulty partition in case of node failure."}, {"heading": "2.2 Fault tolerant graph architecture", "text": "While MapReduce and its different implementations process data in batch mode, they are not very expressive when complex computational dependencies exist among data. Most of the machine learning and statistical methods inhibit high data dependencies. Therefore, MapReduce is not the best architecture for them. Alternate architectures are needed to process the complex and iterative problems efficiently, while supporting fault tolerance. Fault tolerance is important for scalability also, since it allows to use unreliable networks, such as the Internet.\nIn order to achieve that, a fault tolerant graph-based architecture, called GraphLab, was first proposed by Low et al. [34] and later many other big data solutions adopted similar architectures. In this architecture, the computation is divided among nodes in a heterogeneous way, with each of them performing some particular tasks. The data model is divided into two parts, i) a graph with computing nodes and ii) a shared global memory (distributed). The generic architecture is depicted in Figure 4. The Ni\u2019s represent the computing nodes and the dotted arrows show the dependencies among the nodes, whereas actual communication is performed via the communication network.\nSimilar to MapReduce, the computation is carried out in execution cycles in a synchronous manner. The shared database is initialized with the input data. At the beginning of each cycle, a node first reads the shared database and then performs computation using its own and its neighbor\u2019s data. Then the results are merged and then written back to the global shared database, for use in the next execution cycle. If a particular node fails in one cycle, it is recomputed and the dependent nodes lose one cycle. Although it reduces the\n19. spark.apache.org\nefficiency by a cycle, the fault tolerance is guaranteed. If a node fails permanently, then it is replaced.\nThis architecture provides high expressiveness for complex problems with data dependency and iterations. However, the architecture demands high disk I/O and therefore, it is not optimized for performance. To the best of our knowledge, an improvement using RDD to facilitate inmemory processing and fault tolerance is not yet proposed.\nApart from GraphLab, other major graph-based big data solutions are Pregel and Giraph. Graph packages are also developed for the MapReduce architecture, such as GraphX and the Hama20 graph package called Angrapa."}, {"heading": "2.3 Streaming graph architecture", "text": "The graph-based architecture discussed above allows scalable distributed computation, complex data dependency among operations, efficient iterative processing, and fault tolerance. However, due to its high disk read/write overhead, it is not efficient for stream data. Although there are packages to perform analytics on stream data on the MapReduce architecture, such as Spark Streaming21, they internally convert stream data to batches for processing. Stream applications require in-memory processing for high bandwidth. The well known Message Passing Interface (MPI) [35] is a good fit for this problem. At the application level, MPI has similar API as MapReduce and almost all MapReduce programs can also be implemented using MPI. Figure 5 depicts the graph-based architecture for large scale distributed processing, for high bandwidth and iterative applications with high data dependency among operations. This architecture is in line with the ever increasing computing speed and improved network bandwidth and reliability.\nThere are three major differences between this architecture and the previous one. First, in this architecture, a global shared memory is not used, rather the nodes exchange data using peer-to-peer communications directly. Second, the operations are performed in an asynchronous manner. The different data flows become synchronous only during their merge operations. Finally, in this architecture, data need not be stored into disks. As memories are becoming cheaper everyday, in-memory processing of large volume data is possible, which significantly increases the overall throughput.\n20. hama.apache.org 21. spark.apache.org/streaming\nThe main disadvantage of this architecture is the absence of fault tolerance. If any one of the nodes fail, the process has to start from the beginning all over again. Consequently, this architecture is unsuitable in unreliable networks, such as the Internet. This in turn causes scalability issues. However, if a reliable network is available and the algorithm has high data dependency, then this architecture can provide higher throughput than the other architectures. This architecture can be implemented on standalone clusters using MPI to perform analytics on big data ."}, {"heading": "3 MACHINE LEARNING FOR BIG DATA ANALYTICS", "text": "Machine learning techniques have been found very effective and relevant to many real world applications in bioinformatics, network security, healthcare, banking and finance, and transportations. Over time, bioinformatics and healthrelated data are created and accumulated continuously, resulting in an incredible volume of data. Newer forms of big data, such as 3D imaging, genomics and biometric sensor readings are also fueling this exponential growth. Future applications of real-time data, such as early detection of infections/diseases and fast application of the appropriate treatments (not just broad-spectrum antibiotics) could reduce patient morbidity and mortality. Already, realtime streaming data monitors neonates in the ICU, catching life-threatening infections at real time. The ability to perform real-time analytics against such voluminous stream data across all specialties would revolutionize healthcare. Therein lies data with volume, velocity, and variety.\nMachine learning is a field of computer science that studies the computational methods that learn from data [36]. There are mainly two types of learning methods in machine learning, viz., supervised and unsupervised learning methods [37]. In supervised learning, a method learns from a set of objects with class label, often called a training set. The acquired knowledge is used to assign label to unknown objects often called test objects. On the other hand, unsupervised learning methods do not depend on the availability of prior knowledge or training instances with class labels. All these machine learning methods require preprocessing of datasets for effective results. Feature selection is one of the important preprocessing tasks that leads to improved result and reduced time requirement. Hybrid learning methods, such as Deep learning, have become popular in the recent years and provide significantly high accuracy.\nAdvanced data capturing technologies have led to accumulation of a very high volume of data, growing rapidly over time. Although the computational technologies have improved over time, this improvement is not proportionate to the rate of increase in data volume. The traditional machine learning methods are found inadequate in handling voluminous data using the current computational resources [38]. Figure 6 depicts the contrast between traditional data mining and mining of big data.\nTo apply a traditional, or enhanced a new machine learning method to analyze big data, following properties are desirable.\n\u2022 Scalable to high volume: The method should be able to handle large chunk of data with low space complexity and less disk overhead.\n\u2022 Robust with high velocity: The method should have low time complexity and be able to digest and process stream data in real time without any degradation in performance. \u2022 Transparent to variety: Big data can be semi-structured or unstructured in nature. However, most traditional machine learning methods are able to process datasets with a fixed schema, which is normally generated from a single source. By the term schema, we refer to an ordered set of features and the relations among them. A machine learning method for big data analytics should be able to handle data from multiple sources with different schema. \u2022 Incremental: Typically, machine learning methods operate on entire datasets at once without accounting for the situation where dataset dynamically grows over time. A machine learning method for big data analytics should consider the inconsistent arrival of data over time and should be able to handle such data with minimum cost, without compromising quality. \u2022 Distributed: A machine learning method should allow distributed processing on partial data and merging of the partial results. With big data sources distributed around the world, all data may not be available at a single location for big data analytics."}, {"heading": "3.1 Feature Selection", "text": "The main objective of feature selection is to select a subset of most relevant and non-redundant features that can increase the performance of a learning method. A feature selection method can improve the performance of prediction models by removing irrelevant and redundant features with alleviating the effect of the curse of dimensionality, enhancing the generalization performance, speeding up the learning process, and improving the model interpretability [39]. Due to wide application of computer networks and Internet, data over Internet communication as well as in many other online services must deal with large volume of data with\nvolume, velocity, and variety. Moreover, in many business applications, handling big data is an essential requirement but taking instant decision reliably on big data is still an open research issue. Such big data pose great challenges for feature selection in terms of performance, scalability, robustness, universality, nonlinearity, and cost and implementation complexity.\nA feature selection plays a major role in identifying the most important features from a ultrahigh dimensional big dataset. The selected feature set can be used for processing large volume of data to take instant decision in short period of time. Especially, in big data analytics, relevant features can be selected from large data using both supervised learning as well as unsupervised learning. Hence, ranking the features based on their relevance and selecting the most relevant features can vastly improve the generalization performance.\nFeature selection is also considered very important for big data analytics due to its characteristics of semi-infinite programming (SIP) problem [40]. The SIP is an optimization problem that can be stated either it is associated with a finite number of variables and an infinite number of constraints, or an infinite number of variables and a finite number of constraints. To address the SIP problem, Tan et al. [41] propose an efficient feature selection algorithm works iteratively and selects a subset of features, and solves a sequence of multiple kernel learning (MKL) subproblems. Authors claim that the proposed method converges globally under mild condition and yields low biasness on feature selection.\nIn bioinformatics, protein sequence analysis and PPI analysis are complex problems in functional genomics. A feature vector exhibits protein sequences with distinguished characters and the feature vector plays a major role during analysis of protein sequence. However, a major problem of PPI dataset is that it contains huge number of enormous features which increase not only the complexity of analysis but reduce prediction accuracy. To overcome this problem, Bagyamathi et al. [42] propose a new feature selection method combining Improved harmony search algorithm with rough set theory to tackle the feature selection problem in big data.\nBarbu et. al. [43] propose a novel feature selection method with annealing technique for big data learning. In this method they reduce the dimensionality of an instance from M to k using an annealing plan to decrease greediness and remove the most irrelevant variables to facilitate complex computation. They termed the feature selection problem as a constrained optimization problem defined as \u03b2=arg min L(\u03b2), such that, |{j : \u03b2j 6= 0} \u2264 k|, where k is the number of relevant features. The algorithm is extremely suitable for big data computation due to its simplicity and ability to reduce the problem size throughout the iteration.\nIncremental learning is useful to predict behavior of big data in terms of adaptiveness. An incremental learning method considers subset of features selected incrementally from samples of data over time. For efficient analysis of high volume of data with random velocity and multiple varieties, incremental feature selection method selects those features that can predict the behavior of data efficiently. Zeng et al. [44] propose an incremental feature selection method called FRSA-IFS-HIS (AD) using fuzzy-rough set\ntheory on hybrid information systems. The method has been found effective compared to non-incremental fuzzy-rough set feature selection method applied on big data."}, {"heading": "3.2 Supervised Learning", "text": "In supervised learning, labeled training examples are used to train the learning algorithm. The objective of a supervised learning model is to predict the class labels of test instances based on knowledge gained from the available training instances. Within supervised learning family we can further distinguish between classification models which focus on prediction of discrete (categorical) outputs or regression models which predict continuous outputs. Among large number of models reported in the literature linear and nonlinear density-based classifiers, decision trees, naive Bayes, support vector machines (SVMs), neural networks and K-nearest neighbour (KNN) are the most frequently used methods in many applications [45] [46] [47] [48].\nIn big data analytics, we need some advanced supervised approaches for parallel and distributed learning such as Multi-hyperplane Machine (MM) classification model [49], divide-and-conquer SVM [50], and neural network classifiers. Among these SVM is one of the most efficient and widely used supervised learning method and several modified SVM methods have been introduced for big data analytics. Nie et al. propose a modified SVM called New Primal SVM for big data classification [51]. The method uses a novel linear computational cost primal SVM solver using two loss functions called L1-norm and L2-norm in Augmented Lagrange Multipliers (ALM). Individual detection of patients with Parkinson disease using SVM analysis was proposed by Haller et. al. [52]. In this work, the authors adopt a complex methodology including a chain of tract-based spatial statistics (TBSS) preprocessing of DTI fractional anisotropy data, feature selection of the most discriminative voxels, and subsequent SVM classification. Experimental results establish the effectiveness of the proposed method and feasibility of performing SVM individual classification of DTI data in patient diagnosis, which may merit future prospective and larger scale follow-up studies. Giveki et. al. [53] propose a weighted SVM based on mutual information and modified cuckoo search for automatic detection of diabetics diagnosis. The method first applies principal component analysis for feature selection from diabetes dataset and estimates the best feature weights using mutual information. Afterwards, the method is applied to classify patients where modified cuckoo search is used to find the best value for C and \u03b3 parameters of the proposed methods.\nAnother SVM-based decision support system for heart disease classification with integer-coded genetic algorithm to select crucial features was proposed by Bhatia et. al. [54]. The method uses an integer-coded genetic algorithm to select an optimal subset of features from Cleveland heart disease database which maximizes the SVM classification accuracy with a reduced number of features used by the SVM classifier to classify heart disease. Son et al. [55] use SVM to classify heart failure patients.\nDistributed decision tree is another significant effort to improve the performance of decision tree induction when processing on big data by parallelizing the induction process\nand by performing the induction process in distributed environment. Ye et. al. propose techniques to distribute and parallelize Gradient Boosted Decision Trees(GBDT) [56]. It is very straightforward to convert GBDT to MapReduced model and in this method a MapReduced-based GBDT was employed for horizontal data partitioning. According to the authors, due to the high communication overhead of HDFS [57], Hadoop is not suitable for this algorithm.\nCalaway et al. [58] propose fast, scalable and distributable decision tree called rxDTree which can estimate decision trees efficiently on big data. This algorithm is widely used in classification and regression problems of big data. It computes histograms to create empirical distribution functions of the data and builds the decision tree in a breadth-first fashion. The algorithm can be executed in parallel settings, such as a multicore machine or a distributed (cluster or grid) environment.\nFor big data, an intelligent agent could provide hint on areas of data that might the users would be very useful. If the dataset has categories for different user classes as class labels, then the labels can be used to train a decision tree to classify unseen data. But, the training set will be much larger than usual and hence, the rule generation for decision tree is a complex and time consuming process. To handle this problem, Hall et al. [59] propose a modified decision tree learning that generates rules from a set of decision trees built in parallel on tractable size training dataset."}, {"heading": "3.3 Unsupervised learning", "text": "Unsupervised learning do not use the class labels of the objects for learning [60]. Clustering is an unsupervised technique that attempts to group objects to optimize the criterion that states that distance among objects in the same cluster is minimized and distance among objects in different clusters is maximized [61]. A major issue in clustering is the computation of distance between a pair of objects. Various proximity measures have been used for this purpose, such as Euclidean, Cosine, and city block distance. In traditional clustering, all the features are used while computing the distance between a pair of objects. A cluster is a group of objects that are close to each other with respect to their mutual distance. In other words, they are similar in nature over the entire set of features. However, in a number of applications, especially where number of available features in a dataset is very large, researchers are interested in finding groups of objects that are similar over subset of the available features [62]. This requirement has led to the emergence of another variant of clustering called biclustering, where each bicluster is associated with a subset of features.\nClustering and biclustering analyses two dimensional data, where each feature corresponds to an attribute of the objects. Value of an object over a feature is some form of quantification of the concerned attribute. With advent of data capturing technologies, it has been possible to trace down dynamic nature of the object attributes by capturing values over multiple consecutive time instances. This arrangement leads to generation of three dimensional datasets. Another variant of clustering, called triclustering operates on such datasets to generate triclusters. A tricluster is a group of objects that are not only similar over a subset\nof features, but are also similar across a subset of time points [63]. Triclustering promotes grouping of objects, features and time points simultaneously."}, {"heading": "3.3.1 Existing clustering methods", "text": "Numerous clustering methods have been proposed so far in the field of machine learning. These clustering methods are mainly classified into partitional clustering, hierarchical clustering, density-based clustering, graph theoretic clustering, soft computing-based clustering, and matrix operation based clustering [64]. Partitional clustering methods assign objects to one of the k clusters, where k is a user given parameter, to iteratively optimize a criterion function. Kmeans [65] assign objects to the nearest cluster centroid iteratively until there is no more assignment possible. Partitioning Around Medoids (PAM) [66] is another partitional clustering method that uses medoids instead of centroids. PAM is robust, but inefficient in handling large dataset due to its O(n2) complexity. CLARA [66] and CLARANS [67] are two popular partitional clustering methods that use sampling for large datasets. CLARA draws a sample of objects on which PAM is applied. CLARAN uses sampling during neighborhood search operation. Both CLARA and CLARANS attempt to handle large dataset.\nHierarchical clustering methods can be classified into agglomerative and divisive methods [68]. Agglomerative approaches operate in bottom up direction on a tree and starts with nodes with individual objects. These nodes are iteratively merged to reach the root of the tree. In divisive approach, root with all the nodes are iteratively splitted to finally reach the leaf nodes. BIRCH [19] is a popular agglomerative hierarchical clustering method that constructs clustering feature (CF) tree first, which is operated in a bottom up fashion to extract the clusters. CURE [69] is another popular hierarchical clustering method that starts with some scattered objects to form clusters. These clusters are then shrunk towards theirs centers. DIANA [66] is a divisive hierarchical clustering method that splits largest cluster iteratively to find splinter groups.\nDensity-based clustering methods find clusters characterized as dense areas and separated by low dense regions [70]. Density of a node is measured using neighbourhood analysis. DBSCAN [71] is a very popular density-based clustering method that starts from an initial object and includes objects from it\u2019s neighbourhood iteratively if they satisfy a user defined threshold to form a cluster. DENCLUE [72] is another popular density-based clustering method that uses kernel density function. A cluster is defined as a local maximum of the density function.\nGraph theoretic clustering methods use properties and concepts of graph theory [73]. CLIQUE [68] a graph theoretical clustering method tries to locate maximally complete subgraphs in the connectivity graph derived from actual datasets. These subgraphs correspond to detected clusters. Chameleon [74] is another graph theoretic agglomerative hierarchical clustering method that uses k-nearest neighbor graph. Here, edges are iteratively deleted if connecting nodes are not included in the k-nearest neighbour sets of each other.\nSoft computing-based clustering methods use soft computation tools, such as fuzzy set and neural network. Fuzzy\nc-means [75] a soft computing-based clustering method, is a crisp method, which allows objects to belong to more than one cluster with the constraint that the sum of membership of an object across all the clusters is equal to one. This method tries to find a crisp partition that minimizes a cost function. SOM [76] is another very popular soft computing based clustering method that projects high dimensional vectors to a two dimensional grid space. Iteratively these objects are moved to dense regions that correspond to the clusters.\nThere are a lot of biclustering algorithms proposed by researchers. Cheng and Church [62] propose a biclustering method that iteratively deletes and adds objects and features in a greedy manner. OPSM [77] detects order preserving sub-matrices in data matrix which correspond to clusters. BIMAX [78] biclustering method binarizes the data matrix and locate submatrices with zero entries in all the cells. Spectral biclustering [79] uses eigen vectors to detect checkerboard structures in the data matrix that correspond to biclusters. SAMBA [80] formulates biclustering problem from graph theoretic view point and tries to find heavy subgraphs in a weighted bipartite graph. FLOC [81] starts with some initial seeds and then iteratively moves rows and columns to improve quality of biclustering with respect to a criterion function. ISA [82] is another biclustering method that incorporates randomization and finds biclusters with objects that possess constant values or coherently increasing values over the associated feature subset. To address the needs of biclustering problem effort is still underway [83], [84], [85], [86]\nIn comparison to the number of clustering and biclustering methods, there are not many triclustering methods. Jiang et al. [87] propose a set enumeration-based method to mine triclusters from 3-dimensional datasets using Pearson correlation coefficient. Authors propose two variant of the method to extract triclusters that are spread over all the time points. TRICLUSTER [88] is another well known triclustering method that extracts maximal triclusters from 3-dimensional datasets using a graph theoretic approach. gTRICLUSTER [89] uses Spearmen correlation coefficient to measure correlation among objects across time points while mining triclusters. ICSM [63] is a triclustering method that operates on possible pairs of time planes and detects some initial modules which are further extended to triclusters."}, {"heading": "3.3.2 Clustering methods for Big Data", "text": "Though researchers have been trying to design clustering methods to address the issues mentioned above, none of these methods are seemed to handle all these issues simultaneously. Parallel clustering methods are seemed to be a solution for huge volume of data and incremental clustering methods handle high velocity data. Similarly, multi-view clustering methods are designed to handle data with variety.\nThe DBSCAN, DENCLUE, CLARA, CLARANS, and CURE methods discussed earlier are designed to handle large scale data. Two versions of k-means, namely k-mode and k-prototype methods [90] operate on large scale categorical and mixed type data, respectively. These methods use a low-complexity dissimilarity measure and cost function to make k-means suitable for large scale datasets. Ordonez et al. propose a variant of k-means [91] to minimize memory\nrequirement and number of scans over the dataset. Bradley et al. propose a framework [92] to iteratively perform sampling from a large scale dataset and in each iteration, a model is improved to finally produce clusters. WaveCluster [93] uses wavelet transform to convert spatial domain data to frequency domain using a kernel function.\nLi et al. propose parallel partitional and parallel single likange hierarchical clustering methods [94] on SIMD computers. Zhao et al. propose a parallel k-means clustering method that uses MapReduce architecture [95] to analyze parallel portions of the method. Similarly, PDBCSCAN [96] finds clusters from data distributed over multiple machines and the results are merged. P-cluster [97] partitions the objects to minimize the error. PBIRCH [98] is a parallel version of BIRCH that continuously distributes the incoming data among multiple processors using message passing and shared nothing architecture.\nThere exists incremental clustering methods to accomodate new objects without rerunning the clustering method on old objects. Chakraborty et al. propose an incremental k-means clustering [99] that computes new cluster centers by only using the existing cluster centers and the newly arrived object. Widyantoro et al. propose an incremental hierarchical clustering [100] that restructures the region of the object hierarchy by inserting the new object through a sequence of restructuring processes. IGDCA [101] is an incremental density based clustering method that divides data space into units with high density to form clusters. Insertion or deletion of an object only affects density of the unit to which it belongs.\nThough multi-view learning is mostly in supervised and semi supervised learning, there are a few works that addresses the problem of unsupervised multi-view learning. Kailing et al. propose a multi-view clustering method [102] based on DBSCAN clustering method. The method operates on different feature spaces of various objects separately without combining these spaces. Zeng et al. propose a framework [103] that performs clustering in different feature spaces separately and then iteratively project and propagate the clustering results in multiple graph-based link layers until they converge. Chaudhuri et al. propose a multi-view clustering method [104] to project multi-views to lower dimensional space. The method tries to locate low dimensional subspace using a subspace learning method based on canonical correlation analysis. Kumar et al. propose a multiview version of spectral clustering [105] that computes eigen vectors in different feature spaces and use these eigen vectors to improve graph structures of other view iteratively. The authors propose another multi-view version of spectral clustering [105] method that uses coregulation-based approaches to find coherent eigen vectors from different graphs."}, {"heading": "3.4 Deep Learning", "text": "Deep learning attempts to model high-level abstractions in data using supervised and/or unsupervised learning algorithms, in order to learn from multiple levels of abstractions. It uses hierarchical representations of data for classification. Deep learning methods have been used in many applications, viz., pattern recognition, computer vision, natural language processing and speech recognition. Due to\nexponential increase of data in these applications, deep learning is useful for accurate prediction from voluminous data. In recent years, reseachers have developed effective and scalable parallel algorithms for training deep models [106]. Many organizations use deep learning for decision making, information retrieval, and semantic indexing. A deep learning architecture is shown in Figure 7. Input data are partitioned into multiple samples for data abstarctions. The intermediate layers are used to process the features at multiple levels for prediction from data. The final prediction is performed at the output layer using the outputs of its immediate upper layer.\nDeep learning represents data in multiple layers. It can efficiently process high volume of data, where shallow learning fails to explore due to the complexities of data patterns. Moreover, deep learning is quiet suitable for analyzing unstructured and heterogeneous data collected form various sources.\nTraditional neural networks pose two problems, viz., poor performance due to local optima of a non-convex object function and incapability to exploit unlabeled data, which are abundant and cheap. To overcome these limitations of traditional neural networks, Deep Belief Networks (DBN) [107] was introduced with a deep learning architecture to learn from both labeled and unlabeled data. The deep architecture of DBN integrates unsupervised pretraining and supervised fine-tuning strategies. The unsupervised pre-training is used to learn data distribution, whereas the supervised fine-tuning is used for local optima search. Ngiam et al. [108] propose a deep learning method by integrating both audio and video data for learning representations. The solution is effective in learning from multiple abstractions and can capture correlations across multiple abstractions.\nBig data are continuously generated at a very high speed and require fast processing. Therefore, learning solution should not only be fast and efficient, but also be able to handle incremental data. However, we could not find a deep learning method that considers incremental data. Moreover,\ndeep learning can handle only volume and variety of big data [109]."}, {"heading": "3.5 Inference of Large Scale GRN with Association Rule Mining", "text": "In system biology, complex dynamic behavior of a group of genes and how it influences the expression of other genes, may be represented as GRN. By comparing between normal and diseased networks, one can identify potential drug targets for the target disease [110]. Research labs are producing a large number of expression data and consequently, the state-of-the-art inference techniques are insufficient in handling such large scale GRN. Microarray experiments conducted in different growth environments leads to heterogeneous data. Presence of steady-state and perturb expression data make the task of inference more challenging. Looking into the magnitude of difficulties in handling such voluminous, continuous, and heterogeneous data, the task of GRN inference may be considered as a big data analytics problem [111], [112]. Specialized inference methods in big data paradigm are very much necessary. A number of inference methods have been proposed for last several years [113], [114], [115], [116], [117], [118], [119]. However, they are limited in handling data sets with more than thousands of genes. In most of the cases, execution performance degrades exponentially with the increase in number of nodes or genes in the network. The scenario becomes more adverse with the increase in number of conditions (dimensions) or time points in time series expression data."}, {"heading": "3.5.1 Serial Association Mining", "text": "Association rule mining (ARM) came into existence as market basket analysis on boolean datasets. In association mining the sizes of datasets are semi large that can usually be accommodated on main memory. Typically, they are static in nature.\nThe AIS (Agrawal, Imielinski, Swami) [120] and Apriori [121] are two pioneering algorithms for mining association rule Although, they are robust, two major limitations are that they generate too many candidate itemsets and require too many passes over the whole database. SETM [122] was motivated by the desire to use SQL to calculate large itemsets, whereas, DHP (Direct Hashing and Pruning) attempts to reduce the number of candidate itemsets [123].\nThe partition approach [124] mines frequent itemsets from large datasets by dividing into smaller partitions, whereas, sampling [125] reduces the number of database scans. DIC (Dynamic Itemset Counting) [126] drastically reduces the number of scans of the database during frequent itemset finding. FP-Growth [127] finds frequent itemsets without candidate generation. However, the time taken to construct the FP-tree is quite large and its performance degrades with the increase in support count. Recently, another effective algorithm called OPAM [128] has been proposed, for finding all the frequent itemsets without generating any candidate sets. OPAM adopts an integrated approach to solve the frequent itemset finding problem in a single pass over the database.\nToday, most real-world databases are heterogeneous in nature, contain only quantitative data or both quantitative\nand categorical data. Further, such databases are multidimensional and their volume seems to be large. And this is where the conventional ARM techniques almost fail to satisfy the demand of mining fast growing varied voluminous data.\nAttribute partitioning approach is the most evident one to deal quantitative attributes [129]. As reported in [130], a possible solution to figure out meaningful quantitative regions for the discovery of association rules is clustering approach."}, {"heading": "3.5.2 Distributed and Parallel Association Mining", "text": "Sequential techniques are inadequate to provide scalability in terms of dimension, size or data which are spread around geographically dispersed locations. To cope up with such circumstances, researchers are looking for high-performance parallel and distributed association mining techniques. In the yester-years, a number of such techniques have been developed. These are mostly the extensions of already existing sequential methods.\nThe Count Distribution [131] is a simple parallelization of Apriori. This algorithm minimizes communication, because only the counts are exchanged among the processors. However, the algorithm replicates the entire hash tree on each processor and does not use the aggregate system memory effectively. PDM [132] is based on DHP [123]. In PDM, each processor generates the local supports of 1- itemsets and approximate counts for the 2-itemsets with a hash table. Next, PDM obtains the local counts for all candidates and exchanges them among all processors to determine the globally frequent itemsets. FDM (Fast Distributed Mining) [133] builds on Count Distribution [131], and the authors propose new techniques to reduce the number of candidates considered for counting and hence minimizes communication. FDM also suggests three optimizations: local pruning, global pruning, and count polling. To address the issues of FDM, a parallel version of FDM, called Fast Parallel Mining (FPM) [134] was introduced. FPM generates fewer candidates and retains the local and global pruning steps. But instead of count polling and subsequent broadcast of frequent itemsets, it simply broadcasts local supports to all processors.\nOther than improvement in computational cost, distribute or parallel versions inherently carry all the demerits suffered by their respective serial methods. Recently, several efforts have been made to extend some of the serial rule mining methods to be implemented in MapReduce framework for faster execution [135] and handling for voluminous data."}, {"heading": "3.5.3 Dynamic Association Mining", "text": "The techniques discussed in the previous sections are mostly based on the assumption that the datasets used as input does not change. In practice, no transaction database is static. Subsequent update of dataset could potentially invalidate existing association rules. Database updates require rediscovering the rules afresh by scanning the entire old and new data. Rediscovering of rules with the updates of the database leads to time consuming computation and leads to significant I/O overheads. The dynamics of databases can be represented as i) incremental updates and ii) decremental\nupdates. A number of efficient techniques have been developed for mining dynamic datasets.\nFast UPdate (FUP) [136] was proposed to compute large itemsets in a dataset that is updated regularly. The framework of FUP is similar to that of Apriori and DHP and is referred to as a k-pass algorithm because it scans the dataset k times. The Borders [137] algorithm is based on the concept of border sets introduced in [138]. It is another incremental method to generate frequent sets. The Decrement Updating Algorithm [139] tries to detect all the frequent itemsets from dynamically deleted databases.\nFor faster handling of varied, voluminous data, current association mining techniques are inadequate. Big data paradigm demands an integrated solution encompassing almost all the approaches to handle dynamic, large, and heterogeneous data. Several attempts have been made to infer GRN based on steady-state time series data. However, none of them can handle dynamic time-series data [112]. There is an urgent need of a scalable GRN reconstruction method that can work to infer reliable GRNs."}, {"heading": "4 CHALLENGES AND ISSUES IN BIG DATA ANALYTICS", "text": "Bioinformatics research has rapidly become a big data problem in the recent years. Big data not only possesses volume, velocity, and variety, but also are incremental and distributed. These properties of big data make it extremely difficult for the traditional data analytics to perform fast and accurately. Machine learning methods may be useful in handling big data analytics, since they have evolved in the computer science domain with objectives like performance and efficiency. The machine learning techniques for bioinformatics, the existing ones as well as those developed for handling big data, are discussed in the previous section. This section summarizes some of the challenges and research issues in big data analytics using machine learning methods."}, {"heading": "4.1 Challenges in big data analytics", "text": "The techniques used for analysis and visualization of traditional databases are not adequate on big data. The volume, velocity, variety, distributedness, and incremental nature of such data impose challenges on the traditional methods for data analytics. The volume of data generation and the speed of data transmission are growing rapidly. Napatech, a manufacturer of high speed network accelerators reported in 2014 that all network data will grow with an annual growth rate of 23% through 2018. The exponential increase in the use of hand-held devices and their associated sensors have mostly contributed to the growth of big data in the recent years.\nAlong with the increase in the data volume, the speed of data generation and transmission are also increasing. According to the Cisco report [140], the average mobile network connection speed in 2014 was 1,683 kbps, which will reach approximately 4.0 Mbps by 2019. Real time analytics on big data become more difficult with high data velocity. Although batch mode analytics may be scalable to high data velocity using distributed and parallel computing techniques, the slow I/O operations severely affect\nthe analytics performance. In this era, I/O speed is lagging far behind computing speed, acting as the limiting factor of computational throughput.\nMoreover, these continuously generated data are highly heterogeneous in nature. Traditional databases are arranged in terms of a set of defined schemas. Data warehouses store and update data following the extraction-transformationloading operations. Since big data systems continuously fetch new data in high velocity and high variety from heterogeneous sources, a structured database, such as data warehouse, is not at all suitable for dynamic storage and real time retrieval.\nGiven these challenges, the traditional data analytics techniques, such as machine learning and statistical analysis, are inefficient with big data in their original form. Consequently, the problem of machine learning enabled analytics has to be studied from the perspective of big data.\nData privacy is another major challenge of big data analytics, particularly in the bioinformatics and healthcare domain. In order to protect sensitive information, data sources might use data anonymity or publish only partial data. Analytics on partial or anonymous data might be more complex and inefficient."}, {"heading": "4.2 Issues in big data Analytics", "text": "Big data analytics require processing of massive amount of structured, semi-structured, poly-structured, and unstructured data, that grow over time. Real time analytics impose an additional requirement of time bound computation. Techniques from AI may be applied to find patterns and relations in unstructured data. Similarly, big data analytics can be scaled using parallel and distributed computing technologies, without compromising on accuracy of results. However, traditional data analytics on big data have certain issues regarding scalability and performance, which are discussed below.\n1) An integrated big data analytics architecture that is fault tolerant and able to handle voluminous and varied data in batches as well as in a continuous stream in real time is still missing. 2) Distributed computing is the prime solution to handle the massive volume of big data. However, most of the AI, data mining, and statistical analysis approaches are not originally designed for distributed computation. Although distributed algorithms have been proposed in the literature [141], [142], [143], they are mostly academic research and lack robust implementation, considering various MapReduce frameworks. 3) A big data store does not have a uniform data format. Rather big data analytics need to process heterogeneous data captured through sensors of various types. Therefore, intelligent algorithms are required to find a coherent meaning from disparate data. This increases the complexity of analytics. 4) Unstructured, semi-structured and poly-structured data introduce more problems, such as data inconsistency and redundancy. Data pre-processing is costly due to their heterogeneous nature and massive volume. Traditional data analytics techniques attempting to handle inconsistent and noisy data are found costly in terms of time and space complexities.\n5) Big data analytics need to mine datasets at different levels of abstraction. This significantly increases the complexity of the analytics methods, however, enabling biologists to analyze the data at various level of abstractions help understanding the interest of sematics biological data."}, {"heading": "5 TOOLS FOR BIG DATA ANALYTICS IN BIOINFORMATICS", "text": "Various tools have been developed over the years to handle the bioinformatics problems. The tools developed before the big data era are mostly standalone and not designed for very large scale data. In the last decade many large scale data analysis tools have been developed for several problems, such as microarray data analysis to idetify coexpressed patterns, gene-gene network analysis and salient module extraction, PPI complex finding, and RNA/DNA and sequence analysis. However, apart from certain sequence analysis tools, the other exsting tools are not adequate for handling big data or not suitable for cloud computing infrastructures. Along with specific tools, several cloud-based bioinformatics platforms have also been developed to integrate specific tools and to provide a fast comprehensive solution to multiple problems, such as Galaxy [144] and CloudBLAST [145].\n1) Tools for microarray data analysis Large number of software tools are available to perform various analysis on microarray data. However, not all the software are designed to handle large scale data. With the increase in the size of data sets, the time required to generate samples and sequences to identify complexes and to process heterogeneous disease query to find relevant complexes has become prohibitive. Beeline22 handles big data size by parallel computations and reduction in the data size with adaptive filtering. A quality assurance tool called caCORRECT [146] removes artifactual noises from high throughput microarray data. caCORRECT may be used to improve integrity and quality of both public microarray archives as well as reproduced data and to provide a universal quality score for validation. A web-based application called omniBiomarker [147] uses knowledge-driven algorithms to find differentially expressed genes for biomarker identification from high throughput gene expression data. The approach requires complex computation and validation, and omniBiomarker helps in identifying stable and reproducible biomarkers.\n2) Tools for gene-gene network analysis Gene expression datasets are already massive in size and getting bigger everyday. FastGCN [148] tool exploits parallelism with GPU architectures to find the co-expression networks in an optimized way. Similar GPU accelerated co-expression networks analysis methods are proposed by Arefin et al. [149] and McArt et al. [150]. The UCLA Gene Expression\n22. illumina.com/applications/microarrays/microarraysoftware/beeline.html\nTool (UGET) [151] performs large scale co-expression analysis to find disease gene associations. Disease networks have significantly higher gene correlations and UGET calculates the correlations among all possible pairs of genes. UGET has been found effective when tested on Celsius [152], which is the largest co-normalized microarray dataset of Affymetrix-based gene expression datawarehouse. WGCNA [153] is a popular R package for performing weighted gene co-expression network analysis and can be used in an R-Hadoop distributed computing system.\n3) Tools for PPI data analysis PPI complex finding problem is a highly time consuming process. From our research experience, standalone implementations for both supervised and unsupervised PPI complex finding, such as MATLAB programs, require days or even weeks of time to find complexes from a dataset of approximately 1 million interactions on standard workstations. Therefore, there is an urgent need to develop fast big data tools for PPI complex finding and ranking, w.r.t. any heterogenious disease network query. Several relatively fast tools have been developed for PPI complex (isolated and overlapping) finding, such as NeMo [154], MCODE [155], and ClusterONE [156], either as a standalone tool or as a Cytoscape plugin. However, these tools cannot be used in distributed systems for better efficiency on large-scale PPI data. Finally, PathBLAST [157] is an important web-based tool for fast alignment of protein interaction networks.\n4) Tools for sequence analysis For sequence analysis problems, several tools have been developed on top of the Hadoop MapReduce platform to perform analytics on large scale sequence data. BioPig [158] is a notable hadoop-based tool for sequence analysis that scales automatically with the data size and can be ported directly to many hadoop infrastructures. SeqPig [159] is another such tool. The Crossbow [160] tool combines Bowtie [161], an ultrafast and memory efficient short read aligner, and SoapSNP [162], an accurate genotyper, to perform large scale whole genome sequence analytics on cloud platforms or on a local hadoop cluster. Other cloudbased tools that have been developed for large scale sequence analysis are Stormbow [163], CloVR [164], and Rainbow [165]. There exist other programs for large scale sequence analysis that do not use big data technologies, such as Vmatch [166] and SeqMonk23.\n5) Tools for pathway analysis To support pathway analysis, a good number of tools have been developed for pathway analysis, such as GO-Elite [167] to describe particular genes or metabolites, PathVisio [168] for analysis and drawing, directPA [169] to perform analysis in a high-dimensional space for identifying pathways, Pathway Processor [170] to analyze expression data\n23. www.bioinformatics.bbsrc.ac.uk/projects/seqmonk\nregarding metabolic pathways, Pathway-PDT [171] to perform analysis using raw genotypes in general nuclear families, and Pathview [172] for pathway based data integration. However, these tools neither use distributed computing platforms, nor they are developed as a cloud-based application, for high scalability.\nAlthough evolutionary research may use tools developed for more specific problems, such as sequence analysis or gene-gene network analysis, a big data tool for comprehensive evolutionary research is still not known. The existing tools for evolution research, such as MEGA [173] and EvoPipes.net [174] are not developed for big data in evolutionary research."}, {"heading": "6 CONCLUSION", "text": "This paper discusses the recent surge in bioinformatics data stores in terms of volume as well as dimension. With the advent of new high throughput and cheap data capturing tools, this rapid growth in data will continue in the coming years. Bioinformatics data are voluminous, heterogeneous, incremental, and distributed geographically all over the world. Consequently, the big data analytics techniques are required to solve the problems in bioinformatics.\nThe problems, data sources and data types in bioinformatics are diverse in nature. The existing big data architectures do not provide a comprehensive solution for big data analytics, which is fast, fault tolerant, large scale, incremental, distributed, and optimized for iterative and complex computations. The well known MapReduce architecture for distributed computing executes in a batch mode and has high disk read/write overhead. On the other hand, the graph-based architectures for streaming applications fail to provide fault tolerance. An integrated big data analytics architecture that fulfills the requirements of the problems in bioinformatics is an urgent need.\nMachine learning has been the most utilized tool for data analytics. Large scale data existed well before the big data era, particularly in bioinformatics. Machine learning tools have been successfully used to analyze both small scale as well as large scale data using various techniques such as, sampling, feature selection, and distributed computations. However, big data poses more challenges on the traditional learning methods in terms of velocity, variety, and incremental data. Traditional learning methods usually embed iterative processing and complex data dependency among operations. Consequently, the traditional machine learning methods cannot be used to perform fast processing on massive data using big data platforms, such as MapReduce. This paper discusses the traditional machine learning methods , their limitations, and the efforts made in the recent years to extend them for big data, such as the incremental, parallel, and multi-view clustering methods to handle complex bioinformatics problems.\nThe existing tools for many bioinformatics problems are still not adequate for big data. A few tools have been developed for sequence analysis using the Hadoop MapReduce platform, such as BioPig [158] and Crossbow [160] in the\nrecent years. Apart from that, other important bioinformatics problems, such as PPI network analysis or disease network analysis, still lacking Hadoop or cloud-based big data tools. Considering the big data boom in bioinformatics and the emerging research opportunities, big data analytics in bioinformatics need to be properly addressed from the perspectives of big data technologies and effective data analytics approaches, such as machine learning."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank the Ministry of HRD, Govt. of India for funding as a Centre of Excellence with thrust area in Machine Learning Research and Big Data Analytics for the period 2014-2019."}], "references": [{"title": "Data-intensive applications, challenges, techniques and technologies: A survey on big data", "author": ["C.P. Chen", "C.-Y. Zhang"], "venue": "Information Sciences, vol. 275, pp. 314\u2013347, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "The digital universe of opportunities: Rich data and the increasing value of the internet of things", "author": ["V. Turner", "J. Gantz", "D. Reinsel", "S. Minton"], "venue": "International Data Corporation, White Paper, IDC 1672, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "The internet of things", "author": ["N. Gershenfeld", "R. Krikorian", "D. Cohen"], "venue": "Scientific American, vol. 291, no. 4, p. 76, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "The mobile economy 2013", "author": ["M. Page", "M. Molina", "J. Gordon"], "venue": "ATKearney.[Online] URL: http://www. atkearney. com/documents/10192/760890/The Mobile Economy 2013. pdf (Accessed on Feb 09, 2015), 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Artificial intelligence and big data", "author": ["D.E. O\u2019Leary"], "venue": "IEEE Intelligent Systems, vol. 28, no. 2, pp. 0096\u201399, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Big data opportunities and challenges: Discussions from data analytics perspectives [discussion forum", "author": ["Z. Zhou", "N. Chawla", "Y. Jin", "G. Williams"], "venue": "Computational Intelligence Magazine, IEEE, vol. 9, no. 4, pp. 62\u201374, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "How big is the human genome?", "author": ["R.J. Robison"], "venue": "Precision Medicine,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "EMBL-EBI annual scientific report 2013", "author": ["EMBL-European Bioinformatics Institute"], "venue": "2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Biology: The big challenges of big data", "author": ["V. Marx"], "venue": "Nature, vol. 498, no. 7453, pp. 255\u2013260, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Breaking the Genome Bottleneck", "author": ["S.Y. Rojahn"], "venue": "MIT Technology Review, May 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Next-generation sequencing data interpretation: enhancing reproducibility and accessibility", "author": ["A. Nekrutenko", "J. Taylor"], "venue": "Nature Reviews Genetics, vol. 13, no. 9, pp. 667\u2013672, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "KEGG: kyoto encyclopedia of genes and genomes", "author": ["M. Kanehisa", "S. Goto"], "venue": "Nucleic acids research, vol. 28, no. 1, pp. 27\u201330, 2000.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Reactome: a database of reactions, pathways and biological processes", "author": ["D. Croft", "G. OKelly", "G. Wu", "R. Haw", "M. Gillespie", "L. Matthews", "M. Caudy", "P. Garapati", "G. Gopinath", "B. Jassal"], "venue": "Nucleic acids research, p. gkq1018, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Pathway commons, a web resource for biological pathway data", "author": ["E.G. Cerami", "B.E. Gross", "E. Demir", "I. Rodchenkov", "\u00d6. Babur", "N. Anwar", "N. Schultz", "G.D. Bader", "C. Sander"], "venue": "Nucleic acids research, vol. 39, no. suppl 1, pp. D685\u2013D690, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Serbgo: searching for the best go tool", "author": ["J. Mosquera", "A. S\u00e1nchez-Pla"], "venue": "Nucleic acids research, vol. 36, no. suppl 2, pp. W368\u2013 W371, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Cours d\u2019\u00e9conomie politique", "author": ["V. Pareto"], "venue": "Librairie Droz,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1964}, {"title": "Evolutionary computation: Toward a new philosophy of machine intelligence", "author": ["T. B\u00e4ck"], "venue": "1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Finding groups in data. an introduction to cluster analysis", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": "Wiley Series in Probability and Mathematical Statistics. Applied Probability and Statistics, New York: Wiley, 1990, vol. 1, 1990.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1990}, {"title": "Birch: an efficient data clustering method for very large databases", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "ACM SIG- MOD Record, vol. 25, no. 2. ACM, 1996, pp. 103\u2013114.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1996}, {"title": "Srda: An efficient algorithm for largescale discriminant analysis", "author": ["D. Cai", "X. He", "J. Han"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 20, no. 1, pp. 1\u201312, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "An improved generalized discriminant analysis for large-scale data set", "author": ["W. Shi", "Y.-F. Guo", "C. Jin", "X. Xue"], "venue": "Machine Learning and Applications, 2008. ICMLA\u201908. Seventh International Conference on. IEEE, 2008, pp. 769\u2013772.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributed data mining: Algorithms, systems, and applications", "author": ["B.-H. Park", "H. Kargupta"], "venue": "2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Paddmas: parallel and distributed data mining application suite", "author": ["O. Rana", "D. Walker", "M. Li", "S. Lynden", "M. Ward"], "venue": "Parallel and Distributed Processing Symposium, 2000. IPDPS 2000. Proceedings. 14th International. IEEE, 2000, pp. 387\u2013392.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "Mlbase: A distributed machine-learning system.", "author": ["T. Kraska", "A. Talwalkar", "J.C. Duchi", "R. Griffith", "M.J. Franklin", "M.I. Jordan"], "venue": "CIDR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Distributed graphlab: a framework for machine learning and data mining in the cloud", "author": ["Y. Low", "D. Bickson", "J. Gonzalez", "C. Guestrin", "A. Kyrola", "J.M. Hellerstein"], "venue": "Proceedings of the VLDB Endowment, vol. 5, no. 8, pp. 716\u2013727, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Mapreduce: Simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "OSDI\\\u201904, 2005, pp. 137\u2013150.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Pregel: a system for large-scale graph processing", "author": ["G. Malewicz", "M.H. Austern", "A.J. Bik", "J.C. Dehnert", "I. Horn", "N. Leiser", "G. Czajkowski"], "venue": "Proceedings of the 2010 ACM SIGMOD International Conference on Management of data. ACM, 2010, pp. 135\u2013146.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "The hadoop distributed file system", "author": ["K. Shvachko", "H. Kuang", "S. Radia", "R. Chansler"], "venue": "Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium on. IEEE, 2010, pp. 1\u201310.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "The quantcast file system", "author": ["M. Ovsiannikov", "S. Rus", "D. Reeves", "P. Sutter", "S. Rao", "J. Kelly"], "venue": "Proceedings of the VLDB Endowment, vol. 6, no. 11, pp. 1092\u20131101, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM, vol. 51, no. 1, pp. 107\u2013113, 2008.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Twister: a runtime for iterative mapreduce", "author": ["J. Ekanayake", "H. Li", "B. Zhang", "T. Gunarathne", "S.-H. Bae", "J. Qiu", "G. Fox"], "venue": "Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing. ACM, 2010, pp. 810\u2013818.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012, pp. 2\u20132.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Graphlab: A new framework for parallel machine learning", "author": ["Y. Low", "J.E. Gonzalez", "A. Kyrola", "D. Bickson", "C.E. Guestrin", "J. Hellerstein"], "venue": "arXiv preprint arXiv:1408.2041, 2014.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "A highperformance, portable implementation of the mpi message passing interface standard", "author": ["W. Gropp", "E. Lusk", "N. Doss", "A. Skjellum"], "venue": "Parallel computing, vol. 22, no. 6, pp. 789\u2013 828, 1996.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1996}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "springer New York,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "Network anomaly detection: A machine learning perspective", "author": ["D.K. Bhattacharyya", "J.K. Kalita"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Big data and their epistemological challenge", "author": ["L. Floridi"], "venue": "Philosophy & Technology, vol. 25, no. 4, pp. 435\u2013437, 2012.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Online feature selection for mining big data", "author": ["S.C. Hoi", "J. Wang", "P. Zhao", "R. Jin"], "venue": "Proceedings of the 1st international workshop on big data, streams and heterogeneous source mining: Algorithms, systems, programming models and applications. ACM, 2012, pp. 93\u2013100.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-infinite programming", "author": ["M. L\u00f3pez", "G. Still"], "venue": "European Journal of Operational Research, vol. 180, no. 2, pp. 491\u2013518, 2007.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards ultrahigh dimensional feature selection for big data", "author": ["M. Tan", "I.W. Tsang", "L. Wang"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1371\u20131429, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "A novel hybridized rough set and improved harmony search based feature selection for protein sequence classification", "author": ["M. Bagyamathi", "H.H. Inbarani"], "venue": "Big Data in Complex Systems. Springer, 2015, pp. 173\u2013204.  JOURNAL OF  LTEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014  17", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature selection with annealing for big data learning", "author": ["A. Barbu", "Y. She", "L. Ding", "G. Gramajo"], "venue": "arXiv preprint arXiv:1310.2880, 2013.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2013}, {"title": "A fuzzy rough set approach for incremental feature selection on hybrid information systems", "author": ["A. Zeng", "T. Li", "D. Liu", "J. Zhang", "H. Chen"], "venue": "Fuzzy Sets and Systems, vol. 258, pp. 39\u201360, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning. 1997", "author": ["T.M. Mitchell"], "venue": "Burr Ridge, IL: McGraw Hill, vol. 45, 1997.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1997}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "springer New York,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2006}, {"title": "Big data algorithms for visualization and supervised learning", "author": ["N. Djuric"], "venue": "Ph.D. dissertation, Temple University, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "A divide-and-conquer solver for kernel support vector machines", "author": ["C.-J. Hsieh", "S. Si", "I.S. Dhillon"], "venue": "arXiv preprint arXiv:1311.0914, 2013.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2013}, {"title": "New primal svm solver with linear computational cost for big data classifications", "author": ["F. Nei", "Y. Huang", "X. Wang", "H. Huang"], "venue": "Proceedings of the 31st international conference on Machine Learning. JMLR, 2014, pp. 1\u20139.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Individual detection of patients with parkinson disease using support vector machine analysis of diffusion tensor imaging data: initial results", "author": ["S. Haller", "S. Badoud", "D. Nguyen", "V. Garibotto", "K. Lovblad", "P. Burkhard"], "venue": "American Journal of Neuroradiology, vol. 33, no. 11, pp. 2123\u20132128, 2012.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic detection of diabetes diagnosis using feature weighted support vector machines based on mutual information and modified cuckoo search", "author": ["D. Giveki", "H. Salimi", "G. Bahmanyar", "Y. Khademian"], "venue": "arXiv preprint arXiv:1201.2173, 2012.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2012}, {"title": "Svm based decision support system for heart disease classification with integer-coded genetic algorithm to select critical features", "author": ["S. Bhatia", "P. Prakash", "G. Pillai"], "venue": "Proceedings of the World Congress on Engineering and Computer Science, WCECS, 2008, pp. 22\u201324.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2008}, {"title": "Application of support vector machine for prediction of medication adherence in heart failure patients", "author": ["Y.-J. Son", "H.-G. Kim", "E.-H. Kim", "S. Choi", "S.-K. Lee"], "venue": "Healthcare informatics research, vol. 16, no. 4, pp. 253\u2013259, 2010.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic gradient boosted distributed decision trees", "author": ["J. Ye", "J.-H. Chow", "J. Chen", "Z. Zheng"], "venue": "Proceedings of the 18th ACM conference on Information and knowledge management. ACM, 2009, pp. 2061\u20132064.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2009}, {"title": "The hadoop distributed file system: Architecture and design", "author": ["D. Borthakur"], "venue": "Hadoop Project Website, vol. 11, no. 2007, p. 21, 2007.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2007}, {"title": "Big data decision trees with r", "author": ["R. Calaway", "L. Edlefsen", "L. Gong", "S. Fast"], "venue": "Revolution.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 0}, {"title": "Decision tree learning on very large data sets", "author": ["L.O. Hall", "N. Chawla", "K.W. Bowyer"], "venue": "Systems, Man, and Cybernetics, 1998. 1998 IEEE International Conference on, vol. 3. IEEE, 1998, pp. 2579\u20132584.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1998}, {"title": "Data clustering: algorithms and applications", "author": ["C.C. Aggarwal", "C.K. Reddy"], "venue": "CRC Press,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2013}, {"title": "Data mining cluster analysis: Basic concepts and algorithms", "author": ["P.N. Tan", "K. Steinbach", "V. Kumar"], "venue": "2006.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2006}, {"title": "Biclustering of expression data.", "author": ["Y. Cheng", "G.M. Church"], "venue": "in Ismb,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2000}, {"title": "Intersected coexpressed subcube miner: An effective triclustering algorithm", "author": ["H. Ahmed", "P. Mahanta", "D. Bhattacharyya", "J. Kalita", "A. Ghosh"], "venue": "Information and Communication Technologies (WICT), 2011 World Congress on. IEEE, 2011, pp. 846\u2013851.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2011}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM computing surveys (CSUR), vol. 31, no. 3, pp. 264\u2013 323, 1999.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1999}, {"title": "Clustering by means of medoids", "author": ["L. Kaufman", "P. Rousseeuw"], "venue": null, "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1987}, {"title": "Finding groups in data: an introduction to cluster analysis", "author": ["L. Kaufman", "P.J. Rousseeuw"], "venue": null, "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2009}, {"title": "Clarans: A method for clustering objects for spatial data mining", "author": ["R.T. Ng", "J. Han"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 14, no. 5, pp. 1003\u20131016, 2002.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2002}, {"title": "A survey of clustering data mining techniques", "author": ["P. Berkhin"], "venue": "Grouping multidimensional data. Springer, 2006, pp. 25\u201371.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2006}, {"title": "Cure: an efficient clustering algorithm for large databases", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "ACM SIGMOD Record, vol. 27, no. 2. ACM, 1998, pp. 73\u201384.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 1998}, {"title": "Densitybased clustering", "author": ["H.-P. Kriegel", "P. Kr\u00f6ger", "J. Sander", "A. Zimek"], "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, vol. 1, no. 3, pp. 231\u2013240, 2011.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2011}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise.", "author": ["M. Ester", "H.-P. Kriegel", "J. Sander", "X. Xu"], "venue": "in Kdd,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 1996}, {"title": "An efficient approach to clustering in large multimedia databases with noise", "author": ["A. Hinneburg", "D.A. Keim"], "venue": "KDD, vol. 98, 1998, pp. 58\u201365.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1998}, {"title": "Some applications of graph theory to clustering", "author": ["L.J. Hubert"], "venue": "Psychometrika, vol. 39, no. 3, pp. 283\u2013309, 1974.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 1974}, {"title": "Chameleon: Hierarchical clustering using dynamic modeling", "author": ["G. Karypis", "E.-H. Han", "V. Kumar"], "venue": "Computer, vol. 32, no. 8, pp. 68\u201375, 1999.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 1999}, {"title": "Fuzzy cluster analysis: methods for classification, data analysis and image recognition", "author": ["F. H\u00f6ppner"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 1999}, {"title": "The self-organizing map", "author": ["T. Kohonen"], "venue": "Proceedings of the IEEE, vol. 78, no. 9, pp. 1464\u20131480, 1990.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 1990}, {"title": "Discovering local structure in gene expression data: The Order-Preserving submatrix problem", "author": ["A. Ben-Dor", "B. Chor", "R. Karp", "Z. Yakhini"], "venue": "Journal of Computational Biology, vol. 10, pp. 373\u2013384, 2003.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2003}, {"title": "A systematic comparison and evaluation of biclustering methods for gene expression data", "author": ["A. Preli\u0107", "S. Bleuler", "P. Zimmermann", "A. Wille", "P. B\u00fchlmann", "W. Gruissem", "L. Hennig", "L. Thiele", "E. Zitzler"], "venue": "Bioinformatics, vol. 22, no. 9, pp. 1122\u20131129, 2006.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral biclustering of microarray data: coclustering genes and conditions", "author": ["Y. Kluger", "R. Basri", "J. Chang", "M. Gerstein"], "venue": "Genome research, vol. 13, no. 4, pp. 703\u2013716, 2003.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2003}, {"title": "Revealing modularity and organization in the yeast molecular network by integrated analysis of highly heterogeneous genomewide data", "author": ["A. Tanay", "R. Sharan", "M. Kupiec", "R. Shamir"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 101, no. 9, pp. 2981\u20132986, 2004.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2004}, {"title": "Enhanced biclustering on expression data", "author": ["J. Yang", "H. Wang", "W. Wang", "P. Yu"], "venue": "Proceedings of Third IEEE Symposium on Bioinformatics and Bioengineering, pp. 321 \u2013 327, 2003.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2003}, {"title": "Iterative signature algorithm for the analysis of large-scale gene expression data", "author": ["S. Bergmann", "J. Ihmels", "N. Barkai"], "venue": "Phys. Rev. E, vol. 67, pp. 031 902\u2013031 919, 2003.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2003}, {"title": "Measuring the quality of shifting and scaling patterns in biclusters", "author": ["B. Pontes", "R. Gir\u00e1ldez", "J. Aguilar-Ruiz"], "venue": "Pattern Recognition in Bioinformatics, vol. 6282, pp. 242\u2013252, 2010.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2010}, {"title": "An effective measure for assessing the quality of biclusters", "author": ["F. Divina", "B. Pontes", "R. Gir\u00e1ldez", "J.S. Aguilar-Ruiz"], "venue": "Computers in biology and medicine, vol. 42, no. 2, pp. 245\u2013256, 2011.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding correlated biclusters from gene expression data", "author": ["W.-H. Yang", "D.-Q. Dai", "H. Yan"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 23, no. 4, pp. 568\u2013584, 2011.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2011}, {"title": "Shiftingand-scaling correlation based biclustering algorithm", "author": ["H. Ahmed", "P. Mahanta", "D. Bhattacharyya", "J. Kalita"], "venue": "Computational Biology and Bioinformatics, IEEE/ACM Transactions on, vol. 11, no. 6, pp. 1239\u20131252, 2014.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining coherent gene clusters from gene-sample-time microarray data.", "author": ["M.R.C.T.D.Jiang", "J. Pei", "A. Zhang"], "venue": "Proc of the 10 th ACM SIGKDD Conference(KDD\u201904).,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2004}, {"title": "Tricluster: an effective algorithm for mining coherent clusters in 3D microarray data.", "author": ["L. Zhao", "M.J. Zaki"], "venue": null, "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2005}, {"title": "gtricluster: A more general and effective 3d clustering algorithm for gene-sampletime microarray data.", "author": ["H. Jiang", "S. Zhou", "J. Guan", "Y. Zheng"], "venue": "in BioDM\u201906,", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2006}, {"title": "Extensions to the k-means algorithm for clustering large data sets with categorical values", "author": ["Z. Huang"], "venue": "Data mining and knowledge discovery, vol. 2, no. 3, pp. 283\u2013304, 1998.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient disk-based k-means clustering for relational databases", "author": ["C. Ordonez", "E. Omiecinski"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 16, no. 8, pp. 909\u2013921, 2004.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2004}, {"title": "Scaling clustering algorithms to large databases.", "author": ["P.S. Bradley", "U.M. Fayyad", "C. Reina"], "venue": "in KDD,", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 1998}, {"title": "Wavecluster: a wavelet-based clustering approach for spatial data in very large databases", "author": ["G. Sheikholeslami", "S. Chatterjee", "A. Zhang"], "venue": "The VLDB Journal, vol. 8, no. 3-4, pp. 289\u2013304, 2000.  JOURNAL OF  LTEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014  18", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2000}, {"title": "Parallel clustering algorithms", "author": ["X. Li", "Z. Fang"], "venue": "Parallel Computing, vol. 11, no. 3, pp. 275\u2013290, 1989.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 1989}, {"title": "Parallel k-means clustering based on mapreduce", "author": ["W. Zhao", "H. Ma", "Q. He"], "venue": "Cloud Computing. Springer, 2009, pp. 674\u2013679.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast parallel clustering algorithm for large spatial databases", "author": ["X. Xu", "J. J\u00e4ger", "H.-P. Kriegel"], "venue": "High Performance Data Mining. Springer, 2002, pp. 263\u2013290.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2002}, {"title": "Large-scale parallel data clustering", "author": ["D. Judd", "P.K. McKinley", "A.K. Jain"], "venue": "Pattern Recognition, 1996., Proceedings of the 13th International Conference on, vol. 4. IEEE, 1996, pp. 488\u2013493.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 1996}, {"title": "Pbirch: a scalable parallel clustering algorithm for incremental data", "author": ["A. Garg", "A. Mangla", "N. Gupta", "V. Bhatnagar"], "venue": "Database Engineering and Applications Symposium, 2006. IDEAS\u201906. 10th International. IEEE, 2006, pp. 315\u2013316.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2006}, {"title": "Analysis and study of incremental k-means clustering algorithm", "author": ["S. Chakraborty", "N. Nagwani"], "venue": "High Performance Architecture and Grid Computing. Springer, 2011, pp. 338\u2013341.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2011}, {"title": "An incremental approach to building a cluster hierarchy", "author": ["D.H. Widyantoro", "T.R. Ioerger", "J. Yen"], "venue": "Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International Conference on. IEEE, 2002, pp. 705\u2013708.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2002}, {"title": "An incremental grid density-based clustering algorithm", "author": ["N. Chen", "A.-z. Chen", "L.-x. Zhou"], "venue": "Journal of software, vol. 13, no. 1, pp. 1\u20137, 2002.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2002}, {"title": "Clustering multi-represented objects with noise", "author": ["K. Kailing", "H.-P. Kriegel", "A. Pryakhin", "M. Schubert"], "venue": "Advances in Knowledge Discovery and Data Mining. Springer, 2004, pp. 394\u2013403.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2004}, {"title": "A unified framework for clustering heterogeneous web objects", "author": ["H.-J. Zeng", "Z. Chen", "W.-Y. Ma"], "venue": "Web Information Systems Engineering, 2002. WISE 2002. Proceedings of the Third International Conference on. IEEE, 2002, pp. 161\u2013170.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "Proceedings of the 26th annual international conference on machine learning. ACM, 2009, pp. 129\u2013136.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2009}, {"title": "A co-training approach for multiview spectral clustering", "author": ["A. Kumar", "H. Daum\u00e9"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 393\u2013400.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2006}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 2011, pp. 689\u2013 696.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep learning applications and challenges in big data analytics", "author": ["M.M. Najafabadi", "F. Villanustre", "T.M. Khoshgoftaar", "N. Seliya", "R. Wald", "E. Muharemagic"], "venue": "Journal of Big Data, vol. 2, no. 1, pp. 1\u201321, 2015.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2015}, {"title": "Gene regulatory network inference: evaluation and application to ovarian cancer allows the prioritization of drug targets", "author": ["P.B. Madhamshettiwar", "S.R. Maetschke", "M.J. Davis", "A. Reverter", "M.A. Ragan"], "venue": "Genome medicine, vol. 4, no. 5, pp. 1\u201316, 2012.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2012}, {"title": "Modeling genomic regulatory networks with big data", "author": ["H. Bolouri"], "venue": "Trends in Genetics, vol. 30, no. 5, pp. 182\u2013191, 2014.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2014}, {"title": "Reconstructing biological gene regulatory networks: where optimization meets big data", "author": ["S.A. Thomas", "Y. Jin"], "venue": "Evolutionary Intelligence, vol. 7, no. 1, pp. 29\u201347, 2014.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2014}, {"title": "Coexpression analysis of human genes across many microarray data sets", "author": ["H. Lee", "A. Hsu", "J. Sajdak", "J. Qin", "P. Pavlidis"], "venue": "Genome research, vol. 14, no. 6, pp. 1085\u20131094, 2004.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2004}, {"title": "Using bayesian networks to analyze expression data", "author": ["N. Friedman", "M. Linial", "I. Nachman", "D. Pe\u2019er"], "venue": "J. of computational biology, vol. 7, no. 3-4, pp. 601\u2013620, 2000.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2000}, {"title": "Boolean network model predicts cell cycle sequence of fission yeast", "author": ["M. Davidich", "S. Bornholdt"], "venue": "PLoS One, vol. 3, no. 2, p. e1672, 2008.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2008}, {"title": "Large-scale mapping and validation of escherichia coli transcriptional regulation from a compendium of expression profiles", "author": ["J. Faith", "B. Hayete", "J. Thaden", "I. Mogno", "J. Wierzbowski", "G. Cottarel", "S. Kasif", "J. Collins", "T. Gardner"], "venue": "PLoS biology, vol. 5, no. 1, p. e8, 2007.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2007}, {"title": "Aracne: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context", "author": ["A. Margolin", "I. Nemenman", "K. Basso", "C. Wiggins", "G. Stolovitzky", "R. Favera", "A. Califano"], "venue": "BMC bioinformatics, vol. 7, no. Suppl 1, p. S7, 2006.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2006}, {"title": "Informationtheoretic inference of large transcriptional regulatory networks", "author": ["P. Meyer", "K. Kontos", "F. Lafitte", "G. Bontempi"], "venue": "EURASIP Journal on Bioinformatics and Systems Biology, vol. 2007, 2007.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2007}, {"title": "Reconstruction of gene co-expression network from microarray data using local expression patterns", "author": ["S. Roy", "D.K. Bhattacharyya", "J.K. Kalita"], "venue": "BMC bioinformatics, vol. 15, no. Suppl 7, p. S10, 2014.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining association rules between sets of items in large databases", "author": ["R. Agrawal", "T. Imieli\u0144ski", "A. Swami"], "venue": "ACM SIGMOD Record, vol. 22, no. 2. ACM, 1993, pp. 207\u2013216.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 1993}, {"title": "Fast algorithms for mining association rules", "author": ["R. Agrawal", "R. Srikant"], "venue": "Proc. 20th int. conf. very large data bases, VLDB, vol. 1215, 1994, pp. 487\u2013499.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 1994}, {"title": "Set-oriented mining for association rules in relational databases", "author": ["M. Houtsma", "A. Swami"], "venue": "Data Engineering, 1995. Proceedings of the Eleventh International Conference on. IEEE, 1995, pp. 25\u201333.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 1995}, {"title": "An effective hash-based algorithm for mining association", "author": ["J.S. Park", "M.-S. Chen", "P.S. Yu"], "venue": "rules. ACM,", "citeRegEx": "123", "shortCiteRegEx": "123", "year": 1995}, {"title": "An efficient algorithm for mining association rules in large databases", "author": ["A. Savasere", "E.R. Omiecinski", "S.B. Navathe"], "venue": "1995.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 1995}, {"title": "Sampling large databases for association rules", "author": ["H. Toivonen"], "venue": "VLDB, vol. 96, 1996, pp. 134\u2013145.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 1996}, {"title": "Dynamic itemset counting and implication rules for market basket data", "author": ["S. Brin", "R. Motwani", "J.D. Ullman", "S. Tsur"], "venue": "ACM SIGMOD Record, vol. 26, no. 2. ACM, 1997, pp. 255\u2013264.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 1997}, {"title": "Mining frequent patterns by patterngrowth: methodology and implications", "author": ["J. Han", "J. Pei"], "venue": "ACM SIGKDD explorations newsletter, vol. 2, no. 2, pp. 14\u201320, 2000.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2000}, {"title": "Opam: An efficient one pass association mining technique without candidate generation", "author": ["S. Roy", "D.K. Bhattacharyya"], "venue": "Journal of Convergence Informarion Technology, vol. 3, no. 3, 2008.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2008}, {"title": "Mining quantitative association rules in large relational tables", "author": ["R. Srikant", "R. Agrawal"], "venue": "ACM SIGMOD Record, vol. 25, no. 2. ACM, 1996, pp. 1\u201312.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 1996}, {"title": "An efficient clustering algorithm for mining fuzzy quantitative association rules", "author": ["B.-C. Chien", "Z.-L. Lin", "T.-P. Hong"], "venue": "IFSA World Congress and 20th NAFIPS International Conference, 2001. Joint 9th, vol. 3. IEEE, 2001, pp. 1306\u20131311.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2001}, {"title": "Parallel mining of association rules", "author": ["R. Agrawal", "J.C. Shafer"], "venue": "IEEE Transactions on knowledge and Data Engineering, vol. 8, no. 6, pp. 962\u2013969, 1996.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient parallel data mining for association rules", "author": ["J.S. Park", "M.-S. Chen", "P.S. Yu"], "venue": "Proceedings of the fourth international conference on Information and knowledge management. ACM, 1995, pp. 31\u201336.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 1995}, {"title": "A fast distributed algorithm for mining association rules", "author": ["D.W. Cheung", "J. Han", "V.T. Ng", "A.W. Fu", "Y. Fu"], "venue": "Parallel and Distributed Information Systems, 1996., Fourth International Conference on. IEEE, 1996, pp. 31\u201342.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 1996}, {"title": "Effect of data skewness in parallel mining of association rules", "author": ["D.W. Cheung", "Y. Xiao"], "venue": "Research and Development in Knowledge Discovery and Data Mining. Springer, 1998, pp. 48\u2013 60.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 1998}, {"title": "Frequent itemset mining for big data", "author": ["S. Moens", "E. Aksehirli", "B. Goethals"], "venue": "Big Data, 2013 IEEE International Conference on. IEEE, 2013, pp. 111\u2013118.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2013}, {"title": "An efficient algorithm for the incremental updation of association rules in large databases.", "author": ["S. Thomas", "S. Bodagala", "K. Alsabti", "S. Ranka"], "venue": "in KDD,", "citeRegEx": "136", "shortCiteRegEx": "136", "year": 1997}, {"title": "Borders: An efficient algorithm for association generation in dynamic databases", "author": ["Y. Aumann", "R. Feldman", "O. Lipshtat", "H. Manilla"], "venue": "Journal of Intelligent Information Systems, vol. 12, no. 1, pp. 61\u201373, 1999.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 1999}, {"title": "Levelwise search and borders of theories in knowledge discovery", "author": ["H. Mannila", "H. Toivonen"], "venue": "Data mining and knowledge discovery, vol. 1, no. 3, pp. 241\u2013258, 1997.", "citeRegEx": "138", "shortCiteRegEx": null, "year": 1997}, {"title": "A decremental algorithm for maintaining frequent itemsets in dynamic databases", "author": ["S. Zhang", "X. Wu", "J. Zhang", "C. Zhang"], "venue": "Data Warehousing and Knowledge Discovery. Springer, 2005, pp. 305\u2013314.  JOURNAL OF  LTEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014  19", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2005}, {"title": "Cisco visual networking index: global mobile data traffic forecast update, 2014\u20132019", "author": ["Cisco"], "venue": "Cisco Public Information, 2015.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2015}, {"title": "A data parallel approach for large-scale gaussian process modeling.", "author": ["A. Choudhury", "P.B. Nair", "A.J. Keane"], "venue": "in SDM. SIAM,", "citeRegEx": "141", "shortCiteRegEx": "141", "year": 2002}, {"title": "Using bayesian model averaging to calibrate forecast ensembles", "author": ["A.E. Raftery", "T. Gneiting", "F. Balabdaoui", "M. Polakowski"], "venue": "Monthly Weather Review, vol. 133, no. 5, pp. 1155\u20131174, 2005.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2005}, {"title": "Privacy-preserving bayesian network structure computation on distributed heterogeneous data", "author": ["R. Wright", "Z. Yang"], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2004, pp. 713\u2013718.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2004}, {"title": "Galaxy: a comprehensive approach for supporting accessible, reproducible, and transparent computational research in the life sciences", "author": ["J. Goecks", "A. Nekrutenko", "J. Taylor"], "venue": "Genome Biol, vol. 11, no. 8, p. R86, 2010.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2010}, {"title": "Cloudblast: Combining mapreduce and virtualization on distributed resources for bioinformatics applications", "author": ["A. Matsunaga", "M. Tsugawa", "J. Fortes"], "venue": "eScience, 2008. eScience\u201908. IEEE Fourth International Conference on. IEEE, 2008, pp. 222\u2013229.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2008}, {"title": "chip artifact CORRECTion (caCORRECT): a bioinformatics system for quality assurance of genomics and proteomics array data", "author": ["T.H. Stokes", "R.A. Moffitt", "J.H. Phan", "M.D. Wang"], "venue": "Annals of biomedical engineering, vol. 35, no. 6, pp. 1068\u20131080, 2007.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2007}, {"title": "omniBiomarker: a web-based application for knowledge-driven biomarker identification", "author": ["J.H. Phan", "A.N. Young", "M.D. Wang"], "venue": "Biomedical Engineering, IEEE Transactions on, vol. 60, no. 12, pp. 3364\u20133367, 2013.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2013}, {"title": "FastGCN: A GPU Accelerated Tool for Fast Gene Co-Expression Networks.", "author": ["M. Liang", "F. Zhang", "G. Jin", "J. Zhu"], "venue": "PloS one,", "citeRegEx": "148", "shortCiteRegEx": "148", "year": 2014}, {"title": "A GPU-based method for computing eigenvector centrality of gene-expression networks", "author": ["A.S. Arefin", "R. Berretta", "P. Moscato"], "venue": "Proceedings of the Eleventh Australasian Symposium on Parallel and Distributed Computing-Volume 140. Australian Computer Society, Inc., 2013, pp. 3\u201311.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2013}, {"title": "cudaMap: a GPU accelerated program for gene expression connectivity mapping", "author": ["D.G. McArt", "P. Bankhead", "P.D. Dunne", "M. Salto-Tellez", "P. Hamilton", "S.-D. Zhang"], "venue": "BMC bioinformatics, vol. 14, no. 1, p. 305, 2013.", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2013}, {"title": "Disease gene characterization through largescale co-expression analysis", "author": ["A. Day", "J. Dong", "V.A. Funari", "B. Harry", "S.P. Strom", "D.H. Cohn", "S.F. Nelson"], "venue": "PLoS One, vol. 4, no. 12, p. e8491, 2009.", "citeRegEx": "151", "shortCiteRegEx": null, "year": 2009}, {"title": "Celsius: a community resource for Affymetrix microarray data", "author": ["A. Day", "M.R. Carlson", "J. Dong", "B.D. O\u2019Connor", "S.F. Nelson"], "venue": "Genome Biol, vol. 8, no. 6, p. R112, 2007.", "citeRegEx": "152", "shortCiteRegEx": null, "year": 2007}, {"title": "WGCNA: an R package for weighted correlation network analysis", "author": ["P. Langfelder", "S. Horvath"], "venue": "BMC bioinformatics, vol. 9, no. 1, p. 559, 2008.", "citeRegEx": "153", "shortCiteRegEx": null, "year": 2008}, {"title": "NeMo: network module identification in Cytoscape", "author": ["C.G. Rivera", "R. Vakil", "J.S. Bader"], "venue": "BMC bioinformatics, vol. 11, no. Suppl 1, p. S61, 2010.", "citeRegEx": "154", "shortCiteRegEx": null, "year": 2010}, {"title": "An automated method for finding molecular complexes in large protein interaction networks", "author": ["G.D. Bader", "C.W. Hogue"], "venue": "BMC bioinformatics, vol. 4, no. 1, p. 2, 2003.", "citeRegEx": "155", "shortCiteRegEx": null, "year": 2003}, {"title": "Detecting overlapping protein complexes in protein-protein interaction networks", "author": ["T. Nepusz", "H. Yu", "A. Paccanaro"], "venue": "Nature methods, vol. 9, no. 5, pp. 471\u2013472, 2012.", "citeRegEx": "156", "shortCiteRegEx": null, "year": 2012}, {"title": "PathBLAST: a tool for alignment of protein interaction networks", "author": ["B.P. Kelley", "B. Yuan", "F. Lewitter", "R. Sharan", "B.R. Stockwell", "T. Ideker"], "venue": "Nucleic acids research, vol. 32, no. suppl 2, pp. W83\u2013 W88, 2004.", "citeRegEx": "157", "shortCiteRegEx": null, "year": 2004}, {"title": "BioPig: a Hadoop-based analytic toolkit for large-scale sequence data", "author": ["H. Nordberg", "K. Bhatia", "K. Wang", "Z. Wang"], "venue": "Bioinformatics, vol. 29, no. 23, pp. 3014\u20133019, 2013.", "citeRegEx": "158", "shortCiteRegEx": null, "year": 2013}, {"title": "SeqPig: simple and scalable scripting for large sequencing data sets in Hadoop", "author": ["A. Schumacher", "L. Pireddu", "M. Niemenmaa", "A. Kallio", "E. Korpelainen", "G. Zanetti", "K. Heljanko"], "venue": "Bioinformatics, vol. 30, no. 1, pp. 119\u2013120, 2014.", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2014}, {"title": "Searching for SNPs with cloud computing", "author": ["B. Langmead", "M.C. Schatz", "J. Lin", "M. Pop", "S.L. Salzberg"], "venue": "Genome Biol, vol. 10, no. 11, p. R134, 2009.", "citeRegEx": "160", "shortCiteRegEx": null, "year": 2009}, {"title": "Ultrafast and memory-efficient alignment of short DNA sequences to the human genome", "author": ["B. Langmead", "C. Trapnell", "M. Pop", "S.L. Salzberg"], "venue": "Genome Biol, vol. 10, no. 3, p. R25, 2009.", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2009}, {"title": "SNP detection for massively parallel whole-genome resequencing", "author": ["R. Li", "Y. Li", "X. Fang", "H. Yang", "J. Wang", "K. Kristiansen", "J. Wang"], "venue": "Genome research, vol. 19, no. 6, pp. 1124\u20131132, 2009.", "citeRegEx": "162", "shortCiteRegEx": null, "year": 2009}, {"title": "Stormbow: a cloud-based tool for reads mapping and expression quantification in largescale RNA-Seq studies", "author": ["S. Zhao", "K. Prenger", "L. Smith"], "venue": "International Scholarly Research Notices, vol. 2013, 2013.", "citeRegEx": "163", "shortCiteRegEx": null, "year": 2013}, {"title": "CloVR: a virtual machine for automated and portable sequence analysis from the desktop using cloud computing", "author": ["S.V. Angiuoli", "M. Matalka", "A. Gussman", "K. Galens", "M. Vangala", "D.R. Riley", "C. Arze", "J.R. White", "O. White", "W.F. Fricke"], "venue": "BMC bioinformatics, vol. 12, no. 1, p. 356, 2011.", "citeRegEx": "164", "shortCiteRegEx": null, "year": 2011}, {"title": "Rainbow: a tool for large-scale whole-genome sequencing data analysis using cloud computing", "author": ["S. Zhao", "K. Prenger", "L. Smith", "T. Messina", "H. Fan", "E. Jaeger", "S. Stephens"], "venue": "BMC genomics, vol. 14, no. 1, p. 425, 2013.", "citeRegEx": "165", "shortCiteRegEx": null, "year": 2013}, {"title": "The vmatch large scale sequence analysis software", "author": ["S. Kurtz"], "venue": "Ref Type: Computer Program, pp. 4\u201312, 2003.", "citeRegEx": "166", "shortCiteRegEx": null, "year": 2003}, {"title": "GO-Elite: a flexible solution for pathway and ontology over-representation", "author": ["A.C. Zambon", "S. Gaj", "I. Ho", "K. Hanspers", "K. Vranizan", "C.T. Evelo", "B.R. Conklin", "A.R. Pico", "N. Salomonis"], "venue": "Bioinformatics, vol. 28, no. 16, pp. 2209\u20132210, 2012.", "citeRegEx": "167", "shortCiteRegEx": null, "year": 2012}, {"title": "Presenting and exploring biological pathways with PathVisio", "author": ["M.P. van Iersel", "T. Kelder", "A.R. Pico", "K. Hanspers", "S. Coort", "B.R. Conklin", "C. Evelo"], "venue": "BMC bioinformatics, vol. 9, no. 1, p. 399, 2008.", "citeRegEx": "168", "shortCiteRegEx": null, "year": 2008}, {"title": "Direction pathway analysis of large-scale proteomics data reveals novel features of the insulin action pathway", "author": ["P. Yang", "E. Patrick", "S.-X. Tan", "D.J. Fazakerley", "J. Burchfield", "C. Gribben", "M.J. Prior", "D.E. James", "Y.H. Yang"], "venue": "Bioinformatics, vol. 30, no. 6, pp. 808\u2013814, 2014.", "citeRegEx": "169", "shortCiteRegEx": null, "year": 2014}, {"title": "Pathway Processor: a tool for integrating whole-genome expression results into metabolic networks", "author": ["P. Grosu", "J.P. Townsend", "D.L. Hartl", "D. Cavalieri"], "venue": "Genome research, vol. 12, no. 7, pp. 1121\u20131126, 2002.", "citeRegEx": "170", "shortCiteRegEx": null, "year": 2002}, {"title": "Pathway-PDT: a flexible pathway analysis tool for nuclear families", "author": ["Y.S. Park", "M. Schmidt", "E.R. Martin", "M.A. Pericak-Vance", "R.-H. Chung"], "venue": "BMC bioinformatics, vol. 14, no. 1, p. 267, 2013.", "citeRegEx": "171", "shortCiteRegEx": null, "year": 2013}, {"title": "Pathview: an R/Bioconductor package for pathway-based data integration and visualization", "author": ["W. Luo", "C. Brouwer"], "venue": "Bioinformatics, vol. 29, no. 14, pp. 1830\u20131831, 2013.", "citeRegEx": "172", "shortCiteRegEx": null, "year": 1830}, {"title": "MEGA: a biologistcentric software for evolutionary analysis of DNA and protein sequences", "author": ["S. Kumar", "M. Nei", "J. Dudley", "K. Tamura"], "venue": "Briefings in bioinformatics, vol. 9, no. 4, pp. 299\u2013306, 2008.", "citeRegEx": "173", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Considering the annual growth of data generation, the digital universe - data we generate annually - will reach 44 zettabytes, or 44 trillion gigabytes by the year 2020, which is ten times the size of the digital universe in 2013 [2].", "startOffset": 230, "endOffset": 233}, {"referenceID": 2, "context": "This network has been termed as the Internet of Things (IoT) [3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "2 billion in 2017, up from only 200 million in 2012 [4].", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "[2], approximately all of the target rich data were general IT data in the year 2014; however, by the year 2020, IoT data will occupy more than 20% of the target-rich data lake.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Figure 1 shows the forecast made in the IDC report [2] regarding the size of the digital universe and the target-rich portion of it", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "In the literature, big data has been characterized as either 3Vs or 4Vs [5], [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "In the literature, big data has been characterized as either 3Vs or 4Vs [5], [6].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "For instance, the size of a single sequenced human genome is approximately 200 gigabytes [7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "small molecules in 2014, in comparsion to 18 petabytes in 2013 [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Quantity of data stored by EBI over the years [8]", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "their size (and hence inefficient), cost, privacy, and other ethical issues [9].", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "The best policy is to use cloud for both data store as well as for computation [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": "This solution claims to improve the throughput of genome analytics by orders of magnitude higher than the traditional approaches [10].", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "edu infrastructures, in order to deal with massive amount of sequence data [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 11, "context": "The most notable pathway data sources are KEGG [12], Reactome [13], and Pathway Commons [14].", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "The most notable pathway data sources are KEGG [12], Reactome [13], and Pathway Commons [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "The most notable pathway data sources are KEGG [12], Reactome [13], and Pathway Commons [14].", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "The GO-based tool chain is so huge that there exist tools, such as SerbGO [15], to search the appropriate GO tools for a particular bioinformatics problem.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Other important optimization methods include multi-objective and multi-modal optimization methods, such as pareto optimization [16] and evolutionary algorithms [17], respectively.", "startOffset": 127, "endOffset": 131}, {"referenceID": 16, "context": "Other important optimization methods include multi-objective and multi-modal optimization methods, such as pareto optimization [16] and evolutionary algorithms [17], respectively.", "startOffset": 160, "endOffset": 164}, {"referenceID": 17, "context": "This kind of clustering algorithms include CLARA (Clustering LARge Applications) [18] and BIRCH (Balanced Iterative Reducing using Cluster Hierarchies) [19].", "startOffset": 81, "endOffset": 85}, {"referenceID": 18, "context": "This kind of clustering algorithms include CLARA (Clustering LARge Applications) [18] and BIRCH (Balanced Iterative Reducing using Cluster Hierarchies) [19].", "startOffset": 152, "endOffset": 156}, {"referenceID": 19, "context": "For example, spectral regression discriminant analysis significantly reduces the time and space complexity by simplifying discriminant analysis to a set of regularized least squares problems [20].", "startOffset": 191, "endOffset": 195}, {"referenceID": 20, "context": "[21] reduce the space complexity of non-linear discriminant analysis from O(n) to O(n), to minimize computation and storage problem on large-scale datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Park and Kargupta [22] discuss the distributed algorithms for classifier learning, association rule mining, and clustering.", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "propose a component-based system, designated as PaDDMAS, for developing distributed data mining applications [23].", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "Similar systems for distributed machine learning methods are proposed, such as MLbase [24].", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": "Further, cloud computing infrastructure-based systems are also proposed for performing distributed machine learning, such as the Distributed GraphLab [25] framework that emphasizes on consistency and fault-tolerance in distributed analytics.", "startOffset": 150, "endOffset": 154}, {"referenceID": 25, "context": "When cloud computing infrastructure and distributed processing platforms, such as MapReduce [26], and their open source implementations became widely available, the research on big data analytics", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "The proprietary graph processing architecture developed at Google, known as Pregel [27], addresses dis-", "startOffset": 83, "endOffset": 87}, {"referenceID": 27, "context": "distributed file system technologies, such as HDFS [28] and QFS [29], as well as NoSQL databases for unstructured data, such as MongoDB15 and CouchDB16 have been widely used for big data analytics.", "startOffset": 51, "endOffset": 55}, {"referenceID": 28, "context": "distributed file system technologies, such as HDFS [28] and QFS [29], as well as NoSQL databases for unstructured data, such as MongoDB15 and CouchDB16 have been widely used for big data analytics.", "startOffset": 64, "endOffset": 68}, {"referenceID": 29, "context": "MapReduce is a data-parallel architecture, originally developed by Google [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "Twister [32] optimizes iterative computations on the MapReduce architecture by using in-memory computations, rather than writing onto the distributed memory after each phase.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "Apache Spark19 extends Hadoop by using Resilient Distributed Database (RDD) [33] to allow in-memory processing as well as fault-tolerance by reconstructing a faulty partition in case of node failure.", "startOffset": 76, "endOffset": 80}, {"referenceID": 32, "context": "[34] and later many other big data solutions adopted similar architectures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "The well known Message Passing Interface (MPI) [35] is a good fit for this problem.", "startOffset": 47, "endOffset": 51}, {"referenceID": 34, "context": "Machine learning is a field of computer science that studies the computational methods that learn from data [36].", "startOffset": 108, "endOffset": 112}, {"referenceID": 35, "context": ", supervised and unsupervised learning methods [37].", "startOffset": 47, "endOffset": 51}, {"referenceID": 36, "context": "The traditional machine learning methods are found inadequate in handling voluminous data using the current computational resources [38].", "startOffset": 132, "endOffset": 136}, {"referenceID": 37, "context": "A feature selection method can improve the performance of prediction models by removing irrelevant and redundant features with alleviating the effect of the curse of dimensionality, enhancing the generalization performance, speeding up the learning process, and improving the model interpretability [39].", "startOffset": 299, "endOffset": 303}, {"referenceID": 38, "context": "Feature selection is also considered very important for big data analytics due to its characteristics of semi-infinite programming (SIP) problem [40].", "startOffset": 145, "endOffset": 149}, {"referenceID": 39, "context": "[41] propose an efficient feature selection algorithm works iteratively and selects a subset of features, and solves a sequence of multiple kernel learning (MKL) subproblems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[42] propose a new feature selection method combining Improved harmony search algorithm with rough set theory to tackle the feature selection problem in big data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[43] propose a novel feature selection method with annealing technique for big data learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[44] propose an incremental feature selection method called FRSA-IFS-HIS (AD) using fuzzy-rough set theory on hybrid information systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "Among large number of models reported in the literature linear and nonlinear density-based classifiers, decision trees, naive Bayes, support vector machines (SVMs), neural networks and K-nearest neighbour (KNN) are the most frequently used methods in many applications [45] [46] [47] [48].", "startOffset": 269, "endOffset": 273}, {"referenceID": 44, "context": "Among large number of models reported in the literature linear and nonlinear density-based classifiers, decision trees, naive Bayes, support vector machines (SVMs), neural networks and K-nearest neighbour (KNN) are the most frequently used methods in many applications [45] [46] [47] [48].", "startOffset": 279, "endOffset": 283}, {"referenceID": 45, "context": "In big data analytics, we need some advanced supervised approaches for parallel and distributed learning such as Multi-hyperplane Machine (MM) classification model [49], divide-and-conquer SVM [50], and neural network classifiers.", "startOffset": 164, "endOffset": 168}, {"referenceID": 46, "context": "In big data analytics, we need some advanced supervised approaches for parallel and distributed learning such as Multi-hyperplane Machine (MM) classification model [49], divide-and-conquer SVM [50], and neural network classifiers.", "startOffset": 193, "endOffset": 197}, {"referenceID": 47, "context": "propose a modified SVM called New Primal SVM for big data classification [51].", "startOffset": 73, "endOffset": 77}, {"referenceID": 48, "context": "[52].", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[53] propose a weighted SVM based on mutual information and modified", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[54].", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[55] use", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "propose techniques to distribute and parallelize Gradient Boosted Decision Trees(GBDT) [56].", "startOffset": 87, "endOffset": 91}, {"referenceID": 53, "context": "According to the authors, due to the high communication overhead of HDFS [57], Hadoop is not suitable for this algorithm.", "startOffset": 73, "endOffset": 77}, {"referenceID": 54, "context": "[58] propose fast, scalable and distributable decision tree called rxDTree which can estimate decision trees efficiently on big data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[59] propose a modified decision tree learning that generates rules from a set of decision trees built in parallel on tractable size training dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "Unsupervised learning do not use the class labels of the objects for learning [60].", "startOffset": 78, "endOffset": 82}, {"referenceID": 57, "context": "Clustering is an unsupervised technique that attempts to group objects to optimize the criterion that states that distance among objects in the same cluster is minimized and distance among objects in different clusters is maximized [61].", "startOffset": 232, "endOffset": 236}, {"referenceID": 58, "context": "a dataset is very large, researchers are interested in finding groups of objects that are similar over subset of the available features [62].", "startOffset": 136, "endOffset": 140}, {"referenceID": 59, "context": "is a group of objects that are not only similar over a subset of features, but are also similar across a subset of time points [63].", "startOffset": 127, "endOffset": 131}, {"referenceID": 60, "context": "These clustering methods are mainly classified into partitional clustering, hierarchical clustering, density-based clustering, graph theoretic clustering, soft computing-based clustering, and matrix operation based clustering [64].", "startOffset": 226, "endOffset": 230}, {"referenceID": 61, "context": "Kmeans [65] assign objects to the nearest cluster centroid iteratively until there is no more assignment possible.", "startOffset": 7, "endOffset": 11}, {"referenceID": 62, "context": "Partitioning Around Medoids (PAM) [66] is another partitional clustering method that uses medoids instead of centroids.", "startOffset": 34, "endOffset": 38}, {"referenceID": 62, "context": "CLARA [66] and CLARANS [67] are two popular partitional clustering methods that use sampling for large datasets.", "startOffset": 6, "endOffset": 10}, {"referenceID": 63, "context": "CLARA [66] and CLARANS [67] are two popular partitional clustering methods that use sampling for large datasets.", "startOffset": 23, "endOffset": 27}, {"referenceID": 64, "context": "Hierarchical clustering methods can be classified into agglomerative and divisive methods [68].", "startOffset": 90, "endOffset": 94}, {"referenceID": 18, "context": "BIRCH [19] is a popular agglomerative hierarchical clustering method that constructs clustering feature (CF) tree first, which is operated in a bottom up fashion to extract the clusters.", "startOffset": 6, "endOffset": 10}, {"referenceID": 65, "context": "CURE [69] is another popular hierarchical clustering method that starts with some scattered objects to form clusters.", "startOffset": 5, "endOffset": 9}, {"referenceID": 62, "context": "DIANA [66] is a divisive hierarchical clustering method that splits largest cluster iteratively to find splinter groups.", "startOffset": 6, "endOffset": 10}, {"referenceID": 66, "context": "[70].", "startOffset": 0, "endOffset": 4}, {"referenceID": 67, "context": "DBSCAN [71] is a very popular density-based", "startOffset": 7, "endOffset": 11}, {"referenceID": 68, "context": "DENCLUE [72] is another popular density-based clustering method that uses kernel density function.", "startOffset": 8, "endOffset": 12}, {"referenceID": 69, "context": "Graph theoretic clustering methods use properties and concepts of graph theory [73].", "startOffset": 79, "endOffset": 83}, {"referenceID": 64, "context": "CLIQUE [68] a graph theoretical clustering method tries to locate maximally complete subgraphs in the connectivity graph derived from actual datasets.", "startOffset": 7, "endOffset": 11}, {"referenceID": 70, "context": "Chameleon [74] is another graph theoretic agglomerative hierarchical clustering method that uses k-nearest neighbor graph.", "startOffset": 10, "endOffset": 14}, {"referenceID": 71, "context": "c-means [75] a soft computing-based clustering method, is a crisp method, which allows objects to belong to more than one cluster with the constraint that the sum of membership of an object across all the clusters is equal to one.", "startOffset": 8, "endOffset": 12}, {"referenceID": 72, "context": "SOM [76] is another very popular soft computing based clustering method that projects high dimensional vectors to a two dimensional grid space.", "startOffset": 4, "endOffset": 8}, {"referenceID": 58, "context": "Cheng and Church [62] propose a biclustering method that iteratively deletes and adds objects and features in a greedy manner.", "startOffset": 17, "endOffset": 21}, {"referenceID": 73, "context": "OPSM [77] detects order preserving sub-matrices in data matrix which correspond to clusters.", "startOffset": 5, "endOffset": 9}, {"referenceID": 74, "context": "BIMAX [78] biclustering method binarizes the data matrix and locate submatrices with zero entries in all the cells.", "startOffset": 6, "endOffset": 10}, {"referenceID": 75, "context": "Spectral biclustering [79] uses eigen vectors to detect checkerboard structures in the data matrix that correspond to biclusters.", "startOffset": 22, "endOffset": 26}, {"referenceID": 76, "context": "SAMBA [80] formulates biclustering problem from graph theoretic view point and tries to find heavy subgraphs in a weighted bipartite graph.", "startOffset": 6, "endOffset": 10}, {"referenceID": 77, "context": "FLOC [81] starts with some initial seeds and then iteratively moves rows and columns to improve quality of biclustering with respect to a criterion function.", "startOffset": 5, "endOffset": 9}, {"referenceID": 78, "context": "ISA [82] is another biclustering method that incorporates randomization and finds biclusters with objects that possess constant values or coherently increasing values over the associated feature subset.", "startOffset": 4, "endOffset": 8}, {"referenceID": 79, "context": "To address the needs of biclustering problem effort is still underway [83], [84], [85], [86] In comparison to the number of clustering and biclustering methods, there are not many triclustering methods.", "startOffset": 70, "endOffset": 74}, {"referenceID": 80, "context": "To address the needs of biclustering problem effort is still underway [83], [84], [85], [86] In comparison to the number of clustering and biclustering methods, there are not many triclustering methods.", "startOffset": 76, "endOffset": 80}, {"referenceID": 81, "context": "To address the needs of biclustering problem effort is still underway [83], [84], [85], [86] In comparison to the number of clustering and biclustering methods, there are not many triclustering methods.", "startOffset": 82, "endOffset": 86}, {"referenceID": 82, "context": "To address the needs of biclustering problem effort is still underway [83], [84], [85], [86] In comparison to the number of clustering and biclustering methods, there are not many triclustering methods.", "startOffset": 88, "endOffset": 92}, {"referenceID": 83, "context": "[87] propose a set enumeration-based method to mine triclusters from 3-dimensional datasets using Pearson correlation coefficient.", "startOffset": 0, "endOffset": 4}, {"referenceID": 84, "context": "TRICLUSTER [88] is another well known triclustering method that extracts maximal triclusters from 3-dimensional datasets using a graph theoretic approach.", "startOffset": 11, "endOffset": 15}, {"referenceID": 85, "context": "gTRICLUSTER [89] uses Spearmen correlation coefficient to measure correlation among objects across time points while", "startOffset": 12, "endOffset": 16}, {"referenceID": 59, "context": "ICSM [63] is a triclustering method that operates on possible pairs of time planes and detects some", "startOffset": 5, "endOffset": 9}, {"referenceID": 86, "context": "Two versions of k-means, namely k-mode and k-prototype methods [90] operate on large scale categorical and mixed type data, respectively.", "startOffset": 63, "endOffset": 67}, {"referenceID": 87, "context": "propose a variant of k-means [91] to minimize memory requirement and number of scans over the dataset.", "startOffset": 29, "endOffset": 33}, {"referenceID": 88, "context": "propose a framework [92] to iteratively perform sampling from a large scale dataset and in each iteration, a model is improved to finally produce clusters.", "startOffset": 20, "endOffset": 24}, {"referenceID": 89, "context": "WaveCluster [93] uses wavelet transform to convert spatial domain data to frequency domain using a kernel function.", "startOffset": 12, "endOffset": 16}, {"referenceID": 90, "context": "propose parallel partitional and parallel single likange hierarchical clustering methods [94] on SIMD computers.", "startOffset": 89, "endOffset": 93}, {"referenceID": 91, "context": "propose a parallel k-means clustering method that uses MapReduce architecture [95] to analyze parallel portions of the method.", "startOffset": 78, "endOffset": 82}, {"referenceID": 92, "context": "Similarly, PDBCSCAN [96] finds clusters from data distributed over multiple machines and the results are merged.", "startOffset": 20, "endOffset": 24}, {"referenceID": 93, "context": "P-cluster [97] partitions the objects to minimize the error.", "startOffset": 10, "endOffset": 14}, {"referenceID": 94, "context": "PBIRCH [98] is a parallel version of BIRCH that continuously distributes the incoming data among multiple processors using message passing and shared nothing architecture.", "startOffset": 7, "endOffset": 11}, {"referenceID": 95, "context": "k-means clustering [99] that computes new cluster centers by only using the existing cluster centers and the newly arrived object.", "startOffset": 19, "endOffset": 23}, {"referenceID": 96, "context": "propose an incremental hierarchical clustering [100] that restructures the region of the object hierarchy by inserting the new object through a sequence of restructuring processes.", "startOffset": 47, "endOffset": 52}, {"referenceID": 97, "context": "IGDCA [101] is an incremental density based clustering method that divides data space into units with high density to form clusters.", "startOffset": 6, "endOffset": 11}, {"referenceID": 98, "context": "propose a multi-view clustering method [102] based on DBSCAN clustering method.", "startOffset": 39, "endOffset": 44}, {"referenceID": 99, "context": "propose a framework [103] that performs clustering in different feature spaces separately and then iteratively project and propagate the clustering results in multiple graph-based link layers", "startOffset": 20, "endOffset": 25}, {"referenceID": 100, "context": "propose a multi-view clustering method [104] to project multi-views to lower", "startOffset": 39, "endOffset": 44}, {"referenceID": 101, "context": "propose a multiview version of spectral clustering [105] that computes eigen vectors in different feature spaces and use these eigen vectors to improve graph structures of other view iteratively.", "startOffset": 51, "endOffset": 56}, {"referenceID": 101, "context": "The authors propose another multi-view version of spectral clustering [105] method that uses coregulation-based approaches to find coherent eigen vectors from different graphs.", "startOffset": 70, "endOffset": 75}, {"referenceID": 102, "context": "In recent years, reseachers have developed effective and scalable parallel algorithms for training deep models [106].", "startOffset": 111, "endOffset": 116}, {"referenceID": 103, "context": "To overcome these limitations of traditional neural networks, Deep Belief Networks (DBN) [107] was introduced with a deep learning architecture to learn from both labeled and unlabeled data.", "startOffset": 89, "endOffset": 94}, {"referenceID": 104, "context": "[108] propose a deep learning method by integrating both audio and video data for learning representations.", "startOffset": 0, "endOffset": 5}, {"referenceID": 105, "context": "data [109].", "startOffset": 5, "endOffset": 10}, {"referenceID": 106, "context": "potential drug targets for the target disease [110].", "startOffset": 46, "endOffset": 51}, {"referenceID": 107, "context": "data, the task of GRN inference may be considered as a big data analytics problem [111], [112].", "startOffset": 82, "endOffset": 87}, {"referenceID": 108, "context": "data, the task of GRN inference may be considered as a big data analytics problem [111], [112].", "startOffset": 89, "endOffset": 94}, {"referenceID": 109, "context": "A number of inference methods have been proposed for last several years [113], [114], [115], [116], [117], [118], [119].", "startOffset": 72, "endOffset": 77}, {"referenceID": 110, "context": "A number of inference methods have been proposed for last several years [113], [114], [115], [116], [117], [118], [119].", "startOffset": 79, "endOffset": 84}, {"referenceID": 111, "context": "A number of inference methods have been proposed for last several years [113], [114], [115], [116], [117], [118], [119].", "startOffset": 86, "endOffset": 91}, {"referenceID": 112, "context": "A number of inference methods have been proposed for last several years [113], [114], [115], [116], [117], [118], [119].", "startOffset": 93, "endOffset": 98}, {"referenceID": 113, "context": "A number of inference methods have been proposed for last several years [113], [114], [115], [116], [117], [118], [119].", "startOffset": 100, "endOffset": 105}, {"referenceID": 114, "context": "A number of inference methods have been proposed for last several years [113], [114], [115], [116], [117], [118], [119].", "startOffset": 107, "endOffset": 112}, {"referenceID": 115, "context": "A number of inference methods have been proposed for last several years [113], [114], [115], [116], [117], [118], [119].", "startOffset": 114, "endOffset": 119}, {"referenceID": 116, "context": "The AIS (Agrawal, Imielinski, Swami) [120] and Apriori [121] are two pioneering algorithms for mining association rule Although, they are robust, two major limitations are that they generate too many candidate itemsets and re-", "startOffset": 37, "endOffset": 42}, {"referenceID": 117, "context": "The AIS (Agrawal, Imielinski, Swami) [120] and Apriori [121] are two pioneering algorithms for mining association rule Although, they are robust, two major limitations are that they generate too many candidate itemsets and re-", "startOffset": 55, "endOffset": 60}, {"referenceID": 118, "context": "SETM [122] was motivated by the desire to use SQL to calculate large itemsets, whereas, DHP (Direct Hashing and Pruning) attempts to reduce the number of candidate itemsets [123].", "startOffset": 5, "endOffset": 10}, {"referenceID": 119, "context": "SETM [122] was motivated by the desire to use SQL to calculate large itemsets, whereas, DHP (Direct Hashing and Pruning) attempts to reduce the number of candidate itemsets [123].", "startOffset": 173, "endOffset": 178}, {"referenceID": 120, "context": "The partition approach [124] mines frequent itemsets from large datasets by dividing into smaller partitions, whereas, sampling [125] reduces the number of database scans.", "startOffset": 23, "endOffset": 28}, {"referenceID": 121, "context": "The partition approach [124] mines frequent itemsets from large datasets by dividing into smaller partitions, whereas, sampling [125] reduces the number of database scans.", "startOffset": 128, "endOffset": 133}, {"referenceID": 122, "context": "DIC (Dynamic Itemset Counting) [126] drastically reduces the number of scans of the database during frequent", "startOffset": 31, "endOffset": 36}, {"referenceID": 123, "context": "FP-Growth [127] finds frequent itemsets without candidate generation.", "startOffset": 10, "endOffset": 15}, {"referenceID": 124, "context": "Recently, another effective algorithm called OPAM [128] has been proposed, for finding all the frequent itemsets without generating any candidate sets.", "startOffset": 50, "endOffset": 55}, {"referenceID": 125, "context": "Attribute partitioning approach is the most evident one to deal quantitative attributes [129].", "startOffset": 88, "endOffset": 93}, {"referenceID": 126, "context": "As reported in [130], a possible solution to figure out meaningful quantitative regions for the discovery of association rules is clustering approach.", "startOffset": 15, "endOffset": 20}, {"referenceID": 127, "context": "The Count Distribution [131] is a simple parallelization of Apriori.", "startOffset": 23, "endOffset": 28}, {"referenceID": 128, "context": "PDM [132] is based on DHP [123].", "startOffset": 4, "endOffset": 9}, {"referenceID": 119, "context": "PDM [132] is based on DHP [123].", "startOffset": 26, "endOffset": 31}, {"referenceID": 129, "context": "FDM (Fast Distributed Mining) [133] builds on Count Distribution [131], and the authors propose new techniques to reduce the number of candidates considered for counting and hence minimizes communication.", "startOffset": 30, "endOffset": 35}, {"referenceID": 127, "context": "FDM (Fast Distributed Mining) [133] builds on Count Distribution [131], and the authors propose new techniques to reduce the number of candidates considered for counting and hence minimizes communication.", "startOffset": 65, "endOffset": 70}, {"referenceID": 130, "context": "To address the issues of FDM, a parallel version of FDM, called Fast Parallel Mining (FPM) [134] was introduced.", "startOffset": 91, "endOffset": 96}, {"referenceID": 131, "context": "Recently, several efforts have been made to extend some of the serial rule mining methods to be implemented in MapReduce framework for faster execution [135] and handling for voluminous data.", "startOffset": 152, "endOffset": 157}, {"referenceID": 132, "context": "Fast UPdate (FUP) [136] was proposed to compute large itemsets in a dataset that is updated regularly.", "startOffset": 18, "endOffset": 23}, {"referenceID": 133, "context": "The Borders [137] algorithm is based on the concept of border sets introduced in [138].", "startOffset": 12, "endOffset": 17}, {"referenceID": 134, "context": "The Borders [137] algorithm is based on the concept of border sets introduced in [138].", "startOffset": 81, "endOffset": 86}, {"referenceID": 135, "context": "The Decrement Updating Algorithm [139] tries to detect all the frequent itemsets from dynamically deleted databases.", "startOffset": 33, "endOffset": 38}, {"referenceID": 108, "context": "However, none of them can handle dynamic time-series data [112].", "startOffset": 58, "endOffset": 63}, {"referenceID": 136, "context": "According to the Cisco report [140], the average mobile network connection speed in 2014 was 1,683 kbps, which will reach approximately 4.", "startOffset": 30, "endOffset": 35}, {"referenceID": 137, "context": "Although distributed algorithms have been proposed in the literature [141], [142], [143], they are mostly academic research and lack robust implementation, considering various MapReduce frameworks.", "startOffset": 69, "endOffset": 74}, {"referenceID": 138, "context": "Although distributed algorithms have been proposed in the literature [141], [142], [143], they are mostly academic research and lack robust implementation, considering various MapReduce frameworks.", "startOffset": 76, "endOffset": 81}, {"referenceID": 139, "context": "Although distributed algorithms have been proposed in the literature [141], [142], [143], they are mostly academic research and lack robust implementation, considering various MapReduce frameworks.", "startOffset": 83, "endOffset": 88}, {"referenceID": 140, "context": "Along with specific tools, several cloud-based bioinformatics platforms have also been developed to integrate specific tools and to provide a fast comprehensive solution to multiple problems, such as Galaxy [144] and CloudBLAST [145].", "startOffset": 207, "endOffset": 212}, {"referenceID": 141, "context": "Along with specific tools, several cloud-based bioinformatics platforms have also been developed to integrate specific tools and to provide a fast comprehensive solution to multiple problems, such as Galaxy [144] and CloudBLAST [145].", "startOffset": 228, "endOffset": 233}, {"referenceID": 142, "context": "A quality assurance tool called caCORRECT [146] removes artifactual noises from high throughput microarray data.", "startOffset": 42, "endOffset": 47}, {"referenceID": 143, "context": "A web-based application called omniBiomarker [147] uses knowledge-driven algorithms to find differentially expressed genes for biomarker identification from high throughput gene expression data.", "startOffset": 45, "endOffset": 50}, {"referenceID": 144, "context": "FastGCN [148] tool exploits parallelism with GPU architectures to find", "startOffset": 8, "endOffset": 13}, {"referenceID": 145, "context": "[149] and McArt et al.", "startOffset": 0, "endOffset": 5}, {"referenceID": 146, "context": "[150].", "startOffset": 0, "endOffset": 5}, {"referenceID": 147, "context": "Tool (UGET) [151] performs large scale co-expression analysis to find disease gene associations.", "startOffset": 12, "endOffset": 17}, {"referenceID": 148, "context": "UGET has been found effective when tested on Celsius [152], which is the largest co-normalized microarray dataset of Affymetrix-based gene expression datawarehouse.", "startOffset": 53, "endOffset": 58}, {"referenceID": 149, "context": "WGCNA [153] is a popular R package for performing weighted gene co-expression network analysis and can be used in an R-Hadoop distributed computing system.", "startOffset": 6, "endOffset": 11}, {"referenceID": 150, "context": "Several relatively fast tools have been developed for PPI complex (isolated and overlapping) finding, such as NeMo [154], MCODE [155], and ClusterONE [156], either as a standalone tool or as a Cytoscape plugin.", "startOffset": 115, "endOffset": 120}, {"referenceID": 151, "context": "Several relatively fast tools have been developed for PPI complex (isolated and overlapping) finding, such as NeMo [154], MCODE [155], and ClusterONE [156], either as a standalone tool or as a Cytoscape plugin.", "startOffset": 128, "endOffset": 133}, {"referenceID": 152, "context": "Several relatively fast tools have been developed for PPI complex (isolated and overlapping) finding, such as NeMo [154], MCODE [155], and ClusterONE [156], either as a standalone tool or as a Cytoscape plugin.", "startOffset": 150, "endOffset": 155}, {"referenceID": 153, "context": "Finally, PathBLAST [157] is an important web-based tool for fast alignment of protein interaction networks.", "startOffset": 19, "endOffset": 24}, {"referenceID": 154, "context": "BioPig [158] is a notable hadoop-based tool for sequence analysis that scales automatically with the data size and can be ported directly to many hadoop", "startOffset": 7, "endOffset": 12}, {"referenceID": 155, "context": "SeqPig [159] is another such tool.", "startOffset": 7, "endOffset": 12}, {"referenceID": 156, "context": "The Crossbow [160] tool combines Bowtie [161], an", "startOffset": 13, "endOffset": 18}, {"referenceID": 157, "context": "The Crossbow [160] tool combines Bowtie [161], an", "startOffset": 40, "endOffset": 45}, {"referenceID": 158, "context": "ultrafast and memory efficient short read aligner, and SoapSNP [162], an accurate genotyper, to perform", "startOffset": 63, "endOffset": 68}, {"referenceID": 159, "context": "Other cloudbased tools that have been developed for large scale sequence analysis are Stormbow [163], CloVR [164], and Rainbow [165].", "startOffset": 95, "endOffset": 100}, {"referenceID": 160, "context": "Other cloudbased tools that have been developed for large scale sequence analysis are Stormbow [163], CloVR [164], and Rainbow [165].", "startOffset": 108, "endOffset": 113}, {"referenceID": 161, "context": "Other cloudbased tools that have been developed for large scale sequence analysis are Stormbow [163], CloVR [164], and Rainbow [165].", "startOffset": 127, "endOffset": 132}, {"referenceID": 162, "context": "There exist other programs for large scale sequence analysis that do not use big data technologies, such as Vmatch [166] and SeqMonk23.", "startOffset": 115, "endOffset": 120}, {"referenceID": 163, "context": "5) Tools for pathway analysis To support pathway analysis, a good number of tools have been developed for pathway analysis, such as GO-Elite [167] to describe particular genes or metabolites, PathVisio [168] for analysis and drawing, directPA [169] to perform analysis in a high-dimensional space for identifying pathways, Pathway Processor [170] to analyze expression data", "startOffset": 141, "endOffset": 146}, {"referenceID": 164, "context": "5) Tools for pathway analysis To support pathway analysis, a good number of tools have been developed for pathway analysis, such as GO-Elite [167] to describe particular genes or metabolites, PathVisio [168] for analysis and drawing, directPA [169] to perform analysis in a high-dimensional space for identifying pathways, Pathway Processor [170] to analyze expression data", "startOffset": 202, "endOffset": 207}, {"referenceID": 165, "context": "5) Tools for pathway analysis To support pathway analysis, a good number of tools have been developed for pathway analysis, such as GO-Elite [167] to describe particular genes or metabolites, PathVisio [168] for analysis and drawing, directPA [169] to perform analysis in a high-dimensional space for identifying pathways, Pathway Processor [170] to analyze expression data", "startOffset": 243, "endOffset": 248}, {"referenceID": 166, "context": "5) Tools for pathway analysis To support pathway analysis, a good number of tools have been developed for pathway analysis, such as GO-Elite [167] to describe particular genes or metabolites, PathVisio [168] for analysis and drawing, directPA [169] to perform analysis in a high-dimensional space for identifying pathways, Pathway Processor [170] to analyze expression data", "startOffset": 341, "endOffset": 346}, {"referenceID": 167, "context": "uk/projects/seqmonk regarding metabolic pathways, Pathway-PDT [171] to perform analysis using raw genotypes in general nuclear families, and Pathview [172] for pathway based data integration.", "startOffset": 62, "endOffset": 67}, {"referenceID": 168, "context": "uk/projects/seqmonk regarding metabolic pathways, Pathway-PDT [171] to perform analysis using raw genotypes in general nuclear families, and Pathview [172] for pathway based data integration.", "startOffset": 150, "endOffset": 155}, {"referenceID": 169, "context": "The existing tools for evolution research, such as MEGA [173] and EvoPipes.", "startOffset": 56, "endOffset": 61}, {"referenceID": 154, "context": "oped for sequence analysis using the Hadoop MapReduce platform, such as BioPig [158] and Crossbow [160] in the", "startOffset": 79, "endOffset": 84}, {"referenceID": 156, "context": "oped for sequence analysis using the Hadoop MapReduce platform, such as BioPig [158] and Crossbow [160] in the", "startOffset": 98, "endOffset": 103}], "year": 2015, "abstractText": "Bioinformatics research is characterized by voluminous and incremental datasets and complex data analytics methods. The machine learning methods used in bioinformatics are iterative and parallel. These methods can be scaled to handle big data using the distributed and parallel computing technologies. Usually big data tools perform computation in batch-mode and are not optimized for iterative processing and high data dependency among operations. In the recent years, parallel, incremental, and multi-view machine learning algorithms have been proposed. Similarly, graph-based architectures and in-memory big data tools have been developed to minimize I/O cost and optimize iterative processing. However, there lack standard big data architectures and tools for many important bioinformatics problems, such as fast construction of co-expression and regulatory networks and salient module identification, detection of complexes over growing protein-protein interaction data, fast analysis of massive DNA, RNA, and protein sequence data, and fast querying on incremental and heterogeneous disease networks. This paper addresses the issues and challenges posed by several big data problems in bioinformatics, and gives an overview of the state of the art and the future research opportunities.", "creator": "LaTeX with hyperref package"}}}