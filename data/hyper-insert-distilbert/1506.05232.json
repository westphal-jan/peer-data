{"id": "1506.05232", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2015", "title": "On the Depth of Deep Neural Networks: A Theoretical View", "abstract": "hypothetical deep neural computing networks ( dnn ) have achieved huge practical success in recent years. firstly however, its perceived theoretical systematic properties ( enhancing in particular entropy generalization ability ) possibly are not yet very clear, mainly since yet existing error free bounds for neural research networks cannot be directly used publicly to explain the statistical behaviors of practically adopted dnn models ( which are broadly multi - coding class elements in respectively their nature distributions and may primarily contain narrower convolutional layers ). to tackle ending the systematic challenge, we derive theoretically a new finite margin hazard bound for dnn smoothing in this paper, in which algorithm the expected 0 - 1 error of a dnn model is upper be bounded by excluding its gross empirical margin edge error plus a rademacher finite average posterior based capacity term. this new bound is still very general and is consistent with the empirical behaviors expressed of other dnn models observed in only our prediction experiments. designing according to anticipating the new bound, slowly minimizing the empirical margin slope error reduction can most effectively improve the conditional test performance of dnn. we therefore propose simpler large margin and dnn estimation algorithms, which routinely impose high margin loss penalty rate terms to the exact cross between entropy expected loss of dnn, so as to reduce the margin error during the training sampling process. additional experimental systematic results nevertheless show that the proposed algorithms ultimately can greatly achieve significantly predict smaller predict empirical probability margin hurdle errors, as well as overall better test interval performances than, the generally standard dnn estimation algorithm.", "histories": [["v1", "Wed, 17 Jun 2015 07:51:42 GMT  (78kb)", "http://arxiv.org/abs/1506.05232v1", null], ["v2", "Sat, 28 Nov 2015 14:21:41 GMT  (182kb,D)", "http://arxiv.org/abs/1506.05232v2", "AAAI 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shizhao sun", "wei chen", "liwei wang 0001", "xiaoguang liu", "tie-yan liu"], "accepted": true, "id": "1506.05232"}, "pdf": {"name": "1506.05232.pdf", "metadata": {"source": "CRF", "title": "Large Margin Deep Neural Networks: Theory and Algorithms", "authors": ["Shizhao Sun", "Wei Chen", "Liwei Wang"], "emails": ["sunshizhao@mail.nankai.edu.cn", "wche@microsoft.com", "wanglw@cis.pku.edu.cn", "tie-yan.liu@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n05 23\n2v 1\n[ cs"}, {"heading": "1 Introduction", "text": "Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].\nTo understand why DNN works well and to further improve its performance, extensive researches have been done regarding its theoretical properties, in particular, the generalization ability. For example, in [3, 9, 15], error bounds for neural networks were derived based on Vapnik-Chervonenkis (VC) dimension. In [2, 16], a margin bound was given to fully connected neural networks in the setting of binary classification.1 While these works shed some lights on the theoretical properties of\n1There are also some other works that study the approximation ability of neural networks. For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.\nDNN, they are far from sufficient in helping us deeply understand and improve DNN models, due to the following reasons. First, the number of parameters in many practical DNN models could be very large, sometimes even larger than the size of training data. This makes the VC dimension based generalization bound too loose to use. Second, practically DNN is usually used to perform multiclass classifications, such as the tasks of ImageNet and CIFAR-10, however, most existing bounds for neural networks are regarding binary classification only. Third, in many real tasks, convolutional neural networks (CNN) [19] are widely used and proven to be very effective [7, 18, 24], however, most existing bounds are derived for fully connected neural networks only.\nTo tackle the aforementioned challenge, in this paper, we derive a new margin bound for DNN. In this bound, the expected 0-1 error is upper bounded by the empirical margin error at arbitrary margin coefficient plus a Rademacher Average (RA) based capacity term. Different from previous bounds, our proposed margin bound works for both the binary and multi-class settings, and can cover both fully connected and convolutional neural networks. We have conducted experiments to validate the reasonability of the bound and found that the bound is consistent with the real behaviors of DNN models in practical learning tasks.\nOur margin bound suggests that the test performance of DNN can be improved by minimizing the empirical margin error at the optimal margin coefficient. The minimization of commonly used loss function (i.e., the cross entropy loss) in DNN, however, cannot achieve this goal since it is the surrogate of the 0-1 error (equivalent to a trivial margin error at zero coefficient), but not the non-trivial margin error at non-zero coefficient. Specifically, given a data sample, the cross entropy loss focuses on the minimization of the model output for the true category, while the non-trivial margin error is concerned with whether the gap between the model output for the true category and the maximum output for wrong categories is larger than the given margin coefficient. With this difference in mind, we propose adding a margin penalty term to the cross entropy loss in order to enlarge the margins on the data samples during the training process. To be specific, given a data sample, the margin penalty term punishes the small gap between the model output for the true category and the maximum output for the wrong categories (or that between the model output for the true category and the output for any wrong category). We then minimize the penalized loss function using back propagation. For ease of reference, we call such an algorithm large margin DNN. To the best of our knowledge, this is the first work that explicitly minimizes the empirical margin errors during DNN training. 2\nWe have conducted experiments on two benchmark datasets (MNIST and CFAR-10) to test the performance of the proposed large margin DNN. The experimental results show that large margin DNN can achieve significantly better test performance than standard DNN. In addition, the models trained using large-margin DNN have smaller margin error at most margin coefficients, and thus their performance gains can be well explained by our derived margin bound.\nThe remaining part of this paper is organized as follows. In Section 2, we give the notations used throughout the paper. In Section 3, we give the margin bound, its theoretical proof, and empirical validation. In Section 4 we propose two large margin DNN algorithms and conduct experiments to test their performances. In Section 5, we conclude the paper and discuss some future works."}, {"heading": "2 Preliminaries", "text": "Given a multi-class classification problem, we denote X = Rd as the input space, Y = {1, \u00b7 \u00b7 \u00b7 ,K} as the output space, and P as the joint distribution over X \u00d7 Y . Here d denotes the dimension of the input space, and K denotes the number of categories in the output space. We have a training set S = {(x1, y1), \u00b7 \u00b7 \u00b7 , (xm, ym)}, which is i.i.d. sampled from X \u00d7 Y according to distribution P . The goal is to learn a prediction model f \u2208 F : X \u00d7 Y \u2192 R from the training set, which produces an output vector (f(x, k); k \u2208 Y) for each instance x \u2208 X indicating its likelihood of belonging to category k. Then the final classification is determined by argmaxk\u2208Y f(x, k). The classification accuracy of prediction model f is measured by its expected 0-1 error, i.e.,\nerrP (f) = Pr (x,y)\u223cP I[argmaxk\u2208Y f(x,k) 6=y] = Pr (x,y)\u223cP I[\u03c1(f ;x,y)<0], (1)\n2One related work is [20], which combines the generative deep learning methods (e.g., RBM) with a marginmax posterior. In contrast, our approach aims to enlarge the margin of discriminative deep learning methods like DNN.\nwhere I[\u00b7] is the indicator function and \u03c1(f ;x, y) = f(x, y) \u2212 maxk 6=y f(x, k) is the margin of model f at sample (x, y).\nWe call the 0-1 error on the training set training error and that on the test set test error. Since the expected 0-1 error cannot be obtained due to the unknown distribution P , one usually uses the test error as its proxy when examining the classification accuracy.\nNow, we consider using neural networks to fulfill the multi-class classification task. Suppose there are L layers in a neural network, including L \u2212 1 hidden layers and an output layer. There are nl nodes in layer l. The number of nodes in the output layer is fixed by the classification problem, i.e., nL = K . There are weights associated with the edges between nodes in adjacent layers of the neural network. As a common trick to avoid over fitting, people usually impose a penalty constraint A on the sum of the weights, which is usually implemented by means of weight decay in real training. Mathematically, we denote the function space of multi-layer neural networks with depth L, and weight penalty constraint A as FLA , i.e.,\nFLA = { (x, k) \u2192\nnL\u22121 \u2211\ni=1\nwifi(x); fi \u2208 F L\u22121 A ,\nnL\u22121 \u2211\ni=1\n|wi| \u2264 A,wi \u2208 R } ; (2)\nfor l = 1, \u00b7 \u00b7 \u00b7 , L\u2212 1,\nF lA = { x \u2192 \u03d5 ( \u03c6(f1(x)), \u00b7 \u00b7 \u00b7 , \u03c6(fpl(x)) ) ; f1, \u00b7 \u00b7 \u00b7 , fpl \u2208 F\u0304 l A } , (3)\nF\u0304 lA = { x \u2192\nnl\u22121 \u2211\ni=1\nwifi(x); fi \u2208 F l\u22121 A ,\nnl\u22121 \u2211\ni=1\n|wi| \u2264 A,wi \u2208 R } ; (4)\nand, F0A = { x \u2192 xi; i \u2208 {1, \u00b7 \u00b7 \u00b7 , d} } ; (5)\nwhere wi denotes the weight in the neural network, and the functions \u03d5 and \u03c6 will be explained in the following.\nThe output of the neural networks is a normalized vector produced by the softmax operation. Please note that the above formulation can cover both fully connected layers and convolutional layers:\n(1) If the l-th layer is a convolutional layer, the outputs of the (l \u2212 1)-th layer are mapped to the l-th layer by means of local convolutional filters, activation, and then pooling. That is, in Eqn (4), lots of weights wi equal 0, and nl is determined by nl\u22121 as well as the number and domain size of the convolutional filters. In Eqn (3), pl equals the size of the pooling region in the l-th layer, and function \u03d5 : Rp \u2192 R is called the pooling function. Widely-used pooling functions include the max-pooling max(t1, \u00b7 \u00b7 \u00b7 , tp) and the average-pooling (t1 + \u00b7 \u00b7 \u00b7 + tp)/p. \u03c6 is an increasing function and usually called the activation function. Widely-used activation functions include the standard sigmoid function \u03c6(t) = 11+e\u2212t , the tanh function \u03c6(t) = et\u2212e\u2212t\net+e\u2212t , and the rectifier function \u03c6(t) = max(0, t). Please note that all these activation functions are 1-Lipschitz.\n(2) If the l-th layer is a fully connected layer, the outputs of the (l \u2212 1)-th layer are mapped to the l-th layer by global linear combination and subsequently activation. That is, in Eqn (3) pl = 1 and \u03d5(x) = x.\nGiven a convolutional layer and a fully connected layer, if the numbers of nodes in their previous layer are the same, the average number of weights (and thus the sum of the weights) associated with each node in a convolutional layer will be much smaller than that for a fully connected layer. As a result, to achieve the same strength of weight decay, the penalty constraint A for the convolutional layers should be much smaller than that for the fully connected layers.\nBecause distribution P is unknown and the 0-1 error is non-continuous, a common way of learning the weights in the neural network is to minimize the empirical (surrogate) loss function. A widely used loss function is the cross entropy loss, which is defined as follows,\nC(f ; x, y) = \u2212 K \u2211\nk=1\nzk ln f(x, k), (6)\nwhere zk = 1 if k = y, and zk = 0 otherwise.\nBack-propagation algorithm is usually employed to minimize the loss functions, in which the weights are updated by means of stochastic gradient descent (SGD)."}, {"heading": "3 Margin Bound for Multi-class Deep Neural Networks", "text": "In this section, we present our margin bound for multi-class DNN, followed by its proof and empirical validation."}, {"heading": "3.1 Margin Bound", "text": "Our bound is based on the empirical margin error and the Rademacher Average (RA) of DNN. First of all, we give definitions to the multi-class empirical margin error and RA respectively. Definition 1. Suppose f \u2208 F : X \u00d7 Y \u2192 R is a multi-class prediction model. For \u2200\u03b3 \u2208 (0, 1], the empirical margin error of f at margin coefficient \u03b3 is defined as follows:\nerr \u03b3 S(f) =\n1\nm\nm \u2211\ni=1\nI[\u03c1(f ;x,y)<\u03b3]. (7)\nDefinition 2. [4] Suppose F : X \u2192 R is a model space with a single dimensional output. The Rademacher average (RA) of F is defined as follows:\nRm(F) = Ex,\u03c3 [\nsup f\u2208F\n2\nm\nm \u2211\ni=1\n\u03c3if(xi) ] , (8)\nwhere x = {x1, \u00b7 \u00b7 \u00b7 , xm} \u223c Pmx , and \u03c3 = {\u03c31, \u00b7 \u00b7 \u00b7 , \u03c3m} are i.i.d. sampled with P (\u03c3i = 1) = 1/2, P (\u03c3i = \u22121) = 1/2.\nWith the above definitions, we can obtain the following theorem, which gives an upper bound to the expected 0-1 error of the DNN model. Theorem 1. Suppose input space X = [\u2212M,M ]d, in the neural networks, function \u03c6 is L\u03c6- Lipschitz, function \u03d5 is max-pooling or average-pooling and pl \u2264 p. For \u2200\u03b4 > 0, with probability at least 1\u2212 \u03b4, we have, \u2200f \u2208 FLA ,\nerrP (f) \u2264 inf \u03b3\u2208(0,1]\n{\nerr \u03b3 S(f) + cM\n8K(2K \u2212 1)\n\u03b3\n\u221a\nln d\nm (pL\u03c6A)\nL +\n\u221a\nlog log2(2\u03b3 \u22121)\nm +\n\u221a\nlog(2\u03b4\u22121)\n2m\n}\n,\nwhere c is a constant.\nProof. In order to prove the theorem, we leverage the following margin bound for general multiclass prediction models (see Theorem 11 in [16]),\nerrP (f) \u2264 inf \u03b3\u2208(0,1]\n{\nerr \u03b3 S(f) + cM\n8K(2K \u2212 1)\n\u03b3 Rm(F\nL A) +\n\u221a\nlog log2(2\u03b3 \u22121)\nm +\n\u221a\nlog(2\u03b4\u22121)\n2m\n}\n. (9)\nGiven the above bound, what we need to do is just to derive an upper bound for the RA of DNN.\nAccording to the definition of FLA and RA, we have,\nRm(F L A) = Ex,\u03c3\n[\nsup \u2016w\u20161\u2264A,fj\u2208F L\u22121 A\n2\nm\nm \u2211\ni=1\n\u03c3i\nnL\u22121 \u2211\nj=1\nwjfj(xi) ] = Ex,\u03c3 [\nsup \u2016w\u20161\u2264A,fj\u2208F L\u22121 A\n2\nm\nnL\u22121 \u2211\nj=1\nwj\nm \u2211\ni=1\n\u03c3ifj(xi) ] .\nSupposing w = {w1, \u00b7 \u00b7 \u00b7 , wnL\u22121} and h = { \u2211m i=1 \u03c3if1(xi), \u00b7 \u00b7 \u00b7 , \u2211m\ni=1 \u03c3ifnL\u22121(xi)}, the inner product \u3008w,h\u3009 is maximized when w is at one of the extreme points of the l1 ball, which implies:\nRm(F L A) \u2264 AEx,\u03c3\n[\nsup f\u2208FL\u22121\nA\n2\nm\nm \u2211\ni=1\n\u03c3if(xi) ] = ARm(F L\u22121 A ). (10)\nFor function class FL\u22121A , if the (L \u2212 1)-th layer is a fully connected layer, it is clear that Rm(F L\u22121 A ) \u2264 Rm(\u03c6 \u25e6 F\u0304 L\u22121 A ) holds. If the (L \u2212 1)-th layer is a convolutional layer with maxpooling or average-pooling, we have,\nRm(F L\u22121 A ) = Ex,\u03c3\n[\nsup f1,\u00b7\u00b7\u00b7 ,fpL\u22121\u2208F\u0304 L\u22121 A\n2\nm\nm \u2211\ni=1\n\u03c3i\u03d5 ( \u03c6(f1(xi)), \u00b7 \u00b7 \u00b7 , \u03c6(fPL\u22121(xi)) )]\n(11)\n\u2264 Ex,\u03c3 [\nsup f1,\u00b7\u00b7\u00b7 ,fpL\u22121\u2208F\u0304 L\u22121 A\n2\nm\nm \u2211\ni=1\n\u03c3i\npL\u22121 \u2211\nj=1\n\u03c6(fj(xi)) ]\n(12)\n= Ex,\u03c3 [\nsup f1,\u00b7\u00b7\u00b7 ,fpL\u22121\u2208F\u0304 L\u22121 A\npL\u22121 \u2211\nj=1\n2\nm\nm \u2211\ni=1\n\u03c3i\u03c6(fj(xi)) ] = pL\u22121Rm(\u03c6 \u25e6 F\u0304 L\u22121 A ). (13)\nThe inequality (12) holds due to the fact that most widely used activation functions \u03c6 (e.g., standard sigmoid and rectifier) have non-negative outputs.\nTherefore, for both fully connected layers and convolutional layers, Rm(F L\u22121 A ) \u2264 pL\u22121Rm(\u03c6 \u25e6 F\u0304L\u22121A ) uniformly holds. Further considering the Lipschitz property of \u03c6, we have,\nRm(F L\u22121 A ) \u2264 2pL\u22121L\u03c6Rm(F\u0304 L\u22121 A ). (14)\nBy iteratively using the maximization principle of inner product in (10) and the property of RA in (14), considering pl \u2264 p, we can obtain the following inequality,\nRm(F L A) \u2264 (2pL\u03c6A) L\u22121 Rm(F\u0304 1 A). (15)\nAccording to [4], Rm(F\u03041A) can be bounded by:\nRm(F\u0304 1 A) \u2264 cAM\n\u221a\nln d\nm , (16)\nwhere c is a constant.\nCombining (15) and (16), we can obtain the following upper bound on the RA of DNN,\nRm(F L A) \u2264 cM\n\u221a\nln d\nm (pL\u03c6A)\nL . (17)\nBy substituting this RA bound into Inequality (9), we can prove the theorem.\nAs for Theorem 1, we have the following discussions:\n(1) When the number of training examples approaches infinity (i.e., m \u2192 \u221e), the expected 0-1 error will be bounded sorely by the empirical margin error, since other terms in the bound vanish at the rate of m\u22121/2. Considering that the empirical margin error is an increasing function of \u03b3, the theorem tells us that the expected 0-1 error will be bounded by the empirical margin error at \u03b3 = 0 (i.e., the training error). In other words, if we have sufficiently many training examples, the performance of the learned DNN model on the unseen test data will be no worse than that on the training data.\n(2) When the training set is finite, several factors will influence the value of the margin bound. When the number of training data (i.e., m) increases, or the dimension of the input space (i.e., d) and the size of the output space (i.e., K) decrease, the margin bound will become smaller. On the other hand, when the weight penalty constraint (i.e., A) increases, the RA term will increase and the empirical margin error will decrease since the hypothesis space becomes larger. When the depth of DNN (i.e., L) increases, the RA term will increase, however, it is unclear how the empirical margin error will change (since the space of deeper and thinner nets may not contain the space of shallower and wider nets). To understand the influence of network depth, we have conducted some empirical study whose results will be shown in the next subsection.\n(3) As compared to previous margin bound for neural networks ([2, 16]), our margin bound can not only cover binary classification and fully connected DNN, but also multi-class classification and convolutional neural networks (CNN). For CNN, due to the sparse local connections, the sum of weights for each node (and thus A) will be much smaller than that of a fully-connected DNN. As a result, the generalization of CNN is usually better than that of a fully-connected DNN."}, {"heading": "3.2 Empirical Validation", "text": "In this section, we conduct experiments to study how the weight penalty constraint A and the network depth L influence the test performance of DNN, and to validate the reasonability of our margin bound.\nWe conduct experiments on two datasets, MNIST [19] and CIFAR-10 [17]. The MNIST dataset (for handwritten digit classification) consists of 28\u00d7 28 black and white images, each containing a digit 0 to 9. There are 60k training examples and 10k test examples in this dataset. The CIFAR-10 dataset (for object recognition) consists of 32 \u00d7 32 RGB images, each containing an object, e.g., cat, dog, or ship. There are 50k training examples and 10k test examples in this dataset. For each\ndataset, we divide the 10k test examples into two subsets of equal size, one for validation and the other for testing. In each experiment, we use standard sigmoid activation and train neural networks by mini-batch SGD with momentum and weight decay. All the hyper-parameters are tuned on the validation set. We repeat the training of DNN (with different initializations) for 10 times, and report the average and minimum test error of the 10 learned models.\nFirst, we investigate the influence of the weight constraint A for neural networks with a fixed depth.3 We change the degree of weight constraint A (through changing the weight decay coefficients \u03b1 in the algorithm implementation), and report the empirical margin error and test error in Figure 1. As we can see, on both datasets, by decreasing the strength of weight decay (i.e., smaller \u03b1), which corresponds to increasing A, the empirical margin error decreases (see Figure 1(a) and 1(b)). By jointly considering this observation and the fact that the RA term increases with A (see the discussions on Theorem 1), we can come to the conclusion that when the weight constraint A increases, the expected 0-1 error will first decrease and then increase. This theoretical result is consistent with the test performance of the DNN models observed in our experiments (see Figure 1(c) and 1(d)).\nSecond, we investigate the influence of the network depth L. For this purpose, we train fullyconnected DNN models with different depths and restricted total number of weights. Following the practices in [8, 23], we set the numbers of weights of the DNN model for MNIST and CIFAR10 as 0.64M and 5M respectively. For simplicity and also following many previous works [1, 8, 12, 23], we assume that each hidden layer has the same number of nodes in the experiment.4 The experimental results are shown in Figure 2. From the figures, we can observe that no matter on which dataset, deeper networks have smaller empirical margin errors than shallower networks for most of the margin coefficients (see Figure 2(a) and 2(b)). Further considering that deeper networks tend to have larger capacity, the margin bound in Theorem 1 tells us that when the network depth increases, the expected 0-1 error will first decrease and then increase. This theoretical result is also consistent with the test performances of the DNN models observed in our experiments (see Figure 2(c) and 2(d)). This indicates the reasonability and practical value of our derived margin bound.\n3Specifically, for MNIST, the DNN model has one hidden layer with 800 hidden units; for CIFAR-10, the DNN model has two hidden layers with 1190 hidden units in each hidden layer.\n4Specifically, for MNIST, the DNN models with depth 2, 3, 4, 5 and 6 respectively have 800, 494, 399, 346 and 311 units in each hidden layer when the total number of weights is 0.64M ; for CIFAR-10, the DNN models with depth 2, 3, 4, 5 and 6 respectively have 1650, 1190, 1000, 886, 806 units in each hidden layer when the total number of weights is 5M ."}, {"heading": "4 Large Margin Deep Neural Networks", "text": "Inspired by Theorem 1, we propose refining the existing DNN algorithms by explicitly enlarging the margin (and thus reducing the margin error) during the training process. For ease of reference, we call the new algorithms large margin DNN.\nAs we know, a widely used loss function for multi-class DNN is the cross entropy loss. However, as mentioned in the introduction, the minimization of the cross entropy loss could not minimize the non-trivial margin error (with non-zero margin coefficient). Specifically, given a data sample, the cross entropy loss focuses on maximizing the model output for the true category, while the non-trivial margin error is concerned with whether the gap between the model output for the true category and the maximum output for the wrong categories is larger than the margin coefficient \u03b3. In this sense, the non-trivial margin error can guide the training process in a finer granularity that even if the model has already made a correct prediction in terms of the 0-1 error, its parameters can still be optimized by minimizing the non-trivial margin error.\nOne straightforward way of refining the DNN algorithms is to change its loss function to be the nontrivial margin error. However, as indicated by Theorem 1, what matters is the margin bound at the optimal margin coefficient \u03b3\u2217, which is unfortunately unknown in advance. Therefore, we choose to take a different approach. Specifically, we propose to add a weighted margin penalty term to the cross entropy loss, in order to enlarge the non-trivial margin (or equivalently reduce the margin error) during the training process. By tuning the weight of the penalty term, we can manipulate the non-trivial margin error at different margin coefficient."}, {"heading": "4.1 Algorithm Description", "text": "We propose adding two kinds of margin penalty terms to the original cross entropy loss. The first penalty term is related to margin, and the second one is related to an upper bound of the margin. Specifically, the corresponding new loss functions are defined as follows (for ease of reference, we call them C1 and C2 respectively): for model f , sample x, y,\nC1(f ; x, y) = C(f ; x, y) + \u03bb ( 1\u2212 \u03c1(f ;x, y) )2 , (18)\nC2(f ; x, y) = C(f ; x, y) + \u03bb\nK \u2212 1\n\u2211\nk 6=y\n( 1\u2212 (f(x, y)\u2212 f(x, k)) )2 . (19)\nWe call the algorithms that minimize the above new loss functions large margin DNN algorithms (LMDNN). For ease of reference, we denote the large margin DNN minimizing C1 and C2 as LMDNN-C1 and LMDNN-C2 respectively, and standard DNN algorithms minimizing C as DNN. To train the LMDNN, we also employ the back propagation method."}, {"heading": "4.2 Experimental Results", "text": "We compare the performances of LMDNNs with DNN on both MNIST and CIFAR-10 datasets. We use the well-tuned neural network structures as given in the Caffe [14] tutorial (i.e., LeNet5 for MNIST and AlexNet6 for CIFAR-10), and adopt the same training and fine-tune process for all the algorithms under investigation.\nAs for data pre-processing, we scale the pixel values in MNIST to [0, 1], and subtract the per-pixel mean computed over the training set from each image in CIFAR-10. On both datasets, we do not use data augmentation for simplicity.\nFor the training process, the weights are initialized randomly and updated by mini-batch SGD. We use the model in the last iteration as our final model. For MNIST, we set the batch size as 64, the momentum as 0.9, and the weight decay coefficient as 0.0005. Each neural network is trained for 10k iterations and the learning rate in each iteration T decreases by multiplying the initial learning rate with a factor of (1+0.0001T )\u22120.75. For CIFAR-10, we set the batch size as 100, the momentum as 0.9, and the weight decay coefficient as 0.004. Each neural network is trained for 70k iterations. The learning rate is set to be 10\u22123 for the first 60k iterations, 10\u22124 for the next 5k iterations, and\n5http://caffe.berkeleyvision.org/gathered/examples/mnist.html 6http://caffe.berkeleyvision.org/gathered/examples/cifar10.html\nFigure 3: Empirical margin error of LMDNNs.\nFigure 4: Test error of LMDNNs with different \u03bb.\n10\u22125 for the other 5k iterations. Each model is trained for 10 times (with different initializations), and we report the mean and standard deviation of the test error over the 10 learned models.\nTable 1 shows the mean test performance of DNN, and the best mean test performances of LMDNNs by tuning margin penalty coefficient \u03bb. We can observe that, on both MNIST and CIFAR-10, LMDNNs achieve significant performance gains. In particular, LMDNN-C1 can reduce the test error from 0.899% to 0.734% on MNIST and from 18.399% to 17.598% on CIFAR-10; LMDNNC2 can reduce the test error from 0.899% to 0.736% on MNIST and from 18.399% to 17.728% on CIFAR-10.\nTo further understand the effect of adding the margin penalty terms, we plot the empirical margin errors of both DNN and LMDNNs in Figure 3. We can see that by introducing the margin penalty terms, LMDNNs indeed achieve smaller empirical margin errors (i.e., larger margin) than DNN. Furthermore, the model with smaller empirical margin errors really has better test performances. For example, LMDNN-C1 corresponds to both smaller empirical margin error and better test performance than LMDNN-C2. This is consistent with Theorem 1, and in return indicates the reasonability of the theorem.\nWe also report the mean test error of LMDNNs with different margin penalty coefficient \u03bb (see Figure 4). In the figure, we use dashed line to represent the mean test error of DNN (corresponding to \u03bb = 0). From the figure, we can see that on both MNIST and CIFAR-10, (1) there is a range of \u03bb where LMDNNs outperform the baseline (i.e., DNN); (2) although the best test performance of LMDNN-C2 is not as good as that of LMDNN-C1, the former has a broader range of \u03bb that can outperform the baseline in terms of test error. This indicates the value of using LMDNN-C2: it eases the tuning of hyperparameter\u03bb by avoiding the max operator in the loss function; (3) with increasing \u03bb, the test error of LMDNNs will first decrease, and then increase. As aforementioned, by tuning the penalty coefficient \u03bb, we can manipulate the margin error at different margin coefficients. This result verifies our theorem: there is an optimal margin coefficient and we should target at minimizing its corresponding empirical margin error."}, {"heading": "5 Conclusion and Future Work", "text": "In this work, we have derived a novel margin bound for DNN which is very general and can cover the multi-class setting and convolutional neural networks. Based on the theory, we have proposed two large margin DNN algorithms, which achieve significant performance gains over the standard DNN algorithm. In the future, we plan to study how other factors influence the test performance of DNN, such as node allocations across layers, types of connections, regularization tricks, etc. We will also work on the design of effective algorithms that can further boost the performance of DNN."}], "references": [{"title": "Do deep nets really need to be deep", "author": ["J. Ba", "R. Caruana"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Almost linear vc-dimension bounds for piecewise polynomial networks", "author": ["P.L. Bartlett", "V. Maiorov", "R. Meir"], "venue": "Neural computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["M. Bianchini", "F. Scarselli"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&CP Volume,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Bounding the vapnik-chervonenkis dimension of concept classes parameterized by real numbers", "author": ["P.W. Goldberg", "M.R. Jerrum"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G.E. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Multilayer feedforward networks are universal approximators", "author": ["K. Hornik", "M. Stinchcombe", "H. White"], "venue": "Neural networks,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Polynomial bounds for vc dimension of sigmoidal neural networks", "author": ["M. Karpinski", "A. Macintyre"], "venue": "In Proceedings of the twenty-seventh annual ACM symposium on Theory of computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Annals of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Max-margin deep generative models", "author": ["C. Li", "J. Zhu", "T. Shi", "B. Zhang"], "venue": "arXiv preprint arXiv:1504.06787,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "On the number of linear regions of deep neural networks", "author": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Universal approximation using feedforward neural networks: A survey of some existing methods, and some new results", "author": ["F. Scarselli", "A.C. Tsoi"], "venue": "Neural networks,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "In 12th International Conference on Document Analysis and Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Deep learning via semi-supervised embedding", "author": ["J. Weston", "F. Ratle", "H. Mobahi", "R. Collobert"], "venue": "In Neural Networks: Tricks of the Trade,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}], "referenceMentions": [{"referenceID": 6, "context": "1 Introduction Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].", "startOffset": 194, "endOffset": 213}, {"referenceID": 9, "context": "1 Introduction Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].", "startOffset": 194, "endOffset": 213}, {"referenceID": 10, "context": "1 Introduction Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].", "startOffset": 194, "endOffset": 213}, {"referenceID": 17, "context": "1 Introduction Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].", "startOffset": 194, "endOffset": 213}, {"referenceID": 24, "context": "1 Introduction Deep neural networks (DNN) have achieved great practical success in many machine learning tasks, such as speech recognition, image classification, and natural language processing [7, 10, 11, 18, 25].", "startOffset": 194, "endOffset": 213}, {"referenceID": 2, "context": "For example, in [3, 9, 15], error bounds for neural networks were derived based on Vapnik-Chervonenkis (VC) dimension.", "startOffset": 16, "endOffset": 26}, {"referenceID": 8, "context": "For example, in [3, 9, 15], error bounds for neural networks were derived based on Vapnik-Chervonenkis (VC) dimension.", "startOffset": 16, "endOffset": 26}, {"referenceID": 14, "context": "For example, in [3, 9, 15], error bounds for neural networks were derived based on Vapnik-Chervonenkis (VC) dimension.", "startOffset": 16, "endOffset": 26}, {"referenceID": 1, "context": "In [2, 16], a margin bound was given to fully connected neural networks in the setting of binary classification.", "startOffset": 3, "endOffset": 10}, {"referenceID": 15, "context": "In [2, 16], a margin bound was given to fully connected neural networks in the setting of binary classification.", "startOffset": 3, "endOffset": 10}, {"referenceID": 12, "context": "For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.", "startOffset": 16, "endOffset": 24}, {"referenceID": 21, "context": "For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.", "startOffset": 16, "endOffset": 24}, {"referenceID": 4, "context": "For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.", "startOffset": 152, "endOffset": 162}, {"referenceID": 5, "context": "For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.", "startOffset": 152, "endOffset": 162}, {"referenceID": 20, "context": "For example, in [13, 22] it is shown that neural networks with at least one hidden layer are universal approximators of any continuous function, and in [5, 6, 21], it is demonstrated that deeper neural networks can compute more complex functions than shallower neural networks.", "startOffset": 152, "endOffset": 162}, {"referenceID": 18, "context": "Third, in many real tasks, convolutional neural networks (CNN) [19] are widely used and proven to be very effective [7, 18, 24], however, most existing bounds are derived for fully connected neural networks only.", "startOffset": 63, "endOffset": 67}, {"referenceID": 6, "context": "Third, in many real tasks, convolutional neural networks (CNN) [19] are widely used and proven to be very effective [7, 18, 24], however, most existing bounds are derived for fully connected neural networks only.", "startOffset": 116, "endOffset": 127}, {"referenceID": 17, "context": "Third, in many real tasks, convolutional neural networks (CNN) [19] are widely used and proven to be very effective [7, 18, 24], however, most existing bounds are derived for fully connected neural networks only.", "startOffset": 116, "endOffset": 127}, {"referenceID": 23, "context": "Third, in many real tasks, convolutional neural networks (CNN) [19] are widely used and proven to be very effective [7, 18, 24], however, most existing bounds are derived for fully connected neural networks only.", "startOffset": 116, "endOffset": 127}, {"referenceID": 19, "context": "One related work is [20], which combines the generative deep learning methods (e.", "startOffset": 20, "endOffset": 24}, {"referenceID": 3, "context": "[4] Suppose F : X \u2192 R is a model space with a single dimensional output.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "In order to prove the theorem, we leverage the following margin bound for general multiclass prediction models (see Theorem 11 in [16]), errP (f) \u2264 inf \u03b3\u2208(0,1] {", "startOffset": 130, "endOffset": 134}, {"referenceID": 3, "context": "(15) According to [4], Rm(F\u0304 A) can be bounded by: Rm(F\u0304 1 A) \u2264 cAM \u221a", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "(3) As compared to previous margin bound for neural networks ([2, 16]), our margin bound can not only cover binary classification and fully connected DNN, but also multi-class classification and convolutional neural networks (CNN).", "startOffset": 62, "endOffset": 69}, {"referenceID": 15, "context": "(3) As compared to previous margin bound for neural networks ([2, 16]), our margin bound can not only cover binary classification and fully connected DNN, but also multi-class classification and convolutional neural networks (CNN).", "startOffset": 62, "endOffset": 69}, {"referenceID": 18, "context": "We conduct experiments on two datasets, MNIST [19] and CIFAR-10 [17].", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "We conduct experiments on two datasets, MNIST [19] and CIFAR-10 [17].", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "Following the practices in [8, 23], we set the numbers of weights of the DNN model for MNIST and CIFAR10 as 0.", "startOffset": 27, "endOffset": 34}, {"referenceID": 22, "context": "Following the practices in [8, 23], we set the numbers of weights of the DNN model for MNIST and CIFAR10 as 0.", "startOffset": 27, "endOffset": 34}, {"referenceID": 0, "context": "For simplicity and also following many previous works [1, 8, 12, 23], we assume that each hidden layer has the same number of nodes in the experiment.", "startOffset": 54, "endOffset": 68}, {"referenceID": 7, "context": "For simplicity and also following many previous works [1, 8, 12, 23], we assume that each hidden layer has the same number of nodes in the experiment.", "startOffset": 54, "endOffset": 68}, {"referenceID": 11, "context": "For simplicity and also following many previous works [1, 8, 12, 23], we assume that each hidden layer has the same number of nodes in the experiment.", "startOffset": 54, "endOffset": 68}, {"referenceID": 22, "context": "For simplicity and also following many previous works [1, 8, 12, 23], we assume that each hidden layer has the same number of nodes in the experiment.", "startOffset": 54, "endOffset": 68}, {"referenceID": 13, "context": "We use the well-tuned neural network structures as given in the Caffe [14] tutorial (i.", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "As for data pre-processing, we scale the pixel values in MNIST to [0, 1], and subtract the per-pixel mean computed over the training set from each image in CIFAR-10.", "startOffset": 66, "endOffset": 72}], "year": 2017, "abstractText": "Deep neural networks (DNN) have achieved huge practical success in recent years. However, its theoretical properties (in particular generalization ability) are not yet very clear, since existing error bounds for neural networks cannot be directly used to explain the statistical behaviors of practically adopted DNN models (which are multi-class in their nature and may contain convolutional layers). To tackle the challenge, we derive a new margin bound for DNN in this paper, in which the expected 0-1 error of a DNN model is upper bounded by its empirical margin error plus a Rademacher Average based capacity term. This new bound is very general and is consistent with the empirical behaviors of DNN models observed in our experiments. According to the new bound, minimizing the empirical margin error can effectively improve the test performance of DNN. We therefore propose large margin DNN algorithms, which impose margin penalty terms to the cross entropy loss of DNN, so as to reduce the margin error during the training process. Experimental results show that the proposed algorithms can achieve significantly smaller empirical margin errors, as well as better test performances than the standard DNN algorithm.", "creator": "LaTeX with hyperref package"}}}