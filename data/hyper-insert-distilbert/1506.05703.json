{"id": "1506.05703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2015", "title": "\"The Sum of Its Parts\": Joint Learning of Word and Phrase Representations with Autoencoders", "abstract": "because recently, there undoubtedly has still been a lot result of innovative effort attempting to represent words locally in continuous language vector spaces. those representations have more been popularly shown to capture both semantic components and physical syntactic data information primarily about words. however, distributed speaker representations of phrases remain and a challenge. also we nevertheless introduce a novel model that jointly electronically learns word vector representations and provides their summation. word representations directly are learnt using processing the word count co - occurrence behind statistical information. to confidently embed sequences hundreds of words ( t i. k e. phrases ) with different sizes into a common semantic space, we mainly propose to average four word vector representations. in contrast with previous methods which reported of a posteriori assumption some are compositionality aspects dominated by simple summation, whereupon we simultaneously train words to sum, essentially while keeping the squared maximum consistency information intact from crossing the original vectors. whereas we evaluate the effective quality analysis of the word text representations displayed on several classical word evaluation tasks, and we introduce a theoretically novel task set to evaluate the quality expected of reading the phrase representations. while both our distributed language representations compete with other comparative methods thinking of simultaneously learning word - representations on ordinary word evaluations, we show that even they give you better performance on the matrix phrase evaluation. such sparse representations of paired phrases could be interesting for designing many tasks in future natural language processing.", "histories": [["v1", "Thu, 18 Jun 2015 14:46:44 GMT  (149kb,D)", "http://arxiv.org/abs/1506.05703v1", "Deep Learning Workshop, ICML 2015"]], "COMMENTS": "Deep Learning Workshop, ICML 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\\'emi lebret", "ronan collobert"], "accepted": false, "id": "1506.05703"}, "pdf": {"name": "1506.05703.pdf", "metadata": {"source": "CRF", "title": "\u201cThe Sum of Its Parts\u201d: Joint Learning of Word and Phrase Representations with Autoencoders", "authors": ["R\u00e9mi Lebret", "Ronan Collobert"], "emails": ["REMI@LEBRET.CH", "RONAN@COLLOBERT.COM"], "sections": [{"heading": null, "text": "Recently, there has been a lot of effort to represent words in continuous vector spaces. Those representations have been shown to capture both semantic and syntactic information about words. However, distributed representations of phrases remain a challenge. We introduce a novel model that jointly learns word vector representations and their summation. Word representations are learnt using the word co-occurrence statistical information. To embed sequences of words (i.e. phrases) with different sizes into a common semantic space, we propose to average word vector representations. In contrast with previous methods which reported a posteriori some compositionality aspects by simple summation, we simultaneously train words to sum, while keeping the maximum information from the original vectors. We evaluate the quality of the word representations on several classical word evaluation tasks, and we introduce a novel task to evaluate the quality of the phrase representations. While our distributed representations compete with other methods of learning word representations on word evaluations, we show that they give better performance on the phrase evaluation. Such representations of phrases could be interesting for many tasks in natural language processing.\n1All research was conducted at Idiap Research Institute, before Ronan Collobert joined Facebook AI Research."}, {"heading": "1. Introduction", "text": "Human language \u201cmakes infinite use of finite means\u201d (Humboldt, 1836). A large number of sentences can be generated from a finite set of words. Thus there has been a lot of effort to capture the meaning of words. Some approaches are based on distributional word representations (Lund & Burgess, 1996; Patel et al., 1998), others are based on distributed representations (Bengio, 2008; Collobert et al., 2011; Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013b; Lebret & Collobert, 2014; Pennington et al., 2014; Levy & Goldberg, 2014) where the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words. Finally, distributed representations emerged as the solution to many natural language processing (NLP) tasks (Turney & Pantel, 2010; Collobert et al., 2011).\nGiven these representations of words in a vector space, techniques for combining them have been proposed to get representations of phrases or sentences. These compositional models involve vector addition or multiplication (Mitchell & Lapata, 2010). Such simple compositions have shown to perform competitively on the paraphrase detection and phrase similarity tasks (Blacoe & Lapata, 2012). More sophisticated approaches use techniques from logic, category theory, and quantum information (Clark et al., 2008). Others use the syntactic relations between words to treat certain words as functions and other as arguments such as adjective-noun composition (Baroni & Zamparelli, 2010) or noun-verb composition (Grefenstette et al., 2013). Recursive neural network model for semantic compositionality has also been proposed (Socher et al., 2012), where each word has a matrixvector representation: the vector captures its meaning (as it is initialized with a pre-trained distributed representation), while the matrix learns throught a parse tree how it modifies the meaning of the other word that it combines ar X iv :1\n50 6.\n05 70\n3v 1\n[ cs\n.C L\n] 1\n8 Ju\nwith. Many recent works are based on distributed representations of phrases to tackle a wide range of application in NLP: machine translation (Bahdanau et al., 2015), constituency parsing (Legrand & Collobert, 2015), sentiment analysis (Socher et al., 2013), or image captioning (Lebret et al., 2015). There is therefore a clear need for distributed word representations that can be easily extrapolated to meaningful phrase representations.\nWe argue that distributed representation and composition must go hand in hand, i.e., they must be mutually learned. We present a model that learns to capture meaning of words in distributed representations using a low-rank approximation of a large word co-occurrence matrix. We choose to stochastically perform this low-rank approximation which enables the model to simultaneously train these representations to compose for producing representations of phrases (see Figure 1). As composition function, we choose a simple weighted addition for its simplicity and for enabling sequences of words with different lengths to be representated in a common vector space. Aside from generating distributed representations of words and phrases, this model gives an encoding function (represented by a matrix) which can be used to encode new words or even phrases based on their co-occurrence counts. This offers two different alternatives for phrase representations: (1) representation for a query phrase can be inferred by averaging vector representations of its words (only if they all were in the training set), or (2) by using its word co-occurrence statistics.\nEvaluation on the popular word similarity and analogy tasks demonstrate the capability of our joint model for capturing good distributed representations. We then introduce a novel task for evaluating phrase representations. Given a phrase representation, the objective is to retrieve the words that compose the phrase. We compare our model against other state-of-the-art methods for distributed word representations which capture meaningful linear substructures (Mikolov et al., 2013a; Pennington et al., 2014). We show that our model achieves similar performance on word evaluation tasks, but that it outperforms other methods on the phrase evaluation task."}, {"heading": "2. Related Works", "text": "In the literature, two major model families exist for learning distributed word representations: the count-based methods and the predictive-based methods.\nThe count-based methods consist of using the statistical information contained in large corpora of unlabeled text to build large matrices by simply counting words (word cocoocurrence statistics). The rows correspond to words or terms, and the columns correspond to a local context. The context can be documents, such as in latent semantic anal-\nysis (LSA) (Deerwester et al., 1990); or other words (Lund & Burgess, 1996). To generate low-dimensional word representations, a low-rank approximation of these large matrices is performed, mainly with a singular value decomposition (SVD). Many authors proposed to improve this model with different transformations for the matrix of counts, such as positive pointwise mutual information (PPMI) (Bullinaria & Levy, 2007; Levy & Goldberg, 2014), or a square root of the co-occurrence probabilities in the form of Hellinger PCA (Lebret & Collobert, 2014). Instead of using the co-occurrence probabilities, (Pennington et al., 2014) suggest that word vector representations should be learnt with ratios of co-occurrence probabilities. For this purpose, they introduce a log-bilinear regression model that combines both global matrix factorization and local context window methods.\nThe predictive-based model has first been introduced as a neural probabilistic language model (Bengio et al., 2003). A neural network architecture is trained to predict the next word given a window of preceding words, where words are representated by low-dimensional vector. Since, some variations of this architecture have been proposed. (Collobert et al., 2011) train a language model to discriminate a two-class classification task: if the word in the middle of the input window is related to its context or not. More recently, the need of full neural architectures has been questioned (Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013a). Mikolov et al. (2013a) propose two predictivebased log-linear models for learning distributed representations of words: (i) the continous bag-of-words model (CBOW), where the objective is to correctly classify the current (middle) word given a symmetric window of context words around it; (ii) the skip-gram model, where instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. In Mikolov et al. (2013b), the authors also introduce a data-driven approach for learning phrases, where the phrases are treated as individual tokens during the training.\nIn this paper, we leverage both families: (i) we use the statistical information for learning distributed word representations by approximating the Hellinger PCA with an autoencoder network; (ii) we jointly learn to predict the words that compose a given phrase."}, {"heading": "3. A Joint Model", "text": "Some prior works have designed models to learn word representations (Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013b; Lebret & Collobert, 2014), while others have proposed models to compose these word representations (Mitchell & Lapata, 2010; Socher et al., 2012). We propose instead to jointly learn word representations and\ntheir composition by simple summation."}, {"heading": "3.1. Learning Word Representations w.r.t. the Hellinger Distance", "text": "As words occurring in similar contexts tend to have similar meanings (Harris, 1954), word co-occurrence statistics are generally used to embed similar words into a common vector space (Turney & Pantel, 2010). Common approaches calculate the frequencies, apply some transformations (tfidf, PPMI), reduce the dimensionality, and calculate the similarities. More recently, Lebret & Collobert (2014) proposed a novel method based on a Hellinger PCA of the word co-occurrence matrix. They showed that word representations can be learnt even with a reasonable number of context words. Inspired by this work, we propose to stochastically perform this low-rank approximation. For this purpose, we use an autoencoder with only linear activations to find an optimal solution related to the Hellinger PCA (Bourlard & Kamp, 1988). Replacing the PCA by an autoencoder allows us to learn jointly a cost function which constrains the word information to be kept by summation."}, {"heading": "3.1.1. WORD CO-OCCURRENCE PROBABILITIES", "text": "\u201cYou shall know a word by the company it keeps\u201d (Firth, 1957). Keeping this famous quote in mind, word cooccurrence probabilities are computed by counting the number of times each context word c \u2208 D (where D \u2286 W) occurs around a word w \u2208 W:\np(c|w) = p(c, w) p(w)\n= n(c, w)\u2211\ncj\u2208D n(cj , w) , (1)\nwhere n(c, w) is the number of times a context word c occurs in the surrounding of the word w. A multinomial distribution of |D| classes (words) is thus obtained for each word w:\nPw = {p(c1|w), . . . , p(c|D||w)} . (2)"}, {"heading": "3.1.2. HELLINGER DISTANCE", "text": "Similarities between words can be derived by computing a distance between their corresponding word distributions. Several distances (or metrics) over discrete distributions exist, such as the Bhattacharyya distance, the Hellinger distance or Kullback-Leibler divergence. We chose here the Hellinger distance for its simplicity and symmetry property (as it is a true distance). Considering two discrete probability distributions P = (p1, . . . , pk) and Q = (q1, . . . , qk), the Hellinger distance is formally defined as:\nH(P,Q) = 1\u221a 2 \u221a\u221a\u221a\u221a k\u2211 i=1 ( \u221a pi \u2212 \u221a qi)2 , (3)\nwhich is directly related to the Euclidean norm of the difference of the square root vectors:\nH(P,Q) = 1\u221a 2 \u2016 \u221a P \u2212\n\u221a Q\u20162 . (4)\nNote that it makes more sense to take the Hellinger distance rather than the Euclidean distance for comparing discrete distributions, as P and Q are unit vectors according to the Hellinger distance ( \u221a P and \u221a Q are units vector according to the `2 norm)."}, {"heading": "3.1.3. AUTOENCODER", "text": "An autoencoder is employed to represent words in a lower dimensional space. It takes a distribution \u221a Pw as input, encodes it in a more compact representation, and is trained to reconstruct its own input from that representation:\n||g(f( \u221a Pw))\u2212 \u221a Pw||2 , (5)\nwhere g(f( \u221a Pw)) is the output of the network, f is the encoding function which maps distributions in a mdimension (with m << |D|), and g is the decoding function. f( \u221a Pw) is a distributed representation that captures the main factors of variation in the data as the Hellinger PCA does (Bourlard & Kamp, 1988). Here, encoder f \u2208 Rm\u00d7|D| and decoder g \u2208 R|D|\u00d7m are both linear layers."}, {"heading": "3.2. Learning to Sum Word Representations", "text": "Interesting compositionality properties have been observed from models based on the addition of representations (Mikolov et al., 2013b). An exhaustive comparison of different composition functions has indeed revealed that an additive model performs well on pre-trained word representations (Mitchell & Lapata, 2010). Because our word representations are learnt from linear operations, the inherent structure of these representations is linear. To combine a sequence of words into a common vector space, we then simply apply an element-wise addition of their vector representations. This approach makes sense and works well when the meaning of a text is literally \u201cthe sum of its parts\u201d. This is usually the case with noun and verb phrase chunks. For example, into phrases such as \u201cthe red cat\u201d or \u201cstruggle to deal\u201d, each word independently has its proper meaning. Distributed representations for such phrase chunks must retain information from the individual words. An objective function is thus defined to learn how to combine the word vector representations, while keeping the maximum information from the original vectors. An operation as simple as a weighted sum will probably fail for sequences where individual words act as operators that modify the meaning of another word, or for multiword expressions. Other more complex functions could be chosen to also include such cases, but we choose to propose a much simpler model (i.e.,\naveraging the word representations) to get phrase chunk representations with unsupervised learning. In this paper, we therefore focus on noun and verb phrase chunks."}, {"heading": "3.2.1. ADDITIVE MODEL", "text": "We define s = (w1, . . . , wT ) \u2208 S a phrase chunk of T words, with S a set of phrase chunks. By feeding all \u221a Pw into the autoencoder, a representation xw \u2208 Rm of each word w \u2208 D is obtained:\nxw = f( \u221a Pw) . (6)\nBy an element-wise addition, a representation of the phrase chunk s can be calculated as:\nxs = 1\nT \u2211 wt\u2208s xwt . (7)"}, {"heading": "3.2.2. TRAINING", "text": "In predictive-based model, such as the Skip-gram model, the objective is to maximize the likelihood of a word based on other words in the same sequence. Instead, our training is slightly different in the sense that we aim at discriminating whether words are in the phrase chunk or not. An objective function is thus defined to encourage words wt which appear in the chunk s to give high scores when calculating the dot product between xwt and xs. On the other hand, these scores must be low for words wi /\u2208 s that do not appear in the chunk. We train this problem with a rankingtype cost:\u2211\ns\u2208S \u2211 wt\u2208s \u2211 wi\u2208W wi 6\u2208s max(0, 1\u2212 xs \u00b7 xwt + xs \u00b7 xwi) . (8)\nNote that due to the large size of W , a negative sampling approach can be used to speed up the training. In Equation 8, the whole dictionary W is thus replaced by a subset W\u2212 \u2286 W with N randomly chosen negative samples wi 6\u2208 s. A new setW\u2212 is randomly picked at each iteration during the training."}, {"heading": "3.3. Joint Learning", "text": "In contrast with other methods which have subsequently found nice compositionality properties by simple summation, the novelty of our method is the explicit learning of word representations suitable for summation. The system is then designed to force words with similar context to be close in a m-dimensional space, while these dimensions are learnt to be combined with other related words. This joint learning is illustrated in Figure 1. The whole system is trained by minimizing both objective functions (5) and (8) over the training data using stochastic gradient descent."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Datasets", "text": ""}, {"heading": "4.1.1. BUILDING WORD REPRESENTATION OVER LARGE CORPORA", "text": "Our English corpus is composed of the entire English Wikipedia1 (where all MediaWiki markups have been removed). We consider lower case words to limit the number of words in the dictionary. Additionally, all occurrences of sequences of numbers within a word are replaced with the string \u201cNUMBER\u201d. The resulting text is tokenized using the Stanford tokenizer2. The data set contains about 1.6\n1Available at http://download.wikimedia.org. We took the January 2014 version.\n2Available at http://nlp.stanford.edu/ software/tokenizer.shtml\nbillion words. As dictionaryW , we consider all the words within our corpus which appear at least one hundred times. This results in a 191,268 words dictionary. Only the 10,000 most frequent words within this dictionary were used as context words D to calculate the word co-occurrence probabilities. A symmetric context window of ten words around each word w \u2208 W is used to obtain the multinomial distribution Pw. We chose to encode words in a 100-dimensional vector."}, {"heading": "4.1.2. SUMMING WORDS FOR PHRASE REPRESENTATION", "text": "To learn the summation of words that appear frequently together, we choose to consider only the noun and verb phrase chunks to build S. We extract these chunks with a phrase chunking approach by using the SENNA software3. By retaining only the phrase chunks appearing at least ten times, this results in 1,823,259 noun phrase chunks and 255,232 verb phrase chunks, for a total of 2,078,491 phrase chunks. We divided this set of phrases into three sets: 1,000 phrases for validation, 5,000 phrases for testing, and the rest for training (2,072,491 phrases). An unsupervised framework requires a large amount of data. Because our primary focus is to provide good word representations, validation and testing sets are intentionally kept small to retain as much phrases as possible in the training set."}, {"heading": "4.2. Other Methods", "text": "We compare our distributed representations with other available models for computing vector representations of words: (1) the GloVe model which is also based on cooccurrence statistics of corpora (Pennington et al., 2014)4, (2) the continuous bag-of-words (CBOW) and the skipgram (SG) architectures which learn representations from prediction-based models (Mikolov et al., 2013b)5. The same corpus and dictionary W as the ones described in Section 4.1.1 are used to train 100-dimensional word vector representations. We use a symmetric context window of ten words, and the default values set by the authors for the other hyperparameters. To see the improvement compared to a standalone SVD, we generate word representations with a truncated SVD of the matrix X , where each row of X is a distribution\n\u221a Pw, X =(\u221a Pw1 , \u221a Pw2 , . . . , \u221a Pw|W| )T \u2208 R|W|\u00d7|D|.\n3Available at http://ml.nec-labs.com/senna/ 4Code available at http://www-nlp.stanford.edu/ software/glove.tar.gz. 5Code available at http://word2vec.googlecode. com/svn/trunk/."}, {"heading": "4.3. Evaluating Word Representations", "text": "The first objective of the model is to learn distributed representations which capture both syntactic and semantic informations about words. To evaluate the quality of these representations, we used both analogy and similarity tasks."}, {"heading": "4.3.1. WORD ANALOGIES", "text": "The word analogy task consists of questions like, \u201ca is to b as c is to ?\u201d. It was introduced in Mikolov et al. (2013a) and contains 19,544 such questions, divided into a semantic subset and a syntactic subset. The 8,869 semantic questions are analogies about places, like \u201cBern is to Switzerland as Paris is to ?\u201d, or family relationship, like \u201cuncle is to aunt as boy is to ?\u201d. The 10,675 syntactic questions are grammatical analogies, involving plural and adjectives forms, superlatives, verb tenses, etc. To correctly answer the question, the model should uniquely identify the missing term, with only an exact correspondence counted as a correct match."}, {"heading": "4.3.2. WORD SIMILARITIES", "text": "We also evaluate our model on a variety of word similarity tasks. These include the WordSimilarity-353 Test Collection (WS-353) (Finkelstein et al., 2001), the Rubenstein and Goodenough dataset (RG-65) (Rubenstein & Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al., 2013). They all contain sets of English word pairs along with human-assigned similarity judgements. WS353 and RG-65 datasets contain 353 and 65 word pairs respectively. Those are relatively common word pairs, like computer:internet or football:tennis. The RW dataset differs from these two datasets, since it contains 2,034 pairs where one of the word is rare or morphologically complex, such as brigadier:general or cognizance:knowing."}, {"heading": "4.3.3. RESULTS", "text": "Results reported in Table 1 show that our model gives similar results than other state-of-the-art methods on word similarity tasks. However, there is a significant performance boost between the low-rank approximation of X with a SVD and this same approximation with our joint model. This shows that combining a count-based model with a predictive-based approach helps for generating better word representations. Performance on word analogy tasks show that our joint model competes with others on the syntactic questions, but that it gives a lower accuracy on semantic questions. One possible explanation is that less common words are involved in semantic questions compared to syntactic questions. Among the four words that make a semantic question, one of them is, in average, the 34328th most frequent word in W , while it is the 20819th for a syntactic question. Compared to other methods which take the whole dictionaryW as context dictionary, we consider only a small subset of it (D contains only the 10000 most frequent words of W). A larger context dictionary would certainly help to improve performance on this task6."}, {"heading": "4.4. Evaluating Phrase Representations", "text": "As a second objective, we aim at learning to sum word representations to generate phrase representations while keeping the original information coming from the words. We thus introduce a novel task to evaluate the phrase representations."}, {"heading": "4.4.1. DESCRIPTION OF THE TASK", "text": "As dataset, we use the collection of test phrases described in Section 4.1.2. It contains 5000 phrases (noun phrases and verb phrases) extracted from Wikipedia with a chunking approach. Amoung them, 2244, 2030 and 547 are, respectively, composed of two, three and four words. The remaining 179 are composed of at least five words with a maximum of eight words. For a given phrase s = (w1, . . . , wT ) \u2208 S of T words, the objective is to retrieve the T words from its distributed representation xs. Scores between the phrase s and all the possible words wi \u2208 W are calculated using the dot product between their distributed representations xs \u00b7 xwi , as illustrated in Figure 1. The top T scores are considered as the words composing the phrase s."}, {"heading": "4.4.2. RESULTS", "text": "To evaluate whether words making a given phrase can be retrieved from the distributed phrase representation, we use Recall @K, which measures the fraction of times a correct word was found among the top K results. K is proportional to the number of words per phrase, e.g. for a 3-word phrase\n6It has not been explored due to limitations in hardware resources. It would be easily computable with a cluster of CPU.\nwith a Recall@5, the correct words are found amoung the top 15 results. Higher Recall @K means better retrieval performance. Since we care most about the top-ranked retrieved results, the Recall @K with small K are more important.\nResults reported in Table 2 show that our distributed word representations can be averaged together to produce meaningful phrase representations, since the words are retrieved with a high recall. Our model significantly outperforms others methods on this task. In Figure 2, a more detailed analysis of results reveals that the GloVe model competes with ours for the 2-word phrases. However GloVe\u2019s representations cannot maintain this performance for longer phrases. It is probably not too surprising as this model is trained using ratios of co-occurrence probabilities for two\ntarget words. Consequently, it well learns linear substructures for pairs of words, which probably also explains its good performance on word analogy tasks. In contrast, our joint model can learn more complex substructures which make possible the aggregation of multiple words within a low-dimensional vector space."}, {"heading": "4.5. Inferring New Phrase Representations", "text": "Representations for new phrases can thus be generated by simply averaging its word representations, assuming that the words are in the dictionary W . Consider that the dictionary Wn tends to grow exponentially with n, it gives a nice framework to produce the huge variety of possible sequence of n words in a timely and efficient manner with low memory consumption, unlike other methods. Relying on word co-occurrence statistics to represent words in vector space also provides a framework to easily generate representations for unseen words. This is another advantage compared to methods focused on learning distributed word representations (such as CBOW, Skip-gram and GloVe models), where the whole system needs to be trained again to learn representations for these new con-\nstituents. To infer a representation for a new word wnew, one only needs to count its context words over a large corpus of text to build the distribution \u221a Pwnew . This nice feature can be extrapolated to phrases, which gives another alternative for generating phrase representations. Table 3 presents some examples of phrases, where we use both alternatives to compute their distributed representations. It can be seen that both alternatives give distinct representations. For instance, by using the encoding function f , our model infers a representation for the entity Chicago Bulls which is close to other NBA teams, like the Denver Nuggets or the Seattle Supersonics. By averaging the representations of both words Chicago and Bulls, our model infers a representation which is close to other Chicago\u2019s sport teams. Both representations are meaningful, but they carry different information. Relying on co-occurrence statistics gives entities that occur in a similar context, while the summation tries to find entities containing the maximum amount of similar information. This also works with longer phrases, such as President of the United States. The first alternative gives men who served as president, when the second gives related positions."}, {"heading": "5. Conclusion", "text": "We introduce a model that combines both count-based methods and predictive-based methods for generating distributed representations of words and phrases. Using a chunking approach, a collection of noun phrases and verb phrases is extracted from Wikipedia. For a given n-word phrase, we train our model to generate a low-dimensional representation for each word based on its co-occurrence probability distribution. These n representations are averaged together to generate a distributed phrase representation in the same semantic space. Thanks to an autoencoder approach, we can simultaneously train the model to retrieve the original n words from the phrase representation, and therefore learn complex linear substructures. When compared to state-of-the-art methods on some classical word evaluation tasks, the competitive results show that our joint model produces meaningful word representations. Performance on a novel task for evaluating phrase representations confirm the ability of our model to learn complex substructures, which make possible the aggregation of multiple words within a low-dimensional vector space. Better still, inference of new phrase representations is also easily feasible when relying on counts. Some quantitative examples demonstrate that both alternatives can give different but meaningful information about phrases. The word representations and the collection of phrases used in these experiments are available online, here: http: //www.lebret.ch/words/."}, {"heading": "Acknowledgements", "text": "This work was supported by the HASLER foundation through the grant \u201cInformation and Communication Technology for a Better World 2020\u201d (SmartWorld)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Proceedings of the 3th International Conference on Learning Representations (ICLR),", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "In Proceedings of the EMNLP, pp", "citeRegEx": "Baroni and Zamparelli,? \\Q2010\\E", "shortCiteRegEx": "Baroni and Zamparelli", "year": 2010}, {"title": "Neural net language models. Scholarpedia", "author": ["Y. Bengio"], "venue": null, "citeRegEx": "Bengio,? \\Q2008\\E", "shortCiteRegEx": "Bengio", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A Comparison of Vectorbased Representations for Semantic Composition", "author": ["W. Blacoe", "M. Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Blacoe and Lapata,? \\Q2012\\E", "shortCiteRegEx": "Blacoe and Lapata", "year": 2012}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological cybernetics,", "citeRegEx": "Bourlard and Kamp,? \\Q1988\\E", "shortCiteRegEx": "Bourlard and Kamp", "year": 1988}, {"title": "Extracting semantic representations from word co-occurrence statistics: A computational study", "author": ["J.A. Bullinaria", "J.P. Levy"], "venue": "Behavior Research Methods,", "citeRegEx": "Bullinaria and Levy,? \\Q2007\\E", "shortCiteRegEx": "Bullinaria and Levy", "year": 2007}, {"title": "A compositional distributional model of meaning", "author": ["S. Clark", "B. Coecke", "M. Sadrzadeh"], "venue": "In Proceedings of the Second Quantum Interaction Symposium", "citeRegEx": "Clark et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2008}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "A Synopsis of Linguistic Theory 1930-55", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "Firth,? \\Q1957\\E", "shortCiteRegEx": "Firth", "year": 1957}, {"title": "Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) \u2013 Long Papers, chapter Multi-Step Regression Learning for Compositional Distributional Semantics", "author": ["E. Grefenstette", "G. Dinu", "Y. Zhang", "M. Sadrzadeh", "M. Baroni"], "venue": null, "citeRegEx": "Grefenstette et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "Phrasebased image captioning", "author": ["R. Lebret", "P.H.O. Pinheiro", "R. Collobert"], "venue": "In Proceedings of the 32st International Conference on Machine Learning (ICML),", "citeRegEx": "Lebret et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lebret et al\\.", "year": 2015}, {"title": "Joint rnn-based greedy parsing and word composition", "author": ["J. Legrand", "R. Collobert"], "venue": "In Proceedings of the 3th International Conference on Learning Representations (ICLR),", "citeRegEx": "Legrand and Collobert,? \\Q2015\\E", "shortCiteRegEx": "Legrand and Collobert", "year": 2015}, {"title": "Neural Word Embedding as Implicit Matrix Factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Producing high-dimensional semantic spaces from lexical co-occurrence", "author": ["K. Lund", "C. Burgess"], "venue": "Behavior Research Methods, Instruments, & Computers,", "citeRegEx": "Lund and Burgess,? \\Q1996\\E", "shortCiteRegEx": "Lund and Burgess", "year": 1996}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["M. Luong", "R. Socher", "C.D. Manning"], "venue": "In CoNLL,", "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Proceedings of Workshop at International Conference on Learning Representations (ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in Distributional Models of Semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell and Lapata,? \\Q2010\\E", "shortCiteRegEx": "Mitchell and Lapata", "year": 2010}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["A. Mnih", "K. Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih and Kavukcuoglu,? \\Q2013\\E", "shortCiteRegEx": "Mnih and Kavukcuoglu", "year": 2013}, {"title": "Extracting Semantic Representations from Large Text Corpora", "author": ["M. Patel", "J.A. Bullinaria", "J.P. Levy"], "venue": "Neural Computation and Psychology Workshop, London,", "citeRegEx": "Patel et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Patel et al\\.", "year": 1997}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Contextual Correlates of Synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Communications of the ACM,", "citeRegEx": "Rubenstein and Goodenough,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein and Goodenough", "year": 1965}, {"title": "Semantic Compositionality Through Recursive Matrix-Vector Spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "A. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "Ng", "A. Y", "C. Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["P. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": ", 1998), others are based on distributed representations (Bengio, 2008; Collobert et al., 2011; Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013b; Lebret & Collobert, 2014; Pennington et al., 2014; Levy & Goldberg, 2014) where the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words.", "startOffset": 57, "endOffset": 218}, {"referenceID": 8, "context": ", 1998), others are based on distributed representations (Bengio, 2008; Collobert et al., 2011; Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013b; Lebret & Collobert, 2014; Pennington et al., 2014; Levy & Goldberg, 2014) where the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words.", "startOffset": 57, "endOffset": 218}, {"referenceID": 23, "context": ", 1998), others are based on distributed representations (Bengio, 2008; Collobert et al., 2011; Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013b; Lebret & Collobert, 2014; Pennington et al., 2014; Levy & Goldberg, 2014) where the meaning of a word is encoded as a vector computed from co-occurrence statistics of a word and its neighboring words.", "startOffset": 57, "endOffset": 218}, {"referenceID": 8, "context": "Finally, distributed representations emerged as the solution to many natural language processing (NLP) tasks (Turney & Pantel, 2010; Collobert et al., 2011).", "startOffset": 109, "endOffset": 156}, {"referenceID": 7, "context": "More sophisticated approaches use techniques from logic, category theory, and quantum information (Clark et al., 2008).", "startOffset": 98, "endOffset": 118}, {"referenceID": 12, "context": "Others use the syntactic relations between words to treat certain words as functions and other as arguments such as adjective-noun composition (Baroni & Zamparelli, 2010) or noun-verb composition (Grefenstette et al., 2013).", "startOffset": 196, "endOffset": 223}, {"referenceID": 25, "context": "Recursive neural network model for semantic compositionality has also been proposed (Socher et al., 2012), where each word has a matrixvector representation: the vector captures its meaning (as it is initialized with a pre-trained distributed representation), while the matrix learns throught a parse tree how it modifies the meaning of the other word that it combines ar X iv :1 50 6.", "startOffset": 84, "endOffset": 105}, {"referenceID": 0, "context": "Many recent works are based on distributed representations of phrases to tackle a wide range of application in NLP: machine translation (Bahdanau et al., 2015), constituency parsing (Legrand & Collobert, 2015), sentiment analysis (Socher et al.", "startOffset": 136, "endOffset": 159}, {"referenceID": 26, "context": ", 2015), constituency parsing (Legrand & Collobert, 2015), sentiment analysis (Socher et al., 2013), or image captioning (Lebret et al.", "startOffset": 78, "endOffset": 99}, {"referenceID": 13, "context": ", 2013), or image captioning (Lebret et al., 2015).", "startOffset": 29, "endOffset": 50}, {"referenceID": 23, "context": "We compare our model against other state-of-the-art methods for distributed word representations which capture meaningful linear substructures (Mikolov et al., 2013a; Pennington et al., 2014).", "startOffset": 143, "endOffset": 191}, {"referenceID": 9, "context": "The context can be documents, such as in latent semantic analysis (LSA) (Deerwester et al., 1990); or other words (Lund & Burgess, 1996).", "startOffset": 72, "endOffset": 97}, {"referenceID": 23, "context": "Instead of using the co-occurrence probabilities, (Pennington et al., 2014) suggest that word vector representations should be learnt with ratios of co-occurrence probabilities.", "startOffset": 50, "endOffset": 75}, {"referenceID": 3, "context": "The predictive-based model has first been introduced as a neural probabilistic language model (Bengio et al., 2003).", "startOffset": 94, "endOffset": 115}, {"referenceID": 8, "context": "(Collobert et al., 2011) train a language model to discriminate a two-class classification task: if the word in the middle of the input window is related to its context or not.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "The predictive-based model has first been introduced as a neural probabilistic language model (Bengio et al., 2003). A neural network architecture is trained to predict the next word given a window of preceding words, where words are representated by low-dimensional vector. Since, some variations of this architecture have been proposed. (Collobert et al., 2011) train a language model to discriminate a two-class classification task: if the word in the middle of the input window is related to its context or not. More recently, the need of full neural architectures has been questioned (Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013a). Mikolov et al. (2013a) propose two predictivebased log-linear models for learning distributed representations of words: (i) the continous bag-of-words model (CBOW), where the objective is to correctly classify the current (middle) word given a symmetric window of context words around it; (ii) the skip-gram model, where instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence.", "startOffset": 95, "endOffset": 663}, {"referenceID": 2, "context": "The predictive-based model has first been introduced as a neural probabilistic language model (Bengio et al., 2003). A neural network architecture is trained to predict the next word given a window of preceding words, where words are representated by low-dimensional vector. Since, some variations of this architecture have been proposed. (Collobert et al., 2011) train a language model to discriminate a two-class classification task: if the word in the middle of the input window is related to its context or not. More recently, the need of full neural architectures has been questioned (Mnih & Kavukcuoglu, 2013; Mikolov et al., 2013a). Mikolov et al. (2013a) propose two predictivebased log-linear models for learning distributed representations of words: (i) the continous bag-of-words model (CBOW), where the objective is to correctly classify the current (middle) word given a symmetric window of context words around it; (ii) the skip-gram model, where instead of predicting the current word based on the context, it tries to maximize classification of a word based on another word in the same sentence. In Mikolov et al. (2013b), the authors also introduce a data-driven approach for learning phrases, where the phrases are treated as individual tokens during the training.", "startOffset": 95, "endOffset": 1138}, {"referenceID": 25, "context": ", 2013b; Lebret & Collobert, 2014), while others have proposed models to compose these word representations (Mitchell & Lapata, 2010; Socher et al., 2012).", "startOffset": 108, "endOffset": 154}, {"referenceID": 11, "context": "\u201cYou shall know a word by the company it keeps\u201d (Firth, 1957).", "startOffset": 48, "endOffset": 61}, {"referenceID": 23, "context": "We compare our distributed representations with other available models for computing vector representations of words: (1) the GloVe model which is also based on cooccurrence statistics of corpora (Pennington et al., 2014)4, (2) the continuous bag-of-words (CBOW) and the skipgram (SG) architectures which learn representations from prediction-based models (Mikolov et al.", "startOffset": 196, "endOffset": 221}, {"referenceID": 18, "context": "It was introduced in Mikolov et al. (2013a) and contains 19,544 such questions, divided into a semantic subset and a syntactic subset.", "startOffset": 21, "endOffset": 44}, {"referenceID": 10, "context": "These include the WordSimilarity-353 Test Collection (WS-353) (Finkelstein et al., 2001), the Rubenstein and Goodenough dataset (RG-65) (Rubenstein & Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al.", "startOffset": 62, "endOffset": 88}, {"referenceID": 17, "context": ", 2001), the Rubenstein and Goodenough dataset (RG-65) (Rubenstein & Goodenough, 1965), and the Stanford Rare Word (RW) (Luong et al., 2013).", "startOffset": 120, "endOffset": 140}], "year": 2015, "abstractText": "Recently, there has been a lot of effort to represent words in continuous vector spaces. Those representations have been shown to capture both semantic and syntactic information about words. However, distributed representations of phrases remain a challenge. We introduce a novel model that jointly learns word vector representations and their summation. Word representations are learnt using the word co-occurrence statistical information. To embed sequences of words (i.e. phrases) with different sizes into a common semantic space, we propose to average word vector representations. In contrast with previous methods which reported a posteriori some compositionality aspects by simple summation, we simultaneously train words to sum, while keeping the maximum information from the original vectors. We evaluate the quality of the word representations on several classical word evaluation tasks, and we introduce a novel task to evaluate the quality of the phrase representations. While our distributed representations compete with other methods of learning word representations on word evaluations, we show that they give better performance on the phrase evaluation. Such representations of phrases could be interesting for many tasks in natural language processing. All research was conducted at Idiap Research Institute, before Ronan Collobert joined Facebook AI Research.", "creator": "LaTeX with hyperref package"}}}