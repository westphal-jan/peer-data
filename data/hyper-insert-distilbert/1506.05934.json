{"id": "1506.05934", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jun-2015", "title": "Expectation Particle Belief Propagation", "abstract": "informally we propose namely an original particle - parameter based functional implementation of the loopy belief propagation ( lpb ) algorithm solely for pairwise markov symmetric random fields ( without mrf ) on a uniformly continuous state public space. the algorithm constructs weakly adaptively repeated efficient underlying proposal posterior distributions approximating the quantum local beliefs cost at predicting each note of the specific mrf. this scheme is systematically achieved by critically considering proposal distributions in the exponential family tree whose parameters are updated automatically iterately in an expectation and propagation ( ep ) learning framework. the second proposed particle scheme provides continuously consistent entropy estimation of the lbp marginals as the computational number of particles involved increases. we specifically demonstrate yet that notwithstanding it provides generally more universally accurate overall results domestically than the linear particle assisted belief propagation ( with pbp ) algorithm of ihler and mcallester ( 2009 ) at a fraction of utilities the computational cost and output is additionally nevertheless more robust more empirically. addressing the computational complexity representation of our state algorithm at each separate iteration stages is quadratic intensive in recognizing the number sum of particles. we also propose an exponential accelerated implementation version with fewer sub - standard quadratic computational complexity measures which basically still provides consistent partial estimates of the loopy bp marginal distributions and presently performs almost as well as the original error procedure.", "histories": [["v1", "Fri, 19 Jun 2015 09:34:21 GMT  (1512kb,D)", "http://arxiv.org/abs/1506.05934v1", "submitted to NIPS 2015"]], "COMMENTS": "submitted to NIPS 2015", "reviews": [], "SUBJECTS": "stat.CO cs.AI stat.ML", "authors": ["thibaut lienart", "yee whye teh", "arnaud doucet"], "accepted": true, "id": "1506.05934"}, "pdf": {"name": "1506.05934.pdf", "metadata": {"source": "META", "title": "Expectation Particle Belief Propagation", "authors": ["Thibaut Lienart", "Yee Whye Teh", "Arnaud Doucet"], "emails": ["lienart@stats.ox.ac.uk", "teh@stats.ox.ac.uk", "doucet@stats.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Undirected Graphical Models (also known as Markov Random Fields) provide a flexible framework to represent networks of random variables and have been used in a large variety of applications in machine learning, statistics, signal processing and related fields [2]. For many applications such as tracking, sensor networks or imaging [1, 3], it can be beneficial to define MRF on continuous state-spaces.\nGiven a pairwise MRF, we are here interested in computing the marginal distributions at the nodes of the graph. A popular approach to do this is to consider the Loopy Belief Propagation (LBP) algorithm [4, 5, 2]. LBP relies on the transmission of messages between nodes. However when dealing with continuous random variables, computing these messages exactly is generally intractable. In practice, one must select a way to tractably represent these messages and a way to update these representations following the LBP algorithm. The Nonparametric Belief Propagation (NBP) algorithm [6] represents the messages with mixtures of Gaussians while the Particle Belief Propagation (PBP) algorithm [1] uses an importance sampling approach. NBP relies on restrictive integrability conditions and does not offer consistent estimators of the LBP messages. PBP offers a way to circumvent these two issues but the implementation suggested proposes sampling from the estimated beliefs which need not be integrable. Moreover, even when they are integrable, sampling from the estimated beliefs is very expensive computationally. Practically the authors of [1] only sample approximately from them using short MCMC runs, leading to biased estimators.\nIn our method, we consider a sequence of proposal distributions at each node from which one can sample particles at a given iteration of the LBP algorithm. The messages are then computed using importance sampling. The novelty of the approach is to propose a principled and automated way of designing a sequence of proposals in a tractable exponential family using the Expectation Propaga-\nar X\niv :1\n50 6.\n05 93\n4v 1\n[ st\nat .C\nO ]\n1 9\nJu n\ntion framework [7]. The resulting algorithm, which we call Expectation Particle Belief Propagation (EPBP), does not suffer from restrictive integrability conditions and sampling is done exactly which implies that we obtain consistent estimators of the LBP messages. The method is empirically shown to yield better approximations to the LBP beliefs than the implementation suggested in [1], at a much reduced computational cost, and than EP."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Notations", "text": "We consider a pairwise MRF, i.e. a distribution over a set of p random variables indexed by a set V = {1, . . . , p}, which factorises according to an undirected graph G = (V,E) with\np(xV ) \u221d \u220f\nu\u2208V \u03c8u(xu)\n\u220f\n(u,v)\u2208E \u03c8uv(xu, xv). (1)\nThe random variables are assumed to take values on a continuous, possibly unbounded, space X . The positive functions \u03c8u : X 7\u2192 R+ and \u03c8uv : X \u00d7 X 7\u2192 R+ are respectively known as the node and edge potentials. The aim is to approximate the marginals pu(xu) for all u \u2208 V . A popular approach is the LBP algorithm discussed earlier. This algorithm is a fixed point iteration scheme yielding approximations called the beliefs at each node [4, 2]. When the underlying graph is a tree, the resulting beliefs can be shown to be proportional to the exact marginals. This is not the case in the presence of loops in the graph. However, even in these cases, LBP has been shown to provide good approximations in a wide range of situations [8, 5]. The LBP fixed-point iteration can be written as follows at iteration t:\n   mtuv(xv) = \u222b \u03c8uv(xu, xv)\u03c8u(xu) \u220f w\u2208\u0393u\\v mt\u22121wu (xu)dxu Btu(xu) = \u03c8u(xu) \u220f\nw\u2208\u0393u mtwu(xu)\n, (2)\nwhere \u0393u denotes the neighborhood of u i.e., the set of nodes {w | (w, u) \u2208 E}, muv is known as the message from node u to node v and Bu is the belief at node u."}, {"heading": "2.2 Related work", "text": "The crux of any generic implementation of LBP for continuous state spaces is to select a way to represent the messages and design an appropriate method to compute/approximate the message update.\nIn Nonparametric BP (NBP) [6], the messages are represented by mixtures of Gaussians. In theory, computing the product of such messages can be done analytically but in practice this is impractical due to the exponential growth in the number of terms to consider when calculating the product of mixtures. To circumvent this issue, the authors suggest an importance sampling approach targeting the beliefs and fitting mixtures of Gaussians to the resulting weighted particles. The computation of the update (2) is then always done over a constant number of terms.\nA restriction of \u201cvanilla\u201d Nonparametric BP is that the messages must be finitely integrable for the message representation to make sense. This is the case if the following two conditions hold:\nsup xv\n\u222b \u03c8uv(xu, xv)dxu < \u221e, and \u222b \u03c8u(xu)dxu < \u221e. (3)\nThese conditions do however not hold in a number of important cases as acknowledged in [3]. For instance, the potential \u03c8u(xu) is usually proportional to a likelihood of the form p(yu|xu) which needs not be integrable in xu. Similarly, in imaging applications for example, the edge potential can encode similarity between pixels which also need not verify the integrability condition as in [9]. Further, NBP does not offer consistent estimators of the LBP messages.\nParticle BP (PBP) [1] offers a way to overcome the shortcomings of NBP: the authors also consider importance sampling to tackle the update of the messages but without fitting a mixture of Gaussians.\nFor a chosen proposal distribution qu on node u and a draw of N particles {x(i)u }Ni=1 \u223c qu(xu), the messages are represented as mixtures:\nm\u0302PBPuv (xv) := N\u2211\ni=1\n\u03c9(i)uv\u03c8uv(x (i) u , xv), with \u03c9 (i) uv :=\n1\nN\n\u03c8u(x (i) u )\nqu(x (i) u )\n\u220f\nw\u2208\u0393u\\v m\u0302PBPwu (x (i) u ). (4)\nThis algorithm has the advantage that it does not require the conditions (3) to hold. The authors suggest two possible choices of sampling distributions: sampling from the local potential \u03c8u, or sampling from the current belief estimate. The first case is only valid if \u03c8u is integrable w.r.t. xu which, as we have mentioned earlier, might not be the case in general and the second case implies sampling from a distribution of the form\nB\u0302PBPu (xu) \u221d \u03c8u(xu) \u220f\nw\u2208\u0393u m\u0302PBPwu (xu) (5)\nwhich is a product of mixtures. As in NBP, na\u0131\u0308ve sampling of the proposal has complexityO(N |\u0393u|) and is thus in general too expensive to consider. Alternatively, as the authors suggest, one can run a short MCMC simulation targeting it which reduces the complexity to order O(|\u0393u|N2) since the cost of each iteration, which requires evaluating B\u0302PBPu point-wise, is of order O(|\u0393u|N), and we need O(N) iterations of the MCMC simulation. The issue with this approach is that it is still computationally expensive, and it is unclear how many iterations are necessary to get N good samples."}, {"heading": "2.3 Our contribution", "text": "In this paper, we consider the general context where the edge and node-potentials might be nonnormalizable and non-Gaussian. Our proposed method is based on PBP, as PBP is theoretically better suited than NBP since, as discussed earlier, it does not require the conditions (3) to hold, and, provided that one samples from the proposals exactly, it yields consistent estimators of the LBP messages while NBP does not. Further, the development of our method also formally shows that considering proposals close to the beliefs, as suggested by [1], is a good idea. Our core observation is that since sampling from a proposal of the form (5) using MCMC simulation is very expensive, we should consider using a more tractable proposal distribution instead. However it is important that the proposal distribution is constructed adaptively, taking into account evidence collected through the message passing itself, and we propose to achieve this by using proposal distributions lying in a tractable exponential family, and adapted using the Expectation Propagation (EP) framework [7]."}, {"heading": "3 Expectation Particle Belief Propagation", "text": "Our aim is to address the issue of selecting the proposals in the PBP algorithm. We suggest using exponential family distributions as the proposals on a node for computational efficiency reasons, with parameters chosen adaptively based on current estimates of beliefs and EP. Each step of our algorithm involves both a projection onto the exponential family as in EP, as well as a particle approximation of the LBP message, hence we will refer to our method as Expectation Particle Belief Propagation or EPBP for short.\nFor each pair of adjacent nodes u and v, we will use muv(xv) to denote the exact (but unavailable) LBP message from u to v, m\u0302uv(xv) to denote the particle approximation of muv , and \u03b7uv an exponential family projection of m\u0302uv . In addition, let \u03b7\u25e6u denote an exponential family projection of the node potential \u03c8u. We will consider approximations consisting of N particles. In the following, we will derive the form of our particle approximated message m\u0302uv(xv), along with the choice of the proposal distribution qu(xu) used to construct m\u0302uv . Our starting point is the edge-wise belief over xu and xv , given the incoming particle approximated messages,\nB\u0302uv(xu, xv) \u221d \u03c8uv(xu, xv)\u03c8u(xu)\u03c8v(xv) \u220f\nw\u2208\u0393u\\v m\u0302wu(xu)\n\u220f\n\u03bd\u2208\u0393v\\u m\u0302\u03bdv(xv). (6)\nThe exact LBP message muv(xv) can be derived by computing the marginal distribution B\u0302uv(xv), and constructing muv(xv) such that\nB\u0302uv(xv) \u221d muv(xv)M\u0302vu(xv), (7)\nwhere M\u0302vu(xv) = \u03c8v(xv) \u220f \u03bd\u2208\u0393v\\u m\u0302\u03bdv(xv) is the (particle approximated) pre-message from v to u. It is easy to see that the resulting message is as expected,\nmuv(xv) \u221d \u222b \u03c8uv(xu, xv)\u03c8u(xu) \u220f\nw\u2208\u0393u\\v m\u0302wu(xu)dxu. (8)\nSince the above exact LBP belief and message are intractable in our scenario of interest, the idea is to use an importance sampler targeting B\u0302uv(xu, xv) instead. Consider a proposal distribution of the form qu(xu)qv(xv). Since xu and xv are independent under the proposal, we can draw N independent samples, say {x(i)u }Ni=1 and {x (j) v }Nj=1, from qu and qv respectively. We can then approximate the belief using a N \u00d7N cross product of the particles,\nB\u0302uv(xu, xv) \u2248 1\nN2\nN\u2211\ni,j=1\nB\u0302uv(x (i) u , x (j) v )\nqu(x (i) u )qv(x (j) v )\n\u03b4 (x (i) u ,x (j) v )\n(9)\n\u221d 1 N2\nN\u2211\ni,j=1\n\u03c8uv(x (i) u , x (j) v )\u03c8u(x (i) u )M\u0302vu(x (j) v ) \u220f w\u2208\u0393u\\v m\u0302wu(x (i) u )\nqu(x (i) u )qv(x (j) v )\n\u03b4 (x (i) u ,x (j) v ) (xu, xv)\nMarginalizing onto xv , we have the following particle approximation to B\u0302uv(xv),\nB\u0302uv(xv) \u2248 1\nN\nN\u2211\nj=1\nm\u0302uv(x (j) v )M\u0302vu(x (j) v )\nqv(x (j) v )\n\u03b4 x (j) v (xv) (10)\nwhere the particle approximated message m\u0302uv(xv) from u to v has the form of the message representation in the PBP algorithm (4).\nTo determine sensible proposal distributions, we can find qu and qv that are close to the target B\u0302uv . Using the KL divergence KL(B\u0302uv\u2016quqv) as the measure of closeness, the optimal qu required for the u to v message is the node belief,\nB\u0302uv(xu) \u221d \u03c8u(xu) \u220f\nw\u2208\u0393u m\u0302wu(xu) (11)\nthus supporting the claim in [1] that a good proposal to use is the current estimate of the node belief. As pointed out in Section 2, it is computationally inefficient to use the particle approximated node belief as the proposal distribution. An idea is to use a tractable exponential family distribution for qu instead, say\nqu(xu) \u221d \u03b7\u25e6u(xu) \u220f\nw\u2208\u0393u \u03b7wu(xu) (12)\nwhere \u03b7\u25e6u and \u03b7wu are exponential family approximations of \u03c8u and m\u0302wu respectively. In Section 4 we use a Gaussian family, but we are not limited to this. Using the framework of expectation propogation (EP) [7], we can iteratively find good exponential family approximations as follows. For each w \u2208 \u0393u, to update the \u03b7wu, we form the cavity distribution q\\wu \u221d qu/\u03b7wu and the corresponding tilted distribution m\u0302wuq \\w u . The updated \u03b7+wu is the exponential family factor minimising the KL divergence,\n\u03b7+wu = arg min \u03b7\u2208exp.fam.\nKL [ m\u0302wu(xu)q \\w u (xu) \u2225\u2225\u2225 \u03b7(xu)q\\wu (xu) ] . (13)\nGeometrically, the update projects the tilted distribution onto the exponential family manifold. The optimal solution requires computing the moments of the tilted distribution through numerical quadrature, and selecting \u03b7wu so that \u03b7wuq \\w u matches the moments of the tilted distribution. In our scenario the moment computation can be performed crudely on a small number of evaluation points since it only concerns the updating of the IS proposal. If an optimal \u03b7 in the exponential family does not exist, e.g. in the Gaussian case that the optimal \u03b7 has a negative variance, we simply revert \u03b7wu to its previous value [7]. An analogous update is used for \u03b7\u25e6u.\nIn the above derivation, the expectation propagation steps for each incoming message into u and for the node potential are performed first, to fit the proposal to the current estimated belief at u, before\nit is used to draw N particles, which can then be used to form the particle approximated messages from u to each of its neighbours. Alternatively, once each particle approximated message m\u0302uv(xv) is formed, we can update its exponential family projection \u03b7uv(xv) immediately. This alternative scheme is described in Algorithm 1.\nAlgorithm 1 Node update\n1: sample {x(i)u } \u223c qu( \u00b7 ) 2: compute B\u0302u(x (i) u ) = \u03c8u(x (i) u ) \u220f w\u2208\u0393u m\u0302wu(x (i) u ) 3: for v \u2208 \u0393u do 4: compute M\u0302uv(x (i) u ) := B\u0302u(x (i) u )/m\u0302vu(x (i) u ) 5: compute the normalized weights w(i)uv \u221d M\u0302uv(x(i)u )/qu(x(i)u ) 6: update the estimator of the outgoing message m\u0302uv(xv) = \u2211N i=1 w (i) uv\u03c8uv(x (i) u , xv) 7: compute the cavity distribution q\\\u25e6v \u221d qv/\u03b7\u25e6v , get \u03b7+\u25e6v in the exponential family such that \u03b7+\u25e6vq \\\u25e6 v approximates \u03c8vq \\\u25e6 v , update qv \u221d \u03b7+\u25e6v and let \u03b7\u25e6v \u2190 \u03b7+\u25e6v 8: compute the cavity distribution q\\uv \u221d qv/\u03b7uv , get \u03b7+uv in the exponential family such that \u03b7+uvq \\u v approximates m\u0302uvq \\u v 9: end for"}, {"heading": "3.1 Computational complexity and sub-quadratic implementation", "text": "Each EP projection step costsO(N) computations since the message m\u0302wu is a mixture ofN components (see (4)). Drawing N particles from the exponential family proposal qu costs O(N). The step with highest computational complexity is in evaluating the particle weights in (4). Indeed, evaluating the mixture representation of a message on a single point is O(N), and we need to compute this for each of N particles. Similarly, evaluating the estimator of the belief on N sampling points at node u requires O(|\u0393u|N2). This can be reduced since the algorithm still provides consistent estimators if we consider the evaluation of unbiased estimators of the messages instead. Since the messages have the form m\u0302uv(xv) = \u2211N i=1 w i uv\u03c8 i uv(xv), we can follow a method presented in [10] where one draws M indices {i?`}M`=1 from a multinomial with weights {wiuv}Ni=1 and evaluates the corresponding M components \u03c8i ? ` uv . This reduces the cost of the evaluation of the beliefs to O(|\u0393u|MN) which leads to an overall sub-quadratic complexity if M is o(N). We show in the next section how it compares to the quadratic implementation when M = O(logN)."}, {"heading": "4 Experiments", "text": "We investigate the performance of our method on MRFs for two simple graphs. This allows us to compare the performance of EPBP to the performance of PBP in depth. We also illustrate the behavior of the sub-quadratic version of EPBP. Finally we show that EPBP provides good results in a simple denoising application."}, {"heading": "4.1 Comparison with PBP", "text": "We start by comparing EPBP to PBP as implemented by Ihler et al. on a 3 \u00d7 3 grid (figure 1) with random variables taking values on R. The node and edge potentials are selected such that the marginals are multimodal, non-Gaussian and skewed with{\n\u03c8u(xu) = \u03b11N (xu \u2212 yu;\u22122, 1) + \u03b12G(xu \u2212 yu; 2, 1.3) \u03c8uv(xu, xv) = L(xu \u2212 xv; 0, 2) , (14)\nwhere yu denotes the observation at node u, N (x;\u00b5, \u03c3) \u221d exp(\u2212x2/2\u03c32) (density of a Normal distribution), G(x;\u00b5, \u03b2) \u221d exp(\u2212(x\u2212\u00b5)/\u03b2+exp(\u2212(x\u2212\u00b5)/\u03b2)) (density of a Gumbel distribution) and L(x;\u00b5, \u03b2) \u221d exp(\u2212|x \u2212 \u00b5|/\u03b2) (density of a Laplace distribution). The parameters \u03b11 and \u03b12 are respectively set to 0.6 and 0.4. We compare the two methods after 20 LBP iterations.1\n1The scheduling used alternates between the classical orderings: top-down-left-right, left-right-top-down, down-up-right-left and right-left-down-up. One \u201cLBP iteration\u201d implies that all nodes have been updated once.\nPBP as presented in [1] is implemented using the same parameters than those in an implementation code provided by the authors: the proposal on each node is the last estimated belief and sampled with a 20-step MCMC chain, the MH proposal is a normal distribution. For EPBP, the approximation of the messages are Gaussians. The ground truth is approximated by running LBP on a deterministic equally spaced mesh with 200 points. All simulations were run with Julia on a Mac with 2.5 GHz Intel Core i5 processor, the code is available online.2\nFigure 2 compares the performances of both methods. The error is computed as the mean L1 error over all nodes between the estimated beliefs and the ground truth evaluated over the same deterministic mesh. One can observe that not only does PBP perform worse than EPBP but also that the error plateaus with increasing number of samples. This is because the sampling within PBP is done approximately and hence the consistency of the estimators is lost. The speed-up offered by EPBP is very substantial (figure 4 left). Hence, although it would be possible to use more MCMC ((Metropolis-Hastings) iterations within PBP to improve its performance, it would make the method prohibitively expensive to use since every MCMC step has quadratic cost. Note that for EPBP, one observes the usual 1/ \u221a N convergence of particle methods. Figure 3 compares the estimator of the beliefs obtained by the two methods for three arbitrarily picked nodes (node 1, 5 and 9 as illustrated on figure 1). The figure also illustrates the last proposals constructed with our approach and one notices that their supports match closely the support of the true beliefs. Figure 4 left illustrates how the estimated beliefs converge as compared to the true beliefs with increasing number of iterations. One can observe that PBP converges more slowly and that the results display more variability which might be due to the MCMC runs being too short.\nWe repeated the experiments on a tree with 8 nodes (figure 1 right) where we know that, at convergence, the beliefs computed using BP are proportional to the true marginals. The node and edge potentials are again picked such that the marginals are multimodal with{\n\u03c8u(xu) = \u03b11N (xu \u2212 yu;\u22122, 1) + \u03b12N (xu \u2212 yu; 1, 0.5) \u03c8uv(xu, xv) = L(xu \u2212 xv; 0, 1) , (15)\nwith \u03b11 = 0.3 and \u03b12 = 0.7. On this example, we also show how \u201cpure EP\u201d with normal distributions performs. We also try using the distributions obtained with EP as proposals for PBP (referred to as \u201cPBP after EP\u201d in figures). Both methods underperform compared to EPBP as illustrated visually in Figure 5. In particular one can observe in Figure 3 that \u201cPBP after EP\u201d converges slower than EPBP with increasing number of samples."}, {"heading": "4.2 Sub-quadratic implementation and denoising application", "text": "As outlined in Section 3.1, in the implementation of EPBP one can use an unbiased estimator of the edge weights based on a draw of M components from a multinomial. The complexity of the resulting algorithm is O(MN). We apply this method to the 3 \u00d7 3 grid example in the case where M is picked to be roughly of order log(N): i.e., for N = {10, 20, 50, 100, 200, 500}, we pick M = {5, 6, 8, 10, 11, 13}. The results are illustrated in Figure 6 where one can see that the N logN implementation compares very well to the original quadratic implementation at a much reduced cost. We apply this sub-quadratic method on a simple probabilistic model for an image denoising problem. The aim of this example is to show that the method can be applied to larger graphs and still provide good results. The model underlined is chosen to showcase the flexibility and applicability of our method in particular when the edge-potential is non-integrable and is not claimed to be optimal. The node and edge potentials are defined as follows:{\n\u03c8u(xu) = N (xu \u2212 yu; 0, 0.1) \u03c8uv(xu, xv) = L\u03bb(xu \u2212 xv; 0, 0.03) , (16)\n2https://github.com/tlienart/EPBP\nwhere L\u03bb(x;\u00b5, \u03b2) = L(x;\u00b5, \u03b2) if |x| \u2264 \u03bb and L(\u03bb;\u00b5, \u03b2) otherwise. In this example we set \u03bb = 0.2. The value assigned to each pixel of the reconstruction is the estimated mean obtained over the corresponding node (figure 7). The image has size 50 \u00d7 50 and the simulation was run with N = 30 particles per nodes, M = 5 and 10 BP iterations taking under 2 minutes to complete. We compare it with the result obtained with EP on the same model."}, {"heading": "5 Discussion", "text": "We have presented an original way to design adaptively efficient and easy-to-sample-from proposals for a particle implement of Loopy Belief Propagation. Our proposal is inspired by the Expectation Propagation framework.\nWe have demonstrated empirically that the resulting algorithm is significantly faster and more accurate than an implementation of PBP using the estimated beliefs as proposals and sampling from them using MCMC as proposed in [1]. It is also more accurate than EP due to the nonparametric nature of the messages and offers consistent estimators of the LBP messages. A sub-quadratic version of\nthe method was also outlined and shown to perform almost as well as the original method, it was also applied successfully in an image denoising example.\nWe believe that our method could be applied successfully to a wide range of applications such as smoothing for Hidden Markov Models [11], tracking or computer vision [12, 13]. In future work, we will look at considering other divergences than the KL and the \u201cPower EP\u201d framework [14], we will also look at encapsulating the present algorithm within a sequential Monte Carlo framework and the recent work of Naesseth et al. [15]."}, {"heading": "Acknowledgments", "text": "We thank Alexander Ihler and Drew Frank for sharing their implementation of Particle Belief Propagation. TL gratefully acknowledges funding from EPSRC and the Scatcherd European scholarship scheme. YWT\u2019s research leading to these results has received funding from EPSRC (grant EP/K009362/1) and ERC under the EU\u2019s FP7 Programme (grant agreement no. 617411). AD\u2019s research was supported by the EPSRC (grant EP/K000276/1, EP/K009850/1) and by AFOSR/AOARD (grant AOARD-144042)."}], "references": [{"title": "Particle belief propagation", "author": ["Alexander T. Ihler", "David A. McAllester"], "venue": "In Proc. 12th AIS- TATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Graphical models, exponential families, and variational inference", "author": ["Martin J. Wainwright", "Michael I. Jordan"], "venue": "Found. and Tr. in Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Nonparametric belief propagation", "author": ["Erik B. Sudderth", "Alexander T. Ihler", "Michael Isard", "William T. Freeman", "Alan S. Willsky"], "venue": "Commun. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Constructing free energy approximations and generalized belief propagation algorithms", "author": ["Jonathan S. Yedidia", "William T. Freeman", "Yair Weiss"], "venue": "MERL Technical Report,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Nonparametric belief propagation", "author": ["Erik B. Sudderth", "Alexander T. Ihler", "William T. Freeman", "Alan S. Willsky"], "venue": "In Procs. IEEE Comp. Vis. Patt. Rec.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["Thomas P. Minka"], "venue": "In Proc. 17th UAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Loopy belief propagation for approximate inference: an empirical study", "author": ["Kevin P. Murphy", "Yair Weiss", "Michael I. Jordan"], "venue": "In Proc. 15th UAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Thresholding implied by truncated quadratic regularization", "author": ["Mila Nikolova"], "venue": "IEEE Trans. Sig. Proc.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Sequential auxiliary particle belief propagation", "author": ["Mark Briers", "Arnaud Doucet", "Sumeetpal S. Singh"], "venue": "In Proc. 8th ICIF,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Smoothing algorithms for state-space models", "author": ["M. Briers", "A. Doucet", "S. Maskell"], "venue": "Ann. Inst. Stat. Math.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Visual hand tracking using nonparametric belief propagation", "author": ["Erik B. Sudderth", "Michael I. Mandel", "William T. Freeman", "Alan S. Willsky"], "venue": "In Procs. IEEE Comp. Vis. Patt. Rec.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Sequential monte carlo for graphical models", "author": ["Lindsten Fredrik Sch\u00f6n Naesseth", "Christian A"], "venue": "In NIPS", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of [1] at a fraction of the computational cost and is additionally more robust empirically.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "Undirected Graphical Models (also known as Markov Random Fields) provide a flexible framework to represent networks of random variables and have been used in a large variety of applications in machine learning, statistics, signal processing and related fields [2].", "startOffset": 260, "endOffset": 263}, {"referenceID": 0, "context": "For many applications such as tracking, sensor networks or imaging [1, 3], it can be beneficial to define MRF on continuous state-spaces.", "startOffset": 67, "endOffset": 73}, {"referenceID": 2, "context": "For many applications such as tracking, sensor networks or imaging [1, 3], it can be beneficial to define MRF on continuous state-spaces.", "startOffset": 67, "endOffset": 73}, {"referenceID": 3, "context": "A popular approach to do this is to consider the Loopy Belief Propagation (LBP) algorithm [4, 5, 2].", "startOffset": 90, "endOffset": 99}, {"referenceID": 4, "context": "A popular approach to do this is to consider the Loopy Belief Propagation (LBP) algorithm [4, 5, 2].", "startOffset": 90, "endOffset": 99}, {"referenceID": 1, "context": "A popular approach to do this is to consider the Loopy Belief Propagation (LBP) algorithm [4, 5, 2].", "startOffset": 90, "endOffset": 99}, {"referenceID": 5, "context": "The Nonparametric Belief Propagation (NBP) algorithm [6] represents the messages with mixtures of Gaussians while the Particle Belief Propagation (PBP) algorithm [1] uses an importance sampling approach.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "The Nonparametric Belief Propagation (NBP) algorithm [6] represents the messages with mixtures of Gaussians while the Particle Belief Propagation (PBP) algorithm [1] uses an importance sampling approach.", "startOffset": 162, "endOffset": 165}, {"referenceID": 0, "context": "Practically the authors of [1] only sample approximately from them using short MCMC runs, leading to biased estimators.", "startOffset": 27, "endOffset": 30}, {"referenceID": 6, "context": "tion framework [7].", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "The method is empirically shown to yield better approximations to the LBP beliefs than the implementation suggested in [1], at a much reduced computational cost, and than EP.", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "This algorithm is a fixed point iteration scheme yielding approximations called the beliefs at each node [4, 2].", "startOffset": 105, "endOffset": 111}, {"referenceID": 1, "context": "This algorithm is a fixed point iteration scheme yielding approximations called the beliefs at each node [4, 2].", "startOffset": 105, "endOffset": 111}, {"referenceID": 7, "context": "However, even in these cases, LBP has been shown to provide good approximations in a wide range of situations [8, 5].", "startOffset": 110, "endOffset": 116}, {"referenceID": 4, "context": "However, even in these cases, LBP has been shown to provide good approximations in a wide range of situations [8, 5].", "startOffset": 110, "endOffset": 116}, {"referenceID": 5, "context": "In Nonparametric BP (NBP) [6], the messages are represented by mixtures of Gaussians.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "These conditions do however not hold in a number of important cases as acknowledged in [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "Similarly, in imaging applications for example, the edge potential can encode similarity between pixels which also need not verify the integrability condition as in [9].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "Particle BP (PBP) [1] offers a way to overcome the shortcomings of NBP: the authors also consider importance sampling to tackle the update of the messages but without fitting a mixture of Gaussians.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "Further, the development of our method also formally shows that considering proposals close to the beliefs, as suggested by [1], is a good idea.", "startOffset": 124, "endOffset": 127}, {"referenceID": 6, "context": "However it is important that the proposal distribution is constructed adaptively, taking into account evidence collected through the message passing itself, and we propose to achieve this by using proposal distributions lying in a tractable exponential family, and adapted using the Expectation Propagation (EP) framework [7].", "startOffset": 322, "endOffset": 325}, {"referenceID": 0, "context": "thus supporting the claim in [1] that a good proposal to use is the current estimate of the node belief.", "startOffset": 29, "endOffset": 32}, {"referenceID": 6, "context": "Using the framework of expectation propogation (EP) [7], we can iteratively find good exponential family approximations as follows.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "in the Gaussian case that the optimal \u03b7 has a negative variance, we simply revert \u03b7wu to its previous value [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 9, "context": "Since the messages have the form m\u0302uv(xv) = \u2211N i=1 w i uv\u03c8 i uv(xv), we can follow a method presented in [10] where one draws M indices {i`}`=1 from a multinomial with weights {wi uv}i=1 and evaluates the corresponding M components \u03c8 ? ` uv .", "startOffset": 105, "endOffset": 109}, {"referenceID": 0, "context": "PBP as presented in [1] is implemented using the same parameters than those in an implementation code provided by the authors: the proposal on each node is the last estimated belief and sampled with a 20-step MCMC chain, the MH proposal is a normal distribution.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "We have demonstrated empirically that the resulting algorithm is significantly faster and more accurate than an implementation of PBP using the estimated beliefs as proposals and sampling from them using MCMC as proposed in [1].", "startOffset": 224, "endOffset": 227}, {"referenceID": 10, "context": "We believe that our method could be applied successfully to a wide range of applications such as smoothing for Hidden Markov Models [11], tracking or computer vision [12, 13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 11, "context": "We believe that our method could be applied successfully to a wide range of applications such as smoothing for Hidden Markov Models [11], tracking or computer vision [12, 13].", "startOffset": 166, "endOffset": 174}, {"referenceID": 12, "context": "[15].", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of [1] at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.", "creator": "LaTeX with hyperref package"}}}