{"id": "1506.06868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2015", "title": "Learning Discriminative Bayesian Networks from High-dimensional Continuous Neuroimaging Data", "abstract": "due indeed to increasing its causal sensor semantics, bayesian networks ( bn ) engines have been widely employed to effectively discover the underlying data relationship in subsequent exploratory studies, such as primate brain research. despite indicating its success in exclusively modeling the unique probability distribution data of environmental variables, comparative bn is nevertheless naturally a generative model, at which essentially is not necessarily precisely discriminative. this result may seldom cause rarely the ignorance bias of purely subtle effects but critical network changes \u2026 that are unaware of unique investigation values observed across overlapping populations. in this paper, only we propose to improve naturally the discriminative analytical power of bn numerical models for continuous constraint variables from linking two truly different viewpoint perspectives. this brings two general global discriminative learning frameworks for gaussian bayesian linear networks ( gbn ). in the new first framework, we employ fisher spectral kernel equations to bridge the generative models variable of gbn and the resulting discriminative classifiers of svms, and convert the gbn parameter learning to fisher kernel model learning via minimizing along a generalization positive error backward bound correlation of svms. for in the second framework, we employ defining the max - set margin criterion and indeed build it again directly upon particular gbn models to continually explicitly optimize comparing the cumulative classification comparison performance of ignoring the referenced gbns. the advantages and additional disadvantages characteristic of the two frameworks derived are rapidly discussed and experimentally compared. practically both areas of our them directly demonstrate strong model power there in learning discriminative invariant parameters dependent of relevant gbns for neuroimaging based of brain network dependence analysis,, as well work as systematically maintaining fully reasonable theoretical representation structure capacity. the key contributions of this paper also include a large new data directed progressive acyclic graph ( also dag ) digital constraint matrix with precise theoretical guarantee to ensure the graph validity of gbn.", "histories": [["v1", "Tue, 23 Jun 2015 05:39:34 GMT  (464kb)", "http://arxiv.org/abs/1506.06868v1", "16 pages and 5 figures for the article (excluding appendix)"]], "COMMENTS": "16 pages and 5 figures for the article (excluding appendix)", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["luping zhou", "lei wang", "lingqiao liu", "philip ogunbona", "dinggang shen"], "accepted": false, "id": "1506.06868"}, "pdf": {"name": "1506.06868.pdf", "metadata": {"source": "CRF", "title": "Learning Discriminative Bayesian Networks from High-dimensional Continuous Neuroimaging Data", "authors": ["Luping Zhou", "Lei Wang", "Lingqiao Liu", "Philip Ogunbona", "Dinggang Shen"], "emails": ["philipo@uow.edu.au."], "sections": [{"heading": null, "text": "ar X\niv :1\n50 6.\n06 86\n8v 1\n[ cs\n.C V\n] 2\n3 Ju\nn 20\n15 1\nIndex Terms\u2014Bayesian network, discriminative learning, Fisher kernel learning, max-margin, brain network.\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "A S an important probabilistic graphical model,Bayesian network (BN) has been used to model the probability distribution of a set of random variables for a wide spectrum of applications, e.g., diagnosis, troubleshooting, web mining, meteorology and bioinformatics. It combines graph representation with Bayesian analysis, providing an effective way to model and infer the conditional dependency of the variables. A BN has to be a directed acyclic graph (DAG). Two factors characterize a BN, i.e., the structure of the network (the presence / absence of edges in the graph) and the parameters of the probability distribution. Recent research of BN focuses on how to learn the structure and the parameters of BN directly from the data.\nThe approaches of learning BN structures can be roughly categorized into the constraint-based, the scorebased, and the hybrid approaches. The constraint-based approaches use a serie of conditional independence testing to ensure the model structure is consistent with the conditional independency entailed by the observations. Methods in this class include the IC algorithm [1], PC algorithm [2], and more recent methods [3], [4]. Scorebased approaches define a scoring function over the\n\u2022 L. Zhou, L. Wang and P. Ogunbona are with the School of Computing and Information Technology, University of Wollongong, NSW 2500, Australia. E-mail: lupingz, leiw, philipo@uow.edu.au. L. Liu is with Shool of Computer Science, University of Adelaide, Australia. D. Shen is with Department of Radiology, University of North Carolina at Chapel Hill, USA.\nspace of candidate DAGs and optimize this function through certain search strategies. Methods in this class vary with scoring criteria, e.g., the posterior probability [5], [6], [7] and the minimum description length [8], or vary with search strategies, e.g., the heuristic search [9] and the Monte Carlo methods [5]. Hybrid approaches usually employ constraint-based methods to prune the search space of DAG structures and consequently restrict a subsequent score-based search [10], [11]. Many existing BN learning methods, such as LIMB-DAG [12], MMHC [10], TC and TC-bw [13], comprise of two stages: the identification of candidate parent sets in the first stage and the further pruning of them based on certain criteria in the second stage. Despite the mitigation of computational complexity, a drawback arises that if a parent node is missed in the first stage, it will never be recovered in the second stage [14]. To address this issue, one-stage learning process has been preferred in recent research work [14], [15]. In these studies, based on Gaussian Bayesian network (GBN), the parent sets of all variables are learned together to optimize a LASSObased score function in a single stage. The related optimization problems are solved either approximately [14] or exactly [15]. They have demonstrated improved reliability of BN edge identification over traditional two stage methods. Although BN is naturally a generative method, it has also been used in classification tasks for diagnostic or predictive purposes. A straightforward usage is to train each class a BN and classify a new sample into the class with the highest likelihood value [14]. Another kind of\n2 approaches trains \u201cBayesian network classifiers\u201d with discriminative objective functions [16], [17], [18]. In these approaches, usually a single BN is learned to optimize the discrimination performance. Either the structure or the parameters of the BN are adjusted to reflect the class difference for better classification. Therefore, the resulting BN does not model the distribution of any individual class. The \u201cBayesian network classifiers\u201d in [16], [17], [18] are designed for discrete variables of multinomial distribution. They still inherit the two-stage learning process, i.e., have to predefine candidate parent sets as mentioned above.\nLearning BN from the data faces new challenges in exploratory domains, such as brain research, where the mechanism of brain and mental diseases remain unclear and need to be explored. These domains usually cater for both interpretation and discrimination. \u201cInterpretation\u201d requires interpretable models of the data and the findings explained by domain language rather than mathematical terms. This requirement comes from the demand of understanding the domain problems. \u201cDiscrimination\u201d requires the models to have sufficient discriminative power to distinguish groups of interest (such as identifying the diseased from the healthy), for the purpose of prediction. To some extent, a high accuracy of the predictive model also provides a measure of the amount of information captured by that model.\nBeing a generative method, BN represents the distribution of the data and is naturally amenable for interpretation. However, it is known that generative methods are not necessarily discriminative. They are prone to emphasizing major structures that are shared within each group, and neglecting the subtle but critical changes across groups. The latter, unfortunately, often happens, for example, in disease-induced brain changes across clinical groups. Consequently, generative methods are usually inferior in prediction compared with the discriminative methods that target only the boundary of classes (such as Support Vector Machines (SVMs)). On the other hand, discriminative methods often encounter the difficulty of interpretation, which is critical in exploratory research aimed at both the understanding and the prediction. Thus, this paper is motivated by the advantages that can be gained by learning BNs that are both representative and discriminative. Different from the Bayesian network classifiers in [16], [17], [18] that address discrete variables, we learn discriminative BNs for continuous variables, which is often needed in many domains including neuroimaging-based brain research. Moreover, we learn for each class a BN with enhanced discrimination and maintain the BN repre-\nsentation of each individual class for interpretation1. To achieve our goal, we propose two discriminative learning frameworks based on sparse Gaussian Bayesian network (SGBN). In the first framework (termed KL-SGBN), we employ Fisher kernel [19] to link the generative models of SGBN to the discriminative classifiers of SVMs, and convert the SGBN parameter learning to Fisher kernel learning via maximizing a generalization bound of SVMs. The contributions of this framework include the following. i) By inducing Fisher kernel on SGBN models, we provide a way to obtain sample-specific SGBN-induced feature vectors that can be used by the discriminative classifiers such as SVMs. Through this, we bridge the generative models and the discriminative classifiers. ii) We propose a kernel learning approach to discriminatively learn the parameters of SGBNs by optimizing the performance of SVM. iii) As a by-product, the manipulation of Fisher kernel on SGBN provides a new way of variable selection for SGBNs. This framework has a computational advantage: through the mapping of Fisher kernel, the SGBN-induced feature vectors become linear functions of the SGBN parameters, which significantly simplifies the optimization problem in the learning process. Unlike KL-SGBN where the discrimination is obtained by optimizing the classification performance of SVMs, in the second learning framework (termed MM-SGBN), we propose to optimize a criterion directly built upon the classification performance of SGBNs. The motivation is that optimizing the performance of SVMs may not necessarily guarantee an equivalent improvement on SGBNs when SGBNs are the goal of applications. The contribution of this framework is a max-margin based method to jointly learn SGBNs, one for each class, for both representation and discrimination. In addition to the two discriminative SGBN learning frameworks, our contributions in this paper also include a new DAG constraint of SGBN based on topological ordering to ensure the validity of the graph. This new DAG constraint circumvents the awkward hard binarization of SGBN parameters in the process of optimization in [14], and simplifies the related optimization problems. This consequently makes it possible to optimize all the SGBN parameters together to avoid the influence of feature ordering encountered in the Block Coordinate Descent (BCD) optimization in [14]. Moreover, this new DAG constraint also circumvents the need for presetting candidate parent sets as in [17]. Although the discriminative learning frameworks proposed in this paper are general methods, we focus on their applications in neuroimaging analysis for the early\n1. In this paper, we deal with the scenario that maintaining the BN representation of individual class is critical for the understanding of domain problems, such as the brain network models for the healthy and the diseased groups. However, it is not difficulty to see our discriminative learning frameworks could be slightly modified to learn only a single BN as the existing \u201cBayesian network classifiers\u201d for continuous variables. However, this deviates from our motivation and therefore is not unfolded in this paper.\n3\ndiagnosis of mental diseases. A newly emerging field in the imaging-based neuroscience, called brain network analysis, attempts to model the brain as a complex network and study the interactions of brain regions via imaging-based features [20]. Such research is important because brain network change is often found to be a response of the brain to damages. Due to its causal semantics, BN has been employed to model the \u201ceffective connectivity\u201d of the brain [14], [21], [22]. The directionality of the connections may disclose the pathways of how one brain region affects another. The discoveries may lend further credence to the evidence of causal relationship changes found in many mental diseases, such as the Alzheimer\u2019s disease (AD) [23], [14], [24], [22], and uncover novel connectivity-based biomarkers for disease diagnosis. The proposed learning frameworks has been tested on multiple neuroimaging data sets. As demonstrated, our methods can significantly improve the discriminative power of the obtained SGBNs, as well as maintaining their representation capacity.\nEarly conference versions of this work were published in [25], [26]. In this paper, a significant extension has been made on the following aspects. First, we analyze the problems of the DAG constraint used in [25], [26], [14], and propose a new constraint with theoretically guaranteed DAG property to overcome those drawbacks. Second, we experimentally verify the new DAG constraint on benchmark Bayesian network data sets for network structure learning, and compare our method with another eight competing methods in the literature. Third, we update our two discriminative learning frameworks with the new DAG constraint and redo all the experiments in our early work [25], [26]. Fourth, we analyze the connections and differences between the two proposed discriminative learning frameworks, and conduct more comprehensive experiments to explore the characteristics of our frameworks with varied parameters, which has not been done in [25], [26].\nThe rest of the paper is organized as follows. Section 2 reviews SGBN and introduces the background of brain network analysis. Sections 3 elaborates two frameworks to learn discriminative and representative SGBNs from continuous data. Section 4 revisits the problem of the existing DAG constraint of SGBN, and proposes a new one based on topological ordering. The proposed two learning frameworks with the new DAG constraint are experimentally tested in Section 5. This paper is concluded in Section 6. The notations of symbols frequently occurring in this paper are summarized in Table 1."}, {"heading": "2 BACKGROUND", "text": "To make this paper self-contained, we introduce the background for both the methodology and its application to brain network analysis. Please note that the methodology could be generalized to applications beyond the example given in this paper."}, {"heading": "2.1 Sparse Gaussian Bayesian Network (SGBN)", "text": "Because this paper is based on SGBN model, in the following, we review the fundamentals of SGBN in [14]. All the symbols are defined in Table 1. A Bayesian network (BN) G is a directed acyclic graph (DAG), i.e. there is no closed path within the graph. It expresses the factorization property of a joint distribution p(x) = \u220f\ni=1,\u00b7\u00b7\u00b7 ,m p(xi|Pai). The conditional probability p(xi|Pai) is assumed to follow a Gaussian distribution in Gaussian Bayesian Network (GBN). Each node xi is regressed over its parent nodes Pai: xi = \u03b8 \u22a4 i Pai + \u03b5i, where the vector \u03b8i is the regression coefficients, and \u03b5i \u223c N (0, \u03c32i ). The structure of BN could be characterized by the m \u00d7 m matrix G or P (defined in Table 1), representing the edges / paths in the graph, respectively. Identifying parent sets is critical for BN learning. Traditional methods often consist of two stages: the candidate parent sets are initially identified in the first stage and further pruned by some criteria in the second stage. A drawback arises that when a true parent is missing in the first stage, it will never be recovered in the second stage. The work in [14] proposed a different approach based on sparse GBN (SGBN), denoted as H-SGBN in this paper. In H-SGBN, each node xi is regressed over all the other nodes, and its parent set is implicitly selected by the regression coefficients \u03b8i that are estimated through a constrained LASSO regression. The following optimization is solved in [14]:\nmin \u0398\nm \u2211\ni=1\n\u2016x:,i \u2212PA\u22a4i \u03b8i\u201622 + \u03bb1\u2016\u03b8i\u20161 (2.1)\ns.t. \u0398ji \u00d7Pij = 0, \u2200i, j = 1, \u00b7 \u00b7 \u00b7 ,m, i 6= j.\nA challenge for BN learning is how to enforce the DAG property, i.e., avoiding directed cycles in the graph. A sufficient and necessary condition for being a DAG is proposed in [14], which requires \u0398ji \u00d7 Pij = 0 for all\n4 i and j. Note that Pij is an implicit function of \u0398ji (i.e., P = expm(\u0398), the matrix exponential function of \u0398, as in [14]). Eqn. (2.1) is difficult to solve. In [14], a block coordinate descent (BCD) method is employed to solve a LASSO-like problem efficiently. The whole \u0398 is optimized column-wisely and iteratively. In each iteration t, only one column of \u0398, say \u0398:,j , is optimized with P fixed as P(t\u22121) in the last iteration. Then \u0398(t), with the updated column \u0398:,j , is binarized to obtain G(t), based on which, P(t) is recalculated by a Breadthfirst search with xi being the root node. The process is repeated until convergence. H-SGBN simultaneously obtains the structure and the parameters of an SGBN via learning \u0398, e.g., there is no edge i \u2192 j if \u0398ij is zero. It has been demonstrated to outperform the conventional two-stage methods in network edge recovery."}, {"heading": "2.2 Brain Network Analysis", "text": "Neuroimaging modalities and analysis techniques can provide more sensitive and consistent measurements than traditional cognitive assessment for the early diagnosis of disease. Many mental disorders are found associated with subtle abnormalities distributed over the entire brain, rather than an individual brain region. The \u201cdistributive\u201d nature of mental disorders suggests the alteration of interactions between brain regions (neuronal systems) and thus the necessity of studying the brain as a complex network. Brain networks are mathematically represented by graphical models, which can be constructed from neuroimaging data as follows. The brain images belonging to different subjects are first spatially aligned to a common stereotaxic space by affine or deformable transformation, and then partitioned into regions of interest (ROI), i.e., clusters of imaging voxels, using either data-driven methods or predefined brain atlas. A brain network is then modeled by a graph with each node corresponding to a brain region and each edge corresponding to the connectivity between regions. Brain network analysis studies three kinds of brain connectivity. In this paper, we focus on the \u201ceffective connectivity\u201d that describes the influence one brain region exert upon another. Some early works in this field require a prior model of brain connectivity and most have only considered a small number (\u2264 10) of brain regions using techniques such as structural equation modeling [27] and dynamic causal modeling [28]. More recently, models such as BN and Granger Causality have also been introduced into this field. It is suggested that BN may have advantages over those lag-based methods for brain network analysis by an experimental fMRI study [21]. Among BN-related methods, it is worth noting that the work in [14] is completely data-driven, which recovers SGBN from more than 40 brain regions in fluorodeoxyglucose PET (FDG-PET) images. The method employs the strategy of sparsity constraint to handle relatively larger scale BN construction, and circumvents the traditional two-stage procedure for identifying parent\nsets in many sparse BN learning methods [12], [10]."}, {"heading": "3 PROPOSED DISCRIMINATIVE LEARNING OF GENERATIVE SGBN", "text": "BN models are by definition generative models, focusing on how the data could be generated through an underlying process. In the context of neuroimage analysis, these models represent the effective brain connectivity of the given population. When used for classification, e.g., identifying AD patients from the healthy, the SGBN models are trained for each class separately. A new sample xi is then assigned to the class with the higher likelihood of SGBN. This may ignore some subtle but critical network differences that distinguish the classes. Therefore, we argue that the parameters of the generative model should be learned from the two classes jointly to keep the essential discrimination. Integrating generative and discriminative models is an important research topic in machine learning. In [29], the related approaches are roughly divided into three categories: blending, staging and iterative methods. In blending methods, both the discriminative and the generative terms are incorporated into the same objective function. In staging methods, the discriminative model is trained on features provided by the generative model. In iterative methods, the generative and the discriminative models are trained iteratively to influence each other. In this paper, we propose two kinds of discriminative learning frameworks to achieve our goal. One is a staging method, called Fisher-kernel-induced discriminative learning (KL-SGBN). It extracts sample-based features from SGBN by Fisher kernel to optimize the classification performance of SVM. The other is a blending method, called max-margin-based discriminative learning (MMSGBN). It directly optimizes the classification performance of SGBNs subject to maintaining SGBN\u2019s representation capacity. The two frameworks are elaborated in the following sections, respectively."}, {"heading": "3.1 Proposed Fisher-kernel-induced Discriminative Learning (KL-SGBN)", "text": "We first introduce the Fisher-kernel-induced discriminative learning of SGBN, i.e., KL-SGBN. The algorithm is illustrated in Fig. 1 and overviewed as follows. Given two classes in comparison, two SGBN models (with the parameters of \u03981 and \u03982) are learned, one for each individual class. The original samples are then mapped into the gradient space of the SGBN parameters \u03981 and \u03982 by Fisher kernel (Section 3.1.1). Through this mapping, each sample is represented by a new feature vector (called Fisher vector [19]) that is a function of \u0398 = [\u03981,\u03982]. These sample-specific feature vectors are then fed into an SVM classifier to minimize its generalization errors by adjusting \u0398 (Section 3.1.2). The obtained optimal \u0398\u22c61 and \u0398 \u22c6 2 encode the discriminative information and therefore improve the original SGBNs.\n5 In this way, we convert the discriminative learning of SGBN parameters to the discriminative learning of Fisher kernels.\n3.1.1 Induction of Fisher vectors from SGBN\nBelow we introduce how to use Fisher kernel on SGBNs to obtain feature vectors required for kernel learning. Fisher kernel [19] provides a way to compare samples induced by a generative model. It maps a sample to a feature vector in the gradient space of the model parameters. The intuition is that similar objects induce similar log-likelihood gradients of the model parameters. Fisher kernel is computed as K(x,x\u2032) = g\u22a4xU\n\u22121gx\u2032 , where the Fisher vector gx = \u2207\u03b8 log(p(x|\u03b8)) describes the changing direction of parameters to better fit the model. The Fisher information metric U weights the similarity measure, but is often set as an identity matrix in practice [19]. Fisher kernel has recently witnessed successful applications in image categorization [30], [31] for inducing feature vectors from Gaussian Mixture Model (GMM) of a visual vocabulary. Despite its success, in the applications above, Fisher kernel is mainly used as a feature extractor2. It has not been applied to learning the parameters of probability distributions before the early work of this paper in [25]. The advantage of learning discriminative Fisher kernel has also been confirmed by a recent study that maximizes the class separability [33] of samples based on Fisher kernel, which is developed with different context and different criteria from ours. Following [14], we only consider \u0398 as parameters and predefine \u03c3. Let L(x|\u0398) = log(p(x|\u0398)) denote the loglikelihood. Our Fisher vector for each sample x is\n\u03a6\u0398(x) = [\u2207\u03981L(x|\u03981)\u22a4, \u2207\u03982L(x|\u03982)\u22a4]\u22a4,\nwhere \u03981 and \u03982 are the parameters of the SGBNs for the two classes (y = 1, 2), respectively. Recall that, using a BN, the probability p(x|\u0398) can be factorized as\n2. An exception [32] is discussed in \u201cGeneralization\u201d in Section 3.3, which is published after our work [25].\np(x|\u0398) = \u220f i=1,\u00b7\u00b7\u00b7 ,m p(xi|Pai, \u03b8i). Therefore, for GBN it can be shown that\nL(x|\u0398) = m \u2211\ni=1\nlog p(xi|Pai, \u03b8i) (3.1)\n= m \u2211\ni=1\n\u2212(xi \u2212Pa\u22a4i \u03b8i)2 2\u03c32i \u2212 log(2\u03c0\u221a\u03c3i).\nTaking partial derivative over \u03b8i, we have\n\u2202L(x|\u0398) \u2202\u03b8i = \u2212PaiPa \u22a4 i \u03c32i \u03b8i \u2212 xiPai \u03c32i\n(3.2)\n, S(xi)\u03b8i + s0(xi),\nwhere S(xi) is a squared matrix and s0(xi) is a vector. As shown, both S(xi) and s0(xi) are constant with respect to \u0398. Therefore, the Fisher vector \u03a6\u0398(x) is a linear function of \u0398. This simple form of \u03a6\u0398(x) significantly facilitates our further kernel learning.\n3.1.2 Discriminative Fisher kernel learning via SVM As each Fisher vector is a function of the SGBN parameters, discriminatively learning these parameters can thus be converted to learning discriminative Fisher kernels. We require that the learned SGBN models possess the following properties. Firstly, the Fisher vectors induced by the learned SGBN model should be well separated between classes. Secondly, the learned SGBN models should maintain reasonable capacity of representation. Thirdly, the learned SGBN models should not violate DAG. We use the following strategies to achieve our goal. Firstly, to obtain a discriminative Fisher kernel, we jointly learn the parameters of SGBN and the separating hyper-plane of SVMs with Fisher kernel. Radiusmargin bound, the upper bound of the Leave-One-Out error, is minimized to keep good generalization of the SVMs. Secondly, to maintain reasonable representation, we explicitly control the fitting (regression) errors of the learned model during optimization. Recall that GBN learns the network by minimizing the regression errors of each node over its parent nodes. Thirdly, we enforce the DAG constraint to ensure the validity of the graph. Our method is developed as follows. In order to use radius-margin bound, L2-SVM with soft margin is be employed [34]3, which optimizes\nmin w,\u03be\n1 2 \u2016w\u201622 + C\u03be\u22a4\u03be (3.3)\ns.t. yi(w \u22a4\u03a6(xi) + b) \u2265 1\u2212 \u03bei, \u03bei \u2265 0, \u2200i.\nFollowing the convention in SVMs, w is the normal of the separating plane, b the bias term, \u03be the slack variables and C the regularization parameter. Here yi is the class label of the i-th sample. L2-SVM can be rewritten as SVM with hard margin by slightly modifying the kernel\n3. Radius-margin bound is rooted in hard-margin SVM. L2-SVM with soft-margin can be rewritten as SVM with hard margin.\n6 K := K + I/C, where I is identity matrix. For convenience, in the following, we redefine w := [w\u22a4 \u221a C\u03be\u22a4]\u22a4 and \u03a6(xi) := [\u03a6 \u22a4(xi) e \u22a4 i yi/ \u221a C]\u22a4. The vector ei has the value of 1 at the i-th element, and 0 elsewhere.\nIncorporating radius information leads to solving\nmin w\n1 2 R2\u2016w\u201622 (3.4)\ns.t. yi(w \u22a4\u03a6(xi) + b) \u2265 1, \u2200i,\nwhere R2 denotes the radius of Minimal Enclosing Ball (MEB). It has been observed that when the sample size is small, the estimation of R2 may become noisy and unstable [35]. Therefore, it has been proposed to use the trace of the total scatter matrix instead for such cases [35], [36]. We finally solve the following optimization problem:\nmin \u03b8,w\n1 2 tr(ST )\u2016w\u201622 (3.5)\ns.t. yi(w \u22a4\u03a6\u0398(xi) + b) \u2265 1, \u2200i\nh(X1,\u03981) \u2264 T1, h(X2,\u03982) \u2264 T2, \u03981 \u2208 DAG, \u03982 \u2208 DAG.\nHere tr(ST ) is the trace of the total scatter matrix ST , where ST = \u2211n i=1(\u03a6(xi) \u2212 m)(\u03a6(xi) \u2212 m)\u22a4, and m is the mean of total n samples in the kernel-induced space. It can be shown that tr(ST ) = tr(K) \u2212 1\u22a4K1/n, where 1 denotes a vector whose elements are all 1, and K the kernel matrix. Fisher vector \u03a6\u0398(xi) is obtained as in Section 3.1.1. The function h(\u00b7) measures the squared fitting errors of the corresponding SGBNs for the data X1 and X2 from the two classes. It is defined as\nh(X,\u0398) = m \u2211\ni=1\n\u2016x:,i \u2212PA\u22a4i \u03b8i\u201622. (3.6)\nThe two user-defined parameters T1 and T2 explicitly control the degree of fitting during the learning. Adding these constraints also avoids the scaling problem of \u0398.\nThe DAG constraint in H-SGBN could be employed to enforce the validity of the graph. However, here we adopt a new DAG constraint proposed in Section 4 due to its advantages over that of H-SGBN. The new DAG constraint employs a set of topological ordering variables (o,\u03a5) to guarantee DAG. It is a bilinear function of the ordering variables (o,\u03a5) and the SGBN parameters \u0398. An elaboration is given in Section 4. At the moment, let us temporarily skip the details of this DAG constraint and concentrate on the discriminative learning.\nOne possible approach for solving Eqn. (3.5) is to alternately optimize the separating hyperplane w and the parameter \u0398. That is,\nmin \u0398,o,\u03a5 J(\u0398) (3.7)\ns.t. h(X1,\u03981) \u2264 T1, h(X2,\u03982) \u2264 T2, \u03981 \u2208 DAG(o1,\u03a51), \u03982 \u2208 DAG(o2,\u03a52).\nAlgorithm 1 KL-SGBN: Discriminative Learning\nInput: data X1,X2 \u2208 Rn\u00d7m, label y \u2208 Rn\u00d71 Denote \u0398 = [\u03981,\u03982]\nInitialize \u0398(0),o(0),\u03a5(0) by Algorithm 3 for each class.\nLet \u0398(t\u22121) = \u0398(0), o(t\u22121) = o(0), \u03a5(t\u22121) = \u03a5(0) repeat\n1. Compute \u03a6 (t\u22121) \u0398 and K (t\u22121) \u0398 by Eqn. (3.2) 2. Compute tr(ST ) (t\u22121) = tr(K (t\u22121) \u0398 )\u2212 1\u22a4K(t\u22121) \u0398 1/n\n3. Solve J0(\u0398 (t\u22121)) and \u03b1\u22c6 by Eqn. (3.9) 4. J(\u0398(t\u22121)) = J0(\u0398 (t\u22121))\u00d7 tr(ST )(t\u22121) 6. Minimize Eqn. (3.7) with \u03b1\u22c6 and obtain \u0398(t): 6.1 Let o = o(t\u22121),\u03a5 = \u03a5(t\u22121), solve \u0398(t) by Eqn. (3.7); 6.2 Let \u0398 = \u0398(t), solve o(t),\u03a5(t) by Eqn. (4.2). 7. Let \u0398(t\u22121) = \u0398(t), o(t\u22121) = o(t), \u03a5(t\u22121) = \u03a5(t)\nuntil convergence/max number of iterations\nOutput: \u0398\u22c6 = \u0398(t)\nwhere\nJ(\u0398) = min w\n1 2 tr(ST )\u2016w\u201622 (3.8)\ns.t. yi(w \u22a4\u03a6\u0398(xi) + b) \u2265 1, \u2200i.\nNote that for a given \u0398, the term tr(ST ) is constant in Eqn. (3.8). Due to the strong duality in SVM optimization, we solve the term \u2016w\u201622 by\nJ0(\u0398) = max \u03b1\nn \u2211\ni=1\n\u03b1i \u2212 1\n2\nn \u2211\ni=1\nn \u2211\nj=1\nyiyj\u03b1i\u03b1jK\u0398(xi,xj)\n(3.9)\ns.t.\nn \u2211\ni=1\n\u03b1iyi = 0, \u03b1i \u2265 0 \u2200i,\nwhere \u03b1i is the Lagrangian multiplier and K\u0398(xi,xj) = \u3008\u03a6\u0398(xi),\u03a6\u0398(xj)\u3009. As mentioned above, the DAG constraint is a bilinear function of (o,\u03a5) and \u0398. Many quadratic programming packages could be used to solve Eqn. (3.7). We use fmincon-SQP (sequential quadratic programming) in Matlab. Gradient information is required by many optimization algorithms (including fmincon-SQP) to speed up the line search. It is not difficult to find that the gradient ofK\u0398(xi,xj) is just a linear function of\u0398, making the evaluation of gradient \u2207\u0398J easy. Our learning process is summarized in Algorithm 1."}, {"heading": "3.2 Proposed Max-margin-based Discriminative Learning (MM-SGBN)", "text": "KL-SGBN introduces group discrimination into SGBNs by optimizing the performance of SVM classifiers with\n7 SGBN-induced features. Although this leads to a relatively simple optimization problem, optimizing the performance of SVMs does not necessarily imply optimizing the discrimination of SGBNs. We believe that, the discrimination of SGBNs can be further improved if we directly optimize their (instead of SVMs\u2019) classification performance. Therefore we propose a new learning framework based on max-margin formulation directly built on SGBNs. We call this method MM-SGBN. For binary classification, maximizing the minimum margin between two classes can be obtained by maximizing the minimum conditional likelihood ratio (MCLR) [18]:\nMCLR(\u0398) = n\nmin i=1 P (yi|xi,\u0398yi) P (y\u0304i|xi,\u0398y\u0304i) ,\nWithout loss of generality, yi and y\u0304i \u2208 {\u22121, 1}, representing the true and false labels for the i-th sample, respectively. The parameter \u0398yi = \u03981 if yi = 1, or \u0398yi = \u03982 if yi = \u22121. We can see that MCLR identifies the most confusing sample whose probability of the true class assignment is close to or even less than that of the false class assignment. Hence, maximizing MCLR targets the maximal separation of the most confusing samples in the two classes. It is not difficult to see that MCLR can naturally handle multi-class case when replacing the denominator by the maximal probability induced by all false class assignments. Let \u0398 = [\u03981,\u03982]. Taking loglikelihood of MCLR, we have\nlog MCLR(\u0398)\n= n\nmin i=1 (log p(xi|yi,\u0398yi)\u2212 log p(xi|y\u0304i,\u0398y\u0304i)) + const, (3.10)\nwhere the prior probabilities of P (yi) and P (y\u0304i) that are irrelevant to \u0398 are absorbed into the constant term. Eqn. (3.10) can be shown to be a quadratic function of \u0398 in the case of SGBN. In order to maximize MCLR, we require the difference of log-likelihood function in Eqn. (3.10) be larger than a margin for all samples, r, and maximize the margin r. To deal with hard separations, we employ a soft margin formulation as follows.\nmin \u03981,\u03982,\u03bei,r,o,\u03a5 \u03bb\nn \u2211\ni=1\n\u03bei \u2212 r (3.11)\ns.t. yi (L(\u03981,xi)\u2212 L(\u03982,xi)) \u2265 r \u2212 \u03bei, \u2200i (3.11a) \u03bei \u2265 0, r \u2265 0, (3.11b) h(X1,\u03981) \u2264 T1, h(X2,\u03982) \u2264 T2 (3.11c) \u03981 \u2208 DAG(o1,\u03a51), \u03982 \u2208 DAG(o2,\u03a52) (3.11d)\nThe constraints in (3.11a) enforce the likelihood of xi to its true class larger than that to its false class by a margin r. The variables \u03bei are slack variables indicating the intrusion of the margin. The function L(\u00b7) denotes the loglikelihood, defined in Eqn. (3.1). We require L(\u03981,xi) larger than L(\u03982,xi) when yi = 1, and L(\u03982,xi) larger than L(\u03981,xi) when yi = \u22121.\nAlgorithm 2 MM-SGBN: Discriminative Learning\nInput: data X1,X2 \u2208 Rn\u00d7m, label y \u2208 Rn\u00d71 Denote \u0398 = [\u03981,\u03982]\nInitialize \u0398(0),o(0),\u03a5(0) by Algorithm 3 for each class.\nFix \u0398 = \u0398(0) and estimate r(0) and \u03be (0) i by Eqn. (3.11)\nonly with the two constraints (3.11a) and (3.11b). Initialize t = 1. repeat Step 1: Fixing o = o(t\u22121) and \u03a5 = \u03a5(t\u22121), optimize Eqn. (3.11) with the constraints (3.11a \u223c 3.11c) to update \u0398(t), r(t) and \u01eb\n(t) i ;\nStep 2: Fixing \u0398(t), optimize Eqn. (4.2) to update o(t) and \u03a5(t) to enforce DAG. Let t = t+ 1\nuntil convergence/max number of iterations\nOutput: \u0398\u22c6 = \u0398(t)\nThe constraints in (3.11c) control the fitting errors, same to that used in KL-SGBN, and the function h(\u00b7) is defined in Eqn. (3.6).\nThe constraints in (3.11d) are the DAG constraint proposed in Section 4, Eqn. (4.1). To enforce the validity of DAG on both graphs, we introduce a set of order variables o = {o1, o2, \u00b7 \u00b7 \u00b7 , om} and \u03a5 for each class separately, and employ the constraints stated in Eqn. (4.1). Please refer to Eqn. (4.1) for details.\nThe optimization in Eqn. (3.11) can be solved iteratively by optimizing (\u0398, \u03bei, r) and (o,\u03a5) alternately, as summarized in Algorithm 2. In Step 1, we solve a linear objective function with n non-convex and two convex quadratic constraints by fmincon-SQP (sequential quadratic programming) in Matlab. In Step 2, we solve the linear programming by the package of CVX4.\nIt is worthy noting that, we learn an SGBN model for each individual class in order to meet the requirement of both interpretation and discrimination in exploratory research. For example, each SGBN may model the brain network of the healthy or the diseased class, as well as carrying the essential class discrimination. Both the network modelling and the discrimination are of interest in such cases. Our method is different from the conventional BN classifers [16], [17], [18] that solely focus on classification. In those methods, only a single BN is learned to reflect the \u201cdifference\u201d of the two classes. It does not model any individual class as our method does, and hence deviates from our purpose of both representing and discriminating brain networks. Moreover, the works in [16], [17], [18] cannot handle the continuous variables of brain imaging measures, and inherit the drawbacks of the traditional two-stage methods.\n4. http://cvxr.com/cvx/\n8"}, {"heading": "3.3 Discussion and Analysis", "text": "In the following, some issues regarding the two proposed discriminative learning frameworks are discussed.\nClassifiers. The proposed discriminative learning frameworks produce a set of jointly learned SGBN models, one for each class. Based on these SGBN models, two kinds of classifiers can be constructed, i.e., the SGBN classifier and the SVM classifier. The SGBN classifier categorizes a sample by comparing the sample\u2019s likelihood according to each SGBN model. The SVM classifier is trained by the sample-specific Fisher vectors induced from the SGBN models. These two classifiers are tightly coupled by the underlying SGBN models. Specifically, more discriminative SGBN models directly lead to a better SGBN classifier, and can provide discriminative Fisher vectors to SVM for better classification. Rooted in this relationship, both the KL-SGBN and the MMSGBN can improve the classification performance of these two classifiers simultaneously. Put simply, KLSGBN explicitly optimizes the SVM classifier and in turn implicitly improves the SGBN classifier; while MMSGBN explicitly optimizes the SGBN classifier, bringing an implicit improvement of the SVM classifier as well. When evaluating the discriminative power of the learned SGBN models by the SGBN classifier (a direct measurement), it is therefore expected that MM-SGBN can outperform KL-SGBN. However, KL-SGBN has some computational advantages and provides a new perspective to manipulate BN models, analyzed as follows.\nComputational Issues. Compared with KL-SGBN, MM-SGBN requires to solve more complicated optimization problems, which may become problematic when the number of training samples increase. Let us compare Eqn. (3.7) for KL-SGBN and Eqn. (3.11) for MMSGBN. For KL-SGBN, Eqn. (3.7) optimizes J(\u0398) with two convex quadratic constraints of data fitting and two DAG constraints, which are independent of the number of training samples n. The evaluation of J(\u0398) needs to solve an SVM-like problem in Eqn. (3.8), taking just n linear constraints of \u0398, which could be efficiently solved by off-the-shelf SVM packages. For MM-SGBN, in addition to the data fitting and DAG constraints as in Eqn. (3.7), the optimization problem in Eqn. (3.11) also has to satisfy n non-convex quadratic constraints. When n increases to a medium or large value, the optimization problem could be quite hard to solve.\nEdge Selection. In addition to the discriminative learning of SGBN, the employment of Fisher kernel in KL-SGBN also provides a new perspective of edge selection for GBN. As introduced in Section 3.1.1, applying Fisher kernel on GBN produces sample-specific feature vectors whose component is the gradient of the log likelihood, i.e., \u2202L(x|\u0398) \u2202\u0398ij . In other words, each feature now corresponds to an edge \u0398ij in the SGBN. This makes it possible to convert the SGBN edge selection to a more traditional feature selection problem that has been well studied and has a large body of options in\nthe literature. Edge selection has been employed in our work to deal with the \u201csmall sample size\u201d problem that is often encountered in medical applications. For example, it is common to have only 100 training samples but 3200 parameters (for SGBNs of 40 nodes from two classes) to learn in brain network analysis. To handle this issue, we keep using the whole \u0398 for computing K\u0398, but only optimize a selected subset \u0398s. There are many options to determine \u0398s. We just compute the Fisher vector \u03a6\u0398 for each sample, calculate the Pearson correlation between each component of \u03a6\u0398 and the class labels on the training data, and select the top \u03b8i with the highest correlations. To keep our problem simple, only the parameters associated with edges present in the graph are optimized to avoid the violation of DAG. It is remarkable that even this simple selection process has significantly improved the discrimination for both KL-SGBN and MM-SGBN. Note that this edge selection step is essentially different from that of the traditional two-stage methods. It is just an empirical method to handle the small sample size problem and will become unnecessary when sufficient training data are available. In contrast, identifying the candidate-parent sets is an indispensable step in two-stage methods to obtain computationally tractable solutions. Generalization. We would like to point out that our learning framework of KL-SGBN could be easily generalized. It could be used to discriminatively learn the parameters of distributions other than that represented by GBN by just simply switching GBN to the target distribution, such as Gaussian Mixture Model (GMM). Indeed, this has been seen in [32], after our work [25]. However, as shown in this paper, the Fisher vector of GBN is a linear function of the model parameters, which significantly simplifies the learning problem. This favorable property may not be guaranteed with other distributions, including GMM."}, {"heading": "4 PROPOSED DAG CONSTRAINT", "text": "In this section, we revisit H-SGBN and propose a new DAG constraint that could simplify the optimization problems in SGBN and its discriminative learning process as introduced in Sections 3.1 and 3.2."}, {"heading": "4.1 H-SGBN Revisited", "text": "Recall that, the DAG constraint in H-SGBN (Section 2.1) utilizes the matrix P, an implicit function of \u0398, which significantly complicates the optimization problem in Eqn. (2.1). In [14], for simplicity, in each optimization iteration, P is first treated as a constant while optimizing \u0398, and then recalculated by searching on the binarized new \u0398. This hard binarization could introduce high discontinuity of \u0398 into the optimization. Solving \u0398 column-wisely by BCD may mitigate this problem since only one column of \u0398 is changed in each iteration, inducing less discontinuity. However, we observe that the solution of BCD depends on which column of \u0398\n9 to be optimized first. In other words, if we randomly permute the ordering of features (the columns in X), we will obtain different SGBNs, which impairs the interpretability of the SGBN model. The optimization ordering matters because the matrix P used in the DAG constraint changes with the ordering. This problem has been demonstrated in our experiment. Moreover, we find experimentally that if P is solved as a whole instead of BCD, the optimization in Eqn. (2.1) will not converge but oscillate between some non-DAG solutions, possibly due to the high discontinuity mentioned above 5. Early stop cannot help because no premature solution satisfies DAG. These optimization difficulties motivate our work of proposing a new DAG constraint that is much simpler for SGBN, as described below."}, {"heading": "4.2 Proposed DAG constraint", "text": "It is known that, a BN is equivalent to a topological ordering (Page 362 in [37]). Therefore, we propose a new DAG constraint applicable to continuous variables with GBN based on this equivalence. With a few linear inequalities and variables separable from \u0398, the new DAG constraint significantly simplifies that used in [14]. Specifically, given a directed graph G and the parameters \u0398, a real-valued order variable oi is assigned to each node i, where 0 \u2264 oi \u2264 \u2206, and\u2206 is a predefined arbitrary positive number. We propose a sufficient and necessary condition for G to be DAG as in Proposition 1. Proposition 1. Given a sparse Gaussian Bayesian Network parameterized by \u0398 and its associated directed graph G with m nodes, the graph G is DAG if and only if there exist some oi (i = 1, \u00b7 \u00b7 \u00b7 ,m) and \u03a5 \u2208 Rm\u00d7m, such that for arbitrary \u2206 > 0, the following constraints are satisfied:\n(4.1)\noj \u2212 oi \u2265 \u2206\nm \u2212\u03a5ij , \u2200i, j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, i 6= j (4.1a)\n\u03a5ij \u2265 0, (4.1b) \u03a5ij \u00d7\u0398ij = 0, (4.1c) \u2206 \u2265 oi \u2265 0. (4.1d)\nEqn.(4.1) leads to a topological ordering equivalent to DAG. The topological ordering means that if node j comes after node i in the ordering (oj > oi), there cannot be a link from node j to node i, which guarantees the acyclicity. The proof of Proposition 1 is given in Appendix. By Proposition 1, we remove the awkward hard binarization for computing P in [14]. The inequalities of (4.1a, 4.1b, 4.1d) are linear to the ordering variables oi and \u03a5. The equation (4.1c) differs from the equation \u0398ji \u00d7 Pij = 0 in [14] in that the variable \u03a5ij is now separable from \u0398ij (while Pij is not) and does not require the binarization of \u0398. This makes it tractable to\n5. Please note that, solving \u0398 column-wisely without updating P in each iteration will only lead to non-DAG solutions\nsolve \u0398 as a whole instead of BCD (to avoid the feature ordering problem). It is worth noting that, provided \u0398 is sparse, the number of constraints in Eqn. (4.1) could be significantly reduced. As can be seen, for any \u0398ij = 0, as long as we set the corresponding \u03a5ij an arbitary value greater than ( 1\nm + 1)\u2206, all the conditions in Eqn. (4.1) will\nbe automatically satisfied. Therefore, we only need to consider the constraints related to \u0398ij 6= 0. The idea of topological ordering is also used to design DAG constraint for the discrete variables in [38]. However, the work in [38] addresses the multinominal distribution of discrete variables, while here we target the Gaussian distribution of continuous variables. It is worthy noting that the constraint in [38] has to predefine candidate parent-node sets. Therefore, it inherits the drawbacks of the two-stage methods as pointed out in Section 1. This has been circumvented in our proposed DAG constraint for SGBN."}, {"heading": "4.3 Estimation of SGBN from A Single Class", "text": "With our DAG constraint proposed in Eqn. (4.1), we could estimate SGBN from a single class as the initial solution to our discriminative learning of KL-SGBN or MM-SGBN. In particular, we optimize\nmin \u0398,o,\u03a5\nm \u2211\ni=1\n\u2016x:,i \u2212PA\u22a4i \u03b8i\u201622 + \u03bb1\u2016\u03b8i\u20161 + \u03bbdag\u01ebi\u22a4|\u03b8i| (4.2)\ns.t. oj \u2212 oi \u2265 \u2206\nm \u2212\u03a5ij , \u2200i, j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, i 6= j\n0 \u2264 oi \u2264 \u2206, \u03a5ij \u2265 0,\nwhere \u01ebi is the i-th column of the matrix \u03a5, and |\u03b8i| the component-wise absolute value of \u03b8i. This optimization problem is solved in an iterative way with two alternate steps in each iteration: i) optimize o and\u03a5 (with\u0398 fixed) and ii) optimize \u0398 (with o and \u03a5 fixed). This process is repeated until convergence. We call this proposed method OR-SGBN (Algorithm 3). When the coefficient \u03bbdag is sufficiently large, the alternate optimization strategy of Eqn. (4.2) will converge to a DAG solution, as shown in Proposition 2 in Appendix. In practice, for numerical stability, we adopt a \u201cwarm start\u201d strategy as in [14], that is, to gradually increase the values of \u03bbdag until the resulting G becomes DAG. Specifically, we use a set of values of \u03bbdag: \u03bb (1) dag < \u03bb (2) dag < \u00b7 \u00b7 \u00b7 < \u03bb(M)dag to solve Eqn. (4.2) (Algorithm 3). We use a bias variable x0 = 1 in the regression model to improve data fitting, thus xi = [\u03b8i \u03b80] \u22a4[Pai 1] + \u03b5i (i > 1). In the following part, we denote \u03b8i , [\u03b8i \u03b80] and Pai , [Pai 1]. The bias term \u03b80 is learned together with other \u03b8i. This equals to introducing a bias node into the graph. It has no parent but is the parent of all the other nodes. If the original graph is a DAG, this does not cause the violation of DAG. It is interesting yet challenging to analyze the network consistency of OR-SGBN. It is noted that Eqn. (4.2)\n10\nAlgorithm 3 OR-SGBN: SGBN from a single class\nInput: data X \u2208 Rn\u00d7m\nInitialize \u0398(0) by least square fitting. Initialize o(0) and \u03a5(0) by solving Eqn. (4.2) with \u0398 = \u0398(0). Let T = 1. repeat Fixing \u03a5 = \u03a5(T\u22121) and o = o(T\u22121). Let t = 1, \u0398(T\u22121,t=0) = \u0398(T\u22121). for \u03bbdag = \u03bb (1) dag to \u03bb (M) dag do\nOptimize Eqn. (4.2) with the initial solution \u0398(T\u22121,t\u22121) to obtain \u0398(T\u22121,t). Let t=t+1. end for Let \u0398(T ) = \u0398(T\u22121,M). Fixing \u0398(T ), optimize Eqn. (4.2) to update o(T ) and \u03a5(T ) to enforce DAG. Let T = T + 1.\nuntil convergence/max number of iterations\nOutput: \u0398\u22c6 = \u0398(T )\ncan be reorganized into a weighted LASSO problem, which can be conceptually linked to \u201cadaptive LASSO\u201d in the literature [39], [40], [41]. The analysis framework provided by these works is suggestive of promising strategies to analyze the network consistency for L1-penalized Gaussian networks. However, a complete treatment of this analysis for OR-SGBN requires a deep investigation. Considering the significant amount of the required workload and its importance, we will explore this problem in a separate paper in our future work."}, {"heading": "5 EXPERIMENT", "text": "In this section, we investigate the properties of our proposed methods from three aspects: the DAG constraint, the discriminative learning process, and the resulting connectivity for brain network analysis. Four experiments are conducted, summarized in Table 2. The data sets and the experiments are elaborated as follows."}, {"heading": "5.1 Neuroimaging Data Sets", "text": "We conduct our experiment on the publicly accessible ADNI [42] database to analyze brain effective connectivity for the Alzheimer\u2019s disease. Three data sets are used from two imaging modalities of MRI and FDG-PET downloaded from ADNI. They are elaborated as follows.\nMRI data set includes 120 T1-weighted MR images belonging to 50 mild cognitive impairment (MCI) patients and 70 normal controls (NC). These images are preprocessed by the typical procedure of intensity correction, skull stripping, and cerebellum removal. We segment the images into gray matter (GM), white matter (WM), and\ncerebrospinal fluid (CSF) using the standard FSL6 package, and parcellate them into 93 Region of Interest (ROI) based on an ROI atlas [43] after spatial normalization. The GM volume of each ROI is used as the imaging feature to characterize each network node. Forty ROIs are included in this study, following [14]. They have higher correlation with the disease and are mainly located in the temporal lobe and subcortical region. Studying brain morphology as a network can take the advantage of statistical tools from graph theory. Moreover, it has been reported that the covariation of gray matter morphology might be related to the anatomical connectivity [44]. PET data set includes 103 FDG-PET images (and their corresponding MR images) of 51 AD patients and 52 NC. The MR images belonging to different subjects are co-registered and partitioned into ROIs as before. The ROI partitions are copied onto their corresponding PET images by a rigid transformation. The average tracer uptakes within each ROI is used as the imaging feature to characterize each network node. Forty ROIs discriminative to the disease are used in the study. The retention of tracer in FDG-PET is analogous to the glucose uptake, thus reflecting the tissue metabolic activity. MRI-II data set is similar to the MRI data set but using 40 different ROIs covering the typical brain regions spread over the frontal, parietal, occipital and temporal lobes. We randomly partition each data set into 30 groups of training-test pairs. Each group includes 80 training and 40 test samples in MRI and MRI-II, or 60 training and 43 test samples in PET."}, {"heading": "5.2 DAG Constraint", "text": "With our proposed DAG constraint, the SGBN model for an individual class can be learned with all the parameters \u0398 optimized together (OR-SGBN), instead of column-wisely as did in [14], [25], [26]. To explore the properties of our DAG constraint, we test three experimental configurations, namely, OR-SGBN (WHOLE), H-SGBN (BCD) and H-SGBN (WHOLE). The word in the parenthesis is used to explicitly indicate whether the parameters \u0398 are optimized together (WHOLE) or column-wisely (BCD). OR-SGBN (WHOLE) is our SGBN learning method for a single class in Algorithm 3, implemented with the package of CVX. H-SGBN (BCD) is the column-wise method in [14] and implemented with the code downloaded from the authors\u2019 website. HSGBN (WHOLE) is our attempt to optimize \u0398 together for the objective function of H-SGBN in [14], which is implemented with the package of CVX7. The same \u0398 that is computed by a sparse least square fitting of the training set is provided to all the methods to initialize the optimizations. The \u201cwarm-start\u201d strategy is applied wherever applicable in all methods.\n6. http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/ 7. The optimization problem is solved by a series of convex sub-\nproblems.\n11\nTABLE 2 Summary of Experiment Purpose\nExperiment Test Subject Purpose\nExp-I (Sec. 5.2) DAG constraint Test the invariance of solution to feature ordering\nExp-II (Sec. 5.2) DAG constraint Test the ability of network structure recovery\nExp-III (Sec. 5.3) discriminative learning Test the improvement of discriminative power of SGBN models\nExp-IV (Sec. 5.4) brain network analysis Investigate the learned brain connectivity patterns\nIt is found that when solving all \u0398 as a whole, HSGBN (WHOLE) that uses the DAG constraint in [14] does not converge: the optimization is trapped to oscillate between a few solutions that are not DAG. Therefore, from now on, we only consider H-SGBN (BCD) and OR-SGBN (WHOLE). Exp-I. In this experiment, we compare the solutions of OR-SGBN (WHOLE) and H-SGBN (BCD) with respect to the change of feature ordering. To do that, for the neuroimaging data sets, we randomly permute the feature ordering for 100 times. The estimated \u0398 of the resulting 100 SGBNs are re-arranged according to the initial feature ordering and then averaged as in Fig. 2. As shown, the averaged result from OR-SGBN (WHOLE) (Fig. 2 (d)) is almost identical to the result using the original feature ordering (Fig. 2 (c)), reflecting its robustness to feature ordering. In contrast, H-SGBN (BCD) generates SGBNs with large variations when the feature ordering changes ((Fig. 2 (a) versus (b)). To give a quantitative evaluation, the Euclidean distance and the correlation between the averaged \u0398 and the original \u0398 are presented in Table 3. Consistently, the solutions from OR-SGBN (WHOLE) are much less affected by the ordering permutation, indicating the advantage of solving \u0398 as a whole via the proposed DAG constraint.\nExp-II. In this experiment, we test the ability of ORSGBN (WHOLE) at identifying network structures from data. Since no ground-truth is available for the three neuroimaging data sets due to the unknown mechanism of the disease, we conduct experiments on nine benchmark network data sets mostly coming from the Bayesian Network Repository [45] as was done in the literature [12], [46]. The nine benchmark data sets are: Factors (27 nodes, 68 arcs), Alarm (37 nodes, 46 arcs),\nBarley (48 nodes, 84 arcs), Carpo (61 nodes, 74 arcs), Chain (7 nodes, 6 arcs), Hailfinder (56 nodes, 66 arcs), Insurance (27 nodes, 52 arcs), Mildew (35 nodes, 46 arcs) and Water (32 nodes, 66 arcs). We compare the OR-SGBN (WHOLE) with another eight BN learning methods, including L1MB [12], GS [47], TC and its variant TCbw [13] and three variants of IAMB [48]. The experiment is repeated for 50 simulations. In each simulation, for each network, we randomly sample 1000 samples from \u00b1Uniform(0.5, 1) for the regression coefficients of each variable on its parents. The parameters of the eight methods to be compared are set according to [14]. A predefined \u03bb that controls the sparsity of OR-SGBN is uniformly applied to all the nine data sets, which simply brings the number of the resulting edges to a reasonable range 8. We use the first stage estimate of L1MB as the initial solution of OR-SGBN. Table 4 shows the total numbers of mis-identified edges (including both the false and the missing edges), while Table 5 shows the numbers of falsely identified edges (false positive).\n8. The Bayesian Information Criterion is used to select \u03bb in [14]. However, it did not behave well in our experiment.\n12\nIn addition, Table 6 lists the numbers of falsely identified PDAG structures. PDAG structures are statistically indistinguishable structures, i.e., representing the same statistical dependency. The PDAG of BN is obtained by the method in [49]. From Tables 4 \u223c 6, it can be seen that OR-SGBN shows significantly smaller errors on six data sets (Factors, Alarm, Barley, Carpo, Hailfinder and Insurance) in identifying both edges and PDAG structures. For the data sets of Mildew and Water, ORSGBN performs similarly to the other methods. It only performs relatively inferior on Chain. This experiment demonstrates that the proposed DAG constraint for SGBN can perform effectively for BN structure identification. Its relatively low risk of mis-edge identification is a favorable property for exploratory research."}, {"heading": "5.3 Comparison of Discrimination", "text": "After testing the effectiveness of the proposed DAG constraint, we now investigate the theme of this paper: the discriminative learning frameworks. We consider two kinds of classifiers: i) the SGBN classifier (with two SGBN models, one for each class), and ii) the SVM classifier learned by the Fisher vectors induced from the SGBN models. Exp-III. In this experiment, we test whether our learning methods (KL-SGBN and MM-SGBN) can improve the discriminative power on both kinds of classifiers for the real neuroimaging data sets. The initial SGBNmodels are obtained by our proposed OR-SGBN (WHOLE), since it has been shown more robust to feature ordering than H-SGBN as above. For the SGBN classifier, assuming equal prior, we assign a test sample to the class with a higher likelihood. For the SVM classifier, we use L2-SVM with Fisher kernels. In order to maintain representation capability, we allow maximal 1% additional squared fitting errors (that is, Ti = 1.01 \u00d7 Ti0, (i = 1, 2), where Ti0 is the squared fitting error of the initial solution) to be introduced during the learning process of KL-SGBN or MM-SGBN. The test accuracies are averaged over the 30 randomly partitioned training-test groups. The classification performances of SGBN and SVM classifiers are evaluated with the varied parameter \u03bb that controls the sparsity level and the number of edges optimized in the learning process in Fig. 3. The results of our proposed KL-SGBN and MM-SGBN are plotted by the green and the red lines, respectively. The results of the individually learned OR-SGBN and H-SGBN are plotted by the blue and the black lines, respectively. The top two rows in Fig. 3 correspond to the results from the SGBN classifiers, while the bottom two rows correspond to those from the SVM classifiers. From Fig. 3, we have the following observations. i) Both KL-SGBN and MM-SGBN can significantly improve the discriminative power of the individually learned SGBNs (Fig. 3, the top two rows), as well as their associated SVM classifiers (Fig. 3, the third row).\nSuch improvements are consistent over the three neuroimaging data sets and different parameter settings, and could reach the significant increases of 10% \u223c 20% on most occasions. When the network becomes more sparse, the classification performance of the individually learned SGBNs (H-SGBN and OR-SGBN) drops significantly possibly due to the insufficient modeling of data. However, under such circumstances, KL-SGBN andMMSGBN can still maintain high classification accuracies, which may indicate the necessity and effectiveness of the discriminative learning in classification scenarios. ii) When using SGBN classifiers, for all the three data sets, MM-SGBN consistently achieves higher test accuracies at all sparsity levels (Fig. 3, the first row) with different numbers of optimized edges than KLSGBN (Fig. 3, the second row). The advantage of MMSGBN over KL-SGBN comes from explicitly optimizing the discriminative power of SGBNs, instead of getting help from optimizing the performance of SVM on SGBNinduced features. iii) When using SVM classifiers, the SVM built upon KL-SGBN-induced features performs slightly better than that built upon MM-SGBN-induced features at all sparsity levels (Fig. 3, the third row). This is expected since KL-SGBN optimizes the performance of its associated SVM classifier. iv) When cross-referencing the first and the third rows in Fig. 3, it is noticed that SVM classifiers in general perform worse than the discriminatively learned SGBN classifiers. These may be because our Fisher vectors have very high dimensionality, which causes serious overfitting of data in SVM classifiers. Such situation might be somewhat improved for SGBN-classifiers since the simple Gaussian model may \u201cregularize\u201d the model fitting. Based on this assumption, we further select a number of leading features from Fisher vectors by computing the Pearson correlation of the features and the labels, and use the selected features to construct the Fisher kernel for the SVM classifiers. As shown in the fourth row of Fig. 3, the simple feature selection step can further significantly improve the classification performance of the Fisher-kernel based SVM. v) The individually learned OR-SGBN and H-SGBN perform similarly for classification. However, as mentioned above, OR-SGBN has an additional advantage of being invariant to the feature ordering. vi) Recall that these improvement on discrimination are achieved with no more than 1% increase of squared fitting errors, which is explicitly controlled through the user defined parameters T1 and T2. Note that the rate of 1% is application dependent. More tolerance of fitting errors can potentially bring more discrimination."}, {"heading": "5.4 Comparison of Connectivity", "text": "We also conduct an investigation to gain some insights into the learned brain networks for the diseased and the healthy populations, respectively.\n13\nExp-IV. In this experiment, we visualize the learned brain networks and compare the connectivity patterns obtained by different methods and from different populations. MRI-II data set is used for this study since it covers regions spread over the four lobes of brain. The structures of the brain networks recovered from NC and MCI groups are displayed in Fig. 4 by using H-SGBN (BCD) and OR-SGBN (WHOLE), respectively. The network structure is obtained by thresholding the edge weights \u0398 with a cutoff value of 0.01 [14]. Each row i represents the effective connections (dark dots) starting from the node i, and each column j represents the effective connections ending at the node j. Note that, due to the different optimization problems involved, the same parameter \u03bb leads to different sparsity levels for H-SGBN and OR-SGBN. However, for a given method, different \u03bb values do not change the major structures of the resulting networks. In Fig. 4, it is noticed that H-SGBN (BCD) usually generates more connections in the upper triangle of the graphs even when we randomly permute the nodes. We suspect that this is caused by the column-wise optimization. The parameters \u03b8i (corresponding to the columns in the graph) optimized at the early stage tend to be made more sparse than those optimized later in order to satisfy the DAG constraint. This phenomenon is not observed in OR-SGBN (WHOLE) that is used to initialize the discriminative learning. Let us focus on OR-SGBN. Compared with H-SGBN, OR-SGBN has an additional bias node corresponding to the last row and column in Fig. 4. Visualizing \u0398 can provide rich information for medical analysis. Here we just list a few observations as examples. With the same \u03bb, OR-SGBN produces 183 edges for NC, and 145 edges for MCI. Such loss of connectivity also happens at the temporal lobe (24% loss) for MCI. The temporal lobe (and some subcortical structures) is known to play a very important role in the progression of AD. The loss of connectivity in this region has been well-documented in wide AD-related studies [50], [51], [14]. In Fig. 4, we also observe an increase of connectivity (the left bottom corner in the figure) between the frontal and the temporal\n14\nlobe in MCI. Some study [52] mentioned that the frontal lobe may have connectivity increase at the early stage of AD as a compensation of cognitive functions for the patients. Moreover, significant directionality changes are also found for the left (node 35) and the right (node 38) hippocampi, an important structure among the earliest ones affected by AD. Both hippocampi have reduced incoming connectivity (communications dominated by other nodes) but increased outgoing connectivity (communications dominated by themselves) in MCI. Please\n15\nnote that the above observations could be influenced by the factors such as the limited number of data, the degree of disease progression and the imaging modality used in this study. More reliable medical analysis should be validated on larger data sets and worth further exploration, which is, however, beyond the scope of this paper.\nTo illustrate the difference of edge weights learned by KL-SGBN and MM-SGBN, an example of 30 edge weight changes (from the initial OR-SGBN) learned by these two methods is given in Fig. 5, where the SGBN networks from the two classes are vectorized and concatenated as x-axis. As shown, the signs of weight changes are quite similar in both methods. The most significant difference is that, MM-SGBN gives negative weight changes to the bias node of the left Amygdala and the right Parahippocampus (red lines in Fig. 5) while KL-SGBN gives them positive weight changes. The adjustment of edge weights leads to 10% increase of test accuracy for MMSGBN in this example."}, {"heading": "6 CONCLUSION", "text": "In this paper, we focus on the discriminative learning of Bayesian network for continuous variables, especially for neuroimaging data. Two discriminative learning frameworks are proposed to achieve this goal, i.e., KL-SGBN improves the performance of SVM classifiers based on SGBN-induced features, and MM-SGBN explicitly optimizes an SGBN-based criterion for classification. We demonstrate how to utilize Fisher-kernel to bridge the generative methods of SGBN and the discriminative classifiers of SVM, and how to embed the max-margin\ncriterion into SGBN for discriminative learning. The optimization problems are analyzed in details, and the advantages and disadvantages of the proposed methods are discussed. Moreover, a new DAG constraint is proposed to ensure the validity of the graph with theoretical guarantee and validation on the benchmark data. We apply the proposed methods to modeling brain effective connectivity for early AD prediction. Significant improvements have been observed in the discriminative power of both the SGBN models and the associated SVM classifiers, without sacrificing much representation power."}, {"heading": "It holds that f(\u0398(t+1),o(t+1),\u03a5(t+1)) \u2264 f(\u0398(t+1),o(t),\u03a5(t)). Also, f(\u0398(t+1),o,\u03a5) is a linear function with respect", "text": "to o and \u03a5. Consequently we have\nf(\u0398(t+1),o(t+1),\u03a5(t+1)) \u2264 f(\u0398(t+1),o(t),\u03a5(t)) \u2264 f(\u0398(t),o(t),\u03a5(t)).\nTherefore, the optimization problem in Eqn. (2) is guaranteed to converge with the alternate optimization strategy, because the objective function is lower-bounded and monotonically decreases with the iteration numbers.\nSecond, we prove that when \u03bbdag > 2m(m\u22122)(n\u22121)2+m\u03bb1(2n\u22122\u2212\u03bb1) \u03bb1(1+m)\u2206 , the output \u0398\u22c6 is guaranteed to be DAG. This could be proven by contradiction. Suppose that such a \u03bbdag does not lead to a DAG, say, \u03a5ji \u00d7\u0398\u22c6ji 6= 0 for at least one pair of nodes i and j, with \u0398\u22c6ji 6= 0 and \u03a5ji > 0. Without loss of generality, we assume \u03a5ji \u2265 ( 1m +1)\u2206 (where \u2206 is an arbitary positive number), so the ordering constraints in Eqn. (2) always hold regardless of the variables oi and oj . This is because oi and oj are constrained by 0 \u2264 oi \u2264 \u2206 and 0 \u2264 oj \u2264 \u2206, and oj\u2212oi \u2265 \u2212\u2206 = 1m\u2206\u2212( 1m +1)\u2206. Based on the first-order optimality condition, \u0398\u22c6ji 6= 0 i.f.f. 2 \u2223 \u2223 \u2223\n\u2223\n( x:,i \u2212PA\u22a4i(\\j,:)\u03b8\u22c6i\\j )\u22a4 x:,j\n\u2223 \u2223 \u2223 \u2223\n\u2212 (\u03bb1 +\u03bbdag\u03a5ij) > 0. Here, PAi(\\j,:) denotes the elements in the matrix PAi with the j-th row removed (i.e., parents of the node i without the\n19\nnode j), and \u03b8\u22c6i\\j denotes the elements in \u03b8 \u22c6 i without \u0398 \u22c6 ji. However, it can be shown that,\n\u2223 \u2223 \u2223 \u2223 ( x:,i \u2212PA\u22a4i(\\j,:)\u03b8\u22c6i\\j )\u22a4 x:,j \u2223 \u2223 \u2223 \u2223 \u2264 \u2223 \u2223x\u22a4:,ix:,j \u2223 \u2223+ \u2223 \u2223 \u2223 \u03b8\u22c6 \u22a4i\\j PAi(\\j,:)x:,j \u2223 \u2223 \u2223 (5)\n= \u2223 \u2223x\u22a4:,ix:,j \u2223 \u2223+\nm \u2211\nk=1,k 6=i,j\n\u2223 \u2223\u0398\u22c6kix \u22a4 :,kx:,j \u2223 \u2223\n\u2264 (n\u2212 1) + (m\u2212 2)(n\u2212 1)max |\u0398\u22c6ki| \u2264 (n\u2212 1) + (m\u2212 2)(n\u2212 1) 2\n\u03bb1 .\nThe second last inequality holds due to the normalization of features x:,i (to zero mean and unit std). The last inequality holds because max |\u0398\u22c6ki| \u2264 \u2016\u03b8\u22c6i \u20161 \u2264 1\u03bb1 ( \u2016x:,i \u2212PA\u22a4i \u03b8\u22c6i \u201622 + \u03bb1\u2016\u03b8\u22c6i \u20161 + \u03bbdag\u01ebi\u22c6\u22a4|\u03b8\u22c6i | ) = 1 \u03bb1 f(\u0398\u22c6,o\u22c6,\u03a5\u22c6) \u2264\n1 \u03bb1 f(0,o\u22c6,\u03a5\u22c6) = 1 \u03bb1 x\u22a4:,ix:,i = n\u22121 \u03bb1 . With the given \u03bbdag , Eqn. (5) results in\n2\n\u2223 \u2223 \u2223 \u2223 ( x:,i \u2212PA\u22a4i(\\j,:)\u03b8\u22c6i\\j )\u22a4 x:,j \u2223 \u2223 \u2223 \u2223 \u2212 (\u03bb1 + \u03bbdag\u03a5ij) < 0,\nwhich contradicts the above first-order optimality condition with \u0398\u22c6ji 6= 0. Therefore, when \u03bbdag is sufficiently large, the output \u0398\u22c6 is guaranteed to be DAG.\nSumming up the proofs above, the alternate optimization of Eqn. (2) converges and the output \u0398\u22c6 is guaranteed to be DAG when \u03bbdag is sufficiently large."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Due to its causal semantics, Bayesian networks (BN) have been widely employed to discover the underlying data<lb>relationship in exploratory studies, such as brain research. Despite its success in modeling the probability distribution of variables,<lb>BN is naturally a generative model, which is not necessarily discriminative. This may cause the ignorance of subtle but critical<lb>network changes that are of investigation values across populations. In this paper, we propose to improve the discriminative power<lb>of BN models for continuous variables from two different perspectives. This brings two general discriminative learning frameworks for<lb>Gaussian Bayesian networks (GBN). In the first framework, we employ Fisher kernel to bridge the generative models of GBN and the<lb>discriminative classifiers of SVMs, and convert the GBN parameter learning to Fisher kernel learning via minimizing a generalization<lb>error bound of SVMs. In the second framework, we employ the max-margin criterion and build it directly upon GBN models to explicitly<lb>optimize the classification performance of the GBNs. The advantages and disadvantages of the two frameworks are discussed and<lb>experimentally compared. Both of them demonstrate strong power in learning discriminative parameters of GBNs for neuroimaging<lb>based brain network analysis, as well as maintaining reasonable representation capacity. The contributions of this paper also include<lb>a new Directed Acyclic Graph (DAG) constraint with theoretical guarantee to ensure the graph validity of GBN.", "creator": "LaTeX with hyperref package"}}}