{"id": "1506.07190", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2015", "title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "abstract": "dialog tracking state tracking is quite a key tool component of introducing many seemingly modern dialog systems, most of which are loosely designed with a specifically single, well - loosely defined domain in subconscious mind. this paper shows promising that dialog data drawn automatically from systematically different competing dialog domains can be used to directly train a general belief tracking model module which can operate across all members of currently these domains, exhibiting superior performance to each subject of a the domain - specific measurement models. we should propose a training procedure procedure which uses random out - of - domain data generated to initialise belief tracking models available for virtually entirely new domains. building this procedure leads today to improvements involved in belief path tracking performance overall regardless because of essentially the amount of in - domain data coverage available for training - the system model.", "histories": [["v1", "Tue, 23 Jun 2015 20:16:06 GMT  (926kb,D)", "http://arxiv.org/abs/1506.07190v1", "Accepted as a short paper in the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)"]], "COMMENTS": "Accepted as a short paper in the 53rd Annual Meeting of the Association for Computational Linguistics (ACL 2015)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["nikola mrksic", "diarmuid \u00f3 s\u00e9aghdha", "blaise thomson", "milica gasic", "pei-hao su", "david vandyke", "tsung-hsien wen", "steve j young"], "accepted": true, "id": "1506.07190"}, "pdf": {"name": "1506.07190.pdf", "metadata": {"source": "CRF", "title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "authors": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "emails": ["nm480@cam.ac.uk", "mg436@cam.ac.uk", "phs26@cam.ac.uk", "djv27@cam.ac.uk", "thw28@cam.ac.uk", "sjy@cam.ac.uk", "blaise}@vocaliq.com"], "sections": [{"heading": "1 Introduction", "text": "Spoken dialog systems allow users to interact with computer applications through a conversational interface. Modern dialog systems are typically designed with a well-defined domain in mind, e.g., restaurant search, travel reservations or shopping for a new laptop. The goal of building open-domain dialog systems capable of conversing about any topic remains far off. In this work, we move towards this goal by showing how to build dialog state tracking models which can operate across entirely different domains. The state tracking component of a dialog system is responsible for interpreting the users\u2019 utterances and thus updating the system\u2019s belief state: a probability distribution over all possible states of the dialog. This belief state is used by the system to decide what to do next.\nRecurrent Neural Networks (RNNs) are well suited to dialog state tracking, as their ability to capture contextual information allows them to model and label complex dynamic sequences (Graves, 2012). In recent shared tasks, approaches based on\nthese models have shown competitive performance (Henderson et al., 2014d; Henderson et al., 2014c). This approach is particularly well suited to our goal of building open-domain dialog systems, as it does not require handcrafted domain-specific resources for semantic interpretation.\nWe propose a method for training multi-domain RNN dialog state tracking models. Our hierarchical training procedure first uses all the data available to train a very general belief tracking model. This model learns the most frequent and general dialog features present across the various domains. The general model is then specialised for each domain, learning domain-specific behaviour while retaining the cross-domain dialog patterns learned during the initial training stages. These models show robust performance across all the domains investigated, typically outperforming trackers trained on targetdomain data alone. The procedure can also be used to initialise dialog systems for entirely new domains. In the evaluation, we show that such initialisation always improves performance, regardless of the amount of the in-domain training data available. We believe that this work is the first to address the question of multi-domain belief tracking."}, {"heading": "2 Related Work", "text": "Traditional rule-based approaches to understanding in dialog systems (e.g. Goddeau et al. (1996)) have been superseded by data-driven systems that are more robust and can provide the probabilistic dialog state distributions that are needed by POMDPbased dialog managers. The recent Dialog State Tracking Challenge (DSTC) shared tasks (Williams et al., 2013; Henderson et al., 2014a; Henderson et al., 2014b) saw a variety of novel approaches, including robust sets of hand-crafted rules (Wang and Lemon, 2013), conditional random fields (Lee and Eskenazi, 2013; Lee, 2013; Ren et al., 2013), maximum entropy models (Williams, 2013) and web-style ranking (Williams, 2014). ar X\niv :1\n50 6.\n07 19\n0v 1\n[ cs\n.C L\n] 2\n3 Ju\nn 20\n15\nHenderson et al. (2013; 2014d; 2014c) proposed a belief tracker based on recurrent neural networks. This approach maps directly from the ASR (automatic speech recognition) output to the belief state update, avoiding the use of complex semantic decoders while still attaining state-of-the-art performance. We adopt this RNN framework as the starting point for the work described here.\nIt is well-known in machine learning that a system trained on data from one domain may not perform as well when deployed in a different domain. Researchers have investigated methods for mitigating this problem, with NLP applications in parsing (McClosky et al., 2006; McClosky et al., 2010), sentiment analysis (Blitzer et al., 2007; Glorot et al., 2011) and many other tasks. There has been a small amount of previous work on domain adaptation for dialog systems. Tur et al. (2007) and Margolis et al. (2010) investigated domain adaptation for dialog act tagging. Walker et al. (2007) trained a sentence planner/generator that adapts to different individuals and domains. In the third DSTC shared task (Henderson et al., 2014b), participants deployed belief trackers trained on a restaurant domain in an expanded version of the same domain, with a richer output space but essentially the same topic. To the best of our knowledge, our work is the first attempt to build a belief tracker capable of operating across disjoint dialog domains."}, {"heading": "3 Dialog State Tracking using RNNs", "text": "Belief tracking models capture users\u2019 goals given their utterances. Goals are represented as sets of constraints expressed by slot-value mappings such as [food: chinese] or [wifi: available]. The set of slots S and the set of values Vs for each slot make up the ontology for an application domain.\nOur starting point is the RNN framework for belief tracking that was introduced by Henderson et al. (2014d; 2014c). This is a single-hidden-layer recurrent neural network that outputs a distribution over all goal slot-value pairs for each user utterance in a dialog. It also maintains a memory vector that stores internal information about the dialog context. The input for each user utterance consists of the ASR hypotheses, the last system action, the current memory vector and the previous belief state. Rather than using a spoken language understanding (SLU) decoder to convert this input into a meaning representation, the system uses the turn input to extract a large number of word n-gram features.\nThese features capture some of the dialog dynamics but are not ideal for sharing information across different slots and domains.\nDelexicalised n-gram features overcome this problem by replacing all references to slot names and values with generic symbols. Lexical n-grams such as [want cheap price] and [want Chinese food] map to the same delexicalised feature, represented by [want tagged-slot-value tagged-slotname]. Such features facilitate transfer learning between slots and allow the system to operate on unseen values or entirely new slots. As an example, [want available internet] would be delexicalised to [want tagged-slot-value tagged-slot-name] as well, a useful feature even if there is no training data available for the internet slot. The delexicalised model learns the belief state update corresponding to this feature from its occurrences across the other slots and domains. Subsequently, it can apply the learned behaviour to slots in entirely new domains.\nThe system maintains a separate belief state for each slot s, represented by the distribution ps over all possible slot values v \u2208 Vs. The model input at turn t, xt , consists of the previous belief state pt\u22121s , the previous memory state mt\u22121, as well as the vectors fl and fd of lexical and delexicalised features extracted from the turn input1. The belief state of each slot s is updated for each of its slot values v \u2208Vs. The RNN memory layer is updated as well. The updates are as follows2:\nxtv = f t l \u2295 ftd \u2295 mt\u22121 \u2295 pt\u22121v \u2295 pt\u22121/0 gtv = w s 1 \u00b7\u03c3 ( Ws0x t v +b s 0 ) +bs1 ptv = exp(gtv)\nexp(gt/0)+\u2211v\u2032\u2208V exp(g t v\u2032) mt = \u03c3 ( Wsm0xt +W s m1m t\u22121) where\u2295 denotes vector concatenation and pt/0 is the probability that the user has expressed no constraint up to turn t. Matrices Ws0, W s m0 , W s m1 and the vector ws1 are the RNN weights, and b0 and b1 are the hidden and output layer RNN bias terms.\nFor training, the model is unrolled across turns and trained using backpropagation through time and stochastic gradient descent (Graves, 2012).\n1Henderson et al.\u2019s work distinguished between three types of features: the delexicalised feature sets fs and fv are subsumed by our delexicalised feature vector fd , and the turn input f corresponds to our lexical feature vector fl .\n2The original RNN architecture had a second component which learned mappings from lexical n-grams to specific slot values. In order to move towards domain-independence, we do not use this part of the network."}, {"heading": "4 Hierarchical Model Training", "text": "Delexicalised features allow transfer learning between slots. We extend this approach to achieve transfer learning between domains: a model trained to talk about hotels should have some success talking about restaurants, or even laptops. If we can incorporate features learned from different domains into a single model, this model should be able to track belief state across all of these domains.\nThe training procedure starts by performing shared initialisation: the RNN parameters of all the slots are tied and all the slot value occurrences are replaced with a single generic tag. These slotagnostic delexicalised dialogs are then used to train the parameters of the shared RNN model.\nExtending shared initialisation to training across multiple domains is straightforward. We first delexicalise all slot value occurrences for all slots across the different domains in the training data. This combined (delexicalised) dataset is then used to train the multi-domain shared model.\nThe shared RNN model is trained with the purpose of extracting a very rich set of lexical and delexicalised features which capture general dialog dynamics. While the features are general, the RNN parameters are not, since not all of the features are equally relevant for different slots. For example, [eat tagged-slot-value food] and [near taggedslot-value] are clearly features related to food and area slots respectively. To ensure that the model learns the relative importance of different features for each of the slots, we train slot specific models for each slot across all the available domains. To train these slot-specialised models, the shared RNN\u2019s parameters are replicated for each slot and specialised further by performing additional runs of stochastic gradient descent using only the slotspecific (delexicalised) training data."}, {"heading": "5 Dialog domains considered", "text": "We use the experimental setup of the Dialog State Tracking Challenges. The key metric used to measure the success of belief tracking is goal accuracy, which represents the ability of the system to correctly infer users\u2019 constraints. We report the joint goal accuracy, which represents the marginal test accuracy across all slots in the domain.\nWe evaluate on data from six domains, varying across topic and geographical location (Table 1). The Cambridge Restaurants data is the data from DSTC 2. The San Francisco Restaurants and Ho-\ntels data was collected during the Parlance project (Gas\u030cic\u0301 et al., 2014). The Tourist Information domain is the DSTC 3 dataset: it contains dialogs about hotels, restaurants, pubs and coffee shops.\nThe Michigan Restaurants and Laptops datasets are collections of dialogs sourced using Amazon Mechanical Turk. The Laptops domain contains conversations with users instructed to find laptops with certain characteristics. This domain is substantially different from the other ones, making it particularly useful for assessing the quality of the multi-domain models trained.\nWe introduce three combined datasets used to train increasingly general belief tracking models:\n1. All Restaurants model: trained using the combined data of all three restaurant domains;\n2. R+T+H model: trained on all dialogs related to restaurants, hotels, pubs and coffee shops;\n3. R+T+H+L model: the most general model, trained using all the available dialog data."}, {"heading": "6 Results", "text": "As part of the evaluation, we use the three combinations of our dialog domains to build increasingly general belief tracking models. The domainspecific models trained using only data from each of the six dialog domains provide the baseline performance for the three general models."}, {"heading": "6.1 Training General Models", "text": "Training the shared RNN models is the first step of the training procedure. Table 2 shows the performance of shared models trained using dialogs from the six individual and the three combined domains. The joint accuracies are not comparable between the domains as each of them contains a different number of slots. The geometric mean of the six accuracies is calculated to determine how well these models operate across different dialog domains.\nThe parameters of the three multi-domain models are not slot or even domain specific. Nonetheless, all of them improve over the domain-specific model for all but one of their constituent domains. The R+T+H model outperforms the R+T+H+L model across four domains, showing that the use of laptops-related dialogs decreases performance slightly across other more closely related domains. However, the latter model is much better at balancing its performance across all six domains, achieving the highest geometric mean and still improving over all but one of the domain-specific models."}, {"heading": "6.2 Slot-specialising the General Models", "text": "Slot specialising the shared model allows the training procedure to learn the relative importance of different delexicalised features for each slot in a given domain. Table 3 shows the effect of slotspecialising shared models across the six dialog domains. Moving down in these tables corresponds to adding more out-of-domain training data and moving right corresponds to slot-specialising the shared model for each slot in the current domain.\nSlot-specialisation improved performance in the vast majority of the experiments. All three slotspecialised general models outperformed the RNN model\u2019s performance reported in DSTC 2."}, {"heading": "6.3 Out of Domain Initialisation", "text": "The hierarchical training procedure can exploit the available out-of-domain dialogs to initialise improved shared models for new dialog domains.\nIn our experiments, we choose one of the domains to act as the new domain, and we use a subset of the remaining ones as out-of-domain data. The number of in-domain dialogs available for training is increased at each stage of the experiment and used to train and compare the performance of two slot-specialised models. These models slotspecialise from two different shared models. One is trained using in-domain data only, and the other is trained on all the out-of-domain data as well.\nThe two experiments vary in the degree of similarity between the in-domain and out-of-domain dialogs. In the first experiment, Michigan Restaurants act as the new domain and the remaining R+T+H dialogs are used as out-of-domain data. In the second experiment, Laptops dialogs are the indomain data and the remaining dialog domains are used to initialise the more general shared model.\nFigure 1 shows how the performance of the two differently initialised models improves as additional in-domain dialogs are introduced. In both experiments, the use of out-of-domain data helps to\ninitialise the model to a much better starting point when the in-domain training data set is small. The out-of-domain initialisation consistently improves performance: the joint goal accuracy is improved even when the entire in-domain dataset becomes available to the training procedure.\nThese results are not surprising in the case of the system trained to talk about Michigan Restaurants. Dialog systems trained to help users find restaurants or hotels should have no trouble finding restaurants in alternative geographies. In line with these expectations, the use of a shared model initialised using R+T+H dialogs results in a model with strong starting performance. As additional restaurants dialogs are revealed to the training procedure, this model shows relatively minor performance gains over the domain-specific one.\nThe results of the Laptops experiment are even more compelling, as the difference in performance between the differently initialised models becomes larger and more consistent. There are two factors at play here: exposing the training procedure to substantially different out-of-domain dialogs allows it to learn delexicalised features not present in the in-domain training data. These features are applicable to the Laptops domain, as evidenced by the very strong starting performance. As additional in-domain dialogs are introduced, the delexicalised features not present in the out-of-domain data are learned as well, leading to consistent improvements in belief tracking performance.\nIn the context of these results, it is clear that the out-of-domain training data has the potential to be even more beneficial to tracking performance\nthan data from relatively similar domains. This is especially the case when the available in-domain training datasets are too small to allow the procedure to learn appropriate delexicalised features."}, {"heading": "7 Conclusion", "text": "We have shown that it is possible to train general belief tracking models capable of talking about many different topics at once. The most general model exhibits robust performance across all domains, outperforming most domain-specific models. This shows that training using diverse dialog domains allows the model to better capture general dialog dynamics applicable to different domains at once.\nThe proposed hierarchical training procedure can also be used to adapt the general model to new dialog domains, with very small in-domain data sets required for adaptation. This procedure improves tracking performance even when substantial amounts of in-domain data become available."}, {"heading": "7.1 Further Work", "text": "The suggested domain adaptation procedure requires a small collection of annotated in-domain dialogs to adapt the general model to a new domain. In our future work, we intend to focus on initialising good belief tracking models when no annotated dialogs are available for the new dialog domain."}], "references": [{"title": "Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["Blitzer et al.2007] John Blitzer", "Mark Dredze", "Fernando Pereira"], "venue": "In Proceedings of ACL", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Incremental on-line adaptation of POMDPbased dialogue managers to extended domains", "author": ["Ga\u0161i\u0107 et al.2014] Milica Ga\u0161i\u0107", "Dongho Kim", "Pirros Tsiakoulis", "Catherine Breslin", "Matthew Henderson", "Martin Szummer", "Blaise Thomson", "Steve Young"], "venue": null, "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2014}, {"title": "Domain adaptation for largescale sentiment classification: A deep learning approach", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Proceedings of ICML", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "A form-based dialogue manager for spoken language applications", "author": ["Goddeau et al.1996] D. Goddeau", "H. Meng", "J. Polifroni", "S. Seneff", "S. Busayapongchai"], "venue": "In Proceedings of ICSLP", "citeRegEx": "Goddeau et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Goddeau et al\\.", "year": 1996}, {"title": "Supervised Sequence Labelling with Recurrent Neural Networks", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Deep neural network approach for the Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "The Second Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Jason D. Wiliams"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "The Third Dialog State Tracking Challenge", "author": ["Blaise Thomson", "Jason D. Wiliams"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Blaise Thomson", "Steve Young"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Recipe for building robust spoken dialog state trackers: Dialog State Tracking Challenge system description", "author": ["Lee", "Eskenazi2013] Sungjin Lee", "Maxine Eskenazi"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Lee et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2013}, {"title": "Structured discriminative model for dialog state tracking", "author": ["Sungjin Lee"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Lee.,? \\Q2013\\E", "shortCiteRegEx": "Lee.", "year": 2013}, {"title": "Domain adaptation with unlabeled data for dialog act tagging", "author": ["Karen Livescu", "Mari Ostendorf"], "venue": "In Proceedings of the ACL Workshop on Domain Adaptation", "citeRegEx": "Margolis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Margolis et al\\.", "year": 2010}, {"title": "Effective selftraining for parsing", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "McClosky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Automatic domain adaptation for parsing", "author": ["Eugene Charniak", "Mark Johnson"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "McClosky et al\\.,? \\Q2010\\E", "shortCiteRegEx": "McClosky et al\\.", "year": 2010}, {"title": "Dialog state tracking using conditional random fields", "author": ["Ren et al.2013] Hang Ren", "Weiqun Xu", "Yan Zhang", "Yonghong Yan"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Ren et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2013}, {"title": "Model adaptation for dialog act tagging", "author": ["Tur et al.2007] Gokhan Tur", "Umit Guz", "Dilek Hakkani-T\u00fcr"], "venue": "In Proceedings of IEEE SLT", "citeRegEx": "Tur et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tur et al\\.", "year": 2007}, {"title": "Individual and domain adaptation in sentence planning for dialogue", "author": ["Amanda Stent", "Fran\u00e7ois Mairesse", "Rashmi Prasad"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Walker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2007}, {"title": "A simple and generic belief tracking mechanism for the Dialog State Tracking Challenge: On the believability of observed information", "author": ["Wang", "Lemon2013] Zhuoran Wang", "Oliver Lemon"], "venue": "Proceedings of SIGDIAL", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "The Dialogue State Tracking Challenge", "author": ["Antoine Raux", "Deepak Ramachandran", "Alan W. Black"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Williams et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2013}, {"title": "Multidomain learning and generalization in dialog state tracking", "author": ["Jason D. Williams"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Williams.,? \\Q2013\\E", "shortCiteRegEx": "Williams.", "year": 2013}, {"title": "Web-style ranking and slu combination for dialog state tracking", "author": ["Jason D. Williams"], "venue": "In Proceedings of SIGDIAL", "citeRegEx": "Williams.,? \\Q2014\\E", "shortCiteRegEx": "Williams.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Recurrent Neural Networks (RNNs) are well suited to dialog state tracking, as their ability to capture contextual information allows them to model and label complex dynamic sequences (Graves, 2012).", "startOffset": 183, "endOffset": 197}, {"referenceID": 19, "context": "The recent Dialog State Tracking Challenge (DSTC) shared tasks (Williams et al., 2013; Henderson et al., 2014a; Henderson et al., 2014b) saw a variety of novel approaches, including robust sets of hand-crafted rules (Wang and Lemon, 2013), conditional random fields (Lee and Eskenazi, 2013; Lee, 2013; Ren et al.", "startOffset": 63, "endOffset": 136}, {"referenceID": 11, "context": ", 2014b) saw a variety of novel approaches, including robust sets of hand-crafted rules (Wang and Lemon, 2013), conditional random fields (Lee and Eskenazi, 2013; Lee, 2013; Ren et al., 2013), maximum entropy models (Williams, 2013) and web-style ranking (Williams, 2014).", "startOffset": 138, "endOffset": 191}, {"referenceID": 15, "context": ", 2014b) saw a variety of novel approaches, including robust sets of hand-crafted rules (Wang and Lemon, 2013), conditional random fields (Lee and Eskenazi, 2013; Lee, 2013; Ren et al., 2013), maximum entropy models (Williams, 2013) and web-style ranking (Williams, 2014).", "startOffset": 138, "endOffset": 191}, {"referenceID": 20, "context": ", 2013), maximum entropy models (Williams, 2013) and web-style ranking (Williams, 2014).", "startOffset": 32, "endOffset": 48}, {"referenceID": 21, "context": ", 2013), maximum entropy models (Williams, 2013) and web-style ranking (Williams, 2014).", "startOffset": 71, "endOffset": 87}, {"referenceID": 3, "context": "Goddeau et al. (1996)) have been superseded by data-driven systems that are more robust and can provide the probabilistic dialog state distributions that are needed by POMDPbased dialog managers.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "Researchers have investigated methods for mitigating this problem, with NLP applications in parsing (McClosky et al., 2006; McClosky et al., 2010), sentiment analysis (Blitzer et al.", "startOffset": 100, "endOffset": 146}, {"referenceID": 14, "context": "Researchers have investigated methods for mitigating this problem, with NLP applications in parsing (McClosky et al., 2006; McClosky et al., 2010), sentiment analysis (Blitzer et al.", "startOffset": 100, "endOffset": 146}, {"referenceID": 0, "context": ", 2010), sentiment analysis (Blitzer et al., 2007; Glorot et al., 2011) and many other tasks.", "startOffset": 28, "endOffset": 71}, {"referenceID": 2, "context": ", 2010), sentiment analysis (Blitzer et al., 2007; Glorot et al., 2011) and many other tasks.", "startOffset": 28, "endOffset": 71}, {"referenceID": 0, "context": ", 2010), sentiment analysis (Blitzer et al., 2007; Glorot et al., 2011) and many other tasks. There has been a small amount of previous work on domain adaptation for dialog systems. Tur et al. (2007) and Margolis et al.", "startOffset": 29, "endOffset": 200}, {"referenceID": 0, "context": ", 2010), sentiment analysis (Blitzer et al., 2007; Glorot et al., 2011) and many other tasks. There has been a small amount of previous work on domain adaptation for dialog systems. Tur et al. (2007) and Margolis et al. (2010) investigated domain adaptation", "startOffset": 29, "endOffset": 227}, {"referenceID": 12, "context": "Walker et al. (2007) trained a sentence planner/generator that adapts to different individuals and domains.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "For training, the model is unrolled across turns and trained using backpropagation through time and stochastic gradient descent (Graves, 2012).", "startOffset": 128, "endOffset": 142}, {"referenceID": 1, "context": "tels data was collected during the Parlance project (Ga\u0161i\u0107 et al., 2014).", "startOffset": 52, "endOffset": 72}], "year": 2015, "abstractText": "Dialog state tracking is a key component of many modern dialog systems, most of which are designed with a single, welldefined domain in mind. This paper shows that dialog data drawn from different dialog domains can be used to train a general belief tracking model which can operate across all of these domains, exhibiting superior performance to each of the domainspecific models. We propose a training procedure which uses out-of-domain data to initialise belief tracking models for entirely new domains. This procedure leads to improvements in belief tracking performance regardless of the amount of in-domain data available for training the model.", "creator": "LaTeX with hyperref package"}}}