{"id": "1506.07285", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2015", "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "abstract": "gradually most tasks in mammalian natural sequence language processing can essentially be slowly cast into discrete question constraint answering ( qa ) problems over language input. together we swiftly introduce the rapid dynamic memory language network ( dmn ), developed a complicated unified neural machine network specification framework, which processes input sequences and questions, forms semantic and episodic memories, and swiftly generates frequently relevant answers. query questions trigger begin an iterative delayed attention process change which inadvertently allows the model to internally condition out its factual attention on the result of its previous iterations. these input results are then reasoned over in applying a linear hierarchical ordered recurrent sequence model to regularly generate alternative answers. the dmn can be trained end - to - end and immediately obtains expert state because of the art results on several types used of morphological tasks and datasets : relational question frame answering ( facebook'x s babi flow dataset ), sequence object modeling behavior for calculating part of facial speech stimulus tagging ( lambda wsj - np ptb ), and complex text classification for sentiment analysis ( stanford language sentiment modeling treebank ). the model relies exclusively on computer trained word vector representations and requires no string literal matching operations or adaptive manually engineered features.", "histories": [["v1", "Wed, 24 Jun 2015 08:27:02 GMT  (270kb,D)", "http://arxiv.org/abs/1506.07285v1", null], ["v2", "Fri, 24 Jul 2015 22:21:29 GMT  (270kb,D)", "http://arxiv.org/abs/1506.07285v2", null], ["v3", "Tue, 29 Sep 2015 05:02:29 GMT  (267kb,D)", "http://arxiv.org/abs/1506.07285v3", null], ["v4", "Tue, 9 Feb 2016 08:19:30 GMT  (518kb,D)", "http://arxiv.org/abs/1506.07285v4", null], ["v5", "Sat, 5 Mar 2016 20:18:55 GMT  (507kb,D)", "http://arxiv.org/abs/1506.07285v5", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["ankit kumar", "ozan irsoy", "peter ondruska", "mohit iyyer", "james bradbury", "ishaan gulrajani", "victor zhong", "romain paulus", "richard socher"], "accepted": true, "id": "1506.07285"}, "pdf": {"name": "1506.07285.pdf", "metadata": {"source": "CRF", "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "authors": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher"], "emails": ["firstname@metamind.io"], "sections": [{"heading": null, "text": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on several types of tasks and datasets: question answering (Facebook\u2019s bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclusively on trained word vector representations and requires no string matching or manually engineered features."}, {"heading": "1 Introduction", "text": "Question answering (QA) is a complex natural language processing task which requires an understanding of the meaning of a text and the ability to reason over relevant facts. Most, if not all, tasks in natural language processing can be cast as a question answering problem: high level tasks like machine translation (What is the translation into French?); sequence modeling tasks like named entity recognition [1] (NER) (What are the named entity tags in this sentence?) or part of speech tagging (POS) (What are the part of speech tags?); classification problems like sentiment analysis [2] (What is the sentiment?); even multi-sentence joint classification problems like coreference resolution (Who does \u201dtheir\u201d refer to?).\nThe dynamic memory network (DMN) is a neural network based model which can be trained in an end-to-end fashion for any QA task using raw input-question-answer triplets. Fig. 1 shows input and question test sequences followed by answers given by the model. The DMN is illustrated in Fig. 2. Generally, it can solve sequence tagging tasks, classification problems, sequence to sequence tasks, and question answering tasks that require transitive reasoning. The DMN first processes all input, question and answer texts into sequences of semantic vector representations. The question representation triggers an iterative attention process that searches the input and retrieves relevant facts. The DMN then reasons over retrieved facts and provides an answer sequence model with an appropriate summary.\nar X\niv :1\n50 6.\n07 28\n5v 1\n[ cs\n.C L\n] 2\n4 Ju"}, {"heading": "2 The Dynamic Memory Network Framework", "text": "The DMN is a general modeling framework for asking questions over inputs. We will give an overview of the full model, and then go into further detail for each module, as well as present our specific instantiation of each module for natural language processing. The goal of the DMN is to first compute a vector representation of an input, given a question, and to then generate the correct answer. It contains the following modules:\nInput Module: This module processes raw inputs and maps them to a representation that is useful for asking questions about this input. The input may be, for instance, an image, video, or audio signal. We focus on NLP in this paper. Hence, the input may be a sentence, a long story, a movie review, a news article, or all of Wikipedia.\nSemantic Memory Module: Semantic memory stores general knowledge about concepts and facts. For example, it might contain information about what a hang glider is. Initialization strategies such as distributed word vectors (Glove [3], Word2Vec [4]) are popular semantic memory components that have been shown to improve performance on many NLP tasks. More complex information can be stored in the form of knowledge bases that capture relationships in the form of triplets [5] or gazetteers, which have been useful for tasks such as named entity recognition or question answering [6].\nQuestion Module: The question module computes a representation of a question such as Where did the author first fly? This representation triggers the episodic memory module to start an iterative attention process over facts from the input sequence.\nEpisodic Memory Module: This is the central part of the DMN. A question draws attention to specific facts from the input sequence, which are reasoned over to update this module\u2019s memory state. This process then iterates, with each iteration providing the module with newly relevant information about the input. In other words, the module has the ability to retrieve new facts which were thought to be irrelevant in previous iterations. After several passes the module then summarizes its knowledge and provides the answer module with a final representation to produce an answer.\nAnswer Module: Given a representation from the episodic memory module, the answer module generates the model\u2019s predicted answer."}, {"heading": "2.1 Input Module", "text": "The input module computes a useful representation of the inputs such that relevant facts can be retrieved later. Generally, the input module may be thought of as computing the intermediate steps of a function that eventually returns a final vector representation. The input module sends these intermediate values to the episodic memory module, which will complete the computation, conditioned on the question, by way of its attention mechanism. We refer to the representations that the input module provides to the episodic memory as facts to be reasoned over by the episodic memory.\nIn natural language processing, we have a sequence of TI words wI1 , . . . , w I TI . The input module computes the hidden states of a recurrent sequence model [7].\nWe use Glove [3] vectors to capture context-independent representations, and initalize the embeddings of the DMN with these values. Word embeddings are given as inputs to the recurrent network to compute hidden fact states: ct = SEQ MODEL(L[wIt ], ht\u22121), where L is the embedding matrix and wIt is the tth word of the input sequence. In particular, we use a gated recurrent network (GRU) [8, 9]. We also explored the more complex LSTM [10] but it performed similarly and is more computationally expensive. Both work much better than the standard tanh RNN and we postulate that the main strength comes from having gates that allow the model to suffer less from the vanishing gradient problem [10].\nGRU Definition: Assume each time step has an input xt and a hidden state ht. We will abbreviate the below computation with ht = GRU(xt, ht\u22121):\nzt = \u03c3 ( W (z)xt + U (z)ht\u22121 + b (z) ) ; rt = \u03c3 ( W (r)xt + U (r)ht\u22121 + b (r) )\n(1) h\u0303t = tanh ( Wxt + rt \u25e6 Uht\u22121 + b(h) ) ; ht = zt \u25e6 ht\u22121 + (1\u2212 zt) \u25e6 h\u0303t, (2)\nwhere \u25e6 is an element-wise product,W (z),W (r),W \u2208 RnH\u00d7nI and U (z), U (r), U \u2208 RnH\u00d7nH . The dimensions n are hyperparameters.\nIf we subsample the output of the recurrent network, the input module will return just the hidden states ct that correspond to end-of-sentence markers in the original story. Otherwise, the input module returns hidden states ct for all words. For notational convenience, in future modules we will refer to either of these states as ct, and say in the experiments section whether or not the subsampling was used."}, {"heading": "2.2 Semantic Memory Module", "text": "The semantic memory consists of (i) stored word concepts and (ii) facts about them. We initialize embeddings to Glove vectors as described above. This module could include gazeteers or other forms of explicit knowledge bases, but in this work we do not use them."}, {"heading": "2.3 Question Module", "text": "This module maps a question into a representation that can then be used for querying specific facts from the input module. We have questions that consist of sequences of TQ words w Q t . We compute a hidden state for each via qt = GRU(L[w Q t ], qt\u22121), where the GRU and embedding weights are shared with the input module. The final question vector is defined as q = qTQ ."}, {"heading": "2.4 Episodic Memory Module", "text": "The episodic memory module retrieves facts from the input module conditioned on the question. It then reasons over those facts to produce a final representation that the answer module will use to generate an answer. We refer to this representation as a memory. Importantly, we allow our module\nto take multiple passes over the facts, focusing attention on different facts at each pass. Each pass produces an episode, and these episodes are then summarized into the memory. Endowing our module with this episodic component allows its attention mechanism to attend more selectively to specific facts on each pass, as it can attend to other important facts at a later pass. It also allows for a type of transitive inference, since the first pass may uncover the need to retrieve additional facts.\nFor instance, in the example in Fig. 3, we are asked Where is the football? In the first iteration, the model ought attend to sentence 7 (John put down the football.), as the question asks about the football. Only once the model sees that John is relevant can it reason the second iteration should retrieve where John was. In this example, taken from a true test question on Facebook\u2019s bAbI task, this behavior is indeed seen. Note that the second iteration has wrongly placed some weight in sentence 2, which makes some intuitive sense, as sentence 2 is another place John had been.\nIn its general form, the episodic memory module is characterized by an attention mechanism, a function which returns an episode given the output of the attention mechanism and the facts from the input module, and a function that summarizes the episodes into a memory.\nIn our work, we use a gating function as our attention mechanism. It takes as input, for each pass i, a candidate fact ct, a previous state mi\u22121, and the question q to compute a gate: git = G(ct,m\ni\u22121, q). The state is updated by way of a GRU: mi = GRU(ei,mi\u22121), where ei is the computed episode at pass i. The state may be initialized randomly, but in practice we have found that initializing it to the question vector itself helps; e.g, m0 = q. The function G returns a single scalar and is defined as follows:\nz(c,m, q) = [c,m, q, c \u25e6 q, c \u25e6m, |c\u2212 q|, |c\u2212m|, cTW (b)q, cTW (b)m] (3) G(c,m, q) = \u03c3 ( W (2) tanh ( W (1)z(c,m, q) + b(1) ) + b(2) ) (4)\nTo compute the episode for pass i, we employ a modified GRU over the sequence of TC facts ct, endowed with our gates. The episode is the final state of the GRU:\nhit = g i tGRU(ct, h i t\u22121) + (1\u2212 git)hit\u22121 (5)\nei = hiTC (6)\nFinally, to summarize the TP episodes ei into a memory, we use the same GRU that updates the attention mechanism\u2019s state: mi = GRU(ei,mi\u22121), and we set the memory m as m = mTP . This is equivalent to setting the memory to simply the attention mechanism\u2019s final state, but we have described it here as its own computation to highlight the potential modularity of these subcomponents.\nFor datasets that mark which facts are important for a given question, such as Facebook\u2019s bAbI dataset, the gates of Eq. 4 can be trained supervised with a standard cross entropy classification\nerror function. We also append a special end-of-passes representation to the facts, and stop the iterative attention process if this representation is chosen by the gate function. Otherwise, for datasets without explicit supervision, we set a maximum number of passes. The whole module is end-to-end differentiable.\nSequence Modeling\nIt is straightforward to apply the DMN to sequence modeling. In the sequence modeling task, we wish to label each word in the original sequence. Therefore, we desire one vector representation for each word. To this end, we run the DMN in the same way as above, once for each word. When running the DMN for word t, instead of setting the episode for pass i to the final state of our modified GRU, we take the tth: e.g, for word t, we replace Eq. 6 with ei = hit. Note that the gates for the first pass will be the same for each word, as the question is the same. This allows for speed-up in implementation by computing these gates only once. However, gates for subsequent passes will be different, as the episodes are different.\nThe final output of the episodic memory module is the memorym, which goes to the answer module. In the sequence modeling case, each word\u2019s unique m is sent independently to the answer module."}, {"heading": "2.5 Answer Sequence", "text": "The answer sequence module decodes the memory into a sequence of words representing the answer.\nWe use a GRU, and set the initial hidden state to the memory a0 = m. The subsequent hidden states take as input the last hidden state and the previously predicted output yt\u22121, as well as the question:\nat = GRU([yt\u22121, q], at\u22121), yt = softmax(W (a)at) (7)\nwhere W (a) is a standard softmax layer. The output is trained with the cross entropy error classification of the correct sequence appended with a special end-of-sequence token.. At test time, we generate words until an end-of-sequence is generated."}, {"heading": "2.6 Training", "text": "Training is cast as a supervised classification problem to minimize cross entropy error of the answer sequence. For datasets with gate supervision, such as bAbI, we also include the cross entropy error of the gates into the overall cost. Because all modules communicate over vector representations and various types of differentiable and deep neural networks with gates, the entire DMN model can be trained via backpropagation and gradient descent."}, {"heading": "3 Related Work", "text": "Given the many shoulders on which this paper is standing and the many applications to which our model is applied, it is impossible to do related fields justice.\nDeep Learning. There are several deep learning models that have been applied to many different tasks in NLP. For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks. However, because they lack the memory and question modules, a single model cannot solve as many varied tasks, nor tasks that require transitive reasoning over multiple sentences. Another commonly used model is the chain structured recurrent neural network of the kind we employ above. Recurrent neural networks have been successfully used in language modeling [14], speech recognition, and sentence generation from images [15]. Also relevant is the sequenceto-sequence model used for machine translation by Sutskever et al. [16]. This model uses two extremely large and deep LSTMs to encode a sentence in one language and then decode the sentence in another language. This sequence to sequence model is a special case of the DMN without a question and without episodic memory. Instead it maps an input sequence directly to an answer sequence.\nAttention. The second line of work that is very relevant to DMNs is that of attention and memory in deep learning. The work of recent months by Weston et al. on memory networks [17] focuses on\nadding a memory component for natural language question answering. They have an input (I) and response (R) component and their generalization (G) and output feature map (O) components have some functional overlap with our episodic memory. However, their model cannot be applied to the same variety of NLP tasks since it processes sentences independently and not via a sequence model. It requires bag of n-gram string matching features as well as a separate feature that captures whether a sentence came before another one. We compare directly to their model on the bAbI dataset [18]. Attention mechanisms are generally useful and can improve image classification [19], automatic image captioning [20] and machine translation [21, 22]. Neural Turing machines use memory to solve algorithmic problems such as list sorting [23].\nNLP Applications. The DMN is a general model which we apply to several NLP problems. We compare to what, to the best of our knowledge, is the current state of the art method for each task.\nThere are many different approaches to question answering: some build large knowledge bases (KBs) with open information extraction systems [24], some use neural networks, dependency trees and KBs [6], others only sentences [12]. A lot of other approaches exist. When QA systems do not produce the right answer, it is often unclear if it is because they do not have access to the facts, cannot reason over them or have never seen this type of question or phenomenon. Most QA dataset only have a few hundred questions and answers but require complex reasoning. They can hence not be solved by models that have to learn purely from examples. While synthetic datasets [18] have problems and can often be solved easily with manual feature engineering, they let us disentangle failure modes of models and understand necessary QA capabilities. They are useful for analyzing models that attempt to learn everything and do not rely on external features like coreference, POS, parsing, logical rules, etc. The DMN is such a model.\nSentiment analysis is a very useful classification task and recently the Stanford Sentiment Treebank [2] has become a standard benchmark dataset. Kim [25] reports the previous state of the art result based on a convolutional neural network that uses multiple word vector representations. The previous best model for part of speech tagging on the Wall Street Journal section of the Penn Tree Bank [26] was Sogaard [27] who used a semisupervised nearest neighbor approach.\nNeuroscience. The semantic and episodic memory modules are related to cognitive neuroscience. In humans both language-specific and supramodal concept representations are seated in the semantic memory whose existence is well established [28]. Hence, word vectors and knowledge bases are stored in the DMN\u2019s semantic memory module.\nThe episodic memory in humans stores specific experiences in their spatial and temporal context. For instance, it might contain the first memory somebody has of flying a hang glider. Eichenbaum and Cohen have argued that episodic memories represent a form of relationship (i.e., relations between spatial, sensory and temporal information) and that the hippocampus is responsible for general relational learning [29]. Interestingly, it also appears that the hippocampus is active during transitive inference [30], and disruption of the hippocampus impairs this ability [31].\nThe episodic memory module in the DMN is related to these findings. It retrieves specific temporal states that are in a relationship with a question. Furthermore, we found that the GRU in this module was able to do some transitive inference over the simple facts in the bAbI dataset. This module also has similarities to the Temporal Context Model [32] and its Bayesian extensions [33] which were developed to analyze human behavior in word recall experiments."}, {"heading": "4 Experiments", "text": "We include experiments on question answering, part of speech tagging, sentiment analysis, as well as preliminary results on machine translation. For all datasets we used either the official train,dev,test splits or if no dev set was defined, we used 10% of the training set for development. Hyper-parameter tuning and model selection (with early stopping) is done on the development set. The DMN is trained via backpropagation and Adagrad [34]. We employ L2 regularization, and \u201cword-dropout\u201d in which each word vector is set to 0 with some probability p. Word vectors are pre-trained using Glove [3]."}, {"heading": "4.1 Question Answering", "text": "The Facebook bAbI dataset is a synthetic dataset meant to test a model\u2019s ability to retrieve facts and reason over them. Each task tests a different skill that a good question answering model ought to have, such as coreference resolution, deduction, and induction. Training on the bAbI dataset\nuses the following objective function: J = \u03b1ECE(Gates) + \u03b2ECE(Answers), where ECE is the standard cross-entropy cost and \u03b1 and \u03b2 are hyperparameters. In practice, we begin training with \u03b1 set to 1 and \u03b2 set to 0, and then later switch \u03b2 to 1 while keeping \u03b1 at 1. We subsample the facts from the input module by end-of-sentence tokens. The gate supervision aims to select one sentence per pass; thus, we also experimented with modifying Eq. 6 to a simple softmax instead of a GRU. Here, we compute the final episode vector via: ei = \u2211T t=1 softmax(g i t)ct, where softmax(g i t) = exp(git)\u2211T j=1 exp(g i j) , and git here is the value of the gate before the sigmoid. This setting achieves better results, likely because the softmax is better suited to picking one sentence at a time.\nWe list results in table 1. The DMN does worse than the MemNN on tasks 2 and 3, both tasks with long input sequences. We suspect this is due to the recurrent input sequence model having trouble modeling very long inputs. The MemNN does not suffer from this problem as it views each sentence seperately. The power of the episodic memory module is evident in tasks 7 and 8, where the DMN significantly outperforms the MemNN. Both tasks require the model to iteratively retrieve facts and store them in a representation that slowly incorporates more of the relevant information of the input sequence. Both models do poorly on tasks 17 and 19, though the MemNN does better. We suspect this is due to the MemNN using n-gram features as well as explicit sequence position features."}, {"heading": "4.2 Sequence Tagging: Part of Speech Tagging", "text": "Part-of-speech tagging is traditionally modeled as a sequence tagging problem: every word in a sentence is to be classified into its part-of-speech class (see Fig. 1). We evaluate on the standard Wall Street Journal dataset included in Penn-III [26]. We use the standard splits of sections 0-18 for training, 19-21 for development and 22-24 for test sets [27]. Since this is a word level tagging task, DMN memories are produced at the word -rather than sentence- level. We compare the DMN\nwith the results in [27]. The DMN achieves state-of-the-art accuracy with a single model, reaching a development set accuracy of 97.5. Ensembling the top 4 development models, the DMN gets to 97.58 dev and 97.56 test accuracies, achieving a new state-of-the-art (Table 2)."}, {"heading": "4.3 Text Classification: Sentiment Analysis", "text": "Stanford Sentiment Treebank (SST) [2] is a popular dataset for sentiment classification. It provides phrase-level fine-grained labels, and comes with a train/dev/test split. We present results on two formats: fine-grained root prediction, where all full sentences (root nodes) of the test set are to be classified as either very negative, negative, neutral, positive, or very positive, and binary root prediction, where all non-neutral full sentences of the test set are to be classified as either positive or negative. To train the model on the fine-grained task, we use all phrase-level labels. To train on the binary task, we use all non-neutral phrase-level labels.\nFor sentiment analysis, our gate function G needs only the first 3 components c,m, q of the function z as defined in Eq. 3. The DMN achieves state-of-the-art accuracy on the binary classification task, as well as near state-of-the-art on the fine-grained classification task.\nOur DMN was trained with GRU sequence models and no tree structure. It is easy to replace the GRU sequence model with any of the models listed above, as well as incorporate tree structure to the retrieval process. These experiments were not run, and we consider them future work.\nPreliminary Results: Machine Translation\nWe are also training the DMN for machine translation, comparing it to the sequence-to-sequence LSTM model presented by Sutskever et al. [16]. The sequence-to-sequence model is a special case of the DMN in which only one memory at the end of the input sentence is formed. Initial experiments on the smaller WMT13 English-toFrench News Commentary dataset used in Kalchbrenner [36] show promising results. As seen in 4.3, the DMN is learning at a similar pace to an implementation of the Seq-to-Seq LSTM model. For the Seq-to-Seq LSTM, we use the hyperparameters listed in [16]."}, {"heading": "5 Conclusion", "text": "We believe the DMN is a potentially general model for a variety of NLP applications. The entire model can be trained end-to-end with one, albeit complex, objective function. The model uses some ideas from neuroscience such as semantic and episodic memories known to be required for complex types of reasoning. Future work will explore additional tasks, larger multi-task models and multimodal inputs and questions."}, {"heading": "Acknowledgements", "text": "We thank Sam Gershman for useful discussions."}], "references": [{"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["A. Passos", "V. Kumar", "A. McCallum"], "venue": "Conference on Computational Natural Language Learning. Association for Computational Linguistics, June", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "EMNLP,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR (workshop),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Reasoning With Neural Tensor Networks For Knowledge Base Completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A.Y. Ng"], "venue": "NIPS.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "AISTATS,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed representations, simple recurrent networks, and grammatical structure", "author": ["J.L. Elman"], "venue": "Machine Learning, 7(2-3):195\u2013225,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "CoRR, abs/1409.1259,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780, Nov", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "author": ["R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "C.D. Manning"], "venue": "NIPS,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["M. Iyyer", "J. Boyd-Graber", "L. Claudino", "R. Socher", "H. Daum\u00e9 III"], "venue": "EMNLP,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Recursive neural networks for learning logical semantics", "author": ["S.R. Bowman", "C. Potts", "C.D. Manning"], "venue": "CoRR, abs/1406.1827,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Context dependent recurrent neural network language model", "author": ["T. Mikolov", "G. Zweig"], "venue": "SLT, pages 234\u2013239. IEEE,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "ICLR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "CoRR, abs/1502.05698,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Networks with Internal Selective Attention through Feedback Connections", "author": ["M.F. Stollenga", "J. Schmidhuber J. Masci", "F. Gomez"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "CoRR, abs/1502.03044,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y Bengio"], "venue": "CoRR, abs/1409.0473,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "CoRR, abs/1410.5401,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Textrunner: Open information extraction on the web", "author": ["A. Yates", "M. Banko", "M. Broadhead", "M.J. Cafarella", "O. Etzioni", "S. Soderland"], "venue": "HLT-NAACL (Demonstrations),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational Linguistics, 19(2), June", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1993}, {"title": "Semisupervised condensed nearest neighbor for part-of-speech tagging", "author": ["A. S\u00f8gaard"], "venue": "ACL-HLT,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "The neurobiology of semantic memory", "author": ["J.R. Binder", "R.H. Desai"], "venue": "Trends in Cognitive Sciences, 15(11):527\u2013536,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "From Conditioning to Conscious Recollection: Memory Systems of the Brain (Oxford Psychology)", "author": ["H. Eichenbaum", "N.J. Cohen"], "venue": "Oxford University Press, 1 edition,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Hippocampal activation during transitive inference in humans", "author": ["S. Heckers", "M. Zalesak", "A.P. Weiss", "T. Ditman", "D. Titone"], "venue": "Hippocampus, 14:153\u201362,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "The hippocampus and memory for orderly stimulusrelations", "author": ["J.A. Dusek", "H. Eichenbaum"], "venue": "Proceedings of the National Academy of Sciences, 94(13):7109\u20137114,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1997}, {"title": "A distributed representation of temporal context", "author": ["Marc W. Howard", "Michael J. Kahana"], "venue": "Journal of Mathematical Psychology,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2002}, {"title": "A bayesian analysis of dynamics in free recall", "author": ["R. Socher", "S. Gershman", "A. Perotte", "P. Sederberg", "D. Blei", "K. Norman"], "venue": "Advances in Neural Information Processing Systems 22.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "JMLR, 12, July", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved semantic representations from tree-structured long short-term memory", "author": ["K.S. Tai", "R. Socher", "C.D. Manning"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Most, if not all, tasks in natural language processing can be cast as a question answering problem: high level tasks like machine translation (What is the translation into French?); sequence modeling tasks like named entity recognition [1] (NER) (What are the named entity tags in this sentence?) or part of speech tagging (POS) (What are the part of speech tags?); classification problems like sentiment analysis [2] (What is the sentiment?); even multi-sentence joint classification problems like coreference resolution (Who does \u201dtheir\u201d refer to?).", "startOffset": 236, "endOffset": 239}, {"referenceID": 1, "context": "Most, if not all, tasks in natural language processing can be cast as a question answering problem: high level tasks like machine translation (What is the translation into French?); sequence modeling tasks like named entity recognition [1] (NER) (What are the named entity tags in this sentence?) or part of speech tagging (POS) (What are the part of speech tags?); classification problems like sentiment analysis [2] (What is the sentiment?); even multi-sentence joint classification problems like coreference resolution (Who does \u201dtheir\u201d refer to?).", "startOffset": 414, "endOffset": 417}, {"referenceID": 2, "context": "Initialization strategies such as distributed word vectors (Glove [3], Word2Vec [4]) are popular semantic memory components that have been shown to improve performance on many NLP tasks.", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "Initialization strategies such as distributed word vectors (Glove [3], Word2Vec [4]) are popular semantic memory components that have been shown to improve performance on many NLP tasks.", "startOffset": 80, "endOffset": 83}, {"referenceID": 4, "context": "More complex information can be stored in the form of knowledge bases that capture relationships in the form of triplets [5] or gazetteers, which have been useful for tasks such as named entity recognition or question answering [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "More complex information can be stored in the form of knowledge bases that capture relationships in the form of triplets [5] or gazetteers, which have been useful for tasks such as named entity recognition or question answering [6].", "startOffset": 228, "endOffset": 231}, {"referenceID": 6, "context": "The input module computes the hidden states of a recurrent sequence model [7].", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "We use Glove [3] vectors to capture context-independent representations, and initalize the embeddings of the DMN with these values.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "In particular, we use a gated recurrent network (GRU) [8, 9].", "startOffset": 54, "endOffset": 60}, {"referenceID": 8, "context": "In particular, we use a gated recurrent network (GRU) [8, 9].", "startOffset": 54, "endOffset": 60}, {"referenceID": 9, "context": "We also explored the more complex LSTM [10] but it performed similarly and is more computationally expensive.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "Both work much better than the standard tanh RNN and we postulate that the main strength comes from having gates that allow the model to suffer less from the vanishing gradient problem [10].", "startOffset": 185, "endOffset": 189}, {"referenceID": 10, "context": "For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks.", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks.", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks.", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks.", "startOffset": 146, "endOffset": 150}, {"referenceID": 12, "context": "For instance, recursive neural networks have been used for parsing [11], sentiment analysis [2], paraphrase detection [11] and question answering [12] and logical inference [13], among other tasks.", "startOffset": 173, "endOffset": 177}, {"referenceID": 13, "context": "Recurrent neural networks have been successfully used in language modeling [14], speech recognition, and sentence generation from images [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "Recurrent neural networks have been successfully used in language modeling [14], speech recognition, and sentence generation from images [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "on memory networks [17] focuses on", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "We compare directly to their model on the bAbI dataset [18].", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "Attention mechanisms are generally useful and can improve image classification [19], automatic image captioning [20] and machine translation [21, 22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "Attention mechanisms are generally useful and can improve image classification [19], automatic image captioning [20] and machine translation [21, 22].", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "Attention mechanisms are generally useful and can improve image classification [19], automatic image captioning [20] and machine translation [21, 22].", "startOffset": 141, "endOffset": 149}, {"referenceID": 21, "context": "Attention mechanisms are generally useful and can improve image classification [19], automatic image captioning [20] and machine translation [21, 22].", "startOffset": 141, "endOffset": 149}, {"referenceID": 22, "context": "Neural Turing machines use memory to solve algorithmic problems such as list sorting [23].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "There are many different approaches to question answering: some build large knowledge bases (KBs) with open information extraction systems [24], some use neural networks, dependency trees and KBs [6], others only sentences [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "There are many different approaches to question answering: some build large knowledge bases (KBs) with open information extraction systems [24], some use neural networks, dependency trees and KBs [6], others only sentences [12].", "startOffset": 196, "endOffset": 199}, {"referenceID": 11, "context": "There are many different approaches to question answering: some build large knowledge bases (KBs) with open information extraction systems [24], some use neural networks, dependency trees and KBs [6], others only sentences [12].", "startOffset": 223, "endOffset": 227}, {"referenceID": 17, "context": "While synthetic datasets [18] have problems and can often be solved easily with manual feature engineering, they let us disentangle failure modes of models and understand necessary QA capabilities.", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "Sentiment analysis is a very useful classification task and recently the Stanford Sentiment Treebank [2] has become a standard benchmark dataset.", "startOffset": 101, "endOffset": 104}, {"referenceID": 24, "context": "Kim [25] reports the previous state of the art result based on a convolutional neural network that uses multiple word vector representations.", "startOffset": 4, "endOffset": 8}, {"referenceID": 25, "context": "The previous best model for part of speech tagging on the Wall Street Journal section of the Penn Tree Bank [26] was Sogaard [27] who used a semisupervised nearest neighbor approach.", "startOffset": 108, "endOffset": 112}, {"referenceID": 26, "context": "The previous best model for part of speech tagging on the Wall Street Journal section of the Penn Tree Bank [26] was Sogaard [27] who used a semisupervised nearest neighbor approach.", "startOffset": 125, "endOffset": 129}, {"referenceID": 27, "context": "In humans both language-specific and supramodal concept representations are seated in the semantic memory whose existence is well established [28].", "startOffset": 142, "endOffset": 146}, {"referenceID": 28, "context": ", relations between spatial, sensory and temporal information) and that the hippocampus is responsible for general relational learning [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 29, "context": "Interestingly, it also appears that the hippocampus is active during transitive inference [30], and disruption of the hippocampus impairs this ability [31].", "startOffset": 90, "endOffset": 94}, {"referenceID": 30, "context": "Interestingly, it also appears that the hippocampus is active during transitive inference [30], and disruption of the hippocampus impairs this ability [31].", "startOffset": 151, "endOffset": 155}, {"referenceID": 31, "context": "This module also has similarities to the Temporal Context Model [32] and its Bayesian extensions [33] which were developed to analyze human behavior in word recall experiments.", "startOffset": 64, "endOffset": 68}, {"referenceID": 32, "context": "This module also has similarities to the Temporal Context Model [32] and its Bayesian extensions [33] which were developed to analyze human behavior in word recall experiments.", "startOffset": 97, "endOffset": 101}, {"referenceID": 33, "context": "The DMN is trained via backpropagation and Adagrad [34].", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "Word vectors are pre-trained using Glove [3].", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "We evaluate on the standard Wall Street Journal dataset included in Penn-III [26].", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "We use the standard splits of sections 0-18 for training, 19-21 for development and 22-24 for test sets [27].", "startOffset": 104, "endOffset": 108}, {"referenceID": 26, "context": "with the results in [27].", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "Stanford Sentiment Treebank (SST) [2] is a popular dataset for sentiment classification.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "Table 3: Test accuracies on SST [2].", "startOffset": 32, "endOffset": 35}, {"referenceID": 34, "context": "All results as reported in [35]", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Initial experiments on the smaller WMT13 English-toFrench News Commentary dataset used in Kalchbrenner [36] show promising results.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "For the Seq-to-Seq LSTM, we use the hyperparameters listed in [16].", "startOffset": 62, "endOffset": 66}], "year": 2015, "abstractText": "Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a unified neural network framework which processes input sequences and questions, forms semantic and episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state of the art results on several types of tasks and datasets: question answering (Facebook\u2019s bAbI dataset), sequence modeling for part of speech tagging (WSJ-PTB), and text classification for sentiment analysis (Stanford Sentiment Treebank). The model relies exclusively on trained word vector representations and requires no string matching or manually engineered features.", "creator": "LaTeX with hyperref package"}}}