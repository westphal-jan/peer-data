{"id": "1506.08425", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2015", "title": "Deep-Plant: Plant Identification with convolutional neural networks", "abstract": "at this paper studies convolutional neural networks ( cnn ), to learn its unsupervised descriptive feature representations for approx 44 genuinely different plant species, collected at \" the cheltenham royal botanic gardens, great kew, england. trying to greatly gain morphological intuition conditioned on considering the chosen features far from calculating the cnn model ( opposed thus to adding a'thin black coloured box'solution ), perhaps a visualisation technique fully based based on the deconvolutional vocal networks ( dn ) is utilized. since it is mostly found that tree venations of vastly different spectral order have been directly chosen to globally uniquely represent explicitly each of the plant species. both experimental scientific results using these cnn programming features with 256 different classifiers show similar consistency and mutual superiority compared to rejecting the state - < of - the art vector solutions which rely on hand - crafted features.", "histories": [["v1", "Sun, 28 Jun 2015 16:58:47 GMT  (4084kb,D)", "http://arxiv.org/abs/1506.08425v1", "6 pages, 8 figures, accepted as oral presentation in ICIP2015, Qu\\'ebec City, Canada"]], "COMMENTS": "6 pages, 8 figures, accepted as oral presentation in ICIP2015, Qu\\'ebec City, Canada", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["sue han lee", "chee seng chan", "paul wilkin", "paolo remagnino"], "accepted": false, "id": "1506.08425"}, "pdf": {"name": "1506.08425.pdf", "metadata": {"source": "CRF", "title": "DEEP-PLANT: PLANT IDENTIFICATION WITH CONVOLUTIONAL NEURAL NETWORKS", "authors": ["Sue Han Lee", "Chee Seng Chan", "Paul Wilkin", "Paolo Remagnino"], "emails": ["{leesuehan@siswa.um.edu.my;", "cs.chan@um.edu.my;", "p.wilkin@kew.org;", "p.remagnino@kingston.ac.uk}"], "sections": [{"heading": null, "text": "Index Terms\u2014 plant classification, deep learning, feature visualisation"}, {"heading": "1. INTRODUCTION", "text": "Plants are the backbone of all life on earth providing us with food and oxygen. A good understanding of plants is essential to help in identifying new or rare plant species in order to improve the drug industry, balance the ecosystem as well as the agricultural productivity and sustainability [1]. Amongst all, botanists use variations on leaf characteristics as a comparative tool for their study on plants [1, 2]. This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials\nIn computer vision, despite many efforts [3\u20138] (i.e with sophisticated computer vision algorithms) have been conducted, plant identification is still considered a challenging and unsolved problem. This is because a plant in nature has very similar shape and colour representation as illustrated in Fig. 1. Kumar et al. [3] proposed an automatic plant species identification system namely Leafsnap. They identified plants based on curvature-based shape features of the leaf by utilizing integral measure to compute functions of the curvature at the boundary. Then, identification is done by nearest neighbours (NN). Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.\nAlthough successful, one must note that the performance of these aforementioned solutions is highly dependent on the chosen set of features which are task or dataset dependent. That is, it may suffer from the dataset bias problem [10].\nIn this paper, we propose to employ deep learning in a bottom-up and top-down manner for plant identification. In the former, we choose to use a convolutional neural networks (CNN) model to learn the leaf features as a means to perform plant classification. In the latter, rather than using the CNN as a black box mechanism, we employ deconvolutional networks (DN) to visualize the learned features. This is in order to gain visual understanding on which features are important to identify a leaf from different classes, thus avoiding the necessity of designing hand-crafted features. Empirically, our method outperforms state-of-the-art approaches [3, 9, 11] using the features learned from CNN model in classifying 44 different plant species.\nThis paper presents two contributions. First, we propose a CNN model to automatically learn the features representation for plant categories, replacing the need of designing hand-crafted features as to previous approaches [3, 9, 12, 13]. Second, we identify and diagnose the feature representation learnt by the CNN model through a visualisation strategy based on the DN. This is to avoid the use of the CNN model as a black box solution, and also provide an insight to re-\nar X\niv :1\n50 6.\n08 42\n5v 1\n[ cs\n.C V\n] 2\n8 Ju\nn 20\nsearchers on how the algorithm \"see\" or \"perceives\" a leaf. Finally, a new leaf dataset, named as MalayaKew (MK) Leaf Dataset is also collected with full annotation.\nThe rest of the paper is organized as follows: Section 2 reviews the concept of deep learning, in particular our CNN and DN model for plant identification. Section 3 presents our findings and a comparison with conventional solutions. Finally, conclusions are drawn in Section 4."}, {"heading": "2. PROPOSED APPROACH", "text": "In this section, we first explain how we employ the pre-trained CNN model to perform plant identification. Then, we detail how a DN model is utilised with our new visualisation strategy, to understand how the CNN model work in identifying different plant species. Fig. 3 depicts the overall framework of our approach."}, {"heading": "2.1. Convolutional Neural Network", "text": "The CNN model used in this paper is based on the model proposed in [14] with ILSVRC2012 dataset used for pre-training. Rather than training a new CNN architecture, we re-used the pre-trained network due to a) recent work [15] reported that features extracted from the activation of a CNN trained in a fully supervised manner on large-scale object recognition works can be re-purposed to a novel generic task; 2) our training set is not large as the ILSVRC2012 dataset. Indicated in [16], the performance of the CNN model is highly depending on the quantity and the level of diversity of training set, and finally c) training a deep model requires skill and experience. Also, it is time-consuming.\nFor our CNN model, we perform fine-tuning using a 44 classes leaf dataset collected at the Royal Botanic Gardens, Kew, England. Thus, the final fully connected layer is set to have 44 neurons replacing the original 1000 neurons. The full model of our CNN architecture is depicted in Fig. 2. The first convolutional layer filters the 227\u00d7227\u00d73 input leaf images with 96 kernels of size 11\u00d711\u00d73 with stride of 4 pixels. Then, the second convolutional layer takes the pooled feature maps from the first layer and convolved with 256 filters of size 5\u00d75\u00d748. Following this, the output is fed to the third and later to the fourth convolutional layer. The third and fourth convolutional layers which have 384 kernels of size 3\u00d73\u00d7256 and 384 kernels of size 3\u00d73\u00d7192 respectively perform only convolution without pooling. The fifth convolutional layer has 256 kernels of size 3\u00d73\u00d7192. After performing convolution and pooling in the fifth layer, the output is fed into fully-connected layers which have 4096 neurons. For the parameter setting, the learning rate multiplier of the filters and biases are set to 10 and 20, respectively."}, {"heading": "2.2. Deconvolutional Network", "text": "The CNN model learns and optimises the filters in each layer through the back propagation mechanism. These learned filters extract important features that uniquely represent the input leaf image. Therefore, in order to understand why and how the CNN model operates (instead of treating it as a \"black box\"), filter visualisation is required to observe the transformation of the features, as well as to understand the internal operation and the characteristic of the CNN model. Moreover, we can identify the unique features on the leaf images that are deemed important to characterize a plant from this process. [17, 18] introduced multi-layered DN that enable us to observe the transformation of the features by projecting the feature maps back to the input pixel space. Specifically, the feature maps from layer n are alternately deconvolved and unpooled continuously down to input pixel space. That is, given the feature maps, Y (l\u22121)i as:\nY (l\u22121) i =\nm (l) 1\u2211\nj=1\n(K (i) j,i ) T \u2217 Y (l)j (1)\nwhere layer l be a deconvolutional layer and K are the filters. To visualize our CNN model, we employ a strategy named as V1 based on the DN approach [17,18]. The purpose of V1 is to examine the overall highest activation parts across all feature maps for that layer l. So that, through the reconstructed image, we could observe the highly activated regions of the leaf in that layer. In order to do that, for all the absolute activations in that layer n, we consider only the first S largest pixel value with the rest are set to zero and projected down to pixel space to reconstruct an image as defined:\nY (l\u22121)s =\nm (l) 1\u2211\nj=1\n(K (i) j,i ) T \u2217 Y (l)j (2)\nwhere S = 1, 2, ....., size(Y (l)j ). With this, we could observe the highly activated regions of the leaf in that layer. The visual results of S = 1, S = 5 and S = \u2019All\u2019 are illustrated in Fig. 4."}, {"heading": "3. EXPERIMENTAL RESULTS", "text": ""}, {"heading": "3.1. Data Preparation", "text": "A new leaf dataset, named as MalayaKew (MK) Leaf Dataset which consists of 44 classes, collected at the Royal Botanic Gardens, Kew, England are employed in the experiment. Samples of the leaf dataset is illustrated in Fig. 1, and we could see that this dataset is very challenging as leaves from different classes have very similar appearance. A data (D1) is prepared to compare the performance of the trained CNN. That is, we use leaf images as a whole where in each leaf image, foreground pixels are extracted using the HSV colour space information. To enlarge the D1 dataset, we rotate the each leaf images in 7 different orientations, e.g. 45\u25e6, 90\u25e6, 135\u25e6, 180\u25e6, 225\u25e6, 270\u25e6 and 315\u25e6. We then randomly select 528 leaf images for testing and 2288 images for training."}, {"heading": "3.2. Results and Failure Analysis - D1", "text": "In this section, we present a comparative performance evaluation of the CNN model on plant identification. From Table 1, it is noticeable that using the features learnt from the CNN model (98.1%) outperforms state-of-the-art solutions [3,9,11] that employed carefully chosen hand-crafted features even when different classifiers are used. We performed failure analysis and observed that most of the misclassified leaves are from Class 2(4 misclassified), follow by Class 23(3), Class 9 & 27(2 each), and Class 38(1). From our investigation as illustrated in Fig. 5, the Q. robur f. purpurascens (i.e Class 2)\nwere misclassified as Q. acutissima (i.e Class 9) , Q. rubra \u2018Aurea\u2019 (i.e. Class 27) and Q. macranthera (Class 39), respectively; have almost the same outline shape as to Class 2. The rest of the misclassified testing images are also found to be misled by the same reason.\nIn order to further understand how and why the CNN fails, here we delve into the internal operation and behaviour of the CNN model via V1 strategy. We evaluate the one largest pixel value across the feature maps. Our observation from the reconstructed images in Fig 7 shows that the highly activated parts fall at the shape of the leaves. So, we deduce that leaf shape is not a good choice to identify plants."}, {"heading": "3.3. Results and Failure Analysis - D2", "text": "Here, we built a variant dataset (D2), where we manually crop each leaf image in the D1 into patches within the area of the leaf (so that no shape is included). This investigation is twofold. On one hand, we intend to know what is the precision of the plant identification classifier when the leaf shape is excluded ? On the other hand, we would like to find out if plant identification could be just done by patch of the leaf. Since\nthe original images range from 3000 \u00d7 3000 to 500 \u00d7 500, three different leave patch sizes (i.e 500 \u00d7 500, 400 \u00d7 400 and 256 \u00d7 256) are chosen. Similarly, we increase the diversity of the leaf patches by rotating them it in the same manner as to D1. We randomly select 8800 leaf patches for testing\nand 34672 leaf patches for training. In Table 1, we can see that the classification accuracy of the CNN model, trained using D2 (99.6%), is higher than using D1 (97.7%). Again, we perform the visualisation via V1 strategy as depicted in Fig. 8 to understand why the CNN trained with D2 has a better performance. From layer to layer, we notice that the activation part falls on not only the primary venation but also on the secondary venation and the departure between different order venations. Therefore, we could deduce that venation of different orders are more robust features for plant identification. This also agrees with some studies [19, 20] highlighting that quantitative leaf venation data have the potential to revolutionize the plant identification task. Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22]. However, as opposed to these solutions, we automatically learn the venation of different orders, while they use a set of heuristic rules that are hard to replicate.\nWe also analysed the drawbacks of our CNN model with D2 and observe that most of the misclassified patches are from Class 9(18 misclassified), follow by Class 2(13), Class 30(5), Class 28(3) and Class 1 , 31 & 42(1 each). The contributing factor of the misclassification seems to be the condition of the leaves, where the samples are noticeable affected by environmental factors such as wrinkled surface and insect damages. Example of such conditions are shown in Fig. 6."}, {"heading": "4. CONCLUSION", "text": "This paper studied a deep learning approach to learn discriminative features from leaf images with classifiers for plant identification. From the experimental results, we justified that learning the features through CNN can provide better feature representation for leaf images compared to hand-crafted features. Moreover, we demonstrated that venation structure is an important feature to identify different plant species with performance of 99.6%, outperforming conventional solutions. This is verified by analysing the internal operation and behaviour of the network through DN visualisation technique. In future work, we will extend the work to recognize in the wild."}, {"heading": "Acknowledgment", "text": "This research is supported by the High Impact MoE Grant UM.C/625/1/HIR/MoE/FCSIT/08, H-22001-00-B00008 from the Ministry of Education Malaysia."}, {"heading": "5. REFERENCES", "text": "[1] James S Cope, David Corney, Jonathan Y Clark, Paolo Remagnino, and Paul Wilkin, \u201cPlant species identification using digital morphometrics: A review,\u201d Expert\nSystems with Applications, vol. 39, no. 8, pp. 7562\u2013 7573, 2012.\n[2] James Clarke, Sarah Barman, Paolo Remagnino, Ken Bailey, Don Kirkup, Simon Mayo, and Paul Wilkin, \u201cVenation pattern analysis of leaf images,\u201d in Advances in Visual Computing, pp. 427\u2013436. Springer, 2006.\n[3] Neeraj Kumar, Peter N Belhumeur, Arijit Biswas, David W Jacobs, W John Kress, Ida C Lopez, and Jo\u00e3o VB Soares, \u201cLeafsnap: A computer vision system for automatic plant species identification,\u201d in ECCV, pp. 502\u2013516. Springer, 2012.\n[4] Cem Kalyoncu and \u00d6nsen Toygar, \u201cGeometric leaf classification,\u201d Computer Vision and Image Understanding, in Press, http://dx.doi.org/10.1016/j.cviu.2014.11.001.\n[5] Abdul Kadir, Lukito Edi Nugroho, Adhi Susanto, and Paulus Insap Santosa, \u201cLeaf classification using shape, color, and texture features,\u201d arXiv preprint arXiv:1401.4447, 2013.\n[6] Thibaut Beghin, James S Cope, Paolo Remagnino, and Sarah Barman, \u201cShape and texture based plant leaf classification,\u201d in Advanced Concepts for Intelligent Vision Systems, 2010, pp. 345\u2013353.\n[7] James Charters, Zhiyong Wang, Zheru Chi, Ah Chung Tsoi, and David Dagan Feng, \u201cEagle: A novel descriptor for identifying plant species using leaf lamina vascular features,\u201d in ICME-Workshop, 2014, pp. 1\u20136.\n[8] James S Cope, Paolo Remagnino, Sarah Barman, and Paul Wilkin, \u201cThe extraction of venation from leaf images by evolved vein classifiers and ant colony algorithms,\u201d in Advanced Concepts for Intelligent Vision Systems. Springer, 2010, pp. 135\u2013144.\n[9] David Hall, Chris McCool, Feras Dayoub, Niko Sunderhauf, and Ben Upcroft, \u201cEvaluation of features for leaf classification in challenging conditions,\u201d 2015.\n[10] Antonio Torralba and Alexei A Efros, \u201cUnbiased look at dataset bias,\u201d in CVPR, 2011, pp. 1521\u20131528.\n[11] Jianchao Yang, Kai Yu, Yihong Gong, and Thomas Huang, \u201cLinear spatial pyramid matching using sparse coding for image classification,\u201d in CVPR, 2009, pp. 1794\u20131801.\n[12] M\u00f3nica G Larese, Ariel E Bay\u00e1, Roque M Craviotto, Miriam R Arango, Carina Gallo, and Pablo M Granitto, \u201cMultiscale recognition of legume varieties based on leaf venation images,\u201d Expert Systems with Applications, vol. 41, no. 10, pp. 4638\u20134647, 2014.\n[13] Dalcimar Casanova, Jarbas Joaci de Mesquita Sa Junior, and Odemir Martinez Bruno, \u201cPlant leaf identification using gabor wavelets,\u201d International Journal of Imaging Systems and Technology, vol. 19, no. 3, pp. 236\u2013243, 2009.\n[14] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in NIPS, 2012, pp. 1097\u20131105.\n[15] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell, \u201cDecaf: A deep convolutional activation feature for generic visual recognition,\u201d arXiv preprint arXiv:1310.1531, 2013.\n[16] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang, \u201cLearning a deep convolutional network for image super-resolution,\u201d in ECCV, pp. 184\u2013199. Springer, 2014.\n[17] Matthew D Zeiler, Graham W Taylor, and Rob Fergus, \u201cAdaptive deconvolutional networks for mid and high level feature learning,\u201d in ICCV, 2011, pp. 2018\u20132025.\n[18] Matthew D Zeiler and Rob Fergus, \u201cVisualizing and understanding convolutional networks,\u201d in ECCV, pp. 818\u2013833. Springer, 2014.\n[19] Anita Roth-Nebelsick, Dieter Uhl, Volker Mosbrugger, and Hans Kerp, \u201cEvolution and function of leaf venation architecture: a review,\u201d Annals of Botany, vol. 87, no. 5, pp. 553\u2013566, 2001.\n[20] H\u00e9ctor Candela, Antonio Mart\u0131nez-Laborda, and Jos\u00e9 Luis Micol, \u201cVenation pattern formation in arabidopsis thaliana vegetative leaves,\u201d Developmental biology, vol. 205, no. 1, pp. 205\u2013216, 1999.\n[21] Adam Runions, Martin Fuhrer, Brendan Lane, Pavol Federl, Anne-Ga\u00eblle Rolland-Lagan, and Przemyslaw Prusinkiewicz, \u201cModeling and visualization of leaf venation patterns,\u201d ACM Transactions on Graphics, vol. 24, no. 3, pp. 702\u2013711, 2005.\n[22] Robert J Mullen, Dorothy Monekosso, Sarah Barman, Paolo Remagnino, and Paul Wilkin, \u201cArtificial ants to extract leaf outlines and primary venation patterns,\u201d in Ant Colony Optimization and Swarm Intelligence, pp. 251\u2013258. Springer, 2008."}], "references": [{"title": "Plant species identification using digital morphometrics: A review", "author": ["James S Cope", "David Corney", "Jonathan Y Clark", "Paolo Remagnino", "Paul Wilkin"], "venue": "Expert  Systems with Applications, vol. 39, no. 8, pp. 7562\u2013 7573, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Venation pattern analysis of leaf images", "author": ["James Clarke", "Sarah Barman", "Paolo Remagnino", "Ken Bailey", "Don Kirkup", "Simon Mayo", "Paul Wilkin"], "venue": "Advances in Visual Computing, pp. 427\u2013436. Springer, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Leafsnap: A computer vision system for automatic plant species identification", "author": ["Neeraj Kumar", "Peter N Belhumeur", "Arijit Biswas", "David W Jacobs", "W John Kress", "Ida C Lopez", "Jo\u00e3o VB Soares"], "venue": "ECCV, pp. 502\u2013516. Springer, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric leaf classification", "author": ["Cem Kalyoncu", "\u00d6nsen Toygar"], "venue": "Computer Vision and Image Understanding, in Press, http://dx.doi.org/10.1016/j.cviu.2014.11.001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Leaf classification using shape, color, and texture features", "author": ["Abdul Kadir", "Lukito Edi Nugroho", "Adhi Susanto", "Paulus Insap Santosa"], "venue": "arXiv preprint arXiv:1401.4447, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Shape and texture based plant leaf classification", "author": ["Thibaut Beghin", "James S Cope", "Paolo Remagnino", "Sarah Barman"], "venue": "Advanced Concepts for Intelligent Vision Systems, 2010, pp. 345\u2013353.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Eagle: A novel descriptor for identifying plant species using leaf lamina vascular features", "author": ["James Charters", "Zhiyong Wang", "Zheru Chi", "Ah Chung Tsoi", "David Dagan Feng"], "venue": "ICME-Workshop, 2014, pp. 1\u20136.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The extraction of venation from leaf images by evolved vein classifiers and ant colony algorithms", "author": ["James S Cope", "Paolo Remagnino", "Sarah Barman", "Paul Wilkin"], "venue": "Advanced Concepts for Intelligent Vision Systems. Springer, 2010, pp. 135\u2013144.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation of features for leaf classification in challenging conditions", "author": ["David Hall", "Chris McCool", "Feras Dayoub", "Niko Sunderhauf", "Ben Upcroft"], "venue": "2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Unbiased look at dataset bias", "author": ["Antonio Torralba", "Alexei A Efros"], "venue": "CVPR, 2011, pp. 1521\u20131528.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["Jianchao Yang", "Kai Yu", "Yihong Gong", "Thomas Huang"], "venue": "CVPR, 2009, pp. 1794\u20131801.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiscale recognition of legume varieties based on leaf venation images", "author": ["M\u00f3nica G Larese", "Ariel E Bay\u00e1", "Roque M Craviotto", "Miriam R Arango", "Carina Gallo", "Pablo M Granitto"], "venue": "Expert Systems with Applications, vol. 41, no. 10, pp. 4638\u20134647, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Plant leaf identification using gabor wavelets", "author": ["Dalcimar Casanova", "Jarbas Joaci de Mesquita Sa Junior", "Odemir Martinez Bruno"], "venue": "International Journal of Imaging Systems and Technology, vol. 19, no. 3, pp. 236\u2013243, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "NIPS, 2012, pp. 1097\u20131105.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1310.1531, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning a deep convolutional network for image super-resolution", "author": ["Chao Dong", "Chen Change Loy", "Kaiming He", "Xiaoou Tang"], "venue": "ECCV, pp. 184\u2013199. Springer, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["Matthew D Zeiler", "Graham W Taylor", "Rob Fergus"], "venue": "ICCV, 2011, pp. 2018\u20132025.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "ECCV, pp. 818\u2013833. Springer, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Evolution and function of leaf venation architecture: a review", "author": ["Anita Roth-Nebelsick", "Dieter Uhl", "Volker Mosbrugger", "Hans Kerp"], "venue": "Annals of Botany, vol. 87, no. 5, pp. 553\u2013566, 2001.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Venation pattern formation in arabidopsis thaliana vegetative leaves", "author": ["H\u00e9ctor Candela", "Antonio Mart\u0131nez-Laborda", "Jos\u00e9 Luis Micol"], "venue": "Developmental biology, vol. 205, no. 1, pp. 205\u2013216, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Modeling and visualization of leaf venation patterns", "author": ["Adam Runions", "Martin Fuhrer", "Brendan Lane", "Pavol Federl", "Anne-Ga\u00eblle Rolland-Lagan", "Przemyslaw Prusinkiewicz"], "venue": "ACM Transactions on Graphics, vol. 24, no. 3, pp. 702\u2013711, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Artificial ants to extract leaf outlines and primary venation patterns", "author": ["Robert J Mullen", "Dorothy Monekosso", "Sarah Barman", "Paolo Remagnino", "Paul Wilkin"], "venue": "Ant Colony Optimization and Swarm Intelligence, pp. 251\u2013258. Springer, 2008.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "A good understanding of plants is essential to help in identifying new or rare plant species in order to improve the drug industry, balance the ecosystem as well as the agricultural productivity and sustainability [1].", "startOffset": 214, "endOffset": 217}, {"referenceID": 0, "context": "Amongst all, botanists use variations on leaf characteristics as a comparative tool for their study on plants [1, 2].", "startOffset": 110, "endOffset": 116}, {"referenceID": 1, "context": "Amongst all, botanists use variations on leaf characteristics as a comparative tool for their study on plants [1, 2].", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 3, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 4, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 5, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 6, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 7, "context": "This is because leaf characteristics are available to be observed and examined throughout the year in deciduous, annual plants or year-round in evergreen perennials In computer vision, despite many efforts [3\u20138] (i.", "startOffset": 206, "endOffset": 211}, {"referenceID": 2, "context": "[3] proposed an automatic plant species identification system namely Leafsnap.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 108, "endOffset": 114}, {"referenceID": 5, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 108, "endOffset": 114}, {"referenceID": 6, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 137, "endOffset": 143}, {"referenceID": 7, "context": "Other solutions employed geometric [9], multi-scale distance matrix, moment invariants [4], colour, texture [5, 6] and venation features [7, 8] to identify a plant.", "startOffset": 137, "endOffset": 143}, {"referenceID": 9, "context": "That is, it may suffer from the dataset bias problem [10].", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "Empirically, our method outperforms state-of-the-art approaches [3, 9, 11] using the features learned from CNN model in classifying 44 different plant species.", "startOffset": 64, "endOffset": 74}, {"referenceID": 8, "context": "Empirically, our method outperforms state-of-the-art approaches [3, 9, 11] using the features learned from CNN model in classifying 44 different plant species.", "startOffset": 64, "endOffset": 74}, {"referenceID": 10, "context": "Empirically, our method outperforms state-of-the-art approaches [3, 9, 11] using the features learned from CNN model in classifying 44 different plant species.", "startOffset": 64, "endOffset": 74}, {"referenceID": 2, "context": "First, we propose a CNN model to automatically learn the features representation for plant categories, replacing the need of designing hand-crafted features as to previous approaches [3, 9, 12, 13].", "startOffset": 183, "endOffset": 197}, {"referenceID": 8, "context": "First, we propose a CNN model to automatically learn the features representation for plant categories, replacing the need of designing hand-crafted features as to previous approaches [3, 9, 12, 13].", "startOffset": 183, "endOffset": 197}, {"referenceID": 11, "context": "First, we propose a CNN model to automatically learn the features representation for plant categories, replacing the need of designing hand-crafted features as to previous approaches [3, 9, 12, 13].", "startOffset": 183, "endOffset": 197}, {"referenceID": 12, "context": "First, we propose a CNN model to automatically learn the features representation for plant categories, replacing the need of designing hand-crafted features as to previous approaches [3, 9, 12, 13].", "startOffset": 183, "endOffset": 197}, {"referenceID": 13, "context": "The CNN model used in this paper is based on the model proposed in [14] with ILSVRC2012 dataset used for pre-training.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "Rather than training a new CNN architecture, we re-used the pre-trained network due to a) recent work [15] reported that features extracted from the activation of a CNN trained in a fully supervised manner on large-scale object recognition works can be re-purposed to a novel generic task; 2) our training set is not large as the ILSVRC2012 dataset.", "startOffset": 102, "endOffset": 106}, {"referenceID": 15, "context": "Indicated in [16], the performance of the CNN model is highly depending on the quantity and the level of diversity of training set, and finally c) training a deep model requires skill and experience.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "[17, 18] introduced multi-layered DN that enable us to observe the transformation of the features by projecting the feature maps back to the input pixel space.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "[17, 18] introduced multi-layered DN that enable us to observe the transformation of the features by projecting the feature maps back to the input pixel space.", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "To visualize our CNN model, we employ a strategy named as V1 based on the DN approach [17,18].", "startOffset": 86, "endOffset": 93}, {"referenceID": 17, "context": "To visualize our CNN model, we employ a strategy named as V1 based on the DN approach [17,18].", "startOffset": 86, "endOffset": 93}, {"referenceID": 2, "context": "1%) outperforms state-of-the-art solutions [3,9,11] that employed carefully chosen hand-crafted features even when different classifiers are used.", "startOffset": 43, "endOffset": 51}, {"referenceID": 8, "context": "1%) outperforms state-of-the-art solutions [3,9,11] that employed carefully chosen hand-crafted features even when different classifiers are used.", "startOffset": 43, "endOffset": 51}, {"referenceID": 10, "context": "1%) outperforms state-of-the-art solutions [3,9,11] that employed carefully chosen hand-crafted features even when different classifiers are used.", "startOffset": 43, "endOffset": 51}, {"referenceID": 2, "context": "LeafSnap [3] SVM (RBF) 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "LeafSnap [3] NN 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "HCF [9] SVM (RBF) 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 8, "context": "HCF-ScaleRobust [9] SVM (RBF) 0.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "Combine [9] Sum rule (SVM (linear)) 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 10, "context": "SIFT [11] SVM (linear) 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 18, "context": "This also agrees with some studies [19, 20] highlighting that quantitative leaf venation data have the potential to revolutionize the plant identification task.", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "This also agrees with some studies [19, 20] highlighting that quantitative leaf venation data have the potential to revolutionize the plant identification task.", "startOffset": 35, "endOffset": 43}, {"referenceID": 1, "context": "Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22].", "startOffset": 77, "endOffset": 95}, {"referenceID": 7, "context": "Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22].", "startOffset": 77, "endOffset": 95}, {"referenceID": 11, "context": "Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22].", "startOffset": 77, "endOffset": 95}, {"referenceID": 20, "context": "Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22].", "startOffset": 77, "endOffset": 95}, {"referenceID": 21, "context": "Existing work that had employed venation to perform plant classification are [2, 8, 12, 21, 22].", "startOffset": 77, "endOffset": 95}], "year": 2015, "abstractText": "This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a \u2019black box\u2019 solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features.", "creator": "LaTeX with hyperref package"}}}