{"id": "1507.01698", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2015", "title": "Learning Tractable Probabilistic Models for Fault Localization", "abstract": "in recent 50 years, ` several probabilistic techniques have been recently applied informally to addressing various internal debugging analytics problems. \u2022 however, most substantially existing probabilistic debugging systems use relatively simple vector statistical forecast models, and necessarily fail to generalize it across multiple individual programs. as in this conceptual work, clearly we propose providing tractable multiple fault resistant localization regression models ( tflms ) entities that likely can be individually learned from dispersed data, internally and probabilistically infer the true location of learning the bug. while most than previous distributed statistical information debugging methods generalize over comparatively many simultaneous executions portion of a single program, general tflms are mostly trained based on a standard corpus portfolio of diverse previously seen generalized buggy predicting programs, and learn to identify recurring patterns north of bugs. widely - used fault localization techniques those such as tarantula closely evaluate the behavior suspiciousness of each line in isolation ; { in contrast, a traditional tflm expression defines how a joint probability distribution derived over buggy indicator variables for each long line. joint stability distributions linked with intrinsic rich branching dependency structure are already often computationally typically intractable ; mainstream tflms avoid this by exploiting recent concurrent developments in powerful tractable probabilistic simulation models ( - specifically, relational spns ). go further, tflms procedures can clearly incorporate small additional sources of information, without including coverage - risk based analytical features such as tarantula. usually we evaluate investigating the performance fault localization performance gains of tflms and that include tarantula scores as specific features in the stable probabilistic model. running our routine study test shows showed that testing the learned modern tflms isolate bugs more effectively than previous robust statistical methods or using tarantula directly.", "histories": [["v1", "Tue, 7 Jul 2015 08:04:56 GMT  (59kb,D)", "http://arxiv.org/abs/1507.01698v1", "Fifth International Workshop on Statistical Relational AI (StaR-AI 2015)"]], "COMMENTS": "Fifth International Workshop on Statistical Relational AI (StaR-AI 2015)", "reviews": [], "SUBJECTS": "cs.SE cs.LG", "authors": ["aniruddh nath", "pedro m domingos"], "accepted": true, "id": "1507.01698"}, "pdf": {"name": "1507.01698.pdf", "metadata": {"source": "CRF", "title": "Learning Tractable Probabilistic Models for Fault Localization", "authors": ["Aniruddh Nath", "Pedro Domingos"], "emails": ["pedrod}@cs.washington.edu"], "sections": [{"heading": "Introduction", "text": "According to a 2002 NIST study (RTI International 2002), software bugs cost the US economy an estimated $59.5 billion per year. While some of these costs are unavoidable, the report claimed that an estimated $22.2 billion could be saved with more effective tools for the identification and removal of software errors. Several other sources estimate that over 50% of software development costs are spent on debugging and testing (Hailpern and Santhanam 2002).\nThe need for better debugging tools has long been recognized. The goal of automating various debugging tasks has motivated a large body of research in the software engineering community. However, this line of work has only recently begun to take advantage of recent advances in probabilistic models, and their inference and learning algorithms.\nIn this work, we apply state-of-the-art probabilistic methods to the problem of fault localization. We propose Tractable Fault Localization Models (TFLMs) that can be learned from a corpus of known buggy programs (with the\nbug locations annotated). The trained model can then be used to infer the probable locations of buggy lines in a previously unseen program. Conceptually, a TFLM is a probability distribution over programs in a given language, modeled jointly with any attributes of interest (such as bug location indicator variables, or diagnostic features). Conditioned on a specific program, a TFLM defines a joint probability distribution over the attributes.\nThe key advantage of probabilistic models is their ability to learn from experience. Many software faults are instances of a few common error patterns, such as off-by-one errors and use of uninitialized values (Brun and Ernst 2004). Human debuggers improve with experience as they encounter more of these common fault patterns, and learn to recognize them in new programs. Automated debugging systems should be able to do the same.\nAnother advantage of probabilistic models is that they allow multiple sources of information to be combined in a principled manner. The relative contribution of each feature determined by its predictive value in the training corpus, rather than by a human expert. A TFLM can incorporate as features the outputs of other fault localization systems, such as the TARANTULA hue (Jones, Harrold, and Stasko 2002) of each line.\nIn recent years, there has been renewed interest in learning rich, tractable models, on which exact probabilistic inference can be performed in polynomial time (e.g. SumProduct Networks; Poon and Domingos 2011). TFLMs build on Relational Sum-Product Networks (Nath and Domingos 2015) to enable exact inference in space and time linear in the size of the program. We empirically compare TFLMs to the widely-used TARANTULA fault localization method, as well as the Statistical Bug Isolation (SBI) system, on four mid-sized C programs. TFLMs outperform the other systems on three of the four test subjects."}, {"heading": "Background", "text": ""}, {"heading": "Coverage-based Fault Localization", "text": "Coverage-based debugging methods isolate the bug\u2019s location by analyzing the program\u2019s coverage spectrum on a set of test inputs. These approaches take the following as input:\n1. a set of unit tests; 2. a record of whether or not the program passed each test;\nar X\niv :1\n50 7.\n01 69\n8v 1\n[ cs\n.S E\n] 7\nJ ul\n2 01\n5\n3. program traces, indicating which components (usually lines) of the program were executed when running each unit test.\nUsing this information, these methods produce a suspiciousness score for each component in the program. The most well-known method in this class is the TARANTULA system (Jones, Harrold, and Stasko 2002), which uses the following scoring function:\nSTarantula(s) = Failed(s)\nTotalFailed Passed(s)\nTotalPassed + Failed(s) TotalFailed\nHere, Passed(s) and Failed(s) are respectively the number of passing and failing test cases that include statement s, and TotalPassed and TotalFailed are the number of passing and failing test cases respectively. In an empirical evaluation (Jones and Harrold 2005), TARANTULA was shown to outperform previous methods such as cause transitions (Cleve and Zeller 2005), set union, set intersection and nearest neighbor (Renieris and Reiss 2003), making it the state of the art in fault localization at the time. Since the publication of that experiment, a few other scoring functions have been shown to outperform TARANTULA under certain conditions (Abreu et al. 2009). Nonetheless, TARANTULA remains the most well-known method in this class."}, {"heading": "Probabilistic Debugging Methods", "text": "Per-Program Learning Several approaches to fault localization make use of statistical and probabilistic methods. Liblit et al. proposed several influential statistical debugging methods. Their initial approach (Liblit et al. 2003) used `1- regularized logistic regression to predict non-deterministic program failures. (The instances are runs of a program, the features are instrumented program predicates, and the models are trained to predict a binary \u2018failure\u2019 variable. The learned weights of the features indicate which predicates are the most predictive of failure.) In later work (Liblit et al. 2005), they use a likelihood ratio hypothesis test to determine which predicates (e.g. branches, sign of return value) in an instrumented program are predictive of program failure. Zhang et al. (2011) evaluate several other hypothesis testing methods in a similar setting.\nThe SOBER system (Liu et al. 2005; Liu et al. 2006) improves on Liblit et al.\u2019s 2005 approach by taking into account the fact that a program predicate can be evaluated multiple times in a single test case. They learn conditional distributions over the probability of a predicate evaluating to true, conditioned on the success or failure of the test case. When these conditional distributions differ (according a statistical hypothesis test), the predicate is considered to be \u2018relevant\u2019 to the bug. The HOLMES system (Chilimbi et al. 2009) extends Liblit et al.\u2019s approach along another direction, analyzing path profiles instead of instrumented predicates.\nWong et al. (2008; 2012) use a crosstab-based statistical analysis to quantify the dependence between statement coverage and program failure. Their approach can be seen as a hybrid between the Liblit-style statistical analysis and TARANTULA-style spectrum-based analysis. Wong et al. (2009; 2012) also proposed two neural network-based fault\nlocalization techniques trained on program traces. Ascari et al. (2009) investigate the use of SVMs in a similar setting.\nMany of the methods described above operate under the assumption that the program contains exactly one bug. Some of these techniques have nevertheless been evaluated on programs with multiple faults, using an iterative process where the bugs are isolated one by one. Briand et al. (2007) explicitly extend TARANTULA to the multiple-bug case, by learning a decision tree to partition failing test cases. Each partition is assumed to model a different bug. Statements are ranked by suspiciousness using a TARANTULA-like scoring function, with the scores computed separately for each partition. Other clustering methods have also been applied to test cases; for example, Andrzejewski et al. (2007) use a form of LDA to discover latent \u2018bug topics\u2019.\nGeneralizing Across Programs The key limitation of the statistical and machine learning-based approaches discussed above is that they only generalize over many executions of a single program. Ideally, a machine learning-based debugging system should be able to generalize over multiple programs (or, at least, multiple sequential versions of a program). As discussed above, many software defects are instances of frequently occurring fault patterns; in principle, a machine learning model can be trained to recognize these patterns and use them to more effectively localize faults in new programs.\nThis line of reasoning has received relatively little attention in the automated debugging literature. The most prominent approach is the Fault Invariant Classifier (FIC) of Brun and Ernst (2004). FIC is not a fault localization algorithm in the sense of TARANTULA and the other approaches discussed above. Instead of localizing the error to a particular line, FIC outputs fault-revealing properties that can guide a human debugger to the underlying error. These properties can be computed using static or dynamic program analysis; FIC uses the Daikon dynamic invariant detector (Ernst et al. 2001). At training time, properties are computed for pairs of buggy and fixed programs; properties that occur in the buggy programs but not the fixed programs are labeled as \u2018fault-revealing\u2019. The properties are converted into programindependent feature vectors, and an SVM or decision tree is trained to classify properties as fault-revealing or non-faultrevealing. The trained classifier is then applied to properties extracted from a previously unseen, potentially faulty program, to reveal properties that indicate latent errors."}, {"heading": "Tractable Probabilistic Models Sum-Product Networks", "text": "A sum-product network (SPN) is a rooted directed acyclic graph with univariate distributions at the leaves; the internal nodes are (weighted) sums and (unweighted) products. Definition 1. (Gens and Domingos 2013)\n1. A tractable univariate distribution is an SPN. 2. A product of SPNs with disjoint scopes is an SPN. (The\nscope of an SPN is the set of variables that appear in it.) 3. A weighted sum of SPNs with the same scope is an SPN,\nprovided all weights are positive.\n4. Nothing else is an SPN.\nIntuitively, an SPN (fig. 1) can be thought of as an alternating set of mixtures (sums) and decompositions (products) of the leaf variables. If the values at the leaf nodes are set to the partition functions of the corresponding univariate distributions, then the value at the root is the partition function (i.e. the sum of the unnormalized probabilities of all possible assignments to the leaf variables). This allows the partition function to be computed in time linear in the size of the SPN.\nIf the values of some variables are known, the leaves corresponding to those variables\u2019 distributions are set to those values\u2019 probabilities, and the remainder are replaced by their (univariate) partition functions. This yields the unnormalized probability of the evidence, which can be divided by the partition function to obtain the normalized probability. The most probable state of the SPN, viewing sums as marginalized-out hidden variables, can also be computed in linear time. The first learning algorithms for sum-product networks used a fixed network structure, and only optimized the weights (Poon and Domingos 2011; Amer and Todorovic 2012; Gens and Domingos 2012). More recently, several structure learning algorithms for SPNs have also been proposed (Dennis and Ventura 2012; Gens and Domingos 2013; Peharz, Geiger, and Pernkopf 2013)."}, {"heading": "Relational Sum-Product Networks", "text": "SPNs are a propositional representation, modeling instances as independent and identically distributed (i.i.d.). Although the i.i.d. assumption is widely used in statistical machine learning, it is often an unrealistic assumption. In practice, objects usually interact with each other; Statistical Relational Learning algorithms can capture dependencies between objects, and make predictions about relationships between them.\nRelational Sum-Product Networks (RSPNs; Nath and Domingos 2015) generalize SPNs by modeling a set of instances jointly, allowing them to influence each other\u2019s probability distributions, as well as modeling probabilities of relations between objects. RSPNs can be seen as templates\nfor constructing SPNs, much like Markov Logic Networks (Richardson and Domingos 2006) are templates for Markov networks. RSPNs also require as input a part decomposition, which describes the part-of relationships among the objects in the mega-example. Unlike previous high-treewidth tractable relational models such as TML (Domingos and Webb 2012), RSPNs can generalize across mega-examples of varying size and structure."}, {"heading": "Tractable Fault Localization", "text": ""}, {"heading": "Tractable Fault Localization Models", "text": "A Tractable Fault Localization Model (TFLM) defines a probability distribution over programs in some deterministic language L. The distribution may also model additional variables of interest that are not part of the program itself; we refer to such variables as attributes. In the fault localization setting, the important attribute is a buggy indicator variable on each line. Other informative features may also be included as attributes; for instance, one or more coveragebased metrics may be included for each line. More formally, consider a language whose grammar L = (V,\u03a3, R, S) consists of: \u2022 V is a set of non-terminal symbols; \u2022 \u03a3 is a set of terminal symbols; \u2022 R is a set of production rules of the form \u03b1 \u2192 \u03b2, where \u03b1 \u2208 V and \u03b2 is a string of symbols in V \u222a \u03a3;\n\u2022 S \u2208 V is the start symbol. Definition 2. A Tractable Fault Localization Model for language L consists of: \u2022 a map from non-terminals in V to sets of attribute vari-\nables (discrete or continuous); \u2022 for each symbol \u03b1 \u2208 V , a set of latent subclasses \u03b11, . . . , \u03b1k; \u2022 \u03c0S , a probability distribution over subclasses of the start symbol S;\n\u2022 for each each subclass \u03b1i of \u03b1, \u2013 a univariate distribution \u03c8\u03b1i,x over each attribute x as-\nsociated with \u03b1; \u2013 for each rule \u03b1\u2192 \u03b2 in L, a probability distribution \u03c1\u03b1i\nover rules \u03b1i \u2192 \u03b2; \u2013 for each rule \u03b1i \u2192 \u03b2, for each non-terminal \u03b1\u2032 \u2208 \u03b2, a\ndistribution \u03c0(\u03b1i\u2192\u03b2),\u03b1\u2032 over subclasses of \u03b1 \u2032.\nThe univariate distribution over each attribute may be replaced with a joint distribution over all attributes, such as a logistic regression model within each subclass that predicts the value of the buggy attribute, using one or more other attributes as features. However, for simplicity, we present the remainder of this section with the attributes modeled as a product of univariate distributions, and assume that the attributes are discrete.\nBeing defined over the grammar of the programming language, TFLMs can capture information extremely useful for the fault localization task. For example, a TFLM can represent different fault probabilities for different symbols in the grammar. In addition, the latent subclasses give TFLMs a\ndegree of context-sensitivity; the same symbol can be more or less likely to contain a fault depending on its latent subclass, which is probabilistically dependent on the subclasses of ancestor and descendent symbols in the parse tree. This makes TFLMs much richer than models like logistic regression, where the features are independent conditioned on the class variable. Despite this representational power, exact inference in TFLMs is still computationally efficient. Example 1. The following rules are a fragment of the grammar of a Python-like language:\nw h i l e s t m t \u2192 \u2019 whi le \u2019 c o n d i t i o n \u2019 : \u2019 s u i t e c o n d i t i o n \u2192 exp r o p e r a t o r exp r c o n d i t i o n \u2192 \u2019 not \u2019 c o n d i t i o n\nWe refer to the above rules as r1, r2 and r3 respectively. The following is a partial specification of a TFLM over this grammar, with the while stmt symbol as root. \u2022 All non-terminal symbols have buggy and suspiciousness\nattributes. buggy is a fault indicator, and suspiciousness is a diagnostic attribute, such as a TARANTULA score.\n\u2022 Each non-terminal has two latent subclass symbols. For example, while stmt has subclasses while stmt1 and while stmt2. \u2022 The distribution over start symbols is: \u03c0(while stmt1) = 0.4, \u03c0(while stmt2) = 0.6. \u2022 For subclass symbol while stmt1 (subclass subscripts omitted): \u2013 \u03c8buggy \u223c Bernoulli(0.01) \u2013 \u03c8suspiciousness \u223c N (0.4, 0.05) \u2013 \u03c1(r1) = 1.0, since while stmt has a single rule. \u2013 The distributions over child symbol subclasses\nfor r1 are: \u03c0r1,condition(condition1) = 0.7, \u03c0r1,condition(condition2) = 0.3, \u03c0r1,suite(suite1) = 0.2, \u03c0r1,suite(suite2) = 0.8.\n(The complete TFLM specification would have similar definitions for all the other subclass symbols in the model.)\nSemantics Conceptually, a TFLM defines a probability distribution over all programs inL, and their attributes. More formally, the joint distribution P (T,A,C) is defined over: \u2022 a parse tree T ; \u2022 an attribute assignment A, specifying values of all at-\ntributes of all non-terminal symbols in T ; \u2022 latent subclass assignment C for each non-terminal in T .\nFor parse tree T containing rules r1 = \u03b11 \u2192 \u03b21, r2 = \u03b12 \u2192 \u03b22, . . . , rn = \u03b1n \u2192 \u03b2n, and root symbol \u03b1R, P (T,A,C) =\u220f ri ( \u03c1C(\u03b1i)(\u03b1i \u2192 \u03b2i)\u00d7 \u220f \u03b1\u2032\u2208\u03b2i \u03c0(C(\u03b1i)\u2192\u03b2i),\u03b1\u2032(C(\u03b1\n\u2032))\u00d7\u220f x\u2208attr(\u03b1i) \u03c8C(\u03b1i),x(A(x)) ) \u00d7 \u03c0S(C(\u03b1R))\nInference Like RSPNs, inference in TFLMs is performed by grounding out the model into an SPN. The SPN is constructed in a recursive top-down manner, beginning with the start node:\n\u2022 Emit a sum node over subclasses for the started node, weighted according to \u03c0S . Let the current symbol \u03b1 be the start symbol, and let \u03b1i be its subclass.\n\u2022 Emit a product node with one child for each attribute of \u03b1, and a child for the subprogram rooted at \u03b1.\n\u2022 For each attribute x of \u03b1, emit a sum node over the attribute values for the current symbol, weighted by \u03c8\u03b1i,x.\n\u2022 Emit a sum node over production rules \u03b1\u2192 \u03b2 for the current symbol, according to \u03c1\u03b1i . (Note that when grounding a TFLM over a known parse tree, all but one child of this sum node is zeroed out, and need not be grounded.)\n\u2022 Recurse over each non-terminal \u03b1\u2032 in \u03b2, choosing its subclass via a sum node weighted by \u03c0(\u03b1i\u2192\u03b2),\u03b1\u2032 .\nLearning The learning problem in TFLMs is to estimate the \u03c0 and \u03c8 distributions from a training corpus of programs, with known attribute values but unknown latent subclasses. (The \u03c1 distributions have no effect on the distribution of interest, since we assume that every program can be unambiguously mapped to a parse tree.)\nAs is commonly done with SPNs, we train the model via hard EM. In the E-step, given the current parameters of \u03c0 and \u03c8, we compute the MAP state of the training programs (i.e. the latent subclass assignment that maximizes the logprobability). In the M-step, we re-estimate the parameters of \u03c0 and \u03c8, choosing the values that maximize the logprobability. These two steps are repeated until convergence, or for a fixed number of iterations.\nIf the attributes are modeled jointly rather than as a product of univariate distributions, retraining the joint model in each iteration of EM is computationally expensive. A more efficient alternative is to use a product of univariates during EM, in order to learn a good subclass assignment. The joint model is then only trained once, at the conclusion of EM."}, {"heading": "Experiments", "text": "We performed an experiment to determine whether TFLM\u2019s ability to combine a coverage-based fault localization system with learned bug patterns improves fault localization performance, relative to using the coverage-based system directly. As a representative coverage-based method, our study used TARANTULA, one of the most widely-used approaches in this class, and a common comparison system for fault localization algorithms. We also compared to the statementbased version of Liblit et al.\u2019s Statistical Bug Isolation (SBI) system (Liblit et al. 2005), as adapted by Yu et al. (2008). SBI serves as a representative example of a lightweight statistical method for fault localization."}, {"heading": "Subjects", "text": "We evaluated TFLMs on four mid-sized C programs (table 1) from the Software-artifact Infrastructure Repository (Khurshid et al. 2004). All four test subjects are real-world programs, commonly used to evaluate fault localization approaches. The repository contained several sequential versions of each program, each with several buggy versions. The repository also contained a suite of between 124 and\n525 TSL tests for each version, which we used to compute the TARANTULA scores.\nThe number of executable lines was measured by the gcov tool. We excluded buggy versions where the bug occurred in a non-executable line (e.g. lines excluded by preprocessor directives), or consisted of line insertions or deletions. Unlike most previous fault localization studies that use these subjects, we do not exclude versions for which the test results were uniform (i.e. consisting entirely of passing or failing tests). Although coverage-only methods such as TARANTULA can provide no useful information in the case of uniform test suites, TFLMs can still make use of learned contextual information to determine that some lines are more likely than others to contain a fault."}, {"heading": "Methodology", "text": "We implemented TFLMs for a simplified version of the C grammar with 23 non-terminal symbols, ranging from compound statements like if and while to atomic single-line statements such as assignments and break and continue statements. Each symbol has a buggy attribute, and a suspiciousness attribute, which is the TARANTULA score of the corresponding line. (For AST nodes that correspond to multiple lines in the original code, we use the highest TARANTULA score among all lines). As described in the previous section, the attributes are modeled as independent univariates during EM (buggy as a Bernoulli distribution, and suspiciousness as a Gaussian), and then via a logistic regression model within each subclass. The model predicts the buggy attribute, using the TARANTULA score and a bias term as features. We use the SCIKIT-LEARN (Pedregosa et al. 2011) implementation of logistic regression, with the class weight=\u2018auto\u2019 parameter, to compensate for\nthe sparsity of the buggy lines relative to bug-free lines. For TFLMs, we ran hard EM for 100 iterations. For each subject program, TFLMs were learned via cross-validation, training on all versions of the program except the one being evaluated. The number of latent subclasses was also chosen via cross-validation, from the range [1, 4].\nThe output of a fault localization system is a ranking of the lines of code from most to least suspicious. For TFLMs, we ranked the lines by predicted probability that buggy = 1. (Each line in the original program is modeled by the finestgrained AST node that encloses it.) The evaluation metric was the \u2018fraction skipped\u2019 (FS) score, i.e. the fraction of executable lines ranked below the highest-ranked buggy line. Despite its limitations (Parnin and Orso 2011), this is a widely-used metric for fault localization (Jones and Harrold 2005; Abreu et al. 2009)."}, {"heading": "Results", "text": "Results of our experiments are displayed in tables 2 and 3, and figure 2. TFLMs outperform TARANTULA and SBI on three of the four subjects, isolating the majority of bugs more effectively, and earning a higher average FS score. However, TFLMs perform poorly on the gzip domain. This demonstrates the main threat to the validity of our method: machine learning algorithms operate under the assumption that the test data is drawn from a similar distribution to the training data. If the bugs occur in different contexts in the training and test datasets (as in gzip), learning-based methods may perform worse than methods that try to localize each program independently. This risk is particularly great when the learning from a small corpus of buggy programs.\nHowever, in three of the four subjects in our experiment, the training and test distributions are sufficiently similar to allow useful generalization, resulting in improved fault localization performance. TFLMs\u2019 advantage arises from its ability to localize faults even when the coverage matrix used by TARANTULA does not provide useful information (e.g. when the tests are not sufficiently discriminative). TFLMs combine the coverage-based information used by TARANTULA with learned bug probabilities for different symbols, in different contexts. Context sensitivity is captured via latent subclass assignments for each symbol.\nAs seen in table 4, our unoptimized Python implementation predicts bug probabilities in a few seconds for programs a few thousand lines in length. An optimized implementation may be able to make predictions at interactive speeds; this makes TFLMs a practical choice of inference engine for a debugging tool in a software development environment. Learning TFLMs can take several minutes, but note that the model can be trained offline, either from previous versions\nof the software being developed (as in our experiments), or from other related software projects expected to have a similar bug distribution (e.g. projects of a similar scale, written in the same language)."}, {"heading": "Conclusions", "text": "This paper presented TFLMs, a probabilistic model for fault localization that can be learned from a corpus of buggy programs. This allows the model to generalize from previously seen bugs to more accurately localize faults in a new context. TFLMs can also incorporate the output of other faultlocalization systems as features in the probabilistic model, with a learned weight that depends on the context. TFLMs take advantage of recent advances in tractable probabilistic models to ensure that the fault location probabilities can be inferred efficiently even as the size of the program grows. In our experiments, a TFLM trained with TARANTULA as a feature localized bugs more effectively than TARANTULA or SBI alone, on three of the four subject programs.\nIn this work, we used TFLMs to generalize across sequential versions of a single program. Given adequate training data, TFLMs could also be used to generalize across more distantly-related programs. The success of this approach relies on the assumption that there is some regularity in software faults, i.e. the same kinds of errors occur repeatedly\nin unrelated software projects, with sufficient regularity that a machine learning algorithm can generalize over these programs. Testing this assumption is a direction for future work.\nAnother direction for future work is extending TFLMs with additional sources of information, such as including multiple fault localization systems, and richer program features derived from static or dynamic analysis (e.g. invariants (Hangal and Lam 2002; Brun and Ernst 2004)). TFLMlike models may also be applicable to debugging methods that use path profiling (Chilimbi et al. 2009), giving the user more contextual information about the bug, rather than just a ranked list of statements. The recent developments in tractable probabilistic models may also enable advances in other software engineering problems, such as fault correction, code completion, and program synthesis."}, {"heading": "Acknowledgments", "text": "This research was partly funded by ARO grant W911NF-081-0242, ONR grants N00014-13-1-0720 and N00014-12-10312, and AFRL contract FA8750-13-2-0019. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ARO, ONR, AFRL, or the United States Government. We thank Michael Ernst and Sai Zhang for helpful discussions."}], "references": [{"title": "A practical evaluation of spectrum-based fault localization", "author": ["Abreu"], "venue": "The Journal of Systems and Software 82(11):1780\u20131792", "citeRegEx": "Abreu,? \\Q2009\\E", "shortCiteRegEx": "Abreu", "year": 2009}, {"title": "and Todorovic", "author": ["M.R. Amer"], "venue": "S.", "citeRegEx": "Amer and Todorovic 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical debugging using latent topic models", "author": ["Andrzejewski"], "venue": "In Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD)", "citeRegEx": "Andrzejewski,? \\Q2007\\E", "shortCiteRegEx": "Andrzejewski", "year": 2007}, {"title": "A", "author": ["L.C. Ascari", "L.Y. Araki", "Pozo"], "venue": "R. T.; ; and Vergilio, S. R.", "citeRegEx": "Ascari et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "L", "author": ["Briand"], "venue": "C.; Labiche, Y.; and Liu, X.", "citeRegEx": "Briand. Labiche. and Liu 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "and Ernst", "author": ["Y. Brun"], "venue": "M.", "citeRegEx": "Brun and Ernst 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "A", "author": ["T.M. Chilimbi", "B. Liblit", "K. Mehra", "Nori"], "venue": "V.; and Vaswani, K.", "citeRegEx": "Chilimbi et al. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "and Zeller", "author": ["H. Cleve"], "venue": "A.", "citeRegEx": "Cleve and Zeller 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "and Ventura", "author": ["A. Dennis"], "venue": "D.", "citeRegEx": "Dennis and Ventura 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Webb", "author": ["P. Domingos"], "venue": "A.", "citeRegEx": "Domingos and Webb 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "W", "author": ["M.D. Ernst", "J. Cockrell", "Griswold"], "venue": "G.; and Notkin, D.", "citeRegEx": "Ernst et al. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "and Domingos", "author": ["R. Gens"], "venue": "P.", "citeRegEx": "Gens and Domingos 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Domingos", "author": ["R. Gens"], "venue": "P.", "citeRegEx": "Gens and Domingos 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Santhanam", "author": ["B. Hailpern"], "venue": "P.", "citeRegEx": "Hailpern and Santhanam 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "M", "author": ["S. Hangal", "Lam"], "venue": "S.", "citeRegEx": "Hangal and Lam 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "M", "author": ["J.A. Jones", "Harrold"], "venue": "J.", "citeRegEx": "Jones and Harrold 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "M", "author": ["Jones, J.A.", "Harrold"], "venue": "J.; and Stasko, J.", "citeRegEx": "Jones. Harrold. and Stasko 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Software-artifact infrastructure repository (SIR)", "author": ["Khurshid"], "venue": "Retrieved 15 August,", "citeRegEx": "Khurshid,? \\Q2004\\E", "shortCiteRegEx": "Khurshid", "year": 2004}, {"title": "M", "author": ["B. Liblit", "A. Aiken", "A.X. Zheng", "Jordan"], "venue": "I.", "citeRegEx": "Liblit et al. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "M", "author": ["B. Liblit", "M. Naik", "A.X. Zheng", "A. Aiken", "Jordan"], "venue": "I.", "citeRegEx": "Liblit et al. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "S", "author": ["C. Liu", "X. Yan", "L. Fei", "J. Han", "Midkiff"], "venue": "P.", "citeRegEx": "Liu et al. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "S", "author": ["C. Liu", "L. Fei", "X. Yan", "J. Han", "Midkiff"], "venue": "P.", "citeRegEx": "Liu et al. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Domingos", "author": ["A. Nath"], "venue": "P.", "citeRegEx": "Nath and Domingos 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "and Orso", "author": ["C. Parnin"], "venue": "A.", "citeRegEx": "Parnin and Orso 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Scikit-learn: Machine learning in Python", "author": ["Pedregosa"], "venue": null, "citeRegEx": "Pedregosa,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa", "year": 2011}, {"title": "B", "author": ["Peharz, R.", "Geiger"], "venue": "C.; and Pernkopf, F.", "citeRegEx": "Peharz. Geiger. and Pernkopf 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Domingos", "author": ["H. Poon"], "venue": "P.", "citeRegEx": "Poon and Domingos 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "S", "author": ["M. Renieris", "Reiss"], "venue": "P.", "citeRegEx": "Renieris and Reiss 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "and Domingos", "author": ["M. Richardson"], "venue": "P.", "citeRegEx": "Richardson and Domingos 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Qi", "author": ["W.E. Wong"], "venue": "Y.", "citeRegEx": "Wong and Qi 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "W", "author": ["Wong"], "venue": "E.; Wei, T.; Qi, Y.; and Zhao, L.", "citeRegEx": "Wong et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "W", "author": ["Wong"], "venue": "E.; Debroy, V.; Golden, R.; Xu, X.; and Thuraisingham, B.", "citeRegEx": "Wong et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "W", "author": ["Wong"], "venue": "E.; Debroy, V.; and Xu, D.", "citeRegEx": "Wong. Debroy. and Xu 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "M", "author": ["Y. Yu", "J.A. Jones", "Harrold"], "venue": "J.", "citeRegEx": "Yu. Jones. and Harrold 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-parametric statistical fault localization", "author": ["Zhang"], "venue": "Journal of Systems and Software 84(6):885\u2013905", "citeRegEx": "Zhang,? \\Q2011\\E", "shortCiteRegEx": "Zhang", "year": 2011}], "referenceMentions": [], "year": 2015, "abstractText": "In recent years, several probabilistic techniques have been applied to various debugging problems. However, most existing probabilistic debugging systems use relatively simple statistical models, and fail to generalize across multiple programs. In this work, we propose Tractable Fault Localization Models (TFLMs) that can be learned from data, and probabilistically infer the location of the bug. While most previous statistical debugging methods generalize over many executions of a single program, TFLMs are trained on a corpus of previously seen buggy programs, and learn to identify recurring patterns of bugs. Widely-used fault localization techniques such as TARANTULA evaluate the suspiciousness of each line in isolation; in contrast, a TFLM defines a joint probability distribution over buggy indicator variables for each line. Joint distributions with rich dependency structure are often computationally intractable; TFLMs avoid this by exploiting recent developments in tractable probabilistic models (specifically, Relational SPNs). Further, TFLMs can incorporate additional sources of information, including coverage-based features such as TARANTULA. We evaluate the fault localization performance of TFLMs that include TARANTULA scores as features in the probabilistic model. Our study shows that the learned TFLMs isolate bugs more effectively than previous statistical methods or using TARANTULA directly.", "creator": "LaTeX with hyperref package"}}}