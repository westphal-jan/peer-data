{"id": "1507.01784", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2015", "title": "Rethinking LDA: Moment Matching for Discrete ICA", "abstract": "we consider moment matching techniques feasible for slope estimation in latent dirichlet allocation ( functional lda ). by drawing explicit links between quantitative lda and discrete dimensional versions of differential independent component analysis ( intrinsic ica ), we first substantially derive a largely new conceptual set of cumulant - based gradient tensors, with an improved local sample complexity. perhaps moreover, previously we readily reuse standard quantitative ica derivation techniques such ( as utilizing joint diagonalization of matrix tensors presented to improve compression over existing 2d methods - based uniformly on the tensor processing power method. in an extensive set of experiments elaborated on both traditional synthetic geometric and real datasets, first we show now that as our new combination of 2d tensors arrays and orthogonal joint group diagonalization scheme techniques outperforms existing classical moment matching methods.", "histories": [["v1", "Tue, 7 Jul 2015 12:48:30 GMT  (122kb,D)", "http://arxiv.org/abs/1507.01784v1", "27 pages. Under review"], ["v2", "Thu, 5 Nov 2015 20:16:04 GMT  (1318kb)", "http://arxiv.org/abs/1507.01784v2", "30 pages; added plate diagrams and clarifications, changed style, corrected typos, updated figures. in Proceedings of the 29-th Conference on Neural Information Processing Systems (NIPS), 2015"]], "COMMENTS": "27 pages. Under review", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["anastasia podosinnikova", "francis r bach", "simon lacoste-julien"], "accepted": true, "id": "1507.01784"}, "pdf": {"name": "1507.01784.pdf", "metadata": {"source": "CRF", "title": "Rethinking LDA: moment matching for discrete ICA", "authors": ["Anastasia Podosinnikova", "Francis Bach", "Simon Lacoste-Julien"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Topic models have emerged as flexible and important tools for the modelisation of text corpora. While early work has focused on graphical-model approximate inference techniques such as variational inference [6] or Gibbs sampling [21], tensor-based moment matching techniques have recently emerged as strong competitors due to their computational speed and theoretical guarantees [1, 3]. In this paper, we draw explicit links with the independent component analysis (ICA) literature (e.g. [19] and references therein) by showing a strong relationship between latent Dirichlet allocation (LDA) [6] and ICA [24, 25, 18]. We can then reuse standard ICA techniques and results, and derive new tensors with better sample complexity and new algorithms based on joint diagonalization."}, {"heading": "2 Is LDA discrete PCA or discrete ICA?", "text": "Notation. Following the text modeling terminology, we define a corpus X = {x1, . . . , xN} as a collection of N documents. Each document is a collection {wn1, . . . , wnLn} of Ln tokens. It is convenient to represent the `-th token of the n-th document as a 1-of-M encoding with an indicator vector wn` \u2208 {0, 1}M with only one non-zero, where M is the vocabulary size, and each document as the count vector xn := \u2211 ` wn` \u2208 RM . In such representation, the length Ln of the n-th document\nis Ln = \u2211 m xnm. We will always use index k \u2208 {1, . . . ,K} to refer to topics, index n \u2208 {1, . . . , N} to refer to documents, index m \u2208 {1, . . . ,M} to refer to words from the vocabulary, and index ` \u2208 {1, . . . , Ln} to refer to tokens of the n-th document.\nLatent Dirichlet allocation [6] is a generative probabilistic model for discrete data such as text corpora. In accordance to this model, the n-th document is modeled as an admixture over the vocabulary of M words with K latent topics. Specifically, the latent variable \u03b8n, which is sampled from the Dirichlet distribution, represents the topic mixture proportion over K topics for the n-th document. Given \u03b8n, the topic choice zn`|\u03b8n for the `-th token is sampled from the multinomial distribution with the probability vector \u03b8n. The token wn`|zn`, \u03b8n is then sampled from the multinomial distribution with the probability vector dzn` , or dk if k is the index of the non-zero element in zn`. This vector dk is the k-th topic, that is a vector of probabilities over the words from the vocabulary subject to the simplex constraint, i.e. dk \u2208 \u2206M , where \u2206M := {d \u2208 RM : d 0, \u2211 m dm = 1}. This generative process of a document (the index n is omitted for simplicity) can be summarized as\n\u03b8 \u223c Dirichlet(c), z`|\u03b8 \u223c Multinomial(1, \u03b8),\nw`|z`, \u03b8 \u223c Multinomial(1, dz`), (1)\nar X\niv :1\n50 7.\n01 78\n4v 1\n[ st\nat .M\nL ]\n7 J\nul 2\n01 5\nOne can think of the latent variables z` as auxiliary variables which were introduced for convenience of inference, but can in fact be marginalized out, which leads to the following model\n\u03b8 \u223c Dirichlet(c), x|\u03b8 \u223c Multinomial(L,D\u03b8),\nLDA model (2)\nwhere D \u2208 RM\u00d7K is the topic matrix with the k-th column equal to the k-th topic dk, and c \u2208 RK++ is the vector of parameters for the Dirichlet distribution. While a document is represented as a set of tokens w` in the formulation (1), the formulation (2) instead compactly represents a document as the count vector x. Although the two representations are equivalent, we focus on the second one in this paper and therefore refer to it as the LDA model.\nImportantly, the LDA model does not model the length of documents. Indeed, although the original paper [6] proposes to model the document length as L|\u03bb \u223c Poisson(\u03bb), this is never used in practice and, in particular, the parameter \u03bb is not learned. Therefore, in the way that the LDA model is typically used, it does not provide a complete generative process of a document as there is no rule to sample L|\u03bb. In this paper, this fact is important, as we need to model the document length in order to make the link with discrete ICA.\nDiscrete PCA. Principal component analysis has the following probabilistic interpretation [29, 28]\n\u03b8 \u223c Normal(0, IK), x|\u03b8 \u223c Normal(D\u03b8, \u03c32IM ),\n(3)\nwhere D \u2208 RM\u00d7K is a transformation matrix and \u03c3 is a parameter. As Buntine [9] mentions, the LDA model (2) can be seen as a discretization of the probabilistic PCA model (3) by replacing the normal likelihood with the multinomial one. By analogy with the self-conjuagacy of the normal distribution, the Dirichlet prior is chosen as the conjugate prior for the multinomial distribution. Due to the close relation of models (2) and (3), LDA can be interpreted as a discrete PCA model.\nDiscrete ICA (DICA). Interestingly, a small extension of the LDA model allows its interpretation as a discrete independent component analysis (DICA) model. The extension naturally arises when the document length for the LDA model is modeled as a random variable from the gamma-Poisson mixture (which is equivalent to a negative binomial random variable), i.e. L|\u03bb \u223c Poisson(\u03bb) and \u03bb \u223c Gamma(c0, b), where c0 := \u2211 k ck is the shape parameter and b > 0 is the rate parameter. The LDA model (2) with such document length is equivalent (see Appendix A.1) to\n\u03b1k \u223c Gamma(ck, b), xm|\u03b1 \u223c Poisson([D\u03b1]m),\nGP model (4)\nwhere all \u03b11, \u03b12, . . . , \u03b1K are mutually independent, the parameters ck coincide with the ones of the LDA model in (2), and the free parameter b can be seen (see Appendix A.2) as a scaling parameter for the document length when c0 is already prescribed.\nThis model was originally introduced by Canny [11] and later named as a discrete ICA model [10]. It is more natural, however, to name model (4) as the gamma-Poisson (GP) model and the model\n\u03b11, . . . , \u03b1K \u223c mutually independent, xm|\u03b1 \u223c Poisson([D\u03b1]m)\nDICA model (5)\nas the discrete ICA model. The only difference between (5) and the standard ICA model [24, 25, 18] (without additive noise) is the presence of the Poisson noise which enforces discrete, instead of continuous, values of xm. Note also that (a) the discrete ICA model is a semi-parametric model that can adapt to any distribution on the topic intensities \u03b1k and that (b) the GP model (4) is a particular case of both the LDA model (2) and the DICA model (5).\nThanks to this close connection between LDA and ICA, we can reuse standard ICA techniques to derive new efficient algorithms for topic modeling."}, {"heading": "3 Moment matching for topic modeling", "text": "In general, the method of moments is based on the idea of estimating latent parameters of a probabilistic model by matching theoretical expressions of moments with their sample estimates. The\nrecent line of work by Anandkumar et al. [1, 3] discusses applications of the method of moments to several latent variable models including LDA, which results in computationally fast learning algorithms with theoretical guarantees. For LDA, their key ideas are (a) construction of the moments of the LDA model with some particular diagonal structure (called the \u201cLDA moments\u201d in the following) and (b) development of algorithms for estimating the model parameters by exploiting this particular diagonal structure. As discussed later, these algorithms are a particular kind of joint diagonalization algorithms on the sample estimates of expressions involving moments.\nThis paper has a similar high-level structure. In Section 3.1, we introduce novel cumulants for the GP/DICA models (called the \u201cGP/DICA cumulants\u201d in the following), which have a similar structure to the one of the LDA moments. This structure allows to reapply the algorithms of [1, 3] for the estimation of the model parameters, with the same theoretical guarantees. In addition, in Section 4, we consider another algorithm, which in turn is applicable to both the LDA moments and the GP/DICA cumulants. In Section 5, we experimentally compare these algorithms."}, {"heading": "3.1 Cumulants of the GP and DICA models", "text": "In this section, we derive and analyze the novel cumulants of the DICA model. As the GP model is a particular case of the DICA model, all results of this section extend to the GP model.\nThe first three cumulant tensors1 for the random vector x can be defined as follows\ncum(x) := E(x), (6) cum(x, x) := cov(x, x) = E [ (x\u2212 E(x))(x\u2212 E(x))> ] , (7)\ncum(x, x, x) := E [(x\u2212 E(x))\u2297 (x\u2212 E(x))\u2297 (x\u2212 E(x))] , (8)\nwhere \u2297 denotes the tensor product (see some properties of cumulants in Appendix B.1). The essential property of the cumulants (which does not hold for moments) that we use in this paper is that the cumulant tensor for a random vector with independent components is diagonal.\nLet y = D\u03b1; then for the Poisson random variable xm|ym \u223c Poisson(ym), the expectation is E(xm|ym) = ym. Hence, by the law of total expectation and the linearity of expectation, the expectation in (6) has the following form\nE(x) = E(E(x|y)) = E(y) = DE(\u03b1). (9)\nFurther, the variance of the Poisson random variable xm is var(xm|ym) = ym and, as x1, x2, . . . , xM are conditionally independent given y, then their covariance matrix is diagonal, i.e. cov(x, x|y) = diag(y). Therefore, by the law of total covariance, the covariance in (7) has the form\ncov(x, x) = E [cov(x, x|y)] + cov [E(x|y),E(x|y)] = diag [E(y)] + cov(y, y) = diag [E(x)] +Dcov(\u03b1, \u03b1)D>,\n(10)\nwhere the last equality follows by the multilinearity property of cumulants (see Appendix B.1). Moving the first term from the RHS of (10) to the LHS, we define\nS := cov(x, x)\u2212 diag [E(x)] . DICA S-cum. (11)\nFrom (10) and by the independence of \u03b11, . . . , \u03b1K (see Appendix B.3), S has the following diagonal structure S = \u2211\nk var(\u03b1k)dkd\n> k = Ddiag [var(\u03b1)]D >. (12)\nBy analogy with the second order case, using the law of total cumulance, the multilinearity property of cumulants, and the independence of \u03b11, . . . , \u03b1K , we derive in Appendix B.2 expression (22), similar to (10), for the third cumulant (8). Moving the terms in this expression, we define a tensor T with the following element\n[T ]m1m2m3 := cum(xm1 , xm2 , xm3) + 2\u03b4(m1,m2,m3)E(xm1) DICA T-cum. (13) \u2212 \u03b4(m2,m3)cov(xm1 , xm2)\u2212 \u03b4(m1,m3)cov(xm1 , xm2)\u2212 \u03b4(m1,m2)cov(xm1 , xm3),\n1The 2nd and 3rd cumulants coincide with the 2nd and 3rd central moments (but not at higher order).\nwhere \u03b4 is the Kronecker delta. By analogy with (12) (Appendix B.3), the tensor T has the diagonal structure T = \u2211\nk cum(\u03b1k, \u03b1k, \u03b1k)dk \u2297 dk \u2297 dk. (14)\nIn Appendix D.1, we recall (in our notation) the matrix S (37) and the tensor T (38) for the LDA model [1], which are analogues of the matrix S (11) and the tensor T (13) for the GP/DICA models. Slightly abusing terminology, we refer to the matrix S (37) and the tensor T (38) as the \u201cLDA moments\u201d and to the matrix S (11) and the tensor T (13) as the \u201cGP/DICA cumulants\u201d. The diagonal structure (39) & (40) of the LDA moments is similar to the diagonal structure (12) & (14) of the GP/DICA cumulants, though arising through a slightly different argument, as discussed at the end of Appendix D.1. Importantly, due to this similarity, the algorithmic frameworks for both the GP/DICA cumulants and the LDA moments coincide.\nThe following sample complexity results apply to the sample estimates of the GP cumulants:2 Proposition 3.1. Under the GP model, the expected error for the sample estimator S\u0302 (27) for the GP cumulant S (11) is:\nE [ \u2016S\u0302 \u2212 S\u2016F ] \u2264 \u221a E [ \u2016S\u0302 \u2212 S\u20162F ] \u2264 O ( 1\u221a N max [ \u2206L\u03042, c\u03040L\u0304 ]) , (15)\nwhere \u2206 := max k \u2016dk\u201622, c\u03040 := min(1, c0) and L\u0304 := E(L).\nA high probability bound could be derived using concentration inequalities for Poisson random variables [7]; but the expectation already gives the right order of magnitude for the error (for example via Markov\u2019s inequality). By following a similar analysis as in [2], we can rephrase the topic recovery error in term of the error on the GP cumulant. Importantly, the whitening transformation redivides the error on S (15) by L\u03042, which is the scale of S (see Appendix C.5 for details). This means that the contribution from S\u0302 to the recovery error will scale as O( 1\u221a\nN max{\u2206, c\u03040 L\u0304 }), where both \u2206 and c\u03040 L\u0304\nare smaller than 1 and can be very small. We do not present the exact expression for the expected squared error for the estimator of T , but due to a similar structure in the derivation, we expect the analogous bound of E [ \u2016T\u0302 \u2212 T\u2016F ] \u2264 1\u221a\nN max\n[ \u22063/2L\u03043, c\u0304\n3/2 0 L\u0304\n3/2 ] . In Appendix B.4, we present\nthe expression (27) for an unbiased finite sample estimate S\u0302 of S and the expression (28) for an\nunbiased finite sample estimate T\u0302 of T . A sketch of a proof for Proposition 3.1 can be found in Appendix C. Current sample complexity results of the LDA moments [1] can be summarized as O(1/ \u221a N). However, the proof (which can be found in the supplementary material [2]) analyzes only the case when finite sample estimates of the LDA moments are constructed from one triple per document, i.e. w1 \u2297 w2 \u2297 w3 only, and not from the U-statistics that average multiple (dependent) triples per document as in the practical expressions (41) and (42). Moreover, one has to be careful when comparing upper bounds. Nevertheless, comparing the bound (15) with the current theoretical results for the LDA moments, we see that the GP/DICA cumulants sample complexity contains the `2- norm of the columns of the topic matrix D in the numerator, as opposed to the O(1) coefficient for the LDA moments. This norm can be significantly smaller than 1 for vectors in the simplex (e.g. \u2206 = O(1/\u2016dk\u20160) for sparse topics). This suggests that the GP/DICA cumulants may have better finite sample convergence properties than the LDA moments and our experimental results in Section 5.2 are indeed consistent with this statement.\nThe GP/DICA cumulants have a somewhat more intuitive derivation than the LDA moments as they are expressed via the count vectors x (which are the sufficient statistics for the model) and not the tokens w`\u2019s. Note also that the construction of the LDA moments depend on the unknown parameter c0. Given that we are in an unsupervised setting and that moreover the evaluation of LDA is a difficult task [31], setting this parameter is non-trivial. In Appendix F.1, we investigate this dependence experimentally and observe that the LDA moments are somewhat sensitive to the choice of c0."}, {"heading": "4 Diagonalization algorithms", "text": "How is the diagonal structure (12) of S (11) and (14) of T (13) going to be helpful for the estimation of the model parameters? This question has already been thoroughly investigated in the signal\n2Note that the expected squared error for the DICA cumulants is similar, but the expressions are less compact and, in general, depend on the prior on \u03b1k.\nprocessing literature more than two decades ago (see, e.g., [12, 13, 15, 23, 16, 19] and references therein) and was recently brought back to the machine learning community (see [1, 3] and references therein), approach that we review in this section. Note that the algorithms of this section apply to both the LDA moments and the GP/DICA cumulants due to their similar diagonal structure.\nFor simplicity, let us rewrite expressions (12) and (14) for S and T as follows S = \u2211\nk skdkd\n> k , T = \u2211 k tkdk \u2297 dk \u2297 dk, (16)\nwhere sk := var(\u03b1k) and tk := cum(\u03b1k, \u03b1k, \u03b1k). Introducing the rescaled topics d\u0303k := \u221a skdk, we can also rewrite S = D\u0303D\u0303>. Following the same assumption from [1] that the topic vectors are linearly independent (and thus D\u0303 has full rank), we can compute a whitening matrix W \u2208 RK\u00d7M of S, i.e. a matrix such that WSW> = IK where IK is the K-by-K identity matrix (see Appendix E.1 for more details). We then obtain that the vectors zk := Wd\u0303k form an orthonormal set of vectors.\nFurther, let us define a projection T (v) \u2208 RK\u00d7K of a tensor T \u2208 RK\u00d7K\u00d7K onto a vector u \u2208 RK :\nT (u)k1k2 := \u2211\nk3 Tk1k2k3uk3 . (17)\nApplying the multilinear transformation (see, e.g., [3] for the definition) with W> to the tensor T from (16) and then projecting the resulting tensor T := T (W>,W>,W>) onto some vector u \u2208 RK , we obtain\nT (u) = \u2211\nk t\u0303k\u3008zk, u\u3009zkz>k (18)\nwhere t\u0303k := tk/s 3/2 k is due to the rescaling of topics and \u3008\u00b7, \u00b7\u3009 stands for the inner product. As the vectors zk are orthonormal, the pairs zk and \u03bbk := t\u0303k\u3008zk, u\u3009 can be seen as eigenpairs of the matrix T (u), which are uniquely defined if the eigenvalues \u03bbk are all different. If they are unique, we can recover the GP/DICA (as well as LDA) model parameters via d\u0303k = W \u2020zk and t\u0303k = \u03bbk/\u3008zk, u\u3009.\nThis, in fact, is the spectral algorithm for LDA [1] and its predecessor, the fourth-order3 blind identification algorithm [12, 13]. Indeed, one can define finite sample estimates4 S\u0302 and T\u0302 of S (11) and T (13) and expect that they possess approximately the diagonal structure (12) and (14) and, therefore, the reasoning from above can be applied, under the assumption that the effect of the sampling error is controlled.\nThis spectral algorithm, however, is known to be quite unstable in practice (see, e.g., [14]). To overcome this problem, some other algorithms were proposed. The most notable ones are probably the FastICA algorithm [23] and the JADE algorithm [16]. The FastICA algorithm, with appropriate choice of a contrast function, estimates iteratively the topics, making use of the orthonormal structure (18), and performs the deflation procedure at every step. The recently introduced tensor power method (TPM) for the LDA model [3] is close to the FastICA algorithm. Alternatively, the JADE algorithm modifies the spectral algorithm by performing multiple projections for (18) and then jointly diagonalizing the resulting matrices with an orthogonal matrix. The spectral algorithm is a special case of this orthogonal joint diagonalization algorithm when only one projection is chosen. Importantly, a fast implementation [17] of the orthogonal joint diagonalization algorithm from [8] was proposed, which is based on closed-form iterative Jacobi updates (see, e.g., [27] for the later).\nIn practice, the orthogonal joint diagonalization (JD) algorithm is more robust than FastICA (see, e.g., [5, p.30]) or the spectral algorithm. Moreover, although the application of the JD algorithm for the learning of topic models was mentioned in the literature [3, 26], it was never implemented in practice. In this paper, we apply the JD algorithm for the diagonalization of the GP/DICA cumulants as well as the LDA moments, which is described in Algorithm 1. Note that the choice of a projection vector vp \u2208 RM obtained as vp = W\u0302>up for some vector up \u2208 RK is important and corresponds to the multilinear transformation of T\u0302 with W\u0302> along the third mode. Importantly, in Algorithm 1, the joint diagonalization routine is performed over (P + 1) K \u00d7K matrices, where\n3Note that (a) the factorization of S = D\u0303D\u0303> does not uniquely determine D\u0303 as one can equivalently use S = (D\u0303U)(D\u0303U)> with any orthogonal K \u00d7 K matrix U . Therefore, one has to consider higher than the second order information; (b) in ICA the fourth-order tensors are used, because the third cumulant of the Gaussian distribution is zero, which is not the case in the DICA/LDA models, where the third order information is sufficient.\n4The precise expression for our finite sample estimates S\u0302 (27) and T\u0302 (28) of S (11) and T (13) for the GP/DICA cumulants is somewhat cumbersome and is therefore moved to Appendix E.2. For completeness, we also present the finite sample estimates S\u0302 (41) and T\u0302 (42) of S (37) and T (38) for the LDA moments (which are consistent with the ones suggested in [3]) in Appendix E.3.\nthe number of topics K is usually not too big. This makes the algorithm computationally fast (Appendix E discusses the computational complexity). The same is true for the spectral algorithm, but not for TPM.\nAlgorithm 1 Orthogonal joint diagonalization (JD) algorithm for GP/DICA cumulants (or LDA moments)\n1: Input: X \u2208 RM\u00d7N , K, P (number of random projections); (and c0 for LDA moments) 2: Compute sample estimate S\u0302 \u2208 RM\u00d7M ((27) for GP/DICA / (41) for LDA in Appendix E) 3: Estimate whitening matrix W\u0302 \u2208 RK\u00d7M of S\u0302 (see Appendix E.1) 4: option (a) Choose vectors {u1, u2, . . . , uP } \u2286 RK uniformly at random from the unit `2-sphere\nand set vp = W\u0302 >up \u2208 RM for all p = 1, . . . , P (P = 1 yields spectral algorithm)\n5: option (b) Choose vectors {u1, u2, . . . , uP } \u2286 RK as the canonical basis e1, e2, . . . , eK of RK and set vp = W\u0302\n>up \u2208 RM for all p = 1, . . . ,K 6: For \u2200p, compute Bp = W\u0302 T\u0302 (vp)W\u0302> \u2208 RK\u00d7K ((50) for GP/DICA / (52) for LDA; Appendix E) 7: Perform orthogonal joint diagonalization of matrices {W\u0302 S\u0302W\u0302> = IK , Bp, p = 1, . . . , P} (see [8]\nand [17]) to find an orthogonal matrix V \u2208 RK\u00d7K and vectors {a1, a2, . . . , aP } \u2282 RK such that\nV W\u0302 S\u0302W\u0302>V > = IK , and V BpV > \u2248 diag(ap), p = 1, . . . , P\n8: Output: joint diagonalization matrix A = V W\u0302 and values ap, p = 1, . . . , P\nIn Section 5.1, we compare experimentally the performance of the spectral, JD, and TPM algorithms for the estimation of the parameters of the GP/DICA as well as LDA models. We are not aware of any experimental comparison of these algorithms in the LDA context. While already working on this manuscript, the JD algorithm was also independently analyzed by [26] in the context of tensor factorization for general latent variable models. However, [26] focused mostly on the comparison of approaches for tensor factorization and their stability properties, with brief experiments using a latent variable model related but not equivalent to LDA for community detection. In contrast, we provide a detailed experimental comparison in the context of LDA in this paper, as well as propose a novel cumulant-based estimator.\nModel parameters recovery. Algorithm 1 outputs a joint diagonalization matrix A \u2208 RK\u00d7M that has the property that AD should be approximately diagonal up to a permutation of the columns of D. The standard approach [1] of taking the pseudo-inverse of A to get an estimate of the topic matrix D has a problem that it does not preserve the simplex constraint of the topics (in\nparticular, the non-negativity of D\u0303). Due to the space constraints, we do not discuss this issue here, but we observed experimentally that this can potentially significantly deteriorate performance of all moment matching algorithms for LDA. We made an attempt to solve this problem by integrating the non-negativity constraint into the Jacobi-updates procedure of the orthogonal joint diagonalization algorithm, but the obtained results did not lead to any significant improvement. Therefore, for our experiments, we estimate the topic matrix by thresholding the negative values of the pseudo-inverse of A: d\u0302k := max(0, [A\n\u2020]:k)/\u2016max(0, [A\u2020]:k)\u20161, where [A\u2020]:k is the k-th column of the pseudo-inverse A\u2020 of A (see Appendix E.4 for more details on the recovery of the model parameters for the GP model), and leave this issue as an open question for future research."}, {"heading": "5 Experiments", "text": "In this section, (a) we compare experimentally the GP/DICA cumulants with the LDA moments and (b) we compare experimentally the spectral algorithm [1], the tensor power method [3] (TPM), the orthogonal joint diagonalization (JD) algorithm from Algorithm 1, and the variational inference algorithm for LDA [6].\nReal data: the associated press (AP) dataset, from D. Blei\u2019s web page5, with N = 2, 243 documents\nand M = 10, 473 words in the vocabulary and the average document length L\u0302 = 194; the NIPS papers dataset6 [20] of N = 2, 483 NIPS papers and M = 14, 036 words in the vocabulary, and the average document length L\u0302 = 1, 321; the KOS blog entries dataset7, from the UCI Repository, with\n5http://www.cs.columbia.edu/~blei/lda-c/index.html 6http://ai.stanford.edu/~gal/data.html 7https://archive.ics.uci.edu/ml/datasets/Bag+of+Words\nN = 3, 430 documents and M = 6, 906 words in the vocabulary, and the average document length L\u0302 = 136. As the LDA moments require at least three tokens in each document, 1 document from the NIPS dataset and 3 documents from the AP dataset, which did not fulfill this requirement, were removed.\nSemi-synthetic data are constructed by analogy with [4] and provide ground truth information for evaluation. First, the LDA parameters D and c are learned from the real datasets with the variational inference LDA and, then, toy data are sampled from a model of interest with the given parameters D and c. For each setting, data are sampled 5 times and the results are averaged. We plot error bars that are the minimum and maximum values. This provides the ground truth parameters D and c. For the AP data, K \u2208 {10, 50} topics are learned and, for the NIPS data, K \u2208 {10, 90} topics are learned. For larger K, the obtained topic matrix is ill-conditioned, which violates the identifiability condition for topic recovery using moment matching techniques [1]. All the documents with less than 3 tokens were resampled.\nSampling techniques. All the sampling models have the parameter c which is set to c = c0c\u0304/ \u2016c\u0304\u20161, where c\u0304 is the learned c from the real dataset with variational LDA, and c0 is a parameter that we can vary. The GP data are sampled from the gamma-Poisson model (4) with b = c0/L\u0302 so that the expected document length is L\u0302 (see Appendix A.2). The LDA-fix(L) data are sampled from the LDA model (2) with the document length being fixed to a given L. The LDA-fix2(\u03b3,L1,L2) data are sampled as follows: (1\u2212 \u03b3)-portion of the documents are sampled from the LDA-fix(L1) model with a given document length L1 and \u03b3-portion of the documents are sampled from the LDA-fix(L2) model with a given document length L2.\nEvaluation. Evaluation of topic recovery for semi-synthetic data is performed with the `1-error between the recovered D\u0302 and true D topic matrices with the best permutation of columns:\nerr`1(D\u0302,D) := min \u03c0\u2208PERM\n1\n2K \u2211 k \u2016d\u0302\u03c0k \u2212 dk\u20161 \u2208 [0, 1]. (19)\nThe minimization is over the possible permutations \u03c0 \u2208 PERM of the columns of D\u0302 and can be efficiently obtained with the Hungarian algorithm for bipartite matching. For the evaluation of topic recovery in the real data case, we use an approximation of the log-likelihood for held out documents as the metric. The approximation is computed using a Chib-style method as described by [31] using the authors\u2019 implementation8. Note that this evaluation method is also applicable for the GP model, as it is a particular case of the LDA model.\nCode and complexity. We used our own Matlab implementations of the GP/DICA cumulants, the LDA moments, the spectral algorithm, and the tensor power method, as, to our knowledge, no efficient implementation of these algorithms was available for LDA. The expressions (50) and (52) provide efficient formulas for fast computation of the GP/DICA cumulants and LDA moments (O(RNK), where R is the largest number of non-zeros in the count vector x over all documents), which makes even the Matlab implementation fast for large datasets. For the orthogonal joint diagonalization algorithm, we implemented a faster C++ version of the previous Matlab implementation by J.-F. Cardoso. For variational inference, we used the code of D. Blei and modified it for the estimation of a non-symmetric Dirichlet prior c, which is known to be important [30]. The default values of the tolerance/maximum number of iterations parameters are used for variational inference. The computational complexity of one iteration for one document of the variational inference algorithm is O(RK), where R is the number of non-zeros in the count vector for this document, which is then performed a significant number of times. Each experiment was run in a single thread.\nNote that (a) for the large vocabulary size M , the computation of a whitening matrix can be expensive (in terms of both memory and time) and (b) the bottle-neck for the spectral, JD, and TPM algorithms is the computation of the cumulants/moments. One possible solution for (a) is to reduce the vocabulary size with, e.g., TF-IDF score, which is a standard practice in the topic modeling context. Another option is using a stochastic eigendecomposition (see, e.g., [22]) to approximate the whitening matrix. For (b): the spectral algorithm estimates the cumulants/moments only once and, therefore, is fast; joint estimation of P cumulants/moments for JD can be (and is) implemented much faster than estimation of P cumulants/moments by precomputing and reusing terms (e.g. WX) which appear in all cumulants/moments; for TPM, some parts of the cumulants/moments can also be precomputed, but as TPM normally does many more iterations than P , it is significantly slower. Note that the number of random restarts for TPM within one deflation step is set to 10 and\n8http://homepages.inf.ed.ac.uk/imurray2/pub/09etm/\nthe maximum number of iterations for every run is set to 100; the run with the best objective is chosen. It is known that the runs which converge to a good solution converge fast [3].\nParameter c0 for LDA. The construction of the LDA moments requires the parameter c0. For the semi-synthetic experiments, the true value of c0 is provided to the algorithms. It means that the LDA moments, in this case, have access to some oracle information, which in practice is never available. For real data experiments, c0 is set to the value obtained with variational inference. Experiments in Appendix F.1 show that this choice was somewhat important, however, this requires more thorough investigation."}, {"heading": "5.1 Comparison of the diagonalization algorithms", "text": "In Figure 1, we give a comparison of the diagonalization algorithms on the semi-synthetic AP dataset for K = 50 using the GP model for sampling. We compare the tensor power method (TPM) [3], the spectral algorithm (Spec), the orthogonal joint diagonalization algorithm (JD) described in Algorithm 1 with different options to choose the random projections: JD(k) takes P = K vectors up sampled uniformly from the unit `2-sphere in RK and selects vp = W>up (option (a) in Algorithm 1); JD selects the full basis e1, . . . , eK in RK and sets vp = W>ep (as JADE [16]) (option (b) in Algorithm 1); JD(f) chooses the full canonical basis of RM as the vectors to project onto (is computationally expensive).\nAlthough both the GP/DICA cumulants and LDA moments are well-specified for sampling from the GP model, the LDA moments have a slower finite sample convergence and, hence, a larger estimation error for the same value N . As expected, the spectral algorithm is always slightly inferior to the joint diagonalization algorithms. With the GP/DICA cumulants, where the estimation error is low, all algorithms demonstrate good performance, which also fulfills our expectations. However, although TPM shows almost perfect performance in the case of the GP/DICA cumulants (left), it significantly deteriorates for the LDA moments (right), which can be explained by the larger estimation error of the LDA moments and lack of robustness of TPM. For N = 50, 000, the algorithms have the following runtimes (min, mean, max) in sec: JD-GP (140, 197, 267), JD(k)-GP (121, 176, 269), JD(f)-GP (1935, 2114, 2259), Spec-GP (87, 106, 125), TPM-GP (1857, 1993, 2102), JD-LDA (238, 308, 447), JD(k)-LDA (236, 292, 441), JD(f)-LDA (2998, 3547, 4939), Spec-LDA (89, 106, 147), TPM-LDA (1490, 2116, 2796). Note, due to random restarts, two runs of TPM/Spec are never the same. Computation of a whitening matrix is roughly 30 sec (this time is the same for all algorithms and is included in the numbers above). Overall, the orthogonal joint diagonalization algorithm with initialization of random projections as W> multiplied with the canonical basis in RK (JD) is both computationally efficient and fast."}, {"heading": "5.2 Comparison of the GP/DICA cumulants and the LDA moments", "text": "In Figure 2, when sampling from the GP model (top, left), both the GP/DICA cumulants and LDA moments are well specified, which implies that the approximation error is low for both. The GP/DICA cumulants achieve low values of the estimation error already for N = 10, 000 documents\nindependently of the number of topics, while the convergence is slower for the LDA moments. When sampling from the LDA-fix(200) model (top, right), the GP/DICA cumulants are mis-specified and their approximation error is high, although the estimation error is low due to the faster finite sample convergence. One reason of poor performance of the GP/DICA cumulants, in this case, is the absence of variance in document length. Indeed, if documents with two different lengths are mixed by sampling from the LDA-fix2(0.5,20,200) model (bottom, left), the GP/DICA cumulants\u2019 performance improves. Moreover, the experiment with a changing fraction \u03b3 of documents (bottom, right) shows that a non-zero variance on the length improves the performance of the GP/DICA cumulants. As in practice real corpora usually have a non-zero variance for the document length, this bad scenario for the GP/DICA cumulants is not likely to happen."}, {"heading": "5.3 Real data experiments", "text": "Each dataset is separated into 5 training/evaluation pairs, where the documents for evaluation are chosen randomly and non-repetitively among the folds (400 documents are held out for AP; 600 documents are held out for KOS). Then, the model parameters are learned for a different number of topics. The evaluation of the held-out documents is performed with averaging over 5 folds. In Figure 3, on the y-axis, the predictive log-likelihood in bits averaged per token is presented. JD-GP, Spec-GP, JD-LDA, and Spec-LDA are compared with variational inference (VI) and with variational inference initialized with the output of JD-GP (VI-JD). The orthogonal joint diagonalization algorithm with the GP/DICA cumulants (JD-GP) demonstrates competitive performance. In particular, the GP/DICA cumulants significantly outperform the LDA moments, and are better than variational inference. Interestingly, using variational inference after the topics have been learned by moment matching decreased performance in some cases. In Appendix F.3, a similar experiment for the NIPS dataset is presented."}, {"heading": "6 Conclusion", "text": "In this paper, we have proposed a new set of tensors for a discrete ICA model related to LDA, where word counts are directly modelled. These moments make fewer assumptions regarding distributions, and are theoretically and empirically more robust than previously proposed tensors for LDA, both on synthetic and real data. Following the ICA literature, we showed that our joint diagonalization\nprocedure is also more robust. Once the topic matrix has been estimated, it would be interesting to learn the unknown distributions of the independent topic intensities.\nAknowledgements. This work was partially supported by the MSR-Inria Joint Center."}, {"heading": "A Appendix. The GP model", "text": "A.1 The connection between the LDA and GP models\nTo show that the LDA model (2) with the additional assumption that the document length is modeled as a gamma-Poisson random variable is equivalent to the GP model (4), we show that:\n- when modeling the document length L as a Poisson random variable with parameter \u03bb, the count vectors x1, x2, . . . , xM are mutually independent Poisson random variables;\n- the Gamma prior on \u03bb reveals the connection \u03b1k = \u03bb\u03b8k between the Dirichlet random variable \u03b8 and the mutually independent gamma random variables \u03b11, \u03b12, . . . , \u03b1K .\nFor completeness, we repeat the known result that if L \u223c Poisson(\u03bb) and x|L \u223c Multinomial(L,D\u03b8) (which thus means that L = \u2211 m xm with probability one), then x1, x2, . . . , xM are mutually independent Poisson random variables with parameters \u03bb [D\u03b8]1, \u03bb [D\u03b8]2, . . . , \u03bb [D\u03b8]M . Indeed, we consider the following joint probability mass function where x and L are assumed to be non-negative integers:\np(x, L|\u03b8, \u03bb) =p(L|\u03bb)p(x|L, \u03b8)\n=1{L= \u2211 m xm} exp (\u2212\u03bb)\u03bbL\nL!\nL!\u220f m xm! \u220f m [D\u03b8] xm m\n=1{L= \u2211 m xm} exp(\u2212\u03bb \u2211 m [D\u03b8]m)\u03bb \u2211 m xm \u220f m [D\u03b8] xm m xm! =1{L= \u2211\nm xm} \u220f m exp(\u2212\u03bb [D\u03b8]m)(\u03bb [D\u03b8]m)xm xm!\n=1{L= \u2211 m xm} \u220f m Poisson(xm;\u03bb [D\u03b8]m),\nwhere in the third equation we used the fact that\u2211 m [D\u03b8]m = \u2211 m,k Dmk\u03b8k = \u2211 k \u03b8k \u2211 m Dmk = 1.\nWe thus have p(x, L|\u03b8, \u03bb) = p(L|x) \u220f m p(xm|\u03bb[D\u03b8]m) where p(L|x) is simply the deterministic\ndistribution 1{L= \u2211\nm xm} and p(xm|\u03bb[D\u03b8]m) for m = 1, . . . ,M are independent Poisson(\u03bb[D\u03b8]m) distributions (and thus do not depend on L). Note that in the notation introduced in the paper, Dmk = dkm. Hence, by using the construction of the Dirichlet distribution from the normalization of independent gamma random variables, we can show that the LDA model with a gamma-Poisson prior over the length is equivalent to the following model (recall, that c0 = \u2211 k ck):\n\u03bb \u223c Gamma(c0, b), \u03b8 \u223c Dirichlet(c),\nxm|\u03bb, \u03b8 \u223c Poisson([D(\u03bb\u03b8)]m). (20)\nMore specifically, we complete the second part of the argument with the following properties. When \u03b11, \u03b12, . . . , \u03b1K are mutually independent gamma random variables, each \u03b1k \u223c Gamma(ck, b), their sum is also a gamma random variable \u2211 k \u03b1k \u223c Gamma( \u2211 k ck, b). The former is equivalent to \u03bb. It is known (e.g. [33]) that a Dirichlet random variable can be sampled by first sampling independent gamma random variables (\u03b1k) and then dividing each of them by their sum (\u03bb): \u03b8k = \u03b1k/ \u2211 k\u2032 \u03b1k\u2032 , and, in other direction, the variables \u03b1k = \u03bb\u03b8k are mutually independent, giving back the GP model (4).\nA.2 The expectation and the variance of the document length for the GP model\nFrom the drivations in Appendix A.1, it follows that the document length of the GP model (4) is a gamma-Poisson random variable, i.e. L|\u03bb \u223c Poisson(\u03bb) and \u03bb \u223c Gamma(c0, b). Therefore, the following follows from the law of total expectation and the law of total variance\nE(L) = E [E(L|\u03bb)] = E(\u03bb) = c0/b var(L) = var [E(L|\u03bb)] + E [var(L|\u03bb)] = var(\u03bb) + E(\u03bb) = c0/b+ c0/b2\nThe first expression shows that the parameter b controls the expected document length E(L) for a given parameter c0: the smaller b, the larger E(L). On the other hand, if we allow c0 to vary as well, only the ratio c0/b is important for the document length. We can then interpret the role of c0 as actually controlling the concentration of the distribution for the length L (through the variance). More specifically, we have that:\nvar(L)\n(E(L))2 =\n1\nE(L) +\n1 c0 . (21)\nFor a fixed target document length E(L), we can increase the variance (and thus decrease the concentration) by using a smaller c0."}, {"heading": "B Appendix. The cumulants of the GP and DICA models", "text": "B.1 Cumulants\nFor a random vector x \u2208 RM , the first three cumulant tensors9 are\ncum(xm) = E(xm), cum(xm1 , xm2) = E [(xm1 \u2212 E(xm1))(xm2 \u2212 E(xm2)] = cov(xm1 , xm2),\ncum(xm1 , xm2 , xm3) = E [(xm1 \u2212 E(xm1))(xm2 \u2212 E(xm2))(xm3 \u2212 E(xm3)))] .\nNote that the 2nd and 3rd cumulants coincide with the 2nd and 3rd central moments (but not for higher orders). In the following, cum(x, x, x) \u2208 RM\u00d7M\u00d7M denotes the third order tensor with elements cum(xm1 , xm2 , xm3). Some of the properties of cumulants are listed below (see [19, chap. 5]). The most important property that motivate us to use cumulants in this paper (and the ICA literature) is the independence property, which says that the cumulant tensor for a random vector with independent components is diagonal (this property does not hold for the (non-central) moment tensors of any order, and neither for the central moments of order 4 or more).\n- Independence. If the elements of x \u2208 RM are independent, then their cross-cumulants are zero as soon as two indices are different, i.e. cum(xm1 , xm2) = \u03b4(m1,m2)E[(xm1 \u2212Em1))2] and cum(xm1 , xm2 , xm3) = \u03b4(m1,m2,m3)E[(xm1 \u2212 E(xm1))3], where \u03b4 is the Kronecker delta.\n- Multilinearity. If two random vectors y \u2208 RM and \u03b1 \u2208 RK are linearly dependent, i.e. y = D\u03b1 for some D \u2208 RM\u00d7K , then\ncum(ym) = \u2211 k cum(\u03b1k)Dmk,\ncum(ym1 , ym2) = \u2211 k1,k2 cum(\u03b1k1 , \u03b1k2)Dm1k1Dm2k2 ,\ncum(ym1 , ym2 , ym3) = \u2211\nk1,k2,k3\ncum(\u03b1k1 , \u03b1k2 , \u03b1k3)Dm1k1Dm2k2Dm3k3 ,\nwhich can also be denoted10 by\nE(y) = DE(\u03b1), cov(y, y) = Dcov(\u03b1, \u03b1)D>,\ncum(y, y, y) = cum(\u03b1, \u03b1, \u03b1)(D>, D>, D>).\n- The law of total cumulance. For two random vectors x \u2208 RM and y \u2208 RM , it holds\ncum(xm) = E [E(xm|y)] , cum(xm1 , xm2) = E [cov(xm1 , xm2 |y)] + cov [E(xm1 |y),E(xm2 |y)] ,\ncum(xm1 , xm2 , xm3) = E [cum(xm1 , xm2 , xm3 |y)] + cum [E(xm1 |y),E(xm2 |y),E(xm3 |y)] + cov [E(xm1 |y), cov(xm2 , xm3 |y)] + cov [E(xm2 |y), cov(xm1 , xm3 |y)] + cov [E(xm3 |y), cov(xm1 , xm2 |y)] .\n9Strictly speaking, the (scalar) n-th cumulant \u03ban of a random variable X is defined via the cumulant-generating function g(t), which is the natural logarithm of the moment-generating function, i.e g(t) := logE [ etX ] . The cumulant\n\u03ban is then obtained from a power series expansion of the cumulant-generating function, that is g(t) = \u2211\u221e n=1 \u03bant n/n! [Wikipedia]. 10In [3], given a tensor T \u2208 RK\u00d7K\u00d7K , T (D>, D>, D>) is referred to as the multilinear map. In [34], the same entity is denoted by T \u00d71 D> \u00d72 D> \u00d73 D>, where \u00d7n denotes the n-mode tensor-matrix product.\nNote that the first expression is also well known as the law of total expectation or the tower property, while the second one is known as the law of total covariance.\nB.2 The third cumulant of the GP/DICA models\nIn this section, by analogy with Section 3.1, we derive the third GP/DICA cumulant.\nAs the third cumulant of a Poisson random variable xm with parameter ym is E((xm\u2212E(xm))3|ym) = ym, then by the independence property of cumulants from Section B.1, the cumulant of x|y is diagonal:\ncum(xm1 , xm2 , xm3 |y) = \u03b4(m1,m2,m3) ym1 .\nSubstituting the cumulant of x|y into the law of total cumulance, we obtain\ncum(xm1 , xm2 , xm3) = E [cum(xm1 , xm2 , xm3 |y)] + cum [E(xm1 |y),E(xm2 |y),E(xm3 |y)] + cov [E(xm1 |y), cov(xm2 , xm3 |y)] + cov [E(xm2 |y), cov(xm1 , xm3 |y)] + cov [E(xm3 |y), cov(xm1 , xm2 |y)]\n= \u03b4(m1,m2,m3)E(ym1) + cum(ym1 , ym2 , ym3) + \u03b4(m2,m3)cov(ym1 , ym2) + \u03b4(m1,m3)cov(ym1 , ym2) + \u03b4(m1,m2)cov(ym1 , ym3) = \u03b4(m1,m2,m3)E(xm1) + cum(ym1 , ym2 , ym3) + \u03b4(m2,m3)cov(xm1 , xm2)\u2212 \u03b4(m1,m2,m3)E(xm1) + \u03b4(m1,m3)cov(xm1 , xm2)\u2212 \u03b4(m1,m2,m3)E(xm1) + \u03b4(m1,m2)cov(xm1 , xm3)\u2212 \u03b4(m1,m2,m3)E(xm1) = cum(ym1 , ym2 , ym3)\u2212 2\u03b4(m1,m2,m3)E(xm1) + \u03b4(m2,m3)cov(xm1 , xm2) + \u03b4(m1,m3)cov(xm1 , xm2) + \u03b4(m1,m2)cov(xm1 , xm3)\n= [ cum(\u03b1, \u03b1, \u03b1)(D>, D>, D>) ] m1m2m3 \u2212 2\u03b4(m1,m2,m3)E(xm1)\n+ \u03b4(m2,m3)cov(xm1 , xm2) + \u03b4(m1,m3)cov(xm1 , xm2) + \u03b4(m1,m2)cov(xm1 , xm3), (22)\nwhere, in the third equality, we used the previous result from (10) that cov(y, y) = cov(x, x) \u2212 diag(E(x)).\nB.3 The diagonal structure of the GP/DICA cumulants\nIn this section, we provide detailed derivation of the diagonal structure (12) of the matrix S (11) and the diagonal structure (14) of the tensor T (13).\nFrom the independence of \u03b11, \u03b12, . . . , \u03b1K and by the independence property of cumulants from Section B.1, it follows that cov(\u03b1, \u03b1) is a diagonal matrix and cum(\u03b1, \u03b1, \u03b1) is a diagonal tensor, i.e. cov(\u03b1k1 , \u03b1k2) = \u03b4(k1, k2)cov(\u03b1k1 , \u03b1k2) and cum(\u03b1k1 , \u03b1k2 , \u03b1k3) = \u03b4(k1, k2, k3)cum(\u03b1k1 , \u03b1k1 , \u03b1k1). Therefore, the following holds\ncov(ym1 , ym2) = \u2211 k cov(\u03b1k, \u03b1k)Dm1kDm2k,\ncum(ym1 , ym2 , ym3) = \u2211 k cum(\u03b1k, \u03b1k, \u03b1k)Dm1kDm2kDm3k,\nwhich we can rewrite in a matrix/tensor form as cov(y, y) = \u2211 k cov(\u03b1k, \u03b1k)dkd > k ,\ncum(y, y, y) = \u2211 k cum(\u03b1k, \u03b1k, \u03b1k)dk \u2297 dk \u2297 dk.\nMoving cov(y, y) / cum(y, y, y) in the expression for cov(x, x) (10) / cum(x, x, x) (22) on one side of equality and all other terms on the other side, we define matrix S \u2208 RM\u00d7M / tensor T \u2208 RM\u00d7M\u00d7M\nas follows\nS := cov(x, x)\u2212 diag (E(x)) , (23) Tm1m2m3 := cum(xm1 , xm2 , xm3) + 2\u03b4(m1,m2,m3)E(xm1)\n\u2212 \u03b4(m2,m3)cov(xm1 , xm2) \u2212 \u03b4(m1,m3)cov(xm1 , xm2) \u2212 \u03b4(m1,m2)cov(xm1 , xm3). (24)\nBy construction, S = cov(y, y) and T = cum(y, y, y) and, therefore, it holds that S = \u2211 k cov(\u03b1k, \u03b1k)dkd > k , (25)\nT = \u2211 k cum(\u03b1k, \u03b1k, \u03b1k)dk \u2297 dk \u2297 dk. (26)\nThis means that both the matrix S and the tensor T are sums of rank-1 matrices and tensors, respectively11. This structure of the matrix S and the tensor T is the basis for the algorithms considered in this paper.\nB.4 Unbiased finite sample estimators for the GP/DICA cumulants\nGiven a sample {x1, x2, . . . , xN}, we obtain a finite sample estimate S\u0302 of S (11) / T\u0302 of T (13) for the GP/DICA cumulants:\nS\u0302 := c\u0302ov(x, x)\u2212 diag ( E\u0302(x) ) , (27)\nT\u0302m1m2m3 := c\u0302um(xm1 , xm2 , xm3) + 2\u03b4(m1,m2,m3)E\u0302(xm1) \u2212 \u03b4(m2,m3)c\u0302ov(xm1 , xm2) \u2212 \u03b4(m1,m3)c\u0302ov(xm1 , xm2) \u2212 \u03b4(m1,m2)c\u0302ov(xm1 , xm3), (28)\nwhere unbiased estimators of the first three cumulants are\nE\u0302(xm1) = 1\nN \u2211 n xnm1 ,\nc\u0302ov(xm1 , xm2) = 1 N \u2212 1 \u2211 n znm1znm2 ,\nc\u0302um(xm1 , xm2 , xm3) = N (N \u2212 1)(N \u2212 2) \u2211 n znm1znm2znm3 ,\n(29)\nwhere the word vocabulary indexes are m1,m2,m3 = 1, 2, . . . ,M and the centered documents znm := xnm\u2212 E\u0302(xm). (The latter is introduced only for compact representation of (29) and is different from z in the LDA model.)"}, {"heading": "C Appendix. The sketch of the proof for Proposition 3.1", "text": "C.1 Expected squared error for the sample expectation The sample expectation is E\u0302(x) = 1N \u2211 n xn is an unbiased estimator of the expectation and:\nE ( \u2016E\u0302(x)\u2212 E(x)\u201622 ) = \u2211 m E [( E\u0302(xm)\u2212 E(xm) )2]\n= 1\nN2 \u2211 m E(\u2211 n (xnm \u2212 E(xm))2 ) + E \u2211 n \u2211 n6=n\u2032 (xnm \u2212 E(xm)) (xn\u2032m \u2212 E(xm))  = 1\nN \u2211 m E [ (xm \u2212 E(xm))2 ] = 1 N \u2211 m var(xm).\n11For tensors, such decomposition is also known under the names CANDECOMP/PARAFAC or, simply, the CP decomposition (see, e.g., [34]).\nFurther, by the law of total variance: E ( \u2016E\u0302(x)\u2212 E(x)\u201622 ) = 1\nN \u2211 m [E(var(xm|y)) + var(E(xm|y))] = 1 N \u2211 m [E(ym) + var(ym)]\n= 1\nN [\u2211 k E(\u03b1k) + \u2211 k \u3008dk, dk\u3009var(\u03b1k) ] ,\nusing the fact that \u2211 mDmk = 1 for any k.\nC.2 Expected squared error for the sample covariance\nThe following finite sample estimator of the covariance cov(x, x) = E(xx>)\u2212 E(x)E(x)>\nc\u0302ov(x, x) = 1 N \u2212 1 \u2211 n xnx > n \u2212 E\u0302(x)E\u0302(x)> = 1 N \u2212 1 \u2211 n\n( xnx > n \u2212 1\nN2 \u2211 n\u2032 \u2211 n\u2032\u2032 xn\u2032x > n\u2032\u2032\n)\n= 1\nN \u2211 n xnx>n \u2212 1N \u2212 1xn \u2211 n\u2032 6=n x>n\u2032  (30)\nis unbiased, i.e. E(c\u0302ov(x, x)) = cov(x, x). Its squared error is\nE ( \u2016c\u0302ov(x, x)\u2212 cov(x, x)\u20162F ) = \u2211 m,m\u2032 E [ (c\u0302ov(xm, xm\u2032)\u2212 E[c\u0302ov(xm, xm\u2032)])2 ] .\nThe m,m\u2032-th element of the sum above is equal to\n1\nN2 \u2211 n,n\u2032 cov xnmxnm\u2032 \u2212 1 N \u2212 1 xnm \u2211 n\u2032\u2032 6=n xn\u2032\u2032m\u2032 , xn\u2032mxn\u2032m\u2032 \u2212 1 N \u2212 1 xn\u2032m \u2211 n\u2032\u2032\u2032 6=n\u2032 xn\u2032\u2032\u2032m\u2032  = 1\nN2 \u2211 n,n\u2032 cov (xnmxnm\u2032 , xn\u2032mxn\u2032m\u2032)\u2212 2 N2(N \u2212 1) \u2211 n,n\u2032 cov xnm \u2211 n\u2032\u2032 6=n xn\u2032\u2032m\u2032 , xn\u2032mxn\u2032m\u2032  + 1\nN2(N \u2212 1)2 \u2211 n,n\u2032 cov xnm \u2211 n\u2032\u2032 6=n xn\u2032\u2032m\u2032 , xn\u2032m \u2211 n\u2032\u2032\u2032 6=n\u2032 xn\u2032\u2032\u2032m\u2032  = 1\nN2 \u2211 n cov (xnmxnm\u2032 , xnmxnm\u2032)\n\u2212 2 N2(N \u2212 1) \u2211 n \u2211 n\u2032\u2032 6=n cov (xnmxn\u2032\u2032m\u2032 , xnmxnm\u2032) + \u2211 n \u2211 n\u2032 6=n cov (xnmxn\u2032m\u2032 , xn\u2032mxn\u2032m\u2032)  + 1\nN2(N \u2212 1)2 \u2211 n \u2211 n\u2032\u2032 6=n \u2211 n\u2032\u2032\u2032 6=n cov (xnmxn\u2032\u2032m\u2032 , xnmxn\u2032\u2032\u2032m\u2032) + \u2211 n\u2032 \u2211 n6=n\u2032 \u2211 n\u2032\u2032 6=n cov (xnmxn\u2032\u2032m\u2032 , xn\u2032mxnm\u2032)  + 1\nN2(N \u2212 1)2 \u2211 n\u2032 \u2211 n 6=n\u2032 \u2211 n\u2032\u2032\u2032 6=n\u2032 cov (xnmxn\u2032m\u2032 , xn\u2032mxn\u2032\u2032\u2032m\u2032) + \u2211 n\u2032 \u2211 n 6=n\u2032 \u2211 n\u2032\u2032 6=n cov (xnmxn\u2032\u2032m\u2032 , xn\u2032mxn\u2032\u2032m\u2032)  , where we used mutual independence of the observations xn in a sample {xn}Nn=1 to conclude that the covariance between two expressions involving only independent variables is zero. Further: E ( \u2016c\u0302ov(x, x)\u2212 cov(x, x)\u20162F ) = 1\nN2 \u2211 m,m\u2032 N ( E(x2mx2m\u2032)\u2212 [E(xmxm\u2032)] 2 )\n\u2212 4 N2(N \u2212 1) \u2211 m,m\u2032 N(N \u2212 1) ( E(x2mxm\u2032)E(xm\u2032)\u2212 E(xmxm\u2032)E(xm)E(xm\u2032) ) + 2\nN2(N \u2212 1)2 \u2211 m,m\u2032 N(N \u2212 1)(N \u2212 2) ( E(x2m) [E(xm\u2032)] 2 \u2212 [E(xm)]2 [E(xm\u2032)]2 )\n+ 2 N2(N \u2212 1)2 \u2211 m,m\u2032 N(N \u2212 1)(N \u2212 2) ( E(xmxm\u2032)E(xm)E(xm\u2032)\u2212 [E(xm)]2 [E(xm\u2032)]2 ) +O ( 1 N2 ) ,\nwhich after simplification gives\nE ( \u2016c\u0302ov(x, x)\u2212 cov(x, x)\u20162F ) = 1\nN \u2211 m,m\u2032 [ var(xmxm\u2032) + 2 [E(xm)]2 var(xm\u2032) ] + 1\nN \u2211 m,m\u2032 [2E(xm)E(xm\u2032)cov(xm, xm\u2032)\u2212 4E(xm)cov(xmxm\u2032 , xm\u2032)] +O ( 1 N2 ) ,\nwhere in the last equality, by symmetry, the summation indexes m and m\u2032 can be exchanged. As xm \u223c Poisson(ym), by the law of total expectation and law of total covariance, it follows, for m 6= m\u2032 (and using the auxiliary expressions from Section C.4):\nvar(xmxm\u2032) = E(x2mx2m\u2032)\u2212 [E[xmxm\u2032 ]] 2 = E [ y2my 2 m\u2032 + y 2 mym\u2032 + ymy 2 m\u2032 + ymym\u2032 ] \u2212 [E(ymym\u2032)]2\n[E(xm)]2 var(xm\u2032) = [E(ym)]2 [E[var(x\u2032m|y)] + var[E(x\u2032m|y)]]\n= [E(ym)]2 E(ym\u2032) + [E(ym)]2 E(y2m\u2032)\u2212 [E(ym)] 2 [E(ym\u2032)]2 ,\nE(xm)E(xm\u2032)cov(xm, xm\u2032) = E(ymym\u2032)E(ym)E(ym\u2032)\u2212 [E(ym)]2 [E(ym\u2032)]2 , E(xm)cov(xmxm\u2032 , xm\u2032) = E(ym) [ E(ymym\u2032) + E(ymy2m\u2032)\u2212 E(ymym\u2032)E(ym\u2032) ] .\nNow, considering the m = m\u2032 case, we have:\nvar(x2m) = E[E(x4m|y)]\u2212 ( E[E(x2m|y)] )2 = E [ y4m + 6y 3 m + 7y 2 m + ym ] \u2212 ( E [ y2m + ym\n])2 E(xm)E(xm)cov(xm, xm) = E(y2m) [ E(y2m) + E(ym)\u2212 [E(ym)] 2 ] ,\nE(xm)cov(x2m, xm) = E(ym) [ E(y3m) + 3E(y3m) + E(ym)\u2212 E(ym) [ E(y2m) + E(ym) ]] .\nSubstitution of ym = \u2211 kDmk\u03b1k gives the following E ( \u2016c\u0302ov(x, x)\u2212 cov(x, x)\u20162F ) = 1\nN \u2211 k,k\u2032,k\u2032\u2032,k\u2032\u2032\u2032 \u3008dk, dk\u2032\u3009\u3008dk\u2032\u2032 , dk\u2032\u2032\u2032\u3009Akk\u2032k\u2032\u2032k\u2032\u2032\u2032\n+ 1\nN \u2211 k,k\u2032,k\u2032\u2032 \u3008dk, dk\u2032\u3009\u3008dk\u2032\u2032 ,~1\u3009Bkk\u2032k\u2032\u2032 + 1 N \u2211 k,k\u2032 \u3008dk,~1\u3009\u3008dk\u2032 ,~1\u3009E(\u03b1k\u03b1k\u2032) +O ( 1 N2 ) ,\nwhere ~1 is the vector with all the elements equal to 1 and\nAkk\u2032k\u2032\u2032k\u2032\u2032\u2032 = E(\u03b1k\u03b1k\u2032\u03b1k\u2032\u2032\u03b1k\u2032\u2032\u2032)\u2212 E(\u03b1k\u03b1k\u2032\u2032)E(\u03b1k\u2032\u03b1k\u2032\u2032\u2032) + 2E(\u03b1k)E(\u03b1k\u2032)E(\u03b1k\u2032\u2032\u03b1k\u2032\u2032\u2032) \u2212 2E(\u03b1k)E(\u03b1k\u2032)E(\u03b1k\u2032\u2032)E(\u03b1k\u2032\u2032\u2032) + 2E(\u03b1k\u03b1k\u2032\u2032)E(\u03b1k\u2032)E(\u03b1k\u2032\u2032\u2032)\u2212 2E(\u03b1k)E(\u03b1k\u2032)E(\u03b1k\u2032\u2032)E(\u03b1k\u2032\u2032\u2032) \u2212 4E(\u03b1k)E(\u03b1k\u2032\u03b1k\u2032\u2032\u03b1k\u2032\u2032\u2032) + 4E(\u03b1k)E(\u03b1k\u2032\u03b1k\u2032\u2032)E(\u03b1k\u2032\u2032\u2032),\nBkk\u2032k\u2032\u2032 = 2E(\u03b1k\u03b1k\u2032\u03b1k\u2032\u2032) + 4E(\u03b1k)E(\u03b1k\u2032)E(\u03b1k\u2032\u2032)\u2212 4E(\u03b1k)E(\u03b1k\u2032\u03b1k\u2032\u2032),\nwhere we used the expressions from Section C.4.\nC.3 Expected squared error of the estimator S\u0302 for the GP/DICA cumulants\nAs the estimator S\u0302 (27) of S (11) is unbiased, its expected squared error is\nE [ \u2016S\u0302 \u2212 S\u20162F ] =E [\u2225\u2225\u2225(c\u0302ov(x, x)\u2212 cov(x, x)) + (diag[E\u0302(x)]\u2212 diag [E(x)])\u2225\u2225\u22252 F ] = E [ \u2016E\u0302(x)\u2212 E(x)\u20162F ] + E [ \u2016c\u0302ov(x, x)\u2212 cov(x, x)\u20162F\n] + 2\n\u2211 m E [( E\u0302(xm)\u2212 E(xm) ) (c\u0302ov(xm, xm)\u2212 cov(xm, xm)) ] .\n(31)\nAs E\u0302(xm) and c\u0302ov(xm, xm) are unbiased, the m-th element of the last sum is equal to\ncov [ E\u0302(xm), c\u0302ov(xm, xm) ] = 1\nN2 \u2211 n,n\u2032 cov [ xnm, x 2 n\u2032m ] \u2212 1 N2(N \u2212 1) \u2211 n,n\u2032,n\u2032\u2032 6=n\u2032 cov [xnm, xn\u2032mxn\u2032\u2032m]\n= 1\nN2 \u2211 n cov [ xnm, x 2 nm ] \u2212 2 N2(N \u2212 1) \u2211 n,n\u2032 6=n cov [xnm, xn\u2032mxnm] +O ( 1 N2 )\n= 1\nN E(x3m)\u2212\n2\nN\n( E(x2m)E(xm)\u2212 [E(xm)] 3 ) +O ( 1\nN2 ) \u2264 1 N E(x3m) + 2 N [E(xm)]3 +O ( 1 N2 ) = 1 N [ E(y3m) + 3E(y2m) + E(ym) + 2 [E(ym)] 3 ] +O ( 1 N2 ) ,\nwhere we neglected the negative term \u2212E(x2m)E(xm) for the inequality, and the last equality follows from the expressions in Section C.4. Further, the fact that ym = \u2211 kDmk\u03b1k gives\u2211\nm\ncov [ E\u0302(xm), c\u0302ov(xm, xm) ] = 1\nN \u2211 k,k\u2032,k\u2032\u2032 \u3008dk \u25e6 dk\u2032 , dk\u2032\u2032\u3009Ckk\u2032k\u2032\u2032\n+ 3\nN \u2211 k,k\u2032 \u3008dk, dk\u2032\u3009E(\u03b1k\u03b1k\u2032) + 1 N \u2211 k \u3008dk,~1\u3009E(\u03b1k) +O ( 1 N2 ) ,\nwhere \u25e6 denotes the elementwise Hadamard product and\nCkk\u2032k\u2032\u2032 = E(\u03b1k\u03b1k\u2032\u03b1k\u2032\u2032) + 2E(\u03b1k)E(\u03b1k\u2032)E(\u03b1k\u2032\u2032).\nPlugging this and the expressions for E(\u2016E\u0302(x) \u2212 E(x)\u20162F ) and E(\u2016c\u0302ov(x, x) \u2212 cov(x, x)\u20162F ) from Sections C.1 and C.2, respectively, into (31) gives E [ \u2016S\u0302 \u2212 S\u20162F ] = 1\nN \u2211 k \u3008dk, dk\u3009var(\u03b1k) + \u2211 k E(\u03b1k) + \u2211 k,k\u2032,k\u2032\u2032,k\u2032\u2032\u2032 \u3008dk, dk\u2032\u3009\u3008dk\u2032\u2032 , dk\u2032\u2032\u2032\u3009Akk\u2032k\u2032\u2032k\u2032\u2032\u2032 +O( 1 N2 )\n+ 1\nN  \u2211 k,k\u2032,k\u2032\u2032 [\u3008dk, dk\u2032\u3009Bkk\u2032k\u2032\u2032 + 2\u3008dk \u25e6 dk\u2032 , dk\u2032\u2032\u3009Ckk\u2032k\u2032\u2032 ] + \u2211 k,k\u2032 (1 + 6 \u3008dk, dk\u2032\u3009)E(\u03b1k\u03b1k\u2032) + 2 \u2211 k E(\u03b1k)  , where we used that, by the simplex constraint on the topics, \u3008dk,~1\u3009 = 1 for all k. To analyze this expression in more details, let us now consider the GP model, i.e. \u03b1k \u223c Gamma(ck, b):\u2211\nk,k\u2032,k\u2032\u2032,k\u2032\u2032\u2032\nAkk\u2032k\u2032\u2032k\u2032\u2032\u2032 \u2264 30c40 + 23c 3 0 + 14c 2 0 + 8c0\nb4 , and \u2211 k,k\u2032,k\u2032\u2032 Bkk\u2032k\u2032\u2032 \u2264 14c30 + 12c 2 0 + 4c0 b3 ,\n\u2211 k,k\u2032,k\u2032\u2032 Ckk\u2032k\u2032\u2032 \u2264 7c30 + 6c 2 0 + 2c0 b3 , and \u2211 k,k\u2032 E(\u03b1k\u03b1k\u2032) \u2264 2c20 + c0 b2 ,\nwhere we used the expressions from Section C.4, which gives E [ \u2016S\u0302 \u2212 S\u20162F ] \u2264 \u03bd N [ max k \u2016dk\u201622 c0 b2 + c0 b + ( max k,k\u2032 \u3008dk, dk\u2032\u3009 )2 max [ c40 b4 , c0 b4 ] + max k,k\u2032 \u3008dk, dk\u2032\u3009max [ c30 b3 , c0 b3 ]]\n+ \u03bd\nN [( max k,k\u2032,k\u2032\u2032 \u3008dk \u25e6 dk\u2032 , dk\u2032\u2032\u3009 ) max [ c30 b3 , c0 b3 ] + ( 1 + max k,k\u2032 \u3008dk, dk\u2032\u3009 ) max [ c20 b2 , c0 b2 ]] +O ( 1 N2 ) ,\nwhere \u03bd \u2264 30 is a universal constant. As, by the Cauchy-Schwarz inequality, max k,k\u2032\u3008dk, dk\u2032\u3009 \u2264 max k \u2016dk\u201622 =: \u22061 and max k,k\u2032,k\u2032\u2032\u3008dk \u25e6 dk\u2032 , dk\u2032\u2032\u3009 \u2264 max k \u2016dk\u2016\u221e \u2016dk\u2016 2 2 \u2264 max k \u2016dk\u2016 3 2 =: \u22062 (note that for the topics in the simplex, \u22062 \u2264 \u22061 as well as \u220621 \u2264 \u22061), it follows that\nE [ \u2016S\u0302 \u2212 S\u20162F ] \u2264 \u03bd N [ \u22061 ( L2 c\u03040 + L3 c\u030420 ) + L+ \u220621 L4 c\u030430 + L2 c\u030420 + \u22062 L3 c\u030420 ] +O ( 1 N2 ) \u2264 2\u03bd N 1\nc\u030430\n[ \u220621L 4 + c\u03040\u22061L 3 + c\u030420L 2 + c\u030430L ] +O\n( 1\nN2\n) ,\nwhere c\u03040 = min(1, c0) \u2264 1 and, from Section A.2, c0 = bL where L is the expected document length. The second term c\u03040\u22061L 3 cannot be dominant as the system c\u03040\u22061L 3 > c\u030420L 2 and c\u03040\u22061L 3 > \u220621L 4 is infeasible. Also, with the reasonable assumption that L \u2265 1, we also have that the 4th term c\u030430L \u2264 c\u030420L2. Therefore,\nE [ \u2016S\u0302 \u2212 S\u20162F ] \u2264 3\u03bd N max [ \u220621L 4, c\u030420L 2 ] +O ( 1 N2 ) .\nC.4 Auxiliary expressions\nAs {xm}Mm=1 are conditionally independent given y in the DICA model (4), we have the following expressions by using the law of total expectation for m 6= m\u2032 and using the moments of the Poisson distribution with parameter ym:\nE(xm) = E[E(xm|ym)] = E(ym), E(x2m) = E[E(x2m|ym)] = E(y2m) + E(ym), E(x3m) = E[E(x3m|ym)] = E(y3m) + 3E(y2m) + E(ym), E(x4m) = E[E(x4m|ym)] = E(y4m) + 6E(y3m) + 7E(y2m) + E(ym),\nE(xmxm\u2032) = E[E(xmxm\u2032 |y)] = E[E(xm|ym)E(xm\u2032 |ym\u2032)] = E(ymym\u2032), E(xmx2m\u2032) = E[E(xmx2m\u2032 |y)] = E[E(xm|ym)E(x2m\u2032 |ym\u2032)] = E(ymy2m\u2032) + E(ymym\u2032), E(x2mx2m\u2032) = E[E(x2m|ym)E(x2m\u2032 |ym\u2032)] = E(y2my2m\u2032) + E(y2mym\u2032) + E(ymy2m\u2032) + E(ymym\u2032).\nMoreover, the moments of \u03b1k \u223c Gamma(ck, b) are\nE(\u03b1k) = ck b , E(\u03b12k) = c2k + ck b2 , E(\u03b13k) = c3k + 3c 2 k + 2ck b3 , E(\u03b14k) = c4k + 6c 3 k + 11c 2 k + 6ck b4 , etc.\nC.5 Analysis of whitening and recovery error\nWe can follow a similar analysis as in Appendix C of [2] to derive the topic recovery error given the sample estimate error. In particular, if we define the following sampling errors ES and ET :\n\u2016S\u0302 \u2212 S\u2016 \u2264 ES ,\n\u2016T\u0302 (u)\u2212 T (u)\u2016 \u2264 \u2016u\u20162ET ,\nthen the following form of their Lemma C.2 holds for both the LDA moments and the GP/DICA cumulants:\n\u2016W\u0302 T\u0302 (W\u0302>u)W\u0302> \u2212WT (W>u)W>\u2016 \u2264 \u03bd\n[ (maxk\u03b3k)ES\n\u03c3K ( D\u0303 )2 + ET \u03c3K ( D\u0303 )3 ] , (32)\nwhere \u03c3k(\u00b7) denotes the k-th singular value of a matrix, \u03bd is some universal constant, and in both cases D\u0303 was defined such that S = D\u0303D\u0303>. For the LDA moments, \u03b3k = 2 \u221a c0(c0+1)) ck(c0+2)2 , whereas for the GP/DICA cumulants, \u03b3k takes the simpler form \u03b3k := cum(\u03b1k)/[var(\u03b1k)] 3/2 = 2/ \u221a ck.\nWe note that the scaling for S is O(L2) for the GP/DICA cumulants, in contrast to O(1) for the LDA moments. Thus, to compare the upper bound (32) for the two types of moments, we need to put it in quantities which are common. In the first section of the Appendix C of [2], it was mentioned\nthat \u03c3K ( D\u0303 ) \u2265 \u221a\ncmin c0(c0+1) \u03c3K(D) for the LDA moments, where cmin := mink ck. In contrast, for the GP/DICA cumulants, we can show that \u03c3K ( D\u0303 ) \u2265 L \u221a cmin c0\n\u03c3K(D), where L := c0/b is the average length of a document in the GP model. Using this lower bound for the singular vector, we thus get the following bound in the case of the GP cumulant:\n\u2016W\u0302 T\u0302 (W\u0302>u)W\u0302> \u2212WT (W>u)W>\u2016 \u2264 \u03bd c 3/2 min\n[ ES L2 2c20[ \u03c3K ( D )]2 + ETL3 c30[\u03c3K(D)]3 ] . (33)\nThe c 3/2 min factor is common for both the LDA moment and GP cumulant, but as we mentioned after Proposition 3.1, the sample error ES term gets divided by L 2 for the GP cumulant, as expected.\nThe recovery error bound in [2] is based on the bound (33), and thus by showing that the error ES/L\n2 for the GP cumulant is lower than the ES term for the LDA moment, we expect to also gain a similar gain for the recovery error, as the rest of the argument is the same for both types of moments (see Appendix C.2, C.3 and C.4 in [2] for the completion)."}, {"heading": "D Appendix. The LDA moments", "text": "D.1 Our notation\nThe LDA moments were derived in [1]. Note that the full version of the paper with proofs appeared in [2] and a later version of the paper also appeared in [32]. In this section, we recall the form of the LDA moments using our notation. This section does not contain any novel results and is included for the reader\u2019s convenience. We also refer to this section when deriving the practical expressions for computation of the sample estimates of the LDA moments in Appendix E.3.\nFor deriving the LDA moments, a document is assumed to be composed of at least three tokens: L \u2265 3. As the LDA generative model (1) is only defined conditional on the length L, this is not too problematic. But given that we present models in this paper which also model L, we mention for clarity that we can suppose that all expectations and probabilities defined below are implicitly conditioning on L \u2265 3.12 The theoretical LDA moments are derived only using the first three words w1, w2 and w3 of a document. But note that since the words w`\u2019s are conditionally i.i.d. given \u03b8 (for 1 \u2264 ` \u2264 L), we have M3 := E(w1 \u2297w2 \u2297w3) = E(w`1 \u2297w`2 \u2297w`3) for any three distinct tokens `1, `2 and `3. The tensor M3 is thus symmetric, and could have been defined using any distinct `1, `2 and `3 that are less than L. To highlight this arbitrary choice and to make the links with the U-statistics estimator presented later, we thus use generic distinct `1, `2 and `3 in the definition of the LDA moments below, instead of `1 = 1, `2 = 2 and `3 = 1 as in [1].\nUsing this notation, then by the law of total expectation and the properties of the Dirichlet distribution, the non-central moments13 of the LDA model (1) take the form [1]:\nM1 = E(w`1) = D c\nc0 , (34)\nM2 = E(w`1w>`2) = c0\nc0 + 1 M1M\n> 1 +\n1\nc0(c0 + 1) Ddiag (c)D>, (35)\nM3 = E(w`1 \u2297 w`2 \u2297 w`3)\n= c0\nc0 + 2 [E(w`1 \u2297 w`2 \u2297M1) + E(w`1 \u2297M1 \u2297 w`3) + E(M1 \u2297 w`2 \u2297 w`3)] ,\n\u2212 2c 3 0\nc0(c0 + 1)(c0 + 2) M1 \u2297M1 \u2297M1 +\n2\nc0(c0 + 1)(c0 + 2) K\u2211 k=1 ckdk \u2297 dk \u2297 dk. (36)\nwhere \u2297 denotes the tensor product.\nSimilarly to the GP/DICA cumulants (as discussed in Appendix B.3), moving the terms in the non-central moments (34), (35), (36), the following quantities are defined\n(Pairs) = S := M2 \u2212 c0\nc0 + 1 M1M\n> 1 , LDA S-moment (37)\n(Triples) = T := M3 \u2212 c0\nc0 + 2 [E(w`1 \u2297 w`2 \u2297M1) + E(w`1 \u2297M1 \u2297 w`3) + E(M1 \u2297 w`2 \u2297 w`3)]\n+ 2c20\n(c0 + 1)(c0 + 2) M1 \u2297M1 \u2297M1. LDA T-moment (38)\n12Note that another advantage of the DICA cumulants from Section 3.1 is that they do not require such a somewhat artificial condition: they are well-defined for any document length (even a document of length zero!).\n13Note, the difference in the notation for the LDA moments in papers [1] and [3]. In [1], M1 = E(w`1 ), M2 = E(w`1 \u2297w`2 ), and M3 = E(w`1 \u2297w`2 \u2297w`3 ). However, in [3], M2 is equivalent to S in our notation and to Pairs in the notation of [1]; similarly, M3 is T in our notation or Triples in the notation of [1].\nSlightly abusing terminology, we refer to the entities S and T as the \u201cLDA moments\u201d. They have the following diagonal structure\nS = 1\nc0(c0 + 1) K\u2211 k=1 ckdkd > k , (39)\nT = 2\nc0(c0 + 1)(c0 + 2) K\u2211 k=1 ckdk \u2297 dk \u2297 dk. (40)\nNote however that this form of the LDA moments has a slightly different nature than the similar form (12) and (14) of the GP/DICA cumulants. Indeed, the former is the result of properties of the Dirichlet distribution, while the latter is the result of the independence of \u03b1\u2019s. However, one can think of the elements of a Dirichlet random vector as being almost independent (as, e.g., a Dirichlet random vector can be obtained from independent gamma variables through dividing each by their sum). Also, this closeness of the structures of the LDA moments and the GP cumulants can be explained by the closeness of the respective models as discussed in Section 2.\nD.2 Asymptotically unbiased finite sample estimators for the LDA moments\nGiven realizations wn`, n = 1, . . . , N , ` = 1, . . . , Ln, of the token random variable w`, we now give the expressions for the finite sample estimates of S (37) and T (38) for the LDA model (and we re-write them as a function of the sample counts xn). 14 We use the notation E\u0302 below to express a U-statistics empirical expectation over the token within a documents, uniformly averaged over the whole corpus. For example, E\u0302(w`1 \u2297 w`2 \u2297 M\u03021) := 1N \u2211N n=1 1 Ln(Ln\u22121) \u2211Ln `1=1 \u2211Ln `2=1 `2 6=`1 w`1 \u2297 w`2 \u2297 M\u03021.\nS\u0302 := M\u03022 \u2212 c0\nc0 + 1 M\u03021M\u0302\n> 1 , (41)\nT\u0302 := M\u03023 \u2212 c0\nc0 + 2\n[ E\u0302(w`1 \u2297 w`2 \u2297 M\u03021) + E\u0302(w`1 \u2297 M\u03021 \u2297 w`3) + E\u0302(M\u03021 \u2297 w`2 \u2297 w`3) ] +\n2c20 (c0 + 1)(c0 + 2) M\u03021 \u2297 M\u03021 \u2297 M\u03021, (42)\nwhere, as suggested in [3], unbiased U-statistics estimates of M1, M2 and M3 are:\nM\u03021 := E\u0302(w`) = 1\nN N\u2211 n=1 1 Ln Ln\u2211 `=1 wn` = 1 N N\u2211 n=1 [\u03b41]nxn = 1 N X\u03b41, (43)\nM\u03022 := E\u0302(w`1w>`2) = 1\nN N\u2211 n=1\n1\nLn(Ln \u2212 1) Ln\u2211 `1=1 Ln\u2211 `2=1 `2 6=`1 wn`1w > n`2\n= 1\nN N\u2211 n=1 [\u03b42]n\n( xnx > n \u2212 Ln\u2211 `=1 wn`w > n` )\n= 1\nN N\u2211 n=1 [\u03b42]n ( xnx > n \u2212 diag(xn) ) = 1\nN\n[ Xdiag(\u03b42)X > \u2212 diag(X\u03b42) ] , (44)\n(45)\n14Note that because non-linear functions of M\u03021 appear in the expression for S\u0302 (41) and T\u0302 (42), the estimator is biased, i.e. E(S\u0302) 6= S. The bias is small though: \u2016E(S\u0302)\u2212 S\u2016 = O(1/N) and the estimator is asymptotically unbiased. This is in contrast with the estimator for the GP/DICA moments which is easily made unbiased.\nM\u03023 := E\u0302(w`1 \u2297 w`2 \u2297 w`3) = 1\nN N\u2211 n=1 \u03b43n Ln\u2211 `1=1 Ln\u2211 `2=1 `2 6=`1 Ln\u2211 `3=1 `3 6=`2 `3 6=`1 wn`1 \u2297 wn`2 \u2297 wn`3\n= 1\nN N\u2211 n=1 [\u03b43]n\n( xn \u2297 xn \u2297 xn \u2212\nLn\u2211 `=1 wn` \u2297 wn` \u2297 wn`\n\u2212 Ln\u2211 `1=1 Ln\u2211 `2=1 `2 6=`1 (wn`1 \u2297 wn`1 \u2297 wn`2 + wn`1 \u2297 wn`2 \u2297 wn`1 + wn`1 \u2297 wn`2 \u2297 wn`2)  = 1\nN N\u2211 n=1 [\u03b43]n\n( xn \u2297 xn \u2297 xn + 2\nM\u2211 m=1 xnm(em \u2297 em \u2297 em)\n\u2212 M\u2211\nm1=1 M\u2211 m2=1 xnm1xnm2(em1 \u2297 em1 \u2297 em2 + em1 \u2297 em2 \u2297 em1 + em1 \u2297 em2 \u2297 em2)\n) . (46)\nHere, the vectors \u03b41, \u03b42 and \u03b43 \u2208 RN are defined as [\u03b41]n := L\u22121n ; [\u03b42]n := (Ln(Ln \u2212 1))\u22121, i.e. [\u03b42]n = [( Ln 2 ) 2! ]\u22121 is the number of times to choose an ordered pair of tokens out of Ln tokens;\n[\u03b43]n := (Ln(Ln\u22121)(Ln\u22122))\u22121, i.e. [\u03b43]n = [( Ln 3 ) 3! ]\u22121 is the number of times to choose an ordered triple of tokens out of Ln tokens. Note that the vectors \u03b41, \u03b42, and \u03b43 have nothing to do with the Kronecker delta \u03b4.\nFor a vector a \u2208 RN , we sometimes use notation [a]n to denote its n-th element. Similarly, for a matrix A \u2208 RM\u00d7N we use notation [A]mn to denote its (m,n)-th element.\nThere is a slight abuse of notation in the expressions above as w` is sometimes treated as a random variable (i.e. in E\u0302(w`), E\u0302(w`1w>`2), etc.) and sometimes as its realization. However, the difference is clear from the context."}, {"heading": "E Appendix. Practical aspects and implementation details", "text": "E.1 Whitening of S and dimensionality reduction\nThe algorithms from Section 4 require the computation of a whitening matrix W of S. Due to the similar diagonal structure ((39) and (12)) of the matrix S for both the LDA moments (37) and the GP/DICA cumulants (11), the computation of a whitening matrix is exactly the same in both cases.\nBy a whitening matrix, we mean a matrix W \u2208 RK\u00d7M (in practice, M K) that does not only whiten S \u2208 RM\u00d7M , but also reduces its dimensionality such that15 WSW> = IK .\nLet S = U\u03a3U> be an orthogonal eigendecomposition of the symmetric matrix S. Let \u03a31:K denotes the diagonal matrix that contains the largest K eigenvalues16 of S on its diagonal and let U1:K be a matrix with the respective eigenvalues in its columns. Then, a whitening matrix is\nW = \u03a3 \u20201/2 1:K U > 1:K , (47)\nwhere \u03a3 \u20201/2 1:K is a diagonal matrix constructed from \u03a31:K by taking the inverse and the square root of its non-zero diagonal values (\u2020 stands for the pseudo-inverse).\nIn practice, when only a finite sample estimator S\u0302 of S is available, the following finite sample estimator W\u0302 of W can be introduced\nW\u0302 := \u03a3\u0302 \u20201/2 1:K U\u0302 > 1:K , (48)\nwhere S\u0302 = U\u0302 \u03a3\u0302U\u0302>. 15Note that such a whitening matrix W \u2208 RK\u00d7M is not uniquely defined as left multiplication by any orthogonal matrix V \u2208 RK\u00d7K does not change anything. Indeed, let W\u0303 = VW , then W\u0303SW\u0303> = VWSW>V > = IK . 16We mean the largest non-negative eigenvalues. In theory, S have to be PSD. In practice, when we deal with finite number of samples, respective estimate of S can have negative eigenvalues. However, for K sufficiently small, S should have enough positive eigenvalues. Moreover, it is standard practice to use eigenvalues of S for estimation of a good value of K, e.g., by thresholding all negative and close to zero eigenvalues.\nE.2 Computation of the finite sample estimators of the GP/DICA cumulants\nIn this section, we present efficient formulas for computation of the finite sample estimate (see Appendix B.4 for the definition of T\u0302 ) of W\u0302 T\u0302 (v)W\u0302> for the GP/DICA models. The construction\nof the finite sample estimator W\u0302 is discussed in Appendix E.1, while the computation of S\u0302 (27) is straightforward.\nBy plugging the definition of the tensor T\u0302 (28) in the formula (17) for the projection of a tensor onto a vector, we obtain for a given v \u2208 RM :[\nT\u0302 (v) ] m1m2 = \u2211 m3 c\u0302um(xm1 , xm2 , xm3)vm3 + 2 \u2211 m3 \u03b4(m1,m2,m3)E\u0302(xm3)vm3\n\u2212 \u2211 m3 \u03b4(m2,m3)c\u0302ov(xm1 , xm2)vm3\n\u2212 \u2211 m3 \u03b4(m1,m3)c\u0302ov(xm1 , xm2)vm3\n\u2212 \u2211 m3 \u03b4(m1,m2)c\u0302ov(xm1 , xm3)vm3\n= \u2211 m3 c\u0302um(xm1 , xm2 , xm3)vm3 + 2\u03b4(m1,m2)E\u0302(xm1)vm1\n\u2212 c\u0302ov(xm1 , xm2)vm2 \u2212 c\u0302ov(xm1 , xm2)vm1 \u2212 \u03b4(m1,m2) \u2211 m3 c\u0302ov(xm1 , xm3)vm3 .\nThis gives the following for the expression W\u0302 T\u0302 (v)W\u0302>:[ W\u0302 T\u0302 (v)W\u0302> ] k1k2 = W\u0302k1 >T\u0302 (v)W\u0302k2\n= \u2211\nm1,m2,m3\nc\u0302um(xm1 , xm2 , xm3)vm3W\u0302k1m1W\u0302k2m2\n+ 2 \u2211 m1,m2 \u03b4(m1,m2)E\u0302(xm1)vm1W\u0302k1m1W\u0302k2m2\n\u2212 \u2211 m1,m2 c\u0302ov(xm1 , xm2)vm2W\u0302k1m1W\u0302k2m2\n\u2212 \u2211 m1,m2 c\u0302ov(xm1 , xm2)vm1W\u0302k1m1W\u0302k2m2\n\u2212 \u2211 m1,m3 c\u0302ov(xm1 , xm3)vm3W\u0302k1m1W\u0302k2m1 .\nwhere W\u0302k denotes the k-th row of W\u0302 as a column vector. By further plugging in the expressions (29) for the unbiased finite sample estimates of c\u0302ov and c\u0302um, we further get[\nW\u0302 T\u0302 (v)W\u0302> ] k1k2 = N (N \u2212 1)(N \u2212 2) \u2211 n \u2329 W\u0302k1 , xn \u2212 E\u0302(x) \u232a\u2329 W\u0302k2 , xn \u2212 E\u0302(x) \u232a\u2329 v, xn \u2212 E\u0302(x) \u232a + 2\n\u2211 m E\u0302(xm)vmW\u0302k1mW\u0302k2m\n\u2212 1 N \u2212 1 \u2211 n \u2329 W\u0302k1 , xn \u2212 E\u0302(x) \u232a\u2329 v \u25e6 W\u0302k2 , xn \u2212 E\u0302(x) \u232a \u2212 1 N \u2212 1 \u2211 n \u2329 v \u25e6 W\u0302k1 , xn \u2212 E\u0302(x) \u232a\u2329 W\u0302k2 , xn \u2212 E\u0302(x)\n\u232a \u2212 1 N \u2212 1 \u2211 n \u2329 W\u0302k1 \u25e6 W\u0302k2 , xn \u2212 E\u0302(x) \u232a\u2329 v, xn \u2212 E\u0302(x) \u232a ,\nwhere \u25e6 denotes the elementwise Hadamard product. Introducing the counts matrix X \u2208 RM\u00d7N where each element Xmn is the count of the m-th word in the n-th document (note, the matrix X\ncontain the vector xn in the n-th column), we further simplify the above expression\nW\u0302 T\u0302 (v)W\u0302> = N\n(N \u2212 1)(N \u2212 2) (W\u0302X)diag[X>v](W\u0302X)>\n+ N\n(N \u2212 1)(N \u2212 2)\n\u2329 v, E\u0302(x) \u232a [ 2N(W\u0302 E\u0302(x))(W\u0302 E\u0302(x))> \u2212 (W\u0302X)(W\u0302X)> ] \u2212 N\n(N \u2212 1)(N \u2212 2)\n[ W\u0302X(X>v)(W\u0302 E\u0302(x))> + W\u0302 E\u0302(x)(W\u0302X(X>v))> ] + 2W\u0302diag[v \u25e6 E\u0302(x)]W\u0302>\n\u2212 1 N \u2212 1\n[ (W\u0302X)(W\u0302diag(v)X)> + (W\u0302diag(v)X)(W\u0302X)> + W\u0302diag[X(X>v)]W\u0302> ] + N\nN \u2212 1\n[ (W\u0302 E\u0302(x))(W\u0302diag[v]E\u0302(x))> + (W\u0302diag[v]E\u0302(x))(W\u0302 E\u0302(x))> ] + N\nN \u2212 1\n\u2329 v, E\u0302(x) \u232a W\u0302diag[E\u0302(x)]W\u0302>.\n(49)\nFrom expression (49), we can see that the most computationally expensive part of computing W\u0302 T\u0302 (v)W\u0302> is the computation of the product of the whitening matrix W\u0302 \u2208 RK\u00d7M and counts matrix X \u2208 RM\u00d7N . As the latter is a sparse matrix, the complexity of this operation is approximately O(RmaxNK), where Rmax is the largest number of unique words (non-zero counts) in a document. Moreover, if (49) has to be computed multiple times for different vectors {v1, . . . , vP }, many operations, e.g. W\u0302X, (W\u0302X)(W\u0302X)>, (W\u0302 E\u0302(x))(W\u0302 E\u0302(x))>, do not have to be recomputed P times, which makes the overall computation time significantly faster.\nA more compact way to write down expression (49) is as follows\nW\u0302 T\u0302 (v)W\u0302> = N\n(N \u2212 1)(N \u2212 2)\n[ T1 + \u3008v, E\u0302(x)\u3009 (T2 \u2212 T3)\u2212 (T4 + T>4 ) ] + 1\nN \u2212 1\n[ T5 + T > 5 \u2212 T6 \u2212 T>6 + W\u0302diag(a)W\u0302> ] ,\n(50)\nwhere\nT1 = (W\u0302X)diag[X >v](W\u0302X)>, T2 = 2N(W\u0302 E\u0302(x))(W\u0302 E\u0302(x))>, T3 = (W\u0302X)(W\u0302X) >, T4 = W\u0302X(X >v)(W\u0302 E\u0302(x))>, T5 = (W\u0302X)(W\u0302diag(v)X) >, T6 = (W\u0302diag(v)E\u0302(x))(W\u0302 E\u0302(x))>,\na = 2(N \u2212 1)[v \u25e6 E\u0302(x)] + \u3008v, E\u0302(x)\u3009E\u0302(x)\u2212X(X>v).\nE.3 Computation of the finite sample estimators of the LDA moments\nIn this section, we present efficient formulas for computation of the finite sample estimate (see Appendix D.2 for the definition of T\u0302 ) of W\u0302 T\u0302 (v)W\u0302> for the LDA model. Note that the construction\nof the sample estimator W\u0302 of a whitening matrix W is discussed in Appendix E.1). The computation\nof S\u0302 (41) is straightforward. This approach to efficient implementation was discussed in [3], however, to the best of our knowledge, the final expressions were not explicitly stated before. All derivations are straightforward, but quite tedious.\nBy analogy with the GP/DICA case, a projection (17) of the tensor T\u0302 \u2208 RM\u00d7M\u00d7M (42) onto some vector v \u2208 RM in the LDA is[\nT\u0302 (v) ] m1m2 = M\u2211 m3=1 [ M\u03023 ] m1m2m3 vm3 + 2c20 (c0 + 1)(c0 + 2) \u2211 m3 [M\u03021]m1 [M\u03021]m2 [M\u03021]m3vm3\n\u2212 c0 c0 + 2 M\u2211 m3=1 [ E\u0302(w`1 \u2297 w`2 \u2297 M\u03021) + E\u0302(w`1 \u2297 M\u03021 \u2297 w`3) + E\u0302(M\u03021 \u2297 w`2 \u2297 w`3) ] m1m2m3 vm3 .\nPlugging in the expression (46) for an unbiased sample estimate M\u03023 of M3, we get [ T\u0302 (v) ] m1m2 = 1 N N\u2211 n=1 [\u03b43]n ( xnm1xnm2 \u3008xn, v\u3009+ 2 \u2211 m3 \u03b4(m1,m2,m3)xnm3vm3 )\n\u2212 1 N N\u2211 n=1 [\u03b43]n M\u2211 m3=1  M\u2211 i,j=1 xnixnj (ei \u2297 ei \u2297 ej + ei \u2297 ej \u2297 ei + ei \u2297 ej \u2297 ej)  m1m2m3 vm3 + 2c20\n(c0 + 1)(c0 + 2) [M\u03021]m1 [M\u03021]m2\n\u2329 M\u03021, v \u232a \u2212 c0 c0 + 2 ( [M\u03022]m1m2 \u2329 M\u03021, v \u232a + M\u2211 m3=1 ( [M\u03022]m1m3 [M\u03021]m2vm3 + [M\u03022]m2m3 [M\u03021]m1vm3 )) ,\nwhere e1, e2, . . . , eM denote the canonical vectors of RM (i.e. the columns of the identity matrix IM ). Further, this gives the following for the expression W\u0302 T\u0302 (v)W\u0302 >: [ W\u0302 T\u0302 (v)W\u0302>\n] k1k2 = 1 N N\u2211 n=1 [\u03b43]n ( \u3008xn, v\u3009 \u2329 xn, W\u0302k1 \u232a\u2329 xn, W\u0302k2 \u232a + 2 M\u2211 m=1 xnmvmW\u0302k1mW\u0302k2m )\n\u2212 1 N N\u2211 n=1 \u03b43n M\u2211 i,j=1 xnixnj ( W\u0302k1iW\u0302k2ivj + W\u0302k1iW\u0302k2jvi + W\u0302k1iW\u0302k2jvj ) \u2212 c0 c0 + 2 (\u2329 W\u0302k1 , [ M\u03022 ] W\u0302k2 \u232a + \u2329 W\u0302k1 , M\u03022v \u232a\u2329 M\u03021W\u0302k2 \u232a + \u2329 W\u0302k2 , M\u03022v \u232a\u2329 M\u03021, W\u0302k1\n\u232a) +\n2c20 (c0 + 1)(c0 + 2)\n\u2329 M\u03021, W\u0302k1 \u232a\u2329 M\u03021, W\u0302k2 \u232a\u2329 M\u03021, v \u232a ,\nwhere W\u0302k denotes the k-th row of W\u0302 as a column-vector. This further simplifies to\nW\u0302 T\u0302 (v)W\u0302> = 1\nN (W\u0302X)diag\n[ (X>v) \u25e6 \u03b43 ] (W\u0302X)>\n+ 1\nN W\u0302diag\n[ 2[(X\u03b43) \u25e6 v]\u2212X[(X>v) \u25e6 \u03b43] ] W\u0302>\n\u2212 1 N (W\u0302diag[v]X)diag[\u03b43](W\u0302X) > \u2212 1 N (W\u0302X)diag[\u03b43](W\u0302diag[v]X) > \u2212 c0 c0 + 2 [\u2329 M\u03021, v \u232a (W\u0302M\u03022W\u0302 >) + (W\u0302 (M\u03022v))(W\u0302M\u03021) > + (W\u0302M\u03021)(W\u0302 (M\u03022v)) > ] + 2c20\n(c0 + 1)(c0 + 2)\n\u2329 M\u03021, v \u232a (W\u0302M\u03021)(W\u0302M\u03021) >.\n(51)\nA more compact representation gives:\nW\u0302 T\u0302 (v)W\u0302> = 1\nN\n[ T1 + T2 \u2212 T3 \u2212 T>3 ] \u2212 c0 c0 + 2 [ \u3008M\u03021, v\u3009(W\u0302M\u03022W\u0302>) + T4 + T>4 ] +\n2c20 (c0 + 1)(c0 + 2)\n\u3008M\u03021, v\u3009(W\u0302M\u03021)(W\u0302M\u03021)>, (52)\nwhere\nT1 = (W\u0302X)diag [ (X>v) \u25e6 \u03b43 ] (W\u0302X)>,\nT2 = W\u0302diag [ 2[(X\u03b43) \u25e6 v]\u2212X[(X>v) \u25e6 \u03b43] ] W\u0302>, T3 = [W\u0302diag(v)X]diag(\u03b43)(W\u0302X) >, T4 = [W\u0302 (M\u03022v)](W\u0302M\u03021) >.\nThe comments regarding the computational time of (50) also apply here. However, in practice we noticed that the computation of (50) is slightly faster for large datasets than the computation of (52) (although the code for both was equally well optimized). This means that the constant in O(RNK) for the LDA moments is, probably, slightly larger than for the GP/DICA cumulants.\nE.4 Estimation of the model parameters for GP/DICA model\nGiven the output A and ap of Algorithm 1, the topic matrix is estimated as\nd\u0302k := max(0, [A \u2020]:k)/\u2016max(0, [A\u2020]:k)\u20161.\nThe truncation is necessary to enforce non-negativity and the normalization corresponds to the transformation from D\u0303 to D, where in the latter each topic is in the simplex.\nTo estimate the parameters for the prior distribution over topic intensities \u03b1k for the DICA model (5), we use the diagonalized form of the projected tensor from (18) and relate it to the output diagonal elements ap for the p-th projection:\n[ap]k = t\u0303k\u3008zk, up\u3009 = tk\ns 3/2 k\n\u3008zk, up\u3009 = cum(\u03b1k, \u03b1k, \u03b1k) [var(\u03b1k)]3/2 \u2329 dk,W >up \u232a . (53)\nThis formula is valid for any prior on \u03b1k in the DICA model. For the GP model (4) where \u03b1k \u223c Gamma(ck, b), we have that var(\u03b1k) = ck b2 and cum(\u03b1k, \u03b1k, \u03b1k) = 2ck b3 , and thus t\u0303k = 2\u221a ck , which enables us to estimate ck. Plugging this value of t\u0303k in (53), and solving for ck gives the following expression:\nck = 4 \u2329 dk,W >up \u232a2\n[ap]2k .\nBy replacing the quantities on the RHS with their estimated ones, we get one estimate for ck per projection. We use as our final estimate the average estimate over the projections:\nc\u0302k := 1\nP P\u2211 p=1\n4 \u2329 d\u0302k, W\u0302 >up \u232a2 [ap]2k . (54)\nReusing the properties of the length of documents for the GP model as described in Appendix A.2, we finally use the following estimates for rate parameter b of the gamma distribution:\nb\u0302 := c\u03020\nL\u0302 , (55)\nwhere c\u03020 := \u2211 k c\u0302k and L\u0302 is the average document length in the corpus."}, {"heading": "F Appendix. Supplementary experiments", "text": "F.1 The LDA moments vs parameter c0\nThe LDA moments depend on the parameter c0, which is not trivial to set in the unsupervised setting of topic modeling, especially taking into account the complexity of evaluation for topic models [31]. In Figure 4, the joint diagonalization algorithm with the LDA moment is compared for different values of c0 provided to the algorithm. The data is generated similarly to Figure 2. The experiment indicates that the LDA moments are somewhat sensitive to the choice of c0. For example, the recovery `1-error doubles when moving from the correct choice c0 = 1 to the plausible alternative c0 = 0.1 for K = 10 on the LDAfix1(200) dataset (JD-LDA(10) line on the right of Figure 4). When the error is already high (for both datasets when K = 50 for example), then the choice of c0 seems to matter less: the performance is uniformly bad.\nF.2 Comparison of the `1- and `2-errors\nThe sample complexity results [1] for the spectral algorithm for the LDA moments allow straightforward extension to the GP/DICA cumulants, if the results from Proposition 3.1 are taken into account. The analysis is, however, in terms of the `2-norm. Therefore, in Figure 5, we provide experimental comparison of the `1- and `2-errors to verify that they are indeed behaving similarly.\nF.3 One more real data experiment\nIn Figure 6 (right), we demonstrate one more experiment with real data as described in Section 5.3. Although the variational inference outperforms the joint diagonalization algorithm, the variational inference with warm JD-restarts is the best. Note that the fact that the joint diagonalization algorithm for the LDA moments is worse than the spectral algorithm indicates that the diagonal structure (39) and (40) might not be present in the sample estimates (41) and (42).\nSupplementary References\n[32] A. Anandkumar, D.P. Foster, D. Hsu, S.M. Kakade, and Y.-K. Liu. A spectral algorithm for latent Dirichlet allocation. Algorithmica, 72(1):193\u2013214, 2015.\n[33] B.A. Frigyik, A. Kapila, and M.R. Gupta. Introduction to the Dirichlet distribution and related processes. Technical report, University of Washington, 2010.\n[34] T.G. Kolda and B.W. Bader. Tensor decompositions and applications. SIAM Rev., 51(3):455\u2013 500, 2009."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We consider moment matching techniques for estimation in Latent Dirichlet Allocation (LDA). By drawing explicit links between LDA and discrete versions of independent component analysis (ICA), we first derive a new set of cumulant-based tensors, with an improved sample complexity. Moreover, we reuse standard ICA techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method. In an extensive set of experiments on both synthetic and real datasets, we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods.", "creator": "LaTeX with hyperref package"}}}