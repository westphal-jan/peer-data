{"id": "1507.05952", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jul-2015", "title": "Optimal Testing for Properties of Distributions", "abstract": "often given particular samples from an extremely unknown arithmetic distribution $ p $, is it practically possible to distinguish fully whether $ p $ belongs, to some class 1 of periodic distributions $ \\ mathcal { c } $ versus $ @ p $ being far from every specific distribution f in $ \\ lower mathcal { : c } $? when this fundamental scientific question it has undoubtedly received substantial tremendous attention in statistics, historically focusing primarily on harmonic asymptotic analysis, and more notably recently in visual information theory and beyond theoretical mainstream computer science, issues where quite the emphasis has firmly been on evaluating small sample size and computational simulation complexity. nevertheless, slightly even. for basic underlying properties required of distributions such as monotonicity, log - truth concavity, unimodality, algorithm independence, and monotone - hazard risk rate, simply the optimal sample complexity requirement is obviously unknown.", "histories": [["v1", "Tue, 21 Jul 2015 19:52:56 GMT  (986kb)", "https://arxiv.org/abs/1507.05952v1", null], ["v2", "Wed, 22 Jul 2015 14:12:33 GMT  (806kb)", "http://arxiv.org/abs/1507.05952v2", null], ["v3", "Tue, 8 Dec 2015 20:00:11 GMT  (77kb)", "http://arxiv.org/abs/1507.05952v3", "31 pages, extended abstract appeared as a spotlight in NIPS 2015"]], "reviews": [], "SUBJECTS": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "authors": ["jayadev acharya", "constantinos daskalakis", "gautam kamath"], "accepted": true, "id": "1507.05952"}, "pdf": {"name": "1507.05952.pdf", "metadata": {"source": "CRF", "title": "Optimal Testing for Properties of Distributions", "authors": ["Jayadev Acharya", "Constantinos Daskalakis"], "emails": ["jayadev@csail.mit.edu", "costis@mit.edu", "g@csail.mit.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 7.\n05 95\n2v 3\n[ cs\n.D S]\n8 D\nec 2\nWe provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution p, and a known distribution q, are p and q close in \u03c72-distance, or far in total variation distance? With this tool in place, we develop a general testing framework which leads to the following results:\n\u2022 Testing identity to any distribution over [n] requires \u0398(\u221an/\u03b52) samples. This is optimal for the uniform distribution. This gives an alternate argument for the minimax sample complexity of testing identity (proved in [VV14]). \u2022 For all d \u2265 1 and n sufficiently large, testing whether a discrete distribution over [n]d is monotone requires an optimal \u0398(nd/2/\u03b52) samples. The single-dimensional version of our theorem improves a long line of research starting with [BKR04], where the previous best tester required \u2126( \u221a n log(n)/\u03b54) samples, while the high-dimensional version\nimproves [BFRV11], which requires \u2126\u0303(nd\u2212 1 2poly(1\u03b5 )) samples.\n\u2022 For all d \u2265 1, testing whether a collection of random variables over [n1]\u00d7\u00b7 \u00b7 \u00b7\u00d7[nd] are independent requiresO (( ( \u220f\nl nl) 1/2 +\n\u2211 l nl ) /\u03b52 ) samples. A lower bound of \u2126 ( ( \u220f l nl) 1/2/\u03b52 )\nsamples is also proved. This result extends the known results for testing independence to more than two random variables. For the special case of d = 2, when n1 = n2 = n, this improves the results of [BFF+01] to the optimal \u0398(n/\u03b52) sample complexity. \u2022 Testing whether a discrete distribution over [n] is log-concave requires an optimal \u0398(\u221an/\u03b52) samples. The same is true for testing whether a distribution has a monotone hazard rate, and testing whether it is unimodal.\nThe optimality of our testers is established by providing matching lower bounds with respect to both n and \u03b5. Finally, a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave and monotone hazard rate distributions.\n\u2217Supported by a grant from MITEI-Shell program. \u2020Supported by a Sloan Foundation Fellowship, a Microsoft Research Faculty Fellowship and NSF Award CCF0953960 (CAREER) and CCF-1101491. \u2021Supported by NSF Award CCF-0953960 (CAREER). Work done in part while the author was at Microsoft Research Cambridge."}, {"heading": "1 Introduction", "text": "The quintessential scientific question is whether an unknown object has some property, i.e. whether a model from a specific class fits the object\u2019s observed behavior. If the unknown object is a probability distribution, p, to which we have sample access, we are typically asked to distinguish whether p belongs to some class C or whether it is sufficiently far from it.\nThis question has received tremendous attention in the field of statistics (see, e.g., [Fis25, LR06]), where test statistics for important properties such as the ones we consider here have been proposed. Nevertheless, the emphasis has been on asymptotic analysis, characterizing the rates of convergence of test statistics under null hypotheses, as the number of samples tends to infinity. In contrast, we wish to study the following problem in the small sample regime:\n\u03a0(C, \u03b5): Given a family of distributions C, some \u03b5 > 0, and sample access to an unknown distribution p over a discrete support, how many samples are required to distinguish between p \u2208 C versus dTV(p, C) > \u03b5?\nThe problem has been studied intensely in the literature on property testing and sublinear algorithms [Gol98, Fis01, Rub06, Ron08, Can15], where the emphasis has been on characterizing the optimal tradeoff between p\u2019s support size and the accuracy \u03b5 in the number of samples. Several results have been obtained, roughly clustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [BKR04, BFRV11]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [BFF+01, AAK+07]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [BFF+01, Pan08, VV14].\nWith respect to (iii), [VV14] exactly characterizes the number of samples required to test identity to each distribution q, providing a single tester matching this bound simultaneously for all q. Nevertheless, this tester and its precursors are not applicable to the composite identity testing problem that we consider. If our class C were finite, we could test against each element in the class, albeit this would not necessarily be sample optimal. If our class C were a continuum, we would need tolerant identity testers, which tend to be more expensive in terms of sample complexity [VV11], and result in substantially suboptimal testers for the classes we consider. Or we could use approaches related to generalized likelihood ratio test, but their behavior is not well-understood in our regime, and optimizing likelihood over our classes becomes computationally intense.\nOur Contributions In this paper, we obtain sample-optimal and computationally efficient testers for \u03a0(C, \u03b5) for the most fundamental shape restrictions to a distribution. Our contributions are the following:\n1. For a known distribution q over [n], and given samples from an unknown distribution p, we show that distinguishing the cases: (a) whether the \u03c72-distance between p and q is at most \u03b52/2, versus (b) the \u21131 distance between p and q is at least \u03b5, requires \u0398( \u221a n/\u03b52) samples. As\na corollary, we provide a simpler argument to show that identity testing requires \u0398( \u221a n/\u03b52) samples (previously shown in [VV14]).\n2. For the class C = Mdn of monotone distributions over [n]d we require an optimal \u0398 ( nd/2\n\u03b52\n)\nnum-\nber of samples, where prior work requires \u2126 (\u221a\nn logn \u03b54\n) samples for d = 1 and \u2126\u0303 ( nd\u2212 1 2poly (\n1 \u03b5\n)\n)\nfor d > 1 [BKR04, BFRV11]. Our results improve the exponent of n with respect to d, shave all logarithmic factors in n, and improve the exponent of \u03b5 by at least a factor of 2.\n(a) A useful building block and interesting byproduct of our analysis is extending Birge\u0301\u2019s oblivious decomposition for single-dimensional monotone distributions [Bir87] to monotone distributions in d \u2265 1, and to the stronger notion of \u03c72-distance. See Section C.1.\n(b) Moreover, we show that O(logd n) samples suffice to learn a monotone distribution over [n]d in \u03c72-distance. See Lemma 5 for the precise statement.\n3. For the class C = \u03a0d of product distributions over [n1] \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd], our algorithm requires O (( ( \u220f\n\u2113 n\u2113) 1/2 +\n\u2211 \u2113 n\u2113 ) /\u03b52 )\nsamples. We note that a product distribution is one where all marginals are independent, so this is equivalent to testing if a collection of random variables are all independent. In the case where n\u2113\u2019s are large, then the first term dominates, and the sample complexity is O(( \u220f\n\u2113 n\u2113) 1/2 /\u03b52). In particular, when d is a constant and all n\u2113\u2019s\nare equal to n, we achieve the optimal sample complexity of \u0398(nd/2/\u03b52). To the best of our knowledge, this is the first result for d \u2265 3, and when d = 2, this improves the previously known complexity from O (\nn \u03b56polylog(n/\u03b5)\n)\n[BFF+01, LRR13], significantly improving the dependence on \u03b5 and shaving all logarithmic factors.\n4. For the classes C = LCDn, C = MHRn and C = Un of log-concave, monotone-hazard-rate and unimodal distributions over [n], we require an optimal \u0398 (\u221a n\n\u03b52\n)\nnumber of samples. Our\ntesters for LCDn and C = MHRn are to our knowledge the first for these classes for the low sample regime we are studying\u2014see [HVK05] and its references for statistics literature on the asymptotic regime. Our tester for Un improves the dependence of the sample complexity on \u03b5 by at least a factor of 2 in the exponent, and shaves all logarithmic factors in n, compared to testers based on testing monotonicity.\n(a) A useful building block and important byproduct of our analysis are the first computationally efficient algorithms for properly learning log-concave and monotone-hazard-rate distributions, to within \u03b5 in total variation distance, from poly(1/\u03b5) samples, independent of the domain size n. See Corollaries 5 and 7. Again, these are the first computationally efficient algorithms to our knowledge in the low sample regime. [ADLS15, CDSS14] provide algorithms for density estimation, which are non-proper, i.e. will approximate an unknown distribution from these classes with a distribution that does not belong to these classes. On the other hand, the statistics literature focuses on maximum-likelihood estimation in the asymptotic regime\u2014see e.g. [CS10] and its references.\n5. For all the above classes we obtain matching lower bounds, showing that the sample complexity of our testers is optimal with respect to n, \u03b5 and when applicable d. See Section 10. Our lower bounds are based on extending Paninski\u2019s lower bound for testing uniformity [Pan08].\nAt the heart of our tester lies a novel use of the \u03c72 statistic. Naturally, the \u03c72 and its related \u21132 statistic have been used in several of the afore-cited results. We propose a new use of the \u03c7 2 statistic enabling our optimal sample complexity. The essence of our approach is to first draw a small number of samples (independent of n for log-concave and monotone-hazard-rate distributions and only logarithmic in n for monotone and unimodal distributions) to approximate the unknown distribution p in \u03c72 distance. If p \u2208 C, our learner is required to output a distribution q that is O(\u03b5)-close to C in total variation and O(\u03b52)-close to p in \u03c72 distance. Then some analysis reduces our testing problem to distinguishing the following cases:\n\u2022 p and q are O(\u03b52)-close in \u03c72 distance; this case corresponds to p \u2208 C.\n\u2022 p and q are \u2126(\u03b5)-far in total variation distance; this case corresponds to dTV(p, C) > \u03b5. We draw a comparison with robust identity testing, in which one must distinguish whether p and q are c1\u03b5-close or c2\u03b5-far in total variation distance, for constants c2 > c1 > 0. In [VV11], Valiant and Valiant show that \u2126(n/ log n) samples are required for this problem \u2013 a nearly-linear sample complexity, which may be prohibitively large in many settings. In comparison, the problem we study tests for \u03c72 closeness rather than total variation closeness: a relaxation of the previous problem. However, our tester demonstrates that this relaxation allows us to achieve a substantially sublinear complexity of O( \u221a n/\u03b52). On the other hand, this relaxation is still tight enough to be useful, demonstrated by our application in obtaining sample-optimal testers. We note that while the \u03c72 statistic for testing hypothesis is prevalent in statistics providing optimal error exponents in the large-sample regime, to the best of our knowledge, in the smallsample regime, modified-versions of the \u03c72 statistic have only been recently used for closenesstesting in [ADJ+12, CDVV14] and for testing uniformity of monotone distributions in [AJOT13]. In particular, [ADJ+12] design an unbiased statistic for estimating the \u03c72 distance between two unknown distributions.\nIn Section 4, we show that a version of the \u03c72 statistic, appropriately excluding certain elements of the support, is sufficiently well-concentrated to distinguish between the above cases. Moreover, the sample complexity of our algorithm is optimal for most classes. Our base tester is combined with the afore-mentioned extension of Birge\u0301\u2019s decomposition theorem to test monotone distributions in Section 5 (see Theorem 3 and Corollary 1), and is also used to test independence of distributions in Section 7 (see Theorem 6).\nNaturally, there are several bells and whistles that we need to add to the above skeleton to accommodate all classes of distributions that we are considering. For log-concave and monotonehazard distributions, we are unable to obtain a cheap (in terms of samples) learner that \u03c72approximates the unknown distribution p throughout its support. Still, we can identify a subset of the support where the \u03c72-approximation is tight and which captures almost all the probability mass of p. We extend our tester to accommodate excluding subsets of the support from the \u03c72-approximation. See Theorems 7 and 8 in Sections 8 and 9.\nFor unimodal distributions, we are even unable to identify a large enough subset of the support where the \u03c72 approximation is guaranteed to be tight. But we can show that there exists a light enough piece of the support (in terms of probability mass under p) that we can exclude to make the \u03c72 approximation tight. Given that we only use Chebyshev\u2019s inequality to prove the concentration of the test statistic, it would seem that our lack of knowledge of the piece to exclude would involve a union bound and a corresponding increase in the required number of samples. We avoid this through a careful application of Kolmogorov\u2019s max inequality in our setting. See Theorem 5 of Section 6.\nRelated Work. For the problems that we study in thie paper, we have provided the related works in the previous section along with our contributions. We cannot do justice to the role of shape restrictions of probability distributions in probabilistic modeling and testing. It suffices to say that the classes of distributions that we study are fundamental, motivating extensive literature on their learning and testing [BBBB72]. In the recent times, there has been work on shape restricted statistics, pioneered by Jon Wellner, and others. [JW09, BW10] study estimation of monotone and k\u2212 monotone densities, and [BJR11, SW14] study estimation of log-concave distributions.\nAs we have mentioned, statistics has focused on the asymptotic regime as the number of samples tends to infinity. Instead we are considering the low sample regime and are more stringent about the\nbehavior of our testers, requiring 2-sided guarantees. We want to accept if the unknown distribution is in our class of interest, and also reject if it is far from the class. For this problem, as discussed above, there are few results when C is a whole class of distributions. Closer related to our paper is the line of papers [BKR04, ACS10, BFRV11] for monotonicity testing, albeit these papers have sub-optimal sample complexity as discussed above. Testing independence of random variables has a long history in statisics [RS81, AK11]. The theoretical computer science community has also considered the problem of testing independence of two random variables [BFF+01, LRR13]. While our results sharpen the case where the variables are over domains of equal size, they demonstrate an interesting asymmetric upper bound when this is not the case. More recently, Acharya and Daskalakis provide optimal testers for the family of Poisson Binomial Distributions [AD15].\nFinally, contemporaneous work of Canonne et al [CDGR15a, CDGR15b] provides a generic algorithm and lower bounds for the single-dimensional families of distributions considered here. We note that their algorithm has a sample complexity which is suboptimal in both n and \u03b5, while our algorithms are optimal. Their algorithm also extends to mixtures of these classes, though some of these extensions are not computationally efficient. They also provide a framework for proving lower bounds, giving the optimal bounds for many classes when \u03b5 is sufficiently large with respect to 1/n. In comparison, we provide these lower bounds unconditionally by modifying Paninski\u2019s construction [Pan08] to suit the classes we consider."}, {"heading": "2 Preliminaries", "text": "We use the following probability distances in our paper.\nDefinition 1. The total variation distance between distributions p and q is defined as\ndTV(p, q) def = sup A |p(A)\u2212 q(A)| = 1 2 \u2016p\u2212 q\u20161.\nFor a subset of the domain, the total variation distance is defined as half of the \u21131 distance restricted to the subset.\nDefinition 2. The \u03c72-distance between p and q over [n] is defined by\n\u03c72(p, q) def =\n\u2211\ni\u2208[n]\n(pi \u2212 qi)2 qi =\n\n\n\u2211\ni\u2208[n]\np2i qi\n\n\u2212 1.\nDefinition 3. The Kolmogorov distance between two probability measures p and q over an ordered set (e.g., R) with cumulative density functions (CDF) Fp and Fq is defined as\ndK(p, q) def = sup x\u2208R |Fp(x)\u2212 Fq(x)|.\nOur paper is primarily concerned with testing against classes of distributions, defined formally as follows:\nDefinition 4. Given \u03b5 \u2208 (0, 1] and sample access to a distribution p, an algorithm is said to test a class C if it has the following guarantees:\n\u2022 If p \u2208 C, the algorithm outputs Accept with probability at least 2/3;\n\u2022 If dTV(p, C) \u2265 \u03b5, the algorithm outputs Reject with probability at least 2/3.\nThe Dvoretzky-Kiefer-Wolfowitz (DKW) inequality gives a generic algorithm for learning any distribution with respect to the Kolmogorov distance [DKW56].\nLemma 1. (See [DKW56],[Mas90]) Suppose we have n i.i.d. samples X1, . . . Xn from a distribution with CDF F . Let Fn(x) def = 1n \u2211n i=1 1{Xi\u2264x} be the empirical CDF. Then Pr[dK(F,Fn) \u2265 \u03b5] \u2264 2e\u22122n\u03b5 2 . In particular, if n = \u2126((1/\u03b52) \u00b7 log(1/\u03b4)), then Pr[dK(F,Fn) \u2265 \u03b5] \u2264 \u03b4.\nWe note the following useful relationships between these distances [GS02]:\nProposition 1. dK(p, q) 2 \u2264 dTV(p, q)2 \u2264 14\u03c72(p, q).\nIn this paper, we will consider the following classes of distributions:\n\u2022 Monotone distributions over [n]d (denoted by Mdn), for which i . j implies fi \u2265 fj1;\n\u2022 Unimodal distributions over [n] (denoted by Un), for which there exists an i\u2217 such that fi is non-decreasing for i \u2264 i\u2217 and non-increasing for i \u2265 i\u2217;\n\u2022 Log-concave distributions over [n] (denoted by LCDn), the sub-class of unimodal distributions for which fi\u22121fi+1 \u2264 f2i ;\n\u2022 Monotone hazard rate (MHR) distributions over [n] (denoted by MHRn), for which i < j implies fi1\u2212Fi \u2264 fj 1\u2212Fj .\nDefinition 5. An \u03b7-effective support of a distribution p is any set S such that p(S) \u2265 1\u2212 \u03b7.\nThe flattening of a function f over a subset S is the function f\u0304 such that f\u0304i = p(S)/|S|.\nDefinition 6. Let p be a distribution, and support I1, . . . is a partition of the domain. The flattening of p with respect to I1, . . . is the distribution p\u0304 which is the flattening of p over the intervals I1, . . ..\nPoisson Sampling Throughout this paper, we use the standard Poissonization approach. Instead of drawing exactly m samples from a distribution p, we first draw m\u2032 \u223c Poisson(m), and then draw m\u2032 samples from p. As a result, the number of times different elements in the support of p occur in the sample become independent, giving much simpler analyses. In particular, the number of times we will observe domain element i will be distributed as Poisson(mpi), independently for each i. Since Poisson(m) is tightly concentrated around m, this additional flexibility comes only at a sub-constant cost in the sample complexity with an inversely exponential in m, additive increase in the error probability."}, {"heading": "3 Overview", "text": "Our algorithm for testing a distribution p can be decomposed into three steps.\n1This definition describes monotone non-increasing distributions. By symmetry, identical results hold for monotone non-decreasing distributions.\nNear-proper learning in \u03c72-distance. Our first step requires a learning algorithm with very specific guarantees. In proper learning, we are given sample access to a distribution p \u2208 C, where C is some class of distributions, and we wish to output q \u2208 C such that p and q are close in total variation distance. In our setting, given sample access to p \u2208 C, we wish to output q such that q is close to C in total variation distance, and p and q are close in \u03c72-distance on an effective support2 of p. From an information theoretic standpoint, this problem is harder than proper learning, since \u03c72distance is more restrictive than total variation distance. Nonetheless, this problem can be shown to have comparable sample complexity to proper learning for the structured classes we consider in this paper.\nComputation of distance to class. The next step is to see if the hypothesis q is close to the class C or not. Since we have an explicit description of q, this step requires no further samples from p, i.e. it is purely computational. If we find that q is far from the class C, then it must be that p 6\u2208 C, as otherwise the guarantees from the previous step would imply that q is close to C. Thus, if it is not, we can terminate the algorithm at this point.\n\u03c72-testing. At this point, the previous two steps guarantee that our distribution q is such that:\n\u2022 If p \u2208 C, then p and q are close in \u03c72 distance on a (known) effective support of p;\n\u2022 If dTV(p, C) \u2265 \u03b5, then p and q are far in total variation distance.\nWe can distinguish between these two cases using O( \u221a n/\u03b52) samples with a simple statistical \u03c72-test, that we describe in Section 4.\nUsing the above three-step approach, our tester, as described in the next section, can directly test monotonicity, log-concavity, and monotone hazard rate. With an extra trick, using Kolmogorov\u2019s max inequality, it can also test unimodality."}, {"heading": "4 A Robust \u03c72-\u21131 Identity Test", "text": "Our main result in the Section is Theorem 2. As an immediate corollary, we obtain the following result on testing whether an unknown distribution is close in \u03c72 or far in \u21131 distance to a known distribution. In particular, we show the following:\nTheorem 1. For a known distribution q, there exists an algorithm with sample complexity\nO( \u221a n/\u03b52)\ndistinguishes between the cases\n\u2022 \u03c72(p, q) < \u03b52/10 versus\n\u2022 \u2016p \u2212 q\u2016 > \u03b52.\nwith probability at least 5/6.\nThis theorem follows from our main result of this section, stated next, slightly more generally for classes of distributions.\n2We also require the algorithm to output a description of an effective support for which this property holds. This requirement can be slightly relaxed, as we show in our results for testing unimodality.\nTheorem 2. Suppose we are given \u03b5 \u2208 (0, 1], a class of probability distributions C, sample access to a distribution p over [n], and an explicit description of a distribution q with the following properties:\nProperty 1. dTV(q, C) \u2264 \u03b52 .\nProperty 2. If p \u2208 C, then \u03c72(p, q) \u2264 \u03b52500 .\nThen there exists an algorithm with the following guarantees:\n\u2022 If p \u2208 C, the algorithm outputs Accept with probability at least 2/3;\n\u2022 If dTV(p, C) \u2265 \u03b5, the algorithm outputs Reject with probability at least 2/3.\nThe time and sample complexity of this algorithm are O (\u221a\nn \u03b52\n)\n.\nRemark 1. As stated in Theorem 2, Property 2 requires that q is O(\u03b52)-close in \u03c72-distance to p over its entire domain. For the class of monotone distributions, we are able to efficiently obtain such a q, which immediately implies sample-optimal learning algorithms for this class. However, for some classes, we cannot learn a q with such strong guarantees, and we must consider modifications to our base testing algorithm.\nFor example, for log-concave and monotone hazard rate distributions, we can obtain a distribution q and a set S with the following guarantees:\n\u2022 If p \u2208 C, then \u03c72(pS, qS) \u2264 O(\u03b52) and p(S) \u2265 1\u2212O(\u03b5);\n\u2022 If dTV(p, C) \u2265 \u03b5, then dTV(p, q) \u2265 \u03b5/2.\nIn this scenario, the tester will simply pretend the support of p and q is S, ignoring any samples and support elements in [n] \\ S. Analysis of this tester is extremely similar to what we present below. In particular, we can still show that the statistic Z will be separated in the two cases. When p \u2208 C, excluding [n]\\S will only reduce Z. On the other hand, when dTV(p, C) \u2265 \u03b5, since p(S) \u2265 1\u2212O(\u03b5), p and q must still be far on the remaining support, and we can show that Z is still sufficiently large. Therefore, a small modification allows us to handle this case with the same sample complexity of O(\n\u221a n/\u03b52). A further modification can handle even weaker learning guarantees. We could handle the previous case because the tester \u201cknows what we don\u2019t know\u201d \u2013 it can explicitly ignore the support over which we do not have a \u03c72-closeness guarantee. A more difficult case is when there may be a low measure interval hidden in our effective support, over which p and q have a large \u03c72-distance. While we may have insufficient samples to reliably identify this interval, it may still have a large effect on our statistic. A naive solution would be to consider a tester which tries all possible \u201cguesses\u201d for this \u201cbad\u201d interval, but a union bound would incur an extra logarithmic factor in the sample complexity. We manage to avoid this cost through a careful analysis involving Kolmogorov\u2019s max inequality, maintaining the O( \u221a n/\u03b52) sample complexity even in this more difficult case.\nBeing more precise, we can handle cases where we can obtain a distribution q and a set of intervals S = {I1, . . . , Ib} with the following guarantees:\n\u2022 If p \u2208 C, then p(S) \u2265 1\u2212O(\u03b5), p(Ij) = \u0398(p(S)/b) for all j \u2208 [b], and there exists a set T \u2286 [b] such that |T | \u2265 b\u2212 t (for t = O(1)) and \u03c72(pR, qR) \u2264 O(\u03b52), where R = \u222aT Ij ;\n\u2022 If dTV(p, C) \u2265 \u03b5, then dTV(p, q) \u2265 \u03b5/2.\nThis allows us to additionally test against the class of unimodal distributions. The tester requires that an effective support is divided into several intervals of roughly equal measure. It computes our statistic over each of these intervals, and we let our statistic Z be the sum of all but the largest t of these values. In the case when p \u2208 C, Z will only become smaller by performing this operation. We use Kolmogorov\u2019s maximal inequality to show that Z remains large when dTV(p, C) \u2265 \u03b5. More details on this tester are provided in Section D.\nAlgorithm 1 Chi-squared testing algorithm\n1: Input: \u03b5; an explicit distribution q; (Poisson) m samples from a distribution p, where Ni denotes the number of occurrences of the ith domain element. 2: A \u2190 {i : qi \u2265 \u03b5/50n} 3: Z \u2190 \u2211i\u2208A (Ni\u2212mqi)2\u2212Ni mqi 4: if Z \u2264 m\u03b52/10 then 5: return Accept\n6: else\n7: return Reject\n8: end if\nProof of Theorem 2: Theorem 2 is proven by analyzing Algorithm 1. As shown in Section A, Z has the following mean and variance:\nE [Z] = m \u00b7 \u2211\ni\u2208A\n(pi \u2212 qi)2 qi = m \u00b7 \u03c72(pA, qA) (1)\nVar [Z] = \u2211\ni\u2208A\n[\n2 p2i q2i + 4m \u00b7 pi \u00b7 (pi \u2212 qi) 2 q2i ]\n(2)\nwhere by pA and qA we denote respectively the vectors p and q restricted to the coordinates in A, and we slightly abuse notation when we write \u03c72(pA, qA), as these do not then correspond to probability distributions.\nLemma 2 demonstrates the separation in the means of the statistic Z in the two cases of interest, i.e., p \u2208 C versus dTV(p, C) \u2265 \u03b5, and Lemma 3 shows the separation in the variances in the two cases. These two results are proved in Section B.\nLemma 2. If p \u2208 C, then E [Z] \u2264 1500m\u03b52. If dTV(p, C) \u2265 \u03b5, then E [Z] \u2265 15m\u03b52.\nLemma 3. If p \u2208 C, then Var [Z] \u2264 1500000m2\u03b54. If dTV(p, C) \u2265 \u03b5, then Var [Z] \u2264 1100E[Z]2.\nAssuming Lemmas 2 and 3, Theorem 2 is now a simple application of Chebyshev\u2019s inequality. When p \u2208 C, we have that\nE [Z] + \u221a 3Var [Z]1/2 \u2264\n(\n1\n500 +\n\u221a 3 ( 1\n500000\n)1/2 )\nm\u03b52 \u2264 1 200 m\u03b52.\nThus, Chebyshev\u2019s inequality gives\nPr [ Z \u2265 m\u03b52/10 ] \u2264 Pr [ Z \u2265 m\u03b52/200 ] \u2264 Pr [ Z \u2212 E [Z] \u2265 \u221a 3Var [Z]1/2 ]\n\u2264 1 3 .\nThe case for dTV(p, C) \u2265 \u03b5 is similar. Here,\nE [Z]\u2212 \u221a 3Var [Z]1/2 \u2265\n(\n1\u2212 \u221a 3 ( 1\n100\n)1/2 )\nE[Z] \u2265 3m\u03b52/20.\nTherefore,\nPr [ Z \u2264 m\u03b52/10 ] \u2264 Pr [ Z \u2264 3m\u03b52/20 ] \u2264 Pr [ Z \u2212 E [Z] \u2264 \u2212 \u221a 3Var [Z]1/2 ]\n\u2264 1 3 ."}, {"heading": "5 Testing Monotonicity", "text": "As an application of our testing framework, we will demonstrate how to test for monotonicity. Let d \u2265 1, and i = (i1, . . . , id), j = (j1, . . . , jd) \u2208 [n]d. We say i < j if il > jl for l = 1, . . . , d.\nDefinition 7. A distribution p over [n]d is monotone (decreasing) if for all i < j, pi \u2264 pj.\nOur main result of this section is as follows:\nTheorem 3. For any d \u2265 1, there exists an algorithm for testing monotonicity over [n]d with sample complexity\nO\n(\nnd/2\n\u03b52 +\n(\nd log n\n\u03b52\n)d\n\u00b7 1 \u03b52\n)\nand time complexity O ( nd/2\n\u03b52 + poly(log n, 1/\u03b5)d\n)\n.\nIn particular, this implies the following optimal algorithms for monotonicity testing for all d \u2265 1:\nCorollary 1. Fix any d \u2265 1, and suppose \u03b5 > \u221a d logn n1/4 . Then there exists an algorithm for testing monotonicity over [n]d with sample complexity O ( nd/2/\u03b52 ) .\nOur analysis starts with a structural lemma about monotone distributions. In [Bir87], Birge\u0301 showed that any monotone distribution p over [n] can be obliviously decomposed into O(log(n)/\u03b5) intervals, such that the flattening p\u0304 (recall Definition 6) of p over these intervals is \u03b5-close to p in total variation distance. [AJOS14] extend this result, giving a bound between the \u03c72-distance of p and p\u0304. We strengthen these results by extending them to monotone distributions over [n]d. In particular, we partition the domain [n]d of p into O((d log(n)/\u03b52)d) rectangles, and compare it with p\u0304, the flattening over these rectangles.\nLemma 4. Let d \u2265 1. There is an oblivious decomposition of [n]d into O((d log(n)/\u03b52)d) rectangles such that for any monotone distribution p over [n]d, its flattening p\u0304 over these rectangles satisfy \u03c72(p, p\u0304) \u2264 \u03b52.\nThis effectively reduces the support size to logarithmic in n. At this point, we can apply the Laplace estimator (along the lines of [KOPS15]) and learn a q such that if p was monotone, then q will be O(\u03b52)-close in \u03c72-distance.\nLemma 5. Let d \u2265 1, and p be a monotone distribution over [n]d. There is an algorithm which outputs a distribution q such that E [ \u03c72(p, q) ]\n\u2264 \u03b52500 . The time and sample complexity are both O((d log(n)/\u03b52)d/\u03b52).\nThe final step before we apply our \u03c72-tester is to compute the distance between q and Mdn. This subroutine is similar to the one introduced by [BKR04]. The key idea is to write a linear program, which searches for any distribution f which is close to q in total variation distance. We note that the desired properties of f (i.e., monotonicity, normalization, and \u03b5-closeness to q) are easy to enforce as linear constraints. If we find that such an f exists, we will apply our \u03c72-test to q. If not, we output Reject, as this is sufficient evidence to conclude that p 6\u2208 Mdn. Note that the linear program operates over the oblivious decomposition used in our structural result, so the complexity is polynomial in (d log(n)/\u03b5)d, rather than the naive nd.\nAt this point, we have precisely the guarantees needed to apply Theorem 2, directly implying Theorem 3. Proof of the lemmas in this section are provided in Section C. We note that the class of monotone distributions is the simplest of the classes we consider. We now consider testing for log-concavity, monotone hazard rate, and unimodality, all of which are much more challenging to test. In particular, these classes require a more sophisticated structural understanding, more complex proper \u03c72-learning algorithms, and non-trivial modifications to our \u03c72-tester. We have already given some details on the required adaptations to the tester in Remark 1.\nOur algorithms for learning these classes use convex programming. One of the main challenges is to enforce log-concavity of the PDF when learning LCDn (respectively, of the CDF when learning MHRn), while simultaneously enforcing closeness in total variation distance. This involves a careful choice of our variables, and we exploit structural properties of the classes to ensure the soundness of particular Taylor approximations. We encourage the reader to refer to the proofs of Theorems 5, 7,and 8 for more details."}, {"heading": "6 Testing Unimodality", "text": "One striking feature of Birge\u0301\u2019s result is that the decomposition of the domain is oblivious to the samples, and therefore to the unknown distribution. However, such an oblivious decomposition will not work for the unimodal distribution, since the mode is unknown. Suppose we know where the mode of the unknown distribution might be, then the problem can be decomposed into monotone functions over two intervals. Therefore, in theory, one can modify the monotonicity testing algorithm by iterating over all the possible n modes. Indeed, by applying a union bound, it then follows that\nTheorem 4. (Follows from Monotone) For \u03b5 > 1/n1/4, there exists an algorithm for testing unimodality over [n] with sample complexity O (\u221a\nn \u03b52\nlog n ) .\nHowever, this is unsatisfactory, since our lower bound (and as we will demonstrate, the true complexity of this problem) is \u221a n/\u03b52. We overcome the logarithmic barrier introduced by the union bound, by employing a non-oblivious decomposition of the domain, and using Kolmogorov\u2019s max-inequality.\nOur main result for testing unimodality is the following theorem, which is proved in Section D.\nTheorem 5. Suppose \u03b5 > n\u22121/4. Then there exists an algorithm for testing unimodality over [n] with sample complexity O( \u221a n/\u03b52)."}, {"heading": "7 Testing Independence of Random Variables", "text": "Let X def= [n1] \u00d7 . . . \u00d7 [nd], and let \u03a0d be the class of all product distributions over X . We first bound the \u03c72-distance between product distributions in terms of the individual coordinates.\nLemma 6. Let p = p1 \u00d7 p2 . . .\u00d7 pd, and q = q1 \u00d7 q2 . . .\u00d7 qd be two distributions in \u03a0d. Then\n\u03c72(p, q) = d \u220f\n\u2113=1\n(1 + \u03c72(p\u2113, q\u2113))\u2212 1.\nProof. By the definition of \u03c72-distance\n\u03c72(p, q) = \u2211\ni\u2208X\np2i qi \u2212 1 (3)\n=\nd \u220f\n\u2113=1\n\n\n\u2211\ni\u2208[n\u2113]\n( p\u2113i )2\nq\u2113i\n\n\u2212 1 (4)\n=\nd \u220f\n\u2113=1\n( 1 + \u03c72(p\u2113, q\u2113) ) \u2212 1. (5)\nAlong the lines of learning monotone distributions in \u03c72 distance we obtain the following result, proved in Section E.\nLemma 7. There is an algorithm that takes\nO\n(\nd \u2211\n\u2113=1\nn\u2113 \u03b52\n)\nsamples from a distribution p in \u03a0d and outputs a distribution q \u2208 \u03a0d such that with probability at least 5/6,\n\u03c72(p, q) \u2264 O(\u03b52).\nThis fits precisely in our framework of robust \u03c72-\u21131 testing. In particular, applying Theorem 2, we obtain the following result.\nTheorem 6. For any d \u2265 1, there exists an algorithm for testing independence of random variables over [n1]\u00d7 . . . [nd] with sample and time complexity\nO\n(\n( \u220fd \u2113=1 n\u2113) 1/2 + \u2211d \u2113=1 n\u2113\n\u03b52\n)\n.\nThe following corollaries are immediate.\nCorollary 2. Suppose \u220fd \u2113=1 n 1/2 \u2113 \u2265 \u2211d \u2113=1 n\u2113. Then there exists an algorithm for testing independence over [n1]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd] with sample complexity \u0398(( \u220fd \u2113=1 n\u2113) 1/2/\u03b52).\nIn particular,\nCorollary 3. There exists an algorithm for testing if two distributions over [n] are independent with sample complexity \u0398(n/\u03b52)."}, {"heading": "8 Testing Log-Concavity", "text": "In this section we describe our results for testing log-concavity of distributions. Our main result is as follows:\nTheorem 7. There exists an algorithm for testing log-concavity over [n] with sample complexity\nO\n(\u221a n\n\u03b52 +\n1\n\u03b55\n)\nand time complexity poly(n, 1/\u03b5).\nIn particular, this implies the following optimal tester for this class:\nCorollary 4. Suppose \u03b5 > 1/n1/5. Then there exists an algorithm for testing log-concavity over [n] with sample complexity O (\u221a n/\u03b52 ) .\nOur algorithm will fit into the structure of our general framework. We first perform a very particular type of learning algorithm, whose guarantees are summarized in the following lemma:\nLemma 8. Given \u03b5 > 0 and sample access to a distribution p, there exists an algorithm with the following guarantees:\n\u2022 If p \u2208 LCDn, the algorithm outputs a distribution q \u2208 LCDn and an O(\u03b5)-effective support S of p such that \u03c72(pS , qS) \u2264 \u03b5 2\n500 with probability at least 5/6;\n\u2022 If dTV(p,LCDn) \u2265 \u03b5, the algorithm either outputs a distribution q \u2208 LCDn or Reject.\nThe sample complexity is O(1/\u03b55) and the time complexity is poly(n, 1/\u03b5).\nWe note that as a corollary, one immediately obtains aO(1/\u03b55) proper learning algorithm for logconcave distributions. The result is immediate from the first item of Lemma 8 and Proposition 1. We can actually do a bit better \u2013 in the proof of Lemma 8, we partition [n] into intervals of probability mass \u0398(\u03b53/2). If one instead partitions into intervals of probability mass \u0398(\u03b5/ log(1/\u03b5)) and works directly with total variation distance instead of \u03c72 distance, one can show that O\u0303(1/\u03b54) samples suffice.\nCorollary 5. Given \u03b5 > 0 and sample access to a distribution p \u2208 LCDn, there exists an algorithm which outputs a distribution q \u2208 LCDn such that dTV(p, q) \u2264 \u03b5. The sample complexity is O\u0303(1/\u03b54) and the time complexity is poly(n, 1/\u03b5).\nThen, given the guarantees of Lemma 8, Theorem 7 follows from Theorem 23. The details of these results are presented in Section F."}, {"heading": "9 Testing for Monotone Hazard Rate", "text": "In this section, we obtain our main result for testing for monotone hazard rate:\n3To be more precise, we require the modification of Theorem 7 which is described in Section 4, in order to handle the case where the \u03c72-distance guarantees only hold for a known effective support.\nTheorem 8. There exists an algorithm for testing monotone hazard rate over [n] with sample complexity\nO\n(\u221a n\n\u03b52 +\nlog(n/\u03b5)\n\u03b54\n)\nand time complexity poly(n, 1/\u03b5).\nThis implies the following optimal tester for the class:\nCorollary 6. Suppose \u03b5 > \u221a log(n/\u03b5)/n1/4. Then there exists an algorithm for testing monotone hazard rate over [n] with sample complexity O (\u221a n/\u03b52 ) .\nWe obey the same framework as before, first applying a \u03c72-learner with the following guarantees:\nLemma 9. Given \u03b5 > 0 and sample access to a distribution p, there exists an algorithm with the following guarantees:\n\u2022 If p \u2208 MHRn, the algorithm outputs a distribution q \u2208 MHRn and an O(\u03b5)-effective support S of p such that \u03c72(pS , qS) \u2264 \u03b5 2\n500 with probability at least 5/6;\n\u2022 If dTV(p,MHRn) \u2265 \u03b5, the algorithm either outputs a distribution q \u2208 MHRn and a set S \u2286 [n] or Reject.\nThe sample complexity is O(log(n/\u03b5)/\u03b54) and the time complexity is poly(n, 1/\u03b5).\nAs with log-concave distributions, this implies the following proper learning result:\nCorollary 7. Given \u03b5 > 0 and sample access to a distribution p \u2208 MHRn, there exists an algorithm which outputs a distribution q \u2208 MHRn such that dTV(p, q) \u2264 \u03b5. The sample complexity is O(log(n/\u03b5)/\u03b54) and the time complexity is poly(n, 1/\u03b5).\nAgain, combining the learning guarantees of Lemma 9 with the appropriate variant of Theorem 2, we obtain Theorem 8. The details of the argument and proofs are presented in Section G."}, {"heading": "10 Lower Bounds", "text": "We now prove sharp lower bounds for the classes of distributions we consider. We show that the example studied by Paninski [Pan08] to prove lower bounds on testing uniformity can be used to prove lower bounds for the classes we consider. They consider a class Q consisting of 2n/2 distributions defined as follows. Without loss of generality assume that n is even. For each of the 2n/2 vectors z0z1 . . . zn/2\u22121 \u2208 {\u22121, 1}n/2, define a distribution q \u2208 Q over [n] as follows.\nqi =\n{\n(1+z\u2113c\u03b5) n for i = 2\u2113+ 1 (1\u2212z\u2113c\u03b5) n for i = 2\u2113.\n(6)\nEach distribution in Q has a total variation distance c\u03b5/2 from Un, the uniform distribution over [n]. By choosing c to be an appropriate constant, Paninski [Pan08] showed that a distribution picked uniformly at random fromQ cannot be distinguished from Un with fewer than \u221a n/\u03b52 samples with probability at least 2/3. Suppose C is a class of distributions such that\n\u2022 The uniform distribution Un is in C,\n\u2022 For appropriately chosen c, dTV(C,Q) \u2265 \u03b5, then testing C is not easier than distinguishing Un from Q. Invoking [Pan08] immediately implies that testing the class C requires \u2126(\u221an/\u03b52) samples.\nThe lower bounds for all the one dimensional distributions will follow directly from this construction, and for testing monotonicity in higher dimensions, we extend this construction to d \u2265 1, appropriately. These arguments are proved in Section H, leading to the following lower bounds for testing these classes:\nTheorem 9.\n\u2022 For any d \u2265 1, any algorithm for testing monotonicity over [n]d requires \u2126(nd/2/\u03b52) samples.\n\u2022 For d \u2265 1, any algorithm for testing independence over [n1]\u00d7\u00b7 \u00b7 \u00b7\u00d7[nd] requires \u2126 ( (n1\u00b7n2...\u00b7nd)1/2 \u03b52 )\nsamples.\n\u2022 Any algorithm for testing unimodality, log-concavity, or monotone hazard rate over [n] requires \u2126( \u221a n/\u03b52) samples."}, {"heading": "Acknowledgements", "text": "The authors thank Cle\u0301ment Canonne and Jerry Li; the former for several useful comments and suggestions on previous drafts of this work, and both for helpful discussions and thoughts regarding independence testing."}, {"heading": "A Moments of the Chi-Squared Statistic", "text": "We analyze the mean and variance of the statistic\nZ = \u2211\ni\u2208A\n(Xi \u2212mqi)2 \u2212Xi mqi ,\nwhere each Xi is independently distributed according to Poisson(mpi). We start with the mean:\nE [Z] = \u2211\ni\u2208A E\n[ (Xi \u2212mqi)2 \u2212Xi mqi ]\n= \u2211\ni\u2208A\nE [ X2i ] \u2212 2mqiE [Xi] +m2q2i \u2212 E [Xi] mqi\n= \u2211\ni\u2208A\nm2p2i +mpi \u2212 2m2qipi +m2q2i \u2212mpi mqi\n= m \u2211\ni\u2208A\n(pi \u2212 qi)2 qi\n= m \u00b7 \u03c72(pA, qA)\nNext, we analyze the variance. Let \u03bbi = E [Xi] = mpi and \u03bb \u2032 i = mqi.\nVar [Z] = \u2211\ni\u2208A\n1\n\u03bb\u20322i Var\n[ (Xi \u2212 \u03bbi)2 + 2(Xi \u2212 \u03bbi)(\u03bbi \u2212 \u03bb\u2032i)\u2212 (Xi \u2212 \u03bbi) ]\n= \u2211\ni\u2208A\n1\n\u03bb\u20322i Var\n[ (Xi \u2212 \u03bbi)2 + (Xi \u2212 \u03bbi)(2\u03bbi \u2212 2\u03bb\u2032i \u2212 1) ]\n= \u2211\ni\u2208A\n1\n\u03bb\u20322i E [ (Xi \u2212 \u03bbi)4 + 2(Xi \u2212 \u03bbi)3(2\u03bbi \u2212 2\u03bb\u2032i \u2212 1) + (Xi \u2212 \u03bbi)2(2\u03bbi \u2212 2\u03bb\u2032i \u2212 1)2 \u2212 \u03bb2i ]\n= \u2211\ni\u2208A\n1\n\u03bb\u20322i [3\u03bb2i + \u03bbi + 2\u03bbi(2\u03bbi \u2212 2\u03bb\u2032i \u2212 1) + \u03bbi(2\u03bbi \u2212 2\u03bb\u2032i \u2212 1)2 \u2212 \u03bb2i ]\n= \u2211\ni\u2208A\n1\n\u03bb\u20322i [2\u03bb2i + \u03bbi + 4\u03bbi(\u03bbi \u2212 \u03bb\u2032i)\u2212 2\u03bbi + \u03bbi(4(\u03bbi \u2212 \u03bb\u2032i)2 \u2212 4(\u03bbi \u2212 \u03bb\u2032i) + 1)]\n= \u2211\ni\u2208A\n1\n\u03bb\u20322i [2\u03bb2i + 4\u03bbi(\u03bbi \u2212 \u03bb\u2032i)2]\n= \u2211\ni\u2208A\n[\n2 p2i q2i + 4m \u00b7 pi \u00b7 (pi \u2212 qi) 2 q2i ]\n(7)\nThe third equality is by noting the random variable has expectation \u03bbi and the fourth equality substitutes the values of centralized moments of the Poisson distribution."}, {"heading": "B Analysis of our \u03c72-Test Statistic", "text": "We first prove the key lemmas in the analysis of our \u03c72-test. Proof of Lemma 2: The former case is straightforward from (1) and Property 2 of q.\nWe turn to the latter case. Recall that A = {i : qi \u2265 \u03b5/50n}, and thus q(A\u0304) \u2264 \u03b5/50. We first show that dTV(pA, qA) \u2265 6\u03b525 , where pA, qA are defined as above and in our slight abuse of notation we use dTV(pA, qA) for non-probability vectors to denote 12\u2016pA \u2212 qA\u20161.\nPartitioning the support into A and A\u0304, we have dTV(p, q) = dTV(pA, qA) + dTV(pA\u0304, qA\u0304). (8)\nWe consider the following cases separately:\n\u2022 p(A\u0304) \u2264 \u03b5/2: In this case,\ndTV(pA\u0304, qA\u0304) = 1\n2\n\u2211 i\u2208A\u0304 |pi \u2212 qi| \u2264\n1 2 (p(A\u0304) + q(A\u0304)) \u2264 1 2 (\u03b5 2 + \u03b5 50 ) = 13\u03b5 50 .\nPlugging this in (8), and using the fact that dTV(p, q) \u2265 \u03b5 shows that dTV(pA, qA) \u2265 6\u03b525 . \u2022 p(A\u0304) > \u03b5/2: In this case, by the reverse triangle inequality,\ndTV(pA, qA) \u2265 1 2 (q(A)\u2212 p(A)) \u2265 1 2 ((1\u2212 \u03b5/50) \u2212 (1\u2212 \u03b5/2)) = 6\u03b5 25 .\nBy the Cauchy-Schwarz inequality,\n\u03c72(pA, qA) \u2265 4 dTV(pA, qA)2\nq(A)\n\u2265 \u03b5 2\n5 .\nWe conclude by recalling (1).\nProof of Lemma 3: We bound the terms of (2) separately, starting with the first.\n2 \u2211\ni\u2208A\np2i q2i = 2 \u2211\ni\u2208A\n( (pi \u2212 qi)2 q2i + 2piqi \u2212 q2i q2i )\n= 2 \u2211\ni\u2208A\n( (pi \u2212 qi)2 q2i + 2qi(pi \u2212 qi) + q2i q2i )\n\u2264 2n+ 2 \u2211\ni\u2208A\n( (pi \u2212 qi)2 q2i + 2 (pi \u2212 qi) qi )\n\u2264 4n+ 4 \u2211\ni\u2208A\n(pi \u2212 qi)2 q2i\n\u2264 4n+ 200n \u03b5 \u2211\ni\u2208A\n(pi \u2212 qi)2 qi\n= 4n+ 200n\n\u03b5\nE[Z]\nm\n\u2264 4n+ 1 100\n\u221a nE[Z] (9)\nThe second inequality is the AM-GM inequality, the third inequality uses that qi \u2265 \u03b550n for all i \u2208 A, the last equality uses (1), and the final inequality substitutes a value m \u2265 20000 \u221a n\n\u03b52 . The second term can be similarly bounded:\n4m \u2211\ni\u2208A\npi(pi \u2212 qi)2 q2i \u2264 4m ( \u2211\ni\u2208A\np2i q2i\n)1/2 ( \u2211\ni\u2208A\n(pi \u2212 qi)4 q2i\n)1/2\n\u2264 4m ( 4n+ 1\n100\n\u221a nE[Z]\n)1/2 (\n\u2211\ni\u2208A\n(pi \u2212 qi)4 q2i\n)1/2\n\u2264 4m ( 2 \u221a n+ 1\n10 n1/4E[Z]1/2\n)\n(\n\u2211\ni\u2208A\n(pi \u2212 qi)2 qi\n)\n=\n( 8 \u221a n+ 2\n5 n1/4E[Z]1/2\n)\nE[Z]\nThe first inequality is Cauchy-Schwarz, the second inequality uses (9), the third inequality uses the monotonicity of the \u2113p norms, and the equality uses (1).\nCombining the two terms, we get\nVar [Z] \u2264 4n+ 9\u221anE [Z] + 2 5 n1/4E [Z]3/2 .\nWe now consider the two cases in the statement of our lemma.\n\u2022 When p \u2208 C, we know from Lemma 2 that E [Z] \u2264 1500m\u03b52. Combined with a choice of m \u2265 20000 \u221a n\n\u03b52 and the above expression for the variance, this gives:\nVar [Z] \u2264 4 200002 m2\u03b54 + 9 20000 \u00b7 500m 2\u03b54 +\n\u221a 10\n12500000 m2\u03b54 \u2264 1 500000 m2\u03b54.\n\u2022 When dTV(p, C) \u2265 \u03b5, Lemma 2 and m \u2265 20000 \u221a n\n\u03b52 give:\nE [Z] \u2265 1 5 m\u03b52 \u2265 4000\u221an.\nCombining this with our expression for variance we get:\nVar [Z] \u2264 4 40002 E [Z]2 + 9 4000 E [Z]2 +\n2\n5 \u221a 4000 E [Z]2 \u2264 1 100 E [Z]2 ."}, {"heading": "C Details on Testing Monotonicity", "text": "In this section, we prove the lemmas necessary for our monotonicity testing result.\nC.1 A Structural Result for Monotone Distributions on the Hypergrid\nBirge\u0301 [Bir87] showed that any monotone distribution is estimated to a total variation \u03b5 with a O(log(n)/\u03b5)-piecewise constant distribution. Moreover, the intervals over which the output is constant is independent of the distribution p. This result, was strengthened to the Kullback-Leibler divergence by [AJOS14] to study the compression of monotone distributions. They upper bound the KL divergence by \u03c72 distance and then bound the \u03c72 distance. We extend this result to [n]d. We divide [n]d into bd rectangles as follows. Let {I1, . . . , Ib} be a partition of [n] into consecutive intervals defined as:\n|Ij| = { 1 for 1 \u2264 j \u2264 b2 , \u230a2(1 + \u03b3)j\u2212b/2\u230b for b2 < j \u2264 b.\nFor j = (j1, . . . , jd) \u2208 [b]d, let Ij def= Ij1 \u00d7 Ij2 \u00d7 . . . \u00d7 Ijd . The \u03c72 distance between p and p\u0304 can be bounded as\n\u03c72(p, p\u0304) =\n\n\n\u2211\nj\u2208[b]d\n\u2211\ni\u2208Ij\np2i p\u0304i\n\n\u2212 1\n\u2264\n\n\n\u2211\nj\u2208[b]d p+ j |Ij|\n\n\u2212 1\nFor j = (j1, . . . , jd) \u2208 Slarge, let j\u2217 = (j\u22171 , . . . , j\u2217b ) be\nj\u2217i =\n{\nji if ji \u2264 b/2 + 1 ji \u2212 1 otherwise.\nWe bound the expression above as follows. Let T \u2286 [d] be any subset of d. Suppose the size of T is \u2113. Let T\u0304 be the set of all j that satisfy ji = b/2+1 for i \u2208 T . In other words, over the dimensions determined by T , the value of the index is equal to d/2 + 1. The map j \u2192 j\u2217 restricted to T is one-to-one, and since at most d \u2212 \u2113 of the coordinates drop,\n|Ij| \u2264 |Ij\u2217 | \u00b7 (1 + \u03b3)d\u2212\u2113. Since there are \u2113 coordinates that do not change, and each of them have 2(1 + \u03b3) coordinates, we obtain\n\u2211 j\u2208T\u0304 pj \u2264 \u2211 j\u2208T\u0304 p\u2212 j\u2217 \u00b7 |Ij| \u00b7 (2(1 + \u03b3))\u2113 \u00b7 (1 + \u03b3)d\u2212\u2113\n= \u2211 j\u2208T\u0304 p\u2212j\u2217 \u00b7 |Ij\u2217 | \u00b7 2\u2113(1 + \u03b3)d.\nSince the mapping is one-to-one, the probability of observing as element in T\u0304 is the probability of observing b/2+1 in \u2113 coordinates, which is at most (2/(b+2))\u2113 under any monotone distribution. Therefore,\n\u2211 j\u2208T\u0304 pj \u2264\n(\n2\nb+ 2\n)\u2113\n\u00b7 2\u2113(1 + \u03b3)d.\nFor any \u2113 there are (d \u2113 ) choices for T . Therefore,\n\u03c72(p, p\u0304) \u2264 d \u2211\n\u2113=0\n(\nd\n\u2113\n)(\n4\nb+ 2\n)\u2113\n(1 + \u03b3)d \u2212 1\n= (1 + \u03b3)d ( 1 + 4\nb+ 2\n)d\n\u2212 1\n=\n(\n1 + \u03b3 + 4\nb+ 2 +\n4\u03b3\nb+ 2\n)d\n\u2212 1\nRecall that \u03b3 = 2 log(n)/b > 1/b, implies that the expression above is at most (1 + 2\u03b3)d \u2212 1. This implies Lemma 4.\nC.2 Monotone Learning\nOur algorithm requires a distribution q satisfying the properties discussed earlier. We learn a monotone distribution from samples as follows.\nBefore proving this result, we prove a general result for \u03c72 learning of arbitrary discrete distributions, adapting the result from [KOPS15]. For a distribution p, and a partition of the domain into b intervals I1, . . . , Ib, let p\u0304i = p(Ii)/|Ii| be the flattening of p over these intervals. We saw that for monotone distributions there exists a partition of the domain such that p\u0304 is close to the underlying distribution in \u03c72 distance.\nSuppose we are given m samples from a distribution p and a partition I1, . . . , Ib. Let mj be the number of samples that fall in Ij . For i \u2208 Ij, let\nqi def =\n1 |Ij | mj + 1 m+ b .\nLet Sj = \u2211 i\u2208Ij p 2 i . The expected \u03c7 2 distance between p and q can be bounded as follows.\nE [ \u03c72(p, q) ] =\n\n\nb \u2211\nj=1\n\u2211\ni\u2208Ij\nm \u2211\n\u2113=0\n(\nm\n\u2113\n)\n(p(Ij)) \u2113(1\u2212 p(Ij))m\u2212\u2113 p2i (\u2113+ 1)/(|Ij |(m+ b))\n\n\u2212 1\n=\n\n\nm+ b\nm+ 1\nb \u2211\nj=1\nSj p\u0304(Ij)/|Ij |\n(\nm \u2211\n\u2113=0\n(\nm+ 1\n\u2113+ 1\n)\n(p(Ij)) \u2113+1(1\u2212 p(Ij))m+1\u2212\u2113+1\n)\n\n\u2212 1\n=\n\n\nm+ b\nm+ 1\nb \u2211\nj=1\nSj p\u0304(Ij)/|Ij | ( 1\u2212 (1\u2212 p(Ij)m+1 )\n\n\u2212 1\n\u2264\n\n\nm+ b\nm+ 1\nb \u2211\nj=1\nSj p\u0304(Ij)/|Ij |\n\n\u2212 1\n=\n[\nm+ b\nm+ 1\n( \u03c72(p, p\u0304) + 1 )\n]\n\u2212 1\n= m+ b m+ 1 \u00b7 \u03c72(p, p\u0304) + b m+ 1 . (10)\nSuppose \u03b3 = O(log(n)/b), and b = O(d \u00b7 log(n)/\u03b52). Then, by Lemma 4, \u03c72(p, p\u0304) \u2264 \u03b52. (11)\nCombining this with (10) gives Lemma 5."}, {"heading": "D Details on testing Unimodality", "text": "Recall that to circumvent Birge\u0301\u2019s decomposition, we want to decompose the interval into disjoint intervals such that the probability of each interval is about O(1/b), where b is a parameter, specified later. In particular we consider a decomposition of [n] with the following properties:\n1. For each element i with probability at least 1/b, there is an I\u2113 = {i}.\n2. There are at most two intervals with p(I) \u2264 1/2b.\n3. Every other interval I satisfies p(I) \u2208 [ 1 2b , 2 b ] .\nLet I1, . . . , IL denote the partition of [n] corresponding to these intervals. Note that L = O(b).\nClaim 1. There is an algorithm that takes O(b log b) samples and outputs I1, . . . , IL satisfying the properties above.\nThe first step in our algorithm is to estimate the total probability within each of these intervals. In particular,\nLemma 10. There is an algorithm that takes m\u2032 = O(b log b/\u03b52) samples from a distribution p, and with probability at least 9/10 outputs a distribution q\u0304 that is constant on each IL. Moreover, for any j such that p(Ij) > 1/2b, q\u0304(Ij) \u2208 (1\u00b1 \u03b5)p(Ij).\nProof. Consider any interval Ij with p(Ij) \u2265 1/2b. The number of samples NIj that fall in that interval is distributed Binomial(m\u2032, p(Ij). Then by Chernoff bounds for m\u2032 > 12b log b/\u03b52,\nPr ( |NIj \u2212m\u2032p(Ij)| > \u03b5m\u2032p(Ij) ) \u22642 exp ( \u03b52m\u2032p(Ij)/2 )\n(12)\n\u2264 1 b2 , (13)\nwhere the last inequality uses the fact that p(Ij) \u2265 1/2b.\nThe next step is estimate the distance of q from Un. This is possible by a simple dynamic program, similar to the one used for monotonicity. If the estimated distance is more than \u03b5/2, we output Reject.\nOur next step is to remove certain intervals. This will be to ensure that when the underlying distribution is unimodal, we are able to estimate the distribution multiplicatively over the remaining intervals. In particular, we do the following preprocessing step:\n\u2022 A = \u2205.\n\u2022 For interval Ij,\n\u2013 If\nq(Ij) /\u2208 ((1\u2212 \u03b5) \u00b7 q(Ij+1), (1 + \u03b5) \u00b7 q(Ij+1)) OR (14) q(Ij) /\u2208 ((1\u2212 \u03b5) \u00b7 q(Ij\u22121), (1 + \u03b5) \u00b7 q(Ij\u22121)) , (15)\nadd Ij to A.\n\u2022 Add the (at most 2) intervals with mass at most 1/2b to A.\n\u2022 Add all intervals j with q(Ij)/|Ij | < \u03b5/50n to A\nIf the distribution is unimodal, we can prove the following about the set of intervals Ac.\nLemma 11. If p is unimodal then,\n\u2022 p(IAc) \u2265 1\u2212 \u03b5/25 \u2212 1/b\u2212O (log n/(\u03b5b)) .\n\u2022 Except at most one interval in Ac every other interval Ij satisfies,\np+j p\u2212j \u2264 (1 + \u03b5).\nIf this holds, then the \u03c72 distance between p and q constrained to Ac, is at most \u03b52. This lemma follows from the following result.\nLemma 12. Let C > 2. For a unimodal distribution over [n], there are at most 4 log(50n/\u03b5)C\u03b5 intervals Ij that satisfy p+j p\u2212j < (1 + \u03b5/C).\nProof. To the contrary, if there are more than 4 log(50n/\u03b5)C\u03b5 intervals, then at least half of them are on one side of the mode, however this implies that the ratio of the largest probability and smallest probability is at least (1 + \u03b5/C)j , and if j > 2 log(50n/\u03b5)C\u03b5 , is at least 50n/\u03b5, contradicting that we have removed all such elements.\nWe have one additional pre-processing step here. We compute q(Ac) and if it is smaller than 1\u2212 \u03b5/25, we output Reject.\nSuppose there are L\u2032 intervals in Ac. Then, except at most one interval in L\u2032 we know that the \u03c72 distance between p and q is at most \u03b52 when p is unimodal, and the TV distance between p and q is at least \u03b5/2 over Ac. We propose the following simple modification to take into account, the one interval that might introduce a high \u03c72 distance in spite of having a small total variation. If we knew the interval, we can simply remove it and proceed. Since we do not know where the interval lies, we do the following.\n1. Let Zj be the \u03c7 2 statistic over the ith interval in Ac, computed with O( \u221a n/\u03b52) samples.\n2. Let Zl be the largest among all Zj\u2019s.\n3. If \u2211 j,j 6=l Zj > m\u03b5 2/10, output Reject.\n4. Output Accept.\nThe objective of removing the largest \u03c72 statistic is our substitute for not knowing the largest interval. We now prove the correctness of this algorithm.\nCase 1 p \u2208 UMn: We only concentrate on the final step. The \u03c72 statistic over all but one interval are at most c \u00b7m\u03b52, and the variance is bounded as before. Since we remove the largest statistic, the expected value of the new statistic is strictly dominated by that of these intervals. Therefore, the algorithm outputs Accept with at least the same probability as if we removed the spurious interval.\nCase 2 p /\u2208 UMn: This is the hard case to prove for unimodal distributions. We know that the \u03c72 statistic is large in this case, and we therefore have to prove that it remains large even after removing the largest test statistic Zl.\nWe invoke Kolmogorov\u2019s Maximal Inequality to this end.\nLemma 13 (Kolmogorov\u2019s Maximal Inequality). For independent zero mean random variables X1, . . . ,XL with finite variance, let S\u2113 = X1 + . . . X\u2113. Then for any \u03bb > 0,\nPr\n(\nmax 1\u2264\u2113\u2264L\n|S\u2113| \u2265 \u03bb ) \u2264 1 \u03bb2 \u00b7 V ar (SL) . (16)\nAs a corollary, it follows that Pr (max\u2113 |X\u2113| > 2\u03bb) \u2264 1\u03bb2 \u00b7 V ar (SL). In the case we are interested in, we let Xi = Z\u2113 \u2212 E [Z\u2113]. Then, similar to the computations before, and the fact that each interval has a small mass, it follows that that the variance of the summation is at most E [Z\u2113] 2 /100. Taking \u03bb = E [ SL \u2212m\u03b52/3 ]2\n/100, it follows that the statistic does not fall below to \u221a n. This completes the proof of Theorem 5."}, {"heading": "E Learning product distributions in \u03c72 distance", "text": "In this section we prove Lemma 7. The proof is analogous to the proof for learning monotone distributions, and hinges on the following result of [KOPS15]. Given m samples from a distribution q over n elements, the add-1 estimator (Laplace estimator) q satisfies:\nE [ \u03c72(p, q) ] \u2264 n m+ 1 .\nNow, suppose p is a product distribution over X = [n1] \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd]. We simply perform the add-1 estimation over each coordinate independently, giving a distribution q1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 qd. Since p is a product distribution the estimates in each coordinate is independent. Therefore, a simple application of the previous result and independence of the coordinates implies\nE [ \u03c72(p, q) ] =\nd \u220f\nl=1\n( 1 + E [ \u03c72(pl, ql) ]) \u2212 1\n\u2264 d \u220f\nl=1\n(\n1 + nl\nm+ 1\n)\n\u2212 1\n\u2264 exp (\u2211 l nl m+ 1 ) \u2212 1, (17)\nwhere (17) follows from ex \u2265 1 + x. Using ex \u2264 1 + 2x for 0 \u2264 x \u2264 1, we have\nE [ \u03c72(p, q) ] \u2264 2 \u2211 l nl m+ 1 , (18)\nwhenm \u2265 \u2211l nl. Therefore, following an application of Markov\u2019s inequality, whenm = \u2126(( \u2211 l nl)/\u03b5 2), Lemma 7 is proved."}, {"heading": "F Details on Testing Log-Concavity", "text": "It will suffice to prove Lemma 8. Proof of Lemma 8: We first draw samples from p and obtain a O(1/\u03b53/2)-piecewise constant distribution f by appropriately flattening the empirical distribution. The proof is now in two parts. In the first part, we show that if p \u2208 LCDn then f will be close to p in \u03c72 distance over its effective support. The second part involves proper learning of p. We will use a linear program on f to find a distribution q \u2208 LCDn. This distribution is such that if p \u2208 LCDn, then \u03c72(p, q) is small, and otherwise the algorithm will either output some q \u2208 LCDn (with no other relevant guarantees) or Reject.\nWe first construct f . Let p\u0302 be the empirical distribution obtained by sampling O(1/\u03b55) samples from p. By Lemma 1, with probability at least 5/6, dK(p, p\u0302) \u2264 \u03b55/2/10. In particular, note that |pi \u2212 p\u0302i| \u2264 \u03b55/2/10. Condition on this event in the remainder of the proof.\nLet a be the minimum i such that pi \u2265 \u03b53/2/5, and let b be the maximum i satisfying the same condition. Let M = {a, . . . , b} or \u2205 if a and b are undefined. By the guarantee provided by the DKW inequality, pi \u2265 \u03b53/2/10 for all i \u2208 M . Furthermore, p\u0302i \u2208 pi \u00b1 \u03b53/2/10 \u2208 (1\u00b1 \u03b5) \u00b7 pi. For each i \u2208 M , let fi = p\u0302i. We note that |M | = O(1/\u03b5), so this contributes O(1/\u03b5) constant pieces to f .\nWe now divide the rest of the domain into t intervals, all but constantly many of measure \u0398(\u03b53/2) (under p). This is done via the following iterative procedure. As a base case, set r0 = 0. Define Ij as [lj , rj ], where lj = rj\u22121+1 and rj is the largest j \u2208 [n] such that p\u0302(Ij) \u2264 9\u03b53/2/10. The exception is if Ij would intersect M \u2013 in this case, we \u201cskip\u201d M : set rj = a\u2212 1 and lj+1 = b+ 1. If such a j exists, denote it by j\u2217. We note that p(Ij) \u2264 p\u0302(Ij) + \u03b55/2/10 \u2264 \u03b53/2. Furthermore, for all j except j\u2217 and t, rj + 1 6\u2208 M , so p(Ij) \u2265 9\u03b53/2/10 \u2212 \u03b53/2/5 \u2212 \u03b55/2/10 \u2265 3\u03b53/2/5. Observe that this lower bound implies that t \u2264 2\n\u03b53/2 for \u03b5 sufficiently small.\nPart 1. For this part of the algorithm, we only care about the guarantees when p \u2208 LCDn, so we assume this is the case.\nFor the domain [n] \\M , we let f be the flattening of p\u0302 over the intervals I1, . . . It. To analyze f , we need a structural property of log-concave distributions due to Chan, Diakonikolas, Servedio, and Sun [CDSS13]. This essentially states that a log-concave distribution cannot have a sudden increase in probability.\nLemma 14 (Lemma 4.1 in [CDSS13]). Let p be a distribution over [n] that is non-decreasing and log-concave on [1, x] \u2286 [n]. Let I = [x, y] be an interval of mass P (I) = \u03c4 , and suppose that the interval J = [1, x\u2212 1] has mass p(J) = \u03c3 > 0. Then\np(y)/p(x) \u2264 1 + \u03c4/\u03c3.\nRecall that any log-concave distribution is unimodal, and suppose the mode of p is at i0. We will first focus on the intervals I1, . . . , ItL which lie entirely to the left of i0 and M . We will refer to Ij as Lj for all j \u2264 tL. Note that p is non-decreasing over these intervals.\nThe next steps to the analysis are as follows. First we show that the flattening of p over Lj is a multiplicative (1 +O(1/j)) estimate for each pi \u2208 Lj. Then, we show that flattening the empirical distribution p\u0302 over Lj is a multiplicative (1 +O(1/j)) estimate of p(i) for each i \u2208 Lj. Finally, we exclude a small number of intervals (those corresponding to O(\u03b5) mass at the left and right side of the domain, as well as j\u2217) in order to get the \u03c72 approximation we desire on an effective support.\n\u2022 First, recall that p(Lj) \u2264 \u03b53/2 for all j. Also, letting Jj = [1, rj\u22121], we have that p(Jj) \u2265 (j \u2212 1) \u00b7 3\u03b53/2/5. Thus by Lemma 14, p(rj) \u2264 p(lj)(1 + 2/(j \u2212 1)). Since the distribution is non-decreasing in Lj , the flattening p\u0304 of p is such that p\u0304(i) \u2208 p(i)(1 \u00b1 2j\u22121) for all i \u2208 Lj .\n\u2022 We have that p(Lj) \u2265 3\u03b53/2/5, and p\u0302(Lj) \u2208 p(Lj) \u00b1 \u03b55/2/10, so p\u0302(Lj) \u2208 p(Lj) \u00b7 (1 \u00b1 \u03b56 ), and hence p\u0302(i) \u2208 p\u0304(i) \u00b7 (1\u00b1 \u03b56) for all i \u2208 Lj. Combining with the previous point, we have that\np\u0302(i) \u2208 p(i) \u00b7 ( 1\u00b1 ( 2\u03b5 3(j \u2212 1) + \u03b5 6 + 2 j \u2212 1 )) \u2208 p(i) \u00b7 ( 1\u00b1 11 3(j \u2212 1) ) .\nA symmetric statement holds for the intervals that lie entirely to the right of i0 and M . We will refer to Ij as Rt\u2212j for all j > tL.\nTo summarize, we have the following guarantees for the distribution f :\n\u2022 For all i \u2208 M , f(i) \u2208 p(i) \u00b7 (1\u00b1 \u03b5);\n\u2022 For all i \u2208 Lj (except L1 and Lj\u2217), f(i) \u2208 p(i) \u00b7 ( 1\u00b1 223j ) ;\n\u2022 For all i \u2208 Rj (except R1), f(i) \u2208 p(i) \u00b7 ( 1\u00b1 223j ) ;\nNote that, in particular, we have multiplicative estimates for all intervals, except those in L1, Lj\u2217 , R1 and the interval containing i0. Let S be the set of all intervals except Lj\u2217, Lj and Rj for j \u2264 1/\u221a\u03b5, and the one containing i0 Then, since each interval has probability mass at most O(\u03b53/2) and we are excluding O(1/ \u221a \u03b5) intervals, p(S) > 1\u2212O(\u03b5).\nWe now compute the \u03c72-distance induced by this approximation for elements in S. For an element i \u2208 Lj \u2229 S, we have\n(f(i)\u2212 p(i))2 p(i) \u2264 60p(i) j2 .\nSumming over all i \u2208 Lj \u2229 S gives 60\u03b53/2\nj2\nsince the probability mass of Lj is at most \u03b5 3/2. Summing this over all Lj for j \u2265 1/ \u221a \u03b5 and j 6= j\u2217 gives\n60\u03b53/2 2/\u03b53/2 \u2211\nj=1/ \u221a \u03b5\n1\nj2 \u2264 60\u03b53/2\n\u222b \u221e\n1/ \u221a \u03b5\n1\nx2 dx\n= 60\u03b53/2( \u221a \u03b5)\n= O(\u03b52)\nas desired.\nPart 2. To obtain a distribution q \u2208 LCDn, we write a linear program. We will work in the log domain, so our variables will be Qi, representing log q(i) for i \u2208 [n]. We will use Fi = log f(i) as parameters in our LP. There will be no objective function, we simply search for a feasible point. Our constraints will be\nQi\u22121 +Qi+1 \u2264 2Qi \u2200i \u2208 [n\u2212 1] Qi \u2264 0 \u2200i \u2208 [n]\nlog(1 + \u03b5) \u2264 |Qi \u2212 Fi| \u2264 log(1 + \u03b5) for i \u2208 M\nlog\n(\n1\u2212 22 3j\n) \u2264 |Qi \u2212 Fi| \u2264 log ( 1 + 22\n3j\n) for i \u2208 Lj , j \u2265 1/ \u221a \u03b5 and j 6= j\u2217\nlog\n(\n1\u2212 22 3j\n) \u2264 |Qi \u2212 Fi| \u2264 log ( 1 + 22\n3j\n) for i \u2208 Rj , j \u2265 1/ \u221a \u03b5\nIf we run the linear program, then after a rescaling and summing the error over all the intervals in the LP gives us that the distance between p and q to be O(\u03b52) \u03c72-distance in a set S which has measure p(S) \u2265 1\u2212 4\u03b5, as desired.\nIf the linear program finds a feasible point, then we obtain a q \u2208 LCDn. Furthermore, if p \u2208 LCDn, this also tells us that (after a rescaling of \u03b5), summing the error over all intervals implies that \u03c72(pS , qS) \u2264 \u03b5 2\n500 for a known set S with p(S) \u2265 1\u2212O(\u03b5), as desired. If M 6= \u2205, this algorithm works as described. The issue is if M = \u2205, then we don\u2019t know when the L intervals end and the R intervals begin. In this case, we run O(1/\u03b5) LPs, using each interval as the one containing i0, and thus acting as the barrier between the L intervals (to its left) and the R intervals (to its right). If p truly was log-concave, then one of these guesses will be correct and the corresponding LP will find a feasible point."}, {"heading": "G Details on MHR testing", "text": "Proof of Lemma 9: As with log-concave distributions, our method for MHR distributions can be split into two parts. In the first step, if p \u2208 MHRn, we obtain a distribution q which is O(\u03b52)-close to p in \u03c72 distance on a set A of intervals such that p(A) \u2265 1\u2212O(\u03b5). q will achieve this by being a multiplicative (1 + O(\u03b5)) approximation for each element within these intervals. This step is very similar to the decomposition used for unimodal distributions (described in Section D), so we sketch the argument and highlight the key differences.\nThe second step will be to find a feasible point in a linear program. If p \u2208 MHRn, there should always be a feasible point, indicating that q is close to a distribution in MHRn (leveraging the particular guarantees for our algorithm for generating q). If dTV(p,MHRn) \u2265 \u03b5, there may or may not be a feasible point, but when there is, it should imply the existence of a distribution p\u2217 \u2208 MHRn such that dTV(q, p\u2217) \u2264 \u03b5/2.\nThe analysis will rely on the following lemma from [CDSS13], which roughly states that an MHR distribution is \u201calmost\u201d non-decreasing.\nLemma 15 (Lemma 5.1 in [CDSS13]). Let p be an MHR distribution over [n]. Let I = [a, b] \u2282 [n] be an interval, and R = [b + 1, n] be the elements to the right of I. Let \u03b7 = p(I)/p(R). Then p(b+ 1) \u2265 11+\u03b7p(a).\nPart 1. As before, with unimodal distributions, we start by taking O( b log b \u03b52\n) samples, with the goal of partitioning the domain into intervals of mass approximately \u0398(1/b). First, we will ignore the left and rightmost intervals of mass \u0398(\u03b5). For all \u201cheavy\u201d elements with mass \u2265 \u0398(1/b), we consider them as singletons. We note that Lemma 15 implies that there will be at most O(1/\u03b5) contiguous intervals of such elements. The rest of the domain is greedily divided (from left to right) into intervals of mass \u0398(1/b), cutting an interval short if we reach one of the heavy elements. This will result in the guarantee that all but potentially O(1/\u03b5) intervals have \u0398(1/b) mass.\nNext, similar to unimodal distributions, considering the flattened distribution, we discard all intervals for which the per-element probability is not within a (1 \u00b1 O(\u03b5)) multiplicative factor of the same value for both neighboring intervals. The claim is that all remaining intervals will have the property that the per-element probability is within a (1\u00b1O(\u03b5)) multiplicative factor of the true probability. This is implied by Lemma 15. If there were a point in an interval which was above this range, the distribution must decrease slowly, and the next interval would have a much larger\nper-element weight, thus leading to the removal of this interval. A similar argument forbids us from missing an interval which contains a point that lies outside this range. Relying on the fact that truncating the left and rightmost intervals eliminates elements with low probability mass, similar to the unimodal case, one can show that we will remove at most log(n/\u03b5)/\u03b5 intervals, and thus a log(n/\u03b5)/b\u03b5 probability mass. Choosing b = \u2126(\u03b52/ log(n/\u03b5)) limits this to be O(\u03b5), as desired. At this point, if p is indeed MHR, the multiplicative estimates guarantee that the result is O(\u03b52)-close in \u03c72-distance among the remaining intervals.\nPart 2. We note that an equivalent condition for distribution f being MHR is log-concavity of log(1 \u2212 F ), where F is the CDF of f . Therefore, our approach for this part will be similar to the approach used for log-concave distributions.\nGiven the output distribution q from the previous part of this algorithm, our goal will be check if there exists an MHR distribution f which is O(\u03b5)-close to q. We will run a linear program with variables fi = log(1 \u2212 Fi). First, we ensure that f is a distribution. This can be done with the following constraints:\nfi \u2264 0 \u2200i \u2208 [n] fi \u2265 fi+1 \u2200i \u2208 [n\u2212 1] fn = \u2212\u221e\nTo ensure that f is MHR, we use the following constraint:\nfi\u22121 + fi+1 \u2264 2fi \u2200i \u2208 [2, n\u2212 1]\nNow, ideally, we would like to ensure f and q are \u03b5-close in total variation distance by ensuring they are pointwise within a multiplicative (1\u00b1 \u03b5) factor of each other:\n(1\u2212 \u03b5) \u2264 fi/qi \u2264 (1 + \u03b5)\nWe note that this is a stronger condition than f and q being \u03b5-close, but if p \u2208 MHRn, the guarantees of the previous step would imply the existence of such an f .\nWe have a separate treatment for the identified singletons (i.e., those with probability \u2265 1/b) and the remainder of the support. For each element qi identified to have \u2265 1/b mass, we add two constraints:\nlog((1 \u2212 \u03b5/2b)(1 \u2212Qi)) \u2264 fi \u2264 log((1 + \u03b5/2b)(1 \u2212Qi)) log((1\u2212 \u03b5/2b)(1 \u2212Qi\u22121)) \u2264 fi\u22121 \u2264 log((1 + \u03b5/2b)(1 \u2212Qi\u22121))\nIf we satisfy these constraints, it implies that\nqi \u2212 \u03b5/b \u2264 fi \u2264 qi + \u03b5/b.\nSince qi \u2265 1/b, this implies (1\u2212 \u03b5)qi \u2264 fi \u2264 (1 + \u03b5)qi\nas desired. Now, the remaining elements each have \u2264 1/b mass. For each such element qi, we create a constraint (1\u2212O(\u03b5)) qi\n1 \u2212Qi\u22121 \u2264 fi\u22121 \u2212 fi \u2264 (1 +O(\u03b5)) qi 1 \u2212Qi\u22121\nNote that the middle term is\n\u2212 log ( 1\u2212 Fi 1\u2212 Fi\u22121 ) = \u2212 log ( 1\u2212 fi 1\u2212 Fi\u22121 ) \u2208 fi 1\u2212 Fi\u22121 (1\u00b1 2\u03b5) ,\nwhere the second equality uses the Taylor expansion and the facts that fi \u2264 1/b and 1\u2212 Fi\u22121 \u2265 \u03b5 (since during the previous part, we ignored the rightmost O(\u03b5) probability mass). If we satisfy the desired constraints, it implies that\nfi \u2208 1 (1\u00b1 2\u03b5) 1\u2212 Fi\u22121 1\u2212Qi\u22121 (1\u2213O(\u03b5))qi.\nSince we are taking \u2126(1/\u03b54) samples and 1 \u2212 Fi\u22121 \u2265 \u2126(\u03b5), Lemma 1 implies that fi is indeed a multiplicative (1\u00b1 \u03b5) approximation for these points as well.\nWe note that all points which do not fall into these two cases make up a total of O(\u03b5) probability mass. Therefore, f may be arbitrary at these points and only incur O(\u03b5) cost in total variation distance.\nIf we find a feasible point for this linear program, it implies the existence of an MHR distribution withinO(\u03b5) total variation distance. In this case, we continue to the testing portion of the algorithm. Furthermore, if p \u2208 MHRn, our method for generating q certifies that such a distribution exists, and we continue on to the testing portion of the algorithm."}, {"heading": "H Details of the Lower Bounds", "text": "In this section, for the class of distributions Q described in discussion on lower bounds and a class of interest C, we show that dTV(C,Q) \u2265 \u03b5, thus implying a lower bound of \u2126( \u221a n/\u03b52) for testing C.\nH.1 Monotone distributions\nWe first consider d = 1 and prove that for appropriately chosen c, any monotone distribution over [n] is \u03b5-far from all distributions in Q. Consider any q \u2208 Q. For this distribution, we say that i \u2208 [n] is a raise-point if qi < qi+1. Let Rq be the set of raise points of q. For q \u2208 Q, (6) implies at least one in every four consecutive integers in [n] is a raise point, and therefore, |Rq| \u2265 n/4. Moreover, note that if i is a raise-point, then i+1 is not a raise point. For any monotone (decreasing) distribution p, pi \u2265 pi+1. For any raise-point i \u2208 Rq, by the triangle inequality,\n|pi \u2212 qi|+ |pi+1 \u2212 qi+1| \u2265 |pi \u2212 pi+1 + qi+1 \u2212 qi| \u2265 qi+1 \u2212 qi = 2c\u03b5\nn . (19)\nSumming over the set Rq, we obtain dTV(p, q) \u2265 12 |Rq| \u00b7 2c\u03b5n \u2265 c\u03b5/4. Therefore, if c \u2265 4, then dTV(Mn, q) \u2265 \u03b5. This proves the lower bound for d = 1.\nThis argument can be extended to [n]d. Consider the following class of distributions on [n]d. For each point i = (i1, . . . , id) \u2208 [n]d, where i1 is even, generate a random z \u2208 {\u22121, 1}, and assign to i a probability of (1 + zc\u03b5)/nd. Let e1 def = (1, 0, . . . , 0). Similar to d = 1, assign a probability (1 \u2212 zc\u03b5)/nd to the point i + e1 = (i1 + 1, i2, . . . , id). This class consists of 2 nd/2\n2 distributions, and Paninski\u2019s arguments extend to give a lower bound of \u2126(nd/2/\u03b52) samples to distinguish this class from the uniform distribution over [n]d. It remains to show that all these distributions are \u03b5 far from Mdn. Call a point i as a raise point if pi < pi+e1 . For any i, one of the points i, i + e1, i+2e1, i+3e1 is a raise point, and the number of raise points is at least n\nd/4. Invoking the triangle inequality (identical to (19)) over the raise-points, in the first dimension shows that any monotone distribution over [n]d is at a distance c\u03b54 from any distribution in this class. Choosing c = 4 yields a bound of \u03b5.\nH.2 Testing Product Distributions\nOur idea for testing independence is similar to the previous section. We sketch the construction of a class of distributions on X = [n1]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd]. Then |X | = n1 \u00b7 n2 . . . \u00b7 nd. For each element in X assign a value (1\u00b1c\u03b5) and then for each such assignment, normalize the values so that they add to 1, giving rise to a distribution. This gives us a class of 2|X | distributions. The key argument is to show that a large fraction of these distributions are far from being a product distribution. This follows since the degrees of freedom of a product distribution is exponentially smaller than the number of possible distributions. The second step is to simply apply Paninski\u2019s argument, now over the larger set of distributions, where we show that distinguishing the collection of distributions we constructed from the uniform distribution over X (which is a product distribution) requires \u221a |X |/\u03b52 samples.\nH.3 Log-concave and Unimodal distributions\nWe will show that any log-concave or unimodal distribution is \u03b5-far from all distributions in Q. Since LCDn \u2282 Un, it will suffice to show this for every unimodal distribution. Consider any unimodal distribution p, with mode \u2113. Then, p is monotone non-decreasing over the interval [\u2113] and non-increasing over {\u2113+1, . . . , n}. By the argument for monotone distributions, the total variation distance between p and any distribution q over elements greater than \u2113 is at least n\u2212\u2113\u22121n c\u03b5 4 , and over elements less than \u2113 is at least \u2113\u22121n c\u03b5 4 . Summing these two gives the desired bound.\nH.4 Monotone Hazard distributions\nWe will show that any monotone hazard rate distribution is \u03b5-far from all distributions in Q. Let p be any monotone-hazard distribution. Any distribution q \u2208 Q has mass at least 1/2 over the interval I = [n/4, 3n/4]. Therefore, by Lemma 15, for any i \u2208 I, pi+1 ( 1 + pi1/4 )\n\u2265 pi. As noted before, at least n/8 of the raise-points are in I.\nFor any i \u2208 I \u2229Rq, qi = (1 + c\u03b5)/n, qi+1 = (1\u2212 c\u03b5)/n\ndi = |pi \u2212 qi|+ |pi+1 \u2212 qi+1|. (20)\nIf pi \u2265 (1 + 2c\u03b5)/n or pi \u2264 1/n, then the first term, and therefore di is at least c\u03b5/n. If pi \u2208 (1/n, (1 + 2c\u03b5)/n), then for n > 5/(c\u03b5)\npi+1 \u2265 1 n \u00b7 1 1 + 4n \u2265 1\u2212 c\u03b5/2 n .\nTherefore the second term of di is at c\u03b5/2n. Since there are at least n/8 raise points in I,\ndTV(p, q) \u2265 1\n2\nn 8 \u00b7 c\u03b5 2n \u2265 c\u03b5 16 . (21)\nThus any MHR distribution is \u03b5-far from Q for c \u2265 16."}], "references": [{"title": "Testing k-wise and almost k-wise independence", "author": ["Noga Alon", "Alexandr Andoni", "Tali Kaufman", "Kevin Matulef", "Ronitt Rubinfeld", "Ning Xie"], "venue": "In Proceedings of STOC,", "citeRegEx": "Alon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2007}, {"title": "Testing monotone continuous distributions on high-dimensional real cubes", "author": ["Michal Adamaszek", "Artur Czumaj", "Christian Sohler"], "venue": "In SODA,", "citeRegEx": "Adamaszek et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Adamaszek et al\\.", "year": 2010}, {"title": "Testing Poisson Binomial Distributions", "author": ["Jayadev Acharya", "Constantinos Daskalakis"], "venue": "In Proceedings of SODA,", "citeRegEx": "Acharya and Daskalakis.,? \\Q2015\\E", "shortCiteRegEx": "Acharya and Daskalakis.", "year": 2015}, {"title": "Competitive classification and closeness testing", "author": ["Jayadev Acharya", "Hirakendu Das", "Ashkan Jafarpour", "Alon Orlitsky", "Shengjun Pan", "Ananda Theertha Suresh"], "venue": "In COLT,", "citeRegEx": "Acharya et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2012}, {"title": "Sample-optimal density estimation in nearly-linear time", "author": ["Jayadev Acharya", "Ilias Diakonikolas", "Jerry Li", "Ludwig Schmidt"], "venue": "arXiv preprint arXiv:1506.00671,", "citeRegEx": "Acharya et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2015}, {"title": "Efficient compression of monotone and m-modal distributions", "author": ["Jayadev Acharya", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh"], "venue": "In Proceedings of the 2014 IEEE International Symposium on Information Theory, ISIT", "citeRegEx": "Acharya et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2014}, {"title": "A competitive test for uniformity of monotone distributions", "author": ["Jayadev Acharya", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Acharya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Acharya et al\\.", "year": 2013}, {"title": "Categorical data analysis", "author": ["Alan Agresti", "Maria Kateri"], "venue": null, "citeRegEx": "Agresti and Kateri.,? \\Q2011\\E", "shortCiteRegEx": "Agresti and Kateri.", "year": 2011}, {"title": "Statistical Inference under Order Restrictions", "author": ["R.E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H.D. Brunk"], "venue": null, "citeRegEx": "Barlow et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Barlow et al\\.", "year": 1972}, {"title": "Testing random variables for independence and identity", "author": ["Tugkan Batu", "Eldar Fischer", "Lance Fortnow", "Ravi Kumar", "Ronitt Rubinfeld", "Patrick White"], "venue": "In Proceedings of FOCS,", "citeRegEx": "Batu et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Batu et al\\.", "year": 2001}, {"title": "Testing monotonicity of distributions over general partial orders", "author": ["Arnab Bhattacharyya", "Eldar Fischer", "Ronitt Rubinfeld", "Paul Valiant"], "venue": "In ICS,", "citeRegEx": "Bhattacharyya et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2011}, {"title": "Estimating a density under order restrictions: Nonasymptotic minimax risk", "author": ["Lucien Birg\u00e9"], "venue": "The Annals of Statistics,", "citeRegEx": "Birg\u00e9.,? \\Q1987\\E", "shortCiteRegEx": "Birg\u00e9.", "year": 1987}, {"title": "Maximum likelihood estimation and confidence bands for a discrete log-concave", "author": ["Fadoua Balabdaoui", "Hanna Jankowski", "Kaspar Rufibach"], "venue": null, "citeRegEx": "Balabdaoui et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balabdaoui et al\\.", "year": 2011}, {"title": "Sublinear algorithms for testing monotone and unimodal distributions", "author": ["Tu\u011fkan Batu", "Ravi Kumar", "Ronitt Rubinfeld"], "venue": "In Proceedings of the 36th Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "Batu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Batu et al\\.", "year": 2004}, {"title": "Estimation of a k-monotone density: characterizations, consistency and minimax lower bounds", "author": ["Fadoua Balabdaoui", "Jon A. Wellner"], "venue": "Statistica Neerlandica,", "citeRegEx": "Balabdaoui and Wellner.,? \\Q2010\\E", "shortCiteRegEx": "Balabdaoui and Wellner.", "year": 2010}, {"title": "A survey on distribution testing: your data is big, but is it blue", "author": ["Cl\u00e9ment L Canonne"], "venue": "In Electronic Colloquium on Computational Complexity (ECCC),", "citeRegEx": "Canonne.,? \\Q2015\\E", "shortCiteRegEx": "Canonne.", "year": 2015}, {"title": "Testing shape restrictions of discrete distributions", "author": ["Clement Canonne", "Ilias Diakonikolas", "Themis Gouleakis", "Ronitt Rubinfeld"], "venue": "In STACS,", "citeRegEx": "Canonne et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Canonne et al\\.", "year": 2015}, {"title": "Learning mixtures of structured distributions over discrete domains", "author": ["Siu On Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "venue": "In Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Chan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2013}, {"title": "Efficient density estimation via piecewise polynomial approximation", "author": ["Siu On Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun"], "venue": "In Proceedings of the 46th Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "Chan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2014}, {"title": "Optimal algorithms for testing closeness of discrete distributions", "author": ["Siu-On Chan", "Ilias Diakonikolas", "Gregory Valiant", "Paul Valiant"], "venue": "In Proceedings of SODA,", "citeRegEx": "Chan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2014}, {"title": "Theoretical properties of the log-concave maximum likelihood estimator of a multidimensional density", "author": ["Madeleine Cule", "Richard Samworth"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Cule and Samworth.,? \\Q2010\\E", "shortCiteRegEx": "Cule and Samworth.", "year": 2010}, {"title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "author": ["A. Dvoretzky", "J. Kiefer", "J. Wolfowitz"], "venue": "The Annals of Mathematical Statistics, 27(3):642\u2013669,", "citeRegEx": "Dvoretzky et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Dvoretzky et al\\.", "year": 1956}, {"title": "Statistical Methods for Research Workers", "author": ["Ronald Aylmer Fisher"], "venue": "Oliver and Boyd, Edinburgh,", "citeRegEx": "Fisher.,? \\Q1925\\E", "shortCiteRegEx": "Fisher.", "year": 1925}, {"title": "The art of uninformed decisions: A primer to property testing", "author": ["Eldar Fischer"], "venue": "Science, 75:97\u2013126,", "citeRegEx": "Fischer.,? \\Q2001\\E", "shortCiteRegEx": "Fischer.", "year": 2001}, {"title": "Combinatorial property testing (a survey)", "author": ["Oded Goldreich"], "venue": "American Mathematical Society,", "citeRegEx": "Goldreich.,? \\Q1998\\E", "shortCiteRegEx": "Goldreich.", "year": 1998}, {"title": "On choosing and bounding probability metrics", "author": ["Alison L. Gibbs", "Francis E. Su"], "venue": "International Statistical Review,", "citeRegEx": "Gibbs and Su.,? \\Q2002\\E", "shortCiteRegEx": "Gibbs and Su.", "year": 2002}, {"title": "Testing for monotone increasing hazard rate", "author": ["Peter Hall", "Ingrid Van Keilegom"], "venue": "Annals of Statistics,", "citeRegEx": "Hall and Keilegom.,? \\Q2005\\E", "shortCiteRegEx": "Hall and Keilegom.", "year": 2005}, {"title": "Estimation of a discrete monotone density", "author": ["Hanna K. Jankowski", "Jon A. Wellner"], "venue": "Electronic Journal of Statistics,", "citeRegEx": "Jankowski and Wellner.,? \\Q2009\\E", "shortCiteRegEx": "Jankowski and Wellner.", "year": 2009}, {"title": "On learning distributions from their samples", "author": ["Sudeep Kamath", "Alon Orlitsky", "Dheeraj Pichapati", "Ananda T. Suresh"], "venue": "In COLT,", "citeRegEx": "Kamath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kamath et al\\.", "year": 2015}, {"title": "Testing statistical hypotheses", "author": ["Erich L Lehmann", "Joseph P Romano"], "venue": "Springer Science & Business Media,", "citeRegEx": "Lehmann and Romano.,? \\Q2006\\E", "shortCiteRegEx": "Lehmann and Romano.", "year": 2006}, {"title": "Testing properties of collections of distributions", "author": ["Reut Levi", "Dana Ron", "Ronitt Rubinfeld"], "venue": "Theory of Computing,", "citeRegEx": "Levi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Levi et al\\.", "year": 2013}, {"title": "The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality", "author": ["Pascal Massart"], "venue": "The Annals of Probability, 18(3):1269\u20131283,", "citeRegEx": "Massart.,? \\Q1990\\E", "shortCiteRegEx": "Massart.", "year": 1990}, {"title": "A coincidence-based test for uniformity given very sparsely sampled discrete data", "author": ["Liam Paninski"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Paninski.,? \\Q2008\\E", "shortCiteRegEx": "Paninski.", "year": 2008}, {"title": "Property testing: A learning theory perspective", "author": ["Dana Ron"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Ron.,? \\Q2008\\E", "shortCiteRegEx": "Ron.", "year": 2008}, {"title": "The analysis of categorical data from complex sample surveys: chi-squared tests for goodness of fit and independence in two-way tables", "author": ["Jon NK Rao", "Alastair J Scott"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rao and Scott.,? \\Q1981\\E", "shortCiteRegEx": "Rao and Scott.", "year": 1981}, {"title": "Log-concavity and strong log-concavity: a review", "author": ["Adrien Saumard", "Jon AWellner"], "venue": "Statistics Surveys,", "citeRegEx": "Saumard and AWellner.,? \\Q2014\\E", "shortCiteRegEx": "Saumard and AWellner.", "year": 2014}, {"title": "Estimating the unseen: An n/ log n-sample estimator for entropy and support size, shown optimal via new CLTs", "author": ["Gregory Valiant", "Paul Valiant"], "venue": "In Proceedings of the 43rd Annual ACM Symposium on the Theory of Computing,", "citeRegEx": "Valiant and Valiant.,? \\Q2011\\E", "shortCiteRegEx": "Valiant and Valiant.", "year": 2011}, {"title": "An automatic inequality prover and instance optimal identity testing", "author": ["Gregory Valiant", "Paul Valiant"], "venue": "In FOCS,", "citeRegEx": "Valiant and Valiant.,? \\Q2014\\E", "shortCiteRegEx": "Valiant and Valiant.", "year": 2014}, {"title": "A, the last equality uses (1), and the final inequality substitutes a value m", "author": [], "venue": null, "citeRegEx": "\u2208,? \\Q2000\\E", "shortCiteRegEx": "\u2208", "year": 2000}, {"title": "When dTV(p, C) \u2265 \u03b5, Lemma 2 and m", "author": ["m\u03b5"], "venue": null, "citeRegEx": "m\u03b5.,? \\Q2000\\E", "shortCiteRegEx": "m\u03b5.", "year": 2000}], "referenceMentions": [], "year": 2015, "abstractText": "Given samples from an unknown distribution p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, and more recently in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of distributions such as monotonicity, log-concavity, unimodality, independence, and monotone-hazard rate, the optimal sample complexity is unknown. We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution p, and a known distribution q, are p and q close in \u03c7-distance, or far in total variation distance? With this tool in place, we develop a general testing framework which leads to the following results: \u2022 Testing identity to any distribution over [n] requires \u0398(\u221an/\u03b52) samples. This is optimal for the uniform distribution. This gives an alternate argument for the minimax sample complexity of testing identity (proved in [VV14]). \u2022 For all d \u2265 1 and n sufficiently large, testing whether a discrete distribution over [n] is monotone requires an optimal \u0398(n/\u03b5) samples. The single-dimensional version of our theorem improves a long line of research starting with [BKR04], where the previous best tester required \u03a9( \u221a n log(n)/\u03b5) samples, while the high-dimensional version improves [BFRV11], which requires \u03a9\u0303(n 1 2poly(1\u03b5 )) samples. \u2022 For all d \u2265 1, testing whether a collection of random variables over [n1]\u00d7\u00b7 \u00b7 \u00b7\u00d7[nd] are independent requiresO ((", "creator": "LaTeX with hyperref package"}}}