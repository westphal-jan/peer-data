{"id": "1507.08788", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jul-2015", "title": "Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and Convexity", "abstract": "we study the convergence properties of precisely the stable vr - filtered pca algorithm introduced by \\ cite { shamir2015stochastic } formulated for reasonably fast computation of constant leading and singular point vectors. we then prove several new results, ideally including a formal flow analysis solution of a simpler block dwelling version of the ras algorithm, duration and convergence probability from sequential random basis initialization. we also make note a handful few observations solely of independent interest, many such as testing how pre - initializing parameters with just a rare single exact low power iteration can significantly improve the runtime of stochastic methods, derive and investigate what are the relative convexity and explicit non - convexity properties of being the underlying dynamic optimization problem.", "histories": [["v1", "Fri, 31 Jul 2015 07:57:18 GMT  (362kb,D)", "http://arxiv.org/abs/1507.08788v1", "35 pages, 2 figures"]], "COMMENTS": "35 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NA math.NA math.OC stat.ML", "authors": ["ohad shamir"], "accepted": true, "id": "1507.08788"}, "pdf": {"name": "1507.08788.pdf", "metadata": {"source": "CRF", "title": "Fast Stochastic Algorithms for SVD and PCA: Convergence Properties and Convexity", "authors": ["Ohad Shamir"], "emails": ["ohad.shamir@weizmann.ac.il"], "sections": [{"heading": "1 Introduction", "text": "We consider the problem of recovering the top k left singular vectors of a d\u00d7 n matrix X = (x1, . . . ,xn), where k d. This is equivalent to recovering the top k eigenvectors of XX>, or equivalently, solving the optimization problem\nmin W\u2208Rd\u00d7k:W>W=I\n\u2212W> ( 1\nn n\u2211 i=1 xix > i\n) W. (1)\nThis is one of the most fundamental matrix computation problems, and has numerous uses (such as low-rank matrix approximation and principal component analysis).\nFor large-scale matrices X , where exact eigendecomposition is infeasible, standard deterministic approaches are based on power iterations or variants thereof (e.g. the Lanczos method) [8]. Alternatively, one can exploit the structure of Eq. (1) and apply stochastic iterative algorithms, where in each iteration we update a current d \u00d7 k matrix W based on one or more randomly-drawn columns xi of X . Such algorithms have been known for several decades ([14, 17]), and enjoyed renewed interest in recent years, e.g. [2, 4, 3, 10, 6]. Another stochastic approach is based on random projections, e.g. [9, 20].\nUnfortunately, each of these algorithms suffer from a different disadvantage: The deterministic algorithms are accurate (runtime logarithmic in the required accuracy , under an eigengap condition), but require a full pass over the matrix for each iteration, and in the worst-case many such passes would be required (polynomial in the eigengap). On the other hand, each iteration of the stochastic algorithms is cheap, and their number is independent of the size of the matrix, but on the flip side, their noisy stochastic nature means they are not suitable for obtaining a high-accuracy solution (the runtime scales polynomially with ).\nRecently, [19] proposed a new practical algorithm, VR-PCA, for solving Eq. (1), which has a \u201cbest-ofboth-worlds\u201d property: The algorithm is based on cheap stochastic iterations, yet the algorithm\u2019s runtime is logarithmic in the required accuracy . More precisely, for the case k = 1, xi of bounded norm, and when\nar X\niv :1\n50 7.\n08 78\n8v 1\n[ cs\n.L G\n] 3\n1 Ju\nl 2 01\nthere is an eigengap of \u03bb between the first and second leading eigenvalues of the covariance matrix 1nXX >, the required runtime was shown to be on the order of\nd ( n+ 1\n\u03bb2\n) log ( 1 ) . (2)\nThe algorithm is therefore suitable for obtaining high accuracy solutions (the dependence on is logarithmic), but essentially at the cost of onlyO(log(1/ )) passes over the data. The algorithm is based on a recent variance-reduction technique designed to speed up stochastic algorithms for convex optimization problems ([13]), although the optimization problem in Eq. (1) is inherently non-convex. See Section 3 for a more detailed description of this algorithm, and [19] for more discussions as well as empirical results.\nThe results and analysis in [19] left several issues open. For example, it is not clear if the quadratic dependence on 1/\u03bb in Eq. (2) is necessary, since it is worse than the linear (or better) dependence that can be obtained with the deterministic algorithms mentioned earlier, as well as analogous results that can be obtained with similar techniques for convex optimization problems (where \u03bb is the strong convexity parameter). Also, the analysis was only shown for the case k = 1, whereas often in practice, we may want to recover k > 1 singular vectors simultaneously. Although [19] proposed a variant of the algorithm for that case, and studied it empirically, no analysis was provided. Finally, the convergence guarantee assumed that the algorithm is initialized from a point closer to the optimum than what is attained with standard random initialization. Although one can use some other, existing stochastic algorithm to do this \u201cwarm-start\u201d, no end-to-end analysis of the algorithm, starting from random initialization, was provided.\nIn this paper, we study these and related questions, and make the following contributions:\n\u2022 We propose a variant of VR-PCA to handle the k > 1 case, and formally analyze its convergence (Section 3). The extension to k > 1 is non-trivial, and requires tracking the evolution of the subspace spanned by the current solution at each iteration.\n\u2022 In Section 4, we study the convergence of VR-PCA starting from a random initialization. And show that with a slightly smarter initialization \u2013 essentially, random initialization followed by a single power iteration \u2013 the convergence results can be substantially improved. In fact, a similar initialization scheme should assist in the convergence of other stochastic algorithms for this problem, as long as a single power iteration can be performed.\n\u2022 In Section 5, we study whether functions similar to Eq. (1) have hidden convexity properties, which would allow applying existing convex optimization tools as-is, and improve the required runtime. For the k = 1 case, we show that this is in fact true: Close enough to the optimum, and on a suitablydesigned convex set, such a function is indeed \u03bb-strongly convex. Unfortunately, the distance from the optimum has to be O(\u03bb), and this precludes a better runtime in most practical regimes. However, it still indicates that a better runtime and dependence on \u03bb should be possible."}, {"heading": "2 Some Preliminaries and Notation", "text": "We consider a d\u00d7 n matrix X composed of n columns (x1, . . . ,xn), and let\nA = 1\nn XX> =\n1\nn n\u2211 i=1 xix > i .\nThus, Eq. (1) is equivalent to finding the k leading eigenvectors of A.\nWe generally use bold-face letters to denote vectors, and capital letters to denote matrices. We let Tr(\u00b7) denote the trace of a matrix, \u2016 \u00b7 \u2016F to denote the Frobenius norm, and \u2016 \u00b7 \u2016sp to denote the spectral norm. A symmetric d \u00d7 d matrix B is positive semidefinite, if infz\u2208Rd z>Bz \u2265 0. A is positive definite if the inequality is strict. Following standard notation, we write B 0 to denote that A is positive semidefinite, and B C if B \u2212 C 0. B 0 means that B is positive definite.\nA twice-differentiable function F on a subset of Rd is convex, if its Hessian is alway positive semidefinite. If it is always positive definite, and \u03bbI for some \u03bb > 0, we say that the function is \u03bb-strongly convex. If the Hessian is always \u227a sI for some s \u2265 0, then the function is s-smooth."}, {"heading": "3 The VR-PCA Algorithm and a Block Version", "text": "We begin by recalling the algorithm of [19] for the k = 1 case (Algorithm 1), and then discuss its generalization for k > 1.\nAlgorithm 1 VR-PCA: Vector version (k = 1) 1: Parameters: Step size \u03b7, epoch length m 2: Input: Data matrix X = (x1, . . . ,xn); Initial unit vector w\u03030 3: for s = 1, 2, . . . do 4: u\u0303 = 1n \u2211n i=1 xi ( x>i w\u0303s\u22121\n) 5: w0 = w\u0303s\u22121 6: for t = 1, 2, . . . ,m do 7: Pick it \u2208 {1, . . . , n} uniformly at random 8: w\u2032t = wt\u22121 + \u03b7 ( xit ( x>itwt\u22121 \u2212 x > it w\u0303s\u22121 ) + u\u0303\n) 9: wt =\n1 \u2016w\u2032t\u2016 w\u2032t 10: end for 11: w\u0303s = wm 12: end for\nThe basic idea of the algorithm is to perform stochastic updates using randomly-sampled columns xi of the matrix, but interlace them with occasional exact power iterations, and use that to gradually reduce the variance of the stochastic updates. Specifically, the algorithm is split into epochs s = 1, 2, . . ., where in each epoch we do a single exact power iteration with respect to the matrix A (by computing u\u0303), and then perform m stochastic updates, which can be re-written as\nw\u2032t = (I + \u03b7A)wt\u22121 + \u03b7 ( xitx > it \u2212A ) (wt\u22121 \u2212 w\u0303s\u22121) , wt = 1\n\u2016w\u2032t\u2016 wt,\nThe first term is essentially a power iteration (with a finite step size \u03b7), whereas the second term is zeromean, and with variance dominated by \u2016wt\u22121 \u2212 w\u0303s\u22121\u20162. As the algorithm progresses, wt\u22121 and w\u0303s\u22121 both converge toward the same optimal point, hence \u2016wt\u22121 \u2212 w\u0303s\u22121\u20162 shrinks, eventually leading to an exponential convergence rate.\nTo handle the k > 1 case (where more than one eigenvector should be recovered), one simple technique is deflation, where we recover the leading eigenvectors v1,v2, . . . ,vk one-by-one, each time using the k = 1 algorithm. However, a disadvantage of this approach is that it requires a positive eigengap between all top k eigenvalues, otherwise the algorithm is not guaranteed to converge. Thus, an algorithm which simultaneously recovers all k leading eigenvectors is preferable.\nWe will study a block version of Algorithm 1, presented as Algorithm 2. It is mostly a straightforward generalization (similar to how power iterations are generalized to orthogonal iterations), where the d-dimensional vectors wt\u22121, w\u0303s\u22121,u are replaced by d\u00d7 k matrices Wt\u22121, W\u0303s\u22121, U\u0303 , and normalization is replaced by orthogonalization1. Indeed, Algorithm 1 is equivalent to Algorithm 2 when k = 1. The main twist in Algorithm 2 is that instead of using W\u0303s\u22121, U\u0303 as-is, we perform a unitary transformation (via the k \u00d7 k orthogonal matrix Bt\u22121) which maximally aligns them with Wt\u22121. Note that Bt\u22121 is a k \u00d7 k matrix, and since k is assumed to be small, this does not introduce significant computational overhead.\nAlgorithm 2 VR-PCA: Block version Parameters: Rank k, Step size \u03b7, epoch length m Input: Data matrix X = (x1, . . . ,xn); Initial d \u00d7 k matrix W\u03030 with orthonormal columns for s = 1, 2, . . . do U\u0303 = 1n \u2211n i=1 xi ( x>i W\u0303s\u22121\n) W0 = W\u0303s\u22121 for t = 1, 2, . . . ,m do Bt\u22121 = V U\n>, where USV > is an SVD decomposition of W>t\u22121W\u0303s\u22121 B Equivalent to Bt\u22121 = arg minB>B=I \u2016Wt\u22121 \u2212 W\u0303s\u22121B\u20162F\nPick it \u2208 {1, . . . , n} uniformly at random W \u2032t = Wt\u22121 + \u03b7 ( xit ( x>itWt\u22121 \u2212 x > it W\u0303s\u22121Bt\u22121 ) + U\u0303Bt\u22121 ) Wt = W \u2032 t ( W \u2032> t W \u2032 t\n)\u22121/2 end for W\u0303s = Wm\nend for\nWe now turn to provide a formal analysis of Algorithm 2, which directly generalizes the analysis of Algorithm 1 given in [19]:\nTheorem 1. Define the d \u00d7 d matrix A as 1nXX > = 1n \u2211n i=1 xix > i , and let Vk denote the d \u00d7 k matrix composed of the eigenvectors corresponding to the largest k eigenvalues. Suppose that\n\u2022 maxi \u2016xi\u20162 \u2264 r for some r > 0.\n\u2022 A has eigenvalues s1 > s2 \u2265 . . . \u2265 sd, where sk \u2212 sk+1 = \u03bb for some \u03bb > 0.\n\u2022 k \u2212 \u2016V >k W\u03030\u20162F \u2264 1 2 .\nLet \u03b4, \u2208 (0, 1) be fixed. If we run the algorithm with any epoch length parameter m and step size \u03b7, such that\n\u03b7 \u2264 c\u03b4 2 r2 \u03bb , m \u2265 c \u2032 log(2/\u03b4) \u03b7\u03bb , km\u03b72r2 + rk\n\u221a m\u03b72 log(2/\u03b4) \u2264 c\u2032\u2032 (3)\n1The normalization Wt = W \u2032t ( W \u2032> t W \u2032 t )\u22121/2 ensures that Wt has orthonormal columns. We note that in our analysis, \u03b7 is\nchosen sufficiently small so that W \u2032> t W \u2032 t is always invertible, hence the operation is well-defined.\n(where c, c\u2032, c\u2032\u2032 designate certain positive numerical constants), and for T = \u2308 log(1/ ) log(2/\u03b4) \u2309 epochs, then with probability at least 1\u2212 dlog2(1/ )e\u03b4, it holds that\nk \u2212 \u2016V >k W\u0303T \u20162F \u2264 .\nFor any orthogonal W , k \u2212 \u2016V >k W\u20162F lies between 0 and k, and equals 0 when the column spaces of Vk and W are the same (i.e., when W spans the k leading singular vectors). According to the theorem, taking appropriate2 \u03b7 = \u0398(\u03bb/(kr)2), and m = \u0398((rk/\u03bb)2), the algorithm converges with high probability to a high-accuracy approximation of Vk. Moreover, the runtime of each epoch of the algorithm equals O(mdk2 + dnk). Overall, we get the following corollary:\nCorollary 1. Under the conditions of Theorem 1, there exists an algorithm returning W\u0303T such that k \u2212 \u2016V >k W\u0303T \u20162F \u2264 with arbitrary constant accuracy, in runtime O ( dk(n+ r 2k3 \u03bb2 ) log(1/ ) ) .\nThis runtime bound is the same3 as that of [19] for k = 1. The proof of Theorem 1 appears in Subsection 6.1, and relies on a careful tracking of the evolution of the potential function k \u2212 \u2016V >k W\u0303t\u20162F . An important challenge compared to the k = 1 case is that the matrices Wt\u22121 and W\u0303s\u22121 do not necessarily become closer over time, so the variance-reduction intuition discussed earlier no longer applies. However, the column space of Wt\u22121 and W\u0303s\u22121 do become closer, and this is utilized by introducing the transformation matrix Bt\u22121. We note that although Bt\u22121 appears essential for our analysis, it isn\u2019t clear that using it is necessary in practice: In [19], the suggested block algorithm was Algorithm 2 with Bt\u22121 = I , which seemed to work well in experiments. In any case, using this matrix doesn\u2019t affect the overall runtime beyond constants, since the additional runtime of computing and using this matrix (O(dk2)) is the same as the other computations performed at each iteration.\nA limitation of the theorem above is the assumption that the initial point W\u03030 is such that k\u2212\u2016V >k W\u03030\u20162F \u2264 1 2 . This is a non-trivial assumption, since if we initialize the algorithm from a random d \u00d7 O(1) orthogonal matrix W\u03030, then with overwhelming probability, \u2016V >k W\u03030\u20162F = O(1/d). However, experimentally the algorithm seems to work well even with random initialization [19]. Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W\u03030. The idea is that W\u03030 is only required to approximate Vk up to constant accuracy, so purely stochastic algorithms (which are good in obtaining a low-accuracy solution) are quite suitable. In the next section, we further delve into these issues, and show that in our setting such algorithms in fact can be substantially improved."}, {"heading": "4 Warm-Start and the Power of a Power Iteration", "text": "In this section, we study the runtime required to compute a starting point satisfying the conditions of Theorem 1, starting from a random initialization. Combined with Theorem 1, this gives us an end-to-end analysis of the runtime required to find an -accurate solution, starting from a random point. For simplicity, we will only discuss the case k = 1, i.e. where our goal is to compute the single leading eigenvector v1, although\n2Specifically, we can take m = c\u2032 log(2/\u03b4)/\u03b7\u03bb and \u03b7 = a\u03b42/r2\u03bb, where a is sufficiently small to ensure that the first and third condition in Eq. (3) holds. It can be verified that it\u2019s enough to take a = min { c, c \u2032\u2032\n4\u03b42ck log(2/\u03b4) , 1 4\u03b42c\n( c\u2032\u2032\nk log(2/\u03b4)\n)2} .\n3[19] showed that it\u2019s possible to further improve the runtime for sparse X , replacing d by the average column sparsity ds. This is done by maintaining parameters in an implicit form, but it\u2019s not clear how to implement a similar trick in the block version, where k > 1.\nour observations can be generalized to k > 1. In the k = 1 case, Theorem 1 kicks in once we find a vector w satisfying \u3008v1,w\u30092 \u2265 12 .\nAs mentioned previously, one way to get such a w is to run a purely stochastic algorithm, which computes the leading eigenvector of a covariance matrix E[xx>] given a stream of i.i.d. samples x. We can easily use such an algorithm in our setting, by sampling columns from our matrix X = (x1, . . . ,xn) uniformly at random, and feed to such a stochastic optimization algorithm, guaranteed to approximate the leading eigenvector of 1n \u2211n i=1 xix > i .\nTo the best of our knowledge, the existing iteration complexity guarantees for such algorithms (assuming the norm constraint r \u2264 1 for simplicity) scale at least4 as d/\u03bb2. Since the runtime of each iteration isO(d), we get an overall runtime of O((d/\u03bb)2).\nThe dependence on d in the iteration bound stems from the fact that with a random initial unit vector w0, we have \u3008v1,w0\u30092 \u2248 1d . Thus, we begin with a vector almost orthogonal to the leading eigenvector v1 (depending on d). In a purely stochastic setting, where only noisy information is available, this necessitates conservative updates at first, and in all the analyses we are aware of, the number of iterations appear to necessarily scale at least linearly with d.\nHowever, it turns out that in our setting, with a finite matrix X , we can perform a smarter initialization: Sample w from the standard Gaussian distribution on Rd, perform a single power iteration w.r.t. the covariance matrix A = 1nXX\n>, i.e. w0 = Aw/\u2016Aw\u2016, and initialize from w0. For such a procedure, we have the following simple observation:\nLemma 1. For w0 as above, it holds for any \u03b4 that with probability at least 1\u2212 1d \u2212 \u03b4,\n\u3008v1,w0\u30092 \u2265 \u03b42\n12 log(d) nrank(A) ,\nwhere nrank(A) = \u2016A\u2016 2 F\n\u2016A\u20162sp is the numerical rank of A.\nThe numerical rank (see e.g. [18]) is a relaxation of the standard notion of rank: For any d \u00d7 d matrix A, nrank(A) is at most the rank of A (which in turn is at most d). However, it will be small even if A is just close to being low-rank. In many if not most machine learning applications, we are interested in matrices which tend to be approximately low-rank, in which case nrank(A) is much smaller than d or even a constant. Therefore, by a single power iteration, we get an initial point w0 for which \u3008v1,w0\u30092 is on the order of 1/nrank(A), which can be much larger than the 1/d given by a random initialization, and is never substantially worse.\nProof of Lemma 1. Let s1 \u2265 s2 \u2265 . . . \u2265 sd \u2265 0 be the d eigenvalues of A, with eigenvectors v1, . . . ,vd. We have\n\u3008v1,w0\u30092 = \u3008v1, Aw\u30092\n\u2016Aw\u20162 = (s1\u3008v1,w\u3009)2(\u2211d i=1 sivi\u3008vi,w\u3009 )2 = s21\u3008v1,w\u30092\u2211d i=1 s 2 i \u3008vi,w\u30092 .\nSince w is distributed according to a standard Gaussian distribution, which is rotationally symmetric, we can assume without loss of generality that v1, . . . ,vd correspond to the standard basis vectors e1, . . . , ed, in which case the above reduces to\ns21w 2 1\u2211d\ni=1 s 2 iw 2 i\n\u2265 s 2 1\u2211d\ni=1 s 2 i w21 maxiw2i ,\n4For example, this holds for [6], although the bound only guarantees the existence of some iteration which produces the desired output. The guarantee of [4] scale as d2/\u03bb2, and the guarantee of [10] scales as d/\u03bb3 in our setting.\nwhere w1, . . . , wd are independent and scalar random variables with a standard Gaussian distribution. First, we note that s21 equals \u2016A\u20162sp, the spectral norm of A, whereas \u2211d i=1 s 2 i equals \u2016A\u20162F , the Frobenius norm of A. Therefore, s 2 1\u2211 i s 2 i = \u2016A\u20162sp \u2016A\u20162F = 1nrank(A) , and we get overall that\n\u3008v1,w0\u30092 \u2265 1 nrank(A) w21 maxiw2i . (4)\nWe consider the random quantity w21/maxiw 2 i , and independently bound the deviation probability of\nthe numerator and denominator. First, for any t \u2265 0 we have\nPr(w21 \u2264 t) = Pr(w1 \u2208 [\u2212 \u221a t, \u221a t]) = \u222b \u221at z=\u2212 \u221a t \u221a 1 2\u03c0 exp ( \u2212z 2 2 ) \u2264 \u221a 1 2\u03c0 \u2217 2 \u221a t = \u221a 2 \u03c0 t . (5)\nSecond, by combining two standard Gaussian concentration results (namely, that ifW = max{|w1|, . . . , |wd|}, then 0 \u2264 E[W ] \u2264 2 \u221a 2 log(d), and by the Cirelson-Ibragimov-Sudakov inequality, Pr(W \u2212 E[W ] > t) \u2264 exp(\u2212t2/2)), we get that\nPr(max i |wi| > 2\n\u221a 2 log(d) + t) \u2264 exp(\u2212t2/2),\nand therefore Pr(max\ni w2i > (2\n\u221a 2 log(d) + t)2) \u2264 exp(\u2212t/2). (6)\nCombining Eq. (5) and Eq. (6), with a union bound, we get that for any t1, t2 \u2265 0, it holds with probability at least 1\u2212 \u221a 2 \u03c0 t1 \u2212 exp(\u2212t 2 2/2) that\nw21 maxiw2i \u2265 t1 (2 \u221a 2 log(d) + t2)2 .\nTo slightly simplify this for readability, we take t2 = \u221a 2 log(d), and substitute \u03b4 = \u221a 2 \u03c0 t1. This implies that with probability at least 1\u2212 \u03b4 \u2212 1/d,\nw21 maxiw2i \u2265 \u03c0 2 \u03b4 2 18 log(d) >\n\u03b42\n12 log(d) .\nPlugging back into Eq. (4), the result follows.\nThis result can be plugged into the existing analyses of purely stochastic PCA/SVD algorithms, and can often improve the dependence on the d factor in the iteration complexity bounds to a dependence on the numerical rank of A. We again emphasize that this is applicable in a situation where we can actually perform a power iteration, and not in a purely stochastic setting where we only have access to an i.i.d. data stream (nevertheless, it would be interesting to explore whether this idea can be utilized in such a streaming setting as well).\nTo give a concrete example of this, we provide a convergence analysis of the VR-PCA algorithm (Algorithm 1), starting from an arbitrary initial point, bounding the total number of stochastic iterations required by the algorithm in order to produce a point satisfying the conditions of Theorem 1 (from which point the analysis of Theorem 1 takes over). Combined with Theorem 1, this analysis also justifies that VR-PCA indeed converges starting from a random initialization.\nTheorem 2. Using the notation of Theorem 1 (where \u03bb is the eigengap, v1 is the leading eigenvector, and r = maxi \u2016xi\u20162), and for any \u03b4 \u2208 (0, 12), suppose we run Algorithm 1 with some initial unit-norm vector w\u03030 such that\n\u3008v1, w\u03030\u30092 \u2265 \u03b6 > 0,\nand a step size \u03b7 satisfying\n\u03b7 \u2264 c\u03b4 2\u03bb\u03b63\nr2 log2(2/\u03b4) (7)\n(for some universal constant c). Then with probability at least 1\u2212 \u03b4, after\nT =\n\u230a c\u2032 log(2/\u03b4)\n\u03b7\u03bb\u03b6 \u230b stochastic iterations (lines 6\u2212 10 in the pseudocode, where c\u2032 is again a universal constant), we get a point wT satisfying 1\u2212\u3008v1,wT \u30092 \u2264 12 . Moreover, if \u03b7 is chosen on the same order as the upper bound in Eq. (7), then\nT = \u0398\n( r2 log3(2/\u03b4)\n\u03b42\u03bb2\u03b64\n) .\nNote that the analysis does not depend on the choice of the epoch size m, and does not use the special structure of VR-PCA (in fact, the technique we use is applicable to any algorithm which takes stochastic gradient steps to solve this type of problem5). The proof of the theorem appears in Section 6.2.\nConsidering \u03b4, r as a constants, we get that the runtime required by VR-PCA to find a point w such that 1 \u2212 \u3008v1,wT \u30092 \u2264 12 is O(d/\u03bb\n2\u03b64) where \u03b6 is a lower bound on \u3008v1, w\u03030\u30092. As discussed earlier, if w\u03030 is a result of random initialization followed by a power iteration (requiring O(nd) time), and the covariance matrix A has small numerical rank, then \u03b6 = \u3008v1, w\u03030\u30092 = \u2126\u0303(1/ log(d)), and the runtime is\nO ( nd+ d\n\u03bb2 log4(d)\n) = O ( d ( n+ ( log2(d)\n\u03bb\n)2)) .\nBy Corollary 1, the runtime required by VR-PCA from that point to get an -accurate solution is O ( d ( n+ 1\n\u03bb2\n) log ( 1 )) ,\nso the sum of the two expressions (which is d ( n+ 1\n\u03bb2\n) up to log-factors), represents the total runtime\nrequired by the algorithm. Finally, we note that this bound holds under the reasonable assumption that the numeric rank of A is constant. If this assumption doesn\u2019t hold, \u03b6 can be as large as d, and the resulting bound will have a worse polynomial dependence on d. We suspect that this is due to a looseness in the dependence on \u03b6 = \u3008v1, w\u03030\u30092 in Theorem 2, since better dependencies can be obtained, at least for slightly different algorithmic approaches (e.g. [4, 10, 6]). We leave a sharpening of the bound w.r.t. \u03b6 as an open problem.\n5Although there exist previous analyses of such algorithms in the literature, they unfortunately do not quite apply to our algorithm, for various technical reasons."}, {"heading": "5 Convexity and Non-Convexity of the Rayleigh Quotient", "text": "As mentioned in the introduction, an intriguing open question is whether the d ( n+ 1\n\u03bb2\n) log ( 1 ) runtime\nguarantees from the previous sections can be further improved. Although a linear dependence on d, n seems unavoidable, this is not the case for the quadratic dependence on 1/\u03bb. Indeed, when using deterministic methods such as power iterations or the Lanczos method, the dependence on \u03bb in the runtime is only 1/\u03bb or even \u221a 1/\u03bb [15]. In the world of convex optimization from which our algorithmic techniques are derived, the analog of \u03bb is the strong convexity parameter of the function, and again, it is possible to get a dependence of 1/\u03bb, or even \u221a 1/\u03bb with accelerated schemes (see e.g. [13, 16, 7] in the context of the variance-reduction technique we use). Is it possible to get such a dependence for our problem as well? Another question is whether the non-convex problem that we are tackling (Eq. (1)) is really that nonconvex. Clearly, it has a nice structure (since we can solve the problem in polynomial time), but perhaps it actually has hidden convexity properties, at least close enough to the optimal points? We note that Eq. (1) can be \u201ctrivially\u201d convexified, by re-casting it as an equivalent semidefinite program [5]. However, that would require optimization over d \u00d7 d matrices, leading to poor runtime and memory requirements. The question here is whether we have any convexity with respect to the original optimization problem over \u201cthin\u201d d\u00d7 k matrices.\nIn fact, the two questions of improved runtime and convexity are closely related: If we can show that the optimization problem is convex in some domain containing an optimal point, then we may be able to use fast stochastic algorithms designed for convex optimization problems, inheriting their good guarantees.\nTo discuss these questions, we will focus on the k = 1 case for simplicity (i.e., our goal is to find a leading eigenvector of the matrix A = 1nXX > = 1n \u2211n i=1 xix > i ), and study potential convexity properties of the negative Rayleigh quotient,\nFA(w) = \u2212 w>Aw\n\u2016w\u20162 =\n1\nn n\u2211 i=1 ( \u2212\u3008w,xi\u3009 2 \u2016w\u20162 ) .\nNote that for k = 1, this function coincides with Eq. (1) on the unit Euclidean sphere, and with the same optimal points, but has the nice property of being defined on the entire Euclidean space (thus, at least its domain is convex).\nAt a first glance, such functions FA appear to potentially be convex at some bounded distance from an optimum, as illustrated for instance in the case where A = (\n1 0 0 0\n) (see Figure 1). Unfortunately, it turns\nout that the figure is misleading, and in fact the function is not convex almost everywhere:\nTheorem 3. For the matrixA above, the Hessian of FA is not positive semidefinite for all but a measure-zero set.\nProof. The leading eigenvector of A is v1 = (1, 0), and FA(w) = \u2212 w21\nw21+w 2 2 . The Hessian of this function at some w equals\n2\n(w21 + w 2 2) 3\n( w22(3w 2 1 \u2212 w22) \u22122w1w2(w21 \u2212 w22)\n\u22122w1w2(w21 \u2212 w22) w21(w21 \u2212 3w22)\n) .\nw21+w\n2 2 ,\ncorresponding to FA(w) where A = (1 0 ; 0 0). It is invariant to re-scaling of w, and attains a minimum at (a, 0) for any a 6= 0.\nThe determinant of this 2\u00d7 2 matrix equals\n4\n(w21 + w 2 2) 6\n( w21w 2 2(3w 2 1 \u2212 w22)(w21 \u2212 3w22)\u2212 4w21w22(w21 \u2212 w22)2 ) = 4w21w 2 2\n(w21 + w 2 2) 6\n( (3w21 \u2212 w22)(w21 \u2212 3w22)\u2212 4(w21 \u2212 w22)2 ) = 4w21w 2 2\n(w21 + w 2 2) 6\n( \u2212(w21 + w22)2 ) = \u2212 4w 2 1w 2 2\n(w21 + w 2 2)\n4 ,\nwhich is always non-positive, and strictly negative for w for which w1w2 6= 0 (which holds for all but a measure-zero set of Rd). Since the determinant of a positive semidefinite matrix is always non-negative, this implies that the Hessian isn\u2019t positive semidefinite for any such w.\nThe theorem implies that we indeed cannot use convex optimization tools as-is on the function FA, even if we\u2019re close to an optimum. However, the non-convexity was shown for FA as a function over the entire Euclidean space, so the result does not preclude the possibility of having convexity on a more constrained, lower-dimensional set. In fact, this is what we are going to do next: We will show that if we are given some point w0 close enough to an optimum, then we can explicitly construct a simple convex set, such that\n\u2022 The set includes an optimal point of FA.\n\u2022 The function FA is O(1)-smooth and \u03bb-strongly convex in that set.\nThis means that we can potentially use a two-stage approach: First, we use some existing algorithm (such as VR-PCA) to find w0, and then switch to a convex optimization algorithm designed to handle functions with a finite sum structure (such as FA). Since the runtime of such algorithms scale better than VR-PCA, in terms of the dependence on \u03bb, we can hope for an overall runtime improvement.\nUnfortunately, this has a catch: To make it work, we need to have w0 very close to the optimum \u2013 in fact, we require \u2016v1\u2212w0\u2016 \u2264 O(\u03bb), and we show (in Theorem 5) that such a dependence on the eigengap \u03bb cannot be avoided (perhaps up to a small polynomial factor). The issue is that the runtime to get such a w0, using stochastic-based approaches we are aware of, would scale at least quadratically with 1/\u03bb, but getting dependence better than quadratic was our problem to begin with. For example, the runtime guarantee using VR-PCA to get such a point w0 (even if we start from a good point as specified in Theorem 1) is on the order of\nd ( n+ 1\n\u03bb2\n) log ( 1\n\u03bb\n) ,\nwhereas the best known guarantees on getting an -optimal solution for \u03bb-strongly convex and smooth functions (see [1]) is on the order of\nd ( n+ \u221a n\n\u03bb\n) log ( 1 ) .\nTherefore, the total runtime we can hope for would be on the order of\nd (( n+ 1\n\u03bb2\n) log ( 1\n\u03bb\n) + ( n+ \u221a n\n\u03bb\n) log ( 1 )) . (8)\nIn comparison, the runtime guarantee of using just VR-PCA to get an -accurate solution is on the order of\nd ( n+ 1\n\u03bb2\n) log ( 1 ) . (9)\nUnfortunately, Eq. (9) is the same as Eq. (8) up to log-factors, and the difference is not significant unless the required accuracy is extremely small (exponentially small in n, 1/\u03bb). Therefore, our construction is mostly of theoretical interest. However, it still shows that asymptotically, as \u2192 0, it is indeed possible to have runtime scaling better than Eq. (9). This might hint that designing practical algorithms, with better runtime guarantees for our problem, may indeed be possible.\nTo explain our construction, we need to consider two convex sets: Given a unit vector w0, define the hyperplane tangent to w0,\nHw0 = {w : \u3008w,w0\u3009 = 1}\nas well as a Euclidean ball of radius r centered at w0:\nBw0(r) = {w : \u2016w \u2212w0\u2016 \u2264 r}\nThe convex set we use, given such a w0, is simply the intersection of the two, Hw0 \u2229 Bw0(r), where r is a sufficiently small number (see Figure 2).\nThe following theorem shows that if w0 is O(\u03bb)-close to an optimal point (a leading eigenvector v1 of A), and we choose the radius of Bw0(r) appropriately, then Hw0 \u2229 Bw0(r) contains an optimal point, and the function FA is indeed \u03bb-strongly convex and smooth on that set. For simplicity, we will assume that A is scaled to have spectral norm of 1, but the result can be easily generalized.\nTheorem 4. For any positive semidefinite A with spectral norm 1, eigengap \u03bb and a leading eigenvector v1, and any unit vector w0 such that \u2016w0 \u2212 v1\u2016 \u2264 \u03bb44 , the function FA(w) is 20-smooth and \u03bb-strongly convex on the convex set Hw0 \u2229Bw0 ( \u03bb 22 ) , which contains a global optimum of FA.\nThe proof of the theorem appears in Subsection 6.3. Finally, we show below that a polynomial dependence on the eigengap \u03bb is unavoidable, in the sense that the convexity property is lost if w0 is significantly further away from v1.\nTheorem 5. For any \u03bb, \u2208 ( 0, 12 ) , there exists a positive semidefinite matrix A with spectral norm 1,\neigengap \u03bb, and leading eigenvector v1, as well as a unit vector w0 for which \u2016v1 \u2212w0\u2016 \u2264 \u221a\n2(1 + )\u03bb), such that FA is not convex in any neighborhood of w0 on Hw0 .\nProof. Let\nA =  1 0 00 1\u2212 \u03bb 0 0 0 0  , for which v1 = (1, 0, 0), and take\nw0 = ( \u221a 1\u2212 p2, 0, p),\nwhere p = \u221a (1 + )\u03bb (which ensures \u2016v1\u2212w0\u20162 = \u221a 2p2 = \u221a 2(1 + )\u03bb). Consider the ray {( \u221a\n1\u2212 p2, t, p) : t \u2265 0}, and note that it starts from w0 and lies in Hw0 . The function FA along that ray (considering it as a function of t) is of the form\n\u2212(1\u2212 p 2) + (1\u2212 \u03bb)t2 (1\u2212 p2) + t2 + p2 = \u2212 1\u2212 p 2 + (1\u2212 \u03bb)t2 1 + t2 .\nThe second derivative with respect to t equals\n\u22122(3t 2 \u2212 1)(\u03bb\u2212 p2) (t2 + 1)3 = 2 (3t2 \u2212 1) \u03bb (t2 + 1)3 ,\nwhere we plugged in the definition of p. This is a negative quantity for any t < 1\u221a 3 . Therefore, the function FA is strictly concave (and not convex) along the ray we have defined and close enough to w0, and therefore isn\u2019t convex in any neighborhood of w0 on Hw0 ."}, {"heading": "6 Proofs", "text": ""}, {"heading": "6.1 Proof of Theorem 1", "text": "Although the proof structure generally mimics the proof of Theorem 1 in [19] for the k = 1 special case, it is more intricate and requires several new technical tools. To streamline the presentation of the proof, we begin with proving a series of auxiliary lemmas in Subsection 6.1.1, and then move to the main proof in Subsection 6.1. The main proof itself is divided into several steps, each constituting one or more lemmas.\nThroughout the proof, we use the well-known facts that for all matrices B,C,D of suitable dimensions, Tr(B + C) = Tr(B) + Tr(C), Tr(BC) = Tr(CB), Tr(BCD) = Tr(DBC), and Tr(B>B) = \u2016B\u20162F . Moreover, since Tr is a linear operation, E[Tr(B)] = E[Tr(B)] for a random matrix B."}, {"heading": "6.1.1 Auxiliary Lemmas", "text": "Lemma 2. For any B,C,D 0, it holds that Tr(BC) \u2265 Tr(B(C \u2212D)) and Tr(BC) \u2265 Tr((B \u2212D)C).\nProof. It is enough to prove that for any positive semidefinite matrices E,G, it holds that Tr(EG) \u2265 0. The lemma follows by taking either E = B,G = D (in which case, Tr(BC) = Tr(B(C \u2212D)) + Tr(BD) \u2265 Tr(B(C\u2212D))), orE = D,G = C (in which case, Tr(BC) = Tr((B\u2212D)C)+Tr(DC) \u2265 Tr((B\u2212D)C)).\nAny positive semidefinite matrixM can be written as the productM1/2M1/2 for some symmetric matrix M1/2 (known as the matrix square root of M ). Therefore,\nTr(EG) = Tr(E1/2E1/2G1/2G1/2) = Tr(G1/2E1/2E1/2G1/2)\n= Tr((E1/2G1/2)>(E1/2G1/2)) = \u2016E1/2G1/2\u20162F \u2265 0.\nLemma 3. If B 0 and C 0, then\nTr(BC\u22121) \u2265 Tr(B(2I \u2212 C)),\nwhere I is the identity matrix.\nProof. We begin by proving the one-dimensional case, where B,C are scalars b \u2265 0, c > 0. The inequality then becomes bc\u22121 \u2265 b(2 \u2212 c), which is equivalent to 1 \u2265 c(2 \u2212 c), or upon rearranging, (c \u2212 1)2 \u2265 0, which trivially holds.\nTurning to the general case, we note that by Lemma 2, it is enough to prove that C\u22121 \u2212 (2I \u2212 C) 0. To prove this, we make a couple of observations. The positive definite matrix C (like any positive definite matrix) has a singular value decomposition which can be written asUSU>, whereU is an orthogonal matrix, and S is a diagonal matrix with positive entries. Its inverse is US\u22121U>, and 2I \u2212 C = 2I \u2212 USU> = U(2I \u2212 S)U>. Therefore,\nC\u22121 \u2212 (2I \u2212 C) = US\u22121U> \u2212 U(2I \u2212 S)U> = U(S\u22121 \u2212 (2I \u2212 S))U>.\nTo show this matrix is positive semidefinite, it is enough to show that each diagonal entry of S\u22121\u2212 (2I\u2212S) is non-negative. But this reduces to the one-dimensional result we already proved, when b = 1 and c > 0 is any diagonal entry in S. Therefore, C\u22121 \u2212 (2I \u2212 C) 0, from which the result follows.\nLemma 4. For any matrices B,C, Tr(BC) \u2264 \u2016B\u2016F \u2016C\u2016F\nand \u2016BC\u2016F \u2264 \u2016B\u2016sp\u2016C\u2016F .\nProof. The first inequality is immediate from Cauchy-Shwartz. As to the second inequality, letting ci denote the i-th column of C, and \u2016 \u00b7 \u20162 the Euclidean norm for vectors,\n\u2016BC\u2016F = \u221a\u2211\ni\n\u2016Bci\u201622 \u2264 \u221a\u2211\ni\n(\u2016B\u2016sp\u2016ci\u20162)2 = \u2016B\u2016sp \u221a\u2211\ni\n\u2016ci\u201622 = \u2016B\u2016sp\u2016C\u2016F .\nLemma 5. Let B1, B2, Z1, Z2 be k \u00d7 k square matrices, where B1, B2 are fixed and Z1, Z2 are stochastic and zero-mean (i.e. their expectation is the all-zeros matrix). Furthermore, suppose that for some fixed \u03b1, \u03b3, \u03b4 > 0, it holds with probability 1 that\n\u2022 For all \u03bd \u2208 [0, 1], B2 + \u03bdZ2 \u03b4I .\n\u2022 max{\u2016Z1\u2016F , \u2016Z2\u2016F } \u2264 \u03b1.\n\u2022 \u2016B1 + \u03b7Z1\u2016sp \u2264 \u03b3.\nThen\nE [ Tr ( (B1 + Z1)(B2 + Z2) \u22121)] \u2265 Tr(B1B\u221212 )\u2212 \u03b12(1 + \u03b3/\u03b4)\u03b42 . Proof. Define the function\nf(\u03bd) = Tr ( (B1 + \u03bdZ1)(B2 + \u03bdZ2) \u22121) , \u03bd \u2208 [0, 1]. Since B2 + \u03bdZ2 is positive definite, it is always invertible, hence f(\u03bd) is indeed well-defined. Moreover, it can be differentiated with respect to \u03bd, and we have\nf \u2032(\u03bd) = Tr ( Z1(B2 + \u03bdZ2) \u22121 \u2212 (B1 + \u03bdZ1)(B2 + \u03bdZ2)\u22121Z2(B2 + \u03bdZ2)\u22121 ) .\nAgain differentiating with respect to \u03bd, we have f \u2032\u2032(\u03bd) = Tr ( \u2212 2Z1(B2 + \u03bdZ2)\u22121Z2(B2 + \u03bdZ2)\u22121\n+ 2(B1 + \u03bdZ1)(B2 + \u03bdZ2) \u22121Z2(B2 + \u03bdZ2) \u22121Z2(B2 + \u03bdZ2) \u22121 )\n= 2 Tr (( \u2212 Z1 + (B1 + \u03bdZ1)(B2 + \u03bdZ2)\u22121Z2 ) (B2 + \u03bdZ2) \u22121Z2(B2 + \u03bdZ2) \u22121 ) .\nUsing Lemma 4 and the triangle inequality, this is at most\n2\u2016 \u2212 Z1 + (B1 + \u03bdZ1)(B2 + \u03bdZ2)\u22121Z2\u2016F \u2016(B2 + \u03bdZ2)\u22121Z2(B2 + \u03bdZ2)\u22121\u2016F \u2264 2 ( \u2016Z1\u2016F + \u2016(B1 + \u03bdZ1)(B2 + \u03bdZ2)\u22121Z2\u2016F ) \u2016(B2 + \u03bdZ2)\u22121\u20162sp\u2016Z2\u2016F\n\u2264 2 ( \u2016Z1\u2016F + \u2016B1 + \u03bdZ1\u2016sp\u2016 (B2 + \u03bdZ2)\u22121 \u2016sp\u2016Z2\u2016F ) \u2016(B2 + \u03bdZ2)\u22121\u20162sp\u2016Z2\u2016F\n\u2264 2 ( \u03b1+ \u03b3 1\n\u03b4 \u03b1\n) 1\n\u03b42 \u03b1 =\n2\u03b12(1 + \u03b3/\u03b4)\n\u03b42 .\nApplying a Taylor expansion to f(\u00b7) around \u03bd = 0, with a Lagrangian remainder term, and substituting the values for f \u2032(\u03bd), f \u2032\u2032(\u03bd), we can lower bound f(1) as follows:\nf(1) \u2265 f(0) + f \u2032(0) \u2217 (1\u2212 0)\u2212 1 2 max \u03bd |f \u2032\u2032(\u03bd)| \u2217 (1\u2212 0)2\n= Tr ( B1B \u22121 2 ) + Tr ( Z1B \u22121 2 \u2212B1B \u22121 2 Z2B \u22121 2 ) \u2212 \u03b1 2(1 + \u03b3/\u03b4)\n\u03b42 .\nTaking expectation over Z1, Z2, and recalling they are zero-mean, we get that\nE[f(1)] \u2265 Tr ( B1B \u22121 2 ) \u2212 \u03b1 2(1 + \u03b3/\u03b4)\n\u03b42 . Since E[f(1)] = E [ Tr ( (B1 + Z1)(B2 + Z2) \u22121)], the result in the lemma follows.\nLemma 6. Let U1, . . . , Uk and R1, R2 be positive semidefinite matrices, such that R2\u2212R1 0, and define the function\nf(x1 . . . xk) = Tr ( k\u2211 i=1 xiUi +R1 )( k\u2211 i=1 xiUi +R2 )\u22121 . over all (x1 . . . xk) \u2208 [\u03b1, \u03b2]d for some \u03b2 \u2265 \u03b1 \u2265 0. Then min(x1...xk)\u2208[\u03b1,\u03b2]d f(x) = f(\u03b1, . . . , \u03b1). Proof. Taking a partial derivative of f with respect to some xj , we have\n\u2202\n\u2202xj f(x)\n= Tr Uj ( k\u2211 i=1 xiUi +R2 )\u22121 \u2212 ( k\u2211 i=1 xiUi +R1 )( k\u2211 i=1 xiUi +R2 )\u22121 Uj ( k\u2211 i=1 xiUi +R2 )\u22121 = Tr\nI \u2212( k\u2211 i=1 Ui +R1 )( k\u2211 i=1 xiUi +R2 )\u22121Uj ( k\u2211 i=1 xiUi +R2 )\u22121 = Tr\n(( k\u2211 i=1 xiUi +R2 ) \u2212 ( k\u2211 i=1 xiUi +R1 ))( k\u2211 i=1 xiUi +R2 )\u22121 Uj ( k\u2211 i=1 xiUi +R2 )\u22121 = Tr\n(R2 \u2212R1)( k\u2211 i=1 xiUi +R2 )\u22121 Uj ( k\u2211 i=1 xiUi +R2 )\u22121 . By the lemma\u2019s assumptions, each matrix in the product above is positive semidefinite, hence the product is positive semidefinite, and the trace is non-negative. Therefore, \u2202\u2202xj f(x) \u2265 0, which implies that the function is minimized when each xj takes its smallest possible value, i.e. \u03b1.\nLemma 7. Let B be a k \u00d7 k matrix with minimal singular value \u03b4. Then\n1\u2212 \u2016B>B\u20162F \u2016B\u20162F\n\u2265 max { 1\u2212 \u2016B\u20162F , \u03b42\nk\n( k \u2212 \u2016B\u20162F )} .\nProof. We have\n1\u2212 \u2016B>B\u20162F \u2016B\u20162F \u2265 1\u2212 \u2016B\u20162F \u2016B\u20162F \u2016B\u20162F = 1\u2212 \u2016B\u20162F ,\nso it remains to prove 1 \u2212 \u2016B >B\u20162F \u2016B\u20162F \u2265 \u03b42k ( k \u2212 \u2016B\u20162F ) . Let \u03c31, . . . , \u03c3k denote the vector of singular values of B. The singular values of B>B are \u03c321, . . . , \u03c3 2 k, and the Frobenius norm of a matrix equals the Euclidean norm of its vector of singular values. Therefore, the lemma is equivalent to requiring\n1\u2212 \u2211k i=1 \u03c3 4 i\u2211k\ni=1 \u03c3 2 i\n\u2265 \u03b4 2\nk\n( k \u2212\nk\u2211 i=1 \u03c32i\n) ,\nassuming \u03c3i \u2208 [\u03b4, 1] for all i. This holds since 1\u2212 \u2211 i \u03c3 4 i\u2211\ni \u03c3 2 i\n=\n\u2211 i \u03c3 2 i \u2212 \u2211 i \u03c3 4 i\u2211\ni \u03c3 2 i\n=\n\u2211 i \u03c3 2 i ( 1\u2212 \u03c32i )\u2211 i \u03c3 2 i \u2265 \u03b42 \u2211 i ( 1\u2212 \u03c32i ) k = \u03b42 k ( k \u2212 \u2211 i \u03c32i ) .\nLemma 8. For any d\u00d7 k matrices C,D with orthonormal columns, let\nDC = arg min DB : (DB)>(DB)=I\n\u2016C \u2212DB\u20162F\nbe the nearest orthonormal-columns matrix toC in the column space ofD (whereB is a k\u00d7k matrix). Then the matrix B minimizing the above equals B = V U>, where C>D = USV > is the SVD decomposition of C>D, and it holds that\n\u2016C \u2212DC\u20162F \u2264 2(k \u2212 \u2016C>D\u20162F ).\nProof. Since D has orthonormal columns, we have D>D = I , so the definition of B is equivalent to\nB = arg min B : B>B=I\n\u2016C \u2212DB\u20162F .\nThis is the orthogonal Procrustes problem (see e.g. [8]), and the solution is easily shown to be B = V U> where USV > is the SVD decomposition of C>D. In this case, and using the fact that \u2016C\u20162F = \u2016D\u20162F = k (as C,D have orthonormal columns), we have that \u2016C \u2212DC\u20162F equals\n\u2016C \u2212DB\u20162F = \u2016C\u20162F + \u2016D\u20162F \u2212 2 Tr(C>DB) = 2 ( k \u2212 Tr(USV >(V U>)) ) = 2 ( k \u2212 Tr(USU>) ) .\nSince the trace function is similarity-invariant, this equals 2k \u2212 Tr(S). Let s1 . . . , sk be the diagonal elements of S, and note that they can be at most 1 (since they are the singular values of C>D, and both C and D have orthonormal columns). Recalling that the Frobenius norm equals the Euclidean norm of the singular values, we can therefore upper bound the above as follows:\n2 ( k \u2212 Tr(USU>) ) = 2 (k \u2212 Tr(S)) = 2 ( k \u2212\nk\u2211 i=1 si\n) \u2264 2 ( k \u2212\nk\u2211 i=1 s2i\n) = 2 ( k \u2212 \u2016C>D\u20162F ) .\nLemma 9. Let Wt,W \u2032t be as defined in Algorithm 2, where we assume \u03b7 < 13 . Then for any d \u00d7 k matrix Vk with orthonormal columns, it holds that\u2223\u2223\u2223\u2016V >k Wt\u20162F \u2212 \u2016V >k Wt\u22121\u20162F \u2223\u2223\u2223 \u2264 12k\u03b71\u2212 3\u03b7 . Proof. Letting st, st\u22121 denote the vectors of singular values of V >k Wt and V > k Wt\u22121, and noting that they are both in [0, 1]k (as Vk,Wt\u22121,Wt all have orthonormal columns), the left hand side of the inequality in the lemma statement equals\n|\u2016st\u20162 \u2212 \u2016st\u22121\u20162| = (\u2016st\u20162 + \u2016st\u22121\u20162) | \u2016st\u20162 \u2212 \u2016st\u22121\u20162 | \u2264 2 \u221a k\u2016st \u2212 st\u22121\u20162 \u2264 2k\u2016st \u2212 st\u22121\u2016\u221e,\nwhere \u2016 \u00b7 \u2016\u221e is the infinity norm. By Weyl\u2019s matrix perturbation theorem6 [12], this is upper bounded by\n2k\u2016V >k Wt \u2212 V >k Wt\u22121\u2016sp \u2264 2k\u2016Vk\u2016sp\u2016Wt \u2212Wt\u22121\u2016sp \u2264 2k\u2016Wt \u2212Wt\u22121\u2016sp. (10) 6Using its version for singular values, which implies that the singular values of matrices B and B + E are different by at most\n\u2016E\u2016sp.\nRecalling the relationship between Wt and Wt\u22121 from Algorithm 2, we have that\nW \u2032t = Wt\u22121 + \u03b7N,\nwhere\n\u2016N\u2016sp \u2264 \u2016xitx>itWt\u22121\u2016sp + \u2016xitx > itW\u0303s\u22121Bt\u22121\u2016sp + \u2016\n1\nn n\u2211 i=1 xix > i W\u0303s\u22121Bt\u22121\u2016sp \u2264 3,\nas Wt\u22121, W\u0303s\u22121, Bt\u22121 all have orthonormal columns, and xitx > it and 1n \u2211n i=1 xix > i have spectral norm at most 1. Therefore, W \u2032t equals Wt\u22121, up to a matrix perturbation of spectral norm at most 3\u03b7. Again by Weyl\u2019s theorem, this implies that the k non-zero singular values of the d \u00d7 k matrix W \u2032t are different from those of Wt\u22121 (which has orthonormal columns) by at most 3\u03b7, and hence all lie in [1 \u2212 3\u03b7, 1 + 3\u03b7]. As a\nresult, the singular values of ( W \u2032> t W \u2032 t )\u22121/2 all lie in [ 1 1+3\u03b7 , 1 1\u22123\u03b7 ] . Collecting these observations, we have\n\u2016Wt \u2212Wt\u22121\u2016sp = \u2016(Wt\u22121 + \u03b7N) ( W \u2032> t\u22121W \u2032 t\u22121 )\u22121/2 \u2212Wt\u22121\u2016sp\n\u2264 \u2016Wt\u22121 (( W \u2032> t\u22121W \u2032 t\u22121 )\u22121/2 \u2212 I ) + \u03b7N ( W \u2032> t\u22121W \u2032 t\u22121 )\u22121/2 \u2016sp\n\u2264 \u2016 ( W \u2032> t\u22121W \u2032 t\u22121 )\u22121/2 \u2212 I\u2016sp + \u03b7\u2016N\u2016sp\u2016 ( W \u2032> t\u22121W \u2032 t\u22121 )\u22121/2 \u2016sp\n\u2264 3\u03b7 1\u2212 3\u03b7 + 3\u03b7 1\u2212 3\u03b7 =\n6\u03b7\n1\u2212 3\u03b7 .\nPlugging back to Eq. (10), the result follows."}, {"heading": "6.1.2 Main Proof", "text": "To simplify the technical derivations, note that the algorithm remains the same if we divide each xi by \u221a r, and multiply \u03b7 by r. Since maxi \u2016xi\u20162 \u2264 r, this corresponds to running the algorithm with step-size \u03b7r rather than \u03b7, on a re-scaled dataset of points with squared norm at most 1, and with an eigengap of \u03bb/r instead of \u03bb. Therefore, we can simply analyze the algorithm assuming that maxi \u2016xi\u20162 \u2264 1, and in the end plug in \u03bb/r instead of \u03bb, and \u03b7r instead of \u03b7, to get a result which holds for data with squared norm at most r.\nPart I: Establishing a Stochastic Recurrence Relation\nWe begin by focusing on a single iteration t of the algorithm, and analyze how \u2016V >k Wt\u20162F (which measures the similarity between the column spaces of Vk and Wt) evolves during that iteration. The key result we need is Lemma 10 below, which is specialized for our algorithm in Lemma 11.\nLemma 10. Let A be a d \u00d7 d symmetric matrix with all eigenvalues s1 \u2265 s2 \u2265 . . . \u2265 sd in [0, 1], and suppose that sk \u2212 sk+1 \u2265 \u03bb for some \u03bb > 0.\nLet N be a d \u00d7 k zero-mean random matrix such that \u2016N\u2016F \u2264 \u03c3FN and \u2016N\u2016sp \u2264 \u03c3 sp N with probability\n1, and define\nrN = 46 (\u03c3 F N ) 2\n( 1 + 8\n3\n( 1\n4 \u03c3spN + 2\n)2)\nLet W be a d\u00d7 k matrix with orthonormal columns, and define\nW \u2032 = (I + \u03b7A)W + \u03b7N , W \u2032\u2032 = W \u2032(W \u2032>W \u2032)\u22121/2,\nfor some \u03b7 \u2208 [ 0, 1\n4max{1,\u03c3FN}\n] .\nIf Vk = [v1,v2 . . . ,vk] is the d\u00d7 k matrix of A\u2019s first k eigenvectors, then the following holds: \u2022 E [ 1\u2212 \u2016V >k W \u2032\u2032\u20162F ] \u2264 ( 1\u2212 45\u03b7\u03bb\u2016V > k W\u20162F ) ( 1\u2212 \u2016V >k W\u20162F ) + \u03b72rN\n\u2022 If \u2016V >k W\u20162F \u2265 k \u2212 1 2 , then\nEN [ k \u2212 \u2016V >k W \u2032\u2032\u20162F ] \u2264 ( k \u2212 \u2016V >k W\u20162F )( 1\u2212 1\n10 \u03b7\u03bb\n) + \u03b72rN .\nProof. Using the fact that Tr(BCD) = Tr(CDB) for any matrices B,C,D, we have\nE [ \u2016V >k W \u2032\u2032\u20162F ] = E [ Tr ( W \u2032\u2032>VkV > k W \u2032\u2032 )]\n= E [ Tr (( W \u2032>W \u2032 )\u22121/2 W \u2032>VkV > k W \u2032 ( W \u2032>W \u2032 )\u22121/2)] = E [ Tr (( W \u2032>VkV > k W \u2032 )( W \u2032>W \u2032 )\u22121)] . (11)\nBy definition of W \u2032, we have\nW \u2032>VkV > k W \u2032 = ((I + \u03b7A)W + \u03b7N)> VkV > k ((I + \u03b7A)W + \u03b7N)\n= B1 + Z1,\nwhere we define\nB1 = W >(I + \u03b7A)VkV > k (I + \u03b7A)W + \u03b7 2N>VkV > k N Z1 = \u03b7N >VkV > k (I + \u03b7A)W + \u03b7W >(I + \u03b7A)VkV > k N.\nAlso, we have\nW \u2032>W \u2032 = ((I + \u03b7A)W + \u03b7N)> ((I + \u03b7A)W + \u03b7N)\n= B2 + Z2,\nwhere\nB2 = W >(I + \u03b7A)(I + \u03b7A)W + \u03b72N>N Z2 = \u03b7N >(I + \u03b7A)W + \u03b7W>(I + \u03b7A)N.\nWith these definitions, we can rewrite Eq. (11) as E [ Tr((B1 + Z1)(B2 + Z2) \u22121) ] . We now wish to remove Z1, Z2, by applying Lemma 5. To do so, we check the lemma\u2019s conditions:\n\u2022 Z1, Z2 are zero mean: This holds since they are linear in N , and N is assumed to be zero-mean.\n\u2022 B2 + \u03bdZ2 38I for all \u03bd \u2208 [0, 1]: Recalling the definition of B2, Z2, and the facts that A 0, N>N 0 (by construction), and W>W = I , we have that B2 I . Moreover, the spectral norm of Z2 is at most\n2\u03b7\u2016N>(I + \u03b7A)W\u2016sp \u2264 2\u03b7\u2016N\u2016sp\u2016I + \u03b7A\u2016sp\u2016W\u2016sp \u2264 2\u03b7\u03c3spN (1 + \u03b7) \u2264 2\u03b7\u03c3 F N (1 + \u03b7),\nwhich by the assumption on \u03b7 is at most 214 ( 1 + 14 ) = 58 . This implies that the smallest singular value of B2 + \u03bdZ2 is at least 1\u2212 \u03bd(5/8) \u2265 3/8.\n\u2022 max{\u2016Z1\u2016F , \u2016Z2\u2016F } \u2264 52\u03b7\u03c3 F N : By definition of Z1, Z2, and using Lemma 4, the Frobenius norm of\nthese two matrices is at most\n2\u03b7\u2016N\u2016F \u2016(I + \u03b7A)\u2016sp\u2016W\u2016sp \u2264 2\u03b7\u03c3FN (1 + \u03b7),\nwhich by the assumption on \u03b7 is at most 2\u03b7\u03c3FN ( 1 + 14 ) = 52\u03b7\u03c3 F N .\n\u2022 \u2016B1 + \u03b7Z1\u2016sp \u2264 ( 1 4\u03c3 sp N + 2 )2: Using the definition of B1, Z1 and the assumption \u03b7 \u2264 14 , \u2016B1 + \u03b7Z1\u2016sp \u2264 \u2016B1\u2016sp + \u03b7\u2016Z1\u2016sp\n\u2264 (1 + \u03b7)2 + \u03b72(\u03c3spN ) 2 + 2\u03b7\u03c3spN (1 + \u03b7) \u2264 ( 5\n4\n)2 + 1\n16 (\u03c3spN )\n2 + 5\n8 \u03c3spN\n<\n( 1\n4 \u03c3spN + 2\n)2 .\nApplying Lemma 5 and plugging back to Eq. (11), we get E [ \u2016V >k W \u2032\u2032\u20162F ] \u2265 E [ Tr((B1 + Z1)(B2 + Z2) \u22121) ]\n\u2265 Tr ( B1B \u22121 2 ) \u2212 400\n9 (\u03b7\u03c3FN ) 2\n( 1 + 8\n3\n( 1\n4 \u03c3spN + 2\n)2) . (12)\nWe now turn to lower bound Tr ( B1B \u22121 2 ) , by first re-writing B1, B2 in a different form. For i = 1, . . . , d, let Ui = W >viv > i W,\nwhere vi is the eigenvector of A corresponding to the eigenvalue si. Note that each Ui is positive semidefinite, and \u2211d i=1 Ui = W >W = I . We have\nB1 = W >(I + \u03b7A)VkV > k (I + \u03b7A)W + \u03b7 2N>VkV > k N\n= W> ((I + \u03b7A)Vk) ((I + \u03b7A)Vk) >W + \u03b72N>VkV > k N\n= k\u2211 i=1 (1 + \u03b7si) 2W>viv > i W + \u03b7 2N>VkV > k N\n= k\u2211 i=1 (1 + \u03b7si) 2Ui + \u03b7 2N>VkV > k N. (13)\nSimilarly,\nB2 = W >(I + \u03b7A)(I + \u03b7A)W + \u03b72N>N\n= d\u2211 i=1 (1 + \u03b7si) 2W>viv > i W + \u03b7 2N>N\n= d\u2211 i=1 (1 + \u03b7si) 2Ui + \u03b7 2N>N. (14)\nPlugging Eq. (13) and Eq. (14) back into Eq. (12), we get\nE [ \u2016V >k W \u2032\u2032\u20162F ] \u2265 Tr ( k\u2211 i=1 (1 + \u03b7s1) 2Ui + \u03b7 2N>VkV > k N )( d\u2211 i=1 (1 + \u03b7si) 2Ui + \u03b7 2N>N )\u22121 \u2212 400\n9 (\u03b7\u03c3FN ) 2\n( 1 + 8\n3\n( 1\n4 \u03c3spN + 2\n)2) . (15)\nRecalling that s1 \u2265 s2 \u2265 . . . \u2265 sk and letting \u03b1 = (1 + \u03b7sk)2, \u03b2 = (1 + \u03b7s1)2, the trace term can be lower bounded by\nmin x1,...,xk\u2208[\u03b1,\u03b2] Tr ( k\u2211 i=1 xiUi + \u03b7 2N>VkV > k N )( k\u2211 i=1 xiUi + d\u2211 i=k+1 (1 + \u03b7si) 2Ui + \u03b7 2N>N )\u22121 . Applying Lemma 6 (noting that as required by the lemma, \u2211d i=k+1(1+\u03b7si)\n2Ui+\u03b7 2N>N\u2212\u03b72N>VkV >k N =\u2211d\ni=k+1(1 + \u03b7si) 2Ui + \u03b7 2N> ( I \u2212 VkV >k ) N 0), we can lower bound the above by\nTr ((1 + \u03b7sk)2 k\u2211 i=1 Ui + \u03b7 2N>VkV > k N )( (1 + \u03b7sk) 2 k\u2211 i=1 Ui + d\u2211 i=k+1 (1 + \u03b7si) 2Ui + \u03b7 2N>N )\u22121 . Using Lemma 2, this can be lower bounded by\nTr ((1 + \u03b7sk)2 k\u2211 i=1 Ui )( (1 + \u03b7sk) 2 k\u2211 i=1 Ui + d\u2211 i=k+1 (1 + \u03b7si) 2Ui + \u03b7 2N>N )\u22121 = Tr\n( k\u2211 i=1 Ui )( k\u2211 i=1 Ui + d\u2211 i=k+1 ( 1 + \u03b7si 1 + \u03b7sk )2 Ui + ( \u03b7 1 + \u03b7sk )2 N>N )\u22121\nApplying Lemma 3, this is at least\nTr (( k\u2211 i=1 Ui )( 2I \u2212 k\u2211 i=1 Ui \u2212 d\u2211 i=k+1 ( 1 + \u03b7si 1 + \u03b7sk )2 Ui \u2212 ( \u03b7 1 + \u03b7sk )2 N>N )) .\nRecalling that I = \u2211d i=1 Ui = \u2211k i=1 Ui + \u2211d i=k+1 Ui, this can be simplified to\nTr (( k\u2211 i=1 Ui )( k\u2211 i=1 Ui + d\u2211 i=k+1 ( 2\u2212 ( 1 + \u03b7si 1 + \u03b7sk )2) Ui \u2212 ( \u03b7 1 + \u03b7sk )2 N>N )) . (16)\nSince Ui 0, then using Lemma 3, we can lower bound the expression above by shrinking each of the( 2\u2212 ( 1+\u03b7si 1+\u03b7sk )2) terms. In particular, since si \u2264 sk \u2212 \u03bb for each i \u2265 k + 1,\n2\u2212 (\n1 + \u03b7si 1 + \u03b7sk\n)2 \u2265 2\u2212 1 + \u03b7si\n1 + \u03b7sk \u2265 2\u2212 1 + \u03b7(sk \u2212 \u03bb) 1 + \u03b7sk = 1 + \u03b7\u03bb 1 + \u03b7sk ,\nwhich by the assumption that \u03b7 \u2264 1/4 and sk \u2264 s1 \u2264 1, is at least 1+ 45\u03b7\u03bb. Plugging this back into Eq. (16), and recalling that \u2211d i=1 Ui = I , we get the lower bound\nTr (( k\u2211 i=1 Ui )( k\u2211 i=1 Ui + d\u2211 i=k+1 ( 1 + 4 5 \u03b7\u03bb ) Ui \u2212 ( \u03b7 1 + \u03b7sk )2 N>N ))\n= Tr (( k\u2211 i=1 Ui )( I + 4 5 \u03b7\u03bb ( I \u2212 k\u2211 i=1 Ui ) \u2212 (\n\u03b7\n1 + \u03b7sk\n)2 N>N )) .\nAgain using Lemma 2, this is at least\nTr (( k\u2211 i=1 Ui )( I + 4 5 \u03b7\u03bb ( I \u2212 k\u2211 i=1 Ui ))) \u2212 (\n\u03b7\n1 + \u03b7sk\n)2 Tr (( k\u2211 i=1 Ui ) N>N )\n\u2265 Tr (( k\u2211 i=1 Ui )( I + 4 5 \u03b7\u03bb ( I \u2212 k\u2211 i=1 Ui ))) \u2212 (\n\u03b7\n1 + \u03b7sk\n)2 Tr ( N>N ) \u2265 Tr\n(( k\u2211 i=1 Ui )( I + 4 5 \u03b7\u03bb ( I \u2212 k\u2211 i=1 Ui ))) \u2212 \u03b72 ( \u03c3FN )2 .\nRecall that this is a lower bound on the trace term in Eq. (15). Plugging it back and slightly simplifying, we get\nE [ \u2016V >k W \u2032\u2032\u20162F ] \u2265 Tr (( k\u2211 i=1 Ui )( I + 4 5 \u03b7\u03bb ( I \u2212 k\u2211 i=1 Ui ))) \u2212 \u03b72rN ,\nwhere\nrN = 46 (\u03c3 F N ) 2\n( 1 + 8\n3\n( 1\n4 \u03c3spN + 2\n)2) .\nThe trace term above can be re-written (using the definition of Ui and the fact that Tr(B>B) = \u2016B\u20162F ) as\nTr (( W>\nk\u2211 i=1 viv > i W\n)( I + 4\n5 \u03b7\u03bb\n( I \u2212W>\nk\u2211 i=1 viv > i W\n)))\n= ( 1 + 4\n5 \u03b7\u03bb\n) Tr ( W>VkV > k W ) \u2212 4\n5 \u03b7\u03bbTr\n(( W>VkV > k W )( W>VkV > k W )) = ( 1 + 4\n5 \u03b7\u03bb\n) \u2016V >k W\u20162F \u2212 4\n5 \u03b7\u03bb\u2016W>VkV >k W\u20162F\n= \u2016V >k W\u20162F ( 1 + 4\n5 \u03b7\u03bb ( 1\u2212 \u2016W>VkV >k W\u20162F \u2016V >k W\u20162F )) .\nApplying Lemma 7, and letting \u03b4 denote the minimal singular value of V >k W , this is lower bounded by\n\u2016V >k W\u20162F ( 1 + 4\n5 \u03b7\u03bbmax\n{ 1\u2212 \u2016V >k W\u20162F , \u03b42\nk\n( k \u2212 \u2016V >k W\u20162F )}) .\nOverall, we get that\nE [ \u2016V >k W \u2032\u2032\u20162F ] \u2265 \u2016V >k W\u20162F ( 1 + 4\n5 \u03b7\u03bbmax\n{ 1\u2212 \u2016V >k W\u20162F , \u03b42\nk\n( k \u2212 \u2016V >k W\u20162F )}) \u2212 \u03b72rN . (17)\nWe now consider two options:\n\u2022 Taking the first argument of the max term in Eq. (17), we get\nE [ \u2016V >k W \u2032\u2032\u20162F ] \u2265 \u2016V >k W\u20162F ( 1 + 4 5 \u03b7\u03bb ( 1\u2212 \u2016V >k W\u20162F )) \u2212 \u03b72rN .\nSubtracting 1 from both sides and simplifying, we get\nE [ 1\u2212 \u2016V >k W \u2032\u2032\u20162F ] \u2264 (\n1\u2212 4 5 \u03b7\u03bb\u2016V >k W\u20162F\n)( 1\u2212 \u2016V >k W\u20162F ) + \u03b72rN .\n\u2022 Suppose that \u2016V >k W\u20162F \u2265 k \u2212 1 2 . Taking the second argument of the max term in Eq. (17), we get\nE [ \u2016V >k W \u2032\u2032\u20162F ] \u2265 \u2016V >k W\u20162F ( 1 + 4\u03b7\u03bb\u03b42\n5k\n( k \u2212 \u2016V >k W\u20162F )) \u2212 \u03b72rN .\nSubtracting both sides from k, , we get\nE [ k \u2212 \u2016V >k W \u2032\u2032\u20162F ] \u2264 ( k \u2212 \u2016V >k W\u20162F ) \u2212 4\u03b7\u03bb\u03b4 2\n5k \u2016V >k W\u20162F\n( k \u2212 \u2016V >k W\u20162F ) + \u03b72rN\n= ( k \u2212 \u2016V >k W\u20162F )( 1\u2212 4\u03b7\u03bb\u03b4 2\n5k \u2016V >k W\u20162F\n) + \u03b72rN\n\u2264 ( k \u2212 \u2016V >k W\u20162F )( 1\u2212 4\u03b7\u03bb\u03b4 2\n5k\n( k \u2212 1\n2\n)) + \u03b72rN\nSince k \u2265 1, we can lower bound the ( k \u2212 12 ) term by k2 . Moreover, the condition k\u2212\u2016V > k W\u20162F \u2264 1 2\nimplies that the singular values \u03c31, . . . , \u03c3k of V >k W satisfy k\u2212 \u2211k i=1 \u03c3 2 i \u2264 12 . But each \u03c3i is in [0, 1] (as Vk,W have orthonormal columns), so no \u03c3i can be less than 12 . This implies that \u03b4 \u2265 1 2 . Plugging the lower bounds k \u2212 12 \u2265 k 2 and \u03b4 \u2265 1 2 into the above, we get\nE [ k \u2212 \u2016V >k W \u2032\u2032\u20162F ] \u2264 ( k \u2212 \u2016V >k W\u20162F )( 1\u2212 1\n10 \u03b7\u03bb\n) + \u03b72rN .\nLemma 11. Let A,Wt be as defined in Algorithm 2, and suppose that \u03b7 \u2208 [ 0, 1\n23 \u221a k\n] . Then the following\nholds for some positive numerical constants c1, c2, c3: \u2022 E [ 1\u2212 \u2016V >k W \u2032\u2032\u20162F ] \u2264 ( 1\u2212 c1\u03b7\u03bb\u2016V >k W\u20162F ) ( 1\u2212 \u2016V >k W\u20162F ) + c2k\u03b7 2\n\u2022 If \u2016V >k Wt\u20162F \u2265 k \u2212 1 2 , then E [ k \u2212 \u2016V >k Wt+1\u20162F ] \u2264 ( k \u2212 \u2016V >k Wt\u20162F ) (1\u2212 c1\u03b7 (\u03bb\u2212 c2\u03b7)) + c3\u03b72(k \u2212 \u2016V >k W\u0303s\u22121\u20162F ).\nIn the above, the expectation is over the random draw of the index it, conditioned on Wt and W\u0303s\u22121.\nProof. To apply Lemma 10, we need to compute upper bounds \u03c3FN and \u03c3 sp N on the Frobenius and spectral norms of N , which in our case equals (xitx > it \u2212 A)(Wt \u2212 W\u0303s\u22121Bt). Since \u2016A\u2016sp, \u2016xitx>it\u2016sp \u2264 1, and Wt, W\u0303s\u22121, Bt have orthonormal columns, the spectral norm of N is at most\n\u2016(xitx>it \u2212A)(Wt \u2212 W\u0303s\u22121Bt)\u2016sp \u2264 ( \u2016xitx>it\u2016sp + \u2016A\u2016sp )( \u2016Wt\u2016sp + \u2016W\u0303s\u22121\u2016sp\u2016Bt\u2016sp ) \u2264 4,\nso we may take \u03c3spN = 4. As to the Frobenius norm, using Lemma 4 and a similar calculation, we have\n\u2016N\u20162F \u2264 4\u2016Wt \u2212 W\u0303s\u22121Bt\u20162F .\nTo upper bound this, define\nVWt = arg min VkB:(VkB)>(VkB)=I\n\u2016Wt \u2212 VkB\u20162F\nto be the nearest orthonormal-columns matrix to Wt in the column space of Vk, and\nW\u0303V = arg min W\u0303s\u22121B:(W\u0303s\u22121B)>(W\u0303s\u22121B)=I\n\u2016VWt \u2212 W\u0303s\u22121B\u20162F\nto be the nearest orthonormal-columns matrix to VWt in the column space of W\u0303s\u22121. Also, recall that by definition,\nW\u0303s\u22121Bt = arg min W\u0303s\u22121B:(W\u0303s\u22121B)>(W\u0303s\u22121B)=I\n\u2016Wt \u2212 W\u0303s\u22121B\u20162F\nis the nearest orthonormal-columns matrix to Wt in the column space of W\u0303s\u22121. Therefore, we must have \u2016Wt \u2212 W\u0303s\u22121Bt\u20162F \u2264 \u2016Wt \u2212 W\u0303V \u20162F . Using this and Lemma 8, we have\n\u2016Wt \u2212 W\u0303s\u22121Bt\u20162F \u2264 \u2016Wt \u2212 W\u0303V \u20162F = \u2016(Wt \u2212 VWt)\u2212 (W\u0303V \u2212 VWt)\u20162F \u2264 2\u2016Wt \u2212 VWt\u20162F + 2\u2016W\u0303V \u2212 VWt\u20162F = 4 ( k \u2212 \u2016V >k Wt\u20162F ) + 4 ( k \u2212 \u2016V >WtW\u0303s\u22121\u2016 2 F ) .\nBy definition of VWt , we have VWt = VkB where B >B = B>V >k VkB = (VkB) >(VkB) = I . ThereforeB is an orthogonal k\u00d7 k matrix, and \u2016V >WtW\u0303s\u22121\u2016 2 F = \u2016B>V >k W\u0303s\u22121\u20162F = \u2016V >k W\u0303s\u22121\u20162F , so the above equals 4(k \u2212 \u2016V >k Wt\u20162F ) + 4(k \u2212 \u2016V >k W\u0303s\u22121\u20162F ). Overall, we get that the squared Frobenius norm of N can be upper bounded by\n(\u03c3FN ) 2 = 16 ( (k \u2212 \u2016V >k Wt\u20162F ) + (k \u2212 \u2016V >k W\u0303s\u22121\u20162F ) ) .\nPlugging \u03c3spN and (\u03c3 F N ) 2 into the rN as defined in Lemma 10, and picking any \u03b7 \u2208 [0, 123\u221ak ] (which satisfies the condition in Lemma 10 that \u03b7 \u2208 [ 0, 1\n4max{1,\u03c3FN}\n] , since 4 max{1, \u03c3Fn } \u2264 4 max{1, \u221a 16 \u2217 2k} <\n23 \u221a k), we get\nrN = 736 ( (k \u2212 \u2016V >k Wt\u20162F ) + (k \u2212 \u2016V >k W\u0303s\u22121\u20162F ) )( 1 + 8\n3\n( 1\n4 4 + 2 )2) \u2264 18400 ( (k \u2212 \u2016V >k Wt\u20162F ) + (k \u2212 \u2016V >k W\u0303s\u22121\u20162F ) ) .\nThis implies that rN \u2264 36800k always, which by application of Lemma 10, gives the first part of our lemma. As to the second part, assuming \u2016V >k Wt\u20162F \u2265 k \u2212 1 2 and applying Lemma 10, we get that\nE [ k \u2212 \u2016V >k Wt+1\u20162F ] \u2264 ( k \u2212 \u2016V >k Wt\u20162F )( 1\u2212 1\n10 \u03b7\u03bb ) + 18400 \u03b72 ( (k \u2212 \u2016V >k Wt\u20162F ) + (k \u2212 \u2016V >k W\u0303s\u22121\u20162F )\n) = ( k \u2212 \u2016V >k Wt\u20162F )( 1\u2212 \u03b7 ( 1\n10 \u03bb\u2212 18400\u03b7 )) + 18400 \u03b72(k \u2212 \u2016V >k W\u0303s\u22121\u20162F ).\nThis corresponds to the lemma statement.\nPart II: Solving the Recurrence Relation for a Single Epoch\nSince we focus on a single epoch, we drop the subscript from W\u0303s\u22121 and denote it simply as W\u0303 . Suppose that \u03b7 = \u03b1\u03bb, where \u03b1 is a sufficiently small constant to be chosen later. Also, let\nbt = k \u2212 \u2016V >k Wt\u20162F and b\u0303 = k \u2212 \u2016V >k W\u0303\u20162F .\nThen Lemma 11 tells us that if \u03b1 is a sufficiently small constant, bt \u2264 12 , then\nE [bt+1|Wt] \u2264 ( 1\u2212 c\u03b1\u03bb2 ) bt + c \u2032\u03b12\u03bb2b\u0303 (18)\nfor some numerical constants c, c\u2032.\nLemma 12. Let B be the event that bt \u2264 12 for all t = 0, 1, 2, . . . ,m. Then for certain positive numerical constants c1, c2, c3, if \u03b1 \u2264 c1, then\nE[bm|B] \u2264 (( 1\u2212 c2\u03b1\u03bb2 )m + c3\u03b1 ) b\u0303,\nwhere the expectation is over the randomness in the current epoch.\nProof. Recall that bt is a deterministic function of the random variable Wt, which depends in turn on Wt\u22121 and the random instance chosen at round t. We assume that W0 (and hence b\u0303) are fixed, and consider how bt evolves as a function of t. Using Eq. (18), we have\nE[bt+1|Wt, B] = E [ bt+1|Wt, bt+1 \u2264 1\n2\n] \u2264 E[bt+1|Wt] \u2264 ( 1\u2212 c\u03b1\u03bb2 ) bt + c \u2032\u03b12\u03bb2b\u0303.\nNote that the first equality holds, since conditioned on Wt, bt+1 is independent of b1, . . . , bt, so the event B is equivalent to just requiring bt+1 \u2264 1/2.\nTaking expectation over Wt (conditioned on B), we get that E[bt+1|B] \u2264 E [( 1\u2212 c\u03b1\u03bb2 ) bt + c \u2032\u03b12\u03bb2b\u0303 \u2223\u2223\u2223B]\n= ( 1\u2212 c\u03b1\u03bb2 ) E [bt|B] + c\u2032\u03b12\u03bb2b\u0303.\nUnwinding the recursion, and using that b0 = b\u0303, we therefore get that\nE[bm|B] \u2264 ( 1\u2212 c\u03b1\u03bb2 )m b\u0303+ c\u2032\u03b12\u03bb2b\u0303 m\u22121\u2211 i=0 ( 1\u2212 c\u03b1\u03bb2 )i \u2264 ( 1\u2212 c\u03b1\u03bb2 )m b\u0303+ c\u2032\u03b12\u03bb2b\u0303\n\u221e\u2211 i=0 ( 1\u2212 c\u03b1\u03bb2 )i = ( 1\u2212 c\u03b1\u03bb2 )m b\u0303+ c\u2032\u03b12\u03bb2b\u0303 1\nc\u03b1\u03bb2\n= (( 1\u2212 c\u03b1\u03bb2 )m + c\u2032\nc \u03b1\n) b\u0303.\nas required.\nWe now turn to prove that the event B assumed in Lemma 12 indeed holds with high probability:\nLemma 13. The following holds for certain positive numerical constants c1, c2, c3: If \u03b1 \u2264 c1, then for any \u03b2 \u2208 (0, 1) and m, if\nb\u0303+ c2km\u03b1 2\u03bb2 + c3k \u221a m\u03b12\u03bb2 log(1/\u03b2) \u2264 1\n2 , (19)\nthen it holds with probability at least 1\u2212 \u03b2 that\nbt \u2264 b\u0303+ c2km\u03b12\u03bb2 + c3k \u221a m\u03b12\u03bb2 log(1/\u03b2) \u2264 1\n2\nfor all t = 0, 1, 2, . . . ,m.\nProof. To prove the lemma, we analyze the stochastic process b0(= b\u0303), b1, b2, . . . , bm, and use a concentration of measure argument. First, we collect the following facts:\n\u2022 b\u0303 = b0 \u2264 12 : This directly follows from the assumption stated in the lemma.\n\u2022 As long as bt \u2264 12 , E [bt+1|Wt] \u2264 bt + c2\u03b1 2\u03bb2b\u0303 for some constant c2: Supposing \u03b1 is sufficiently\nsmall, then by Eq. (18), E [bt+1|Wt] \u2264 ( 1\u2212 c\u03b1\u03bb2 ) bt + c \u2032\u03b12\u03bb2b\u0303 \u2264 bt + c\u2032\u03b12\u03bb2b\u0303.\n\u2022 |bt+1 \u2212 bt| is bounded by c\u20323k\u03b1\u03bb for some constant c\u20323: Applying Lemma 9, and assuming that \u03b1 is at most some sufficiently small constant c1 (e.g. \u03b1 \u2264 112 , so \u03b7 = \u03b1\u03bb \u2264 1 12 ),\n|bt+1 \u2212 bt| = \u2223\u2223\u2223\u2016V >k Wt+1\u20162F \u2212 \u2016V >k Wt\u20162F \u2223\u2223\u2223 \u2264 12k\u03b71\u2212 3\u03b7 \u2264 12k\u03b1\u03bb3/4 = 16k\u03b1\u03bb.\nArmed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows that with probability at least 1\u2212\u03b2, it holds simultaneously for all t = 1, . . . ,m (and for t = 0 by assumption) that\nbt \u2264 b\u0303+ c2m\u03b12\u03bb2b\u0303+ c3k \u221a m\u03b12\u03bb2 log(1/\u03b2)\nfor some constants c2, c3, as long as the expression above is less than 12 . If the expression is indeed less than 1 2 , then we get that bt \u2264 1 2 for all t. Upper bounding b\u0303 by k and slightly simplifying, we get the statement in the lemma.\nCombining Lemma 12 and Lemma 13, and using Markov\u2019s inequality, we get the following corollary:\nLemma 14. Let confidence parameters \u03b2, \u03b3 \u2208 (0, 1) be fixed. Suppose that m,\u03b1 are chosen such that \u03b1 \u2264 c1 and\nb\u0303+ c2km\u03b1 2\u03bb2 + c3k \u221a m\u03b12\u03bb2 log(1/\u03b2) \u2264 1\n2 ,\nwhere c1, c2, c3 are certain positive numerical constants. Then with probability at least 1\u2212 (\u03b2+\u03b3), it holds that\nbm \u2264 1\n\u03b3\n(( 1\u2212 c\u03b1\u03bb2 )m + c\u2032\u03b1 ) b\u0303.\nfor some positive numerical constants c, c\u2032.\nPart III: Analyzing the Entire Algorithm\u2019s Run\nGiven the analysis in Lemma 14 for a single epoch, we are now ready to prove our theorem. Let\nb\u0303s = k \u2212 \u2016V >k W\u0303s\u20162F .\nBy assumption, at the beginning of the first epoch, we have b\u03030 = k \u2212 \u2016V >k W\u03030\u20162F \u2264 1 2 . Therefore, by Lemma 14, for any \u03b2, \u03b3 \u2208 ( 0, 12 ) , if we pick any\n\u03b1 \u2264 min { c1, 1 2c\u2032 \u03b32 } and m \u2265 3 log(1/\u03b3) c\u03b1\u03bb2 such that 1 2 +c2km\u03b1 2\u03bb2+c3k \u221a m\u03b12\u03bb2 log(1/\u03b2) \u2264 1 2 , (20) then we get with probability at least 1\u2212 (\u03b2 + \u03b3) that\nbm \u2264 1\n\u03b3\n(( 1\u2212 c\u03b1\u03bb2 ) 3 log(1/\u03b3) c\u03b1\u03bb2 + 1 2 \u03b32 ) b\u03030\nUsing the inequality (1 \u2212 (1/x))ax \u2264 exp(\u2212a), which holds for any x > 1 and any a, and taking x = 1/(c\u03b1\u03bb2) and a = 3 log(1/\u03b3), we can upper bound the above by\n1\n\u03b3\n( exp ( \u22123 log ( 1\n\u03b3\n)) + 1 2 \u03b32 ) b\u03030\n= 1\n\u03b3\n( \u03b33 + 1 2 \u03b32 ) b\u03030 \u2264 \u03b3b\u03030.\nSince bm equals the starting point b\u03031 for the next epoch, we get that b\u03031 \u2264 \u03b3b\u03030 \u2264 \u03b3 12 . Again applying Lemma 14, and performing the same calculation we have that with probability at least 1\u2212 (\u03b2 + \u03b3) over the next epoch, b\u03032 \u2264 \u03b3b\u03031 \u2264 \u03b32b\u03030. Repeatedly applying Lemma 14 and using a union bound, we get that after T epochs, with probability at least 1\u2212 T (\u03b2 + \u03b3),\nk \u2212 \u2016V >k W\u0303T \u20162F = b\u0303T \u2264 \u03b3T b\u03030 < \u03b3T .\nTherefore, for any desired accuracy parameter , we simply need to use T = \u2308 log(1/ ) log(1/\u03b3) \u2309 epochs, and get\nk \u2212 \u2016V >k W\u0303s\u20162F \u2264 with probability at least 1\u2212 T (\u03b2 + \u03b3) = 1\u2212 \u2308 log(1/ ) log(1/\u03b3) \u2309 (\u03b2 + \u03b3).\nUsing a confidence parameter \u03b4, we pick \u03b2 = \u03b3 = \u03b42 , which ensures that the accuracy bound above holds with probability at least\n1\u2212 \u2308 log(1/ )\nlog(2/\u03b4)\n\u2309 \u03b4 \u2265 1\u2212 \u2308 log(1/ )\nlog(2)\n\u2309 \u03b4 = 1\u2212 \u2308 log2 ( 1 )\u2309 \u03b4.\nSubstituting this choice of \u03b2, \u03b3 into Eq. (20), and recalling that the step size \u03b7 equals \u03b1\u03bb, we get that k \u2212 \u2016V >k W\u0303T \u20162F \u2264 with probability at least 1\u2212 dlog2(1/ )e\u03b4, provided that\n\u03b7 \u2264 c\u03b42\u03bb , m \u2265 c \u2032 log(2/\u03b4)\n\u03b7\u03bb , km\u03b72 + k\n\u221a m\u03b72 log(2/\u03b4) \u2264 c\u2032\u2032\nfor suitable positive constants c, c\u2032, c\u2032\u2032. To get the theorem statement, recall that the analysis we performed pertains to data whose squared norm is bounded by 1. By the reduction discussed at the beginning of the proof, we can apply it to data with squared norm at most r, by replacing \u03bb with \u03bb/r, and \u03b7 with \u03b7r, leading to the condition\n\u03b7 \u2264 c\u03b4 2 r2 \u03bb , m \u2265 c \u2032 log(2/\u03b4) \u03b7\u03bb , km\u03b72r2 + rk\n\u221a m\u03b72 log(2/\u03b4) \u2264 c\u2032\u2032\nand establishing the theorem."}, {"heading": "6.2 Proof of Theorem 2", "text": "The proof relies mainly on the techniques and lemmas of Section 6.1, used to prove Theorem 1. As done in Section 6.1, we will assume without loss of generality that r = maxi \u2016xi\u20162 is at most 1, and then transform the bound to a bound for general r (see the discussion at the beginning of Subsection 6.1.2)\nFirst, we extract the following result, which is essentially the first part of Lemma 11 (for k = 1): Lemma 15. Let A,wt be as defined in Algorithm 1, and suppose that \u03b7 \u2208 [ 0, 123 ] . Then\nEit [ 1\u2212 \u3008v1,wt+1\u30092 \u2223\u2223wt, w\u0303s\u22121] \u2264 (1\u2212 c\u03b7\u03bb\u3008v1,wt\u30092) (1\u2212 \u3008v1,wt\u30092)+ c\u2032\u03b72, for some positive numerical constants c, c\u2032.\nNote that this bound holds regardless of what is w\u0303s\u22121, and in particular holds across different epochs of Algorithm 1. Therefore, it is enough to show that starting from some initial point w0, after sufficiently many stochastic updates as specified in line 6-10 of the algorithm (or in terms of the analysis, sufficiently many applications of Lemma 15), we end up with a point wT for which 1 \u2212 \u3008v1,wT \u3009 \u2264 12 , as required.\nNote that to simplify the notation, we will use here a single running index w0,w1,w2, . . . ,wT (whereas in the algorithm we restarted the indexing after every epoch).\nThe proof is based on martingale arguments, quite similar to the ones in Subsection 6.1.2 but with slight changes. First, we let\nbt = 1\u2212 \u3008v1,wt\u30092\nto simplify notation. We note that b0 = 1 \u2212 \u3008v1,w0\u30092 is assumed fixed, whereas b1, b2, . . . are random variables based on the sampling process. Lemma 11 tells us that if \u03b7 is sufficiently small, and bt \u2264 1\u2212 \u03be for some \u03be \u2208 (0, 1), then\nE [bt+1|bt] \u2264 (1\u2212 c\u03b7\u03bb\u03be) bt + c\u2032\u03b72. (21)\nfor some numerical constants c, c\u2032.\nLemma 16. Let B be the event that bt \u2264 1\u2212 \u03be for all t = 0, 1, . . . , T . Then for certain positive numerical constants c1, c2, c3, if \u03b7 \u2264 c1\u03bb, then\nE[bT |B] \u2264 ( (1\u2212 c2\u03b7\u03bb\u03be)T + c3 \u03b7\n\u03bb\u03be\n) .\nProof. Using Eq. (21), we have for any bt satisfying event B that\nE[bt+1|bt, B] = E [bt+1|bt, bt+1 \u2264 1\u2212 \u03be] \u2264 E[bt+1|bt] \u2264 (1\u2212 c\u03b7\u03bb\u03be) bt + c\u2032\u03b72.\nTaking expectation over bt (conditioned on B), we get that E[bt+1|B] \u2264 E [ (1\u2212 c\u03b7\u03bb\u03be) bt + c\u2032\u03b72 \u2223\u2223B] = (1\u2212 c\u03b7\u03bb\u03be)E [bt|B] + c\u2032\u03b72.\nUnwinding the recursion, we get\nE[bT |B] \u2264 (1\u2212 c\u03b7\u03bb\u03be)T b0 + c\u2032\u03b72 T\u22121\u2211 i=0 (1\u2212 c\u03b7\u03bb\u03be)i\n\u2264 (1\u2212 c\u03b7\u03bb\u03be)T + c\u2032\u03b72 \u221e\u2211 i=0 (1\u2212 c\u03b7\u03bb\u03be)i\n= (1\u2212 c\u03b7\u03bb\u03be)T + c\u2032\u03b72 1 c\u03b7\u03bb\u03be\n\u2264 (1\u2212 c\u03b7\u03bb\u03be)T + c \u2032\nc\n\u03b7\n\u03bb\u03be .\nWe now turn to prove that the event B assumed in Lemma 12 indeed holds with high probability:\nLemma 17. The following holds for certain positive numerical constants c1, c2, c3: If \u03b7 \u2264 c1\u03bb, then for any \u03b2 \u2208 (0, 1), if\nb0 + c2T\u03b7 2 + c3 \u221a T\u03b72 log(1/\u03b2) \u2264 1\u2212 \u03be, (22)\nthen it holds with probability at least 1\u2212 \u03b2 that bt \u2264 b0 + c2T\u03b72 + c3 \u221a T\u03b72 log(1/\u03b2) \u2264 1\u2212 \u03be\nfor all t = 0, 1, . . . , T .\nProof. To prove the lemma, we analyze the stochastic process b1, b2, . . . , bT , and use a concentration of measure argument. First, we collect the following facts:\n\u2022 b0 \u2264 1\u2212 \u03be: This directly follows from the assumption stated in the lemma.\n\u2022 E [bt+1|bt] \u2264 bt + c\u2032\u03b72 for some constant c\u2032: By Eq. (21),\nE [bt+1|Wt] \u2264 (1\u2212 c\u03b7\u03bb\u03be) bt + c\u2032\u03b72 \u2264 bt + c\u2032\u03b72.\n\u2022 |bt+1\u2212bt| is bounded by c\u03b7 for some constant c: Applying Lemma 9 for the case k = 1, and assuming \u03b7 \u2264 1/12,\n|bt+1 \u2212 bt| = \u2223\u2223\u3008v1,wt+1\u30092 \u2212 \u3008v,wt\u30092\u2223\u2223 \u2264 12\u03b7\n1\u2212 3\u03b7 \u2264 12\u03b7 3/4 = 16\u03b7.\nArmed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows that with probability at least 1\u2212 \u03b2, it holds simultaneously for all t = 0, 1, . . . , T that\nbt \u2264 b0 + c2T\u03b72 + c3 \u221a T\u03b72 log(1/\u03b2)\nfor some constants c2, c3. If the expression is indeed less than 1 \u2212 \u03be, then we get that bt \u2264 1 \u2212 \u03be for all t, from which the lemma follows.\nCombining Lemma 16 and Lemma 17, and using Markov\u2019s inequality, we get the following corollary:\nLemma 18. Let confidence parameters \u03b2, \u03b3 \u2208 (0, 1) be fixed. Then for some positive numerical constants c1, c2, c3, c, c \u2032, if \u03b7 \u2264 c1\u03bb and\nb0 + c2T\u03b7 2 + c3 \u221a T\u03b72 log(1/\u03b2) \u2264 1\u2212 \u03be,\nthen with probability at least 1\u2212 (\u03b2 + \u03b3), it holds that\nbT \u2264 1\n\u03b3\n( (1\u2212 c\u03b7\u03bb\u03be)T + c\u2032 \u03b7\n\u03bb\u03be\n) .\nWe are now ready to prove our theorem. By Lemma 18, for any \u03b2, \u03b3 \u2208 ( 0, 12 ) and any\n\u03b7 \u2264 min { c1, 1 2c\u2032 \u03b32 } \u03bb\u03be and T \u2265 3 log(1/\u03b3) c\u03b7\u03bb\u03be\nsuch that b0 + c2T\u03b72 + c3 \u221a T\u03b72 log(1/\u03b2) \u2264 1\u2212 \u03be, (23)\nwe get with probability at least 1\u2212 (\u03b2 + \u03b3) that\nbT \u2264 1\n\u03b3\n( (1\u2212 c\u03b7\u03bb\u03be) 3 log(1/\u03b3) c\u03b7\u03bb\u03be + 1 2 \u03b32 ) .\nUsing the inequality (1 \u2212 (1/x))ax \u2264 exp(\u2212a), which holds for any x > 1 and any a, and taking x = 1/(c\u03b7\u03bb\u03be) and a = 3 log(1/\u03b3), we can upper bound the above by\n1\n\u03b3\n( exp ( \u22123 log ( 1\n\u03b3\n)) + 1 2 \u03b32 ) = 1 \u03b3 ( \u03b33 + 1 2 \u03b32 ) ,\nand since we assume \u03b3 < 12 , this is at most 1 2 . Overall, we got that with probability at least 1 \u2212 \u03b2 \u2212 \u03b3, bT \u2264 12 , and therefore 1\u2212 \u3008v1,wT \u3009 2 \u2264 12 as required.\nIt remains to show that the parameter choices in Eq. (23) can indeed be satisfied. First, we fix \u03be = 12\u03b6 (where we recall that 0 < \u03b6 \u2264 \u3008v1,w0\u30092), which trivially ensures that b0 = 1\u2212\u3008v1,w0\u30092 is at most 1\u2212 2\u03be. Moreover, suppose we pick \u03b2 = \u03b3 in (0, exp(\u22121)), and \u03b7, T so that\n\u03b7 \u2264 c\u2217\u03b3 2\u03bb\u03be3\nlog2(1/\u03b3) , T =\n\u230a 3 log(1/\u03b3)\nc\u2032\u2217\u03b7\u03bb\u03be\n\u230b , (24)\nwhere c\u2217, c\u2032\u2217 are sufficiently small constants so that the bounds on \u03b7, T in Eq. (23) are satisfied. This implies that the third bound in Eq. (23) is also satisfied, since by plugging in the values / bounds of T and \u03b7, and using the assumptions \u03b3 = \u03b2 \u2264 exp(\u22121) and \u03be \u2264 1, we have\nb0 + c2T\u03b7 2 + c3 \u221a T\u03b72 log(1/\u03b3)\n\u2264 1\u2212 2\u03be + c2 3 log(1/\u03b3)\nc\u2032\u2217\u03bb\u03be \u03b7 + c3\n\u221a 3 log(1/\u03b3)\nc\u2032\u2217\u03bb\u03be \u03b7 log(1/\u03b3)\n\u2264 1\u2212 2\u03be + c2 3c\u2217\u03b3\n2\u03be2\nc\u2032\u2217 log(1/\u03b3) + c3\n\u221a 3c\u2217\u03b32\u03be2\nc\u2032\u2217\n\u2264 1\u2212 2\u03be + ( 3c2c\u2217 c\u2032\u2217 + c3 \u221a 3c\u2217 c\u2032\u2217 ) \u03be,\nwhich is less than 1\u2212 \u03be if we pick c\u2217 sufficiently small compared to c\u2032\u2217. To summarize, we get that for any \u03b3 \u2208 (0, exp(\u22121)), by picking \u03b7 as in Eq. (24), we have that after T iterations (where T is specified in Eq. (24)), with probability at least 1 \u2212 2\u03b3, we get wT such that 1\u2212 \u3008v1,wT \u3009 \u2264 12 . Substituting \u03b4 = 2\u03b3 and \u03b6 = 2\u03be, we get that if\n\u3008v1, w\u03030\u30092 \u2265 \u03b6 > 0,\nand \u03b7 satisfies\n\u03b7 \u2264 c1\u03b4 2\u03bb\u03b63\nlog2(2/\u03b4)\n(for some universal constant c1), then with probability at least 1\u2212 \u03b4, after\nT =\n\u230a c2 log(2/\u03b4)\n\u03b7\u03bb\u03b6\n\u230b .\nstochastic iterations, we get a satisfactory point wT . As discussed at the beginning of the proof, this analysis is valid assuming r = maxi \u2016xi\u20162 \u2264 1. By the reduction discussed at the beginning of Subsection 6.1.2, we can get an analysis for any r by substituting \u03bb\u2192 \u03bb/r and \u03b7 \u2192 \u03b7r. This means that we should pick \u03b7 satisfying\n\u03b7r \u2264 c1\u03b4 2(\u03bb/r)\u03b63 log2(2/\u03b4) \u21d2 \u03b7 \u2264 c1\u03b4 2\u03bb\u03b63 r2 log2(2/\u03b4) ,\nand getting the required point after\nT =\n\u230a c2 log(2/\u03b4)\n(\u03b7r)(\u03bb/r)\u03b6\n\u230b = \u230a c2 log(2/\u03b4)\n\u03b7\u03bb\u03b6 \u230b iterations."}, {"heading": "6.3 Proof of Theorem 4", "text": "For simplicity of notation, we drop the A subscript from FA, and refer simply to F . We first prove the following two auxiliary lemmas:\nLemma 19. If A is a symmetric matrix, then the gradient of the function F (w) = \u2212w>Aw\u2016w\u20162 at some w equals\n\u2212 2 \u2016w\u20162 (F (w)I +A)w,\nand its Hessian equals\n\u2212 1 \u2016w\u20162 (( I \u2212 4 \u2016w\u20162 ww> )( F (w)I +A ))\u22a5 ,\nwhere B\u22a5 = B +B> (i.e., a matrix B plus its transpose).\nProof. By the product and chain rules (using the fact that 1\u2016w\u20162 is a composition of w 7\u2192 \u2016w\u2016 2 and z 7\u2192 1z ), the gradient of F (w) = \u2212 1\u2016w\u20162 ( w>Aw ) equals\nw 2 \u2016w\u20164 ( w>Aw ) \u2212 (Aw) 2 \u2016w\u20162 , (25)\ngiving the gradient bound in the lemma statement after a few simplifications. Differentiating the vector-valued Eq. (25) with respect to w (using the product and chain rules, and the fact that 1\u2016w\u20164 is a composition of w 7\u2192 \u2016w\u2016 2, z 7\u2192 z2, and z 7\u2192 1z ), we get that the Hessian of F equals\nI 2\n\u2016w\u20164 (w>Aw) + w ( \u2212 2 \u2016w\u20168 \u2217 2\u2016w\u20162 \u2217 2w )> ( w>Aw ) + w 2 \u2016w4\u2016 (2Aw)>\n\u2212A 2 \u2016w\u20162 \u2212 (Aw) ( \u2212 2 \u2016w\u20164 \u2217 2w )>\n= \u2212 2F (w) \u2016w\u20162 I + 8F (w) \u2016w\u20164 ww> +\n4 \u2016w\u20164 ww>A \u2212 2 \u2016w\u20162 A +\n4\n\u2016w\u20164 Aww>\n= \u2212 1 \u2016w\u20162\n( 2F (w)I \u2212 8F (w)\n\u2016w\u20162 ww> \u2212 4 \u2016w\u20162 ww>A+ 2A\u2212 4 \u2016w\u20162 Aww>\n) ,\nwhich can be verified to equal the expression in the lemma statement (using the fact that A,ww> and I are all symmetric matrices, hence equal their transpose).\nLemma 20. Let w0,v1 be two unit vectors such that \u2016w0 \u2212 v1\u2016 \u2264 < 12 (which implies \u3008w0,v1\u3009 > 0). Let v\u20321 be the intersection of the ray {av1 : a \u2265 0} with the hyperplane Hw0 = {w : \u3008w,w0\u3009 = 1}. Then \u2016v\u20321 \u2212w0\u2016 \u2264 54 .\nProof. See Figure 2 in the main text for a graphical illustration. Letting v\u20321 = av, a must satisfy \u3008av1,w0\u3009 = 1. Since v1,w0 are unit vectors, this implies\na = 1\n\u3008v1,w0\u3009 =\n2\n2\u2212 \u2016v1 \u2212w0\u20162 ,\nand since \u2016v1 \u2212w0\u2016 \u2264 , this means that a \u2208 [ 1, 2\n2\u2212 2\n] .\nTherefore,\n\u2016v\u20321\u2212w0\u2016 \u2264 \u2016v1\u2212w0\u2016+\u2016v\u20321\u2212v1\u2016 \u2264 +\u2016av1\u2212v1\u2016 \u2264 + |a\u22121| \u2264 + 2\n2\u2212 2 \u22121 = +\n2\n2 + 2 ,\nand since < 12 , this is at most 5 4 .\nWe now turn to prove the theorem. Let\u22072(w) denote the Hessian at some point w. To show smoothness and strong convexity as stated in the theorem, it is enough to fix some unit w0 which is -close to the leading eigenvector v1 (where is assumed to be sufficiently small), and show that for any point w on Hw0 which is O( ) close to w0, and any direction g along Hw0 (i.e. any unit g such that \u3008g,w0\u3009 = 0), it holds that g>\u22072(w)g \u2208 [\u03bb, 20]. This implies that the second derivative in an O( ) neighborhood of w0 on Hw0 is always in [\u03bb, 20], hence the function is both \u03bb-strongly convex in that neighborhood.\nMore formally, letting \u2208 (0, 1) be a small parameter to be chosen later, consider any w0 such that\n\u2016w0\u2016 = 1 , \u2016w0 \u2212 v1\u2016 \u2264 ,\nany w such that \u3008w \u2212w0,w0\u3009 = 0 , \u2016w \u2212w0\u2016 \u2264 2 ,\nand any g such that \u2016g\u2016 = 1 , \u3008g,w0\u3009 = 0.\nOur goal is to show that for an appropriate , we have g>\u22072(w)g \u2208 [\u03bb, 20]. Moreover, by Lemma 20, the neighborhood set Hw0 \u2229Bw0(2 ) would also contain a point av1 for some a, which is a global optimum of F due to its scale-invariance. This would establish the theorem.\nThe easier part is to show the upper bound on g>\u22072(w)g. Since g is a unit vector, it is enough to bound the spectral norm of\u22072(w), which equals\u2225\u2225\u2225\u2225\u2225 1\u2016w\u20162 (( I \u2212 4 \u2016w\u20162 ww> )( F (w)I +A ))\u22a5\u2225\u2225\u2225\u2225\u2225 sp\n\u2264 2 \u2016w\u20162 \u2225\u2225\u2225\u2225(I \u2212 4\u2016w\u20162ww> )( F (w)I +A )\u2225\u2225\u2225\u2225 sp\n\u2264 2 \u2016w\u20162 \u2225\u2225\u2225\u2225I \u2212 4\u2016w\u20162ww> \u2225\u2225\u2225\u2225 sp \u2016F (w)I +A\u2016sp\n\u2264 2 \u2016w\u20162\n( \u2016I\u2016sp + \u2225\u2225\u2225\u2225 4\u2016w\u20162ww> \u2225\u2225\u2225\u2225 sp ) (\u2016F (w)I\u2016sp + \u2016A\u2016sp) .\nSince the spectral norm of A is 1, and \u2016w\u20162 \u2265 1 (as w lies on a hyperplane Hw0 tangent to a unit vector w0), it is easy to verify that this is at most 2(1 + 4)(1 + 1) = 20 as required.\nWe now turn to lower bound g>\u22072(w)g, which by Lemma 19 equals\n\u2212 1 \u2016w\u20162\ng> ((\nI \u2212 4 \u2016w\u20162\nww> )( F (w)I +A ))\u22a5 g.\nSince g>B\u22a5g = g>Bg + g>B>g = 2g>Bg, the above equals\n\u2212 2 \u2016w\u20162 g> ( I \u2212 4 \u2016w\u20162 ww> )( F (w)I +A ) g. (26)\nUsing the fact that w = w0 + (w \u2212w0), and \u3008g,w0\u3009 = 0, we get that \u3008g,w\u3009 = \u3008g,w \u2212w0\u3009. Moreover, sinceA is positive semidefinite and has spectral norm of 1, F (w) = \u2212w>Aw\u2016w\u20162 \u2208 [\u22121, 0]. Expanding Eq. (26) and plugging these in, we get\n\u2212 2 \u2016w\u20162\n( F (w)g> ( I \u2212 4 \u2016w\u20162 ww> ) g + g> ( I \u2212 4 \u2016w\u20162 ww> ) Ag ) = 2\n\u2016w\u20162\n( \u2212F (w)\u2016g\u20162 + 4F (w)\n\u2016w\u20162 \u3008g,w \u2212w0\u30092 \u2212 g>Ag +\n4\n\u2016w\u20162 \u3008g,w \u2212w0\u3009w>Ag ) \u2265 2 \u2016w\u20162 ( \u2212F (w)\u2016g\u20162 \u2212 4 \u2016w\u20162 \u2016g\u20162\u2016w \u2212w0\u20162 \u2212 g>Ag \u2212 4 \u2016w\u20162 \u2016g\u2016\u2016w \u2212w0\u2016\u2016w\u2016\u2016A\u2016sp\u2016g\u2016 ) .\nSince \u2016g\u2016 = 1, \u2016A\u2016sp = 1, \u2016w \u2212w0\u2016 \u2264 2 , and \u2016w\u20162 = \u2016w0\u20162 + \u2016w \u2212w0\u20162 is between 1 and 1 + 4 2, this is at least\n2 \u2016w\u20162 ( (\u2212F (w))\u2212 16 2 \u2212 g>Ag \u2212 8 \u221a 1 + 4 2 ) = 2 \u2016w\u20162 ( \u2212F (w)\u2212 g>Ag \u2212 8 ( 2 + \u221a 1 + 4 2 )) .\n(27) Let us now analyze \u2212F (w) and g>Ag more carefully. The idea will be to show that since we are close to the optimum, \u2212F (w) is very close to 1, and g (which is orthogonal to the near-optimal w0) is such that g>Ag is strictly smaller than 1. This would give us a positive lower bound on Eq. (27).\n\u2022 By the triangle inequality and the assumptions \u2016w0\u2212v1\u2016 \u2264 , \u2016w\u2212w0\u2016 \u2264 2 , we have \u2016w\u2212v1\u2016 \u2264 3 . Also, we claim that F (\u00b7) is 4-Lipschitz outside the unit Euclidean ball (since the gradient of F at any point with norm \u2265 1, according to Lemma 19, has norm at most 4). Therefore, |F (w) + 1| = |F (w)\u2212 F (v1)| \u2264 4\u2016w \u2212 v1\u2016 \u2264 12 , so overall,\nF (w) \u2264 \u22121 + 12 . (28)\n\u2022 Since \u3008w0,g\u3009 = 0, and \u2016w0 \u2212 v1\u2016 \u2264 , it follows that\n|\u3008v1,g\u3009| \u2264 |\u3008v1 \u2212w0,g\u3009|+ |\u3008w0,g\u3009| \u2264 \u2016v1 \u2212w0\u2016\u2016g\u2016+ 0 \u2264 .\nLetting v1, . . . ,vd and 1 = s1 > s2 \u2265 .. \u2265 sd \u2265 0 be the eigenvectors and eigenvalues of A in decreasing order (and recalling that s2 \u2264 s1 \u2212 \u03bb = 1\u2212 \u03bb for some eigengap \u03bb > 0), we get\ng>Ag = d\u2211 i=1 si\u3008vi,g\u30092 \u2264 \u3008v1,g\u30092 + (1\u2212 \u03bb) d\u2211 i=1 \u3008vi,g\u30092\n= \u3008v1,g\u30092 + (1\u2212 \u03bb)(1\u2212 \u3008v1,g\u30092) = \u03bb\u3008v1,g\u30092 + (1\u2212 \u03bb) \u2264 \u03bb 2 + (1\u2212 \u03bb) = 1\u2212 (1\u2212 2)\u03bb. (29)\nPlugging Eq. (28) and Eq. (29) back into Eq. (27), we get a lower bound of\n2 \u2016w\u20162 ( 1\u2212 12 \u2212 ( 1\u2212 (1\u2212 2)\u03bb ) \u2212 8 ( 2 + \u221a 1 + 4 2 )) = 2 \u2016w\u20162 ( (1\u2212 2)\u03bb\u2212 8 ( 1.5 + 2 + \u221a 1 + 4 2 ))\n= 2\n\u2016w\u20162\n1\u2212 2 \u2212 8 ( 1.5 + 2 + \u221a 1 + 4 2 )\n\u03bb \u03bb. Using the fact that \u221a 1 + z2 \u2264 1 + z, this can be loosely lower bounded by\n2\n\u2016w\u20162\n( 1\u2212 \u2212 8 (2.5 + 4 )\n\u03bb\n) \u03bb.\nRecalling that \u2016w\u20162 = \u2016w0\u20162 +\u2016w\u2212w0\u20162 is at most 1+4 2, and picking sufficiently small compared to \u03bb, (say = \u03bb/44), we get that the above is at least \u03bb, which implies the required strong convexity condition.\nTo summarize, by picking = \u03bb/44, we have shown that the function F (w) is \u03bb-strongly convex and 20-smooth in a neighborhood of size 2 = \u03bb22 around w0 on the hyperplaneHw0 , provided that \u2016w0\u2212v1\u2016 \u2264 = \u03bb44 . By Lemma 20, we are guaranteed that this neighborhood contains v1 up to some rescaling (which is immaterial for our scale-invariant function F ), hence by optimizing F in that neighborhood, we will get a globally optimal solution."}, {"heading": "Acknowledgments", "text": "This research is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel Science Foundation grant 425/13."}], "references": [{"title": "A lower bound for the optimization of finite sums", "author": ["A. Agarwal", "L. Bottou"], "venue": "ICML", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "2012 50th Annual Allerton Conference on Communication, Control, and Computing", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic optimization of PCA with capped MSG", "author": ["R. Arora", "A. Cotter", "N. Srebro"], "venue": "NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "The fast convergence of incremental PCA", "author": ["A. Balsubramani", "S. Dasgupta", "Y. Freund"], "venue": "NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge university press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Global convergence of stochastic gradient descent for some nonconvex matrix problems", "author": ["C. De Sa", "K. Olukotun", "C. R\u00e9"], "venue": "ICML", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Matrix computations", "author": ["G. H Golub", "C. Van Loan"], "venue": "volume 3. John Hopkins University Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["N. Halko", "P. Martinsson", "J. Tropp"], "venue": "SIAM review, 53(2):217\u2013288", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "The noisy power method: A meta algorithm with applications", "author": ["M. Hardt", "E. Price"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American statistical association, 58(301):13\u201330", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1963}, {"title": "Matrix analysis", "author": ["R. Horn", "C. Johnson"], "venue": "Cambridge university press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "The method of stochastic approximation for the determination of the least eigenvalue of a symmetrical matrix", "author": ["T.P. Krasulina"], "venue": "USSR Computational Mathematics and Mathematical Physics, 9(6):189\u2013195", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1969}, {"title": "Estimating the largest eigenvalue by the power and lanczos algorithms with a random start", "author": ["J. Kuczynski", "H. Wozniakowski"], "venue": "SIAM journal on matrix analysis and applications, 13(4):1094\u20131122", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "Stochastic proximal gradient descent with acceleration techniques", "author": ["A. Nitanda"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Simplified neuron model as a principal component analyzer", "author": ["E. Oja"], "venue": "Journal of mathematical biology, 15(3):267\u2013273", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1982}, {"title": "Sampling from large matrices: An approach through geometric functional analysis", "author": ["M. Rudelson", "R. Vershynin"], "venue": "Journal of the ACM (JACM), 54(4):21", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "A stochastic PCA and SVD algorithm with an exponential convergence rate", "author": ["O. Shamir"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Sketching as a tool for numerical linear algebra", "author": ["D. Woodruff"], "venue": "Theoretical Computer Science, 10(1- 2):1\u2013157", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 17, "context": "Abstract We study the convergence properties of the VR-PCA algorithm introduced by [19] for fast computation of leading singular vectors.", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "the Lanczos method) [8].", "startOffset": 20, "endOffset": 23}, {"referenceID": 12, "context": "Such algorithms have been known for several decades ([14, 17]), and enjoyed renewed interest in recent years, e.", "startOffset": 53, "endOffset": 61}, {"referenceID": 15, "context": "Such algorithms have been known for several decades ([14, 17]), and enjoyed renewed interest in recent years, e.", "startOffset": 53, "endOffset": 61}, {"referenceID": 1, "context": "[2, 4, 3, 10, 6].", "startOffset": 0, "endOffset": 16}, {"referenceID": 3, "context": "[2, 4, 3, 10, 6].", "startOffset": 0, "endOffset": 16}, {"referenceID": 2, "context": "[2, 4, 3, 10, 6].", "startOffset": 0, "endOffset": 16}, {"referenceID": 8, "context": "[2, 4, 3, 10, 6].", "startOffset": 0, "endOffset": 16}, {"referenceID": 5, "context": "[2, 4, 3, 10, 6].", "startOffset": 0, "endOffset": 16}, {"referenceID": 7, "context": "[9, 20].", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "[9, 20].", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "Recently, [19] proposed a new practical algorithm, VR-PCA, for solving Eq.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "The algorithm is based on a recent variance-reduction technique designed to speed up stochastic algorithms for convex optimization problems ([13]), although the optimization problem in Eq.", "startOffset": 141, "endOffset": 145}, {"referenceID": 17, "context": "See Section 3 for a more detailed description of this algorithm, and [19] for more discussions as well as empirical results.", "startOffset": 69, "endOffset": 73}, {"referenceID": 17, "context": "The results and analysis in [19] left several issues open.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "Although [19] proposed a variant of the algorithm for that case, and studied it empirically, no analysis was provided.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "We begin by recalling the algorithm of [19] for the k = 1 case (Algorithm 1), and then discuss its generalization for k > 1.", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "We now turn to provide a formal analysis of Algorithm 2, which directly generalizes the analysis of Algorithm 1 given in [19]: Theorem 1.", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "This runtime bound is the same3 as that of [19] for k = 1.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "We note that although Bt\u22121 appears essential for our analysis, it isn\u2019t clear that using it is necessary in practice: In [19], the suggested block algorithm was Algorithm 2 with Bt\u22121 = I , which seemed to work well in experiments.", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "However, experimentally the algorithm seems to work well even with random initialization [19].", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W\u03030.", "startOffset": 171, "endOffset": 181}, {"referenceID": 8, "context": "Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W\u03030.", "startOffset": 171, "endOffset": 181}, {"referenceID": 3, "context": "Moreover, if we are interested in a theoretical guarantee, one simple solution is to warm-start the algorithm with a purely stochastic algorithm for this problem (such as [6, 10, 4]), with runtime guarantees on getting such a W\u03030.", "startOffset": 171, "endOffset": 181}, {"referenceID": 17, "context": "[19] showed that it\u2019s possible to further improve the runtime for sparse X , replacing d by the average column sparsity ds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18]) is a relaxation of the standard notion of rank: For any d \u00d7 d matrix A, nrank(A) is at most the rank of A (which in turn is at most d).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "For example, this holds for [6], although the bound only guarantees the existence of some iteration which produces the desired output.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "The guarantee of [4] scale as d/\u03bb, and the guarantee of [10] scales as d/\u03bb in our setting.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "The guarantee of [4] scale as d/\u03bb, and the guarantee of [10] scales as d/\u03bb in our setting.", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "[4, 10, 6]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 8, "context": "[4, 10, 6]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 5, "context": "[4, 10, 6]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 13, "context": "Indeed, when using deterministic methods such as power iterations or the Lanczos method, the dependence on \u03bb in the runtime is only 1/\u03bb or even \u221a 1/\u03bb [15].", "startOffset": 150, "endOffset": 154}, {"referenceID": 11, "context": "[13, 16, 7] in the context of the variance-reduction technique we use).", "startOffset": 0, "endOffset": 11}, {"referenceID": 14, "context": "[13, 16, 7] in the context of the variance-reduction technique we use).", "startOffset": 0, "endOffset": 11}, {"referenceID": 4, "context": "(1) can be \u201ctrivially\u201d convexified, by re-casting it as an equivalent semidefinite program [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "whereas the best known guarantees on getting an -optimal solution for \u03bb-strongly convex and smooth functions (see [1]) is on the order of", "startOffset": 114, "endOffset": 117}, {"referenceID": 17, "context": "1 Proof of Theorem 1 Although the proof structure generally mimics the proof of Theorem 1 in [19] for the k = 1 special case, it is more intricate and requires several new technical tools.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "Furthermore, suppose that for some fixed \u03b1, \u03b3, \u03b4 > 0, it holds with probability 1 that \u2022 For all \u03bd \u2208 [0, 1], B2 + \u03bdZ2 \u03b4I .", "startOffset": 101, "endOffset": 107}, {"referenceID": 0, "context": "f(\u03bd) = Tr ( (B1 + \u03bdZ1)(B2 + \u03bdZ2) \u22121) , \u03bd \u2208 [0, 1].", "startOffset": 43, "endOffset": 49}, {"referenceID": 6, "context": "[8]), and the solution is easily shown to be B = V U> where USV > is the SVD decomposition of C>D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Letting st, st\u22121 denote the vectors of singular values of V > k Wt and V > k Wt\u22121, and noting that they are both in [0, 1]k (as Vk,Wt\u22121,Wt all have orthonormal columns), the left hand side of the inequality in the lemma statement equals", "startOffset": 116, "endOffset": 122}, {"referenceID": 10, "context": "By Weyl\u2019s matrix perturbation theorem6 [12], this is upper bounded by 2k\u2016V > k Wt \u2212 V > k Wt\u22121\u2016sp \u2264 2k\u2016Vk\u2016sp\u2016Wt \u2212Wt\u22121\u2016sp \u2264 2k\u2016Wt \u2212Wt\u22121\u2016sp.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "\u2265 sd in [0, 1], and suppose that sk \u2212 sk+1 \u2265 \u03bb for some \u03bb > 0.", "startOffset": 8, "endOffset": 14}, {"referenceID": 0, "context": "\u2022 B2 + \u03bdZ2 3 8I for all \u03bd \u2208 [0, 1]: Recalling the definition of B2, Z2, and the facts that A 0, N>N 0 (by construction), and W>W = I , we have that B2 I .", "startOffset": 28, "endOffset": 34}, {"referenceID": 0, "context": "But each \u03c3i is in [0, 1] (as Vk,W have orthonormal columns), so no \u03c3i can be less than 12 .", "startOffset": 18, "endOffset": 24}, {"referenceID": 9, "context": "Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows that with probability at least 1\u2212\u03b2, it holds simultaneously for all t = 1, .", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "Let A,wt be as defined in Algorithm 1, and suppose that \u03b7 \u2208 [ 0, 1 23 ] .", "startOffset": 60, "endOffset": 71}, {"referenceID": 9, "context": "Armed with these facts, and using the maximal version of the Hoeffding-Azuma inequality [11], it follows that with probability at least 1\u2212 \u03b2, it holds simultaneously for all t = 0, 1, .", "startOffset": 88, "endOffset": 92}], "year": 2015, "abstractText": "We study the convergence properties of the VR-PCA algorithm introduced by [19] for fast computation of leading singular vectors. We prove several new results, including a formal analysis of a block version of the algorithm, and convergence from random initialization. We also make a few observations of independent interest, such as how pre-initializing with just a single exact power iteration can significantly improve the runtime of stochastic methods, and what are the convexity and non-convexity properties of the underlying optimization problem.", "creator": "LaTeX with hyperref package"}}}