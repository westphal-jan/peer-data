{"id": "1508.03790", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2015", "title": "Depth-Gated LSTM", "abstract": "in this additional short note, we present an extension proof of your lstm approach to use essentially a depth gate circuit to continuously connect memory master cells efficiently of adjacent layers. doing so introduces a linear dependence transformation between opposing lower and upper recurrent units. importantly, the second linear dependence module is gated through a voltage gating function, which ultimately we call forget one gate. thus this buffer gate is a function of lower layer memory cell, and its temporal input, nucleus and its past memory. thus we merely conducted experiments and verified observations that pioneering this new machine architecture application of global lstms is able to improve machine lobe translation and language modeling computer performances.", "histories": [["v1", "Sun, 16 Aug 2015 04:31:37 GMT  (44kb,D)", "https://arxiv.org/abs/1508.03790v1", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"], ["v2", "Wed, 19 Aug 2015 19:38:58 GMT  (73kb,D)", "http://arxiv.org/abs/1508.03790v2", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"], ["v3", "Thu, 20 Aug 2015 07:13:04 GMT  (73kb,D)", "http://arxiv.org/abs/1508.03790v3", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"], ["v4", "Tue, 25 Aug 2015 04:24:20 GMT  (101kb,D)", "http://arxiv.org/abs/1508.03790v4", "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015"]], "COMMENTS": "Content presented in 2015 Jelinek Summer Workshop on Speech and Language Technology on August 14th 2015", "reviews": [], "SUBJECTS": "cs.NE cs.CL", "authors": ["kaisheng yao", "trevor cohn", "katerina vylomova", "kevin duh", "chris dyer"], "accepted": false, "id": "1508.03790"}, "pdf": {"name": "1508.03790.pdf", "metadata": {"source": "CRF", "title": "Depth-Gated LSTM", "authors": ["Kaisheng Yao", "Trevor Cohn", "Katerina Vylomova"], "emails": ["kaisheny@microsoft.com", "tcohn@unimelb.edu.au", "cdyer@cs.cmu.edu"], "sections": [{"heading": null, "text": "In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances."}, {"heading": "1 Introduction", "text": "Deep neural networks (DNNs) have been successfully applied to many areas, including speech [1] and vision [2]. On natural language processing tasks, recurrent neural networks (RNNs) [3\u20135] are widely used because of their ability to memorize long-term dependency.\nA typical problem of training deep networks, including RNNs, is gradient diminishing and explosion. This problem is apparent when training a simple RNN. The long short-term memory (LSTM) [6, 7] neural networks is an extension of simple RNN [3]. In LSTM, a memory cell has linear dependence of its current activity and its past activity. Importantly, a forget gate is used to modulate the information flow between the past and the current activities. LSTMs also have input and output gates to modulate its input and output.\nPerhaps the introduction of gating functions in [6, 7] is the most significant improvement to the recurrent neural networks [3]. More recently, the Gated Recurrent Unit [8] has also adopted the concept of using gates. LSTMs and GRUs are widely used in many natural language processing tasks [9, 10].\nTo construct a deep neural networks, the standard way is to stack many layers of neural networks. This however has the same problem of building simple recurrent networks. The difference here is that the error signals from the top, instead of from the last time instance, have to be back-propagated through many layers of nonlinear transformations and therefore the error signals might be either diminished or exploded.\n\u2217Presented at Jelinek Summer Workshop on August 14 2015.\nar X\niv :1\n50 8.\n03 79\n0v 4\n[ cs\n.N E\n] 2\nThis short note investigates an extension of LSTMs that uses a depth-gate to connect memory cells of the lower and upper layers. We review recurrent neural networks in Sec. 2. Section 3 presents the extension. Experiments are in Sec. 4. We relate this extension with other works in Sec. 5 and conclude in Sec. 6."}, {"heading": "2 Review of recurrent neural networks", "text": "A recurrent neural network [3,4] has a hidden state ht that depends on its past value ht-1 recursively; i.e.,\nht = g(Whhht-1 +Wxhxt) (1)\nwhere g(\u00b7) is usually a nonlinear function such as tanh. xt is the input. Whh and Wxh are the weight matrices."}, {"heading": "2.1 Long short-term memory (LSTM)", "text": "LSTM was initially proposed in [6, 7] and later modified in [11]. We follow the implementation in [11], which is illustrated in Fig. 1. LSTM introduces a linear dependence between its memory cells ct and its past ct-1. Additionally, LSTM has input and output gates. Specially, LSTM is written below as\nit = \u03c3(Wxixt +Whiht-1 +Wcict-1) (2) ft = \u03c3(Wxfxt +Whfht-1 +Wcfct-1) (3) ct = ft ct-1 + it tanh(Wxcxt +Whcht-1) (4) ot = \u03c3(Wxoxt +Whoht-1 +Wcoct) (5) ht = ot tanh(ct) (6)\nwhere it, ft and ot are input gate, forget gate and output gate of LSTM. ht is the output from the LSTM. \u03c3(\u00b7) is the logistic function. denotes element-wise product. In our application of LSTM, the forget gate and input gate share the same parameters but are computed as ft = 1\u2212 it. Note that bias terms are omitted in the above equations but they are applied by default."}, {"heading": "2.2 Stacked LSTMs", "text": "Typically, LSTMs are stacked to form deep recurrent neural networks, illustrated in the left figure of Figure 2.\nThe output from the lower layer LSTM at layer L is h(L)t . With a possible affine transformation, this output is used as input x(L+1)t in the upper layer LSTM at layer L + 1. Except for this output-input connection, there is no other connections between the two layers."}, {"heading": "3 The Depth-gated LSTM", "text": "The depth-gated LSTM (DGLSTM) 1 is illustrated in the right figure of Fig. 2. It has a depth gate that connects the memory cells c(L+1)t in the upper layer L+ 1 and the memory cell cLt in the lower layer L. The depth-gate controls how much flow from the lower memory cell directly to the upper layer memory cell. The gate function at layer L+ 1 at time t is a logistic function as\nd (L+1) t = \u03c3(b (L+1) d +W (L+1) xd x (L+1) t +w (L+1) cd c (L+1) t-1 +w (L+1) ld c (L) t ) (7)\nwhere b(L+1)d is a bias term. W (L+1) xd is the weight matrix to relate depth gate to the input of this layer. The past memory cell is also related via a weight vector w(L+1)cd . To relate the lower layer memory, it uses a weight vector w(L+1)ld . Note that, if lower and upper layer memory cells have different dimension, w(L+1)ld should be a matrix instead of a vector.\nUsing the depth gate, a DGLSTM computes the memory cell at layer L+ 1 as follows\nc (L+1) t = d (L+1) t c (L) t + f (L+1) t c (L+1) t-1 + i (L+1) t tanh(W(L+1)xc x (L+1) t +W (L+1) hc h (L+1) t-1 )(8)\nIn DGLSTM, equations (2), (3), (5) and (6) are the same as the standard LSTM, except that DGLSTM uses a superscript L+ 1 to denote operations at layer L+ 1.\nThe idea of using gated linear dependence can also be used to connect the first layer memory cell c (1) t with the feature observation x (0) t . In this case, the depth-gate is computed for L = 0 as follows\nd (1) t = \u03c3(b (1) d +W (1) xd x (1) t +w (1) cd c (1) t-1 ), (9)\nand the memory cell is computed as\nc (1) t = d (1) t ( W (1) xd x (0) t ) + f (1) t c (1) t-1 + i (1) t tanh(W(1)xc x (0) t +W (1) hc h (1) t-1 ) (10)\n1Implementation at https://github.com/kaishengyao/cnn/blob/master/cnn/dglstm.cc and https://github.com/kaishengyao/cnn/blob/master/cnn/dglstm.h."}, {"heading": "4 Experiments", "text": "We applied DGLSTMs on two datasets. The first is BTEC Chinese to English machine translation task. Its training set consists of 44016 sentence pairs. We use its devset1 and devset 2 for validation, which in total have 1006 sentence pairs. We use its devset3 for test, which has 506 sentence pairs.\nThe second dataset is PennTreeBank (PTB) for language modeling. It consists of 42075 sentences for training, 3371 sentences for development, and 3762 sentences for test."}, {"heading": "4.1 Machine translation results", "text": "We conducted preliminary experiments and observed that the attention model [9] performed better than the encoder-decoder method [10]. We therefore applied the attention model [9] in our experiments. Both encoder and decoder used recurrent neural networks in [9]. However, in this experiment, we only used recurrent neural networks for decoder. For encoder, we used word embedding learned in the training set.\nA preliminary experiment showed that the simple RNN [3] performed the worst. We therefore don\u2019t include the simple RNN results in this paper. We compared DGLSTM with GRU and LSTM. All these models used 200-dimension hidden layer. We varied the depth of RNNs. Results in Table 1 show that DGLSTM outperforms LSTM and GRU in all of the tested depths.\nIn another experiment for the machine translation experiment, we used attention model with DGLSTM to rescore test set k-best lists. We first trained two attention models, one was with 3 layers of DGLSTMs and the other was with 5 layers of DGLSTMs, on training set. Both used 50-dimension hidden layers. We then trained a reranker model using the development data with 100-best lists for each translation pair. The top 100 best lists were generated from the baseline. The features for the reranker models are the scores from the attention model. The 100-best lists on the test set were reranked using the trained reranker model. We ran the above described reranking processes 10 times to get an averaged BLEU scores, which was obtained using one reference. The BLEU scores are listed in Table 2. Compared to the baseline, DGLSTM improved BLEU scores by 3 points on the Test set."}, {"heading": "4.2 Language modeling", "text": "We conducted experiments on PTB dataset. We trained a two layer DGLSTM. Each layer has 200 dimension vector. Test set perplexity results are shown in Table 3. Compared against the previously published results on PTB dataset, DGLSTM obtained the lowest perplexity on PTB test set to our knowledge."}, {"heading": "5 Related works", "text": "We developed this method independently in a summer workshop and later knew the works in [13,14]. In highway networks in [13], the output from a layer yt is a linear function to the input xt, in addition\nto the output from a nonlinear path. Both of them are gated as follows yt = H(xt,Whh) T(xt,WxT) + xt C(xt,Wc) (11)\nwhere T and C are called transform gate and carry gate, respectively. H(\u00b7) is the output from a nonlinear path. Whh, WxT, and Wc are matrices. Therefore, the highway network output has a direct and linear connection, albeit gated, to the input. This allows highway networks to train extremely deep networks easily.\nDGLSTM is related to the highway networks in using the same idea of linear and gated connection to input. It differs from the highway networks in not using a specific gate on the non-linear path; DGLSTM keeps the input and output gates which are applied on the non-linear transformations in LSTMs. However, the overall effect of using the input and output gates may be similar to the transfer gate in the highway networks. An additional but important difference is that DGLSTM linearly connects the memory cells in the lower and upper layers. Because of this, the memory cell in DGLSTM has errors back-propagated both from the future and from the top layer, linearly albeit gated. This might be the biggest difference from the highway networks [13] in its current implementation.\nPerhaps the closet work to this research is Grid LSTM [14], which uses LSTMs in different dimensions and connects them using gated linear connections. Because the dimensions can include not only time, as the typical recurrent neural networks, but also depth and others, Grid LSTM is more general than DGLSTM, which only considers time and depth. Also, Grid LSTM uses a generic form of input, memory, and output. Doing so allows a memory cell to have a gated linear dependence not only on its past memory cell but also on the past observations. Therefore, we consider DGLSTM as a specific and simple case of Grid LSTM that has gate applied to time and depth only on memory cells. However, DGLSTM, Grid LSTM and highway networks share the same idea of stacking networks with both linear but gated connections and nonlinear paths. This idea can be applied to fully connected, convolutional or recurrent layers."}, {"heading": "6 Conclusions", "text": "We have presented a depth-gated LSTM architecture, which uses a depth-gate to have gated linear connection between lower and upper layer memory cells. We observed better performances using this new architecture on machine translation and language modeling tasks. This architecture is related to the highway networks [13] and Grid LSTM [14] in using an additional linear connection with gates to regulate information flow across layers."}, {"heading": "7 Acknowledgment", "text": "We thank Juergen Schmidhuber for pointing out highway networks [13] and grid LSTM [14] works."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolution neural networks", "author": ["A Krizhevsky", "I Sutskever", "G Hinton"], "venue": "NIPS, 2012, vol. 25, pp. 1090\u20131098. 5", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding structure in time", "author": ["J. Elman"], "venue": "Cognitive science, vol. 14, no. 2, pp. 179\u2013211, 1990.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "Serial order: a parallel distributed processing approach", "author": ["M. Jordan"], "venue": "Tech. Rep., Univ. of California San Diego, 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafiat", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "INTERSPEECH, 2010, pp. 1045\u20131048.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, pp. 1735\u20131780, 1997.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural Computation, vol. 12, pp. 2451\u20132471, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "On the properties of neural machine translation", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv:1409.125, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv:1409.0473 [cs.CL], 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv:1308.0850 [cs.NE], 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv:1312.6026 [cs.NE], 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "Jurgen Schmidhuber"], "venue": "arxiv:1505.00387v1, May 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "arXiv:1507.01526 [cs.NE], 2015. 6", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) have been successfully applied to many areas, including speech [1] and vision [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Deep neural networks (DNNs) have been successfully applied to many areas, including speech [1] and vision [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 2, "context": "On natural language processing tasks, recurrent neural networks (RNNs) [3\u20135] are widely used because of their ability to memorize long-term dependency.", "startOffset": 71, "endOffset": 76}, {"referenceID": 3, "context": "On natural language processing tasks, recurrent neural networks (RNNs) [3\u20135] are widely used because of their ability to memorize long-term dependency.", "startOffset": 71, "endOffset": 76}, {"referenceID": 4, "context": "On natural language processing tasks, recurrent neural networks (RNNs) [3\u20135] are widely used because of their ability to memorize long-term dependency.", "startOffset": 71, "endOffset": 76}, {"referenceID": 5, "context": "The long short-term memory (LSTM) [6, 7] neural networks is an extension of simple RNN [3].", "startOffset": 34, "endOffset": 40}, {"referenceID": 6, "context": "The long short-term memory (LSTM) [6, 7] neural networks is an extension of simple RNN [3].", "startOffset": 34, "endOffset": 40}, {"referenceID": 2, "context": "The long short-term memory (LSTM) [6, 7] neural networks is an extension of simple RNN [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "Perhaps the introduction of gating functions in [6, 7] is the most significant improvement to the recurrent neural networks [3].", "startOffset": 48, "endOffset": 54}, {"referenceID": 6, "context": "Perhaps the introduction of gating functions in [6, 7] is the most significant improvement to the recurrent neural networks [3].", "startOffset": 48, "endOffset": 54}, {"referenceID": 2, "context": "Perhaps the introduction of gating functions in [6, 7] is the most significant improvement to the recurrent neural networks [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "More recently, the Gated Recurrent Unit [8] has also adopted the concept of using gates.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "LSTMs and GRUs are widely used in many natural language processing tasks [9, 10].", "startOffset": 73, "endOffset": 80}, {"referenceID": 9, "context": "LSTMs and GRUs are widely used in many natural language processing tasks [9, 10].", "startOffset": 73, "endOffset": 80}, {"referenceID": 2, "context": "A recurrent neural network [3,4] has a hidden state ht that depends on its past value ht-1 recursively; i.", "startOffset": 27, "endOffset": 32}, {"referenceID": 3, "context": "A recurrent neural network [3,4] has a hidden state ht that depends on its past value ht-1 recursively; i.", "startOffset": 27, "endOffset": 32}, {"referenceID": 5, "context": "LSTM was initially proposed in [6, 7] and later modified in [11].", "startOffset": 31, "endOffset": 37}, {"referenceID": 6, "context": "LSTM was initially proposed in [6, 7] and later modified in [11].", "startOffset": 31, "endOffset": 37}, {"referenceID": 10, "context": "LSTM was initially proposed in [6, 7] and later modified in [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "We follow the implementation in [11], which is illustrated in Fig.", "startOffset": 32, "endOffset": 36}, {"referenceID": 8, "context": "We conducted preliminary experiments and observed that the attention model [9] performed better than the encoder-decoder method [10].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "We conducted preliminary experiments and observed that the attention model [9] performed better than the encoder-decoder method [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "We therefore applied the attention model [9] in our experiments.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "Both encoder and decoder used recurrent neural networks in [9].", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "A preliminary experiment showed that the simple RNN [3] performed the worst.", "startOffset": 52, "endOffset": 55}, {"referenceID": 12, "context": "We developed this method independently in a summer workshop and later knew the works in [13,14].", "startOffset": 88, "endOffset": 95}, {"referenceID": 13, "context": "We developed this method independently in a summer workshop and later knew the works in [13,14].", "startOffset": 88, "endOffset": 95}, {"referenceID": 12, "context": "In highway networks in [13], the output from a layer yt is a linear function to the input xt, in addition", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "Model Perplexity RNN [5] 123 LSTM [11] 117 sRNN [12] 110 DOT(s)-RNN [12] 108 DGLSTM 96", "startOffset": 21, "endOffset": 24}, {"referenceID": 10, "context": "Model Perplexity RNN [5] 123 LSTM [11] 117 sRNN [12] 110 DOT(s)-RNN [12] 108 DGLSTM 96", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "Model Perplexity RNN [5] 123 LSTM [11] 117 sRNN [12] 110 DOT(s)-RNN [12] 108 DGLSTM 96", "startOffset": 48, "endOffset": 52}, {"referenceID": 11, "context": "Model Perplexity RNN [5] 123 LSTM [11] 117 sRNN [12] 110 DOT(s)-RNN [12] 108 DGLSTM 96", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "This might be the biggest difference from the highway networks [13] in its current implementation.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "Perhaps the closet work to this research is Grid LSTM [14], which uses LSTMs in different dimensions and connects them using gated linear connections.", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "This architecture is related to the highway networks [13] and Grid LSTM [14] in using an additional linear connection with gates to regulate information flow across layers.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "This architecture is related to the highway networks [13] and Grid LSTM [14] in using an additional linear connection with gates to regulate information flow across layers.", "startOffset": 72, "endOffset": 76}], "year": 2015, "abstractText": "In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances.", "creator": "LaTeX with hyperref package"}}}