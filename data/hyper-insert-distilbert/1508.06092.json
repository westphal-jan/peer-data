{"id": "1508.06092", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2015", "title": "An analysis of numerical issues in neural training by pseudoinversion", "abstract": "some novel methods strategies have recently been erroneously proposed for single hidden layer neural network sensitivity training that must set randomly the minimum weights from individual input to hidden layer, additionally while their weights from inputs hidden to output layer decomposition are more analytically determined by pseudoinversion. potentially these techniques are gaining particular popularity in spite of their known numerical modeling issues when singular differential and / or almost singular filter matrices are involved. in exchanging this paper we discuss a critical use of singular value coefficient analysis for identification of beyond these desirable drawbacks and however we propose making an original use of regularisation to determine the output gate weights, largely based on the broader concept calculation of critical hidden networks layer path size. this specific approach too also allows integration to partially limit the training computational effort. besides, firstly we introduce a rather novel technique which relies an effective determination of input sensor weights proportional to the estimated hidden layers layer dimension. this emerging approach formulation is tested widely for managing both regression and nonlinear classification tasks, resulting in both a significant statistical performance improvement potential with proper respect to alternative implementation methods.", "histories": [["v1", "Tue, 25 Aug 2015 09:51:35 GMT  (90kb)", "http://arxiv.org/abs/1508.06092v1", "11 pages, submitted to: Comp. Appl. Math"]], "COMMENTS": "11 pages, submitted to: Comp. Appl. Math", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["r cancelliere", "r deluca", "m gai", "p gallinari", "l rubini"], "accepted": false, "id": "1508.06092"}, "pdf": {"name": "1508.06092.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["rossella.cancelliere@unito.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n50 8.\n06 09\n2v 1\n[ cs\n.L G\n] 2\n5 A\nug 2\nKeywords pseudoinverse matrix \u00b7 weights setting \u00b7 regularisation \u00b7 supervised learning"}, {"heading": "1 Introduction", "text": "The training of one of the most common neural architecture, the single hidden layer feedforward neural network (SLFN) was mainly accomplished in past decades by methods based on gradient descent, and among them the large family of techniques based on backpropagation (Rumelhart et al., 1986). The start-up of these techniques assigns random values to the weights connecting input, hidden and\nR. Cancelliere, R. Deluca, L. Rubini University of Turin, Department of Computer Sciences, Turin, Italy Tel.: +39 011 6706737, Fax: +39 011 751603 E-mail: rossella.cancelliere@unito.it\nM. Gai National Institute of Astrophysics, Astrophysical Observatory of Torino, Turin, Italy\nP. Gallinari Laboratory of Computer Sciences, LIP6, Universite\u0301 Pierre et Marie Curie, Paris, France\noutput nodes that are then iteratively modified according to the error gradient steepest descent direction. Some common drawbacks with gradient descent-based learning are anyway the high computational cost because of slow convergence and the relevant risk of converging to poor local minima on the landscape of the error function (LeCun et al., 1998).\nThe idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al., 2010; Kohno et al., 2010; Ajorloo et al., 2007) and among them the extreme learning machine ELM, (Huang et al., 2006) which has been successfully applied to a number of real-world applications (Sun et al., 2008; Wang et al., 2008; Malathi et al., 2010; Minhas et al., 2010), showing a good generalization performance with an extremely fast learning speed.\nELM main result states that SLFNs with randomly chosen input weights and hidden layer biases can learn distinct observations with a desired precision, provided that activation functions in the hidden layer are infinitely differentiable.\nAfter input weights and hidden layer biases have been randomly set, output weights are directly evaluated by Moore-Penrose generalised inverse (or pseudoinverse) of the hidden layer output matrix: these two steps conclude one training phase and weights are no more modified, so that their determination is no more iterative in the sense of back-propagation based techniques.\nBesides, all pseudoinversion based methods are multi-start, i.e. the above procedure is repeated meny times in order to find a good minimum of the error surface. Each training procedure so implies many random settings of input weights and as many evaluations of output weights through pseudoinversion.\nHowever, such techniques seem to require more hidden units than typical values from backpropagation training to achieve comparable accuracy, as discussed in Yu and Deng (Yu and Deng, 2012). Moreover, pseudoinversion, commonly evaluated by Singular Value Decomposition (SVD), is a powerful method but some caution is required, since its numerical instability is a well known issue when singular and almost singular matrices are involved.\nOne aim of this paper is the analysis of these instability issues; a preliminary assessment of the context and our initial results are discussed in (Cancelliere et al., 2012). Here we present further advances on the theoretical framework and we propose a novel approach to carry out a more efficient learning, showing how singular values of SVD can be used to detect the occurrence of numerical instability.\nBesides, we prove the existence of a critical hidden layer dimension that allows a careful tuning of the regularisation parameter and the use of regularisation to replace unstable, ill-posed problems with well-posed ones. We also propose an original method to set input weights that links their size to the hidden layer dimension and we show its effectiveness.\nIn section 2 we introduce the notation used for describing SLFN architectures, and the main ideas concerning input weights setting and output weights evaluation by pseudoinversion. In section 3 we discuss the problem of ill-posedness, and the basic regularisation concepts.\nIn section 4 our framework is tested on some applications selected from the UCI database. A substantial improvement in performance with respect to unregularised state-of-the-art techniques is shown."}, {"heading": "2 Input and output weights determination", "text": "Fig. 1 shows a standard SLFN with P input neurons, M hidden neurons with non-linear activation functions \u03c6, and Q output neurons with linear activation functions.\nIf we have a training set made by N distinct training samples of (input, output) pairs (xj , tj), where xj \u2208 RP and tj \u2208 RQ, the training aims at obtaining the matrix of desired outputs T \u2208 RN\u00d7Q when the matrix of all input instances X \u2208 RN\u00d7P is presented as input.\nWe emphasise that, in the state of the art pseudoinverse approach, input weights cij (and hidden neurons biases) are randomly sampled from a uniform distribution in a fixed interval and no longer modified. Therefore this step gives the actual input weights values.\nAfter having fixed the input weights matrix C, the use of linear output units allows to determine output weights wij as the solution of the linear system\nHW = T, (1)\nwhere H \u2208 RN\u00d7M is the hidden layer output matrix of the neural network, H = \u03a6(X C). It is important to underline that because usually the number of hidden nodes is much lower than the number of distinct training samples, i.e. M << N , H is a rectangular matrix.\nThe least-squares solution W\u0304 of the linear system (1), as shown e.g. in (Penrose and Todd, 1956; Bishop, 2006), is W\u0304 = H+T, where H+ is the Moore-Penrose generalised inverse (or pseudoinverse) of matrixH. It can be computed in a computationally simple and accurate way by using singular value decomposition (SVD) (Golub and Van Loan, 1996).\nWe know that every matrix H \u2208 \u211cN\u00d7M can be decomposed as follows:\nH = U\u03a3V T , (2)\nwhere U \u2208 \u211cN\u00d7N and V \u2208 \u211cM\u00d7M are orthogonal matrices and \u03a3 \u2208 \u211cN\u00d7M is a rectangular diagonal matrix whose elements \u03c3ii \u2261 \u03c3i, called singular values,\nare nonnegative (usually the singular values are listed in descending order, i.e. \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3p \u2265 0, p = min {N,M}, so that \u03a3 is uniquely determined).\nThe pseudoinverse matrix H+ has the form\nH+ = V \u03a3+UT , (3)\nwhere \u03a3+ is obtained from \u03a3 by taking the reciprocal of each non-zero element \u03c3i, and transposing the resulting matrix (Rao and Mitra, 1971). The presence of very small element \u03c3i is therefore a potential drawback of this method.\nFor computational reasons, the elements \u03c3i equal to zero or smaller than a predefined threshold are replaced by zeros (Golub and Van Loan, 1996)."}, {"heading": "3 Singular value decomposition of regularised problems", "text": "To turn an original ill-posed problem into a well-posed one, i.e. roughly speaking into a problem insensitive to small changes in training conditions, regularisation methods are often used (Badeva and Morozov, 1991), and Tikhonov regularisation is one of the most common (Tikhonov and Arsenin, 1977; Tikhonov, 1963).\nThe error functional to be minimised is characterized by a penalty term ER that depends from the so called Tikhonov matrix \u0393 :\nE \u2261 ED + ER = ||HW \u2212 T ||22 + ||\u0393W ||22, (4)\nThis matrix can for instance derive from the choice of using highpass operators (e.g. a difference operator or a weighted Fourier operator) to enforce smoothness.\nThe regularised solution, that we denote by W\u0302 , is now given by:\nW\u0302 = (HTH + \u0393T\u0393 )\u22121HTT . (5)\nThe penalty term improves on stability, making the problem less sensitive to initial conditions, and contains model complexity avoiding overfitting, as largely discussed in (Gallinari and Cibas, 1999).\nTo give preference to solutions W\u0302 with smaller norm (Bishop, 2006) a frequent choice is \u0393 = \u221a \u03bbI, so eqs. (4) and (5) can be rewritten as\nE \u2261 ED + ER = ||HW \u2212 T ||22 + \u03bb||W ||22, (6)\nW\u0302 = (HTH + \u03bbI)\u22121HTT . (7)\nThe role of the control parameter \u03bb is to trade off between the two error terms ED and ER. If \u03bb = 0, eq.(7) reduces to the unregularised least-squares solution, provided that (HTH)\u22121 exists.\nThe regularised solution (7) can also be expressed (see e.g. (Fuhry and Reichel, 2012)) as:\nW\u0302 = V DUT T (8)\nwhere V,U are from the singular value decomposition of H (eq.(2)) and D is a rectangular diagonal matrix with elements\nDi = \u03c3i\n\u03c32i + \u03bb . (9)\nobtained using the singular values of H.\nIt is evident that, when unregularised pseudoinversion is used, the presence of very small singular values can easily causes numerical instability in H+; on the contrary, regularisation has a dramatic impact because, even in presence of very small values \u03c3i of the original unregularised problem, a careful choice of the parameter \u03bb allows to tune singular values Di of the regularised matrix, preventing them from divergence.\nIt is clear at this point that a suitable value for the parameter \u03bb has to derive from a compromise between the necessity to have it sufficiently large to control the approaching to zero of \u03c3 in eq.(9) while avoiding predominance of penalty term in eq.(6). Its tuning is therefore crucial to simultaneously control numerical instability and overfitting.\nIn the next section we propose a strategy to obtain this result showing that we achieve better performance and more stable solutions."}, {"heading": "4 Experiments and Results", "text": "Some numerical instability issues have already been evidenced in our previous investigations (Cancelliere et al., 2012); we provided suggestions on possible mitigation techniques like selection of a convenient activation function and normalisation of the input weights. Hereafter we show that adding regularisation to the implementation prescriptions already analysed provides a convenient and effective approach to deal with such problem.\nThe use of sigmoidal activation functions has recently been subject of debate because they seem to be more easily driven towards saturation due to their nonzero mean value (Bengio and Glorot, 2010), while hyperbolic tangent seems less sensitive to this problem.\nWe select therefore both activation functions for a test aiming at comparing their performance in a context where we also compare our proposed regularised approach and the unregularised one. Four different experimental settings will be analysed, namely HypT-reg, Sigm-reg, HypT-unreg and Sigm-unreg.\nTo further mitigate saturation issues, in our previous work (Cancelliere et al., 2012) we selected input weights according to a uniform random distribution in the range (\u22121/ \u221a M , 1/ \u221a M), where M is the number of hidden nodes. This links the size of input weights, and therefore of hidden neurons inputs, to the network architecture, thus forcing the use of the almost linear central part of the hyperbolic and sigmoidal activation functions when exploring the performance as a function\nof an increasing number of nodes. Such prescriptions are retained in the current work.\nWe emphasise that so doing input weights are automatically chosen \u201dsmall\u201d, because the size of interval (\u22121/ \u221a M , 1/ \u221a M) decreases when the number of hidden neurons M increases: for instance with 10 hidden neurons, weights values are roughly selected in the range (\u22121/3,1/3), with 100 hidden neurons in the range (\u22121/10,1/10), and so on.\nBecause of the wide use among researchers belonging to ELM-community (see for instance (Helmy and Rasheed, 2009; Huang et al., 2006; Sun et al., 2008) our performance is also compared with that from unregularised pseudoinversion, input weights selected according to a random uniform distribution in the interval (\u22121,1) and sigmoidal activation functions (hereafter, ELM).\nThe numerical experiment compares these frameworks applying them to six benchmark datasets from the UCI repository (Bache and Lichman, 2013), listed in Table 1.\nFor each proposed method the number of hidden nodes is gradually increased by unity steps, and, for each selected size of SLFN, average RMSE or average misclassification rate on the validation set are computed over a set of 100 simulation trials, i.e. over 100 different initial choices of input weights. All simulations are carried out in Matlab 7.3 environment.\nFigure 2 gives an insight on the performance trend (resp. average RMSE for regression or average misclassification rate for classification tasks) as a function of hidden space dimensionality for the cases HypT-reg, HypT-unreg and ELM. In Figure 3 performance trends are shown for the cases SigmT-reg, Sigm-unreg and, for the sake of comparison, again ELM; because of their similarity with the HypT cases, plots are shown for only two datasets (i.e. Abalone and Iris).\nIt is interesting to note that when unregularised techniques are used, all datasets except Landsat show a fast error growth; besides, the curves have different characteristics for HypT-unreg and Sigm-unreg on one side and ELM on the other, showing error peaks in the formers while monotonically increasing error values are obtained in the latter. We conjecture the presence of two distinct phenomena: numerical instability and overfitting.\nIn order to address the former issue, i.e. numerical instability, we evaluate for each dataset the ratio between the minimum singular value of hidden output matrix H and the Matlab default threshold (below which singular values are considered too small and therefore treated as zero).\nWe checked that for each dataset processed with HypT-unreg or Sigm-unreg methods there is a critical hidden layer size above which the ratio becomes smaller than one; its trend is plotted in logarithmic units (red line, right scale), in Figure 2 for HypT-unreg case.\nWhen approaching critical size, inversion of singular values causes a wrong evaluation of H+ and therefore a significant growth in the error; when the critical dimension is reached, singular values under threshold are automatically removed, thus allowing the subsequent decrease of error. The same trend was detected analysing the astronomical dataset in (Cancelliere et al., 2012).\nThis decrease is anyway not sufficient to reach optimal error values because of overfitting, which is known to arise when a large amount of hidden neurons is available to reproduce almost exactly the training data.\nAn even more severe overfitting affects ELM in fact in this case test error is a monotonically increasing function of the number of hidden neurons. A possible explanation is that the setting of input weights in the interval (-1, 1) may allow \u2018specialisation\u2019 of some hidden neurons on particular training instances, thus creating a sort of network partition, carried out thanks to saturation. On the contrary, when weights are randomly selected in the interval (\u22121/ \u221a M, 1/ \u221a M), as for HypT-unreg and Sigm-unreg cases, input weights are automatically kept small when the network size increases, thus exploiting the central part of both activation functions: consequently saturation is avoided (Cancelliere et al., 2012).\nAfter clarification of the main issues affecting unregularised approaches, we now discuss the regularised one, derived according to eq.(8).\nLooking at HypT-reg and Sigm-reg cases in Figures 2 and 3, it appears that not only numerical instability (i.e. the error peak) is removed, but also that the penalty term provides control of overfitting, avoiding error growth and allowing optimal exploitation of the superior potential of larger architectures. The error curves feature now a monotonic decrease, becoming increasingly smoother.\nWe obtained this result basing on the implications of eq.(9) that clearly suggests the role of the \u03bb parameter in preventing instability: our original idea is to address the issue of its determination with an \u2018ad hoc\u2019 tuning whenever growing error drift is experienced by the unregularised approach, as follows.\nWe evaluate the validation error trend inside the critical region, looking for its minimum as a function of \u03bb; this allows to select a suitable value for this parameter for the subsequent experimentation with regularised pseudoinversion.\nWe then made 100 random input weight choices and evaluated the mean test error (RMSE or misclassification rate) and the standard deviation S for any number of hidden neurons. In Table 2 we list for HypT-reg and Sigm-reg the best ones of these mean values, called Optimal performance, together with the number of hidden neurons used to reach the associated performance (in parenthesis), and the corresponding standard deviation, as well as the selected value of \u03bb.\nError values significantly better, basing on the Student\u2019s t-test evaluation of statistical confidence intervals, are recorded in bold. We can see that there is roughly no \u201cwinner\u201d between HypT-reg and Sigm-reg.\nSome other interesting considerations can be made noting that the regularised error plots in Figures 2 and 3, after an initial rapid decrease, achieve a nearly constant regime, with small variation vs. increasing numbers of hidden neurons. We can thus define a \u201cnear optimal\u201d network size in both cases, as the one associated to a near optimal error level, i.e. an error significantly better than that from other methods. The assessment is again based on the Student\u2019s t-test evaluation of statistical confidence intervals.\nTable 3 compares the performance of this near optimal network with those obtained with the other methods, listing the best mean error (with the associated number of hidden neurons in parenthesis) and the corresponding standard deviation.\nThus, it appears that regularisation provides, except for Landsat dataset, the best performance not only in terms of lowest error values (see table 2) but\neven with usage of smaller networks, limiting in this way computational cost and model complexity, and therefore fulfilling the goals set by previous researches (e.g. (Yu and Deng, 2012)). The smaller standard deviations almost always associated with the regularised methods also suggest a lower dependence from initial conditions.\nWe also highlight that the use of small input weights and sigmoidal activation functions, which characterizes the Sigm-unreg case, allows to obtain error values lower with respect to the ELM case, so confirming the effectiveness of this choice in order to contain saturation and overfitting issues.\nLandsat dataset constitutes an exception because best performance is reached using ELM. In this case, regularisation does not seem to be required, because overfitting and/or numerical instability do not take place, as it appears from unregularised error curves. The behaviour of the threshold ratio, which remains always larger than unity, is consistent with the lack of numerical instability and with our hypothesis of its relationship with error peaks. The lack of overfitting appears to be specific to the complexity of the dataset, having input vectors with size much larger than others, and therefore requiring a significantly larger number of parameters.\nIn table 4 are listed the computational times (in seconds) recorded for all datasets for completing one training step, i.e. one random setting of input weights\nand one output weights evaluation through pseudoinversion of the hidden layer output matrix; the number of hidden neurons has been fixed to 100 for the sake of comparison.\nWe can see that, for each dataset, the times associated to each method do not differ significantly, because after having fixed the number of hidden neurons, the computational load necessary for the processing is comparable.\nThe interested reader can find, for the common datatsets, a comparison among training times of ELM and backpropagation in (Huang et al., 2006), and can verify that ELM turns out to be two or three orders of magnitude faster."}, {"heading": "5 Conclusions", "text": "We have considered the numerical instability ad overfitting problems for single hidden layer neural networks trained by pseudoinversion. We have shown how to use singular value analysis for the diagnosis of numerical instability, and how to solve this problem through the determination of a critical hidden layer region from which the regularisation technique benefits. This method also contributes to reduce the overfitting.\nTests have been performed for both regression and classification tasks. For five out of six cases, the proposed regularisation is proven necessary and provides a significant performance improvement with respect to unregularised techniques; it also allows to built lean architectures which achieve near optimal performance with a reduced number of hidden neurons.\nMoreover the use of sigmoidal activation functions and \u201csmall\u201d input weights (small because their values are linked to the hidden layer size), which characterizes the Sigm-unreg case, allows to obtain error values lower with respect to the ELM case, so confirming the effectiveness of this choice in order to contain saturation and overfitting issues.\nComparing our results on the common regression datasets with the alternative method proposed by Miche et al. (Miche et al., 2011), we note that our technique achieves RMSE values lower than those corresponding to their MSE values, with a\nsomewhat lower number of neurons. Besides, in our opinion, our method is simpler, in the sense that it uses a single step of regularisation rather than two in their method, and we also deal with classification tasks."}, {"heading": "6 Acknowledgment", "text": "The activity has been partially carried on in the context of the Visiting Professor Program of the Gruppo Nazionale per il Calcolo Scientifico (GNCS) of the Italian Istituto Nazionale di Alta Matematica (INdAM)."}], "references": [{"title": "Restoration of damaged slices in images using matrix pseudo inversion", "author": ["H. Ajorloo", "M.T. Manzuri-Shalmani", "A. Lakdashti"], "venue": "Proceedings of the 22nd International symposium on computer and information sciences.", "citeRegEx": "Ajorloo et al\\.,? 2007", "shortCiteRegEx": "Ajorloo et al\\.", "year": 2007}, {"title": "Probl\u00e8mes incorrectement pos\u00e9s: Th\u00e9orie et applications en identification , filtrage optimal, contr\u00f4le optimal, analyse et synth\u00e8se de syst\u00e8mes, reconnaissance d\u2019images", "author": ["V. Badeva", "V. Morozov"], "venue": "S\u00e9rie Automatique. Masson.", "citeRegEx": "Badeva and Morozov,? 1991", "shortCiteRegEx": "Badeva and Morozov", "year": 1991}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Y. Bengio", "X. Glorot"], "venue": "Proceedings of AISTATS 2010 , volume 9, pages 249\u2013256.", "citeRegEx": "Bengio and Glorot,? 2010", "shortCiteRegEx": "Bengio and Glorot", "year": 2010}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["C.M. Bishop"], "venue": "Springer-Verlag New York, Inc., Secaucus, NJ, USA.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "A high parallel procedure to initialize the output weights of a radial basis function or bp neural network", "author": ["R. Cancelliere"], "venue": "Proceedings of the 5th International Workshop on Applied Parallel Computing, New Paradigms for HPC in Industry and Academia, PARA \u201900, pages 384\u2013390, London, UK, UK. Springer-Verlag.", "citeRegEx": "Cancelliere,? 2001", "shortCiteRegEx": "Cancelliere", "year": 2001}, {"title": "Matrix pseudoinversion for image neural processing", "author": ["R. Cancelliere", "M. Gai", "T. Arti\u00e8res", "P. Gallinari"], "venue": "T. Huang, Z. Zeng, C. Li, and C. Leung, editors, Neural Information Processing, volume 7667 of Lecture Notes in Computer Science, pages 116\u2013125. Springer Berlin Heidelberg.", "citeRegEx": "Cancelliere et al\\.,? 2012", "shortCiteRegEx": "Cancelliere et al\\.", "year": 2012}, {"title": "A new tikhonov regularization method", "author": ["M. Fuhry", "L. Reichel"], "venue": "Numerical Algorithms, 59(3), 433\u2013445.", "citeRegEx": "Fuhry and Reichel,? 2012", "shortCiteRegEx": "Fuhry and Reichel", "year": 2012}, {"title": "Practical complexity control in multilayer perceptrons", "author": ["P. Gallinari", "T. Cibas"], "venue": "Signal Processing, 74, 29\u201346.", "citeRegEx": "Gallinari and Cibas,? 1999", "shortCiteRegEx": "Gallinari and Cibas", "year": 1999}, {"title": "Matrix computations (3rd ed.)", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan", "year": 1996}, {"title": "Multi-category bioinformatics dataset classification using extreme learning machine", "author": ["T. Helmy", "Z. Rasheed"], "venue": "Proceedings of the Eleventh conference on Congress on Evolutionary Computation, CEC\u201909, pages 3234\u20133240, Piscataway, NJ, USA. IEEE Press.", "citeRegEx": "Helmy and Rasheed,? 2009", "shortCiteRegEx": "Helmy and Rasheed", "year": 2009}, {"title": "Extreme learning machine: theory and applications", "author": ["Huang", "G.-B.", "Zhu", "Q.-Y.", "Siew", "C.-K."], "venue": "Neurocomputing, 70(1), 489\u2013501.", "citeRegEx": "Huang et al\\.,? 2006", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "A matrix pseudoinversion lemma and its application to block-based adaptive blind deconvolution for mimo systems", "author": ["K. Kohno", "M. Kawamoto", "Y. Inouye"], "venue": "Trans. Cir. Sys. Part I , 57(7), 1449\u20131462.", "citeRegEx": "Kohno et al\\.,? 2010", "shortCiteRegEx": "Kohno et al\\.", "year": 2010}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "Mller", "K.-R."], "venue": "pages 9\u201350. Springer Berlin Heidelberg.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Intelligent approaches using support vector machine and extreme learning machine for transmission line protection", "author": ["V. Malathi", "N. Marimuthu", "S. Baskar"], "venue": "Neurocomputing, 73(1012), 2160 \u2013 2167. Subspace Learning / Selected papers from the European Symposium on Time Series Prediction.", "citeRegEx": "Malathi et al\\.,? 2010", "shortCiteRegEx": "Malathi et al\\.", "year": 2010}, {"title": "Tropelm: A double-regularized elm using lars and tikhonov regularization", "author": ["Y. Miche", "M. van Heeswijk", "P. Bas", "O. Simula", "A. Lendasse"], "venue": null, "citeRegEx": "Miche et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Miche et al\\.", "year": 2011}, {"title": "Human action recognition using extreme learning machine based on visual vocabularies", "author": ["R. Minhas", "A. Baradarani", "S. Seifzadeh", "Q.J. Wu"], "venue": "Neurocomputing, 73(1012), 1906 \u2013 1917. Subspace Learning / Selected papers from the European Symposium on Time Series Prediction.", "citeRegEx": "Minhas et al\\.,? 2010", "shortCiteRegEx": "Minhas et al\\.", "year": 2010}, {"title": "An efficient pseudo inverse matrix-based solution for secure auditing", "author": ["T.D. Nguyen", "H.T.B. Pham", "V.H. Dang"], "venue": "Proceedings of the IEEE International Conference on Computing and Communication Technologies, Research, Innovation, and Vision for the Future, IEEE International Conference.", "citeRegEx": "Nguyen et al\\.,? 2010", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "On best approximate solutions of linear matrix equations", "author": ["R. Penrose", "J.A. Todd"], "venue": "Mathematical Proceedings of the Cambridge Philosophical Society , null, 17\u201319.", "citeRegEx": "Penrose and Todd,? 1956", "shortCiteRegEx": "Penrose and Todd", "year": 1956}, {"title": "Networks for approximation and learning", "author": ["T. Poggio", "F. Girosi"], "venue": "Proceedings of the IEEE , 78(9), 1481\u20131497.", "citeRegEx": "Poggio and Girosi,? 1990", "shortCiteRegEx": "Poggio and Girosi", "year": 1990}, {"title": "Generalized inverse of matrices and its applications", "author": ["C. Rao", "S. Mitra"], "venue": "Wiley series in probability and mathematical statistics: Applied probability and statistics. Wiley.", "citeRegEx": "Rao and Mitra,? 1971", "shortCiteRegEx": "Rao and Mitra", "year": 1971}, {"title": "Parallel distributed processing: explorations in the microstructure of cognition, vol", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "1. chapter Learning internal representations by error propagation, pages 318\u2013362. MIT Press, Cambridge, MA, USA.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Sales forecasting using extreme learning machine with applications in fashion retailing", "author": ["Sun", "Z.-L.", "Choi", "T.-M.", "Au", "K.-F.", "Y. Yu"], "venue": "Decision Support Systems, 46(1), 411 \u2013 419.", "citeRegEx": "Sun et al\\.,? 2008", "shortCiteRegEx": "Sun et al\\.", "year": 2008}, {"title": "Solutions of ill-posed problems", "author": ["A. Tikhonov", "V. Arsenin"], "venue": "Scripta series in mathematics. Winston.", "citeRegEx": "Tikhonov and Arsenin,? 1977", "shortCiteRegEx": "Tikhonov and Arsenin", "year": 1977}, {"title": "Solution of incorrectly formulated problems and the regularization method", "author": ["A.N. Tikhonov"], "venue": "Soviet Math. Dokl., 4, 1035\u20131038.", "citeRegEx": "Tikhonov,? 1963", "shortCiteRegEx": "Tikhonov", "year": 1963}, {"title": "A protein secondary structure prediction framework based on the extreme learning machine", "author": ["G. Wang", "Y. Zhao", "D. Wang"], "venue": "Neurocomputing, 72(13), 262 \u2013 268. Machine Learning for Signal Processing (MLSP 2006) / Life System Modelling, Simulation, and Bio-inspired Computing (LSMS 2007).", "citeRegEx": "Wang et al\\.,? 2008", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Efficient and effective algorithms for training singlehidden-layer neural networks", "author": ["D. Yu", "L. Deng"], "venue": "Pattern Recogn. Lett., 33(5), 554\u2013558.", "citeRegEx": "Yu and Deng,? 2012", "shortCiteRegEx": "Yu and Deng", "year": 2012}], "referenceMentions": [{"referenceID": 20, "context": "The training of one of the most common neural architecture, the single hidden layer feedforward neural network (SLFN) was mainly accomplished in past decades by methods based on gradient descent, and among them the large family of techniques based on backpropagation (Rumelhart et al., 1986).", "startOffset": 267, "endOffset": 291}, {"referenceID": 12, "context": "Some common drawbacks with gradient descent-based learning are anyway the high computational cost because of slow convergence and the relevant risk of converging to poor local minima on the landscape of the error function (LeCun et al., 1998).", "startOffset": 222, "endOffset": 242}, {"referenceID": 18, "context": "The idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al.", "startOffset": 140, "endOffset": 165}, {"referenceID": 4, "context": "The idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al.", "startOffset": 217, "endOffset": 236}, {"referenceID": 16, "context": "The idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al., 2010; Kohno et al., 2010; Ajorloo et al., 2007) and among them the extreme learning machine ELM, (Huang et al.", "startOffset": 284, "endOffset": 347}, {"referenceID": 11, "context": "The idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al., 2010; Kohno et al., 2010; Ajorloo et al., 2007) and among them the extreme learning machine ELM, (Huang et al.", "startOffset": 284, "endOffset": 347}, {"referenceID": 0, "context": "The idea of using the simple and efficient training algorithms of radial basis function neural networks, based on matricial pseudoinversion (Poggio and Girosi, 1990), also for SLFN learning was initially suggested in (Cancelliere, 2001); some appealing techniques were than developed (Nguyen et al., 2010; Kohno et al., 2010; Ajorloo et al., 2007) and among them the extreme learning machine ELM, (Huang et al.", "startOffset": 284, "endOffset": 347}, {"referenceID": 10, "context": ", 2007) and among them the extreme learning machine ELM, (Huang et al., 2006) which has been successfully applied to a number of real-world applications (Sun et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 21, "context": ", 2006) which has been successfully applied to a number of real-world applications (Sun et al., 2008; Wang et al., 2008; Malathi et al., 2010; Minhas et al., 2010), showing a good generalization performance with an extremely fast learning speed.", "startOffset": 83, "endOffset": 163}, {"referenceID": 24, "context": ", 2006) which has been successfully applied to a number of real-world applications (Sun et al., 2008; Wang et al., 2008; Malathi et al., 2010; Minhas et al., 2010), showing a good generalization performance with an extremely fast learning speed.", "startOffset": 83, "endOffset": 163}, {"referenceID": 13, "context": ", 2006) which has been successfully applied to a number of real-world applications (Sun et al., 2008; Wang et al., 2008; Malathi et al., 2010; Minhas et al., 2010), showing a good generalization performance with an extremely fast learning speed.", "startOffset": 83, "endOffset": 163}, {"referenceID": 15, "context": ", 2006) which has been successfully applied to a number of real-world applications (Sun et al., 2008; Wang et al., 2008; Malathi et al., 2010; Minhas et al., 2010), showing a good generalization performance with an extremely fast learning speed.", "startOffset": 83, "endOffset": 163}, {"referenceID": 25, "context": "However, such techniques seem to require more hidden units than typical values from backpropagation training to achieve comparable accuracy, as discussed in Yu and Deng (Yu and Deng, 2012).", "startOffset": 169, "endOffset": 188}, {"referenceID": 5, "context": "One aim of this paper is the analysis of these instability issues; a preliminary assessment of the context and our initial results are discussed in (Cancelliere et al., 2012).", "startOffset": 148, "endOffset": 174}, {"referenceID": 17, "context": "in (Penrose and Todd, 1956; Bishop, 2006), is W\u0304 = HT, where H is the Moore-Penrose generalised inverse (or pseudoinverse) of matrixH.", "startOffset": 3, "endOffset": 41}, {"referenceID": 3, "context": "in (Penrose and Todd, 1956; Bishop, 2006), is W\u0304 = HT, where H is the Moore-Penrose generalised inverse (or pseudoinverse) of matrixH.", "startOffset": 3, "endOffset": 41}, {"referenceID": 19, "context": "where \u03a3 is obtained from \u03a3 by taking the reciprocal of each non-zero element \u03c3i, and transposing the resulting matrix (Rao and Mitra, 1971).", "startOffset": 118, "endOffset": 139}, {"referenceID": 1, "context": "roughly speaking into a problem insensitive to small changes in training conditions, regularisation methods are often used (Badeva and Morozov, 1991), and Tikhonov regularisation is one of the most common (Tikhonov and Arsenin, 1977; Tikhonov, 1963).", "startOffset": 123, "endOffset": 149}, {"referenceID": 22, "context": "roughly speaking into a problem insensitive to small changes in training conditions, regularisation methods are often used (Badeva and Morozov, 1991), and Tikhonov regularisation is one of the most common (Tikhonov and Arsenin, 1977; Tikhonov, 1963).", "startOffset": 205, "endOffset": 249}, {"referenceID": 23, "context": "roughly speaking into a problem insensitive to small changes in training conditions, regularisation methods are often used (Badeva and Morozov, 1991), and Tikhonov regularisation is one of the most common (Tikhonov and Arsenin, 1977; Tikhonov, 1963).", "startOffset": 205, "endOffset": 249}, {"referenceID": 7, "context": "The penalty term improves on stability, making the problem less sensitive to initial conditions, and contains model complexity avoiding overfitting, as largely discussed in (Gallinari and Cibas, 1999).", "startOffset": 173, "endOffset": 200}, {"referenceID": 3, "context": "To give preference to solutions \u0174 with smaller norm (Bishop, 2006) a frequent choice is \u0393 = \u221a \u03bbI, so eqs.", "startOffset": 52, "endOffset": 66}, {"referenceID": 6, "context": "(Fuhry and Reichel, 2012)) as: \u0174 = V DU T (8)", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "Some numerical instability issues have already been evidenced in our previous investigations (Cancelliere et al., 2012); we provided suggestions on possible mitigation techniques like selection of a convenient activation function and normalisation of the input weights.", "startOffset": 93, "endOffset": 119}, {"referenceID": 2, "context": "The use of sigmoidal activation functions has recently been subject of debate because they seem to be more easily driven towards saturation due to their nonzero mean value (Bengio and Glorot, 2010), while hyperbolic tangent seems less sensitive to this problem.", "startOffset": 172, "endOffset": 197}, {"referenceID": 5, "context": "To further mitigate saturation issues, in our previous work (Cancelliere et al., 2012) we selected input weights according to a uniform random distribution in the range (\u22121/ \u221a M , 1/ \u221a M), where M is the number of hidden nodes.", "startOffset": 60, "endOffset": 86}, {"referenceID": 9, "context": "Because of the wide use among researchers belonging to ELM-community (see for instance (Helmy and Rasheed, 2009; Huang et al., 2006; Sun et al., 2008) our performance is also compared with that from unregularised pseudoinversion, input weights selected according to a random uniform distribution in the interval (\u22121,1) and sigmoidal activation functions (hereafter, ELM).", "startOffset": 87, "endOffset": 150}, {"referenceID": 10, "context": "Because of the wide use among researchers belonging to ELM-community (see for instance (Helmy and Rasheed, 2009; Huang et al., 2006; Sun et al., 2008) our performance is also compared with that from unregularised pseudoinversion, input weights selected according to a random uniform distribution in the interval (\u22121,1) and sigmoidal activation functions (hereafter, ELM).", "startOffset": 87, "endOffset": 150}, {"referenceID": 21, "context": "Because of the wide use among researchers belonging to ELM-community (see for instance (Helmy and Rasheed, 2009; Huang et al., 2006; Sun et al., 2008) our performance is also compared with that from unregularised pseudoinversion, input weights selected according to a random uniform distribution in the interval (\u22121,1) and sigmoidal activation functions (hereafter, ELM).", "startOffset": 87, "endOffset": 150}, {"referenceID": 5, "context": "The same trend was detected analysing the astronomical dataset in (Cancelliere et al., 2012).", "startOffset": 66, "endOffset": 92}, {"referenceID": 5, "context": "On the contrary, when weights are randomly selected in the interval (\u22121/ \u221a M, 1/ \u221a M), as for HypT-unreg and Sigm-unreg cases, input weights are automatically kept small when the network size increases, thus exploiting the central part of both activation functions: consequently saturation is avoided (Cancelliere et al., 2012).", "startOffset": 301, "endOffset": 327}, {"referenceID": 25, "context": "(Yu and Deng, 2012)).", "startOffset": 0, "endOffset": 19}, {"referenceID": 10, "context": "The interested reader can find, for the common datatsets, a comparison among training times of ELM and backpropagation in (Huang et al., 2006), and can verify that ELM turns out to be two or three orders of magnitude faster.", "startOffset": 122, "endOffset": 142}, {"referenceID": 14, "context": "(Miche et al., 2011), we note that our technique achieves RMSE values lower than those corresponding to their MSE values, with a", "startOffset": 0, "endOffset": 20}], "year": 2017, "abstractText": "Some novel strategies have recently been proposed for single hidden layer neural network training that set randomly the weights from input to hidden layer, while weights from hidden to output layer are analytically determined by pseudoinversion. These techniques are gaining popularity in spite of their known numerical issues when singular and/or almost singular matrices are involved. In this paper we discuss a critical use of Singular Value Analysis for identification of these drawbacks and we propose an original use of regularisation to determine the output weights, based on the concept of critical hidden layer size. This approach also allows to limit the training computational effort. Besides, we introduce a novel technique which relies an effective determination of input weights to the hidden layer dimension. This approach is tested for both regression and classification tasks, resulting in a significant performance improvement with respect to alternative methods.", "creator": "LaTeX with hyperref package"}}}