{"id": "1508.06669", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2015", "title": "Component-Enhanced Chinese Character Embeddings", "abstract": "distributed natural word representations are very wildly useful for capturing semantic common information ; and however have not been successfully applied in utilizing a significant variety of nlp tasks, especially solely on english. incorporated in this combined work, we still innovatively develop two digital component - space enhanced chinese character network embedding modeling models and their bigram extensions. distinguished generally from english word embeddings, our models actually explore the compositions system of chinese characters, which often incorrectly serve as dynamic semantic indictors inherently. the evaluations on solving both word frame similarity and hierarchical text classification demonstrate now the theoretical effectiveness requirements of our semantic models.", "histories": [["v1", "Wed, 26 Aug 2015 21:25:25 GMT  (150kb)", "http://arxiv.org/abs/1508.06669v1", "6 pages, 2 figures, conference, EMNLP 2015"]], "COMMENTS": "6 pages, 2 figures, conference, EMNLP 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yanran li", "wenjie li", "fei sun", "sujian li"], "accepted": true, "id": "1508.06669"}, "pdf": {"name": "1508.06669.pdf", "metadata": {"source": "CRF", "title": "Component-Enhanced Chinese Character Embeddings", "authors": ["Yanran Li", "Wenjie Li", "Fei Sun", "Sujian Li"], "emails": ["cswjli}@comp.polyu.edu.hk,", "ofey.sunfei@gmail.com,", "lisujian@pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Due to its advantage over traditional one-hot representation, distributed word representation has demonstrated its benefit for semantic representation in various NLP tasks. Among the existing approaches (Huang et al., 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al., 2013a; Mikolov et al., 2013b). These two models learn the distributed representation of a word based on its context. The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the wordswith important and relevantmeanings (Levy and Goldberg, 2014). To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze,\n2014; Bansal et al., 2014) or directly keeping the order features (Ling et al., 2015). As to another line, Luong et al. (2013) captures morphological composition by using neural networks and Qiu et al. (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al. (2013) extracts the syntactical morphemes and Cheng et al. (2014) incorporates the POS tags and dependency relations. Basically, the work in Chinese follows the same ideas as in English. Distinguished from English, Chinese characters are logograms, of which over 80% are phonosemantic compounds, with a semantic component giving a broad category of meaning and a phonetic component suggesting the sound1. For example, the semantic component \u4ebb (human) of the Chinese character\u4ed6 (he) provides the meaning connected with human. In fact, the components of most Chinese characters inherently bring with certain levels of semantics regardless of the contexts. Being aware that the components of Chinese characters are finer grained semantic units, then an important question arises before slipping to the applications of word embeddings\u2014would it be better to learn the semantic representations from the character components in Chinese? We approach this question from both the practical and the cognitive points of view. In practice, we expect the representations to be optimized for good generalization. As analyzed before, the components are more generic unit inside Chinese characters that provides semantics. Such inherent information somehow alleviates the shortcoming of the external contexts. From the cognitive point of view, it has been found that the knowledge of semantic components significantly corre-\n1http://en.wikipedia.org/wiki/Radical_ (Chinese_characters)\nlate to Chinese word reading and sentence comprehension (Ho et al., 2003). These evidences inspire us to explore novel Chinese character embedding models. Different from word embeddings, character embeddings relate Chinese characters that occur in similar contexts with their component information. Chinese characters convey the meanings from their components, and beyond that, the meanings of most Chinese words also take roots in their composite characters. For example, the meaning of the Chinese word\u6447\u7bee (cradle) can be interpreted in terms of its composite characters\u6447 (sway) and\u7bee (basket). Considering this, we further extend character embeddings from uni-gram models to bi-gram models. At the core of our work is the exploration of Chinese semantic representations from a novel character-based perspective. Our proposed Chinese character embeddings incorporate the finergrained semantics from the components of characters and in turn enrich the representations inherently in addition to utilizing the external contexts. The evaluations on both intrinsic word similarity and extrinsic text classification demonstrate the effectiveness and potentials of the new models."}, {"heading": "2 Component-Enhanced Character Embeddings", "text": "Chinese characters are often composed of smaller and primitive components called radicals or radical-like components, which serve as the most basic units for building character meanings. Dating back to the 2nd century AD, the Han dynasty scholar Shen XU organizes his etymological dictionary shu\u014d w\u00e9n ji\u011b z\u00ec (word and expression) by selecting 540 recurring graphic components that he called b\u00f9 (means \u201ccategories\u201d). B\u00f9 is nearly the same as what we call radicals today2. Most radicals are common semantic components. Over time, some original radicals evolve into radicallike components. Nowadays, a Chinese character often contains exactly one radical (rarely has two) and several other radical-like components. In what follows, we refer to as components both radicals and radical-like components. Distinguished from English, these composite components are unique and inherent features inside Chinese characters. A lot of times, they allow\n2http://en.wikipedia.org/wiki/Radical_ (Chinese_characters)\nus to assumingly understand or infer the meanings of characters without any context. In other words, the component-level features inherently bring with additional information that benefits semantic representations of characters. For example, we know that the characters\u4f60 (you),\u4ed6 (he),\u4f19 (companion),\u4fa3 (companion), and\u4eec (people) all have the meanings related to human because of their shared component \u4ebb (human), a variant of the Chinese character\u4eba (human). This kind of component information is intrinsically different from the contexts deriving by dependency relations and POS tags. It motivates us to investigate the componentenhanced Chinese character embedding models. While Sun et al. (2014) utilizes radical information in a supervised fashion, we build our models in a holistic unsupervised and bottom-up way. It is important to note the variation of a radical inside a character. There are two types of variations. The main type is position-related. For example, the radical of the Chinese character\u6c34 (water) is itself, but it becomes \u6c35 as the radical of \u6c60 (pool). The original radicals are stretched or squeezed so that they can fit into the general Chinese character shape of a square. The second variation type emerges along with the history of character simplification when traditional characters are converted into simplified characters. For instance, \u98df (eat) is written as \u98e0 when it forms as a part of some traditional characters, but is written as\u9963 in simplified characters. To cope with these variations and recover the semantics, we match all the radical variants back into their original forms. We extract all the components to build a component list for each Chinese character. With the assumption that a character\u2019s radical often bring more important semantics than the rest3, we regard the radical of a character as the first component in its component list. Let a sequence of charactersD = {z1, . . . , zN} denotes a corpus of N characters over the character vocabulary V . And z, c, e,K, T,M, |V | denote the Chinese character, the context character, the component list, the corresponding embedding dimension, the context window size, the number of components taken into account for each character, and the vocabulary size, respectively. We develop two component-enhanced character embedding models, namely charCBOW\n3Inside a character, its radical often serves as the semanticcomponent while its other radical-like components may be phonetics.\nand charSkipGram. charCBOW follows the original continuous bag-of-words model (CBOW) proposed by (Mikolov et al., 2013a). We predict the central character zi conditioned on a 2(M+1)TKdimensional vector that is the concatenation of the remaining character-level contexts (ci\u2212T , . . . , ci\u22121, ci+1, . . . , ci+T ) and the components in their component lists. More formally, we wish to maximize the log likelihood of all the characters as follows,\nL = \u2211 zni \u2208D log p(zi|hi),\nhi = cat(ci\u2212T , ei\u2212T , . . . , ci+T , ei+T )\nwhere hi denotes the concatenation of the component-enhanced contexts. We make prediction using a 2KT (M+1)|V |-dimensional matrix O. Different from the original CBOW model, the extra parameter introduced in the matrix O allows us to maintain the relative order of the components and treat the radical differently from the rest components. The development of charSkipGram is straightforward. We derive the component-enhanced contexts as (\u27e8ci\u2212T , ei\u2212T \u27e9, . . . , \u27e8ci+T , ei+T \u27e9) based on the central character zi. The sum of log probabilities given zi is maximized:\nL = \u2211 zi\u2208D T\u2211 j=\u2212T j \u0338=0 ( log p(cj+i|zi) + log p(ej+i|zi) )\nFigure 1 illustrates the two componentenhanced character embedding models. It is easy to extend charCBOW and charSkipGram to their corresponding bi-character extensions. Denote the zi, ci and ei in charCBOW and charSkipGram as uni-character zui, cui and eui, the bi-character extensions are the models fed by bi-character formed zbi, cbi and ebi."}, {"heading": "3 Evaluations", "text": "We examine the quality of the proposed two Chinese character embedding models as well as their corresponding extensions on both intrinsic word similarity evaluation and extrinsic text classification evaluation. Word Similarity. As the widely used public word similarity datasets like WS-353 (Finkelstein et al., 2001), RG-65 (Rubenstein and Goodenough, 1965) are built for English embeddings,\nOUTPUTPROJECTIONINPUT\nwe start from developing appropriate Chinese synonym sets. Two candidate choices are Chinese dictionaries HowNet (Dong and Dong, 2006) and HIT-CIR\u2019s Extended Tongyici Cilin (denoted as E-TC)4. As HowNet contains less modern words, such as \u8c37\u6b4c (Google), we select E-TC as our benchmark for word similarity evaluation. Text Classification. We use Tencent news titles as our text classification dataset5. A total of 8,826 titles of four categories (society, entertainment, healthcare, and military) are extracted. The lengths of titles range from 10 to 20 words. We train \u21132-regularized logistic regression classifiers using the LIBLINEAR package (Fan et al., 2008) with the learned embeddings. To build the component-enhanced character embeddings, we employ the GB2312 character set\n4http://ir.hit.edu.cn/demo/ltp/Sharing_ Plan.htm\n5http://www.datatang.com/data/44341\nand extract all their component lists. It is easy to obtain the first components (i.e., the radicals), as they are readily available in the online Xinhua Dictionary6. For the rest radical-like components, we extract them by matching the patterns like \u201c\u4ece (from)+X\u201d in the Xinhua dictionary. Such a pattern indicates that a character has a component of X. We also enrich the component lists by matching the pattern \u201cX is only seen\u201d in Hong Kong Computer Chinese Basic Component Reference7 . It is observed that nearly 65% Chinese characters have only one component (their radicals), and 95% Chinese characters have two components (including their radicals). Thus, we decide to maintain up to two extracted components to build the character embeddings according to the frequency of their occurrences. To cope with the radical variation problem, we transform 24 radical variants to their origins, such as \u4ebb to \u4eba (human), \u624c to \u624b (hand), \u6c35 to\u6c34 (water) and\u8fb6 to\u8fb5 (foot). The complete list of the transformations is provided in\n6http://xh.5156edu.com/ 7http://www.ogcio.gov.hk/tc/business/tech_\npromotion/ccli/cliac/glyphs_guidelines.htm\nAppendix for easy reference. We adopt Chinese Wikipedia Dump8 to train our models as well as the original CBOW and SkipGram, implemented in the Word2Vec tool9 for comparison. The corpus in total contains 232,894 articles. In preprocessing, we remove pure digit words and non-Chinese characters, and ignore the words less than 10 occurrences during training. We set the context window size T as 2 and use 5 negative samples experimentally. All the embedding dimensionsK are set to 50. In the word similarity evaluation, we compute the Spearman\u2019s rank correlation (Myers et al., 2010) between the similarity scores based on the learned embedding models and the E-TC similarity scores computed by following Jiu-le and Wei (2010). The bi-character embeddings are concatenation of the composite character embeddings. For the text classification evaluation, we average the composite single character embeddings for each bi-gram. And each bi-gram overlaps with the previous one. The titles are represented by averaging\n8http://download.wikipedia.com/zhwiki/ 9https://code.google.com/p/word2vec/\nthe embeddings of their composite grams10. Table 1 presents the word similarity evaluation results of the eight embedding models mentioned above, where A\u2013L denote the twelve categories in E-TC. The first four rows are the results with the uni-character inputs, and the last four rows correspond to the bi-character embeddings results. We can see that both CBOW and CBOW-bi perform worse than the corresponding SkipGram and SkipGram-bi. This result is consistent with the finding in the previous work (Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015). To some extent, CBOW and its extension CBOW-bi are the most different among the eight (the first four models in Table 1 and the first four models in Table 2). They tie together the characters in each context window by representing the context vector as the sum of their characters\u2019 vectors. Although they have a potential of deriving better representations (Levy et al., 2015), they lose some particular information from each unit of input in the average operations. Although the performance on twelve different categories varies, in overall charCBOW, charSkipGram and their extensions consistently better correlate to E-TC. It provides the evidence that the component information in Chinese characters is of significance. Clearly, the bi-character models achieve higher rank correlations. These results are not surprised. As amatter of fact, a majority of Chinesewords are compounds of two characters. Thus, in many cases two characters together is equivalent to a Chinese word. Considering the superiority of the bi-character models, we only apply them in the text classification evaluations. The results shown in the first four rows of Table 2 are similar to those in the word similarity evaluation. Please notice the significant improvement of charCBOW and charCBOW-bi. We conjecture this as a hint of the importance of the order information, which is introduced by the extra parameter in the output matrixes. Their better performances verify our assumption that the radicals are more important than non-radicals. This is also attributed to the benefit from the order of the characters in the contexts. Actually, we also conduct an additional experiment to combine the uni-gram and the bi-gram embeddings for text classification and notice in aver-\n10We do not compare the uni-formed characters with biformed compound characters. The word pairs that cannot be found in the vocabulary are removed.\nage about 8.4% of gain over the bi-gram embeddings alone. The detailed results are presented in the last four rows of Table 2."}, {"heading": "4 Conclusions and Future Work", "text": "In this paper, we propose two componentenhanced Chinese character embedding models and their extensions to explore both the internal compositions and the external contexts of Chinese characters. Experimental results demonstrate their benefits in learning rich semantic representations. For the future work, we plan to devise embedding models based together on the composition of component-character and of character-word. The two types of compositions will serve in a coordinate fashion for the distributional representations."}, {"heading": "Acknowledgements", "text": "The work described in this paper was supported by the grants from the Research Grants Council of Hong Kong (PolyU 5202/12E and PolyU 152094/14E), the grants from the National Natural Science Foundation of China (61272291 and 61273278) and a PolyU internal grant (4-BCB5).\nAppendix\nAs mentioned in Section 3, we present the complete list of transformations of the variant and original forms of 24 radicals. The meaning columns provide the corresponding meanings of the components in the left.\ntransform meaning transform meaning \u8279 \u2192 \u8278 grass \u624c \u2192 \u624b hand \u4ebb \u2192 \u4eba human \u6c35 \u2192 \u6c34 water \u5202 \u2192 \u5200 knife \u8eca \u2192 \u8f66 vehicle \u72ad \u2192 \u72ac dog \u6535 \u2192 \u6534 hit \u706c \u2192 \u706b fire \u7e9f \u2192 \u7cf8 silk \u9485 \u2192 \u91d1 gold \u8002 \u2192 \u8001 old \u9ea5 \u2192 \u9ea6 wheat \u725c \u2192 \u725b cattle \u9963 \u2192 \u98df eat \u98e0 \u2192 \u98df eat \u793b \u2192 \u793a memory \u5fc4 \u2192 \u5fc3 heart \u7f52 \u2192 \u7f51 nest \u738b \u2192 \u7389 jade \u8ba0 \u2192 \u8a00 speak \u8864 \u2192 \u8863 cloth \u6708 \u2192 \u8089 body \u8fb6 \u2192 \u8fb5 walk"}], "references": [{"title": "Tailoring continuous word representations for dependency parsing", "author": ["References Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Parsing chinese synthetic words with a characterbased dependency model", "author": ["Fei Cheng", "Kevin Duh", "Yuji Matsumoto"], "venue": null, "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "HowNet and the Computation of Meaning", "author": ["Zhendong Dong", "Qiang Dong."], "venue": "World Scientific.", "citeRegEx": "Dong and Dong.,? 2006", "shortCiteRegEx": "Dong and Dong.", "year": 2006}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin."], "venue": "The Journal of Machine Learning Research, 9:1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th international conference on World Wide Web, pages 406\u2013", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "A ?radical? approach to reading development in chinese: The role of semantic radicals and phonetic radicals", "author": ["Connie Suk-Han Ho", "Ting-Ting Ng", "Wing-Kin Ng."], "venue": "Journal of Literacy Research, 35(3): 849\u2013878.", "citeRegEx": "Ho et al\\.,? 2003", "shortCiteRegEx": "Ho et al\\.", "year": 2003}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Words similarity algorithm based on tongyici cilin in semantic web adaptive learning system [j", "author": ["TIAN Jiu-le", "ZHAO Wei."], "venue": "Journal of Jilin University (Information Science Edition), 6:010.", "citeRegEx": "Jiu.le and Wei.,? 2010", "shortCiteRegEx": "Jiu.le and Wei.", "year": 2010}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 2, pages 302\u2013308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Two/ too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning."], "venue": "CoNLL-2013, 104.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv: 1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Research design and statistical analysis", "author": ["Jerome L Myers", "Arnold Well", "Robert Frederick Lorch."], "venue": "Routledge.", "citeRegEx": "Myers et al\\.,? 2010", "shortCiteRegEx": "Myers et al\\.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12:1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Co-learning of word representations and morpheme representations", "author": ["Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "Tie-Yan Liu."], "venue": "COLING.", "citeRegEx": "Qiu et al\\.,? 2014", "shortCiteRegEx": "Qiu et al\\.", "year": 2014}, {"title": "Contextual correlates of synonymy", "author": ["Herbert Rubenstein", "John B Goodenough."], "venue": "Communications of the ACM, 8(10):627\u2013633.", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Radical-enhanced chinese character embedding", "author": ["Yaming Sun", "Lei Lin", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang."], "venue": "Neural Information Processing, pages 279\u2013286. Springer.", "citeRegEx": "Sun et al\\.,? 2014", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Unsupervised multi-domain adaptation with feature embeddings", "author": ["Yi Yang", "Jacob Eisenstein"], "venue": null, "citeRegEx": "Yang and Eisenstein.,? \\Q2015\\E", "shortCiteRegEx": "Yang and Eisenstein.", "year": 2015}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Mo Yu", "Mark Dredze."], "venue": "Association for Computational Linguistics (ACL), pages 545\u2013550.", "citeRegEx": "Yu and Dredze.,? 2014", "shortCiteRegEx": "Yu and Dredze.", "year": 2014}, {"title": "Chinese parsing exploiting characters", "author": ["Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu."], "venue": "ACL (1), pages 125\u2013134.", "citeRegEx": "Zhang et al\\.,? 2013", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "Among the existing approaches (Huang et al., 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al.", "startOffset": 30, "endOffset": 102}, {"referenceID": 8, "context": "Among the existing approaches (Huang et al., 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al.", "startOffset": 30, "endOffset": 102}, {"referenceID": 19, "context": "Among the existing approaches (Huang et al., 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al.", "startOffset": 30, "endOffset": 102}, {"referenceID": 12, "context": ", 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al., 2013a; Mikolov et al., 2013b).", "startOffset": 230, "endOffset": 276}, {"referenceID": 13, "context": ", 2012; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), the continuous bag-of-wordsmodel (CBOW) and the continuous skip-gram model (SkipGram) remain the most popular ones that one can use to build word embeddings efficiently (Mikolov et al., 2013a; Mikolov et al., 2013b).", "startOffset": 230, "endOffset": 276}, {"referenceID": 8, "context": "The context defined by the window of surrounding words may unavoidably include certain less semantically-relevant words and/or miss the wordswith important and relevantmeanings (Levy and Goldberg, 2014).", "startOffset": 177, "endOffset": 202}, {"referenceID": 8, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al.", "startOffset": 202, "endOffset": 269}, {"referenceID": 20, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al.", "startOffset": 202, "endOffset": 269}, {"referenceID": 0, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al.", "startOffset": 202, "endOffset": 269}, {"referenceID": 10, "context": ", 2014) or directly keeping the order features (Ling et al., 2015).", "startOffset": 47, "endOffset": 66}, {"referenceID": 0, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al., 2015). As to another line, Luong et al. (2013) captures morphological composition by using neural networks and Qiu et al.", "startOffset": 249, "endOffset": 370}, {"referenceID": 0, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al., 2015). As to another line, Luong et al. (2013) captures morphological composition by using neural networks and Qiu et al. (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework.", "startOffset": 249, "endOffset": 452}, {"referenceID": 0, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al., 2015). As to another line, Luong et al. (2013) captures morphological composition by using neural networks and Qiu et al. (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al. (2013) extracts the syntactical morphemes and Cheng et al.", "startOffset": 249, "endOffset": 690}, {"referenceID": 0, "context": "To overcome this shortcoming, a line of research deploys the order information of the words in the contexts by either deriving the contexts using dependency relations where the target word participates (Levy and Goldberg, 2014; Yu and Dredze, 2014; Bansal et al., 2014) or directly keeping the order features (Ling et al., 2015). As to another line, Luong et al. (2013) captures morphological composition by using neural networks and Qiu et al. (2014) introduces the morphological knowledge as both additional input representation and auxiliary supervision to the neural network framework. While most previous work focuses on English, there is a little work on Chinese. Zhang et al. (2013) extracts the syntactical morphemes and Cheng et al. (2014) incorporates the POS tags and dependency relations.", "startOffset": 249, "endOffset": 749}, {"referenceID": 5, "context": "late to Chinese word reading and sentence comprehension (Ho et al., 2003).", "startOffset": 56, "endOffset": 73}, {"referenceID": 18, "context": "While Sun et al. (2014) utilizes radical information in a supervised fashion, we build our models in a holistic unsupervised and bottom-up way.", "startOffset": 6, "endOffset": 24}, {"referenceID": 12, "context": "charCBOW follows the original continuous bag-of-words model (CBOW) proposed by (Mikolov et al., 2013a).", "startOffset": 79, "endOffset": 102}, {"referenceID": 4, "context": "As the widely used public word similarity datasets like WS-353 (Finkelstein et al., 2001), RG-65 (Rubenstein and Goodenough, 1965) are built for English embeddings, .", "startOffset": 63, "endOffset": 89}, {"referenceID": 17, "context": ", 2001), RG-65 (Rubenstein and Goodenough, 1965) are built for English embeddings, .", "startOffset": 15, "endOffset": 48}, {"referenceID": 2, "context": "Two candidate choices are Chinese dictionaries HowNet (Dong and Dong, 2006) and HIT-CIR\u2019s Extended Tongyici Cilin (denoted as E-TC)4.", "startOffset": 54, "endOffset": 75}, {"referenceID": 3, "context": "We train l2-regularized logistic regression classifiers using the LIBLINEAR package (Fan et al., 2008) with the learned embeddings.", "startOffset": 84, "endOffset": 102}, {"referenceID": 14, "context": "In the word similarity evaluation, we compute the Spearman\u2019s rank correlation (Myers et al., 2010) between the similarity scores based on the learned embedding models and the E-TC similarity scores computed by following Jiu-le and Wei (2010).", "startOffset": 78, "endOffset": 98}, {"referenceID": 7, "context": ", 2010) between the similarity scores based on the learned embedding models and the E-TC similarity scores computed by following Jiu-le and Wei (2010). The bi-character embeddings are concatenation of the composite character embeddings.", "startOffset": 129, "endOffset": 151}, {"referenceID": 15, "context": "This result is consistent with the finding in the previous work (Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015).", "startOffset": 64, "endOffset": 133}, {"referenceID": 8, "context": "This result is consistent with the finding in the previous work (Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015).", "startOffset": 64, "endOffset": 133}, {"referenceID": 9, "context": "This result is consistent with the finding in the previous work (Pennington et al., 2014; Levy and Goldberg, 2014; Levy et al., 2015).", "startOffset": 64, "endOffset": 133}, {"referenceID": 9, "context": "Although they have a potential of deriving better representations (Levy et al., 2015), they lose some particular information from each unit of input in the average operations.", "startOffset": 66, "endOffset": 85}], "year": 2015, "abstractText": "Distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of NLP tasks, especially on English. In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models.", "creator": " XeTeX output 2015.08.26:0516"}}}