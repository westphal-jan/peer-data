{"id": "1509.01349", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2015", "title": "Parallel and Distributed Approaches for Graph Based Semi-supervised Learning", "abstract": "the two approaches developing for comparing graph data based exponential semi - adaptive supervised learning are simultaneously proposed. the firstapproach is already based originally on iteration of sampling an affine map. a commonly key missing element transformation of the affine exponential map iteration is sparsematrix - gradient vector multiplication, itself which has several very efficient parallel filtering implementations. initially the same secondapproach algorithms belongs higher to practically the advanced class of markov sequential chain monte carlo ( mcmc ) algorithms. later it is based onsampling of nodes vertices by performing a random walk on the graph. the latter approach is distributedby its nature and can too be easily extensively implemented add on to several different processors / or projects over the host network. boththeoretical and easy practical evaluations are provided. it is unanimously found that the worst nodes are consistently classified originally intotheir class with nearly very small error. the sampling tree algorithm'original s conditional ability method to track various new incoming nodesand to classify them later is clearly also demonstrated.", "histories": [["v1", "Fri, 4 Sep 2015 06:35:55 GMT  (3325kb,D)", "http://arxiv.org/abs/1509.01349v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["konstantin avrachenkov", "vivek borkar", "krishnakant saboo"], "accepted": false, "id": "1509.01349"}, "pdf": {"name": "1509.01349.pdf", "metadata": {"source": "CRF", "title": "Parallel and Distributed Approaches for Graph Based Semi-supervised Learning", "authors": ["K. Avrachenkov", "V.S. Borkar", "K. Saboo"], "emails": ["k.avrachenkov@inria.fr", "borkar.vs@gmail.com", "kvsaboo.2004@ee.iitb.ac.in"], "sections": [{"heading": null, "text": "IS S\nN 02\n49 -6\n39 9\nIS R\nN IN\nR IA\n/R R\n-- 87\n67 --\nFR +E\nN G\nRESEARCH REPORT N\u00b0 8767 August 2015\nProject-Team Maestro"}, {"heading": "Parallel and Distributed", "text": ""}, {"heading": "Approaches for", "text": ""}, {"heading": "Graph Based", "text": ""}, {"heading": "Semi-supervised", "text": ""}, {"heading": "Learning", "text": "K. Avrachenkov, V.S. Borkar, K. Saboo\nar X\niv :1\n50 9.\n01 34\n9v 1\n[ cs\n.L G\n] 4\nS ep\n2 01\n5\nRESEARCH CENTRE SOPHIA ANTIPOLIS \u2013 M\u00c9DITERRAN\u00c9E\n2004 route des Lucioles - BP 93 06902 Sophia Antipolis Cedex\nParallel and Distributed Approaches for Graph Based Semi-supervised Learning\nK. Avrachenkov\u2217, V.S. Borkar \u2020, K. Saboo\u2021 \u00a7\nProject-Team Maestro\nResearch Report n\u00b0 8767 \u2014 August 2015 \u2014 21 pages\nAbstract: Two approaches for graph based semi-supervised learning are proposed. The first approach is based on iteration of an affine map. A key element of the affine map iteration is sparse matrix-vector multiplication, which has several very efficient parallel implementations. The second approach belongs to the class of Markov Chain Monte Carlo (MCMC) algorithms. It is based on sampling of nodes by performing a random walk on the graph. The latter approach is distributed by its nature and can be easily implemented on several processors or over the network. Both theoretical and practical evaluations are provided. It is found that the nodes are classified into their class with very small error. The sampling algorithm\u2019s ability to track new incoming nodes and to classify them is also demonstrated.\nKey-words: Semi-supervised learning, Graph-based learning, Distributed algorithms, Parallel algorithms\n\u2217 K. Avrachenkov is with Inria Sophia Antipolis, France, k.avrachenkov@inria.fr \u2020 V.S. Borkar is with Department of Electrical Engineering, IIT Bombay, India, borkar.vs@gmail.com \u2021 K. Saboo is with Department of Electrical Engineering, IIT Bombay, India, kvsaboo.2004@ee.iitb.ac.in \u00a7 The work of KA and VSB was supported in part by grant no. 5100-ITA from the Indo-French Centre for the\nPromotion of Advanced Research (IFCPAR) and Alcatel-Lucent Inria Joint Lab."}, {"heading": "Les Approches Parall\u00e8les et Distribu\u00e9s pour l\u2019Apprentissage", "text": "Semi-supervis\u00e9\nR\u00e9sum\u00e9 : Deux approches pour l\u2019apprentissage semi-supervis\u00e9 bas\u00e9 sur le graphe de similarit\u00e9 sont propos\u00e9s. La premi\u00e8re approche est bas\u00e9e sur l\u2019it\u00e9ration d\u2019un op\u00e9rateur affine. Un \u00e9l\u00e9ment cl\u00e9 de l\u2019it\u00e9ration de l\u2019op\u00e9rateur affine est la multiplication vecteur par matrice de type sparse, qui a plusieurs impl\u00e9mentations parall\u00e8les tr\u00e8s efficaces. La seconde approche appartient \u00e0 la classe des algorithmes de Monte-Carlo par cha\u00eenes de Markov (MCMC). Elle est bas\u00e9 sur un \u00e9chantillonnage de noeuds en effectuant une marche al\u00e9atoire sur le graphe de similarit\u00e9. Cette derni\u00e8re approche est distribu\u00e9 par sa nature et peut \u00eatre facilement mis en oeuvre sur plusieurs processeurs ou sur un r\u00e9seau. \u00c9valuations th\u00e9oriques ainsi que pratiques sont fournis. On constate que les noeuds sont class\u00e9s dans leurs classes avec tr\u00e8s petite erreur. La capacit\u00e9 de l\u2019algorithme MCMC de suivre les nouveaux noeuds arrivants online et de les classer est \u00e9galement d\u00e9montr\u00e9.\nMots-cl\u00e9s : Apprentissage semi-supervis\u00e9, Apprentissage bas\u00e9 sur le graphe de similarit\u00e9, Algorithmes distribu\u00e9s, Algorithmes parall\u00e8les"}, {"heading": "Distributed Semi-supervised Learning 3", "text": ""}, {"heading": "Contents", "text": ""}, {"heading": "1 Introduction 4", "text": ""}, {"heading": "2 Optimization Problem and Algorithms 4", "text": ""}, {"heading": "3 Convergence Analysis 6", "text": ""}, {"heading": "4 Experiments and Results 8", "text": "4.1 Les Miserables graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 4.2 WebKB Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.3 Gaussian Mixture Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.4 Tracking of new incoming nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.5 Stochastic Block Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13"}, {"heading": "5 Conclusion 15", "text": "RR n\u00b0 8767"}, {"heading": "4 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": ""}, {"heading": "1 Introduction", "text": "Semi-supervised learning is a special type of learning which uses labelled as well as unlabelled data for training [7]. In reality, the amount of labelled data is much less compared to the unlabelled data. This makes this type of learning a powerful tool for processing the huge amount of data available nowadays. The present work focuses on graph based semi-supervised learning [7, 9, 10, 18]. Consider a (weighted) graph in which nodes belong to one of K classes and the true class of a few nodes is given. An edge and its weight of the graph indicate a similarity and similarity degree, respectively, between two nodes. Thus, such graph is called the similarity graph. We assume that the (weighted) similarity graph is known. Semi-supervised learning aims at using the true class information of few labelled nodes and the structure of the graph to estimate the class of each of the remaining nodes.\nGraph based semi-supervised learning finds application in recommendation systems [9], classification of web pages into categories [16], person name disambiguation [14], etc. Being of practical importance, graph based semi-supervised learning has been widely studied. Most methods formulate the learning as an optimization problem for feature vectors and then solve it iteratively or by deriving closed form solutions. The problem assumes smoothness of node features across the graph and penalizes the deviation of a feature from the true class of a node in case the true class is known. Iterative solutions become computationally intensive because they involve matrix operations which grow in size as a polynomial with the graph size, though the growth can be linear or quasi-linear in the case of sparse graphs. In [2] the authors have formulated a generalized optimization problem which gives as important particular cases: Standard Laplacian [17], Normalised Laplacian [15] and PageRank [1] algorithms.\nIn the present work, starting from the generalized formulation of [2], we propose two approaches for the computation of the feature vectors. The first approach is based on iteration of an affine map. A key element of the affine map iteration is sparse matrix-vector multiplication, which has several very efficient parallel implementations.\nThe second approach belongs to the class of Markov Chain Monte Carlo algorithms. The concept of sampling as in reinforcement learning is useful because it provides room for developing distributed computing schemes which reduce cost of per iterate computation, a crucial advantage in large problems. The realm of reinforcement learning is similar to semi-supervised learning in that the labelled data is suggestive but not exact. Borkar et al. [5] have developed a sampling based reinforcement learning scheme for the PageRank algorithm. At each step of the iteration, a node is sampled uniformly from all the nodes. A neighbour of the sampled node is chosen according to the conditional probability of the neighbour given the sampled node. The stationary probability of the sampled node is then updated using a stochastic approximation of the ODE involving stationary distribution and the PageRank matrix. This approach can be implemented in a distributed setting and hence the requirement of the entire adjacency matrix being known locally can also be relaxed, since information regarding only a neighbourhood is required.\nThe rest of the article is organised as follows: Section 2 gives the algorithms, Section 3 proves their convergence, Section 4 details the experimental results and Section 5 concludes with some observations."}, {"heading": "2 Optimization Problem and Algorithms", "text": "Consider a graph with adjacency matrix A which has N nodes, each one belonging to one of K classes. The class information of some of the nodes is also given, which will be referred to as labelled nodes. Define D as a diagonal matrix with Dii = d(i), where d(i) is the degree of node i. Y is a N \u00d7K matrix which contains the information about the labelled nodes. More specifically,\nInria"}, {"heading": "Distributed Semi-supervised Learning 5", "text": "Yij = { 1 if labelled node i belongs to class j, 0 otherwise.\nF is a N \u00d7K matrix with each element as a real value. Fik represents the \u2018belongingness\u2019 of node i to class k. It is referred to as \u2018classification function\u2019 or \u2018feature vector\u2019. The aim of the semi-supervised learning problem is to find F.k such that it is close to the labelling function and it varies smoothly over the graph. The class of node i is calculated from F using the following relation. Node i belongs to class k if\nFik > Fij \u2200 j 6= k.\nThe optimization problem associated with the above stated requirements can be written as: Minimize\nQ(F ) = 2 K\u2211 k=1 FT.kD \u03c3\u22121LD\u03c3F.k + \u00b5 K\u2211 k=1 (F.k \u2212 Y.k)TD2\u03c3\u22121(F.k \u2212 Y.k),\nwhere L = D \u2212 A is the Standard Laplacian of the similarity graph. It can be verified that the above problem is a convex optimization problem. The first order optimality condition gives the following solution to this problem:\nF.k = (1\u2212 \u03b1)(I \u2212 \u03b1B)\u22121Y.k,\nwhere B = D\u2212\u03c3AD\u03c3\u22121 and \u03b1 = 22+\u00b5 for \u00b5 > 0 [2]. Rearranging the terms of the closed form solution gives the power iteration based algorithm:\nAlgorithm 1: F t+1 = \u03b1BF t + (1\u2212 \u03b1)Y,\nwhere F t is the estimate of F after t \u2265 0 iterations. In each iteration, the feature values for all the nodes are updated for all the classes. This algorithm is henceforth referred to as the power iteration algorithm. Note that the (sparse) matrix-vector multiplication BF t is the bottleneck of Algorithm 1. There is a number of very efficient parallel implementations of the matrix-vector multiplications. One of the state of the art implementations is provided by NVIDIA CUDA platform [4, 13, 19].\nLet H be a diagonal matrix with elements as row sums of B, i.e.,\nHii = \u2211 j Bij .\nP is defined as P = H\u22121B and is used as the transition probability matrix on the graph with weighted edges. Since P might be reducible, its irreducible counterpart as in the PageRank algorithm is:\nQ = (1\u2212 )P + N E,\nwhere E is an N \u00d7N matrix with all 1\u2019s. A stochastic approximation of the closed form solution gives the following sampling based algorithm. Let Xt, t \u2265 0, be a Markov chain with transition matrix Q. Then for t \u2265 0, do:\nRR n\u00b0 8767"}, {"heading": "6 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "Algorithm 2:\nF t+1ij = F t ij + \u03b7tI{Xt = i}\np(i,Xt+1)\nq(i,Xt+1)\n( HiiF t Xt+1j \u2212 F t ij + \u03b1Yij ) ,\nwhere {\u03b7t}t\u22650 is a positive step-size sequence satisfying \u2211 t\u22650 \u03b7t =\u221e and \u2211 t\u22650 \u03b7 2 t <\u221e.\nHere p(i, j), q(i, j) are the elements of P andQ respectively and the ratio p(i,Xt+1)/q(i,Xt+1) is the (conditional) likelihood ratio correcting for the fact that we are simulating the chain with transitions governed by Q and not P . This is a form of importance sampling. We run this update step \u2200j \u2208 {1, 2, ..,K}. In each iteration, the feature value of only one node is updated. This node is selected by the Markov sequence. Since we are sampling from the possible nodes each time before updating, this algorithm is henceforth referred to as the sampling algorithm. It must be noted that a round robin policy or any other policy that samples the nodes sufficiently often provides a viable alternative. We return to this later.\nFrom the very formulation of Algorithm 2, it is clear that this approach has immediate distributed asynchronous implementation."}, {"heading": "3 Convergence Analysis", "text": "We now present proofs of convergence for the two algorithms. Consider the problem of finding a unique solution x\u2217 of the system\nx = G(x) = B\u0303x+ Y\u0303 ,\nwhere c \u2208 Rd and B\u0303 = [[b(i, j)]] \u2208 Rd\u00d7d is irreducible non-negative with Perron-Frobenius eigenvalue \u03bb \u2208 (0, 1) and the corresponding (suitably normalized) positive eigenvector w = [w1, ...wd] T . Define the weighted norm\n\u2016x\u2016w = max i \u2223\u2223\u2223\u2223xjwi \u2223\u2223\u2223\u2223 .\nLemma The map G is a contraction w.r.t. the above norm, specifically,\n\u2016G(x)\u2212G(y)\u2016w \u2264 \u03bb\u2016x\u2212 y\u2016w.\nProof We have\n\u2016G(x)\u2212G(y)\u2016w = max i\n\u2223\u2223\u2223\u2223 \u2211 j b(i, j)(xj \u2212 yj)\nwi \u2223\u2223\u2223\u2223 = max\ni \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 j b(i, j) (xj\u2212yj) wj wj wi \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 (\nmax j \u2223\u2223\u2223\u2223xj \u2212 yjwj \u2223\u2223\u2223\u2223)(maxi \u2223\u2223\u2223\u2223 \u2211 j b(i, j)wj wi \u2223\u2223\u2223\u2223) = \u03bb\u2016x\u2212 y\u2016w.\nThis proves the claim. 2 Identifying B\u0303, Y\u0303 with \u03b1B, (1\u2212 \u03b1)Y , our Algorithm 1 reduces to the above column-wise. Algorithm 1 is then simply a repeated application of a contraction map with contraction coefficient \u03bb and by the celebrated contration mapping theorem, converges to its unique fixed point (i.e.,\nInria"}, {"heading": "Distributed Semi-supervised Learning 7", "text": "our desired solution) at an exponential rate with exponent \u03bb. Since matrix B is similar to the stochastic matrix D\u22121A with one as its largest in modulus eigenvalue, we have that \u03bb = \u03b1. We consider Algorithm 2 next. Write B\u0303 = H\u0303P\u0303 where H\u0303 = diag(l(1), ...l(d))T , l(i) =\u2211 j b(i, j), is a diagonal matrix and P\u0303 = [[p\u0303(i, j)]] is a stochastic matrix given by p\u0303(i, j) = b(i,j) l(i) . Consider the scheme similar to algorithm 2:\nxt+1i = x t i + \u03b7tI{Xt = i}\np\u0303(i,Xt+1) q\u0303(i,Xt+1) (l(i)xtXt+1 \u2212 x t i), (1)\nwhere Xt is a Markov Chain with transition probability matrix\nQ\u0303 = [[q\u0303(i, j)]], q\u0303(i, j) = d + (1\u2212 )p\u0303(i, j)\n. We have the following result.\nTheorem Almost surely, xt \u2192 x\u2217 as t\u2192\u221e.\nProof (sketch) Note that\nE\n[ p\u0303(i,Xt+1)\nq\u0303(i,Xt+1) l(i)xtXt+1 |Xt = i\n] = \u2211 j q\u0303(i, j) p\u0303(i, j) q\u0303(i, j) l(i)xtj\n= \u2211 j b(i, j)xtj .\nThen we can rewrite (1) as\nxt+1i = x t i + \u03b7tI{Xt = i} (\u2211 j b(i, j)xtj \u2212 xti +M t+1i ) ,\nwhere M t+1i := [ p\u0303(i,Xt+1) q\u0303(i,Xt+1) (l(i)xtXt+1)\u2212 \u2211 j b(i, j)xtj ] defines a martingale difference sequence (i.e., a sequence of integrable random variables uncorrelated with the past). Iteration (2) is a stochastic approximation algorithm that can be analyzed using the \u2018ODE\u2019 (for Ordinary Differential Equation\u2019) approach which treats the discrete time algorithm as a noisy discretization (or \u2018Euler scheme\u2019) of a limiting ODE with slowly decreasing step-size, decremented at just the right rate so that the errors due to discretization and noise are asymptotically negligible and the correct asymptotic behavior of the ODE is replicated [6]. See p. 74-75, Chapter 6 of ibid. for a precise statement. In particular, if the ODE has a globally asymptotically stable equilibrium, the algorithm converges to it with probability one under mild conditions stated in ibid. which are easily verified in the present case. Let \u03c0 = [\u03c0(1), .., , \u03c0(d)]T\ndenote the stationary distribution under Q\u0303. Let \u0393 = diag(\u03c0(1), ..., \u03c0(d)). The limiting ODE in this case can be derived as in Chapter 6, [6] and is\nx\u0307(t) = \u0393(G(x(t))\u2212 x(t))\n= G\u0303(x(t))\u2212 x(t),\nwhere G\u0303(x) = (I \u2212 \u0393)x+ \u0393G(x).\nRR n\u00b0 8767"}, {"heading": "8 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "Now,\n\u2016G\u0303(x)\u2212 G\u0303(y)\u2016w \u2264 max i\n[ (1\u2212 \u03c0(i)) \u2223\u2223\u2223\u2223xi \u2212 yiwi \u2223\u2223\u2223\u2223+ \u03c0(i) \u2223\u2223\u2223\u2223 \u2211 j b(i, j)(xj \u2212 yj) wi \u2223\u2223\u2223\u2223] \u2264 max\ni\n[ (1\u2212 \u03c0(i)) \u2223\u2223\u2223\u2223xi \u2212 yiwi \u2223\u2223\u2223\u2223+ \u03c0(i)\u03bb\u2016x\u2212 y\u2016w]\n\u2264 \u03bb\u0303\u2016x\u2212 y\u2016w,\nwhere, \u03bb\u0303 = max\ni (1\u2212 \u03c0(i) + \u03bb\u03c0(i)) \u2208 (0, 1).\nThus, G\u0303 is also a contraction w.r.t \u2016.\u2016w. By results of p. 126-127 of [6], the ODE converges to the unique fixed point of G\u0303, equivalently of G, i.e. the desired solution x\u2217. Furthermore, if we consider the scaled limit G\u0303\u221e(x) := lima\u2191\u221e G\u0303(ax) a (which corresponds to replacing Y\u0303 by the zero vector in the above), then the ODE y\u0307(t) = G\u0303\u221e(y(t)) \u2212 y(t) is seen to have the origin as its unique globally asymptotically stable equilibrium. By Theorem 9, p. 75, [6], supt \u2016xt\u2016 < \u221e with probability one. In other words, the iteration is stable. Hence by the theory developed in Chapter 2 (particularly Theorems 7 and Corollary 8, p. 74) of ibid., {xt} converges to x\u2217 with probability one. 2\nIn Algorithm 2, if we replace \u03b1B by B\u0303, then the algorithm is column-wise exactly the same as (1). Hence Algorithm 2 converges to the correct value of the feature matrix F . Note the key role of the factor \u03b1 in ensuring the contraction property.\nIf we consider round robin sampling, the results of [6], Section 4.2 on sample complexity become applicable here. In particular, we conclude that at time n, the probability of remaining within a prescribed small neighborhood of x\u2217 after n+ \u03c4 iterates (where \u03c4 can be prescribed in\nterms of problem parameters) remains greater than 1\u2212O ( e C\u2211 t\u2265n \u03b7 2 t ) . If for example, \u03b7t = \u0398 ( 1 t ) ,\nthen \u2211 t\u2265n \u03b7 2 t = \u0398 ( 1 t ) and this implies exponential decay of probability of ever escaping from this neighborhood after n+\u03c4 iterations. We may expect a similar behavior for Markov sampling, but analogous estimates for asynchronous stochastic approximation with \u2018Markov\u2019 noise (of which Markov sampling is a special case) appear to be currently unavailable."}, {"heading": "4 Experiments and Results", "text": "To evaluate the classification accuracy of the two algorithms, we have used the following networks (real and synthetic): the graph based on the French classic Les Miserables written by Victor Hugo [11], the graph of university webpages dataset as found on WebKB [20], Gaussian mixture graphs, and finally dynamic stochastic block model graph based on M/M/K/K queue. The number of updates in the power method in one iteration is N while the classification function of only one node is updated in each step of the sampling algorithm. Hence we shall refer to N steps of the sampling algorithm as one iteration, to keep the comparison fair. The number of misclassified nodes is a measure of error and is used as the metric for performance testing. It must be noted that the rate of decrease of the step size also influences the rate of convergence of the error."}, {"heading": "4.1 Les Miserables graph", "text": "Characters of the novel Les Miserables form the nodes of the graph [11]. The nodes that appear on the same page in the novel are connected by a weighted directed edge. We will consider the undirected version of this graph with the weight of all the nodes set to 1. This graph has 77 nodes\nInria"}, {"heading": "Distributed Semi-supervised Learning 9", "text": "and 6 clusters, namely Myriel (10), Valjean (17), Fantine (10), Cosette (10), Thenardier (12), Gavroche (18) where the class name is the name of a character and in the bracket is indicated the number of nodes in that cluster. Class information of the nodes named above is available to the algorithms. We use the decreasing step size 12+bt/100c .\nAlgorithms 1 and 2 provide \u00b5 and \u03c3 as two parameters that can be varied. For different values of these parameters, Algorithm 1 was run for 500 iterations and then the class of each node was found. This was done 3 times for each value of \u00b5 and \u03c3 and then the average error across the 3 classifications was calculated. The average error is shown in Figure1 (a). Figure1(b) shows the same information for the values of \u03c3 corresponding to the Standard Laplacian (\u03c3 = 1), Normalised Laplacian (\u03c3 = 0.5) and PageRank (\u03c3 = 0).\nFor further experiments, we will be considering the value \u03c3 = 0.5 and \u00b5 = 1. According to Figure1 this is a good choice of the parameters. This choice of the parameters can also be backed up by the results from [3] and [15]. Note that \u03c3 = 0.5 corresponds to the Normalised Laplacian algorithm. The difference in performance for \u03c3\u2019s can be understood by taking the analogy of a random walk on the graph starting from the labelled node of the cluster [3]. In general, if there is a node that is connected to the labelled node of two clusters, one of which is denser with the labelled node having degree higher than the other labelled node and the other cluster is less dense or smaller, then the PageRank algorithm tends to classify it into the smaller cluster while the Standard Laplacian classifies it into the denser cluster.\nThe convergence of classification function for the two algorithms can be seen in Figure2, Figure3, Figure4. It can also be seen that the classification function for the sampling algorithm converges to the same values as the classification function for the power iteration algorithm.\nLes Miserables graph has interesting nodes like Woman2, Tholomyes and Mlle Baptistine among others which are connected to multiple labelled nodes. Since the class density and the class sizes for different nodes is different, these nodes get classified into different classes for different values of \u03c3. The node Woman2 is connected to the labelled nodes Cosette, Valjean and Javert(which belongs to class Thenardier). As seen in the Figure3, the classification function values for these nodes/classes is higher compared to the other 3 classes. Similar is the case for Tholomyes as shown in Figure4 and Mlle Baptistine as shown in Figure4. The error as a function of iteration number is shown in Figure5.\nRR n\u00b0 8767"}, {"heading": "10 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": ""}, {"heading": "4.2 WebKB Data", "text": "We next see the classification of webpages of 4 universities - Cornell, Texas, Washington and Wisconsin - corresponding to the popular WebKB dataset [20]. We considered the graph formed by the hyperlinks connecting these pages. Webpages in the original dataset that do not have hyperlinks to webpages within the dataset are not considered. This gives the following clusters Cornell (677), Texas (591), Washington (983) and Wisconsin (616). The node with the largest degree is selected from each class (university) as the labelled node. We use the decreasing step size 12+bt/1000c . We will be using this decreasing step size for the other experiments unless stated otherwise.\nFigure6 shows the %error evolution. Majority of this error is because of the nodes belonging to Wisconsin getting classified as nodes belonging to Cornell. The degree of the labelled node of class Cornell is almost thrice that of class Wisconsin, while the number of nodes in these classes is approximately the same. The average degree, and hence the density, of the class corresponding to Wisconsin (3.20) is lesser as compared to the others (3.46, 3.54, 3.52). Iterations/average\nInria"}, {"heading": "Distributed Semi-supervised Learning 11", "text": "degree is used as the x-axis for the sampling algorithm in Figure6 while iterations is used for the power iteration algorithm. The rationale for dividing by the average degree is that the sampling algorithm obtains information from only one neighbour in one step while the power iteration obtains information from all the neighbours in one iteration. Division by average degree makes the two methods comparable in computational cost. As can be seen, the two methods are comparable in terms of accuracy as well as the computational cost for attaining a given accuracy.\nIt is interesting to observe that if one would like to obtain quickly a very rough classification, the sampling algorithm might be preferable. This situation is typical for MCMC type methods."}, {"heading": "4.3 Gaussian Mixture Models", "text": "Classification into clusters in a Gaussian Mixture graph was also tested. A Gaussian mixture graph with 3 classes was created. A node belongs to class 1 with probability 0.33, to class 2 with probability 0.33 and to class 3 with probability 0.34. A node shares an edge with all the nodes within a given radius of itself. Two nodes with the highest degree from each class were chosen as the labelled nodes while the remaining nodes were unlabelled. It is known that choosing high degree nodes as labelled nodes is beneficial [3]. A Gaussian mixture graph with 500 nodes was created with the above parameters. Figure7(a) shows the graph with the nodes coloured according to their class - nodes of class 1 are shown in pink, class 2 in green and Class 3 in red. Figure7(b) shows the classes that the nodes were classified into using the sampling based algorithm. Nodes classified into Class 1 are shown in pink, Class 2 in green and Class 3 in red. Shown in yellow are the nodes that were wrongly classified. The nodes that are misclassified lie either on the boundary of two classes or are surrounded completely by nodes of the neighbouring class as seen in Figure7(b). When this happens, the number of neighbours the node has of the other class is more than the neighbours it has of its own class, leading to misclassification.\nThe sampling algorithm scales very well which can be seen in Figure8. Gaussian mixture graphs with 10000 nodes were generated and the distributed sampling scheme was applied to them. The error decreases as the number of iterations increase, till it converges to a constant value. Figure8 shows the error evolution for three Gaussian graphs with different cluster densities - High Cluster Density (HCD), Medium Cluster Density (MCD), Low Cluster Density(LCD). The convergence is shown for the sampling based algorithm as well as the power iteration method.\nRR n\u00b0 8767"}, {"heading": "12 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "On the X-axis, the log of iterations/average degree is used for sampling method while log of iterations is used for power iteration; intention being the same as in Figure6. The sampling algorithm outperforms the power iteration method in terms of computational cost required to reach a given accuracy. The number of iterations for the two methods is of the same order(5-8 for PI while 15-20 for sampling) and average node degree is between 180-570. The convergence is faster in the graph with higher cluster density. Also, the final error is smaller in graphs with higher cluster density compared to others for both methods. The misclassified nodes lie on the boundary of two classes.\nA round robin node selection method was also tried instead of the MCMC based selection in the sampling algorithm. Node for which update was to be made was selected in a round robin fashion and then one of neighbors was sampled. It can be seen in Figure10 that the performance of the two node selection methods is comparable in terms of number of iterations for convergence as well as the error on convergence."}, {"heading": "4.4 Tracking of new incoming nodes", "text": "The tracking ability of the sampling algorithm is demonstrated next. After the sampling algorithm ran for 200 iterations on the original graph with 500 nodes, a new node was introduced using the same parameters into the graph. The algorithm continued from the classification function found after 200 iterations for other 20 iterations, this time with 501 nodes. The class of all the nodes was recomputed. The evolution of the feature vector of the new node is shown in Figure11. In the example presented here, the new node belonged to class 2. It can be seen from Figure11 that the classification function value for the class 2 increases as the number of iterations increase and hence the new node gets classified correctly, i.e., it has been correctly tracked.\nInstead of the MCMC based sampling, the class of the new incoming node can also be found by using an alternate node selection scheme. In this, we first select the incoming node and sample from its neighbors. After that, we select the neighbors of the incoming node in a round robin fashion and sample from their neighbors. Once all the 1-neighbors of the incoming node have been exhausted, we again select the incoming node and sample from its neighbors. The evolution\nInria"}, {"heading": "Distributed Semi-supervised Learning 13", "text": "of classification function of the new incoming node for such a node selection technique has been shown in Figure12. The node is classified to its correct class by this technique as well. The idea behind its working is that an unlabelled incoming node will not change the classification function of the other nodes very much. In fact, it will hardly change the values for nodes that are multiple hops away. So selecting nodes as described in this experiment is sufficient to get a good estimate of the classification function. Indeed, this technique is much faster than the MCMC technique because we do not select nodes from the entire graph, but only from 1-hop neighbors."}, {"heading": "4.5 Stochastic Block Model", "text": "We now consider a dynamic graph in which nodes can enter and leave the graph. It has been demonstrated that the sampling algorithm can classify new incoming nodes. A dynamic Stochastic Block Model with multiple classes is taken. Stochastic block models [8, 12] have been found to be closer to reality than classical models such as preferential attachment, in applications such as social networks and there is a growing interest in them. Intuitively, they have relatively denser subnetworks which are interconnected to each other through weak ties. A node is connected with another node of the same class with probability pin and node from a different class with probability pout; pin >> pout. In the present work we introduce an extension of the Stochastic Block Model to the dynamic setting. Specifically, there is a Poisson arrival of nodes into the graph with rate \u03bbarr. The class of the arriving node is chosen according to a pre-specified probability distribution. Each node stays in the graph for a random time that is exponentially distributed with mean \u00b5dep after which it leaves. The maximum number of nodes in the graph is limited to K, i.e., nodes arriving when the number of nodes in the graph is K do not become a part of the graph. This system can be modelled as a M/M/K/K queue. As a result, irrespective of the number of nodes that the graph has initially, the average the number of nodes in the graph will reach a steady state value given by \u03bbarr\u00b5dep .\nSimulations were performed for various values of \u03bbarr and \u00b5dep with different number of nodes\nRR n\u00b0 8767"}, {"heading": "14 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "(a) Graph with class of node indicated by the node\u2019s colour. (b) Graph with class of node as found by the sampling algorithm indicated by the node\u2019s colour. Misclassified nodes are shown in yellow.\nFigure 7: Gaussian Mixture Graphs\nduring initialisation. The graph had 3 classes and an incoming node could belong to either of the classes with equal probability. K = 1000, \u03bbarr = 1/(2\u00d7 104) and \u00b5dep = 1/107 was chosen. Therefore, \u03bbarr\u00b5dep = 500. Two nodes with the maximum degree from each class were chosen as the labelled nodes during initialization. K steps were considered as one iteration. Figure13, Figure14 and Figure15 show the evolution of %error and the size of the graph for initial graph size of 500, 600 and 400 respectively. The step size for these simulations was kept constant at 1/1000 so that the algorithm does not lose its tracking ability. This is common practice in tracking applications. Since a constant stepsize \u03b7t \u2261 \u03b7 > 0 does not satisfy the condition \u2211 t \u03b7 2 t <\u221e, one cannot claim convergence with probability one even in the stationary environment. Instead, one has the weaker claim that the asymptotic probability of the iterates not being in a small neighborhood of the desired limit is small, i.e., O(\u03b7) ([6], Chapter 9). The problem with using decreasing stepsize in time-varying environment can be explained as follows. With the interpretation of stochastic approximation as a noisy discretization of the ODE, one views the step-size as a time step, whence it dictates the time scale on which the algorithm moves. A decreasing step-size schedule begins with high step-sizes and slowly decreases it to zero, thus beginning with a fast time scale for rapid exploration at the expense of larger errors, ending with slower and slower time scales that exploit the problem structure ensuring more better error suppression and graceful convergence. For tracking applications, the environment is non-stationary, thus the decreasing step-size scheme loses its tracking ability once the algorithmic time scale becomes slower than that of the environment. A judiciously chosen constant step-size avoids this problem at the expense of some accuracy. In our algorithms, the Perron-Frobenius eigenvalue \u03bb of the matrix \u03b1B is the key factor dictating the convergence rate, hence the tracking scheme will work well if \u03bbarr, \u00b5dep << \u03bb.\nThe number of misclassified nodes is higher in the stochastic block model (3-7%) than in the Gaussian mixture model. Most of these errors are because node of one class is connected to the labelled node of some other class. Error in some other nodes is because the classification function value is very close for 2 out of the 3 classes. It was also observed that on increasing cluster density, the average error decreases in steady state since some nodes which got wrongly classified by being close to an influential node in some other cluster now will have more neighbors\nInria"}, {"heading": "Distributed Semi-supervised Learning 15", "text": "of the same class. Figure16 shows the effect of increasing the arrival rate to \u03bbarr = 1/(2\u00d7103) while keeping \u00b5dep the same. Similarly, Figure17 shows the effect of decreasing the departure rate to \u00b5dep = 1/108 while keeping \u03bbarr = 1/(2 \u00d7 104). For both these cases, the number of nodes in steady state should be 5000. It can be seen that the decrease in error is much more smoother in this case as compared to Figure13(b),Figure14(b) and Figure15(b) which is indicative of the slowly varying size of the graph.\nDuring the experiment, the number of labelled nodes from each cluster were kept constant. In case a labelled node left, then a neighbour of the leaving node belonging to the same cluster was randomly chosen as the labelled node. If this wasn\u2019t done, the misclassification error became as high as 70% in some cases. This can be reasoned as being due to the absence of labelled nodes resulting in increase in cluster size of other clusters. A similar effect was observed if the number of labelled nodes in one cluster was much more compared to the others.\nTo better understand how the departure of a labelled node effects the error, we performed tests with graphs in which the labelled nodes were permanent, i.e., the labelled nodes did not leave. Figure18 shows the variation of error for this case. It can be seen that in this case, the variance in error after reaching steady state is much less as compared to Figure13(a), Figure14(a) and Figure15(a)."}, {"heading": "5 Conclusion", "text": "Two iterative algorithms for graph based semi-supervised learning have been proposed. The first algorithm is based on iteration of an affine contraction (w.r.t,. a weighted norm) while the second algorithm is based on a sampling scheme inspired by reinforcement learning. The classification accuracy of the nodes of the graph into classes for two algorithms was evaluated and confirmed to be the same. The performance of the sampling algorithm was also evaluated on Gaussian\nRR n\u00b0 8767"}, {"heading": "16 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "mixture graphs and it was found that the nodes other than the boundary nodes are classified correctly. The ability to track the feature vector of a new node that arrives during the simulation was tested. The sampling algorithm correctly classifies the new incoming node in considerably fewer iterations. This ability was then applied to a dynamic stochastic block model graph modelled based on M/M/K/K queue. It was demonstrated that the sampling algorithm can be applied to dynamically changing systems and still achieve a small error. The sampling algorithm can be implemented in a distributed manner unlike the power iteration algorithm and has very little per iterate computation, hence it should be the preferred methodology for very large graphs."}, {"heading": "Distributed Semi-supervised Learning 17", "text": "[5] Borkar, V.S., and Mathkar, A.S. \u201cReinforcement Learning for Matrix Computations: PageRank as an Example.\u201d Distributed Computing and Internet Technology, Proceedings of the 10th ICDCIT, Bhubaneshwar, India (R. Natarajan, ed.) Springer Lecture Notes in Computer Science No. 8337, Springer International Publishing, Switzerland, 2014. 14-24.\n[6] Borkar, V.S. Stochastic approximation: A Dynamical Systems Viewpoint. Hindustan Publishing Agency, New Delhi, and Cambridge Uni. Press, Cambridge, UK, 2008.\n[7] Chapelle, O., Sch\u00f6lkopf, B. and Zien A. Semi-supervised learning, MIT Press, 2006.\n[8] Condon, A., and Karp, R. \u201cAlgorithms for graph partitioning on the planted partition model\u201d. Random Structures and Algorithms, v.18, pp.116-140, 2001.\n[9] Fouss, F., Yen, L., Pirotte, A., and Saerens, M. \u201cAn experimental investigation of graph kernels on a collaborative recommendation task\u201d. In Sixth International Conference on Data Mining (ICDM\u201906), pp.863-868, 2006.\n[10] Fouss, F., Francoisse, K., Yen, L., Pirotte A., and Saerens, M. \u201cAn experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification\u201d. Neural Networks, 31, pp.53-72, 2012.\n[11] Knuth, D.E. The Stanford GraphBase: A Platform for Combinatorial Computing, AddisonWesley, Reading, MA 1993.\n[12] Heimlicher, S., Lelarge, M., and Massouli\u00e9, L. \u201cCommunity detection in the labelled stochastic block model\u201d. ArXiv preprint arXiv:1209.2910, 2012.\n[13] John Nickolls, J., Ian Buck, I., Michael Garland, M., and Kevin Skadron, K. \u201cScalable parallel programming with CUDA\u201d. Queue, v.6(2), pp.40-53, 2008.\n[14] Smirnova, E., Avrachenkov, K., and Trousse, B. \u201cUsing Web Graph Structure for Person Name Disambiguation\u201d. In Proceedings of CLEF, v.77, 2010.\nRR n\u00b0 8767"}, {"heading": "18 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "[15] Zhou, D., Bousquet, O., Navin Lal, T., Weston, J., Sch\u00f6lkopf, B. \u201cLearning with local and global consistency\u201d. In: Advances in Neural Information Processing Systems, 16, pp. 321\u2013328, 2004.\n[16] Zhou D., Hofmann T., Sch\u00f6lkopf B. \u201cSemi-supervised learning on directed graphs\u201d, In Advances in neural information processing systems., pp.1633\u20131640, 2004.\n[17] Zhou, D., and Burges, C. J. C. \u201cSpectral clustering and transductive learning with multiple views\u201d. In Proceedings of ICML 2007, pp. 1159\u20131166, 2007.\n[18] Zhu, X. \u201cSemi-supervised learning literature survey\u201d. University of Wisconsin-Madison Research Report TR 1530, 2005.\n[19] NVIDIA CUDA Programming Guide, available at http://docs.nvidia.com/cuda/\n[20] Craven, Mark, et al. Learning to extract symbolic knowledge from the World Wide Web. No. CMU-CS-98-122. Carnegie-Mellon Univ Pittsburgh pa School of Computer Science, 1998.\nInria"}, {"heading": "Distributed Semi-supervised Learning 19", "text": "RR n\u00b0 8767"}, {"heading": "20 K. Avrachenkov & V.S. Borkar & K. Saboo", "text": "Inria"}, {"heading": "Distributed Semi-supervised Learning 21", "text": "RR n\u00b0 8767\nRESEARCH CENTRE SOPHIA ANTIPOLIS \u2013 M\u00c9DITERRAN\u00c9E\n2004 route des Lucioles - BP 93 06902 Sophia Antipolis Cedex\nPublisher Inria Domaine de Voluceau - Rocquencourt BP 105 - 78153 Le Chesnay Cedex inria.fr\nISSN 0249-6399"}], "references": [{"title": "Pagerank based clustering of hypertext document collections", "author": ["K. Avrachenkov", "V. Dobrynin", "D. Nemirovsky", "S.K. Pham", "E. Smirnova"], "venue": "In Proceedings of ACM SIGIR 2008,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Generalized optimization framework for graph-based semi-supervised learning.", "author": ["K. Avrachenkov", "P. Gon\u00e7alves", "A. Mishenin", "M. Sokol"], "venue": "Proceedings of SIAM Conference on Data Mining (SDM 2012)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "On the choice of kernel and labelled data in semi-supervised learning methods", "author": ["K. Avrachenkov", "P. Gon\u00e7alves", "M. Sokol"], "venue": "In Proceedings of Algorithms and Models for the Web Graph (WAW 2013),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Implementing sparse matrix-vector multiplication on throughputoriented processors", "author": ["N. Bell", "M. Garland"], "venue": "Proceedings of the ACM Conference on High Performance Computing Networking, Storage and Analysis", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Reinforcement Learning for Matrix Computations: PageRank as an Example.", "author": ["V.S. Borkar", "A.S. Mathkar"], "venue": "Distributed Computing and Internet Technology, Proceedings of the 10th ICDCIT, Bhubaneshwar, India (R. Natarajan,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Stochastic approximation: A Dynamical Systems Viewpoint", "author": ["V.S. Borkar"], "venue": "Hindustan Publishing Agency,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R. Karp"], "venue": "Random Structures and Algorithms, v.18,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "An experimental investigation of graph kernels on a collaborative recommendation task", "author": ["F. Fouss", "L. Yen", "A. Pirotte", "M. Saerens"], "venue": "In Sixth International Conference on Data Mining (ICDM\u201906),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification", "author": ["F. Fouss", "K. Francoisse", "L. Yen", "Pirotte A", "M. Saerens"], "venue": "Neural Networks, 31,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "The Stanford GraphBase: A Platform for Combinatorial Computing, Addison- Wesley", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}, {"title": "Community detection in the labelled stochastic block model", "author": ["S. Heimlicher", "M. Lelarge", "L. Massouli\u00e9"], "venue": "ArXiv preprint arXiv:1209.2910,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Scalable parallel programming with CUDA", "author": ["J. John Nickolls", "I. Ian Buck", "M. Michael Garland", "K. Kevin Skadron"], "venue": "Queue, v.6(2),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Using Web Graph Structure for Person Name Disambiguation", "author": ["E. Smirnova", "K. Avrachenkov", "B. Trousse"], "venue": "In Proceedings of CLEF,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T. Navin Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Semi-supervised learning on directed graphs", "author": ["D. Zhou", "T. Hofmann", "B. Sch\u00f6lkopf"], "venue": "In Advances in neural information processing systems.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Spectral clustering and transductive learning with multiple views", "author": ["D. Zhou", "C.J.C. Burges"], "venue": "In Proceedings of ICML", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "University of Wisconsin-Madison Research Report TR 1530,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Learning to extract symbolic knowledge from the World", "author": ["Craven", "Mark"], "venue": "Wide Web. No. CMU-CS-98-122. Carnegie-Mellon Univ Pittsburgh pa School of Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}], "referenceMentions": [{"referenceID": 7, "context": "The present work focuses on graph based semi-supervised learning [7, 9, 10, 18].", "startOffset": 65, "endOffset": 79}, {"referenceID": 8, "context": "The present work focuses on graph based semi-supervised learning [7, 9, 10, 18].", "startOffset": 65, "endOffset": 79}, {"referenceID": 16, "context": "The present work focuses on graph based semi-supervised learning [7, 9, 10, 18].", "startOffset": 65, "endOffset": 79}, {"referenceID": 7, "context": "Graph based semi-supervised learning finds application in recommendation systems [9], classification of web pages into categories [16], person name disambiguation [14], etc.", "startOffset": 81, "endOffset": 84}, {"referenceID": 14, "context": "Graph based semi-supervised learning finds application in recommendation systems [9], classification of web pages into categories [16], person name disambiguation [14], etc.", "startOffset": 130, "endOffset": 134}, {"referenceID": 12, "context": "Graph based semi-supervised learning finds application in recommendation systems [9], classification of web pages into categories [16], person name disambiguation [14], etc.", "startOffset": 163, "endOffset": 167}, {"referenceID": 1, "context": "In [2] the authors have formulated a generalized optimization problem which gives as important particular cases: Standard Laplacian [17], Normalised Laplacian [15] and PageRank [1] algorithms.", "startOffset": 3, "endOffset": 6}, {"referenceID": 15, "context": "In [2] the authors have formulated a generalized optimization problem which gives as important particular cases: Standard Laplacian [17], Normalised Laplacian [15] and PageRank [1] algorithms.", "startOffset": 132, "endOffset": 136}, {"referenceID": 13, "context": "In [2] the authors have formulated a generalized optimization problem which gives as important particular cases: Standard Laplacian [17], Normalised Laplacian [15] and PageRank [1] algorithms.", "startOffset": 159, "endOffset": 163}, {"referenceID": 0, "context": "In [2] the authors have formulated a generalized optimization problem which gives as important particular cases: Standard Laplacian [17], Normalised Laplacian [15] and PageRank [1] algorithms.", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "In the present work, starting from the generalized formulation of [2], we propose two approaches for the computation of the feature vectors.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "[5] have developed a sampling based reinforcement learning scheme for the PageRank algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "k, where B = D\u2212\u03c3AD\u03c3\u22121 and \u03b1 = 2 2+\u03bc for \u03bc > 0 [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "One of the state of the art implementations is provided by NVIDIA CUDA platform [4, 13, 19].", "startOffset": 80, "endOffset": 91}, {"referenceID": 11, "context": "One of the state of the art implementations is provided by NVIDIA CUDA platform [4, 13, 19].", "startOffset": 80, "endOffset": 91}, {"referenceID": 5, "context": "Iteration (2) is a stochastic approximation algorithm that can be analyzed using the \u2018ODE\u2019 (for Ordinary Differential Equation\u2019) approach which treats the discrete time algorithm as a noisy discretization (or \u2018Euler scheme\u2019) of a limiting ODE with slowly decreasing step-size, decremented at just the right rate so that the errors due to discretization and noise are asymptotically negligible and the correct asymptotic behavior of the ODE is replicated [6].", "startOffset": 454, "endOffset": 457}, {"referenceID": 5, "context": "The limiting ODE in this case can be derived as in Chapter 6, [6] and is \u1e8b(t) = \u0393(G(x(t))\u2212 x(t)) = G\u0303(x(t))\u2212 x(t), where G\u0303(x) = (I \u2212 \u0393)x+ \u0393G(x).", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "126-127 of [6], the ODE converges to the unique fixed point of G\u0303, equivalently of G, i.", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "75, [6], supt \u2016x\u2016 < \u221e with probability one.", "startOffset": 4, "endOffset": 7}, {"referenceID": 5, "context": "If we consider round robin sampling, the results of [6], Section 4.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "To evaluate the classification accuracy of the two algorithms, we have used the following networks (real and synthetic): the graph based on the French classic Les Miserables written by Victor Hugo [11], the graph of university webpages dataset as found on WebKB [20], Gaussian mixture graphs, and finally dynamic stochastic block model graph based on M/M/K/K queue.", "startOffset": 197, "endOffset": 201}, {"referenceID": 17, "context": "To evaluate the classification accuracy of the two algorithms, we have used the following networks (real and synthetic): the graph based on the French classic Les Miserables written by Victor Hugo [11], the graph of university webpages dataset as found on WebKB [20], Gaussian mixture graphs, and finally dynamic stochastic block model graph based on M/M/K/K queue.", "startOffset": 262, "endOffset": 266}, {"referenceID": 9, "context": "1 Les Miserables graph Characters of the novel Les Miserables form the nodes of the graph [11].", "startOffset": 90, "endOffset": 94}, {"referenceID": 2, "context": "This choice of the parameters can also be backed up by the results from [3] and [15].", "startOffset": 72, "endOffset": 75}, {"referenceID": 13, "context": "This choice of the parameters can also be backed up by the results from [3] and [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "The difference in performance for \u03c3\u2019s can be understood by taking the analogy of a random walk on the graph starting from the labelled node of the cluster [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 17, "context": "2 WebKB Data We next see the classification of webpages of 4 universities - Cornell, Texas, Washington and Wisconsin - corresponding to the popular WebKB dataset [20].", "startOffset": 162, "endOffset": 166}, {"referenceID": 2, "context": "It is known that choosing high degree nodes as labelled nodes is beneficial [3].", "startOffset": 76, "endOffset": 79}, {"referenceID": 6, "context": "Stochastic block models [8, 12] have been found to be closer to reality than classical models such as preferential attachment, in applications such as social networks and there is a growing interest in them.", "startOffset": 24, "endOffset": 31}, {"referenceID": 10, "context": "Stochastic block models [8, 12] have been found to be closer to reality than classical models such as preferential attachment, in applications such as social networks and there is a growing interest in them.", "startOffset": 24, "endOffset": 31}, {"referenceID": 5, "context": ", O(\u03b7) ([6], Chapter 9).", "startOffset": 8, "endOffset": 11}], "year": 2015, "abstractText": "Two approaches for graph based semi-supervised learning are proposed. The first approach is based on iteration of an affine map. A key element of the affine map iteration is sparse matrix-vector multiplication, which has several very efficient parallel implementations. The second approach belongs to the class of Markov Chain Monte Carlo (MCMC) algorithms. It is based on sampling of nodes by performing a random walk on the graph. The latter approach is distributed by its nature and can be easily implemented on several processors or over the network. Both theoretical and practical evaluations are provided. It is found that the nodes are classified into their class with very small error. The sampling algorithm\u2019s ability to track new incoming nodes and to classify them is also demonstrated. Key-words: Semi-supervised learning, Graph-based learning, Distributed algorithms, Parallel algorithms \u2217 K. Avrachenkov is with Inria Sophia Antipolis, France, k.avrachenkov@inria.fr \u2020 V.S. Borkar is with Department of Electrical Engineering, IIT Bombay, India, borkar.vs@gmail.com \u2021 K. Saboo is with Department of Electrical Engineering, IIT Bombay, India, kvsaboo.2004@ee.iitb.ac.in \u00a7 The work of KA and VSB was supported in part by grant no. 5100-ITA from the Indo-French Centre for the Promotion of Advanced Research (IFCPAR) and Alcatel-Lucent Inria Joint Lab. Les Approches Parall\u00e8les et Distribu\u00e9s pour l\u2019Apprentissage Semi-supervis\u00e9 R\u00e9sum\u00e9 : Deux approches pour l\u2019apprentissage semi-supervis\u00e9 bas\u00e9 sur le graphe de similarit\u00e9 sont propos\u00e9s. La premi\u00e8re approche est bas\u00e9e sur l\u2019it\u00e9ration d\u2019un op\u00e9rateur affine. Un \u00e9l\u00e9ment cl\u00e9 de l\u2019it\u00e9ration de l\u2019op\u00e9rateur affine est la multiplication vecteur par matrice de type sparse, qui a plusieurs impl\u00e9mentations parall\u00e8les tr\u00e8s efficaces. La seconde approche appartient \u00e0 la classe des algorithmes de Monte-Carlo par cha\u00eenes de Markov (MCMC). Elle est bas\u00e9 sur un \u00e9chantillonnage de noeuds en effectuant une marche al\u00e9atoire sur le graphe de similarit\u00e9. Cette derni\u00e8re approche est distribu\u00e9 par sa nature et peut \u00eatre facilement mis en oeuvre sur plusieurs processeurs ou sur un r\u00e9seau. \u00c9valuations th\u00e9oriques ainsi que pratiques sont fournis. On constate que les noeuds sont class\u00e9s dans leurs classes avec tr\u00e8s petite erreur. La capacit\u00e9 de l\u2019algorithme MCMC de suivre les nouveaux noeuds arrivants online et de les classer est \u00e9galement d\u00e9montr\u00e9. Mots-cl\u00e9s : Apprentissage semi-supervis\u00e9, Apprentissage bas\u00e9 sur le graphe de similarit\u00e9, Algorithmes distribu\u00e9s, Algorithmes parall\u00e8les Distributed Semi-supervised Learning 3", "creator": "LaTeX with hyperref package"}}}