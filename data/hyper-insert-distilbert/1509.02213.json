{"id": "1509.02213", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2015", "title": "Unsupervised Spoken Term Detection with Spoken Queries by Multi-level Acoustic Patterns with Varying Model Granularity", "abstract": "this 2007 paper presents a new cognitive approach for unsupervised english spoken term detection with spoken item queries produced using multiple feature sets of synchronized acoustic signal patterns automatically discovered mainly from the target corpus. the two different pattern hmm configurations ( number of ambiguous states seen per membrane model, number difference of distinct speaker models, number of distinguished gaussians known per state ) form to a flexible three - dimensional selection model employing granularity screening space. different study sets consisted of differing acoustic device patterns randomly automatically discovered on different viewing points which properly distributed looking over this three - dimensional space are complementary meanings to one another another, meaning thus can jointly capture vastly the characteristics of generating the spoken feature terms. by representing the spoken line content and spoken query expressions as sequences diagnostic of acoustic specification patterns, developing a series of approaches for matching exactly the pattern model index sequences while considering the signal variations automatically are gradually developed. in this way, not only the on - line computation load can be reduced, but the discrete signal distributions caused also by representing different speakers and appropriate acoustic conditions can also be remarkably reasonably taken care of. the systematic results cited indicate that, this enhanced approach significantly outperformed across the original unsupervised silent feature - based dtw baseline implementations by predicting 16. 16 \\ % of in mean average precision applied on the timit corpus.", "histories": [["v1", "Mon, 7 Sep 2015 22:40:31 GMT  (1759kb,D)", "http://arxiv.org/abs/1509.02213v1", "Accepted by ICASSP 2014"]], "COMMENTS": "Accepted by ICASSP 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["cheng-tao chung", "chun-an chan", "lin-shan lee"], "accepted": false, "id": "1509.02213"}, "pdf": {"name": "1509.02213.pdf", "metadata": {"source": "CRF", "title": "UNSUPERVISED SPOKEN TERM DETECTION WITH SPOKEN QUERIES BY MULTI-LEVEL ACOUSTIC PATTERNS WITH VARYING MODEL GRANULARITY", "authors": ["Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee"], "emails": ["b97901182@gmail.com,", "chunanchan@gmail.com,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": null, "text": "Index Terms\u2014 zero resource speech recognition, unsupervised learning, dynamic time warping, hidden Markov models, spoken term detection"}, {"heading": "1. INTRODUCTION", "text": "The fast growing quantity of video and audio content over the Internet implies a very high demand for efficient and accurate approaches to search through the spoken contents. Spoken term detection (STD) usually refers to the task of finding all occurrences of the text query term from a large spoken archive [1]. Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5]. This implies annotated training corpora properly matched to the spoken content are necessary. When the input query is spoken, it becomes possible to directly match the spoken content with the spoken query without conventional ASR. In this way, the difficulties in conventional ASR such as recognition errors and need for annotated training data may be bypassed, which is especially attractive for languages with very limited annotated data [6][7] or spoken content with unknown languages. This leads to recent efforts in unsupervised STD with spoken queries without using annotated data during training [8] [9], which is also the focus of this work. Hereafter we assume all queries are in spoken, and no annotated speech data is available.\nPrevailing approaches to the task considered here rely on dynamic time warping (DTW) to directly match the spoken query to the spoken documents. However, a major limitation of DTW-based approaches is that the DTW distances are easily affected by speaker mismatch and varying acoustic conditions. Many related works focused on feature\nrepresentations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13]. The other limitation for DTW-based approaches is that the computation load for the matching process is linear to the number of frames to be searched through. Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].\nIn recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22]. In this paper, we propose a new approach for unsupervised STD with spoken queries using multilevel acoustic patterns automatically discovered from the target corpus with varying model granularity discovery from the corpus of interest. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a threedimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to on another, thus can jointly capture the characteristics of the spoken terms. By converting the spoken content and query into parallel sequences of acoustic patterns with different model granularity, token matching can be performed with pattern indices representing highly varying signals. Very encouraging results were obtained the preliminary experiments."}, {"heading": "2. ACOUSTIC PATTERNS WITH VARYING MODEL GRANULARITY", "text": ""}, {"heading": "2.1. Pattern Discovery for a Given Model Configuration", "text": "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the topology of the HMMs [19][20][21]. This can be achieved by first finding an initial label \u03c90 based on a set of assumed patterns for all observations in the corpus \u03c7 as in (1)[17]. Then in each iteration t the HMM parameter set \u03b8\u03c8t can be trained with the label \u03c9t\u22121 obtained in the previous iteration as in (2), and the new label \u03c9t can be obtained by free-pattern decoding with the obtained parameter set \u03b8\u03c8t as in (3).\n\u03c90 = initialization(\u03c7), (1) \u03b8\u03c8t = arg max \u03b8\u03c8 P (\u03c7|\u03b8\u03c8 , \u03c9t\u22121), (2)\n\u03c9t = arg max \u03c9\nP (\u03c7|\u03b8\u03c8t , \u03c9). (3)\nThe training process can be repeated with enough number of iterations until the difference between \u03c9t\u22121 and \u03c9t becomes insignificant. This\nar X\niv :1\n50 9.\n02 21\n3v 1\n[ cs\n.C L\n] 7\nS ep\n2 01\ngives a converged set of acoustic pattern HMMs which we denote as \u0398\u03c8 ."}, {"heading": "2.2. Model Granularity Space", "text": "The above process can be performed with many different HMM configurations, each characterized by three hyperparameters: the number of states m in each acoustic pattern HMM, the total number of acoustic patterns n during initialization, and the number of Gaussians l in each HMM state, \u03c8 = (m,n, l). The transcription of a speech signal decoded with these acoustic pattern HMMs may be considered as a temporal segmentation of the signal, so the HMM length (or number of states in each HMM) m represents the temporal granularity. The set of all acoustic pattern HMMs may be considered as a segmentation of the phonetic space, so the total number n of acoustic pattern HMMs represents the phonetic granularity. The different Gaussians in each state then jointly model the distributions of the signals in the acoustic feature space represented by MFCCs, so the number of Gaussians l in each state represents the acoustic granularity. This gives a three-dimensional representation of the acoustic pattern configurations in terms of temporal, phonetic and acoustic granularities as in Fig. 1. Any point in the three-dimensional space in Fig. 1 corresponds to an acoustic pattern configuration.\nAlthough the hyperparameters \u03c8 = (m,n, l) are difficult to determine for a given corpus of unknown language and unknown linguistic characteristics, it is possible to have many different sets of acoustic patterns with hyperparameters and HMMs {\u0398\u03c8k , \u03c8k = (mk, nk, lk), k = 1, 2, ...K} independently learned in parallel, considered as on different levels. Because different model granularities (m,n, l) give different characteristics to the acoustic patterns as mentioned above, with enough number of pattern sets and the model granularities (m,n, l) properly distributed in the three-dimensional space, this multi-level set of acoustic pattern may jointly represent the behavior of the signal for the given corpus. There can be a variety of applications for these patterns, but here in the work below we use these patterns in spoken term detection, assuming the characteristics of the spoken terms can be captured by different sets of these patterns."}, {"heading": "3. SPOKEN TERM DETECTION AND SEARCH METHODS", "text": ""}, {"heading": "3.1. Off-line Processing", "text": "All the spoken documents in the archive are first off-line decoded into sequences of acoustic patterns using each level of acoustic pattern HMM set \u0398(mk,nk,lk). Let {pr, r = 1, 2, 3, .., nk} denote the nk acoustic\npatterns in the set \u0398\u03c8k . We further construct a similarity matrix S of size nk \u00d7 nk for every \u03c8k, for which the component S(i, j) is the similarity between any two pattern HMMs pi and pj in the set \u0398\u03c8k . Two similarity matrices used for this work are in (4).\nS(i, j) =\n{ \u03b4(i, j), for hard similarity, or (4a)\nexp(\u2212KL(i, j)/\u03b2, for soft similarity. (4b)\nThe matrix in (4a) is simply the identity matrix. The KL-divergence KL(i, j) between two pattern HMMs in (4b) is defined as the KLdivergence between the states based on the variational approximation [23] summed over the states. To transform the KL divergence into a similarity measure between 0 and 1, a negative exponential was applied [24] with a scaling factor \u03b2. When \u03b2 is small, similarity between distinct patterns in (4b) approaches zero, so (4b) becomes similar to (4a). \u03b2 can be determined with a held out data set, but here we simply set it to 100 times the number of states mk."}, {"heading": "3.2. On-line Matching Matrix Construction", "text": "In the on-line phase, we perform the following for each entered spoken query q and each document d in the archive. Assume for the pattern set \u0398\u03c8k a document d is decoded into a sequence of D acoustic patterns with indices (d1, d2, ..., dD) and the query q into a sequence of Q patterns with indices (q1, ..., qQ). We thus construct a matching matrix W of size D \u00d7Q for every document-query pair, in which each entry (i, j) is the similarity between acoustic patterns with indices di and qj as in (5a).\nW (i, j) =\n{ S(di, qj), for 1-best sequences, or (5a)\nPTi SPj , for N-best sequences. (5b)\nAlternatively, we can also match the N-best sequences of documents to N-best sequences of queries as depicted in Fig (2) and (5b). We extend each pattern position i in a sequence into a posteriorgram vector Pi of size nk \u00d7 1 by accumulating the duration for each of the nk patterns within the pattern boundaries of the best transcription, across the N-best transcriptions considered, uniformly weighted and normalized(Pi for d and Pj for q). When only the best transcription is considered, (5b) reduces to (5a). The price paid here is the increased computation time by a factor of O(n2k) over that in (5a). One can also choose to use the posteriorgram only on the query, which increases the load by O(nk)."}, {"heading": "3.3. On-line Matching Policy", "text": "There can be two methods to calculate the relevance score between document d and query q. In the sub-sequence matching(SUB) method in (6a) , we sum the elements in the matrixW along the diagonal direction, generating the accumulated similarities for all sub-sequences starting at all pattern positions in d as shown in Fig (3)(a). The maximum is selected to represent the relevance between document d and query q on the\npattern set \u0398\u03c8k as in (6a).\nR(d, q) =  max i:index Q\u2211 j=1 W (i+ j, j), for SUB, or (6a)\nmax u: path |u|\u2211 j=1 W (ud(j), uq(j)), for DTW. (6b)\nIn order to alleviate the problem of insertion/deletion, we can also perform dynamic time warping (DTW) on the matrix W as in (6b) and Fig 3(b), refered to as the pattern-based DTW here. This is an extended version of (6a), except now the elements W are accumulated along any allowed DTW path in the matrix W . The summation in (6b) is over a single DTW path, while the maximization in (6b) is over all DTW paths. Although DTW takes longer time, performing DTW in the matrix W on-line is significantly faster than the conventional frame-based DTW, because most of the calculation was performed offline when evaluating (4). Since table lookup is constant time operation, asymptotically speaking, the computational load online is reduced by a factor of O(FT 2) = O(Fm2k), where F is the feature dimension in the framebased DTW and T is the duration of the acoustic patterns in frames which scales linearly with number of states mk in HMMs."}, {"heading": "3.4. Overall Relevance Score", "text": "In each of (4)(5)(6) there are two options, leading to a total 8 search methods. We thus use three binary digits to specify these methods in reporting experimental results below: \u03b3 = (Soft,Nbest,DTW), i.e. Soft=1 for soft similarity in (4b) and 0 for (4a); Nbest=1 for N-best sequence in (5b) and 0 for (5a); DTW=1 for DTW in (6b) and 0 for (6a). These search methods,{\u03b3s, s = 1, ..., 8} give different relevance scores for each pattern set \u0398\u03c8k , R(\u03c8k,\u03b3s)(d, q) as in (6). The overall relevance score R\u0304(d, q) between d and q is then simply the weighted sum of (6) over all K different sets of acoustic patterns \u0398\u03c8k and the 8 search methods with weights \u03bb(\u03c8k, \u03b3s) as in (7). Below, we simply set the weights \u03bb(\u03c8k, \u03b3s) to either 0 or 1.\nR\u0304(d, q) = K\u2211 k=1 8\u2211 s=1 \u03bb(\u03c8k, \u03b3s)R (\u03c8k,\u03b3s)(d, q). (7)\nNote that the acoustic pattern sets {\u0398\u03c8k , k = 1, 2, ...,K} for different model granularities are complementary to one another. By adding the scores obtained via different pattern sets as in (7) the signal characteristics can be better captured. In addition, the limitation caused by temporal granularity (model length mk) may be alleviated to some degree by the pattern-based DTW in (6b), the limitation caused by phonetic granularity (number of patterns nk) may be alleviated to some degree by the Nbest sequences in (5b), and the limitation caused by acoustic granularity (number of Gaussian lk) may be taken care to some degree by the soft similarity in (4b). Therefore, the choices of \u03c8k and \u03b3s are correlated for good scores of R\u0304(d, q) in (7). Also, the process above, including constructing the KL-divergence matrices and decoding the spoken documents into acoustic patterns off-line, and generating the pattern sequence for the query on-line can all be performed in a highly parallel manner, so\nthe computational load can be scalable with the computational resources available."}, {"heading": "4. EXPERIMENTS", "text": "The proposed approach was tested in preliminary experiments performed on the TIMIT corpus. 20 sets of acoustic patterns with number of states m=3, 5, 7, 9, 11 and number of HMMs n=50, 100, 200, 300 were first generated with l=1 Gaussian per state on the TIMIT training set. Then we increased the number of Gaussians per state by 1 and perform (2) and (3) until the sets converge. We repeated the process until we had l=1, 2, 3, 4 for all the 20 pattern sets, obtaining K=80 pattern HMM sets in the end. With the 8 search methods mentioned in section 3, this gave a total of 640 scores for every query-document pair in (7). The TIMIT training set was also taken as the spoken archive from which we wish to detect the spoken terms.\nThe query set consisted of 32 spoken words randomly selected from the TIMIT testing set. An instance of every query word was randomly selected from the testing set, and used as the spoken query to search for other instances in the training set. Note that although choices of \u03bb(\u03c8k, \u03b3s) can be based on the optimization with respect to an evaluation metric [25][26], the preliminary experiments in this work were to verify the feasibility of the proposed frameworks instead. The baseline we compared to was frame-based DTW on MFCC sequences. The same acoustic features were used for training the pattern HMMs. In principle, the framework should generalize to other features as well. For spoken term detection the performance measures we used were the mean average precision(MAP), precision at 5 and 10(P@5 and P@10). All three measures gave very similar trends. Below we only report results for MAP due to space limitation."}, {"heading": "4.1. Feature Selection and Achievable Performance", "text": "In this experiment, our goal was to learn the 640 weights \u03bb(\u03c8, \u03b3) in (7) to be either 1 or 0 in order to optimize the MAP. We randomly split the query set into 2 disjoint sets A and B, each containing 16 queries. We use set B as the development set for learning these weights and focus our discussion on the performance on set A. Starting with every \u03bb(\u03c8, \u03b3)=0, we greedily select the next (\u03c8, \u03b3) that would yield the best MAP and set it to 1. This process was repeated until 20 pairs of (\u03c8, \u03b3) were selected. The results are shown in pink in Fig. 4, in which the oracle results by learning on set A itself are also shown in cyan. The baseline of framebased DTW was 10.16% for set A. It was low probably because most queries unexpectedly had less than 10 relevant documents in the training set, and the various dialects of TIMIT made it even more difficult for simply comparing the feature sequences. We can clearly see with only 20 out of 640 scores selected based on set B, the MAP reaches 25.88%, significantly higher than the baseline. The oracle results learning on set A itself reached as high as 35.60%, which implied that if a more elaborate learning algorithm was applied, there was still much room for improvement.\nThe parameter sets (\u03c8, \u03b3) for the 20 scores selected based on set B are also printed on Fig. 4. Some observations can be made here. In all the 20 cases Soft=1, so Soft=1 was certainly better. For the selection of N-best sequences or DTW there were no clear trends. However, some correlation between search methods and pattern configurations can be observed. For example, it can be found that Nbest=1 were preferred when l=1 or l=2, probably because too few number of Gaussians limited the accuracies in the 1-best sequence. Also, DTW=1 was preferred very often when m=5 or 3, probably because for shorter patterns it made better sense to merge more than one patterns into a longer pattern, which is actually what the pattern-based DTW did. These results seem to imply\nthe need to learn from a development set, which is not always available. This will be further discussed in the next section."}, {"heading": "4.2. Performance Analysis without a Development Set", "text": "Here we consider the case without a development set. We first consider the different search method \u03b3 by summing all the 80 scores for all combinations of m,n,l, but not \u03b3 as shown in Part (A) of Fig. (5). Several conclusions may be drawn: (a) Soft similarity brought massive improvement (Soft=1 > Soft=0 for all combinations of Nbest and DTW), (b) N-best Sequences brought only a negligible improvement (Nbest=1 \u223c Nbest=0 for all combinations of Soft and DTW), (c) DTW degraded the performance in general (DTW=1<DTW=0 for all combinations of Soft and Nbest). Conclusion (a) is consistent with the conclusion drawn from the top 20 scores in Fig 4. Since generating a soft similarity metric is also the only part of the search that could be conducted off-line, it is certainly attractive. Conclusion (b) and (c) may be a little surprising, since intuitively N-best sequences and DTW can help and quite several of the top 20 scores in Fig 4 had Nbest=1 or DTW=1. As discussed previously with Fig 4, Nbest and DTW could be helpful for specific pattern configurations \u03c8 but not necessarily all. When such specific configurations cannot be properly chosen with a development set, these improvements could be diluted when averaged with all possible configurations. Because the different pattern sets carried complementary information, jointly considering the 80 1-best sequences obtained from the 80 pattern sets itself can be viewed as considering a single large lattice best representing the utterance with all time warping and N-best information included. This is probably why N-best and DTW didn\u2019t help here. The MAP obtained by summing all 640 scores without any selection was 20.62% as shown in Part(A) of Fig (5), which was also significantly higher than the baseline.\nTherefore below we focus our discussion on the selection of pattern configurations of \u03c8 assuming \u03b3=(1,0,0) without using Nbest or DTW. The 80 scores for different \u03c8 forms a 3 dimensional space over (m,n, l). We summed the relevance score over 2 of the dimensions and plotted the performance on the remaining dimension. Summing over (m,n), (m, l), (n, l), we get Parts (B)(C)(D) of Fig. (5) respectively. The average of the MAP values for Parts (B)(C)(D) of Fig. (5) are also listed for comparison between the dimensions. Several additional conclusions may be drawn: (d) the performance is better for larger l as shown in Part(B) of Fig. 5, (e) Model combination on the (m,n) plane was the most effective (the average MAP of Part(B) was much higher than those in Part (C)(D) of Fig. 5), (f) several optimal values seem to exist for m and n (the maximum occurs for m=3 in Part (D) and n=100 in Part (C)). We highly suspect that these optimal points for m, n as mentioned in Conclusion (f) are inert characteristics of the underlying language that should stay approximately the same for a different corpus of the same\nlanguage. Conclusion (d) will probably hold until the phenomenon of over-fitting begins to happen, since the number of Gaussians is limited by the training corpus size. Conclusion (e) implies when blending score with respect to (m,n), a good policy may be to have m and n as diverse as possible, if there is no information regarding the selection of m, and n. From conclusion (f), m=3 seemed close to the average phoneme duration of TIMIT, while n=100 seemed close to the number of phonemes with some context dependency considered. This may imply selection of (m,n) is a language dependent characteristic although we do not have strong evidence yet to back up this claim. This would be useful if verified to be true, especially for under-resourced languages, since in that case the learned weights could be similarly useful for different corpora on the same language.\nWe further plot the MAP of R(\u03c8,\u03b3)(d, q) for \u03b3=(1,0,0) and l=3 for sets A and B in Fig 6, the best performing \u03b3 and l in Parts (A)(B) of Fig 5. The 20 points for (m,n) were interpolated with a 2D spline function to show the smoothed MAP distributions over the plane. As can be seen, the score distributions look similar to a good degree even for completely different query sets in Fig 6(a) and (b). When we simply selected the 20 scores for set A here (m=3,5,7,9,11; n=50,100,200,300; l=3,\u03b3=(1,0,0)), the MAP is 26.32%, slightly higher than 25.88% achieved above by the 20 scores selected greedily from a development set."}, {"heading": "5. CONCLUSION", "text": "In this work, we present a new approach for unsupervised spoken term detection using multi-level acoustic patterns discovered from the target corpus. The different pattern sets with different model configurations are complementary, thus can jointly capture the information for the spoken terms. Significantly better performance than frame-based DTW on TIMIT corpus as obtained."}, {"heading": "6. REFERENCES", "text": "[1] David RH Miller, Michael Kleber, Chia-Lin Kao, Owen Kimball, Thomas Colthurst, Stephen A Lowe, Richard M Schwartz, and Herbert Gish, \u201cRapid and accurate spoken term detection.,\u201d in INTERSPEECH, 2007, pp. 314\u2013317.\n[2] Jonathan Mamou, Bhuvana Ramabhadran, and Olivier Siohan, \u201cVocabulary independent spoken term detection,\u201d in Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 615\u2013622.\n[3] Roy G Wallace, Robert J Vogt, and Sridha Sridharan, \u201cA phonetic search approach to the 2006 nist spoken term detection evaluation,\u201d 2007.\n[4] Yi-cheng Pan and Lin-shan Lee, \u201cPerformance analysis for lattice-based speech indexing approaches using words and subword units,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, no. 6, pp. 1562\u20131574, 2010.\n[5] Murat Saraclar and Richard Sproat, \u201cLattice-based search for spoken utterance retrieval,\u201d Urbana, vol. 51, pp. 61801, 2004.\n[6] Lou Boves, Rolf Carlson, Erhard W Hinrichs, David House, Steven Krauwer, Lothar Lemnitzer, Martti Vainio, and Peter Wittenburg, \u201cResources for speech research: present and future infrastructure needs.,\u201d in INTERSPEECH. Citeseer, 2009, pp. 1803\u20131806.\n[7] Arun Kumar, Nitendra Rajput, Dipanjan Chakraborty, Sheetal K Agarwal, and Amit A Nanavati, \u201cWwtw: the world wide telecom web,\u201d in Proceedings of the 2007 workshop on Networked systems for developing regions. ACM, 2007, p. 7.\n[8] Florian Metze, Nitendra Rajput, Xavier Anguera, Marelie Davel, Guillaume Gravier, Charl Van Heerden, Gautam V Mantena, Armando Muscariello, Kishore Prahallad, Igor Szoke, et al., \u201cThe spoken web search task at mediaeval 2011,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5165\u20135168.\n[9] Chun-An Chan and Lin-Shan Lee, \u201cUnsupervised hidden markov modeling of spoken queries for spoken term detection without speech recognition.,\u201d in INTERSPEECH, 2011, pp. 2141\u20132144.\n[10] Michael A Carlin, Samuel Thomas, Aren Jansen, and Hynek Hermansky, \u201cRapid evaluation of speech representations for spoken term discovery.,\u201d in INTERSPEECH, 2011, pp. 821\u2013824.\n[11] Yaodong Zhang and James R Glass, \u201cUnsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams,\u201d in Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398\u2013403.\n[12] Marijn Huijbregts, Mitchell McLaren, and David van Leeuwen, \u201cUnsupervised acoustic sub-word unit detection for query-byexample spoken term detection,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4436\u20134439.\n[13] Haipeng Wang, Cheung-Chi Leung, Tan Lee, Bin Ma, and Haizhou Li, \u201cAn acoustic segment modeling approach to query-by-example spoken term detection,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5157\u20135160.\n[14] Yaodong Zhang and James R Glass, \u201cA piecewise aggregate approximation lower-bound estimate for posteriorgram-based dynamic time warping.,\u201d in INTERSPEECH, 2011, pp. 1909\u20131912.\n[15] Yaodong Zhang, Kiarash Adl, and James Glass, \u201cFast spoken query detection using lower-bound dynamic time warping on graphical processing units,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5173\u20135176.\n[16] Aren Jansen and Benjamin Van Durme, \u201cIndexing raw acoustic features for scalable zero resource search.,\u201d in INTERSPEECH, 2012.\n[17] Cheng-Tao Chung, Chun-an Chan, and Lin-shan Lee, \u201cUnsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u20138085.\n[18] Aren Jansen, Kenneth Church, and Hynek Hermansky, \u201cTowards spoken term discovery at scale with zero resources.,\u201d in INTERSPEECH, 2010, pp. 1676\u20131679.\n[19] Aren Jansen and Kenneth Church, \u201cTowards unsupervised training of speaker independent acoustic models.,\u201d in INTERSPEECH, 2011, pp. 1693\u20131692.\n[20] Herbert Gish, Man-hung Siu, Arthur Chan, and William Belfield, \u201cUnsupervised training of an hmm-based speech recognizer for topic classification.,\u201d in INTERSPEECH, 2009, pp. 1935\u20131938.\n[21] Mathias Creutz and Krista Lagus, \u201cUnsupervised models for morpheme segmentation and morphology learning,\u201d ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, pp. 3, 2007.\n[22] Hung-yi Lee, Yun-Chiao Li, Cheng-Tao Chung, and Lin-shan Lee, \u201cEnhancing query expansion for semantic retrieval of spoken content with automatically discovered acoustic patterns,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8297\u20138301.\n[23] John R Hershey and Peder A Olsen, \u201cApproximating the kullback leibler divergence between gaussian mixture models,\u201d in Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on. IEEE, 2007, vol. 4, pp. IV\u2013317.\n[24] Marcin Marsza\u0142ek and Cordelia Schmid, \u201cConstructing category hierarchies for visual recognition,\u201d in Computer Vision\u2013ECCV 2008, pp. 479\u2013491. Springer, 2008.\n[25] Filip Radlinski and Thorsten Joachims, \u201cQuery chains: learning to rank from implicit feedback,\u201d in Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. ACM, 2005, pp. 239\u2013248.\n[26] Tie-Yan Liu, \u201cLearning to rank for information retrieval,\u201d Foundations and Trends in Information Retrieval, vol. 3, no. 3, pp. 225\u2013 331, 2009."}], "references": [{"title": "Rapid and accurate spoken term detection", "author": ["David RH Miller", "Michael Kleber", "Chia-Lin Kao", "Owen Kimball", "Thomas Colthurst", "Stephen A Lowe", "Richard M Schwartz", "Herbert Gish"], "venue": "INTERSPEECH, 2007, pp. 314\u2013317.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Vocabulary independent spoken term detection", "author": ["Jonathan Mamou", "Bhuvana Ramabhadran", "Olivier Siohan"], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 615\u2013622.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "A phonetic search approach to the 2006 nist spoken term detection evaluation", "author": ["Roy G Wallace", "Robert J Vogt", "Sridha Sridharan"], "venue": "2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Performance analysis for lattice-based speech indexing approaches using words and subword units", "author": ["Yi-cheng Pan", "Lin-shan Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 18, no. 6, pp. 1562\u20131574, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Lattice-based search for spoken utterance retrieval", "author": ["Murat Saraclar", "Richard Sproat"], "venue": "Urbana, vol. 51, pp. 61801, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1801}, {"title": "Resources for speech research: present and future infrastructure needs", "author": ["Lou Boves", "Rolf Carlson", "Erhard W Hinrichs", "David House", "Steven Krauwer", "Lothar Lemnitzer", "Martti Vainio", "Peter Wittenburg"], "venue": "INTERSPEECH. Citeseer, 2009, pp. 1803\u20131806.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Wwtw: the world wide telecom web", "author": ["Arun Kumar", "Nitendra Rajput", "Dipanjan Chakraborty", "Sheetal K Agarwal", "Amit A Nanavati"], "venue": "Proceedings of the 2007 workshop on Networked systems for developing regions. ACM, 2007, p. 7.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "The spoken web search task at mediaeval 2011", "author": ["Florian Metze", "Nitendra Rajput", "Xavier Anguera", "Marelie Davel", "Guillaume Gravier", "Charl Van Heerden", "Gautam V Mantena", "Armando Muscariello", "Kishore Prahallad", "Igor Szoke"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5165\u20135168.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised hidden markov modeling of spoken queries for spoken term detection without speech recognition", "author": ["Chun-An Chan", "Lin-Shan Lee"], "venue": "INTERSPEECH, 2011, pp. 2141\u20132144.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Rapid evaluation of speech representations for spoken term discovery", "author": ["Michael A Carlin", "Samuel Thomas", "Aren Jansen", "Hynek Hermansky"], "venue": "INTERSPEECH, 2011, pp. 821\u2013824.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams", "author": ["Yaodong Zhang", "James R Glass"], "venue": "Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 398\u2013403.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised acoustic sub-word unit detection for query-byexample spoken term detection", "author": ["Marijn Huijbregts", "Mitchell McLaren", "David van Leeuwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on. IEEE, 2011, pp. 4436\u20134439.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "An acoustic segment modeling approach to query-by-example spoken term detection", "author": ["Haipeng Wang", "Cheung-Chi Leung", "Tan Lee", "Bin Ma", "Haizhou Li"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5157\u20135160.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "A piecewise aggregate approximation lower-bound estimate for posteriorgram-based dynamic time warping", "author": ["Yaodong Zhang", "James R Glass"], "venue": "INTERSPEECH, 2011, pp. 1909\u20131912.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast spoken query detection using lower-bound dynamic time warping on graphical processing units", "author": ["Yaodong Zhang", "Kiarash Adl", "James Glass"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 5173\u20135176.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Indexing raw acoustic features for scalable zero resource search", "author": ["Aren Jansen", "Benjamin Van Durme"], "venue": "INTERSPEECH, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised discovery of linguistic structure including two-level acoustic patterns using three cascaded stages of iterative optimization", "author": ["Cheng-Tao Chung", "Chun-an Chan", "Lin-shan Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8081\u20138085.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards spoken term discovery at scale with zero resources", "author": ["Aren Jansen", "Kenneth Church", "Hynek Hermansky"], "venue": "INTER- SPEECH, 2010, pp. 1676\u20131679.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards unsupervised training of speaker independent acoustic models", "author": ["Aren Jansen", "Kenneth Church"], "venue": "INTERSPEECH, 2011, pp. 1693\u20131692.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised training of an hmm-based speech recognizer for topic classification", "author": ["Herbert Gish", "Man-hung Siu", "Arthur Chan", "William Belfield"], "venue": "INTERSPEECH, 2009, pp. 1935\u20131938.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised models for morpheme segmentation and morphology learning", "author": ["Mathias Creutz", "Krista Lagus"], "venue": "ACM Transactions on Speech and Language Processing (TSLP), vol. 4, no. 1, pp. 3, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Enhancing query expansion for semantic retrieval of spoken content with automatically discovered acoustic patterns", "author": ["Hung-yi Lee", "Yun-Chiao Li", "Cheng-Tao Chung", "Lin-shan Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8297\u20138301.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Approximating the kullback leibler divergence between gaussian mixture models", "author": ["John R Hershey", "Peder A Olsen"], "venue": "Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on. IEEE, 2007, vol. 4, pp. IV\u2013317.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Constructing category hierarchies for visual recognition", "author": ["Marcin Marsza\u0142ek", "Cordelia Schmid"], "venue": "Computer Vision\u2013ECCV 2008, pp. 479\u2013491. Springer, 2008.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Query chains: learning to rank from implicit feedback", "author": ["Filip Radlinski", "Thorsten Joachims"], "venue": "Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. ACM, 2005, pp. 239\u2013248.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to rank for information retrieval", "author": ["Tie-Yan Liu"], "venue": "Foundations and Trends in Information Retrieval, vol. 3, no. 3, pp. 225\u2013 331, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Spoken term detection (STD) usually refers to the task of finding all occurrences of the text query term from a large spoken archive [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 0, "context": "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "Most STD approaches were based on automatic speech recognition (ASR), transforming speech into words or subwords for token matching [1][2][3][4], with performance relying heavily on the ASR accuracy [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 5, "context": "In this way, the difficulties in conventional ASR such as recognition errors and need for annotated training data may be bypassed, which is especially attractive for languages with very limited annotated data [6][7] or spoken content with unknown languages.", "startOffset": 209, "endOffset": 212}, {"referenceID": 6, "context": "In this way, the difficulties in conventional ASR such as recognition errors and need for annotated training data may be bypassed, which is especially attractive for languages with very limited annotated data [6][7] or spoken content with unknown languages.", "startOffset": 212, "endOffset": 215}, {"referenceID": 7, "context": "This leads to recent efforts in unsupervised STD with spoken queries without using annotated data during training [8] [9], which is also the focus of this work.", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "This leads to recent efforts in unsupervised STD with spoken queries without using annotated data during training [8] [9], which is also the focus of this work.", "startOffset": 118, "endOffset": 121}, {"referenceID": 9, "context": "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 10, "context": "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].", "startOffset": 247, "endOffset": 251}, {"referenceID": 11, "context": "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].", "startOffset": 285, "endOffset": 289}, {"referenceID": 12, "context": "Many related works focused on feature representations and distance measures within the DTW framework that are more robust to speaker and acoustic condition variations[10], including using the posteriorgrams from a universal Gaussian mixture model [11], and the acoustic segment models [12][13].", "startOffset": 289, "endOffset": 293}, {"referenceID": 8, "context": "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].", "startOffset": 94, "endOffset": 97}, {"referenceID": 13, "context": "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 15, "context": "Substantial efforts were devoted to reducing this computation load, such as segment-based DTW [9], lowerbound estimation for DTW [14][15], and a locality sensitive hashing technique for indexing speech frames [16].", "startOffset": 209, "endOffset": 213}, {"referenceID": 16, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 147, "endOffset": 151}, {"referenceID": 17, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 151, "endOffset": 155}, {"referenceID": 18, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 155, "endOffset": 159}, {"referenceID": 19, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 159, "endOffset": 163}, {"referenceID": 20, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 163, "endOffset": 167}, {"referenceID": 21, "context": "In recent years substantial effort has been made for unsupervised model based discovery of acoustic patterns from corpora without manual annotation[17][18][19][20][21][22].", "startOffset": 167, "endOffset": 171}, {"referenceID": 18, "context": "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the topology of the HMMs [19][20][21].", "startOffset": 206, "endOffset": 210}, {"referenceID": 19, "context": "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the topology of the HMMs [19][20][21].", "startOffset": 210, "endOffset": 214}, {"referenceID": 20, "context": "Given an unlabelled speech corpus, it is not difficult for unsupervised discovery of the desired acoustic patterns from the corpus for a chosen hyperparameter set \u03c8 that determines the topology of the HMMs [19][20][21].", "startOffset": 214, "endOffset": 218}, {"referenceID": 16, "context": "This can be achieved by first finding an initial label \u03c90 based on a set of assumed patterns for all observations in the corpus \u03c7 as in (1)[17].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "The KL-divergence KL(i, j) between two pattern HMMs in (4b) is defined as the KLdivergence between the states based on the variational approximation [23] summed over the states.", "startOffset": 149, "endOffset": 153}, {"referenceID": 23, "context": "To transform the KL divergence into a similarity measure between 0 and 1, a negative exponential was applied [24] with a scaling factor \u03b2.", "startOffset": 109, "endOffset": 113}, {"referenceID": 24, "context": "Note that although choices of \u03bb(\u03c8k, \u03b3s) can be based on the optimization with respect to an evaluation metric [25][26], the preliminary experiments in this work were to verify the feasibility of the proposed frameworks instead.", "startOffset": 110, "endOffset": 114}, {"referenceID": 25, "context": "Note that although choices of \u03bb(\u03c8k, \u03b3s) can be based on the optimization with respect to an evaluation metric [25][26], the preliminary experiments in this work were to verify the feasibility of the proposed frameworks instead.", "startOffset": 114, "endOffset": 118}], "year": 2015, "abstractText": "This paper presents a new approach for unsupervised Spoken Term Detection with spoken queries using multiple sets of acoustic patterns automatically discovered from the target corpus. The different pattern HMM configurations(number of states per model, number of distinct models, number of Gaussians per state)form a three-dimensional model granularity space. Different sets of acoustic patterns automatically discovered on different points properly distributed over this three-dimensional space are complementary to one another, thus can jointly capture the characteristics of the spoken terms. By representing the spoken content and spoken query as sequences of acoustic patterns, a series of approaches for matching the pattern index sequences while considering the signal variations are developed. In this way, not only the on-line computation load can be reduced, but the signal distributions caused by different speakers and acoustic conditions can be reasonably taken care of. The results indicate that this approach significantly outperformed the unsupervised feature-based DTW baseline by 16.16% in mean average precision on the TIMIT corpus.", "creator": "LaTeX with hyperref package"}}}