{"id": "1509.02512", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2015", "title": "DeepCough: A Deep Convolutional Neural Network in A Wearable Cough Detection System", "abstract": "outlined in this paper, we present a system that principally employs a wearable acoustic ion sensor matrix and suggests a complementary deep convolutional neural network for detecting coughs. instead we strongly evaluate the performance of our system modelled on considering 14 normally healthy volunteers and compare evaluating it to that of other cough detection agent systems that which have just been earlier reported in the literature. preliminary experimental results show that calculating our dynamic system achieves a serum classification sensitivity complexity of perhaps 95. 0 1 % and a dose specificity effectiveness of 99. 5 %.", "histories": [["v1", "Tue, 8 Sep 2015 19:59:19 GMT  (528kb,D)", "http://arxiv.org/abs/1509.02512v1", "BioCAS-2015"]], "COMMENTS": "BioCAS-2015", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["justice amoh", "kofi odame"], "accepted": false, "id": "1509.02512"}, "pdf": {"name": "1509.02512.pdf", "metadata": {"source": "CRF", "title": "DeepCough: A Deep Convolutional Neural Network in A Wearable Cough Detection System", "authors": ["Justice Amoh"], "emails": ["justice.amoh.jr.th@dartmouth.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nAutomated real-time cough detection could be valuable for diagnosis and treatment of airway diseases. Since coughs are relatively rare events, a cough detector has to have a very low false alarm rate in order to be useful at all. On the other hand, such a system needs a high enough sensitivity in order to detect the infrequent event of a cough.\nResearchers have attempted to address these challenges by employing complex hand-crafted or hand-tuned features [1], [2]. Unfortunately, these features can be time-consuming to develop and may not necessarily be optimal for cough detection. In this study, we propose a deep neural network framework for learning good features that yield better cough detection."}, {"heading": "II. SYSTEM OVERVIEW", "text": "The proposed system for cough detection involves a wearable sensor for acquiring the user\u2019s respiratory or vocal sounds in real-time. The sensor, shown in Figure 1, is attached to the chest via a medical-grade foam adhesive. Once worn, it streams lung and abdominal auscultation sounds to a computer or smartphone for further processing and classification of events.\nIn contrast to previous systems that relied on conventional condenser microphones [3], [4], ours is a more applicationspecific sensor consisting of a piezoelectric transducer and additional signal conditioning electronics that enhance acoustic events of interest. In particular, the sensor amplifies respiratory sounds, attenuates voiced speech and eliminates environmental sounds altogether. Also, the contact piezo-transducer captures the additional vibrational energy that coughs induce in the body, which helps to improve discriminability.\nIn the actual classification of coughs, previous works have thoroughly explored hand-crafted features that are good for discriminating between coughs and non-cough sounds. The Mel-Frequency Cepstral Coefficients (MFCCs) and Linear Predictive Coding (LPC) are two popular examples. While\nsuch features have proved effective in speech recognition and have yielded impressive results in cough studies [1], [2], they are not necessarily optimal for the specific task of cough detection. Furthermore, those features have typically been used with conventional condenser microphone signals and may not be appropriate for our contact piezo sensor. So, our proposed system employs the deep convolutional neural network framework to jointly learn good feature representations and to train a robust classifier, suitable for our sensor and for the cough classification task."}, {"heading": "III. COUGH DETECTION DETAILS", "text": "Our cough detection algorithm involves a preprocessing stage, followed by a classifier. The preprocessing stage extracts some preliminary features, and it also limits the amount of uninteresting data that is admitted to the classifier; the classifier is a convolutional neural network (CNN) that performs further feature extraction and labels an audio event as either cough or not."}, {"heading": "A. Preprocessing", "text": "During preprocessing, the stream of acoustic data is segmented into frames which are each 4 ms long. To eliminate irrelevant data such as silence and background noise, the preprocessor implements the frame admission process suggested by Lu et al [5]. For each 16-frame (64 ms) window, the RMS energy is calculated and compared with a predetermined threshold. Windows with low energy are assumed to be silence or ambient sound, and they are discarded. High energy windows are \"admitted\" and undergo further processing.\nFor each admitted window, a 128-bin Short Time Fourier Transform (STFT) is performed to yield a 64\u00d716 spectral seg-\nar X\niv :1\n50 9.\n02 51\n2v 1\n[ cs\n.N E\n] 8\nS ep\n2 01\n5\nment (i.e. spectrogram of 64 ms data). These spectral segments are the inputs which are fed to the CNN for classification."}, {"heading": "B. Convolutional Neural Network Architecture", "text": "CNNs are multilayered perceptron (MLP) models, whose neurons mimic the mammalian visual cortex in that they are sensitive to small overlapping regions of the receptive field [6]. Recently, CNNs have been successfully applied to several computer vision tasks such as object recognition, localization and tracking [7], [8]. CNNs have also been applied to other domains such as action recognition, speech recognition, natural language processing and music genre classification [9]. In our work, we implement a CNN to discriminate between cough and speech sounds.\nOur network consists of five layers: 2 convolutional layers, 2 fully connected layers and a softmax classification layer. Each convolutional layer has 16 rectified linear units (ReLU). The first convolutional layer takes in the 64\u00d716 spectral segments as inputs, and has filters of size 9\u00d73. This is followed by a 2\u00d71 maxpooling layer. The second convolutional layer has filters of size 5\u00d73 and is also followed by a 2\u00d71 maxpooling layer. Convolutions are performed with a stride of 1. The convolutional layers are followed by 2 fully connected layers with 256 rectified linear units each. Each fully connected layer also employs dropout regularization (p=0.5) to reduce overfitting. Finally, the last layer takes the outputs of the second fully connected layer and classifies the input as either a cough or speech event using the softmax function. The network architecture is illustrated in Figure 2.\nWe chose ReLU activations over the traditional tanh or sigmoid functions because ReLU doesn\u2019t have the vanishing gradient problem and often leads to a faster convergence [7]. The convolutional filter sizes are chosen to enable 2D convolutions: across both frequency and temporal domains. Previous applications of convolutional networks in audio sometimes convolved along either time or frequency axis [10]. For our application however, since we know both short-term temporal and spectral patterns can be discriminative for cough and speech events, we convolve along both dimensions. In addition, since our input segments cover a relatively short\ntime window (16 frames, 64 ms), we fix the size of the filters along the time axis (at 3 frames). Pooling layers downsample outputs of convolutions to make computations manageable in subsequent layers. So just like in our filter sizing, we perform no pooling along the time axis to avoid further reducing the rather limited temporal resolution of segments."}, {"heading": "IV. EXPERIMENTAL METHODS", "text": ""}, {"heading": "A. Data Collection", "text": "To build and evaluate the proposed system, we created a database of lung sound recordings from 14 healthy volunteers: 7 males and 7 females. All subjects provided informed consent, and the experimental protocol was approved by the Dartmouth College Institutional Review Board. The piezo sensor was used to collect acoustic data as subjects were guided through a series of procedures, including producing forced bouts of coughs and reading some prompts out loud. Each subject produced an average of 40 cough sounds, yielding a total of 627 cough examples in our database. For the speech data, subjects read out 20 phonetically-balanced prompts from the Harvard Sentences database [11]. Snippets were extracted from these sentence recordings, so that there were equal numbers of speech and cough samples, and each sample was of similar length. For an objective assessment of our piezo sensor, we also used a professional digital recorder, an Olympus LS12, to simultaneously record all the speech and cough sounds. Both the piezo sensor and the Olympus LS-12 recorder were sampled at a 44.1 kHz rate and later down-sampled to 16 kHz."}, {"heading": "B. Network Training", "text": "In training our neural network, we first split our database in two parts: 70% for building the model, and 30% for testing. We further split the model-building data into training and validation sets in an 80:20 ratio. The training set is used to actually train the network. The trained model is then run several times against the validation set to find optimal model hyper-parameters (eg. learning rate, number of filters, etc). Once all hyperparameters are found, the model is retrained and run against the test set for the final evaluation.\nWe augment our input data to introduce some translational invariance in the learning. This is done by re-buffering the 16-frame spectral segments from the same events to have a 4- frame overlap (25%). Segments at the edges are zero-padded as necessary. Our training database resolves into 10,279 segments, with which we train our network. We also standardize the entire training data across all components as is often done in training deep neural networks.\nThe convolutional network is trained using stochastic gradient descent, with a learning rate of 0.001, batch size of 20 and a momentum of 0.9. Training converges after roughly 50 epochs, with a run time of about one hour. Implementation of the convolutional network is done using Lasagne [12], a Theano-based library for training neural networks."}, {"heading": "C. Experiments", "text": "To evaluate the performance of the proposed system, two experiments are undertaken. The first investigates the hypothesis that the CNN extracts better features for cough detection than the traditional hand-crafted MFCC features. The second experiment compares the entire end-to-end detection system with alternative approaches.\n1) Experiment 1: To verify how effective the learned CNN features are for cough classification, we compare against MFCC features. We extract 13 MFCC coefficients from every 8 ms of training examples, with 4 ms overlap (50%). This yields an equivalent number of frames as in the STFT spectral segments (16 per 64 ms) used in training the CNN. Since the classifcation layer of the CNN is a softmax, a softmax function is also trained on these MFCC features. This is to allow for a direct comparison of the representational abilities of the MFCC and CNN with respect to our cough detection task. We also train a Support Vector Machine (SVM) on the MFCC features to observe the potential improvements a more complex classifier could provide. In addition, we train a linear SVM on the raw STFT data to serve as a reference bar for comparison with the CNN features.\n2) Experiment 2: In the second experiment, we compare the two main aspects of our proposed system with the conventional Hidden Markov Model approach.\nThe duration of coughs in our database ranges from 250 ms to about 800 ms. To ensure our speech data is in a similar format, we split each prompt recording into smaller segments with random durations in the same range as those of the cough examples. In all, the average duration of a test example is 320 ms, which makes for a comparable test setup for both the CNN model and the HMM model. For each such ~320 ms window of acoustic data, the HMM model outputs a single class prediction. On the other hand, the CNN model would yield ~4 class predictions, one for each 64ms segment. Hence, to obtain a CNN prediction for the entire window, we average over the classification scores (probabilities) for all segments in the window.\nWith the above testing framework, we first investigate how our system compares with traditional MFCC-based HMM. An HMM with 10 states is trained for each of the classes. The\nfirst and last states are non-emitting, but all middle states have an emission probability distribution modeled by a 13- dimensional mixture of Gaussians. For each training example, 13 MFCC coefficients are extracted for every 25 ms frame and the sequence of these coefficient vectors is used to train the HMMs. At test time, a similar feature vector sequence extracted from the test example is fitted to both HMMs. The resulting log-likelihood values of both fits determines whether the sound pertains to a cough or speech event. This HMM configuration is fairly common in cough studies and in speech recognition [13].\nUsing the same model setup and testing framework, we also investigate how the piezo sensor compares with conventional condenser microphones, for cough detection. Since we have accompanying microphone recordings for all collected sensor data, we train HMM and CNN models on microphone data and compare its performance with that of the piezo sensor."}, {"heading": "V. RESULTS & DISCUSSION", "text": "Table 1 shows the results for experiment 1. First, we observe that our CNN model performs much better (~10% more) than training an SVM on the raw STFT data. This agrees with the notion that the CNN is indeed extracting features from the STFT, that are useful for the classification task. Furthermore, based on this dataset, the CNN model appears to outperform the MFCC with either the softmax classifier or the SVM, suggesting that the CNN is a more effective feature extractor for cough classification. An interesting observation we note is that the MFCC+SVM model yields a specificity comparable with that of the CNN model. Specificity, in this binary cough-speech discrimination, refers to the model\u2019s accuracy in detecting speech events. And so a possible explanation for why MFCC still manage to yield a high specificity could be the fact that MFCCs are particularly designed to mimic how we humans hear and is known to be very effective for speech recognition and processing. A fairly able classifier like the SVM can leverage this innate MFCC representational abilities for speech for a better specificity. The STFT+SVM model performance supports this idea of an MFCC speech advantage, seeing as it has comparable sensitivity as the MFCC+SVM model, but fails to match its specificity.\nThe Receiver Operating Characteristic (ROC) curves in Figure 3 show how our proposed system compares with the conventional microphone and HMM approaches. Our CNN based system substantially outperforms the HMM models with\nan AUC close to 1 in both piezo sensor and microphone scenarios.\nIn Table 2, we compare our system with other previous works in cough detection. Based on our data, our model seems to outperform the others with a sensitivity of 95.1%. However, the reported specificity values may not be directly comparable with our work since the other studies consider silence and low energy events as false negatives too."}, {"heading": "VI. CONCLUSIONS", "text": "In this study, we proposed a system that employs a wearable acoustic sensor and a deep convolutional neural network for detecting coughs. We evaluated our model\u2019s ability to extract good features for the custom sensor and for the cough detection task. We also show that our convolutional network model outperforms previous works in the literature.\nFuture works will explore other neural network architectures more suitable to time varying inputs such as Recurrent Neural Networks. A more immediate follow up study would be an extended passive data collection where subjects or patients can wear our sensors for hours or days to capture un-forced coughs.\nACKNOWLEGMENT\nThis work is supported by the Neukom Institute for Computational Science at Dartmouth College. We would like to thank Professor Lorenzo Toressani (Computer Science Department, Dartmouth College) for discussions and suggestions on deep learning methods and resources."}], "references": [{"title": "The automatic recognition and counting of cough.", "author": ["S.J. Barry", "A.D. Dane", "A.H. Morice", "A.D. Walmsley"], "venue": "Cough (London, England),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "An automated system for 24-h monitoring of cough frequency: the leicester cough monitor.", "author": ["S. Matos", "S.S. Birring", "I.D. Pavord", "D.H. Evans"], "venue": "IEEE transactions on bio-medical engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "The Leicester Cough Monitor: Preliminary validation of an automated cough detection system in chronic cough", "author": ["S.S. Birring", "T. Fleming", "S. Matos", "a. a. Raj", "D.H. Evans", "I.D. Pavord"], "venue": "European Respiratory Journal, vol. 31, no. 5, pp. 1013\u20131018, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Accurate and privacy preserving cough sensing using a low-cost microphone", "author": ["E.C. Larson", "T. Lee", "S. Liu", "M. Rosenfeld", "S.N. Patel"], "venue": "Proceedings of the 13th international conference on Ubiquitous computing - UbiComp \u201911, p. 375, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Sound- Sense: scalable sound sensing for people-centric applications on mobile phones", "author": ["H. Lu", "W. Pan", "N. Lane", "T. Choudhury", "A. Campbell"], "venue": "Proceedings of the 7th international conference on Mobile systems, applications, and services, pp. 165\u2013178, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances In Neural Information Processing Systems, pp. 1\u20139, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "International Conference on Machine Learning, vol. 32, pp. 647\u2013655, 2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "Ieee Signal Processing Magazine, no. November, pp. 82\u201397, 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised feature learning for audio classification using convolutional deep belief networks.", "author": ["H. Lee", "P. Pham", "Y. Largman", "A. Ng"], "venue": "Nips, pp", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "IEEE recommended practice for speech quality measurements", "author": ["E.H. Rothauser", "W.D. Chapman", "N. Guttman", "H.R. Silbiger", "J.L. Sullivan"], "venue": "IEEE Trans. Audio . . . , vol. AU-17, no. 297, pp. 225\u2013246, 1969.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1969}, {"title": "Lasagne: First release.", "author": ["S. Dieleman", "J. Schl\u00fcter", "C. Raffel"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Detection of Cough Sounds in Continuous Audio Recordings Using Hidden Markov Models", "author": ["S. Matos", "S. Member", "S.S. Birring", "I.D. Pavord", "D.H. Evans", "S. Member"], "venue": "vol. 53, no. 6, pp. 1078\u2013 1083, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "A Neural Network Based Algorithm for Automatic Identification of Cough Sounds", "author": ["V. Swarnkar", "U.R. Abeyratne", "S.M. Ieee", "Y. Amrulloh", "C. Hukins", "R. Triasih"], "venue": "pp. 1764\u20131767, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Researchers have attempted to address these challenges by employing complex hand-crafted or hand-tuned features [1], [2].", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "Researchers have attempted to address these challenges by employing complex hand-crafted or hand-tuned features [1], [2].", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "In contrast to previous systems that relied on conventional condenser microphones [3], [4], ours is a more applicationspecific sensor consisting of a piezoelectric transducer and additional signal conditioning electronics that enhance acoustic events of interest.", "startOffset": 82, "endOffset": 85}, {"referenceID": 3, "context": "In contrast to previous systems that relied on conventional condenser microphones [3], [4], ours is a more applicationspecific sensor consisting of a piezoelectric transducer and additional signal conditioning electronics that enhance acoustic events of interest.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "such features have proved effective in speech recognition and have yielded impressive results in cough studies [1], [2], they are not necessarily optimal for the specific task of cough detection.", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "such features have proved effective in speech recognition and have yielded impressive results in cough studies [1], [2], they are not necessarily optimal for the specific task of cough detection.", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "To eliminate irrelevant data such as silence and background noise, the preprocessor implements the frame admission process suggested by Lu et al [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "Recently, CNNs have been successfully applied to several computer vision tasks such as object recognition, localization and tracking [7], [8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": "Recently, CNNs have been successfully applied to several computer vision tasks such as object recognition, localization and tracking [7], [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 7, "context": "CNNs have also been applied to other domains such as action recognition, speech recognition, natural language processing and music genre classification [9].", "startOffset": 152, "endOffset": 155}, {"referenceID": 5, "context": "We chose ReLU activations over the traditional tanh or sigmoid functions because ReLU doesn\u2019t have the vanishing gradient problem and often leads to a faster convergence [7].", "startOffset": 170, "endOffset": 173}, {"referenceID": 8, "context": "Previous applications of convolutional networks in audio sometimes convolved along either time or frequency axis [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "For the speech data, subjects read out 20 phonetically-balanced prompts from the Harvard Sentences database [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "Implementation of the convolutional network is done using Lasagne [12], a Theano-based library for training neural networks.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "This HMM configuration is fairly common in cough studies and in speech recognition [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "HACC [1] LPC-PNN 15 1000 80.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "LCM [2] MFCC-HMM 19 10000 85.", "startOffset": 4, "endOffset": 7}, {"referenceID": 12, "context": "[14] NN 3 100 93.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] PCA-Random Forest 17 150 92.", "startOffset": 0, "endOffset": 3}], "year": 2015, "abstractText": "In this paper, we present a system that employs a wearable acoustic sensor and a deep convolutional neural network for detecting coughs. We evaluate the performance of our system on 14 healthy volunteers and compare it to that of other cough detection systems that have been reported in the literature. Experimental results show that our system achieves a classification sensitivity of 95.1% and a specificity of 99.5%.", "creator": "LaTeX with hyperref package"}}}