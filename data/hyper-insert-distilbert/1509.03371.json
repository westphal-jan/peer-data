{"id": "1509.03371", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2015", "title": "Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems", "abstract": "this comparative work therefore presents and analyzes three convolutional neural network ( cnn ) detection models for combining efficient pixelwise classification mechanisms of corrupted images. besides when using convolutional neural research networks together to classify single pixels in patches nodes of typically a whole map image, suppose a lot of redundant computations analyses are carried out when using sliding transparent window networks. this sophisticated set of fairly new architectures theoretically solve this specific issue efficiently by simultaneously either removing redundant redundant computations or integrate using seemingly fully convolutional architectures filters that cannot inherently predict to many pixels at such once.", "histories": [["v1", "Fri, 11 Sep 2015 01:20:46 GMT  (3683kb,D)", "http://arxiv.org/abs/1509.03371v1", "92 pages, project source code available atthis https URL, technical report written at ETH Z\\\"urich, in collaboration with AMD, UZH INI and HHMI Janelia"]], "COMMENTS": "92 pages, project source code available atthis https URL, technical report written at ETH Z\\\"urich, in collaboration with AMD, UZH INI and HHMI Janelia", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["fabian tschopp"], "accepted": false, "id": "1509.03371"}, "pdf": {"name": "1509.03371.pdf", "metadata": {"source": "CRF", "title": "Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems", "authors": ["Fabian Tschopp"], "emails": [], "sections": [{"heading": null, "text": "The implementations of the three models are accessible through a new utility on top of the Caffe library. The utility provides support for a wide range of image input and output formats, pre-processing parameters and methods to equalize the label histogram during training. The Caffe library has been extended by new layers and a new backend for availability on a wider range of hardware such as CPUs and GPUs through OpenCL.\nOn AMD GPUs, speedups of 54\u00d7 (SK-Net), 437\u00d7 (U-Net) and 320\u00d7 (USKNet) have been observed, taking the SK equivalent SW (sliding window) network as the baseline. The label throughput is up to one megapixel per second.\nThe analyzed neural networks have distinctive characteristics that apply during training or processing, and not every data set is suitable to every architecture. The quality of the predictions is assessed on two neural tissue data sets, of which one is the ISBI 2012 challenge data set. Two different loss functions, Malis loss and Softmax loss, were used during training.\nThe whole pipeline, consisting of models, interface and modified Caffe library, is available as Open Source software under the working title Project Greentea.\ni"}, {"heading": "Acknowledgements", "text": "University of Zurich, Institute of Neuroinformatics\nFirstly I would like to express my gratitude to my supervisor Dr. Jan Funke for his guidance, motivation and the opportunity to visit HHMI Janelia in Ashburn, Virginia, USA. I also thank my supervisor Prof. Dr. Angelika Steger for collaborating with the Institute of Neuroinformatics, which made this research project possible. I thank Stephan Gerhard and Julien Martel for interesting discussions about neural networks and this technical report.\nHoward Hughes Medical Institute, Janelia\nBesides my advisors I would like to thank Dr. Srinivas Turaga and Dr. Stephan Saalfeld for their collaboration at HHMI Janelia, which inspired me to extend the scope of my research and gave me insight into the applications of neural networks for image segmentation in connectomics. Besides this, Janelia has the nicest campus of all research institutes that I have seen so far.\nAMD (Advanced Micro Devices)\nI would like to thank AMD and especially Roy Taylor, Greg Stoner and Bruno Stefanizzi for the generous hardware sponsoring, which empowered a lot of the development on the Caffe library and enabled me to use neural network models beyond what is possible on regular hardware. I also thank Timmy Liu for his assistance and development of clBLAS and Dr. Ing. Herve\u0301 Chevanne for providing drivers and support for the AMD GPUs. Being a fan of AMD hardware and using their devices for over ten years, it was a pleasure for me to work together with AMD engineers and using their newest hardware and software technology for research.\nFamily\nLast but not the least, I would like to thank my family, my parents and my sister, for supporting me during my Bachelor studies at ETH Zurich.\nii\nContents\nContents iii"}, {"heading": "1 Introduction 1", "text": "1.1 Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . 1 1.2 Caffe Library . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 Pixelwise Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.4 Existing Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.5 New Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.6 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"}, {"heading": "2 Datasets 7", "text": "2.1 DS1 - Segmented anisotropic ssTEM dataset of neural tissue . . . . . 7 2.2 DS2 - ISBI 2012 dataset of neural tissue . . . . . . . . . . . . . . . . . 9"}, {"heading": "3 Models 11", "text": "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Sliding Window (SW-Net) . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.3 SK-Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.3.1 Converting SW Networks to SK . . . . . . . . . . . . . . . . . 13 3.3.2 SK Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.4 U-Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.5 USK-Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20"}, {"heading": "4 Caffe Neural Tool 24", "text": "4.1 Functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.2 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.3 Histogram Equalization . . . . . . . . . . . . . . . . . . . . . . . . . . 26\niii\nContents"}, {"heading": "5 Caffe Library 29", "text": "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.2 Modified Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n5.2.1 SK Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.2.2 N-Dimensional Layers . . . . . . . . . . . . . . . . . . . . . . . 30\n5.3 New Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5.3.1 Merge Crop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5.3.2 Malis Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5.3.3 Affinity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.3.4 Connected Components . . . . . . . . . . . . . . . . . . . . . . 36 5.4 OpenCL Backend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 5.4.1 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 5.4.2 OpenCL Hybrid . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.5 Convolution Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 38"}, {"heading": "6 Benchmarks 40", "text": "6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 6.2 Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 6.3 Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 6.4 Device Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 6.5 Labeling Throughput . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 6.6 Layer Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . . 47\n6.6.1 SK-Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.6.2 U-Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6.6.3 USK-Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n6.7 NUMA Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 6.8 Alexnet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55"}, {"heading": "7 Results 56", "text": "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 7.2 Analysis on DS1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n7.2.1 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 7.2.2 Numerical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 7.2.3 Visual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n7.3 Analysis on DS2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 7.3.1 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 7.3.2 Numerical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 7.3.3 Visual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\niv\nContents"}, {"heading": "8 Conclusion 66", "text": "8.1 Research Time Line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 8.2 Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 8.3 Difficulties Encountered . . . . . . . . . . . . . . . . . . . . . . . . . . 67 8.4 Reproducibility of Results . . . . . . . . . . . . . . . . . . . . . . . . . 68 8.5 Outlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n8.5.1 Device Abstracted Backend . . . . . . . . . . . . . . . . . . . . 69 8.5.2 Improving Training Data . . . . . . . . . . . . . . . . . . . . . 69 8.5.3 Parameter Grid Search . . . . . . . . . . . . . . . . . . . . . . . 69 8.5.4 Testing of Volumetric Architectures . . . . . . . . . . . . . . . 69 8.5.5 Improving Test Metrics . . . . . . . . . . . . . . . . . . . . . . 70\n8.6 Final Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70"}, {"heading": "A Network Architectures 72", "text": "A.1 SK-Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 A.2 U-Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 A.3 USK-Net . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\nv\nChapter 1\nIntroduction"}, {"heading": "1.1 Convolutional Neural Networks", "text": "Convolutional neural networks are forward-backward neural networks that are mostly based on convolutions with machine learnable kernels, pooling operations and element-wise non-linear activation functions. The networks can be employed for various image classification and object recognition tasks. A prominent example is the ImageNet / AlexNet [1] for object recognition. Recent networks [2] can have very many, in this case over 20, layers and millions of learnable parameters.\nThis work is focused on classifying biomedical data, in particular neural tissue electron microscopy images (see Chapter 2). The challenge with this kind of data sets is that training data is more scarce than with data sets that can be generated from everyday pictures such as handwritten letters or online collections of images. Annotating ground truth for neural tissue images is a lot of manual work, as every single pixel has to be labeled.\nConsequently, improving training speeds is not a primary objective to optimize for. The data that has to be processed with a trained model afterwards however can easily reach terabyte-scale. It is therefore crucial to develop networks that are as efficient as possible in the forwarding step. This work presents three such efficient pixel classification networks.\nWhen training, the classical mislabeling objectives such as Softmax or Cross-entropy loss might not be the most useful for pixelwise classifications of biological images, and using spatial context information to generate an error signal to train the neural networks can perform better. Therefore, this research also considers different training methods including the Malis [3] criterion.\nTo relate the objectives of this technical report with the title, it needs to be dissected into its components:\n\u2022 Efficient Convolutional Neural Networks means the network models analyzed and designed are as efficient as possible in getting the task done - in this case, pixelwise classification of electron microscopy neural tissue images.\n\u2022 Pixelwise Classification, as opposed to image classification, aims to propose a label to each pixel in a given image. It can also be seen as many separate\n1\n1.2. Caffe Library\nimage classifications of small patches in a bigger image. This gives rise to new optimization possibilities as the contexts for the predictions overlap spatially.\n\u2022 Heterogeneous Hardware Systems means the network models used should also run as efficient as possible on a variety of compute devices. This objective makes efficient neural networks more accessible to users and allows to use existing hardware and clusters to get segmentation tasks with neural networks done."}, {"heading": "1.2 Caffe Library", "text": "Caffe stands for Convolutional Architecture for Fast Feature Embedding [4]. It is a state-of-the-art neural network library that has been heavily optimized for the use with nVidias CUDA technology. In many cases, the library is therefore already very efficient using certain GPUs. What was missing until now [5] is fast CPU support (the current CPU backend is mostly single threaded) and support for GPUs and accelerator devices from AMD and Intel. The library is still under development and has a large community [6].\nNetwork models and trained weights (usually called model zoo) can be shared in Google\u2019s prototxt (network and learning configurations) and protocol buffer (trained weights and solver states) format.\nThe library is typically used on the command line with the Caffe binary or through a python (Pycaffe) interface (see Figure 1.3). For more advanced and intrusive interfaces, C++ interfaces can be programmed on top of the library.\nAll models, utilities and backend additions programmed for this project are based on and around Caffe. The changes to the library are documented in Chapter 5."}, {"heading": "1.3 Pixelwise Classification", "text": "Pixelwise classification means labeling each pixel in an image based on a local context around the pixel. Figure 1.1 shows how this works with sliding window networks that outputs a single pixel per input tile of size v + w = 101 + 1 = 102.\nWhile minibatch processing can output many pixels at once, this is still inefficient (see Sections 3.2 and 6.5). The work by Hongsheng Li et al. [7] allows to make existing SW networks more efficient while giving identical prediction results (see Section 3.3.1). Alternatively, fully convolutional models (U and partially also USK) directly output a bigger patch, as depicted in Figure 1.2. This method of training and processing is called patch-based (n = 1, w > 1), as opposed to minibatch-based (n > 1, w = 1). A combination of both (n > 1, w > 1) is possible but only useful when the images in the data set can not be tiled with large w 1 (see Section 6.4).\nMinibatches can still have advantages during training (see Section 4.3) because every element in the minibatch can be picked independently. During processing, networks that output large patches are always performing better.\n2\n1.3. Pixelwise Classification\nIn all cases, the images have to be extended (padded) by mirroring on the borders by v2 pixels on each side if every pixel of the image is to be labeled.\n3\n1.4. Existing Work"}, {"heading": "1.4 Existing Work", "text": "This technical report is based on the following existing work:\n\u2022 SW (sliding window) network designed by Julien Martel [10], not published. The architecture is trimmed for segmenting the data set DS1.\n\u2022 Strided kernel convolution and pooling kernels by Hongsheng Li et al. [7]. This is the fundamental approach in speeding up existing SW networks.\n\u2022 Malis criterion, first introduced by Srinivas Turaga et al. [3]. The criterion supports an alternative way of training neural networks through affinity graphs, which is very specific and useful on biomedical data, where areas are separated by background borders.\n\u2022 The Open Source Caffe library maintained by the Berkeley Vision and Learning Center [6], [4].\n\u2022 U network designed by Ronneberger et al. [2]. This model is also optimized for biomedical images and especially the data set DS2 (ISBI 2012 [11]).\n\u2022 N-dimensional convolution kernels by Jeff Donahue [12].\n\u2022 Segmentation evaluation scripts of the ISBI 2012 challenge [11], [13]."}, {"heading": "1.5 New Contributions", "text": "An overview of new contributions to the Caffe landscape in terms of models, utilities and library changes is given in Figure 1.3. This work tackles the given problem on all levels - from using efficient BLAS libraries over backend development and frontends for easy use to new network models.\nOn the side of neural network models, this project introduces two new neural network architectures, the SK-Net (Section 3.3) and USK-Net (Section 3.5).\nA meta-analysis of the three efficient networks (SK, U, USK) is given based on:\n\u2022 Differences and characteristics of the network designs (Chapter 3).\n\u2022 Computational cost and efficiency (Chapter 6).\n\u2022 Image segmentation quality, assessed both numerically and visually (Chapter 7) for the typical foreground-background two label classification.\nIn order to be able to train the network models easily on various data sets, the Caffe Neural Tool (Chapter 4) has been developed.\nThe Caffe library (see Chapter 5) has been extended with new layers and adaptions for compability with the new backend. The new layers also affect the functionality of the CUDA backend. OpenCL backend development (see Section 5.4) was mostly focused on versatility and completeness, so that CPUs and all kinds of compute devices can be used on all network models. This includes compability to three different BLAS libraries: clBLAS, ViennaCL-BLAS and cBLAS.\n4\nThis report is also providing an introduction into segmentation and pixelwise classification with neural networks. It contains all the details necessary to understand existing models and readers should be able to easily design their own neural networks based on the findings of this research project.\nThe combination of the new contributions and the Caffe library infrastructure is summarized under the working title Project Greentea. However, Greentea also stands for the new OpenCL backend architecture which has been optimized for high flexibility (see Section 5.4).\nGreentea was a name of my choice because I simply prefer greentea over coffee (Caffe). Greentea can not be used as an abbreviation like it is the case with Caffe, but as greentea is supposed to be good for the brain, using it for a neural network machine learning toolset seems to be appropriate.\n5\n1.6. Terminology"}, {"heading": "1.6 Terminology", "text": "The most used symbols and abbreviations in the report:\n\u2022 Forwarding, processing: Computing data through a neural network from input to output.\n\u2022 Backwarding, training, backpropagation: Computing neural network gradients (diff maps) in the backward direction and updating the network weights.\n\u2022 Data blob: Memory blob containing feature maps of forward processing in the neural network.\n\u2022 Diff blob: Memory blob containing the differential / error signal map during backpropagation.\n\u2022 BLAS: Basic Linear Algebra Subprograms. Includes functions such as efficient matrix multiplications.\n\u2022 DS1: Data set 1, see Section 2.1.\n\u2022 DS2: Data set 2, see Section 2.2.\n\u2022 SW: Sliding window networks for pixelwise classification, see Section 3.2.\n\u2022 SK: Strided kernel networks for pixelwise classification, see Section 3.3.\n\u2022 U: Ronneberger et al. [2] network architecture, see Section 3.4.\n\u2022 USK: Network architecture combining SK and U aspects, see Section 3.5.\n\u2022 f : Number of feature maps after ( fout) and before ( fin) a network layer.\n\u2022 w: Size (in each dimension) of a feature map in a network layer. When not indexed or otherwise noted, it refers to the output size of a network layer.\n\u2022 v: Size of the total network input padding (context).\n\u2022 p: Network layer padding.\n\u2022 s: Network layer stride.\n\u2022 k: Network layer kernel size.\n\u2022 d: Network layer kernel stride.\n\u2022 L: Set of layers with layers l \u2208 L. \u2022 B: Set of memory blobs with blobs b \u2208 B. \u2022 W: Set of network weights, |W| denotes the number of weights. \u2022 M: Device or host memory usage.\n\u2022 q: Number of queues in the Caffe OpenCL backend.\n\u2022 n: Network minibatch size.\n\u2022 A: Affinity graph data, \u2206A denotes the affinity graph diff.\n\u2022 I: Pixel image data, \u2206I denotes the image diff.\nSome symbols are used differently in some sections of the report and are explained in-place.\n6\nChapter 2\nDatasets"}, {"heading": "2.1 DS1 - Segmented anisotropic ssTEM dataset of neural", "text": "tissue\nThis data set shows neural tissue from a Drosophila larva ventral nerve cord and was acquired using serial section Transmission Electron Microscopy at HHMI Janelia Research Campus [8]. The training data consists of 20 images of 1024 by 1024 pixels raw ssTEM and the corresponding segmentation. It is segmented into nine different labels, which are consolidated into foreground and background for two label training and evaluation (see Section 7.2):\n7\n2.1. DS1 - Segmented anisotropic ssTEM dataset of neural tissue\n\u2022 #0: Horizontal cell membranes - background\n\u2022 #1: +45\u25e6 to vertical cell membranes - background\n\u2022 #2: Vertical cell membranes - background\n\u2022 #3: -45\u25e6 to vertical cell membranes - background\n\u2022 #4: Cell membrane junctions - background\n\u2022 #5: Glia cells - background\n\u2022 #6: Mitochondria - foreground\n\u2022 #7: Synapses - background\n\u2022 #8: Cell interior - foreground\nThe idea behind label consolidation in this way is that the network, during training, can learn the separation borders between neural cells. This is especially important with the Malis criterion loss (see Section 5.3.2), which can only segment into foreground and background. With the Softmax loss, it is also possible to let the network learn the labels separately and combine them accordingly afterwards. The network has to learn the features separately either way, as the membrane for example depends on different orientations in the convolution filters. This means a network can be trained on two labels only and afterwards, all nine labels can be extracted by applying a short fine-tuning training phase to the network.\nFor evaluation, the training data set has been split into training and testing data because of the lack of a segmented test data stack:\n\u2022 Train images: 0, 1, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 18 and 19\n8\n2.2. DS2 - ISBI 2012 dataset of neural tissue\n\u2022 Test images: 2, 7, 12 and 17\nAs all slices of the data set are very similar, cross validation by splitting the data set in different ways was not applied. The total amount of pixels for training is therefore 16 \u00b7 10242 \u2248 16 Mpixel.\nThe data set with the corresponding test and train scripts is available in the Caffe Neural Models repository [9] as dataset 01."}, {"heading": "2.2 DS2 - ISBI 2012 dataset of neural tissue", "text": "This data set is from the ISBI 2012 challenge [11], [14], [15]. The raw images and corresponding segmentation images for training are 512 by 512 pixels. The neural tissue features have a similar scale to the data set DS1. The raw images have a bit less contrast and are more fuzzy.\nThe training set has 30 images, which gives a total of 30 \u00b7 5122 \u2248 7.8 Mpixel, which is about half as much as on DS1.\nThe test data used on DS2 is a separate stack of 30 images of size 512 by 512 pixels. For those images, segmentation ground truth is not available for public download, thus the evaluation reported in Section 7.3 is solely based on the reports of the official ISBI 2012 evaluation [11], which is still open for new results.\nFor both stacks, the data spans 2 x 2 x 1.5 microns with a resolution of 4 by 4 by 50 nm/pixel [11].\n9\nThe data set with the corresponding test and train scripts is available in the Caffe Neural Models repository [9] as dataset 02.\n10\nChapter 3\nModels"}, {"heading": "3.1 Introduction", "text": "This chapter describes how the network architectures were set up for training and processing the data sets DS1 and DS2. They were configured for two label classification, however also nine labels were tested and even more could be learned through the Softmax loss. With Malis loss (Section 5.3.2), only foreground and background separation is implemented.\nFor all networks architectures, no special striding or padding was used. Therefore the striding parameter is set to be s = 1, except for downsampling and sliding window pooling operations, which use s = k (stride matches kernel size). The padding is always p = 0. All networks use square size filters and feature maps, which simplifies the descriptions to just one value per parameter. The models can be generalized to arbitrary dimensions with different sizes in each dimension. At the time of the project, SK networks up to 6 dimensional can be configured with the modified Caffe library provided [5].\nThe mini batch size used for processing in the SW network is n = 256, but the choice is arbitrary and only limited by GPU memory. With SK, U and USK networks, a minibatch size of one (n = 1) is sufficient to reach 100% GPU utilization, and often the GPU memory is not sufficient for bigger mini batch sizes (see Section 6.4). It is rather useful to increase the network output size than using minibatches, if the data set allows it by having big enough input pictures.\nAll efficient networks presented here (excluding the sliding window) can be run with almost any size of output prediction maps. Only constraints on even divisibility with pooling operations, as well as the size constraints given by convolutions and strided kernels (see Section 3.3) have to be met. The networks can therefore be run on different input image sizes, depending on memory requirements (see Section 6.4) and data set image size, without re-design and re-training of the networks. The results are numerically identical in this case.\nThe total padding (v) of the networks is a characteristic of the network itself and can not be changed without re-design and re-training. It describes the amount of context considered for each pixel prediction.\n11\n3.2. Sliding Window (SW-Net)\nTo fit the networks to data sets with features of different scales than DS1 and DS2 (see Chapter 2), it may be necessary to adapt kernel sizes and layers to get good predictions after training. Here, the networks are configured so that a big mitochondrion (about 100 by 100 pixels) would fit into the context of a pixel prediction centered on the mitochondrion."}, {"heading": "3.2 Sliding Window (SW-Net)", "text": "Sliding window networks classify an image by taking a pixel and a border padding v of some size around it as input and classify the center pixel by running the patch through a neural network. Then the next pixel is labeled by shifting the window patch by one pixel, classifying the neighboring pixel of the first one. The pixels can also be processed in a minibatch to increase GPU utilization and amortize direct memory access transfer times (from host to device memory). This is still very inefficient as most of the context of two neighboring pixels overlaps and the same filters are applied over the whole context. The redundant computations can be reduced for a patch of input pixels by using SK networks.\nThe sliding window network described here was developed by Julien Martel [10]. It is the baseline for calculating the speedups obtained with the SK, U and USK networks. The structure of the SW network was also used when designing the SK network and the core of the USK network. The reason why this was used as a basis is that it has already been trained on the data set DS1 and had good results. Learning rate, weight decay and training parameters were available. However, no numerical evaluation or publication about the network architecture exists.\nThe sliding window network has not been evaluated in-depth in terms of benchmarking and quality assessment. This is because the Caffe Neural Tool (see Chapter 4) used for detailed benchmarking and processing does not work with minibatches in its current form. Its segmentation performance should however be numerically equal to the SK network, which is derived from the SW network.\n12\n3.3. SK-Net\nAny differences would be due to different training methods (patches on SK versus minibatches on SW). Information about how single layers speed up from SW to SK networks (both theory and experimental) can be found in the work of Hongsheng Li et al. [7].\nFor weight initialization, the SW network uses random initialization drawn from a Gaussian distribution with \u00b5 = 0 and \u03c3 = 0.01."}, {"heading": "3.3 SK-Net", "text": ""}, {"heading": "3.3.1 Converting SW Networks to SK", "text": "Hongsheng Li et al. [7] provide a pseudo code (page 4) on how to convert a sliding window network to a strided kernel network. However, it is incomplete on consistency checking, kernel sizes and feature map output sizes. Also, the theory of converting inner product (fully connected) layers is not described. Therefore I provide a more complete version (see Algorithm 1), although without considering padding and striding. This enforces s = 1 and p = 0 in all layers of the SK network. Each data dimension (width, height, depth) can be processed separately for the kernel size (k), kernel stride (d) and output dimension (w). The algorithm is able to convert networks and find consistency issues fully automatized.\nFor Caffe prototxt network configurations, only the kernel size k and kernel stride d have to be provided. Output dimensions will be computed on the fly, given the network input size w(0)SK . Padding and striding parameters can be left away, they will default to the correct values.\nAlgorithm 1 assumes that |LSW| = |LSK| = N, not taking into account the input data layer, which is at i = 0. If a layer type is not handled in a special if -case, it is handled by using the exact same configuration as in the original network. This fails however if the layer does anything other than an element-wise operation (this implies k = 1), because of the kernel stride d > 1. During the conversion, the initial input size w(0)SK is equal to what the SW network used. Afterwards, an arbitrary input size w(0)SK \u2265 w (0) SW can be used during training and processing, and the output will be of size w(N)SK = w (0) SK \u2212 w (0) SW + 1.\nUsing w(0)SW = w (0) SK also helps to prove that the results stay numerically the same. In this case, the strides introduced by pooling operations will be implicitly ignored. They will not be taken into account at the first inner product layer (ip1), which will span the whole feature map size, because the external kernel size is\nw(i\u22121)SK = (k (i) SK \u2212 1)d (i) SK + 1 =\u21d2 w (i) SK = 1 (3.1)\nand w(i\u22121)SW = k (i) SW = k (i) SK =\u21d2 w (i) SW = 1 (3.2)\nat that layer. This property can be visualized as well, as in Figure 3.1, where the first inner product layer (ip1) has a kernel size of kSK = 3 = w (9) SW.\n13\n3.3. SK-Net\nFor the originally inner product (fully connected) layers in SW-Net, which are now normal convolutions, (ip1 to ip3) there are no computational savings compared to the SW network anymore. This is clear from the observation that the ip1 layer isolates the context of each pixel and no overlappings in the feature maps exist after this layer. The number of input ( fin) and output ( fout) feature maps remains exactly the same for SW and SK networks in all layers.\nAlgorithm 1 Convert SW-Net to SK-Net\n1: procedure Convert 2: \u2200i \u2208 [1, N].s(i)SK \u2190 1 3: \u2200i \u2208 [1, N].p(i)SK \u2190 0 4: w(0)SK \u2190 w (0) SW 5: dtemp \u2190 1 6: for i = 1; i \u2264 N; i\u2190 i + 1 do 7: if l(i)SW = convolution then 8: l(i)SK \u2190 convolution SK 9: k(i)SK \u2190 k (i) SW\n10: d(i)SK \u2190 dtemp 11: w(i)SK \u2190 w (i\u22121) SK \u2212 (k (i) SK \u2212 1) \u00b7 d (i) SK . w (i) SW \u2190 w (i\u22121) SW \u2212 (k (i) SW \u2212 1) 12: else if l(i)SW = pooling then 13: if w(i\u22121)SW mod k (i) SW 6= 0\u2228 k (i) SW 6= s (i) SW then return error 14: l(i)SK \u2190 pooling SK 15: k(i)SK \u2190 k (i) SW 16: d(i)SK \u2190 dtemp 17: w(i)SK \u2190 w (i\u22121) SK \u2212 (k (i) SK \u2212 1) \u00b7 d (i) SK . w (i) SW \u2190 \u2308 w(i\u22121)SW k(i)SW\n\u2309 18: dtemp \u2190 dtemp \u00b7 k(i)SK 19: else if l(i)SW = inner product then 20: l(i)SK \u2190 convolution SK 21: k(i)SK \u2190 w (i\u22121) SW . k (i) SW = w (i\u22121) SW is implicit 22: d(i)SK \u2190 dtemp 23: w(i)SK \u2190 w (i\u22121) SK \u2212 (k (i) SK \u2212 1) \u00b7 d (i) SK . w (i) SW \u2190 1 24: dtemp \u2190 1 25: else 26: if k(i)SW > 1 then return error 27: lSK \u2190 lSW 28: w(i)SK \u2190 w (i\u22121) SK . w (i) SW \u2190 w (i\u22121) SW 29: if dtemp = 1 then return success 30: else return error\n14\n3.3. SK-Net\nactivations are not displayed separately.\nObservations on Algorithm 1:\n\u2022 Line 13: Implies only downsamlpling pooling layers can be converted. Pooling with s = 1 instead of s = k would need to be handled like convolutions in terms of kernel and output size, and would not change the kernel stride d. This has not been assessed further.\n\u2022 Line 17: Caffe can handle downsampling poolings that overlap the border in SW networks, which causes implicit zero padding. This is not allowed in SK networks, therefore Line 13 checks if w mod k = 0. Continuing without this check would cause the new strided pooling layer to overlap on data that would normally be separated by a stride, and also output feature maps of wrong sizes to continue.\n\u2022 Line 18: Using downsampling is the only operation that increases the kernel stride. All other operations either keep the context local (convolutions) or have k = 1 (element-wise operations).\n\u2022 Lines 11, 17, 23: Interestingly, all converted layer types with kernel sizes now\n15\n3.3. SK-Net\ndecrease the feature map sizes by the same formula. Convolution and inner product layers implicitly inherit this behavior. Pooling does this because it separates output pixels by a stride equal to the downsampling kernel size k.\n\u2022 Line 29: Having a kernel stride d > 1 in the last layer implies pixels in the output feature maps are not independent from each other. In this case, the network has not been converted correctly and possibly lacks at least one inner product layer."}, {"heading": "3.3.2 SK Network", "text": "When processing the network in Table 3.1, the condition w(i\u22121)SW mod k (i) SW 6= 0 given by Algorithm 1 is actually violated by the second pooling layer, having 43 mod 2 6= 0. It is easy to fix this by starting at the last layer of the network and computing\nw(i\u22121)SW = k (i) SWw (i) SW (3.3)\nfor pooling layers and w(i\u22121)SW = (k (i) SW \u2212 1) + w (i) SW (3.4)\nfor inner product and convolution layers. The corrected network is given in Table 3.2. Converting this to SK results in the network in Table 3.3.\n16\nThe final three inner product layers from the SW network actually become convolutions with special properties: For ip1, the rules stay the same as for converted convolution layers. Afterwards, ip2 and ip3 can have an arbitrary kernel stride d because the kernel size k only spans one pixel in each feature map. To not cause confusion, it should be configured so that d = 1 for those layers.\nThe network still has one issue, which is not nice but acceptable and the network will still work. The issue is that there is no center pixel, because\n(w(0)SW=\u0302102) mod 2 = 0 (3.5)\nEach patch in the original network has a context of 102 pixels. When converting to SK and classifying a patch of 128 by 128 pixels as given in Table 3.3, the padding to add in the beginning actually becomes v = 101 pixels. This padding can not be split up into a border around the patch to classify. To simplify this issue, a border of 51 pixels on each side is assumed and then cropped by one pixel on the bottom and right side. This results the same behavior as running a 102 by 102 pixel sliding window network across the input, which was also padded with 51 pixels in the corrected version and 50 pixels in the original version.\nTo estimate the number of free parameters (all convolution weights |W|) that can be trained in a model, the following formula is used:\n|W| = \u2211 l(i)\u2208Lconv.\nf (i)in \u00b7 f (i) out \u00b7 (k(i))2 (3.6)\nUsing the values in Table 3.3, this gives |W| \u2248 20.5 \u00b7 106 parameters, of which most (\u2248 19.6 \u00b7 106) are within the ip1 layer.\nFor weight initialization, the SK network uses random initialization drawn from a Gaussian distribution with \u00b5 = 0 and \u03c3 = 0.01.\n17\n3.4. U-Net\nA directed acyclic graph representation of the network can be found in the appendix A.1."}, {"heading": "3.4 U-Net", "text": "The U-Net presented here is the network configuration as described in the Ronneberger et al. paper [2]. Table 3.4 describes the network in the same style as Table 3.3 for the SK network in order to compare them. The layer names are chosen in the same style as with SW and SK networks. The U-Net has contracting and expanding sections:\n\u2022 Contracting: Two convolutions followed by one max pooling layer.\n\u2022 Expanding: Deconvolution followed by a convolution to reduce the number of feature maps, a mergecrop and two convoluton layers.\nThe source code for running U-Net as well as the prototxt configuration files were not available for download at the time of this project, thus the network presented here, which is my own interpretation, might differ from the original design. The paper does not give all details, such as how the MergeCrop layer and Upconvolution work. The configurations of this U-Net included in the Caffe Neural Models [9] are therefore incompatible to the original work [2]. Segmentation results should be comparable.\nLayer Type w fin fout k s data MemoryData 572 3 3 1 1 conv1 + relu1 Convolution + ReLU 570 3 64 3 1 conv2 + relu2 Convolution + ReLU 568 64 64 3 1 pool1 Max Pooling 284 64 64 2 2 conv3 + relu3 Convolution + ReLU 282 64 128 3 1\n18\n3.4. U-Net\nThe U-Net architecture has |W| \u2248 29 \u00b7 106 parameters, using Equation 3.6 and the corresponding values in Table 3.4. Thus there are about 30% more parameters than in SK-Net. The number of weights rises towards the middle of the U network, due to having the same convolution kernel size (3 by 3) throughout the network and more feature maps with every contracting step.\nFor weight initialization, the U network uses random initialization drawn from a Gaussian distribution with \u00b5 = 0 and \u03c3 = \u221a 2/( fin \u00b7 k2). This is the same as used in the original paper [2]. With \u03c3 = 0.01 as in SK and SW models, the network could not be trained properly.\n19\n3.5. USK-Net\nThe upconvolutions are set to nearest neighbor interpolation, which causes each pixel in the fin maps to fill exactly four pixels (two by two) in the fout maps.\nFigure 3.3 represents the network in a style identical to the one used by Ronneberger et al. [2]. A directed acyclic graph representation of the network can be found in the appendix A.2."}, {"heading": "3.5 USK-Net", "text": "The USK network architecture combines ideas from the U and SK models. The majority of the convolutions and therefore free parameters can be trained on downsampled feature maps by using one or more contracting path steps (and their expanding counterpart) from the U-Net. After one contracting step, the same sequence of layers as in the SK network is\n20\n3.5. USK-Net\napplied, however with different kernel sizes to match the sizes of the inputs and outputs as required by the U subnetwork. The network has been configured so that:\n\u2022 The network outputs 512 by 512 pixel labels.\n\u2022 The context considered for each pixel classification is 180 by 180 pixels.\n\u2022 As a result, the network input is 692 by 692 pixels.\n\u2022 The SK network part (conv3 to ip2) is required to accept 344 by 344 pixels as input and outputs 258 by 258 pixels. It therefore sees a context of 86 pixels. This is on once by a factor of two downsampled feature maps.\nPrior experiments with two contracting and expanding steps were not very successful, as many features of both datasets (DS1 and DS2) vanished at two times downsampling and the network also became much harder to train because it gets very deep. The network mainly relied on the filter results in the two contracting and expanding step pairs (U subnet) to classify the given input. The signals coming from the SK subnetwork were largely ignored by setting their weights close to zero in the convolution following after the first mergecrop layer. Using only one downsampling step fixed this issue and the SK subnetwork contributed properly after training. As the SK subnetwork contains the main computational costs (see Figures 6.9 and 6.10 in Section 6.6.3) and also carries most parameters, the features have to be meaningful enough at the beginning of the subnetwork, after the first few layers of the U subnetwork.\n21\n3.5. USK-Net\nThe USK-Net architecture has |W| \u2248 5.5 \u00b7 106 parameters, using Equation 3.6 and the corresponding values in Table 3.5. This is a fraction (about 25%) of the SKand U-Net weights. The savings mainly come from reducing the ip1 layer, which now only has \u2248 4.2 \u00b7 106 weights. While the ip1 layer is still the most expensive one, the network is more balanced than SK-Net. The inner product layers are less important, because the U subnetwork merges and convolves the feature maps from the beginning of the network together with upsampled signals from the ip2 layer. Figure 3.4 displays the balanced feature maps on the two processing paths. Furthermore, the USK-Net inherits the advantage of having bigger kernel sizes than only 3 by 3 (U-Net), going up to 8 by 8 in the ip1 and 6 by 6 in the conv2 layer. This means the USK-Net can learn features with looking at a bigger context inside the feature maps, while still almost reaching the speed of U-Net (see Section 6.5) in forward processing.\n22\n3.5. USK-Net\nWith less weights and less layers (less depth) than the U-Net, it is easier to train and also gave better segmentation results on the two evaluation data sets (see Chapter 7).\nThe USK network uses random Gaussian weight initialization with \u00b5 = 0 and \u03c3 = 0.01 for the SK subnet and \u03c3 = \u221a 2/( fin \u00b7 k2) for the U subnet. Other initializations, such as \u03c3 = 0.01 for the whole network caused the network to either disable many neurons (causing feature maps with zero activation) or stopped the loss from decreasing early during training.\nA directed acyclic graph representation of the network can be found in the appendix A.3.\n23\nChapter 4\nCaffe Neural Tool"}, {"heading": "4.1 Functionality", "text": "As the standard Caffe binary [6] does not support training with patches, a new interface had to be written on top of the Caffe library. One option is to use the Pycaffe interface with custom python code to load images and pre-process them for patch training and processing as depicted in Figure 1.2.\nI chose to implement a C++ interface similar to the original binary, because OpenCV and OpenMP can be used for efficient, parallelized algorithms to preprocess raw and label images.\nThe Caffe Neural Tool [16] can be configured for training, processing and benchmarking with a prototxt file similar to the configuration files used to set up networks and solvers (learning configurations) in Caffe. The two most important functionalities are histogram equalization during training and the image preprocessor, which can prepare the label and raw images in various ways before filling up the neural network.\nTemplate configurations for training and processing on the data sets DS1 and DS2 as well as benchmark scripts for U-, SK- and USK-Net are available in the Caffe Neural Models repository [9].\nFor training, the following parameters have to be provided:\n\u2022 A Caffe solver prototxt configuration which contains parameters such as learning rate, weight decay, solver method and the network to use for training.\n\u2022 The padding size (v), network output (patch) size (w), network input ( fin) and output ( fout) feature map count.\n\u2022 A folder with raw images and a folder with the corresponding label images, which are matched by alphabetic order.\n\u2022 The preprocessor configuration block, including histogram equalization settings.\n24\n4.2. Preprocessing\n\u2022 Optionally, a solverstate file to resume a training. The already learned weights and current learning rate will be loaded instead of the initial network configuration.\nFor processing, the following parameters have to be provided:\n\u2022 A Caffe network prototxt configuration to use for processing.\n\u2022 A caffemodel file, containing the trained network weights.\n\u2022 A folder with raw images and the segmentation output options (file type, pixel format and folder).\n\u2022 The padding size (v), network output (patch) size (w), network input ( fin) and output ( fout) feature map count.\n\u2022 The preprocessor configuration block (without histogram equalization).\n\u2022 Optionally, all memory blobs in the network can be stored during processing. This feature is called filter output and is useful to check if the network learns the right convolution filters.\nThe tool also supports a variety of input and output image formats: Normal JPEG, PNG, TIF and BMP files as well as TIF image stacks (multiple images in one file) and 32 bit floating point TIF instead of integer pixel values.\nWhen benchmarking, the tool re-uses existing training and processing configurations, but only fills the network with random data. It will report memory usage, layer wise forward and backward times and the total processing time of the network. For convolution layers, it will also estimate and store the computational complexity. This was used to generate the results in Chapter 6."}, {"heading": "4.2 Preprocessing", "text": "The preprocessing options available are:\n\u2022 Label consolidation, allowing to combine multiple labels into one. This technique was used on the data set DS1 (see Section 2.1). The consolidation is applied after histogram equalization, allowing to balance out difficult and rare labels before consolidating to only background and foreground. This also allows to mark important small, difficult features in the training data.\n\u2022 Rotation of the training patches to a random multiple of 90\u25e6.\n\u2022 Random mirroring of the training patches.\n\u2022 Blurring training patches with a Gaussian kernel of any size. The blurring has a zero mean and the variance is picked at random from a normal distribution. The mean and variance of the distribution can be selected.\n\u2022 CLAHE (contrast limited adaptive histogram equalization), with a clipping parameter. The function is integrated with OpenCV.\n\u2022 Patch normalization to [\u22121.0, 1.0] in floating point before feeding the neural network.\n25\n4.3. Histogram Equalization\nThere are a few reasons why arbitrary rotation is not available:\n\u2022 Interpolating to any angle that is not a multiple of 90\u25e6 causes aliasing of training patches (labels and raw images).\n\u2022 Arbitrary rotations make the training patch smaller due to corner cutting. This means there are less available total patches and the computation of histogram equalization methods gets much more complicated. The assumptions about the patch prior and label posterior distributions do not hold anymore.\n\u2022 It is questionable how useful the additional training data would be. Elastic deformations would have more potential in generating unique new training data for biological images such as neural tissue EM images of DS1 and DS2."}, {"heading": "4.3 Histogram Equalization", "text": "Histogram equalization is a technique to balance out the frequency of labels that the network sees during training. As training with patches leads to a set of dependent pixels which are in close proximity on an image (see Figure 1.2), the training results can be worse than with minibatches. With minibatches (see Figure 1.1), it is possible to create a database of single pixel labels and the minibatch can draw n independent pixels to train with in each stochastic gradient descent step.\nThe first equalization approach is a patch prior, which will prefer patches with rare labels. This is done by comparing the label distribution within each patch to the total label frequency in all training images.\nrj = n\u22121 \u2211 i=0 aj,i ci\n(4.1)\nc\u0302i = 1 Zi m\u22121 \u2211 j=0 rj \u00b7 aj,i (4.2)\nFor patches j = 0, . . . , m\u2212 1 and labels i = 0, . . . , n\u2212 1, Equation 4.1 calculates the weight for each patch (rj) based on the total label distribution ci and the frequency within each patch (aj,i). Equation 4.2 calculates the label posterior distribution c\u0302i based on the patch weights rj and the label frequency within the patch. Zi is a normalization factor to get \u2211n\u22121i=0 c\u0302i = 1.\nThe method does help if there are patches that have rare labels, because those patches will be drawn at random with a higher probability than others. An example is the synapse label (number 7) (see Figure 4.1 and Table 4.1). It cannot balance the labels which have a similar distribution in every patch - for example cell membranes versus cell interior.\nWhen the patch size gets bigger and approaches the size of the training images, the label distribution after the patch prior approaches the original label distribution. Thus the patch prior only works with small training patches. It can also completely\n26\n4.3. Histogram Equalization\nequalize the histogram when using a single pixel as patch size (w = 1), as this is the same situation as with independent pixels.\nWhen calculating label frequencies ci, it is taken into account that pixel labels closer to the border of the image are covered by less patches than those in the center of the image. A patch can start and end at any offset inside the image. Corner pixels for example are only covered by the one patch that starts in that corner.\nAfter applying the patch prior, the new label distribution c\u0302i is depicted in Figure 4.1 with values from Table 4.1.\nThe second method masks pixels randomly in each patch, where the random function is thresholded by the inverse frequency of the label. This means a pixel of label type i gets masked (removed) from the error map if:\nc\u22121i \u00b7 (minj cj) \u22121 < p (4.3)\nIn Equation 4.3, p \u2208 [0, 1] is a random value picked at uniform and ci \u2208 [0, 1] the label frequency for label i = 0, . . . , n \u2212 1. Less frequent labels are less likely to\n27\n4.3. Histogram Equalization\nbe masked out, while membrane and cell interior labels, which are very common, get masked with a high probability. The result is a completely balanced label histogram (see Table 4.1). The least frequent label consequently never gets masked.\nThe best solution to train on an exact label distribution as desired remains to do minibatch training. This is theoretically also possible with SK-, USK- and U-Net, but very inefficient in terms of training speed.\nMinibatch training is the standard for most networks, also for SW-Net. The network weights are updated every time after a patch or minibatch has run through forwarding and backwarding. The gradients get accumulated over all error pixels in the Softmax loss layer and are then normalized for stable training.\nHaving a balanced label distribution is mostly important with Softmax and multi label classification. When doing background-foreground separation with Malis, the patch prior has little to no effect and masking can not be used at all. Malis does already focus the error on problematic zones globally over the whole patch that runs through the network (see Section 5.3.2) and therefore the label distribution during training does not matter.\n28\nChapter 5\nCaffe Library"}, {"heading": "5.1 Introduction", "text": "Adapting the Caffe library for efficient pixelwise classification on heterogeneous hardware contains the most programming work in the scope of the project, changing 20\u2019000 lines of code compared to the BVLC master branch [5], [6].\nThe changes can be grouped into adaptions on different levels:\n\u2022 Modified solver and network code to support the Caffe Neural Tool C++ interface.\n\u2022 Modified existing layers to fit the pixelwise inputs and outputs.\n\u2022 Additional layers for SK, U and USK architectures.\n\u2022 Additional layers for the Malis loss.\n\u2022 Redesign of N-dimensional layers to support up to 6D convolutions and max pooling with strided kernels.\n\u2022 OpenCL and OpenCL hybrid code for supporting a wide range of GPUs and CPUs.\n\u2022 Backend adaptions to allow dynamic backend dispatching at run- and compiletime.\n\u2022 Adapted GNU Makefile and CMake build infrastructures to support the new OpenCL backend.\nThe changes are implemented in a way that does not break backward compability to the original library. All existing network models and trained networks can still be used. The source code remains highly maintainable and was ahead of the Caffe BVLC branch [6] during the whole scope of the project.\n29\n5.2. Modified Layers"}, {"heading": "5.2 Modified Layers", "text": ""}, {"heading": "5.2.1 SK Layers", "text": "SK (strided kernel) layers are layers with a kernel size k > 1 and an inner stride in the kernel (d > 1). The result is a kernel that looks at a feature map in a context of (k\u2212 1) \u00b7 d + 1 pixels, which is also called the external kernel size. The motivation to have such kernels is to be able to convert single pixel prediction networks (sliding window (SW) networks), to patch prediction networks (SK). How this works is visible in the Figures 1.1 and 1.2, as well as the strided representation in Figure 3.1.\nFor SK convolutions, the matrix multiplication (see Section 5.5) stays the same as with normal convolutions. Only the im2col and col2im memory copy kernels have to be adapted. For this, I used the existing kernel codes provided by Hongsheng Li et al. [7].\nChanging existing im2col and col2im kernels to support d > 1 is trivial: The kernels just read the data by iterating over all input feature maps and dimensions. Within each dimension, the iteration goes over the kernel size k, copying data into the convolution buffer. With strided kernels, when iterating over the kernel, the reading pointer has to be increased by the dimension stride multiplied by the kernel stride d instead of just adding the dimension stride. The dimension stride is (w(i\u22121))j, starting at j = 0 for the first dimension and w(i\u22121) denoting the input feature map size.\nThe same iteration scheme applies for pooling operation kernels. All other layers do not have to be adapted and work together with SK layers as long as they have a kernel size of k = 1 (see Algorithm 1).\nStrided kernels are only implemented in CUDA and OpenCL and do not run as native CPU code."}, {"heading": "5.2.2 N-Dimensional Layers", "text": "The Caffe library is able to specify an arbitrary amount of dimensions for the blob memory infrastructure used to pass data between layers. However, not all layers automatically work in higher dimensions. For most element-wise kernels, nothing has to be changed. Convolutions and pooling operations require a slight redesign.\nFor convolutions, the matrix multiplication stays again the same as with normal and strided kernel convolutions (see Section 5.5). There were existing kernels for normal N-dimensional convolutions by Jeff Donahue [12]. However, those kernels only support the default kernel stride d = 1.\nIn the scope of this research project, at HHMI Janelia, I combined the existing code of the strided kernel and N dimensional convolutions to get the most generalized form of convolutions, supporting up to 6 dimensions and kernel strides. The 6 dimension limit exists because allocating local arrays of dynamic sizes is not allowed in OpenCL and CUDA. The arrays are required to store temporary variables such as iterators for each dimension.\n30\n5.3. New Layers\nDerived from the convolution code, I also implemented the max pooling function as ND-SK kernel.\nIn the case of 1D or 2D, the SK and normal convolution layers should be considered, as looping over two dimensions is more efficient than having an outer loop over dimensions and an inner loop which processes one dimension at a time. The ND layers should be used from 3D to 6D.\nThis report does not analyze any networks with a dimension higher than two, but it is possible to replace ConvolutionSK layers by ConvolutionND layers in the SKNet to get an arbitrary dimensional network. It may be required to reduce the network output size and feature map count in order to meet memory constraints.\nN-dimensional kernels are only implemented in CUDA and OpenCL and do not run as native CPU code."}, {"heading": "5.3 New Layers", "text": ""}, {"heading": "5.3.1 Merge Crop", "text": "The MergeCrop layer is required in U- and USK-Net architectures (see Sections 3.4 and 3.5). The layer accepts two input blobs:\n\u2022 Blob A of size w(i\u22121)A .\n\u2022 Blob B of size w(i\u22121)B \u2265 w (i\u22121) A .\nThe layer outputs a blob containing all feature maps of A and B, which can have a different amount of feature maps. Input B has to be cropped to the size of A. The output feature maps are of size w(i) = w(i\u22121)A .\nDuring backpropagation, the error maps are propagated through by copying them in the inverse direction. For U- and USK-Net, backwarding is only enabled for the input A, as input B gets the differential data on a different path (from the down sampling pooling layer) in the neural network. Copying back B would overwrite the gradients and interfere with the intended training.\nMergeCrop is only implemented in CUDA and OpenCL and does not run as native CPU code."}, {"heading": "5.3.2 Malis Loss", "text": "MALIS stands for maximum affinity learning of image segmentation. The implementation in Caffe [5] that I provide is based on existing code to compute the Malis criterion for Matlab [17] and Torch [18] by Srinivas Turaga et al. [3].\nAdditional layer forward and backward functions (for interaction with Caffe), memory management and the two additional layers ConnectedComponent and Affinity are new contributions. The layers are only implemented as CPU code and do not run on OpenCL or CUDA.\nFigure 5.1 describes how the Malis criterion loss is used together with the network models presented in this report (see Chapter 3). In place of the Softmax activation\n31\n5.3. New Layers\nfor two labels, a rectified linear unit or other activation could also be used. The split layer is required to feed the ground truth label blob into the two following components and affinity layers.\nThe layer structure is separable so that the Malis loss can be used with feeding in external connected component and label affinity maps instead of computing them indirectly. Likewise, the neural network can directly learn affinity graphs instead of pixelwise labels.\nAs the Malis criterion is calculating minimum spanning trees between pixel pairs on affinity graphs and is only selecting the minimum edge of the tree as error edge, if the position is indeed an error in the prediction, the actual Malis function is called twice on two affinity maps A+ and A\u2212.\n32\n5.3. New Layers\nHaving the predicted affinity graph Apred (=prob affinity) and the ground truth to it (Alabel (=label affinity)), two new affinity maps are generated:\nA+ = min(Apred, Alabel) (5.1) A\u2212 = max(Apred, Alabel) (5.2)\nThen, A+ will not contain any errors on the background prediction while A\u2212 has no errors in the foreground. The result is that the Malis criterion is able to isolate errors in the background prediction (membranes with gaps) on A\u2212 and find errors on the cell interior with A+. The resulting error map for backpropagation is \u2206A = \u2206A\u2212 + \u2206A+.\nIn Equations 5.1 and 5.2 the affinity maps A denote the combination of vertical (A(y)) and horizontal (A(x)) affinity maps. The affinity graphs contain one value per edge between two neighboring pixels. A value close to 1 stands for high affinity while 0 means unconnected.\nDetails on how the Malis criterion works internally can be found in the original paper on Malis by Srinivas Turaga et al. [3]. The implementation is quite efficient and does not contribute massively to the training time.\nThe error map \u2206I+, which is derived from \u2206A using the affinity layer during backpropagation, is depicted in Figure 5.3. On a first look, it seems like only cell interior errors, the light gray areas, exist in the picture. When enhancing the contrast and marking errors in the membrane with red spots, it becomes more clear what happens: The Malis criterion calculates minimum spanning trees between pixels of the cell interior of two merged cells that should\n33\n5.3. New Layers\nbe separated. The edge that will be corrected is almost always the same between two cells for all pixel pairs within those cells, due to shared edges in the spanning tree. This edge will therefore accumulate a high error value in \u2206A\u2212, but there are only a few such spots in every training patch.\nOn the cell interior errors (\u2206A+), the edges to be corrected occur on the border of cell interior areas that are mislabeled as cell membrane. This leads to a denser distribution of the error, but less error intensity per edge.\nMalis loss and the U/USK-Net models match perfectly: As the networks can output very large patches up to 512 by 512 pixels without reaching memory limits of current GPUs (see Section 6.4), connected components and the Malis loss have a very large context to work on. As soon as the patch size is so small that it barely covers a cell, Malis becomes useless. With sliding window networks for example, it would be necessary to compute many forwarding iterations first before being able to run Malis and generate an error map to backpropagate. It would also be necessary to retain all blobs of all forwarding iterations. The whole process would not be efficient in terms of memory consumption and training times."}, {"heading": "5.3.3 Affinity", "text": "The affinity layer is an additional layer that has to be used in conjunction with Malis loss (see Figure 5.1).\nThe idea is that not only affinity graphs but also pixelwise classifications can be\n34\n5.3. New Layers\nlearned with the Malis loss. During forwarding, the affinity layer has to look at neighboring pixels in the horizontal and vertical direction of the input and compute their connectivity (affinity).\nA(x)x,y = min z=x,x+1 I+z,y (5.3)\nA(y)x,y = min z=y,y+1 I+x,z (5.4)\nM(x)x,y = arg min z=x,x+1 I+z,y (5.5)\nM(y)x,y = arg min z=y,y+1 I+x,z (5.6)\nIn Equations 5.3 to 5.6, A(x) is the horizontal oriented affinity map, A(y) the vertical one. The affinity maps have the same size as the input image I. Additionally, the minimum index maps M(x) and M(y) have to be stored so that the loss can be distribute accordingly during backpropagation.\nThe Softmax layer (blob prob) in Figure 5.1 and the ground truth labels (blob label a) actually store both the foreground and background (see Figure 5.2). For the image I used to produce the affinity map, only the foreground prediction map (I+) is considered because it stores 1 for foreground (connected) and 0 for background (disconnected). The resulting affinity graph will correctly have higher values for connected pixels than disconnected ones.\nThe affinity graph is computed twice: Once for the ground truth (blob label affinity in Figure 5.1) and once for the current network prediction (blob prob affinity).\nDuring backpropagation, the affinity loss has to be attributed to single pixels again, as the Malis criterion will attribute the error map to an affinity graph. Now, both the foregound (\u2206I+) and background (\u2206I\u2212) have to get an error map to balance out the Softmax function. The loss is attributed symmetrically, \u2206I+ = \u2212\u2206I\u2212.\nInitialization:\n\u2206I+ (x,y) = \u2206I \u2212 (x,y) = 0 (5.7)\n(5.8)\nUpdate:\n\u2206I+ M(x)x,y ,y\n+ = \u2206A(x)x,y (5.9)\n\u2206I\u2212 M(x)x,y ,y\n\u2212 = \u2206A(x)x,y (5.10)\n\u2206I+ x,M(y)x,y\n+ = \u2206A(y)x,y (5.11)\n\u2206I\u2212 x,M(y)x,y\n\u2212 = \u2206A(y)x,y (5.12)\n35\n5.4. OpenCL Backend\nEquations 5.8 to 5.12 describe how to attribute the affinity loss back to pixel loss, given the minimum index maps M(x) and M(y) computed in the forward processing step.\nThere are also other ways to compute an estimation to an affinity graph, such as averaging neighboring pixels. The choice for the minimum selection worked particularly well because there is no loss of resolution or aliasing when computing it this way. The Malis criterion selects the minimum edge of the affinity graph for creating the loss maps \u2206A(x) and \u2206A(y). Thus using the minimum valued pixel through M(x) and M(y) for attributing the pixel loss makes sense. Both objectives minimize the same error by either increasing or decreasing the affinity of the neighboring pixels."}, {"heading": "5.3.4 Connected Components", "text": "The connected components layer is a small layer based on the OpenCV floodfilling algorithm and outputs separated connected components from a foregroundbackground labeled ground truth (Figure 5.4). Based on this map, the Malis loss knows which areas have to be separated and which are connected. Here, the cell membrane, which is considered background, is not assigned to any component."}, {"heading": "5.4 OpenCL Backend", "text": ""}, {"heading": "5.4.1 Implementation", "text": "In my version of the Caffe library, an additional versatile backend for various compute devices, based on OpenCL and ViennaCL [19], is available. The backend is called Greentea and is part of the Project Greentea consisting of frontend, models and modified Caffe library (see Figure 1.3). In this section, an overview of interesting aspects how the backend works and how the Caffe library had to be changed\n36\n5.4. OpenCL Backend\nis given. Further details and a full documentation is available within the source code, which is available for download [5].\nA key feature is that the OpenCL backend is feature equivalent to the CUDA backend. All GPU layers can be used on both backends. The OpenCL backend is also unit test verified and passes all test cases of the original Caffe library. The tests can be invoked by executing \u201cmake runtest\u201d on the source code folder.\nIt remains possible to compile the library with support for all backends at once. The compute kernel and BLAS calls can be dispatched dynamically at runtime, depending on what kind of device is selected. Every device available is registered in a new DeviceContext object that stores the device and backend type.\nThe following aspects of the library had to be changed:\n\u2022 The Caffe library enumerates all devices on all enabled backends, starting with CUDA devices. The selected GPU number determines which DeviceContext is set as the default.\n\u2022 The SyncedMem class that is used to manage the device memory can now store either a CUDA GPU pointer or an OpenCL cl mem memory object, depending on which device and backend the memory object belongs to.\n\u2022 The Forward gpu and Backward gpu functions now contain both OpenCL and CUDA code to call compute kernels and BLAS functions.\n\u2022 Network layers, SyncedMem and blob objects carry a pointer to a DeviceContext, which allows to express neural network object to device relationships. This is a feature for allowing future multi-device networks, where keeping track of which memory blob is on which device is essential.\n\u2022 All CUDA compute kernels are translated to OpenCL code. This is trivial for the most part, as the syntax is very similar.\n\u2022 The OpenCL backend can dispatch BLAS calls to ViennaCL-BLAS or clBLAS on GPUs. ViennaCL-BLAS is header-only and therefore easier to use, while clBLAS is optimized for certain AMD GPUs but has to be compiled separately.\n\u2022 On CPU devices, the Greentea backend mixes native CPU code with OpenCL code to achieve an optimal performance (see Section 5.4.2)."}, {"heading": "5.4.2 OpenCL Hybrid", "text": "The OpenCL hybrid implementation describes how the OpenCL backend is used when selecting a CPU device instead of a GPU device. The two fundamental differences are memory allocation and BLAS library calls.\nWhen a SyncedMem object is instantiated, the memory is allocated as host memory rather than device memory. Differently than with the CPU backend, the memory is allocated through OpenCL. This allows to access the underlying memory pointer of OpenCL memory objects while also being able to use the memory in OpenCL kernels.\n37\n5.5. Convolution Methods\nFor BLAS calls, the following steps are executed:\n1. For all involved cl mem memory objects, the underlying host pointer is recovered and mapped to a new CPU pointer. At this point, the OpenCL backend ensures all compute kernels accessing the memory concurrently are done executing so that it is safe to access the memory over CPU pointers.\n2. The BLAS call is dispatched to a cBLAS library (Intel MKL, ATLAS or OpenBLAS). Here, the most optimized BLAS for the CPU device can be selected. Optimally, it should be a BLAS that is fully parallelized and uses all CPU cores. NUMA issues (see Section 6.7) might occur.\n3. As soon as the BLAS call returns, all CPU pointers are unmapped. This signals to the OpenCL backend that it is safe again to start OpenCL kernels on the cl mem objects involved.\nWhen mapping cl mem objects, it can be specified that the access is read-only. In this case, OpenCL kernels that also only use the object in read-only mode can continue to run during the BLAS call.\nUsing the OpenCL backend on CPUs is a design decision. An alternative would be to parallelize the existing CPU backend with OpenMP pragmas. However, as most of the computational complexity resides with the BLAS calls (see Section 5.5) and the OpenCL kernels are not using local memory extensively, they run very well also on CPUs. Only the BLAS, which is very device specific, and needs to be optimized for the memory architecture (see Section 6.3), needs to be different from the GPU version of the OpenCL backend."}, {"heading": "5.5 Convolution Methods", "text": "Convolutions are usually computed using three different methods:\n1. GEMM (matrix multiplication) convolutions, requiring a reshape of the input to fit the BLAS SGEMM scheme.\n2. Direct convolutions, shifting the convolution kernel directly over the input.\n3. FFT domain convolutions, requiring to compute at least two Fourier transforms and one inverse Fourier.\nCaffe implements GEMM convolutions. The advantage is that highly efficient BLAS libraries are available specifically for various devices, such as clBLAS, cuBLAS and OpenBLAS. It is very hard to implement convolutions more efficiently using direct convolution. The performance of such implementations is not portable for different kernel sizes and hardware types. In my own preliminary experiments, not even 10 % efficiency could be reached in the ip1 layer of SK-Net, compared to up to 90 % using GEMM convolution, including the time for input reshaping (see Section 6.6.1, Table 6.6 and Figure 6.6).\nIt is particularly difficult to get good local and global memory access patterns when programming kernels for direct convolution. BLAS libraries have already been optimized to use GPU local memory and CPU caches optimally.\n38\n5.5. Convolution Methods\nGEMM convolutions also simplify the implementation of higher dimension and strided kernel convolutions, as only the code for reshaping the input has to be adapted. In Caffe, these functions are called im2col and col2im.\nA huge disadvantage with GEMM convolution is the memory requirement for the convolution buffer, discussed in Section 6.4. Assuming square sized kernels and output images, Equation 5.13 gives the buffer size in float elements. The kernel and output dimension is denoted by x. The network architectures in this report all use x = 2 (2D).\nMbuffer = fin \u00b7 kx \u00b7 wx = K \u00b7 N (5.13)\nA matrix multiplication consists of three matrices, A \u2208 RM\u00d7K, B \u2208 RK\u00d7N and C \u2208 RM\u00d7N . In Caffe, A is the weight matrix, B the column data after im2col and C = A \u00b7 B the layer output.\nThe dimensions are M = fout, N = wx and K = kx \u00b7 fin. The resulting computational complexity is O(M \u00b7 N \u00b7 (2 \u00b7 K\u2212 1)) = O( fout \u00b7 wx \u00b7 (2 \u00b7 fin \u00b7 kx \u2212 1)).\nIn the BLAS libraries, row-major NN-SGEMM has to be used. Row-major means the leading dimension in memory is the matrix row. NN means that the matrices A and B are both not transposed. SGEMM stands for single precision general matrix matrix multiplication.\nIn the scope of this project, FFT convolutions have not been considered. There were no suitable FFT libraries available for all devices that needed to be supported. Normal FFT convolution also uses a lot of additional memory, as all kernels are stretched to the size of the input in Fourier space. The device memory is typically insufficient (see Section 6.2) to store all FFT kernels for reuse, and recomputing every kernel is too time intensive.\nThe cuDNN library also implements a modified form of GEMM convolutions, and they also evaluated FFT and direct convolution as options [20]. A recent paper about using fast FFT convolutions exists (Vasilache et al. [21]), but managing the device memory remains difficult.\n39\nChapter 6\nBenchmarks"}, {"heading": "6.1 Introduction", "text": "This chapter assesses the performance (or efficiency) of different models across a variety of hardware devices. This is essential for speeding up neural networks by finding and eliminating computation and memory bottlenecks. All theoretical computations in this chapter assume two dimensional square sized compute kernels and feature maps. This is in accordance to how the network architectures are set up (see Chapter 3)."}, {"heading": "6.2 Hardware", "text": "The device specifications in Table 6.1 are taken from the official white papers (AMD [22], nVidia [23], Intel ARK [24]). The FLOP/s performances indicated assume the fused-multiply-add (FMA) operation.\nThe two AMD W9100 graphics cards have kindly been sponsored by AMD [25], as noted in the acknowledgments. The card has special features such as high 64 bit precision performance and error correcting memory. Those were not used for Caffe, only the OpenCL 2.0 driver and the large amount of memory was of importance. The W9100 is a workstation card, but consumer cards from both nVidia and AMD can also be used without restrictions as long as there is enough\n40\n6.3. Software\ndevice memory (R9 290X, R9 390X, Titan, Titan X). This is to be considered when building low-cost, high-throughput systems for neural networks.\nThe i7-4790K CPU and GTX 980 GPU are devices of my personal workstation, specially acquired to test with up to date hardware (as of 2015). Extended testing with the E5-2697v3 processors and its issues (see Section 6.7) was not possible as this device was a workstation that I could only access briefly during my time at HHMI Janelia.\nFor all benchmarks in this chapter, the W9100 GPUs have been used, because they were the only available GPUs capable of running all models in forward- and backward-mode on a wide range of output sizes, due to memory requirements. The only exceptions to this are the hardware comparison benchmarks and where indicated explicitly.\nUnless otherwise noted, Intel is used as an alias for the i7-4790K processor, nVidia for the GTX 980 GPU and AMD for the W9100 GPU in this chapter."}, {"heading": "6.3 Software", "text": "The modified version of Caffe [5] in Project Greentea supports a variety of configurations that perform differently depending on the compute device. Table 6.2 represents the setup used for benchmarking. It is the best performing setup possible for each combination of backend and device for the models in Chapter 3.\nOpenBLAS is compiled to use all CPU cores through OpenMP and supports all vector extensions available on the CPUs used. Alternatively, the cBLAS header interface also supports Intel MKL and ATLAS as replacements for OpenBLAS. The CPU could also be used with clBLAS. This is not advisable, as clBLAS is optimized for GPUs, which have a different memory architecture than CPUs. While there is a cache hierarchy on the CPU, the GPU needs to use fast local memory to buffer blocks of the matrix (from global device memory) temporarily. Local memory does not exist on the CPU, therefore it would result in copying data needlessly on the host memory. This results in low efficiency, because CPUs already have a slow memory interface compared to GPUs (see Table 6.1). As ViennaCL [19] was used as a part of the OpenCL backend, ViennaCL-BLAS is also available as an alternative to clBLAS. It is slower than clBLAS, but more\n41\n6.4. Device Memory\nconvenient to use as it does not need to be compiled separately and only consists of C++ header files."}, {"heading": "6.4 Device Memory", "text": "As all networks presented can be run with almost any size of output (up to restrictions given by layers such as even divisibility of the input feature maps, see Chapter 3), the networks can be set up so that they fit the memory and computational restrictions given by each device. The networks do not have to be re-trained in this case and results are numerically identical. A second objective may be to use processing output sizes matching the dataset: Non-square outputs such as 256 by 32 pixels are also possible. Then, the sizes can be set so that the image sizes to process are divisible by the output size of the network. Like that, no computations are wasted.\nIt is also important to note that there is no easy scaling rule as to how the memory requirements will change with different output sizes as this depends on the number of feature maps and their sizes on all layers as well as the maximum convolution buffer size (Mbuffer).\nAn upper bound estimation for w v is that the memory usage increases proportional to w2, where w is the output size and v the total padding size as in Table 6.3. It follows that the network gets more efficient with a bigger ratio wv , removing more overlapping computations. Using w = 1 on the SK network for example results in having the same efficiency as a minibatch sliding window (SW) network, which is about 50 times slower (see Table 6.5) than the corresponding SK network with 128 by 128 output.\nOn 3D networks, the memory allocation would scale in the order of w3, limiting the output patch-cube size very quickly.\nThe smallest memory available in the test hardware was 4 GiB (see Table 6.1), thus the networks have been set up as in Table 6.3 for forward processing. The configurations are the same as for training and as described in Chapter 3, except for the USK net, where half the size was used (256 instead of 512). The resulting memory requirements are visible in Figure 6.1 and Table 6.4.\nTraining the USK model with 512\u00d7 512 pixels, which improves training with Malis loss, requires up to 8 GiB of device memory, which is close to what scaling proportional to w2 predicts.\n42\nThe memory usage during training is always higher than the processing requirements. This is because the error / difference is stored during back propagation. The data blobs therefore have to be stored twice (data and difference) for all layers that are back propagated - making the difference between training and processing roughly the size of the data blobs, as can be seen in Figure 6.1.\nWith SK and USK models, data, difference and weights are the smallest contributors to memory usage. The biggest consumption comes from the temporary buffer needed to compute convolutions with matrix multiplications. The effect is much smaller with U-Net architectures. To assess this, the size of the buffer can be estimated:\nMbuffer = max l(i)\u2208Lconv.\nf (i)in (k (i))2(w(i))2 (6.1)\nThe size of Mbuffer is in floating point elements. To get an estimate in bytes, it has\n43\n6.4. Device Memory\nto be multiplied by the GPUs floating point precision size, which is typically 4 bytes.\nThe maximum buffer allocation that worked with OpenCL and CUDA was 4 GiB for a single memory block. This is the upper limit for Mbuffer. With the most expensive convolution in the ip1 layer of SK-Net (see Section 6.6.1), it is possible to compute the limits for the network output size w, according to Equation 6.1. fin = 192, fout = 1024 and k = 10 are fixed network parameters. Thus, the\nmaximum is w = \u221a\n4 GiB 4 B \u00b7 (192 \u00b7 102)\u22121 \u2248 236 pixels.\nFor U-Net, this buffer is not the dominant factor that limits the network. In the USK-Net design, the ip1 layer is of less importance and has less input feature maps and a smaller kernel size. This reduces the Mbuffer limitation and allows bigger output patches, making the network more efficient.\nWith the OpenCL backend (Section 5.4), the memory overhead is up to min(n, q) \u00b7 Mbuffer, where q is the number of parallel work queues and n the minibatch size. This helps to speed up the many small convolutions that occur in SW networks by starting up to q convolutions in parallel. This feature can not be used with the CUDA and CPU backend. For CUDA, the solution in this case is to use cuDNN, which streams the convolutions in batches to be more efficient [20]. Both parallel queues and cuDNN are not of importance here, because the SK, U and USK architectures all work efficiently with having n = 1.\nReusing the convolution buffer is also a new feature in the improved Caffe library [5]. With the original Caffe library [6], the memory overhead would have been much higher, allocating one buffer per convolution layer:\nMbuffer = \u2211 l(i)\u2208Lconv.\nf (i)in (k (i))2(w(i))2 (6.2)\nThose buffers are in both cases persistent, because freeing them and re-allocating would cause too much run time overhead. With re-using the buffer, it would also not decrease the peak allocation any further. Alternative ways to implement convolutions are discussed in Section 5.5.\nThe memory consumption can not be decreased further during training, because the data blobs have to persist during forward- and backward-computation to calculate the difference and update the weights in training. However, during processing, a lower bound estimate is given by:\nMtotal = min(n, q) \u00b7Mbuffer + n \u00b7 max b(i\u22121)\u2208B, b(i)\u2208B\n[ f (i)in (w (i\u22121))2 + f (i)out(w (i))2] (6.3)\nThis assumes that the blobs B of a network are not persistent and at most the input and output of the most memory consuming layer has to be stored. A network with its blobs can be seen as a directed acyclic graph (DAG). Therefore, the estimate is higher if there are layers / blobs that exist in parallel with each other. Currently (as of August 2015) there is no Caffe implementation that re-uses the blobs in this way. It would be necessary to analyze the DAG when instantiating the network first. Then, blobs would have to be allocated and assigned so that no\n44\n6.5. Labeling Throughput\nconflicts exist. The DAG would need to be split up for analysis in the case that there are multiple devices with independent memory working on different parts of the network.\nThe implication is that lower end devices such as GPUs with less memory and even mobile devices would be capable of classifying with larger networks such as those presented in this report. I chose to not implement this because there was enough GPU memory available and Figure 6.1 indicates the reduction would not be very large, as most memory is consumed by matrix-matrix multiplication buffer (Mbuffer)."}, {"heading": "6.5 Labeling Throughput", "text": "The labeling throughput (Table 6.5) is an overall performance measure for neural networks. It also shows how different devices and backends perform. Even on the CPU, using the fastest network (U) gives a speedup of 447\u00d7 compared to what was achievable with Caffe [6] prior to Project Greentea [5]. When using a network (SK) that gives identical results as the original SW network, the CPU speedup is still a factor of 48\u00d7.\nOn AMD GPUs, speedups of 54\u00d7 (SK-Net), 437\u00d7 (U-Net) and 320\u00d7 (USK-Net) compared to SW-Net are possible. The nVidia GPU scales similarly on both the OpenCL (SK: 33\u00d7, U: 459\u00d7, USK: 315\u00d7) and CUDA (SK: 57\u00d7, U: 711\u00d7, USK: 442\u00d7) backend.\nSK, USK and U networks can not be executed directly on the legacy CPU backend, as layers such as strided kernel layers and merge crop have no native CPU kernels implemented. They are only available on CUDA and OpenCL. The fact that the OpenCL backend on CPUs is better parallelized (see Section 5.4.2) than native CPU execution in Caffe and that the speedups between networks are similar on CPUs and GPUs justifies not implementing the CPU kernels.\nFigures 6.2 and 6.3 represent the values of Table 6.5 in both logarithmic and linear scale.\nWhile the CUDA backend is usually the fastest, the nVidia GPU performs slower, as expected by the FLOP values in Table 6.1, relative to the AMD GPU when using OpenCL on both of them. Explaining this behavior needs insight into the\n45\n6.5. Labeling Throughput\nindividual network layer performance (Section 6.6) and the convolution operations (Section 5.5)."}, {"heading": "SW SK USK U", "text": ""}, {"heading": "SW SK USK U", "text": "The network performance during training was not assessed in this report, as training the models on the available data was not a limiting objective. Back propagation\n46\n6.6. Layer Performance Analysis\nis usually slower than forwarding, as the differential maps, gradients and weight updates have to be computed."}, {"heading": "6.6 Layer Performance Analysis", "text": "This section takes apart the neural networks down to individual layers to assess why certain networks are faster than others and find potential to optimize models."}, {"heading": "6.6.1 SK-Net", "text": "Looking at Figure 6.5, the convolution layers and especially the ip1 layer (83 %) are responsible for the overall performance. There are many input (192) and output (1024) feature maps (see Section 3.3) and therefore a high computational complexity (O( fout \u00b7 w2 \u00b7 (2 \u00b7 fin \u00b7 k2 \u2212 1))) (see Section 5.5) on the ip1 layer.\nAs the convolution layers only consist of a fast memory copy operation to arrange the data, so that matrix-matrix multiplications through an optimized BLAS become possible, and the SGEMM (single precision general matrix-matrix multi-\n47\n6.6. Layer Performance Analysis\nplication) call itself, the efficiency values in Table 6.6 and Figure 6.6 are direct proxies for how efficient the BLAS works on the devices.\nLayers that operate with complexity O( fin \u00b7 w2), which includes all other layers used in the SK, USK and U networks, only contribute a small fraction to the total forwarding time (less than 1 % per layer).\nLayer GFLOP AMD OCL nV OCL nV CUDA Intel OCL conv1 0.70 3.83 % 8.79 % 13.84 % 6.16 % conv2 14.06 18.33 % 27.13 % 56.45 % 12.89 % conv3 18.40 36.46 % 22.05 % 21.78 % 18.14 % ip1 644.23 53.32 % 27.15 % 90.50 % 34.83 % ip2 17.17 32.72 % 23.83 % 62.15 % 22.35 % ip3 0.03 0.41 % 0.75 % 0.75 % 0.12 %\nLayer\nFigure 6.6: FLOP efficiency of the convolution layers in the SK-Net.\nEven though ip1 is the most expensive layer, it also is the most efficient (see Figure 6.6) on all devices, when using the BLAS which is most optimized. This means\n48\n6.6. Layer Performance Analysis\nclBLAS for AMD, cuBLAS for nVidia and OpenBLAS for Intel.\nIt is important to note that clBLAS is not yet as optimized as cuBLAS for this type of matrix-matrix multiplications (see Section 5.5). Both GPUs currently perform worse than expected with the OpenCL backend. As both clBLAS and the Project Greentea are still quite recent projects, the performance is expected to increase with further optimizations in the future.\nOther convolution layers are over 30 times less expensive and down to half as efficient. With small convolutions, the memory copy and kernel launch overhead lower the efficiency. Very small convolutions such as ip3 and conv1 may also not fully utilize the many threads available on GPUs (see Table 6.1). As the inefficient layers have short computation times here, they are not very important in optimization.\nOnly if the network mostly consists of such inefficient layers (which is not the case with SK-Net, but does apply to SW-Net), the number of feature maps and convolution sizes should be increased, if it improves the segmentation. Using minibatches (n > 1) can increase the GPU utilization on OpenCL when using multiple queues (q > 1) and the efficiency on CUDA when using cuDNN [20]."}, {"heading": "6.6.2 U-Net", "text": "49\nIn the U-Net, most convolutions have the same order of magnitude in complexity (see Table 6.7). The result is a very balanced network in terms of forward timings (see Figure 6.7). The balancing comes from trading feature map size against feature map count towards the middle of the network.\nThe upconvolution layers contribute each up to 5.2 % of the network forwarding time. This is less efficient than the optimum as only 4-nearest neighbor interpolation with constant weights is computed. This would not be more effort than a memory copy operation during forwarding and accumulating of the four nearest neighbor values during backward computation. Currently, it is implemented using a Caffe deconvolution, which is a reversed convolution layer. This layer already existed in Caffe, while no direct upsampling of feature maps is implemented. The convolution kernels are grouped, meaning each upsampling kernel only considers a single feature map. One advantage is that the deconvolution layer also allows adaptive weights and other interpolations such as bilinear upsampling.\n50\n6.6. Layer Performance Analysis"}, {"heading": "6.6.3 USK-Net", "text": "51\n6.6. Layer Performance Analysis\nLayer GFLOP AMD OCL nV OCL nV CUDA Intel OCL conv1 0.64 2.51 % 4.97 % 7.08 % 5.43 % conv2 13.75 21.55 % 19.25 % 42.11 % 8.04 % conv3 26.25 27.06 % 20.14 % 62.58 % 11.73 % conv4 21.81 26.99 % 21.78 % 62.61 % 12.57 % conv5 18.92 24.93 % 27.45 % 63.93 % 13.06 % ip1 141.76 45.93 % 31.25 % 86.64 % 27.82 % ip2 4.43 23.53 % 28.14 % 72.69 % 42.59 % conv6 4.42 23.56 % 21.29 % 63.39 % 31.44 % conv7 29.44 27.31 % 27.88 % 61.29 % 17.26 % conv8 9.66 18.44 % 20.45 % 46.17 % 7.34 % ip3 0.02 0.53 % 1.73 % 1.73 % 0.36 %\nLayer\nFigure 6.10: FLOP efficiency of the convolution layers in USK-Net.\nIn the USK network, the ip1 layer is again the most expensive and efficient one, but the network is more balanced as ip1 only contributes 32 % of the forwarding time, compared to 83 % in the original SK-Net. Efficiency and forwarding times are, as expected, a mixture of the SK- and U-Net.\n52\n6.7. NUMA Issues\nExcept for ip3 and conv1, all layers have a relatively high efficiency. The inefficient layers go from fin = 64 to fout = 2 and fin = 3 to fout = 64 respectively. This will directly result in matrix multiplications with matrices that have strongly nonsquare shapes and are therefore less efficient (see Section 5.5)."}, {"heading": "6.7 NUMA Issues", "text": "An issue that came up testing the OpenCL hybrid backend (see Section 5.4.2) was that the performance did not scale as expected with systems that have more than one CPU. Such systems have non-unified memory access (NUMA) because the CPUs share one address space for memory, but every processor has its own cache and memory interface. Accessing data across the other CPU comes with a large performance penalty. Compute kernels, such as the matrix-matrix multiplication in the BLAS library or the custom OpenCL kernels, cause the threads to work on adjacent data. This means a write operation of one CPU is likely to invalidate cache lines across both CPUs. At this point, the synchronization overhead seems to become larger than any speedup of having additional cores working on the algorithms.\nEach CPU has 14 cores, which gives 28 cores for the whole system. The number of threads for the frontend was set to 14, which gave the best performance by keeping at least the BLAS library temporarily tied to one processor by the operating system\u2019s scheduler. The OpenCL backend, which also allocates the memory, used 56 threads and allocated memory on both interfaces. Table 6.9 shows the impact of having NUMA issues, taking the SK network as\n53\n6.7. NUMA Issues\nan example. The layers are all sufficiently parallelized, which is evident when looking at Figure 6.6. Differences of the CPU architectures are also not a possible explanation as both processors are based on the same instruction sets and generation. The memory interface should also be fast enough to keep up with the computations (see Table 6.1).\nCorrecting for processor frequency, the speedup should be up to a factor 4.55\u00d7 using all 28 cores (ReLU and pooling layers) and up to 2.275\u00d7 using only one processor (inner product and convolution layers). The effective speedups measured are much slower:\n\u2022 Speedups between 0.1\u00d7 and 0.97\u00d7 on element-wise layers with complexity up to O( fin \u00b7 w2). Cache invalidation between the two processors (28 cores) seems to be dominant. The element-wise kernels run on the OpenCL backend.\n\u2022 Speedups between 0.47\u00d7 and 1.20\u00d7 on convolution layers with complexity up to O( fout \u00b7w2 \u00b7 (2 \u00b7 fin \u00b7 k2\u2212 1)) (matrix-matrix multiplication). The effects of OpenBLAS running on 14 cores and sub optimal memory allocations are dominant. It is a proxy for how the matrix-matrix multiplications used in all convolutions perform.\nTo get the expected speedup, the two processors need to be presented to the Caffe library as two separate devices. Then the library can be used in two individual instances. As the OpenCL hybrid backend uses two separate parallelization mechanisms (OpenCL kernels and a parallelized BLAS), two solutions would need to be applied:\n\u2022 The Caffe frontend needs to be tied to the cores of one CPU, so that the BLAS library does not show NUMA issues.\n\u2022 The OpenCL backend needs to split up the processor setup into sub-devices using device fission. The splitting rule needs to be that all cores belonging to one processor (tested by cache affinity) are tied to the same sub-device. Only one is then used per Caffe instance. Device fission is an extension to OpenCL that is already available (cl ext fission [26]).\n\u2022 The cores used in the frontend and selected sub-device need to be the same.\nDue to not having permanent access to a system with two processors and OpenCL installed, I did not have time to test out the solutions. Implementing the solutions remains as an open issue at the time of the project.\n54\n6.8. Alexnet"}, {"heading": "6.8 Alexnet", "text": "For comparison how the backends and devices perform on a widely used network for image classification that uses minibatches (with n = 10, w = 227, fin = 3) and multiple OpenCL queues (q = 8), the Alexnet [1] included in the Caffe library was also evaluated.\nThe CUDA backend has an advantage over the OpenCL backend in terms of speed, but is less versatile. The AMD GPU seems to be less efficient with minibatches and smaller matrix-matrix multiplications than the nVidia GPU, which is why the AMD GPU performs worse than the nVidia GPU on the same backend. With the SK-, USK- and U-Net (Section 6.5), the AMD GPU performs better using the same (OpenCL) backend on both GPUs.\nEspecially the backward computation is much slower (by a factor of three) using OpenCL instead of CUDA. The algorithms used are the same, therefore this difference is difficult to explain. Possibly, different optimizations need to be applied in the backward step of convolution layers, which can have a sequential bottleneck by adding up the gradients over the minibatch.\nUsing the OpenCL hybrid backend on the Intel CPU outperforms the (legacy) CPU backend by almost a factor of two. The speedup comes from parallelization of Greentea\u2019s OpenCL compute kernels, which are only single-threaded in the Caffe CPU backend. The BLAS library used for convolutions is multithreaded in both cases.\n55\nChapter 7\nResults"}, {"heading": "7.1 Introduction", "text": "In this chapter, the results of training the models from Chapter 3 are presented.\nThe evaluation is based on:\n\u2022 Two data sets (DS1 and DS2: see Chapter 2).\n\u2022 Three models (SK, U, USK: see Chapter 3).\n\u2022 Two training loss functions (Softmax and Malis).\n\u2022 One processing loss function (Softmax).\n\u2022 Three training configurations (Softmax, Malis and Softmax + Malis).\n\u2022 10\u2019000 training iterations per configuration, respectively 20\u2019000 for combined training.\n\u2022 Three error objectives (rand, warping and pixel error).\nThe amount of training iterations was chosen so that training of all 18 combinations was feasible during one week on two AMD W9100 GPUs, providing a total of 10 TFLOP/s [22]. The training data is not very large in both cases (see Chapter 2) and thus the loss always converged in under 10\u2019000 iterations for each training method. It is possible that some trainings did overfit as no early stopping was applied. Technically, the networks did not get to see the same amount of examples during training even though the iterations are the same, as the chosen output sizes of each network were set differently. However, due to gradient accumulation, different learning rates and weight initialization it is hard to estimate the effect of the amount of labels seen. As all networks converged to a stable loss, it should negligible."}, {"heading": "7.2 Analysis on DS1", "text": ""}, {"heading": "7.2.1 Training", "text": "The training parameters used on DS1 were set to not use the error masking functionality. Masking usually gives thicker membrane (background) labels when\n56\n7.2. Analysis on DS1\nused with Softmax by balancing out the amount of background error against foreground (cell interior) error.\nMalis loss was run without using a patch prior for preferring training patches based on their label histogram. Softmax loss on the other hand was used with the patch prior enabled. This is justified by the different characters of the loss functions: Softmax computes a per-pixel error while Malis gives errors at problematic pixels only, which can be very concentrated on a few pixels (see Section 5.3.2).\nIn the patch pre-processing step, the images were enhanced with CLAHE (contrast limited adaptive histogram equalization) and normalized in the range of [\u22121.0, 1.0].\nTo get more training data, the images were blurred with a randomly chosen 5 by 5 Gaussian kernel. The training patches were also rotated to multiples of 90\u25e6 and mirrored randomly (horizontal and vertical).\nDetails of the pre-processing and label priors are described in Chapter 4. The exact training parameters are stored as prototxt configuration in the Caffe Neural Models repository [9].\nInterestingly, it was not possible to start training of the SK network with Malis loss directly. The loss did not converge, and the output feature maps drifted to being classified as only foreground or only background. Therefore, the weights of the network in Malis only-training were initialized using 4000 iterations of Softmax training first. This was the lowest number of iterations where the training converged to a small loss afterwards. The other network architectures did not show such a behavior and trained well when starting with Malis directly. This is related to weight initialization as well as the fact that Malis works better on bigger training patch sizes. Training of SK was limited to 128 by 128 pixels output, which is much less context for Malis to work on than with USK and U with 512 by 512 and 388 by 388 pixels respectively."}, {"heading": "7.2.2 Numerical", "text": "57\n7.2. Analysis on DS1\nExplanations for the Tables 7.1 and 7.2:\n\u2022 Rank: The internal ranking, as indicated by the rand error.\n\u2022 Loss Function: Softmax indicates 10\u2019000 iterations with the Softmax loss function. Malis indicates 10\u2019000 iterations with the Malis loss function. When both are indicated, the training was executed with 10\u2019000 Softmax and then 10\u2019000 Malis training iterations.\n\u2022 Rand, warping and pixel: Error metrics, as proposed by Jain et al. [27] and used in the ISBI 2012 challenge [11]. A script for evaluation is available for Fiji [13] in the Caffe Neural Models repository [9].\nThe ranking on data set DS1 is as expected: Taking the average rank, USK-Net performs better than SK-Net, which is more precise than U-Net. The same goes for training methods: Using Softmax + Malis minimizes the rand error better than using Malis only, and Softmax outperforms Malis on pixel precision. On one hand, this is because the Malis criterion improves the rand error by penalizing merge and split errors. On the other hand, training with Malis will also decrease the pixel accuracy, which can be seen when inspecting the result visually. As Softmax training initializes the networks better than Malis, the best training method is to start with Softmax and then transit to Malis for fine tuning. Obviously, the USK network architecture performed best on both pixel accuracy and rand error. Only for the warping error, the much slower SK-Net (see Section 6.5) performs best."}, {"heading": "7.2.3 Visual", "text": "The visual analysis is based on the image number 2 of the DS1 stack (see Section 2.1). It includes a glia cell (Figure 7.1, label A) which is considered as background. The thickness of it makes it harder to label correctly, especially with Malis loss.\n58\n59\nExplanation to the labels in Figure 7.2:\n(A) Glia cell that should be considered background. Malis loss does focus on separating foreground objects, so this part gets only labeled correctly in the trainings that use Softmax only (a, b and c).\n(B) Diffuse membrane with light texture, which is hard to separate from cell interior. The separation is correct only in SK + Malis (d), while all other configurations lead to connected cells, although all trainings with Malis (d to i) show uncertainty, which could be sufficient to segment correctly. Diffuse parts are always on the edge of being pushed to foreground or background, which makes the segmentation fragile.\n(C) Membrane with close proximity to a mitochondrion. Except for U networks that have been trained with Softmax (b and h), this gets labeled correctly. The downsampling of U-Net is sub-optimal here when pixel error (without\n60\n7.3. Analysis on DS2\nscaling) instead of foreground separation is the training objective.\n(D) Same case as label C, but here the mitochondrion is labeled with high certainty by all networks, leading to mislabeling of the nearby membrane.\n(E) Removing an isolated mitochondrion with high certainty. The USK network (c and i) performed best on this task. Malis trainings are usually worse, again because Malis does not focus on pixel accuracy.\n(F) Mitochondrion with long, thin structure. It consequently leads to a wrong classification as membrane because of its shape. This is not an issue, as isolated misclassifications within a cell can be rejected on a higher level when reconstructing the connectome.\n(G) Mitochondrion close to the image border. As the training and classification uses border mirroring to make up for the missing context, it can lead to more errors near the border, independent of the network architecture.\n(H) Same case as label G. Softmax + Malis on USK-Net performed best on removing the mitochondria on E, F, G and H.\n(I) Diffuse membrane with a dark texture. It is expected that this gets completely classified as membrane. However, this was not the case on all networks and trainings. The worst case applies on U-Net (b and h), where the cells are almost connected.\nAs expected, membranes get labeled thinner with Malis as this error criterion is stopping to provide loss at membranes as soon as two cells are sufficiently separated (see Section 5.3.2). Even though the separation border is thin, training with Malis separates cells very well and scores best on both DS1 (Table 7.1) and DS2 (Table 7.2).\nThe visual results match the numerical evaluation. USK performs best on pixel error with Softmax and best on rand error with Softmax + Malis. However, when using Softmax, there is not a huge difference in pixel error and visual results between the networks. Malis does not work very well on SK networks (d), leaving a lot of uncertainty in the prediction. It can still be a good segmentation, as thresholding of gray scale values can be applied. The numeric evaluation [13] does test different thresholds, which is why the score is rather good (see Table 7.1) despite the uncertainty."}, {"heading": "7.3 Analysis on DS2", "text": ""}, {"heading": "7.3.1 Training", "text": "Training on DS2 was similar to training on DS1. The main difference is that the patch prior was not used on both Softmax and Malis. Instead, the error masking was enabled when using Softmax, which gives thicker borders. This is motivated by having input images which are slightly more blurred and thus the cell membranes are less sharp and harder to distinguish from cell interior.\nHere, the SK network already converged using 2000 training iterations of Softmax before switching over to Malis. Starting with Malis directly was also not possible.\n61\n7.3. Analysis on DS2"}, {"heading": "7.3.2 Numerical", "text": "Interestingly, the USK-Net with Softmax + Malis loss training performs unexpectedly worse on the dataset DS2 than on DS1, where it performed best. What is common for both DS1 and DS2 is that the USK network combined with Softmax training performs best on the pixel error. Overall, this data set has a ranking much harder to explain than on DS1. Visual inspection reveals that the USK-Net was often overconfident when labeling the cell interior, which connected cells that should be separated. On the DS1 data set, this did not happen."}, {"heading": "U 0.0382 0.000353 0.0611", "text": "Finally, the results of U-Net obtained by Ronneberger et al. [2] (Table 7.3) could not be reached, probably due to having used less transformations to extend the training data. It is not inherently clear if the U network would perform better than SK and USK given the bigger training data set.\nIdeas to improve training include:\n\u2022 Weight maps to scale the loss instead of only masking it.\n\u2022 Add elastic deformations, shifting and scaling instead of only rotation and mirroring to increase the amount of training data.\n\u2022 Experiment with different loss functions than Malis and Softmax, or alternate between them during training."}, {"heading": "7.3.3 Visual", "text": "The visual analysis is based on the image number 16 of the DS2 test stack (see Section 2.2).\n62\nExplanation to the labels in Figure 7.2:\nThe visual results of Softmax training (a to c) have thicker membrane labels than on DS1. This is because the error masking was enabled here and, as a result, the network has seen the same amount of error pixels for both foreground and background.\n(A) A bright spot, which is an error in the electron microscopy image. The membrane affected by it is misclassified by all networks and trainings. The local contrast enhancement with CLAHE did not help here.\n(B) Mitochondrion with close proximity parallel to a cell membrane. The USK network removes the membrane with all trainings (c, f and i). U-Net (b, e and h) performs a little better, but still merges the two cells. Only the SK network does it correctly (a, d and g), but has some uncertainty on the mitochondrion instead (d).\n(C) Diffuse section of a cell membrane. This is not an issue on most training/architecture combinations, except for U-Net with Softmax (b and h).\n(D) Oriented structures, even when faint, are partially labeled as membrane when sharp enough and of similar thickness as the membrane. This is no issue when isolated within the cell and not cutting through a cell that should be connected. With convolutional networks, this is hard to impossible to label correctly. It would require more high level knowledge of the object, such as if the predicted membrane is enclosing a cell or not.\n63\n64\n(E) Diffuse mitochondrion. The same situation as with B applies. Especially Uand USK-Net with Softmax training (b and c) get it wrong.\n(F) Diffuse cell interior that is similar to the membrane texture. All networks see a membrane connection through this area. In the training data there are some examples of diffuse membranes, so the networks have slightly overfitted on the training data for this case.\n65\nChapter 8\nConclusion"}, {"heading": "8.1 Research Time Line", "text": "An overview of the research time line, in order to give a context on what shaped the objectives and decisions made during the project:\nFrom To Activity / Event 05.11.2014 05.11.2014 Collaboration request at UZH INI. 09.11.2014 22.01.2014 Discussing ideas with Dr. Jan Funke. 14.12.2014 14.12.2014 Hongsheng Li et al. paper released (SK kernels) [7]. 07.02.2015 07.02.2015 Research proposal finished and accepted. 23.02.2015 23.02.2015 Research beginning. 26.02.2015 06.03.2015 Getting the sliding window network to work [10]. 08.03.2015 18.04.2015 OpenCL backend development [5]. 10.04.2015 21.04.2015 Discussing the project with AMD [25]. 22.04.2015 22.04.2015 Arrival of AMD\u2019s W9100 GPUs (hardware sponsoring). 19.04.2015 19.04.2015 Pull request of the modified Caffe to BVLC [4]. 09.05.2015 09.05.2015 Public release of the Caffe Neural Models [9]. 09.05.2015 09.05.2015 Public release of the Caffe Neural Tool [16]. 15.05.2015 15.05.2015 Ronneberger et al. paper released (U-Net) [2]. 20.05.2015 25.06.2015 Testing of U-Net and design of USK-Net. 27.06.2015 12.07.2015 Collaboration at HHMI Janelia, Virginia, USA [28]. 29.06.2015 14.07.2015 Implementing Malis loss and N-D SK kernels for Caffe. 12.07.2015 15.07.2015 Critical source code development finished. 13.07.2015 20.08.2015 Writing the report and final evaluation experiments. 24.08.2015 - Post-research support of Project Greentea and ongoing\ndevelopment in collaboration with AMD, HHMI Janelia and the Caffe community.\n66\n8.2. Implications"}, {"heading": "8.2 Implications", "text": "The first idea for the research project was to implement strided kernels. However, with the release of the Hongsheng Li et al. [7] paper, the problem already got solved. We got their source code and I was able to translate existing sliding window networks to strided kernel networks. These events lead to a shift of focus to implement the OpenCL backend and support a variety of hardware. This was important to see how existing CPU clusters and AMD GPUs could be used instead of only nVidia GPUs.\nA nice side effect of completely re-writing the whole Caffe library to OpenCL was gaining a complete understanding of the library, the bottlenecks, how all layers work and what the most important objectives for optimizations and network design are.\nIt turned out that running networks across devices does not give an advantage in the case of SK, U and USK architectures, as perfect scaling is possible when running independent instances of the network on each device. This only requires from the devices to have enough memory to hold the networks. This assumption was met when AMD\u2019s W9100 GPUs became available to me.\nSK networks did not scale as desired and up to 100\u2019000 pixel classifications per second were only about 1/10th of the desired speed. The original ideas to speed up the layers of the SK network by using methods such as multi device execution, Fourier transform convolutions or direct convolutions did not work. With the release of the Ronneberger et al. paper [2], the research focus was shifted to analyzing the U-Net approach, which is able to classify up to one megapixel per second. Training of U-Net was more difficult than SK-Net and thus I tried to implement my own network architecture based on the findings of SK-Net and U-Net, which resulted in the experimental USK-Net. The USK-Net performs similarly to U-Net and produced better results with small training data sets (see Chapter 7).\nLooking at ISBI 2012 results [11] and their test metrics, as well as the fact that one of the authors of the Malis criterion [3], Dr. Srinivas Turaga, was at HHMI Janelia for collaboration, lead to the development of an additional loss layer for Caffe (see Section 5.3.2).\nFinally, the last feature implemented before freezing the source code was N dimensional strided kernel support for max pooling and convolution layers, as this was a feature requested by Dr. Stephan Saalfeld and Dr. Srinivas Turaga at HHMI Janelia. This can be used to run modified SK, U and USK network architectures on 3D blocks of volumetric-isotropic data sets, or even 3D over time (4D)."}, {"heading": "8.3 Difficulties Encountered", "text": "The obvious difficulty was to keep up with the general research in pixelwise classification of images, as important papers [7], [2] were released during the project research. A shift of focus from the original plans were required a few times. This includes taking into account new results and dropping planned approaches.\n67\n8.4. Reproducibility of Results\nIt was also a lot of work to keep up with the changes of the Caffe library [4], as they changed many core aspects such as network file format and shape specifications for memory blobs. This broke compability with existing code from Honghsheng Li et al.\u2019s approach [7] as well as the existing sliding window network [10], [9]. Constantly pulling new changes from the BVLC master branch [6] and adapting my own branch to those changes was necessary. The benefit gained by doing this is that backwards compatibility to the official version is always guaranteed and that my own branch was ahead during the whole scope of the project.\nWith programming the Caffe Neural Tool, the diversity of formats for labels and input data was complex to handle. Especially loading and storing TIF pictures that can have a variety of pixel formats and support stacking multiple images in a single file can be tricky.\nAt last, it was not always obvious why a network does learn the expected features or not. Training parameter tests require up to ten hours of training on a GPU, which is very acceptable during production, but rather cumbersome during debugging. Evaluation and training of the networks for numeric results was only possible after freezing critical parts of the source code (computation kernels and layer implementations), because the results can differ after fixing bugs and other changes of the library. This resulted in having only two weeks left for this stage."}, {"heading": "8.4 Reproducibility of Results", "text": "The results obtained in this report are guaranteed to be reproducible by the use of the following software pipeline, using CUDA or OpenCL hardware equivalent to the hardware used in this report.\nRepositories belonging to Project Greentea:\n\u2022 Caffe [5]\nURL: https://github.com/naibaf7/caffe\nCommit checksum: f84c2a4fb8d633bc7d8fc9771eb06a3cf2215212\n\u2022 Caffe Neural Tool [16]\nURL: https://github.com/naibaf7/caffe_neural_tool\nCommit checksum: 780e7dd72e4f88c80729e2b33d6c0137d479016a\n\u2022 Caffe Neural Models [9]\nURL: https://github.com/naibaf7/caffe_neural_models\nCommit checksum: dbb06d8352aa9c2ba99458cb9e9068500ebacc11\nAs long as the ISBI 2012 challenge is ongoing, the data set DS2 results can be reproduced on their website [11]. The training and test data for the data set DS1 remains included in the Caffe Neural Models repository.\n68\n8.5. Outlook"}, {"heading": "8.5 Outlook", "text": "In this outlook I give a brief introduction on future plans for the improved Caffe version [5], extended use cases for the models [9], missing features for the Caffe Neural Tool [16] and ideas that did not fit into the time scope of the project."}, {"heading": "8.5.1 Device Abstracted Backend", "text": "Currently, the OpenCL, CUDA and (legacy) CPU backend are implemented sideby-side and there is quite some code duplication in the Caffe library. To support future multi-device training methods and remove redundancy, the backend should be further unified so that the only remaining code duplication resides with the actual compute kernels used inside the layers. This will minimize bugs that occur on only one backend and make software verification much easier to handle. It will also shorten the time required for newly developed layers to become available on all devices. At the time of the project, the improved Caffe library [5] drops full support on the legacy CPU backend in favor of an OpenCL hybrid solution (see Section 5.4.2) on CPUs. Some tuning to work correctly on NUMA processors (see Section 6.7) is still required. The CPU backend remains as a fallback for layers that do not work on OpenCL and CUDA, such as the Malis loss criterion (see Section 5.3.2)."}, {"heading": "8.5.2 Improving Training Data", "text": "As a first step to improve results, all network architectures should be evaluated using more training data, acquired artificially or from more ground truth. It is advisable to try out all models on a given data set, as there is no clear winner among the networks. Results may vary strongly as the numerical analysis revealed (see Section 7.2 versus 7.3)."}, {"heading": "8.5.3 Parameter Grid Search", "text": "The network architectures presented here are only examples of a whole class of possible networks. Many parameters such as kernel sizes and how SK networks are combined with U type networks can be evaluated. Especially deep multi-path networks can be useful, merging the feature maps of different architectures before making the ouput label predictions. The new USK architecture is such an example."}, {"heading": "8.5.4 Testing of Volumetric Architectures", "text": "Depending on the data set, SK, U and USK networks can be configured in many more ways for 3D than for 2D. For example, the depth direction is likely to have less physical resolution than the width and height dimension, due to how the data is acquired with slicing and electron microscopy. This should be considered when choosing the kernel size , kernel stride and span of the depth dimension. An example is the ISBI 2012 dataset which spans 2 x 2 x 1.5 microns with a resolution of 4 by 4 by 50 nm/pixel [14], [11], [15].\n69\n8.6. Final Words"}, {"heading": "8.5.5 Improving Test Metrics", "text": "Visual inspection and the error metrics used in this report can only give information about how accurate the label predictions are compared to the ground truth. For connectomics, this may not be the most important objective. The tools which will further process the segmentations may be able to correct certain errors in the predictions by testing how likely the final result is, while other merge and split errors lead to uncorrectable errors. This is an objective that could be more useful when selecting the network architecture, loss function and training method."}, {"heading": "8.6 Final Words", "text": "This research project combines many disciplines, such as high performance computing, machine learning, visual computing and a bit of connectomics. I was able to implement most of the originally planned features and found replacements for ideas that turned out to work badly. Finally, the results of the research include a useful, versatile stack of Open Source software (Project Greentea) that can be extended in the future. During working on the project, it was already possible to establish a growing user base [29]. The models and tools introduced with this project can be used efficiently on a large variety of data sets and hardware, making it very flexible. The collaboration at HHMI Janelia was also a great experience. Hardware sponsoring by AMD shows that programming of the Project Greentea and efficient machine learning libraries in general is of high interest also for hardware manufacturers.\n70"}, {"heading": "Appendix A", "text": "Network Architectures\n71\nA.1. SK-Net\nA.1 SK-Net\n(0,1)\npool2 (MAX PoolingSK) kernel size: 2 stride: 1 pad: 0 kstride: 2\npool2\nconv2 (ConvolutionSK) kernel size: 5 stride: 1 pad: 0 kstride: 2\nconv2\n128\ndata\nconv1 (ConvolutionSK) kernel size: 7 stride: 1 pad: 0 kstride: 1\nconv3 (ConvolutionSK) kernel size: 3 stride: 1 pad: 0 kstride: 4\nlabel (MemoryData)\nlabel\nlabeli\npool1\nrelu2 (ReLU)\npool1 (MAX PoolingSK) kernel size: 2 stride: 1 pad: 0 kstride: 1 relu1 (ReLU)\nconv1\nconv3\ndata (MemoryData)\ndatai\nsilence (Silence)\n48\n192\n72\nA.1. SK-Net\n(0,0)\nip1 (ConvolutionSK) kernel size: 10 stride: 1 pad: 0 kstride: 8\nip1\n1024\nprob_affinity\nloss (MalisLoss)\nprob (Softmax)\nprob\nip2 (ConvolutionSK) kernel size: 1 stride: 1 pad: 0 kstride: 1 relu4 (ReLU)\ncomponent\nip2\nrelu5 (ReLU)ip3 (ConvolutionSK) kernel size: 1 stride: 1 pad: 0 kstride: 1\nlabel_b\ncomponents (ConnectedComponent)affinity (Affinity)\nlabel_affinity\n512\npool3\nrelu3 (ReLU)pool3 (MAX PoolingSK) kernel size: 2 stride: 1 pad: 0 kstride: 4\nsplit (Split)\nlabel_a\nip3\n2\n73\nA.2. U-Net\nA.2 U-Net\n(0,5)\npool1\nconv3 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\nlabel (MemoryData)\nlabeli\nlabel\nconv1 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\nconv1\n64\nrelu2 (ReLU)\nconv2\ndata (MemoryData)\ndata datai\nrelu1 (ReLU)\nconv2 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\nsilence (Silence)\npool1 (MAX Pooling)\nkernel size: 2 stride: 2 pad: 0\nkstride: 1\nconv3\n128\nconv4 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\nrelu3 (ReLU)\n64\n74\nA.2. U-Net\n(0,4)\npool2\nconv5 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\nrelu7 (ReLU)\nconv7\nconv6\npool3 (MAX Pooling)\nkernel size: 2 stride: 2 pad: 0\nkstride: 1\nrelu6 (ReLU)\npool3\nconv8 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\n512\nconv5\n256\npool2 (MAX Pooling)\nkernel size: 2 stride: 2 pad: 0\nkstride: 1\nconv6 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\n256\nconv7 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\n512\nconv4\n128\nrelu4 (ReLU)\nrelu5 (ReLU)\n75\nA.2. U-Net\n(0,3)\nconv9 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\nconv9\n1024\npool4 (MAX Pooling)\nkernel size: 2 stride: 2 pad: 0\nkstride: 1\npool4\nmergecrop1\nconv12 (Convolution) kernel size: 3\nstride: 1 pad: 0\nupconv1\nconv11 (Convolution) kernel size: 1\nstride: 1 pad: 0 kstride: 1\nmergecrop1 (MergeCrop)\nconv10\nrelu10 (ReLU) upconv1 (Deconvolution)\nconv10 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\n1024\nconv8\n512\nrelu9 (ReLU)\nrelu8 (ReLU)\nconv11\n512\n76\nA.2. U-Net\n(0,2)\nrelu13 (ReLU)\nconv15\npad: 0 kstride: 1\nupconv2\nconv14 (Convolution) kernel size: 1\nstride: 1 pad: 0 kstride: 1\nconv12\nrelu11 (ReLU)\nconv13 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\nmergecrop2\nconv15 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\n512\nconv13\nrelu12 (ReLU)\nupconv2 (Deconvolution)\nconv16\nmergecrop2 (MergeCrop)\nconv16 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\n256\n512\n256\nconv14\n256\n77\nA.2. U-Net\n(0,1)\nconv17 (Convolution) kernel size: 1\nstride: 1 pad: 0 kstride: 1\nconv17\n128\nconv19 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\nconv19\n128\nrelu14 (ReLU)\nmergecrop3 (MergeCrop)\nupconv4 (Deconvolution)\nrelu16 (ReLU)\nupconv4\nupconv3 (Deconvolution)\nconv20 (Convolution) kernel size: 1\nstride: 1 pad: 0 kstride: 1\nconv20\n64\nconv18\nrelu15 (ReLU)\nmergecrop3\nconv18 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\nupconv3\n128\n78\nA.2. U-Net\n(0,0)\nsplit (Split)\nlabel_blabel_a\nconv21\nconv22 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\nrelu17 (ReLU)\nconv22\nip1 (Convolution) kernel size: 1\nstride: 1 pad: 0 kstride: 1\nrelu18 (ReLU)\nlabel_affinity\nloss (MalisLoss)\naffinity (Affinity)\nprob_affinity\nip1\nprob (Softmax)\n64\ncomponents (ConnectedComponent)\nprob\ncomponent\n2\nconv21 (Convolution) kernel size: 3\nstride: 1 pad: 0 kstride: 1\n64\nmergecrop4\nmergecrop4 (MergeCrop)\n79\nA.3. USK-Net\nA.3 USK-Net\n(0,4)\ndata\nconv1 (Convolution) kernel size: 3\nstride: 1 pad: 0\nkstride: 1\nconv1\n64\nrelu2 (ReLU)\nconv2\nrelu1 (ReLU)\ndata (MemoryData)\nconv2 (Convolution) kernel size: 3\nstride: 1 pad: 0\nkstride: 1\npool1 (MAX Pooling)\nkernel size: 2 stride: 2 pad: 0\nkstride: 1\n64\n80\nA.3. USK-Net\n(0,3)\npool2 (MAX PoolingSK)\nkernel size: 2 stride: 1 pad: 0\nkstride: 1\npool2\nconv4\nrelu4 (ReLU)\npool3 (MAX PoolingSK)\nkernel size: 2 stride: 1 pad: 0\nkstride: 2\nconv4 (ConvolutionSK)\nkernel size: 4 stride: 1 pad: 0\nkstride: 2\npool1\nconv3 (ConvolutionSK)\nkernel size: 6 stride: 1 pad: 0\nkstride: 1\nconv3\nrelu3 (ReLU)\n128\n128\n81\nA.3. USK-Net\n(0,2)\nip1 (ConvolutionSK)\nkernel size: 8 stride: 1 pad: 0\nkstride: 8\nip1\n512\nconv5 (ConvolutionSK)\nkernel size: 4 stride: 1 pad: 0\nkstride: 4\nconv5\n128\nrelu5 (ReLU)\nip2 (Convolution) kernel size: 1\nstride: 1 pad: 0\nkstride: 1\n256\npool4\npool3\npool4 (MAX PoolingSK)\nkernel size: 2 stride: 1 pad: 0\nkstride: 4\nrelu6 (ReLU)\n82\nA.3. USK-Net\n(0,1) relu8 (ReLU)\nconv7\nip2\nupconv1 (Deconvolution)\nrelu7 (ReLU)\nmergecrop1\nconv7 (Convolution) kernel size: 3\nstride: 1 pad: 0\nkstride: 1\nconv6\nmergecrop1 (MergeCrop)\nupconv1\nconv6 (Convolution) kernel size: 1\nstride: 1 pad: 0\nkstride: 1\n128\nconv8 (Convolution) kernel size: 3\nstride: 1\n128\n83\nA.3. USK-Net\n(0,0)\n(ReLU)\nprob_affinity\nloss (MalisLoss)\nprob\naffinity (Affinity)\nlabel (MemoryData)\nlabel labeli\ncomponent\nprob (Softmax)\nlabel_b\ncomponents (ConnectedComponent)\nlabel_affinity\nip3 (Convolution) kernel size: 1\nstride: 1 pad: 0\nkstride: 1\nip3\n2\nstride: 1 pad: 0\nkstride: 1\nconv8\n64\nsplit (Split)\nlabel_a\nrelu9 (ReLU)\ndatai\nsilence (Silence)\n84"}], "references": [{"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": "ArXiv e-prints (May 2015)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Maximin affinity learning of image segmentation", "author": ["S.C. Turaga"], "venue": "ArXiv e-prints (Nov", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Yangqing Jia"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification", "author": ["H. Li", "R. Zhao", "X. Wang"], "venue": "ArXiv e-prints (Dec", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Segmented anisotropic ssTEM dataset of neural tissue", "author": ["Stephan Gerhard"], "venue": "url: http://dx.doi.org/10.6084/m9.figshare.856713 (visited on 20th Aug", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Sliding Window Network", "author": ["Julien Martel"], "venue": "url: https://www.ini.uzh.ch/ people/jmartel (visited on 20th Aug", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Caffe ND convolutions. url: https", "author": ["Jeff Donahue"], "venue": "BVLC / caffe/pull/2049 (visited on 20th Aug. 2015)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "An Integrated Micro- and Macroarchitectural Analysis of the Drosophila Brain by Computer-Assisted Serial Section Electron Microscopy", "author": ["Albert Cardona"], "venue": "PLoS Biology", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Malis criterion for Matlab. url: https", "author": ["Srinivas Turaga"], "venue": "github . com / srinituraga/malis/ (visited on 20th Aug", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Malis criterion for Torch", "author": ["Srinivas Turaga"], "venue": "url: https://github.com/srinituraga/ lua---imgraph/ (visited on 20th Aug", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "ViennaCL - A High Level Linear Algebra Library for GPUs and Multi-Core CPUs", "author": ["K. Rupp", "F. Rudolf", "J. Weinbub"], "venue": "In: Intl. Workshop on GPUs and Scientific Applications", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "cuDNN: Efficient Primitives for Deep Learning", "author": ["S. Chetlur"], "venue": "ArXiv e-prints (Oct", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation", "author": ["Nicolas Vasilache"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Boundary Learning by Optimization with Topological Constraints", "author": ["Viren Jain"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Institute of Electrical & Electronics Engineers (IEEE),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Caffe Pull Request", "author": ["Fabian Tschopp"], "venue": "url: https://github.com/BVLC/caffe/ pull/2610 (visited on 20th Aug", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A prominent example is the ImageNet / AlexNet [1] for object recognition.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "Recent networks [2] can have very many, in this case over 20, layers and millions of learnable parameters.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Therefore, this research also considers different training methods including the Malis [3] criterion.", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "Caffe stands for Convolutional Architecture for Fast Feature Embedding [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "[7] allows to make existing SW networks more efficient while giving identical prediction results (see Section 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "(Raw image source: ssTEM [8], [9]).", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "(Raw image source: ssTEM [8], [9]).", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "This technical report is based on the following existing work: \u2022 SW (sliding window) network designed by Julien Martel [10], not published.", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "\u2022 The Open Source Caffe library maintained by the Berkeley Vision and Learning Center [6], [4].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "\u2022 N-dimensional convolution kernels by Jeff Donahue [12].", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "[2] network architecture, see Section 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "1: DS1 ssTEM raw image, 512 by 512 pixels of image 2 (right upper corner) (Source: ssTEM [8], [9]).", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "This data set shows neural tissue from a Drosophila larva ventral nerve cord and was acquired using serial section Transmission Electron Microscopy at HHMI Janelia Research Campus [8].", "startOffset": 180, "endOffset": 183}, {"referenceID": 5, "context": "1 (Source: ssTEM [8], [9]).", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "This data set is from the ISBI 2012 challenge [11], [14], [15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "The sliding window network described here was developed by Julien Martel [10].", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] provide a pseudo code (page 4) on how to convert a sliding window network to a strided kernel network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "paper [2].", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "The configurations of this U-Net included in the Caffe Neural Models [9] are therefore incompatible to the original work [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "This is the same as used in the original paper [2].", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "3, p \u2208 [0, 1] is a random value picked at uniform and ci \u2208 [0, 1] the label frequency for label i = 0, .", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "3, p \u2208 [0, 1] is a random value picked at uniform and ci \u2208 [0, 1] the label frequency for label i = 0, .", "startOffset": 59, "endOffset": 65}, {"referenceID": 4, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "There were existing kernels for normal N-dimensional convolutions by Jeff Donahue [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 9, "context": "The implementation in Caffe [5] that I provide is based on existing code to compute the Malis criterion for Matlab [17] and Torch [18] by Srinivas Turaga et al.", "startOffset": 115, "endOffset": 119}, {"referenceID": 10, "context": "The implementation in Caffe [5] that I provide is based on existing code to compute the Malis criterion for Matlab [17] and Torch [18] by Srinivas Turaga et al.", "startOffset": 130, "endOffset": 134}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "1 Implementation In my version of the Caffe library, an additional versatile backend for various compute devices, based on OpenCL and ViennaCL [19], is available.", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "The cuDNN library also implements a modified form of GEMM convolutions, and they also evaluated FFT and direct convolution as options [20].", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "[21]), but managing the device memory remains difficult.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "As ViennaCL [19] was used as a part of the OpenCL backend, ViennaCL-BLAS is also available as an alternative to clBLAS.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "For CUDA, the solution in this case is to use cuDNN, which streams the convolutions in batches to be more efficient [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "Using minibatches (n > 1) can increase the GPU utilization on OpenCL when using multiple queues (q > 1) and the efficiency on CUDA when using cuDNN [20].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "For comparison how the backends and devices perform on a widely used network for image classification that uses minibatches (with n = 10, w = 227, fin = 3) and multiple OpenCL queues (q = 8), the Alexnet [1] included in the Caffe library was also evaluated.", "startOffset": 204, "endOffset": 207}, {"referenceID": 14, "context": "[27] and used in the ISBI 2012 challenge [11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "1: DS1 ssTEM raw and corresponding ground truth, 1024 by 1024 pixels (Source: ssTEM [8], [9]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "[2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] (Table 7.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "paper released (SK kernels) [7].", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "2015 Getting the sliding window network to work [10].", "startOffset": 48, "endOffset": 52}, {"referenceID": 3, "context": "2015 Pull request of the modified Caffe to BVLC [4].", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "paper released (U-Net) [2].", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "[7] paper, the problem already got solved.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "paper [2], the research focus was shifted to analyzing the U-Net approach, which is able to classify up to one megapixel per second.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "Looking at ISBI 2012 results [11] and their test metrics, as well as the fact that one of the authors of the Malis criterion [3], Dr.", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "The obvious difficulty was to keep up with the general research in pixelwise classification of images, as important papers [7], [2] were released during the project research.", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "The obvious difficulty was to keep up with the general research in pixelwise classification of images, as important papers [7], [2] were released during the project research.", "startOffset": 128, "endOffset": 131}, {"referenceID": 3, "context": "It was also a lot of work to keep up with the changes of the Caffe library [4], as they changed many core aspects such as network file format and shape specifications for memory blobs.", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "\u2019s approach [7] as well as the existing sliding window network [10], [9].", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "\u2019s approach [7] as well as the existing sliding window network [10], [9].", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "5 microns with a resolution of 4 by 4 by 50 nm/pixel [14], [11], [15].", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "During working on the project, it was already possible to establish a growing user base [29].", "startOffset": 88, "endOffset": 92}], "year": 2015, "abstractText": "This work presents and analyzes three convolutional neural network (CNN) models for efficient pixelwise classification of images. When using convolutional neural networks to classify single pixels in patches of a whole image, a lot of redundant computations are carried out when using sliding window networks. This set of new architectures solve this issue by either removing redundant computations or using fully convolutional architectures that inherently predict many pixels at once. The implementations of the three models are accessible through a new utility on top of the Caffe library. The utility provides support for a wide range of image input and output formats, pre-processing parameters and methods to equalize the label histogram during training. The Caffe library has been extended by new layers and a new backend for availability on a wider range of hardware such as CPUs and GPUs through OpenCL. On AMD GPUs, speedups of 54\u00d7 (SK-Net), 437\u00d7 (U-Net) and 320\u00d7 (USKNet) have been observed, taking the SK equivalent SW (sliding window) network as the baseline. The label throughput is up to one megapixel per second. The analyzed neural networks have distinctive characteristics that apply during training or processing, and not every data set is suitable to every architecture. The quality of the predictions is assessed on two neural tissue data sets, of which one is the ISBI 2012 challenge data set. Two different loss functions, Malis loss and Softmax loss, were used during training. The whole pipeline, consisting of models, interface and modified Caffe library, is available as Open Source software under the working title Project Greentea.", "creator": "LaTeX with hyperref package"}}}