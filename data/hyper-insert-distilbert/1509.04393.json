{"id": "1509.04393", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2015", "title": "Dependency length minimization: Puzzles and Promises", "abstract": "in approaching the recent issue of pnas, futrell et b al. confidently claims that their subsequent study of 37 languages gives the single first large scale cross - genetic language evidence for comparative dependency mark length dependent minimization, which is notably an overstatement study that ignores similar previous researches. in addition, this study seems to pay no attention to factors structured like making the primary uniformity indicator of verbal genres, which weakens the validity of whenever the argument suggests that dlm programming is universal. maybe another prominent problem is that assuming this study entirely sets the prospective baseline random language tendency as projective, something which fails too to generally truly uncover the difference between natural language and random response language, since paradigm projectivity is presumably an important feature of computational many computational natural languages languages. the finally, later the paper contends instead an \" apparent relationship between head finality preference and basal dependency gap length \" despite citing the lack of any an explicit empirical statistical comparison, nor which otherwise renders at this conclusion rather feels hasty and improper.", "histories": [["v1", "Tue, 15 Sep 2015 04:29:50 GMT  (334kb)", "http://arxiv.org/abs/1509.04393v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haitao liu", "chunshan xu", "junying liang"], "accepted": false, "id": "1509.04393"}, "pdf": {"name": "1509.04393.pdf", "metadata": {"source": "CRF", "title": "Dependency length minimization: Puzzles and Promises", "authors": ["Haitao Liua", "Chunshan Xua", "Junying Lianga"], "emails": ["jyleung@iipc.zju.edu.cn."], "sections": [{"heading": null, "text": "1 / 4\nIn the recent issue of PNAS, Futrell et al. claims that their study of 37 languages gives the first large scale cross-language evidence for Dependency Length Minimization, which is an overstatement that ignores similar previous researches. In addition\uff0cthis study seems to pay no attention to factors like the uniformity of genres, which weakens the validity of the argument that DLM is universal. Another problem is that this study sets the baseline random language as projective, which fails to truly uncover the difference between natural language and random language, since projectivity is an important feature of many natural languages. Finally, the paper contends an \u201capparent relationship between head finality and dependency length\u201d despite the lack of an explicit statistical comparison, which renders this conclusion rather hasty and improper.\nKey words: dependency length minimization, cross-language, projectivity,\nFor decades, dependency length (distance) minimization has been\npursued as a universal underlying force shaping human languages. In a recent issue of PNAS, Futrell, et al. (2015) suggest that dependency length minimization (DLM) is a universal property of human languages and hence supports explanations of linguistic variation in terms of general properties of human information processing. However, this statement is much exaggerated and far-fetched .\nFirst of all, it is claimed in the paper that this is the first large scale\ncross-language evidence for DLM, since \u201cprevious comprehensive corpus-based studies of DLM cover seven languages in total\u201d. However, this is absolutely NOT true. In fact, there have been some large scale cross-language studies of DLM. For example, Liu (2008) has compared dependency distance of 20 natural languages with that of two different random languages, and pointed out that dependency distance minimization is probably universal in human languages. Evidently, the two articles share the\n1 To whom correspondence should be addressed. E-mail: jyleung@iipc.zju.edu.cn.\n2 / 4\nThere are some minor differences in the specific methods used in these\ntwo works. For example, Furtell et al (2015) hold dependency relations constant and draw random word order, while Liu (2008) held word order constant and drew random dependency relations. But such minor differences cannot deny the fact that the two works adopt similar research methods: both are based on the comparison between the dependency length (distance) of natural languages and that of corresponding artificial random languages. This method has also been used in an earlier study of two languages (Ferrer-i-Cancho 2004). The above difference in methods has no significant influence on the results of research, since it merely reflects the different ways to construct random languages in which the distribution of dependency length is randomized. Of course, it is perfectly acceptable and even encouraging for researchers to test previous findings with somewhat different methods. Anyway, any scientific finding must be subject to repeated tests. However, as far as this PNAS paper is concerned, we are much curious and puzzled why and how the authors could cite the work of Ferrer-i-Cancho and Liu (2014), which clearly introduces and largely dwells on previous DLM study based on 20 languages, but still claim that their PNAS paper is the first large scale cross-language evidence for DLM, and that \u201cprevious comprehensive corpus-based studies of DLM cover seven languages in total\u201d.\nWhat is more, dependency length is sensitive to many factors. Linguistic\nproperties, say DLM, may feature in one genre of language, but become vague and weak in another. Therefore, it is more desirable, especially in cross-language studies, to use a parallel corpus, or at least, corpora with the same genres, annotated with similar syntactic annotation schemes or drawn from native dependency treebanks (Jiang and Liu 2015). In the present study, however, it is not clear whether these conditions are satisfied by judging from the materials and methods, and hence there is some doubt in the validity of the argument that DLM is universal in all these languages.\nAs recently suggested, DLM bears closely on the rarity of crossing\ndependencies (Ferrer-i-Cancho 2013), and the authors also mention projectivity as one pervasive property of word order that can explain (or be explained by) DLM. What puzzles is that the baseline word order is set as projective. If projectivity is one feature of human language that contributes to DLM, it is desirable for a study of DLM to set baseline word order as non-projective so as to reveal the influence of projectivity on human languages in general. Projective baseline word order in this article fails to reveal the role of projectivity in DLM. In comparison, two baseline word orders respectively set as non-projective and projective may well throw much more light on DLM in natural languages, which has been adopted in previous works (Liu 2007, 2008). Also directly related to DLM is the distribution of dependency distance or the proportion of adjacent dependencies (AD) in\n3 / 4\n2008), that the frequency of dependency drops dramatically with the increase of length (distance)(Liu 2007), and that a distribution of dependency distance is not influenced by variation in sentence length(Jiang and Liu 2015). These findings explain why DLM is persistently found in human languages and hence should have been mentioned in this article.\nIn the concluding part, the authors contend that an \u201capparent relationship\nbetween head finality and dependency length is a new and unexpected discovery\u201d. Nevertheless, it seems not apparent enough that dependency length is directly related to head-dependent order: no explicit statistical comparison is made in the present paper. Hence, the conclusion seems rather hasty, lacking solid supporting data. Theoretically, SVO order is in favor of DLM, as has been mathematically proven by Ferrer-i-Cancho (2015). But language is complex, constrained by multiple factors whose interactions may lead to no significant distance difference between VO and OV languages. In fact, existent corpus-based researches point to no definite relations between head placement and dependency distance. Gildea and Temperley (2010) find that German, as an OV language, has longer dependency distance than English, a VO language, but Hiranuma (1999) finds no difference between English and Japanese, which is an OV language, while Liu (2008) finds that Chinese, which is a VO language, has the longest mean dependency distance in all the languages that have been investigated. More importantly, another study (Liu and Xu 2012) that has quantitatively investigated 15 different languages clearly suggests no correlation between dependency distance and head placement. These findings indicate that, for complex systems like language, it is too casual to draw a relation between them based on one single study.\nTaken together, Futrell et al. intend to address the dependency length\nminimization as a universal quantitative property of human languages. However they do overstate the significance of their study: it is definitely not the first large scale evidence of DLM, but a repetition of some previous works, though with slightly different methods. Further, they do not include adequate non-cognitive factors in mind. Finally, this paper is impaired by a lack of systematic review and references to related studies mentioned above in particular and dependency grammar in general (Hudson 2010), and due to this lack, it is legitimate to question the originality of this study because it is largely dissociated and disconnected from previous findings.\nFutrell et al. have potentially displayed an intriguing domain for\nlarge-scale cross-linguistic research on dependency distance. However, the methodology itself is basically a repetitive effort of previous studies, and the data presented are not sufficient enough to support the conclusions made in this paper. This work uses more languages than previous studies\u2014\u2014 probably thanks to the fact that much more dependency treebanks are\n4 / 4"}], "references": [{"title": "Euclidean distance between syntactically linked words", "author": ["R. Ferrer-i-Cancho"], "venue": "Physical review E,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Hubiness, length and crossings and their relationships in dependency trees", "author": ["R Ferrer-i-Cancho"], "venue": "Glottometrics", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "The placement of the head that minimizes online memory: a complex systems approach", "author": ["R Ferrer-i-Cancho"], "venue": "Language Dynamics and Change", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "The risks of mixing dependency lengths from sequences of different", "author": ["R. Ferrer-i-Cancho", "H. Liu"], "venue": "length. Glottotheory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Large-scale evidence of dependency length minimization in 37 languages. PNAS", "author": ["R Futrell", "K Mahowald", "E Gibson"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Do Grammars Minimize Dependency Length", "author": ["D Gildea", "D Temperley"], "venue": "Cogn. Sci", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Syntactic difficulty in English and Japanese: a textual study", "author": ["S Hiranuma"], "venue": "UCL Work. Papers Linguist", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "An Introduction to Word Grammar", "author": ["R Hudson"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "The effects of sentence length on dependency distance, dependency direction and the implications\u2013Based on a parallel English\u2013Chinese dependency Treebank", "author": ["JY Jiang", "HT Liu"], "venue": "Lang. Sci", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Probability distribution of dependency distance", "author": ["HT Liu"], "venue": "Glottometrics", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Dependency distance as a metric of language comprehension difficulty", "author": ["HT Liu"], "venue": "J. Cognitvve Science", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Quantitative typological analysis of Romance languages", "author": ["HT Liu", "CS Xu"], "venue": "Poznan Stud. Contemp. Linguist", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}], "referenceMentions": [], "year": 2015, "abstractText": "In the recent issue of PNAS, Futrell et al. claims that their study of 37 languages gives the first large scale cross-language evidence for Dependency Length Minimization, which is an overstatement that ignores similar previous researches. In addition,this study seems to pay no attention to factors like the uniformity of genres, which weakens the validity of the argument that DLM is universal. Another problem is that this study sets the baseline random language as projective, which fails to truly uncover the difference between natural language and random language, since projectivity is an important feature of many natural languages. Finally, the paper contends an \u201capparent relationship between head finality and dependency length\u201d despite the lack of an explicit statistical comparison, which renders this conclusion rather hasty and improper.", "creator": "Microsoft\u00ae Word 2010"}}}