{"id": "1509.05490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2015", "title": "TransA: An Adaptive Approach for Knowledge Graph Embedding", "abstract": "knowledge representation is indeed a major central topic, in web ai, and presently many studies attempt to represent separate entities and relations of available knowledge available base configurations in forming a previously continuous adaptive vector neural space. among these early attempts, many translation - based constraint methods build entity inputs and relation vectors by minimizing the translation loss gradient from a linear head entity to reveal a circular tail length one. in spite of today the extraordinary success of these methods, many translation - coordinate based computation methods also suffer further from the oversimplified loss momentum metric, and approach are not now competitive enough frequently to readily model various simple and complex entities / relations presented in knowledge resources bases. ultimately to address this issue, therefore we propose \\ textbf { flow transa }, an explicitly adaptive space metric approach for embedding, utilizing the metric learning ideas technology to provide as a simplified more flexible tensor embedding method. experiments are conducted elsewhere on the benchmark datasets and under our proposed method makes quite significant and consistent improvements over at the state - of - the - micro art baselines.", "histories": [["v1", "Fri, 18 Sep 2015 02:40:07 GMT  (365kb)", "http://arxiv.org/abs/1509.05490v1", null], ["v2", "Mon, 28 Sep 2015 02:21:20 GMT  (158kb,D)", "http://arxiv.org/abs/1509.05490v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["han xiao", "minlie huang", "yu hao", "xiaoyan zhu"], "accepted": false, "id": "1509.05490"}, "pdf": {"name": "1509.05490.pdf", "metadata": {"source": "CRF", "title": "TransA: An Adaptive Approach for Knowledge Graph Embedding", "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n50 9.\n05 49\n0v 1\n[ cs\n.C L\n] 1\n8 Se\np 20"}, {"heading": "Introduction", "text": "Knowledge graphs such as Wordnet (Miller 1995) and Freebase (Bollacker et al. 2008) play an important role in AI researches and applications. Recent researches such as query expansion prefer involving knowledge graphs (Bao et al. 2014) while some industrial applications such as question answering robots are also powered by knowledge graphs (Fader, Zettlemoyer, and Etzioni 2014). However, knowledge graphs are symbolic and logical, where numerical machine learning methods could hardly be applied. This disadvantage is one of the most important challenges for the usage of knowledge graph. To provide a general paradigm to support computing on knowledge graph, various knowledge graph embedding methods have been proposed, such as TransE (Bordes et al. 2013), TransH (Wang et al. 2014) and TransR (Lin et al. 2015).\nEmbedding is a novel approach to address the representation and reasoning problem for knowledge graph. It transforms entities and relations into continuous vector spaces, where knowledge graph completion and knowledge classification can be done. Most commonly, knowledge graph is composed by triples (h, r, t) where a head entity h, a relation r and a tail entity t are presented. Among all the proposed embedding approaches, geometry-based methods are an important branch, yielding the state-of-the-art predictive\nCopyright c\u00a9 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nFigure 1: Visualization of TransE embedding vectors for Freebase with PCA dimension reduction. The navy crosses are the matched tail entities for an actor\u2019s award nominee, while the red circles are the unmatched ones. TransE applies Euclidean metric and spherical equipotential surfaces, so it must make seven mistakes as (a) shows. Whilst TransA takes advantage of adaptive Mahalanobis metric and elliptical equipotential surfaces in (b), four mistakes are avoided.\nperformance. More specifically, geometry-based embedding methods represent an entity or a relation as k-dimensional vector, then define a score function fr(h, t) to measure the plausibility of a triple (h, r, t). Such approaches almost follow the same geometric principle h+ r \u2248 t and apply the same loss metric ||h+ r\u2212 t||22 but differ in the relation space where a head entity h connects to a tail entity t.\nHowever, the loss metric in translation-based models is oversimplified. This flaw makes the current embedding methods incompetent to model various and complex entities/relations in knowledge base.\nFirstly, due to the inflexibility of loss metric, current translation-based methods apply spherical equipotential hyper-surfaces with different plausibilities, where more near to the centre, more plausible the triple is. As illustrated in Fig.1, spherical equipotential hyper-surfaces are applied in (a), so it is difficult to identify the matched tail entities from the unmatched ones. As a common sense in knowledge graph, complex relations, such as one-to-many, many-to-one and many-to-many relations, always lead to complex embedding topologies. Though complex embedding situation is an urgent challenge, spherical equipotential hyper-surfaces are not flexible enough to characterise the topologies, making current translation-based methods incompetent for this task.\nSecondly, because of the oversimplified loss metric, current translation-based methods treat each dimension identically. This observation leads to a flaw illustrated in Fig.2. As each dimension is treated identically in (a)1, the incorrect entities are matched, because they are closer than the correct ones, measured by isotropic Euclidean distance. Therefore,\n1The dash lines indicate the x-axis component of the loss (hx+ rx \u2212 tx) and the y-axis component of the loss (hy + ry \u2212 ty).\nFigure 2: Specific illustration of weighting dimensions. The data are selected from Wordnet. The solid dots are correct matches while the circles are not. The arrows indicate HasPart relation. (a) The incorrect circles are matched, due to the isotropic Euclidean distance. (b) By weighting embedding dimensions, we up-weighted y-axis component of loss and down-weighted x-axis component of loss, thus the embeddings are refined because the correct ones have smaller loss in x-axis direction.\nwe have a good reason to conjecture that a relation could only be affected by several specific dimensions while the other unrelated dimensions would be noisy. Treating all the dimensions identically involves much noises and degrades the performance.\nMotivated by these two issues, in this paper, we propose TransA, an embedding method by utilizing an adaptive and flexible metric. First, TransA applies elliptical surfaces instead of spherical surfaces. By this mean, complex embedding topologies induced by complex relations could be represented better. Then, as analysed in \u201cAdaptive Metric Approach\u201d, TransA could be treated as weighting transformed feature dimensions. Thus, the noise from unrelated dimensions is suppressed. We demonstrate our ideas in Fig.1 (b) and Fig.2 (b).\nTo summarize, TransA takes the adaptive metric ideas for better knowledge representation. Our method effectively models various and complex entities/relations in knowledge base, and outperforms all the state-of-the-art baselines with significant improvements in experiments.\nThe rest of the paper is organized as follows: we survey the related researches and then introduce our approach, along with the theoretical analysis. Next, the experiments are present and at the final part, we summarize our paper."}, {"heading": "Related Work", "text": "We classify prior studies into two lines: one is the translation-based embedding methods and the other includes many other embedding methods."}, {"heading": "Translation-Based Embedding Methods", "text": "All the translation-based methods share a common principle h+ r \u2248 t, but differ in defining the relation-related space where a head entity h connects to a tail entity t. This principle indicates that t should be the nearest neighbour of (h+ r). Hence, the translation-based methods all have the same form of score function that applies Euclidean distance to measure the loss, as follows:\nfr(h, t) = ||hr + r\u2212 tr|| 2 2\nwhere hr, tr are the entity embedding vectors projected in the relation-specific space. Note that this branch of methods keeps the state-of-the-art performance.\n\u2022 TransE (Bordes et al. 2013) lays the entities in the original space, say hr = h, tr = t.\n\u2022 TransH (Wang et al. 2014) projects the entities into a hyperplane for addressing the issue of complex relation embedding, say hr = h\u2212w\u22a4r hwr, tr = t\u2212w \u22a4 r twr.\n\u2022 TransR (Lin et al. 2015) transforms the entities by the same matrix to also address the issue of complex relation embedding, as: hr = Mrh, tr = Mrt.\nProjecting entities into different hyperplanes or transforming entities by different matrices allow entities to play different roles under different embedding situations. However, as the \u201cIntroduction\u201d argues, these methods are incompetent to model complex knowledge graphs well and particularly perform unsatisfactorily in various and complex entities/relations situation, because of the oversimplified metric.\nTransM (Fan et al. 2014) pre-calculates the distinct weight for each training triple to perform better."}, {"heading": "Other Embedding Methods", "text": "There are also many other models for knowledge graph embedding.\nUnstructured Model (UM). The UM (Bordes et al. 2012) is a simplified version of TransE by setting all the relation vectors to zero r = 0. Obviously, relation is not considered in this model.\nStructured Embedding (SE). The SE model (Bordes et al. 2011) applies two relation-related matrices, one for head and the other for tail. The score function is defined as fr(h, t) = ||Mh,rh\u2212Mt,rt|| 2 2. According to (Socher et al. 2013), this model cannot capture the relationship among entities and relations.\nSingle Layer Model (SLM). SLM applies neural network to knowledge graph embedding. The score function is defined as\nfr(h, t) = u \u22a4 r g(Mr,1h+Mr,2t)\nNote that SLM is a special case of NTN when the zero tensors are applied. (Collobert and Weston 2008) had proposed a similar method but applied this approach into the language model.\nSemantic Matching Energy (SME). The SME model (Bordes et al. 2012) (Bordes et al. 2014) attempts to capture the correlations between entities and relations by matrix product and Hadamard product. The score functions are defined as follows:\nfr = (M1h+M2r+ b1) \u22a4(M3t+M4r+ b2)\nfr = (M1h\u2297M2r+ b1) \u22a4(M3t\u2297M4r+ b2)\nwhere M1,M2,M3 and M4 are weight matrices, \u2297 is the Hadamard product, b1 and b2 are bias vectors. In some recent work (Bordes et al. 2014), the second form of score function is re-defined with 3-way tensors instead of matrices.\nLatent Factor Model (LFM). The LFM (Jenatton et al. 2012) uses the second-order correlations between entities by a quadratic form, defined as fr(h, t) = h\u22a4Wrt.\nNeural Tensor Network (NTN). The NTN model (Socher et al. 2013) defines an expressive score function for graph embedding to joint the SLM and LFM.\nfr(h, t) = u \u22a4 r g(h\u22a4W\u00b7\u00b7rt+Mr,1h+Mr,2t+ br)\nwhere ur is a relation-specific linear layer, g(\u00b7) is the tanh function, Wr \u2208 Rd\u00d7d\u00d7k is a 3-way tensor. However, the high complexity of NTN may degrade its applicability to large-scale knowledge bases.\nRESCAL. is a collective matrix factorization model as a common embedding method. (Nickel, Tresp, and Kriegel 2011) (Nickel, Tresp, and Kriegel 2012).\nSemantically Smooth Embedding (SSE). (Guo et al. 2015) aims at leveraging the geometric structure of embedding space to make entity representations semantically smooth.\n(Wang et al. 2014) jointly embeds knowledge and texts. (Wang, Wang, and Guo 2015) involves the rules into embedding. (Lin, Liu, and Sun 2015) considers the paths of knowledge graph into embedding."}, {"heading": "Adaptive Metric Approach", "text": "In this section, we would introduce the adaptive metric approach, TransA, and present the theoretical analysis from two perspectives."}, {"heading": "Adaptive Metric Score Function", "text": "As mentioned in \u201cIntroduction\u201d, all the translation-based methods obey the same principle h+ r \u2248 t, but they differ in the relation-specific spaces where entities are projected into. Thus, such methods share a similar score function.\nfr(h, t) = ||h+ r\u2212 t|| 2 2\n= (h+ r\u2212 t)\u22a4(h+ r\u2212 t) (1)\nThis score function is actually Euclidean metric. The disadvantages of the oversimplified metric have been discussed in \u201cIntroduction\u201d. As a consequence, the proposed TransA replaces inflexible Euclidean distance with adaptive Mahalanobis distance of absolute loss, because Mahalanobis distance is more flexible and more adaptive (Wang and Sun 2014). Thus, our score function is as follows:\nfr(h, t) = (|h+ r\u2212 t|) \u22a4 Wr(|h+ r\u2212 t|) (2)\nwhere |h+ r\u2212 t| . = (|h1+r1\u2212 t1|, |h2+r2\u2212 t2|, ..., |hn+ rn \u2212 tn|) and Wr is a relation-specific symmetric nonnegative weight matrix that corresponds to the adaptive metric. Different from the traditional score functions, we take the absolute value, since we want to measure the absolute loss between (h+ r) and t. Furthermore, we would list two main reasons for the applied absolute operator.\nOn one hand, the absolute operator makes the score function as a well-defined norm only under the condition that all the entries of Wr are non-negative. A welldefined norm is necessary for most metric learning scenes (Kulis 2012), and the non-negative condition could be achieved more easily than PSD, so it generalises the common metric learning algebraic form for better rendering the knowledge topologies. Expanding our score function as an induced norm Nr(e) = \u221a\nfr(h, t) where e . = h+ r\u2212 t. Obviously,Nr is non-negative, identical and absolute homogeneous. Besides with the easy-to-verified inequality Nr(e1 + e2) = \u221a |e1 + e2|\u22a4Wr|e1 + e2| \u2264\n\u221a |e1|\u22a4Wr|e1|+ \u221a\n|e2|\u22a4Wr|e2| = Nr(e1)+Nr(e2), the triangle inequality is hold. Totally, absolute operators make the metric a norm with an easy-to-achieve condition, helping to generalise the representation ability.\nOn the other hand, in geometry, negative or positive values indicate the downward or upward direction, while in our approach, we do not consider this factor. Let\u2019s see an instance as shown in Fig.2. For the entity Goniff, the x-axis component of its loss vector is negative, thus enlarging this component would make the overall loss smaller, while this case is supposed to make the overall loss larger. As a result, absolute operator is critical to our approach. For a numerical example without absolute operator, when the embedding dimension is two, weight matrix is [0 1; 1 0] and the loss vector (h+ r\u2212 t) = (e1, e2), the overall loss would be 2e1e2. If e1 \u2265 0 and e2 \u2264 0, much absolute larger e2 would reduce the overall loss and this is not desired."}, {"heading": "Perspective from Equipotential Surfaces", "text": "TransA shares almost the same geometric explanations with other translation-based methods, but they differ in the loss metric. For other translation-based methods, the equipotential hyper-surfaces are spheres as the Euclidean distance defines:\n||(t\u2212 h)\u2212 r||22 = C (3)\nwhere C means the threshold or the equipotential value. However, for TransA, the equipotential hyper-surfaces are elliptical surfaces as the Mahalanobis distance of absolute loss states (Kulis 2012):\n|(t\u2212 h)\u2212 r|\u22a4Wr|(t\u2212 h)\u2212 r| = C (4)\nNote that the elliptical hyper-surfaces would be distorted a bit as the absolute operator applied, but this makes no difference for analysing the performance of TransA. As we know, different equipotential hyper-surfaces correspond to different thresholds and different thresholds decide whether the triples are correct or not. Due to the practical situation that our knowledge base is large-scale and very complex, the topologies of embedding cannot be distributed as uniform as spheres, justified by Fig.1. Thus, replacing the spherical equipotential hyper-surfaces with the elliptical ones would enhance the embedding.\nAs Fig.1 illustrated, TransA would perform better for oneto-many relations. The metric of TransA is symmetric, so it is reasonable that TransA would also perform better for many-to-one relations. Moreover, a many-to-many relation could be treated as both a many-to-one and a one-to-many relation. Generally, TransA would perform better for all the complex relations."}, {"heading": "Perspective from Feature Weighting", "text": "TransA could be regarded as weighting transformed features. For weight matrix Wr that is symmetric, we obtain the equivalent unique form by LDL Decomposition (Golub and Van Loan 2012) as follows:\nWr = L \u22a4 r DrLr (5)\nfr = (Lr|h+ r\u2212 t|) \u22a4 Dr(Lr|h+ r\u2212 t|) (6)\nIn above equations, Lr can be viewed as a transformation matrix, which transforms the loss vector |h+ r\u2212 t| to another space. Furthermore, Dr = diag(w1, w2, w3....) is a diagonal matrix and different embedding dimensions are weighted by wi.\nAs analysed in \u201cIntroduction\u201d, a relation could only be affected by several specific dimensions while the other dimensions would be noisy. Treating different dimensions identically in current translation-based methods can hardly suppress the noise, consequently working out an unsatisfactory performance. We believe that different dimensions play different roles, particularly when entities are distributed divergently. Unlike existing methods, TransA can automatically learn the weights from the data. This may explain why TransA outperforms TransR although both TransA and TransR transform the entity space with matrices."}, {"heading": "Connection to Previous Works", "text": "Regarding TransR that rotates and scales the embedding spaces, TransA holds two advantages against it. Firstly, we weight feature dimensions to avoid the noise. Secondly, we loosen the PSD condition for a flexible representation. Regarding TransM that weights feature dimensions using precomputed coefficients, TransA holds two advantages against it. Firstly, we learn the weights from the data, which makes the score function more adaptive. Secondly, we apply the feature transformation that makes the embedding more effective."}, {"heading": "Training Algorithm", "text": "To train the model, we use the margin-based ranking error. Taking other constraints into account, the target function can be defined as follows:\nmin \u2211\n(h,r,t)\u2208\u2206\n\u2211\n(h\u2032,r\u2032,t\u2032)\u2208\u2206\u2032\n[fr(h, t) + \u03b3 \u2212 fr\u2032(h \u2032, t\u2032)]+ +\n\u03bb\n(\n\u2211\nr\u2208R\n||Wr|| 2 F\n)\n+ C\n(\n\u2211\ne\u2208E\n||e||22 + \u2211\nr\u2208R\n||r||22\n)\ns.t. [Wr]ij \u2265 0 (7)\nwhere [ \u00b7 ]+ . = max(0, \u00b7 ), \u2206 is the set of golden triples and \u2206\u2032 is the set of incorrect ones, \u03b3 is the margin that separates the positive and negative triples. || \u00b7 ||F is the F-norm of matrix. C controls the scaling degree, and \u03bb controls the regularization of adaptive weight matrix. The E means the set of entities and the R means the set of relations. At each round of training process, Wr could be worked out directly by setting the derivation to zero. Then, in order to ensure the non-negative condition ofWr, we set all the negative entries of Wr to zero.\nWr = \u2212 \u2211\n(h,r,t)\u2208\u2206\n( |h+ r\u2212 t||h+ r\u2212 t| \u22a4 )\n(8)\n+ \u2211\n(h\u2032,r\u2032,t\u2032)\u2208\u2206\u2032\n( |h\u2032 + r\u2032 \u2212 t\u2032||h\u2032 + r\u2032 \u2212 t\u2032| \u22a4 )\nAs to the complexity of our model, the weight matrix is completely calculated by the existing embedding vectors,\nwhich means TransA almost has the same free parameter number as TransE. As to the efficiency of our model, the weight matrix has a closed solution, which speeds up the training process to a large extent.\nExperiments We evaluate the proposed model on two benchmark tasks: link prediction and triples classification. Experiments are conducted on four public datasets that are the subsets of Wordnet and Freebase. The statistics of these datasets are listed in Tab.1.\nATPE is short for \u201cAveraged Triple number Per Entity\u201d. This quantity measures the diversity and complexity of datasets. Commonly, more triples lead to more complex structures of knowledge graph. To express the more complex structures, entities would be distributed variously and complexly. Overall, embedding methods produce less satisfactory results in the datasets with higher ATPE, because a large ATPE means a various and complex entities/relations embedding situation."}, {"heading": "Link Prediction", "text": "Link prediction aims to predict a missing entity given the other entity and the relation. In this task, we predict t given (h, r, \u2217), or predict h given (\u2217, r, t). The WN18 and FB15K datasets are the benchmark datasets for this task.\nEvaluation Protocol. We follow the same protocol as used in TransE (Bordes et al. 2013), TransH (Wang et al. 2014) and TransR (Lin et al. 2015). For each testing triple (h, r, t), we replace the tail t by every entity e in the knowledge graph and calculate a dissimilarity score with the score function fr(h, e) for the corrupted triple (h, r, e). Ranking these scores in ascending order, we then get the rank of the original correct triple. There are two metrics for evaluation: the averaged rank (Mean Rank) and the proportion of testing triples, whose ranks are not larger than 10 (HITS@10). This is called \u201cRaw\u201d setting. When we filter out the corrupted triples that exist in all the training, validation and test datasets, this is the\u201cFilter\u201d setting. If a corrupted triple exists in the knowledge graph, ranking it before the original triple is acceptable. To eliminate this issue, the \u201cFilter\u201d setting is more preferred. In both settings, a lower Mean Rank or a higher HITS@10 is better.\nImplementation. As the datasets are the same, we directly copy the experimental results of several baselines from the literature, as in (Bordes et al. 2013), (Wang et al.\n2014) and (Lin et al. 2015). We have tried several settings on the validation dataset to get the best configuration for both Adaptive Metric (PSD) and TransA. Under the \u201cbern.\u201d sampling strategy, the optimal configurations are: learning rate \u03b1 = 0.001, embedding dimension k = 50, \u03b3 = 2.0, C = 0.2 on WN18; \u03b1 = 0.002, k = 200, \u03b3 = 3.2, and C = 0.2 on FB15K.\nResults. Evaluation results on WN18 and FB15K are reported in Tab.2 and Tab.3, respectively. We can conclude that:\n1. TransA outperforms all the baselines significantly and consistently. This result justifies the effectiveness of TransA.\n2. FB15K is a very various and complex entities/relations embedding situation, because its ATPE is absolutely highest among all the datasets. However, TransA performs better than other baselines on this dataset, indicating that TransA performs better in various and complex entities/relations embedding situation. WN18 may be less complex than FB15K because of a smaller ATPE. Compared to TransE, the relative improvement of TransA on WN18 is 5.7% while that on FB15K is 95.2%. This comparison shows TransA has more advantages in the various and complex embedding environment.\n3. TransA promotes the performance for 1-1 relations, which means TransA generally promotes the performance on simple relations. TransA also promotes the performance for 1-N, N-1, N-N relations3, which demonstrates TransA works better for complex relation embedding.\n4. Compared to TransR, better performance of TransA means the feature weighting and the generalised metric form leaded by absolute operators, have significant benefits, as analysed.\n5. Compared to Adaptive Metric (PSD) which applies the score function fr(h, t) = (h+ r\u2212 t)\u22a4Wr(h+ r\u2212 t) and constrains Wr as PSD, TransA is more competent,\n2ATPE:Averaged Triple number Per Entity. Triples are summed up from all the #Train, #Valid and #Test.\n3Mapping properties of relations follow the same rules in (Bordes et al. 2013).\nbecause our score function with non-negative matrix condition and absolute operator produces a more flexible representation than that with PSD matrix condition does, as analysed in \u201cAdaptive Metric Approach\u201d.\n6. TransA performs bad in Mean Rank on WN18 dataset. Digging into the detailed situation, we discover there are 27 testing triples (0.54% of the testing set) whose ranks are more than 30,000, and these few cases would make about 162 mean rank loss. The tail or head entity of all these triples have never been co-occurring with the corresponding relation in the training set. It is the insufficient training data that leads to the over-distorted weight matrix and the over-distorted weight matrix is responsible for the bad Mean Rank."}, {"heading": "Triples Classification", "text": "Triples classification is a classical task in knowledge base embedding, which aims at predicting whether a given triple (h, r, t) is correct or not. Our evaluation protocol is the same as prior studies. Besides, WN11 and FB13 are the benchmark datasets for this task. Evaluation of classification needs negative labels. The datasets have already been built with negative triples, where each correct triple is corrupted to get one negative triple.\nEvaluation Protocol. The decision rule is as follows: for a triple (h, r, t), if fr(h, t) is below a threshold \u03c3r , then positive; otherwise negative. The thresholds {\u03c3r} are determined on the validation dataset. The final accuracy is based on how many triples are classified correctly.\nImplementation. As all methods use the same datasets, we directly copy the results of different methods from the literature. We have tried several settings on the validation dataset to get the best configuration for both Adaptive Metric (PSD) and TransA. The optimal configurations are: \u201cbern\u201d sampling, \u03b1 = 0.02, k = 50, \u03b3 = 10.0, C = 0.2 on WN11, and \u201cbern\u201d sampling, \u03b1 = 0.002, k = 200, \u03b3 = 3.0, C = 0.00002 on FB13.\nResults. Accuracies are reported in Tab.4 and Fig.3. According to \u201cAdaptive Metric Approach\u201d section, we could work out the weights by LDL Decomposition for each relation. Because the minimal weight is too small to make a significant analysis, we choose the median one to rep-\nTable 4: Triples classification: accuracies(%) for different embedding methods\nMethods WN11 FB13 Avg.\nLFM 73.8 84.3 79.0 NTN 70.4 87.1 78.8\nTransE 75.9 81.5 78.7 TransH 78.8 83.3 81.1 TransR 85.9 82.5 84.2\nAdaptive Metric (PSD) 81.4 87.1 84.3 TransA 83.2 87.3 85.3\nFigure 3: Triples classification accuracies for each relation on WN11(left) and FB13(right). The \u201cweight difference\u201d is worked out by the scaled difference between maximal and median weight.\nresent relative small weight. Thus, \u201cWeight Difference\u201d is calculated by ( MaximalWeight\u2212MedianWeight\nMedianWeight\n)\n. Bigger the\nweight difference is, more significant effect, the feature weighting makes. Notably, scaling by the median weight makes the weight differences comparable to each other. We observe that:\n1. Overall, TransA yields the best average accuracy, illustrating the effectiveness of TransA.\n2. Accuracies vary with the weight difference, meaning the feature weighting benefits the accuracies. This proves the theoretical analysis and the effectiveness of TransA.\n3. Compared to Adaptive Metric (PSD) , TransA performs better, because our score function with non-negative matrix condition and absolute operator leads to a more flexible representation than that with PSD matrix condition does."}, {"heading": "Conclusion", "text": "In this paper, we propose TransA, a translation-based knowledge graph embedding method with an adaptive and flexible metric. TransA applies elliptical equipotential hypersurfaces to characterise the embedding topologies and weights several specific feature dimensions for a relation to avoid much noise. Thus, our adaptive metric approach could\neffectively model various and complex entities/relations in knowledge base. Experiments are conducted with two benchmark tasks and the results show TransA achieves consistent and significant improvements over the current stateof-the-art baselines. To reproduce our results, our codes and data will be published in github.\nReferences Bao, J.; Duan, N.; Zhou, M.; and Zhao, T. 2014. Knowledgebased question answering as machine translation. Cell 2:6. Bollacker, K.; Evans, C.; Paritosh, P.; Sturge, T.; and Taylor, J. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 1247\u20131250. ACM. Bordes, A.; Weston, J.; Collobert, R.; Bengio, Y.; et al. 2011. Learning structured embeddings of knowledge bases. In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence. Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y. 2012. Joint learning of words and meaning representations for open-text semantic parsing. In International Conference on Artificial Intelligence and Statistics, 127\u2013135. Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems, 2787\u20132795. Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y. 2014. A semantic matching energy function for learning with multirelational data. Machine Learning 94(2):233\u2013259. Collobert, R., and Weston, J. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, 160\u2013167. ACM. Fader, A.; Zettlemoyer, L.; and Etzioni, O. 2014. Open question answering over curated and extracted knowledge bases. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 1156\u20131165. ACM. Fan, M.; Zhou, Q.; Chang, E.; and Zheng, T. F. 2014. Transition-based knowledge graph embedding with relational mapping properties. In Proceedings of the 28th Pacific"}, {"heading": "Asia Conference on Language, Information, and Computa-", "text": "tion, 328\u2013337.\nGolub, G. H., and Van Loan, C. F. 2012. Matrix computations, volume 3. JHU Press. Guo, S.; Wang, Q.; Wang, B.; Wang, L.; and Guo, L. 2015. Semantically smooth knowledge graph embedding. In Proceedings of ACL. Jenatton, R.; Roux, N. L.; Bordes, A.; and Obozinski, G. R. 2012. A latent factor model for highly multi-relational data. In Advances in Neural Information Processing Systems, 3167\u20133175. Kulis, B. 2012. Metric learning: A survey. Foundations & Trends in Machine Learning 5(4):287\u2013364. Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. Lin, Y.; Liu, Z.; and Sun, M. 2015. Modeling relation paths for representation learning of knowledge bases. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics. Miller, G. A. 1995. Wordnet: a lexical database for english. Communications of the ACM 38(11):39\u201341. Nickel, M.; Tresp, V.; and Kriegel, H.-P. 2011. A threeway model for collective learning on multi-relational data. In Proceedings of the 28th international conference on machine learning (ICML-11), 809\u2013816. Nickel, M.; Tresp, V.; and Kriegel, H.-P. 2012. Factorizing yago: scalable machine learning for linked data. In Proceedings of the 21st international conference on World Wide Web, 271\u2013280. ACM. Socher, R.; Chen, D.; Manning, C. D.; and Ng, A. 2013. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems, 926\u2013934. Wang, F., and Sun, J. 2014. Survey on distance metric learning and dimensionality reduction in data mining. Data Mining and Knowledge Discovery 1\u201331. Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z. 2014. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 1112\u20131119. Wang, Q.; Wang, B.; and Guo, L. 2015. Knowledge base completion using embeddings and rules. In Proceedings of the 24th International Joint Conference on Artificial Intelligence."}], "references": [{"title": "Knowledgebased question answering as machine translation", "author": ["J. Bao", "N. Duan", "M. Zhou", "T. Zhao"], "venue": "Cell 2:6.", "citeRegEx": "Bao et al\\.,? 2014", "shortCiteRegEx": "Bao et al\\.", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 1247\u20131250. ACM.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y Bengio"], "venue": "In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Bordes et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "International Conference on Artificial Intelligence and Statistics, 127\u2013135.", "citeRegEx": "Bordes et al\\.,? 2012", "shortCiteRegEx": "Bordes et al\\.", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Advances in Neural Information Processing Systems, 2787\u20132795.", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "A semantic matching energy function for learning with multirelational data", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "Machine Learning 94(2):233\u2013259.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 1156\u20131165. ACM.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Transition-based knowledge graph embedding with relational mapping properties", "author": ["M. Fan", "Q. Zhou", "E. Chang", "T.F. Zheng"], "venue": "Proceedings of the 28th Pacific Asia Conference on Language, Information, and Computation, 328\u2013337.", "citeRegEx": "Fan et al\\.,? 2014", "shortCiteRegEx": "Fan et al\\.", "year": 2014}, {"title": "Matrix computations, volume 3", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "JHU Press.", "citeRegEx": "Golub and Loan,? 2012", "shortCiteRegEx": "Golub and Loan", "year": 2012}, {"title": "Semantically smooth knowledge graph embedding", "author": ["S. Guo", "Q. Wang", "B. Wang", "L. Wang", "L. Guo"], "venue": "Proceedings of ACL.", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G.R. Obozinski"], "venue": "Advances in Neural Information Processing Systems, 3167\u20133175.", "citeRegEx": "Jenatton et al\\.,? 2012", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Metric learning: A survey", "author": ["B. Kulis"], "venue": "Foundations & Trends in Machine Learning 5(4):287\u2013364.", "citeRegEx": "Kulis,? 2012", "shortCiteRegEx": "Kulis", "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Y. Lin", "Z. Liu", "M. Sun", "Y. Liu", "X. Zhu"], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Y. Lin", "Z. Liu", "M. Sun"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics.", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM 38(11):39\u201341.", "citeRegEx": "Miller,? 1995", "shortCiteRegEx": "Miller", "year": 1995}, {"title": "A threeway model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 809\u2013816.", "citeRegEx": "Nickel et al\\.,? 2011", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the 21st international conference on World Wide Web, 271\u2013280. ACM.", "citeRegEx": "Nickel et al\\.,? 2012", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Advances in Neural Information Processing Systems, 926\u2013934.", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Survey on distance metric learning and dimensionality reduction in data mining", "author": ["F. Wang", "J. Sun"], "venue": "Data Mining and Knowledge Discovery 1\u201331.", "citeRegEx": "Wang and Sun,? 2014", "shortCiteRegEx": "Wang and Sun", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Z. Wang", "J. Zhang", "J. Feng", "Z. Chen"], "venue": "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge base completion using embeddings and rules", "author": ["Q. Wang", "B. Wang", "L. Guo"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Knowledge graphs such as Wordnet (Miller 1995) and Freebase (Bollacker et al.", "startOffset": 33, "endOffset": 46}, {"referenceID": 1, "context": "Knowledge graphs such as Wordnet (Miller 1995) and Freebase (Bollacker et al. 2008) play an important role in AI researches and applications.", "startOffset": 60, "endOffset": 83}, {"referenceID": 0, "context": "Recent researches such as query expansion prefer involving knowledge graphs (Bao et al. 2014) while some industrial applications such as question answering robots are also powered by knowledge graphs (Fader, Zettlemoyer, and Etzioni 2014).", "startOffset": 76, "endOffset": 93}, {"referenceID": 4, "context": "To provide a general paradigm to support computing on knowledge graph, various knowledge graph embedding methods have been proposed, such as TransE (Bordes et al. 2013), TransH (Wang et al.", "startOffset": 148, "endOffset": 168}, {"referenceID": 20, "context": "2013), TransH (Wang et al. 2014) and TransR (Lin et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 13, "context": "2014) and TransR (Lin et al. 2015).", "startOffset": 17, "endOffset": 34}, {"referenceID": 4, "context": "\u2022 TransE (Bordes et al. 2013) lays the entities in the original space, say hr = h, tr = t.", "startOffset": 9, "endOffset": 29}, {"referenceID": 20, "context": "\u2022 TransH (Wang et al. 2014) projects the entities into a hyperplane for addressing the issue of complex relation embedding, say hr = h\u2212w r hwr, tr = t\u2212w \u22a4 r twr.", "startOffset": 9, "endOffset": 27}, {"referenceID": 13, "context": "\u2022 TransR (Lin et al. 2015) transforms the entities by the same matrix to also address the issue of complex relation embedding, as: hr = Mrh, tr = Mrt.", "startOffset": 9, "endOffset": 26}, {"referenceID": 8, "context": "TransM (Fan et al. 2014) pre-calculates the distinct weight for each training triple to perform better.", "startOffset": 7, "endOffset": 24}, {"referenceID": 3, "context": "The UM (Bordes et al. 2012) is a simplified version of TransE by setting all the relation vectors to zero r = 0.", "startOffset": 7, "endOffset": 27}, {"referenceID": 2, "context": "The SE model (Bordes et al. 2011) applies two relation-related matrices, one for head and the other for tail.", "startOffset": 13, "endOffset": 33}, {"referenceID": 18, "context": "According to (Socher et al. 2013), this model cannot capture the relationship among entities and relations.", "startOffset": 13, "endOffset": 33}, {"referenceID": 6, "context": "(Collobert and Weston 2008) had proposed a similar method but applied this approach into the language model.", "startOffset": 0, "endOffset": 27}, {"referenceID": 3, "context": "The SME model (Bordes et al. 2012) (Bordes et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 5, "context": "2012) (Bordes et al. 2014) attempts to capture the correlations between entities and relations by matrix product and Hadamard product.", "startOffset": 6, "endOffset": 26}, {"referenceID": 5, "context": "In some recent work (Bordes et al. 2014), the second form of score function is re-defined with 3-way tensors instead of matrices.", "startOffset": 20, "endOffset": 40}, {"referenceID": 11, "context": "The LFM (Jenatton et al. 2012) uses the second-order correlations between entities by a quadratic form, defined as fr(h, t) = hWrt.", "startOffset": 8, "endOffset": 30}, {"referenceID": 18, "context": "The NTN model (Socher et al. 2013) defines an expressive score function for graph embedding to joint the SLM and LFM.", "startOffset": 14, "endOffset": 34}, {"referenceID": 10, "context": "(Guo et al. 2015) aims at leveraging the geometric structure of embedding space to make entity representations semantically smooth.", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "(Wang et al. 2014) jointly embeds knowledge and texts.", "startOffset": 0, "endOffset": 18}, {"referenceID": 19, "context": "As a consequence, the proposed TransA replaces inflexible Euclidean distance with adaptive Mahalanobis distance of absolute loss, because Mahalanobis distance is more flexible and more adaptive (Wang and Sun 2014).", "startOffset": 194, "endOffset": 213}, {"referenceID": 12, "context": "A welldefined norm is necessary for most metric learning scenes (Kulis 2012), and the non-negative condition could be achieved more easily than PSD, so it generalises the common metric learning algebraic form for better rendering the knowledge topologies.", "startOffset": 64, "endOffset": 76}, {"referenceID": 12, "context": "However, for TransA, the equipotential hyper-surfaces are elliptical surfaces as the Mahalanobis distance of absolute loss states (Kulis 2012):", "startOffset": 130, "endOffset": 142}, {"referenceID": 4, "context": "We follow the same protocol as used in TransE (Bordes et al. 2013), TransH (Wang et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 20, "context": "2013), TransH (Wang et al. 2014) and TransR (Lin et al.", "startOffset": 14, "endOffset": 32}, {"referenceID": 13, "context": "2014) and TransR (Lin et al. 2015).", "startOffset": 17, "endOffset": 34}, {"referenceID": 4, "context": "As the datasets are the same, we directly copy the experimental results of several baselines from the literature, as in (Bordes et al. 2013), (Wang et al.", "startOffset": 120, "endOffset": 140}, {"referenceID": 2, "context": "SE(Bordes et al. 2011) 1,011 985 68.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "8 SME (Bordes et al. 2012) 545 533 65.", "startOffset": 6, "endOffset": 26}, {"referenceID": 11, "context": "8 LFM (Jenatton et al. 2012) 469 456 71.", "startOffset": 6, "endOffset": 28}, {"referenceID": 4, "context": "TransE (Bordes et al. 2013) 263 251 75.", "startOffset": 7, "endOffset": 27}, {"referenceID": 20, "context": "1 TransH (Wang et al. 2014) 401 388 73.", "startOffset": 9, "endOffset": 27}, {"referenceID": 13, "context": "4 TransR (Lin et al. 2015) 238 225 79.", "startOffset": 9, "endOffset": 26}, {"referenceID": 13, "context": "2014) and (Lin et al. 2015).", "startOffset": 10, "endOffset": 27}, {"referenceID": 4, "context": "Mapping properties of relations follow the same rules in (Bordes et al. 2013).", "startOffset": 57, "endOffset": 77}, {"referenceID": 2, "context": "SE(Bordes et al. 2011) 35.", "startOffset": 2, "endOffset": 22}, {"referenceID": 3, "context": "3 SME (Bordes et al. 2012) 35.", "startOffset": 6, "endOffset": 26}, {"referenceID": 4, "context": "3 TransE (Bordes et al. 2013) 43.", "startOffset": 9, "endOffset": 29}, {"referenceID": 20, "context": "0 TransH (Wang et al. 2014) 66.", "startOffset": 9, "endOffset": 27}, {"referenceID": 13, "context": "TransR (Lin et al. 2015) 78.", "startOffset": 7, "endOffset": 24}], "year": 2017, "abstractText": "Knowledge representation is a major topic in AI, and many studies attempt to represent entities and relations of knowledge base in a continuous vector space. Among these attempts, translation-based methods build entity and relation vectors by minimizing the translation loss from a head entity to a tail one. In spite of the success of these methods, translation-based methods also suffer from the oversimplified loss metric, and are not competitive enough to model various and complex entities/relations in knowledge bases. To address this issue, we propose TransA, an adaptive metric approach for embedding, utilizing the metric learning ideas to provide a more flexible embedding method. Experiments are conducted on the benchmark datasets and our proposed method makes significant and consistent improvements over the state-of-the-art baselines.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}