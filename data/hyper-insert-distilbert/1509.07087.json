{"id": "1509.07087", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2015", "title": "Deep Temporal Sigmoid Belief Networks for Sequence Modeling", "abstract": "structured deep performance dynamic generative intelligence models hence are developed to learn accurately sequential cumulative dependencies in time - series data. later the multi - process layered model is designed first by constantly constructing a hierarchy of temporal sigmoid belief networks ( tsbns ), originally defined as representing a sequential statistical stack database of sigmoid belief networks ( sbns ). computing each initial sbn has a contextual hidden state, whilst inherited from presenting the instances previous sbns presented in the sequence, correctly and together is used simultaneously to individually regulate its external hidden bias. powerful scalable learning processes and inference algorithms therefore are derived alternatively by introducing a recognition programming model tool that yields fast sampling descent from the variational posterior. this recognition model implementation is appropriately trained jointly concurrent with construct the generative neural model, by maximizing exactly its variational lower level bound influence on the log - likelihood. experimental results on bouncing paint balls, electronic polyphonic music, optical motion capture, simulation and adaptive text content streams show qualities that within the highly proposed quantum approach achieves state - be of - the - art predictive optimization performance, and has the capabilities capacity to synthesize various pattern sequences.", "histories": [["v1", "Wed, 23 Sep 2015 18:36:42 GMT  (438kb,D)", "http://arxiv.org/abs/1509.07087v1", "to appear in NIPS 2015"]], "COMMENTS": "to appear in NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["zhe gan", "chunyuan li", "ricardo henao", "david e carlson", "lawrence carin"], "accepted": true, "id": "1509.07087"}, "pdf": {"name": "1509.07087.pdf", "metadata": {"source": "CRF", "title": "Deep Temporal Sigmoid Belief Networks for Sequence Modeling", "authors": ["Zhe Gan", "Chunyuan Li", "Ricardo Henao", "David Carlson"], "emails": ["lcarin}@duke.edu"], "sections": [{"heading": null, "text": "Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences."}, {"heading": "1 Introduction", "text": "Considerable research has been devoted to developing probabilistic models for high-dimensional time-series data, such as video and music sequences, motion capture data, and text streams. Among them, Hidden Markov Models (HMMs) [1] and Linear Dynamical Systems (LDS) [2] have been widely studied, but they may be limited in the type of dynamical structures they can model. An HMM is a mixture model, which relies on a single multinomial variable to represent the history of a time-series. To representN bits of information about the history, an HMM could require 2N distinct states. On the other hand, real-world sequential data often contain complex non-linear temporal dependencies, while a LDS can only model simple linear dynamics.\nAnother class of time-series models, which are potentially better suited to model complex probability distributions over high-dimensional sequences, relies on the use of Recurrent Neural Networks (RNNs) [3, 4, 5, 6], and variants of a well-known undirected graphical model called the Restricted Boltzmann Machine (RBM) [7, 8, 9, 10, 11]. One such variant is the Temporal Restricted Boltzmann Machine (TRBM) [8], which consists of a sequence of RBMs, where the state of one or more previous RBMs determine the biases of the RBM in the current time step. Learning and inference in the TRBM is non-trivial. The approximate procedure used in [8] is heuristic and not derived from a principled statistical formalism.\nRecently, deep directed generative models [12, 13, 14, 15] are becoming popular. A directed graphical model that is closely related to the RBM is the Sigmoid Belief Network (SBN) [16]. In the work presented here, we introduce the Temporal Sigmoid Belief Network (TSBN), which can be viewed as a temporal stack of SBNs, where each SBN has a contextual hidden state that is inherited from the previous SBNs and is used to adjust its hidden-units bias. Based on this, we further develop a deep dynamic generative model by constructing a hierarchy of TSBNs. This can be considered\nar X\niv :1\n50 9.\n07 08\n7v 1\n[ st\nat .M\nL ]\n2 3\nSe p\nas a deep SBN [15] with temporal feedback loops on each layer. Both stochastic and deterministic hidden layers are considered.\nCompared with previous work, our model: (i) can be viewed as a generalization of an HMM with distributed hidden state representations, and with a deep architecture; (ii) can be seen as a generalization of a LDS with complex non-linear dynamics; (iii) can be considered as a probabilistic construction of the traditionally deterministic RNN; (iv) is closely related to the TRBM, but it has a fully generative process, where data are readily generated from the model using ancestral sampling; (v) can be utilized to model different kinds of data, e.g., binary, real-valued and counts.\nThe \u201cexplaining away\u201d effect described in [17] makes inference slow, if one uses traditional inference methods. Another important contribution we present here is to develop fast and scalable learning and inference algorithms, by introducing a recognition model [12, 13, 14], that learns an inverse mapping from observations to hidden variables, based on a loss function derived from a variational principle. By utilizing the recognition model and variance-reduction techniques from [13], we achieve fast inference both at training and testing time."}, {"heading": "2 Model Formulation", "text": ""}, {"heading": "2.1 Sigmoid Belief Networks", "text": "Deep dynamic generative models are considered, based on the Sigmoid Belief Network (SBN) [16]. An SBN is a Bayesian network that models a binary visible vector v \u2208 {0, 1}M , in terms of binary hidden variables h \u2208 {0, 1}J and weights W \u2208 RM\u00d7J with\np(vm = 1|h) = \u03c3(w>mh+ cm), p(hj = 1) = \u03c3(bj), (1)\nwhere v = [v1, . . . , vM ]>, h = [h1, . . . , hJ ]>, W = [w1, . . . ,wM ]>, c = [c1, . . . , cM ]>, b = [b1, . . . , bJ ]\n>, and the logistic function, \u03c3(x) , 1/(1 + e\u2212x). The parameters W, b and c characterize all data, and the hidden variables, h, are specific to particular visible data, v.\nThe SBN is closely related to the RBM [18], which is a Markov random field with the same bipartite structure as the SBN. The RBM defines a distribution over a binary vector that is proportional to the exponential of its energy, defined as \u2212E(v,h) = v>c + v>Wh + h>b. The conditional distributions, p(v|h) and p(h|v), in the RBM are factorial, which makes inference fast, while parameter estimation usually relies on an approximation technique known as Contrastive Divergence (CD) [18].\nThe energy function of an SBN may be written as\u2212E(v,h) = v>c+v>Wh+h>b\u2212 \u2211\nm log(1+ exp(w>mh + cm)). SBNs explicitly manifest the generative process to obtain data, in which the hidden layer provides a directed \u201cexplanation\u201d for patterns generated in the visible layer. However, the \u201cexplaining away\u201d effect described in [17] makes inference inefficient, the latter can be alleviated by exploiting recent advances in variational inference methods [13]."}, {"heading": "2.2 Temporal Sigmoid Belief Networks", "text": "The proposed Temporal Sigmoid Belief Network (TSBN) model is a sequence of SBNs arranged in such way that at any given time step, the SBN\u2019s biases depend on the state of the SBNs in the previous time steps. Specifically, assume we have a length-T binary visible sequence, the tth time step of which is denoted vt \u2208 {0, 1}M . The TSBN describes the joint probability as\np\u03b8(V,H) = p(h1)p(v1|h1) \u00b7 T\u220f\nt=2\np(ht|ht\u22121,vt\u22121) \u00b7 p(vt|ht,vt\u22121), (2)\nwhere V = [v1, . . . ,vT ], H = [h1, . . . ,hT ], and each ht \u2208 {0, 1}J represents the hidden state corresponding to time step t. For t = 1, . . . , T , each conditional distribution in (2) is expressed as\np(hjt = 1|ht\u22121,vt\u22121) = \u03c3(w>1jht\u22121 +w>3jvt\u22121 + bj), (3) p(vmt = 1|ht,vt\u22121) = \u03c3(w>2mht +w>4mvt\u22121 + cm), (4)\nwhere h0 and v0, needed for the prior model p(h1) and p(v1|h1), are defined as zero vectors, respectively, for conciseness. The model parameters, \u03b8, are specified as W1 \u2208 RJ\u00d7J , W2 \u2208 RM\u00d7J , W3 \u2208 RJ\u00d7M , W4 \u2208 RM\u00d7M . For i = 1, 2, 3, 4,wij is the transpose of the jth row of Wi, and c = [c1, . . . , cM ]> and b = [b1, . . . , bJ ]> are bias terms. The graphical model for the TSBN is shown in Figure 4(a).\nBy setting W3 and W4 to be zero matrices, the TSBN can be viewed as a Hidden Markov Model [1] with an exponentially large state space, that has a compact parameterization of the transition and the emission probabilities. Specifically, each hidden state in the HMM is represented as a one-hot length-J vector, while in the TSBN, the hidden states can be any length-J binary vector. We note that the transition matrix is highly structured, since the number of parameters is only quadratic w.r.t. J . Compared with the TRBM [8], our TSBN is fully directed, which allows for fast sampling of \u201cfantasy\u201d data from the inferred model."}, {"heading": "2.3 TSBN Variants", "text": "Modeling real-valued data The model above can be readily extended to model real-valued sequence data, by substituting (14) with p(vt|ht,vt\u22121) = N (\u00b5t, diag(\u03c32t )), where\n\u00b5mt = w > 2mht +w > 4mvt\u22121 + cm, log \u03c3 2 mt = (w \u2032 2m) >ht + (w \u2032 4m) >vt\u22121 + c \u2032 m, (5)\nand \u00b5mt and \u03c32mt are elements of \u00b5t and \u03c3 2 t , respectively. W \u2032 2 and W \u2032 4 are of the same size of W2 and W4, respectively. Compared with the Gaussian TRBM [9], in which \u03c3mt is fixed to 1, our formalism uses a diagonal matrix to parameterize the variance structure of vt.\nModeling count data We also introduce an approach for modeling time-series data with count observations, by replacing (14) with p(vt|ht,vt\u22121) = \u220fM m=1 y vmt mt , where\nymt = exp(w>2mht +w > 4mvt\u22121 + cm)\u2211M\nm\u2032=1 exp(w > 2m\u2032ht +w > 4m\u2032vt\u22121 + cm\u2032)\n. (6)\nThis formulation is related to the Replicated Softmax Model (RSM) described in [19], however, our approach uses a directed connection from the binary hidden variables to the visible counts, while also learning the dynamics in the count sequences.\nFurthermore, rather than assuming that ht and vt only depend on ht\u22121 and vt\u22121, in the experiments, we also allow for connections from the past n time steps of the hidden and visible states, to the current states, ht and vt. A sliding window is then used to go through the sequence to obtain n frames at each time. We refer to n as the order of the model."}, {"heading": "2.4 Deep Architecture for Sequence Modeling with TSBNs", "text": "Learning the sequential dependencies with the shallow model in (2)-(14) may be restrictive. Therefore, we propose two deep architectures to improve its representational power: (i) adding stochastic hidden layers; (ii) adding deterministic hidden layers. The graphical model for the deep TSBN\nis shown in Figure 4(c). Specifically, we consider a deep TSBN with hidden layers h(`)t for t = 1, . . . , T and ` = 1, . . . , L. Assume layer ` contains J (`) hidden units, and denote the visible layer vt = h (0) t and let h (L+1) t = 0, for convenience. In order to obtain a proper generative model, the top hidden layer h(L) contains stochastic binary hidden variables.\nFor the middle layers, ` = 1, . . . , L\u22121, if stochastic hidden layers are utilized, the generative process is expressed as p(h(`)t ) = \u220fJ(`) j=1 p(h (`) jt |h (`+1) t ,h (`) t\u22121,h (`\u22121) t\u22121 ), where each conditional distribution is parameterized via a logistic function, as in (14). If deterministic hidden layers are employed, we obtain h(`)t = f(h (`+1) t ,h (`) t\u22121,h (`\u22121) t\u22121 ), where f(\u00b7) is chosen to be a rectified linear function. Although the differences between these two approaches are minor, learning and inference algorithms can be quite different, as shown in Section 3.3."}, {"heading": "3 Scalable Learning and Inference", "text": "Computation of the exact posterior over the hidden variables in (2) is intractable. Approximate Bayesian inference, such as Gibbs sampling or mean-field variational Bayes (VB) inference, can be implemented [15, 16]. However, Gibbs sampling is very inefficient, due to the fact that the conditional posterior distribution of the hidden variables does not factorize. The mean-field VB indeed provides a fully factored variational posterior, but this technique increases the gap between the bound being optimized and the true log-likelihood, potentially resulting in a poor fit to the data. To allow for tractable and scalable inference and parameter learning, without loss of the flexibility of the variational posterior, we apply the Neural Variational Inference and Learning (NVIL) algorithm described in [13]."}, {"heading": "3.1 Variational Lower Bound Objective", "text": "We are interested in training the TSBN model, p\u03b8(V,H), described in (2), with parameters \u03b8. Given an observation V, we introduce a fixed-form distribution, q\u03c6(H|V), with parameters \u03c6, that approximates the true posterior distribution, p(H|V). We then follow the variational principle to derive a lower bound on the marginal log-likelihood, expressed as1\nL(V,\u03b8,\u03c6) = Eq\u03c6(H|V)[log p\u03b8(V,H)\u2212 log q\u03c6(H|V)] . (7) We construct the approximate posterior q\u03c6(H|V) as a recognition model. By using this, we avoid the need to compute variational parameters per data point; instead we compute a set of parameters \u03c6 used for all V. In order to achieve fast inference, the recognition model is expressed as\nq\u03c6(H|V) = q(h1|v1) \u00b7 T\u220f\nt=2\nq(ht|ht\u22121,vt,vt\u22121) , (8)\nand each conditional distribution is specified as q(hjt = 1|ht\u22121,vt,vt\u22121) = \u03c3(u>1jht\u22121 + u>2jvt + u>3jvt\u22121 + dj) , (9)\nwhere h0 and v0, for q(h1|v1), are defined as zero vectors. The recognition parameters \u03c6 are specified as U1 \u2208 RJ\u00d7J , U2 \u2208 RJ\u00d7M , U3 \u2208 RJ\u00d7M . For i = 1, 2, 3, uij is the transpose of the jth row of Ui, and d = [d1, . . . , dJ ]> is the bias term. The graphical model is shown in Figure 4(b).\nThe recognition model defined in (9) has the same form as in the approximate inference used for the TRBM [8]. Exact inference for our model consists of a forward and backward pass through the entire sequence, that requires the traversing of each possible hidden state. Our feedforward approximation allows the inference procedure to be fast and implemented in an online fashion."}, {"heading": "3.2 Parameter Learning", "text": "To optimize (7), we utilize Monte Carlo methods to approximate expectations and stochastic gradient descent (SGD) for parameter optimization. The gradients can be expressed as\n\u2207\u03b8L(V) = Eq\u03c6(H|V)[\u2207\u03b8 log p\u03b8(V,H)], (10) \u2207\u03c6L(V) = Eq\u03c6(H|V)[(log p\u03b8(V,H)\u2212 log q\u03c6(H|V))\u00d7\u2207\u03c6 log q\u03c6(H|V)]. (11)\n1This lower bound is equivalent to the marginal log-likelihood if q\u03c6(H|V) = p(H|V).\nSpecifically, in the TSBN model, if we define v\u0302mt = \u03c3(w>2mht + w > 4mvt\u22121 + cm) and h\u0302jt = \u03c3(u>1jht\u22121 + u > 2jvt + u > 3jvt\u22121 + dj), the gradients for w2m and u2j can be calculated as\n\u2202 log p\u03b8(V,H)\n\u2202w2mj = T\u2211 t=1 (vmt \u2212 v\u0302mt) \u00b7 hjt, \u2202 log q\u03c6(H|V) \u2202u2jm = T\u2211 t=1 (hjt \u2212 h\u0302jt) \u00b7 vmt. (12)\nOther update equations, along with the learning details for the TSBN variants in Section 2.3, are provided in the Supplementary Section B. We observe that the gradients in (10) and (11) share many similarities with the wake-sleep algorithm [20]. Wake-sleep alternates between updating \u03b8 in the wake phase and updating \u03c6 in the sleep phase. The update of \u03b8 is based on the samples generated from q\u03c6(H|V), and is identical to (10). However, in contrast to (11), the recognition parameters \u03c6 are estimated from samples generated by the model, i.e., \u2207\u03c6L(V) = Ep\u03b8(V,H)[\u2207\u03c6 log q\u03c6(H|V)]. This update does not optimize the same objective as in (10), hence the wake-sleep algorithm is not guaranteed to converge [13].\nInspecting (11), we see that we are using l\u03c6(V,H) = log p\u03b8(V,H)\u2212 log q\u03c6(H|V) as the learning signal for the recognition parameters \u03c6. The expectation of this learning signal is exactly the lower bound (7), which is easy to evaluate. However, this tractability makes the estimated gradients of the recognition parameters very noisy. In order to make the algorithm practical, we employ the variance reduction techniques proposed in [13], namely: (i) centering the learning signal, by subtracting the data-independent baseline and the data-dependent baseline; (ii) variance normalization, by dividing the centered learning signal by a running estimate of its standard deviation. The data-dependent baseline is implemented using a neural network. Additionally, RMSprop [21], a form of SGD where the gradients are adaptively rescaled by a running average of their recent magnitude, were found in practice to be important for fast convergence; thus utilized throughout all the experiments. The outline of the NVIL algorithm is provided in the Supplementary Section A."}, {"heading": "3.3 Extension to deep models", "text": "The recognition model corresponding to the deep TSBN is shown in Figure 4(d). Two kinds of deep architectures are discussed in Section 2.4. We illustrate the difference of their learning algorithms in two respects: (i) the calculation of the lower bound; and (ii) the calculation of the gradients.\nThe top hidden layer is stochastic. If the middle hidden layers are also stochastic, the calculation of the lower bound is more involved, compared with the shallow model; however, the gradient evaluation remain simple as in (12). On the other hand, if deterministic middle hidden layers (i.e., recurrent neural networks) are employed, the lower bound objective will stay the same as a shallow model, since the only stochasticity in the generative process lies in the top layer; however, the gradients have to be calculated recursively through the back-propagation through time algorithm [22]. All details are provided in the Supplementary Section C."}, {"heading": "4 Related Work", "text": "The RBM has been widely used as building block to learn the sequential dependencies in time-series data, e.g., the conditional-RBM-related models [7, 23], and the temporal RBM [8]. To make exact inference possible, the recurrent temporal RBM was also proposed [9], and further extended to learn the dependency structure within observations [11].\nIn the work reported here, we focus on modeling sequences based on the SBN [16], which recently has been shown to have the potential to build deep generative models [13, 15, 24]. Our work serves as another extension of the SBN that can be utilized to model time-series data. Similar ideas have also been considered in [25] and [26]. However, in [25], the authors focus on grammar learning, and use a feed-forward approximation of the mean-field VB to carry out the inference; while in [26], the wake-sleep algorithm was developed. We apply the model in a different scenario, and develop a fast and scalable inference algorithm, based on the idea of training a recognition model by leveraging the stochastic gradient of the variational bound.\nThere exist two main methods for the training of recognition models. The first one, termed Stochastic Gradient Variational Bayes (SGVB), is based on a reparameterization trick [12, 14], which can be only employed in models with continuous latent variables, e.g., the variational auto-encoder [12]\nand all the recent recurrent extensions of it [27, 28, 29]. The second one, called Neural Variational Inference and Learning (NVIL), is based on the log-derivative trick [13], which is more general and can also be applicable to models with discrete random variables. The NVIL algorithm has been previously applied to the training of SBN in [13]. Our approach serves as a new application of this algorithm for a SBN-based time-series model."}, {"heading": "5 Experiments", "text": "We present experimental results on four publicly available datasets: the bouncing balls [9], polyphonic music [10], motion capture [7] and state-of-the-Union [30]. To assess the performance of the TSBN model, we show sequences generated from the model, and report the average log-probability that the model assigns to a test sequence, and the average squared one-step-ahead prediction error per frame. Code is available at https://github.com/zhegan27/TSBN_code_NIPS2015.\nThe TSBN model with W3 = 0 and W4 = 0 is denoted Hidden Markov SBN (HMSBN), the deep TSBN with stochastic hidden layer is denoted DTSBN-S, and the deep TSBN with deterministic hidden layer is denoted DTSBN-D.\nModel parameters were initialized by sampling randomly from N (0, 0.0012I), except for the bias parameters, that were initialized as 0. The TSBN model is trained using a variant of RMSprop [6], with momentum of 0.9, and a constant learning rate of 10\u22124. The decay over the root mean squared gradients is set to 0.95. The maximum number of iterations we use is 105. The gradient estimates were computed using a single sample from the recognition model. The only regularization we used was a weight decay of 10\u22124. The data-dependent baseline was implemented by using a neural network with a single hidden layer with 100 tanh units.\nFor the prediction of vt given v1:t\u22121, we (i) first obtain a sample from q\u03c6(h1:t\u22121|v1:t\u22121); (ii) calculate the conditional posterior p\u03b8(ht|h1:t\u22121,v1:t\u22121) of the current hidden state ; (iii) make a prediction for vt using p\u03b8(vt|h1:t,v1:t\u22121). On the other hand, synthesizing samples is conceptually simper. Sequences can be readily generated from the model using ancestral sampling."}, {"heading": "5.1 Bouncing balls dataset", "text": "We conducted the first experiment on synthetic videos of 3 bouncing balls, where pixels are binary valued. We followed the procedure in [9], and generated 4000 videos for training, and another 200 videos for testing. Each video is of length 100 and of resolution 30\u00d7 30. The dictionaries learned using the HMSBN are shown in Figure 2 (Left). Compared with previous work [9, 10], our learned bases are more spatially localized. In Table 6, we compare the average squared prediction error per frame over the 200 test videos, with recurrent temporal RBM (RTRBM) and structured RTRBM (SRTRBM). As can be seen, our approach achieves better performance compared with the baselines in the literature. Furthermore, we observe that a high-order TSBN reduces the prediction error significantly, compared with an order-one TSBN. This is due to the fact\nthat by using a high-order TSBN, more information about the past is conveyed. We also examine the advantage of employing deep models. Using stochastic, or deterministic hidden layer improves performances. More results, including log-likelihoods, are provided in Supplementary Section D."}, {"heading": "5.2 Motion capture dataset", "text": "In this experiment, we used the CMU motion capture dataset, that consists of measured joint angles for different motion types. We used the 33 running and walking sequences of subject 35 (23 walking sequences and 10 running sequences). We followed the preprocessing procedure of [11], after which we were left with 58 joint angles. We partitioned the 33 sequences into training and testing set: the first of which had 31 sequences, and the second had 2 sequences (one walking and another running). We averaged the prediction error over 100 trials, as reported in Table 7. The TSBN we implemented is of size 100 in each hidden layer and order 1. It can be seen that the TSBN-based models improves over the Gaussian (G-)RTRBM and the spike-slab (SS-)SRTRBM significantly.\nAnother popular motion capture dataset is the MIT dataset2. To further demonstrate the directed, generative nature of our model, we give our trained HMSBN model different initializations, and show generated, synthetic data and the transitions between different motion styles in Figure 3. These generated data are readily produced from the model and demonstrate realistic behavior. The smooth trajectories are walking movements, while the vibrating ones are running. Corresponding video files (AVI) are provided as mocap 1, 2 and 3 in the Supplementary Material."}, {"heading": "5.3 Polyphonic music dataset", "text": "The third experiment is based on four different polyphonic music sequences of piano [10], i.e., Piano-midi.de (Piano), Nottingham (Nott), MuseData (Muse) and JSB chorales (JSB). Each of these datasets are represented as a collection of 88-dimensional binary sequences, that span the whole range of piano from A0 to C8.\nThe samples generated from the trained HMSBN model are shown in Figure 2 (Middle). As can be seen, different styles of polyphonic music are synthesized. The corresponding MIDI files are provided as music 1 and 2 in the Supplementary Material. Our model has the ability to learn basic harmony rules and local temporal coherence. However, long-term structure and musical melody remain elusive. The variational lower bound, along with the estimated log-likelihood in [10], are presented in Table 3. The TSBN we implemented is of size 100 and order 1. Empirically, adding layers did not improve performance on this dataset, hence no such results are reported. The results of RNN-NADE and RTRBM [10] were obtained by only 100 runs of the annealed importance sampling, which has the potential to overestimate the true log-likelihood. Our variational lower bound provides a more conservative estimate. Though, our performance is still better than that of RNN.\n2Quantitative results on the MIT dataset are provided in Supplementary Section D.\nTable 3: Test log-likelihood for the polyphonic music dataset. ( ) taken from [10].\nMODEL PIANO. NOTT. MUSE. JSB. TSBN -7.98 -3.67 -6.81 -7.48 RNN-NADE -7.05 -2.31 -5.60 -5.56 RTRBM -7.36 -2.62 -6.35 -6.35 RNN -8.37 -4.46 -8.13 -8.71\nTable 4: Average prediction precision for STU. ( ) taken from [31].\nMODEL DIM MP PP HMSBN 25 0.327\u00b1 0.002 0.353\u00b1 0.070 DHMSBN-S 25-25 0.299\u00b1 0.001 0.378\u00b1 0.006 GP-DPFA 100 0.223\u00b1 0.001 0.189\u00b1 0.003 DRFM 25 0.217\u00b1 0.003 0.177\u00b1 0.010"}, {"heading": "5.4 State of the Union dataset", "text": "The State of the Union (STU) dataset contains the transcripts of T = 225 US State of the Union addresses, from 1790 to 2014. Two tasks are considered, i.e., prediction and dynamic topic modeling.\nPrediction The prediction task is concerned with estimating the held-out words. We employ the setup in [31]. After removing stop words and terms that occur fewer than 7 times in one document or less than 20 times overall, there are 2375 unique words. The entire data of the last year is held-out. For the documents in the previous years, we randomly partition the words of each document into 80%/20% split. The model is trained on the 80% portion, and the remaining 20% held-out words are used to test the prediction at each year. The words in both held-out sets are ranked according to the probability estimated from (6).\nTo evaluate the prediction performance, we calculate the precision @top-Mas in [31], which is given by the fraction of the top-M words, predicted by the model, that matches the true ranking of the word counts. M = 50 is used. Two recent works are compared, GP-DPFA [31] and DRFM [30]. The results are summarized in Table 4. Our model is of order 1. The column MP denotes the mean precision over all the years that appear in the training set. The column PP denotes the predictive precision for the final year. Our model achieves significant improvements in both scenarios.\nDynamic Topic Modeling The setup described in [30] is employed, and the number of topics is 200. To understand the temporal dynamic per topic, three topics are selected and the normalized probability that a topic appears at each year are shown in Figure 2 (Right). Their associated top 6 words per topic are shown in Table 5. The learned trajectory exhibits different temporal patterns across the topics. Clearly, we can identify jumps associated with some key historical events. For instance, for Topic 29, we observe a positive jump in 1986 related to military and paramilitary activities in and against Nicaragua brought by the U.S. Topic 30 is related with war, where the War of 1812, World War II and Iraq War all spike up in their corresponding years. In Topic 130, we observe consistent positive jumps from 1890 to 1920, when the American revolution was taking place. Three other interesting topics are also shown in Table 5. Topic 64 appears to be related to education, Topic 70 is about Iraq, and Topic 74 is Axis and World War II. We note that the words for these topics are explicitly related to these matters."}, {"heading": "6 Conclusion", "text": "We have presented the Deep Temporal Sigmoid Belief Networks, an extension of SBN, that models the temporal dependencies in high-dimensional sequences. To allow for scalable inference and learning, an efficient variational optimization algorithm is developed. Experimental results on several datasets show that the proposed approach obtains superior predictive performance, and synthesizes interesting sequences.\nIn this work, we have investigated the modeling of different types of data individually. One interesting future work is to combine them into a unified framework for dynamic multi-modality learning. Furthermore, we can use high-order optimization methods to speed up inference [32].\nAcknowledgements This research was supported in part by ARO, DARPA, DOE, NGA and ONR."}, {"heading": "A Outline of the NVIL algorithm", "text": "The outline of the NVIL algorithm for computing gradients are shown below (reproduced from [13]). C\u03bb(vt) represents the data-dependent baseline, and \u03b1 = 0.8 throughout the experiments.\nAlgorithm 1 Compute gradient estimates for the model parameters and recognition parameters.\n\u2206\u03b8 \u2190 0,\u2206\u03c6\u2190 0,\u2206\u03bb\u2190 0 L \u2190 0 for t\u2190 1 to T do ht \u223c q\u03c6(ht|vt) lt \u2190 log p\u03b8(vt,ht)\u2212 log q\u03c6(ht|vt) L \u2190 L+ lt lt \u2190 lt \u2212 C\u03bb(vt) end for cb \u2190 mean(l1, . . . , lT ) vb \u2190 variance(l1, . . . , lT ) c\u2190 \u03b1c+ (1\u2212 \u03b1)cb v \u2190 \u03b1v + (1\u2212 \u03b1)vb for t\u2190 1 to T do lt \u2190 lt\u2212cmax(1,\u221av) \u2206\u03b8 \u2190 \u2206\u03b8 +\u2207\u03b8 log p\u03b8(vt,ht) \u2206\u03c6\u2190 \u2206\u03c6+ lt\u2207\u03c6 log q\u03c6(ht|vt) \u2206\u03bb\u2190 \u2206\u03bb+ lt\u2207\u03bbC\u03bb(vt) end for"}, {"heading": "B Learning and Inference Details on TSBN", "text": "For t = 1, . . . , T , consider vt \u2208 {0, 1}M ,ht \u2208 {0, 1}J , the model parameters \u03b8 are specified as W1 \u2208 RJ\u00d7J , W2 \u2208 RM\u00d7J , W3 \u2208 RJ\u00d7M , W4 \u2208 RM\u00d7M , b \u2208 RJ , and c \u2208 RM . The generative model is expressed as\np(hjt = 1|ht\u22121,vt\u22121) = \u03c3(w>1jht\u22121 +w>3jvt\u22121 + bj) , (13)\np(vmt = 1|ht,vt\u22121) = \u03c3(w>2mht +w>4mvt\u22121 + cm) , (14) The recognition model is expressed as\nq(hjt = 1|ht\u22121,vt,vt\u22121) = \u03c3(u>1jht\u22121 + u>2jvt + u>3jvt\u22121 + dj) , (15) where the recognition parameters are specified as U1 \u2208 RJ\u00d7J , U2 \u2208 RJ\u00d7M , U3 \u2208 RJ\u00d7M , and d \u2208 RJ . h0 and v0, needed for p(h1), p(v1|h1) and q(h1|v1), are defined as zero vectors, for conciseness.\nIn order to implement the NVIL algorithm described in [13], we need to calculate the lower bound and also the gradients. Specifically, we have the variational lower bound L = \u2211T t=1 Eq\u03c6(h|v)[lt], where lt is expressed as\nlt = J\u2211 j=1 ( \u03c8 (1) jt hjt \u2212 log(1 + exp(\u03c8 (1) jt )) ) + M\u2211 m=1 ( \u03c8 (2) mtvmt \u2212 log(1 + exp(\u03c8 (2) mt)) ) (16)\n\u2212 [ J\u2211 j=1 ( \u03c8 (3) jt hjt \u2212 log(1 + exp(\u03c8 (3) jt )) )] ,\nand we have defined \u03c8\n(1) jt = w > 1jht\u22121 +w > 3jvt\u22121 + bj , (17)\n\u03c8 (2) mt = w > 2mht +w > 4mvt\u22121 + cm , (18)\n\u03c8 (3) jt = u > 1jht\u22121 + u > 2jvt + u > 3jvt\u22121 + dj . (19)\nBy further defining\n\u03c7 (1) jt = hjt \u2212 \u03c3(\u03c8 (1) jt ), \u03c7 (2) mt = vmt \u2212 \u03c3(\u03c8 (2) mt), \u03c7 (3) jt = hjt \u2212 \u03c3(\u03c8 (3) jt ), (20)\nThe gradients for the model parameters \u03b8 are expressed as \u2202 log p\u03b8(vt,ht)\n\u2202w1jj\u2032 = \u03c7\n(1) jt hj\u2032t\u22121,\n\u2202 log p\u03b8(vt,ht)\n\u2202w3jm = \u03c7\n(1) jt vmt\u22121,\n\u2202 log p\u03b8(vt,ht)\n\u2202bj = \u03c7\n(1) jt , (21)\n\u2202 log p\u03b8(vt,ht)\n\u2202w2mj = \u03c7\n(2) mthjt,\n\u2202 log p\u03b8(vt,ht)\n\u2202w4mm\u2032 = \u03c7\n(2) mtvm\u2032t\u22121,\n\u2202 log p\u03b8(vt,ht)\n\u2202cm = \u03c7\n(2) mt . (22)\nThe gradients for the recognition parameters \u03c6 are expressed as\n\u2202 log q\u03c6(ht|vt) \u2202u1jj\u2032 = \u03c7 (3) jt hj\u2032t\u22121,\n\u2202 log q\u03c6(ht|vt) \u2202u2jm = \u03c7 (3) jt vmt , (23)\n\u2202 log q\u03c6(ht|vt) \u2202u3jm = \u03c7 (3) jt vmt\u22121,\n\u2202 log q\u03c6(ht|vt) \u2202dj = \u03c7 (3) jt . (24)\nB.1 Modeling Real-valued Data\nWhen modeling real-valued data, we substitute (14) with p(vt|ht,vt\u22121) = N (\u00b5t, diag(\u03c32t )), where\n\u00b5mt = w > 2mht +w > 4mvt\u22121 + cm, log \u03c3mt = (w \u2032 2m) >ht + (w \u2032 4m) >vt\u22121 + c \u2032 m, (25)\nand we have W\u20322 \u2208 RM\u00d7J and W\u20324 \u2208 RM\u00d7M . The recognition model remains the same as in (15). Let \u03c4mt = log \u03c3mt, we obtain\nlt = J\u2211 j=1 ( \u03c8 (1) jt hjt \u2212 log(1 + exp(\u03c8 (1) jt )) ) \u2212 M\u2211 m=1 ( 1 2 log 2\u03c0 + \u03c4mt + (vmt \u2212 \u00b5mt)2 2e2\u03c4mt ) (26)\n\u2212 [ J\u2211 j=1 ( \u03c8 (3) jt hjt \u2212 log(1 + exp(\u03c8 (3) jt )) )] .\nAll the gradient calculation remains the same as (21)-(24), except the following.\n\u2202 log p\u03b8(vt,ht)\n\u2202w2mj = \u03c7\n(4) mthjt,\n\u2202 log p\u03b8(vt,ht)\n\u2202w4mm\u2032 = \u03c7\n(4) mtvm\u2032t\u22121,\n\u2202 log p\u03b8(vt,ht)\n\u2202cm = \u03c7\n(4) mt , (27)\n\u2202 log p\u03b8(vt,ht)\n\u2202w\u20322mj = \u03c7\n(5) mthjt,\n\u2202 log p\u03b8(vt,ht)\n\u2202w\u20324mm\u2032 = \u03c7\n(5) mtvm\u2032t\u22121,\n\u2202 log p\u03b8(vt,ht)\n\u2202c\u2032m = \u03c7\n(5) mt , (28)\nwhere we have defined\n\u03c7 (4) mt =\n\u2202 log p\u03b8(vt,ht) \u2202\u00b5mt = vmt \u2212 \u00b5mt e2\u03c4mt , \u03c7 (5) mt = \u2202 log p\u03b8(vt,ht) \u2202\u03c4mt = (vmt \u2212 \u00b5mt)2 e2\u03c4mt \u2212 1 . (29)\nB.2 Modeling Count Data\nWe also introduce an approach for modeling time-series data with count observations, by replacing (14) with p(vt|ht,vt\u22121) = \u220fM m=1 y vmt mt , where\nymt = exp(w>2mht +w > 4mvt\u22121 + cm)\u2211M\nm\u2032=1 exp(w > 2m\u2032ht +w > 4m\u2032vt\u22121 + cm\u2032)\n. (30)\nThe recognition model still remains the same as in (15). The lt now is expressed as\nlt = J\u2211 j=1 ( \u03c8 (1) jt hjt \u2212 log(1 + exp(\u03c8 (1) jt )) ) + M\u2211 m=1 ( \u03c8 (2) mtvmt \u2212 vmt log M\u2211 m\u2032=1 exp(\u03c8 (2) mt) ) (31)\n\u2212 [ J\u2211 j=1 ( \u03c8 (3) jt hjt \u2212 log(1 + exp(\u03c8 (3) jt )) )] .\nAll the gradient calculations remain the same as (21)-(24), except the following\n\u2202 log p\u03b8(vt,ht)\n\u2202w2mj = \u03c7\n(6) mthjt,\n\u2202 log p\u03b8(vt,ht)\n\u2202w4mm\u2032 = \u03c7\n(6) mtvm\u2032t\u22121,\n\u2202 log p\u03b8(vt,ht)\n\u2202cm = \u03c7\n(6) mt . (32)\nwhere we have defined \u03c7(6)mt = vmt \u2212 ymt \u2211M m\u2032=1 vm\u2032t."}, {"heading": "C Learning and Inference Details on Deep TSBN", "text": "For the ease of notation, we consider a two-hidden-layer deep TSBN here, which can be readily extended to a deep model with any depth. For t = 1, . . . , T , we consider the observation as vt \u2208 {0, 1}M . The top hidden layer is denoted as zt \u2208 {0, 1}J .\nC.1 Using stochastic hidden layer\nDenote the first stochastic hidden layer as ht \u2208 {0, 1}K . The generative model is expressed as\np(zjt = 1) = \u03c3(w > 1jzt\u22121 +w > 3jht\u22121 + b1j) , (33)\np(hkt = 1) = \u03c3(w > 2kzt +w > 4kht\u22121 +w > 6kvt\u22121 + b2k) , (34)\np(vmt = 1) = \u03c3(w > 5mht +w > 7mvt\u22121 + b3m) , (35)\nwhere we have defined W1 \u2208 RJ\u00d7J , W2 \u2208 RK\u00d7J , W3 \u2208 RJ\u00d7K , W4 \u2208 RK\u00d7K , W5 \u2208 RM\u00d7K , W6 \u2208 RK\u00d7M , and W7 \u2208 RM\u00d7M . The bias terms are b1 \u2208 RJ\u00d71, b2 \u2208 RK\u00d71 and b3 \u2208 RM\u00d71. The corresponding recognition model is expressed as\nq(hkt = 1) = \u03c3(u > 5kvt + u > 4kht\u22121 + u > 6kvt\u22121 + c2k) (36)\nq(zjt = 1) = \u03c3(u > 2jht + u > 1jzt\u22121 + u > 3jht\u22121 + c1j) (37)\nwhere the recognition parameters are specified as U1 \u2208 RJ\u00d7J , U2 \u2208 RJ\u00d7K , U3 \u2208 RJ\u00d7K , U4 \u2208 RK\u00d7K , U5 \u2208 RK\u00d7M and U6 \u2208 RK\u00d7M . The bias terms are c1 \u2208 RJ\u00d71 and c2 \u2208 RK\u00d71. Now, lt is expressed as\nlt = J\u2211 j=1 ( \u03c8 (1) jt zjt \u2212 log(1 + exp(\u03c8 (1) jt )) ) + K\u2211 k=1 ( \u03c8 (2) kt hkt \u2212 log(1 + exp(\u03c8 (2) kt )) )\n+ M\u2211 m=1 ( \u03c8 (3) mtvmt \u2212 log(1 + exp(\u03c8 (3) mt)) ) (38)\n\u2212 [ K\u2211 k=1 ( \u03c8 (4) kt hkt \u2212 log(1 + exp(\u03c8 (4) kt )) ) + J\u2211 j=1 ( \u03c8 (5) jt zjt \u2212 log(1 + exp(\u03c8 (5) jt )) )] ,\nand we have defined\n\u03c8 (1) jt = w > 1jzt\u22121 +w > 3jht\u22121 + b1j , (39)\n\u03c8 (2) kt = w > 2kzt +w > 4kht\u22121 +w > 6kvt\u22121 + b2k , (40)\n\u03c8 (3) mt = w > 5mht +w > 7mvt\u22121 + b3m , (41)\n\u03c8 (4) kt = u > 5kvt + u > 4kht\u22121 + u > 6kvt\u22121 + c2k , (42)\n\u03c8 (3) jt = u > 2jht + u > 1jzt\u22121 + u > 3jht\u22121 + c1j . (43)\nAll the gradients can be calculated readily as in (21)-(24).\nC.2 Using deterministic hidden layer\nFor the generative model, denote the deterministic hidden layer as hgt \u2208 RK . For the recognition model, denote the deterministic hidden layer as hrt \u2208 RK . W3 and U3 are set to be zero matrices for the ease of gradient calculation. The generative model is expressed as\np(zjt = 1) = \u03c3(w > 1jzt\u22121 + b1j) , (44)\nhgkt = f(w > 2kzt +w > 4kh g t\u22121 +w > 6kvt\u22121 + b2k) , (45)\np(vmt = 1) = \u03c3(w > 5mht +w > 7mvt\u22121 + b3m) , (46)\nThe gradients w.r.t. W1,W5,W7,U1 and U2 can be calculated easily. In order to calculate the gradients w.r.t. W2,W4,W6,U4,U5 and U6, we need to obtain \u2202L\u2202hg kt and \u2202L \u2202hr kt , which can be calculated recursively via the back-propagation through time algorithm. Specifically, \u2202L \u2202h\ng kt\n= \u2202Q1 \u2202h\ng kt\nand we have defined\nQ1 = T\u2211 t=1 M\u2211 m=1 ( \u03c8 (2) mtvmt \u2212 log(1 + exp(\u03c8 (2) mt)) ) . (53)\nWe observe thatQ1 can be computed recursively using\nQt = T\u2211 \u03c4=t M\u2211 m=1 ( \u03c8(2)m\u03c4vm\u03c4 \u2212 log(1 + exp(\u03c8(2)m\u03c4 )) ) (54)\n= Qt+1 + M\u2211 m=1 ( \u03c8 (2) mtvmt \u2212 log(1 + exp(\u03c8 (2) mt)) ) , (55)\nwhereQT+1 = 0. Using the chain rule, we have\n\u2202Qt \u2202hgkt = \u2211 k\u2032 \u2202Qt+1 \u2202hgk\u2032t+1 \u00b7 \u2202hgk\u2032t+1 \u2202hgkt + M\u2211 m=1 w5mk(vmt \u2212 \u03c3(\u03c8(2)mt)) (56)\n= \u2211 k\u2032 \u2202Qt+1 \u2202hgk\u2032t+1 \u00b7 f \u2032(\u03c8(4)k\u2032t+1)w4k\u2032k + M\u2211 m=1 w5mk(vmt \u2212 \u03c3(\u03c8(2)mt)) , (57)\nwhere we have defined \u03c8\n(4) kt = w > 2kzt +w > 4kh g t\u22121 +w > 6kv g t\u22121 + b2k , (58)\nand\n\u2202QT \u2202hgkt = M\u2211 m=1 w5mk(vmT \u2212 \u03c3(\u03c8(2)mT )) . (59)\n\u2202L \u2202hr\nkt can be calculated similarly.\nTable 7: Average prediction error obtained for the MIT motion capture dataset.\nMODEL PRED. ERR. DTSBN-S 3.71 \u00b1 0.03 DTSBN-D 4.19 \u00b1 0.01 TSBN 3.86 \u00b1 0.02 HMSBN 17.49 \u00b1 0.20"}, {"heading": "D Additional Results", "text": "D.1 Generated Data\nThe generated, synthetic motion capture data, and polyphonic music data can be downloaded from https: //drive.google.com/drive/u/0/folders/0B1HR6m3IZSO_SWt0aS1oYmlneDQ.\nD.2 Bouncing balls dataset\nAdditional experimental results are shown in Table 6. AR represents an auto-regressive Markov model without latent variables.\nD.3 MIT motion capture dataset\nWe randomly select 10% of the dataset as the test set. Quantitative results are shown in Table 7."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "Deep dynamic generative models are developed to learn sequential dependencies<lb>in time-series data. The multi-layered model is designed by constructing a hierar-<lb>chy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack<lb>of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state,<lb>inherited from the previous SBNs in the sequence, and is used to regulate its hid-<lb>den bias. Scalable learning and inference algorithms are derived by introducing<lb>a recognition model that yields fast sampling from the variational posterior. This<lb>recognition model is trained jointly with the generative model, by maximizing its<lb>variational lower bound on the log-likelihood. Experimental results on bouncing<lb>balls, polyphonic music, motion capture, and text streams show that the proposed<lb>approach achieves state-of-the-art predictive performance, and has the capacity to<lb>synthesize various sequences.", "creator": "LaTeX with hyperref package"}}}