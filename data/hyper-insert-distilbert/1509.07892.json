{"id": "1509.07892", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2015", "title": "Evasion and Hardening of Tree Ensemble Classifiers", "abstract": "ultra recent work has usually successfully constructed partial adversarial \" evading \" instances usable for differentiable prediction matching models. besides however strictly generating adversarial reconstruction instances for tree compression ensembles, a piecewise constant empirical class of models, has remained an quite open research problem. in supporting this paper, we construct together both exact and least approximate ensembles evasion algorithms for tree ensembles : \\ for a given instance x then we find the \" nearest \" maximal instance x'such performance that comparing the perfect classifier predictions algorithm of cases x and x'similarly are different. ( first, together we nevertheless show that finding such instances is practically randomly possible despite two tree optimal ensemble numerical models being non - _ differentiable and the optimal ensemble evasion scheme problem being np - tree hard.", "histories": [["v1", "Fri, 25 Sep 2015 20:57:35 GMT  (317kb,D)", "http://arxiv.org/abs/1509.07892v1", "9 pages, 9 figures"], ["v2", "Fri, 27 May 2016 01:09:22 GMT  (891kb,D)", "http://arxiv.org/abs/1509.07892v2", "11 pages, 7 figures, Appears in Proceedings of the 33rd International Conference on Machine Learning (ICML), New York, NY, USA, 2016. JMLR: W&amp;CP volume 48"]], "COMMENTS": "9 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CR stat.ML", "authors": ["alex kantchelian", "j d tygar", "anthony d joseph"], "accepted": true, "id": "1509.07892"}, "pdf": {"name": "1509.07892.pdf", "metadata": {"source": "CRF", "title": "Evasion and Hardening of Tree Ensemble Classifiers", "authors": ["Alex Kantchelian", "J. D. Tygar", "Anthony D. Joseph"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks (DNN) represent a prominent success of machine learning. These models can successfully and accurately address difficult learning problems, including classification of audio, video, and natural language possible where previous approaches have failed. Yet, the existence of adversarial \u201cevading\u201d instances for the current incarnation of DNNs [1] shows a perhaps surprising brittleness: for virtually any instance x that the model classifies correctly, it is possible to find a negligible perturbation \u03b4 such that x + \u03b4 evades being correctly classified, that is, receives a (sometimes widly) inaccurate prediction.\nThe general study of the evasion problem matters for at least two reasons, one of which is conceptual, the other practical. First, we expect a high-performance learning algorithm to generalize well and be hard to evade: only a \u201clarge enough\u201d perturbation \u03b4 should be able to alter its decision. The existence of small-\u03b4 evading instances shows a defect in the generalization ability of the model, and intuitively calls for either a different model class, or additional model regularization. Second, machine learning is becoming the workhorse of security-oriented applications, the most prominent example being spam email filtering. In those applications, the attacker has a large incentive for finding evading instances. For example, a spammer is interested in finding a small, cheap, set of changes to his emails to get them through the machine learning detector. Thus, showing that only a large enough \u03b4 will cause a change in the prediction implies that the model is immune to practical evasion attacks.\nWhile prior work extensively studies the evasion problem on differentiable models by means of gradient descent or other approximation heuristics, those results are reported in an essentially quali-\nar X\niv :1\n50 9.\n07 89\n2v 1\n[ cs\n.L G\n] 2\n5 Se\np 20\n15\ntative fashion, implicitly defaulting the choice of metric for measuring \u03b4 to the L2 norm. Moreover, only limited and sporadic qualitative comparisons between models from different classes and algorithms are available. However, the largest missing piece on evasion is the conspicuous absence of attacks for non-differentiable, non-continuous models. Tree sum-ensembles as produced by boosting or bagging are perhaps the most important models from this class as they are often able to achieve competitive performance and enjoy good adoption rates in both industrial and academic contexts [2].\nIn this paper, we develop both exact and approximate evasion attacks for sum-ensemble of trees. In the exact (or optimal) evasion attack, we compute the smallest \u03b4 according to a given metric such that the model misclassifies x + \u03b4. The approximate attack does not guarantee optimality but runs faster. We apply these to a concrete handwritten digit classification task. Our exact attack relies on a reduction to Mixed Integer Programming and enables precise quantitative L0, L1, L2 and L\u221e robustness comparisons with several other models for which we can compute optimal attacks: L1 and L2 regularized logistic regression and max-ensemble of linear classifiers (shallow max-out network). We also provide L1, L2 and L\u221e robustness upper-bounds for a 3 hidden layer, sigmoidal activation function neural network and a classic Gaussian kernel SVM. The comparison shows that despite having one of the best classification accuracies, tree ensembles as learned by gradient boosting are consistently the most brittle models for L1, L2 and L\u221e distances.\nFinally, our approximate attack is based on a fast coordinate descent scheme and enables quick generation of evading instances. We use this ability to generate more than 500, 000 L0-attack instances and retrain a gradient boosted tree model on those. This produces a dramatically hardened model: the median number of pixels modifications required for a mislabeling improves from the original 8 to 30 in the retrained model. Additionally, the retraining leads to an increase in testing accuracy, so that retraining on adversarial instances can be understood as a form of model regularization."}, {"heading": "2 Related Work", "text": "Following a taxonomy introduced in Barreno et al. [3], evasion attacks are part of the larger family of exploratory attacks which occur at testing time, for a fixed classifier. Such attacks have been considered from the onset of the adversarial machine learning subfield. For example, Lowd [4] describes an attack on linear spam filters, where the spammer finds and appends enough benign-weighted terms to avoid detection. Similarly, Srndic et al. [5] present and evaluate several concrete evasion attacks for a malware detection system based on linear and RBF-SVM under different adversary power assumptions. Additionally, a number of researchers have considered the theoretical aspect of evasion. Lowd et al. [6] investigate evasion of linear models when the models are kept secret but queries are allowed. Nelson et al. [7] later extend this analysis to any model that induces a convex decision boundary in a continuous feature space. More recently, Fawzi et al. [8] provide theoretical results on the robustness of linear and quadratic models when the models are fully known.\nIn this paper, we forgo application-specific feature extraction and mapping by working on a direct pixel-to-feature mapping for our handwritten digit recognition benchmark learning task. Similar to Fawzi et al. [8], we do not limit the amount of information available to the adversary. Our goal is to establish the intrinsic evasion robustness of the machine learning models themselves, and thus provide a guaranteed lower-bound on the attacker effort in the worst-case.\nCloser to our work, Biggio et al. [9] introduce evasion as a constrained optimization problem and proceed to compute L1-evading instances for RBF-SVM using a projected gradient descent method. We effectively use a similar method to compute evasion attacks for the differentiable models where no optimal evasion algorithm is known: RBF-SVM and (sigmoidal) deep neural network. Those provide the comparison ground for our main result regarding boosted trees. Szegedy et al. [1] use a quasi-Newtonian method to first demonstrate that L2-small perturbations on image recognition tasks significantly impact state-of-the-art deep learning classifiers. This result is later refined in Goodfellow et al. [10], showing that for large-dimensional image data, a single small gradient step on the model score is sufficient to obtain an evading instance. This observation supports the authors\u2019 view that evasion is possible only because practical deep neural networks behave too linearly. Using the single-step strategy, the authors retrain a model on both original and evading instances, and show that both test accuracy and robustness are improved. In this paper, we show a similar positive effect of retraining on L0-evading instances for boosted trees. To do so, we develop both optimal and approximate gradient-free attack algorithms for tree ensemble models. Unlike Goodfellow et\nal. [10], we show that retraining tree ensembles significantly strengthens the resulting model. We also demonstrate that despite their extreme non-linearity, boosted trees are even more susceptible to evasion attacks than deep neural networks.\nFinally, we mention two related works from the deep neural network community. Nguyen et al. [11] study a version of evasion where the small \u03b4 constraint is altogether dropped and replaced by a \u201cregularity\u201d constraint on the attack instance. They show that it is possible to generate well structured but meaningless images which obtain very certain predictions on state-of-the-art DNN models. On the counter-measures side, Gu et al. [12] show some promising results by augmenting DNNs with a pre-filtering step based on a form of contractive auto-encoding. In this paper, we develop a fast evasion attack for tree ensembles which enables investigating the viability of retraining on adversarial instances to increase model robustness. Remarkably, retraining does produce significantly more robust models without loss of accuracy."}, {"heading": "3 Evading Tree Ensemble Models", "text": "Let m : X \u2192 Y (model) be a classifier. For a given instance x \u2208 X and a given distance function d : X \u00d7 X \u2192 R+, the optimal evasion problem is:\nminimize x\u2032\u2208X\nd(x, x\u2032) subject to m(x) 6= m(x\u2032) (1)\nIn this paper, we focus on binary classifiers Y = {\u22121, 1} defined over an n-dimensional feature space X \u2282 Rn. Setting the classifier m aside, the distance function d fully specifies (1), hence we can talk about d-adversarial instances, or d-robustness. As different measures of instance deformation lead to different solutions, we present results for four representative distances. We briefly describe those and their typical effects on the solution of (1).\n\u2022 The L0 distance \u2211n\ni=1 Ixi 6=x\u2032i , or Hamming distance encourages the sparsest, most localized adversarial deformations with arbitrary magnitude.\n\u2022 The L1 distance \u2211n\ni=1 |xi \u2212 x\u2032i| encourages localized adversarial deformations and additionally controls for their magnitude.\n\u2022 The L2 distance \u221a\u2211n\ni=1(xi \u2212 x\u2032i)2 encourages less localized but small adversarial deformations.\n\u2022 the L\u221e distance maxi |xi\u2212x\u2032i| encourages uniformly spread adversarial deformations with the smallest possible magnitude.\nNote that whenm is taken to be a thresholded sum-ensemble of trees, (1) becomes NP-hard. Indeed, 3-SAT is polynomially-time reducible to (1)as follows: first, let x\u2032 represent the assignment of values to the variables of the 3-SAT instance. Construct m by arranging each clause as a binary decision tree where the decision nodes are the variables and all leaf predictions are 0 except for the leaf in the path corresponding to the truthness of the initial clause. Set the prediction of this leaf to 1. The model has as many decision trees as clauses in the 3-SAT instance, and each tree has a maximal depth of 3. Set the threshold of m to the number of trees, so that m(x) = 1 if and only if all trees predict 1, which correspond to all clauses being satisfied. Finally, remove the objective of (1) and set x to all 0s (if m(x) = 1, we are done).\nWe describe two approaches for generating evading instances on sum-ensembles of trees. The first approach solves the optimal problem (1) by reformulation into a Mixed Integer Program (MIP). The resulting program can be subsequently solved by an off-the-shelf MIP solver. This method guarantees optimality of the solution and makes quantitative statements about model robustness possible. As a faster alternative to the exact method, we develop a second approach based on coordinate descent. Although this method does not guarantee optimality, we use it to generate evading instances quickly for the retraining experiment."}, {"heading": "3.1 Optimal Evasion Attack", "text": "We present a reduction of problem (1) into mixed integer programming when m is taken to be a binary classifier derived from a thresholded at 0 sum of regression trees T . Specifically, let M :\nX \u2192 R be the signed margin prediction of the model, that is, m(x) = 1 \u21d4 M(x) \u2265 0. M is simply the sum of each regression tree T \u2208 T . In turn, each regression tree T defines a partition PT of X where each domain L \u2208 PT is associated with a prediction value vL \u2208 R. Let J be the maximum tree depth in the model. As those domains actually correspond to the leafs of T , each domain L is defined as the conjunction of at most J atomic propositions l \u2208 L of the form \u201cxi < t\u201d or \u201cxi \u2265 t\u201d for some feature index i and some threshold t \u2208 R. Thus, M can be written as a weighted sum of indicator functions defined as conjunctions of atomic propositions. We have:\nM(x) = \u2211 T\u2208T T (x) = \u2211 T\u2208T \u2211 L\u2208PT vLI {\u2227 l\u2208L x \u2208 l } (2)\nwhere each domain l is of the form {x|xi < t} or its complement X \\ {x|xi < t}. Our reduction to MIP is based on Form (2). In what follows, we present the mixed integer program by defining four groups of MIP variables. We then model (2) by the atom and rule consistency families of constraints. The model mislabel constraint enforces the condition thatm(x) 6= m(x\u2032) in (1). Finally we reduce the objective of (1) to a set of constraints and an objective in MIP Objective.\nProgram Variables Our reduction uses four families of variables. For clarity, every MIP variable is bolded.\n\u2022 n continuous variables (fi)i (features) encoding the values of the coordinates of the feature vector x\u2032\n\u2022 (\u2211 T\u2208T \u2211 L\u2208Pt 1 )\nbinary variables (ri)i (rules) encoding the values of the indicator functions. Each binary decision tree with maximal depth J has at most 2J leaves, so that there can be no more than |T |2J such variables. \u2022 At most (\u2211 T\u2208T \u2211 L\u2208PT |L| ) binary variables (ai)i (atoms) encoding the truth values of\nall atomic propositions. Our implementation avoids creating redundant a variables by detecting equivalent or negation of existing atoms (replaced by 1 \u2212 a) across all trees of the model. Because each binary decision tree with maximal depth J has at most 2J\u22121 decision nodes, there can be no more than |T |(2J \u2212 1) such variables.\n\u2022 At most n + 1 continuous or binary variables (bi)i (bounds) for expressing the distance d(x, x\u2032) of problem (1). These variables are first used in the MIP Objective paragraph.\nWe enforce the logical equivalence between the reduction and the semantics of M by the following constraints.\nAtom consistency Without loss of generality, consider an atom of the form x\u2032i < t. Let fi be the feature variable corresponding to x\u2032i and a \u2208 {0, 1} the atom variable corresponding to the truthness of the atom. The following two linear constraints are such that a = 1\u21d4 x\u2032i < t: t \u2217 (1\u2212 a) + fmini \u2217 a \u2264 fi \u2264 t \u2217 a\u2212 (3) where > 0 is small enough and fmini is a constant representing the smallest possible value for feature i. Rule consistency Let an indicator function indL(x\u2032) = I {\u2227 l\u2208L x \u2032 \u2208 l } , its associated rule variable ri \u2208 {0, 1} and atom variables a1, . . . , a|L| corresponding to the truthness of the atomic propositions (x\u2032 \u2208 l)l. The following |L|+ 1 constraints are such that ri = 1\u21d4 indL(x\u2032) = 1:\n\u2200k \u2208 {1, . . . , |L|}, ri \u2264 ak and ri \u2265 |L|\u2211 k=1 ak \u2212 |L|+ 1 (4)\nModel mislabel Without loss of generality, consider an original instance x such that M(x) < 0. In order for x\u2032 to be an evading instance, we must have M(x\u2032) \u2265 0. This condition is captured by the following constraint: \u2211\nT\u2208T \u2211 L\u2208PT vL \u2217 rL \u2265 0 (5)\nwhere for notational brevity we have indexed the rule variables (ri)i by their corresponding domains L.\nMIP Objective Finally, we need to translate the objective of problem (1) into an MIP form. In this paper, we consider the four classic distance metrics for d: L0, L1, L2 and L\u221e. We start with the L1 case, and explain how to obtain the objective from there in the other cases. Let n (non-negative) continuous variables (bi)i (bounds). We have \u2016x \u2212 x\u2032\u20161 = \u2211 1\u2264i\u2264n bi under the following 2n constraints: \u2200i \u2208 {1, . . . , n}, \u2212bi \u2264 fi \u2212 xi \u2264 bi (6)\nTo obtain the L0 case, it suffices to make all b variables binary and replace the above constraints by\n\u2200i \u2208 {1, . . . , n}, \u2212(fmaxi \u2212 fmini ) \u2217 bi \u2264 fi \u2212 xi \u2264 (fmaxi \u2212 fmini ) \u2217 bi (7) To obtain the L\u221e case, we can introduce an additional continuous variable B and n additional constraints such that \u2016x\u2212 x\u2032\u2016\u221e = B:\n\u2200i \u2208 {1, . . . , n}, bi \u2264 B (8) Finally, to obtain the L2 case, we need only to change the objective to the quadratic form\u2211\n1\u2264i\u2264n b 2 i = \u2016x \u2212 x\u2032\u201622, at the cost of turning our otherwise purely linear formulation into a\nharder to solve Mixed Integer Quadratic Problem (MIQP).\nIn summary, this is a relatively compact reduction for problem (1). For a model which consists of |T | trees of maximal depth J , the model size is in O(|T |2J) and our formulation contains at most 2|T |2J + 2n \u2212 |T | variables and at most (3 + J)|T |2J + 2n \u2212 2|T | + 1 constraints. Note that the exponential dependency on J is not an issue for practical purposes as tree ensembles and in particular boosted trees are predominantly used with small tree depths, e.g., J \u2264 10."}, {"heading": "3.2 Approximate Evasion Attack", "text": "While the above reduction of problem (1) to an MIP is linear in the size of the model m, the actual solving time can be anywhere from a few seconds to 30 minutes, even for the state-of-the-art industrial solver we use [13]. We thus develop an approximate evasion algorithm to generate good quality evading instances.\nOur approximate evasion attack is based on the following iterative coordinate descent procedure: let B > 0 be the maximal amount of adversarial deformation we are willing to tolerate. Without loss of generality, suppose M(x) < 0. Starting from the original instance x\u2032 \u2190 x, we compute at each iteration the best change of a single dimension of x\u2032 so that M(x\u2032) increases. The algorithm terminates when d(x, x\u2032) > B. The critical part of this approach is finding the best dimension of x\u2032 to change at every round. Formally, we want to solve the following problem:\nmaximize \u2206\u2208Rn\nM(x\u2032 + \u2206) subject to \u2016\u2206\u20160 = 1 (9)\nBecause solving (9) is often expensive in practice, we decouple finding the best dimension (i such that \u2206i 6= 0) and finding the optimal step size \u2206i. Thus, we instead compute the optimal dimension for a fixed step \u03b1 \u2208 R. We consider the following \u201cdiscretized gradient\u201d problem:\nmaximize 1\u2264i\u2264n\nM(x\u2032 + \u03b1u) subject to ui = 1 and \u2200j 6= i, uj = 0 (10)\nA naive approach to (10) is to computeM(x\u2032+\u03b1u) for all n dimensions. Unfortunately, this solution becomes too slow in medium or large dimensional feature spaces. We present a fast algorithm for solving (10) which takes advantage of the structure of tree ensemble models. The idea is to consider the prediction path for each tree of the ensemble, i.e. the sequence of decision nodes from the root node down to the leaf node holding the prediction value. As each decision node references a single dimension, it is easy to check whether perturbing this dimension by amount \u03b1 would produce a different branching decision. If the perturbed decision is different from the original decision, then we follow both resulting prediction paths and update the gradient value of the perturbed dimension by the difference between the perturbed and original predictions.\nAlgorithm 1 formalizes this idea. Each decision tree is represented by its root node. A node object can be either a leaf node or a decision node. Decision nodes have four attributes: node.f and node.thres represent the feature index i and the corresponding threshold t of the decision atom \u201cxi < t\u201d. node.left points to the subtree where the atom is true, node.right correspondingly points to the false case. Leaf nodes have a single attribute, node.prediction holding the float prediction value.\nAlgorithm 1 Fast discretized gradient for ensemble of trees model.\nfunction GRAD(model, x, \u03b1) g \u2190 [0, 0, . . . , 0] (vector of n zeros) for treeRoot \u2208 model do\nTREEGRAD(treeRoot, x, \u03b1, g, \u2205) return g\nfunction TREEGRAD(node, x, \u03b1, g, considered) if node.isLeaf() then return original\u2190 (x[node.f] < node.thres) if original then nextNode\u2190 node.left else nextNode\u2190 node.right if node.f 6\u2208 considered then\npert\u2190 (x[node.f] + \u03b1 < node.thres) if original 6= pert then\nif pert then nextNodeP\u2190 node.left else nextNodeP\u2190 node.right h\u2190PREDICT(nextNode, x) x[node.f]\u2190 x[node.f] + \u03b1 h\u2190 h+PREDICT(nextNodeP, x) x[node.f]\u2190 x[node.f]\u2212 \u03b1 g[node.f]\u2190 g[node.f] + h considered\u2190 considered\u222a {node.f}\nTREEGRAD(nextNode, x, \u03b1, g, considered)\nfunction PREDICT(node, x) if node.isLeaf() then return node.prediction if x[node.f] < node.thres then\nreturn PREDICT(node.left, x) return PREDICT(node.right, x)\nThe fast discrete gradient computation runs in O(|T |J2 +n): TREEGRAD is called at most J times, each call of TREEGRAD calls PREDICT at most twice, and PREDICT recurses at most J times. In contrast, a naive implementation of GRAD would run in O(n|T |J). As J \u221a n, this approach offers considerable speed-up.\nWe use this method to generate a large amount of L0-adversarial instances for the retraining experiment below in subsection 4.2. Specifically, we compute at each round the discretized gradients for all \u03b1 \u2208 {\u22121,\u22122\u22121, . . . ,\u22122\u22128, 2\u22128, . . . , 2\u22121, 1} and select the best combination of dimension and \u03b1 to change. The algorithm terminates once the budget of dimensions to modify is exhausted."}, {"heading": "4 Results", "text": "We turn to the experimental evaluation of the robustness of tree ensembles using the above two attack strategies. We start by describing the evaluation dataset and our choice of models for benchmarking purposes, before moving to a quantitative comparison of the robustness of boosted tree models against a garden variety of learning algorithms. We finally show that the brittleness of boost trees can be effectively addressed by successive iterations of model retraining on evading instances.\nOur running binary classification task is to distinguish between handwritten digits \u201c2\u201d and \u201c6\u201d. To this end, we use the subsets of digits labeled \u201c2\u201d or \u201c6\u201d in the training and testing datasets of MNIST [14]. Our training and testing sets respectively include 11,876 and 1,990 images and each image has 28 \u00d7 28 gray scale pixels (the gray scale has 256 different possible values). So we have n = 784 and X = [0, 1]n. In addition to these two sets, we create an evaluation dataset of a hundred instances from the testing set such that all instances are correctly classified by all of the benchmarked models. These correctly classified instances are to serve the purpose of x, the starting point instances in the evasion problem (1).\nWe choose to use second order gradient boosting (denoted BDT) in the XGBoost [15] implementation as the tree ensemble learner because of its outstanding performance as an off-the-shelf general purpose learning algorithm. Our model uses one thousand trees of maximal depth 4, with a learning rate of 0.05. We also include the classic L1/L2regularized logistic regressions (denoted Lin. L1 and Lin. L2) and RBF-SVM trained using LibLinear [16] and LibSVM [17] respectively. We introduce two more learning models for benchmark purposes.\nTo compare the robustness of boosted trees, we contrast it with a deep neural network. We use Theano [18] to implement a sigmoidal (tanh) activation network with three\nhidden layers (denoted NN) in a 784-40-30-20-1 architecture, and train it by gradient descent for a top logistic regression layer, without pre-training nor drop-out. Finally, our last benchmark model is the equivalent of a shallow neural network made of two max-out units (one unit for each class) each made of thirty linear classifiers. This model corresponds to the difference of two Convex\nPolytope Machines [19] (one for each class) and we use the authors\u2019 implementation (CPM). Two factors motivates the choice of CPM. First, previous work has theoretically considered the evasion robustness of such ensemble of linear classifiers and deemed the problem NP-hard in the general case [20]. Second, unlike RBF-SVM and NN, this model can be readily reduced to a Mixed Integer Program, enabling optimal attacks thanks to a MIP solver. As the reduction is considerably simpler than the one presented for tree ensembles above in subsection 3.1, we omit it here. Finally, we use Gurobi [13] as the MI(Q)P solver. As our main goal is not to compare model accuracies, but rather to obtain the best possible model for each model class, we train on the training set alone and simply tune the hyper-parameters so as to minimize the error on the testing set. Figure 2 shows the testing errors of the learned models. BDT, RBF-SVM and CPM have the lowest indistinguishable error rate, NN closely follows, and the linear models are far behind, with the L1 regularized classifier being slightly worse than its L2 counterpart.\n4.1 Robustness\nFor each of the 6 learned models, and for all of the 100 correctly classified testing instance, we compute optimal or best effort evasion attacks under the deformation metrics. The evasions for all linear models and metrics are optimally computed by the MIP solver. We also reduce all evasions for CPM and BDT to a MIP formulation but can only solve to optimality for the L0, L1 and L\u221e metrics. For the quadratic L2-evasion, we set a time limit of one hour and use the best solution computed in that time. The time out is always reached. We use a classic projected gradient descent method for solving the L1, L2 and L\u221e evasions of NN and RBF-SVM, but do not attempt to address the combinatorially hard L0-evasion in this paper. Figure 3 summarizes the obtained adversarial bounds as one boxplot for each combination of model and distance. Although BDT gives the best testing accuracy, it systematically ranks last for robustness, with all of the metrics we consider. Remarkably negligible L1 or L2 perturbations suffice to evade BDT. On the other end, RBF-SVM is apparently the hardest model to evade, agreeing with results from [10]. While NN and CPM perform generally on par, the L1-regularized linear model exhibits significantly more brittleness than its L2 counterpart. This phenomenon is explained by large weights concentrating in specific dimensions as a result of sparsity. Thus, small modifications in the heavily weighted model dimensions result in potentially large classifier output variations."}, {"heading": "4.2 Robustification", "text": "We demonstrate that it is possible to significantly improve the robustness of the BDT model by retraining on adversarial instances. We iteratively generate L0-adversarial instances for the current model and for all the 11,876 original training instances using the approach outlined in subsection 3.2, with maximum modification budget B = 10 pixels. We then train on the original training dataset augmented with all of the adversarial but correctly labeled instances to obtain the next iteration model. We keep the model hyper-parameters constant at 1,000 trees with maximal depth 4. We perform this procedure for 60 rounds to generate more than 700,000 additional training instances, 60 times more adversarial instances than original instances. We note that our fast attack finds a good quality evading instance in less than a second, while solving the L0 problem to optimality in the exact approach can take up to several hours for the models from the later iterations. Figure 4a shows the exact L0 robustness bounds for the models where this can be computed. Figure 4b shows the evolution of the optimal L0-adversarial bound as retraining occurs. While the first few retraining rounds produce more brittle models, the final model enjoys a median lower-bound of 30 modified pixels, where as few as 8 pixels were initially sufficient to evade the classifier. This is also about twice as good as the CPM bound. We can also observe in figure 4c that the retrained models tend to make fewer testing errors than the original model. To see if the gained robustness against L0 deformations translates into robustness against other deformations, we also measured the L1, L2 and L\u221e robustness bounds. Unfortunately, we found significantly lower scores on all three metrics compared to the original model: hardening against L0 attacks made the model more sensitive to all other types of attacks."}, {"heading": "5 Conclusion", "text": "Gradient boosted trees are simultaneously among the most accurate learning models and the easiest ones to evade. This might have problematic implications when those algorithms are fielded in adversarial settings. On the other hand, boosted trees can be successfully hardened against L0-evasion attacks by successive retraining. The retraining strategy also improves model accuracy and can thus be understood as a form of model regularization. In future research, it would be interesting to investigate how the tree ensembles generated by random forests compare to boosted decision trees on the robustness benchmark. Investigating if boosted trees can simultaneously be hardened against all four types of attacks is also an interesting open problem. Finally, capitalizing on the regularization effect of retraining adversarial instances, a significant breakthrough would be to achieve a similar regularization effect without having to generate new training instances and unduly expand already large-scale training sets. Thus, a lightweight adversarial regularization method has the potential to further increase the accuracy of boosted trees and make them potentially more suitable to security-sensitive application domains."}], "references": [{"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Second place winner entry for the Higgs Boson Machine Learning Kaggle Challenge. https://github.com/TimSalimans/HiggsML", "author": ["Tim Salimans"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Can machine learning be secure", "author": ["Marco Barreno", "Blaine Nelson", "Russell Sears", "Anthony D. Joseph", "J.D. Tygar"], "venue": "In Proceedings of the 2006 ACM Symposium on Information, Computer and Communications Security, ASIACCS \u201906,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Good word attacks on statistical spam filters", "author": ["Daniel Lowd"], "venue": "In Proceedings of the Second Conference on Email and Anti-Spam (CEAS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Practical evasion of a learning-based classifier: A case study", "author": ["Nedim Srndic", "Pavel Laskov"], "venue": "In Proceedings of the 2014 IEEE Symposium on Security and Privacy, S&P", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Adversarial learning", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, KDD", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Query strategies for evading convex-inducing classifiers", "author": ["Blaine Nelson", "Benjamin I.P. Rubinstein", "Ling Huang", "Anthony D. Joseph", "Steven J. Lee", "Satish Rao", "J.D. Tygar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Analysis of classifiers robustness to adversarial perturbations", "author": ["Alhussein Fawzi", "Omar Fawzi", "Pascal Frossard"], "venue": "arXiv preprint,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Evasion attacks against machine learning at test time. In Machine Learning and Knowledge Discovery in Databases, volume 8190 of Lecture Notes in Computer Science", "author": ["Battista Biggio", "Igino Corona", "Davide Maiorca", "Blaine Nelson", "Nedim rndi", "Pavel Laskov", "Giorgio Giacinto", "Fabio Roli"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "venue": "arXiv preprint,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A Nguyen", "J Yosinski", "J Clune"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["Shixiang Gu", "Luca Rigazio"], "venue": "arXiv preprint,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "XGBoost: eXtreme Gradient Boosting", "author": ["Tianqi Chen", "Tong He"], "venue": "https://github. com/dmlc/xgboost", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "LIBLIN- EAR: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Large-margin convex polytope machine", "author": ["Alex Kantchelian", "Michael C. Tschantz", "Ling Huang", "Peter L. Bartlett", "Anthony D. Joseph", "J.D. Tygar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "On the hardness of evading combinations of linear classifiers", "author": ["David Stevens", "Daniel Lowd"], "venue": "In Proceedings of the 2013 ACM Workshop on Artificial Intelligence and Security, AISec", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Yet, the existence of adversarial \u201cevading\u201d instances for the current incarnation of DNNs [1] shows a perhaps surprising brittleness: for virtually any instance x that the model classifies correctly, it is possible to find a negligible perturbation \u03b4 such that x + \u03b4 evades being correctly classified, that is, receives a (sometimes widly) inaccurate prediction.", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "Tree sum-ensembles as produced by boosting or bagging are perhaps the most important models from this class as they are often able to achieve competitive performance and enjoy good adoption rates in both industrial and academic contexts [2].", "startOffset": 237, "endOffset": 240}, {"referenceID": 2, "context": "[3], evasion attacks are part of the larger family of exploratory attacks which occur at testing time, for a fixed classifier.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "For example, Lowd [4] describes an attack on linear spam filters, where the spammer finds and appends enough benign-weighted terms to avoid detection.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "[5] present and evaluate several concrete evasion attacks for a malware detection system based on linear and RBF-SVM under different adversary power assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] investigate evasion of linear models when the models are kept secret but queries are allowed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] later extend this analysis to any model that induces a convex decision boundary in a continuous feature space.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] provide theoretical results on the robustness of linear and quadratic models when the models are fully known.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8], we do not limit the amount of information available to the adversary.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] introduce evasion as a constrained optimization problem and proceed to compute L1-evading instances for RBF-SVM using a projected gradient descent method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] use a quasi-Newtonian method to first demonstrate that L2-small perturbations on image recognition tasks significantly impact state-of-the-art deep learning classifiers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10], showing that for large-dimensional image data, a single small gradient step on the model score is sufficient to obtain an evading instance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10], we show that retraining tree ensembles significantly strengthens the resulting model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] study a version of evasion where the small \u03b4 constraint is altogether dropped and replaced by a \u201cregularity\u201d constraint on the attack instance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] show some promising results by augmenting DNNs with a pre-filtering step based on a form of contractive auto-encoding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "So we have n = 784 and X = [0, 1].", "startOffset": 27, "endOffset": 33}, {"referenceID": 12, "context": "We choose to use second order gradient boosting (denoted BDT) in the XGBoost [15] implementation as the tree ensemble learner because of its outstanding performance as an off-the-shelf general purpose learning algorithm.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "L2) and RBF-SVM trained using LibLinear [16] and LibSVM [17] respectively.", "startOffset": 40, "endOffset": 44}, {"referenceID": 14, "context": "L2) and RBF-SVM trained using LibLinear [16] and LibSVM [17] respectively.", "startOffset": 56, "endOffset": 60}, {"referenceID": 15, "context": "We use Theano [18] to implement a sigmoidal (tanh) activation network with three hidden layers (denoted NN) in a 784-40-30-20-1 architecture, and train it by gradient descent for a top logistic regression layer, without pre-training nor drop-out.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "Polytope Machines [19] (one for each class) and we use the authors\u2019 implementation (CPM).", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "First, previous work has theoretically considered the evasion robustness of such ensemble of linear classifiers and deemed the problem NP-hard in the general case [20].", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "On the other end, RBF-SVM is apparently the hardest model to evade, agreeing with results from [10].", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "Recent work has successfully constructed adversarial \u201cevading\u201d instances for differentiable prediction models. However generating adversarial instances for tree ensembles, a piecewise constant class of models, has remained an open problem. In this paper, we construct both exact and approximate evasion algorithms for tree ensembles: for a given instance x we find the \u201cnearest\u201d instance x\u2032 such that the classifier predictions of x and x\u2032 are different. First, we show that finding such instances is practically possible despite tree ensemble models being nondifferentiable and the optimal evasion problem being NP-hard. In addition, we quantify the susceptibility of such models applied to the task of recognizing handwritten digits by measuring the distance between the original instance and the modified instance under the L0, L1, L2 and L\u221e norms. We also analyze a wide variety of classifiers including linear and RBF-kernel models, maxensemble of linear models, and neural networks for comparison purposes. Our analysis shows that tree ensembles produced by a state-of-the-art gradient boosting method are consistently the least robust models notwithstanding their competitive accuracy. Finally, we show that a sufficient number of retraining rounds with L0-adversarial instances makes the hardened model three times harder to evade. This retraining set also marginally improves classification accuracy, but simultaneously makes the model more susceptible to L1, L2 and L\u221e evasions.", "creator": "LaTeX with hyperref package"}}}