{"id": "1509.08102", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2015", "title": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification", "abstract": "modifying the nearest neighbor classifier is technically one of the fundamental most broadly widely used models for continuous classification. preventing the selection of prototype training instances repeatedly is an important problem for modelling its typical applications, so as the efficiency of the model implementations largely depends essentially on the functional compactness of the prototype membership set. the newly existing prototype studies undertaken on prototype data selection have been constructed primarily motivated by successive instance - alignment based analyses of the comparative class distribution and consistency among individual local neighbors. in this elementary paper, we explore an approximation and achieve a parametrization of fitting the nearest neighbor rule, which allows us perhaps to formulate the violation of the rule over the training set with regards to the common prototypes parameters and greatly take marginal advantage of a discriminative learning principle. we thoroughly show that integrating our functional approach frequently reduces to a consistent large - label margin learning methods based on a sparse representation of clearly the relations studied among the smaller neighbors. we better demonstrate the strategic advantage of managing the proposed algorithm by empirical comparative comparison mechanisms with the recent state - of - believe the - living art testing methods over managing a collection of public address benchmarks.", "histories": [["v1", "Sun, 27 Sep 2015 15:43:33 GMT  (94kb,D)", "https://arxiv.org/abs/1509.08102v1", null], ["v2", "Thu, 31 Dec 2015 00:44:17 GMT  (105kb,D)", "http://arxiv.org/abs/1509.08102v2", null], ["v3", "Tue, 5 Jan 2016 07:49:27 GMT  (105kb,D)", "http://arxiv.org/abs/1509.08102v3", null], ["v4", "Sun, 17 Jan 2016 11:54:57 GMT  (117kb,D)", "http://arxiv.org/abs/1509.08102v4", null], ["v5", "Sun, 21 Aug 2016 14:40:20 GMT  (763kb,D)", "http://arxiv.org/abs/1509.08102v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shin", "o"], "accepted": false, "id": "1509.08102"}, "pdf": {"name": "1509.08102.pdf", "metadata": {"source": "CRF", "title": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification", "authors": ["Shin Ando"], "emails": ["ando@rs.tus.ac.jp"], "sections": [{"heading": null, "text": "Keywords Nearest neighbor classifier, Prototype selection, Soft maximum, Large-margin learning"}, {"heading": "1 Introduction", "text": "Applying the nearest neighbor rule based on a set of prototype instances is one of the most widely used models for classification and a typical example of non-parametric, instance-based learning algorithms [31,40]. Prototype selection is a supervised learning technique related to the nearest neighbor algorithm, which selects a set of prototypes from the labeled training data to store and to use in nearest neighbor prediction on the test data. The prototypes are selected by various strategies in order to achieve the goal of improving both computational efficiency and robustness of the nearest neighbor algorithm. It is a critical problem for practical applications due to the large cost of storing and computing distances in the neighbor algorithms [7].\nThere has been a substantial amount of literature on prototype selection published over the years in the topic of data mining. Many existing techniques for prototype selection have been motivated by instance-based or locallydefined criteria, e.g., removing prototypes far from the decision boundary of two classes to reduce redundancy, or conversely, removing those near the boundary for generalization. Improvements by sophisticated hybridization of such techniques have also been reported [37]. Due to the instance-based nature of these techniques, however, it is generally difficult to learn their optimal parameter values. Alternatively, critical parameters are selected empirically through cross validation and wrapper methods [31].\nS. Ando School of Management, Tokyo University of Science, 1-11-2 Fujimi, Chiyoda-ku, Tokyo, 102-0071, Japan E-mail: ando@rs.tus.ac.jp\nar X\niv :1\n50 9.\n08 10\n2v 5\n[ cs\n.L G\n] 2\n1 A\nRecently, the emergence of Big Data has ignited more approaches to efficiently compute the nearest neighbor algorithm, such as Locality Sensitive Hashing and Approximate Nearest Neighbor Search [2,32]. ANNS methods yield excellent run-time performances without removing any of the training data from the prototypes. There are, however, a number of motivations for revisiting the topic of prototype selection. First, the state-of-the-art prototype selection methods have the advantage in terms of classification accuracy and are competitive in terms of efficiency compared to ANNS [18]. Secondly, ANNS is subject the constraints for the LSH function family, which require the use of standard distance functions and limit its applicable domains. Finally, a prototype selection method can also achieve the function of data summarization, which becomes more important for knowledge discovery with the emergence of larger datasets.\nIn this paper, we revisit the prototype selection problem to explore a parametric approach based on a discriminative learning principle. The key intuition of the proposed approach is two-fold. First, we define a parametric extension of the nearest neighbor rule, in which the choice of the nearest neighbor is adjusted by a numerical parameter assigned to each prototype. The parameter values are also used to infer whether the instances that are relevant or irrelevant for predicting over the training set. Secondly, we introduce a differentiable approximation of the condition that the nearest neighbor rule predicts the class of a training instance correctly, and subsequently derive a constrained optimization problem for minimizing the violation of the condition with regards to the numerical parameters of the prototypes. We further show that this problem reduces to a large-margin principled learning using a sparse representation of the relations between the neighboring instances.\nThe proposed method is also applicable in domains where only pairwise distances among the instances are available, while many techniques for prototype selection rely on the analyses of the feature vector space. These analyses are infeasible for non-vector input data and variables with complex dependency among them. The nearest neighbor algorithms are often used for such data, e.g., in time series classification, the Euclidean distance is often problematic due to the sequential structure of the time series [22], and a dissimilarity function based on non-linear warping is known to be highly effective [13,41].\nIn summary, the contribution of our work is a prototype selection approach which (a) enables an optimization algorithm directly addressing the violation of the nearest neighbor rule and (b) expands the application to the distance space input data. The rest of this paper is organized as follows. Section 2 discusses the related work. Section 3 describes the proposed method Section 5 shows the results of our empirical study. Section 6 presents our conclusion.\n2 Related Work\n2.1 Prototype Selection\nThe weaknesses of the nearest neighbor classifier in the large memory requirements, the time requirements for computing the similarities/dissimilarities, and the sensitivity to noisy labels has been well-known. Ohe of the ways to address these weaknesses is to reduce the number of prototypes at the pre-processing phase. The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33]. The goal of the problem is to find a compact subset of the prototypes which maintains or possibly improve the generalization error of the nearest neighbor classification while decreasing the computational burden.\nSome operations are commonly used in many prototype selection methods, e.g., an operation referred to as editing, eliminates or relocates noisy prototypes that cause misclassifications near the borders of different classes [7,39], and have the effects of smoothing and generalizing the decision boundaries. Condensation is another typical operation which discards prototypes far from the class borders in order to reduce the redundancy of the prototype set without affecting the decision boundary [3]. Combinations of these techniques and others such\nas feature analysis [34] and clustering [30,37]. are used to analyze boundaries and interior instances in hybrid methods [26,36].\nThe implementations of these techniques are generally categorized into wrapper and filtering methods [31]. In the wrapper methods, different sets of prototypes are evaluated and selected based on the performances of the nearest neighbor classifier. The filtering methods uses score functions that are less expensive to compute than the performance measures and select prototypes based on their individual scores. The wrapper methods is a general, flexible framework for empirically selecting a best model among a finite number of settings, but do not provide an intrinsic framework for learning an optimal model. For the filter methods, strong or complex dependency among prototypes are problematic, as it is difficult to represent such dependency sufficiently by individual scores.\nHeuristic combinatorial search such as evolutionary algorithms, a memetic algorithm, an artificial immune model, and tabu search, have been explored to find compact subsets of the training instances [8,16,17,42]. Theoretical approaches for prototype selection shown the upper-bound of generalization error based on VC dimensions, structural risk minimization, and Bayesian analysis [12,15,23]. The looseness of the bounds and the scalability of the algorithms are practical issues in these approaches.\n2.2 Approximate Nearest Neighbor Search\nDue to the importance of the nearest neighbor classification, other means of pre-processing to reduce the computational requirements for the similarity and distance measures also have been explored in wide areas of study. In recent years, Approximate Nearest Neighbor Search (ANNS) [2] have drawn strong interest and various techniques have been developed for image classification. ANNS uses a family of hash functions to obtain buckets to which the prototypes are hashed, and when a test sample is queried, its hash value is used to retrieve relevant prototypes from the bucket. The error bounds of the approximation are derived from the probabilities that points closer and further apart than designated distances are hashed to the same and different buckets, respectively.\nThe ANNS approach yields excellent run-time, typically matching that of the best prototype selection method, while maintaining the full set of prototypes. While the ANNS approach currently receives much more research interest, the prototype selection approach has its merits worth revisiting. As shown in the recent survey [18], the prototype selection methods aimed at reducing the generalization error achieve better accuracy than ANNS. Additionally, the implementation of ANNS requires a class of hash function that can satisfy the conditions for approximation, which is limited to few well-known distance functions, e.g., Hamming, Manhattan, Squared Euclidean, Jaccard, and Arc cos. Finding a suitable LSH family for a domain-specific distance function is generally difficult and is not actively pursued.\nThis paper focuses on a prototype selection approach which requires only the mutual distance matrix as the input, thus is applicable for problems where ANNS approach and also many prototype selection approaches are not.\n2.3 Discriminative Learning and Nearest Neighbor Model\nRecently, discriminative learning approaches that have the nearest neighbor classification model embedded in its formulation have been studied in the context of metric learning. Large-margin Nearest Neighbor Classification [38] addresses the problem of distance metric learning as a constrained optimization problem to minimize the violation of the nearest neighbor rule with regards to the affine transformation. The violation is formalized as the margin from the delineating hyperplane and the problem is solved by semi-definite programming. Max-margin Multiclass Nearest Neighbors [25] also learns the metric space based on the minimization of its entropy.\n3 Proposed Method\n3.1 Preliminaries\nLet X = {xi}ni=1 denote the set of labeled instances and Y = {yi} n i=1 their class values. Instances are not necessarily represented as vectors, as the nearest neighbor prediction requires only similarities/dissimilarities from the prototypes to a new instance. The nearest neighbor classifier, h, returns the class prediction on a test sample t as follows.\nh(t) = yi : argmin i\u2208{1,...,#(X )} d(t, xi) (1)\nwhere d(\u00b7, \u00b7) denotes the dissimilarity function. A prototype selection algorithm selects a subset Z \u2286 X with which the nearest neighbor classifier yields smaller generalization error and run-time.\nIn order to evaluate a candidate prototype set in the training phase, one must avoid trivial cases where the target instance being classified is also one of the prototypes set, as the prototypes and the training instances originally come from the same set of instances, X . In the following sections, we employ a setup similar to the leave-one-out cross validation, such that the target instance is always removed from the prototype set. That is, the prototypes for predicting the class of xi will be a subset of the remainder of the labeled instances, denote by Zi = X \\ {xi}. For distinction, we refer to an instance as a prototype only when it is used as one of the reference objects for the nearest neighbor rule, in this paper. In turn, we also refer to an instance as a training instance only when it is the subject of prediction.\n3.2 Soft-Maximum Function\nThe soft maximum [6] is an approximation of the function max(\u00b7, \u00b7) as log (exp(\u00b7) + exp(\u00b7)). The approximation is convex and differentiable, which are desirable for numerical optimization.\nA natural extension of the soft maximum for m variables x1, . . . , xm is given by\nM(x1, . . . , xm) = log m\u2211 i=1 exp(xi) (2)\nThe base of the exponential, \u03b2, is larger than 1 and modified to adjust for the scale of the input values.\n3.3 Rank-Adjusted Nearest Neighbor Rule\nThis section introduces an extension of the nearest neighbor rule in order to parametrize the classification model and the prototype selection problem.\nLet x denote a target instance and Z denote the set of prototypes. We denote by R(x, xj ;Z) the rank of d(x, xj) in the set { d(x, xj) } xj\u2208Z\n. The rank takes an integer value from {1, . . . ,#Z} and a smaller integer is a higher rank. The nearest neighbor rule for a target instance x is as follows. 1. Compute distances {d(x, xj)}xj\u2208Z 2. Assign rank R(x, xj ;Z) to all xj \u2208 Z based on their distances to x 3. Return the class value of the highest-ranked xj We now introduce a positive-value parameter \u03b1(xj) for each prototype to define the adjusted rank R\u2032.\nR\u2032(x, xj ;Z) = R(x, xj ;Z) + \u03b1(xj) (3)\nThe nearest neighbor rule based on the adjusted rank R\u2032 generalizes the original rule and produces identical predictions when \u03b1(x1) = . . . = \u03b1(xn).\nBased on the adjusted ranks, the nearest prototype can be passed over for another prototype. We thus refer to \u03b1(xj) as the degradation parameter of the prototype xj . For brevity, we will use \u03b1j as \u03b1(xj) when it is clear from context. One may correct individual misclassifications by assigning larger degradations to the related prototypes. That is, if xj and xk are respectively the highest and the second highest-ranked prototypes for a training case (x, y) where yj 6= y and yk = y, the misclassification of the basic rule may be corrected by assigning a relatively larger degradation to \u03b1j than \u03b1k.\nOur goal in this extension is to learn the degradation parameters that improve the average classification performance over all hold-out cases. Based on their values, we can identify which prototypes are more/less relevant in the prediction of the training set and reduce the prototype set accordingly. The motivation for focusing on ranks rather than distance values is to avoid the issue of scaling. The latter may vary substantially in scale depending on the function, and are more difficult to adjust.\n3.4 Approximation of the Nearest Neighbor Rule\nLet (xi, yi) denote a pair of a hold-out instance and its class label. We denote by P the subset of prototypes Zi with the same label as yi and Q that of labels other than yi. The condition that the nearest neighbor rule correctly predicts yi is that its nearest neighbor is an element of P . That is,\nmax ( {\u2212d(xi, p)}p\u2208P ) > max ( {\u2212d(xi, q)}q\u2208Q ) (4)\nNote that the trivial prediction does not occur because xi is not an element of P . In an ideal prototype set, the condition (4) is satisfied over all training cases for all xi \u2208 X . In general, however, there may not exist such an ideal set. A practical principle for selecting the prototypes is, therefore, to reduce the cases where the conditions are violated as much as possible.\nThe formulation of such a principal is non-trivial due to the max function in (4). Alternatively, we rewrite (4) substituting the soft maximum M and the ranks of d(xi, q) and d(xi, p) for max and the distance values, respectively.\nM ( {\u2212R(xi, p;Zi)}p\u2208P ) \u2265M ( {\u2212R(xi, q;Zi)}q\u2208Q ) + 1, \u2200xi \u2208 X (5)\nIn essence, (5) describes the same condition as (4) with regards to the ranks of the prototypes of the same and different classes as the hold-out case. The constant 1 is added to the RHS given that R is the rank and takes an integer value.\nSubstituting (2) to (5), we have\nlog( \u2211 p\u2208P exp (\u2212R (xi, p;Zi))) \u2265 log( \u2211 q\u2208Q exp (\u2212R (xi, q;Zi))) + 1\nTaking the exponential on both sides, we obtain\u2211 p\u2208P exp (\u2212R(xi, p;Zi))\u2212 \u2211 q\u2208Q exp (\u2212R (xi, q;Zi)) \u2265 \u03c1(xi) (6)\nwhere \u03c1(xi) is\n\u03c1(xi) = (\u03b2 \u2212 1) \u2211 q\u2208Q exp (\u2212R (xi, q;Zi)) (7)\nSince P \u222aQ = Zi, the LHS of (6) is rewritten as\u2211 xj\u2208Zi \u03b4(yi, yj) exp (\u2212R (xi, xj ;Zi)) \u2265 \u03c1(xi) (8)\nwhere\n\u03b4(yi, yj) = { 1 if yi = yj \u22121 otherwise\nConsidering (8) as a constraint, we introduce a slack variable \u03bei for each xi, to represent the violation of the nearest neighbor rule. \u2211\nxj\u2208Zi\n\u03b4(yi, yj) exp (\u2212R (xi, xj ;Zi)) \u2265 \u03c1(xi)\u2212 \u03bei (9)\nEach \u03bei takes a non-negative value, and the violation of the nearest neighbor rule over the training set is given as\u2211 xi\u2208X \u03bei.\n3.5 Problem Formulation\nCombining the rank degradation parameter and the soft-max approximation, we formulate a constrained optimization problem for the parametrized nearest neighbor classification model.\nFirst, we rewrite (9) using the adjusted rank.\u2211 xj\u2208Zi \u03b4(yi, yj) exp (\u2212R (xi, xj ;Zi)\u2212 \u03b1(xj)) \u2265 \u03c1\u2032(xi)\u2212 \u03bei (10)\nwhere \u03c1\u2032(xi) is the residual term that corresponds to \u03c1(xi) in (9), i.e.,\n\u03c1\u2032(xi) = (\u03b2 \u2212 1) \u2211 q\u2208Q exp ( \u2212R\u2032 (xi, q;Zi) ) = (\u03b2 \u2212 1)\n\u2211 q\u2208Q exp (\u2212R (xi, q;Zi)\u2212 \u03b1(q)) (11)\n(10) is not a desirable form for a constrained optimization problem because the parameter being trained appears on both sides. That is, \u03b1(xj) on the LHS and \u03b1(q) in \u03c1\u2032(xi). Alternatively, we replace \u03c1\u2032(xi) with \u03c1(xi), which is constant for each xi, and the solution that satisfies (9) will also satisfy (10) since \u03c1\u2032(xi) \u2264 \u03c1(xi). For brevity, we denote \u03c1(xi) as \u03c1i when it is clear in the context.\nSecondly, we consider the regularization for the degradation parameters. By definition, the nearest neighbor rule with adjusted ranks behaves the same when all parameters are increased or decreased by the same amount, which is problematic for convergence. The issue can be addressed by the regularization on {\u03b1(xj)}, to promote less adjustments of ranks given the same amount of violations.\nLet \u03bb denote the regularization function on {\u03b1(xj)} and C the trade-off coefficient. The constrained optimization problem is formalized as\nargmin \u03b1j\u22650,\u03bei\u22650 \u03bb(\u03b11, . . . , \u03b1n) + C n\u2211 i=1 \u03bei (12)\nsubject to \u2211 xj\u2208Zi \u03b4(yi, yj) exp (\u2212R (xi, xj ;Zi)\u2212 \u03b1j) \u2265 \u03c1i \u2212 \u03bei, \u2200xi \u2208 X\nTo gain further insight on the above problem, we rewrite the LHS of the constraints as a linear combination of w = (w1, . . . , wn) and ri = (ri1, . . . rin), where wj = exp(\u03b1j) and rij = \u03b4(yi, yj) exp(\u2212R(xi, xj ;Zi)). Using the `-2 norm of w, whose elements monotonically increase with {\u03b1j}, as the regularizer, we obtain\nargmin wi\u22650,\u03bei\u22650 \u2016w\u20162 + C \u2211 i \u03bei (13)\nsubject to\nw>ri \u2265 \u03c1i \u2212 \u03bei, \u2200xi \u2208 X (14)\nIn (13), the problem has reduced to a quadratic programming for learning the vector w of a linear classifier by maxmargin principle. In turn, ri interprets as a mapping of the hold-out case to a feature vector space. Each feature in ri corresponds to a prototype, and characterizes the training instance xi. Between the exponential definition of ri and the regularization on w, the problem induces a sparse representation which highlights the relevant prototypes.\nGiven the form of (14), the problem is solved by a constrained gradient descent [24]. From the learned degradation values, each prototype is scored by its relevance, i.e., the highest adjusted rank, in the prediction of the hold-out cases,\ns(xj) = \u03b1j +min ( {R(xi, xj ;Zi)}xi\u2208Zj ) (15)\nThe prototypes with low scores are removed from the prototype set as they have less effect on the predictions. If the size of the prototype set is fixed, the prototypes are selected by their scores from the highest to the lowest. When the size of the prototypes is not given, one can choose the size of set by cross validation. In the rest of the paper, we refer to the proposed method as Prototype Selection based on Rank-Exponential (REPS) and ri as the Exponential Rank representation of xi."}, {"heading": "4 Algorithm", "text": "The procedure of prototype selection by REPS is briefly summarized as follows.\n\u2013 Compute the rank matrix from the input distance matrix \u2013 Compute the parameter values in the constraints \u2013 Solve the Quadratic Programming problem with regards to the degradation parameters \u2013 Eliminate candidates with high degradation parameters\nThe description of the algorithm with more detail is shown in Algorithm 1. In order to address large datasets, we used an efficient approximation algorithm for SVM training, shown in Algorithm 2. Algorithm 2 is a modification of the Structural SVM [21] learning algorithm, which obtains the approximate solution to the linear SVM problem using the cutting-plane method. It solves a modified SVM problem where \u03c1i is used as the relatively scaled margins of respective prototypes.\nWith regards to the computational requirements, the space complexity is dominated by that of storing the mutual distance matrix among labeled instances, which is O(n2). The time complexity is dominated by the computation of ranks, which is O(n log n) for each instance."}, {"heading": "5 Empirical Results", "text": "This section presents an empirical study to evaluate REPS using public benchmark datasets.\nThe empirical results are provided in supplementary materials\nAlgorithm 1 REPS algorithm INPUT: Training set X = {xi}ni=1, regularization parameter C, prototype selection size k OUTPUT: Prototype set Z \u2282 X METHOD: Compute the distance matrix [d(xi, xj)]n\u00d7n and the ranks {R(xi, x;Zi)}x\u2208Zi Compute rij = \u03b4(yi, yj) exp (\u2212R(xi, xj ;Zi)) and \u03c1(xi) (7) Solve (13) for w for j = 1 to n do\nCompute \u03b1j = logwj and s(xj) from (15) end for return argmax\nZ\u2282X :#Z=k min({s(xj)}xj\u2208Z)\nAlgorithm 2 Cutting-plane Training algorithm INPUT OUTPUT Method InitializeW = \u2205 repeat\n(w, \u03be)\u2190 argmin w,\u03be\u22650 1 2 w>w + C\u03be s.t. \u2200c \u2208 W : 1 n w>\n\u2211n i=1 ciyixi \u2265 1 n \u2211n i=1 ci\u03c1i \u2212 \u03be\nfor i = 1, . . . , n do ci \u2190 { 1 if yi(w>xi < \u03c1i) 0 otherwise end for W \u2190W \u222a {c}\nuntil {\n1 n \u2211n i=1 ci \u2212 1 n \u2211n i=1 ciyi(w >xi) } \u2264 \u03be +\nreturn (w, \u03be)\n5.1 Datasets\nFor the first part of the experiment, we employed a collection of 34 datasets with vector features from the UCI Machine Learning Repository [4] and the KEEL datasets [1]. The summary of the datasets is shown in Table 4 in Appendix 6. Although some of the benchmarks are not large datasets, a substantial reduction of the prototype set, even when the training set is small, is not irrelevant in practical aspect, as the execution time for the nearest neighbor prediction is a considerable obstacle in its applications.\nIn the second part of the experiment, we use a collection of 27 datasets from the UCR Time Series Dataset Repository [9]. The input data for time series classification is given as a dissimilarity matrix of Dynamic Time Warping (DTW) [5], which is known to be highly effective in time series classification [13]. We tested window sizes between five and fifty for DTW, and the results were generally consistent among them. The window size is five for all results shown here.\n5.2 Baseline Methods\nWe selected five recent prototype selection methods, which ranked highly among the 42 algorithm reported in a recent survey [18], as baselines for comparative analysis. Class conditional instance selection (CCIS) uses pairwise relations among prototypes to define the selection criteria [27]. Fast condensed nearest neighbor (FCNN) exploits the condensation operation for prototype selection [3]. DROP3 uses a decremental procedure to reduce prototypes and is fast, efficient, and accurate among algorithms of the same approach [39]. Random Mutation Hill Climbing (RMHC) uses stochastic hill climbing search to explore the prototype subset combination space [35]. SSMA conducts a stochastic population search based on the memetic algorithm, and exhibited the best overall\nperformances among the methods reported in the above survey. All baseline algorithms were executed in the KEEL software [1]. We also report the performance of the nearest neighbor classifier using the full training set as prototypes (NoPS).\nThe parameter specifications for the baseline methods are summarized in Table 3 in Appendix A. Each method is evaluated using values in the second column and the best result is reported. With regards to the parameters of REPS, the regularization coefficient C was set to 0.001. Its results were robust with regards to the values of C.\n5.3 Evaluation\nThe prototype selection methods are evaluated in two aspects: the accuracy of the nearest neighbor classifier and the compactness of the prototype set, and we use the error rate (ERR) and the selection rate (SLR) for respective measurements. ERR is the ratio of misclassifications with respect to the total number of predictions. SLR is defined as the ratio of the prototype set size with respect to the training set.\nDue to the trade-off between the two measures, however, it does not suffice to compare algorithms by singleobjectives. Here, we employed three approaches to compare the performance measures with the baselines in multiobjective manners. First, we introduce the concept of the Pareto frontier and non-dominated sorting, commonly used in multi-objective population-based search algorithms [10], to compute the ranks of the algorithms based on two evaluation measures.\nGiven a problem and a set of algorithms, the Pareto frontier is the group of algorithms to which no other algorithms are better in all evaluation measures. Extending this concept, we can divide the algorithms into separate groups by iteratively finding and removing the Pareto frontier from the working set until no algorithms remain. We then rank each groups of algorithms by how many separate groups of algorithms by which it is dominated. The rank provides a relative measure of performance among the evaluated algorithms, which we will refer to as the Pareto-rank. The exact definition of the Pareto-rank is presented in Appendix C.\nSecondly, we evaluated the fixed selection rate (FSR) errors, which is the error rate of the proposed method when its selection rate is forced to the value of each baseline. By forcing the same selection rate, we can utilize the single-measure comparison of the error rates.\nFinally, for our sensitivity analysis, we employed the log odds ratio (LOR) of the error and the selection rates. The previous measurements were not suitable as there were no baseline selection rate to which to fix, and the differences between the settings were very small to capture by ranks. Given the error rate of the nearest neighbor algorithm ERRNoPS, it is defined as\nLOR = log O(SLR)O(ERR)\nO(ERR\u2212 ERRNoPS)\nwhere O(p) = p1\u2212p is the odds given the probability p. For all measures, a smaller value indicates a better performance. The performances are averaged over 5-fold cross validation for the vector datasets. The default training/test split is used for the time series datasets. Following the evaluations, we used the Wilcoxon\u2019s signed rank test to verify whether the difference between the proposed algorithm and each baseline algorithm is significant. In [11], Wilcoxon\u2019s signed ranks test and Friedman-Nemenyi test are recommended for comparison of two classifiers and three or more classifiers over multiple datasets, respectively. Since it is not in our interest to compare and rank among the baseline algorithms, the multiple comparison test result is presented only as a reference. We also note that the Friedman-Nemenyi test is highly conservative, due to the substantial loss of power for handling unreplicated blocked data [28].\n5.4 Results\nWe show large figures and tables in Appendix A and Appendix B, respectively, for better readability of the text.\nTable 1 Summary of the Signed Rank Tests\nCCIS FCNN SSMA DROP3 RMHC PARETO 0.00019 4.6 \u00d710\u22126 0.36 1.1\u00d710\u22126 0.00043 FSR ERR 7.1\u00d710\u22127 0.00011 0.39 9.6\u00d710\u22126 0.11"}, {"heading": "5.4.1 Vector Data", "text": "Fig. 7 in Appendix B illustrates the error and the selection rates of the evaluated algorithms. The x- and y-axes indicate the values of respective measures. Each marker represents the performances of one algorithm on one dataset. Each algorithm is distinguished by a unique color and shape of the marker. The numbers inside the markers identifies the dataset, which are the numbers shown in the first column of Table 7.\nWithin the same problem (marker number), the markers generally lie from top-left to bottom right, due to the trade-off between the two measures. The shift toward the x-axis indicate the advantage in terms of compactness and the shift towards the y-axis indicate the advantage in accuracy. Note that there is no intrinsic trade-off between the error and selection rates within the same method (marker shape), as the evaluation measures depend on the difficulty of the problems. The balance of the two measures vary depending on the method, e.g., those of REPS and CCIS tend to lean toward higher selection rates.\nTable 7 in Appendix A presents more detailed summary of the performances. Each row shows ERR and SLR of each method on one dataset. The selection rate for NoPS, which is always 1, is omitted. The values of algorithms whose Pareto rank is 1 are indicated in bold. It shows that REPS and SSMA are Pareto-rank 1 in the largest numbers of datasets.\nTable 2 shows the FSR errors of the proposed algorithm and the error rates of the baselines. The even number columns show the FSR errors of REPS using the same selection rate as the baseline in the next column. The odd numbered columns shows the ERR of the baselines. The selection rates of the baseline algorithms are the same as those shown in through 5th-13th columns of Table 7.\nWe tested the significance of the differences in the two experimental results, using Wilcoxon\u2019s signed rank tests. The error and selection rates from Table 7 are converted into the Pareto ranks of respective algorithms. The ranks of the REPS and each baseline algorithm over all datasets are compared with the alternative hypothesis that the average for REPS is smaller. For comparing the FSR errors, the values from the adjacent, corresponding columns in Table 2 were taken and tested, respectively.\nThe summary of the tests are shown in Table 1. The first and the second rows show the p-values from the comparisons of the Pareto-ranks and the FSR errors, respectively. In the Pareto-ranks comparison test, the null hypotheses for all baselines but SSMA can be rejected with a high confidence. For the comparison of FSR errors, the null hypotheses for baselines other than SSMA and RMHC can be rejected with a high confidence. SSMA and RMHC were two best algorithms in [18], which indicates that REPS has a significant advantage against many prototype selection algorithms and is competitive with the state-of-the-art.\nFor the purpose of reference, we include the summary of Friedman rank sum test with post-hoc Nemenyi test on the Pareto rank in the supplementary materials. We note that the Friedman Test with post-hoc Nemenyi test is a very conservative test, and it did not detect significant difference among any pair of methods."}, {"heading": "5.4.2 Time Series Data", "text": "Table 6 in Appendix A shows the error and the selection rates for the time series datasets. The comparison between REPS and NoPS is shown for this experiment as other baseline methods do not take input of the dissimilarity matrix form. The problems where REPS improves the error rate are indicated by bold. In general, the error rates without prototype selection are generally smaller than those in the previous experiment, which makes the reduction without compromise in the accuracy more difficult.\nWe show the sensitivity analysis of the logarithm base \u03b2 of the soft maximum function. We compared the performances of REPS over the same collection of datasets while changing \u03b2 between 1 and 4.\nFig. 1 summarizes the result of the sensitivity analysis. The y-axis of the box-whisker chart indicates the value of the log odds ratio. It shows that the largest median is achieved with \u03b2 = 2, and the performance of REPS is robust with regards to the value of \u03b2.\nFinally, we present case studies on the examples of the prototypes selected by the proposed method. We focus on the \u2018ECG200\u2019 and \u2018PLANE\u2019 time series. The ECG time series consists of positive (abnormal) and negative (normal) class examples and pose a binary classification problem [29]. The PLANE time series consists of seven classes. For the purpose of the demonstration, classes 1 and 2, whose patterns are mutually the most similar among all pairings, are chosen.\nFig. 3 (1) and (2) show the time series instances of classes 1 and 2 in line plots, respectively. The plots without frames indicate those selected as prototypes and those with dotted frames are the remainders of respective classes in the training set. From these figures, we can see that the overall trends of the two classes are similar, with class 1 having slightly smoother shapes. For both classes, more than half of the training set has been eliminated, and the redundancy of their patterns in relation to the retained prototypes is apparent.\nFig. 2 shows a graph comprised of vertices representing the training examples and edges representing the nearest neighbor relations between the vertices, respectively. The class 2 instances are indicated by the gray background, and the class 1 instances have white backgrounds. The line plots with dotted frames indicate the instances not selected as prototypes. From this figure, we can see that both redundant patterns and potentially erroneous patterns have been thrown out. The class 1 instance on the far right of the graph is potentially erroneous as it neighbors three class 2 instances and has been eliminated.\n0.4 0.45 0.5 0.55 0.65\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n\u03b2-1\nLO R\nFig. 1 Sensitivity Analysis for \u03b2\nIn Figs. 4 and 5, the prototypes of positive and negative classes are shown, respectively. The plots without frames indicate those selected as prototypes and those with dotted frames are the remainders of respective classes in the training set. For both classes, a larger number of prototypes are retained compared to the previous examples, as a result of more noise and variations in the patterns.\nFig. 6 shows a graph comprised of vertices that represent the training examples and edges that represent the nearest neighbor relations among the vertices, respectively. In a similar manner to Fig. 2, the gray background indicates positive examples and the white background indicates the negative examples. The vertices with dotted frames are examples not selected as prototypes. As with the previous examples potentially erroneous patterns, neighboring the other class, and few redundant patterns placed among the same classes are eliminated.\nThese examples demonstrate that the proposed method effectively addresses both the redundant patterns and noisy patterns without exploiting the explicit knowledge of the vector representation space."}, {"heading": "6 Conclusion", "text": "This paper presented an extension of the nearest neighbor rule based on the adjustment of ranks to parametrize the prototype selection problem and also to approximate the violation of the rule over the training set. As a result, the problem is defined as discriminative learning in a sparse feature space. Our empirical results showed that it is competitive with the state-of-the-art prototype selection algorithm has the advantage over many other existing algorithms in multiple-measure comparisons.\nThe key intuition for this approach is to avoid intermediate evaluation and to learn the priorities of the prototypes in a direct relation with the performance over the training set. In effect, the proposed approach exploits the sparse relationships among the neighboring instances instead of dense representations such as feature vectors and the distance matrix. Discarding such a large amount of information is justifiable given the successful empirical results of the k-nearest neighbor algorithm in many applications using a small k. When the number of relevant neighbors is limited, a dense representation can be redundant and a sparse representation can induce efficient learning. The empirical results suggests that the sparse representation can maintain the dependence among the prototypes that are stronger and more complex in a reduced set, which is an advantage of the proposed method that and is difficult to achieve using locally-defined criteria and instance-based analyses."}, {"heading": "A Tables", "text": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification 15 Ta bl e 4 D at as et Pr op er tie s (V ec to r) # E x. # A tts . # C l. ap pe nd ic iti s 10 6 7 2 au st ra lia n 69 0 14 2 ba la nc e 62 5 4 3 ba nd s 53 9 19 2 br ea st 28 6 9 2 bu pa 34 5 6 2 cl ev el an d 29 7 13 5 co nt ra ce pt iv e 14 73 9 3 cr x 69 0 15 2 de rm at ol og y 36 6 33 6 ec ol i 33 6 7 8 ge rm an 10 00 20 2 gl as s 21 4 9 7 ha be rm an 30 6 3 2 ha ye sro th 16 0 4 3 he ar t 27 0 13 2 ho us ev ot es 43 5 16 2 io no sp he re 35 1 33 2 ir is 15 0 4 3 ly m ph og ra ph y 14 8 18 4 m am m og ra ph ic 96 1 5 2 m on k2 43 2 6 2 m ov em en t lib ra s 36 0 90 15 ne w th yr oi d 21 5 5 3 pi m a 76 8 8 2 sa he ar t 46 2 9 2 so na r 20 8 60 2 sp ec tf he ar t 26 7 44 2 ta e 15 1 5 3 ve hi cl e 84 6 18 4 w db c 56 9 30 2 w in e 17 8 13 3 w is co ns in 69 9 9 2 ye as t 14 84 8 10 Ta bl e 5 D at as et Pr op er tie s (T im e Se ri es ) D at a # C l. # Tr ai n # Te st T S L en gt h 50 W or ds 50 45 0 45 5 27 0 A di ac 37 39 0 39 1 17 6 B ee f 5 30 30 47 0 C B F 3 30 90 0 12 8 C hl or in e 3 46 7 38 40 16 6 C of fe e 2 28 28 28 6 D ia to m 4 16 30 6 34 5 E C G 20 0 2 10 0 10 0 96 E C G -5 D ay s 2 23 86 1 13 6 E C G -C in C 4 40 13 80 16 39 Fa ce A ll 14 56 0 16 90 13 1 Fa ce Fo ur 4 24 88 35 0 Fa ce sU C R 14 20 0 20 50 13 1 Fi sh 7 17 5 17 5 46 3 G un Po in t 2 50 15 0 15 0 L ig ht ni ng -2 2 60 61 63 7 L ig ht ni ng -7 7 70 73 31 9 M ed ic al Im ag es 10 38 1 76 0 99 M ot eS tr ai n 2 20 12 52 84 O liv eO il 4 30 30 57 0 Pl an e 7 10 5 10 5 14 4 Po w er D em an d 2 67 10 29 24 R ob ot Su rf ac e 2 20 60 1 70 R ob ot Su rf ac e2 2 27 95 3 65 Sy nt he tic 6 30 0 30 0 60 Tr ac e 4 10 0 10 0 27 5 Tw o Pa tte rn s 4 10 00 40 00 12 8 Ta bl e 6 E rr or an d Se le ct io n R at es (T im e Se ri es ) D at a N oP S E R R SL R 50 w or ds 0. 06 8 0. 06 8 0. 96 A di ac 0. 42 0. 43 0. 94 B ee f 0. 37 0. 37 0. 97 C B F 0. 01 3 0. 01 2 0. 97 C hl or in e 0. 37 0. 38 0. 99 C of fe e 0. 03 6 0. 03 6 0. 96 D ia to m 0. 37 0. 38 0. 99 E C G 20 0 0. 1 0. 13 0. 88 E C G -C in C 0. 37 0. 38 0. 99 E C G -5 D ay s 0. 19 0. 30 0. 52 Fa ce A ll 0. 06 8 0. 06 8 0. 96 Fa ce Fo ur 0. 07 6 0. 07 6 0. 99 Fa ce sU C R 0. 02 7 0. 04 0 0. 88 Fi sh 0. 18 0. 17 0. 90 G un Po in t 0. 06 0 0. 06 0 0. 94 L ig ht in g2 0. 23 0. 23 0. 99 L ig ht in g7 0. 24 0. 25 0. 90 M ed ic al Im ag es 0. 13 0. 16 0. 90 M ot eS tr ai n 0. 16 0. 17 0. 93 O liv eO il 0. 13 0. 13 0. 97 Pl an e 0. 00 95 0. 01 9 0. 70 Po w er D em an d 0. 18 0. 34 0. 72 R ob ot Su rf ac e 0. 08 9 0. 09 8 0. 52 R ob ot Su rf ac e2 0. 29 0. 28 0. 95 Sy nt he tic 0. 08 0 0. 08 0 0. 92 Tr ac e 0. 11 0. 20 0. 52 Tw oP at te rn s 0. 32 0. 32 0. 97\nTa bl\ne 7\nSu m\nm ar\ny of\nE rr\nor an\nd Se\nle ct\nio n\nR at\nes (V\nec to\nrD at\na)\nID D\nat a\nN am\ne N\noP S\nC C\nIS FC\nN N\nSS M\nA D\nR O\nP3 R\nM H\nC R\nE PS\nE R\nR E\nR R\nSL R\nE R\nR SL\nR E\nR R\nSL R\nE R\nR SL\nR E\nR R\nSL R\nE R\nR SL R 1 ap pe nd ic iti s 0. 81 0. 18 0. 03 1 0. 24 0. 32 0. 15 0. 03 8 0. 26 0. 13 0. 13 0. 09 4 0. 20 0.\n05 0\n2 au\nst ra\nlia n\n0. 69\n0. 44\n0. 03\n0 0.\n41 0.\n32 0.\n41 0.\n01 4\n0. 39\n0. 11\n0. 42\n0. 10\n0. 36\n0. 01 3 3 ba la nc e 0. 78 0. 22 0. 05 3 0. 30 0. 33 0. 12 0. 03 0 0. 17 0. 12 0. 12 0. 10 0. 28 0. 02 0 4 ba nd s 0. 71 0. 41 0. 05 4 0. 42 0. 48 0. 42 0. 05 6 0. 39 0. 32 0. 42 0. 09 9 0. 37 0. 01 7 5 br ea st 0. 63 0. 38 0. 05 2 0. 37 0. 49 0. 35 0. 02 4 0. 33 0. 20 0. 37 0. 09 9 0. 34 0. 02 2 6 bu pa 0. 61 0. 34 0. 09 8 0. 41 0. 56 0. 33 0. 05 7 0. 35 0. 30 0. 33 0. 09 8 0. 45 0. 01 7 7 cl ev el an d 0. 53 0. 74 0. 31 0. 68 0. 61 0. 55 0. 01 9 0. 61 0. 17 0. 60 0. 09 8 0. 48 0. 04 5 8 co nt ra ce pt iv e 0. 43 0. 53 0. 23 0. 53 0. 71 0. 50 0. 02 7 0. 52 0. 27 0. 50 0. 09 9 0. 61 0.\n00 64\n9 cr\nx 0.\n78 0.\n47 0.\n03 1\n0. 44\n0. 31\n0. 40\n0. 01\n6 0.\n44 0.\n12 0.\n42 0.\n10 0.\n27 0.\n01 6\n10 de\nrm at\nol og\ny 0.\n95 0.\n61 0.\n04 7\n0. 53\n0. 12\n0. 80\n0. 03\n6 0.\n80 0.\n08 5\n0. 75\n0. 09\n8 0.\n07 3\n0. 04 8 11 ec ol i 0. 79 0. 32 0. 14 0. 24 0. 37 0. 20 0. 05 9 0. 26 0. 16 0. 19 0. 09 7 0. 31 0. 04 5 12 ge rm an 0. 67 0. 49 0. 04 0 0. 42 0. 50 0. 42 0. 02 6 0. 41 0. 22 0. 39 0. 10 0. 36 0. 00 92 13 gl as s 0. 72 0. 55 0. 14 0. 30 0. 48 0. 36 0. 07 7 0. 40 0. 25 0. 36 0. 09 9 0. 50 0. 05 3 14 ha be rm an 0. 65 0. 36 0. 03 4 0. 34 0. 51 0. 28 0. 02 0 0. 35 0. 20 0. 31 0. 09 8 0. 33 0. 02 7 15 ha ye sro th 0. 69 0. 43 0. 14 0. 36 0. 49 0. 41 0. 06 6 0. 56 0. 24 0. 47 0. 09 4 0. 46 0. 05 2 16 he ar t 0. 79 0. 40 0. 04 3 0. 41 0. 37 0. 42 0. 02 6 0. 44 0. 17 0. 37 0. 09 7 0. 26 0. 02 3 17 ho us ev ot es 0. 87 0. 35 0. 01 8 0. 11 0. 15 0. 07 9 0. 02 6 0. 17 0. 06 1 0. 08 4 0. 09 7 0. 11 0. 01 9 18 io no sp he re 0. 91 0. 36 0. 01 7 0. 19 0. 20 0. 10 0. 03 3 0. 25 0. 08 8 0. 11 0. 10 0. 24 0. 04 8 19 ir is 0. 85 0. 31 0. 03 2 0. 41 0. 13 0. 04 0 0. 04 7 0. 08 7 0. 07 3 0. 03 3 0. 10 0. 05 3 0. 15 20 ly m ph og ra ph y 0. 64 0. 28 0. 06 6 0. 25 0. 42 0. 24 0. 05 7 0. 34 0. 18 0. 24 0. 09 3 0. 25 0. 08 1 21 m am m og ra ph ic 0. 80 0. 44 0. 01 7 0. 29 0. 39 0. 34 0. 01 1 0. 38 0. 14 0. 27 0. 09 9 0. 28 0. 01 4 22 m on k2 0. 75 0. 17 0. 04 9 0. 08 8 0. 07 7 0. 03 7 0. 03 1 0. 17 0. 20 0. 11 0. 09 8 0. 24 0. 19 23 m ov em en t lib ra s 0. 76 0. 39 0. 31 0. 25 0. 40 0. 36 0. 15 0. 29 0. 33 0. 40 0. 09 7 0. 58 0. 05 5 24 ne w th yr oi d 0. 85 0. 33 0. 02 6 0. 14 0. 12 0. 19 0. 03 0 0. 15 0. 12 0. 12 0. 09 9 0. 11 0. 03 1 25 pi m a 0. 97 0. 38 0. 04 1 0. 35 0. 46 0. 28 0. 02 6 0. 36 0. 18 0. 29 0. 09 9 0. 32 0. 01 0 26 sa he ar t 0. 65 0. 39 0. 04 4 0. 41 0. 51 0. 40 0. 03 0 0. 42 0. 22 0. 36 0. 09 9 0. 31 0. 01 7 27 so na r 0. 82 0. 46 0. 04 6 0. 35 0. 30 0. 32 0. 07 4 0. 29 0. 26 0. 28 0. 09 6 0. 28 0. 11 28 sp ec tf he ar t 0. 72 1. 0 1. 0 0. 31 0. 43 0. 21 0. 02 4 0. 30 0. 17 0. 25 0. 09 8 0. 20 0. 02 4 29 ta e 0. 58 0. 59 0. 16 0. 58 0. 60 0. 64 0. 08 3 0. 54 0. 20 0. 57 0. 09 9 0. 55 0. 06 0 30 ve hi cl e 0. 71 0. 49 0. 17 0. 38 0. 49 0. 45 0. 06 5 0. 43 0. 24 0. 45 0. 09 9 0. 63 0. 00 92 31 w db c 0. 94 0. 21 0. 00 75 0. 07 7 0. 12 0. 10 0. 01 5 0. 11 0. 04 4 0. 07 7 0. 09 9 0. 04 9 0. 08 2 32 w in e 0. 95 0. 29 0. 02 5 0. 34 0. 13 0. 29 0. 03 4 0. 33 0. 09 8 0. 34 0. 09 8 0. 07 3 0. 12 33 w is co ns in 0. 96 1. 0 1. 0 0. 03 7 0. 09 0 0. 03 1 0. 00 73 0. 06 2 0. 02 5 0. 02 9 0. 09 9 0. 04 5 0. 01 2 34 ye as t 0. 53 0. 55 0. 22 0. 48 0. 66 0. 42 0. 03 8 0. 48 0. 23 0. 42 0. 09 9 0. 53 0. 02 0"}, {"heading": "B Figures", "text": "Fi g.\n6 Pr\not ot\nyp e\nN ea\nre st\nN ei\ngh bo\nrR el\nat io\nns G\nra ph\n(E C\nG )\n11 22 33 44 55 66\n77\n88\n99\n1010\n1111\n1212\n1313\n1414\n1515\n1616\n1717 1818\n1919\n2020\n2121\n2222\n2323\n2424\n2525 2626\n2727\n2929 3030\n3131\n3232\n3434\n11 22 33\n44\n55\n66\n77\n88\n99\n1010\n1111\n1212\n1313\n1414\n1515\n1616\n1717\n1818\n1919\n2020\n2121\n2222\n2323\n2424\n2525\n2626\n2727\n2828\n2929\n3030\n3131 3232\n3333\n3434\n11\n22\n33\n44\n55\n66\n77 88\n99\n1010\n1111\n1212\n1313\n1414\n1515\n1616 1717 1818\n1919\n2020\n2121\n2222\n2323\n2424 2525\n2626\n2727\n2828\n2929\n3030\n3131\n3232\n3333\n3434\n11 22 33\n44\n55\n66\n77\n88\n99\n1010\n1111\n1212\n1313\n1414\n1515\n1616\n1717\n1818\n1919\n2020\n2121\n2222\n2323\n2424\n2525\n2626\n2727\n2828\n2929\n3030\n3131\n3232\n3333\n3434\n11 22 33 44 55 66\n77 88 99 1010\n1111 1212 1313 1414\n1515 1616\n1717 1818 1919 2020 2121 2222\n2323 2424 2525 2626 2727 2828\n2929 3030\n3131 3232 3333 3434\n11\n22 33\n44 55\n66\n77 88 99\n1010 1111\n1212\n1313\n1414\n1515\n1616 1717 1818\n1919\n2020\n2121\n2222\n2323\n2424\n2525 2626\n2727\n2828\n2929\n3030\n3131\n3232\n3333 3434 # \ufffd\ufffd \ufffd\ufffd # \ufffd\ufffd \ufffd\ufffd # \ufffd\ufffd \ufffd\ufffd # \ufffd\ufffd \ufffd\ufffd \ufffd # \ufffd\ufffd \ufffd\ufffd # \ufffd\ufffd \ufffd\ufffd 0. 2 0. 4\n0. 6\n0. 8\nE R R\n0. 1 0. 2 0. 3 0. 4 0. 5 0. 6 0. 7S LR\nFi g.\n7 E\nrr or\nan d\nSe le\nct io\nn R\nat es\n(V ec\nto rD\nat a)"}, {"heading": "C Definition of Pareto-rank", "text": "Let us denote the multi-objective tuple of an algorithm i as f(i) = (ERRi,SLRi). The dominance between two algorithms i and j is defined as follows: algorithm i is dominated by j, if j is better than i in at least one objective and also equal to or better than i in all other objectives.\nWe denote by fi \u227a fj that fi is dominated by fj , and by fi \u2280 fj that fi is not dominated by fj . The exact definition of dominated and non-dominated relations can be written as follows.\nfi \u227a fj if (ERRi \u2264 ERRj \u2228 SLRi \u2264 SLRj) \u2227 (ERRi < ERRj \u2228 SLRi < SLRj) (16) fi \u2280 fj otherwise (17)\nGiven a set of tuples P = {(ERRi,SLRi)}pi=i, the Pareto frontier is the subset of P whose members are not dominated by any member of P . Extending the concept of the Pareto frontier, we can divide the algorithms into separate groups by iteratively finding and removing the Pareto frontier from the set until no algorithms remain. We then define the Pareto-rank as the number of groups of algorithms by which it is dominated.\nLet Fi denote the subset of P with the Pareto-rank i. The exact definition, for F1 and Fi : i > 2 respectively, is given as follows.\nF1 = {f \u2208 P : f \u2280 g \u2200g \u2208 P} (18) Fi = { f \u2208 P : (f \u227a g \u2200g \u2208 Fi\u22121) \u2227 ( f \u2280 g \u2200g \u2208 P \\ i\u22121 \u222a h=1 Fh )} (19)"}], "references": [{"title": "Keel: a software tool to assess evolutionary algorithms for data mining problems", "author": ["J. Alcal\u00e0-Fdez", "L. S\u00e1nchez", "S. Garc\u0131\u0301a", "M. del Jesus", "S. Ventura", "J. Garrell", "J. Otero", "C. Romero", "J. Bacardit", "V. Rivas", "J. Fern\u00e1ndez", "F. Herrera"], "venue": "Soft Computing 13(3), 307\u2013318", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM 51(1), 117\u2013122", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast nearest neighbor condensation for large data sets classification", "author": ["F. Angiulli"], "venue": "IEEE Trans. on Knowl. and Data Eng. 19(11), 1450\u20131464", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "UCI Machine Learning Repository", "author": ["K. Bache", "M. Lichman"], "venue": "http://archive.ics.uci.edu/ml", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Using Dynamic Time Warping to Find Patterns in Time Series", "author": ["D.J. Berndt", "J. Clifford"], "venue": "Proceedings of KDD-94: AAAI Workshop on Knowledge Discovery in Databases, pp. 359\u2013370. Seattle, Washington", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1994}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Advances in instance selection for instance-based learning algorithms", "author": ["H. Brighton", "C. Mellish"], "venue": "Data Min. Knowl. Discov. 6(2), 153\u2013172", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Using evolutionary algorithms as instance selection for data reduction in kdd: An experimental study", "author": ["J.R. Cano", "F. Herrera", "M. Lozano"], "venue": "Trans. Evol. Comp 7(6), 561\u2013575", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "The UCR time series classification archive", "author": ["Y. Chen", "E. Keogh", "B. Hu", "N. Begum", "A. Bagnall", "A. Mueen", "G. Batista"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "A fast and elitist multiobjective genetic algorithm: NSGA-II", "author": ["K. Deb", "A. Pratap", "S. Agarwal", "T. Meyarivan"], "venue": "IEEE Trans. on Evolutionary Computation 6, 182\u2013197", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "J. Mach. Learn. Res. 7, 1\u201330", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Condensed and edited nearest neighbor rules", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "A Probabilistic Theory of Pattern Recognition, Stochastic Modelling and Applied Probability, vol. 31, pp. 303\u2013313. Springer New York", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "Querying and mining of time series data: Experimental comparison of representations and distance measures", "author": ["H. Ding", "G. Trajcevski", "P. Scheuermann", "X. Wang", "E. Keogh"], "venue": "Proc. VLDB Endow. 1(2), 1542\u20131552", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "A novel template reduction approach for the k-nearest neighbor method", "author": ["H.A. Fayed", "A.F. Atiya"], "venue": "Trans. Neur. Netw. 20(5), 890\u2013896", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayesian instance selection for the nearest neighbor rule", "author": ["S. Ferrandiz", "M. Boull\u00e9"], "venue": "Mach. Learn. 81(3), 229\u2013256", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Prototype reduction using an artificial immune model", "author": ["U. Garain"], "venue": "Pattern Anal. Appl. 11(3-4), 353\u2013363", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "A memetic algorithm for evolutionary prototype selection: A scaling up approach", "author": ["S. Garc\u0131\u0301a", "J.R. Cano", "F. Herrera"], "venue": "Pattern Recogn. 41(8), 2693\u20132709", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Prototype selection for nearest neighbor classification: Taxonomy and empirical study", "author": ["S. Garcia", "J. Derrac", "J. Cano", "F. Herrera"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 34(3), 417\u2013435", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Constructing ensembles of classifiers by means of weighted instance selection", "author": ["N. Garc\u0131\u0301a-Pedrajas"], "venue": "Trans. Neur. Netw. 20(2), 258\u2013277", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Boosting instance selection algorithms", "author": ["N. Garc\u0131\u0301a-Pedrajas", "A. De Haro-Garc\u0131\u0301a"], "venue": "Know.-Based Syst. 67, 342\u2013360", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Cutting-plane Training of Structural SVMs", "author": ["T. Joachims", "T. Finley", "C.N.J. Yu"], "venue": "Mach. Learn. 77, 27\u201359", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Classification of Multivariate Time Series and Structured Data Using Constructive Induction", "author": ["M.W. Kadous", "C. Sammut"], "venue": "Mach. Learn. 58(2-3), 179\u2013216", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast minimization of structural risk by nearest neighbor rule", "author": ["B. Karacali", "H. Krim"], "venue": "Trans. Neur. Netw. 14(1), 127\u2013137", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Iterative Methods for Optimization", "author": ["C. Kelley"], "venue": "Frontiers in Applied Mathematics. Society for Industrial and Applied Mathematics", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Maximum margin multiclass nearest neighbors", "author": ["A. Kontorovich", "R. Weiss"], "venue": "Proceedings of The 31st International Conference on Machine Learning, vol. 32, pp. 892\u2013900", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "A clustering method for automatic biometric template selection", "author": ["A. Lumini", "L. Nanni"], "venue": "Pattern Recognition 39(3), 495 \u2013 497", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Class conditional nearest neighbor for large margin instance selection", "author": ["E. Marchiori"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell. 32(2), 364\u2013370", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Distribution-free multiple comparisons", "author": ["P. Nemenyi"], "venue": "Ph.D. thesis, Princeton University", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1963}, {"title": "Generalized feature extraction for structural pattern recognition in time-series data", "author": ["R.T. Olszewski"], "venue": "Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA, USA", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}, {"title": "A new fast prototype selection method based on clustering", "author": ["J.A. Olvera-L\u00f3pez", "J.A. Carrasco-Ochoa", "J.F. Mart\u0131\u0301nez-Trinidad"], "venue": "Pattern Anal. Appl. 13(2), 131\u2013141", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A review of instance selection methods", "author": ["J.A. Olvera-L\u00f3pez", "J.A. Carrasco-Ochoa", "J.F. Mart\u0131\u0301nez-Trinidad", "J. Kittler"], "venue": "Artif. Intell. Rev. 34(2), 133\u2013143", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms", "author": ["L. Paulev\u00e9", "H. J\u00e9gou", "L. Amsaleg"], "venue": "Pattern Recogn. Lett. 31(11), 1348\u20131358", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Prototype selection for dissimilarity-based classifiers", "author": ["E. Pkalska", "R.P.W. Duin", "P. Pacl\u0131\u0301k"], "venue": "Pattern Recogn. 39(2), 189\u2013208", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Finding representative patterns with ordered projections", "author": ["J.C. Riquelme", "J.S. Aguilar-Ruiz", "M. Toro"], "venue": "Pattern Recognition 36(4), 1009 \u2013 1018", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "Prototype and feature selection by sampling and random mutation hill climbing algorithms", "author": ["D.B. Skalak"], "venue": "Proceedings of the Eleventh International Conference on Machine Learning, pp. 293\u2013301. Morgan Kaufmann", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1994}, {"title": "Sv-knnc: An algorithm for improving the efficiency of k-nearest neighbor", "author": ["A. Srisawat", "T. Phienthrakul", "B. Kijsirikul"], "venue": "Q. Yang, G. Webb (eds.) PRICAI 2006: Trends in Artificial Intelligence, Lecture Notes in Computer Science, vol. 4099, pp. 975\u2013 979. Springer Berlin Heidelberg", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "A taxonomy and experimental study on prototype generation for nearest neighbor classification", "author": ["I. Triguero", "J. Derrac", "S. Garcia", "F. Herrera"], "venue": "Trans. Sys. Man Cyber Part C 42(1), 86\u2013100", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "J. Mach. Learn. Res. 10, 207\u2013244", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Reduction techniques for instance-basedlearning algorithms", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Mach. Learn. 38(3), 257\u2013286", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "The Top Ten Algorithms in Data Mining, 1st edn", "author": ["X. Wu", "V. Kumar"], "venue": "Chapman & Hall/CRC", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast Time Series Classification using Numerosity Reduction", "author": ["X. Xi", "E. Keogh", "C. Shelton", "L. Wei", "C.A. Ratanamahatana"], "venue": "ICML \u201906: Proceedings of the 23rd International Conference on Machine Learning, pp. 1033\u20131040. ACM, New York, NY, USA", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2006}, {"title": "Optimal reference subset selection for nearest neighbor classification by tabu search", "author": ["H. Zhang", "G. Sun"], "venue": "Pattern Recognition 35(7), 1481 \u2013 1490", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 30, "context": "Keywords Nearest neighbor classifier, Prototype selection, Soft maximum, Large-margin learning 1 Introduction Applying the nearest neighbor rule based on a set of prototype instances is one of the most widely used models for classification and a typical example of non-parametric, instance-based learning algorithms [31,40].", "startOffset": 316, "endOffset": 323}, {"referenceID": 39, "context": "Keywords Nearest neighbor classifier, Prototype selection, Soft maximum, Large-margin learning 1 Introduction Applying the nearest neighbor rule based on a set of prototype instances is one of the most widely used models for classification and a typical example of non-parametric, instance-based learning algorithms [31,40].", "startOffset": 316, "endOffset": 323}, {"referenceID": 6, "context": "It is a critical problem for practical applications due to the large cost of storing and computing distances in the neighbor algorithms [7].", "startOffset": 136, "endOffset": 139}, {"referenceID": 36, "context": "Improvements by sophisticated hybridization of such techniques have also been reported [37].", "startOffset": 87, "endOffset": 91}, {"referenceID": 30, "context": "Alternatively, critical parameters are selected empirically through cross validation and wrapper methods [31].", "startOffset": 105, "endOffset": 109}, {"referenceID": 1, "context": "2 Shin Ando Recently, the emergence of Big Data has ignited more approaches to efficiently compute the nearest neighbor algorithm, such as Locality Sensitive Hashing and Approximate Nearest Neighbor Search [2,32].", "startOffset": 206, "endOffset": 212}, {"referenceID": 31, "context": "2 Shin Ando Recently, the emergence of Big Data has ignited more approaches to efficiently compute the nearest neighbor algorithm, such as Locality Sensitive Hashing and Approximate Nearest Neighbor Search [2,32].", "startOffset": 206, "endOffset": 212}, {"referenceID": 17, "context": "First, the state-of-the-art prototype selection methods have the advantage in terms of classification accuracy and are competitive in terms of efficiency compared to ANNS [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 21, "context": ", in time series classification, the Euclidean distance is often problematic due to the sequential structure of the time series [22], and a dissimilarity function based on non-linear warping is known to be highly effective [13,41].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": ", in time series classification, the Euclidean distance is often problematic due to the sequential structure of the time series [22], and a dissimilarity function based on non-linear warping is known to be highly effective [13,41].", "startOffset": 223, "endOffset": 230}, {"referenceID": 40, "context": ", in time series classification, the Euclidean distance is often problematic due to the sequential structure of the time series [22], and a dissimilarity function based on non-linear warping is known to be highly effective [13,41].", "startOffset": 223, "endOffset": 230}, {"referenceID": 17, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 56, "endOffset": 60}, {"referenceID": 38, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 102, "endOffset": 112}, {"referenceID": 19, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 102, "endOffset": 112}, {"referenceID": 26, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 102, "endOffset": 112}, {"referenceID": 13, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 137, "endOffset": 144}, {"referenceID": 32, "context": "The problem has been referred to as prototype selection [18], data reduction [39], instance selection [19,20,27], and template reduction [14,33].", "startOffset": 137, "endOffset": 144}, {"referenceID": 6, "context": ", an operation referred to as editing, eliminates or relocates noisy prototypes that cause misclassifications near the borders of different classes [7,39], and have the effects of smoothing and generalizing the decision boundaries.", "startOffset": 148, "endOffset": 154}, {"referenceID": 38, "context": ", an operation referred to as editing, eliminates or relocates noisy prototypes that cause misclassifications near the borders of different classes [7,39], and have the effects of smoothing and generalizing the decision boundaries.", "startOffset": 148, "endOffset": 154}, {"referenceID": 2, "context": "Condensation is another typical operation which discards prototypes far from the class borders in order to reduce the redundancy of the prototype set without affecting the decision boundary [3].", "startOffset": 190, "endOffset": 193}, {"referenceID": 33, "context": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification 3 as feature analysis [34] and clustering [30,37].", "startOffset": 103, "endOffset": 107}, {"referenceID": 29, "context": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification 3 as feature analysis [34] and clustering [30,37].", "startOffset": 123, "endOffset": 130}, {"referenceID": 36, "context": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification 3 as feature analysis [34] and clustering [30,37].", "startOffset": 123, "endOffset": 130}, {"referenceID": 25, "context": "are used to analyze boundaries and interior instances in hybrid methods [26,36].", "startOffset": 72, "endOffset": 79}, {"referenceID": 35, "context": "are used to analyze boundaries and interior instances in hybrid methods [26,36].", "startOffset": 72, "endOffset": 79}, {"referenceID": 30, "context": "The implementations of these techniques are generally categorized into wrapper and filtering methods [31].", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "Heuristic combinatorial search such as evolutionary algorithms, a memetic algorithm, an artificial immune model, and tabu search, have been explored to find compact subsets of the training instances [8,16,17,42].", "startOffset": 199, "endOffset": 211}, {"referenceID": 15, "context": "Heuristic combinatorial search such as evolutionary algorithms, a memetic algorithm, an artificial immune model, and tabu search, have been explored to find compact subsets of the training instances [8,16,17,42].", "startOffset": 199, "endOffset": 211}, {"referenceID": 16, "context": "Heuristic combinatorial search such as evolutionary algorithms, a memetic algorithm, an artificial immune model, and tabu search, have been explored to find compact subsets of the training instances [8,16,17,42].", "startOffset": 199, "endOffset": 211}, {"referenceID": 41, "context": "Heuristic combinatorial search such as evolutionary algorithms, a memetic algorithm, an artificial immune model, and tabu search, have been explored to find compact subsets of the training instances [8,16,17,42].", "startOffset": 199, "endOffset": 211}, {"referenceID": 11, "context": "Theoretical approaches for prototype selection shown the upper-bound of generalization error based on VC dimensions, structural risk minimization, and Bayesian analysis [12,15,23].", "startOffset": 169, "endOffset": 179}, {"referenceID": 14, "context": "Theoretical approaches for prototype selection shown the upper-bound of generalization error based on VC dimensions, structural risk minimization, and Bayesian analysis [12,15,23].", "startOffset": 169, "endOffset": 179}, {"referenceID": 22, "context": "Theoretical approaches for prototype selection shown the upper-bound of generalization error based on VC dimensions, structural risk minimization, and Bayesian analysis [12,15,23].", "startOffset": 169, "endOffset": 179}, {"referenceID": 1, "context": "In recent years, Approximate Nearest Neighbor Search (ANNS) [2] have drawn strong interest and various techniques have been developed for image classification.", "startOffset": 60, "endOffset": 63}, {"referenceID": 17, "context": "As shown in the recent survey [18], the prototype selection methods aimed at reducing the generalization error achieve better accuracy than ANNS.", "startOffset": 30, "endOffset": 34}, {"referenceID": 37, "context": "Large-margin Nearest Neighbor Classification [38] addresses the problem of distance metric learning as a constrained optimization problem to minimize the violation of the nearest neighbor rule with regards to the affine transformation.", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "Max-margin Multiclass Nearest Neighbors [25] also learns the metric space based on the minimization of its entropy.", "startOffset": 40, "endOffset": 44}, {"referenceID": 5, "context": "2 Soft-Maximum Function The soft maximum [6] is an approximation of the function max(\u00b7, \u00b7) as log (exp(\u00b7) + exp(\u00b7)).", "startOffset": 41, "endOffset": 44}, {"referenceID": 23, "context": "Given the form of (14), the problem is solved by a constrained gradient descent [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "Algorithm 2 is a modification of the Structural SVM [21] learning algorithm, which obtains the approximate solution to the linear SVM problem using the cutting-plane method.", "startOffset": 52, "endOffset": 56}, {"referenceID": 3, "context": "1 Datasets For the first part of the experiment, we employed a collection of 34 datasets with vector features from the UCI Machine Learning Repository [4] and the KEEL datasets [1].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "1 Datasets For the first part of the experiment, we employed a collection of 34 datasets with vector features from the UCI Machine Learning Repository [4] and the KEEL datasets [1].", "startOffset": 177, "endOffset": 180}, {"referenceID": 8, "context": "In the second part of the experiment, we use a collection of 27 datasets from the UCR Time Series Dataset Repository [9].", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "The input data for time series classification is given as a dissimilarity matrix of Dynamic Time Warping (DTW) [5], which is known to be highly effective in time series classification [13].", "startOffset": 111, "endOffset": 114}, {"referenceID": 12, "context": "The input data for time series classification is given as a dissimilarity matrix of Dynamic Time Warping (DTW) [5], which is known to be highly effective in time series classification [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 17, "context": "2 Baseline Methods We selected five recent prototype selection methods, which ranked highly among the 42 algorithm reported in a recent survey [18], as baselines for comparative analysis.", "startOffset": 143, "endOffset": 147}, {"referenceID": 26, "context": "Class conditional instance selection (CCIS) uses pairwise relations among prototypes to define the selection criteria [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 2, "context": "Fast condensed nearest neighbor (FCNN) exploits the condensation operation for prototype selection [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 38, "context": "DROP3 uses a decremental procedure to reduce prototypes and is fast, efficient, and accurate among algorithms of the same approach [39].", "startOffset": 131, "endOffset": 135}, {"referenceID": 34, "context": "Random Mutation Hill Climbing (RMHC) uses stochastic hill climbing search to explore the prototype subset combination space [35].", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "All baseline algorithms were executed in the KEEL software [1].", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "First, we introduce the concept of the Pareto frontier and non-dominated sorting, commonly used in multi-objective population-based search algorithms [10], to compute the ranks of the algorithms based on two evaluation measures.", "startOffset": 150, "endOffset": 154}, {"referenceID": 10, "context": "In [11], Wilcoxon\u2019s signed ranks test and Friedman-Nemenyi test are recommended for comparison of two classifiers and three or more classifiers over multiple datasets, respectively.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "We also note that the Friedman-Nemenyi test is highly conservative, due to the substantial loss of power for handling unreplicated blocked data [28].", "startOffset": 144, "endOffset": 148}, {"referenceID": 17, "context": "SSMA and RMHC were two best algorithms in [18], which indicates that REPS has a significant advantage against many prototype selection algorithms and is competitive with the state-of-the-art.", "startOffset": 42, "endOffset": 46}, {"referenceID": 28, "context": "The ECG time series consists of positive (abnormal) and negative (normal) class examples and pose a binary classification problem [29].", "startOffset": 130, "endOffset": 134}], "year": 2016, "abstractText": "Abstract The nearest neighbor rule is one of the most widely used models for classification, and selecting a compact set of prototype instances is a primary challenges for its applications. Many existing approaches for prototype selection exploit instance-based analyses and locally-defined criteria on the class distribution, which are intractable for numerical optimization techniques. In this paper, we explore a parametric framework with an adjusted nearest neighbor rule, in which the selection of the neighboring prototypes is modified by their respective parameters. The framework allows us to formulate a minimization problem of the violation of the adjusted nearest neighbor rule over the training set with regards to numerical parameters. We show that the problem reduces to a large-margin principled learning and demonstrate its advantage by empirical comparisons with recent state-ofthe-art methods using public benchmark data.", "creator": "LaTeX with hyperref package"}}}