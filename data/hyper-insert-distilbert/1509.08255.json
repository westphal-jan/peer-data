{"id": "1509.08255", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2015", "title": "Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory", "abstract": "students in the decade work since 2013 jeff brent hawkins proposed hierarchical 2d temporal multimedia memory ( htm ) as a model used of \u201e neocortical cognitive computation, since the theory and the algorithms \" have considerably evolved dramatically. this paper together presents a simple detailed complexity description of [ htm ', s cortical fuzzy learning algorithm ( cla ), clearly including for the first time a rigorous mathematical analogy formulation of showing all aspects modes of coding the quantum computations. computer prediction assisted cla ( pacla ), how a refinement paper of the cla is presented, which is theoretically both particularly closer to explaining the neuroscience and importantly adds significantly higher to the computational theoretical power. from finally, we summarise respectively the key intelligence functions of both neocortex which are expressed in pacla mathematical implementations.", "histories": [["v1", "Mon, 28 Sep 2015 09:54:08 GMT  (1306kb,D)", "https://arxiv.org/abs/1509.08255v1", null], ["v2", "Thu, 8 Oct 2015 16:13:44 GMT  (1306kb,D)", "http://arxiv.org/abs/1509.08255v2", "Updated reference to unofficial revision of Hawkins and Ahmad, 2011"]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["fergal byrne"], "accepted": false, "id": "1509.08255"}, "pdf": {"name": "1509.08255.pdf", "metadata": {"source": "CRF", "title": "Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory", "authors": ["Fergal Byrne"], "emails": ["fergal@brenter.ie"], "sections": [{"heading": null, "text": "Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory\nFergal Byrne HTM Theory Group, Dublin, Ireland\nfergal@brenter.ie http://inbits.com"}, {"heading": "In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory", "text": "(HTM) as a model of neocortical computation, the theory and the algorithms have evolved dramatically. This paper presents a detailed description of HTM\u2019s Cortical Learning Algorithm (CLA), including for the first time a rigorous mathematical formulation of all aspects of the computations. Prediction Assisted CLA (paCLA), a refinement of the CLA, is presented, which is both closer to the neuroscience and adds significantly to the computational power. Finally, we summarise the key functions of neocortex which are expressed in paCLA implementations. An Open Source project, Comportex, is the leading implementation of this evolving theory of the brain."}, {"heading": "1 Introduction", "text": "We present an up-to-date description of Hierarchical Temporal Memory (HTM) which includes a mathematical model of its computations, developed for this paper. The description and mathematics are presented here in order to provide an axiomatic basis for understanding the computational power of each component in a HTM system, as well as a foundation for comparing HTM computational models with empirical evidence of strcture and function in neocortex.\nIn particular, we demonstrate the following:\n1. A layer of HTM neurons automatically learns to efficiently represent sensory and sensorimotor inputs using semantic encodings in the form of Sparse Distributed Representations\nar X\niv :1\n50 9.\n08 25\n5v 2\n[ cs\n.N E\n] 8\nO ct\n2 01\n5\n(SDRs). These representations are robust to noise and missing or masked inputs, and generalise gracefully in a semantically useful manner.\n2. A HTM layer automatically learns high-dimensional transitions between SDRs, makes predictions of the future evolution of its inputs, detects anomalies in the dynamics, and learns high-order sequences of sensory or sensorimotor patterns.\n3. Transition Memory uses temporal and hierarchical context to assist recognition of feedforward patterns, enhancing bottom-up input pattern recognition and providing for stabilisation in the face of uncertainty.\n4. HTM\u2019s Temporal Pooling models the learnable processing of fast-changing inputs in Layer 4 of cortex into slower-changing, stable representations in Layer 2/3 of sequences, orbits and trajectories of L4 SDRs."}, {"heading": "2 Hierarchical Temporal Memory and the Cortical Learning", "text": "Algorithm\nHierarchical Temporal Memory was developed by Jeff Hawkins and Dileep George [George and Hawkins, 2009] and substantially refined by Hawkins and his colleagues at Numenta. The most recent description produced by Numenta, Hawkins and Ahmad [2011] has been updated by the author in Byrne [2015]. HTM is a model of cortex in which each region in a hierarchy performs an identical process on its own inputs, forming sequence memories of recognised spatial patterns. The Cortical Learning Algorithm (CLA) describes in detail how each region works.\nThe HTM Model Neuron The model neuron in HTM is substantially more realistic and complex when compared to a point neuron in Artificial Neural Networks, such as the McCullochPitts neuron [McCulloch and Pitts, 1943], that led to Rosenblatt\u2019s perceptron. An ANN neuron simply passes its inputs (weighted by synaptic strengths and summed) through a nonlinearity such as a sigmoid or rectifier. The extra complexity is intended to more carefully resemble the structure and function of real cortical neurons, while remaining simple compared with models based on electrical characteristics.\nThe HTM model neuron has two kinds of dendrites, in order to process its two kinds of input. Feedforward inputs appear on a dendrite segment which is adjacent, or proximal, to the cell body (soma), and the sum of these inputs is directly fed into the soma. In addition, however, the cell has a greater number of distal dendrite segments, each of which is an active unit capable of detecting the coincident activity of its own inputs. The distal inputs are from nearby cells in the same layer, as well as cells in higher regions of the network. These cells provide predictive context to assist the neuron in forming a decision to fire. Each distal segment learns to recognise its own set of contextual patterns, and provides input to the neuron only when sufficient input activity appears on its synapses.\nFor simplicity, CLA uses binary activation vectors to communicate between neurons. This is justified by empirical evidence regarding the very high failure rate of individual synapses and the inherent noise found in biological neuron circuits.\nHTM Mini-columns Real layers of cortex have been found to organise their neurons in minicolumns of about 30 cells, which have strongly correlated feedforward response. CLA relies on an interpretation of this structure in order to model prediction and sequence memory. In\nHawkins\u2019 original design, the column and its contained cells play separate, but co-operative roles in the computational model. We present here a more integrated design for the minicolumn, which provides extra power in the computation and directly produces a semantic interpretation of the representations. We\u2019ll return to mini-columns when describing prediction.\nSparse Distributed Representations (SDRs) A final key feature of HTM is the Sparse Distributed Representation (SDR), which is a very sparse binary representation in which each active bit has some semantic meaning. For further detail on SDRs, see Ahmad and Hawkins [2015].\nThe next section describes the processes in CLA in more detail, and provides a full mathematical description in terms of vector operations."}, {"heading": "3 Pattern Memory (aka Spatial Pooling)", "text": "We\u2019ll begin to describe the details and mathematics of HTM by describing the simplest operation in HTM\u2019s Cortical Learning Algorithm: Pattern Memory, also known as Spatial Pooling, forms a Sparse Distributed Representation from a binary feedforward input vector. Pattern Memory is a kind of learned spatial pattern recognition, capable of identifying and representing single inputs.\nWe begin with a layer (a 1- or 2-dimensional array) of single neurons, which will form a pattern of activity aimed at efficiently representing the input vectors."}, {"heading": "3.1 Feedforward Processing on Proximal Dendrites", "text": "The HTM model neuron has a single proximal dendrite, which is used to process and recognise feedforward or afferent inputs to the neuron. We model the entire feedforward input to a cortical layer as a bit vector xFF \u2208 {0, 1}nFF , where nFF is the width of the input.\nThe dendrite is composed of ns synapses which each act as a binary gate for a single bit in the input vector. Each synapse i has a permanence pi \u2208 [0, 1] which represents the size and\nefficiency of the dendritic spine and synaptic junction. The synapse will transmit a 1-bit (or on-bit) if the permanence exceeds a threshold \u03b8i (often a global constant \u03b8i = \u03b8 = 0.2). When this is true, we say the synapse is connected.\nEach neuron samples ns bits from the nFF feedforward inputs, and so there are ( nFF ns ) possible choices of input for a single neuron. A single proximal dendrite represents a projection \u03c0j : {0, 1}nFF \u2192 {0, 1}ns , so a population of neurons corresponds to a set of subspaces of the sensory space. Each dendrite has an input vector xj = \u03c0j(xFF) which is the projection of the entire input into this neuron\u2019s subspace.\nA synapse is connected if its permanence pi exceeds its threshold \u03b8i. If we subtract p \u2212 ~\u03b8, take the elementwise sign of the result, and map to {0, 1}, we derive the binary connection vector cj for the dendrite. Thus:\nci = (1 + sgn(pi \u2212 \u03b8i))/2\nThe dot product oj(x) = cj \u00b7 xj now represents the feedforward overlap of the neuron with the input, ie the number of connected synapses which have an incoming activation potential. Later, we\u2019ll see how this number is used in the neuron\u2019s processing.\nThe elementwise product oj = cj xj is the vector in the neuron\u2019s subspace which represents the input vector xFF as \u201dseen\u201d by this neuron. This is known as the overlap vector. The length oj = \u2016oj\u2016`1 of this vector corresponds to the extent to which the neuron recognises the input, and the direction (in the neuron\u2019s subspace) is that vector which has on-bits shared by both the connection vector and the input.\nIf we project this vector back into the input space, the result x\u0302j = \u03c0\u22121(oj) is this neuron\u2019s approximation of the part of the input vector which this neuron matches. If we add a set of such vectors, we will form an increasingly close approximation to the original input vector as we choose more and more neurons to collectively represent it."}, {"heading": "3.2 Sparse Distributed Representations (SDRs)", "text": "We now show how a layer of neurons transforms an input vector into a sparse representation. From the above description, every neuron is producing an estimate x\u0302j of the input xFF, with length oj nFF reflecting how well the neuron represents or recognises the input. We form a sparse representation of the input by choosing a set YSDR of the top nSDR = sN neurons, whereN is the number of neurons in the layer, and s is the chosen sparsity we wish to impose (typically\ns = 0.02 = 2%). The algorithm for choosing the top nSDR neurons may vary. In neocortex, this is achieved using a mechanism involving cascading inhibition: a cell firing quickly (because it depolarises quickly due to its input) activates nearby inhibitory cells, which shut down neighbouring excitatory cells, and also nearby inhibitory cells, which spread the inhibition outwards. This type of local inhibition can also be used in software simulations, but it is expensive and is only used where the design involves spatial topology (ie where the semantics of the data is to be reflected in the position of the neurons). A more efficient global inhibition algorithm - simply choosing the top nSDR neurons by their depolarisation values - is often used in practise.\nIf we form a bit vector ySDR \u2208 {0, 1}N where yj = 1\u21d4 j \u2208 YSDR, we have a function which maps an input xFF \u2208 {0, 1}nFF to a sparse output ySDR \u2208 {0, 1}N , where the length of each output vector is \u2016ySDR\u2016`1 = sN N .\nThe reverse mapping or estimate x\u0302 of the input vector by the set YSDR of neurons in the SDR is given by the sum:\n\u2211 j\u2208YSDR x\u0302j = \u2211 YSDR \u03c0\u22121j (oj) = \u2211 YSDR \u03c0\u22121j (cj xj) = \u2211 YSDR \u03c0\u22121j (cj \u03c0j(xFF)) = \u2211 j\u2208YSDR \u03c0\u22121j (cj) xFF"}, {"heading": "3.3 Matrix Form", "text": "The above can be represented straightforwardly in matrix form. The projection \u03c0j : {0, 1}nFF \u2192 {0, 1}ns can be represented as a matrix \u03a0j \u2208 {0, 1}ns\u00d7 nFF .\nAlternatively, we can stay in the input space Bnff , and model \u03c0j as a vector ~\u03c0j = \u03c0\u22121j (1ns), ie where \u03c0j,i = 1\u21d4 (\u03c0\u22121j (1ns))i = 1.\nThe elementwise product ~xj = \u03c0\u22121j (xj) = ~\u03c0j xFF represents the neuron\u2019s view of the input vector xFF.\nWe can similarly project the connection vector for the dendrite by elementwise multiplication: ~cj = \u03c0\u22121j (cj), and thus ~oj(xFF) = ~cj xFF is the overlap vector projected back into BnFF , and the dot product oj(xFF) = ~cj \u00b7 xFF gives the same overlap score for the neuron given xFF as input. Note that ~oj(xFF) = x\u0302j , the partial estimate of the input produced by neuron j.\nWe can reconstruct the estimate of the input by an SDR of neurons YSDR:\nx\u0302SDR = \u2211 j\u2208YSDR x\u0302j = \u2211 j\u2208YSDR ~oj = \u2211 j\u2208YSDR ~cj xFF = CSDRxFF\nwhere CSDR is a matrix formed from the ~cj for j \u2208 YSDR."}, {"heading": "3.4 Learning in HTM as an Optimisation Problem", "text": "We can now measure the distance between the input vector xFF and the reconstructed estimate x\u0302SDR by taking a norm of the diference. Using this, we can frame learning in HTM as an optimisation problem. We wish to minimise the estimation error over all inputs to the layer. Given a set of (usually random) projection vectors ~\u03c0j for the N neurons, the parameters of the model are the permanence vectors ~pj , which we adjust using a simple Hebbian update model.\nThe update model for the permanence of a synapse pi on neuron j is:\np (t+1) i =  (1 + \u03b4inc)p (t) i if j \u2208 YSDR, (xj)i = 1 and p (t) i \u2265 \u03b8i (1\u2212 \u03b4dec)p(t)i if j \u2208 YSDR and ((xj)i = 0 or p (t) i < \u03b8i)\np (t) i otherwise\nThis update rule increases the permanence of active synapses, those that were connected to an active input when the cell became active, and decreases those which were either disconnected or received a zero when the cell fired. In addition to this rule, an external process gently boosts synapses on cells which either have a lower than target rate of activation, or a lower than target average overlap score.\nIn the visualisation above (see Figure 4), this will tend to move the vectors belonging to successful neurons closer to the input vector by increasing the number of overlapping synapses, and it will also make the vectors more likely to remain active even in the face of noise in the input."}, {"heading": "3.5 Computational Power of SDR-forming Circuits", "text": "An SDR is a form of k-winner-takes-all (k-WTA) representation. Maass [2000] proves that a single k-WTA gate has the same computational power as a polynomially larger multi-layer network of threshold artificial neurons, and that the soft (continuous) version can approximate any continuous function (exactly as a multilayer ANN network can - see Maass [1997])."}, {"heading": "4 Transition Memory - Making Predictions", "text": "We saw how a layer of neurons learns to form a Sparse Distributed Representation (SDR) of an input pattern. In this section, we\u2019ll describe the process of learning temporal sequences.\nWe showed earlier that the HTM model neuron learns to recognise subpatterns of feedforward input on its proximal dendrites. This is somewhat similar to the manner by which a Restricted Boltzmann Machine can learn to represent its input in an unsupervised learning process. One distinguishing feature of HTM is that the evolution of the world over time is a critical aspect of what, and how, the system learns. The premise for this is that objects and processes in the world persist over time, and may only display a portion of their structure at any given moment. By learning to model this evolving revelation of structure, the neocortex can more efficiently recognise and remember objects and concepts in the world."}, {"heading": "4.1 Distal Dendrites and Prediction", "text": "In addition to its one proximal dendrite, a HTM model neuron has a collection of distal (far) dendrite segments, or simply dendrites, which gather information from sources other than the feedforward inputs to the layer. In some layers of neocortex, these dendrites combine signals from neurons in the same layer as well as from other layers in the same region, and even receive indirect inputs from neurons in higher regions of cortex. We will describe the structure and function of each of these.\nThe simplest case involves distal dendrites which gather signals from neurons within the same layer.\nEarlier, we showed that a layer of N neurons converted an input vector x \u2208 Bnff into a SDR ySDR \u2208 BN , with length \u2016ySDR\u2016`1 = sN N , where the sparsity s is usually of the order of 2% (N is typically 2048, so the SDR ySDR will have 40 active neurons).\nThe layer of HTM neurons can now be extended to treat its own activation pattern as a separate and complementary input for the next timestep. This is done using a collection of distal dendrite segments, which each receive as input the signals from other neurons in the layer itself. Unlike the proximal dendrite, which transmits signals directly to the neuron, each distal dendrite acts individually as an active coincidence detector, firing only when it receives enough signals to exceed its individual threshold.\nWe proceed with the analysis in a manner analogous to the earlier discussion. The input to the distal dendrite segment k at time t is a sample of the bit vector y(t\u22121)SDR . We have nds\ndistal synapses per segment, a permanence vector pk \u2208 [0, 1]nds and a synapse threshold vector ~\u03b8k \u2208 [0, 1]nds , where typically \u03b8i = \u03b8 = 0.2 for all synapses.\nFollowing the process for proximal dendrites, we get the distal segment\u2019s connection vector ck:\nck,i = (1 + sgn(pk,i \u2212 \u03b8k,i))/2\nThe input for segment k is the vector y(t\u22121)k = \u03c6k(y (t\u22121) SDR ) formed by the projection \u03c6k : {0, 1}N\u22121 \u2192 {0, 1}nds from the SDR to the subspace of the segment. There are ( N\u22121 nds ) such projections (there are no connections from a neuron to itself, so there areN\u22121 to choose from). The overlap of the segment for a given y(t\u22121)SDR is the dot product o t k = ck \u00b7 y (t\u22121) k . If this overlap exceeds the threshold \u03bbk of the segment, the segment is active and sends a dendritic spike of size sk to the neuron\u2019s cell body.\nThis process takes place before the processing of the feedforward input, which allows the layer to combine contextual knowledge of recent activity with recognition of the incoming feedforward signals. In order to facilitate this, we will change the algorithm for Pattern Memory as follows.\nEach neuron j begins a timestep t by performing the above processing on its ndd distal dendrites. This results in some number 0 . . . ndd of segments becoming active and sending spikes to the neuron. The total predictive activation potential is given by:\nopred,j = \u2211 otk\u2265\u03bbk sk\nThe predictive potential is combined with the overlap score from the feedforward overlap coming from the proximal dendrite to give the total activation potential:\natj = \u03b1joff,j + \u03b2jopred,j\nand these aj potentials are used to choose the top neurons, forming the SDR YSDR at time t. The mixing factors \u03b1k and \u03b2k are design parameters of the simulation."}, {"heading": "4.2 Learning Predictions", "text": "We use a very similar learning rule for distal dendrite segments as we did for the feedforward inputs:\npi,j (t+1) =  (1 + \u03c3inc)p (t) i if cell j active, segment k active, synapse i active\n(1\u2212 \u03c3dec)p(t)i if cell j active, segment k active, synapse i not active p (t) i otherwise\nAgain, this reinforces synapses which contribute to activity of the cell, and decreases the contribution of synapses which don\u2019t. A boosting rule, similar to that for proximal synapses, allows poorly performing distal connections to improve until they are good enough to use the main rule.\nInterpretation We can now view the layer of neurons as forming a number of representations at each timestep. The field of predictive potentials opred,j can be viewed as a map of the layer\u2019s confidence in its prediction of the next input. The field of feedforward potentials off,j can be viewed as a map of the layer\u2019s recognition of current reality. Combined, these maps allow for prediction-assisted recognition, which, in the presence of temporal correlations between sensory inputs, will improve the recognition and representation significantly.\nWe can quantify the properties of the predictions formed by such a layer in terms of the mutual information between the SDRs at time t and t+ 1. .\nA layer of neurons connected as described here is a Transition Memory, and is a kind of first-order memory of temporally correlated transitions between sensory patterns. This kind of memory may only learn one-step transitions, because the SDR is formed only by combining potentials one timestep in the past with current inputs.\nSince the neocortex clearly learns to identify and model much longer sequences, we need to modify our layer significantly in order to construct a system which can learn high-order sequences. This is the subject of the next section."}, {"heading": "4.3 Higher-order Prediction", "text": "The current Numenta Cortical Learning Algorithm (or CLA, the detailed computational model in HTM) separates feedforward and predictive stages of processing. A modification of this model (which we call prediction-assisted recognition or paCLA) combines these into a single step involving competition between highly predictive pyramidal cells and their surrounding columnar inhibitory sheaths.\nNeural network models generally model a neuron as somehow \u201dcombining\u201d a set of inputs to produce an output. This is based on the idea that input signals cause ion currents to flow\ninto the neuron\u2019s cell body, which raises its voltage (depolarises), until it reaches a threshold level and fires (outputs a signal). paCLA also models this idea, with the added complication that there are two separate pathways (proximal and distal) for input signals to be converted into effects on the voltage of the cell. In addition, paCLA treats the effect of the inputs as a rate of change of potential, rather than as a final potential level as found in standard CLA.\nSlow-motion Timeline of paCLA\nConsider a single column of pyramidal cells in a layer of cortex. Along with the set of pyramidal cells {P1, P2..Pn}, we also model each columnar sheath of inhibitory cells as a single cell I . All the Pi and I are provided with the same feedforward input vector xt, and they also have similar (but not necessarily identical) synaptic connection vectors cPi and cI to those inputs (the bits of xt are the incoming sensory activation potentials, while bit j of a connection vector c is 1 if synapse j is connected). The feedforward overlap offPi(xt) = xt \u00b7 cPi is the output of the proximal dendrite of cell Pi (and similarly for cell I).\nIn addition, each pyramidal cell (but not the inhibitory sheath) receives signals on its distal dendrites. Each dendrite segment acts separately on its own inputs yt\u22121k , which come from other neurons in the same layer as well as other sublayers in the region (and from other regions in some cases). When a dendrite segment k has a sufficient distal overlap, exceeding a threshold \u03bbk, the segment emits a dendritic spike of size sk. The output of the distal dendrites is then given by:\nopred = \u2211 otk\u2265\u03bbk sk\nThe predictive potential is combined with the overlap score from the feedforward overlap coming from the proximal dendrite to give the total depolarisation rate:\ndj = \u2202Vj \u2202t = \u03b1jo ff Pj + \u03b2jo pred Pj\nwhere \u03b1j and \u03b2j are parameters which transform the proximal and distal contributions into a rate of change of potential (and also control the relative effects of feedforward and predictive inputs). For the inhibitory sheath I , there is only the feedforward component \u03b1IoffI , but we assume this is larger than any of the feedforward contributions \u03b1joffPj for the pyramidal cells.\nNow, the time a neuron takes to reach firing threshold is inversely proportional to its depolarisation rate. This imposes an ordering of the set {P1..Pn, I} according to their (prospective)\nfiring times \u03c4Pj = \u03b3P 1 dj (and \u03c4I = \u03b3I 1dI ).\nFormation of the SDR in Transition Memory\nZooming out from the single column to a neighbourhood (or sublayer) L1 of columns Cm, we see that there is a local sequence S in which all the pyramidal cells (and the inhibitory sheaths) would fire if inhibition didn\u2019t take place. The actual sequence of cells which do fire can now be established by taking into account the effects of inhibition.\nLet\u2019s partition the sequence as follows:\nS = Ppred \u2016 Ipred \u2016 Iff \u2016 Pburst \u2016 Ispread\nwhere:\n1. Ppred is the (possibly empty) sequence of pyramidal cells in a highly predictive state, which fire before their inhibitory sheaths (ie Ppred = {P | \u03c4P < \u03c4Im , P \u2208 Cm});\n2. Ipred is the sequence of inhibitory sheaths which fire due to triggering by their contained predictively firing neurons in Ppred - these cells fire in advance of their feedforward times due to inputs from Ppred;\n3. Iff is the sequence of inhibitory sheaths which fire as a result of feedforward input alone;\n4. Pburst is the sequence of cells in columns where the inhibitory sheaths have just fired but their vertical inhibition has not had a chance to reach these cells (this is known as bursting) - ie Pburst = {P | \u03c4P < \u03c4Im + \u2206\u03c4vert, P \u2208 Cm};\n5. Finally, Ispread is the sequence of all the other inhibitory sheaths which are triggered by earlier-firing neighbours, which spreads a wave of inhibition imposing sparsity in the neighbourhood.\nNote that there may be some overlap in these sequences, depending on the exact sequence of firing and the distances between active columns.\nThe output of a sublayer is the SDR composed of the pyramidal cells from Ppred \u2016 Pburst in that order. We say that the sublayer has predicted perfectly if Pburst = \u2205 and that the sublayer is bursting otherwise.\nThe cardinality of the SDR is minimal under perfect prediction, with some columns having a sequence of extra, bursting cells otherwise. The bursting columns represent feedforward inputs which were well recognised (causing their inhibitory sheaths to fire quickly) but less well predicted (no cell was predictive enough to beat the sheath), and the number of cells firing indicates the uncertainty of which prediction corresponds to reality. The actual cells which get to burst are representative of the most plausible contexts for the unexpected input.\nTransmission and Reception of SDRs\nA sublayer L2 which receives this L1 SDR as input will first see the minimal SDR Ppred representing the perfect match of input and prediction, followed by the bursting SDR elements Pburst in decreasing order of prediction-reality match.\nThis favours cells in L2 which have learned to respond to this SDR, and even more so for the subset which are also predictive due to their own contextual inputs (this biasing happens regardless of whether the receiving cells are proximally or distally enervated). The more sparse (well-predicted) the incoming SDR, the more sparse the activation of L2.\nWhen there is a bursting component in the SDR, this will tend to add significant (or overwhelming) extra signal to the minimal SDR, leading to high probability of a change in the SDR formed byL2, because several cells inL2 will have a stronger feedforward response to the extra inputs than those which respond to the small number of signals in the minimal SDR.\nFor example, in software we typically use layers containing 2,048 columns of 32 pyramidal neurons (64K cells), with a minimal column SDR of 40 columns (c. 2%). At perfect prediction, the SDR has 40 cells (0.06%), while total bursting would create an SDR of 1280 cells. In between, the effect is quite uneven, since each bursting column produces several signals, while all non-bursting columns stay at one. Assuming some locality of the mapping between L1 and L2, this will have dramatic local effects where there is bursting.\nThe response in L2 to bursting in its input will not only be a change in the columnar representation, but may also cause bursting in L2 itself if the new state was not well predicted using L2\u2019s context. This will cause bursting to propagate downstream, from sublayer to sublayer (including cycles in feedback loops), until some sublayer can stop the cascade either by predicting its input or by causing a change in its external world which indirectly restores predictability.\nSince we typically do not see reverberating, self-reinforcing cycles of bursting in neocortex, we must assume that the brain has learned to halt these cascades using some combination of eventual predictive resolution and remediating output from regions. Note that each sublayer has its own version of \u201doutput\u201d in this sense - it\u2019s not just the obvious motor output of L5 which can \u201dchange the world\u201d. For example, L6 can output a new SDR which it transmits down to lower regions, changing the high-level context imposed on those regions and thus the environment in which they are trying (and failing somewhat) to predict their own inputs. L6 can also respond by altering its influence over thalamic connections, thus mediating or eliminating the source of disturbance. L2/3 and L5 both send SDRs up to higher regions, which may be able to better handle their deviations from predictability. And of course L5 can cause real changes in the world by acting on motor circuits.\nHow is Self-Stabilisation Learned?\nWhen time is slowed down to the extent we\u2019ve seen in this discussion, it is relatively easy to see how neurons can learn to contribute to self-stabilisation of sparse activation patterns in cortex. Recall the general principle of Hebbian learning in synapses - the more often a synapse receives an input within a short time before its cell fires, the more it grows to respond to that input.\nConsider again the sequence of firing neurons in a sublayer:\nS = Ppred \u2016 Ipred \u2016 Iff \u2016 Pburst \u2016 Ispread\nThis sequence does not include the very many cells in a sublayer which do not fire at all, because they are contained either in columns which become active, but are not fast enough to burst, or more commonly they are in columns inhibited by a spreading wave from active columns. Let\u2019s call this set Pinactive.\nA particular neuron will, at any moment, be a member of one of these sets. How often the cell fires depends on the average amount of time it spends in each set, and how often a cell fires characteristically for each set. Clearly, the highly predictive cells in Ppred will have a higher typical firing frequency than those in Pburst, while those in Pinactive have zero frequency when in that set.\nNote that the numbers used earlier (65536 cells, 40 cells active in perfect prediction, 1280 in total bursting) mean that the percentage of the time cells are firing on average is massively increased if they are in the predictive population. Bursting cells only fire once following a failure of prediction, with the most predictive of them effectively winning and firing if the same input persists.\nSome cells will simply be lucky enough to find themselves in the most predictive set and will strengthen the synapses which will keep them there. Because of their much higher frequency of firing, these cells will be increasingly hard to dislodge and demote from the predictive state.\nSome cells will spend much of their time only bursting. This unstable status will cause a bifurcation among this population. A portion of these cells will simply strengthen the right connections and join the ranks of the sparsely predictive cells (which will eliminate their column from bursting on the current inputs). Others will weaken the optimal connections in favour of some other combination of context and inputs (which will drop them from bursting to inactive on current inputs). The remainder, lacking the ability to improve to predictive and the attraction of an alternative set of inputs, will continue to form part of the short-lived bursting behaviour.\nIn order to compete with inactive cells in the same column, these metastable cells will have to have an output which tends to feed back into the same state which led to them bursting in the first place.\nCells which get to fire (either predictively or by bursting) have a further advantage - they can specialise their sensitivity to feedforward inputs given the contexts which caused them to fire, and this will give them an ever-improving chance of beating the inhibitory sheath (which has no context to help it learn). This is another mechanism which will allow cells to graduate from bursting to predictive on a given set of inputs (and context).\nSince only active cells have any effect in neocortex, we see that there is an emergent drive towards stability and sparsity in a sublayer. Cells, given the opportunity, will graduate up the ladder from inactive to bursting to predictive when presented with the right inputs. Cells which fail to improve will be overtaken by their neighbours in the same column, and demoted back down towards inactive. A cell which has recently started to burst (having been inactive on the same inputs) will be reinforced in that status if its firing gives rise to a transient change in the world which causes its inputs to recur. With enough repetition, a cell will graduate to predictive on its favoured inputs, and will participate in a sparse, stable predictive pattern of activity in the sublayer and its region. The effect of its output will correspondingly change from a transient restorative effect to a self-sustaining, self-reinforcing effect."}, {"heading": "4.4 Spatial/Columnar Interpretation of Transition Memory SDRs", "text": "Since cells in each TM column share very similar feedforward response, we can just consider which columns contain active cells when presented with each input. This columnar SDR will be very similar to the SDR formed by the Pattern Memory alone (ie without prediction), differing only where the prediction has changed the outcome of the inhibition stage, favouring columns which have strongly predictive cells. The TM columnar SDR will be more invariant to occlusion or noise in the inputs, but will also potentially hallucinate some inputs as it causes the layer to see what is expected rather than what is actually seen. It is likely that this balance between error correction and hallucination is dynamically adjusted in real cortex."}, {"heading": "5 Sequence Memory - High-Order Sequences", "text": "A CLA layer which has multi-cell columns is capable of learning high-order sequences of feedforward input patterns, ie sequences in which the next input xt+1 can be predicted based on all\nthe observed patterns {xt\u2212i|0 \u2264 i \u2264 k} for some k steps in the past, rather than just the current input xt. Thus, a layer which has seen the sequences ABCD and XBCY will correctly predict D after seeing ABC and Y after seeing XBC.\nTo explain this important function, consider the columns representing B in the above sequences. In each column, one cell will have a distal dendrite segment which receives inputs representing A, and another will have learned to recognise a previousX . So, while (essentially) the same columns become active for both B\u2019s, the active cells in each case will be different. Thus the TM activation encodes an entire sequence of patterns at every step. This chain of individual cell-level representations persists across multiple common inputs, such as BC in this example, allowing the CLA to correctly predict the D or X as appropriate.\nIn addition, this allows the layer to distinguish between repeated patterns in a sequence, such as the S\u2019s in MISSISSIPPI , notes in music, or words in a sentence (eg 5 distinct repetitions of the word in have already appeared in this one)."}, {"heading": "6 Multiple levels of representation", "text": "Note that a CLA layer is producing a number of representations of its inputs simultaneously, and these representations can be seen as nested one within another.\nColumnar SDR The simplest and least detailed representation is the Columnar SDR, which is just a simple representation of the pattern currently seen by the layer. This is what you would\nsee if you looked down on the layer and just observed which columns had active cells. The number of patterns which can be represented is\n( N\nnSDR\n) . In the typical software layer\n(2048 columns, 40 active), we can have ( 2048 40 ) = 2.37178 \u2217 1084 SDRs. (See [Ahmad and Hawkins, 2015] for a detailed treatment of the combinatorics of SDRs).\nCellular SDR The cell-level SDR encodes both the Columnar SDR (if you ignore the choices of cells) and the context/sequence in which it occurred. We can produce a one-cell-per-column SDR by choosing the most predictive cell in each active column (and choose randomly in the case of bursting cells). In fact, this is how cells are chosen for learning in most implementations of CLA.\nInterestingly, the capacity of this SDR is very large. For every Columnar SDR (ie for each spatial input), there are nnSDR distinct contexts, if each column contains n cells. Again, in typical software, nSDR = 40, n = 32, so each feedforward input can appear in up to 1.60694 \u2217 1060 different contexts. Multiplying these, we get 3.8113 \u2217 10144 distinct Cellular SDRs.\nPredicted/Bursting Columnar SDR This more detailed SDR is composed of the sub-SDRs (or vectors) representing a) what was predicted and confirmed by reality and b) what was present in the input but not well-predicted. The layer\u2019s combined output vector can thus be seen as the sum of two vectors - one representing the correctly predicted reality and the other a perpendicular prediction error vector:\nySDR = ypred + yburst\nAs we\u2019ll see in the next section, this decomposition is crucial to the process of Temporal Pooling, in which a downstream layer can learn to stably represent a single representation by learning to recognise successive ypred vectors.\nPredicted/Bursting Cellular SDR This is the cellular equivalent of the previous SDR (equivalently the previous SDR is the column-level version of this one). This SDR encodes the precise sequence/context identity as well as the split between predicted and prediction error vectors. In addition, looked at columnwise, the error SDR is actually the union of all the vectors representing how the input and prediction differed, thus forming a cloud in the output space whose volume reflects the confusion of the layer.\nAs noted earlier, the size, or `1 norm, of the Predicted/Bursting Cellular SDR varies dramatically with the relative number of predicted vs bursting columns. In a typical CLA software layer, 40 \u2264 \u2016ySDR\u2016`1 \u2264 1280, a 32x range.\nPrediction-ordered SDR Sequences Even more detail is produced by treating the SDR as a sequence of individual activations, as we did earlier when deriving the sequence:\nS = Ppred \u2016 Pburst\nEach of the two subsequences is ordered by the activation levels of the individual cells, in decreasing order. Each thus represents a sequence of recognitions, with the most confident recognitions appearing earliest."}, {"heading": "7 Temporal Pooling: from single- to multi-layer models", "text": "One well-understood aspect of the structure of the neocortex is the hierarchical organisation of visual cortex. The key feature of this hierarchy is that the spatial and temporal scale of receptive fields increases from low-level to high-level regions.\nEarly versions of HTM resembled Artificial Neural Networks, or Deep Learning Networks, in having a single layer for each region in the hierarchy George and Hawkins [2009]. The current CLA as described in [Byrne, 2015] continues this design, modelling only a single layer, equivalent to Layer 2/3 in cortex in each region. The latest developments in HTM involve a new mechanism called Temporal Pooling, which models both Layer 4 and Layer 2/3. This section describes Temporal Pooling and its role in extracting hierarchical spatiotemporal information from sensory and sensorimotor inputs.\nHawkins proposes that each layer is performing a similar task of learning sequences of its inputs, but with differences in the processing in each layer. For this discussion, the layers of interest are Layer 4, which receives direct sensorimotor feedforward input, and Layer 2/3, which receives as input the output of Layer 4, producing the representation which is passed up the hierarchy.\nThe idea of Temporal Pooling is as follows. Layer 4 is receiving a stream of fast-changing sensorimotor inputs, and uses its Transition Memory to form predictions of the next input in the stream. If this succeeds, the sequence of SDRs produced in Layer 4 will each be a sparse set of predictive cells. Temporal Pooling cells in Layer 2/3 which have proximal synapses to many L4 cells in a particular sequence will then repeatedly become active as L4 evolves through the sequence, and will form a stable, slowly-changing representation of the sequence undergone by L4.\nLearning in Temporal Pooling A simple extension of the Pattern Memory learning rule is sufficient to explain Temporal Pooling learning. Recall the original rule, the update model for the permanence of a synapse pi on neuron j is:\np (t+1) i =  (1 + \u03b4inc)p (t) i if j \u2208 YSDR, (xj)i = 1 and p (t) i \u2265 \u03b8i (1\u2212 \u03b4dec)p(t)i if j \u2208 YSDR and ((xj)i = 0 or p (t) i < \u03b8i)\np (t) i otherwise\nTemporal Pooling simply uses different values for \u03b4inc and \u03b4dec depending on whther the input is from a predictive or bursting neuron in L4. For predicted neurons, \u03b4inc is increased, and \u03b4dec is decreased, and for bursting neurons \u03b4inc is decreased and \u03b4dec is increased. This causes the pooling neuron to preferentially learn sequences of predicted SDRs in L4.\nExtension to Cycles and Trajectories in L4\nHawkins\u2019 original description of Temporal Pooling Hawkins [2014] referred to sequences in L4, but since then it appears that his conception of L4 sequence memory involves low- or zeroorder memory rather than the long high-order sequences learned in the CLA we\u2019ve already been describing.\nIt is likely that real cortex exploits the spectrum of L4 sequence capacity in order to match the dynamics of each region\u2019s sensory inputs."}, {"heading": "8 Summary and Resources", "text": "This theory aims to combine a reasonably simple abstraction of neocortical function with several key computational features which we believe are central to understanding both mammalian and artificial intelligence. Using a simple but powerful mathematical description of the paCLA algorithms, we can reason about their computational power and learning capabilities. This paper also provides a sound basis for extending the theory in new directions. Indeed, we are developing a new, multilayer model of neocortex based on the current work.\nComportex [Andrews and Lewis, 2015] is an Open Source implementation of HTM/CLA which demonstrates most of the theory presented here, including both paCLA and Temporal Pooling. For other resources on HTM, we recommend the website of the open source community at Numenta.org [2015].\nReferences and Notes\nSubutai Ahmad and Jeff Hawkins. Properties of Sparse Distributed Representations and their Application to Hierarchical Temporal Memory. arXiv:1503.07469 [q-bio.NC], Jul 2015. URL http://arxiv.org/abs/1503.07469.\nFelix Andrews and Marcus Lewis. Functionally composable cortex, an implementa-\ntion of Hierarchical Temporal Memory, Sep 2015. URL https://github.com/ nupic-community/comportex. Github Repository.\nFergal Byrne. Hierarchical Temporal Memory including HTM Cortical Learning Algorithms. Revision of Hawkins and Ahmad, 2011, Oct 2015. URL http://bit.ly/ htm-white-paper.\nDileep George and Jeff Hawkins. Towards a mathematical theory of cortical micro-circuits. PLoS Comput Biol, 5(10):e1000532, 2009. doi: 10.1371/journal.pcbi.1000532. URL http: //bit.ly/george-2009.\nJeff Hawkins. New ideas about temporal pooling. Wiki Page, Jan 2014. URL http://bit. ly/temporal-pooling.\nJeff Hawkins and Subutai Ahmad. Hierarchical Temporal Memory including HTM Cortical Learning Algorithms. Hosted at Numenta.org, 2011. URL http://numenta.org/ resources/HTM_CorticalLearningAlgorithms.pdf.\nWolfgang Maass. Networks of spiking neurons: the third generation of neural network models. Neural networks, 10(9):1659\u20131671, 1997. URL http://amath.kaist.ac.kr/\n\u02dcnipl/am621/lecturenotes/spiking_neurons_2.pdf.\nWolfgang Maass. On the computational power of winner-take-all. Neural Computation, 12 (11):2519\u20132535, 2015/09/19 2000. doi: 10.1162/089976600300014827. URL http:// dx.doi.org/10.1162/089976600300014827.\nWarren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115\u2013133, 1943. URL http:// bit.ly/mcculloch-pitts.\nNumenta.org. Numenta.org - Hierarchical Temporal Memory open source community, Sep 2015. URL http://numenta.org. Community Website."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM) as a model of neocortical computation, the theory and the algorithms have evolved dramatically. This paper presents a detailed description of HTM\u2019s Cortical Learning Algorithm (CLA), including for the first time a rigorous mathematical formulation of all aspects of the computations. Prediction Assisted CLA (paCLA), a refinement of the CLA, is presented, which is both closer to the neuroscience and adds significantly to the computational power. Finally, we summarise the key functions of neocortex which are expressed in paCLA implementations. An Open Source project, Comportex, is the leading implementation of this evolving theory of the brain.", "creator": "LaTeX with hyperref package"}}}