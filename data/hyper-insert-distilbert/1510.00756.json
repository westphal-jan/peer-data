{"id": "1510.00756", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Oct-2015", "title": "Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using Hierarchy Width", "abstract": "gibbs sampling on factor graphs is a widely used inference technique, this which often produces rather good formal empirical simulation results. theoretical guarantees expected for obtaining its remarkable performance algorithms are weak : even for complex tree structure structured graphs, nevertheless the mixing time of gibbs may be exponential in the number of of variables. to help theorists understand clearly the behavior of gibbs sampling, since we introduce a proposed new ( excluding hyper ) graph property, likewise called hierarchy web width. we show that under socially suitable conditions on the weights, bounded hierarchy width ensures infinitely polynomial process mixing time. our primary study effectiveness of constrained hierarchy table width information is in part motivated entirely by a new class of factor squares graph processing templates, hierarchical templates, which have achieved bounded hierarchy width - - - namely regardless of the data used alone to generate instantiate past them. though we similarly demonstrate a rich application from empirical natural language processing theories in terms which gibbs sampling provably too mixes less rapidly and rarely achieves sound accuracy that exceeds human complexity volunteers.", "histories": [["v1", "Fri, 2 Oct 2015 23:14:05 GMT  (66kb)", "http://arxiv.org/abs/1510.00756v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["christopher de sa", "ce zhang", "kunle olukotun", "christopher r\u00e9"], "accepted": true, "id": "1510.00756"}, "pdf": {"name": "1510.00756.pdf", "metadata": {"source": "CRF", "title": "Rapidly Mixing Gibbs Sampling for a Class of Factor Graphs Using Hierarchy Width", "authors": ["Christopher De Sa"], "emails": ["cdesa@stanford.edu,", "czhang@cs.wisc.edu,", "kunle@stanford.edu,", "chrismre@stanford.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n00 75\n6v 1\n[ cs\n.L G\n] 2\nO ct"}, {"heading": "1 Introduction", "text": "We study inference on factor graphs using Gibbs sampling, the de facto Markov Chain Monte Carlo (MCMC) method [8, p. 505]. Specifically, our goal is to compute the marginal distribution of some query variables using Gibbs sampling, given evidence about some other variables and a set of factor weights. We focus on the case where all variables are discrete. In this situation, a Gibbs sampler randomly updates a single variable at each iteration by sampling from its conditional distribution given the values of all the other variables in the model. Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results. However, theoretical guarantees about Gibbs are lacking. The aim of the technical result of this paper is to provide new cases in which one can guarantee that Gibbs gives accurate results.\nFor an MCMC sampler like Gibbs sampling, the standard measure of efficiency is the mixing time of the underlying Markov chain. We say that a Gibbs sampler mixes rapidly over a class of models if its mixing time is at most polynomial in the number of variables in the model. Gibbs sampling is known to mix rapidly for some models. For example, Gibbs sampling on the Ising model on a graph with bounded degree is known to mix in quasilinear time for high temperatures [10, p. 201]. Recent work has outlined conditions under which Gibbs sampling of Markov Random Fields mixes rapidly [11]. Continuous-valued Gibbs sampling over models with exponential-family distributions is also known to mix rapidly [2, 3]. Each of these celebrated results still leaves a gap: there are many classes of factor graphs on which Gibbs sampling seems to work very well\u2014including as part of systems that have won quality competitions [24]\u2014for which there are no theoretical guarantees of rapid mixing.\nMany graph algorithms that take exponential time in general can be shown to run in polynomial time as long as some graph property is bounded. For inference on factor graphs, the most commonly used property is hypertree width, which bounds the complexity of dynamic programming algorithms on the graph. Many problems, including variable elimination for exact inference, can be solved in polynomial time on graphs with bounded hypertree width [8, p. 1000]. In some sense, bounded hypertree width is a necessary and sufficient condition for tractability of inference in graphical models [1, 9]. Unfortunately, it is not hard to construct examples of factor graphs with bounded weights and hypertree width 1 for which Gibbs sampling takes exponential time to mix. Therefore, bounding hypertree width is insufficient to ensure rapid mixing of Gibbs sampling. To analyze the behavior of Gibbs sampling, we define a new\ngraph property, called the hierarchy width. This is a stronger condition than hypertree width; the hierarchy width of a graph will always be larger than its hypertree width. We show that for graphs with bounded hierarchy width and bounded weights, Gibbs sampling mixes rapidly.\nOur interest in hierarchy width is motivated by so-called factor graph templates, which are common in practice [8, p. 213]. Several types of models, such as Markov Logic Networks (MLN) and Relational Markov Networks (RMN) can be represented as factor graph templates. Many state-of-the-art systems use Gibbs sampling on factor graph templates and achieve better results than competitors using other algorithms [14, 27]. We exhibit a class of factor graph templates, called hierarchical templates, which, when instantiated, have a hierarchy width that is bounded independently of the dataset used; Gibbs sampling on models instantiated from these factor graph templates will mix in polynomial time. This is a kind of sampling analog to tractable Markov logic [4] or so-called \u201csafe plans\u201d in probabilistic databases [23]. We exhibit a real-world templated program that outperforms human annotators at a complex text extraction task\u2014and provably mixes in polynomial time.\nIn summary, this work makes the following contributions: \u2022 We introduce a new notion of width, hierarchy width, and show that Gibbs sampling mixes in polynomial time\nfor all factor graphs with bounded hierarchy width and factor weight. \u2022 We describe a new class of factor graph templates, hierarchical factor graph templates, such that Gibbs sampling\non instantiations of these templates mixes in polynomial time. \u2022 We validate our results experimentally and exhibit factor graph templates that achieve high quality on tasks but\nfor which our new theory is able to provide mixing time guarantees."}, {"heading": "1.1 Related Work", "text": "Gibbs sampling is just one of several algorithms proposed for use in factor graph inference. The variable elimination algorithm [8] is an exact inference method that runs in polynomial time for graphs of bounded hypertree width. Belief propagation is another widely-used inference algorithm that produces an exact result for trees and, although it does not converge in all cases, converges to a good approximation under known conditions [7]. Lifted inference [18] is one way to take advantage of the structural symmetry of factor graphs that are instantiated from a template; there are lifted versions of many common algorithms, such as variable elimination [16], belief propagation [21], and Gibbs sampling [26]. It is also possible to leverage a template for fast computation: Venugopal et al. [27] achieve orders of magnitude of speedup of Gibbs sampling on MLNs. Compared with Gibbs sampling, these inference algorithms typically have better theoretical results; despite this, Gibbs sampling is a ubiquitous algorithm that performs practically well\u2014far outstripping its guarantees.\nOur approach of characterizing runtime in terms of a graph property is typical for the analysis of graph algorithms. Many algorithms are known to run in polynomial time on graphs of bounded treewidth [19], despite being otherwise NP-hard. Sometimes, using a stronger or weaker property than treewidth will produce a better result; for example, the submodular width used for constraint satisfaction problems [13]."}, {"heading": "2 Main Result", "text": "In this section, we describe our main contribution. We analyze some simple example graphs, and use them to show that bounded hypertree width is not sufficient to guarantee rapid mixing of Gibbs sampling. Drawing intuition from this, we define the hierarchy width graph property, and prove that Gibbs sampling mixes in polynomial time for graphs with bounded hierarchy width.\nFirst, we state some basic definitions. A factor graph G is a graphical model that consists of a set of variables V and factors \u03a6, and determines a distribution over those variables. If I is a world for G (an assignment of a value to each variable in V ), then \u01eb, the energy of the world, is defined as\n\u01eb(I) = \u2211\n\u03c6\u2208\u03a6 \u03c6(I). (1)\nThe probability of world I is \u03c0(I) = 1 Z exp(\u01eb(I)), where Z is the normalization constant necessary for this to be a distribution. Typically, each \u03c6 depends only on a subset of the variables; we can draw G as a bipartite graph where a variable v \u2208 V is connected to a factor \u03c6 \u2208 \u03a6 if \u03c6 depends on v.\nDefinition 1 (Mixing Time). The mixing time of a Markov chain is the first time t at which the estimated distribution \u00b5t is within statistical distance 14 of the true distribution [10, p. 55]. That is,\ntmix = min { t : maxA\u2282\u2126 |\u00b5t(A)\u2212 \u03c0(A)| \u2264 1 4 } ."}, {"heading": "2.1 Voting Example", "text": "We start by considering a simple example model [20], called the voting model, that models the sign of a particular \u201cquery\u201d variable Q \u2208 {\u22121, 1} in the presence of other \u201cvoter\u201d variables Ti \u2208 {0, 1} and Fi \u2208 {0, 1}, for i \u2208 {1, . . . , n}, that suggest that Q is positive and negative (true and false), respectively. We consider three versions of this model. The first, the voting model with linear semantics, has energy function\n\u01eb(Q, T, F ) = wQ \u2211n i=1 Ti \u2212 wQ \u2211n i=1 Fi + \u2211n i=1 wTiTi + \u2211n i=1 wFiFi,\nwhere wTi , wFi , and w > 0 are constant weights. This model has a factor connecting each voter variable to the query, which represents the value of that vote, and an additional factor that gives a prior for each voter. It corresponds to the factor graph in Figure 1(a). The second version, the voting model with logical semantics, has energy function\n\u01eb(Q, T, F ) = wQmaxi Ti \u2212 wQmaxi Fi + \u2211n i=1 wTiTi + \u2211n i=1 wFiFi.\nHere, in addition to the prior factors, there are only two other factors, one of which (which we call \u03c6T ) connects all the true-voters to the query, and the other of which (\u03c6F ) connects all the false-voters to the query. The third version, the voting model with ratio semantics, is an intermediate between these two models, and has energy function\n\u01eb(Q, T, F ) = wQ log (1 + \u2211n i=1 Ti)\u2212 wQ log (1 + \u2211n i=1 Fi) + \u2211n i=1 wTiTi + \u2211n i=1 wFiFi.\nWith either logical or ratio semantics, this model can be drawn as the factor graph in Figure 1(b). These three cases model different distributions and therefore different ways of representing the power of a vote; the choice of names is motivated by considering the marginal odds of Q given the other variables. For linear semantics, the odds of Q depend linearly on the difference between the number of nonzero positive-voters Ti and nonzero negativevoters Fi. For ratio semantics, the odds of Q depend roughly on their ratio. For logical semantics, only the presence of nonzero voters matters, not the number of voters.\nWe instantiated this model with random weights wTi and wFi , ran Gibbs sampling on it, and computed the variance of the estimated marginal probability of Q for the different models (Figure 2). The results show that the models with logical and ratio semantics produce much lower-variance estimates than the model with linear semantics. This experiment motivates us to try to prove a bound on the mixing time of Gibbs sampling on this model.\nTheorem 1. Fix any constant \u03c9 > 0, and run Gibbs sampling on the voting model with bounded factor weights {wTi , wFi , w} \u2282 [\u2212\u03c9, \u03c9]. For the voting model with linear semantics, the largest possible mixing time tmix of any such model is tmix = 2\u0398(n). For the voting model with either logical or ratio semantics, the largest possible mixing time is tmix = \u0398(n logn).\nThis result validates our observation that linear semantics mix poorly compared to logical and ratio semantics. Intuitively, the reason why linear semantics performs worse is that the Gibbs sampler will switch the state of Q only very infrequently\u2014in fact exponentially so. This is because the energy roughly depends linearly on the number of voters n, and therefore the probability of switching Q depends exponentially on n. This does not happen in either the logical or ratio models."}, {"heading": "2.2 Hypertree Width", "text": "In this section, we describe the commonly-used graph property of hypertree width, and show using the voting example that bounding it is insufficient to ensure rapid Gibbs sampling. Hypertree width is typically used to bound the complexity of dynamic programming algorithms on a graph; in particular, variable elimination for exact inference runs in polynomial time on factor graphs with bounded hypertree width [8, p. 1000]. The hypertree width of a hypergraph, which we denote tw(G), is a generalization of the notion of acyclicity; since the definition of hypertree width is technical, we instead state the definition of an acyclic hypergraph, which is sufficient for our analysis. In order to apply these notions to factor graphs, we can represent a factor graph as a hypergraph that has one vertex for each node of the factor graph, and one hyperedge for each factor, where that hyperedge contains all variables the factor depends on.\nDefinition 2 (Acyclic Factor Graph [6]). A join tree, also called a junction tree, of a factor graph G is a tree T such that the nodes of T are the factors of G and, if two factors \u03c6 and \u03c1 both depend on the same variable x in G, then every factor on the unique path between \u03c6 and \u03c1 in T also depends on x. A factor graph is acyclic if it has a join tree. All acyclic graphs have hypertree width tw(G) = 1.\nNote that all trees are acyclic; in particular the voting model (with any semantics) has hypertree width 1. Since the voting model with linear semantics and bounded weights mixes in exponential time (Theorem 1), this means that bounding the hypertree width and the factor weights is insufficient to ensure rapid mixing of Gibbs sampling."}, {"heading": "2.3 Hierarchy Width", "text": "Since the hypertree width is insufficient, we define a new graph property, the hierarchy width, which, when bounded, ensures rapid mixing of Gibbs sampling. This result is our main contribution.\nDefinition 3 (Hierarchy Width). The hierarchy width hw(G) of a factor graph G is defined recursively such that, for any connected factor graph G = \u3008V,\u03a6\u3009,\nhw(G) = 1 + min \u03c6\u2217\u2208\u03a6\nhw(\u3008V,\u03a6\u2212 {\u03c6\u2217}\u3009), (2)\nand for any disconnected factor graph G with connected components G1, G2, . . .,\nhw(G) = max i hw(Gi). (3)\nAs a base case, all factor graphs G with no factors have\nhw(\u3008V, \u2205\u3009) = 0. (4)\nTo develop some intuition about how to use the definition of hierarchy width, we derive the hierarchy width of the path graph drawn in Figure 3.\nLemma 1. The path graph model has hierarchy width hw(G) = \u2308log2 n\u2309.\nProof. Let Gn denote the path graph with n variables. For n = 1, the lemma follows from (4). For n > 1, Gn is connected, so we must compute its hierarchy width by applying (2). It turns out that the factor that minimizes this expression is the factor in the middle, and so applying (2) followed by (3) shows that hw(Gn) = 1 + hw(G\u2308 n\n2 \u2309).\nApplying this inductively proves the lemma.\nSimilarly, we are able to compute the hierarchy width of the voting model factor graphs.\nLemma 2. The voting model with logical or ratio semantics has hierarchy width hw(G) = 3.\nLemma 3. The voting model with linear semantics has hierarchy width hw(G) = 2n+ 1.\nThese results are promising, since they separate our polynomially-mixing examples from our exponentially-mixing examples. However, the hierarchy width of a factor graph says nothing about the factors themselves and the functions they compute. This means that it, alone, tells us nothing about the model; for example, any distribution can be represented by a trivial factor graph with a single factor that contains all the variables. Therefore, in order to use hierarchy width to produce a result about the mixing time of Gibbs sampling, we constrain the maximum weight of the factors.\nDefinition 4 (Maximum Factor Weight). A factor graph has maximum factor weight M , where\nM = max \u03c6\u2208\u03a6\n(\nmax I \u03c6(I) \u2212min I\n\u03c6(I) ) .\nFor example, the maximum factor weight of the voting example with linear semantics is M = 2w; with logical semantics, it is M = 2w; and with ratio semantics, it is M = 2w log(n+ 1). We now show that graphs with bounded hierarchy width and maximum factor weight mix rapidly.\nTheorem 2 (Polynomial Mixing Time). If G is a factor graph with n variables, at most s states per variable, e factors, maximum factor weight M , and hierarchy width h, then\ntmix \u2264 (log(4) + n log(s) + eM)n exp(3hM).\nIn particular, if e is polynomial in n, the number of values for each variable is bounded, and hM = O(log n), then tmix(\u01eb) = O(n O(1)).\nTo show why bounding the hierarchy width is necessary for this result, we outline the proof of Theorem 2. Our technique involves bounding the absolute spectral gap \u03b3(G) of the transition matrix of Gibbs sampling on graph G; there are standard results that use the absolute spectral gap to bound the mixing time of a process [10, p. 155]. Our proof proceeds via induction using the definition of hierarchy width and the following three lemmas.\nLemma 4 (Connected Case). Let G and G\u0304 be two factor graphs with maximum factor weight M , which differ only inasmuch as G contains a single additional factor \u03c6\u2217. Then,\n\u03b3(G) \u2265 \u03b3(G\u0304) exp (\u22123M) .\nLemma 5 (Disconnected Case). Let G be a disconnected factor graph with n variables and m connected components G1, G2, . . . , Gm with n1, n2, . . . nm variables, respectively. Then,\n\u03b3(G) \u2265 min i\u2264m ni n \u03b3(Gi).\nLemma 6 (Base Case). Let G be a factor graph with one variable and no factors. The absolute spectral gap of Gibbs sampling running on G will be \u03b3(G) = 1.\nUsing these Lemmas inductively, it is not hard to show that, under the conditions of Theorem 2,\n\u03b3(G) \u2265 1\nn exp (\u22123hM) ;\nconverting this to a bound on the mixing time produces the result of Theorem 2. To gain more intuition about the hierarchy width, we compare its properties to those of the hypertree width. First, we note that, when the hierarchy width is bounded, the hypertree width is also bounded.\nStatement 1. For any factor graph G, tw(G) \u2264 hw(G).\nOne of the useful properties of the hypertree width is that, for any fixed k, computing whether a graph G has hypertree width tw(G) \u2264 k can be done in polynomial time in the size of G. We show the same is true for the hierarchy width.\nStatement 2. For any fixed k, computing whether hw(G) \u2264 k can be done in time polynomial in the number of factors of G.\nFinally, we note that we can also bound the hierarchy width using the degree of the factor graph. Notice that a graph with unbounded node degree contains the voting program with linear semantics as a subgraph. This statement shows that bounding the hierarchy width disallows such graphs.\nStatement 3. Let d be the maximum degree of a variable in factor graph G. Then, hw(G) \u2265 d."}, {"heading": "3 Factor Graph Templates", "text": "Our study of hierarchy width is in part motivated by the desire to analyze the behavior of Gibbs sampling on factor graph templates, which are common in practice and used by many state-of-the-art systems. A factor graph template is an abstract model that can be instantiated on a dataset to produce a factor graph. The dataset consists of objects, each of which represents a thing we want to reason about, which are divided into classes. For example, the object Bart could have class Person and the object Twilight could have class Movie. (There are many ways to define templates; here, we follow the formulation in Koller and Friedman [8, p. 213].)\nA factor graph template consists of a set of template variables and template factors. A template variable represents a property of a tuple of zero or more objects of particular classes. For example, we could have an IsPopular(x) template, which takes a single argument of class Movie. In the instantiated graph, this would take the form of multiple variables like IsPopular(Twilight) or IsPopular(Avengers). Template factors are replicated similarly to produce multiple factors in the instantiated graph. For example, we can have a template factor\n\u03c6 (TweetedAbout(x, y), IsPopular(x))\nfor some factor function \u03c6. This would be instantiated to factors like\n\u03c6 (TweetedAbout(Avengers,Bart), IsPopular(Avengers)) .\nWe call the x and y in a template factor object symbols. For an instantiated factor graph with template factors \u03a6, if we let A\u03c6 denote the set of possible assignments to the object symbols in a template factor \u03c6, and let \u03c6(a, I) denote the value of its factor function in world I under the object symbol assignment a, then the standard way to define the energy function is with\n\u01eb(I) = \u2211\n\u03c6\u2208\u03a6\n\u2211\na\u2208A\u03c6 w\u03c6\u03c6(a, I), (5)\nwhere w\u03c6 is the weight of template factor \u03c6. This energy function results from the creation of a single factor \u03c6a(I) = \u03c6(a, I) for each object symbol assignment a of \u03c6. Unfortunately, this standard energy definition is not suitable for all applications. To deal with this, Shin et al. [20] introduce the notion of a semantic function g, which counts the of energy of instances of the factor template in a non-standard way. In order to do this, they first divide the object symbols of each template factor into two groups, the head symbols and the body symbols. When writing out factor\ntemplates, we distinguish head symbols by writing them with a hat (like x\u0302). If we let H\u03c6 denote the set of possible assignments to the head symbols, let B\u03c6 denote the set of possible assignments to the body symbols, and let \u03c6(h, b, I) denote the value of its factor function in world I under the assignment (h, b), then the energy of a world is defined as\n\u01eb(I) = \u2211\n\u03c6\u2208\u03a6\n\u2211\nh\u2208H\u03c6 w\u03c6(h) g\n(\n\u2211\nb\u2208B\u03c6 \u03c6(h, b, I)\n)\n. (6)\nThis results in the creation of a single factor \u03c6h(I) = g ( \u2211\nb \u03c6(h, b, I)) for each assignment of the template\u2019s head symbols. We focus on three semantic functions in particular [20]. For the first, linear semantics, g(x) = x. This is identical to the standard semantics in (5). For the second, logical semantics, g(x) = sgn(x). For the third, ratio semantics, g(x) = sgn(x) log(1 + |x|). These semantics are analogous to the different semantics used in our voting example. Shin et al. [20] exhibit several classification problems where using logical or ratio semantics gives better F1 scores."}, {"heading": "3.1 Hierarchical Factor Graphs", "text": "In this section, we outline a class of templates, hierarchical templates, that have bounded hierarchy width. We focus on models that have hierarchical structure in their template factors; for example,\n\u03c6(A(x\u0302, y\u0302, z), B(x\u0302, y\u0302), Q(x\u0302, y\u0302)) (7)\nshould have hierarchical structure, while \u03c6(A(z), B(x\u0302), Q(x\u0302, y)) (8)\nshould not. Armed with this intuition, we give the following definitions.\nDefinition 5 (Hierarchy Depth). A template factor \u03c6 has hierarchy depth d if the first d object symbols that appear in each of its terms are the same. We call these symbols hierarchical symbols. For example, (7) has hierarchy depth 2, and x\u0302 and y\u0302 are hierarchical symbols; also, (8) has hierarchy depth 0, and no hierarchical symbols.\nDefinition 6 (Hierarchical). We say that a template factor is hierarchical if all of its head symbols are hierarchical symbols. For example, (7) is hierarchical, while (8) is not. We say that a factor graph template is hierarchical if all its template factors are hierarchical.\nWe can explicitly bound the hierarchy width of instances of hierarchical factor graphs.\nLemma 7. If G is an instance of a hierarchical template with E template factors, then hw(G) \u2264 E.\nWe would now like to use Theorem 2 to prove a bound on the mixing time; this requires us to bound the maximum factor weight of the graph. Unfortunately, for linear semantics, the maximum factor weight of a graph is potentially O(n), so applying Theorem 2 won\u2019t get us useful results. Fortunately, for logical or ratio semantics, hierarchical factor graphs do mix in polynomial time.\nStatement 4. For any fixed hierarchical factor graph template G, if G is an instance of G with bounded weights using either logical or ratio semantics, then the mixing time of Gibbs sampling on G is polynomial in the number of objects n in its dataset. That is, tmix = O ( nO(1) ) .\nSo, if we want to construct models with Gibbs samplers that mix rapidly, one way to do it is with hierarchical factor graph templates using logical or ratio semantics."}, {"heading": "4 Experiments", "text": "Synthetic Data We constructed a synthetic dataset by using an ensemble of Ising model graphs each with 360 nodes, 359 edges, and treewidth 1, but with different hierarchy widths. These graphs ranged from the star graph (like in Figure 1(a)) to the path graph; and each had different hierarchy width. For each graph, we were able to calculate the exact true marginal of each variable because of the small tree-width. We then ran Gibbs sampling on each graph, and calculated the error of the marginal estimate of a single arbitrarily-chosen query variable. Figure 5(a) shows the result with different weights and hierarchy width. It shows that, even for tree graphs with the same number of nodes and edges, the mixing time can still vary depending on the hierarchy width of the model.\nReal-World Applications We observed that the hierarchical templates that we focus on in this work appear frequently in real applications. For example, all five knowledge base population (KBP) systems illustrated by Shin et al. [20] contain subgraphs that are grounded by hierarchical templates. Moreover, sometimes a factor graph is solely grounded by hierarchical templates, and thus provably mixes rapidly by our theorem while achieving high quality. To validate this, we constructed a hierarchical template for the Paleontology application used by Shanan et al. [17]. We found that when using the ratio semantic, we were able to get an F1 score of 0.86 with precision of 0.96. On the same task, this quality is actually higher than professional human volunteers [17]. For comparison, the linear semantic achieved an F1 score of 0.76 and the logical achieved 0.73.\nThe factor graph we used in this Paleontology application is large enough that it is intractable, using exact inference, to estimate the true marginal to investigate the mixing behavior. Therefore, we chose a subgraph of a KBP system used by Shin et al. [20] that can be grounded by a hierarchical template and chose a setting of the weight such that the true marginal was 0.5 for all variables. We then ran Gibbs sampling on this subgraph and report the average error of the marginal estimation in Figure 5(b). Our results illustrate the effect of changing the semantic on a more complicated model from a real application, and show similar behavior to our simple voting example."}, {"heading": "5 Conclusion", "text": "This paper showed that for a class of factor graph templates, hierarchical templates, Gibbs sampling mixes in polynomial time. It also introduced the graph property hierarchy width, and showed that for graphs of bounded factor weight and hierarchy width, Gibbs sampling converges rapidly. These results may aid in better understanding the behavior of Gibbs sampling for both template and general factor graphs."}, {"heading": "Acknowledgments", "text": "Thanks to Stefano Ermon and Percy Liang for helpful conversations.\nThe authors acknowledge the support of: DARPA FA8750-12-2-0335; NSF IIS-1247701; NSF CCF-1111943; DOE 108845; NSF CCF1337375; DARPA FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041 and N000141310129; NIH U54EB020405; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; and Toshiba."}, {"heading": "A Proof of Voting Program Rates", "text": "Here, we prove the convergence rates stated in Theorem 1. The strategy for the upper bound proofs involves constructing a coupling between the Gibbs sampler and another process that attains the equilibrium distribution at each step. First, we restate the definition of mixing time in terms of the total variation distance, a quantity which we will use in the proofs in this section.\nDefinition 7 (Total Variation Distance). The total variation distance [10, p. 48] is a distance metric between two probability measures \u00b5 and \u03bd over probability space \u2126 defined as\n\u2016\u00b5\u2212 \u03bd\u2016TV = max A\u2282\u2126 |\u00b5(A) \u2212 \u03bd(A)| ,\nthat is, the maximum difference between the probabilities that \u00b5 and \u03bd assign to a single event.\nDefinition 8 (Mixing Time). The mixing time of a Markov chain is the first time t at which the estimated distribution \u00b5t is within total variation distance 14 of the true distribution [10, p. 55]. That is,\ntmix = min\n{\nt : \u2016\u00b5t \u2212 \u03c0\u2016TV \u2264 1\n4\n}\n.\nNext, we define a coupling [30].\nDefinition 9 (Coupling). A coupling of two random variables X and X \u2032 defined on some separate probability spaces P and P\u2032 is any new probability space P\u0302 over which there are two random variables X\u0302 and X\u0302 \u2032 such that X has the same distribution as X\u0302 and X \u2032 has the same distribution as X\u0302 \u2032.\nGiven a coupling of two Markov processes Xk and X \u2032k with the same transition matrix, the coupling time is defined as the first time T when X\u0302k = X\u0302 \u2032k. The following theorem lets us bound the total variance distance in terms of the coupling time.\nTheorem 3 (Theorem 5.2 in Levin et al. [10]). For any coupling (X\u0302k, X\u0302 \u2032k) with coupling time T , if we let \u03bdk and \u03bd \u2032 k denote the distributions of Xk and X \u2032k respectively, then\n\u2016\u03bdk \u2212 \u03bd \u2032 k\u2016tv \u2264 P (T > k) .\nAll of the coupling examples in this section use a correlated flip coupler, which consists of two Gibbs samplers X\u0302 and X\u0302 \u2032, each of which is running with the same random inputs. Specifically, both samplers choose to sample the same variable at each timestep. Then, if we define p as the probability of sampling 1 for X\u0302 , and p\u2032 similarly, it assigns both variables to 1 with probability min(p, p\u2032), assigns both variables to 0 with probability min(1\u2212 p, 1\u2212 p\u2032), and assigns different values with probability |p\u2212 p\u2032|. If we initialize X\u0302 with an arbitrary distribution \u03bd and X\u0302 \u2032 with the stationary distribution \u03c0, it is trivial to check that X\u0302k has distribution P k\u03bd, and X\u0302 \u2032k always has distribution \u03c0. Applying Theorem 3 results in\n\u2225 \u2225P k\u03bd \u2212 \u03c0 \u2225 \u2225\ntv \u2264 P (T > k) .\nNow, we prove the bounds stated in Theorem 1 as a collection of four statements about the upper and lower bounds of the mixing time.\nStatement 5 (UB for Voting: Logical and Ratio). For the voting example, assume that all the weights on the variables are bounded by |w| \u2264 M , |wTx | \u2264 M and |wFx | \u2264 M . Then for either the logical or ratio semantics, for sufficiently small constant \u01eb,\ntmix(\u01eb) = O (n log(n)) .\nProof. Recall that, for the voting program, the weight of a world is\nW (q, t, f) = wqg(\u2016t\u20161)\u2212 wqg(\u2016f\u20161) + n \u2211\nx=1\nwT (x)tx + n \u2211\nx=1\nwF (x)fx,\nwhere q \u2208 {\u22121, 1} and {tx, fx} \u2286 {0, 1}.\nConsider a correlated-flip coupler running on this Markov process, producing two chains X and X\u0304 . For either chain, if we sample a variable T (x) at time k, and let u denote the number of T (y) variables for y 6= x that are true, then\nP ( T (x)(k+1) = 1 ) = ( 1 + exp ( wq (g(u)\u2212 g(u+ 1))\u2212 wT (x) ))\u22121 .\nSince g(x+ 1)\u2212 g(x) \u2264 1 for any x, it follows that\nP ( T (x)(k+1) = 1 ) \u2265 (1 + exp(2M))\u22121 .\nWe now define the constant pM = (1 + exp(2M)) \u22121 .\nAlso, if u non-T (x) variables are true in the X process, and u\u0304 variables are true in the X\u0304 process, then the probability of the processes X and X\u0304 not producing the same value for Yi is\npD = \u2223 \u2223 \u2223P ( T (x)(k+1) = 1 ) \u2212 P ( T\u0304 (x)(k+1) = 1 )\u2223 \u2223 \u2223\n= \u2223 \u2223\n\u2223\n( 1 + exp ( wq (g(u)\u2212 g(u+ 1))\u2212 wT (x) ))\u22121 \u2212 ( 1 + exp ( wq\u0304 (g(u\u0304)\u2212 g(u\u0304+ 1))\u2212 wT (x) ))\u22121\n\u2223 \u2223 \u2223 ,\nwhere the last statement follows from continuous differentiability of the function h(x) = (1 + exp(\u2212x))\u22121. Notice that, if u = \u2126(n), then\npD = O(n \u22121)\nfor both logical and ratio semantics. The same logic will show that, if we sample any F (x), then\nP ( F (x)(k+1) = 1 )\n\u2265 pM ,\nand pD = O(g(v + 1)\u2212 g(v) + g(v\u0304 + 1)\u2212 g(v\u0304)),\nwhere v and v\u0304 are the number of non-F (x) variables that are true in the X and X\u0304 processes respectively. Again as above, if v = \u2126(n), then\npD = O(n \u22121).\nNext, consider the situation if we sample Q. In this case,\nP ( Q(k+1) = 1 )\n= (1 + exp (2w (g(\u2016t\u20161)\u2212 g(\u2016f\u20161)))) \u22121 .\nTherefore, of we let u denote the number of true T (x) variables and v denote the number of true F (x) variables, and similarly for u\u0304 and v\u0304, then the probability of the processes X and X\u0304 not producing the same value for Q is\npD = \u2223 \u2223 \u2223P ( Q(k+1) = 1 ) \u2212 P ( Q\u0304(k+1) = 1 )\u2223 \u2223 \u2223\n= \u2223 \u2223 \u2223(1 + exp (2w (f(v)\u2212 f(u)))) \u22121 .\u2212 (1 + exp (2w (f(v\u0304)\u2212 f(u\u0304))))\u22121 . \u2223 \u2223 \u2223\n= O (|f(v)\u2212 f(u)\u2212 f(v\u0304) + f(u\u0304), |)\nwhere as before, the last statement follows from continuous differentiability of the function g(x) = (1 + exp(\u2212x))\u22121. Furthermore, if all of u, v, u\u0304, and v\u0304 are \u2126(n), then\npD = O(1).\nNow, assume that our correlated-flip coupler runs for 8n logn steps on the Gibbs sampler. Let E1 be the event that, after the first 4n logn steps, each of the variables has been sampled at least once. This will have probability at least 12 by the coupon collector\u2019s problem.\nNext, let E2 be the event that, after the first 4n logn steps, min(u, v, u\u0304, v\u0304) \u2265 pMn 2 for the next 4n logn steps for both samplers. After all the entries have been sampled, u, v, u\u0304, and v\u0304 will each be bounded from below by a\nbinomial random variable with parameter pM at each timestep, so from Hoeffding\u2019s inequality, the probability that this constraint will be violated by a sampler at any particular step is less than exp (\n\u2212 12pMn ) . Therefore,\nP (E2|E1) \u2265\n(\n1\u2212 exp\n(\n\u2212 1\n2 pMn\n))4n logn\n= \u2126(1).\nLet E3 be the event that all the variables are resampled at least once between time 2n logn and 4n logn. Again this event has probability at least 12 .\nFinally, let C be the event that coupling occurs at time 4n logn. Given E1, E2, and E3, this probability is equal to the probability that each variable coupled individually the last time it was sampled. For all the Yi, our analysis above showed that this probability is\n1\u2212 pD = 1\u2212O(n \u22121),\nand for Q, this probability is 1\u2212 pD = \u2126(1).\nTherefore, P (C|E1, E2, E3) \u2265 \u2126(1) ( 1\u2212O(n\u22121) )2n = \u2126(1),\nand since P (C) = P (C|E1, E2, E3)P (E3)P (E2|E1)P (E1) ,\nand all these quantities are \u2126(1), we can conclude that P\u0302(C) = \u2126(1). Therefore, this process couples with at least some constant probability P independent of n after 8n logn steps. Since we can run this coupling argument independently an arbitrary number of times, it follows that, after 8Ln logn steps, the probability of coupling will be at least 1\u2212 (1\u2212P )L. Therefore, by Theorem 3, for any initial distribution \u03bd,\n\u2016P 8Ln logn\u03bd \u2212 \u03c0\u2016tv\u2264 P\u0302(T > 8Ln logn) \u2264 (1\u2212 P ) L.\nFor any \u01eb, this will be less than \u01eb when\nL \u2265 log(\u01eb)\nlog(1\u2212 pC) ,\nwhich occurs when t \u2265 8n logn log(\u01eb)log(1\u2212pC) . Letting \u01eb = 1 4 ,\ntmix = 8n logn log(4)\n\u2212 log(1\u2212 pC)\nproduces the desired result.\nStatement 6 (LB for Voting: Logical and Ratio). For the voting example using either the logical or ratio semantics, a lower bound for the mixing time for sufficiently small constant values of \u01eb is \u2126(n logn).\nProof. At a minimum, in order to converge, we must sample all the variables. From the coupon collector\u2019s problem, this requires \u2126(n logn) time.\nStatement 7 (UB for Voting: Linear). For the voting example, assume that all the weights on the variables are bounded by |w| \u2264 M , \u2223 \u2223wT (x) \u2223 \u2223 \u2264 M and \u2223 \u2223wF (x) \u2223 \u2223 \u2264 M . Then for linear semantics,\ntmix = 2 O(n).\nProof. From our algebra above in the proof of Statement 5, we know that at any timestep, the probability of not coupling the sampled variable is bounded; that is\npD = O(1).\nTherefore, if we run a correlated-flip coupler for 2n logn timesteps, the probability that coupling will have occurred is greater than the probability that all variables have been sampled (which is \u2126(1)) times the probability that all variables coupled the last time they were sampled. Thus if C is the event that coupling occurs,\nP (C) \u2265 \u2126(1) (1\u2212O(1))2n+1 = exp(\u2212O(n)).\nTherefore, if we run for some t = 2O(n) timesteps, coupling will have occurred at least once with high probability. It follows that the mixing time is\ntmix = 2 O(n),\nas desired.\nStatement 8 (LB for Voting: Linear). For the voting example using linear semantics and bounded weights, a lower bound for the mixing time of a worst-case model is 2\u2126(n).\nProof. Consider a sampler with unary weights wT (x) = wF (x) = 0. Assume that it starts in a state where Q = 1, all the T (x) = 1, and all the F (x) = 0. We will show that it takes an exponential amount of time until Q = \u22121. From above, the probability of flipping Q will be\nP ( Q(k+1) = \u22121 )\n= (1 + exp (2w (\u2016t\u20161)\u2212 \u2016f\u20161)))) \u22121 .\nMeanwhile, the probability to flip any T (x) while Q = 1 is\nP ( T (x)(k+1) = 0 ) = (1 + exp(w)) \u22121 = p,\nfor constant p, and the probability of flipping any F (x) is similarly\nP ( F (x)(k+1) = 1 ) = p.\nNow, consider the following events which could happen at any timestep. While Q = 1, let ET be the event that \u2016t\u20161 \u2264 (1 \u2212 2p)n, and let EF be the event that \u2016f\u20161 \u2265 2pn. Since \u2016t\u20161 and \u2016f\u20161 are both bounded by binomial random variables with parameters 1\u2212 p and p respectively, Hoeffding\u2019s inequality states that, at any timestep,\nP (ET ) = P (\u2016t\u20161 \u2264 (1\u2212 2p)n) \u2264 exp(\u22122p 2n),\nand similarly P (EF ) = P (\u2016f\u20161 \u2265 2pn) \u2264 exp(\u22122p 2n).\nNow, while these bounds are satisfied, let EQ be the event that Q = \u22121. This will be bounded by\nP (EQ) = (1 + exp (2w(1 \u2212 4p)n)) \u22121 \u2264 exp(\u22122w(1 \u2212 4p)n).\nIt follows that at any timestep, P (ET \u2228 EF \u2228 EQ) = exp(\u2212O(n)),\nso, at any timestep k,\nP ( Q(k) = \u22121 ) = k exp(\u2212O(n)).\nHowever, by symmetry, under the stationary distribution \u03c0, this probability must be 12 . Therefore, the total variation distance is bounded by\n\u2016Pt\u03bd \u2212 \u03c0\u2016tv\u2265 1\n2 \u2212 t exp(\u2212O(n)).\nSo, for convergence to less than \u01eb = 14 , for example, we must require at least 2 O(a) steps. This proves the statement."}, {"heading": "B Proof of Theorem 2", "text": "In this section, we prove the main result of the paper, Theorem 2. First, we state some basic lemmas we will need for the proof in Section B.1. Then, we prove the main result inductively in Section B.2. Finally, we prove the lemmas in Section B.3.\nB.1 Statement of Lemmas\nNote that some of these lemmas are restated from the body of the paper.\nDefinition 10 (Absolute Spectral Gap). Let P be the transition matrix of a Markov process. Since it is a Markov process, one of its eigenvalues, the dominant eigenvalue, must be 1. The absolute spectral gap of the Markov process is the value\n\u03b3 = 1\u2212max \u03bb |\u03bb| ,\nwhere the maximum is taken over all non-dominant eigenvalues of P .\nLemma 5 (Disconnected Case). Let G be a disconnected factor graph with n variables and m connected components G1, G2, . . . , Gm with n1, n2, . . . nm variables, respectively. Then,\n\u03b3(G) \u2265 min i\u2264m ni n \u03b3(Gi).\nLemma 4 (Connected Case). Let G and G\u0304 be two factor graphs with maximum factor weight M , which differ only inasmuch as G contains a single additional factor \u03c6\u2217. Then,\n\u03b3(G) \u2265 \u03b3(G\u0304) exp (\u22123M) .\nLemma 6 (Base Case). Let G be a factor graph with one variable and no factors. The absolute spectral gap of Gibbs sampling running on G will be \u03b3(G) = 1.\nLemma 8. Let P be the transition matrix of a reversible, irreducible Markov chain with state space \u2126 and absolute spectral gap \u03b3, and let \u03c0min = minx\u2208\u03c0 \u03c0(x). Then\ntmix(\u01eb) \u2264 \u2212 log (\u01eb\u03c0min) 1\n\u03b3 .\nProof. This is Theorem 12.3 from Markov Chains and Mixing Times [10, p. 155], and a complete proof can be found in that book.\nB.2 Main Proof\nWe achieve the proof of Theorem 2 in two steps. First, we inductively bound the absolute spectral gap of Gibbs sampling on factor graphs in terms of the hierarchy width. Then, we use the bound on the spectral gap to bound the mixing time.\nLemma 9. If G is a factor graph with n variables, maximum factor weight M and hierarchy width h, then Gibbs sampling running on G will have absolute spectral gap \u03b3, where\n\u03b3 \u2265 1\nn exp (\u22123hM) .\nProof. We will prove this result by multiple induction on the number of variables and number of factors of G. In what follows, we assume that the statement holds for all graphs with either fewer variables and no more factors than G, or fewer factors and no more variables than G.\nConsider the connectedness of G. There are three possibilities: 1. G has one variable and no factors. 2. G is connected and has at least one factor. 3. G is disconnected.\nWe consider these cases separately.\nCase 1 If G has no factors and one variable, then by Lemma 6, its spectral gap will be\n\u03b3 = 1.\nAlso, if G has no factors, by (4), its hierarchy width is h = 0. Therefore,\n\u03b3 = 1\n1 exp(0)\n= 1\nn exp(\u22123hM),\nwhich is the desired result.\nCase 2 Next, we consider the case where G is connected and has at least one factor. In this case, by (2), its hierarchy width is\nhw(G) = 1 +min e\u2208E hw(\u3008N,E \u2212 {e}\u3009).\nLet e be an edge that minimizes this quantity, and let G\u0304 denote the factor graph that results from removing the corresponding factor from G. Clearly,\nhw(G) = 1 + hw(G\u0304).\nSince G and G\u0304 differ by only one factor, by Lemma 4,\n\u03b3 \u2265 \u03b3\u0304 exp (\u22123M) .\nFurthermore, since G\u0304 has fewer factors than G, by the inductive hypothesis,\n\u03b3\u0304 \u2265 1\nn exp\n( \u22123Mhw(G\u0304) ) .\nTherefore, \u03b3 \u2265 \u03b3\u0304 exp (\u2212M)\n\u2265 1\nn exp\n( \u22123Mhw(G\u0304) ) exp (\u22123M)\n= 1\nn exp\n( \u22123M ( 1 + hw(G\u0304) ))\n= 1\nn exp (\u22123Mhw(G)) ,\nwhich is the desired result.\nCase 3 Finally, consider the case where G is disconnected. Let G1, G2, . . . , Gm be the connected components of G for some m \u2265 2. In this case, by (3), its hierarchy width is\nhw(G) = max i hw(Gi).\nBy Lemma 5,\n\u03b3 = min i\u2264m ni n \u03b3i,\nwhere ni are the sizes of the Gi and \u03b3i are their absolute spectral gaps. We further know that each of the Gi has fewer nodes than G, so by the inductive hypothesis,\n\u03b3i \u2265 1\nni exp (\u22123Mhw(Gi)) .\nTherefore, \u03b3 = min\ni\u2264m ni n \u03b3i\n\u2265 min i\u2264m ni n 1 ni exp (\u22123Mhw(Gi))\n= min i\u2264m\n1 n exp (\u22123Mhw(Gi))\n= 1\nn exp\n(\n\u22123M max i\u2264m hw(Gi)\n)\n= 1\nn exp (\u22123Mhw(G)) ,\nwhich is the desired result. Since the statement holds in all three cases, by induction it will hold for all factor graphs. This completes the proof.\nNow, we restate and prove the main theorem.\nTheorem 2 (Polynomial Mixing Time). If G is a factor graph with n variables, at most s states per variable, e factors, maximum factor weight M , and hierarchy width h, then\ntmix \u2264 (log(4) + n log(s) + eM)n exp(3hM).\nIn particular, if e is polynomial in n, the number of values for each variable is bounded, and hM = O(log n), then tmix(\u01eb) = O(n O(1)).\nProof. From Lemma 8, we have that\ntmix(\u01eb) \u2264 \u2212 log (\u01eb\u03c0min) 1\n\u03b3 .\nFor our factor graph G, \u03c0min = min\nI \u03c0(I)\n= minI exp(W (I)) \u2211\nJ exp(W (J)) .\nSince there are only at most sn worlds, it follows that\n\u03c0min \u2265 minI exp(W (I))\nsn maxJ exp(W (J))\n= s\u2212n exp (\nmin I W (I)\u2212max J\nW (J) ) .\nExpanding the world weight in terms of the factors,\n\u03c0min \u2265 s \u2212n exp\n\nmin I\n\u2211\n\u03c6\u2208\u03a6\n\u03c6(I)\u2212max J\n\u2211\n\u03c6\u2208\u03a6\n\u03c6(J)\n\n\n\u2265 s\u2212n exp\n\n\n\u2211\n\u03c6\u2208\u03a6\n(\nmin I \u03c6(I)\u2212max J\n\u03c6(J) )\n\n .\nApplying our maximum factor weight bound,\n\u03c0min \u2265 s \u2212n exp\n\n\u2212 \u2211\n\u03c6\u2208\u03a6\nM\n\n\n= s\u2212n exp (\u2212eM) .\nSubstituting this into the expression from Lemma 8 produces\ntmix(\u01eb) \u2264 \u2212 log ( \u01eb\nsn exp (\u2212eM)\n) 1\n\u03b3\n= (n log(s) + eM \u2212 log(\u01eb)) 1\n\u03b3 ,\nand substituting the result from Lemma 9 gives\ntmix(\u01eb) \u2264 (n log(s) + eM \u2212 log(\u01eb))n exp(3hM).\nSubstituting \u01eb = 14 gives the desired result.\nB.3 Proofs of Lemmas\nIn this statement, we will restate and prove the lemmas stated above in Section B.1.\nLemma 5 (Disconnected Case). Let G be a disconnected factor graph with n variables and m connected components G1, G2, . . . , Gm with n1, n2, . . . nm variables, respectively. Then,\n\u03b3(G) \u2265 min i\u2264m ni n \u03b3(Gi).\nProof. Lemma 5 follows directly from Corollary 12.12 in Markov Chains and Mixing Times [10, p. 161]. For completeness, we restate the proof here.\nSince G is disconnected, Gibbs sampling on G is equivalent to running m independent Gibbs samplers on the Gi, where at each timestep, the Gibbs sampler of Gi is updated if a variable in Gi is chosen; this occurs with probability ni n\n. Therefore, if we let Pi denote the Markov transition matrix of Gibbs sampling on Gi, we can write the transition matrix P of Gibbs sampling on G as\nP (x, y) =\nm \u2211\ni=1\nni n Pi(xi, yi) \u220f\nj 6=i\nI(xj , yj),\nwhere x and y are worlds on G, where xi and yi are subsets of the variables of x and y respectively that correspond to the graph Gi, and I is the identity matrix (I(x, y) = 1 if x = y, and I(x, y) = 0 otherwise).\nNow, we, for each i, we let fi be some eigenvector of Pi with eigenvalue \u03bbi, and define\nf(x) =\nm \u220f\ni=1\nfi(xi).\nIt follows that (Pf)(y) = \u2211\nx\nf(x)P (x, y)\n= \u2211\nx\nf(x)\nm \u2211\ni=1\nni n Pi(xi, yi) \u220f\nj 6=i\nI(xj , yj)\n= \u2211\nx\nm \u2211\ni=1\nni n fi(xi)Pi(xi, yi) \u220f\nj 6=i\nfj(xj)I(xj , yj)\n=\nm \u2211\ni=1\nni n \u2211\nxi\nfi(xi)Pi(xi, yi) \u220f\nj 6=i\n\u2211\nxj\nfj(xj)I(xj , yj)\n=\nm \u2211\ni=1\nni n \u03bbifi(yi) \u220f\nj 6=i\nfj(yj)\n= f(y) m \u2211\ni=1\nni n \u03bbi;\nif follows that f is an eigenvector of P with eigenvalue\n\u03bb =\nm \u2211\ni=1\nni n \u03bbi. (9)\nFurthermore, it is clear that such eigenvectors will form a basis for the space of possible distributions on the worlds of G. Therefore, all eigenvalues of P will be of the form (9), and so\n|\u03bb| \u2264 m \u2211\ni=1\nni n |\u03bbi| .\nTherefore, \u03b3 = min\n\u03bb6=1 (1\u2212 |\u03bb|)\n\u2265 min \u03bb6=1\nm \u2211\ni=1\nni n (1\u2212 |\u03bbi|)\n\u2265 min i min \u03bbi 6=1 ni n (1\u2212 |\u03bbi|)\n= min i ni n \u03b3i,\nwhich is the desired result.\nTo prove Lemma 4, we will need to first prove two other lemmas.\nLemma 10. Let G and G\u0304 be two factor graphs, which differ only inasmuch as G contains a single additional factor \u03c6\u2217 with maximum factor weight M . Then, if \u03c0 and \u03c0\u0304 denote the distributions on worlds for these graphs,\nexp(\u2212M)\u03c0\u0304(x) \u2264 \u03c0(x) \u2264 exp(M)\u03c0\u0304(x).\nProof. From the definition of the distribution function,\n\u03c0(x) = exp(W (x)) \u2211\nz\u2208\u2126 exp(W (z))\n= exp(W\u0304 (x) + \u03c6\u2217(x)) \u2211\nz\u2208\u2126 exp(W\u0304 (z) + \u03c6 \u2217(z))\n\u2265 exp(W\u0304 (x) + minI \u03c6 \u2217(I)) \u2211\nz\u2208\u2126 exp(W\u0304 (z) + maxI \u03c6 \u2217(I))\n= exp(W\u0304 (x)) \u2211\nz\u2208\u2126 exp(W\u0304 (z)) exp\n(\nmin I \u03c6\u2217(I)\u2212max I\n\u03c6\u2217(I) )\n= exp(\u2212M)\u03c0\u0304(x).\nIn the other direction,\n\u03c0(x) = exp(W\u0304 (x) + \u03c6\u2217(x)) \u2211\nz\u2208\u2126 exp(W\u0304 (z) + \u03c6 \u2217(z))\n\u2264 exp(W\u0304 (x) + maxI \u03c6 \u2217(I)) \u2211\nz\u2208\u2126 exp(W\u0304 (z) + minI \u03c6 \u2217(I))\n= exp(W\u0304 (x)) \u2211\nz\u2208\u2126 exp(W\u0304 (z)) exp\n(\nmax I \u03c6\u2217(I)\u2212min I\n\u03c6\u2217(I) )\n= exp(M)\u03c0\u0304(x).\nThis proves the lemma.\nLemma 11. Let G and G\u0304 be two factor graphs, which differ only inasmuch as G contains a single additional factor \u03c6\u2217 with maximum factor weight M . Then, if P and P\u0304 denote the transition matrices for Gibbs sampling on these graphs, for all x 6= y,\nexp(\u2212M)P\u0304 (x, y) \u2264 P (x, y) \u2264 exp(M)P\u0304 (x, y).\nProof. For Gibbs sampling on G, we know that, if worlds x and y differ in the value of only one variable, and L is the set of worlds that only differ from x and y in the same variable, then\nP (x, y) = 1\nn\nexp(W (y)) \u2211\nl\u2208L exp(W (l))\nIf P\u0304 is the transition matrix for Gibbs sampling on G\u0304, then the same argument will show that\nP\u0304 (x, y) = 1\nn\nexp(W\u0304 (y)) \u2211\nl\u2208L exp(W\u0304 (l)) .\nTherefore,\nP (x, y) = 1\nn\nexp(W\u0304 (y) + \u03c6\u2217(y)) \u2211\nl\u2208L exp(W\u0304 (l) + \u03c6 \u2217(y))\n\u2265 1\nn\nexp(W\u0304 (y) + minI \u03c6 \u2217(I))\n\u2211\nl\u2208L exp(W\u0304 (l) + maxI \u03c6 \u2217(I))\n= 1\nn\nexp(W\u0304 (y)) \u2211\nl\u2208L exp(W\u0304 (l)) exp\n(\nmin I \u03c6\u2217(I)\u2212max I\n\u03c6\u2217(I) )\n= exp(\u2212M)P\u0304 (x, y).\nSimilarly,\nP (x, y) = 1\nn\nexp(W\u0304 (y) + \u03c6\u2217(y)) \u2211\nl\u2208L exp(W\u0304 (l) + \u03c6 \u2217(y))\n\u2264 1\nn\nexp(W\u0304 (y) + maxI \u03c6 \u2217(I))\n\u2211\nl\u2208L exp(W\u0304 (l) + minI \u03c6 \u2217(I))\n= 1\nn\nexp(W\u0304 (y)) \u2211\nl\u2208L exp(W\u0304 (l)) exp\n(\nmax I \u03c6\u2217(I)\u2212min I\n\u03c6\u2217(I) )\n= exp(M)P\u0304 (x, y).\nOn the other hand, if x and y differ in more than one variable, then\nP (x, y) = P\u0304 (x, y) = 0.\nTherefore, the lemma holds for any x 6= y.\nNow, we restate and prove Lemma 4.\nLemma 4 (Connected Case). Let G and G\u0304 be two factor graphs with maximum factor weight M , which differ only inasmuch as G contains a single additional factor \u03c6\u2217. Then,\n\u03b3(G) \u2265 \u03b3(G\u0304) exp (\u22123M) .\nProof. The proof is similar to proofs using the Dirichlet form to bound the spectral gap [10, p. 181]. Recall that Gibbs sampling is a reversible Markov chain. That is, if P is the transition matrix associated with Gibbs sampling on G, and \u03c0 is its stationary distribution,\n\u03c0(x)P (x, y) = \u03c0(y)P (y, x).\nIf we let \u03a0 be the diagonal matrix such that \u03a0(x, x) = \u03c0(x), then we can write this as\nP\u03a0 = \u03a0PT ;\nthat is, A = P\u03a0 is a symmetric matrix. Now, consider the form\n\u03b1(f) = 1\n2\n\u2211\nx,y\n(f(x)\u2212 f(y))2 \u03c0(x)P (x, y).\nExpanding this produces\n\u03b1(f) = 1\n2\n\u2211\nx,y\nf(x)2\u03c0(x)P (x, y) \u2212 \u2211\nx,y\nf(x)f(y)\u03c0(x)P (x, y) + 1\n2\n\u2211\nx,y\nf(y)2\u03c0(x)P (x, y).\nBy reversibility of the chain,\n\u03b1(f) = 1\n2\n\u2211\nx,y\nf(x)2\u03c0(x)P (x, y) \u2212 \u2211\nx,y\nf(x)f(y)\u03c0(x)P (x, y) + 1\n2\n\u2211\nx,y\nf(y)2\u03c0(y)P (y, x)\n= \u2211\nx,y\nf(x)2\u03c0(x)P (x, y) \u2212 \u2211\nx,y\nf(x)f(y)\u03c0(x)P (x, y)\n= \u2211\nx\nf(x)2\u03c0(x) \u2211\ny\nP (x, y)\u2212 \u2211\nx,y\nf(x)f(y)\u03c0(x)P (x, y)\n= \u2211\nx\n\u03c0(x)f(x)2 \u2212 \u2211\nx,y\nf(x)f(y)A(x, y)\n= fT\u03a0f \u2212 fTAf = fT (I \u2212 P )\u03a0f.\nSimilarly, if we define\n\u03b2(f) = 1\n2\n\u2211\nx,y\n(f(x) + f(y)) 2 \u03c0(x)P (x, y),\nthen the same logic will show that \u03b2(f) = fT (I + P )\u03a0f.\nWe define A\u0304, \u03b1\u0304 and \u03b2\u0304 similarly for Gibbs sampling on G\u0304. Now, by Lemmas 10 and 11,\n\u03b1(f) = 1\n2\n\u2211\nx,y\n(f(x)\u2212 f(y))2 \u03c0(x)P (x, y)\n\u2265 1\n2\n\u2211\nx,y\n(f(x)\u2212 f(y))2 exp(\u22122M)\u03c0\u0304(x)P\u0304 (x, y)\n= exp(\u22122M)\u03b1\u0304(f).\nIt follows that fT (I \u2212 P )\u03a0f \u2265 exp(\u22122M)fT (I \u2212 P\u0304 )\u03a0\u0304f.\nWe also notice that, by Lemma 10,\nfT ( \u03a0\u2212 \u03c0\u03c0T ) f = fT\u03a0f \u2212 (\u03c0T f)2\n= \u2211\nx\nf(x)2\u03c0(x) \u2212\n(\n\u2211\ny\nf(y)\u03c0(y)\n)2\n= \u2211\nx\n(\nf(x)\u2212 \u2211\ny\nf(y)\u03c0(y)\n)2\n\u03c0(x)\n\u2264 \u2211\nx\n(\nf(x)\u2212 \u2211\ny\nf(y)\u03c0\u0304(y)\n)2\n\u03c0(x)\n\u2264 exp(M) \u2211\nx\n(\nf(x)\u2212 \u2211\ny\nf(y)\u03c0\u0304(y)\n)2\n\u03c0\u0304(x)\n= exp(M)\n\n\n\u2211\nx\nf(x)2\u03c0\u0304(x)\u2212\n(\n\u2211\ny\nf(y)\u03c0\u0304(y)\n)2 \n\n= exp(M)fT ( \u03a0\u0304\u2212 \u03c0\u0304\u03c0\u0304T ) f\nTherefore, fT (I \u2212 P )\u03a0f\nfT (\u03a0\u2212 \u03c0\u03c0T ) f \u2265 exp(\u22123M)\nfT (I \u2212 P\u0304 )\u03a0\u0304f\nfT ( \u03a0\u0304\u2212 \u03c0\u0304\u03c0\u0304T ) f . (10)\nNow, let\u2019s explore the quantity\nmin f\nfT (I \u2212 P )\u03a0f\nfT (\u03a0\u2212 \u03c0\u03c0T ) f . (11)\nFirst, we notice that (I \u2212 P )\u03a01 = (I \u2212 P )\u03c0 = \u03c0 \u2212 \u03c0 = 0,\nso 1 is an eigenvector of (I \u2212 P )\u03a0 with eigenvalue 0. Furthermore, (\n\u03a0\u2212 \u03c0\u03c0T ) 1 = ( \u03c0 \u2212 \u03c0\u03c0T1 ) = \u03c0 \u2212 \u03c0 = 0,\nso 1 is also an eigenvector of (I \u2212 P )\u03a0 with eigenvalue 0. It follows that (11) is invariant to additions of the vector 1 to f , and in particular we can therefore choose to minimize over only those vectors for which \u03c0T f = 0. Therefore,\nmin f\nfT (I \u2212 P )\u03a0f\nfT (\u03a0\u2212 \u03c0\u03c0T ) f = min \u03c0T g=0\ngT (I \u2212 P )\u03a0g\ngT\u03a0g\n= min \u03c0T g=0\ngT\u03a0 1 2\u03a0\u2212 1 2 (I \u2212 P )\u03a0 1 2\u03a0 1 2 g\ngT\u03a0g .\nIf we let h = \u03a0 1 2 g, then\nmin f\nfT (I \u2212 P )\u03a0f\nfT (\u03a0\u2212 \u03c0\u03c0T ) f = min\n1T\u03a0 1 2 h=0\nhT\u03a0\u2212 1 2 (I \u2212 P )\u03a0 1 2 h\nhTh .\nNotice that \u03a0\u2212 1 2 (I \u2212 P )\u03a0 1\n2 is similar to I \u2212 P , and so it must have the same eigenvalues. Since it is symmetric, it has an orthogonal complement of eigenvectors, and the eigenvector that corresponds to \u03bb = 0 is\n(\n\u03a0\u2212 1 2 (I \u2212 P )\u03a0 1 2\n)(\n\u03a0 1 21\n)\n= \u03a0\u2212 1 2 (I \u2212 P )\u03c0 = \u03a0\u2212 1 2 (\u03c0 \u2212 \u03c0) = 0.\nThe expression above is minimizing over all vectors orthogonal to this eigenvector; it follows that the minimum will be the second smallest eigenvalue of I \u2212P . This eigenvalue will be 1\u2212\u03bb2, where \u03bb2 is the second-largest eigenvalue of P . So,\nmin f\nfT (I \u2212 P )\u03a0f\nfT (\u03a0\u2212 \u03c0\u03c0T ) f = 1\u2212 \u03bb2.\nThe same argument will show that\nmin f\nfT (I \u2212 P\u0304 )\u03a0\u0304f\nfT ( \u03a0\u0304\u2212 \u03c0\u0304\u03c0\u0304T ) f = 1\u2212 \u03bb\u03042.\nTherefore, minimizing both sides in (10) produces\n1\u2212 \u03bb2 \u2265 exp(\u22123M) ( 1\u2212 \u03bb\u03042 ) .\nAnd applying the definition of absolute spectral gap,\n1\u2212 \u03bb2 \u2265 exp(\u22123M)\u03b3\u0304\nNow, a similar argument to the above will show that\nfT (I + P )\u03a0f\nfT\u03a0f \u2265 exp(\u22123M)\nfT (I + P\u0304 )\u03a0\u0304f\nfT \u03a0\u0304f .\nAnd we will be able to conclude that\nmin f\nfT (I + P )\u03a0f\nfT\u03a0f = 1\u2212 \u03bbn,\nwhere \u03bbn is the (algebraically) smallest eigenvalue of P , and similarly for \u03bb\u0304n. Therefore, we can conclude that\n1 + \u03bbn \u2265 exp(\u22123M) ( 1 + \u03bb\u0304n ) ,\nand from the definition of absolute spectral gap,\n1 + \u03bbn \u2265 exp(\u22123M)\u03b3\u0304.\nTherefore, \u03b3 \u2265 min(1\u2212 \u03bb2, 1 + \u03bbn) \u2265 exp(\u22123M)\u03b3\u0304.\nThis proves the lemma.\nNext, we restate and prove Lemma 6\nLemma 6 (Base Case). Let G be a factor graph with one variable and no factors. The absolute spectral gap of Gibbs sampling running on G will be \u03b3(G) = 1.\nProof. Gibbs sampling on a factor graph with one variable and no factors will have transition matrix\nP = \u03c01T ,\nwhere \u03c0 is the stationary distribution, and 1 is the vector of all 1s. That is, the process achieves the stationary distribution in a single step. The only eigenvalues of this matrix are 0 and 1, so the absolute spectral gap will be 1, as desired."}, {"heading": "C Proofs of Other Results", "text": "In this section, we will prove the other results about hierarchy decomposition stated in Section 2.3. First, we express it in terms of a hypergraph decomposition; this is how hypertree width is defined so it is useful for comparison.\nDefinition 11 (Hierarchy Decomposition). A hierarchy decomposition of a hypergraph G = \u3008N,E\u3009 is a rooted tree T where each node v of T is labeled with a set \u03c7(v) of edges of G, that satisfies the following conditions: (1) for each edge e \u2208 E, there is some node v of T such that e \u2208 \u03c7(v); (2) if for two nodes u and v of T and for some edge e \u2208 E, both e \u2208 \u03c7(u) and e \u2208 \u03c7(v), then for all nodes w on the (unique) path between u and v in T , e \u2208 \u03c7(w); (3) for every pair of edges e \u2208 E and f \u2208 E, if e \u2229 f 6= \u2205, then there exists a node v of T such that {e, f} \u2286 \u03c7(v); and (4) if node u is the parent of node v in T , then u \u2286 v.\nThe width of a hierarchy decomposition is the size of the largest node in T , that is, maxv |\u03c7(v)|.\nStatement 9. The hierarchy width of a hypergraph G is equal to the minimum width of a hierarchy decomposition of G.\nIn this section, we denote the width of a hierarchy decomposition T as w(T ).\nLemma 12. Let T be a hierarchy decomposition of a hypergraph G. Let u be a node of T that has exactly one child v, and let e be the edge connecting u and v. Let T\u0304 be the graph minor of T that is formed by removing u from T and, if applicable, connecting v with the parent of u; assume that the nodes T\u0304 have the same labeling as the corresponding nodes in T . Then T\u0304 is a hierarchy decomposition of G, and\nw(T ) = w(T\u0304 ).\nProof. We validate the conditions of the definition of hierarchy decomposition individually. 1. Since T is a hierarchy decomposition of G, by condition (1), for any hyperedge e of G, there exists a node x\nof T such that e \u2208 \u03c7T (x). If x 6= u, then e \u2208 \u03c7T\u0304 (x). Otherwise, by condition (4) of T , it will hold that \u03c7T (u) \u2286 \u03c7T (v) = \u03c7T\u0304 (v), so e \u2208 \u03c7T\u0304 (v). Since this is true for any hyperedge e, the condition holds. 2. Since we constructed T\u0304 by removing a node from T and connecting its neighbors, it follows that if the path from node x to node y in T\u0304 passes through a node z, then the path from x to y in T also passes through z. The condition then follows directly from condition (2) of T . 3. The argument here is the same as the argument for condition (1). 4. This condition follows directly from transitivity of the subset relation. Therefore, T\u0304 is a hierarchy decomposition of G. To show that w(T ) = w(T\u0304 ), it suffices to notice that removing node u from T can\u2019t change its width, since node v has at least as many hyperedges in its labeling as u. This proves the lemma.\nLemma 13. Let T be a hierarchy decomposition of a hypergraph G. Let u be a node of T , and let v be a child of T such that \u03c7T (u) = \u03c7T (v). Let T\u0304 be the graph minor of T that is formed by identifying u and v by contracting along their connecting edge. Assume that the nodes T\u0304 have the same labeling as the corresponding nodes in T , and that the new node, w, has \u03c7T\u0304 (w) = \u03c7T (u) = \u03c7T (v). Then T\u0304 is a hierarchy decomposition of G, and\nw(T ) = w(T\u0304 ).\nProof. We validate the conditions of the definition of hierarchy decomposition individually. 1. The set of labelings of T\u0304 is exactly the same as the set of labelings of T . Since T is a hierarchy decomposition\nof G, the condition follows directly from condition (1) of T . 2. Similarly, the set of labelings of any path in T\u0304 is the same as the set of labeling of the corresponding path in T .\nThe condition then follows directly from condition (2) of T . 3. The argument here is the same as the argument for condition (1). 4. The labeling of every node\u2019s parent in T\u0304 will be the same as the labeling of that node\u2019s parent in T . Therefore,\nthe condition follows from condition (3) of T . Therefore, T\u0304 is a hierarchy decomposition of G. To show that w(T ) = w(T\u0304 ), it suffices to notice the set of labelings of nodes of T\u0304 is the same as the set of labelings of nodes of T ; therefore they must have the same width.\nLemma 14. For any factor graph G and for any hierarchy decomposition T of G,\nw(T ) \u2265 hw(G).\nProof. As in the proof of Lemma 9, we will prove this result by multiple induction. In what follows, we assume that the statement holds for all graphs with either fewer vertexes and no more hyperedges than G, or fewer hyperedges and no more vertexes than G. We also assume that for a particular G, the statement holds for all hierarchy decompositions of G that have fewer nodes than T .\nThere are five possibilities: 1. The root r of T is labeled with \u03c7(r) = \u2205, and r has no children. 2. The root r of T is labeled with \u03c7(r) = \u2205, and r has one child. 3. The root r of T is labeled with \u03c7(r) = \u2205, and r has two or more children, at least one of which, x, is also labeled\nwith \u03c7(x) = \u2205. 4. The root r of T is labeled with \u03c7(r) = \u2205, and r has two or more children, none of which are labeled with the\nempty set. 5. The root r of T is labeled with \u03c7(r) 6= \u2205.\nWe consider these cases separately.\nCase 1 If r is labeled with the empty set and has no children, then it follows from condition (1) that there are no hyperedges in G. Therefore, hw(G) = 0. Also, since r is labeled with the empty set, w(T ) = 0. So, the statement holds in this case.\nCase 2 If r has exactly one child, then by Lemma 12, there is a hierarchy decomposition T\u0304 of G such that T\u0304 has fewer nodes than T and w(T ) = w(T\u0304 ). By the inductive hypothesis,\nw(T\u0304 ) \u2265 hw(G).\nSo, the statement holds in this case.\nCase 3 If r is labeled with the empty set and has two or more children, at least one of which is also labeled with the empty set, then by Lemma 13, there is a hierarchy decomposition T\u0304 of G such that T\u0304 has fewer nodes than T and w(T ) = w(T\u0304 ). By the inductive hypothesis,\nw(T\u0304 ) \u2265 hw(G).\nSo, the statement holds in this case.\nCase 4 Consider the case where r is labeled with the empty set and has two or more children, none of which are labeled with the empty set. Let T1, T2, . . . , Tl be the labeled subtrees rooted at the children of r. Notice that if a hyperedge e appears in the labeling of some node of Ti, then it can\u2019t appear in the labeling of any node of Tj for i 6= j, since otherwise by condition (2) it must appear in the labeling of the root node r, and r has an empty labeling. Therefore, the hyperedges of G are partitioned among the subtrees Ti. Furthermore, condition (3) implies that hyperedges associated with different subtrees are disconnected, and therefore that graph G is disconnected. Therefore, if we let Gi denote the m connected components of G.\nhw(G) = max i\u2264m hw(Gi).\nNow, let G\u0304i, for i \u2264 l, denote the subgraph of G that consists of the nodes that are connected to a hyperedge in a labeling of a node of Ti. Clearly, the G\u0304i will be disconnected, so\nhw(G) = max i\u2264l hw(G\u0304i).\nEach G\u0304i will also have fewer vertexes and hyperedges than G, so by the inductive hypothesis,\nw(Ti) \u2265 hw(G\u0304i).\nTherefore, w(T ) = max\ni\u2264l w(Ti) \u2265 max i\u2264l hw(G\u0304i) = hw(G);\nso the statement holds in this case.\nCase 5 Finally, consider the case where the root node r is labeled with some hyperedge e. Let G\u0304 be the graph that results from removing e from G, and let T\u0304 be the hierarchy decomposition that results from removing e from all the labellings of T . Clearly, T\u0304 will be a hierarchy decomposition of G\u0304. Furthermore, G\u0304 has fewer hyperedges and the same number of vertexes as G, so by the inductive hypothesis,\nw(T\u0304 ) \u2265 hw(G\u0304).\nFurthermore, since removing e decreases the size of each of the labelings of T\u0304 by 1,\nw(T\u0304 ) = w(T )\u2212 1.\nTherefore, w(T\u0304 ) \u2265 hw(G\u0304) + 1 \u2265 hw(G),\nso the statement holds in this case. Therefore, the statement holds in all cases, so the lemma follows from induction on all hypergraphs and all hierarchy decompositions.\nLemma 15. For any factor graph G, there exists a hierarchy decomposition T of G such that\nw(T ) = hw(G).\nProof. As in the proof of Lemma 9, we will prove this result by multiple induction. In what follows, we assume that the statement holds for all graphs with either fewer vertexes and no more hyperedges than G, or fewer hyperedges and no more vertexes than G.\nThere are three possibilities: 1. G has no hyperedges. 2. G is disconnected. 3. G is connected.\nWe consider these cases separately.\nCase 1 If G has no hyperedges, then the tree T with a single node labeled with the empty set is a hierarchy decomposition for G. It will satisfy\nw(T ) = 0.\nAlso, by (4), for the case with no hyperedges, hw(G) = 0,\nso the statement holds in this case.\nCase 2 Assume that G is disconnected, and its connected components are Gi. Then by (3),\nhw(G) = max i hw(Gi).\nSince each Gi has fewer nodes and hyperedges than G, by the inductive hypothesis there exists a hierarchy decomposition Ti for each Gi such that\nw(Ti) = hw(Gi).\nLet T be the tree that has a root node labeled with the empty set, and where the subtrees rooted at its children are exactly the Ti above. We now show that T is a hierarchy decomposition for G by validating the conditions.\n1. Any hyperedge e of G must be a hyperedge of exactly one Gi. Since Ti is a hierarchy decomposition of Gi, by condition (1) it follows that e appears in the labeling of some node of Gi. Therefore, e will appear in the labeling of the corresponding node of G, and the condition holds. 2. Again, any hyperedge e of G must be a hyperedge of exactly one Gi. Therefore, if e appears in two nodes of T , those two nodes must both be part of the same subtree Ti. The condition follows from the corresponding condition of Ti. 3. Since G is disconnected, any pair of hyperedges in G that share a vertex must both be part of exactly one Gi. The condition then follows from the corresponding condition of Ti. 4. This condition follows directly from the fact that for any X , \u2205 \u2286 X , and from the corresponding condition for the subgraphs Ti. Therefore T is a hierarchy decomposition for G. Furthermore, since the labelings of nodes of T are the union of the labelings of nodes of Ti,\nw(T ) = max i w(Ti).\nTherefore, hw(G) = max\ni hw(Gi) = max i w(Ti) = w(T ),\nso the statement holds in this case.\nCase 3 Assume that G is connected. Then by (2),\nhw(G) = 1 +min e\u2208E hw(\u3008N,E \u2212 {e}\u3009).\nLet b be a hyperedge that minimizes this quantity, and let G\u0304 be the graph that results from removing this hyperedge from G. Then,\nhw(G) = 1 + hw(G\u0304\u3009).\nNow, G\u0304 has fewer hyperedges than G, so by the inductive hypothesis, there exists a hierarchy decomposition T\u0304 of G\u0304 such that\nhw(G\u0304) = w(T\u0304 ).\nLet T be the tree that results from adding edge b to every labeling of a node of T\u0304 . Clearly,\nw(T ) = w(T\u0304 ) + 1.\nWe now show that T is a hierarchy decomposition for G by validating the conditions. 1. Any hyperedge e of G is either hyperedge b or some hyperedge in G\u0304. If it is b, then it must appear in a labeling\nof a node of T since it appears in all such labelings. Otherwise, the condition follows from the corresponding condition of T\u0304 . 2. Again, any hyperedge e of G is either hyperedge b or some hyperedge in G\u0304. If it is b, then the condition follows from the fact that all node labelings of T contain b. Otherwise, the condition follows from the corresponding condition of T\u0304 . 3. For any pair of edges (e, f) of G, either b /\u2208 {e, f}, or b \u2208 {e, f}. If b \u2208 {e, f}, then the condition follows from condition (1) of T\u0304 and the fact that all node labelings of T contain b; otherwise, the condition follows from the corresponding condition of T\u0304 .\n4. This condition follows directly from the fact that for any X and Y , if X \u2286 Y , then X \u222a {b} \u2286 Y \u222a {b}. Therefore T is a hierarchy decomposition for G, and\nw(T ) = w(T\u0304 ) + 1 = hw(G\u0304) + 1 = hw(G),\nso the statement holds in this case. Therefore the statement holds in all cases, so the lemma follows from induction over all factor graphs.\nNext, we restate and prove Statement 9.\nStatement 9. The hierarchy width of a hypergraph G is equal to the minimum width of a hierarchy decomposition of G.\nProof. This statement follows directly from Lemmas 14 and 15.\nNow, we provide a definition for hypertree width, and prove the comparison statements from the body of the paper.\nDefinition 12 (Hypertree Decomposition [6]). A hypertree decomposition of a hypergraph G is a structure \u3008T, \u03c1, \u03c7\u3009, where \u3008T, \u03c1\u3009 is a tree decomposition for the primal graph of G, \u03c7 is a labeling of the nodes of T with hyperedges of \u03c7, and the following conditions are satisfied: (1) for any node u of T , and for all vertices x \u2208 \u03c1(u), there exists an edge e \u2208 \u03c7(u) such that x \u2208 e; and (2) for any node u of T , for any edge e \u2208 \u03c7(u), and for any descendant v of u in T , e \u2229 \u03c1(v) \u2286 \u03c1(u).\nThe width of a hypertree decomposition, denoted w(T ), is defined (as in the hierarchy decomposition case) to be the cardinality of the largest hyperedge-labeling of a node of T . That is,\nw(T ) = max u |\u03c7(u)| .\nThe hypertree width tw(G) of a graph G is the minimum width among all hypertree decompositions of G.\nWe restate and prove Statement 1.\nStatement 1. For any factor graph G, tw(G) \u2264 hw(G).\nProof. Consider the hierarchy decomposition T of G that satisfies w(T ) = hw(G). Assume that we augment T with an additional node labeling \u03c1(v) that consists of the vertexes of G that are part of at least one edge of \u03c7(v) (that is, \u03c1(v) = \u222a\u03c7(v)). We show that T is a hypertree decomposition of G.\nFirst, we must show that \u3008T, \u03c1\u3009 is a tree decomposition for the primal graph of G; we do so by verifying the conditions independently.\n1. Each vertex of G must appear in the labeling of some node of T , since (by assumption) it must appear in some hyperedge of G, and each hyperedge appears in the labeling of some node of T because T is a hierarchy decomposition of G. 2. Similarly, any two vertexes (x, y) that are connected by an edge in the primal graph of G must appear together in some hyperedge of G. Since each hyperedge of G appears in the edge-labeling of some node of T , it follows that x and y must both appear in the vertex-labeling of that node. 3. Finally, assume that vertex x appears in the labeling of two different nodes u and v of T . Since x \u2208 \u03c1(u), there must exist an edge e \u2208 \u03c7(u) such that x \u2208 e; similarly, there must be an edge f \u2208 \u03c7(v) such that x \u2208 f . By condition (3) of the definition of hierarchy decomposition, there must exist a node w of T such that {e, f} \u2286 \u03c7(w). By condition (2) of the definition of hierarchy decomposition, for all nodes z on the path between u and w, e \u2208 \u03c7(z); similarly, for all nodes z on the path between v and w, f \u2208 \u03c7(z). Now, any node z on the path between u and v must be either part of the path between u and w and the path between v and w, therefore either e \u2208 \u03c7(z) or f \u2208 \u03c7(z). It follows that x \u2208 \u03c1(z), as desired.\nTherefore T is a tree decomposition for the primal graph of G. Next, we show that the T is also a hypertree decomposition for G by verifying the conditions independently.\n1. For any node u of T , and for all vertices x \u2208 \u03c1(u), there must exist an edge e \u2208 \u03c7(u) such that x \u2208 e, because \u03c1(u) = \u222a\u03c7(v). Therefore the condition holds. 2. For any node u of T and for any descendant v of u in T , we know from condition (4) of the definition of hierarchy decomposition that \u03c7(u) \u2286 \u03c7(v). So, for any edge e \u2208 \u03c7(u), it also holds that e \u2208 \u03c7(v). Therefore, e \u2229 \u03c1(v) = e \u2286 \u03c1(u), and so the condition holds. We conclude that T is a hypertree decomposition for G. But, its width is hw(G). Since the hypertree width of G is the minimum width among all hypertree decompositions of G, it follows that\ntw(G) \u2264 hw(G),\nwhich is the desired expression.\nNext, we restate and prove Statement 2.\nStatement 2. For any fixed k, computing whether hw(G) \u2264 k can be done in time polynomial in the number of factors of G.\nProof. Consider Algorithm 1, which computes this quantity for a fixed k.\nAlgorithm 1 HW(G, k): Compute if hw(G) \u2264 k if G has no edges then\nreturn true end if if k = 0 then\nreturn false end if for e \u2208 edges(G) do\nLet G\u0304 be the graph that results from removing e from the connected component of G that contains e if HW(G\u0304, k \u2212 1) then\nreturn true end if\nend for return false\nEach execution of HW requires at most linear time in the number of hyperedges of G to compute its connected components. If we let e be the number of hyperedges of G, it also requires at most e executions of G with parameter k \u2212 1. Therefore, HW will run in time O(ek), which is polynomial in the number of hyperedges, as desired.\nAs an aside here, we note that the hierarchy width can be expressed in terms of an existing graph parameter, the tree-depth [29, p. 115], which we denote td(G). To do this, we let L(G), the line graph of G, denote the graph such that every factor of G is a node of L(G), and two nodes of L(G) are connected by an edge if and only if their corresponding factors share a dependent variable. Using this, it is trivial to show (by definition) that\nhw(G) = td(L(G)).\nSince it is a known result that it is possible to compute whether a graph has tree-depth at most k in time polynomial (in fact, linear) in the number of nodes of the graph, we could have also used this to prove Statement 2; we avoided doing this to make the result more accessible.\nStatement 10. The hierarchy width of a factor graph G is greater than or equal to the maximum degree of a variable in G.\nFinally, we restate and prove Statement 10.\nStatement 10. The hierarchy width of a factor graph G is greater than or equal to the maximum degree of a variable in G.\nProof. The statement follows by induction. Considering the parts of the definition of hierarchy width individually, removing a single hyperedge from a hypergraph only decreases the maximum degree of the hypergraph by at most 1, and splitting the hypergraph into connected components doesn\u2019t change the maximum degree."}, {"heading": "D Proofs of Factor Graph Template Results", "text": "In this section, we prove the results in Section 3.1. First, we prove Lemma 16.\nLemma 16. If G is an instance of a hierarchical factor graph template G with E template factors, then hw(G) \u2264 E.\nLemma 16. If G is an instance of a hierarchical factor graph template G with E template factors, then hw(G) \u2264 E.\nProof. We prove this result using the notion of hierarchy decomposition from the previous section. For the instantiated factor graph G, let T be a tree where each node v of T is associated with a particular assignment of the first n head symbols of the rules. That is, there is a node of T for each tuple of object symbols (x1, . . . , xm)\u2014even for m = 0. (Since the rules are hierarchical, and therefore must contain the same symbols in the same order, these assignments are well-defined.)\nLabel each node v of T with the set containing all the instantiated factors of G that agree with this assignment of head variables. That is, if the head variable assignment for a factor \u03c6 is (y1, . . . , yp), then it will be in the labeling of the node v = (x1, . . . , xm) if and only if m \u2265 p and for all i \u2264 p, yi = xi. It is not hard to show that this is a valid hierarchy decomposition for this factor graph.\nThe width of this hierarchy decomposition is at most the number of template factors E because each node v can only possibly contain a single instantiated factor for each template factor. (This is because for each node, only one possible assignment of head variables is compatible with that node.) The Lemma now follows from an application of Statement 9.\nNext, we restate and prove Statement 4.\nStatement 4. For any fixed hierarchical factor graph template G, if G is an instance of G with bounded weights using either logical or ratio semantics, then the mixing time of Gibbs sampling on G is polynomial in the number of objects n in its dataset. That is, tmix = O ( nO(1) ) .\nProof. If we use either logical or ratio semantics, the maximum factor weight will be bounded with M = O(log n). We furthermore know from Lemma 16 that for a particular hierarchical template, the hierarchy width is bounded independent of the dataset. So, hM = O(log n), and the Statement now follows directly from an application of Theorem 2\nSecondary Literature\n[29] Patrice Ossona de Mendez et al. Sparsity: graphs, structures, and algorithms. Springer Science & Business Media, 2012.\n[30] Frank den Hollander. Probability Theory: The Coupling Method. 2012."}], "references": [{"title": "Complexity of inference in graphical models", "author": ["Venkat Chandrasekaran", "Nathan Srebro", "Prahladh Harsha"], "venue": "arXiv preprint arXiv:1206.3240,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Gibbs sampling, exponential families and orthogonal polynomials", "author": ["Persi Diaconis", "Kshitij Khare", "Laurent Saloff-Coste"], "venue": "Statist. Sci.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Gibbs sampling, conjugate priors and coupling", "author": ["Persi Diaconis", "Kshitij Khare", "Laurent Saloff-Coste"], "venue": "Sankhya A,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A tractable first-order probabilistic logic", "author": ["Pedro Domingos", "William Austin Webb"], "venue": "In AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Parallel gibbs sampling: From colored fields to thin junction trees", "author": ["Joseph Gonzalez", "Yucheng Low", "Arthur Gretton", "Carlos Guestrin"], "venue": "In AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Treewidth and hypertree width. Tractability: Practical Approaches to Hard Problems, page", "author": ["Georg Gottlob", "Gianluigi Greco", "Francesco Scarcello"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Loopy belief propagation: Convergence and effects of message errors", "author": ["Alexander T Ihler", "John Iii", "Alan S Willsky"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["Daphne Koller", "Nir Friedman"], "venue": "MIT press,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "The necessity of bounded treewidth for efficient inference in bayesian networks", "author": ["Johan Kwisthout", "Hans L Bodlaender", "Linda C van der Gaag"], "venue": "In ECAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Markov chains and mixing times", "author": ["David Asher Levin", "Yuval Peres", "Elizabeth Lee Wilmer"], "venue": "American Mathematical Soc.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Projecting markov random field parameters for fast mixing", "author": ["Xianghang Liu", "Justin Domke"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "The BUGS project: evolution, critique and future directions", "author": ["David Lunn", "David Spiegelhalter", "Andrew Thomas", "Nicky Best"], "venue": "Statistics in medicine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Tractable hypergraph properties for constraint satisfaction and conjunctive queries", "author": ["D\u00e1niel Marx"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Factorie: Probabilistic programming via imperatively defined factor graphs", "author": ["Andrew McCallum", "Karl Schultz", "Sameer Singh"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Distributed inference for latent dirichlet allocation", "author": ["David Newman", "Padhraic Smyth", "Max Welling", "Arthur U Asuncion"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Probabilistic modelling, inference and learning using logical theories", "author": ["Kee Siong Ng", "John W Lloyd", "William TB Uther"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "A machine reading system for assembling synthetic Paleontological databases", "author": ["Shanan E Peters", "Ce Zhang", "Miron Livny", "Christopher R\u00e9"], "venue": "PloS ONE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "First-order probabilistic inference", "author": ["David Poole"], "venue": "In IJCAI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Graph minors. ii. algorithmic aspects of tree-width", "author": ["Neil Robertson", "Paul D. Seymour"], "venue": "Journal of algorithms,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1986}, {"title": "Incremental knowledge base construction using deepdive", "author": ["Jaeho Shin", "Sen Wu", "Feiran Wang", "Christopher De Sa", "Ce Zhang", "Christopher R\u00e9"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Lifted first-order belief propagation", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In AAAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "An architecture for parallel topic models", "author": ["Alexander Smola", "Shravan Narayanamurthy"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Probabilistic databases", "author": ["Dan Suciu", "Dan Olteanu", "Christopher R\u00e9", "Christoph Koch"], "venue": "Synthesis Lectures on Data Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Overview of the english slot filling track at the TAC2014 knowledge base population evaluation", "author": ["Mihai Surdeanu", "Heng Ji"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Training sparse natural image models with a fast gibbs sampler of an extended state space", "author": ["Lucas Theis", "Jascha Sohl-dickstein", "Matthias Bethge"], "venue": "In NIPS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "On lifting the gibbs sampling algorithm", "author": ["Deepak Venugopal", "Vibhav Gogate"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Just count the satisfied groundings: Scalable local-search and sampling based inference in mlns", "author": ["Deepak Venugopal", "Somdeb Sarkhel", "Vibhav Gogate"], "venue": "In AAAI Conference on Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Sparsity: graphs, structures, and algorithms", "author": ["Patrice Ossona de Mendez"], "venue": "Springer Science & Business Media,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Probability Theory: The Coupling Method", "author": ["Frank den Hollander"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 59, "endOffset": 62}, {"referenceID": 14, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 92, "endOffset": 104}, {"referenceID": 21, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 92, "endOffset": 104}, {"referenceID": 24, "context": "Many systems\u2014 such as Factorie [14], OpenBugs [12], PGibbs [5], DimmWitted [28], and others [15, 22, 25]\u2014use Gibbs sampling for inference because it is fast to run, simple to implement, and often produces high quality empirical results.", "startOffset": 92, "endOffset": 104}, {"referenceID": 10, "context": "Recent work has outlined conditions under which Gibbs sampling of Markov Random Fields mixes rapidly [11].", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "Continuous-valued Gibbs sampling over models with exponential-family distributions is also known to mix rapidly [2, 3].", "startOffset": 112, "endOffset": 118}, {"referenceID": 2, "context": "Continuous-valued Gibbs sampling over models with exponential-family distributions is also known to mix rapidly [2, 3].", "startOffset": 112, "endOffset": 118}, {"referenceID": 23, "context": "Each of these celebrated results still leaves a gap: there are many classes of factor graphs on which Gibbs sampling seems to work very well\u2014including as part of systems that have won quality competitions [24]\u2014for which there are no theoretical guarantees of rapid mixing.", "startOffset": 205, "endOffset": 209}, {"referenceID": 0, "context": "In some sense, bounded hypertree width is a necessary and sufficient condition for tractability of inference in graphical models [1, 9].", "startOffset": 129, "endOffset": 135}, {"referenceID": 8, "context": "In some sense, bounded hypertree width is a necessary and sufficient condition for tractability of inference in graphical models [1, 9].", "startOffset": 129, "endOffset": 135}, {"referenceID": 13, "context": "Many state-of-the-art systems use Gibbs sampling on factor graph templates and achieve better results than competitors using other algorithms [14, 27].", "startOffset": 142, "endOffset": 150}, {"referenceID": 26, "context": "Many state-of-the-art systems use Gibbs sampling on factor graph templates and achieve better results than competitors using other algorithms [14, 27].", "startOffset": 142, "endOffset": 150}, {"referenceID": 3, "context": "This is a kind of sampling analog to tractable Markov logic [4] or so-called \u201csafe plans\u201d in probabilistic databases [23].", "startOffset": 60, "endOffset": 63}, {"referenceID": 22, "context": "This is a kind of sampling analog to tractable Markov logic [4] or so-called \u201csafe plans\u201d in probabilistic databases [23].", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "The variable elimination algorithm [8] is an exact inference method that runs in polynomial time for graphs of bounded hypertree width.", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "Belief propagation is another widely-used inference algorithm that produces an exact result for trees and, although it does not converge in all cases, converges to a good approximation under known conditions [7].", "startOffset": 208, "endOffset": 211}, {"referenceID": 17, "context": "Lifted inference [18] is one way to take advantage of the structural symmetry of factor graphs that are instantiated from a template; there are lifted versions of many common algorithms, such as variable elimination [16], belief propagation [21], and Gibbs sampling [26].", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "Lifted inference [18] is one way to take advantage of the structural symmetry of factor graphs that are instantiated from a template; there are lifted versions of many common algorithms, such as variable elimination [16], belief propagation [21], and Gibbs sampling [26].", "startOffset": 216, "endOffset": 220}, {"referenceID": 20, "context": "Lifted inference [18] is one way to take advantage of the structural symmetry of factor graphs that are instantiated from a template; there are lifted versions of many common algorithms, such as variable elimination [16], belief propagation [21], and Gibbs sampling [26].", "startOffset": 241, "endOffset": 245}, {"referenceID": 25, "context": "Lifted inference [18] is one way to take advantage of the structural symmetry of factor graphs that are instantiated from a template; there are lifted versions of many common algorithms, such as variable elimination [16], belief propagation [21], and Gibbs sampling [26].", "startOffset": 266, "endOffset": 270}, {"referenceID": 26, "context": "[27] achieve orders of magnitude of speedup of Gibbs sampling on MLNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Many algorithms are known to run in polynomial time on graphs of bounded treewidth [19], despite being otherwise NP-hard.", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "Sometimes, using a stronger or weaker property than treewidth will produce a better result; for example, the submodular width used for constraint satisfaction problems [13].", "startOffset": 168, "endOffset": 172}, {"referenceID": 19, "context": "1 Voting Example We start by considering a simple example model [20], called the voting model, that models the sign of a particular \u201cquery\u201d variable Q \u2208 {\u22121, 1} in the presence of other \u201cvoter\u201d variables Ti \u2208 {0, 1} and Fi \u2208 {0, 1}, for i \u2208 {1, .", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "Definition 2 (Acyclic Factor Graph [6]).", "startOffset": 35, "endOffset": 38}, {"referenceID": 19, "context": "[20] introduce the notion of a semantic function g, which counts the of energy of instances of the factor template in a non-standard way.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "We focus on three semantic functions in particular [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "[20] exhibit several classification problems where using logical or ratio semantics gives better F1 scores.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] contain subgraphs that are grounded by hierarchical templates.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "On the same task, this quality is actually higher than professional human volunteers [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 19, "context": "[20] that can be grounded by a hierarchical template and chose a setting of the weight such that the true marginal was 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "References [1] Venkat Chandrasekaran, Nathan Srebro, and Prahladh Harsha.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Persi Diaconis, Kshitij Khare, and Laurent Saloff-Coste.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Persi Diaconis, Kshitij Khare, and Laurent Saloff-Coste.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Pedro Domingos and William Austin Webb.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Joseph Gonzalez, Yucheng Low, Arthur Gretton, and Carlos Guestrin.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Georg Gottlob, Gianluigi Greco, and Francesco Scarcello.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Alexander T Ihler, John Iii, and Alan S Willsky.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Daphne Koller and Nir Friedman.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Johan Kwisthout, Hans L Bodlaender, and Linda C van der Gaag.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] David Asher Levin, Yuval Peres, and Elizabeth Lee Wilmer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Xianghang Liu and Justin Domke.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] David Lunn, David Spiegelhalter, Andrew Thomas, and Nicky Best.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] D\u00e1niel Marx.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Andrew McCallum, Karl Schultz, and Sameer Singh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] David Newman, Padhraic Smyth, Max Welling, and Arthur U Asuncion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Kee Siong Ng, John W Lloyd, and William TB Uther.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Shanan E Peters, Ce Zhang, Miron Livny, and Christopher R\u00e9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] David Poole.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Neil Robertson and Paul D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Jaeho Shin, Sen Wu, Feiran Wang, Christopher De Sa, Ce Zhang, Feiran Wang, and Christopher R\u00e9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Parag Singla and Pedro Domingos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Alexander Smola and Shravan Narayanamurthy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Dan Suciu, Dan Olteanu, Christopher R\u00e9, and Christoph Koch.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Mihai Surdeanu and Heng Ji.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Lucas Theis, Jascha Sohl-dickstein, and Matthias Bethge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Deepak Venugopal and Vibhav Gogate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Deepak Venugopal, Somdeb Sarkhel, and Vibhav Gogate.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "Next, we define a coupling [30].", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "[10]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Definition 12 (Hypertree Decomposition [6]).", "startOffset": 39, "endOffset": 42}, {"referenceID": 27, "context": "Secondary Literature [29] Patrice Ossona de Mendez et al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 28, "context": "[30] Frank den Hollander.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Gibbs sampling on factor graphs is a widely used inference technique, which often produces good empirical results. Theoretical guarantees for its performance are weak: even for tree structured graphs, the mixing time of Gibbs may be exponential in the number of variables. To help understand the behavior of Gibbs sampling, we introduce a new (hyper)graph property, called hierarchy width. We show that under suitable conditions on the weights, bounded hierarchy width ensures polynomial mixing time. Our study of hierarchy width is in part motivated by a class of factor graph templates, hierarchical templates, which have bounded hierarchy width\u2014regardless of the data used to instantiate them. We demonstrate a rich application from natural language processing in which Gibbs sampling provably mixes rapidly and achieves accuracy that exceeds human volunteers.", "creator": "gnuplot 4.6 patchlevel 5"}}}