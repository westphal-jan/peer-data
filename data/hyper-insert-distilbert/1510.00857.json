{"id": "1510.00857", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2015", "title": "Approximate Fisher Kernels of non-iid Image Models for Image Categorization", "abstract": "the fuzzy bag - lengths of - words ( bow ) model strongly treats large images used as sets solely of fixed local descriptors together and directly represents them by visual word reference histograms. the fisher vector ( then fv ) proportional representation extends bow, by considering evaluating the downstream first equation and relevant second order optimal statistics of relative local descriptors. in both tree representations whose local descriptors values are assumed observed to reasonably be identically and independently distributed ( iid ), whereas which is exactly a poor accuracy assumption as from possessing a modeling perspective. although it has sufficiently been later experimentally critically observed that the performance of bow pictures and fv representations can be improved by employing discounting transformations such as weak power normalization. currently in this lab paper, we introduce contrasting non - iid models by dramatically treating extending the model for parameters given as latent variables which are heavily integrated in out, rendering all local regions dependent. roughly using the fisher kernel inverse principle we encode merely an image by the density gradient of yielding the resulting data whose log - term likelihood w. function r. to t. the model assumes hyper - parameters. our test models naturally generate discounting beta effects differently in handling the representations ; suggesting estimates that most such transformations have proven broadly successful because they closely close correspond to the representations obtained for respective non - iid models. unwilling to accurately enable smooth tractable computation, we rely on small variational free - energy posterior bounds to learn roughly the respective hyper - parameters and to compute themselves approximate elementary fisher filter kernels. our experimental evaluation results correctly validate prove that our models commonly lead to performance utility improvements comparable to it using power normalization, described as employed in several state - of - the - art fuzzy feature aggregation methods.", "histories": [["v1", "Sat, 3 Oct 2015 19:35:38 GMT  (1089kb,D)", "http://arxiv.org/abs/1510.00857v1", "IEEE Transactions on Pattern Analysis and Machine Intelligence, in press, 2015"]], "COMMENTS": "IEEE Transactions on Pattern Analysis and Machine Intelligence, in press, 2015", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ramazan gokberk cinbis", "jakob verbeek", "cordelia schmid"], "accepted": false, "id": "1510.00857"}, "pdf": {"name": "1510.00857.pdf", "metadata": {"source": "CRF", "title": "Approximate Fisher Kernels of non-iid Image Models for Image Categorization", "authors": ["Ramazan Gokberk Cinbis", "Jakob Verbeek"], "emails": ["firstname.lastname@inria.fr"], "sections": [{"heading": null, "text": "Index Terms\u2014Statistical image representations, object recognition, image classification, Fisher kernels.\nF"}, {"heading": "1 INTRODUCTION", "text": "P ATCH-based image representations, such bag of visual words(BoW) [10], [49], are widely utilized in image categorization and retrieval systems. BoW descriptor represents an image as a histogram over visual word counts. The histograms are constructed by mapping local feature vectors in images to cluster indices, where the clustering is typically learned using k-means. Perronnin and Dance [38] have enhanced this basic representation using the notion of Fisher kernels [20]. In this case local descriptors are soft-assigned to components of a mixture of Gaussian (MoG) density, and the image is represented using the gradient of the log-likelihood of the local descriptors w.r.t. the MoG parameters. As we show below, both BoW as well as MoG Fisher vector representations are based on models that assume that local descriptors are independently and identically distributed (iid). However, the iid assumption is a very poor one from a modeling perspective, see the illustration in Figure 1.\nIn this work, we consider models that capture the dependencies among local image regions by means of non-iid but completely exchangeable models, i.e . like iid models our models still treat the image as an unordered set of regions. We treat the parameters of the BoW models as latent variables with prior distributions learned from data. By integrating out the latent variables, all image regions become mutually dependent. We generate image representations\n\u2022 R. G. Cinbis is with Milsoft, Ankara, Turkey. Most of the work in this paper was done when he was with the LEAR team, Inria Grenoble, France. E-mail: firstname.lastname@inria.fr\n\u2022 J. Verbeek and C. Schmid are with LEAR team, Inria Grenoble Rho\u0302neAlpes, Laboratoire Jean Kuntzmann, CNRS, Univ. Grenoble Alpes, France. E-mail: firstname.lastname@inria.fr\nCopyright (c) 2015 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org.\nfrom these models by applying the Fisher kernel principle, in this case by taking the gradient of the log-likelihood of the data in an image w.r.t. the hyper-parameters that control the priors on the latent model parameters.\nHowever, in some cases, the gradient of the log-likelihood of the data can be intractable to compute. To compute a gradientbased representation in such cases, we replace the intractable loglikelihood with a tractable variational bound. We then compute gradients with respect to this bound instead of the likelihood. Following [4], which is the first and one of the very few studies utilizing this approximation method, we refer to the resulting kernel as the variational Fisher kernel. We show that the variational Fisher kernel is equivalent to the actual Fisher kernel when the variational bound is tight. Therefore, the variational Fisher kernel provides not only a technique for approximating intractable Fisher kernels, but also an alternative formulation for computing exact Fisher kernels. We demonstrate through examples that the\nar X\niv :1\n51 0.\n00 85\n7v 1\n[ cs\n.C V\n] 3\nO ct\n2 01\n5\nvariational formulation can be mathematically more convenient for deriving Fisher vectors representations.\nIn this work, we analyze three non-iid image models. Our first model is the multivariate Po\u0301lya model which represents the set of visual word indices of an image as independent draws from an unobserved multinomial distribution, itself drawn from a Dirichlet prior distribution. By integrating out the latent multinomial distribution, a model is obtained in which all visual word indices are mutually dependent. Interestingly, we find that our non-iid models yield gradients that are qualitatively similar to popular transformations of BoW image representations, such as square-rooting histogram entries or more generally applying power normalization [22], [39], [40], [52]. Therefore, our first contribution is to show that such transformations appear naturally if we remove the unrealistic iid assumption, i.e ., to provide an explanation why such transformations are beneficial.\nOur second contribution is the analysis of Fisher vector representations over the latent Dirichlet allocation (LDA) model [3] for image classification purposes. The LDA model can capture the co-occurrence statistics missing in BoW representations. In this case the computation of the gradients is intractable, therefore, we compute approximate variational Fisher vectors [4]. We compare performance to Fisher vectors of PLSA [19], a topic model that does not treat the model parameters as latent variables. We find that topic models improve over BoW models, and that the LDA improves over PLSA even when square-rooting is applied.\nOur third contribution is our most advanced model, which assumes that the local descriptors are iid samples from a latent MoG distribution, and we integrate out the mixing weights, means and variances of the MoG distribution. Since the computation of the gradients is intractable, we also use the variational Fisher kernel framework for this model. This leads to a representation that performs on par with the improved Fisher vector (FV) representation of [40] based on iid MoG models, which includes power normalization.\nIn our experimental analysis, we present a detailed experimental evaluation of the proposed the non-iid image models over local SIFT descriptors. In addition, we demonstrate that the latent MoG image model can effectively be combined with Convolutional Neural Network (CNN) based features. We consider two approaches for this purpose. First, following recent work [16], [31], we compute Fisher vectors over densely sampled image patches that are encoded using CNN features. Second, we propose to extract Fisher vectors over image regions sampled by a selective search method [50]. The experimental results on the PASCAL VOC 2007 [14] and MIT Indoor Scenes [41] datasets confirm the effectiveness of the proposed latent MoG image model, and the corresponding non-iid image descriptors.\nThis paper extends our earlier paper [8]. We present more complete and detailed discussions of related work and the variational Fisher kernel framework. We give a proof that Fisher kernels given by the traditional form and the variational framework are equivalent when the variational bound is tight. We extend the experimental evaluation of the proposed non-iid MoG models by evaluating them over CNN-based local descriptors. We also show that the classification results can be further improved by computing the CNN features over selective search windows, compared to using densely sampled image regions. We perform additional experimental evaluation on the MIT Indoor dataset [41]. Finally, we present a new empirical study on the relationship between the model likelihood and image categorization performance."}, {"heading": "2 RELATED WORK", "text": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56]. These transformations alleviate an obvious shortcoming of linear classifiers on BoW image representations: the fact that a fixed change \u2206 in a BoW histogram, from h to h + \u2206, leads to a score increment that is independent of the original histogram h: f(h + \u2206) \u2212 f(h) = w>(h + \u2206) \u2212 w>h = w>\u2206. This means that the effect on the score for a change \u2206 is not dependent on the context h in which it appears. Therefore, the score increment from images (a) though (d) in Figure 2 will be comparable, which is undesirable: the classifier score for cow should sharply increase from (a) to (b), and then remain stable among (b), (c), and (d).\nPopular remedies to this problem include the use of chi-square kernels [56], or taking the square-root of histogram entries [39], also referred to as the Hellinger kernel [52]. Power normalization [39], defined as f(x) = sign(x)|x|\u03c1, is a similar transformation that can be applied to non-histogram feature vectors, and it is equivalent to signed square-rooting for the coefficient \u03c1 = 1/2. The effect of all of these is similar: they transform the features such that the first few occurrences of visual words will have a more pronounced effect on the classifier score than if the count is increased by the same amount but starting at a larger value. This is desirable, since now the first patches providing evidence for an object category can significantly impact the score, and hence making it for example easier to detect small object instances. The qualitative similarity is illustrated in Figure 3, where we compare the `2, chi-square, and Hellinger distances on the range [0, 1].\nThe motivation for these transformations tends to vary in the literature. Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55]. Recently, Kobayashi [26] showed that a similar discounting transformation based on taking logarithm of histogram entries, can be derived via modeling `1-normalized descriptors by Dirichlet distribution. Rana et al . [43] propose to discriminatively learn power normalization coefficients for image retrieval using a triplet-\nbased objective function, which aims to obtain smaller distances across matching image pairs than non-matching ones. In contrast to these studies, we show that such discounting transformations appear naturally in generative image models that avoid making the unrealistic iid assumption that underlies the standard BoW and MoG-FV image representations.\nSimilar transformations are also used in image retrieval to counter burstiness effects [21], i.e ., if rare visual words occur in an image, they tend to do so in bursts due to the locally repetitive nature of natural images. Burstiness also occurs in text, and the Dirichlet compound multinomial distribution, also known as multivariate Po\u0301lya distribution, has been used to model this effect [33]. This model places a Dirichlet prior on a latent perdocument multinomial, and words in a document are sampled independently from it. Elkan [13] shows the relationship between the Fisher kernel of the multivariate Po\u0301lya distribution and the tf-idf document representation. In Section 4, we investigate the Fisher kernel based on multivariate Po\u0301lya distribution as our most basic non-iid image representation.\nOur use of latent Dirichlet allocation (LDA) [3] differs from earlier work on using topic models such as LDA or PLSA [19] for object recognition [29], [42]. The latter use topic models to compress BoW image representations by using the inferred document-specific topic distribution. Similarly, Chandalia and Beal [4] propose to compress BoW document representation by computing LDA Fisher vector with respect to the parameters of the Dirichlet prior on the topic distributions. We, instead, use the Fisher kernel framework to expand the image representation by decomposing the original BoW histogram into several bags-ofwords, one per topic, so that individual histogram entries not only encode how often a word appears, but also in combination with which other words it appears. Whereas compressed topic model representations were mostly found to at best maintain BoW performance, we find significant gains by using topic models. Finally, in contrast to the PLSA Fisher kernel, which was previously studied as a document similarity measure [5], [18], we show that the LDA Fisher kernel naturally involves discounting transformations.\nSeveral other generative models have been proposed to capture spatial regularities across image regions. For example, the Spatial LDA model [53] extends the LDA model such that spatially neighboring visual words are more likely assigned to the same topic. The counting grid model [36], which is a grid of multinomial distributions, can be considered as an alternative to the spatial topic models. In this approach, the visual words of an image are treated as samples from a latent local neighborhood of the counting grid. Therefore, each local neighborhood of the model can be interpreted as a spatial grid of topics. While these\nstudies show that incorporation of spatial information can improve unsupervised semantic segmentation results [53], or lead to better generative classifiers compared to LDA [36], we limit our focus to Fisher kernels of orderless, i.e . exchangeable, generative models in our study.\nThe computation of the LDA Fisher vector image representation is technically more involved compared to the PLSA model. In the case of the LDA model, the latent model parameters cannot be integrated out analytically, and the computation of the gradients is no longer tractable. Similarly, the Fisher kernel for our Latent MoG image model is intractable since the latent variables (mixing weights, means, and variances) cannot be integrated out analytically. We overcome this difficulty by relying on the variational free-energy bound [24], which is obtained by subtracting the Kullback-Leibler divergence between an approximate posterior on the latent variables and the true posterior. By imposing a certain independence structure on the approximate posterior, tractable approximate inference techniques can be devised. We then compute the gradient of the variational bound as a surrogate for the intractable gradients of the exact log-likelihood. The method of approximating Fisher kernels with the gradient vector of a variational bound was first proposed by Chandalia and Beal [4] in order to obtain the LDA Fisher kernel. The only other work incorporating this technique, to the best of our knowledge, is the recent work of Perina et al . [37], which proposes a variational Fisher kernel for micro-array data. We show that variational Fisher kernel is equivalent to the exact Fisher vector when the variational bound is tight, and demonstrate that in some cases it can be a mathematically more convenient formulation, compared to the original Fisher kernel definition. Finally, we note that the variational approximation method for Fisher kernels differs from Perina et al . [35], which uses the variational free-energy to define an alternative encoding, replacing the Fisher kernel.\nIn the following section we review the Fisher kernel framework, and the variational approximation method. In Section 4 we present our non-iid latent variable models and propose novel Fisher vector representations based on them. We present experimental results in Section 5, and summarize our conclusions in Section 6."}, {"heading": "3 FISHER VECTORS AND VARIATIONAL APPROXIMATION", "text": "In this section we present an overview of the Fisher kernel framework, variational inference, and the variational Fisher kernel."}, {"heading": "3.1 Fisher vectors", "text": "Images can be considered as samples from a generative process, and therefore class-conditional generative models can be used for image categorization. However, it is widely observed that discriminative classifiers typically outperform classification based on generative models, see e.g . [17]. A simple explanation is that discriminative classifiers aim to maximize the end goal, which is to categorize entities based on their content. In contrast, generative classifiers instead require modeling class-conditional data distributions, which is arguably a more difficult task than only learning decision surfaces, and therefore result in inferior categorization performance.\nThe Fisher kernel framework proposed by Jaakkola and Haussler [20] allows combining the power of generative models and\ndiscriminative classifiers. In particular, Fisher kernel provides a framework for deriving a kernel from a probabilistic model. Suppose that p(x) is a generative model with parameters \u03b8.1 Then, the Fisher kernel K(x,x\u2032) is defined as\nK(x,x\u2032) = g(x)TI\u22121g(x\u2032) , (1)\nwhere the gradient g(x) = \u2207\u03b8 log p(x) is called the Fisher score, and I is the Fisher information matrix:\nI = IEx\u223cp(x) [ g(x)g(x)T ] . (2)\nwhich is equivalent to the covariance of the Fisher score as computed using p(x), since IEx\u223cp(x)[g(x)] = 0. The inner product space (i.e . explicit feature mapping) induced by a Fisher kernel is given by\n\u03c6(x) = I\u2212 1 2 g(x), (3)\nwhere I\u2212 1 2 is the whitening transform using the Fisher information matrix. Sa\u0301nchez et al . [45] refer to the normalized gradient given by \u03c6(x) as the Fisher vector. In practice the term \u201cFisher vector\u201d is sometimes also used to refer to the non-normalized gradients (i.e . Fisher score) as well.\nThe essential idea in Fisher kernel is to use gradients g(x) of the data log-likelihood to extract features w.r.t. a generative model. The Fisher information matrix, on the other hand, is of lesser importance. A theoretical motivation for using I is that I\u22121g(x) gives the steepest descent direction along the manifold of the parameter space, which is also known as the natural gradient. Another motivation is that I makes the Fisher kernel invariant to the re-parameterization \u03b8 \u2192 \u03c8(\u03b8) for any differentiable and invertible function \u03c8 [2], which can be easily shown using the chain rule and the Jakobian matrix of the inverse function \u03c8\u22121.\nHowever, the computation of the Fisher information matrix I is intractable for many models. Although in principle it can be approximated empirically as I\u2248 1|X| \u2211 x\u2208X g(x)g(x)\nT, the approximation itself can be costly if g(x) is high dimensional. In such cases, empirical approximation can be used only for the diagonal terms. Alternatively, I can be dropped altogether [20] or analytical approximations can be derived, see e.g . [38], [45], [46]."}, {"heading": "3.2 Variational approximate inference", "text": "Variational methods are a family of mathematical tools that can be used to approximate intractable computations, particularly those involving difficult integrals. Originally developed in statistical physics based on the calculus of variations and the mean field theory, the variational approximation framework that we utilize in this paper is known as the variational inference, and it is now among the most successful approximate probabilistic inference techniques [2], [24], [32].\nIn the context of probabilistic models, the central idea in variational methods is to devise a bound on the log-likelihood function in terms of an approximate posterior distribution over the latent variables. Let X denote the set of observed variables, and \u039b denote the set of latent variables and latent parameters. Suppose that q(\u039b) is an approximate distribution over the latent variables. Then, the distribution p(X) can be decomposed as follows for any choice of the approximate posterior q:\nln p(X) = F (p, q) +D ( q||p ) . (4)\n1. We drop the model parameters \u03b8 from function arguments for brevity.\nIn this equation, F is the variational free-energy given by\nF (p, q) = \u222b q(\u039b) ln ( p(X,\u039b)\nq(\u039b)\n) d\u039b (5)\n= IEq(\u039b)[ln p(X,\u039b)] +H(q), (6)\nwhere H(q) is the entropy of the distribution q. The term D (q||p) in Eq. (4) is the Kullback-Leibler (KL) divergence between the distributions q(\u039b) and p(\u039b|X):\nD ( q||p ) = \u2212 \u222b q(\u039b) ln ( p(\u039b|X) q(\u039b) ) d\u039b. (7)\nSince the KL-divergence term D (q||p) is strictly nonnegative, the variational free energy F (p, q) is a lower-bound on the true log-likelihood ln p(X), i.e . F (p, q) \u2264 ln p(X). When the KL-divergence term is zero, i.e . the distribution q is equivalent to the true posterior distribution, the bound F is tight.\nIn order to effectively utilize the decomposition in Eq. (6) for a given distribution p, we need to choose the distribution q such that it leads to a tractable and as tight as possible lowerbound F (p, q). For this purpose, we constrain q to a family of distributions Q that leads to tractable computations, typically by imposing independence assumptions. For example suppose that \u039b = (\u03bb1, . . . , \u03bbn), we may chooseQ to be the set of distributions that factorize over the \u03bbi, i.e . with q(\u039b) = \u220fn i=1 qi(\u03bbi). Given the family Q, we maximize F (p, q) by minimizing the KL divergence in Eq. (4) over all q \u2208 Q."}, {"heading": "3.3 Variational Fisher kernel", "text": "In this paper, we utilize the variational free-energy bounds for two purposes. The first is to estimate the hyper-parameters of the LDA (Section 4.2) and the Latent MoG (Section 4.3) models from training data using an approximate maximum likelihood procedure. For this purpose, we iteratively update the variational lowerbound with respect to the variational distribution parameters, and the model hyper-parameters; an approach that is known as the variational expectation-maximization procedure [24].\nOur second main use of the variational free-energy is to compute approximate Fisher vectors where the original Fisher vector is intractable to compute. In particular, we approximate the Fisher vector by the gradient of the variational lower-bound given by Eq. (6), i.e . g(x) \u2248 \u2207\u03b8F (p, q), which we refer to as variational Fisher vector. Since, the entropy H(q) is constant w.r.t. model parameters, the variational Fisher vector \u03b8q can equivalently be written as\n\u03c6q(X) = I \u2212 12\u2207\u03b8IEq[ln p(X,\u039b)] . (8)\nwhere I is the (approximate) Fisher information matrix. We have already discussed that the variational bound in Eq. (6) is tight when the distribution q matches the posterior on the hyperparameters. We will now show that its gradient equals that of the data log-likelihood if the bound is tight. In order to prove this, we first write the partial derivative of the lower-bound with respect to some model (hyper-)parameter \u03b8:\n\u2202F \u2202\u03b8 = \u2202IEq[ln p(X,\u039b)] \u2202\u03b8 . (9)\nBy definition, we can interchange the differential operator and the expectation:\n\u2202F \u2202\u03b8 = IEq\n[ \u2202 ln p(X,\u039b)\n\u2202\u03b8\n] . (10)\nWithout loss of generality, we assume that all latent variables are continuous, in which case the expectation is equivalent to\n\u2202F \u2202\u03b8 =\n\u222b q(\u039b) \u2202 ln p(X,\u039b)\n\u2202\u03b8 d\u039b. (11)\nBy following differentiation rules, we obtain the equation:\n\u2202F \u2202\u03b8 =\n\u222b q(\u039b)\n1 p(\u039b|X)p(X) \u2202p(X,\u039b) \u2202\u03b8 d\u039b. (12)\nSince the bound is assumed to be tight, the q(\u039b) and p(\u039b|X) are identical. In addition, we observe that p(X) is a constant with respect to the integration variables. Therefore, we can simplify the equation as follows:\n\u2202F \u2202\u03b8 = 1 p(X)\n\u222b \u2202p(X,\u039b)\n\u2202\u03b8 d\u039b, (13)\nwhich can be re-written as follows: \u2202F\n\u2202\u03b8 =\n1\np(X)\n\u2202 \u222b p(X,\u039b)d\u039b\n\u2202\u03b8 . (14)\nFinally, we integrate out \u039b and simplify the equation into the following form:\n\u2202F \u2202\u03b8 = \u2202 ln p(X) \u2202\u03b8 , (15)\nwhich completes the proof. In addition to presenting a relationship between the original Fisher vector and the variational Fisher vector definitions, the proof shows that the latter formulation can be used as an alternative framework. In fact, we observe that the variational formulation can in some cases be mathematically more convenient to derive Fisher vector representations. Even though our main interest in this paper is to compute approximate representations based on the LDA and latent MoG image models presented in the next section, we present two additional examples in Appendix A that demonstrate the usefulness of the variational formulation."}, {"heading": "4 NON-IID IMAGE REPRESENTATIONS", "text": "In this section we present our non-iid models for local image descriptors. We start with a model for BoW quantization indices, and extend the model to capture co-occurrence statistics across visual words using LDA in Section 4.2. Finally, we consider a non-iid extension of mixture of Gaussian models over sets of local descriptors in Section 4.3."}, {"heading": "4.1 Bag-of-words and the multivariate Po\u0301lya model", "text": "The standard BoW image representation can be interpreted as applying the Fisher kernel framework to a simple iid multinomial model over visual word indices, as shown in [27]. Let w1:N = {w1, . . . , wN} denote the visual word indices corresponding toN patches sampled in an image, and let \u03c0 be a learned multinomial over K visual words, parameterized in log-space, i.e . p(wi = k) = \u03c0k with \u03c0k = exp(\u03b3k)/ \u2211 k\u2032 exp(\u03b3k\u2032). The data likelihood for the BoW model is given by\np(w1:N ) = N\u220f i=1 p(wi = k). (16)\nThe gradient of the data log-likelihood is in this case given by \u2202 \u2211N i=1 ln p(wi)\n\u2202\u03b3k = nk \u2212N\u03c0k, (17)\nwhere nk denotes the number of occurrences of visual word k among the set of indices w1:N . This is a shifted version of the standard BoW histogram, where the mean of all image representations is centered at the origin. We stress that this multinomial interpretation of the BoW model assumes that the visual word indices across all images are iid, which directly generates the product form in the likelihood of Eq. (16), and the count statistic in the gradient of the log-likelihood in Eq. (17).\nOur first non-iid model assumes that for each image there is a different, a-priori unknown, multinomial generating the visual word indices in that image. In this model visual word indices within an image are mutually dependent, since knowing some of the wi provides information on the underlying multinomial \u03c0, and thus also provides information on which subsequent indices could be sampled from it. The model is parameterized by a nonsymmetric Dirichlet prior over the latent image-specific multinomial, p(\u03c0) = D(\u03c0|\u03b1) with \u03b1 = (\u03b11, . . . , \u03b1K), and the wi are modeled as iid samples from \u03c0. The marginal distribution on the wi is obtained by integrating out \u03c0:\np(w1:N ) =\n\u222b p(\u03c0) N\u220f i=1 p(wi|\u03c0)d\u03c0. (18)\nThis model is known as the multivariate Po\u0301lya, or Dirichlet compound multinomial [33], and the integral simplifies to\np(w1:N ) = \u0393(\u03b1\u0302)\n\u0393(N + \u03b1\u0302) K\u220f k=1 \u0393(nk + \u03b1k) \u0393(\u03b1k) , (19)\nwhere \u0393(\u00b7) is the Gamma function, and \u03b1\u0302 = \u2211K k=1 \u03b1k. See Figure 4a and Figure 4b for a graphical representation of the BoW multinomial model, and the Po\u0301lya model.\nFollowing the Fisher kernel framework, we represent an image by the gradient w.r.t. the hyper-parameter \u03b1 of the log-likelihood of the visual word indices w1:N . The partial derivative w.r.t. \u03b1k is given by\n\u2202 ln p(w1:N )\n\u2202\u03b1k =\u03c8(\u03b1k+nk)\u2212\u03c8(\u03b1\u0302+N)\u2212\u03c8(\u03b1k)+\u03c8(\u03b1\u0302), (20)\nwhere \u03c8(x) = \u2202 ln \u0393(x)/\u2202x is the digamma function. Only the first two terms in Eq. (20) depend on the counts nk, and for fixed N the gradient is determined up to additive constants\nby \u03c8(\u03b1k + nk), i.e . it is given by a transformation of the visual word counts nk. Figure 5 shows the transformation \u03c8(\u03b1+ n) for various values of \u03b1, along with the square-root function used in the Hellinger distance for reference. We see that the same monotoneconcave discounting effect is obtained as by taking the squareroot of histogram entries. This transformation arises naturally in our latent variable model, and suggests that such transformations are successful because they correspond to a more realistic non-iid model, c.f . Figure 1.\nObserve that in the limit of \u03b1 \u2192 \u221e the transfer function becomes linear, since for large \u03b1 the Dirichlet prior tends to a delta peak on the multinomial simplex and thus removes the uncertainty on the underlying multinomial, with an observed multinomial BoW model as its limit. In the limit of \u03b1 \u2192 0, corresponding to priors that concentrate their mass at sparse multinomials, the transfer function becomes a step function. This is intuitive, since in the limit of ultimately sparse distributions only one word will be observed, and its count no longer matters, we only need to know which word is observed to determine which \u03b1k should be increased to improve the log-likelihood."}, {"heading": "4.2 Capturing co-occurrence with topic models", "text": "The Po\u0301lya model is non-iid but it does not model co-occurrence across visual words, this can be seen from the posterior distribution p(w = k|w1:N ) = \u222b p(w = k|\u03c0)p(\u03c0|w1:N )d\u03c0 \u221d nk +\u03b1k. The model just predicts to see more visual words of the type it has already seen before. In our second model, we extend the Po\u0301lya model to capture co-occurrence statistics of visual words using latent Dirichlet allocation (LDA) [3]. We model the visual words in an image as a mixture of T topics, encoded by a multinomial \u03b8 mixing the topics, where each topic itself is represented by a multinomial distribution \u03c0t over the K visual words. We associate a variable zi, drawn from \u03b8, with each patch that indicates which topic was used to draw its visual word index wi. We place Dirichlet priors on the topic mixing, p(\u03b8) = D(\u03b8|\u03b1), and the topic distributions p(\u03c0t) = D(\u03c0t|\u03b7t), and integrate these out to obtain the marginal distribution over visual word indices as:\np(w1:N ) =\n\u222b\u222b p(\u03b8)p(\u03c0) N\u220f i=1 p(wi|\u03b8, \u03c0)d\u03b8d\u03c0, (21)\np(wi = k|\u03b8, \u03c0) = T\u2211 t=1 p(zi = t|\u03b8)p(wi = k|\u03c0t). (22)\nSee Figure 6 for a graphical representation of the model. Note that this model is equivalent to the Po\u0301lya model discussed above when there is only a single topic, i.e . for T = 1.\nBoth the log-likelihood and its gradient are intractable to compute for the LDA model. As discussed in Section 3.3, however, we can resort to variational methods to compute a free-energy bound F using an approximate posterior. Here we use a completely factorized approximate posterior as in [3] of the form\nq(\u03b8, \u03c01:T , z1:N ) = q(\u03b8) T\u220f t=1 q(\u03c0t) N\u220f i=1 q(zi). (23)\nThe update equations of the variational distributions q(\u03b8) = D(\u03b8|\u03b1\u2217) and q(\u03c0t) = D(\u03c0t|\u03b7\u2217t ) to maximize the free-energy bound F are given by:\n\u03b1\u2217t = \u03b1t + N\u2211 i=1 qit, \u03b7 \u2217 tk = \u03b7tk + \u2211 i:wi=k qit, (24)\nwhere qit = q(zi = t), which is itself updated according to qit \u221d exp[\u03c8(\u03b1\u2217t ) \u2212 \u03c8(\u03b1\u0302\u2217) + \u03c8(\u03b7\u2217tk) \u2212 \u03c8(\u03b7\u0302\u2217t )]. These update equations can be applied iteratively to monotonically improve the variational bound.\nThe gradients of F w.r.t. the hyper-parameters are obtained from these as\n\u2202F \u2202\u03b1t = \u03c8(\u03b1\u2217t )\u2212 \u03c8(\u03b1\u0302\u2217)\u2212 [\u03c8(\u03b1t)\u2212 \u03c8(\u03b1\u0302)], (25) \u2202F\n\u2202\u03b7tk = \u03c8(\u03b7\u2217tk)\u2212 \u03c8(\u03b7\u0302\u2217t )\u2212 [\u03c8(\u03b7tk)\u2212 \u03c8(\u03b7\u0302t)]. (26)\nThe gradient w.r.t. \u03b1 encodes a discounted version of the topic proportions as they are inferred in the image. The gradients w.r.t. the hyper-parameters \u03b7t can be interpreted as decomposing the bag-of-word histogram over the T topics, and encoding the soft counts of words assigned to each topic. The entries \u2202F\u2202\u03b7tk in this representation not only code how often a word was observed but also in combination with which other words, since the cooccurrence of words throughout the image will determine the inferred topic mixing and thus the word-to-topic posteriors qit.\nIn our experiments we compare LDA with the PLSA model [19]. This model treats the topics \u03c0t, and the topic mixing \u03b8 as non-latent parameters which are estimated by maximum likelihood. To represent images using PLSA we apply the Fisher kernel framework and compute gradients of the log-likelihood w.r.t. \u03b8 and the \u03c0t. The PLSA model with a single topic reduces to the iid multinomial model discussed in the previous section.\n\u03c0\nwi\nxi\n\u03bbk\n\u00b5k\ni=1,2,...,N k=1,2,...,K\n(a) MoG model\nbk\nak\nmk\n\u03b2k\n\u03b1\u03c0\nwi\nxi\n\u03bbk\n\u00b5k\ni=1,2,...,N k=1,2,...,K\n(b) Latent MoG model\nFig. 7. Graphical representation of the models in Section 4.3: (a) MoG model, (b) latent MoG model. The outer plate in (b) without indexing refer to images. The index i runs over the local descriptors, and index k over Gaussians in the mixture which represent the visual words."}, {"heading": "4.3 Modeling descriptors using latent MoG models", "text": "In this section we turn to the image representation of Perronnin and Dance [38] that applies the Fisher kernel framework to mixture of Gaussian (MoG) models over local descriptors. An improved version of this representation using power normalization was presented in [40].\nA MoG density p(x) = \u2211K k=1 \u03c0kN (x;\u00b5k, \u03c3k) is defined by mixing weights \u03c0 = {\u03c0k}, means \u00b5 = {\u00b5k} and variances \u03c3 = {\u03c3k}.2 The K Gaussian components of the mixture correspond to the K visual words in a BoW model. In [38], [40], local descriptors across images are assumed to be iid samples from a single MoG model underlying all images. They represent an image by the gradient of the log-likelihood of the extracted local descriptors x1:N w.r.t. the model parameters. Using the softassignments p(k|x) = \u03c0kN (x;\u00b5k, \u03c3k)/p(x) of local descriptors to mixture components the partial derivatives are computed as:\n\u2202 ln p(x1:N )\n\u2202\u03b3k = N\u2211 i=1 p(k|xi)\u2212 \u03c0k, (27)\n\u2202 ln p(x1:N )\n\u2202\u00b5k = N\u2211 i=1 p(k|xi)(x\u2212 \u00b5k)/\u03c3k, (28)\n\u2202 ln p(x1:N )\n\u2202\u03bbk = N\u2211 i=1 p(k|xi) ( \u03c3k \u2212 (xi \u2212 \u00b5k)2 ) /2, (29)\nwhere we re-parameterize the mixing weights as \u03c0k = exp(\u03b3k)/ \u2211K k\u2032=1 exp(\u03b3k\u2032), and the Gaussians with precisions \u03bbk = \u03c3 \u22121 k , as in [27]. For local descriptors of dimension D, the gradient yields an image representation of size K(1 + 2D), since for each of the K visual words there is one derivative w.r.t. its mixing weight, and 2D derivatives for the means and variances in the D dimensions. This representation thus stores more information about the local descriptors assigned to a visual word than just their count, as a result higher recognition performance can be obtained using the same number of visual words as compared to the BoW representation.\nIn analogy to the Po\u0301lya model, we remove the iid assumption by defining a MoG model per image and treating its parameters as latent variables. We place conjugate priors on the imagespecific parameters: a Dirichlet prior on the mixing weights,\n2. We present here the uni-variate case for clarity, extension to the multivariate case with diagonal covariance matrices is straightforward.\np(\u03c0) = D(\u03c0|\u03b1), and a combined Normal-Gamma prior on the means \u00b5k and precisions \u03bbk = \u03c3 \u22121 k :\np(\u03bbk) = G(\u03bbk|ak, bk), (30) p(\u00b5k|\u03bbk) = N (\u00b5k|mk, (\u03b2k\u03bbk)\u22121). (31)\nThe distribution on the descriptors x1:N in an image is obtained by integrating out the latent MoG parameters:\np(x1:N ) =\n\u222b\u222b\u222b p(\u03c0)p(\u00b5, \u03bb) N\u220f i=1 p(xi|\u03c0, \u00b5, \u03bb)d\u03c0d\u00b5d\u03bb, (32)\np(xi|\u03c0, \u00b5, \u03bb) = K\u2211 k=1 p(wi= k|\u03c0)p(xi|wi= k, \u03bb, \u00b5), (33)\nwhere p(wi = k|\u03c0) = \u03c0k, and p(xi|wi = k, \u03bb, \u00b5) = N (xi|\u00b5k, \u03bb\u22121k ) is the Gaussian corresponding to the k-th visual word. See Figure 7a and Figure 7b for graphical representations of the MoG model and the latent MoG model.\nComputing the log-likelihood in this model is also intractable, as is computing the gradient of the log-likelihood which we need for both hyper-parameter learning and to extract the Fisher vector representation. To overcome these problems we replace the intractable log-likelihood with its variational lower bound.\nBy constraining the variational posterior q in the bound F given by Eq. (6) to factorize as q(\u03c0, \u00b5, \u03bb, w1:N ) = q(\u03c0, \u00b5, \u03bb)q(w1:N ) over the latent MoG parameters and the assignments of local descriptors to visual words, we obtain a bound for which we can tractably compute its value and gradient w.r.t. the hyper-parameters. Given this factorization it is easy to show that the optimal q will further factorize as\nq(\u03c0, \u00b5, \u03bb, w1:N ) = q(\u03c0) K\u220f k=1 q(\u00b5k|\u03bbk)q(\u03bbk) N\u220f i=1 q(wi), (34)\nand that the variational posteriors on the model parameters will have the form of Dirichlet and Normal-Gamma distributions q(\u03c0) = D(\u03c0|\u03b1\u2217), q(\u03bbk) = G(\u03bbk|a\u2217k, b\u2217k), q(\u00b5k|\u03bbk) = N (\u00b5k|m\u2217k, (\u03b2\u2217k\u03bbk)\u22121). Given the hyper-parameters we can update the variational distributions to maximize the variational lower bound. In order to write the update equations, it is convenient to define the following sufficient statistics :\ns0k = N\u2211 i=1 qik, s 1 k = N\u2211 i=1 qikxi, s 2 k = N\u2211 i=1 qikx 2 i . (35)\nwhere qik = q(wi = k). Then, the parameters of the optimal variational distributions on the MoG parameters for a given image are found as:\n\u03b1\u2217k = \u03b1k + s 0 k, (36) \u03b2\u2217k = \u03b2k + s 0 k, (37)\nm\u2217k = (s 1 k + \u03b2kmk)/\u03b2 \u2217 k , (38)\na\u2217k = ak + s 0 k/2, (39) b\u2217k = bk + 1\n2 (\u03b2km\n2 k + s 2 k)\u2212\n1 2 \u03b2\u2217k(m \u2217 k) 2. (40)\nThe component assignments q(wi) that maximize the bound given the variational distributions on the MoG parameters are given by:\nln qik = IEq(\u03c0)q(\u03bbk,\u00b5k) [ ln\u03c0k + lnN (xi|\u00b5k, \u03bb\u22121k ) ] (41)\n= \u03c8(\u03b1\u2217k)\u2212 \u03c8(\u03b1\u0302\u2217) + 1\n2\n[ \u03c8(a\u2217k)\u2212 ln b\u2217k ] (42)\n\u22121 2 [a\u2217k b\u2217k (xi \u2212m\u2217k)2 + (\u03b2\u2217k)\u22121 ] . (43)\nSince the sufficient statistics given by Eq. (35) depend on the component assignments, the distributions on the MoG parameters and the component assignments can be updated iteratively to improve the bound.\nUsing the above variational update equations, we obtain the variational distribution, and therefore the lower-bound on the loglikelihood for each image. During training, we learn the model hyper-parameters by iteratively maximizing the sum of the lowerbounds for the training images w.r.t. the hyper-parameters, and w.r.t. the variational parameters. Once the latent MoG model is trained, we use the per-image lower-bounds to extract the approximate Fisher vector descriptors according to the gradient of F with respect to the model hyper-parameters.\nThe gradient of F w.r.t. the hyper-parameters depends only on the variational distributions on the MoG parameters of an image q(\u03c0), q(\u03bbk), and q(\u00b5k|\u03bbk), and not on the component assignments q(wi). For the precision hyper-parameters we find:\n\u2202F \u2202ak = [\u03c8(a\u2217k)\u2212 ln b\u2217k]\u2212 [\u03c8(ak)\u2212 ln bk] , (44) \u2202F\n\u2202bk = ak bk \u2212 a\n\u2217 k\nb\u2217k , (45)\nFor the hyper-parameters of the means: \u2202F\n\u2202\u03b2k =\n1\n2\n( \u03b2\u22121k \u2212\na\u2217k b\u2217k\n(mk \u2212m\u2217k)2 \u2212 1/\u03b2\u2217k ) , (46)\n\u2202F\n\u2202mk = \u03b2k a\u2217k b\u2217k (m\u2217k \u2212mk), (47)\nand for the hyper-parameters of the mixing weights: \u2202F\n\u2202\u03b1k = [\u03c8(\u03b1\u2217k)\u2212 \u03c8(\u03b1\u0302\u2217)]\u2212 [\u03c8(\u03b1k)\u2212 \u03c8(\u03b1\u0302)] . (48)\nBy substituting the update equation (36) for the variational parameters \u03b1\u2217k in the gradient Eq. (48), we exactly recover the gradient of the multivariate Po\u0301lya model, albeit using soft-counts s0k = \u2211N i=1 q(wi = k) of visual word occurrences here. Thus, the bound leaves the qualitative behavior of the multivariate Po\u0301lya model intact. Similar discounting effects can be observed in the gradients of the hyper-parameters of the means and variances. Substitution of the update equation (38) for the variational parameters m\u2217k in the gradient Eq. (47), reveals that the gradient is similar to the square-root of the gradient obtained in [38] for the MoG mean parameters. The discounting function for this gradient is however slightly different from the \u03c8(\u00b7) function, but has a similar monotone concave form. We consider examples of the learned discounting functions in Section 5.4.\nOur latent MoG model associates two hyper-parameters (mk, \u03b2k) with each mean \u00b5k, and similar for the precisions. Therefore, our image representation are almost twice as long compared to the iid MoG model: K(1 + 4D) vs . K(1 + 2D) dimensions. The updates of the variational parameters \u03b2\u2217k and a \u2217 k in equations (37) and (39), however, only involve the zero-order statistics s0k. In [38] the FV components corresponding to the mixing weights of the MoG, which are also based on zero-order statistics, were shown to be redundant when also including the components corresponding to the means and variances. Therefore, we expect the gradients w.r.t. the corresponding hyper-parameters \u03b2k and ak to be of little importance for image classification purposes. Experimental results, not reported here, have empirically verified this. We therefore fix the number of Gaussians rather than the FV dimension when we compare different representations in the next section, and use all FV components to avoid confusion."}, {"heading": "5 EXPERIMENTAL EVALUATION", "text": "In this section, we present a detailed evaluation of the latent BoW, LDA and the latent MoG models over SIFT local descriptors using the PASCAL VOC\u201907 [14] data set in Section 5.2, Section 5.3 and Section 5.4, respectively. Then, we present a empirical study on the relationship between the model likelihood and image categorization performance in Section 5.5. Finally, we evaluate the Latent MoG model, which is the most advanced model that we consider, over the CNN-based local descriptors, and compare against the state-of-the-art on the PASCAL VOC\u201907 and MIT Indoor [41] data sets in Section 5.6.\nNow, we first describe our experimental setup for the SIFTbased experiments used in the subsequent sections."}, {"heading": "5.1 Experimental setup", "text": "In order to extract SIFT descriptors, we use the experimental setup described in the evaluation paper of Chatfield et al . [6]: we sample local SIFT descriptors from the same dense grid (3 pixel stride, across 4 scales), which results in around 60, 000 patches per image, project the local descriptors to 80 dimensions with PCA, and train the MoG visual vocabularies from 1.5\u00d7106 descriptors. For the PASCAL VOC\u201907 data set, we use the interpolated mAP score specified by the VOC evaluation protocol [14].\nWe compare global image representations, and representations that capture spatial layout by concatenating the signatures computed over various spatial cells as in the spatial pyramid matching (SPM) method [30]. Again, we follow [6] and combine a 1\u00d7 1, a 2\u00d72, and a 3\u00d71 grid. Throughout, we use linear SVM classifiers, and we cross-validate the regularization parameter.\nBefore training the classifiers we apply two normalizations to the image representations. First, we whiten the representations so that each dimension is zero-mean and has unit-variance across images in order to approximate normalization with the inverse Fisher information matrix. Second, following [40], we also `2 normalize the image representations.\nFor the BoW, PLSA and MoG models, we compare using Fisher vectors with and without power normalization, and to using the Fisher vectors of the corresponding latent variable models. As in [40], power normalization is applied after whitening, and before `2 normalization. We evaluate two types of power normalization: (i) signed square-rooting (\u03c1 = 1/2) as in [6], [40], which we denote by a prefix \u201cSqrt\u201d, (ii) more general power normalization, which we denote by a prefix \u201cPn\u201d. In the latter case, we crossvalidate the parameter \u03c1 \u2208 {0, 0.1, 0.2, ..., 1} for each setting, but keeping it fixed across the classes.\nIn Tables 1, 2, 3 and 5, the bold numbers indicate the top performing representations in each setting that are statistically equivalent, which we measure by using the bootstrapping method proposed in Everingham et al . [14], at 95% confidence interval. In Tables 4 and 6, we are unable to run the test on other state-of-theart approaches, as the statistical significance test requires original classification scores on the test images."}, {"heading": "5.2 Evaluating BoW and Po\u0301lya models", "text": "In Table 1 we compare the results obtained using standard BoW histograms, two types of power normalized histograms, and the Po\u0301lya model. In all three cases, we generate the visual word counts from soft assignments of patches to the MoG components. Overall, we see that the spatial information of SPM is useful, and that larger vocabularies increase performance. We observe that both power\nnormalization and the Po\u0301lya model both consistently improve the BoW representation, across all dictionary sizes, and with or without SPM. Furthermore, the Po\u0301lya model generally leads to larger improvements than power normalization. These results are in line with the observation of Section 4.1 that the non-iid Po\u0301lya model generates similar transformations on BoW histograms as power normalization does, and show that normalization by the digamma function is at least as effective as power normalization.\nFigure 8 illustrates the discounting functions learned by the Po\u0301lya model for a dictionary of 64 visual words, without a spatial pyramid. Each solid curve in the figure corresponds to one of the visual words, and shows the corresponding digamma function \u03c8(\u03b1k +nk) as a function of the visual word count nk. Compared to the square-root transformation, which is shown by the dashed curve, we observe that the Po\u0301lya model generally leads to similar but somewhat stronger discounting effect."}, {"heading": "5.3 Evaluating topic model representations", "text": "We compare different topic model representations of Section 4.2: Fisher vectors computed on the PLSA model, its power normalized version, and using the corresponding LDA latent variable model. We compare to the corresponding BoW representations, and include SPM in all experiments. For the sake of brevity, we report only cross-validation based power normalization, as squarerooting gives similar results. In order to train LDA models, we first train a PLSA model, and then fit Dirichlet priors on the topic-word and document-topic distributions as inferred by PLSA.\nIn Figure 9, we consider topic models using T = 2 topics for various dictionary sizes, and in Figure 10 we use dictionaries of K = 1024 visual words, and consider performance as a function of the number of topics.\nWe observe that (i) topic models consistently improve performance over BoW models, and (ii) the plain PLSA representations are consistently outperformed by the power normalized version, and the LDA model. The LDA model requires less topics than (power-normalized) PLSA to obtain similar performance levels. This is in line with our findings with the BoW model of the previous section."}, {"heading": "5.4 Evaluating latent MoG model", "text": "We now turn to the evaluation of the MoG-based image representations. In order to speed-up the learning of the hyperparameters, we fix the patch-to-word soft-assignments as obtained from the MoG dictionary, and pre-compute the sufficient statistics of Eq. (35) once. We then iteratively update the model hyperparameters, and the parameters of the posteriors on the per-image latent MoGs, as detailed in Section 4.3.\nWe initialize the Dirichlet distribution on the mixing weights by matching the moments of the distribution of normalized visual word frequencies s0k, which gives an approximate maximum likelihood estimation [34]. Similarly, we initialize the hyper-parameters ak and bk of the Gamma prior on the precision of visual word k, by matching the mean and variance of empirical precision values computed from the sufficient statistics for each visual word, while weighting the contribution of each image by the count of visual word k in that image. In this step, the empirical precision values of visual words with few associated descriptors can become too large and may lead to poor initialization. To deal with this issue, we truncate per-image empirical precision values with respect to the corresponding global empirical precision values scaled by a\nconstant factor, which is cross-validated among a predefined set of values. Finally, we initialize the hyper-parameters mk and \u03b2k by matching the mean and variance of the per-image empirical mean values computed from the sufficient statistics, again weighting each image by the count of visual word k in that image.3\nIn Table 2, we compare representations based on Fisher vectors computed over MoG models, their two power normalized versions, and the latent MoG model of Section 4.3. We can observe that the MoG representations lead to better performance than the BoW and topic model representations while using smaller vocabularies. Furthermore, the discounting effect of power normalization and our latent variable model has a more pronounced effect here than it has for BoW models, improving mAP scores by around 4 points. Also for the MoG models, our latent variable approach leads to improvements that are comparable to those obtained by power normalization. So again, the benefits of power normalization may be explained by using non-iid latent variable models that generate similar representations.\nSimilar to Figure 8, we present an empirical comparison of the MoG FV and LatMoG FV based on a vocabulary of size K = 64 components in Figure 11. In this case we consider gradients w.r.t. the Gaussian mean parameters. The transformation given by power normalization is given for reference in dashed black. Each LatMoG curve is obtained by sampling a dimension-cluster pair (d, k), and plotting the LatMoG FV with respect to mk,d as a function of the MoG FV with respect to \u00b5k,d over different images. The LatMoG curves are smoothed via a median filter for visualization purposes. We observe that the LatMoG model naturally generates FVs with discounting effects, as demonstrated by the curves similar to square-root transformation. Note that the gradient in Eq. (47) for the LatMoG model is a joint function of the s0k, s 1 k and s 2 k statistics, which makes that plotting LatMoG FVs against MoG FVs results in non-smooth curves."}, {"heading": "5.5 Relationship between model likelihood and categorization performance", "text": "We have seen that the Fisher vectors of our non-iid image models provide significantly better image classification performance compared to the Fisher vectors of the corresponding iid models, unless power normalization is used to implement a discounting transformation on the image descriptors. In a broad sense, our experimental results suggest that Fisher kernels combined with more powerful generative models can possibly lead to better image categorization performance.\n3. Source code for LatMoG is available at http://lear.inrialpes.fr/software.\nIn order to investigate the relationship between the image models and the categorization performance using the corresponding Fisher vectors, we propose to empirically analyze the MoG models and the corresponding image descriptors at a number of PCA projection dimensions (D) and vocabulary sizes (K). Here, we use the log-likelihood of each model on a validation set as a measure of the generative power of the models and evaluate the image categorization performance of the corresponding Fisher vectors in terms of mAP scores on the PASCAL VOC 2007 dataset.\nOne important detail is that it may not be meaningful to compare the image categorization performance across image descriptors of different dimensionality: Our previous experimental results have shown that the mAP scores typically increase as the MoG Fisher vector descriptors become higher dimensional. Therefore, we want to compare the categorization performance across the image descriptors of fixed dimensionality, i.e . across the (D,K) pairs such that the product D \u00d7 K is constant. On the other hand, the log-likelihood of MoG models are comparable only if they operate in the same PCA projection space. In order to overcome this difficulty, we convert each pair of PCA and MoG models into a joint generative model, which allows us to obtain comparable log-likelihood values across different PCA subspaces.\nWe propose to obtain the joint generative models by first defining a shared descriptor space as follows: Let \u03c6(x) = UT(x\u2212\u00b50) be the full-dimensional PCA transformation function for the local descriptors, where \u00b50 is the empirical mean of theD0-dimensional local descriptors and U is theD0\u00d7D0 dimensional matrix of PCA basis column vectors. We note that \u03c6(x) does not apply dimension reduction, and the projection of a local descriptor x onto the D dimensional PCA subspace is given by ID\u00d7D0\u03c6(x), i.e . the first D coordinates of \u03c6(x). Therefore, the density function of a given MoG model in the D-dimensional PCA subspace is given by\np(x) = K\u2211 k=1 \u03c0kN (ID\u00d7D0\u03c6(x);\u00b5k,\u03a3k). (49)\nwhere \u03c0k is the mixing weight, \u00b5k is the D-dimensional mean vector and \u03c3k is the variances vector of the k-th component. Then, we can map the PCA dimension reduction model and the MoG model into a new MoG model in the space of \u03c6(x) descriptors as follows:\np0(x) = \u2211 k \u03c0kN (\u03c6(x);\u00b5\u2032k, \u03c3\u2032k) (50)\n8 16 32 64 128 \u2212572\n\u2212570\n\u2212568\n\u2212566\n\u2212564\n\u2212562\n\u2212560\n\u2212558\nNumber of PCA dims.\nLo g\nLi ke\nlih oo\nd\nD\u00d7K=32768 D\u00d7K=16384 D\u00d7K=8192\n(a)\n8 16 32 64 128 48\n50\n52\n54\n56\n58\n60\nNumber of PCA dims.\nm A\nP\nD\u00d7K=32768 D\u00d7K=16384 D\u00d7K=8192\n(b)\nFig. 12. Evaluation of the model log-likelihood and the classification performance in terms of mAP scores as a function of the number of PCA dimensions (D) and the vocabulary size (K). The x-axis of each plot shows the number of PCA dimensions. Each curve represents a set of (D,K) values such that D \u00d7K stays constant.\nwhere each mean vector is defined as\n\u00b5\u2032k = ID0\u00d7D\u00b5k, (51)\nand each variances vector \u03c3\u2032k is obtained by concatenating the corresponding D-dimensional \u03c3k vector with the empirical global variances of the remaining D0 \u2212D dimensions.\nIn our experiments, we have randomly sampled 300,000 SIFT descriptors to measure the average model log-likelihoods. We evaluate the image categorization performance using squarerooted and `2 normalized MoG Fisher vectors, without a spatial pyramid. We have utilized (D,K) pairs obtained by varying D from 8 to 128 and K from 64 to 4096.\nFigure 12a presents the model log-likelihood values and Figure 12b presents the corresponding image classification mAP scores. The x-axis of each plot shows the number of PCA dimensions. Each curve represents a set of (D,K) values where D\u00d7K is constant. From the experimental results first we can see that increasing the number of PCA dimensions (and hence reducing the number of mixing components) consistently increases the model log-likelihood. Second, the mAP scores similarly increase up to D \u2264 64, but then degrade from D = 64 to D = 128. Therefore, even if the model log-likelihood and categorization performance are related, they are not necessarily tightly correlated. Image categorization performance can be affected by several other factors, including the details of target categorization task, and transformations applied to the Fisher vector representations, such as power and `2 normalization here. Despite these findings, we believe that further investigation of the relationship between the modeling strength of generative models and the performance of the corresponding Fisher vectors for recognition tasks can lead to advances in unsupervised representation learning."}, {"heading": "5.6 Experiments using CNN features", "text": "We have so far utilized the SIFT local descriptors in our experiments. In this section, we evaluate the latent MoG representation based on local descriptors extracted using a convolutional neural network (CNN) model [28]. For this purpose, we consider two feature extraction schemes. First, we utilize the grid based region sampling approach based on the work by Gong et al . [16] and Liu et al . [31], and extract local descriptors by feeding cropped regions to a CNN model. Second, inspired from the R-CNN object detector [15], we propose to extract local CNN features for the image regions sampled by a candidate window generation\nmethod. Unlike the R-CNN detector, however, we utilize the region descriptors to extract image descriptors using the Fisher kernel framework, instead of evaluating individual regions as detection candidates. To the best of our knowledge, we are first to utilize detection proposals for this purpose.\nIn order to extract CNN features from regions sampled on a grid, we follow the local region sampling approach proposed by Liu et al . [31]: a given image is first scaled to a size of 512\u00d7 512 pixels, then, regions of size 227 \u00d7 227 are sampled in a sliding window fashion with a stride of 8 pixels. This procedure results in around 1300 regions per image. The image patch corresponding to each region sample is cropped and feed into the CNN model of Krizhevsky et al . [28], which is pre-trained on the ImageNet ILSVRC2012 dataset [11] using the Caffe library [23]. Finally, the outputs of the CNN model are used as the local descriptors.\nIn our second approach, we utilize the detection proposal regions generated using the selective search method of Uijlings et al . [50]. This method computes multiple hierarchical segmentation trees for a given image, and takes the segment bounding boxes as the detection proposals. This procedure results in around 1, 500 regions per image. Following the R-CNN object detector, we crop and re-size the window proposals to regions of size 224\u00d7 224, as required by the CNN model.\nAs region descriptors we consider the layer six and seven activations of the CNN model. In order to speed up the Fisher vector computations, we project the original 4, 096-dimensional feature vectors to 128 dimensions using PCA. In our preliminary experiments, we have verified that higher dimensional PCA projections does not improve the image categorization performance. Following the iid MoG based experiments in [16] and [31], we use models with K = 100 Gaussian components, `2 normalize the resulting image representations, and do not use SPM grids.\nIn Table 3, we compare the MoG Fisher vector, its power normalized versions, and the latent MoG Fisher vector representations. First, we observe that using selective search regions for descriptor pooling leads to consistently better results than using the grid based regions. Given that both approaches use a comparable number of regions, the improvement using selective\nRegions CNN Layer MoG SqrtMoG PnMoG LatMoG\nGrid fc6 60.1 66.0 67.3 62.2 Grid fc7 57.0 64.8 65.0 61.5 Selective fc6 66.6 69.4 69.7 68.2 Selective fc7 65.2 69.0 69.1 69.1\nTABLE 5 Comparison of classification accuracy on MIT Indoor: plain MoG, two types of power normalized MoG and latent MoG.\nsearch regions is probably due to using regions of multiple scales, and having a better object-to-clutter ratio. Second, we observe that also in this setting using the Latent MoG model leads to improvements that are comparable to those obtained by power normalization. Third, best results are obtained with layer seven activations using power normalization and our latent model.\nIn Table 4 we show that our results are comparable to the recent results based on a similar CNN models. The first row shows the CNN baseline (73.9%), as reported by Razavian et al . [44], which corresponds to training an SVM classifier over the full image CNN descriptors. The same paper also shows that the performance can be improved to 77.2% by applying feature transformations to image descriptors and incorporating additional training examples via transforming images. Bilen et al . [1] (80.9%) explicitly localizes object instances in images using an iterative weakly supervised localization method. The result shows that explicit localization of the objects can help better categorization of the images. Liu et al . [31] (76.9%) extract Fisher vectors of a sparse coding based model over local CNN features (see Appendix A for a detailed discussion of their model). Overall, we observe that our results using power normalized MoG FVs (78.0%) and latent MoG FVs (77.1%) are comparable to the aforementioned recent results, all of which are based on similar CNN models, and validate the effectiveness of our Latent MoG model for local feature aggregation.\nWe note that better results on the VOC\u201907 dataset have recently been reported based on significantly different CNN features and/or architectures. For example, Chatfield et al . [7] achieve 82.4% mAP by utilizing the OverFeat [47] architecture, combined with a carefully selected set of data augmentation, data normalization and CNN fine-tuning techniques. Wei et al . [54] achieve 85.2% by max-pooling the class predictions over candidate windows, utilizing additional training images, and using a two-stage CNN fine-tuning approach. Simonyan and Zisserman [48] report that the classification performance can be improved up to 89.7% mAP by using very deep network architectures, and combining multiple CNN models. We can expect similar improvements in the feature aggregation methods, including ours, by utilizing these betterperforming CNN features.\nIn order to validate our results on a second dataset, we evaluate our latent MoG model on the MIT Indoor dataset. The dataset contains 6,700 images, each of which is labeled with one of the 67 indoor scene categories. Before extracting window proposals, we resize each image such that the larger dimension is 500 pixels. We use the standard split for the dataset, which provides 80 train and 20 test images per class, and evaluate the results in terms of average classification accuracy.\nThe results for MIT Indoor are presented Table 5. In each row, we evaluate a combination of the 6-th or 7-th CNN layer with the grid based or selective search based regions. Our results are overall\nconsistent with those we obtain on VOC 2007: (i) using selective search regions leads to better performance, and (ii) using the Latent MoG model leads to significant improvements, comparable to those obtained by power normalization. Therefore, the results again support that the benefits of power normalization can be explained by their similarity to non-iid latent variable models that generate similar transformations.\nFinally, in Table 6, we compare our results on the MIT Indoor dataset with the state-of-the-art. The first methods, Juneja et al . [25] (63.2%) and Doersch et al . [12] (64.0%), extract midlevel representations by explicitly localizing discriminative image regions. In the next two rows, we observe that the CNN baseline improves from 58.4% to 69.0% using the feature and image transformations proposed by Razavian et al . [44]. The sparse coding Fisher vectors proposed by Liu et al . [31] result in a comparable performance at 68.2%. Gong et al . [16] (68.9%) utilizes power normalized VLAD features over the CNN descriptors extracted from multi-scale grid-based regions, in combinations with the full image CNN features. Overall, we observe that our approach using power normalized MoG FVs (69.1%) and latent MoG FVs (69.1%) over selective search regions provide state-of-theart performance on the MIT Indoor dataset."}, {"heading": "6 CONCLUSIONS", "text": "In this paper we have introduced latent variable models for local image descriptors, which avoid the common but unrealistic iid assumption. The Fisher vectors of our non-iid models are functions computed from the same sufficient statistics as those used to compute Fisher vectors of the corresponding iid models. In fact, these functions are similar to transformations that have been used in earlier work in an ad-hoc manner, such as the power normalization, or signed-square-root. Our models provide an explanation of the success of such transformations, since we derive them here by removing the unrealistic iid assumption from the popular BoW and MoG models. Second, we have shown that gradients of the variational free-energy bound on the log-likelihood gives exact Fisher score vectors as long as the variational posterior distribution is exact. Third, we have shown that approximate Fisher vectors for the proposed latent MoG model can be successfully extracted using the variational Fisher vector framework. Finally, we have shown that the Fisher vectors of our non-iid MoG model over CNN region descriptors extracted on selectively sampled windows lead to image categorization performance that is comparable or superior to that obtained with state-of-the-art feature aggregation representations based on iid models.\nAPPENDIX A. VARIATIONAL FISHER KERNEL EXAMPLES\nIn this section, we give two examples that illustrate applications of the variational FV framework, in addition to the models considered in the main text.\nIn our first example, we derive a fast variant of the MoG FV representation using the variational Fisher kernel formulation. Recall that the final MoG FV image representation is obtained by aggregating K(1 + 2D)-dimensional per-patch FVs. Therefore, the cost of feature extraction grows linearly with respect to K , D and N . One way to speed up this process, without sacrificing the descriptor dimensionality, is to hard-assign each local descriptor to visual word with the highest posterior probability. Using hardassignment, each local descriptor produces a (1+2D) dimensional descriptor, therefore, the aggregation speeds-up by a factor of K . As noted in [45], the MoG FV descriptor in this case can be also interpreted as a generalization of the VLAD descriptor [22].\nAlthough the hard-assignment method can provide significantly speeds up in the descriptor aggregation process, it may also cause significant information loss [51]. This problem can be addressed by utilizing clipped posterior weights within the variational FV framework. More specifically, we can define the family of approximate posteriors Q as those distributions with at most K \u2032 non-zero values. The best approximation to a given p(k|x) is then obtained by re-normalizing the largest K \u2032 values of p(k|x) and setting the other values to zero. In this case, each patch yields a descriptor with at mostK \u2032(1+2D) non-zero values, which translates into a aggregation speed up of factor KK\u2032 . The number of non-zeros K \u2032 can be set to strike a balance between the information loss and the aggregation cost. This shows that clipping the posteriors to speed-up the computation of FVs, as e.g . done in [9], can be justified in the variational framework. The MoG model can also be learned in a coherent manner, by optimizing the obtained variational bound instead of the log-likelihood. This forces the MoG components to be more separated, so that the true posteriors will concentrate on few components.\nAs a second example, we show that the derivation of the sparse coding FVs of Liu et al . [31], which we have experimentally compared against in Section 5.6, can be significantly simplified using the variational formulation. In their approach, a D-dimensional local descriptor x is modeled by a mixture of basis vectors:\np(x) = \u222b p(x|u; B)p(u)du (52)\nwhere u is the latent vector of mixing weights of length K , and B is the dictionary matrix with each of theK columns corresponding to a D-dimensional basis vector. The distribution p(x|u; B) is a Gaussian with mean Bu, and covariance matrix equal to a multiple of the identity matrix, and p(u) is the Laplacian prior on the mixture weights. Liu et al . [31] propose to approximate p(x) by the point estimate for u that maximizes the likelihood:\np(x) \u2248 p(x|u?; B)p(u?) (53)\nwhere\nu? = arg max u p(x|u; B)p(u). (54)\nIn order to compute FVs for this model, we need to compute the gradients of Eq. (53) with respect to the dictionary matrix B.\nHowever, as noted in [31], this is leads to a relatively complicated calculation since u? is dependent on B. Using a series of techniques, it is shown in [31] that the gradient is given by:\n\u2202 log p(x)\n\u2202B = (x\u2212 Bu?)u? (55)\nInstead, we can use the variational Fisher kernel formulation to achieve the same result in a simpler way. For this purpose, we define the class of approximate posteriors Q as the set of delta peaks that put all mass at a single value u. It is then easy to see that the optimal q \u2208 Q that maximizes the variational bound is q(u?) = 1 and q(u 6= u?) = 0. Given the optimal q, the variational FV is given by:\n\u2202F \u2202B = \u2202IEq[ln p(x,u)] \u2202B (56)\n= \u2202 ln p(x,u?)\n\u2202B (57)\nCompared to Eq. (54), this is a much simpler derivative operation since the gradient is now decoupled from the u? estimation step. It can be easily shown that the resulting gradient is equivalent to Eq. (55). This shows that the variational FV formulation can be preferable over the original FV formulation. Acknowledgements. This work was supported by the European integrated project AXES and the ERC advanced grant ALLEGRO."}], "references": [{"title": "Weakly supervised object detection with posterior regularization", "author": ["H. Bilen", "M. Pedersoli", "T. Tuytelaars"], "venue": "In British Machine Vision Conference,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Pattern recognition and machine learning", "author": ["C. Bishop"], "venue": "Spinger-Verlag,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Latent Dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Using Fisher kernels from topic models for dimensionality reduction", "author": ["G. Chandalia", "M. Beal"], "venue": "In NIPS Workshop on Novel Applications of Dimensionality Reduction,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "PLSI: The true Fisher kernel and beyond", "author": ["J.C. Chappelier", "E. Eckard"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "The devil is in the details: an evaluation of recent feature encoding methods", "author": ["K. Chatfield", "V. Lempitsky", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Image categorization using Fisher kernels of non-iid image models", "author": ["R.G. Cinbis", "J. Verbeek", "C. Schmid"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Segmentation driven object detection with Fisher vectors", "author": ["R.G. Cinbis", "J. Verbeek", "C. Schmid"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Visual categorization with bags of keypoints", "author": ["G. Csurka", "C. Dance", "L. Fan", "J. Willamowski", "C. Bray"], "venue": "In ECCV Int. Workshop on Stat. Learning in Computer Vision,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Mid-level Visual Element Discovery as Discriminative Mode Seeking", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Deriving TF-IDF as a Fisher kernel", "author": ["C. Elkan"], "venue": "In String Processing and Information Retrieval,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "The Pascal Visual Object Classes Challenge", "author": ["M. Everingham", "S.M.A. Eslami", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal on Computer Vision,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Multi-scale Orderless Pooling of Deep Convolutional Activation Features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "The unreasonable effectiveness of data", "author": ["A. Halevy", "P. Norvig", "F. Pereira"], "venue": "Intelligent Systems, IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Learning the similarity of documents: An informationgeometric approach to document retrieval and categorization", "author": ["T. Hofmann"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["T. Jaakkola", "D. Haussler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "On the burstiness of visual elements", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Aggregating local image descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin", "M. Douze", "J. S\u00e1nchez", "P. P\u00e9rez", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "An introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "T. Jaakola", "L. Saul"], "venue": "Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Blocks that shout: Distinctive parts for scene classification", "author": ["M. Juneja", "A. Vedaldi", "C.V. Jawahar", "A. Zisserman"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Dirichlet-based histogram feature transform for image classification", "author": ["T. Kobayashi"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Modeling spatial layout with Fisher vectors for image categorization", "author": ["J. Krapac", "J. Verbeek", "F. Jurie"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Latent mixture vocabularies for object categorization and segmentation", "author": ["D. Larlus", "F. Jurie"], "venue": "Image and Vision Computing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Beyond bags of features: spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "Encoding high dimensional local features by sparse coding based Fisher vectors", "author": ["L. Liu", "C. Shen", "L. Wang", "A. van den Hengel", "C. Wang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Information theory, inference, and learning algorithms", "author": ["D.J. MacKay"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Modeling word burstiness using the Dirichlet distribution", "author": ["R. Madsen", "D. Kauchak", "C. Elkan"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "Estimating a Dirichlet distribution", "author": ["T. Minka"], "venue": "http://research.microsoft. com/en-us/um/people/minka/papers/dirichlet/minka-dirichlet.pdf,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Free energy score space", "author": ["A. Perina", "M. Cristani", "U. Castellani", "V. Murino", "N. Jojic"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Capturing spatial interdependence in image features: the counting grid, an epitomic representation for bags of features", "author": ["A. Perina", "N. Jojic"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Expression microarray data classification using counting grids and Fisher kernel", "author": ["A. Perina", "M. Kesa", "M. Bicego"], "venue": "In IAPR International Conference on Pattern Recognition,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Fisher kernels on visual vocabularies for image categorization", "author": ["F. Perronnin", "C. Dance"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Large-scale image categorization with explicit data embedding", "author": ["F. Perronnin", "J. S\u00e1nchez", "Y. Liu"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Improving the Fisher kernel for large-scale image classification", "author": ["F. Perronnin", "J. S\u00e1nchez", "T. Mensink"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2010}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A. Torralba"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2009}, {"title": "Modeling scenes with local descriptors and latent aspects", "author": ["P. Quelhas", "F. Monay", "J.-M. Odobez", "D. Gatica-Perez", "T. Tuytelaars", "L. Van-Gool"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2005}, {"title": "Feature learning for the image retrieval task", "author": ["A. Rana", "J. Zepeda", "P. Perez"], "venue": "In Asian Conference on Computer Vision Workshop on Feature and Similarity Learning for Computer Vision,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Image classification with the Fisher vector: Theory and practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": "International Journal on Computer Vision,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}, {"title": "Exponential family Fisher vector for image classification", "author": ["J. S\u00e1nchez", "J. Redolfi"], "venue": "Pattern Recognition Letters,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. Le- Cun"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Computing Research Repository,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2014}, {"title": "Video Google: a text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2003}, {"title": "Selective search for object recognition", "author": ["J. Uijlings", "K. van de Sande", "T. Gevers", "A. Smeulders"], "venue": "International Journal on Computer Vision,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Visual word ambiguity", "author": ["J. van Gemert", "C. Veenman", "A. Smeulders", "J.-M. Geusebroek"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Efficient additive kernels via explicit feature maps", "author": ["A. Vedaldi", "A. Zisserman"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2010}, {"title": "Spatial latent dirichlet allocation", "author": ["X. Wang", "E. Grimson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2008}, {"title": "CNN: single-label to multi-label", "author": ["Y. Wei", "W. Xia", "J. Huang", "B. Ni", "J. Dong", "Y. Zhao", "S. Yan"], "venue": "Computing Research Repository,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "Object categorization by learned universal visual dictionary", "author": ["J. Winn", "A. Criminisi", "T. Minka"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2005}, {"title": "Local features and kernels for classification of texture and object categories: a comprehensive study", "author": ["J. Zhang", "M. Marsza\u0142ek", "S. Lazebnik", "C. Schmid"], "venue": "International Journal on Computer Vision,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2007}], "referenceMentions": [{"referenceID": 9, "context": "P ATCH-based image representations, such bag of visual words (BoW) [10], [49], are widely utilized in image categorization and retrieval systems.", "startOffset": 67, "endOffset": 71}, {"referenceID": 48, "context": "P ATCH-based image representations, such bag of visual words (BoW) [10], [49], are widely utilized in image categorization and retrieval systems.", "startOffset": 73, "endOffset": 77}, {"referenceID": 37, "context": "Perronnin and Dance [38] have enhanced this basic representation using the notion of Fisher kernels [20].", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "Perronnin and Dance [38] have enhanced this basic representation using the notion of Fisher kernels [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "Following [4], which is the first and one of the very few studies utilizing this approximation method, we refer to the resulting kernel as the variational Fisher kernel.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "Interestingly, we find that our non-iid models yield gradients that are qualitatively similar to popular transformations of BoW image representations, such as square-rooting histogram entries or more generally applying power normalization [22], [39], [40], [52].", "startOffset": 239, "endOffset": 243}, {"referenceID": 38, "context": "Interestingly, we find that our non-iid models yield gradients that are qualitatively similar to popular transformations of BoW image representations, such as square-rooting histogram entries or more generally applying power normalization [22], [39], [40], [52].", "startOffset": 245, "endOffset": 249}, {"referenceID": 39, "context": "Interestingly, we find that our non-iid models yield gradients that are qualitatively similar to popular transformations of BoW image representations, such as square-rooting histogram entries or more generally applying power normalization [22], [39], [40], [52].", "startOffset": 251, "endOffset": 255}, {"referenceID": 51, "context": "Interestingly, we find that our non-iid models yield gradients that are qualitatively similar to popular transformations of BoW image representations, such as square-rooting histogram entries or more generally applying power normalization [22], [39], [40], [52].", "startOffset": 257, "endOffset": 261}, {"referenceID": 2, "context": "Our second contribution is the analysis of Fisher vector representations over the latent Dirichlet allocation (LDA) model [3] for image classification purposes.", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "In this case the computation of the gradients is intractable, therefore, we compute approximate variational Fisher vectors [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 18, "context": "We compare performance to Fisher vectors of PLSA [19], a topic model that does not treat the model parameters as latent variables.", "startOffset": 49, "endOffset": 53}, {"referenceID": 39, "context": "This leads to a representation that performs on par with the improved Fisher vector (FV) representation of [40] based on iid MoG models, which includes power normalization.", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "First, following recent work [16], [31], we compute Fisher vectors over densely sampled image patches that are encoded using CNN features.", "startOffset": 29, "endOffset": 33}, {"referenceID": 30, "context": "First, following recent work [16], [31], we compute Fisher vectors over densely sampled image patches that are encoded using CNN features.", "startOffset": 35, "endOffset": 39}, {"referenceID": 49, "context": "Second, we propose to extract Fisher vectors over image regions sampled by a selective search method [50].", "startOffset": 101, "endOffset": 105}, {"referenceID": 13, "context": "The experimental results on the PASCAL VOC 2007 [14] and MIT Indoor Scenes [41] datasets confirm the effectiveness of the proposed latent MoG image model, and the corresponding non-iid image descriptors.", "startOffset": 48, "endOffset": 52}, {"referenceID": 40, "context": "The experimental results on the PASCAL VOC 2007 [14] and MIT Indoor Scenes [41] datasets confirm the effectiveness of the proposed latent MoG image model, and the corresponding non-iid image descriptors.", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "This paper extends our earlier paper [8].", "startOffset": 37, "endOffset": 40}, {"referenceID": 40, "context": "We perform additional experimental evaluation on the MIT Indoor dataset [41].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56].", "startOffset": 138, "endOffset": 142}, {"referenceID": 38, "context": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56].", "startOffset": 144, "endOffset": 148}, {"referenceID": 39, "context": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56].", "startOffset": 150, "endOffset": 154}, {"referenceID": 51, "context": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56].", "startOffset": 156, "endOffset": 160}, {"referenceID": 55, "context": "The use of non-linear feature transformations in BoW image representations is widely recognized to be beneficial for image categorization [22], [39], [40], [52], [56].", "startOffset": 162, "endOffset": 166}, {"referenceID": 55, "context": "Popular remedies to this problem include the use of chi-square kernels [56], or taking the square-root of histogram entries [39], also referred to as the Hellinger kernel [52].", "startOffset": 71, "endOffset": 75}, {"referenceID": 38, "context": "Popular remedies to this problem include the use of chi-square kernels [56], or taking the square-root of histogram entries [39], also referred to as the Hellinger kernel [52].", "startOffset": 124, "endOffset": 128}, {"referenceID": 51, "context": "Popular remedies to this problem include the use of chi-square kernels [56], or taking the square-root of histogram entries [39], also referred to as the Hellinger kernel [52].", "startOffset": 171, "endOffset": 175}, {"referenceID": 38, "context": "Power normalization [39], defined as f(x) = sign(x)|x|, is a similar transformation that can be applied to non-histogram feature vectors, and it is equivalent to signed square-rooting for the coefficient \u03c1 = 1/2.", "startOffset": 20, "endOffset": 24}, {"referenceID": 0, "context": "The qualitative similarity is illustrated in Figure 3, where we compare the `2, chi-square, and Hellinger distances on the range [0, 1].", "startOffset": 129, "endOffset": 135}, {"referenceID": 38, "context": "Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55].", "startOffset": 72, "endOffset": 76}, {"referenceID": 51, "context": "Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55].", "startOffset": 78, "endOffset": 82}, {"referenceID": 39, "context": "Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55].", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55].", "startOffset": 181, "endOffset": 185}, {"referenceID": 54, "context": "Sometimes it is based on empirical observations of improved performance [39], [52], by reducing sparsity in Fisher vectors [40], or in terms of variance stabilizing transformations [22], [55].", "startOffset": 187, "endOffset": 191}, {"referenceID": 25, "context": "Recently, Kobayashi [26] showed that a similar discounting transformation based on taking logarithm of histogram entries, can be derived via modeling `1-normalized descriptors by Dirichlet distribution.", "startOffset": 20, "endOffset": 24}, {"referenceID": 42, "context": "[43] propose to discriminatively learn power normalization coefficients for image retrieval using a triplet-", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Similar transformations are also used in image retrieval to counter burstiness effects [21], i.", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "Burstiness also occurs in text, and the Dirichlet compound multinomial distribution, also known as multivariate P\u00f3lya distribution, has been used to model this effect [33].", "startOffset": 167, "endOffset": 171}, {"referenceID": 12, "context": "Elkan [13] shows the relationship between the Fisher kernel of the multivariate P\u00f3lya distribution and the tf-idf document representation.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "Our use of latent Dirichlet allocation (LDA) [3] differs from earlier work on using topic models such as LDA or PLSA [19] for object recognition [29], [42].", "startOffset": 45, "endOffset": 48}, {"referenceID": 18, "context": "Our use of latent Dirichlet allocation (LDA) [3] differs from earlier work on using topic models such as LDA or PLSA [19] for object recognition [29], [42].", "startOffset": 117, "endOffset": 121}, {"referenceID": 28, "context": "Our use of latent Dirichlet allocation (LDA) [3] differs from earlier work on using topic models such as LDA or PLSA [19] for object recognition [29], [42].", "startOffset": 145, "endOffset": 149}, {"referenceID": 41, "context": "Our use of latent Dirichlet allocation (LDA) [3] differs from earlier work on using topic models such as LDA or PLSA [19] for object recognition [29], [42].", "startOffset": 151, "endOffset": 155}, {"referenceID": 3, "context": "Similarly, Chandalia and Beal [4] propose to compress BoW document representation by computing LDA Fisher vector with respect to the parameters of the Dirichlet prior on the topic distributions.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "Finally, in contrast to the PLSA Fisher kernel, which was previously studied as a document similarity measure [5], [18], we show that the LDA Fisher kernel naturally involves discounting transformations.", "startOffset": 110, "endOffset": 113}, {"referenceID": 17, "context": "Finally, in contrast to the PLSA Fisher kernel, which was previously studied as a document similarity measure [5], [18], we show that the LDA Fisher kernel naturally involves discounting transformations.", "startOffset": 115, "endOffset": 119}, {"referenceID": 52, "context": "For example, the Spatial LDA model [53] extends the LDA model such that spatially neighboring visual words are more likely assigned to the same topic.", "startOffset": 35, "endOffset": 39}, {"referenceID": 35, "context": "The counting grid model [36], which is a grid of multinomial distributions, can be considered as an alternative to the spatial topic models.", "startOffset": 24, "endOffset": 28}, {"referenceID": 52, "context": "While these studies show that incorporation of spatial information can improve unsupervised semantic segmentation results [53], or lead to better generative classifiers compared to LDA [36], we limit our focus to Fisher kernels of orderless, i.", "startOffset": 122, "endOffset": 126}, {"referenceID": 35, "context": "While these studies show that incorporation of spatial information can improve unsupervised semantic segmentation results [53], or lead to better generative classifiers compared to LDA [36], we limit our focus to Fisher kernels of orderless, i.", "startOffset": 185, "endOffset": 189}, {"referenceID": 23, "context": "We overcome this difficulty by relying on the variational free-energy bound [24], which is obtained by subtracting the Kullback-Leibler divergence between an approximate posterior on the latent variables and the true posterior.", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "The method of approximating Fisher kernels with the gradient vector of a variational bound was first proposed by Chandalia and Beal [4] in order to obtain the LDA Fisher kernel.", "startOffset": 132, "endOffset": 135}, {"referenceID": 36, "context": "[37], which proposes a variational Fisher kernel for micro-array data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35], which uses the variational free-energy to define an alternative encoding, replacing the Fisher kernel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "The Fisher kernel framework proposed by Jaakkola and Haussler [20] allows combining the power of generative models and", "startOffset": 62, "endOffset": 66}, {"referenceID": 44, "context": "[45] refer to the normalized gradient given by \u03c6(x) as the Fisher vector.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Another motivation is that I makes the Fisher kernel invariant to the re-parameterization \u03b8 \u2192 \u03c8(\u03b8) for any differentiable and invertible function \u03c8 [2], which can be easily shown using the chain rule and the Jakobian matrix of the inverse function \u03c8\u22121.", "startOffset": 148, "endOffset": 151}, {"referenceID": 19, "context": "Alternatively, I can be dropped altogether [20] or analytical approximations can be derived, see e.", "startOffset": 43, "endOffset": 47}, {"referenceID": 37, "context": "[38], [45], [46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[38], [45], [46].", "startOffset": 6, "endOffset": 10}, {"referenceID": 45, "context": "[38], [45], [46].", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "Originally developed in statistical physics based on the calculus of variations and the mean field theory, the variational approximation framework that we utilize in this paper is known as the variational inference, and it is now among the most successful approximate probabilistic inference techniques [2], [24], [32].", "startOffset": 303, "endOffset": 306}, {"referenceID": 23, "context": "Originally developed in statistical physics based on the calculus of variations and the mean field theory, the variational approximation framework that we utilize in this paper is known as the variational inference, and it is now among the most successful approximate probabilistic inference techniques [2], [24], [32].", "startOffset": 308, "endOffset": 312}, {"referenceID": 31, "context": "Originally developed in statistical physics based on the calculus of variations and the mean field theory, the variational approximation framework that we utilize in this paper is known as the variational inference, and it is now among the most successful approximate probabilistic inference techniques [2], [24], [32].", "startOffset": 314, "endOffset": 318}, {"referenceID": 23, "context": "For this purpose, we iteratively update the variational lowerbound with respect to the variational distribution parameters, and the model hyper-parameters; an approach that is known as the variational expectation-maximization procedure [24].", "startOffset": 236, "endOffset": 240}, {"referenceID": 26, "context": "The standard BoW image representation can be interpreted as applying the Fisher kernel framework to a simple iid multinomial model over visual word indices, as shown in [27].", "startOffset": 169, "endOffset": 173}, {"referenceID": 32, "context": "This model is known as the multivariate P\u00f3lya, or Dirichlet compound multinomial [33], and the integral simplifies to", "startOffset": 81, "endOffset": 85}, {"referenceID": 0, "context": "Digamma functions \u03c8(\u03b1+n) for various \u03b1, and \u221a n as a function of n; functions have been rescaled to the range [0, 1].", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "In our second model, we extend the P\u00f3lya model to capture co-occurrence statistics of visual words using latent Dirichlet allocation (LDA) [3].", "startOffset": 139, "endOffset": 142}, {"referenceID": 2, "context": "Here we use a completely factorized approximate posterior as in [3] of the form", "startOffset": 64, "endOffset": 67}, {"referenceID": 18, "context": "In our experiments we compare LDA with the PLSA model [19].", "startOffset": 54, "endOffset": 58}, {"referenceID": 37, "context": "In this section we turn to the image representation of Perronnin and Dance [38] that applies the Fisher kernel framework to mixture of Gaussian (MoG) models over local descriptors.", "startOffset": 75, "endOffset": 79}, {"referenceID": 39, "context": "An improved version of this representation using power normalization was presented in [40].", "startOffset": 86, "endOffset": 90}, {"referenceID": 37, "context": "In [38], [40], local descriptors across images are assumed to be iid samples from a single MoG model underlying all images.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "In [38], [40], local descriptors across images are assumed to be iid samples from a single MoG model underlying all images.", "startOffset": 9, "endOffset": 13}, {"referenceID": 26, "context": "where we re-parameterize the mixing weights as \u03c0k = exp(\u03b3k)/ \u2211K k\u2032=1 exp(\u03b3k\u2032), and the Gaussians with precisions \u03bbk = \u03c3 \u22121 k , as in [27].", "startOffset": 133, "endOffset": 137}, {"referenceID": 37, "context": "(47), reveals that the gradient is similar to the square-root of the gradient obtained in [38] for the MoG mean parameters.", "startOffset": 90, "endOffset": 94}, {"referenceID": 37, "context": "In [38] the FV components corresponding to the mixing weights of the MoG, which are also based on zero-order statistics, were shown to be redundant when also including the components corresponding to the means and variances.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In this section, we present a detailed evaluation of the latent BoW, LDA and the latent MoG models over SIFT local descriptors using the PASCAL VOC\u201907 [14] data set in Section 5.", "startOffset": 151, "endOffset": 155}, {"referenceID": 40, "context": "Finally, we evaluate the Latent MoG model, which is the most advanced model that we consider, over the CNN-based local descriptors, and compare against the state-of-the-art on the PASCAL VOC\u201907 and MIT Indoor [41] data sets in Section 5.", "startOffset": 209, "endOffset": 213}, {"referenceID": 5, "context": "[6]: we sample local SIFT descriptors from the same dense grid (3 pixel stride, across 4 scales), which results in around 60, 000 patches per image, project the local descriptors to 80 dimensions with PCA, and train the MoG visual vocabularies from 1.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "For the PASCAL VOC\u201907 data set, we use the interpolated mAP score specified by the VOC evaluation protocol [14].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "We compare global image representations, and representations that capture spatial layout by concatenating the signatures computed over various spatial cells as in the spatial pyramid matching (SPM) method [30].", "startOffset": 205, "endOffset": 209}, {"referenceID": 5, "context": "Again, we follow [6] and combine a 1\u00d7 1, a 2\u00d72, and a 3\u00d71 grid.", "startOffset": 17, "endOffset": 20}, {"referenceID": 39, "context": "Second, following [40], we also `2 normalize the image representations.", "startOffset": 18, "endOffset": 22}, {"referenceID": 39, "context": "As in [40], power normalization is applied after whitening, and before `2 normalization.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "We evaluate two types of power normalization: (i) signed square-rooting (\u03c1 = 1/2) as in [6], [40], which we denote by a prefix \u201cSqrt\u201d, (ii) more general power normalization, which we denote by a prefix \u201cPn\u201d.", "startOffset": 88, "endOffset": 91}, {"referenceID": 39, "context": "We evaluate two types of power normalization: (i) signed square-rooting (\u03c1 = 1/2) as in [6], [40], which we denote by a prefix \u201cSqrt\u201d, (ii) more general power normalization, which we denote by a prefix \u201cPn\u201d.", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "[14], at 95% confidence interval.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Transformed counts are rescaled to the range [0, 1].", "startOffset": 45, "endOffset": 51}, {"referenceID": 33, "context": "We initialize the Dirichlet distribution on the mixing weights by matching the moments of the distribution of normalized visual word frequencies sk, which gives an approximate maximum likelihood estimation [34].", "startOffset": 206, "endOffset": 210}, {"referenceID": 0, "context": "All FV values are scaled to the range [-1,1].", "startOffset": 38, "endOffset": 44}, {"referenceID": 27, "context": "In this section, we evaluate the latent MoG representation based on local descriptors extracted using a convolutional neural network (CNN) model [28].", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "[16] and Liu et al .", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31], and extract local descriptors by feeding cropped regions to a CNN model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Second, inspired from the R-CNN object detector [15], we propose to extract local CNN features for the image regions sampled by a candidate window generation Regions CNN Layer MoG SqrtMoG PnMoG LatMoG", "startOffset": 48, "endOffset": 52}, {"referenceID": 43, "context": "CNN baseline [44] 73.", "startOffset": 13, "endOffset": 17}, {"referenceID": 43, "context": "[44] 77.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] 80.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[31] 76.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31]: a given image is first scaled to a size of 512\u00d7 512 pixels, then, regions of size 227 \u00d7 227 are sampled in a sliding window fashion with a stride of 8 pixels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28], which is pre-trained on the ImageNet ILSVRC2012 dataset [11] using the Caffe library [23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[28], which is pre-trained on the ImageNet ILSVRC2012 dataset [11] using the Caffe library [23].", "startOffset": 62, "endOffset": 66}, {"referenceID": 22, "context": "[28], which is pre-trained on the ImageNet ILSVRC2012 dataset [11] using the Caffe library [23].", "startOffset": 91, "endOffset": 95}, {"referenceID": 49, "context": "[50].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Following the iid MoG based experiments in [16] and [31], we use models with K = 100 Gaussian components, `2 normalize the resulting image representations, and do not use SPM grids.", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "Following the iid MoG based experiments in [16] and [31], we use models with K = 100 Gaussian components, `2 normalize the resulting image representations, and do not use SPM grids.", "startOffset": 52, "endOffset": 56}, {"referenceID": 43, "context": "[44], which corresponds to training an SVM classifier over the full image CNN descriptors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] (80.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[31] (76.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] achieve 82.", "startOffset": 0, "endOffset": 3}, {"referenceID": 46, "context": "4% mAP by utilizing the OverFeat [47] architecture, combined with a carefully selected set of data augmentation, data normalization and CNN fine-tuning techniques.", "startOffset": 33, "endOffset": 37}, {"referenceID": 53, "context": "[54] achieve 85.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "Simonyan and Zisserman [48] report that the classification performance can be improved up to 89.", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "[25] 63.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] 64.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "CNN baseline [44] 58.", "startOffset": 13, "endOffset": 17}, {"referenceID": 43, "context": "[44] 69.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] 68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] 68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] (63.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] (64.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44].", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] result in a comparable performance at 68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] (68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "As noted in [45], the MoG FV descriptor in this case can be also interpreted as a generalization of the VLAD descriptor [22].", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "As noted in [45], the MoG FV descriptor in this case can be also interpreted as a generalization of the VLAD descriptor [22].", "startOffset": 120, "endOffset": 124}, {"referenceID": 50, "context": "Although the hard-assignment method can provide significantly speeds up in the descriptor aggregation process, it may also cause significant information loss [51].", "startOffset": 158, "endOffset": 162}, {"referenceID": 8, "context": "done in [9], can be justified in the variational framework.", "startOffset": 8, "endOffset": 11}, {"referenceID": 30, "context": "[31], which we have experimentally compared against in Section 5.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] propose to approximate p(x) by the point estimate for u that maximizes the likelihood:", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "However, as noted in [31], this is leads to a relatively complicated calculation since u is dependent on B.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "Using a series of techniques, it is shown in [31] that the gradient is given by:", "startOffset": 45, "endOffset": 49}], "year": 2015, "abstractText": "The bag-of-words (BoW) model treats images as sets of local descriptors and represents them by visual word histograms. The Fisher vector (FV) representation extends BoW, by considering the first and second order statistics of local descriptors. In both representations local descriptors are assumed to be identically and independently distributed (iid), which is a poor assumption from a modeling perspective. It has been experimentally observed that the performance of BoW and FV representations can be improved by employing discounting transformations such as power normalization. In this paper, we introduce non-iid models by treating the model parameters as latent variables which are integrated out, rendering all local regions dependent. Using the Fisher kernel principle we encode an image by the gradient of the data log-likelihood w.r.t. the model hyper-parameters. Our models naturally generate discounting effects in the representations; suggesting that such transformations have proven successful because they closely correspond to the representations obtained for non-iid models. To enable tractable computation, we rely on variational free-energy bounds to learn the hyper-parameters and to compute approximate Fisher kernels. Our experimental evaluation results validate that our models lead to performance improvements comparable to using power normalization, as employed in state-of-the-art feature aggregation methods.", "creator": "LaTeX with hyperref package"}}}