{"id": "1510.02387", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2015", "title": "Mapping Unseen Words to Task-Trained Embedding Spaces", "abstract": "yet we consider regarding the setting in specify which context we train a spatial supervised familiarity model component that learns task - length specific word representations. we must assume that we have access efficiently to interpreting some initial word representations ( e. g., finding unsupervised embeddings ), and assumption that the supervised lesson learning procedure updates require them to correct task - scale specific spelling representations explicitly for words mainly contained universally in the training data. but what about words not invariably contained equally in studying the supervised training data data? ~ when imagining such explicitly unseen words are never encountered at random test setup time, they are typically represented by either their initial translation vectors or a fixed single unknown vector, which often leads down to squared errors. in this paper, we actually address indirectly this issue by learning to substantially map from initial semantic representations to task - block specific ones. we present a surprisingly general technique that uses incorporating a classical neural reward network memory mapper with applied a weighted based multiple - vocabulary loss criterion. this allows us to use the otherwise same learned comprehension model parameters at this test duration time - but cease now with appropriate task - specific representations available for unseen words. suppose we consider the same task of dependency parsing and report cumulative improvements in performance ( and reductions in differing out - \u2011 of - vocabulary rates ) across multiple domains such as distributed news, red web, audio and speech. we simply also achieve lower downstream improvements than on finding the task of parsing - based language sentiment analysis.", "histories": [["v1", "Thu, 8 Oct 2015 16:17:47 GMT  (214kb)", "http://arxiv.org/abs/1510.02387v1", "10 + 3 pages, 3 figures"], ["v2", "Thu, 23 Jun 2016 06:24:18 GMT  (223kb)", "http://arxiv.org/abs/1510.02387v2", "8 + 3 pages, 3 figures"]], "COMMENTS": "10 + 3 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["pranava swaroop madhyastha", "mohit bansal", "kevin gimpel", "karen livescu"], "accepted": false, "id": "1510.02387"}, "pdf": {"name": "1510.02387.pdf", "metadata": {"source": "CRF", "title": "Mapping Unseen Words to Task-Trained Embedding Spaces", "authors": ["Pranava Swaroop Madhyastha", "Mohit Bansal Kevin Gimpel", "Karen Livescu"], "emails": ["pranava@cs.upc.edu", "mbansal@ttic.edu", "kgimpel@ttic.edu", "klivescu@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 0.\n02 38\n7v 1\n[ cs\n.C L\n] 8\nO ct\nWe consider the setting in which we train a supervised model that learns task-specific word representations. We assume that we have access to some initial word representations (e.g., unsupervised embeddings), and that the supervised learning procedure updates them to task-specific representations for words contained in the training data. But what about words not contained in the supervised training data? When such unseen words are encountered at test time, they are typically represented by either their initial vectors or a single unknown vector, which often leads to errors. In this paper, we address this issue by learning to map from initial representations to task-specific ones. We present a general technique that uses a neural network mapper with a weighted multiple-loss criterion. This allows us to use the same learned model parameters at test time but now with appropriate task-specific representations for unseen words. We consider the task of dependency parsing and report improvements in performance (and reductions in out-of-vocabulary rates) across multiple domains such as news, Web, and speech. We also achieve downstream improvements on the task of parsingbased sentiment analysis."}, {"heading": "1 Introduction", "text": "In many NLP tasks, state-of-the-art systems achieve very good performance, but only when restricted to standard and heavily edited datasets (Petrov et al., 2010). For example, while\nstate-of-the-art accuracies exceed 97% for part-ofspeech tagging and 90% for dependency parsing, performance on non-standard, real-world datasets is substantially worse, dropping by nearly 10% absolute (Petrov and McDonald, 2012). A major cause of this drop is words that do not appear in the annotated training data but appear in unseen test data, whether in the same domain or in a new domain. We refer to such out-of-training-vocabulary (OOTV) words as unseen words. Supervised NLP systems often make errors on unseen words and, in structured tasks like dependency parsing, this can lead to other cascading errors in the same sentence, producing nonsensical output structures.\nRecently, continuous vector word representations, or embeddings, have shown promise in a variety of NLP tasks (Turian et al., 2010; Collobert et al., 2011; Anderson et al., 2013; Bansal et al., 2014). The typical assumption of word embedding learning is that words with similar statistical properties in a large dataset are similar in meaning. Using embeddings as features in NLP systems can help counter the effects of data sparsity (Necsulescu et al., 2015). However, the quality of such embeddings has been found to be heavily task-dependent (Bansal et al., 2014).\nThere is a great deal of work on updating embeddings during supervised training to make them more task-specific, whether through back-propagation or other techniques (Kalchbrenner et al., 2014; Qu et al., 2015; Chen and Manning, 2014). These task-trained embeddings have shown encouraging results but raise some concerns, especially: (1) for infrequent words, the updated representations\nmay be prone to overfitting, and (2) many words in the test data are not contained in the training data at all. In the latter case, at test time, most trained systems either fall back to some generic, single representation for all unknown words or use the initial representation (typically derived from unlabeled data) (S\u00f8gaard and Johannsen, 2012; Collobert et al., 2011). Neither of these choices is satisfactory: A single unknown word vector may lump too many words together, while the initial representations may be in a space which is not comparable to the trained embedding space.\nIn this paper, we focus on resolving unseen words by mapping from the initial embedding space to the task-trained embedding space. We train a neural network-based mapping function that takes initial word embeddings (learned using unsupervised techniques over a large unannotated dataset) and maps them to task-specific embeddings that are trained for the given task, via a multi-loss objective function. Moreover, due to the efficiency of training our mapper, we can tune its hyperparameters to optimize performance on each domain of interest, thereby achieving some of the benefits of domain adaptation.\nWe show the effectiveness of our mapping approach on the task of dependency parsing on a large set of diverse domains, e.g., news, Web, and speech. We also show the downstream effects of our mapper on the task of sentiment classification, using the recently proposed dependency parse-based tree long short-term memory (LSTM) network of Tai et al. (2015). In all domains and tasks, our method significantly reduces the number of unseen words (i.e., words without task-trained embeddings) and achieves significant performance improvements over the unmapped baselines."}, {"heading": "2 Mapping Unseen Representations", "text": "In this section, we present our approach, including an overview of the pipeline, the mapper architecture, and the training/tuning components (e.g., loss function, regularization, and thresholds)."}, {"heading": "2.1 Pipeline Overview and Definitions", "text": "Let V = {w1, . . . , wV } be the vocabulary of word types in a large, unannotated corpus. Let eoi denote the initial (original) embedding of word wi\ncomputed from this corpus. The initial embeddings are typically learned in an unsupervised way, but for our purposes they can be any initial embeddings. Let T \u2286 V be the subset of words that appear in the annotated training data for some supervised task-specific training. We define unseen words as those in the set V \\ T . While our approach is general, for concreteness, we consider the task of dependency parsing, so the annotated data consists of sentences paired with dependency trees. We assume a dependency parser that learns task-specific word embeddings eti for word wi \u2208 T , starting from the original embedding eoi . In this work, we use the Stanford neural dependency parser (Chen and Manning, 2014).\nThe goal of the mapper is as follows. We are given a training set of N pairs of initial and task-trained embeddings D = {(\neo 1 , et 1\n) , . . . , ( eoN , e t N )}\n, and we want to learn a function G that maps each initial embedding eoi to be as close as possible to its corresponding output embedding eti. We denote the mapped embedding emi , i.e., e m i = G (e o i ).\nFigure 1a describes the training procedure of the mapper. We use a supervised parser which is trained on an annotated dataset and initialized with pre-trained word embeddings eoi . The parser uses back-propagation to update these embeddings during training, producing task-trained embeddings eti for all wi \u2208 T . After we train the parser, the mapping function G is trained to map an initial word embedding eoi to its parser-trained embedding e t i. At test (or development) time, we use the trained mapper G to transform the original embeddings of unseen test words to the parser-trained space (see Figure 1b). When parsing held-out data, we use the same parser model parameters (W ) as shown in Figure 1b. The only difference is that now some of the word embeddings (i.e., for unseen words) have changed to mapped ones."}, {"heading": "2.2 Mapper Architecture", "text": "Our proposed mapper is a multi-layer feedforward neural network that takes an initial word embedding as input and outputs a mapped representation of the same dimensionality. In particular, we use a single hidden layer with a hardtanh non-linearity, so the\nfunction G is defined as:\nG(eoi ) = W2(hard tanh(W1e o i + b1)) + b2 (1)\nwhere W1 and W2 are parameter matrices and b1 and b2 are bias vectors.\nThe \u2018hardtanh\u2019 non-linearity is the standard \u2018hard\u2019 version of hyperbolic tangent:\nhard tanh(z) =\n\n \n \n\u22121 if z < \u22121\nz if \u22121 \u2264 z \u2264 1\n1 if z > 1\nIn preliminary experiments we compared with other non-linear functions (sigmoid, tanh, and ReLU), as well as with zero and more than one nonlinear layers. We found that hard tanh is computationally cheaper and performed better than the other non-linearities, and that the fewer or more non-linear layers did not improve performance."}, {"heading": "2.3 Loss Function", "text": "We use a weighted, multi-loss regression approach, optimizing a weighted sum of mean squared error\nand mean absolute error:\nloss(y, y\u0302) =\n\u03b1\nn \u2211\nj=1\n|yj \u2212 y\u0302j|+ (1\u2212 \u03b1) n \u2211\nj=1\n|yj \u2212 y\u0302j| 2 (2)\nwhere y = eti (the ground truth) and y\u0302 = e m i (the prediction) are n-dimensional vectors. This multiloss approach seeks to make both the conditional mean of the predicted representation close to the task-trained representation (via the squared loss) and the conditional median of the predicted representation close to the task-trained one (via the mean absolute loss). A weighted multi-criterion objective allows us to avoid making strong assumptions about the optimal transformation to be learned. We tune the hyperparameter \u03b1 on domain-specific held-out data.\nFor optimization, we use batch limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989). In preliminary experiments comparing with stochastic optimization, we found L-BFGS to be more stable to train and easier to check for convergence (as has recently been found in other settings as well (Ngiam et al., 2011))."}, {"heading": "2.4 Regularization", "text": "We use elastic net regularization (Liu and Nocedal, 1989), which linearly combines \u21131 and \u21132 penalties on the parameters to control the capacity of the mapper function. This equates to minimizing:\nF (\u03b8) = L(\u03b8) + \u03bb1\u2016\u03b8\u20161 + \u03bb2\n2 \u2016\u03b8\u20162 2\nwhere \u03b8 is the full set of mapper parameters and L(\u03b8) is the loss function (Eq. 2 summed over mapper training examples). We tune the hyperparameters of the regularizer and the loss function separately for each task, using a task-specific development set. This gives us additional flexibility to map the embeddings for the domain of interest, especially when the parser training data comes from a particular domain (e.g., newswire) and we want to use the parser on a new domain (e.g., email). We also tried dropout-based regularization (Srivastava et al., 2014) for the non-linear layer but did not see any significant improvement."}, {"heading": "2.5 Mapper-Parser Thresholds", "text": "Certain words in the parser training data T are very infrequent, which may lead to inferior task-specific embeddings eti learned by the parser. We want our mapper function to be learned on high-quality tasktrained embeddings. After learning a strong mapping function, we can use it to remap the inferior task-trained embeddings.\nWe thus consider several frequency thresholds that control which word embeddings to use to train the mapper and which to map at test time. Below are the specific thresholds that we consider:\nMapper-training Threshold (\u03c4t) The mapper is trained only on embedding pairs for words seen at least \u03c4t times in the training data T .\nMapping Threshold (\u03c4m) For test-time inference, the mapper will map any word whose count in T is less than \u03c4m. That is, we discard parser-trained embeddings eti of these infrequent words and use our mapper to map the initial embeddings eoi instead.\nParser Threshold (\u03c4p) While training the parser, for words that appear fewer than \u03c4p times in T , the\nparser replaces them with the \u2018unknown\u2019 embedding. Thus, no parser-trained embeddings will be learned for these words.\nIn our experiments, we explore a small set of values from this large space of possible threshold combinations (detailed below). We consider only relatively small values for the mapper-training (\u03c4t) and parser thresholds (\u03c4p) because as we increase them, the number of training examples for the mapper decreases, making it harder to learn an accurate mapping function."}, {"heading": "3 Related Work", "text": "The most common approach to resolving unseen words is to replace them with a special unknown word token (S\u00f8gaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011). The representation for the unknown token is either learned specifically or computed from a selection of rare words, for example by averaging their embedding vectors. This approach could be problematic since all unseen words are mapped to the same vector, irrespective of their syntactic or semantic categories.\nThere are several threads of prior work using character-level information for unseen word forms. Some combine unsupervised morphological analysis with compositional neural network architectures (Luong et al., 2013; Botha and Blunsom, 2014). Ling et al. (2015) and Ballesteros et al. (2015) use long short-term memory recurrent neural networks to embed character sequences. Others use convolutional neural networks on character streams (Labeau et al., 2015; Kim et al., 2015; Zhang and LeCun, 2015). Huang and Harper (2009; 2011) use a heuristic based on emission probabilities of individual characters in unknown words.\nThere is also a great deal of work using morphological information for rare or unseen words (Candito and Crabbe\u0301, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010). Other work has focused on using contextual information, such as using n-gram based sequence models or web data (Bansal and Klein, 2011). Keller and Lapata (2003) use the web to obtain\nfrequency information for unseen words. Dyer et al. (2015) represent each word using an embedding learned during parser training concatenated with a fixed embedding from a neural language model, which provides a larger vocabulary. While they still use an unknown word token for singletons, the pretrained embeddings from the neural language model provide additional information about the unknown word. Other work has also found improvements by combining pretrained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014). Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015).\nClosely related to our approach is that of Tafforeau et al. (2015), which also tries to map an initial, unsupervised word embedding space to the word embedding space learned during supervised training. Their method generates updated embeddings for unseen words by combining the embeddings of their k nearest neighbors. In Section 5, we show that our approach outperforms this kNN approach. Another related technique proposed by Kiros et al. (2015) learns a linear mapping from an initial embedding space to their encoder\u2019s vocabulary space by solving an unregularized linear regression problem. Our approach differs in that it can learn a non-linear mapping, and we also learn separate mappings for each domain of interest, tuning the mapper for each domain. We also empirically evaluate the effect of performing this mapping, showing statistically significant improvements. To our knowledge, these are the only approaches that handle unseen words by explicitly mapping initial representations to a task-trained space.\nOur work is also somewhat related to domain adaptation for dependency parsing, which has been extensively studied in recent years (McDonald and Nivre, 2007; Nilsson et al., 2007). The goal of this task is to adapt an existing parser to a target domain with little or no annotated data. Previous work has used co-training (Cohen et al., 2012), word distribution features (Koo et al., 2008; Bansal et al., 2014; Weiss et al., 2015), and selftraining (McClosky et al., 2006).\nOur simple approach learns to directly map initial word-level embeddings to the task-trained embedding space. No character-level compositional model or morphological information is needed. Since the initial embeddings are obtained from a large corpus in an unsupervised manner, data sparseness issues can be mitigated by enlarging this corpus. Tuning the hyperparameters of our mapper on small domain-specific development sets helps us to adapt the transformation to the target domain."}, {"heading": "4 Experimental Setup", "text": "In this section, we describe our primary parsing setup, embeddings, and datasets. We also describe the setup for a downstream task: sentiment analysis using a neural network model based on dependency trees. Finally, we discuss the settings for the mapper."}, {"heading": "4.1 Dependency Parser", "text": "We use the feed-forward neural network dependency parser of Chen and Manning (2014). In all our experiments (unless stated otherwise), we use the default arc-standard parsing configuration and hyperparameter settings. For evaluation, we compute the percentage of words that get the correct head, reporting both unlabeled attachment score (UAS) and labeled attachment score (LAS). LAS additionally requires the predicted dependency label to be correct. To measure statistical significance, we use a bootstrap test (Efron and Tibshirani, 1986) with 100K samples."}, {"heading": "4.2 Pre-Trained Word Embeddings", "text": "We use the 100-dimensional GloVe word embeddings from Pennington et al. (2014). These were trained on Wikipedia 2014 and the Gigaword v5 corpus and have a vocabulary size of approximately 400,000.1 We have also experimented with the downloadable 50-dimensional SENNA embeddings from Collobert et al. (2011) and with word2vec (Mikolov et al., 2013) embeddings that we trained ourselves; in preliminary experiments the GloVe embeddings performed best, so we use them for all experiments below.\n1 http://www-nlp.stanford.edu/data/glove.6B.100d.txt.gz"}, {"heading": "4.3 Datasets", "text": "We consider a number of datasets with varying rates of OOTV words. We define the OOTV rate (or, equivalently, the unseen rate) of a dataset as the percentage of the vocabulary (types) of words occurring in the set that were not seen in training.\nWall Street Journal (WSJ) and OntoNotesWSJ We conduct experiments on the Wall Street Journal portion of the English Penn Treebank dataset (Marcus et al., 1993). We follow the standard splits: sections 2-21 for training, section 22 for validation, and section 23 for testing. We convert the original phrase structure trees into dependency trees using Stanford Basic Dependencies (De Marneffe and Manning, 2008) in the Stanford Dependency Parser. The POS tags are obtained using the Stanford POS tagger (Toutanova et al., 2003) in a 10-fold jackknifing setup on the training data (achieving an accuracy of 96.96%). The OOTV rate in the development and test sets is approximately 2-3%.\nWe also conduct experiments on the OntoNotes 4.0 dataset (which we denote OntoNotes-WSJ). This dataset contains the same sentences as the WSJ corpus (and we use the same data splits), but has significantly different annotations. The OntoNotes-WSJ training data is used for the Web Treebank test experiments. We perform the same pre-processing steps as for the WSJ dataset.\nWeb Treebank We expect our mapper to be most effective when parsing held-out data with many unseen words. This often happens when the held-out data is drawn from a different distribution than the training data. For example, when training a parser on newswire and testing on web data, many errors occur due to differing patterns of syntactic usage and unseen words (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014).\nWe explore this setting by training our parser on OntoNotes-WSJ and testing on the Web Treebank (Petrov and McDonald, 2012), which includes five domains: answers, email, newsgroups, reviews, and weblogs. Each domain contains approximately 2000-4000 manually annotated syntactic parse trees in the OntoNotes 4.0 style. As before, we convert\nthe phrase structure trees to dependency trees using Stanford Basic Dependencies. The parser and the mapper hyperparameters were tuned separately on the development set for each domain. The test and development sets for each domain contain approximately 1000-2500 sentences. We use our mapper to map unseen words in the development and test sets in each domain. The unseen rate is typically 6- 10% in the domains of the Web Treebank. We train a separate mapper for each domain, tuning mapper hyperparameters separately for each domain using the development sets. In this way, we obtain some of the benefits of domain adaptation for each target domain.\nSwitchboard Speech Corpus The NXT Switchboard speech corpus (Calhoun et al., 2010) contains annotated parses of spoken telephone conversations. We obtain ground truth dependencies from phrase structure trees using the Stanford converter as above, as also done by Honnibal and Johnson (2014). We perform their other preprocessing steps of lowercasing the text, removing punctuation, and removing partial utterances and one-token sentences. Since the current version of the Stanford parser cannot perform non-monotonic parsing,2 we also remove disfluent utterances in such a way that we get a purely non-disfluent speech dataset. We use the standard train/development/test splits of Charniak and Johnson (2001).\nDownstream Task: Sentiment Analysis with Dependency Tree LSTMs We also perform experiments to analyze the effects of embedding mapping on a downstream task, in this case sentiment analysis using the Stanford Sentiment Treebank (Socher et al., 2013). We use the dependency tree long short-term memory network (Tree-LSTM) proposed by Tai et al. (2015), simply replacing their default dependency parser with our version that maps unseen words. The dependency parser is trained on the WSJ corpus and mapped using the WSJ development set. We use the same mapper that was optimized for the WSJ development set, with-\n2The arc-standard algorithm, in the Stanford parser, is a monotonic parsing algorithm where once an action has been performed, subsequent actions must be consistent with it (Honnibal and Johnson, 2014). This does not work well with disfluent utterances.\nout further hyperparameter tuning for the mapper. For the Tree-LSTM model, we use the same hyperparameter tuning as described in Tai et al. (2015). We use the standard train/development/test splits of 6820/872/1821 sentences for the binary classification task and 8544/1101/2210 for the fine-grained task."}, {"heading": "4.4 Mapper Settings and Hyperparameters", "text": "The initial embeddings given to the mapper are the same as the initial embeddings given to the parser. These are the 100-dimensional GloVe embeddings mentioned above. The output dimensionality of the mapper is also fixed to 100. All model parameters of the mappers are initialized to zero. We set the dimensionality of the non-linear layer to 400 across all experiments. The model parameters are optimized by maximizing the weighted multiple-loss objective using L-BFGS with elastic-net regularization (Section 2). The hyperparameters include the relative weight of the two objective terms (\u03b1) and the regularization constants (\u03bb1, \u03bb2). For \u03b1, we search over values in {0, 0.1, 0.2, . . . , 1}. For each of \u03bb1 and \u03bb2, we consider values in {10\u22121, 10\u22122, . . . , 10\u22129, 0}. The hyperparameters are tuned via grid search to maximize the UAS on the development set."}, {"heading": "5 Results and Analysis", "text": ""}, {"heading": "5.1 Results on WSJ, OntoNotes, and Switchboard", "text": "The upper half of Table 1 shows our main test results on WSJ, OntoNotes, and Switchboard, the lowOOTV rate datasets. Due to the small initial OOTV rates (<3%), we only see modest gains of 0.3-0.4% in UAS, with statistical significance at p < 0.05 for WSJ and OntoNotes and p < 0.07 for Switchboard. The initial OOTV rates are cut in half by our mapper, with the remaining unknown words largely being numerical strings and misspellings.3 When only considering test sentences containing OOTV words (the row labeled \u201cOOTV subset\u201d), the gains are significantly larger (0.5-0.8% UAS at p < 0.05).\n3We could potentially train the initial embeddings on a larger corpus or use heuristics to convert unknown numbers and misspellings to forms contained in our initial embeddings."}, {"heading": "5.2 Results on Web Treebank", "text": "The lower half of Table 1 shows our main test results on the Web Treebank\u2019s five domains, the highOOTV rate datasets. As expected, the mapper has a much larger impact when parsing these out-ofdomain datasets with high OOTV word rates.4\nThe OOTV rate reduction is much larger than for the WSJ-style datasets, and the parsing improvements (UAS and LAS) are statistically significant at p < 0.05. On subsets containing at least one OOTV word (that also has an initial embedding), we see an average gain of 1.14% UAS (see row labeled \u201cOOTV subset\u201d). In this case, all improvements are statistically significant at p < 0.02. We observe that the relative reduction in OOTV% for the Web Treebanks is larger than for the WSJ, OntoNotes, or Switchboard datasets. In particular, we are able to reduce the OOTV% by 71-95% relative. We also see the intuitive trend that larger relative reductions in OOTV rate correlate with larger accuracy improvements."}, {"heading": "5.3 Downstream Results", "text": "We now report results using the Dependency TreeLSTM of Tai et al. (2015) for sentiment analysis on the Stanford Sentiment Treebank. We consider both the binary (positive/negative) and fine-grained classification tasks ({very negative, negative, neutral, positive, and very positive}). We use the implementation provided by Tai et al. (2015), changing only the dependency parses that are fed to their model. The sentiment dataset contains approximately 25% OOTV words in the training set vocabulary, 5% in the development set vocabulary, and 9% in the test set vocabulary. We map unseen words using the mapper tuned on the WSJ development set. We use the same Dependency Tree-LSTM experimental settings as Tai et al. Results are shown in Table 2. We improve upon the original accuracies in both binary and fine-grained classification. We also reduce the OOTV rate from 25% in the training set vocabulary to about 6%, and from 9% in the test set vocabulary down to 4%.\n4As stated above, we train the parser on the OntoNotes dataset, but tune mapper hyperparameters to maximize parsing performance on each development section of the Web Treebank\u2019s five domains. We then map the OOTV word vectors on each test set domain using the learned mapper for that domain."}, {"heading": "5.4 Effect of Thresholds", "text": "We also experimented with different values for the thresholds described in Section 2. For the mapping threshold \u03c4m, mapper-training threshold \u03c4t, and parser threshold \u03c4p, we consider the following four settings:\nt1 : \u03c4m = \u03c4t = \u03c4p = 1\nt3 : \u03c4m = \u03c4t = \u03c4p = 3\nt5 : \u03c4m = \u03c4t = \u03c4p = 5\nt\u221e : \u03c4m = \u221e, \u03c4p = \u03c4t = 5\nUsing \u03c4m = \u221e corresponds to mapping all words at test time, even words that we have seen many times in the training data and learned task-specific embeddings for.\nWe report the average development set UAS over all Web Treebank domains in Table 3. We see that t3 performs best, though settings t1 and t5 also improve over the baseline. At threshold t3 we have ap-\nproximately 20,000 examples for training the mapper, while at threshold t5 we have only about 10,000 examples. We see a performance drop at t\u221e, so it appears better to directly use the task-specific embeddings for words that appear frequently in the training data. In other results reported in this paper, we used t3 for the Web Treebank test sets and t1 for the rest."}, {"heading": "5.5 Effect of Weighted Multi-Loss Objective", "text": "We analyzed the results when varying \u03b1, which balances between the two components of the mapper\u2019s multi-loss objective function. We found that, for all domains except Answers, the best results are obtained with some \u03b1 between 0 and 1. The optimal values outperformed the cases with \u03b1 = 0 and \u03b1 = 1 by 0.1-0.3% UAS absolute. However, on the Answers domain, the best performance was achieved with \u03b1 = 0; i.e., the mapper preferred mean squared error. For other domains, the optimal \u03b1 tended to be within the range [0.3, 0.7]."}, {"heading": "5.6 Comparison with Related Work", "text": "We compare to the approach presented by Tafforeau et al. (2015). They propose to refine embeddings for unseen words based on the relative shifts of their k nearest neighbors in the original embeddings space. Specifically, they define \u201cartificial refinement\u201d as:\n\u03c6r(t) = \u03c6o(t) + K \u2211\nk=1\n\u03b1k(\u03c6r(nk)\u2212 \u03c6o(nk)) (3)\nwhere \u03c6r(.) is the vector in the refined embedding space and \u03c6o(.) is the vector in the original embedding space. They define \u03b1k to be proportional to the cosine similarity between the target unseen word (t) and neighbor (nk):\n\u03b1k = s(t, nk) = \u03c6o(t).\u03c6o(nk)\n|\u03c6(t)||\u03c6o(nk)|\nTable 4 shows the average performance of the models over the development sets of the Web Treebank. On average, our mapper outperforms the kNN approach (k = 3)."}, {"heading": "5.7 Dependency Parsing Examples", "text": "In Figure 2, we show two sentences: an instance where the mapper helps and another where the mapper hurts the parsing performance.5 In the first sentence (Figure 2a), the parsing model has not seen the word \u2018attempted\u2019 during training. Note that the sentence contains 3 verbs: \u2018attempted\u2019, \u2018adopt\u2019, and \u2018was\u2019. Even with the POS tags, the parser was unable to get the correct dependency attachment. After mapping, the parser correctly makes \u2018attempted\u2019 the root and gets the correct arcs and the correct tree. The 3 nearest neighbors of \u2018attempted\u2019 in the mapped embedding space are \u2018attempting\u2019, \u2018tried\u2019,\n5Sentences in Figure 2 are taken from the development portion of the Answers domain from the Web Treebank.\nand \u2018attempt\u2019. We also see here that a single unseen word can lead to multiple errors in the parse.\nIn the second example (Figure 2b), the default model assigns the correct arcs using the POS information even though it has not seen the word \u2018google\u2019. However, using the mapped representation for \u2018google\u2019, the parser makes errors. The 3- nearest neighbors for \u2018google\u2019 in the mapped space are \u2018damned\u2019, \u2018look\u2019, and \u2018hash\u2019. We hypothesize that the mapper has mapped this noun instance of \u2018google\u2019 to be closer to verbs instead of nouns, which would explain the incorrect attachment."}, {"heading": "5.8 Analyzing Mapped Representations", "text": "To understand the mapped embedding space, we use t-SNE (Van der Maaten and Hinton, 2008) to visualize a small subset of embeddings. In Figure 3, we plot the initial embeddings, the parser-trained embeddings, and finally the mapped embeddings. We include four unseen words (shown in caps): \u2018horrible\u2019, \u2018poor\u2019, \u2018marvelous\u2019, and \u2018magnificent\u2019. In Figure 3a and Figure 3b, the embeddings for the unseen words are identical (even though t-SNE places them in different places when producing its projection). In Figure 3c, we observe that the mapper has placed the unseen words within appropriate areas of the space with respect to similarity with the seen words. We contrast this with Figure 3b, in which the unseen words appear to be within a different region of the space from all seen words."}, {"heading": "6 Conclusion", "text": "We have described a simple method to resolve unseen words when training supervised models that learn task-specific word embeddings: a feedforward neural network that maps initial embeddings to the task-specific embedding space. We demonstrated significant improvements in dependency parsing accuracy across several domains, as well as improvements on a downstream task. Our approach is simple, effective, and applicable to many other settings, both inside and outside NLP."}], "references": [{"title": "Handling unknown words in statistical latent-variable parsing models for arabic, english and french", "author": ["Mohammed Attia", "Jennifer Foster", "Deirdre Hogan", "Joseph Le Roux", "Lamia Tounsi", "Josef Van Genabith."], "venue": "Proceedings of the NAACL HLT 2010 First", "citeRegEx": "Attia et al\\.,? 2010", "shortCiteRegEx": "Attia et al\\.", "year": 2010}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349\u2013359,", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Web-scale features for full-scale parsing", "author": ["Mohit Bansal", "Dan Klein."], "venue": "Proceedings of ACL.", "citeRegEx": "Bansal and Klein.,? 2011", "shortCiteRegEx": "Bansal and Klein.", "year": 2011}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Mohit Bansal", "Kevin Gimpel", "Karen Livescu."], "venue": "Proceedings of ACL.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Jan A. Botha", "Phil Blunsom."], "venue": "International Conference on Machine Learning (ICML).", "citeRegEx": "Botha and Blunsom.,? 2014", "shortCiteRegEx": "Botha and Blunsom.", "year": 2014}, {"title": "The NXT-format Switchboard Corpus: A rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue", "author": ["Sasha Calhoun", "Jean Carletta", "Jason M. Brenier", "Neil Mayo", "Dan Jurafsky", "Mark Steedman", "David Beaver."], "venue": "Lang.", "citeRegEx": "Calhoun et al\\.,? 2010", "shortCiteRegEx": "Calhoun et al\\.", "year": 2010}, {"title": "Improving generative statistical parsing with semi-supervised word clustering", "author": ["Marie Candito", "Beno\u0131\u0302t Crabb\u00e9"], "venue": "In Proceedings of the 11th International Conference on Parsing Technologies,", "citeRegEx": "Candito and Crabb\u00e9.,? \\Q2009\\E", "shortCiteRegEx": "Candito and Crabb\u00e9.", "year": 2009}, {"title": "Edit detection and parsing for transcribed speech", "author": ["Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the Second Conference of the North American chapter of the Association for Computational Linguistics (NAACL \u201901).", "citeRegEx": "Charniak and Johnson.,? 2001", "shortCiteRegEx": "Charniak and Johnson.", "year": 2001}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Variable-length word encodings for neural translation models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2088\u2013 2093, Lisbon, Portugal, September. Association for", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "Bootstrap methods for", "author": ["B. Efron", "R. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani.,? \\Q1986\\E", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1986}, {"title": "Remoov: A tool for online", "author": ["20\u201325. Nizar Habash"], "venue": null, "citeRegEx": "Habash.,? \\Q2009\\E", "shortCiteRegEx": "Habash.", "year": 2009}, {"title": "On using very large target", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2015\\E", "shortCiteRegEx": "Bengio.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 655\u2013665, Balti-", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Using the web to obtain frequencies for unseen bigrams", "author": ["Frank Keller", "Mirella Lapata."], "venue": "Computational linguistics, 29(3):459\u2013484.", "citeRegEx": "Keller and Lapata.,? 2003", "shortCiteRegEx": "Keller and Lapata.", "year": 2003}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "CoRR, abs/1508.06615.", "citeRegEx": "Kim et al\\.,? 2015", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar, October. Association for Computational Linguistics.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S Zemel", "Antonio Torralba", "Raquel Urtasun", "Sanja Fidler."], "venue": "arXiv preprint arXiv:1506.06726.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "A dependency parser for tweets", "author": ["Lingpeng Kong", "Nathan Schneider", "Swabha Swayamdipta", "Archna Bhatia", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Kong et al\\.,? 2014", "shortCiteRegEx": "Kong et al\\.", "year": 2014}, {"title": "Simple semisupervised dependency parsing", "author": ["Terry Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of ACL08: HLT, page 595603, Columbus, Ohio, June. Association for Computational Linguistics.", "citeRegEx": "Koo et al\\.,? 2008", "shortCiteRegEx": "Koo et al\\.", "year": 2008}, {"title": "Non-lexical neural architecture for fine-grained pos tagging", "author": ["Matthieu Labeau", "Kevin L\u00f6ser", "Alexandre Allauzen."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 232\u2013237, Lisbon, Portugal, September. As-", "citeRegEx": "Labeau et al\\.,? 2015", "shortCiteRegEx": "Labeau et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "R\u00e1mon Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal."], "venue": "Math. Programming, 45(3, (Ser. B)):503\u2013528.", "citeRegEx": "Liu and Nocedal.,? 1989", "shortCiteRegEx": "Liu and Nocedal.", "year": 1989}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Thang Luong", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104\u2013113, Sofia, Bul-", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Improving arabic dependency parsing with lexical and inflectional morphological features", "author": ["Yuval Marton", "Nizar Habash", "Owen Rambow."], "venue": "Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages", "citeRegEx": "Marton et al\\.,? 2010", "shortCiteRegEx": "Marton et al\\.", "year": 2010}, {"title": "Reranking and self-training for parser adaptation", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational", "citeRegEx": "McClosky et al\\.,? 2006", "shortCiteRegEx": "McClosky et al\\.", "year": 2006}, {"title": "Characterizing the errors of data-driven dependency parsing models", "author": ["Ryan T McDonald", "Joakim Nivre."], "venue": "EMNLP-CoNLL, pages 122\u2013131.", "citeRegEx": "McDonald and Nivre.,? 2007", "shortCiteRegEx": "McDonald and Nivre.", "year": 2007}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships", "author": ["Silvia Necsulescu", "Sara Mendes", "David Jurgens", "N\u00faria Bel", "Roberto Navigli."], "venue": "Proceedings of the Fourth Joint Conference on Lexical and Computa-", "citeRegEx": "Necsulescu et al\\.,? 2015", "shortCiteRegEx": "Necsulescu et al\\.", "year": 2015}, {"title": "On optimization methods for deep learning", "author": ["Jiquan Ngiam", "Adam Coates", "Ahbik Lahiri", "Bobby Prochnow", "Quoc V. Le", "Andrew Y. Ng."], "venue": "Proceed-", "citeRegEx": "Ngiam et al\\.,? 2011", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "The conll 2007 shared task on dependency parsing", "author": ["Jens Nilsson", "Sebastian Riedel", "Deniz Yuret."], "venue": "Proceedings of the CoNLL shared task session of EMNLP-CoNLL, pages 915\u2013932. sn.", "citeRegEx": "Nilsson et al\\.,? 2007", "shortCiteRegEx": "Nilsson et al\\.", "year": 2007}, {"title": "Global belief recursive neural networks", "author": ["Romain Paulus", "Richard Socher", "Christopher D Manning."], "venue": "Advances in Neural Information Processing Systems, pages 2888\u20132896.", "citeRegEx": "Paulus et al\\.,? 2014", "shortCiteRegEx": "Paulus et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar, Octo-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Overview of the 2012 shared task on parsing the web", "author": ["Slav Petrov", "Ryan McDonald."], "venue": "Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).", "citeRegEx": "Petrov and McDonald.,? 2012", "shortCiteRegEx": "Petrov and McDonald.", "year": 2012}, {"title": "Uptraining for accurate deterministic question parsing", "author": ["Slav Petrov", "Pi-Chuan Chang", "Michael Ringgaard", "Hiyan Alshawi."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705\u2013713. Association for", "citeRegEx": "Petrov et al\\.,? 2010", "shortCiteRegEx": "Petrov et al\\.", "year": 2010}, {"title": "Big data small data, in domain out-of domain, known word unknown word: The impact of word representation on sequence labelling tasks", "author": ["Lizhen Qu", "Gabriela Ferraro", "Liyuan Zhou", "Weiwei Hou", "Nathan Schneider", "Timothy Baldwin."], "venue": "arXiv preprint", "citeRegEx": "Qu et al\\.,? 2015", "shortCiteRegEx": "Qu et al\\.", "year": 2015}, {"title": "Lemmatization and lexicalized statistical parsing of morphologically-rich languages: the case of french", "author": ["Djam\u00e9 Seddah", "Grzegorz Chrupa\u0142a", "Ozlem Cetinoglu", "Josef van Genabith", "Marie Candito."], "venue": "Proceedings of the NAACL HLT 2010 First Workshop", "citeRegEx": "Seddah et al\\.,? 2010", "shortCiteRegEx": "Seddah et al\\.", "year": 2010}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts."], "venue": "Proceedings of the 2013 Conference on Empirical Meth-", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Robust learning in random subspaces: Equipping NLP for OOV effects", "author": ["Anders S\u00f8gaard", "Anders Johannsen."], "venue": "Proceedings of COLING 2012: Posters, Mumbai, India, December.", "citeRegEx": "S\u00f8gaard and Johannsen.,? 2012", "shortCiteRegEx": "S\u00f8gaard and Johannsen.", "year": 2012}, {"title": "Dependency parsing for", "author": ["William W Cohen"], "venue": null, "citeRegEx": "Cohen.,? \\Q2014\\E", "shortCiteRegEx": "Cohen.", "year": 2014}, {"title": "Structured training for neu", "author": ["Slav Petrov"], "venue": null, "citeRegEx": "Petrov.,? \\Q2015\\E", "shortCiteRegEx": "Petrov.", "year": 2015}], "referenceMentions": [{"referenceID": 36, "context": "In many NLP tasks, state-of-the-art systems achieve very good performance, but only when restricted to standard and heavily edited datasets (Petrov et al., 2010).", "startOffset": 140, "endOffset": 161}, {"referenceID": 35, "context": "For example, while state-of-the-art accuracies exceed 97% for part-ofspeech tagging and 90% for dependency parsing, performance on non-standard, real-world datasets is substantially worse, dropping by nearly 10% absolute (Petrov and McDonald, 2012).", "startOffset": 221, "endOffset": 248}, {"referenceID": 3, "context": "Recently, continuous vector word representations, or embeddings, have shown promise in a variety of NLP tasks (Turian et al., 2010; Collobert et al., 2011; Anderson et al., 2013; Bansal et al., 2014).", "startOffset": 110, "endOffset": 199}, {"referenceID": 30, "context": "Using embeddings as features in NLP systems can help counter the effects of data sparsity (Necsulescu et al., 2015).", "startOffset": 90, "endOffset": 115}, {"referenceID": 3, "context": "However, the quality of such embeddings has been found to be heavily task-dependent (Bansal et al., 2014).", "startOffset": 84, "endOffset": 105}, {"referenceID": 13, "context": "There is a great deal of work on updating embeddings during supervised training to make them more task-specific, whether through back-propagation or other techniques (Kalchbrenner et al., 2014; Qu et al., 2015; Chen and Manning, 2014).", "startOffset": 166, "endOffset": 234}, {"referenceID": 37, "context": "There is a great deal of work on updating embeddings during supervised training to make them more task-specific, whether through back-propagation or other techniques (Kalchbrenner et al., 2014; Qu et al., 2015; Chen and Manning, 2014).", "startOffset": 166, "endOffset": 234}, {"referenceID": 8, "context": "There is a great deal of work on updating embeddings during supervised training to make them more task-specific, whether through back-propagation or other techniques (Kalchbrenner et al., 2014; Qu et al., 2015; Chen and Manning, 2014).", "startOffset": 166, "endOffset": 234}, {"referenceID": 40, "context": "In the latter case, at test time, most trained systems either fall back to some generic, single representation for all unknown words or use the initial representation (typically derived from unlabeled data) (S\u00f8gaard and Johannsen, 2012; Collobert et al., 2011).", "startOffset": 207, "endOffset": 260}, {"referenceID": 8, "context": "In this work, we use the Stanford neural dependency parser (Chen and Manning, 2014).", "startOffset": 59, "endOffset": 83}, {"referenceID": 22, "context": "For optimization, we use batch limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989).", "startOffset": 60, "endOffset": 83}, {"referenceID": 31, "context": "In preliminary experiments comparing with stochastic optimization, we found L-BFGS to be more stable to train and easier to check for convergence (as has recently been found in other settings as well (Ngiam et al., 2011)).", "startOffset": 200, "endOffset": 220}, {"referenceID": 22, "context": "We use elastic net regularization (Liu and Nocedal, 1989), which linearly combines l1 and l2 penalties on the parameters to control the capacity of the mapper function.", "startOffset": 34, "endOffset": 57}, {"referenceID": 40, "context": "The most common approach to resolving unseen words is to replace them with a special unknown word token (S\u00f8gaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011).", "startOffset": 104, "endOffset": 181}, {"referenceID": 8, "context": "The most common approach to resolving unseen words is to replace them with a special unknown word token (S\u00f8gaard and Johannsen, 2012; Chen and Manning, 2014; Collobert et al., 2011).", "startOffset": 104, "endOffset": 181}, {"referenceID": 23, "context": "Some combine unsupervised morphological analysis with compositional neural network architectures (Luong et al., 2013; Botha and Blunsom, 2014).", "startOffset": 97, "endOffset": 142}, {"referenceID": 4, "context": "Some combine unsupervised morphological analysis with compositional neural network architectures (Luong et al., 2013; Botha and Blunsom, 2014).", "startOffset": 97, "endOffset": 142}, {"referenceID": 20, "context": "Others use convolutional neural networks on character streams (Labeau et al., 2015; Kim et al., 2015; Zhang and LeCun, 2015).", "startOffset": 62, "endOffset": 124}, {"referenceID": 15, "context": "Others use convolutional neural networks on character streams (Labeau et al., 2015; Kim et al., 2015; Zhang and LeCun, 2015).", "startOffset": 62, "endOffset": 124}, {"referenceID": 3, "context": ", 2013; Botha and Blunsom, 2014). Ling et al. (2015) and Ballesteros et al.", "startOffset": 8, "endOffset": 53}, {"referenceID": 1, "context": "(2015) and Ballesteros et al. (2015) use long short-term memory recurrent neural networks to embed character sequences.", "startOffset": 11, "endOffset": 37}, {"referenceID": 6, "context": "There is also a great deal of work using morphological information for rare or unseen words (Candito and Crabb\u00e9, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010).", "startOffset": 92, "endOffset": 194}, {"referenceID": 11, "context": "There is also a great deal of work using morphological information for rare or unseen words (Candito and Crabb\u00e9, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010).", "startOffset": 92, "endOffset": 194}, {"referenceID": 26, "context": "There is also a great deal of work using morphological information for rare or unseen words (Candito and Crabb\u00e9, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010).", "startOffset": 92, "endOffset": 194}, {"referenceID": 38, "context": "There is also a great deal of work using morphological information for rare or unseen words (Candito and Crabb\u00e9, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010).", "startOffset": 92, "endOffset": 194}, {"referenceID": 0, "context": "There is also a great deal of work using morphological information for rare or unseen words (Candito and Crabb\u00e9, 2009; Habash, 2009; Marton et al., 2010; Seddah et al., 2010; Attia et al., 2010).", "startOffset": 92, "endOffset": 194}, {"referenceID": 2, "context": "Other work has focused on using contextual information, such as using n-gram based sequence models or web data (Bansal and Klein, 2011).", "startOffset": 111, "endOffset": 135}, {"referenceID": 0, "context": ", 2010; Attia et al., 2010). Other work has focused on using contextual information, such as using n-gram based sequence models or web data (Bansal and Klein, 2011). Keller and Lapata (2003) use the web to obtain", "startOffset": 8, "endOffset": 191}, {"referenceID": 16, "context": "Other work has also found improvements by combining pretrained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014).", "startOffset": 110, "endOffset": 142}, {"referenceID": 33, "context": "Other work has also found improvements by combining pretrained, fixed embeddings with task-trained embeddings (Kim, 2014; Paulus et al., 2014).", "startOffset": 110, "endOffset": 142}, {"referenceID": 24, "context": "Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015).", "startOffset": 154, "endOffset": 219}, {"referenceID": 9, "context": "Also relevant are approaches developed specifically to handle large target vocabularies (including many rare words) in neural machine translation systems (Jean et al., 2015; Luong et al., 2015; Chitnis and DeNero, 2015).", "startOffset": 154, "endOffset": 219}, {"referenceID": 9, "context": ", 2015; Chitnis and DeNero, 2015). Closely related to our approach is that of Tafforeau et al. (2015), which also tries to map an initial, unsupervised word embedding space to the word embedding space learned during supervised training.", "startOffset": 8, "endOffset": 102}, {"referenceID": 9, "context": ", 2015; Chitnis and DeNero, 2015). Closely related to our approach is that of Tafforeau et al. (2015), which also tries to map an initial, unsupervised word embedding space to the word embedding space learned during supervised training. Their method generates updated embeddings for unseen words by combining the embeddings of their k nearest neighbors. In Section 5, we show that our approach outperforms this kNN approach. Another related technique proposed by Kiros et al. (2015) learns a linear mapping from an initial embedding space to their encoder\u2019s vocabulary space by solving an unregularized linear regression problem.", "startOffset": 8, "endOffset": 483}, {"referenceID": 28, "context": "Our work is also somewhat related to domain adaptation for dependency parsing, which has been extensively studied in recent years (McDonald and Nivre, 2007; Nilsson et al., 2007).", "startOffset": 130, "endOffset": 178}, {"referenceID": 32, "context": "Our work is also somewhat related to domain adaptation for dependency parsing, which has been extensively studied in recent years (McDonald and Nivre, 2007; Nilsson et al., 2007).", "startOffset": 130, "endOffset": 178}, {"referenceID": 19, "context": ", 2012), word distribution features (Koo et al., 2008; Bansal et al., 2014; Weiss et al., 2015), and selftraining (McClosky et al.", "startOffset": 36, "endOffset": 95}, {"referenceID": 3, "context": ", 2012), word distribution features (Koo et al., 2008; Bansal et al., 2014; Weiss et al., 2015), and selftraining (McClosky et al.", "startOffset": 36, "endOffset": 95}, {"referenceID": 27, "context": ", 2015), and selftraining (McClosky et al., 2006).", "startOffset": 26, "endOffset": 49}, {"referenceID": 10, "context": "To measure statistical significance, we use a bootstrap test (Efron and Tibshirani, 1986) with 100K samples.", "startOffset": 61, "endOffset": 89}, {"referenceID": 8, "context": "We use the feed-forward neural network dependency parser of Chen and Manning (2014). In all our experiments (unless stated otherwise), we use the default arc-standard parsing configuration and hyperparameter settings.", "startOffset": 60, "endOffset": 84}, {"referenceID": 29, "context": "(2011) and with word2vec (Mikolov et al., 2013) embeddings that we trained ourselves; in preliminary experiments the GloVe embeddings performed best, so we use them for all experiments below.", "startOffset": 25, "endOffset": 47}, {"referenceID": 33, "context": "We use the 100-dimensional GloVe word embeddings from Pennington et al. (2014). These were trained on Wikipedia 2014 and the Gigaword v5 corpus and have a vocabulary size of approximately 400,000.", "startOffset": 54, "endOffset": 79}, {"referenceID": 33, "context": "We use the 100-dimensional GloVe word embeddings from Pennington et al. (2014). These were trained on Wikipedia 2014 and the Gigaword v5 corpus and have a vocabulary size of approximately 400,000.1 We have also experimented with the downloadable 50-dimensional SENNA embeddings from Collobert et al. (2011) and with word2vec (Mikolov et al.", "startOffset": 54, "endOffset": 307}, {"referenceID": 25, "context": "Wall Street Journal (WSJ) and OntoNotesWSJ We conduct experiments on the Wall Street Journal portion of the English Penn Treebank dataset (Marcus et al., 1993).", "startOffset": 138, "endOffset": 159}, {"referenceID": 35, "context": "For example, when training a parser on newswire and testing on web data, many errors occur due to differing patterns of syntactic usage and unseen words (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014).", "startOffset": 153, "endOffset": 239}, {"referenceID": 18, "context": "For example, when training a parser on newswire and testing on web data, many errors occur due to differing patterns of syntactic usage and unseen words (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014).", "startOffset": 153, "endOffset": 239}, {"referenceID": 35, "context": "We explore this setting by training our parser on OntoNotes-WSJ and testing on the Web Treebank (Petrov and McDonald, 2012), which includes five domains: answers, email, newsgroups, reviews, and weblogs.", "startOffset": 96, "endOffset": 123}, {"referenceID": 5, "context": "Switchboard Speech Corpus The NXT Switchboard speech corpus (Calhoun et al., 2010) contains annotated parses of spoken telephone conversations.", "startOffset": 60, "endOffset": 82}, {"referenceID": 5, "context": "Switchboard Speech Corpus The NXT Switchboard speech corpus (Calhoun et al., 2010) contains annotated parses of spoken telephone conversations. We obtain ground truth dependencies from phrase structure trees using the Stanford converter as above, as also done by Honnibal and Johnson (2014). We perform their other preprocessing steps of lowercasing the text, removing punctuation, and removing partial utterances and one-token sentences.", "startOffset": 61, "endOffset": 291}, {"referenceID": 5, "context": "Switchboard Speech Corpus The NXT Switchboard speech corpus (Calhoun et al., 2010) contains annotated parses of spoken telephone conversations. We obtain ground truth dependencies from phrase structure trees using the Stanford converter as above, as also done by Honnibal and Johnson (2014). We perform their other preprocessing steps of lowercasing the text, removing punctuation, and removing partial utterances and one-token sentences. Since the current version of the Stanford parser cannot perform non-monotonic parsing,2 we also remove disfluent utterances in such a way that we get a purely non-disfluent speech dataset. We use the standard train/development/test splits of Charniak and Johnson (2001).", "startOffset": 61, "endOffset": 709}, {"referenceID": 39, "context": "Downstream Task: Sentiment Analysis with Dependency Tree LSTMs We also perform experiments to analyze the effects of embedding mapping on a downstream task, in this case sentiment analysis using the Stanford Sentiment Treebank (Socher et al., 2013).", "startOffset": 227, "endOffset": 248}, {"referenceID": 39, "context": "Downstream Task: Sentiment Analysis with Dependency Tree LSTMs We also perform experiments to analyze the effects of embedding mapping on a downstream task, in this case sentiment analysis using the Stanford Sentiment Treebank (Socher et al., 2013). We use the dependency tree long short-term memory network (Tree-LSTM) proposed by Tai et al. (2015), simply replacing their default dependency parser with our version that maps unseen words.", "startOffset": 228, "endOffset": 350}], "year": 2017, "abstractText": "We consider the setting in which we train a supervised model that learns task-specific word representations. We assume that we have access to some initial word representations (e.g., unsupervised embeddings), and that the supervised learning procedure updates them to task-specific representations for words contained in the training data. But what about words not contained in the supervised training data? When such unseen words are encountered at test time, they are typically represented by either their initial vectors or a single unknown vector, which often leads to errors. In this paper, we address this issue by learning to map from initial representations to task-specific ones. We present a general technique that uses a neural network mapper with a weighted multiple-loss criterion. This allows us to use the same learned model parameters at test time but now with appropriate task-specific representations for unseen words. We consider the task of dependency parsing and report improvements in performance (and reductions in out-of-vocabulary rates) across multiple domains such as news, Web, and speech. We also achieve downstream improvements on the task of parsingbased sentiment analysis.", "creator": "LaTeX with hyperref package"}}}