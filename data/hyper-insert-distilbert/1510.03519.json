{"id": "1510.03519", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2015", "title": "Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning", "abstract": "recently over there has continuously been a lot center of interest involved in learning common framing representations for multiple views instances of symbolic data. these views could belong naturally to different modalities or abstract languages. the typically, very such combined common context representations are learned from using a partial parallel narration corpus between the two semantic views ( say, 1m images data and generally their english sounding captions ). in this first work, we address also a real - world scenario theory where no direct parallel data is either available safely between two views engines of interest ( say, v1 and v2 ) separately but parallel framing data is available between each bunch of these unrelated views and becomes a second pivot view ( v3 ). next we also propose choosing a model calculated for learning a simpler common phonetic representation for v1, v2 and v3 using only grouping the parallel data available loosely between v1v3 and abbreviated v2v3. the actually proposed model is fairly generic and takes even worse works when alone there each are n views sources of interest and only provides one global pivot performance view matrix which acts as a possible bridge item between them. there are generally two specific understood downstream applications that we focus on ( would i ) uniformly transfer textual learning between diverse languages + l1, for l2,..., abbreviated ln using a fixed pivot verbal language support l. and ( ii ) cross modal access between images and a language / l1 stream using a distributed pivot language l2. both we confidently evaluate that our model using simply two datasets : ( i ) publicly declare available short multilingual ted lecture corpus text and ( ii ) download a new multilingual vernacular multimodal storytelling dataset created and released earlier as a part release of this digital work. on utilizing both these datasets, our model always outperforms using state bel of the art approaches.", "histories": [["v1", "Tue, 13 Oct 2015 03:25:18 GMT  (4707kb,D)", "http://arxiv.org/abs/1510.03519v1", "12 pages"], ["v2", "Sat, 6 Feb 2016 07:44:01 GMT  (4536kb,D)", "http://arxiv.org/abs/1510.03519v2", "12 pages"], ["v3", "Fri, 1 Jul 2016 09:01:19 GMT  (4515kb,D)", "http://arxiv.org/abs/1510.03519v3", "Published at NAACL-HLT 2016"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["janarthanan rajendran", "mitesh m khapra", "sarath chandar", "balaraman ravindran"], "accepted": true, "id": "1510.03519"}, "pdf": {"name": "1510.03519.pdf", "metadata": {"source": "CRF", "title": "Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning", "authors": ["Janarthanan Rajendran", "Mitesh M. Khapra", "Sarath Chandar", "Balaraman Ravindran"], "emails": ["rsdjjana@gmail.com", "mikhapra@in.ibm.com", "apsarathchandar@gmail.com", "ravi@cse.iitm.ac.in"], "sections": [{"heading": "1 Introduction", "text": "Truth has many faces, and data many views. The proliferation of multilingual and multimodal content online has ensured that multiple views of the same data exist. For example, it is common to find the\nsame article published in multiple languages online in multilingual news articles, multilingual wikipedia articles, etc. Such multiple views can even belong to different modalities. For example, images and their textual descriptions are two views of the same entity. Similarly, audio, video and subtitles of a movie clip are multiple views of the same entity.\nLearning common representations for such multiple views of data will help in several downstream applications. For example, learning a common representation for audio and subtitles could help in generating subtitles from a given audio. Similarly, learning a common representation for images and their textual descriptions could help in finding images which match a given textual description. Further, such common representations can also facilitate transfer learning between views. For example, a document classifier trained on one language (view) can be used to classify documents in another language by representing documents in both languages in a common subspace.\nExisting approaches to common representation learning (Ngiam et al., 2011; Klementiev et al., 2012; Chandar et al., 2013; Chandar et al., 2014; Andrew et al., 2013; Hermann and Blunsom, 2014b; Wang et al., 2015) typically require parallel data between the two views. However, in many realworld scenarios such parallel data may not be available. For example, while there are many publicly available datasets containing images and their corresponding English captions, it is very hard to find datasets containing images and their corresponding captions in French, German, Hindi, Urdu, etc. In this work, we are interested in addressing such sce-\nar X\niv :1\n51 0.\n03 51\n9v 1\n[ cs\n.C L\n] 1\n3 O\nct 2\nnarios. More specifically, we consider scenarios where we have n different views but parallel data is only available between each of these views, and a pivot view. In particular, there is no parallel data available between the non-pivot views. For example, consider the case where the two views of interest are images and French captions. Suppose, there is no direct parallel data between these two views but parallel data is available between (i) images and English captions and (ii) English and French texts. We propose to use English as a pivot view and learn a common representation for English text, French text and images.\nTo this end, we propose bridge correlational neural networks (Bridge CorrNets) which learn aligned representations across multiple views using a pivot view. We build on the work of (Chandar et al., 2015) but unlike their model, which only addresses scenarios where direct parallel data is available between two views, our model can work for n(\u22652) views even when no parallel data is available between them. Our model only requires parallel data between each of these n views and a pivot view. During training, our model maximizes the correlation between the representations of the pivot view and each of the n views. Intuitively, the pivot view ensures that similar entities across different views get mapped close to each other since all of them would be close to the corresponding entity in the pivot view.\nWe evaluate our approach using two downstream applications, viz., (i) transfer learning between multiple languages using English as the pivot language and (ii) cross modal access between images and French or German captions using English as the pivot view. For the first application, we use a publicly available corpus which consists of transcripts of TED talks in multiple languages. An extensive evaluation over 110 source-target language pairs clearly shows that we outperform the current state-of-the art approach (Hermann and Blunsom, 2014b) on this task. For the second application, we created a test dataset consisting of images and their captions in French and German in addition to the English captions which were publicly available. Even on this task of retrieving images given French/German captions (and vice versa) we do better than existing state of the art approaches."}, {"heading": "2 Related Work", "text": "Our work falls under the broad umbrella of multiview learning which has been studied for many years. In this section, we discuss some methods for multiview learning which are relevant to our work.\nCanonical Correlation Analysis (CCA) and its variants (Hotelling, 1936; Vinod, 1976; Nielsen et al., 1998; Cruz-Cano and Lee, 2014; Akaho, 2001) are the most commonly used methods for learning a common representation for two views. However, most of these models generally work with two views only. Even though there are multi-view generalizations of CCA (Tenenhaus and Tenenhaus, 2011; Luo et al., 2015), they require complex computations which makes them unsuitable for larger data sizes.\nAnother class of algorithms for multiview learning is based on Neural Networks. One of the earliest neural network based model for learning common representations was proposed in (Hsieh, 2000). Recently, there has been a renewed interest in this field and several neural network based models have been proposed. Multimodal Autoencoder (MAE) proposed in (Ngiam et al., 2011) uses an encoder/decoder architecture with a hidden layer which is shared across views. Deep Canonically Correlated Autoencoder (DCCAE) which was proposed in (Wang et al., 2015) uses a combination of Autoencoder (encoding/decoding only inside the view) and CCA. Similarly, Deep CCA (Andrew et al., 2013) uses deep neural networks to encode the views in a common space. Correlational Neural Networks (CorrNet) (Chandar et al., 2015) use an encoder/decoder approach with a correlation based regularizer to learn common representations for two views. It performs better than most of the above mentioned methods. We build on their work as discussed in the next section.\nWe now talk about Multilingual Representation Learning which is a specific subclass of Multiview Representation Learning where each view is a language. This is one of the tasks that we address in this work. An important characteristic of Multilingual Representation Learning is that subsets of features in one view are aligned with some subsets of features in the other view. For example, one or more words in one language are aligned with their trans-\nlations in another language. Even though this word level alignment exists, it is difficult to obtain training data where these alignments are already annotated. Most of the earlier work on Multilingual Representation Learning using Neural Networks relied on this word level alignment information (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013). Recently, there has been an interest in learning Multilingual Representations with only sentence level alignments (Hermann and Blunsom, 2014b; Hermann and Blunsom, 2014a; Chandar et al., 2014; Soyer et al., 2015; Gouws et al., 2015). However, except for (Hermann and Blunsom, 2014b), none of the other works mentioned above consider more than 2 languages at a time. Our model also deals with multiple languages and we compare its performance with that of (Hermann and Blunsom, 2014b).\nAnother task addressed in this work is cross modal access between images and text. This comes under MultiModal Representation Learning where each view belongs to a different modality (audio, video, image, etc). Ngiam et al. (2011) proposed an autoencoder based solution to learning common representation for audio and video. Srivastava and Salakhutdinov (2014) extended this idea to RBMs and learned common representations for image and text. Other solutions for image/text representation learning include (Zheng et al., 2014a; Zheng et al., 2014b; Socher et al., 2014). All these approaches require parallel data between the two views and do not address the problem of multimodal, multilingual learning where parallel data is available only between different views and a pivot view.\nWe now discuss some works which use a pivot language to bridge the gap between two other languages. For example, (Wu and Wang, 2007; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Nakov and Ng, 2009) use a pivot language to build or improve a MT system between two languages which do not have direct parallel data. Similar work has also been done for transitive CLIR (Ballesteros, 2000; Lehtokangas et al., 2008), pivot based transliteration and transliteration mining (Khapra et al., 2010a; Kumaran et al., 2010; Khapra et al., 2010b; Zhang et al., 2011). None of these works use neural networks but it is important to mention them here because they use the concept of a pivot language (view) which is central to our work."}, {"heading": "3 Bridge Correlational Neural Network", "text": "In this section, we describe Bridge CorrNet which is an extension of the CorrNet model proposed by (Chandar et al., 2015). They address the problem of learning common representations between two views when parallel data is available between them. We propose an extension to their model which simultaneously learns a common representation forM views when parallel data is available only between one pivot view and the remaining M \u2212 1 views.\nLet these views be denoted by V1, V2, ..., VM and let d1, d2, ..., dM be their respective dimensionalities. Let the training data be Z = {zi}Ni=1 where each training instance contains only two views, i.e., zi = (vij , v i M ) where j \u2208 {1, 2, ..,M\u22121} and M is a pivot view. To be more clear, the training data contain N1 instances for which (vi1, v i M ) are available, N2 instances for which (vi2, v i M ) are available and so on tillNM\u22121 instances for which (viM\u22121, v i M ) are available (such that N1 +N2 + ...+NM = N ). We denote each of these disjoint pairwise training sets by Z1 , Z2 to ZM\u22121 such that Z is the union of all these sets.\nAs an illustration consider the case when English, French and German texts are the three views of interest with English as the pivot view. As training data, we have N1 instances containing English and their corresponding French texts and N2 instances containing English and their corresponding German texts. We are then interested in learning a common representation for English, French and German even though we do not have any training instance containing French and their corresponding German texts.\nBridge CorrNet uses an encoder decoder architecture with a correlation based regularizer to achieve this (see Figure 1). It contains one encoder-decoder pair for each of the M views. For each view Vj , we have,\nhVj (vj) = f(Wjvj + b) (1)\nwhere f is any non-linear function such as sigmoid or tanh, Wj \u2208 Rk\u00d7dj is the encoder matrix for view Vj , b \u2208 Rk is the common bias shared by all the encoders. We also compute a hidden representation for the concatenated training instance z = (vj , vM ) using the following encoder function:\nhZ(z) = f(Wjvj +WM vM + b) (2)\nIn the remainder of this paper, whenever we drop the subscript for the encoder, then the encoder is determined by its argument. For example h(vj) means hVj (vj), h(z) means hZ(z) and so on.\nOur model also has a decoder corresponding to each view as follows:\ngVj (h) = p(W \u2032 jh+ cj) (3)\nwhere p can be any activation function,W \u2032j \u2208 Rdj\u00d7k is the decoder matrix for view Vj , cj \u2208 Rdj is the decoder bias for view Vj . We also define g(h) as simply the concatenation of [gVj (h), gVM (h)].\nIn effect, hVj (.) encodes the input vj into a hidden representation h and then gVj (.) tries to decode/reconstruct vj from this hidden representation h. Note that h can be computed using h(vj) or h(vM ). The decoder can then be trained to decode/reconstruct both vj and vM given a hidden representation computed using any one of them. More formally, we train Bridge CorrNet by minimizing the following objective function: JZ(\u03b8) = N\u2211 i=1 L(zi, g(h(zi))) + N\u2211 i=1 L(zi, g(h(vil(i))))\n+ N\u2211 i=1 L(zi, g(h(vi M )))\u2212 \u03bb corr(h(Vl(i)), h(VM ))\n(4)\nwhere l(i) = j if zi \u2208 Zj and the correlation term corr is defined as follows:\ncorr =\n\u2211N i=1(h(x\ni)\u2212 h(X))(h(yi)\u2212 h(Y ))\u221a\u2211N i=1(h(x i)\u2212 h(X))2 \u2211N i=1(h(y i)\u2212 h(Y ))2\n(5)\nNote that g(h(zi)) is the reconstruction of the input zi after passing through the encoder and decoder. L is a loss function which captures the error in this reconstruction, \u03bb is the scaling parameter to scale the last term with respect to the remaining terms, h(X) is the mean vector for the hidden representations of the first view and h(Y ) is the mean vector for the hidden representations of the second view.\nWe now explain the intuition behind each term in the objective function. The first term captures the error in reconstructing the concatenated input zi from itself. The second term captures the error in reconstructing both views given the non-pivot view, vil(i). The third term captures the error in reconstructing both views given the pivot view, vi\nM . Minimizing the\nsecond and third terms ensures that both the views can be predicted from any one view. Finally, the correlation term ensures that the network learns correlated common representations for all views. The pivot view acts as a bridge and ensures that similar entities across different views get mapped close to each other since all of them would be close to the corresponding entity in the pivot view.\nNote that unlike the objective function of CorrNet (Chandar et al., 2015), the objective function of Equation 4, is a dynamic objective function which changes with each training instance. In other words, l(i) \u2208 {1, 2, ..,M\u22121} varies for each i \u2208 {1, 2, .., N}. For efficient implementation, we propose to construct mini-batches where each minibatch will come from only one of the sets Z1 to ZM\u22121 . We randomly shuffle these mini-batches and use corresponding objective function for each minibatch.\nAs a side note, we would like to mention that in addition to Z1 , Z2 to ZM\u22121 as defined earlier, if additional parallel data is available between some of the non-pivot views then the objective function can be suitably modified to use this parallel data to further improve the learning. However, this is not the focus of this work and we leave this as a possible future work."}, {"heading": "4 Datasets", "text": ""}, {"heading": "4.1 Multlingual TED corpus", "text": "Hermann and Blunsom (2014b) provide a massively\nmultilingual corpus based on the TED corpus1 for IWSLT 2013 (Cettolo et al., 2012). It contains English transcriptions of several talks from the TED conference and their translations in multiple languages. We use the parallel data between English and other languages for training Bridge Corrnet (English, thus, acts as the pivot langauge). Hermann and Blunsom (2014b) also propose a multlingual document classification task using this corpus. The idea is to use the keywords associated with each talk (document) as class labels and then train a classifier to predict these classes. There are one or more such keywords associated with each talk but only the 15 most frequent keywords across all documents are considered as class labels. We used the same pre-processed train/test/valid splits2 as provided by (Hermann and Blunsom, 2014b). The training corpus consists of a total of 12,078 parallel documents distributed across 12 language pairs."}, {"heading": "4.2 Multilingual Image Caption dataset", "text": "The MSCOCO dataset3 contains images and their English captions. On an average there are 5 captions per image. The standard train/valid/test splits for this dataset are also available online. However, the reference captions for the images in the test split are not provided. Since we need such reference captions for evaluations, we create a new train/valid/test of this dataset. Specifically, we take 80K images from the standard train split and 40K images from the standard valid split. We then randomly split the merged 120K images into train(118K), validation (1K) and test set (1K).\nWe then create a multilingual version of the test data by collecting French and German translations for all the 5 captions for each image in the test set. We use crowdsourcing to do this. We use the CrowdFlower4 platform and solicite one French and one German translation for each of the 5000 captions using native speakers. We then randomly picked up 50-200 of these translations and verified them with the help of in-house translators. According to our expert translators, 75% of the French translations and 65% of the German translations were correct.\n12https://wit3.fbk.eu/ 2http://www.clg.ox.ac.uk/tedcorpus 3http://mscoco.org/dataset/$#$download 4https://make.crowdflower.com\nWe used these 5000 translated captions for evaluating all our cross modal experiments. This multilingual image caption test data along with our train, valid and test splits will be made publicly available and will hopefully assist further research in this area."}, {"heading": "5 Experiment 1: Transfer learning using a pivot language", "text": "From the TED corpus described earlier, we consider English transcriptions and their translations in 11 languages, viz., Arabic, German, Spanish, French, Italian, Dutch, Polish, Portuguese (Brazilian), Roman, Russian and Turkish. Following the setup of Hermann and Blunsom (2014b), we consider the task of cross language learning between each of the 11C2 non-English language pairs. The task is to classify documents in a language when no labeled training data is available in this language but training data is available in another language. This involves the following steps: 1. Train classifier: Consider one language as the source language and the remaining 10 languages as target languages. Train a document classifier using the labeled data of the source language, where each training document is represented using the hidden representation computed using a trained Bridge Corrnet model. As in (Hermann and Blunsom, 2014b) we used an averaged perceptron trained for 10 epochs as the classifier for all our experiments. The train split provided by (Hermann and Blunsom, 2014b) is used for training. 2. Cross language classification: For every target language, compute a hidden representation for every document in its test set using Bridge CorrNet. Now use the classifier trained in the previous step to classify this document. The test split provided by (Hermann and Blunsom, 2014b) is used for testing."}, {"heading": "5.1 Training and tuning Bridge Corrnet", "text": "For the above process to work, we first need to train Bridge Corrnet so that it can then be used for computing a common hidden representation for documents in different languages. For training Bridge CorrNet, we treat English as the pivot language (view) and construct parallel training sets Z1 to Z11 . Every instance in Z1 contains the English and Arabic view of the same talk (document). Similarly, ev-\nery instance in Z2 contains the English and German view of the same talk (document) and so on. For every language, we first construct a vocabulary containing all words appearing more than 5 times in the corpus (all talks) of that language. We then use this vocabulary to construct a bag-of-words representation for each document. The size of the vocabulary (|V |) for different languages varied from 31213 to 60326 words. To be more clear, v1 = varabic \u2208 R|V |arabic , v2 = vgerman \u2208 R|V |german and so on.\nWe train Bridge Corrnet for 10 epochs using the above training data Z = {Z1 ,Z2 , ...,Z11}. We use hidden representations of size D = 128, as in (Hermann and Blunsom, 2014b). Further, we used stochastic gradient descent with mini-batches of size 20. Each mini-batch contains data from only one of the Zis. We get a stochastic estimate for the correlation term in the objective function using this minibatch. The hyperparameter \u03bb was tuned to each task using a training/validation split for the source language and using the performance on the validation set of an averaged perceptron trained on the training set (notice that this corresponds to a monolingual classification experiment, since the general assumption is that no labeled data is available in the target language). These train/validation sets were the same as provided by (Hermann and Blunsom, 2014b)."}, {"heading": "5.2 Results", "text": "Before presenting the results for our cross language classification experiment, we would first like to give a qualitative feel for the representations learned using Bridge CorrNet. For this, we randomly select a few English words and find their nearest neighbors in different languages based on the representations learned using Bridge CorrNet. These English words and their neighbors are shown in Table 4. In almost all the cases the nearest neighbors of the English words turn out to be their exact translations or highly semantically related words. Also, we observed that the representations of translation pairs in non-English languages (say, French and German) are also transitively close to each other due to the pivot language.\nWe now present the results of our cross language classification task in Table 1. Each row corresponds to a source language and each column corresponds to a target language. We report the average F1-\nscores over all the 15 classes. We compare our results with the best results reported in (Hermann and Blunsom, 2014b) (see Table 2). Out of the 110 experiments, our model outperforms the model of (Hermann and Blunsom, 2014b) in 107 experiments. This suggests that our model efficiently exploits the pivot language to facilitate cross language learning between other languages.\nFinally, we present the results for a monolingual classification task in Table 3. The idea here is to see if learning common representations for multiple views can also help in improving the performance of a task involving only one view. Hermann and Blunsom (2014b) argue that a Naive Bayes (NB) classifier trained using a bag-of-words representation of the documents is a very strong baseline. In fact, a classifier trained on document representations learned using their model does not beat a NB classifier for the task of monolingual classification (i.e., training and testing on the same language). Rows 2 to 5 in Table 3 show the different settings tried by them (we refer the reader to (Hermann and Blunsom, 2014b) for a detailed description of thesse settings). On the other hand our model is able to beat NB for 5 out of the 11 languages. Further, for 4 other languages (German, French, Romanian, Russian) its performance is only marginally poor than that of NB. This suggests that the representations learned using our model are semantically meaningful."}, {"heading": "6 Experiment 2: Cross modal access using a pivot language", "text": "In this experiment, we are interested in retrieving images given their captions in French (or German) and vice versa. However, for training we do not have any parallel data containing images and their French (or German) captions. Instead, we have the following datasets: (i) a dataset Z1 containing images and their English captions and (ii) a dataset Z2 containing English and their parallel French (or German) documents. For Z1 , we use the training split of MSCOCO dataset which contains 118K images and their English captions (see Section 4.1). For Z2 , we use the English-French (or German) parallel documents from the train split of the TED corpus (see Section 4.2). We use English as the pivot language\nand train Bridge Corrnet using Z = {Z1 ,Z2} to learn common representations for images, English text and French (or German) text. We train Bridge Corrnet for 20 epochs using the above training data. We use hidden representations of size D = 200. Further, we used stochastic gradient descent with mini-batches of size 20. Each mini-batch contains data from only one of the Zis. The hyperparameter \u03bb was tuned to each task using a training/validation split and using the performance on the validation set for (i) retrieving English captions for a given image or (ii) retrieving an image given an English caption (we do not use any image-French/German parallel data for tuning the hyperparameters).\nFor the task of retrieving captions given an image, we consider the 1000 images in our test set (see section 4.2) as queries. The 5000 French (or German) captions corresponding to these images (5 per image) are considered as documents. The task is then to retrieve the relevant captions for each image. We represent all the captions and images in the common space as computed using Bridge Corrnet. For a given query, we rank all the captions based on the Euclidean distance between the representation of the image and the caption. For the task of retrieving captions given an image, we simply reverse the role of the captions and images. In other words, each of the 5000 captions is treated as a query and the 1000 images are treated as documents (note that the document collection is smaller in this case). Socher et al. (2014) use a similar experimental setup at a much smaller scale (using a train/valid/test split of 800/100/100 image-caption pairs). For the task of retrieving captions given an image, they report the rank of the first relevant caption in the list of captions sorted by the above Euclidean distance (and vice-versa for the other task). Following them we also use the same metric. However, note that we cannot directly compare with their model as it does not address the bridge case as described in this work (further, it is highly non-trivial to extend their model for the bridge case). We compare the performance of following methods in Table 6: 1. En-Image CorrNet: This is the CorrNet model trained using only Z1 as defined earlier in this section. The task is to retrieve English captions for a given image (or vice versa). This gives us an idea about the performance we could expect if direct parallel data is available between images and their captions in some language. 2. Fr/De-En-Image BridgeCorrNet: This is the Bridge CorrNet model trained using Z1 and Z2 as defined earlier in this section. The task is to retrieve French (or German) captions for a given image (or vice versa). 3. Fr/De-En-Image MAE: The Multimodal Autoencoder (MAE) proposed by (Ngiam et al., 2011) was the only competing model which was easily extendable to the bridge case. We train their model using Z1 and Z2 to minimize a suitably modified\nobjective function. We then use the representations learned using Bridge MAE to retrieve French (or German) captions for a given image (or vice versa). 4. Random: A random image is returned for the given caption (and vice versa).\nWe make the following observations from Table 6. Looking at the performance of En-Image CorrNet, it is evident that this is a very hard task (the numbers reported in Socher et al. (2014) also suggest the same). However, the focus of this work is not to achieve state-of-the art performance on this task but to show that reasonable performance can be obtained for cross modal retrieval between French/German and images even when no parallel data is available between them. Bridge Corrnet clearly does better than MAE and a random baseline. Further, we will illustrate in section 6.1, Bridge CorrNet effectively captures cross modal semantics between images and French/German. Note that the results for caption-image retrieval are better than image-caption retrieval because there are only 1000 images (smaller document collection) as compared to 5000 captions.\nWe would like to mention two other baselines that we wanted to compare with. The first is the Bridge CCA model proposed by Khapra et al. (2010b) which directly addresses the problem of learning common representations for two views using a pivot view. However, Bridge CCA internally requires CCA and we could not find a publicly available solution for CCA which could scale to our dataset. For example, using some publicly available implementations we were not able to train Bridge CCA even with 10% of our data on a machine having 256 GB RAM. We then tried using the Deep CCA model proposed by Andrew et al. (2013) as a substitute for CCA and trained it using the same process as outlined in (Khapra et al., 2010b). Here again, existing\nimplementations do not scale to our dataset."}, {"heading": "6.1 Qualitative Analysis", "text": "Even though the absolute mean rank as reported in Table 6 looks high, a qualitative analysis of the results indicates that Bridge CorrNet is able to capture cross modal semantics between images and French/German descriptions. We illustrate this with the help of some examples in Table 6 and 7. The first row in Table 6 shows an image and its top-5 nearest German captions (based on Euclidean distance between their common representations). As per our parallel image caption test set, only the second and fourth caption actually correspond to this image. However, we observe that the first and fifth caption are also semantically very related to the image. Both these captions talk about horses, grass or water body (ocean), etc. Similarly the last row in Table 6 shows an image and its top-5 nearest French captions. None of these captions actually correspond to the image as per our parallel image caption test set. However, clearly the first, third and fourth caption are semantically very relevant to this image as all of them talk about baseball. Even the remaining two captions capture the concept of a sport and raquet. We can make a similar observation from Table 7 where most of the top-5 retrieved images do not correspond to the French or German caption but they are semantically very similar. It is indeed impressive that the model is able to capture such cross modal semantics between images and French/German even without any direct parallel data between them."}, {"heading": "7 Conclusion", "text": "In this paper, we propose Bridge Correlational Neural Networks which can learn common representations for multiple views even when parallel data is available only between these views and a pivot view. We evaluate the performance of the representations learned using our model on different tasks using two large datasets. Specifically, we evaluate the performance on cross language classification, monolingual classification and cross modal access. In all these tasks our method performs better than existing state of the art approaches. We also release a new multilingual image caption dataset which will help in further research in this field. In particular, we plan to\nuse this data to build and evaluate a model for multilingual caption generation (as opposed to retrieval)."}], "references": [{"title": "A kernel method for canonical correlation analysis", "author": ["S. Akaho"], "venue": "In Proc. Int\u2019l Meeting on Psychometric Society", "citeRegEx": "Akaho.,? \\Q2001\\E", "shortCiteRegEx": "Akaho.", "year": 2001}, {"title": "Deep canonical correlation analysis", "author": ["Andrew et al.2013] Galen Andrew", "Raman Arora", "Jeff Bilmes", "Karen Livescu"], "venue": null, "citeRegEx": "Andrew et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Andrew et al\\.", "year": 2013}, {"title": "Cross language retrieval via transitive translation", "author": ["L.A. Ballesteros"], "venue": null, "citeRegEx": "Ballesteros.,? \\Q2000\\E", "shortCiteRegEx": "Ballesteros.", "year": 2000}, {"title": "Wit: Web inventory of transcribed and translated talks", "author": ["Christian Girardi", "Marcello Federico"], "venue": "In Proceedings of the 16 Conference of the European Association for Machine Translation (EAMT),", "citeRegEx": "Cettolo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Multilingual deep learning", "author": ["Mitesh M. Khapra", "Balaraman Ravindran", "Vikas C. Raykar", "Amrita Saha"], "venue": "NIPS Deep Learning Workshop", "citeRegEx": "Chandar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2013}, {"title": "An autoencoder approach to learning bilingual word representations", "author": ["Stanislas Lauly", "Hugo Larochelle", "Mitesh M. Khapra", "Balaraman Ravindran", "Vikas C. Raykar", "Amrita Saha"], "venue": null, "citeRegEx": "Chandar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chandar et al\\.", "year": 2014}, {"title": "Machine translation by triangulation: Making effective use of multi-parallel corpora", "author": ["Cohn", "Lapata2007] Trevor Cohn", "Mirella Lapata"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Cohn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2007}, {"title": "Fast regularized canonical correlation analysis", "author": ["Cruz-Cano", "Lee2014] Raul Cruz-Cano", "MeiLing Ting Lee"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Cruz.Cano et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cruz.Cano et al\\.", "year": 2014}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "In Proceedings of the 32nd International Conference", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["Hermann", "Phil Blunsom"], "venue": "In Proceedings of International Conference on Learning Representations (ICLR)", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Multilingual models for compositional distributed semantics", "author": ["Hermann", "Phil Blunsom"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": null, "citeRegEx": "Hotelling.,? \\Q1936\\E", "shortCiteRegEx": "Hotelling.", "year": 1936}, {"title": "Nonlinear canonical correlation analysis by neural networks", "author": ["W.W. Hsieh"], "venue": "Neural Networks,", "citeRegEx": "Hsieh.,? \\Q2000\\E", "shortCiteRegEx": "Hsieh.", "year": 2000}, {"title": "Everybody loves a rich cousin: An empirical study of transliteration through bridge languages. In Human Language Technologies", "author": ["A. Kumaran", "Pushpak Bhattacharyya"], "venue": null, "citeRegEx": "Khapra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Khapra et al\\.", "year": 2010}, {"title": "2010b. PR + RQ ALMOST EQUAL TO PQ: transliteration mining using bridge language", "author": ["Raghavendra Udupa", "A. Kumaran", "Pushpak Bhattacharyya"], "venue": "In Proceedings of the Twenty-Fourth AAAI Conference on Artificial", "citeRegEx": "Khapra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Khapra et al\\.", "year": 2010}, {"title": "Inducing Crosslingual Distributed Representations of Words", "author": ["Ivan Titov", "Binod Bhattarai"], "venue": "In Proceedings of the International Conference on Computational Linguistics (COLING)", "citeRegEx": "Klementiev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Klementiev et al\\.", "year": 2012}, {"title": "Compositional machine transliteration", "author": ["Kumaran et al.2010] A. Kumaran", "Mitesh M. Khapra", "Pushpak Bhattacharyya"], "venue": "ACM Trans. Asian Lang. Inf. Process.,", "citeRegEx": "Kumaran et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumaran et al\\.", "year": 2010}, {"title": "Experiments with transitive dictionary translation and pseudo-relevance feedback using graded relevance assessments", "author": ["Heikki Keskustalo", "Kalervo J\u00e4rvelin"], "venue": "Journal of the American Society", "citeRegEx": "Lehtokangas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lehtokangas et al\\.", "year": 2008}, {"title": "Tensor canonical correlation analysis for multi-view dimension reduction", "author": ["Luo et al.2015] Yong Luo", "Dacheng Tao", "Yonggang Wen", "Kotagiri Ramamohanarao", "Chao Xu"], "venue": "In Arxiv", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Quoc Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improved statistical machine translation for resource-poor languages using related resource-rich languages", "author": ["Nakov", "Ng2009] Preslav Nakov", "Hwee Tou Ng"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Nakov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2009}, {"title": "Multimodal deep learning", "author": ["Ngiam et al.2011] J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "Ng. Andrew"], "venue": null, "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Canonical ridge analysis with ridge parameter optimization, may", "author": ["L.K. Hansen", "S.C. Strother"], "venue": null, "citeRegEx": "Nielsen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Nielsen et al\\.", "year": 1998}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Leveraging monolingual data for crosslingual compositional word representations", "author": ["Soyer et al.2015] Hubert Soyer", "Pontus Stenetorp", "Akiko Aizawa"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations,", "citeRegEx": "Soyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soyer et al\\.", "year": 2015}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["Srivastava", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Regularized generalized canonical correlation analysis", "author": ["Tenenhaus", "Tenenhaus2011] Arthur Tenenhaus", "Michel Tenenhaus"], "venue": null, "citeRegEx": "Tenenhaus et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tenenhaus et al\\.", "year": 2011}, {"title": "A comparison of pivot methods for phrase-based statistical machine translation", "author": ["Utiyama", "Isahara2007] Masao Utiyama", "Hitoshi Isahara"], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Utiyama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Utiyama et al\\.", "year": 2007}, {"title": "Canonical ridge and econometrics of joint production", "author": ["H.D. Vinod"], "venue": "Journal of Econometrics,", "citeRegEx": "Vinod.,? \\Q1976\\E", "shortCiteRegEx": "Vinod.", "year": 1976}, {"title": "On deep multi-view representation learning", "author": ["Wang et al.2015] Weiran Wang", "Raman Arora", "Karen Livescu", "Jeff Bilmes"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Pivot language approach for phrase-based statistical machine translation", "author": ["Wu", "Wang2007] Hua Wu", "Haifeng Wang"], "venue": "Machine Translation,", "citeRegEx": "Wu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2007}, {"title": "A deep and autoregressive approach for topic modeling of multimodal data. CoRR, abs/1409.3970", "author": ["Zheng et al.2014a] Yin Zheng", "Yu-Jin Zhang", "Hugo Larochelle"], "venue": null, "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}, {"title": "Topic modeling of multimodal data: An autoregressive approach", "author": ["Zheng et al.2014b] Yin Zheng", "Yu-Jin Zhang", "Hugo Larochelle"], "venue": "In 2014 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Zheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["Zou et al.2013] Will Y. Zou", "Richard Socher", "Daniel Cer", "Christopher D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 21, "context": "Existing approaches to common representation learning (Ngiam et al., 2011; Klementiev et al., 2012; Chandar et al., 2013; Chandar et al., 2014; Andrew et al., 2013; Hermann and Blunsom, 2014b; Wang et al., 2015) typically require parallel data between the two views.", "startOffset": 54, "endOffset": 211}, {"referenceID": 15, "context": "Existing approaches to common representation learning (Ngiam et al., 2011; Klementiev et al., 2012; Chandar et al., 2013; Chandar et al., 2014; Andrew et al., 2013; Hermann and Blunsom, 2014b; Wang et al., 2015) typically require parallel data between the two views.", "startOffset": 54, "endOffset": 211}, {"referenceID": 4, "context": "Existing approaches to common representation learning (Ngiam et al., 2011; Klementiev et al., 2012; Chandar et al., 2013; Chandar et al., 2014; Andrew et al., 2013; Hermann and Blunsom, 2014b; Wang et al., 2015) typically require parallel data between the two views.", "startOffset": 54, "endOffset": 211}, {"referenceID": 5, "context": "Existing approaches to common representation learning (Ngiam et al., 2011; Klementiev et al., 2012; Chandar et al., 2013; Chandar et al., 2014; Andrew et al., 2013; Hermann and Blunsom, 2014b; Wang et al., 2015) typically require parallel data between the two views.", "startOffset": 54, "endOffset": 211}, {"referenceID": 1, "context": "Existing approaches to common representation learning (Ngiam et al., 2011; Klementiev et al., 2012; Chandar et al., 2013; Chandar et al., 2014; Andrew et al., 2013; Hermann and Blunsom, 2014b; Wang et al., 2015) typically require parallel data between the two views.", "startOffset": 54, "endOffset": 211}, {"referenceID": 29, "context": "Existing approaches to common representation learning (Ngiam et al., 2011; Klementiev et al., 2012; Chandar et al., 2013; Chandar et al., 2014; Andrew et al., 2013; Hermann and Blunsom, 2014b; Wang et al., 2015) typically require parallel data between the two views.", "startOffset": 54, "endOffset": 211}, {"referenceID": 11, "context": "Canonical Correlation Analysis (CCA) and its variants (Hotelling, 1936; Vinod, 1976; Nielsen et al., 1998; Cruz-Cano and Lee, 2014; Akaho, 2001)", "startOffset": 54, "endOffset": 144}, {"referenceID": 28, "context": "Canonical Correlation Analysis (CCA) and its variants (Hotelling, 1936; Vinod, 1976; Nielsen et al., 1998; Cruz-Cano and Lee, 2014; Akaho, 2001)", "startOffset": 54, "endOffset": 144}, {"referenceID": 22, "context": "Canonical Correlation Analysis (CCA) and its variants (Hotelling, 1936; Vinod, 1976; Nielsen et al., 1998; Cruz-Cano and Lee, 2014; Akaho, 2001)", "startOffset": 54, "endOffset": 144}, {"referenceID": 0, "context": "Canonical Correlation Analysis (CCA) and its variants (Hotelling, 1936; Vinod, 1976; Nielsen et al., 1998; Cruz-Cano and Lee, 2014; Akaho, 2001)", "startOffset": 54, "endOffset": 144}, {"referenceID": 12, "context": "common representations was proposed in (Hsieh, 2000).", "startOffset": 39, "endOffset": 52}, {"referenceID": 21, "context": "Multimodal Autoencoder (MAE) proposed in (Ngiam et al., 2011) uses", "startOffset": 41, "endOffset": 61}, {"referenceID": 29, "context": "Deep Canonically Correlated Autoencoder (DCCAE) which was proposed in (Wang et al., 2015) uses a combination of Autoencoder (encoding/decoding only inside the", "startOffset": 70, "endOffset": 89}, {"referenceID": 1, "context": "Similarly, Deep CCA (Andrew et al., 2013) uses deep neural networks to encode the views in a common space.", "startOffset": 20, "endOffset": 41}, {"referenceID": 15, "context": "resentation Learning using Neural Networks relied on this word level alignment information (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013).", "startOffset": 91, "endOffset": 156}, {"referenceID": 33, "context": "resentation Learning using Neural Networks relied on this word level alignment information (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013).", "startOffset": 91, "endOffset": 156}, {"referenceID": 19, "context": "resentation Learning using Neural Networks relied on this word level alignment information (Klementiev et al., 2012; Zou et al., 2013; Mikolov et al., 2013).", "startOffset": 91, "endOffset": 156}, {"referenceID": 5, "context": "level alignments (Hermann and Blunsom, 2014b; Hermann and Blunsom, 2014a; Chandar et al., 2014; Soyer et al., 2015; Gouws et al., 2015).", "startOffset": 17, "endOffset": 135}, {"referenceID": 24, "context": "level alignments (Hermann and Blunsom, 2014b; Hermann and Blunsom, 2014a; Chandar et al., 2014; Soyer et al., 2015; Gouws et al., 2015).", "startOffset": 17, "endOffset": 135}, {"referenceID": 8, "context": "level alignments (Hermann and Blunsom, 2014b; Hermann and Blunsom, 2014a; Chandar et al., 2014; Soyer et al., 2015; Gouws et al., 2015).", "startOffset": 17, "endOffset": 135}, {"referenceID": 21, "context": "Ngiam et al. (2011) proposed", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "learning include (Zheng et al., 2014a; Zheng et al., 2014b; Socher et al., 2014).", "startOffset": 17, "endOffset": 80}, {"referenceID": 16, "context": ", 2008), pivot based transliteration and transliteration mining (Khapra et al., 2010a; Kumaran et al., 2010; Khapra et al., 2010b; Zhang et al., 2011).", "startOffset": 64, "endOffset": 150}, {"referenceID": 3, "context": "multilingual corpus based on the TED corpus1 for IWSLT 2013 (Cettolo et al., 2012).", "startOffset": 60, "endOffset": 82}, {"referenceID": 23, "context": "Socher et al. (2014) use a similar experimental setup at a much smaller scale (using a train/valid/test split of 800/100/100 image-caption pairs).", "startOffset": 0, "endOffset": 21}, {"referenceID": 21, "context": "Fr/De-En-Image MAE: The Multimodal Autoencoder (MAE) proposed by (Ngiam et al., 2011) was the only competing model which was easily ex-", "startOffset": 65, "endOffset": 85}, {"referenceID": 23, "context": "numbers reported in Socher et al. (2014) also suggest the same).", "startOffset": 20, "endOffset": 41}, {"referenceID": 13, "context": "The first is the Bridge CCA model proposed by Khapra et al. (2010b)", "startOffset": 46, "endOffset": 68}, {"referenceID": 1, "context": "We then tried using the Deep CCA model proposed by Andrew et al. (2013) as a substitute for CCA and trained it using the same process as outlined in (Khapra et al.", "startOffset": 51, "endOffset": 72}], "year": 2017, "abstractText": "Recently there has been a lot of interest in learning common representations for multiple views of data. These views could belong to different modalities or languages. Typically, such common representations are learned using a parallel corpus between the two views (say, 1M images and their English captions). In this work, we address a real-world scenario where no direct parallel data is available between two views of interest (say, V1 and V2) but parallel data is available between each of these views and a pivot view (V3). We propose a model for learning a common representation for V1, V2 and V3 using only the parallel data available between V1V3 and V2V3. The proposed model is generic and even works when there are n views of interest and only one pivot view which acts as a bridge between them. There are two specific downstream applications that we focus on (i) Transfer learning between languages L1,L2,...,Ln using a pivot language L and (ii) cross modal access between images and a language L1 using a pivot language L2. We evaluate our model using two datasets : (i) publicly available multilingual TED corpus and (ii) a new multilingual multimodal dataset created and released as a part of this work. On both these datasets, our model outperforms state of the art approaches.", "creator": "LaTeX with hyperref package"}}}