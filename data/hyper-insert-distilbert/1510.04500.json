{"id": "1510.04500", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Oct-2015", "title": "Noisy-parallel and comparable corpora filtering methodology for the extraction of bi-lingual equivalent data at sentence level", "abstract": "text alignment ability and text quality are strategically critical to aiding the accuracy of machine quality translation ( acronym mt ) systems, whether some nlp analytical tools, and any other text query processing tasks requiring bilingual data. this research proposes a language independent bi - integrated sentence filtering analytics approach based on grammatical polish ( consequently not a classical position - sensitive arabic language ) pertaining to english experiments. this cleaning classification approach was developed modelled on approximately the 2005 ted short talks syntax corpus and itself also initially tested on the wikipedia a comparable vocabulary corpus, below but it can be either used for any text domain variant or particular language pair. with the researchers proposed approach implements multiple various graphical heuristics together for sentence comparison. nonetheless some of both them leverage comparative synonyms and semantic and structural analysis of perceived text as semantic additional similarity information. minimization of data loss technique was ensured. again an improvement in mt system score with concurrent text citations processed using the tool is discussed.", "histories": [["v1", "Thu, 15 Oct 2015 12:29:14 GMT  (323kb)", "http://arxiv.org/abs/1510.04500v1", "arXiv admin note: text overlap witharXiv:1509.09093,arXiv:1509.08881"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1509.09093,arXiv:1509.08881", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["krzysztof wo{\\l}k"], "accepted": false, "id": "1510.04500"}, "pdf": {"name": "1510.04500.pdf", "metadata": {"source": "CRF", "title": "Noisy-parallel and comparable corpora filtering methodology for the extraction of bi-lingual equivalent data at sentence level", "authors": ["Krzysztof Wo\u0142k"], "emails": ["kwolk@pja.edu.pl"], "sections": [{"heading": "1 Introduction", "text": "Before a parallel corpus can be used for any processing, the sentences must be aligned. Sentences in a raw corpus are sometimes misaligned, resulting in translated lines whose placement does not correspond to the text lines in the source language. Moreover, some sentences may have no corresponding translation in the corpus at all. The corpus might also contain poor or indirect translations, making the alignment difficult and quality of the data poor. Thus, it is crucial to MT system accuracy [1]. Corpora alignment must also be computationally feasible in order to be of a practical use in various applications [2].\nThe Polish language is a particular challenge to such tools. It is a complicated West-Slavic language with complex elements and grammatical rules. In addition, the Polish language has a large vocabulary, due to many endings and prefixes changed by word declension. These characteristics have a significant effect on the requirements for the data and the data structure itself.\nIn addition, English is a position-sensitive language. The syntactic order, the order of words in a sentence, plays a very significant role. The language has very limited inflection (e.g., due to the lack of declension endings). The word position in an English sentence is often the only indicator of the meaning. The sentence order follows the Subject-Verb-Object (SVO) schema, with the subject phrase preceding the predicate [9].\nOn the other hand, no specific word order is imposed in Polish, and the word order has little effect on the meaning of a sentence. The same thought can be expressed in several ways. For example, the sentence I just tasted a new orange juice.\u201d can be written in Polish as \u201cSpr\u00f3bowa\u0142em w\u0142a\u015bnie nowego soku pomara\u0144czowego,\u201d or \u201cNowego soku pomara\u0144czowego w\u0142a\u015bnie spr\u00f3bowa\u0142em,\u201d \u201cW\u0142a\u015bnie spr\u00f3bowa\u0142em nowego soku pomara\u0144czowego,\u201d or \u201cW\u0142a\u015bnie nowego soku pomara\u0144czowego spr\u00f3bowa\u0142em.\u201d It must be noted that such differences exist in many language pairs and need to be dealt with somehow. This article proposes a language independent corpus filtering method that has been applied to Polish-English parallel corpora.\nThe dataset used for this research was the Translanguage English Database (TED) [3] provided by Fondazione Bruno Kessler (FBK) for IWSLT 20131. The vocabulary sizes of the English and Polish texts in TED are disproportionate. There are 41,684 unique English words and 92,135 unique Polish words. This also presents a challenge for SMT systems.\nAdditionally, a Wikipedia comparable corpus was built and used in this research. The corpus was prepared for the needs of the IWSLT2014 Evaluation Campaign2. In Table 1, a fragment of the Wikipedia corpora that consists of 13 lines obtained with the Yalign tool is presented. There are many similar cases and fragments in the entire corpora that might be quite problematic during machine translation system training. First, the data is quite noisy, and the corpora contain redundant parallel lines that contain just numbers or symbols. In addition, it is easy to find improper translations, e.g., \u201cU.S. Dept.\u201d is clearly not an accurate translation of the sentence \u201cNa pocz\u0105tku lat 30,\u201d which in Polish means \u201cThe beginning of the 30s\u201d. What is more, some translations are too indirect or too distinct from each other. An example of such a pair is: \u201cIn all other cases it is true.\u201d and \u201cW przeciwnym razie alternatywa zda\u0144 jest fa\u0142szywa.,\u201d which in Polish means \u201cOtherwise, the alternative of the sentences is false.\u201d\nDespite these errors, the corpora contain good translations if the problematic data can be removed.\n7 Standard ASN.1 okre\u015bla jedynie sk\u0142adni\u0119 abstrakcyjn\u0105 informacji, nie okre\u015bla natomiast sposobu jej kodowania w pliku. ASN.1 defines the abstract syntax of information but does not restrict the way the information is encoded. 8 Metody kodowania informacji podanych w sk\u0142adni ASN.1 zosta\u0142y opisane w kolejnych standardach ITU-T/ISO. X.683 | ISO/IEC 8824-4 (Parameterization of ASN.1 specifications)Standards describing the ASN.1 encoding rules:* ITU-T Rec. 9 pierwszego). One criterion . 10 problemy nierozstrzygalne. .\".\n11\nJe\u017celi dany algorytm da si\u0119 wykona\u0107 na maszynie o dost\u0119pnej mocy obliczeniowej i pami\u0119ci oraz akceptowalnym czasie, to m\u00f3wi si\u0119, \u017ce jest obliczalny. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".\n12 temperatur\u0119) w optymalnym zakresie. \"Algorithmic theories\"...\n13 Na pocz\u0105tku lat 30. U.S. Dept."}, {"heading": "2 Problem Analysis", "text": "Parallel sentences are a particularly valuable information source for machine translation systems, as well as for other cross-lingual information-dependent tasks. Unfortunately, such data is quite rare, especially for the Polish\u2013English language pair. On the other hand, monolingual data for those languages is accessible in far greater quantities. We can classify the similarity of data as four main corpora types. Most rare parallel corpora can be defined as corpora that contain translation of the same document into two or more languages. Such data should be aligned at least at the sentence level. A noisy parallel corpus contains bilingual sentences that are not perfectly aligned or have poor quality translations. Nevertheless, mostly bilingual translations of a specific document should be present in it. A comparable corpus is built from non-sentence-aligned and untranslated bilingual documents, but the documents should be topic-aligned. A quasi-comparable corpus includes very heterogeneous and non-parallel bilingual documents that may or may not be topicaligned [12].\nPrevious attempts to automatically compare sentences for parallel corpora were based on sentence lengths, together with vocabulary alignment [4]. Brown\u2019s method [11] was based on measuring sentence length by the number of words. Gale and Church [5] measured the number of characters in sentences. Other researchers continued exploring various methods of combining sentence length statistics with alignment of vocabularies [6, 7]. Such methods lead to the creation of noisy parallel corpora at best.\n2.1! TED \u2013 Noisy Parallel Corpora The Polish data in the TED talks (about 15 MB) include almost 2 million words that are not tokenized. The transcripts themselves are provided as pure text encoded i UTF-8 [8]. In addition, they are separated into sentences (one per line) and aligned in language pairs. However, some discrepancies in the text parallelism are present. These discrepancies are mainly repetitions of Polish text not included in the parallel English text.\nAnother problem is that the TED 2013 data is full of errors. This dataset contains spelling errors that artificially increase the dictionary size and make the statistics unreliable. A very large Polish dictionary [2] consisting of 2,532,904 different words was extracted. Then, a dictionary consisting of 92,135 unique words was created from TED 2013 data. The intersection of those two dictionaries resulted in a new dictionary containing 58,393 words. This means that 33,742 words that do not exist in Polish (e.g., due to spelling errors or named entities) were found in TED 2013. This is 36.6% of the whole TED Polish vocabulary [13].\nTo verify this, a manual analysis of a sample of the first 300 lines from the TED corpus was conducted. It was found that there were 4,268 words containing a total of 35 kinds of spelling errors that occurred many times. But what was found to be more problematic was that there were sentences with odd nesting, such as:\nPart A, Part A, Part B, Part B., e.g.: \u201cAle b\u0119d\u0119 stara\u0142 si\u0119 udowodni\u0107, \u017ce mimo z\u0142o\u017cono\u015bci, Ale b\u0119d\u0119 stara\u0142 si\u0119 udowodni\u0107, \u017ce mimo z\u0142o\u017cono\u015bci, istniej\u0105 pewne rzeczy pomagaj\u0105ce w zrozumieniu. istniej\u0105 pewne rzeczy pomagaj\u0105ce w zrozumieniu.\u201d\nWe can see that some parts (words or full phrases or even whole sentences) were duplicated. Furthermore, there are segments containing repetitions of whole sentences inside one segment. For instance:\nSentence A. Sentence A., e.g.: \u201cZakumuluj\u0105 si\u0119 u tych najbardziej pijanych i sk\u0105pych. Zakumuluj\u0105 si\u0119 u tych najbardziej pijanych i sk\u0105pych.\u201d or: Part A, Part B, Part B, Part C, e.g.: \u201dMatka mo\u017ce si\u0119 ponownie rozmna\u017ca\u0107, ale jak wysok\u0105 cen\u0119 p\u0142aci, przez akumulacj\u0119 toksyn w swoim organizmie - przez akumulacj\u0119 toksyn w swoim organizmie - \u015bmier\u0107 pierwszego m\u0142odego.\u201d\nThe analysis identified 51 out of 300 segments that were mistaken in such a way. Overall, 10% of the sample test set contained spelling errors, and about 17% contained insertion errors. However, it must be noted that simply the first 300 lines were taken, while in the whole text there were places where more problems occurred. So, to some extent, this confirms that there were problems related to the dictionary.\nIn addition, there are numerous untranslated English names, words, and phrases (not translated) present in the Polish text. There are also some sentences that originate from other languages (e.g., German and French). Some translations were simply incorrect or too indirect without sufficient precision, e.g., \u201cAnd I remember there sitting at my desk thinking, Well, I know this. This is a great scientific discovery.\u201d was translated into \u201cPami\u0119tam, jak pomy\u015bla\u0142em: To wyj\u0105tkowe, naukowe odkrycie.\u201d The correct translation would be \u201cPami\u0119tam jak siedz\u0105c przy biurku pomy\u015bla\u0142em, dobrze, wiem to. To jest wielkie naukowe odkrycie\u201d.\nAnother serious problem (especially for Statistical Machine Translation) discovered was that English sentences were translated in an improper manner.\nThere were four main problems: 1. Repetitions \u2013 part of the text is repeated several times after translation, i.e.: a. EN: Sentence A. Sentence B. b. PL: Translated Sentence A. Translated Sentence B. Translated Sentence B.\nTranslated Sentence B.\n2. Wrong usage of words \u2013 when one or more words used for the Polish translation slightly change the meaning of the original English sentence, i.e.:\na. EN: We had these data a few years ago. b. PL (the proper meaning of the Polish sentence): We\u2019ve been delivered these\ndata a few years ago. 3. Indirect translations or usage of metaphors \u2013 when the Polish translation\nuses a different vocabulary in order to preserve the meaning of the original sentence, especially when the exact translation would result in a sentence that makes no sense. Many metaphors are translated this way.\n4. Translations that are not precise enough \u2013 when the translated fragment does not contain all the details of the original sentence, but only its overall meaning is the same."}, {"heading": "2.2 Wikipedia \u2013 Comparable Corpora", "text": "For this research, the Wikipedia corpus was extracted f from a comparable corpus generated from Wikipedia articles. It was possible to obtain about 1M topic-aligned articles from Wikipedia. The Wikipedia corpus was about 104MB in size and contained 475,470 parallel sentences. Its first version was acknowledged as permissible data for the IWSLT 2014 evaluation campaign.\nThe Yalign Tool [14] was used to extract parallel sentence pairs. This tool was designed to automate the parallel text mining process by finding sentences that are close translation matches to the comparable corpora. This opened up avenues for harvesting parallel corpora from sources such as translated documents and the web. What is more, Yalign is not limited to any language pair. However, creation of alignment models for the languages is necessary.\nThe Yalign tool was implemented using a sentence similarity metric that produces a rough estimate (a number between 0 and 1) of how likely it is for two sentences to be a translation of each other. Additionally, it uses a sequence aligner that produces an alignment that maximizes the sum of the individual (per sentence pair) similarities between two documents. Yalign\u2019s main algorithm is actually a wrapper applied prior to standard sequence alignment algorithm [14].\nFor the sequence alignment, Yalign uses a variation of the Needleman-Wunch algorithm [15] to find an optimal alignment between the sentences in two given documents. The algorithm has a polynomial time worst-case complexity, and it produces an optimal alignment. Unfortunately, it cannot handle alignments that cross each other or alignments from two sentences into a single one [15].\nSince calculating sentence similarity is a computationally expensive operation, the implemented variation of the Needleman-Wunch algorithm uses an A* approach to explore the search space instead of using the classical dynamic programming method that requires N * M calls to the sentence similarity matrix.\nAfter the alignment, only sentences that have a high probability of being translations are included in the final alignment. The result is filtered in order to deliver high quality alignments. To do this, a threshold value is used. If the sentence similarity metric is low enough, the pair is excluded.\nFor the sentence similarity metric, the algorithm uses a statistical classifier\u2019s likelihood output and adapts it into the <0,1> range.\nThe classifier must be trained in order to determine if a pair of sentences is an acceptable translation of each other or not. The particular classifier used in the Yalign project was a Support Vector Machine (SVM). Besides being excellent classifiers, SVMs can provide a distance to the separation hyper-plane during classification, and this distance can be easily modified using a sigmoid function to return a likelihood between 0 and 1 [16].\nThe use of a classifier means that the quality of the alignment depends not only on the input but also on the quality of the trained classifier.\nTo train the classifier, a good quality parallel dataset was necessary, as well as a dictionary with translation probability included. TED talks corpora were used for these purposes. In order to obtain a dictionary, a phrase table was trained and 1-grams were extracted from it. We used the MGIZA++ tool for word and phrase alignment. The lexical reordering was set to use the msd-bidirectional-fe method and the symmetrization method was set to grow-diag-final-and for word alignment processing [13].\nBefore use of a training translation model, preprocessing that included removal of long sentences (set to 80 words) had to be performed. The Moses toolkit scripts [18] were used for this purpose."}, {"heading": "3 Proposed Corpora Filter", "text": "A bi-sentence filtering tool was designed to find an English translation of each Polish line in a corpus and place it in the correct place in the English file. The tool assumes that each line in a text file represents one full sentence.\nOur first concept is to use the Google Translator Application Programming Interface (API) for lines for which an English translation does not exist and also for comparison between the original and translated texts. The second concept is based on web crawling, using Google Translator, Bing Translator, and Babylon translator. These can work in a parallel manner to improve performance. In addition, each translator can work in many instances. The approach can also accommodate a userprovided translation file in lieu of crowd sourcing. Any machine translation system can be used, as well, as a source of translations.\nThe strategy is to find a correct translation of each Polish line aided by Google Translator or another translation engine. The tool translates all lines of the Polish file\n(src.pl) with Google Translator and puts each line translation in an intermediate English translation file (src.trans). This intermediate translation helps in finding the correct line in the English translation file (src.en) and putting it in the correct position (Figure 1).\nIn reality, the actual solution is more complex. Suppose that we choose one of the English data lines\nFigure 1. Comparison Algorithm\nas the most similar line to the specific translated line and that line has a similarity rate high enough to be accepted as the translation. This line may be more similar to the next line of src.trans, such that the similarity rate of this selected line and the next line of src.trans is higher. For example, consider the sentences and their similarity rating in Table 2.\nIn this situation, we should select \u201cI do not go to school every day\u201d from src.en instead of \u201cI don't go to school every day\u201d from src.trans, and not \u201cI go to school every day.\u201d So, we should consider the similarity of a selected line with the next lines of src.trans to make the best possible selection in the alignment process.\nThere are additional complexities that must be addressed. Comparing the src.trans lines with the src.en lines is not easy, and it becomes harder when we want to use the similarity rate to choose the correct, real-world translation. There are many strategies to compare two sentences. We can split each sentence into its words and find the number of common words in both sentences. However, this approach has some problems. For example, let us compare \u201cIt is origami.\u201d to these sentences: \u201cThe common theme what makes it origami is folding is how we create the form.\u201d; \u201cThis is origami.\u201d\nWith this strategy, the first sentence is more similar, because it contains all 3 words. However, it is clear that the second sentence is the correct choice. We can solve this problem by dividing the number of words in both sentences by the number of total words in the sentences. However, counting stop words in the intersection of sentences sometimes causes incorrect results. So, we remove these words before comparing two sentences.\nAnother problem is that sometimes we find stemmed words in sentences, for example \u201cboy\u201d and \u201cboys.\u201d Despite the fact that these two words should be counted as similar in two sentences, these words are not counted with this strategy.\nThe next comparison problem is the word order in sentences. In Python there are other ways for comparing strings that are better than counting intersection lengths. The Python \u201cdifflib\u201d library for string comparison contains a function that first finds\nmatching blocks of two strings. For example, we can use difflib to find matching blocks in the strings \"abxcd\" and \"abcd.\"\nDifflib\u2019s \u201cratio\u201d function divides the length of matching blocks by the length of two strings and returns a measure of the sequences\u2019 similarity as a float value in the range [0, 1]. This measure is 2.0*M / T, where T is the total number of elements in both sequences, and M is the number of matches. Note that this measure is 1.0 if the sequences are identical, and 0.0 if they have nothing in common. Using this function to compare strings instead of counting similar words helps to solve the problem of the similarity of \u201cboy\u201d and \u201cboys.\u201d It also solves the problem of considering the position of words in sentences.\nAnother problem in comparing lines is synonyms. For example, consider these sentences: \u201cI will call you tomorrow.\u201d; \u201cI would call you tomorrow.\u201d If we want to know if these sentences are the same, we should know that \u201cwill\u201d and \u201cwould\u201d can be used interchangeably.\nThe NLTK Python module and WordNet\u00ae were used to find synonyms for each word and use these synonyms in comparing sentences. Using synonyms of each word, we created multiple sentences from each original sentence.\nFor example, suppose that the word \u201cgame\u201d has the synonyms: \u201cplay\u201d, \u201csport\u201d, \u201cfun\u201d, \u201cgaming\u201d, \u201caction\u201d, and \u201cskittle\u201d. If we use, for example, the sentence \u201cI do not like game.\u201d, we create the following sentences: \u201cI do not like play.\u201d; \u201cI do not like sport.\u201d; \u201cI do not like fun.\u201d; \u201cI do not like gaming.\u201d; \u201cI do not like action.\u201d; and \u201cI do not like skittle.\u201d We must use every word in a sentence.\nNext, we should try to find the best score by comparing all these sentences instead of just comparing the main sentence. One issue is that this type of comparison takes much time, because the algorithm needs to do many comparisons for each selection.\nDifflib has other functions (in SequenceMatcher and Diff class) to compare strings that are faster than the described solution, but their accuracy is worse. To overcome all these problems and obtain the best results, we should consider two criteria: the speed of the comparison function and the comparison acceptance rate.\nTo obtain the best results, the script provides users with the ability to specify multiple functions with multiple acceptance rates. Fast functions with lower quality results are tested first. If they can find results with a very high acceptance rate, we should accept their selection. If the acceptance rate is not sufficient, we can use slower but more accurate functions. The user can configure these rates manually and test the resulting quality to get the best results. All are well described in documentation [19]."}, {"heading": "4 Existing Evaluation Techniques", "text": "This section describes existing SMT evaluation techniques that highly correlate with human judgments. BLEU was developed based on a premise similar to that used for speech recognition, described in [9] as: \u201cThe closer a machine translation is to a professional human translation, the better it is.\u201d So, the BLEU metric is designed to measure how close SMT output is to that of human reference translations. It is\nimportant to note that translations, SMT or human, may differ significantly in word usage, word order, and phrase length [9].\nTo address these complexities, BLEU attempts to match variable length phrases between SMT output and reference translations. Weighted match averages are used to determine the translation score. [30]\nA number of variations of the BLEU metric exist. However, the basic metric requires calculation of a brevity penalty !\", which is calculated as follows:\n!\" = 1, & > (\n) *+, - , & \u2264 (\nwhere r is the length of the reference corpus, and the candidate (reference) translation length is given by c. [30]\nThe basic BLEU metric is then determined as shown in [30]:\n/012 = !\" exp 67\n8\n79:;\nlog ?7\nwhere 67 are positive weights summing to one, and the n-gram precision ?7 is calculated using n-grams with a maximum length of N.\nThere are several other important features of BLEU. First, word and phrase position within text are not evaluated by this metric. To prevent SMT systems from artificially inflating their scores by overuse of words known with high confidence, each candidate word is constrained by the word count of the corresponding reference translation. A geometric mean of individual sentence scores, with consideration of the brevity penalty, is then calculated for the entire corpus. [30]\nThe NIST metric was designed to improve BLEU by rewarding the translation of infrequently used words. This was intended to further prevent inflation of SMT evaluation scores by focusing on common words and high confidence translations. As a result, the NIST metric uses heavier weights for rarer words. The final NIST score is calculated using the arithmetic mean of the n-gram matches between SMT and reference translations. In addition, a smaller brevity penalty is used for smaller variations in phrase lengths. The reliability and quality of the NIST metric has been shown to be superior to the BLEU metric. [31]\nTranslation Edit Rate (TER) was designed to provide a very intuitive SMT evaluation metric, requiring less data than other techniques while avoiding the labor intensity of human evaluation. It calculates the number of edits required to make a machine translation match exactly to the closest reference translation in fluency and semantics. [18, 32]\nCalculation of the TER metric is defined in [18]:\n@1A = ; 1\n6B\nwhere E represents the minimum number of edits required for an exact match, and the average length of the reference text is given by wR. Edits may include the\ndeletion of words, word insertion, word substitutions, as well as changes in word or phrase order. [18]\nThe Metric for Evaluation of Translation with Explicit Ordering (METEOR) metric is intended to take several factors that are indirect in BLEU into account more directly. Recall (the proportion of matched n-grams to total reference n-grams) is used directly in this metric. In addition, METEOR explicitly measures higher order n-grams, considers word-to-word matches, and applies arithmetic averaging for a final score. Best matches against multiple reference translations are used. [33]\nThe METEOR method uses a sophisticated and incremental word alignment method that starts by considering exact word-to-word matches, word stem matches, and synonym matches. Alternative word order similarities are then evaluated based on those matches. [33]\nCalculation of precision is similar in the METEOR and NIST metrics. Recall is calculated at the word level. To combine the precision and recall scores, METEOR uses a harmonic mean. METEOR also rewards longer n-gram matches. [33]\nThe METEOR metric is calculated as shown in [33]:\nC1@1DA = ; 10;!;A\nA + 9;! 1 \u2212;!I\nwhere the unigram recall and precision are given by R and P, respectively. The brevity penalty PM is determined by:\n!I = 0.5; L\nCM\nwhere MU is the number of matching unigrams, and C is the minimum number of phrases required to match unigrams in the SMT output with those found in the reference translations."}, {"heading": "5 Comparison Experiments", "text": "Experiments were performed in order to compare the performance of the proposed method with human judgment. First, the Polish side of corpora was repaired, spellchecked, and cleaned by human translators. They were supposed to remove lines with translation into other languages or improper ones. The same was done with the usage of the filtering tool. Table 3 shows how many sentence pairs remained after the filtering, as well as the number of distinct words and their forms in the TED data.\nFinally, the SMT systems were trained on the original and cleaned data to show the influence on the results. The results were compared using the BLEU, NIST, METEOR and TER metrics, which described in Section 5. The results are shown in Table 4. In these experiments, tuning was disabled because of the known MERT instability [10]. Test and development sets were taken from the IWSLT13 evaluation campaign and cleaned before usage with the help of human translators.\nThe SMT experiments involved a number of steps. Processing of the corpora included: tokenization, cleaning, factorization, conversion to lower case, splitting, and a final cleaning after splitting. Training data was processed, and the language model was developed.\nThe system testing was done using the Moses open source SMT toolkit with its Experiment Management System (EMS) [23]. The SRI Language Modeling Toolkit (SRILM) [18] with an interpolated version of the Kneser-Key discounting (interpolate \u2013unk \u2013kndiscount) was used for 5-gram language model training. We used the MGIZA++ tool for word and phrase alignment. KenLM [24] was used to binarize the language model, with a lexical reordering set to use the msdbidirectional-fe model.\nReordering probabilities of phrases are conditioned on lexical values of a phrase. It considers three different orientation types on source and target phrases: monotone (M), swap (S), and discontinuous (D). The bidirectional reordering model adds probabilities of possible mutual positions of source counterparts to current and subsequent phrases. Probability distribution to a foreign phrase is determined by \u201cf\u201d and to the English phrase by \u201ce\u201d [25].\nMGIZA++ is a multi-threaded version of the well- known GIZA++ tool [26]. The symmetrization method was set to grow-diag-final-and for word alignment processing. First, the two-way direction alignments obtained from GIZA++ were intersected, so only the alignment points that occurred in both alignments remained. In the second phase, additional alignment points existing in their union were added. The growing step adds potential alignment points of unaligned words and neighbors. Neighborhood can be set directly to left, right, top, bottom, or diagonal (grow-diag). In the final step, alignment points between words from which at least one is\nunaligned are added (grow-diag-final). If the grow-diag-final-and method is used, an alignment point between two unaligned words appears. [27]\nFor the Wikipedia comparable corpus filtration, an initial experiment based on 1000 randomly selected bi-sentences from the corpora was conducted. The filtering tool processed the data. Most of the noisy data was removed but also some good translations were lost. Nevertheless, results are promising, and we intend to filter entire corpora in the future. It also must be noted that the filtering tool was not adjusted to this specific text domain. The results are presented in Table 5."}, {"heading": "6 Conclusions and Results", "text": "In general, it is a very important to create high quality parallel text corpora. Obviously, employing humans for this task is far too expensive because of the need for tremendous amounts of data to be processed. Analysis of the results of these experiments leads to the conclusion that the solution fits somewhere in between noisy parallel corpora and a corpora improved by human translators. The proposed approach is also language independent for languages with similar structure to PL or EN. The results show that the proposed method performed well in terms of the statistical machine translation evaluation. The method can also be used for improvement of noisy data, as well as comparable data. The filtering method provided a better score as compared with the baseline system, and would most likely improve the output quality other MT systems.\nIn summary, the bi-sentence extraction task is becoming more popular in unsupervised learning for numerous tasks. This method overcomes the disparities between English and Polish or any other West-Slavic language. It is a language independent method that can easily be adjusted to a new environment, and it only requires the adjustment of initial parameters. The experiments show that the method performs well. The corpora used here increased the MT quality in a wide text domain, the TED Talks. We can assume that even small differences can make a positive influence on real life translation scenarios. From a practical point of view, the method requires neither expensive training nor language-specific grammatical resources, while producing satisfactory results."}, {"heading": "7 Acknowledgements", "text": "This work is supported by the European Community from the European Social Fund within the Interkadra project UDA-POKL-04.01.01-00-014/10-00 and EuBridge 7th FR EU project (grant agreement n\u00b0287658)."}], "references": [{"title": "Segmentation and alignment of parallel text for statistical machine translation,", "author": ["Y. Deng", "S. Kumar", "W. Byrne"], "venue": "Natural Language Engineering,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Improved Unsupervised Sentence Alignment for Symmetrical and Asymmetrical Parallel Corpora", "author": ["Braune F", "A. Fraser"], "venue": "Coling 2010: Poster Volume,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Wit3: Web inventory of transcribed and translated talks", "author": ["M. Cettolo", "C. Girardi", "M. Federico"], "venue": "Proc. of 16th Conference of the European Association for Machine Translation (EAMT),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "A survey on parallel corpora alignment", "author": ["A. Santos"], "venue": "MI-STAR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Identifying word correspondences in parallel texts", "author": ["W.A. Gale", "K.W. Church"], "venue": "Proc. of DARPA Workshop on Speech and Natual Language,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1991}, {"title": "Parallel corpora for medium density languages", "author": ["D Varga"], "venue": "Proc. of the RANLP", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "A Sentence Meaning Based Alignment Method for Parallel Text Corpora Preparation", "author": ["K. Wo\u0142k", "K. Marasek"], "venue": "Advances in Intelligent Systems and Computing volume 275,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Rouskos", "T. Ward", "W.J. Zhu"], "venue": "Proc. of 40 Annual Meeting of the Assoc. for Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Real-Time Statistical Speech Translation", "author": ["K. Wo\u0142k", "K. Marasek"], "venue": "Advances in Intelligent Systems and Computing volume 275,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Methods for Smoothing the Optimizer Instability in SMT", "author": ["Cetollo M", "N. Bertoldi", "M. Federico"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Aligning sentences in Parallel Corpora", "author": ["P.F. Brown", "J.C. Lai", "R.L. Mercer"], "venue": "Proceedings of 20 Annual Meeting of the ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "Inversion Transduction Grammar Constraints for Mining Parallel Sentences from Quasi-Comparable Corpora\u201d", "author": ["D. Wu", "P. Fung"], "venue": "Natural Language Processing \u2013 IJCNLP", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Polish \u2013 English Speech Statistical Machine Translation Systems for the IWSLT 2013.", "author": ["K. Wo\u0142k", "K. Marasek"], "venue": "Proceedings of the 10th International Workshop on Spoken Language Translation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Text Categorization with Support Vector Machines: Learning with Many Relevant Features", "author": ["T. Joachims"], "venue": "Lecture Notes in Computer Science Volume", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Moses: Open Source Toolkit for Statistical Machine Translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "R. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the ACL 2007 Demo and Poster Sessions,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Statistical Machine Translation Between New Language Pairs Using Multiple Intermediaries (Doctoral dissertation, Thesis)", "author": ["A. Schmidt"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Machine translation evaluate versus quality estimation", "author": ["L. Specia", "D. Raj", "M. Turchi"], "venue": "Machine Translation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Web Mining based Extraction of Problem Solution Ideas", "author": ["D. Thorleuchter", "D. Van den Poel"], "venue": "Expert Systems with Applications,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "A Study of Translation Edit Rate with Targeted Human Annotation", "author": ["M. Snover", "B. Dorr", "R. Schwartz", "L. Micciulla", "J. Makhoul"], "venue": "Proc. of 7th Conference of the Assoc. for Machine Translation in the Americas,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "KenLM: Faster and smaller language model queries", "author": ["K. Heafield"], "venue": "Proc. of Sixth Workshop on Statistical Machine Translation, Association for Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Using linear interpolation and weighted reordering hypotheses in the Moses", "author": ["M.R. Costa-Jussa", "J.R. Fonollosa"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "SRILM \u2013 An Extensible Language Modeling Toolkit", "author": ["A. Stolcke"], "venue": "INTERSPEECH,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2002}, {"title": "Parallel Implementations of Word Alignment Tool\u201d, Software Engineering, Testing, and Quality Assurance for Natural Language Processing", "author": ["Q. Gao", "S. Vogel"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Multiple Model Text Normalization for the Polish Language", "author": ["\u0141. Brocki", "K. Marasek", "D. Kor\u017einek"], "venue": "Foundations of Intelligent Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Thus, it is crucial to MT system accuracy [1].", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "Corpora alignment must also be computationally feasible in order to be of a practical use in various applications [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "The sentence order follows the Subject-Verb-Object (SVO) schema, with the subject phrase preceding the predicate [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "The dataset used for this research was the Translanguage English Database (TED) [3] provided by Fondazione Bruno Kessler (FBK) for IWSLT 2013.", "startOffset": 80, "endOffset": 83}, {"referenceID": 11, "context": "A quasi-comparable corpus includes very heterogeneous and non-parallel bilingual documents that may or may not be topicaligned [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 3, "context": "Previous attempts to automatically compare sentences for parallel corpora were based on sentence lengths, together with vocabulary alignment [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 10, "context": "Brown\u2019s method [11] was based on measuring sentence length by the number of words.", "startOffset": 15, "endOffset": 19}, {"referenceID": 4, "context": "Gale and Church [5] measured the number of characters in sentences.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "Other researchers continued exploring various methods of combining sentence length statistics with alignment of vocabularies [6, 7].", "startOffset": 125, "endOffset": 131}, {"referenceID": 6, "context": "Other researchers continued exploring various methods of combining sentence length statistics with alignment of vocabularies [6, 7].", "startOffset": 125, "endOffset": 131}, {"referenceID": 7, "context": "The transcripts themselves are provided as pure text encoded i UTF-8 [8].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "A very large Polish dictionary [2] consisting of 2,532,904 different words was extracted.", "startOffset": 31, "endOffset": 34}, {"referenceID": 12, "context": "6% of the whole TED Polish vocabulary [13].", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "Besides being excellent classifiers, SVMs can provide a distance to the separation hyper-plane during classification, and this distance can be easily modified using a sigmoid function to return a likelihood between 0 and 1 [16].", "startOffset": 223, "endOffset": 227}, {"referenceID": 12, "context": "The lexical reordering was set to use the msd-bidirectional-fe method and the symmetrization method was set to grow-diag-final-and for word alignment processing [13].", "startOffset": 161, "endOffset": 165}, {"referenceID": 14, "context": "The Moses toolkit scripts [18] were used for this purpose.", "startOffset": 26, "endOffset": 30}, {"referenceID": 0, "context": "\" Difflib\u2019s \u201cratio\u201d function divides the length of matching blocks by the length of two strings and returns a measure of the sequences\u2019 similarity as a float value in the range [0, 1].", "startOffset": 177, "endOffset": 183}, {"referenceID": 8, "context": "BLEU was developed based on a premise similar to that used for speech recognition, described in [9] as: \u201cThe closer a machine translation is to a professional human translation, the better it is.", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "important to note that translations, SMT or human, may differ significantly in word usage, word order, and phrase length [9].", "startOffset": 121, "endOffset": 124}, {"referenceID": 14, "context": "[18, 32] Calculation of the TER metric is defined in [18]:", "startOffset": 0, "endOffset": 8}, {"referenceID": 14, "context": "[18, 32] Calculation of the TER metric is defined in [18]:", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "[18] The Metric for Evaluation of Translation with Explicit Ordering (METEOR) metric is intended to take several factors that are indirect in BLEU into account more directly.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "In these experiments, tuning was disabled because of the known MERT instability [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 18, "context": "The system testing was done using the Moses open source SMT toolkit with its Experiment Management System (EMS) [23].", "startOffset": 112, "endOffset": 116}, {"referenceID": 14, "context": "The SRI Language Modeling Toolkit (SRILM) [18] with an interpolated version of the Kneser-Key discounting (interpolate \u2013unk \u2013kndiscount) was used for 5-gram language model training.", "startOffset": 42, "endOffset": 46}, {"referenceID": 19, "context": "KenLM [24] was used to binarize the language model, with a lexical reordering set to use the msd-", "startOffset": 6, "endOffset": 10}, {"referenceID": 20, "context": "Probability distribution to a foreign phrase is determined by \u201cf\u201d and to the English phrase by \u201ce\u201d [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "MGIZA++ is a multi-threaded version of the well- known GIZA++ tool [26].", "startOffset": 67, "endOffset": 71}, {"referenceID": 22, "context": "[27] For the Wikipedia comparable corpus filtration, an initial experiment based on 1000 randomly selected bi-sentences from the corpora was conducted.", "startOffset": 0, "endOffset": 4}], "year": 2015, "abstractText": "Text alignment and text quality are critical to the accuracy of Machine Translation (MT) systems, some NLP tools, and any other text processing tasks requiring bilingual data. This research proposes a language independent bi-sentence filtering approach based on Polish (not a positionsensitive language) to English experiments. This cleaning approach was developed on the TED Talks corpus and also initially tested on the Wikipedia comparable corpus, but it can be used for any text domain or language pair. The proposed approach implements various heuristics for sentence comparison. Some of them leverage synonyms and semantic and structural analysis of text as additional information. Minimization of data loss was ensured. An improvement in MT system score with text processed using the tool is discussed.", "creator": null}}}