{"id": "1510.08906", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Oct-2015", "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning", "abstract": "recently, thus there has rarely been undergone significant substantive progress in understanding such reinforcement algorithm learning procedures in discounted infinite - likelihood horizon posterior markov complex decision processes ( po mdps ) predominantly by deriving tight defined sample complexity bounds. however, in many social real - world applications, an interactive based learning research agent operates for virtually a traditionally fixed priority or bounded period goal of standard time, for every example tutoring of students for exams or externally handling customer internal service evaluation requests. geographically such critical scenarios can generally often be better properly treated informally as episodic truncated fixed - horizon mdps, typically for individuals which seemingly only looser bounds on checking the existing sample tree complexity hurdle exist. a natural basic notion of sample complexity in all this market setting is the analytic number inequality of performance episodes algorithms required to regularly guarantee achieved a globally certain performance with high parameter probability ( pac / guarantee ). continuing in using this 2012 paper, as we derive up an essential upper pac bound $ \\ tilde o ( \\ p frac { | \\ mathcal int s | ^ 2 | \\ mathcal tail a | \u00ab h ^ 2 } { \\ epsilon ^ 2 } \\ ln \\ cn frac \u27e8 1 \\ zeta delta ) $ and lastly a \\ lower lower pac bound $ \\ tilde \\ omega ( \\ - frac { | \\ mathcal s | | \\ mathcal a | for h ^ \u2032 2 } { \\ epsilon ^ 2 } \\... ln \\ ng frac 1 { \\ \u03b4 delta + ] c } ) $ # that extends match divide up substantially to log - terms and an additional linear dependency on reducing the number of states $ | \\ # mathcal s | $. the lower bound q is the ultimate first of its possible kind proposed for this game setting. our upper bound dramatically leverages bernstein's inequality to improve them on using previous standard bounds for episodic finite - horizon mdps which additionally have a time - horizon dependency property of n at least $ h ^ 4 3 $.", "histories": [["v1", "Thu, 29 Oct 2015 21:14:42 GMT  (64kb,D)", "https://arxiv.org/abs/1510.08906v1", "26 pages, appears in Neural Information Processing Systems (NIPS) 2015"], ["v2", "Tue, 19 Jan 2016 16:36:03 GMT  (65kb,D)", "http://arxiv.org/abs/1510.08906v2", "28 pages, appeared in Neural Information Processing Systems (NIPS) 2015, updated version with fixed typos and modified Lemma 1"], ["v3", "Wed, 11 May 2016 15:27:28 GMT  (65kb,D)", "http://arxiv.org/abs/1510.08906v3", "28 pages, appeared in Neural Information Processing Systems (NIPS) 2015, updated version with fixed typos and modified Lemma 1 and Lemma C.5"]], "COMMENTS": "26 pages, appears in Neural Information Processing Systems (NIPS) 2015", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["christoph dann", "emma brunskill"], "accepted": true, "id": "1510.08906"}, "pdf": {"name": "1510.08906.pdf", "metadata": {"source": "CRF", "title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning", "authors": ["Christoph Dann", "Emma Brunskill"], "emails": ["cdann@cdann.net", "ebrun@cs.cmu.edu"], "sections": [{"heading": null, "text": "2|A|H2 2 ln 1 \u03b4 ) and a lower PAC bound \u2126\u0303( |S||A|H2 2 ln 1 \u03b4+c )\nthat match up to log-terms and an additional linear dependency on the number of states |S|. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein\u2019s inequality to improve on previous bounds for episodic finitehorizon MDPs which have a time-horizon dependency of at least H3."}, {"heading": "1 Introduction and Motivation", "text": "Consider test preparation software that tutors students for a national advanced placement exam taken at the end of a year, or maximizing business revenue by the end of each quarter. Each individual task instance requires making a sequence of decisions for a fixed number of steps H (e.g., tutoring one student to take an exam in spring 2015 or maximizing revenue for the end of the second quarter of 2014). Therefore, they can be viewed as a finite-horizon sequential decision making under uncertainty problem, in contrast to an infinite horizon setting in which the number of time steps is infinite. When the domain parameters (e.g. Markov decision process parameters) are not known in advance, and there is the opportunity to repeat the task many times (teaching a new student for each year\u2019s exam, maximizing revenue for each new quarter), this can be treated as episodic fixed-horizon reinforcement learning (RL). One important question is to understand how much experience is required to act well in this setting. We formalize this as the sample complexity of reinforcement learning [1], which is the number of time steps on which the algorithm may select an action whose value is not near-optimal. RL algorithms with a sample complexity that is a polynomial function of the domain parameters are referred to as Probably Approximately Correct (PAC) [2, 3, 4, 1]. Though there has been significant work on PAC RL algorithms for the infinite horizon setting, there has been relatively little work on the finite horizon scenario.\nIn this paper we present the first, to our knowledge, lower bound, and a new upper bound on the sample complexity of episodic finite horizon PAC reinforcement learning in discrete state-action spaces. Our bounds are tight up to log-factors in the time horizon H , the accuracy , the number of actions |A| and up to an additive constant in the failure probability \u03b4. These bounds improve upon existing results by a factor of at least H . Our results also apply when the reward model is a function of the within-episode time step in addition to the state and action space. While we assume a stationary transition model, our results can be extended readily to time-dependent state-\nar X\niv :1\n51 0.\n08 90\n6v 3\n[ st\nat .M\nL ]\n1 1\nM ay\ntransitions. Our proposed UCFH (Upper-confidence fixed-horizon RL) algorithm that achieves our upper PAC guarantee can be applied directly to wide range of fixed-horizon episodic MDPs with known rewards.1 It does not require additional structure such as assuming access to a generative model [8] or that the state transitions are sparse or acyclic [6].\nThe limited prior research on upper bound PAC results for finite horizon MDPs has focused on different settings, such as partitioning a longer trajectory into fixed length segments [4, 1], or considering a sliding time window [9]. The tightest dependence on the horizon in terms of the number of episodes presented in these approaches is at least H3 whereas our dependence is only H2. More importantly, such alternative settings require the optimal policy to be stationary, whereas in general in finite horizon settings the optimal policy is nonstationary (e.g. is a function of both the state and the within episode time-step).2 Fiechter [10, 11] and Reveliotis and Bountourelis [12] do tackle a closely related setting, but find a dependence that is at least H4.\nOur work builds on recent work [6, 8] on PAC infinite horizon discounted RL that offers much tighter upper and lower sample complexity bounds than was previously known. To use an infinite horizon algorithm in a finite horizon setting, a simple change is to augment the state space by the time step (ranging over 1, . . . ,H), which enables the learned policy to be non-stationary in the original state space (or equivalently, stationary in the newly augmented space). Unfortunately, since these recent bounds are in general a quadratic function of the state space size, the proposed state space expansion would introduce at least an additional H2 factor in the sample complexity term, yielding at least a H4 dependence in the number of episodes for the sample complexity.\nSomewhat surprisingly, we prove an upper bound on the sample complexity for the finite horizon case that only scales quadratically with the horizon. A key part of our proof is that the variance of the value function in the finite horizon setting satisfies a Bellman equation. We also leverage recent insights that state\u2013action pairs can be estimated to different precisions depending on the frequency to which they are visited under a policy, extending these ideas to also handle when the policy followed is nonstationary. Our lower bound analysis is quite different than some prior infinite-horizon results, and involves a construction of parallel multi-armed bandits where it is required that the best arm in a certain portion of the bandits is identified with high probability to achieve near-optimality."}, {"heading": "2 Problem Setting and Notation", "text": "We consider episodic fixed-horizon MDPs, which can be formalized as a tuple M = (S,A, r, p, p0, H). Both, the statespace S and the actionspace A are finite sets. The learning agent interacts with the MDP in episodes of H time steps. At time t = 1 . . . H , the agent observes a state st and choses an action at based on a policy \u03c0 that potentially depends on the within-episode time step, i.e., at = \u03c0t(st) for t = 1, . . . ,H . The next state is sampled from the stationary transition kernel st+1 \u223c p(\u00b7|st, at) and the initial state from s1 \u223c p0. In addition the agent receives a reward drawn from a distribution3 with mean rt(st) determined by the reward function. The reward function r is possibly time-dependent and takes values in [0, 1]. The quality of a policy \u03c0 is evaluated by the total expected reward of an episode R\u03c0M = E [\u2211H t=1 rt(st) ] . For simplicity,1 we assume that the reward function r is known to the agent but the transition kernel p is unknown. The question we study is how many episodes does a learning agent follow a policy \u03c0 that is not -optimal, i.e., R\u2217M \u2212 > R\u03c0M , with probability at least 1\u2212 \u03b4 for any chosen accuracy and failure probability \u03b4.\nNotation. In the following sections, we reason about the true MDP M , an empirical MDP M\u0302 and an optimistic MDP M\u0303 which are identical except for their transition probabilities p, p\u0302 and p\u0303t. We will provide more details about these MDPs later. We introduce the notation explicitly only for M but the quantities carry over to M\u0303 and M\u0302 with additional tildes or hats by replacing p with p\u0303t or p\u0302.\n1 Previous works [5] have shown that the complexity of learning state transitions usually dominates learning reward functions. We therefore follow existing sample complexity analyses [6, 7] and assume known rewards for simplicity. The algorithm and PAC bound can be extended readily to the case of unknown reward functions.\n2The best action will generally depend on the state and the number of remaining time steps. In the tutoring example, even if the student has the same state of knowledge, the optimal tutor decision may be to space practice if there is many days till the test and provide intensive short-term practice if the test is tomorrow.\n3It is straightforward to have the reward depend on the state, or state/action or state/action/next state.\nThe (linear) operator P\u03c0i f(s) := E[f(si+1)|si = s] = \u2211 s\u2032\u2208S p(s\n\u2032|s, \u03c0i(s))f(s\u2032) takes any function f : S \u2192 R and returns the expected value of f with respect to the next time step.4 For convenience, we define the multi-step version as P\u03c0i:jf := P \u03c0 i P \u03c0 i+1 . . . P \u03c0 j f . The value function from time i to\ntime j is defined as V \u03c0i:j(s) := E [\u2211j t=i rt(st)|si = s ] = \u2211j t=i P \u03c0 i:t\u22121rt = ( P\u03c0i V \u03c0 i+1:j ) (s) + ri(s) and V \u2217i:j is the optimal value-function. When the policy is clear, we omit the superscript \u03c0.\nWe denote by S(s, a) \u2286 S the set of possible successor states of state s and action a. The maximum number of them is denoted by C = maxs,a\u2208S\u00d7A |S(s, a)|. In general, without making further assumptions, we have C = |S|, though in many practical domains (robotics, user modeling) each state can only transition to a subset of the full set of states (e.g. a robot can\u2019t teleport across the building, but can only take local moves). The notation O\u0303 is similar to the usual O-notation but ignores log-terms. More precisely f = O\u0303(g) if there are constants c1, c2 such that f \u2264 c1g(ln g)c2 and analogously for \u2126\u0303. The natural logarithm is ln and log = log2 is the base-2 logarithm."}, {"heading": "3 Upper PAC-Bound", "text": "We now introduce a new model-based algorithm, UCFH, for RL in finite horizon episodic domains. We will later prove UCFH is PAC with an upper bound on its sample complexity that is smaller than prior approaches. Like many other PAC RL algorithms [3, 13, 14, 15], UCFH uses an optimism under uncertainty approach to balance exploration and exploitation. The algorithm generally works in phases comprised of optimistic planning, policy execution and model updating that take several episodes each. Phases are indexed by k. As the agent acts in the environment and observes (s, a, r, s\u2032) tuples, UCFH maintains a confidence set over the possible transition parameters for each state-action pair that are consistent with the observed transitions. Defining such a confidence set that holds with high probability can be be achieved using concentration inequalities like the Hoeffding inequality. One innovation in our work is to use a particular new set of conditions to define the confidence set that enables us to obtain our tighter bounds. We will discuss the confidence sets further below. The collection of these confidence sets together form a class of MDPsMk that are consistent with the observed data. We define M\u0302k as the maximum likelihood estimate of the MDP given the previous observations.\nGiven Mk, UCFH computes a policy \u03c0k by performing optimistic planning. Specifically, we use a finite horizon variant of extended value iteration (EVI) [5, 14]. EVI performs modified Bellman backups that are optimistic with respect to a given set of parameters. That is, given a confidence set of possible transition model parameters, it selects in each time step the model within that set that maximizes the expected sum of future rewards. Appendix A provides more details about fixed horizon EVI.\nUCFH then executes \u03c0k until there is a state-action pair (s, a) that has been visited often enough since its last update (defined precisely in the until-condition in UCFH). After updating the model statistics for this (s, a)-pair, a new policy \u03c0k+1 is obtained by optimistic planning again. We refer to each such iteration of planning-execution-update as a phase with index k. If there is no ambiguity, we omit the phase indices k to avoid cluttered notation.\nUCFH is inspired by the infinite-horizon UCRL-\u03b3 algorithm by Lattimore and Hutter [6] but has several important differences. First, the policy can only be updated at the end of an episode, so there is no need for explicit delay phases as in UCRL-\u03b3. Second, the policies \u03c0k in UCFH are time-dependent. Finally, UCFH can directly deal with non-sparse transition probabilities, whereas UCRL-\u03b3 only directly allows two possible successor states for each (s, a)-pair (C = 2).\nConfidence sets. The class of MDPsMk consists of fixed-horizon MDPsM \u2032 with the known true reward function r and where the transition probability p\u2032t(s\n\u2032|s, a) from any (s, a) \u2208 S \u00d7 A to s\u2032 \u2208 S(s, a) at any time t is in the confidence set induced by p\u0302(s\u2032|s, a) of the empirical MDP M\u0302 . Solely for the purpose of computationally more efficient optimistic planning, we allow time-dependent transitions (allows choosing different transition models in different time steps to maximize reward), but this does not affect the theoretical guarantees as the true stationary MDP is still inMk with high\n4The definition also works for time-dependent transition probabilities.\nAlgorithm 1: UCFH: Upper-Confidence Fixed-Horizon episodic reinforcement learning algorithm Input: desired accuracy \u2208 (0, 1], failure tolerance \u03b4 \u2208 (0, 1], fixed-horizon MDP M Result: with probability at least 1\u2212 \u03b4: -optimal policy k := 1, wmin := 4H|S| , \u03b41 := \u03b4 2UmaxC , Umax := |S \u00d7 A| log2 |S|H wmin ; m := 512(log2 log2H) 2CH2 2 log 2 ( 8H2|S|2 ) ln 6|S\u00d7A|C log22(4|S| 2H2/ ) \u03b4 ; n(s, a) = v(s, a) = n(s, a, s\u2032) := 0 \u2200, s \u2208 S, a \u2208 A, s\u2032 \u2208 S(s, a); while do\n/* Optimistic planning */ p\u0302(s\u2032|s, a) := n(s, a, s\u2032)/n(s, a), for all (s, a) with n(s, a) > 0 and s\u2032 \u2208 S(s, a); Mk := { M\u0303 \u2208Mnonst. : \u2200(s, a) \u2208 S \u00d7A, t = 1 . . . H, s\u2032 \u2208 S(s, a)\np\u0303t(s \u2032|s, a) \u2208 ConfidenceSet(p\u0302(s\u2032|s, a), n(s, a)) } ;\nM\u0303k, \u03c0 k := FixedHorizonEVI(Mk); /* Execute policy */ repeat\nSampleEpisode(\u03c0k) ; // from M using \u03c0k until there is a (s, a) \u2208 S \u00d7A with v(s, a) \u2265 max{mwmin, n(s, a)} and n(s, a) < |S|mH; /* Update model statistics for one (s, a)-pair with condition above */ n(s, a) := n(s, a) + v(s, a); n(s, a, s\u2032) := n(s, a, s\u2032) + v(s, a, s\u2032) \u2200s\u2032 \u2208 S(s, a); v(s, a) := v(s, a, s\u2032) := 0 \u2200s\u2032 \u2208 S(s, a); k := k + 1\nProcedure SampleEpisode(\u03c0) s0 \u223c p0; for t = 0 to H \u2212 1 do\nat := \u03c0t+1(st) and st+1 \u223c p(\u00b7|st, at); v(st, at) := v(st, at) + 1 and v(st, at, st+1) := v(st, at, st+1) + 1;\nFunction ConfidenceSet(p, n) P := { p\u2032 \u2208 [0, 1] :if n > 1 : \u2223\u2223\u2223\u221ap\u2032(1\u2212 p\u2032)\u2212\u221ap(1\u2212 p)\u2223\u2223\u2223 \u2264\u221a2 ln(6/\u03b41) n\u2212 1 , (1)\n|p\u2212 p\u2032| \u2264 min\n(\u221a ln(6/\u03b41)\n2n ,\n\u221a 2p(1\u2212 p)\nn ln(6/\u03b41) +\n7\n3(n\u2212 1) ln\n6\n\u03b41 )} (2)\nreturn P\nprobability. Unlike the confidence intervals used by Lattimore and Hutter [6], we not only include conditions based on Hoeffding\u2019s inequality5 and Bernstein\u2019s inequality (Eq. 2), but also require that the standard deviation \u221a p(1\u2212 p) of the Bernoulli random variable associated with this transition is close to the empirical one (Eq. 1). This additional condition (Eq. 1) is key for making the algorithm directly applicable to generic MDPs (in which states can transition to any number of next states, e.g. C > 2) while only having a linear dependency on C in the PAC bound."}, {"heading": "3.1 PAC Analysis", "text": "For simplicity we assume that each episode starts in a fixed start state s0. This assumption is not crucial and can easily be removed by additional notational effort.\nTheorem 1. For any 0 < , \u03b4 \u2264 1, the following holds. With probability at least 1 \u2212 \u03b4, UCFH produces a sequence of policies \u03c0k, that yield at most\nO\u0303\n( H2C|S \u00d7 A|\n2 ln\n1\n\u03b4 ) 5The first condition in the min in Equation (2) is actually not necessary for the theoretical results to hold. It\ncan be removed and all 6/\u03b41 can be replaced by 4/\u03b41.\nepisodes with R\u2217 \u2212R\u03c0k = V \u22171:H(s0)\u2212 V \u03c0 k\n1:H(s0) > . The maximum number of possible successor states is denoted by 1 < C \u2264 |S|.\nSimilarities to other analyses. The proof of Theorem 1 is quite long and involved, but builds on similar techniques for sample-complexity bounds in reinforcement learning (see e.g. Brafman and Tennenholtz [3], Strehl and Littman [16]). The general proof strategy is closest to the one of UCRL-\u03b3 [6] and the obtained bounds are similar if we replace the time horizon H with the equivalent in the discounted case 1/(1\u2212 \u03b3). However, there are important differences that we highlight now briefly.\n\u2022 A central quantity in the analysis by Lattimore and Hutter [6] is the local variance of the value function. The exact definition for the fixed-horizon case will be given below. The key insight for the almost tight bounds of Lattimore and Hutter [6] and Azar et al. [8] is to leverage the fact that these local variances satisfy a Bellman equation [17] and so the discounted sum of local variances can be bounded by O((1\u2212\u03b3)\u22122) instead of O((1\u2212\u03b3)\u22123). We prove in Lemma 4 that local value function variances \u03c32i:j also satisfy a Bellman equation for fixed-horizon MDPs even if transition probabilities and rewards are time-dependent. This allows us to bound the total sum of local variances by O(H2) and obtain similarly strong results in this setting.\n\u2022 Lattimore and Hutter [6] assumed there are only two possible successor states (i.e., C = 2) which allows them to easily relate the local variances \u03c32i:j to the difference of the expected value of successor states in the true and optimistic MDP (Pi \u2212 P\u0303i)V\u0303i+1:j . For C > 2, the relation is less clear, but we address this by proving a bound with tight dependencies on C (Lemma C.6).\n\u2022 To avoid super-linear dependency on C in the final PAC bound, we add the additional condition in Equation (1) to the confidence set. We show that this allows us to upper-bound the total reward difference R\u2217 \u2212 R\u03c0k of policy \u03c0k with terms that either depend on \u03c32i:j or decrease linearly in the number of samples. This gives the desired linear dependency on C in the final bound. We therefore avoid assuming C = 2 which makes UCFH directly applicable to generic MDPs with C > 2 without the impractical transformation argument used by Lattimore and Hutter [6].\nWe will now introduce the notion of knownness and importance of state-action pairs that is essential for the analysis of UCFH and subsequently present several lemmas necessary for the proof of Theorem 1. We only sketch proofs here but detailed proofs for all results are available in the appendix.\nFine-grained categorization of (s, a)-pairs. Many PAC RL sample complexity proofs [3, 4, 13, 14] only have a binary notion of \u201cknownness\u201d, distinguishing between known (transition probability estimated sufficiently accurately) and unknown (s, a)-pairs. However, as recently shown by Lattimore and Hutter [6] for the infinite horizon setting, it is possible to obtain much tighter sample complexity results by using a more fine grained categorization. In particular, a key idea is that in order to obtain accurate estimates of the value function of a policy from a starting state, it is sufficient to have only a loose estimate of the parameters of (s, a)-pairs that are unlikely to be visited under this policy.\nLet the weight of a (s, a)-pair given policy \u03c0k be its expected frequency in an episode\nwk(s, a) := H\u2211 t=1 P(st = s, \u03c0kt (st) = a) = H\u2211 t=1 P1:t\u22121I{s = \u00b7, a = \u03c0kt (s)}(s0).\nThe importance \u03b9k of (s, a) is its relative weight compared to wmin := 4H|S| on a log-scale\n\u03b9k(s, a) := min { zi : zi \u2265 wk(s, a)\nwmin\n} where z1 = 0 and zi = 2i\u22122 \u2200i = 2, 3, . . . .\nNote that \u03b9k(s, a) \u2208 {0, 1, 2, 4, 8, 16 . . . } is an integer indicating the influence of the state-action pair on the value function of \u03c0k. Similarly, we define the knownness\n\u03bak(s, a) := max { zi : zi \u2264 nk(s, a)\nmwk(s, a)\n} \u2208 {0, 1, 2, 4, . . . }\nwhich indicates how often (s, a) has been observed relative to its importance. The constant m is defined in Algorithm 1. We can now categorize (s, a)-pairs into subsets\nXk,\u03ba,\u03b9 := {(s, a) \u2208 Xk : \u03bak(s, a) = \u03ba, \u03b9k(s, a) = \u03b9} and X\u0304k = S \u00d7A \\Xk\nwhere Xk = {(s, a) \u2208 S \u00d7 A : \u03b9k(s, a) > 0} is the active set and X\u0304k the set of state-action pairs that are very unlikely under the current policy. Intuitively, the model of UCFH is accurate if only few (s, a) are in categories with low knownness \u2013 that is, important under the current policy but have not been observed often so far. Recall that over time observations are generated under many policies (as the policy is recomputed), so this condition does not always hold. We will therefore distinguish between phases k where |Xk,\u03ba,\u03b9| \u2264 \u03ba for all \u03ba and \u03b9 and phases where this condition is violated. The condition essentially allows for only a few (s, a) in categories that are less known and more and more (s, a) in categories that are more well known. In fact, we will show that the policy is -optimal with high probability in phases that satisfy this condition.\nWe first show the validity of the confidence setsMk. Lemma 1 (Capturing the true MDP whp.). M \u2208Mk for all k with probability at least 1\u2212 \u03b4/2.\nProof Sketch. By combining Hoeffding\u2019s inequality, Bernstein\u2019s inequality and the concentration result on empirical standard deviations by Maurer and Pontil [18] with the union bound, we get that p(s\u2032|s, a) \u2208 P with probability at least 1 \u2212 \u03b41 for a single phase k, fixed s, a \u2208 S \u00d7 A and fixed s\u2032 \u2208 S(s, a). We then show that the number of model updates is bounded by Umax and apply the union bound.\nThe following lemma bounds the number of episodes in which \u2200\u03ba, \u03b9 : |Xk,\u03ba,\u03b9| \u2264 \u03ba is violated with high probability.\nLemma 2. Let E be the number of episodes k for which there are \u03ba and \u03b9 with |Xk,\u03ba,\u03b9| > \u03ba, i.e. E = \u2211\u221e k=1 I{\u2203(\u03ba, \u03b9) : |Xk,\u03ba,\u03b9| > \u03ba} and assume that m \u2265 6H2 ln 2Emax \u03b4 . Then P(E \u2264 6NEmax) \u2265 1\u2212 \u03b4/2 where N = |S \u00d7 A|m and Emax = log2 Hwmin log2 |S|.\nProof Sketch. We first bound the total number of times a fixed pair (s, a) can be observed while being in a particular category Xk,\u03ba,\u03b9 in all phases k for 1 \u2264 \u03ba < |S|. We then show that for a particular (\u03ba, \u03b9), the number of episodes where |Xk,\u03ba,\u03b9| > \u03ba is bounded with high probability, as the value of \u03b9 implies a minimum probability of observing each (s, a) pair inXk,\u03ba,\u03b9 in an episode. Since the observations are not independent we use martingale concentration results to show the statement for a fixed (\u03ba, \u03b9). The desired result follows with the union bound over all relevant \u03ba and \u03b9.\nThe next lemma states that in episodes where the condition \u2200\u03ba, \u03b9 : |Xk,\u03ba,\u03b9| \u2264 \u03ba is satisfied and the true MDP is in the confidence set, the expected optimistic policy value is close to the true value. This lemma is the technically most involved part of the proof.\nLemma 3 (Bound mismatch in total reward). Assume M \u2208 Mk. If |Xk,\u03ba,\u03b9| \u2264 \u03ba for all (\u03ba, \u03b9) and 0 < \u2264 1 andm \u2265 512CH 2\n2 (log2 log2H) 2 log22\n( 8H2|S|2 ) ln 6\u03b41 . Then |V\u0303 \u03c0k 1:H(s0)\u2212V \u03c0 k 1:H(s0)| \u2264 .\nProof Sketch. Using basic algebraic transformations, we show that |p \u2212 p\u0303| \u2264\u221a p\u0303(1\u2212 p\u0303)O (\u221a 1 n ln 1 \u03b41 ) + O ( 1 n ln 1 \u03b41 ) for each p\u0303, p \u2208 P in the confidence set as defined in Eq. 2. Since we assume M \u2208 Mk, we know that p(s\u2032|s, a) and p\u0303(s\u2032|s, a) satisfy this bound with n(s, a) for all s,a and s\u2032. We use that to bound the difference of the expected value function of the successor state in M and M\u0303 , proving that |(Pi \u2212 P\u0303i)V\u0303i+1:j(s)| \u2264 O ( CH\nn(s,\u03c0(s)) ln 1 \u03b41\n) +\nO (\u221a\nC n(s,\u03c0(s)) ln 1 \u03b41\n) \u03c3\u0303i:j(s), where the local variance of the value function is defined as\n\u03c32i:j(s, a) := E [ (V \u03c0i+1:j(si+1)\u2212 P\u03c0i V \u03c0i+1:j(si))2|si = s, ai = a ] and \u03c32i:j(s) := \u03c3 2 i:j(s, \u03c0i(s)).\nThis bound then is applied to |V\u03031:H(s0)\u2212V1:H(s0)| \u2264 \u2211H\u22121 t=0 P1:t|(Pt\u2212 P\u0303t)V\u0303t+1:H(s)|. The basic idea is to split the bound into a sum of two parts by partitioning of the (s, a) space by knownness, e.g. that is (st, at) \u2208 X\u0304\u03ba,\u03b9 for all \u03ba and \u03b9 and (st, at) \u2208 X\u0304 . Using the fact that w(st, at) and n(st, at) are tightly coupled for each (\u03ba, \u03b9), we can bound the expression eventually by . The final key ingredient in the remainder of the proof is to bound \u2211H t=1 P1:t\u22121\u03c3t:H(s)\n2 by O(H2) instead of the trivial bound O(H3). To this end, we show the lemma below.\nLemma 4. The variance of the value function defined as V\u03c0i:j(s) := E [(\u2211j t=i rt(st)\u2212 V \u03c0i:j(si) )2 |si = s ] satisfies a Bellman equation Vi:j = PiVi+1:j + \u03c32i:j\nwhich gives Vi:j = \u2211j t=i Pi:t\u22121\u03c3 2 t:j . Since 0 \u2264 V1:H \u2264 H2r2max, it follows that\n0 \u2264 \u2211j t=1 Pi:t\u22121\u03c3 2 t:j(s) \u2264 H2r2max for all s \u2208 S.\nProof Sketch. The proof works by induction and uses fact that the value function satisfies the Bellman equation and the tower-property of conditional expectations.\nProof Sketch for Theorem 1. The proof of Theorem 1 consists of the following major parts: 1. The true MDP is in the set of MDPsMk for all phases k with probability at least 1\u2212 \u03b42 (Lemma 1). 2. The FixedHorizonEVI algorithm computes a value function whose optimistic value is higher\nthan the optimal reward in the true MDP with probability at least 1\u2212 \u03b4/2 (Lemma A.1). 3. The number of episodes with |Xk,\u03ba,\u03b9| > \u03ba for some \u03ba and \u03b9 are bounded with probability at least\n1\u2212 \u03b4/2 by O\u0303(|S \u00d7 A|m) if m = \u2126\u0303 ( H2\nln |S| \u03b4\n) (Lemma 2).\n4. If |Xk,\u03ba,\u03b9| \u2264 \u03ba for all \u03ba, \u03b9, i.e., relevant state-action pairs are sufficiently known and m = \u2126\u0303 ( CH2\n2 ln 1 \u03b41\n) , then the optimistic value computed is -close to the true MDP value. Together\nwith part 2, we get that with high probability, the policy \u03c0k is -optimal in this case. 5. From parts 3 and 4, with probability 1 \u2212 \u03b4, there are at most O\u0303 ( C|S\u00d7A|H2\n2 ln 1 \u03b4\n) episodes that\nare not -optimal."}, {"heading": "4 Lower PAC Bound", "text": "Theorem 2. There exist positive constants c1, c2, \u03b40, 0 such that for every \u03b4 \u2208 (0, \u03b40) and \u2208 (0, 0) and for every algorithm A that satisfies a PAC guarantee for ( , \u03b4) and outputs a deterministic policy, there is a fixed-horizon episodic MDP Mhard with\nE[nA] \u2265 c1(H \u2212 2)2(|A| \u2212 1)(|S| \u2212 3)\n2 ln\n( c2\n\u03b4 + c3\n) = \u2126 ( |S \u00d7 A|H2\n2 ln\n( c2\n\u03b4 + c3\n)) (3)\nwhere nA is the number of episodes until the algorithm\u2019s policy is ( , \u03b4)-accurate. The constants can be set to \u03b40 = e \u22124 80 \u2248 1 5000 , 0 = H\u22122 640e4 \u2248 H/35000, c2 = 4 and c3 = e \u22124/80.\nThe ranges of possible \u03b4 and are of similar order than in other state-of-the-art lower bounds for multi-armed bandits [19] and discounted MDPs [14, 6]. They are mostly determined by the bandit result by Mannor and Tsitsiklis [19] we build on. Increasing the parameter limits \u03b40 and 0 for bandits would immediately result in larger ranges in our lower bound, but this was not the focus of our analysis.\nProof Sketch. The basic idea is to show that the class of MDPs shown in Figure 1 require at least a number of observed episodes of the order of Equation (3). From the start state 0, the agent ends up in states 1 to n with equal probability, independent of the action. From each such state i, the agent transitions to either a good state + with reward 1 or a bad state \u2212 with reward 0 and stays there for the rest of the episode. Therefore, each state i = 1, . . . , n is essentially a multi-armed bandit with binary rewards of either 0 or H \u2212 2. For each bandit, the probability of ending up in + or \u2212 is equal except for the first action a1 with p(st+1 = +|st = i, at = a1) = 1/2 + /2 and possibly an unknown optimal action a\u2217i (different for each state i) with p(st+1 = +|st = i, at = a\u2217i ) = 1/2 + . In the episodic fixed-horizon setting we are considering, taking a suboptimal action in one of the bandits does not necessarily yield a suboptimal episode. We have to consider the average over all\nbandits instead. In an -optimal episode, the agent therefore needs to follow a policy that would solve at least a certain portion of all n multi-armed bandits with probability at least 1\u2212 \u03b4. We show that the best strategy for the agent to achieve this is to try to solve all bandits with equal probability. The number of samples required to do so then results in the lower bound in Equation (3).\nSimilar MDPs that essentially solve multiple of such multi-armed bandits have been used to prove lower sample-complexity bounds for discounted MDPs [14, 6]. However, the analysis in the infinite horizon case as well as for the sliding-window fixed-horizon optimality criterion considered by Kakade [4] is significantly simpler. For these criteria, every time step the agent follows a policy that is not -optimal counts as a \u201dmistake\u201d. Therefore, every time the agent does not pick the optimal arm in any of the multi-armed bandits counts as a mistake. This contrasts with our fixed-horizon setting where we must instead consider taking an average over all bandits."}, {"heading": "5 Related Work on Fixed-Horizon Sample Complexity Bounds", "text": "We are not aware of any lower sample complexity bounds beyond multi-armed bandit results that directly apply to our setting. Our upper bound in Theorem 1 improves upon existing results by at least a factor of H . We briefly review those existing results in the following.\nTimestep bounds. Kakade [4, Chapter 8] proves upper and lower PAC bounds for a similar setting where the agent interacts indefinitely with the environment but the interactions are divided in segments of equal length and the agent is evaluated by the expected sum of rewards until the end of each segment. The bound states that there are not more than O\u0303 ( |S|2|A|H6\n3 ln 1 \u03b4\n) 6 time steps in\nwhich the agents acts -suboptimal. Strehl et al. [1] improves the state-dependency of these bounds for their delayed Q-learning algorithm to O\u0303\n( |S||A|H5\n4 ln 1 \u03b4\n) . However, in episodic MDP it is more\nnatural to consider performance on the entire episode since suboptimality near the end of the episode is no issue as long as the total reward on the entire episode is sufficiently high. Kolter and Ng [9] use an interesting sliding-window criterion, but prove bounds for a Bayesian setting instead of PAC. Timestep-based bounds can be applied to the episodic case by augmenting the original statespace with a time-index per episode to allow resets after H steps. This adds H dependencies for each |S| in the original bound which results in a horizon-dependency of at least H6 of these existing bounds. Translating the regret bounds of UCRL2 in Corollary 3 by Jaksch et al. [20] yields a PAC-bound on the number of episodes of at least O\u0303 ( |S|2|A|H3\n2 ln 1 \u03b4\n) even if one ignores the reset after H time\nsteps. Timestep-based lower PAC-bounds cannot be applied directly to the episodic reward criterion.\nEpisode bounds. Similar to us, Fiechter [10] uses the value of initial states as optimality-criterion, but defines the value w.r.t. the \u03b3-discounted infinite horizon. His results of order O\u0303\n( |S|2|A|H7\n2 ln 1 \u03b4 ) episodes of length O\u0303(1/(1\u2212 \u03b3)) \u2248 O\u0303(H) are therefore not directly applicable to our setting. Auer and Ortner [5] investigate the same setting as we and propose a UCB-type algorithm that has noregret, which translates into a basic PAC bound of order O\u0303 ( |S|10|A|H7\n3 ln 1 \u03b4\n) episodes. We improve\non this bound substantially in terms of its dependency on H , |S| and . Reveliotis and Bountourelis [12] also consider the episodic undiscounted fixed-horizon setting and present an efficient algorithm in cases where the transition graph is acyclic and the agent knows for each state a policy that visits this state with a known minimum probability q. These assumptions are quite limiting and rarely hold in practice and their bound of order O\u0303 ( |S||A|H4\n2q ln 1 \u03b4\n) explicitly depends on 1/q."}, {"heading": "6 Conclusion", "text": "We have shown upper and lower bounds on the sample complexity of episodic fixed-horizon RL that are tight up to log-factors in the time horizon H , the accuracy , the number of actions |A| and up to an additive constant in the failure probability \u03b4. These bounds improve upon existing results by a factor of at leastH . One might hope to reduce the dependency of the upper bound on |S| to be linear by an analysis similar to Mormax [7] for discounted MDPs which has sample complexity linear in |S| at the penalty of additional dependencies on H . Our proposed UCFH algorithm that achieves our\n6For comparison we adapt existing bounds to our setting. While the original bound stated by Kakade [4] only has H3, an additional H3 comes in through \u22123 due to different normalization of rewards.\nPAC bound can be applied to directly to a wide range of fixed-horizon episodic MDPs with known rewards and does not require additional structure such as sparse or acyclic state transitions assumed in previous work. The empirical evaluation of UCFH is an interesting direction for future work.\nAcknowledgments: We thank Tor Lattimore for the helpful suggestions and comments. We are also grateful to Shiau Hong Lim and Ian Osband for discovering small bugs in previous versions of this paper. This work was supported by an NSF CAREER award and the ONR Young Investigator Program."}, {"heading": "Appendices", "text": ""}, {"heading": "A Fixed-Horizon Extended Value Iteration 12", "text": "B Runtime- and Space-Complexity of UCFH 13"}, {"heading": "C Detailed Proofs for the Upper PAC Bound 13", "text": "C.1 Bound on the Number of Policy Changes of UCFH . . . . . . . . . . . . . . . . . 13\nC.2 Proof of Lemma 1 \u2013 Capturing the true MDP . . . . . . . . . . . . . . . . . . . . 13\nC.3 Bounding the number of episodes with \u03ba > |Xk,\u03ba,\u03b9| for some \u03ba, \u03b9 . . . . . . . . . 14 C.4 Bound on the value function difference for episodes with \u2200\u03ba, \u03b9 : |Xk,\u03ba,\u03b9| \u2264 \u03ba . . . 16 C.5 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nD Proof of the Lower PAC Bound 25"}, {"heading": "A Fixed-Horizon Extended Value Iteration", "text": "We want to find a policy \u03c0k and optimistic M\u0303k \u2208 Mk which have the highest total reward R\u03c0 k\nM\u0303k =\nmax\u03c0,M \u2032\u2208Mk R \u03c0 M \u2032 . Note that \u03c0 k is an optimal policy forMk but not necessarily forM . To facilitate planning, we relax this problem and instead compute a policy and optimistic MDP with R\u03c0 k\nM\u0303k =\nmax\u03c0,M \u2032\u2208M\u2032k R \u03c0 M \u2032 with M\u2032k := { M\u0303 \u2208Mnonst. : \u2200(s, a) \u2208 S \u00d7A, t = 1 . . . H, s\u2032 \u2208 S(s, a) p\u0303t(s \u2032|s, a) \u2208 conv(ConfidenceSet(p\u0302(s\u2032|s, a), n(s, a))) } .\nWe only require the transition probabilities to be in the convex hull of the confidence sets instead of the confidence sets. Since this is a relaxation, we haveMk \u2286 M\u2032k. We can find such a policy by dynamic programming similar to extended value iteration [16, 5]. The optimal Q-function can be computed as Q\u0303\u2217H:H(s, a) = rH(s) and for i = H \u2212 1, . . . , 2, 1 as\nQ\u0303\u2217i:H(s, a) =ri(s) + max p\u0303i\u2208Ps,a  \u2211 s\u2032\u2208S(s,a) p\u0303i max b\u2208A Q\u0303\u2217i+1:H(s \u2032, b)  The feasible set is defined as Ps,a := {p \u2208 [0, 1]|S(s,a)| | \u2016p\u20161 = 1,\u2200s\u2032 \u2208 S(s, a) : p(s\u2032) \u2208 conv(ConfidenceSet(p\u0302(s\u2032|s, a), n(s, a)))}. The optimal policy \u03c0kt (s) at time t is then simply the maximizer of the inner max operator and the transition probability p\u0303t(\u00b7|s, a) is the maximizer of the outer maximum. The inner max can be solved efficiently by enumeration and the outer maximum similar to extended value iteration [16]. The basic idea is to put as much probability mass as possible to successor states with highest value. See the following algorithm for the implementation details.\nFunction FixedHorizonEVI(M) Q\u0303\u2217H:H(s, a) = rH(s) \u2200s, a \u2208 S \u00d7A ; // O(|S||A|) for t = H \u2212 1 to 1 do // O(H|S| log |S|+H|S||A|C))\n\u03c0t+1(s) := arg maxa\u2208A Q\u0303 \u2217 t+1:H(s, a) \u2200s \u2208 S ; // O(|S||A|) sort states s(1), . . . s(|S|) such that Q\u0303\u2217t+1:H(s (i), \u03c0t+1(s (i))) \u2265 Q\u0303\u2217t+1:H(s(i+1), \u03c0t+1(s(i+1))) ; // O(|S| log |S|) for s, a \u2208 S \u00d7A do // O(|S||A|C) p\u0303t(s\n\u2032|s, a) := minConfidenceSet(p\u0302(s\u2032|s, a), n(s, a)) \u2200s\u2032 \u2208 S(s, a) ; // O(C) \u2206 := 1\u2212 \u2211 s\u2032\u2208S(s,a) p\u0303t(s\n\u2032|s, a) ; // O(C) i := 1 ; // O(1) while \u2206 > 0 do // O(C)\ns\u2032 := s(i); \u2206\u2032 := min{\u2206,maxConfidenceSet(p\u0302(s\u2032|s, a), n(s, a))\u2212 p\u0303t(s\u2032|s, a)}; p\u0303t(s\n\u2032|s, a) := p\u0303t(s\u2032|s, a) + \u2206\u2032; \u2206 := \u2206\u2212\u2206\u2032; i := i+ 1;\nQ\u0303\u2217t:H(s, a) = \u2211 s\u2032\u2208S(s,a) p\u0303t(s \u2032|s, a)Q\u0303\u2217t+1:H(s\u2032, \u03c0t+1(s\u2032)) ; // O(C)\n\u03c01(s) := arg maxa\u2208A Q\u0303 \u2217 1:H(s, a) \u2200s \u2208 S ; // O(|S||A| return MDP with transition probabilities p\u0303t, optimal policy \u03c0\nNote that due to the nonlinear constraint in Equation (1), ConfidenceSet(p\u0302(s\u2032|s, a), n(s, a)) may be the union of two disjoint intervals instead of one interval. Still, min- and max-operations on the confidence sets can be computed readily in constant time. Therefore, the transition probabilities p\u0303t(\u00b7|s, a) for a single time step t and state-action pair s, a can be computed in O(|S||A|C) given sorted states. Sorting the states takes O(|S| log |S|) which results in O(H|S| log |S| + H|S||A|C) runtime complexity of FixedHorizonEVI (see comments in Function FixedHorizonEVI ). The Algorithm requires O(H|S||A|C) additional space besides the storage requirements of the input MDPM as the transition probabilities p\u0303t are returned by the algorithm. If those are not required and only the optimal policy is of interest, the additional space can be reduced to O(|S||A|). Lemma A.1 (Validity of optimistic planning). FixedHorizonEVI(Mk) returns M\u0303, \u03c0k = arg maxM\u2208M\u2032k,\u03c0 R \u03c0 M . SinceMk \u2286M\u2032k, it also holds that R\u03c0 k M\u0303 \u2265 maxM\u2208Mk,\u03c0 R\u03c0M .\nProof Sketch. This result can be proved straight-forwardly by showing that \u03c0k is optimal in the last time step H with highest possible reward and then subsequently for all previous time steps inductively. It follows directly from the definition of the algorithm in Function FixedHorizonEVI that the returned MDP is inM\u2032k.\nB Runtime- and Space-Complexity of UCFH\nSampling one episode and updating the respective v variables has O(H) runtime. Theorem 1 states that after at most O\u0303\n( H2C|S\u00d7A|\n2 ln 1 \u03b4\n) observed episodes, the current policy is -optimal with suffi-\nciently high probability. This results in a total runtime for sampling of O\u0303 ( H3C|S\u00d7A|\n2 ln 1 \u03b4\n) .\nEach update of the policy involves updating the n variables andMk which takes runtime O(C) and a call of FixedHorizonEVI with runtime cost O(H|S||A|C +H|S| log |S|). From Lemma C.1 below, we know that the policy can be updated at most Umax times which a gives total runtime for policy updates of\nO(UmaxH|S|(|A|C + log |S|)) =O ( H|S|2|A|(|A|C + log |S|) log |S| 2H2 ) =O\u0303 ( H|S|2|A|2C log 1 ) .\nThe total runtime of UCFH before the policy is -optimal with probability at least 1\u2212 \u03b4 is therefore\nO\u0303\n( H3|S|2|A|2C\n2 ln\n1\n\u03b4\n) .\nThe space complexity of UCFH is dominated by the requirement to store statistics for each possible transition which gives O(|S||A|C) complexity."}, {"heading": "C Detailed Proofs for the Upper PAC Bound", "text": "C.1 Bound on the Number of Policy Changes of UCFH\nLemma C.1. The total number of updates is bounded by Umax = |S \u00d7 A| log2 |S|H wmin .\nProof. First note that n(s, a) is never never decreasing and no updates happen once n(s, a) \u2265 |S|mH for all (s, a). In each update, the n(s, a) of exactly one (s, a) pair increases by max{mwmin, n(s, a)}. For a single (s, a) pair, such updates can happen only log2(|S|mH) \u2212 log2(mwmin) times. Hence, there are at most |S \u00d7 A| log2 |S|mH wminm updates in total."}, {"heading": "C.2 Proof of Lemma 1 \u2013 Capturing the true MDP", "text": "Proof. For a single (s, a) pair, s\u2032 \u2208 S(s, a) and k, we can treat the event that s\u2032 is the successor state of s when chosing action a as a Bernoulli random variable with probability p(s\u2032|s, a). Using Hoeffding\u2019s inequality,7 we then realize that\n|p(s\u2032|s, a)\u2212 p\u0302(s\u2032|s, a)| \u2264 \u221a ln(6/\u03b41)\n2n\nand by Bernstein\u2019s inequality |p(s\u2032|s, a)\u2212 p\u0302(s\u2032|s, a)| \u2264 \u221a\n2p(s\u2032|s, a)(1\u2212 p(s\u2032|s, a)) ln(6/\u03b41) n + 1 3n ln(6/\u03b41)\n7While the considered random variables are strictly speaking not necessarily independent, they can be treated as such for the concentration inequalities applied here. See Appendix A of Strehl and Littman [16] for details.\nwith probability at least 1 \u2212 \u03b41/3 respectively. Using both inequalities of Theorem 10 by Maurer and Pontil [18]8, we have\n| \u221a p(s\u2032|s, a)(1\u2212 p(s\u2032|s, a))\u2212 \u221a p\u0302(s\u2032|s, a)(1\u2212 p\u0302(s\u2032|s, a))| \u2264 \u221a 2 ln(6/\u03b41)\nn\u2212 1 (4)\nfor n > 1 with probability at least 1 \u2212 \u03b41/3. All three inequalities hold with probability 1 \u2212 \u03b41 by the union bound. Applying Inequality (4) to Bernstein\u2019s inequality, we obtain\n|p(s\u2032|s, a)\u2212 p\u0302(s\u2032|s, a)| \u2264 \u221a\n2p(s\u2032|s, a)(1\u2212 p(s\u2032|s, a)) ln(6/\u03b41) n + 1 3n ln(6/\u03b41)\n\u2264 (\u221a p\u0302(s\u2032|s, a)(1\u2212 p\u0302(s\u2032|s, a)) + \u221a 2 ln(6/\u03b41)\nn\u2212 1\n)\u221a 2 ln(6/\u03b41)\nn +\n1\n3n ln(6/\u03b41)\n\u2264 \u221a\n2p\u0302(s\u2032|s, a)(1\u2212 p\u0302(s\u2032|s, a)) ln(6/\u03b41) n + 7 3(n\u2212 1) ln(6/\u03b41).\nBy Lemma C.1, there are at most Umax updates and so there are at most Umax different k to consider. Since in each update, only a single (s, a) pair with at most C successor states is updated, for all k and (s, a), there are only UmaxC different p\u0302(s\u2032|s, a) to consider. Applying the union bound, we get that M /\u2208 Mk for any k with probability at most UmaxC\u03b41. By setting \u03b41 = \u03b42CUmax we get the desired result.\nC.3 Bounding the number of episodes with \u03ba > |Xk,\u03ba,\u03b9| for some \u03ba, \u03b9\nBefore presenting the proof of Lemma 2 which bounds the total number of episodes where there is a \u03ba and \u03b9 such that \u03ba > |Xk,\u03ba,\u03b9|, we establish a bound for each individual \u03ba and \u03b9 in the following two additional lemmas.\nLemma C.2 (Bound on observations ofX\u00b7,\u03ba,\u03b9). The total number of observations of (s, a) \u2208 Xk,\u03ba,\u03b9 where \u03ba \u2208 [1, |S| \u2212 1] and \u03b9 > 0 over all phases k is at most 3|S \u00d7A|mw\u03b9\u03ba. The variable w\u03b9 is the smallest possible weight of a (s, a)-pair that has importance \u03b9.\nProof. We denote the smallest possible weight for any (s, a) pair such that \u03b9(s, a) = \u03b9 by w\u03b9 := min{w(s, a) : \u03b9k(s, a) = \u03b9}. Note that w\u03b9+1 = 2w\u03b9 for \u03b9 > 0. Consider any phase k and fix (s, a) \u2208 Xk,\u03ba,\u03b9 with \u03b9 > 0. Since we assumed \u03b9k(s, a) = \u03b9 > 0, we have w\u03b9 \u2264 wk(s, a) < 2w\u03b9. From \u03bak(s, a) = \u03ba, it follows that\nnk(s, a) 2mwk(s, a) \u2264 \u03ba \u2264 nk(s, a) mwk(s, a)\nwhich implies that\nmw\u03b9\u03ba \u2264 mwk(s, a)\u03ba \u2264 nk(s, a) \u2264 2mwk(s, a)\u03ba \u2264 4mw\u03b9\u03ba. (5)\nHence, each state can only be observed 3mw\u03b9 times while being in {(s, a) \u2208 Xk,\u03ba,\u03b9 : k \u2208 N}.\nLemma C.3. The number of episodes E\u03ba,\u03b9 in phases with |Xk,\u03ba,\u03b9| > \u03ba is bounded for every \u03b1 \u2265 3 with high probability, P (E\u03ba,\u03b9 > \u03b1N) \u2264 exp ( \u2212\u03b2w\u03b9(\u03ba+ 1)N\nH ) where N = |S \u00d7 A|m and \u03b2 = \u03b1(3/\u03b1\u22121) 2\n7/3\u22121/\u03b1 .\nProof. Let \u03bdi := \u2211H t=1 I{(st, at) \u2208 Xk,\u03ba,\u03b9} be the number of observations of (s, a) in Xk,\u03ba,\u03b9 in the ith epsiode with Xk,\u03ba,\u03b9 > \u03ba. We have i \u2208 {1, . . . E\u03ba,\u03b9}) and k is the phase that episode i belongs to. 8The empirical variance denoted by Vn(X) by Maurer and Pontil [18] is p\u0303(s\u2032|s, a)(1 \u2212 p\u0303(s\u2032|s, a)) in our case and EVn is the true variance which amounts to p(s\u2032|s, a)(1\u2212 p(s\u2032|s, a)) for us.\nSince Xk,\u03ba,\u03b9 \u2265 \u03ba+ 1 and all states in partition (\u03ba, \u03b9) have wk(s, a) \u2265 w\u03b9 , we get E[\u03bdi|\u03bd1, . . . \u03bdi\u22121] \u2265 (\u03ba+ 1)w\u03b9. (6)\nAlso V[\u03bdi|\u03bd1 . . . \u03bdi\u22121] \u2264 E[\u03bdi|\u03bd1, . . . \u03bdi\u22121]H as \u03bdi \u2208 [0, H]. To reason about E\u03ba,\u03b9, we define the continuation\n\u03bd+i := { \u03bdi if i \u2264 E\u03ba,\u03b9 w\u03b9(\u03ba+ 1) otherwise\nand the centered auxiliary sequence\n\u03bd\u0304i := \u03bd+i w\u03b9(\u03ba+ 1)\nE[\u03bd+i |\u03bd + 1 , . . . \u03bd + i\u22121]\n.\nBy construction\nE[\u03bd\u0304i|\u03bd\u03041, . . . \u03bd\u0304i\u22121] = w\u03b9(\u03ba+ 1) E[\u03bd+i |\u03bd\u03041, . . . , \u03bd\u0304i\u22121] E[\u03bd+i |\u03bd + 1 , . . . \u03bd + i\u22121] = w\u03b9(\u03ba+ 1).\nBy Lemma C.2, we have that E\u03ba,\u03b9 > \u03b1N only if \u03b1N\u2211 i=1 \u03bd\u0304i \u2264 3Nw\u03b9\u03ba \u2264 3Nw\u03b9(\u03ba+ 1).\nDefine now the martingale\nBi := E \u03b1N\u2211 j=1 \u03bd\u0304j |\u03bd\u03041, . . . \u03bd\u0304i  = i\u2211 j=1 \u03bd\u0304j + \u03b1N\u2211 j=i+1 E[\u03bd\u0304j |\u03bd\u03041 . . . \u03bd\u0304i]\nwhich givesB0 = \u03b1Nw\u03b9(\u03ba+1) andB\u03b1N = \u2211\u03b1N i=1 \u03bd\u0304i. Further, since \u03bd + i \u2208 [0, H] and Equation (6), we have\n|Bi+1 \u2212Bi| = |\u03bd\u0304i \u2212 E[\u03bd\u0304i|\u03bd\u03041, . . . , \u03bd\u0304i\u22121]| = \u2223\u2223\u2223\u2223\u2223w\u03b9(\u03ba+ 1)(\u03bd+i \u2212 E[\u03bd+i |\u03bd\u03041, . . . \u03bd\u0304i\u22121])E[\u03bd+i |\u03bd+1 , . . . \u03bd+i\u22121] \u2223\u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u03bd+i \u2212 E[\u03bd+i |\u03bd\u03041, . . . \u03bd\u0304i\u22121]\u2223\u2223 \u2264 H.\nUsing\n\u03c32 := \u03b1N\u2211 i=1 V[Bi \u2212Bi\u22121|B1 \u2212B0, . . . Bi\u22121 \u2212Bi\u22122]\n= \u03b1N\u2211 i=1 V[\u03bd\u0304i|\u03bd\u03041, . . . \u03bd\u0304i\u22121] \u2264 \u03b1NHw\u03b9(\u03ba+ 1) = HB0\nwe can apply Theorem 22 by Chung and Lu [21] and obtain\nP(E\u03ba,\u03b9 > \u03b1N) \u2264 P ( \u03b1N\u2211 i=1 \u03bd\u0304i \u2264 3Nw\u03b9(\u03ba+ 1) ) = P(B\u03b1N \u2212B0 \u2264 3B0/\u03b1\u2212B0) = P(B\u03b1N \u2212B0 \u2264 \u2212 (1\u2212 3/\u03b1)B0)\n\u2264 exp ( \u2212 (3/\u03b1\u2212 1)\n2B20 2\u03c32 +H(1/3\u2212 1/\u03b1)B0 ) for \u03b1 \u2265 3. We can further simplify the bound to\nP(E\u03ba,\u03b9 > \u03b1N) \u2264 exp ( \u2212 (3/\u03b1\u2212 1)\n2B20 2HB0 +H(1/3\u2212 1/\u03b1)B0 ) \u2264 exp ( \u2212 (3/\u03b1\u2212 1) 2\n2 + (\u22121/\u03b1+ 1/3) B0 H ) = exp ( \u2212\u03b1(3/\u03b1\u2212 1) 2\n7/3\u2212 1/\u03b1 Nw\u03b9(\u03ba+ 1) H\n) .\nWe are now ready to prove Lemma 2 by combining the bound in the previous lemma for all \u03ba and \u03b9.\nProof of Lemma 2. Since wk(s, a) \u2264 H , we have that wk(s,a)wmin < H wmin and so \u03b9k(s, a) \u2264 H/wmin = 4H\n2|S|/ . In addition, |Xk,\u03ba,\u03b9| \u2264 |S| for all k, \u03ba, \u03b9 and so |Xk,\u03ba,\u03b9| > \u03ba can only be true for \u03ba \u2264 |S|. Hence, only\nEmax = log2 H\nwmin log2 |S|\npossible values for (\u03ba, \u03b9) exists that can have |Xk,\u03ba,\u03b9| > \u03ba. Using the union bound over all (\u03ba, \u03b9) and Lemma C.3, we get that\nP(E \u2264 \u03b1NEmax) \u2265P(max (\u03ba,\u03b9)\nE\u03ba,\u03b9 \u2264 \u03b1N) \u2265 1\u2212 Emax exp ( \u2212\u03b2w\u03b9(\u03ba+ 1)N\nH ) \u22651\u2212 Emax exp ( \u2212\u03b2wminN\nH\n) = 1\u2212 Emax exp ( \u2212\u03b2wminm|S \u00d7 A|\nH ) =1\u2212 Emax exp ( \u2212\u03b2 m|S \u00d7 A|\n4H2|S| ) Bounding the right hand-side by 1\u2212 \u03b4/2 and solving for m gives\n1\u2212 Emax exp ( \u2212\u03b2 m|S \u00d7 A|\n4H2|S|\n) \u22651\u2212 \u03b4/2 \u21d4 m \u2265 4H\n2|S| |S \u00d7 A|\u03b2 ln 2Emax \u03b4\nHence, the condition\nm \u2265 4H 2\n\u03b2 ln 2Emax \u03b4\nis sufficient for the desired result to hold. By plugging in \u03b1 = 6 and \u03b2 = \u03b1(3/\u03b1\u22121) 2\n7/3\u22121/\u03b1 = 9 13 \u2265 2 3 , we\nobtain the statement to show.\nC.4 Bound on the value function difference for episodes with \u2200\u03ba, \u03b9 : |Xk,\u03ba,\u03b9| \u2264 \u03ba\nTo prove Lemma 3, it is sufficient to consider a fixed phase k. To avoid notational clutter, we therefore omit the phase indices k in this section.\nFor the proof, we reason about a sequence of MDPsMd which have the same transition probabilities but different reward functions r(d). For d = 0, the reward function is the original reward function r of M , i.e. r(0)t = rt for all t = 1 . . . H . The following reward functions are then defined recursively as r(2d+2)t = \u03c3 (d),2 t:H , where \u03c3 (d),2 t:H is the local variance of the value function w.r.t. the rewards r\n(d). Note that for every d and t = 1 . . . H and s \u2208 S, we have r(d)t (s) \u2208 [0, Hd]. In complete analogy, we define M\u0303d and M\u0302d.\nWe first prove a sequence of lemmas necessary for Lemma 3. Lemma C.4.\nVi,j \u2212 V\u0303i,j = H\u22121\u2211 t=i Pi:t\u22121(Pt \u2212 P\u0303t)V\u0303t+1:j"}, {"heading": "Proof.", "text": "Vi,j(s)\u2212 V\u0303i,j(s) =r(s) + PiVi+1:j(s)\u2212 r(s)\u2212 P\u0303iV\u0303i+1:j(s) + PiV\u0303i+1,j(s)\u2212 PiV\u0303i+1:j(s) =Pi(Vi+1:j \u2212 V\u0303i+1:j) + (Pi \u2212 P\u0303i)V\u0303i+1:j(s)\nSince we have Vj:j(s) = r(s) = V\u0303j:j(s), we can recursively expand the first difference until i = j and get\nVi,j \u2212 V\u0303i,j = j\u22121\u2211 t=i Pi:t\u22121(Pt \u2212 P\u0303t)V\u0303t+1:j\nLemma C.5. Assume p, p\u0302, p\u0303 \u2208 [0, 1] satisfy p \u2208 P and p\u0303 \u2208 conv(P) where\nP := { p\u2032 \u2208 [0, 1] :|p\u0302\u2212 p\u2032| \u2264 \u221a ln(6/\u03b41)\n2n , |p\u0302\u2212 p\u2032| \u2264 \u221a\n2p\u0302(1\u2212 p\u0302) n ln(6/\u03b41) + 7 3(n\u2212 1) ln(6/\u03b41),\nif n > 1 : \u2223\u2223\u2223\u221ap\u2032(1\u2212 p\u2032)\u2212\u221ap\u0302(1\u2212 p\u0302)\u2223\u2223\u2223 \u2264\u221a2 ln(6/\u03b41)\nn\u2212 1\n} .\nThen\n|p\u2212 p\u0303| \u2264 \u221a\n8p\u0303(1\u2212 p\u0303) n ln(6/\u03b41) + 26 3(n\u2212 1) ln(6/\u03b41).\nProof. We have P = P1 \u2229 P2 with P1 = { p\u2032 \u2208 [0, 1] :|p\u0302\u2212 p\u2032| \u2264 \u221a ln(6/\u03b41)\n2n , |p\u0302\u2212 p\u2032| \u2264 \u221a\n2p\u0302(1\u2212 p\u0302) n ln(6/\u03b41) + 7 3(n\u2212 1) ln(6/\u03b41),\nif n > 1 : ( max { 0, \u221a p\u0302(1\u2212 p\u0302)\u2212 \u221a 2 ln(6/\u03b41)\nn\u2212 1\n})2 \u2264 p\u2032(1\u2212 p\u2032) } .\nand\nP2 = { p\u2032 \u2208 R : if n > 1 : \u221a p\u2032(1\u2212 p\u2032) \u2264 \u221a p\u0302(1\u2212 p\u0302) + \u221a 2 ln(6/\u03b41)\nn\u2212 1\n} .\nNote that the last condition of P1 is equivalent to \u221a p\u0302(1\u2212 p\u0302) \u2264 \u221a p\u2032(1\u2212 p\u2032) + \u221a 2 ln(6/\u03b41) n\u22121 as p\u2032 \u2208 [0, 1]. As an intersection of a polytope and the superlevel set of a concave function p\u2032(1\u2212 p\u2032), the set P1 is convex. Hence conv(P) = conv(P1 \u2229 P2) \u2286 conv(P1) = P1. It therefore follows that p\u0303 \u2208 P1. We now bound\n|p\u2212 p\u0303| \u2264|p\u2212 p\u0302|+ |p\u0302\u2212 p\u0303| \u2264 2 \u221a\n2p\u0302(1\u2212 p\u0302) n ln(6/\u03b41) + 2 7 3(n\u2212 1) ln(6/\u03b41)\n= \u221a p\u0302(1\u2212 p\u0302)\n\u221a 8\nn ln(6/\u03b41) +\n14\n3(n\u2212 1) ln(6/\u03b41)\n\u2264 (\u221a p\u0303(1\u2212 p\u0303) + \u221a 2 ln(6/\u03b41)\nn\u2212 1\n)\u221a 8\nn ln(6/\u03b41) +\n14\n3(n\u2212 1) ln(6/\u03b41)\n\u2264 \u221a\n8p\u0303(1\u2212 p\u0303) n ln(6/\u03b41) + 26 3(n\u2212 1) ln(6/\u03b41)\nLemma C.6. Assume |p(s\u2032|s, a)\u2212 p\u0303i(s\u2032|s, a)| \u2264 c1(s, a) + c2(s, a) \u221a p\u0303i(s\u2032|s, a)(1\u2212 p\u0303i(s\u2032|s, a))\nfor a = \u03c0i(s) and all s\u2032, s \u2208 S. Then |(Pi \u2212 P\u0303i)V\u0303i+1:j(s)| \u2264 c1(s, a)|S(s, a)|\u2016V\u0303i+1:j\u2016\u221e + c2(s, a) \u221a |S(s, a)|\u03c3\u0303i:j(s)\nfor any (s, a) \u2208 S \u00d7 A where S(s, a) denotes the set of possible successor states of state s and action a.\nProof. Let s and a = \u03c0i(s) be fixed and define for this fixed s the constant function V\u0304 (s\u2032) = P\u0303iV\u0303i+1:j(s) [sic] as the expected value function of the successor states of s. Note that V\u0304 (s\u2032) is a constant function and so V\u0304 = P\u0303iV\u0304 = PiV\u0304 .\n|(Pi \u2212 P\u0303i)V\u0303i+1:j(s)| = |(Pi \u2212 P\u0303i)V\u0303i+1:j(s) + V\u0304 (s)\u2212 V\u0304 (s)| =|(Pi \u2212 P\u0303i)(V\u0303i+1:j \u2212 V\u0304 )(s)|\n\u2264 \u2211\ns\u2032\u2208S(s,a)\n|p(s\u2032|s, a)\u2212 p\u0303i(s\u2032|s, a)||V\u0303i+1:j(s\u2032)\u2212 V\u0304 (s\u2032)| (7)\n\u2264 \u2211\ns\u2032\u2208S(s,a)\n( c1(s, a) + c2(s, a) \u221a p\u0303i(s\u2032|s, a)(1\u2212 p\u0303i(s\u2032|s, a)) ) |V\u0303i+1:j(s\u2032)\u2212 V\u0304 (s\u2032)|\n\u2264|S(s, a)|c1(s, a)\u2016V\u0303i+1:j\u2016\u221e + c2(s, a) \u2211\ns\u2032\u2208S(s,a)\n\u221a p\u0303i(s\u2032|s, a)(1\u2212 p\u0303i(s\u2032|s, a))(V\u0303i+1:j(s\u2032)\u2212 V\u0304 (s\u2032))2\n\u2264|S(s, a)|c1(s, a)\u2016V\u0303i+1:j\u2016\u221e + c2(s, a) \u221a |S(s, a)| \u2211 s\u2032\u2208S(s,a) p\u0303i(s\u2032|s, a)(1\u2212 p\u0303i(s\u2032|s, a))(V\u0303i+1:j(s\u2032)\u2212 V\u0304 (s\u2032))2\n(8) \u2264|S(s, a)|c1(s, a)\u2016V\u0303i+1:j\u2016\u221e + c2(s, a) \u221a |S(s, a)| \u2211 s\u2032\u2208S(s,a) p\u0303i(s\u2032|s, a)(V\u0303i+1:j(s\u2032)\u2212 V\u0304 (s\u2032))2\n=|S(s, a)|c1(s, a)\u2016V\u0303i+1:j\u2016\u221e + c2(s, a) \u221a |S(s, a)|\u03c3\u0303i:j(s)\nIn Inequality (7), we wrote out the definition ofPi and P\u0303i and applied the triangle inequality. We then applied the assumed bound and bounded |V\u0303i+1:j(s\u2032)\u2212V\u0304 (s\u2032)| by \u2016Vi+1:j\u2016\u221e as all value functions are nonnegative. In Inequality (8), we applied the Cauchy-Schwarz inequality and subsequently used the fact that each term is the sum is nonnegative and that (1 \u2212 p\u0303i(s\u2032|s, a)) \u2264 1. The final equality follows from the definition of \u03c3\u0303i:j .\nC.4.1 Bounding the difference in value function\nLemma C.7. Assume M \u2208Mk. If |X\u03ba,\u03b9| \u2264 \u03ba for all (\u03ba, \u03b9). Then\n|V (d)1:H(s0)\u2212 V\u0303 (d) 1:H(s0)| =: \u2206d \u2264 A\u0302d + B\u0302d + min{C\u0302d, C\u0302 \u2032 d + C\u0302 \u2032\u2032\u221a\u22062d+2} where\nA\u0302d = 4 Hd, B\u0302d = 52Hd+1 |K \u00d7 I|C 3m ln 6 \u03b41 ,\nand\nC\u0302 \u2032d =\n\u221a C |K \u00d7 I| 8\nm H2d+2 ln\n6 \u03b41 C\u0302d = C\u0302 \u2032 d\n\u221a H, C\u0302 \u2032\u2032 = \u221a C |K \u00d7 I| 8\nm ln\n6 \u03b41 ."}, {"heading": "Proof.", "text": "\u2206d =|V (d)1:H(s0)\u2212 V\u0303 (d) 1:H(s0)| = \u2223\u2223\u2223\u2223\u2223 H\u22121\u2211 t=1 P1:t\u22121(Pt \u2212 P\u0303t)V\u0303 (d)t+1:H(s0) \u2223\u2223\u2223\u2223\u2223 \u2264 H\u22121\u2211 t=1 P1:t\u22121|(Pt \u2212 P\u0303t)V\u0303 (d)t+1:H |(s0)\n= H\u22121\u2211 t=1 P1:t\u22121  \u2211 s,a\u2208S\u00d7A I{s = \u00b7, a = \u03c0t(s)}|(Pt \u2212 P\u0303t)V\u0303 (d)t+1:H |  (s0)\n= \u2211\ns,a\u2208S\u00d7A H\u22121\u2211 t=1 P1:t\u22121 ( I{s = \u00b7, a = \u03c0t(s)}|(Pt \u2212 P\u0303t)V\u0303 (d)t+1:H | ) (s0)\n= \u2211\ns,a\u2208S\u00d7A H\u22121\u2211 t=1 P1:t\u22121 ( I{s = \u00b7, a = \u03c0t(s)}|(Pt \u2212 P\u0303t)V\u0303 (d)t+1:H(s)| ) (s0)\nThe first equality follows from Lemma C.4, the second step from the fact that Vt+1:H \u2265 0 and P1:t\u22121 being non-expansive. In the third, we introduce an indicator function which does not change the value as we sum over all (s, a) pairs. The fourth step relies on the linearity of the Pi:j operators. In the fifth step, we realize that I{s = \u00b7, a = \u03c0t(s)}|(Pt \u2212 P\u0303t)V\u0303 (d)t+1:H(\u00b7) is a function that takes nonzero values only for input s. We can therefore replace the argument of the second term with s without changing the value. The term then becomes constant and by linearity of Pi:j , we can write\n|V (d)1:H(s0)\u2212 V\u0303 (d) 1:H(s0)| = \u2206d \u2264 \u2211 s,a\u2208S\u00d7A H\u22121\u2211 t=1 |(Pt \u2212 P\u0303t)V\u0303 (d)t+1:H(s)|(P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0)\n\u2264 \u2211 s,a/\u2208X H\u22121\u2211 t=1 \u2016V\u0303 (d)t+1:H\u2016\u221e(P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0)\n+ \u2211 s,a\u2208X H\u22121\u2211 t=1 |(Pt \u2212 P\u0303t)V\u0303 (d)t+1:H(s)|(P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0)\n\u2264 \u2211 s,a/\u2208X H\u22121\u2211 t=1 Hd+1(P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0)\n+ \u2211 s,a\u2208X H\u22121\u2211 t=1 |(Pt \u2212 P\u0303t)V\u0303 (d)t+1:H(s)|(P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0)\n\u2264 \u2211 s,a/\u2208X H\u22121\u2211 t=1 Hd+1(P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0)\n+ \u2211 s,a\u2208X H\u22121\u2211 t=1 \u2223\u2223\u2223|S(s, a)|c1(s, a)Hd+1 + c2(s, a)\u221a|S(s, a)|\u03c3\u0303(d)t:H(s, a)\u2223\u2223\u2223 (P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0)\n\u2264 \u2211 s,a/\u2208X H\u2211 t=1 Hd+1(P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0)\n+ \u2211 s,a\u2208X H\u2211 t=1 \u2223\u2223|S(s, a)|c1(s, a)Hd+1\u2223\u2223 (P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0) + \u2211 s,a\u2208X H\u22121\u2211 t=1\n\u2223\u2223\u2223c2(s, a)\u221a|S(s, a)|\u03c3\u0303(d)t:H(s, a)\u2223\u2223\u2223 (P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0) \u2264 \u2211 s,a/\u2208X Hd+1w(s, a) + \u2211 s,a\u2208X |S(s, a)|c1(s, a)Hd+1w(s, a)\n+ \u2211 s,a\u2208X \u221a |S(s, a)|c2(s, a) H\u22121\u2211 t=1 \u03c3\u0303 (d) t:H(s, a)(P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0)\n\u2264 \u2211 s,a/\u2208X Hd+1w(s, a) + \u2211 s,a\u2208X Cc1(s, a)H d+1w(s, a)\n+ \u2211 s,a\u2208X \u221a Cc2(s, a) H\u22121\u2211 t=1 \u03c3\u0303 (d) t:H(s, a)(P1:t\u22121I{s = \u00b7, a = \u03c0t(s)})(s0)\nIn the second inequality, we split the sum over all (s, a) pairs and used the fact that Pt and P\u0303t are non-expansive, i.e., |(Pt\u2212 P\u0303t)V\u0303 (d)t+1:H(s)| \u2264 \u2016V (d) t+1:H\u2016\u221e. The next step follows from \u2016V (d) t+1:H\u2016\u221e \u2264 \u2016V (d)1:H\u2016\u221e \u2264 Hd+1. We then apply Lemma C.6 and subsequently use that all terms are nonnegative and the definition ofw(s, a). Eventually, we use that |S(s, a)| \u2264 C for all s, a. Using the assumption that M \u2208Mk and M\u0303 \u2208M\u2032k from Lemma A.1, we can apply Lemma C.5 and get that\nc2(s, a) =\n\u221a 8\nn(s, a) ln\n6 \u03b41 and c1(s, a) =\n26\n3(n(s, a)\u2212 1) ln\n6 \u03b41 .\nHence, we can bound \u2206d \u2264 A(s0) +B(s0) + C(s0)\nas a sum of three terms which we will consider individually in the following. The first term is A(s0) = \u2211 s,a/\u2208X Hd+1w(s, a) \u2264 wmin|S|Hd+1 \u2264 Hd+1|S| 4H|S| = 4 Hd = A\u0302d\nas w(s, a) \u2264 wmin for all s, a not in the active set and that the policy is deterministic, which implies that there are only |S| nonzero w. The next term is\nB(s0) =C \u2211 s,a\u2208X w(s, a)Hd+1 26 3(n(s, a)\u2212 1) ln 6 \u03b41\n=Hd+1C ln 6\n\u03b41 \u2211 \u03ba,\u03b9 \u2211 s,a\u2208X\u03ba,\u03b9 w(s, a) 26 3(n(s, a)\u2212 1)\n\u2264Hd+1 26C 3 ln 6\n\u03b41 \u2211 \u03ba,\u03b9 \u2211 s,a\u2208X\u03ba,\u03b9 w(s, a) n(s, a) n(s, a) n(s, a)\u2212 1 .\nFor s, a \u2208 X\u03ba,\u03b9, we have n(s, a) \u2265 mw(s, a)\u03ba (see Equation (5)) and so w(s, a)\nn(s, a) \u2264 1 \u03bam . (9)\nFurther, for all relevant (s, a)-pairs, we have n(s, a) > 1 (follows from |X\u03ba,\u03b9| \u2264 \u03ba) which implies\nB(s0) \u2264Hd+1 52C\n3 ln\n6\n\u03b41 \u2211 \u03ba,\u03b9 |X\u03ba,\u03b9| \u03bam\nand since we assumed |X\u03ba,\u03b9| \u2264 \u03ba\nB(s0) \u2264 52Hd+1 |K \u00d7 I|C\n3m ln\n6 \u03b41 = B\u0302d\nwhere K \u00d7 I is the set of all possible (\u03ba, \u03b9)-pairs. The last term is C(s0) = \u221a C \u2211 s,a\u2208X c2(s, a) H\u22121\u2211 t=1 \u03c3\u0303 (d) t:H(s, a))P1:t\u22121I{s = \u00b7, a = \u03c0t(s)}\n\u2264 \u221a C \u2211 s,a\u2208X c2(s, a) H\u22121\u2211 t=1 \u03c3\u0303 (d) t:H(s, a))P1:t\u22121I{s = \u00b7, a = \u03c0t(s)} \u2264 \u221a C \u2211 s,a\u2208X c2(s, a) \u221a\u221a\u221a\u221aH\u22121\u2211 t=1 P1:t\u22121I{s = \u00b7, a = \u03c0t(s)} \u221a\u221a\u221a\u221aH\u22121\u2211 t=1 \u03c3\u0303 (d),2 t:H (s, a))P1:t\u22121I{s = \u00b7, a = \u03c0t(s)}\n\u2264 \u221a C \u2211 s,a\u2208X \u221a\u221a\u221a\u221a8w(s, a) n(s, a) ln 6 \u03b41 H\u22121\u2211 t=1 \u03c3\u0303 (d),2 t:H (s, a))P1:t\u22121I{s = \u00b7, a = \u03c0t(s)}\nwhere we first applied the Cauchy-Schwarz inequality and then used the definition of c2(s, a) and w(s, a).\nC(s0) \u2264 \u221a C \u2211 \u03ba,\u03b9 \u2211 s,a\u2208X\u03ba,\u03b9 \u221a\u221a\u221a\u221a8w(s, a) n(s, a) ln 6 \u03b41 H\u22121\u2211 t=1 \u03c3\u0303 (d),2 t:H (s, a))P1:t\u22121I{s = \u00b7, a = \u03c0t(s)}(s0)\n\u2264 \u221a C \u2211 \u03ba,\u03b9 \u221a\u221a\u221a\u221a|X\u03ba,\u03b9| \u2211 s,a\u2208X\u03ba,\u03b9 8w(s, a) n(s, a) ln 6 \u03b41 H\u22121\u2211 t=1 \u03c3\u0303 (d),2 t:H (s, a))P1:t\u22121I{s = \u00b7, a = \u03c0t(s)}(s0)\n\u2264 \u221a C \u2211 \u03ba,\u03b9 \u221a\u221a\u221a\u221a \u2211 s,a\u2208X\u03ba,\u03b9 8 m ln 6 \u03b41 H\u22121\u2211 t=1 \u03c3\u0303 (d),2 t:H (s, a))P1:t\u22121I{s = \u00b7, a = \u03c0t(s)}(s0)\n\u2264 \u221a\u221a\u221a\u221aC |K \u00d7 I| 8 m ln 6\n\u03b41 \u2211 s,a\u2208X H\u22121\u2211 t=1 \u03c3\u0303 (d),2 t:H (s, a))P1:t\u22121I{s = \u00b7, a = \u03c0t(s)}(s0)\n\u2264 \u221a\u221a\u221a\u221aC |K \u00d7 I| 8 m ln 6\n\u03b41 \u2211 s,a\u2208S\u00d7A H\u22121\u2211 t=1 \u03c3\u0303 (d),2 t:H (s, a))P1:t\u22121I{s = \u00b7, a = \u03c0t(s)}(s0)\n= \u221a\u221a\u221a\u221aC |K \u00d7 I| 8 m ln 6\n\u03b41 H\u22121\u2211 t=1 P1:t\u22121\u03c3\u0303 (d),2 t:H (s0) (10)\n\u2264 \u221a C |K \u00d7 I| 8H 2d+3 ln(6/\u03b41)\nm = C\u0302d\nWe first split the sum and applied the Cauchy-Schwarz inequality. Then we used again Inequality (9) and |X\u03ba,\u03b9| \u2264 \u03ba. In the fourth step, we applied Cauchy-Schwarz and the final inequality follows from \u2016\u03c3\u0303(d),2t:H \u2016\u221e \u2264 H2d+2 and the fact that P1:t\u22121 is non-expansive. Alternatively, we can rewrite the bound in Equation (10) as\nC(s0) \u2264 \u221a\u221a\u221a\u221aC |K \u00d7 I| 8 m ln 6\n\u03b41 H\u22121\u2211 t=1 P1:t\u22121\u03c3\u0303 (d),2 t:H (s0)\n= \u221a\u221a\u221a\u221aC |K \u00d7 I| 8 m ln 6\n\u03b41 H\u22121\u2211 t=1 P1:t\u22121\u03c3\u0303 (d),2 t:H (s0)\u2212 P\u03031:t\u22121\u03c3\u0303 (d),2 t:H (s0) + P\u03031:t\u22121\u03c3\u0303 (d),2 t:H (s0).\nLemma 4 shows that the variance V\u0303(d)1:H also satisfies the Bellman equation with the local variances \u03c3\u0303 (d),2 i:j . This insight allows us to bound \u2211H\u22121 t=1 P\u03031:t\u22121\u03c3\u0303 (d),2 t:H (s0) = V\u0303 (d) 1:H(s0) \u2264 H2d+2. Also, note that \u03c3\u0303(d),2t:H = r (2d+2) t which gives us\nC(s0) \u2264 \u221a\u221a\u221a\u221aC |K \u00d7 I| 8 m ln 6\n\u03b41\n( H2d+2 +\nH\u22121\u2211 t=1 P1:t\u22121r (2d+2) t (s0)\u2212 P\u03031:t\u22121r (2d+2) t (s0)\n)\n= \u221a C |K \u00d7 I| 8\nm ln\n6\n\u03b41\n( H2d+2 + V\n(2d+2) 1:H (s0)\u2212 V\u0303 (2d+2) 1:H (s0) ) \u2264 \u221a C |K \u00d7 I| 8\nm ln\n6 \u03b41 (H2d+2 + \u22062d+2)\n\u2264 \u221a C |K \u00d7 I| 8\nm H2d+2 ln\n6 \u03b41 +\n\u221a C |K \u00d7 I| 8\nm \u22062d+2 ln\n6 \u03b41 = C\u0302 \u2032d + C\u0302\n\u2032\u2032\u221a\u22062d+2\nC.4.2 Proof of Lemma 4 (Bellman equation of local value function variances)"}, {"heading": "Proof of Lemma 4.", "text": "Vi:j(s) =E ( j\u2211 t=i rt(st)\u2212 Vi:j(si) )2 |si = s  =E\n( j\u2211 t=i+1 rt(st)\u2212 Vi+1:j(si+1) + Vi+1:j(si+1) + ri(si)\u2212 Vi:j(si) )2 |si = s  =E\n( j\u2211 t=i+1 r(st)\u2212 Vi+1:j(si+1) )2 |si = s  + 2E [( j\u2211\nt=i+1\nrt(st)\u2212 Vi+1:j(si+1) ) (Vi+1:j(si+1) + ri(si)\u2212 V (si)) |si = s ] + E [ (Vi+1:j(si+1) + ri(si)\u2212 Vi:j(si))2 |si = s\n] =E [Vi+1:j(si+1)|si = s]\n+ 2E [ E [( j\u2211\nt=i+1\nrt(st)\u2212 Vi+1:j(si+1) ) (Vi+1:j(si+1) + ri(si)\u2212 Vi:j(si)) |si+1 ] |si = s ]\n+ E [ (Vi+1:j(si+1)\u2212 PiVi+1:j(si))2 |si = s ] where the final equality follows from the tower property of conditional expectations, and the fact that Vi:j(si) = PiVi+1:j(si) + ri(si). Since by the definition of the value function\nE\n[( j\u2211\nt=i+1\nrt(st)\u2212 Vi+1:j(si+1) ) |si+1 ] = 0\nthe middle term vanishes and the last term is by definition \u03c32i:j(s) we obtain\nVi:j(s) = PiVi+1:j(s) + \u03c3 2 i:j(s).\nNoting that Vj:j(s) = (rj(s)\u2212 rj(s))2 = 0, we can unroll the equation and obtain\nVi:j(s) = j\u2211 t=i Pi:t\u22121\u03c3 2 t:j(s).\nFrom the definition of V1:H and the fact that 0 \u2264 r(\u00b7) \u2264 rmax, we see that 0 \u2264 V1:H \u2264 H2r2max and the final statement of the lemma follows."}, {"heading": "C.4.3 Proof of Lemma 3", "text": "Proof of Lemma 3. The recursive bound from Lemma C.7 \u2206d \u2264 A\u0302d + B\u0302d + C\u0302 \u2032d + C\u0302 \u2032\u2032 \u221a \u22062d+2\nhas the form \u2206d \u2264 Yd + Z \u221a \u22062d+2. Expanding this form and using the triangle inequality gives\n\u22060 \u2264Y0 + Z \u221a \u22062 \u2264 Y0 + Z \u221a Y2 + Z \u221a \u22066 \u2264 Y0 + Z \u221a Y2 + Z 3/2\u2206 1/4 6\n\u2264Y0 + Z \u221a Y2 + Z 3/2Y 1/4 6 + Z 7/4\u2206 1/8 14 \u2264 . . .\nand by doing this up to level \u03b3 = d lnH2 ln 2e, we obtain\n\u22060 \u2264 \u2211\nd\u2208D\\{\u03b3}\nZ 2d 2+dY 2 2+d d + Z 2\u03b3 2+\u03b3 \u2206 2 2+\u03b3 \u03b3\nwhere D = {0, 2, 6, 14, . . . \u03b3}. Note that the exponent of H compared to m is the larger in C\u0302 \u2032d than in B\u0302d. Therefore, for sufficiently large m, C\u0302 \u2032d dominates the other term. More precisely, for\nm \u2265 338H 9 C |K \u00d7 I| ln 6 \u03b41\n(11)\nwe have B\u0302d \u2264 C\u0302 \u2032d. We can therefore consider Z = C\u0302 \u2032\u2032 and Yd = 2C\u0302 \u2032d + A\u0302d. Also, since C\u0302d \u2265 C\u0302 \u2032d, we can bound \u2206\u03b3 \u2264 A\u0302d + 2C\u0302d. For notational simplicity, we will use the auxiliary variable\nm1 = 8C|K \u00d7 I|H2\nm 2 ln\n6 \u03b41 .\nand get\nZ = C\u0302 \u2032\u2032 = \u221a m1\nH and\nYd = A\u0302d + 2C\u0302 \u2032 d = (1/4 + 2 \u221a m1)H d and \u2206\u03b3 \u2264 A\u0302\u03b3 + 2C\u0302\u03b3 = (1/4 + 2 \u221a m1H)H \u03b3 .\nThen( Z2dY 2d )(2+d)\u22121 = ( md1 2d+2(1/4 + 2 \u221a m1) 2 )(2+d)\u22121 = ( md1 d(1/4 + 2 \u221a m1) 2 )(2+d)\u22121\nand ( Z2\u03b3\u2206\u03b3 )(2+\u03b3)\u22121 = ( m\u03b31 2\u03b3+2(1/4 + 2 \u221a m1H) 2 )(2+\u03b3)\u22121 = ( m\u03b31 \u03b3(1/4 + 2 \u221a m1H) 2 )(2+\u03b3)\u22121 .\nPutting these pieces together, we obtain\n\u22060 \u2264 \u2211 d\u2208D\\{\u03b3} ( m1) d 2+d ( 1 4 + 2 \u221a m1 ) 2 d+2 + ( m1) \u03b3 \u03b3+2 ( 1 4 + 2 \u221a Hm1 ) 2 \u03b3+2\n= 1 4 + 2 \u221a m1 + \u2211 d\u2208D\\{0,\u03b3} ( m1) d 2+d ( 1 4 + 2 \u221a m1 ) 2 d+2 + ( m1) \u03b3 \u03b3+2 ( 1 4 + 2 \u221a Hm1 ) 2 \u03b3+2\n\u22641 4 + 2 \u221a m1 + \u2211 d\u2208D\\{0,\u03b3} ( m1) d 2+d\n[( 1\n4\n) 2 d+2\n+ (2 \u221a m1) 2 d+2\n]\n+ ( m1) \u03b3 \u03b3+2\n[( 1\n4\n) 2 \u03b3+2 + ( 2 \u221a Hm1 ) 2 \u03b3+2 ]\nwhere we used the fact that (a + b)\u03c6 \u2264 a\u03c6 + b\u03c6 for a, b > 0 and 0 < \u03c6 < 1. We now bound the H1/(2+\u03b3) by using the definition of \u03b3. Since\n1\n2 + \u03b3 =\n2 ln 2\n4 ln 2 + lnH \u2264 2 logH 2\nand since H \u2265 1, we have H1/(2+\u03b3) \u2264 4. Therefore\n\u22060 \u22641 4 + 2 \u221a m1 + \u2211 d\u2208D\\{0,\u03b3} ( m1) d 2+d\n[( 1\n4\n) 2 d+2\n+ (2 \u221a m1) 2 d+2\n]\n+ ( m1) \u03b3 \u03b3+2\n[( 1\n4\n) 2 \u03b3+2\n+ 4 (2 \u221a m1) 2 \u03b3+2\n]\n\u22641 4 + 2 \u221a m1 + \u2211 d\u2208D\\{0} ( m1) d 2+d\n[( 1\n4\n) 2 d+2\n+ 4 (2 \u221a m1) 2 d+2\n]\n\u22641 4 + 2 \u221a m1 + log2 \u03b3\u2211 i=1 ( m1) 1\u22122\u2212i\n[( 1\n4\n)2\u2212i + 4 (2 \u221a m1) 2\u2212i ]\n\u22641 4 + 2 \u221a m1 + log2 \u03b3\u2211 i=1 m1\u22122 \u2212i 1\n[( 1\n4\n)2\u2212i + 4 (2 \u221a m1) 2\u2212i ] In the first inequality, we used the bound for H1/(2+\u03b3) and in the second inequality we simplified the expression by noting that all terms are nonnegative. In the next step, we re-parameterized the sum. In the final inequality, we used the assumption that 0 < \u2264 1 and therefore 1\u22122\u2212i \u2264 1.\n\u22060 \u22641 4 + 2 \u221a m1 + 1 4 log2 \u03b3\u2211 i=1 (4m1) 1\u22122\u2212i + 4 log2 \u03b3\u2211 i=1 (m1) 1\u22122\u2212i (4m1) 2\u2212i\u22121\n\u22641 4 + 2 \u221a m1 + 1 4 log2 \u03b3\u2211 i=1 (4m1) 1\u22122\u2212i + 16 log2 \u03b3\u2211 i=1 (m1 4 )1\u22122\u2212i\u22121 .\nBy requiring that\nm1 \u2264 1\n4 and noting that 1\u2212 2\u2212i \u2265 1/2 and 1\u2212 2\u2212i\u22121 \u2265 3/4 for i \u2265 1, we can bound the expression by\n\u22060 \u22641 4 + 2 \u221a m1 + 1 4 log2(\u03b3) \u221a 4m1 + 16 log2(\u03b3) (m1 4 )3/4 .\nBy requiring that m1 \u2264 1/64 and m1 \u2264 (2 log2 \u03b3)\u22122 and m1 \u2264 1/64(log2 \u03b3)\u22124/3, we can assure that \u22060 \u2264 . Taking all assumptions on m1 we made above together, we realize that\nm1 \u2264 (\n1\n8 log2 log2H\n)2 \u2264 ( 1\n8 log2 \u03b3 )2 is sufficient for them to hold where we used log2 \u03b3 = log2(d 12 log2He) \u2264 log2 log2H . This gives the following condition on m\nm \u2265 512C(log2 log2H)2|K \u00d7 I| H2 2 ln 6\n\u03b41\nwhich is a stronger condition that the one in Equation (11).\nBy construction of \u03b9(s, a), we have \u03b9(s, a) \u2264 2 Hwmin = 8|S|H2 = 8H2|S|\n. Also, \u03bak(s, a) \u2264 |S|mH mwmin = 4|S| 2H2 . Therefore\n|K \u00d7 I| \u2264 log2 4|S|2H2 log2 8H2|S| \u2264 log22 8H2|S|2\nwhich let us conclude that\nm \u2265 512CH 2\n2 (log2 log2H) 2 log22\n( 8H2|S|2 ) ln 6\n\u03b41\nis a sufficient condition and thus, the statement to show, holds."}, {"heading": "C.5 Proof of Theorem 1", "text": "Proof of Theorem 1. By Lemma 2, we know that the number of episodes where |X\u03ba,\u03b9| > \u03ba for some \u03ba, \u03b9 is bounded by 6Emax|S \u00d7A|m with probability at least 1\u2212\u03b4/2. For all other episodes, we have by Lemma 3 that |R\u0303\u03c0k \u2212 R\u03c0k | < . Since, with probability at least 1 \u2212 \u03b4/2, we have by Lemma 1 M \u2208 Mk, we can use Lemma A.1 which gives R\u0303\u03c0k > R\u2217 \u2265 R\u03c0k to conclude that with probabilty at least 1\u2212 \u03b4/2, for all episodes with |X\u03ba,\u03b9| \u2264 \u03ba for all \u03ba, \u03b9, we have R\u2217 \u2212 R\u03c0k < . Applying the union bound, we get the desired result, if m satisfies\nm \u2265512CH 2\n2 (log2 log2H) 2 log22\n( 8H2|S|2 ) ln 6\n\u03b41 and\nm \u22656H 2 ln 2Emax \u03b4 .\nFrom the definitions, we get\nln 6\n\u03b41 = ln 6CUmax \u03b4 = ln 6|S \u00d7 A|C log2(|S|H/wmin) \u03b4 = ln 6|S \u00d7 A|C log2(4|S|2H2/ ) \u03b4\nand\nEmax = log2 |S| log2 4H2|S| \u2264 log22 4H2|S|\nand\nln 2Emax \u03b4 = ln 2 log2 |S| log2(4H2|S|/ ) \u03b4 \u2264 ln 2 log 2 2(4H 2|S|/ ) \u03b4\n\u2264 ln 6 |S \u00d7 A| log 2 2(4|S|2H2/ ) \u03b4 .\nSetting\nm = 512(log2 log2H) 2CH\n2\n2 log2\n( 8H2|S|2 ) ln\n6 |S \u00d7 A|C log22(4|S|2H2/ ) \u03b4\nis therefore a valid choice for m to ensure that with probability at least 1\u2212 \u03b4 , there are at most\n6mEmax =3072(log2 log2H) 2CH 2 |S \u00d7 A| 2\n\u00d7 log22 ( 4H2|S| ) log2 ( 8H2|S|2 ) ln\n6|S \u00d7 A|C log22(4|S|2H2/ ) \u03b4\n-suboptimal episodes."}, {"heading": "D Proof of the Lower PAC Bound", "text": "Proof of Theorem 2. We consider the class of MDPs shown in Figure 1. The MDPs essentially consist of n parallel multi-armed bandits. For each bandit, there exist m + 1 = |A| possible instantiations, which we denote by Ii = 0 . . .m. The instantiation, or hypothesis, Ii = 0 corresponds to i(a) = I{a = a0} \u2032/2, that is, only action a0 has a small bias. The other hypotheses Ii = j for j = 1 . . .m correspond to i(a) = I{a = a0} \u2032/2 + I{a = aj} \u2032. We use I = (I1, . . . In) to indicate the instance of the entire MDP.\nWe defineGi = {\u03c9 \u2208 \u2126 : \u03c0(i) = aIi}, the event that \u03c0, the policy generated byA chooses optimally in bandit i. For a given instance I , the difference between the optimal expected cumulative reward R\u2217I and the expected cumulative reward R \u03c0 I of policy \u03c0 is at least\nR\u2217I \u2212R\u03c0I \u2265 (H \u2212 2)\n( 1\u2212 1\nn n\u2211 i=1 I{Gi}\n) \u2032\n2 .\nFor \u03c0 to be -optimal, we therefore need\n\u2265R\u2217I \u2212R\u03c0I \u2265 (H \u2212 2)\n( 1\u2212 1\nn n\u2211 i=1 I{Gi}\n) \u2032\n2 ,\n2\n(H \u2212 2) \u2032 \u2265\n( 1\u2212 1\nn n\u2211 i=1 I{Gi}\n) ,\n1\nn n\u2211 i=1 I{Gi} \u2265 ( 1\u2212 2 (H \u2212 2) \u2032 ) ,\n1\nn n\u2211 i=1 I{Gi} \u2265 ( 1\u2212 2 (H \u2212 2)\u03b7 (H \u2212 2)16 e4 ) = 1\u2212 \u03b7 8e4\nwhere we chose value \u2032 := 16 e 4 (H\u22122)\u03b7 for \u2032. We will specify the exact value of parameter \u03b7 later. The condition basically states that at least a fraction of \u03c6 := 1\u2212 \u03b7/(8e4) bandits need to be solved optimally by A for the resulting policy \u03c0 to be -accurate. For A to be ( , \u03b4)-correct, we therefore need\nPI\n( 1\nn n\u2211 i=1 I{Gi} \u2265 \u03c6 ) \u2265 PI(R\u2217I \u2212R\u03c0I \u2265 ) \u2265 1\u2212 \u03b4\nfor each instance I . Using Markov\u2019s inequality, we obtain\n1\u2212 \u03b4 \u2264 PI\n( 1\nn n\u2211 i=1 I{Gi} \u2265 \u03c6 ) \u2264 1 n\u03c6 n\u2211 i=1 EI [I{Gi}] \u2264 1 n\u03c6 n\u2211 i=1 PI(Gi)\nAll Gi are independent of each other by construction of the MDP. In fact \u2211n i=1 I{Gi} is Poissonbinomial distributed as I{Gi} are independent Bernoulli random variables with potentially different mean. Therefore, upper bounds \u03b4i must exist such that \u03b4i \u2265 PI(GCi ) for all hypotheses I and such that 1\u2212\u03b4 \u2264 1n\u03c6 \u2211n i=1(1\u2212\u03b4i) or equivalently n(1+\u03b4\u03c6\u2212\u03c6) \u2265 \u2211n i=1 \u03b4i. Since allGi are independent of each other and\n\u2032 = 16 e4 (H \u2212 2)\u03b7 \u2264 16(H \u2212 2)e 4\u03b7 (H \u2212 2)64e4\u03b7 = 1 4\nwe can apply Theorem 1 by Mannor and Tsitsiklis [19] in cases where\n\u03b4i \u2264 1 \u03b7 (1\u2212 \u03c6+ \u03b4\u03c6) \u2264 1 \u03b7 (1\u2212 \u03c6+ \u03b4) \u2264 1 8e4 + \u03b4 \u03b7 \u2264 2 8e4 .\nThis result gives us the minimum expected number of times EI [ni] we need to observe state i to ensure that PI(GCi ) \u2264 \u03b4i\nEI [ni] \u2265 [ c1(|A| \u2212 1)\n\u20322 ln ( c2 \u03b4i )] I{\u03b7\u03b4i \u2264 1\u2212 \u03c6+ \u03c6\u03b4},\nfor appropriate constants c1 and c2 (e.g. c1 = 400 and c2 = 4). We can find a valid lower bound for the total number of samples for any \u03b41, . . . \u03b4n by considering the worst bound over all \u03b41, . . . \u03b4n. The following optimization problem encodes this idea\nmin \u03b41,...\u03b4n n\u2211 i=1 ln 1 \u03b4i I{\u03b7\u03b4i \u2264 1\u2212 \u03c6+ \u03c6\u03b4} (12)\ns.t. n\u2211 i=1 \u03b4i \u2264 n(1 + \u03c6\u03b4 \u2212 \u03c6)\nAs shown in Lemma D.1 in the supplementary material, the optimal solution of the optimization problem in Equation (12) is \u03b41 = \u00b7 \u00b7 \u00b7 = \u03b4n = c if \u03b7(1 \u2212 ln c) \u2264 1 with c = 1 + \u03b4\u03c6 \u2212 \u03c6. Since the left-hand side of this condition is decreasing in c, we can plug in a lower bound of c \u2265 1\u2212 \u03c6 = \u03b78e4 and get the sufficient condition\n\u03b7(1\u2212 ln \u03b7 8e4 ) = \u03b7(1\u2212 ln \u03b7 + 4 + ln 8) \u2264 1.\nIt is easy to verify that \u03b7 = 1/10 satisfies this condition. Hence \u03b41 = \u00b7 \u00b7 \u00b7 = \u03b4n = c is the optimal solution to the problem in Equation (12). In each episode, we only observe a single state i and therefore, there need to be at least\nEI [nA] \u2265 n\u2211 i=1 EI [ni] \u2265 c1(|A| \u2212 1)n \u20322 ln ( c2 \u03b4i ) \u2265 c1(|A| \u2212 1)n \u20322 ln ( c2 \u03b4 + \u03b78e4 ) observed episodes for appropriate constants c1 and c2. Plugging in \u2032 and n = |S| \u2212 3, we obtain the desired statement.\nLemma D.1. The optimization problem\nmin \u03b41...\u03b4n\u2208[0,1] n\u2211 i=1 ln 1 \u03b4 i I{\u03b7\u03b4i \u2264 c}\ns.t. n\u2211 i=1 \u03b4i \u2264 nc\nwith c \u2208 [0, 1] and \u03b7(1\u2212 ln c) \u2264 1\nhas optimal solution \u03b41 = \u00b7 \u00b7 \u00b7 = \u03b4n = c.\nProof. Without the indicator part in the objective, we can show that \u03b41 = \u00b7 \u00b7 \u00b7 = \u03b4n = c is an optimal solution by checking the KKT conditions and noting that the problem is convex. Let k denote the number of \u03b4j that are set such that the indicator function is 0. Without loss of generality we can assume that their value is \u03b4P := c/\u03b7 and the remaining \u03b4j take the same value \u03b4A (for a fixed \u03b4P and k, the problem reduces to the one without the indicator functions). Then the problem transforms into\nmin \u03b4A\u2208(0,1),k\u2208{0,1,...n} (n\u2212 k) ln 1 \u03b4A\n(n\u2212 k)\u03b4A + k\u03b4P \u2264 nc\nWe can rewrite the constraint as\n(n\u2212 k)\u03b4A + k\u03b4P \u2264 nc (n\u2212 k)\u03b4A \u2264 nc\u2212 k\u03b4P = ( n\u2212 k\n\u03b7\n) c\n\u03b4A \u2264 n\u2212 k\u03b7 n\u2212 k c.\nSince the objective decreases with \u03b4A, it is optimal to choose \u03b4A as large as possible. The optimization problem then reduces to\nmin k\u2208{0,...bn/\u03b3c} (n\u2212 k) ln ( n\u2212 k n\u2212 \u03b3k c\u22121 ) .\nwhere we used for convenience \u03b3 := 1/\u03b7. We want to show that the optimal solution to this problem is k = 0. We can therefore relax the problem to the continuous domain without loss of generality\nmin k\u2208[0,n/\u03b3] (n\u2212 k) ln ( n\u2212 k n\u2212 \u03b3k c\u22121 ) .\nBy reparameterizing the problem with \u03b1 = k/n, we get\nmin \u03b1\u2208[0,1/\u03b3]\nn(1\u2212 \u03b1) ln (\n1\u2212 \u03b1 c(1\u2212 \u03b3\u03b1)\n) .\nWe realize that the minimizer does not depend on n (while the value does). The second derivative of the objective function is\nn (\u03b3 \u2212 1)2\n(1\u2212 \u03b3\u03b1)2(1\u2212 \u03b1) ,\nwhich is nonnegative for \u03b1 \u2208 [0, 1/\u03b3]. Hence, the objective is convex in the feasible region and the minimizer of this problem is \u03b1 = 0 if the derivative of the objective is nonnegative in 0. The derivative of the objective in 0 is given by\nn(\u03b3 \u2212 1 + ln(c)).\nA sufficient condition for \u03b1 = 0 being optimal is therefore\n\u03b3 \u2265 1\u2212 ln c\nor, in terms of the original problem with \u03b7 = 1/\u03b3, \u03b41 = . . . \u03b4n = c is optimal if\n\u03b7(1\u2212 ln c) \u2264 1"}], "references": [{"title": "PAC Model-Free Reinforcement Learning", "author": ["Alexander L. Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L. Littman"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms", "author": ["Michael J Kearns", "Satinder P Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "R-MAX \u2013 A General Polynomail Time Algorithm for Near-Optimal Reinforcement Learning", "author": ["Ronen I Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["Sham M. Kakade"], "venue": "PhD thesis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Online Regret Bounds for a New Reinforcement Learning Algorithm", "author": ["Peter Auer", "Ronald Ortner"], "venue": "In Proceedings 1st Austrian Cognitive Vision Workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "PAC bounds for discounted MDPs", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["Istv\u00e0n Szita", "Csaba Szepesv\u00e1ri"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "On the Sample Complexity of Reinforcement Learning with a Generative Model", "author": ["Mohammad Gheshlaghi Azar", "R\u00e9mi Munos", "Hilbert J. Kappen"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["J Zico Kolter", "Andrew Y Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Efficient reinforcement learning", "author": ["Claude-Nicolas Fiechter"], "venue": "In Conference on Learning Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Expected Mistake Bound Model for On-Line Reinforcement Learning", "author": ["Claude-Nicolas Fiechter"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Efficient PAC learning for episodic tasks with acyclic state spaces", "author": ["Spyros Reveliotis", "Theologos Bountourelis"], "venue": "Discrete Event Dynamic Systems: Theory and Applications,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Incremental Model-based Learners With Formal Learning-Time Guarantees", "author": ["Alexander L Strehl", "Lihong Li", "Michael L Littman"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Reinforcement Learning in Finite MDPs : PAC Analysis", "author": ["Alexander L Strehl", "Lihong Li", "Michael L Littman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Near-optimal Regret Bounds for Reinforcement Learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "An analysis of model-based Interval Estimation for Markov Decision Processes", "author": ["Alexander L. Strehl", "Michael L. Littman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "The Variance of Markov Decision Processes", "author": ["Matthew J Sobel"], "venue": "Journal of Applied Probability,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1982}, {"title": "Empirical Bernstein Bounds and Sample-Variance Penalization", "author": ["Andreas Maurer", "Massimiliano Pontil"], "venue": "In Conference on Learning Theory,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "The Sample Complexity of Exploration in the Multi-Armed Bandit Problem", "author": ["Shie Mannor", "John N Tsitsiklis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Near-optimal Regret Bounds for Reinforcement Learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "We formalize this as the sample complexity of reinforcement learning [1], which is the number of time steps on which the algorithm may select an action whose value is not near-optimal.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "RL algorithms with a sample complexity that is a polynomial function of the domain parameters are referred to as Probably Approximately Correct (PAC) [2, 3, 4, 1].", "startOffset": 150, "endOffset": 162}, {"referenceID": 2, "context": "RL algorithms with a sample complexity that is a polynomial function of the domain parameters are referred to as Probably Approximately Correct (PAC) [2, 3, 4, 1].", "startOffset": 150, "endOffset": 162}, {"referenceID": 3, "context": "RL algorithms with a sample complexity that is a polynomial function of the domain parameters are referred to as Probably Approximately Correct (PAC) [2, 3, 4, 1].", "startOffset": 150, "endOffset": 162}, {"referenceID": 0, "context": "RL algorithms with a sample complexity that is a polynomial function of the domain parameters are referred to as Probably Approximately Correct (PAC) [2, 3, 4, 1].", "startOffset": 150, "endOffset": 162}, {"referenceID": 7, "context": "1 It does not require additional structure such as assuming access to a generative model [8] or that the state transitions are sparse or acyclic [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "1 It does not require additional structure such as assuming access to a generative model [8] or that the state transitions are sparse or acyclic [6].", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "The limited prior research on upper bound PAC results for finite horizon MDPs has focused on different settings, such as partitioning a longer trajectory into fixed length segments [4, 1], or considering a sliding time window [9].", "startOffset": 181, "endOffset": 187}, {"referenceID": 0, "context": "The limited prior research on upper bound PAC results for finite horizon MDPs has focused on different settings, such as partitioning a longer trajectory into fixed length segments [4, 1], or considering a sliding time window [9].", "startOffset": 181, "endOffset": 187}, {"referenceID": 8, "context": "The limited prior research on upper bound PAC results for finite horizon MDPs has focused on different settings, such as partitioning a longer trajectory into fixed length segments [4, 1], or considering a sliding time window [9].", "startOffset": 226, "endOffset": 229}, {"referenceID": 9, "context": "2 Fiechter [10, 11] and Reveliotis and Bountourelis [12] do tackle a closely related setting, but find a dependence that is at least H.", "startOffset": 11, "endOffset": 19}, {"referenceID": 10, "context": "2 Fiechter [10, 11] and Reveliotis and Bountourelis [12] do tackle a closely related setting, but find a dependence that is at least H.", "startOffset": 11, "endOffset": 19}, {"referenceID": 11, "context": "2 Fiechter [10, 11] and Reveliotis and Bountourelis [12] do tackle a closely related setting, but find a dependence that is at least H.", "startOffset": 52, "endOffset": 56}, {"referenceID": 5, "context": "Our work builds on recent work [6, 8] on PAC infinite horizon discounted RL that offers much tighter upper and lower sample complexity bounds than was previously known.", "startOffset": 31, "endOffset": 37}, {"referenceID": 7, "context": "Our work builds on recent work [6, 8] on PAC infinite horizon discounted RL that offers much tighter upper and lower sample complexity bounds than was previously known.", "startOffset": 31, "endOffset": 37}, {"referenceID": 0, "context": "The reward function r is possibly time-dependent and takes values in [0, 1].", "startOffset": 69, "endOffset": 75}, {"referenceID": 4, "context": "1 Previous works [5] have shown that the complexity of learning state transitions usually dominates learning reward functions.", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "We therefore follow existing sample complexity analyses [6, 7] and assume known rewards for simplicity.", "startOffset": 56, "endOffset": 62}, {"referenceID": 6, "context": "We therefore follow existing sample complexity analyses [6, 7] and assume known rewards for simplicity.", "startOffset": 56, "endOffset": 62}, {"referenceID": 2, "context": "Like many other PAC RL algorithms [3, 13, 14, 15], UCFH uses an optimism under uncertainty approach to balance exploration and exploitation.", "startOffset": 34, "endOffset": 49}, {"referenceID": 12, "context": "Like many other PAC RL algorithms [3, 13, 14, 15], UCFH uses an optimism under uncertainty approach to balance exploration and exploitation.", "startOffset": 34, "endOffset": 49}, {"referenceID": 13, "context": "Like many other PAC RL algorithms [3, 13, 14, 15], UCFH uses an optimism under uncertainty approach to balance exploration and exploitation.", "startOffset": 34, "endOffset": 49}, {"referenceID": 14, "context": "Like many other PAC RL algorithms [3, 13, 14, 15], UCFH uses an optimism under uncertainty approach to balance exploration and exploitation.", "startOffset": 34, "endOffset": 49}, {"referenceID": 4, "context": "Specifically, we use a finite horizon variant of extended value iteration (EVI) [5, 14].", "startOffset": 80, "endOffset": 87}, {"referenceID": 13, "context": "Specifically, we use a finite horizon variant of extended value iteration (EVI) [5, 14].", "startOffset": 80, "endOffset": 87}, {"referenceID": 5, "context": "UCFH is inspired by the infinite-horizon UCRL-\u03b3 algorithm by Lattimore and Hutter [6] but has several important differences.", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "H, s\u2032 \u2208 S(s, a) p\u0303t(s \u2032|s, a) \u2208 ConfidenceSet(p\u0302(s\u2032|s, a), n(s, a)) } ; M\u0303k, \u03c0 k := FixedHorizonEVI(Mk); /* Execute policy */ repeat SampleEpisode(\u03c0) ; // from M using \u03c0 until there is a (s, a) \u2208 S \u00d7A with v(s, a) \u2265 max{mwmin, n(s, a)} and n(s, a) < |S|mH; /* Update model statistics for one (s, a)-pair with condition above */ n(s, a) := n(s, a) + v(s, a); n(s, a, s\u2032) := n(s, a, s\u2032) + v(s, a, s\u2032) \u2200s\u2032 \u2208 S(s, a); v(s, a) := v(s, a, s\u2032) := 0 \u2200s\u2032 \u2208 S(s, a); k := k + 1 Procedure SampleEpisode(\u03c0) s0 \u223c p0; for t = 0 to H \u2212 1 do at := \u03c0t+1(st) and st+1 \u223c p(\u00b7|st, at); v(st, at) := v(st, at) + 1 and v(st, at, st+1) := v(st, at, st+1) + 1; Function ConfidenceSet(p, n) P := { p\u2032 \u2208 [0, 1] :if n > 1 : \u2223\u2223\u2223\u221ap\u2032(1\u2212 p\u2032)\u2212\u221ap(1\u2212 p)\u2223\u2223\u2223 \u2264\u221a2 ln(6/\u03b41) n\u2212 1 , (1)", "startOffset": 677, "endOffset": 683}, {"referenceID": 5, "context": "Unlike the confidence intervals used by Lattimore and Hutter [6], we not only include conditions based on Hoeffding\u2019s inequality5 and Bernstein\u2019s inequality (Eq.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Brafman and Tennenholtz [3], Strehl and Littman [16]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 15, "context": "Brafman and Tennenholtz [3], Strehl and Littman [16]).", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "The general proof strategy is closest to the one of UCRL-\u03b3 [6] and the obtained bounds are similar if we replace the time horizon H with the equivalent in the discounted case 1/(1\u2212 \u03b3).", "startOffset": 59, "endOffset": 62}, {"referenceID": 5, "context": "\u2022 A central quantity in the analysis by Lattimore and Hutter [6] is the local variance of the value function.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "The key insight for the almost tight bounds of Lattimore and Hutter [6] and Azar et al.", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "[8] is to leverage the fact that these local variances satisfy a Bellman equation [17] and so the discounted sum of local variances can be bounded by O((1\u2212\u03b3)\u22122) instead of O((1\u2212\u03b3)\u22123).", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[8] is to leverage the fact that these local variances satisfy a Bellman equation [17] and so the discounted sum of local variances can be bounded by O((1\u2212\u03b3)\u22122) instead of O((1\u2212\u03b3)\u22123).", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "\u2022 Lattimore and Hutter [6] assumed there are only two possible successor states (i.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "We therefore avoid assuming C = 2 which makes UCFH directly applicable to generic MDPs with C > 2 without the impractical transformation argument used by Lattimore and Hutter [6].", "startOffset": 175, "endOffset": 178}, {"referenceID": 2, "context": "Many PAC RL sample complexity proofs [3, 4, 13, 14] only have a binary notion of \u201cknownness\u201d, distinguishing between known (transition probability estimated sufficiently accurately) and unknown (s, a)-pairs.", "startOffset": 37, "endOffset": 51}, {"referenceID": 3, "context": "Many PAC RL sample complexity proofs [3, 4, 13, 14] only have a binary notion of \u201cknownness\u201d, distinguishing between known (transition probability estimated sufficiently accurately) and unknown (s, a)-pairs.", "startOffset": 37, "endOffset": 51}, {"referenceID": 12, "context": "Many PAC RL sample complexity proofs [3, 4, 13, 14] only have a binary notion of \u201cknownness\u201d, distinguishing between known (transition probability estimated sufficiently accurately) and unknown (s, a)-pairs.", "startOffset": 37, "endOffset": 51}, {"referenceID": 13, "context": "Many PAC RL sample complexity proofs [3, 4, 13, 14] only have a binary notion of \u201cknownness\u201d, distinguishing between known (transition probability estimated sufficiently accurately) and unknown (s, a)-pairs.", "startOffset": 37, "endOffset": 51}, {"referenceID": 5, "context": "However, as recently shown by Lattimore and Hutter [6] for the infinite horizon setting, it is possible to obtain much tighter sample complexity results by using a more fine grained categorization.", "startOffset": 51, "endOffset": 54}, {"referenceID": 17, "context": "By combining Hoeffding\u2019s inequality, Bernstein\u2019s inequality and the concentration result on empirical standard deviations by Maurer and Pontil [18] with the union bound, we get that p(s\u2032|s, a) \u2208 P with probability at least 1 \u2212 \u03b41 for a single phase k, fixed s, a \u2208 S \u00d7 A and fixed s\u2032 \u2208 S(s, a).", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "The ranges of possible \u03b4 and are of similar order than in other state-of-the-art lower bounds for multi-armed bandits [19] and discounted MDPs [14, 6].", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "The ranges of possible \u03b4 and are of similar order than in other state-of-the-art lower bounds for multi-armed bandits [19] and discounted MDPs [14, 6].", "startOffset": 143, "endOffset": 150}, {"referenceID": 5, "context": "The ranges of possible \u03b4 and are of similar order than in other state-of-the-art lower bounds for multi-armed bandits [19] and discounted MDPs [14, 6].", "startOffset": 143, "endOffset": 150}, {"referenceID": 18, "context": "They are mostly determined by the bandit result by Mannor and Tsitsiklis [19] we build on.", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Similar MDPs that essentially solve multiple of such multi-armed bandits have been used to prove lower sample-complexity bounds for discounted MDPs [14, 6].", "startOffset": 148, "endOffset": 155}, {"referenceID": 5, "context": "Similar MDPs that essentially solve multiple of such multi-armed bandits have been used to prove lower sample-complexity bounds for discounted MDPs [14, 6].", "startOffset": 148, "endOffset": 155}, {"referenceID": 3, "context": "However, the analysis in the infinite horizon case as well as for the sliding-window fixed-horizon optimality criterion considered by Kakade [4] is significantly simpler.", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "[1] improves the state-dependency of these bounds for their delayed Q-learning algorithm to \u00d5 ( |S||A|H 4 ln 1 \u03b4 ) .", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Kolter and Ng [9] use an interesting sliding-window criterion, but prove bounds for a Bayesian setting instead of PAC.", "startOffset": 14, "endOffset": 17}, {"referenceID": 19, "context": "[20] yields a PAC-bound on the number of episodes of at least \u00d5 ( |S||A|H 2 ln 1 \u03b4 ) even if one ignores the reset after H time steps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Similar to us, Fiechter [10] uses the value of initial states as optimality-criterion, but defines the value w.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "Auer and Ortner [5] investigate the same setting as we and propose a UCB-type algorithm that has noregret, which translates into a basic PAC bound of order \u00d5 ( |S||A|H 3 ln 1 \u03b4 ) episodes.", "startOffset": 16, "endOffset": 19}, {"referenceID": 11, "context": "Reveliotis and Bountourelis [12] also consider the episodic undiscounted fixed-horizon setting and present an efficient algorithm in cases where the transition graph is acyclic and the agent knows for each state a policy that visits this state with a known minimum probability q.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "One might hope to reduce the dependency of the upper bound on |S| to be linear by an analysis similar to Mormax [7] for discounted MDPs which has sample complexity linear in |S| at the penalty of additional dependencies on H .", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "While the original bound stated by Kakade [4] only has H, an additional H comes in through \u22123 due to different normalization of rewards.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "References [1] Alexander L.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Michael J Kearns and Satinder P Singh.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Ronen I Brafman and Moshe Tennenholtz.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Sham M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Peter Auer and Ronald Ortner.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Tor Lattimore and Marcus Hutter.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Istv\u00e0n Szita and Csaba Szepesv\u00e1ri.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Mohammad Gheshlaghi Azar, R\u00e9mi Munos, and Hilbert J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] J Zico Kolter and Andrew Y Ng.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Claude-Nicolas Fiechter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Claude-Nicolas Fiechter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Spyros Reveliotis and Theologos Bountourelis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Alexander L Strehl, Lihong Li, and Michael L Littman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Alexander L Strehl, Lihong Li, and Michael L Littman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Thomas Jaksch, Ronald Ortner, and Peter Auer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Alexander L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Matthew J Sobel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Andreas Maurer and Massimiliano Pontil.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Shie Mannor and John N Tsitsiklis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Thomas Jaksch, Ronald Ortner, and Peter Auer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "We can find such a policy by dynamic programming similar to extended value iteration [16, 5].", "startOffset": 85, "endOffset": 92}, {"referenceID": 4, "context": "We can find such a policy by dynamic programming similar to extended value iteration [16, 5].", "startOffset": 85, "endOffset": 92}, {"referenceID": 0, "context": "The feasible set is defined as Ps,a := {p \u2208 [0, 1]|S(s,a)| | \u2016p\u20161 = 1,\u2200s\u2032 \u2208 S(s, a) : p(s\u2032) \u2208 conv(ConfidenceSet(p\u0302(s\u2032|s, a), n(s, a)))}.", "startOffset": 44, "endOffset": 50}, {"referenceID": 15, "context": "The inner max can be solved efficiently by enumeration and the outer maximum similar to extended value iteration [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "See Appendix A of Strehl and Littman [16] for details.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Using both inequalities of Theorem 10 by Maurer and Pontil [18]8, we have", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "The empirical variance denoted by Vn(X) by Maurer and Pontil [18] is p\u0303(s\u2032|s, a)(1 \u2212 p\u0303(s\u2032|s, a)) in our case and EVn is the true variance which amounts to p(s\u2032|s, a)(1\u2212 p(s\u2032|s, a)) for us.", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "Assume p, p\u0302, p\u0303 \u2208 [0, 1] satisfy p \u2208 P and p\u0303 \u2208 conv(P) where", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "P := { p\u2032 \u2208 [0, 1] :|p\u0302\u2212 p\u2032| \u2264 \u221a ln(6/\u03b41) 2n ,", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "We have P = P1 \u2229 P2 with P1 = { p\u2032 \u2208 [0, 1] :|p\u0302\u2212 p\u2032| \u2264 \u221a ln(6/\u03b41) 2n ,", "startOffset": 37, "endOffset": 43}, {"referenceID": 0, "context": "Note that the last condition of P1 is equivalent to \u221a p\u0302(1\u2212 p\u0302) \u2264 \u221a p\u2032(1\u2212 p\u2032) + \u221a 2 ln(6/\u03b41) n\u22121 as p\u2032 \u2208 [0, 1].", "startOffset": 105, "endOffset": 111}, {"referenceID": 18, "context": "Since allGi are independent of each other and \u2032 = 16 e (H \u2212 2)\u03b7 \u2264 16(H \u2212 2)e \u03b7 (H \u2212 2)64e4\u03b7 = 1 4 we can apply Theorem 1 by Mannor and Tsitsiklis [19] in cases where \u03b4i \u2264 1 \u03b7 (1\u2212 \u03c6+ \u03b4\u03c6) \u2264 1 \u03b7 (1\u2212 \u03c6+ \u03b4) \u2264 1 8e4 + \u03b4 \u03b7 \u2264 2 8e4 .", "startOffset": 146, "endOffset": 150}, {"referenceID": 0, "context": "\u03b4n\u2208[0,1] n \u2211", "startOffset": 3, "endOffset": 8}, {"referenceID": 0, "context": "with c \u2208 [0, 1] and \u03b7(1\u2212 ln c) \u2264 1 has optimal solution \u03b41 = \u00b7 \u00b7 \u00b7 = \u03b4n = c.", "startOffset": 9, "endOffset": 15}], "year": 2016, "abstractText": "Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound \u00d5( |S| |A|H 2 ln 1 \u03b4 ) and a lower PAC bound \u03a9\u0303( |S||A|H 2 ln 1 \u03b4+c ) that match up to log-terms and an additional linear dependency on the number of states |S|. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein\u2019s inequality to improve on previous bounds for episodic finitehorizon MDPs which have a time-horizon dependency of at least H.", "creator": "LaTeX with hyperref package"}}}