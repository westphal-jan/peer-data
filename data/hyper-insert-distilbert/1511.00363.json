{"id": "1511.00363", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Nov-2015", "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "abstract": "in deep neural networks ( dnn ) have achieved state - range of - high the - art estimate results enabled in a wide widening range of tasks, with notably the common best estimation results obtained performing with considerably large training sets and too large models. in finishing the past, gpus enabled these breakthroughs because of permitting their greater computational optimization speed. in starting the future, faster computation at maximum both training and test time is likely perceived to be increasingly crucial for further progress and progress for consumer targeted applications on low - power devices. as a result, there is surprisingly much interest in concurrent research and development of dedicated software hardware for deep learning ( dl ). binary weights, mb i. the e., weights at which are only constrained to integer only two known possible rounding values ( bytes e. g. - 1 or. 1 ), would practically bring great benefits to specialized for dl hardware vendors by replacing having many multiply - accumulate operations coupled by overcoming simple accumulations, known as discrete multipliers are effectively the most space friendly and simplest power - hungry solution components symbolic of demonstrating the basic digital implementation requirements of neural networks. formally we introduce binaryconnect, a computational method design which consists in training starting a dnn with tight binary implicit weights during the forward fading and backward propagations, therefore while retaining increasing precision operations of calculate the stored weights in which all gradients calculated are accumulated. quite like other bounded dropout approximation schemes, currently we rapidly show that binaryconnect only acts quickly as regularizer and afterwards we constantly obtain their near close state - of - mode the -, art hybrid results with fast binaryconnect on solving the permutation - mode invariant mnist, cifar - 10 + and svhn.", "histories": [["v1", "Mon, 2 Nov 2015 02:50:05 GMT  (1190kb,D)", "http://arxiv.org/abs/1511.00363v1", "Accepted at NIPS 2015, 9 pages, 3 figures"], ["v2", "Thu, 12 Nov 2015 23:31:09 GMT  (1190kb,D)", "http://arxiv.org/abs/1511.00363v2", "Accepted at NIPS 2015, 9 pages, 3 figures"], ["v3", "Mon, 18 Apr 2016 13:11:45 GMT  (1190kb,D)", "http://arxiv.org/abs/1511.00363v3", "Accepted at NIPS 2015, 9 pages, 3 figures"]], "COMMENTS": "Accepted at NIPS 2015, 9 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["matthieu courbariaux", "yoshua bengio", "jean-pierre david"], "accepted": true, "id": "1511.00363"}, "pdf": {"name": "1511.00363.pdf", "metadata": {"source": "CRF", "title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "authors": ["Matthieu Courbariaux", "Yoshua Bengio"], "emails": ["matthieu.courbariaux@polymtl.ca", "yoshua.bengio@gmail.com", "jean-pierre.david@polymtl.ca"], "sections": [{"heading": "1 Introduction", "text": "Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4]. More recently, deep learning is making important strides in natural language processing, especially statistical machine translation [5, 6, 7]. Interestingly, one of the key factors that enabled this major progress has been the advent of Graphics Processing Units (GPUs), with speed-ups on the order of 10 to 30-fold, starting with [8], and similar improvements with distributed training [9, 10]. Indeed, the ability to train larger models on more data has enabled the kind of breakthroughs observed in the last few years. Today, researchers and developers designing new deep learning algorithms and applications often find themselves limited by computational capability. This along, with the drive to put deep learning systems on low-power devices (unlike GPUs) is greatly increasing the interest in research and development of specialized hardware for deep networks [11, 12, 13].\nMost of the computation performed during training and application of deep networks regards the multiplication of a real-valued weight by a real-valued activation (in the recognition or forward propagation phase of the back-propagation algorithm) or gradient (in the backward propagation phase of the back-propagation algorithm). This paper proposes an approach called BinaryConnect\nar X\niv :1\n51 1.\n00 36\n3v 1\n[ cs\n.L G\n] 2\nto eliminate the need for these multiplications by forcing the weights used in these forward and backward propagations to be binary, i.e. constrained to only two values (not necessarily 0 and 1). We show that state-of-the-art results can be achieved with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.\nWhat makes this workable are two ingredients:\n1. Sufficient precision is necessary to accumulate and average a large number of stochastic gradients, but noisy weights (and we can view discretization into a small number of values as a form of noise, especially if we make this discretization stochastic) are quite compatible with Stochastic Gradient Descent (SGD), the main type of optimization algorithm for deep learning. SGD explores the space of parameters by making small and noisy steps and that noise is averaged out by the stochastic gradient contributions accumulated in each weight. Therefore, it is important to keep sufficient resolution for these accumulators, which at first sight suggests that high precision is absolutely required. [14] and [15] show that randomized or stochastic rounding can be used to provide unbiased discretization. [14] have shown that SGD requires weights with a precision of at least 6 to 8 bits and [16] successfully train DNNs with 12 bits dynamic fixed-point computation. Besides, the estimated precision of the brain synapses varies between 6 and 12 bits [17].\n2. Noisy weights actually provide a form of regularization which can help to generalize better, as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights. For instance, DropConnect [21], which is closest to BinaryConnect, is a very efficient regularizer that randomly substitutes half of the weights with zeros during propagations. What these previous works show is that only the expected value of the weight needs to have high precision, and that noise can actually be beneficial.\nThe main contributions of this article are the following.\n\u2022 We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations (Section 2). \u2022 We show that BinaryConnect is a regularizer and we obtain near state-of-the-art results on\nthe permutation-invariant MNIST, CIFAR-10 and SVHN (Section 3). \u2022 We make the code for BinaryConnect available 1."}, {"heading": "2 BinaryConnect", "text": "In this section we give a more detailed view of BinaryConnect, considering which two values to choose, how to discretize, how to train and how to perform inference."}, {"heading": "2.1 +1 or \u22121", "text": "Applying a DNN mainly consists in convolutions and matrix multiplications. The key arithmetic operation of DL is thus the multiply-accumulate operation. Artificial neurons are basically multiplyaccumulators computing weighted sums of their inputs.\nBinaryConnect constraints the weights to either +1 or \u22121 during propagations. As a result, many multiply-accumulate operations are replaced by simple additions (and subtractions). This is a huge gain, as fixed-point adders are much less expensive both in terms of area and energy than fixed-point multiply-accumulators [22]."}, {"heading": "2.2 Deterministic vs stochastic binarization", "text": "The binarization operation transforms the real-valued weights into the two possible values. A very straightforward binarization operation would be based on the sign function:\nwb = { +1 if w \u2265 0, \u22121 otherwise. (1)\n1https://github.com/MatthieuCourbariaux/BinaryConnect\nWhere wb is the binarized weight and w the real-valued weight. Although this is a deterministic operation, averaging this discretization over the many input weights of a hidden unit could compensate for the loss of information. An alternative that allows a finer and more correct averaging process to take place is to binarize stochastically:\nwb = { +1 with probability p = \u03c3(w), \u22121 with probability 1\u2212 p. (2)\nwhere \u03c3 is the \u201chard sigmoid\u201d function:\n\u03c3(x) = clip( x+ 1\n2 , 0, 1) = max(0,min(1,\nx+ 1\n2 )) (3)\nWe use such a hard sigmoid rather than the soft version because it is far less computationally expensive (both in software and specialized hardware implementations) and yielded excellent results in our experiments. It is similar to the \u201chard tanh\u201d non-linearity introduced by [23]. It is also piece-wise linear and corresponds to a bounded form of the rectifier [24]."}, {"heading": "2.3 Propagations vs updates", "text": "Let us consider the different steps of back-propagation with SGD udpates and whether it makes sense, or not, to discretize the weights, at each of these steps.\n1. Given the DNN input, compute the unit activations layer by layer, leading to the top layer which is the output of the DNN, given its input. This step is referred as the forward propagation.\n2. Given the DNN target, compute the training objective\u2019s gradient w.r.t. each layer\u2019s activations, starting from the top layer and going down layer by layer until the first hidden layer. This step is referred to as the backward propagation or backward phase of backpropagation.\n3. Compute the gradient w.r.t. each layer\u2019s parameters and then update the parameters using their computed gradients and their previous values. This step is referred to as the parameter update.\nAlgorithm 1 SGD training with BinaryConnect. C is the cost function for minibatch and the functions binarize(w) and clip(w) specify how to binarize and clip weights. L is the number of layers. Require: a minibatch of (inputs, targets), previous parameters wt\u22121 (weights) and bt\u22121 (biases),\nand learning rate \u03b7. Ensure: updated parameters wt and bt.\n1. Forward propagation: wb \u2190 binarize(wt\u22121) For k = 1 to L, compute ak knowing ak\u22121, wb and bt\u22121 2. Backward propagation: Initialize output layer\u2019s activations gradient \u2202C\u2202aL For k = L to 2, compute \u2202C\u2202ak\u22121 knowing \u2202C \u2202ak\nand wb 3. Parameter update: Compute \u2202C\u2202wb and \u2202C dbt\u22121\nknowing \u2202C\u2202ak and ak\u22121 wt \u2190 clip(wt\u22121 \u2212 \u03b7 \u2202C\u2202wb ) bt \u2190 bt\u22121 \u2212 \u03b7 \u2202C\u2202bt\u22121\nA key point to understand with BinaryConnect is that we only binarize the weights during the forward and backward propagations (steps 1 and 2) but not during the parameter update (step 3), as illustrated in Algorithm 1. Keeping good precision weights during the updates is necessary for SGD to work at all. These parameter changes are tiny by virtue of being obtained by gradient descent, i.e., SGD performs a large number of almost infinitesimal changes in the direction that most improves the training objective (plus noise). One way to picture all this is to hypothesize that what matters\nmost at the end of training is the sign of the weights, w\u2217, but that in order to figure it out, we perform a lot of small changes to a continuous-valued quantity w, and only at the end consider its sign:\nw\u2217 = sign( \u2211 t gt) (4)\nwhere gt is a noisy estimator of \u2202C(f(xt,wt\u22121,bt\u22121),yt)\n\u2202wt\u22121 , where C(f(xt, wt\u22121, bt\u22121), yt) is the value\nof the objective function on (input,target) example (xt, yt), when wt\u22121 are the previous weights and w\u2217 is its final discretized value of the weights.\nAnother way to conceive of this discretization is as a form of corruption, and hence as a regularizer, and our empirical results confirm this hypothesis. In addition, we can make the discretization errors on different weights approximately cancel each other while keeping a lot of precision by randomizing the discretization appropriately. We propose a form of randomized discretization that preserves the expected value of the discretized weight.\nHence, at training time, BinaryConnect randomly picks one of two values for each weight, for each minibatch, for both the forward and backward propagation phases of backprop. However, the SGD update is accumulated in a real-valued variable storing the parameter.\nAn interesting analogy to understand BinaryConnect is the DropConnect algorithm [21]. Just like BinaryConnect, DropConnect only injects noise to the weights during the propagations. Whereas DropConnect\u2019s noise is added Gaussian noise, BinaryConnect\u2019s noise is a binary sampling process. In both cases the corrupted value has as expected value the clean original value."}, {"heading": "2.4 Clipping", "text": "Since the binarization operation is not influenced by variations of the real-valued weights w when its magnitude is beyond the binary values\u00b11, and since it is a common practice to bound weights (usually the weight vector) in order to regularize them, we have chosen to clip the real-valued weights within the [\u22121, 1] interval right after the weight updates, as per Algorithm 1. The real-valued weights would otherwise grow very large without any impact on the binary weights."}, {"heading": "2.5 A few more tricks", "text": "We use Batch Normalization (BN) [26] in all of our experiments, not only because it accelerates the training by reducing internal covariate shift, but also because it reduces the overall impact of the weights scale. Moreover, we use the ADAM learning rule [27] in all of our CNN experiments. Last but not least, we scale the weights learning rates respectively with the weights initialization coefficients from [25] when optimizing with ADAM, and with the squares of those coefficients when optimizing with SGD or Nesterov momentum [28]. Table 1 illustrates the effectiveness of those tricks."}, {"heading": "2.6 Test-Time Inference", "text": "Up to now we have introduced different ways of training a DNN with on-the-fly weight binarization. What are reasonable ways of using such a trained network, i.e., performing test-time inference on new examples? We have considered three reasonable alternatives:\n1. Use the resulting binary weights wb (this makes most sense with the deterministic form of BinaryConnect).\n2. Use the real-valued weights w, i.e., the binarization only helps to achieve faster training but not faster test-time performance.\n3. In the stochastic case, many different networks can be sampled by sampling a wb for each weight according to Eq. 2. The ensemble output of these networks can then be obtained by averaging the outputs from individual networks.\nWe use the first method with the deterministic form of BinaryConnect. As for the stochastic form of BinaryConnect, we focused on the training advantage and used the second method in the experiments, i.e., test-time inference using the real-valued weights. This follows the practice of Dropout methods, where at test-time the \u201cnoise\u201d is removed."}, {"heading": "3 Benchmark results", "text": "In this section, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN."}, {"heading": "3.1 Permutation-invariant MNIST", "text": "MNIST is a benchmark image classification dataset [33]. It consists in a training set of 60000 and a test set of 10000 28 \u00d7 28 gray-scale images representing digits ranging from 0 to 9. Permutationinvariance means that the model must be unaware of the image (2-D) structure of the data (in other words, CNNs are forbidden). Besides, we do not use any data-augmentation, preprocessing or unsupervised pretraining. The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform better than Softmax on several classification benchmarks [30, 32]). The square hinge loss is minimized with SGD without momentum. We use an exponentially decaying learning rate. We use Batch Normalization with a minibatch of size 200 to speed up the training. As typically done, we use the\nlast 10000 samples of the training set as a validation set for early stopping and model selection. We report the test error rate associated with the best validation error rate after 1000 epochs (we do not retrain on the validation set). We repeat each experiment 6 times with different initializations. The results are in Table 2. They suggest that the stochastic version of BinaryConnect can be considered a regularizer, although a slightly less powerful one than Dropout, in this context."}, {"heading": "3.2 CIFAR-10", "text": "CIFAR-10 is a benchmark image classification dataset. It consists in a training set of 50000 and a test set of 10000 32 \u00d7 32 color images representing airplanes, automobiles, birds, cats, deers, dogs, frogs, horses, ships and trucks. We preprocess the data using global contrast normalization and ZCA whitening. We do not use any data-augmentation (which can really be a game changer for this dataset [35]). The architecture of our CNN is:\n(2\u00d7128C3)\u2212MP2\u2212(2\u00d7256C3)\u2212MP2\u2212(2\u00d7512C3)\u2212MP2\u2212(2\u00d71024FC)\u221210SVM (5)\nWhere C3 is a 3 \u00d7 3 ReLU convolution layer, MP2 is a 2 \u00d7 2 max-pooling layer, FC a fully connected layer, and SVM a L2-SVM output layer. This architecture is greatly inspired from VGG [36]. The square hinge loss is minimized with ADAM. We use an exponentially decaying learning rate. We use Batch Normalization with a minibatch of size 50 to speed up the training. We use the last 5000 samples of the training set as a validation set. We report the test error rate associated with\nthe best validation error rate after 500 training epochs (we do not retrain on the validation set). The results are in Table 2 and Figure 3."}, {"heading": "3.3 SVHN", "text": "SVHN is a benchmark image classification dataset. It consists in a training set of 604K and a test set of 26K 32\u00d7 32 color images representing digits ranging from 0 to 9. We follow the same procedure that we used for CIFAR-10, with a few notable exceptions: we use half the number of hidden units and we train for 200 epochs instead of 500 (because SVHN is quite a big dataset). The results are in Table 2."}, {"heading": "4 Related works", "text": "Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40]. Even though we share the same objective, our approaches are quite different. [37, 38] do not train their DNN with Backpropagation (BP) but with a variant called Expectation Backpropagation (EBP). EBP is based on Expectation Propagation (EP) [41], which is a variational Bayes method used to do inference in probabilistic graphical models. Let us compare their method to ours:\n\u2022 It optimizes the weights posterior distribution (which is not binary). In this regard, our method is quite similar as we keep a real-valued version of the weights.\n\u2022 It binarizes both the neurons outputs and weights, which is more hardware friendly than just binarizing the weights.\n\u2022 It yields a good classification accuracy for fully connected networks (on MNIST) but not (yet) for ConvNets.\n[39, 40] retrain neural networks with ternary weights during forward and backward propagations, i.e.:\n\u2022 They train a neural network with high-precision,\n\u2022 After training, they ternarize the weights to three possible values\u2212H , 0 and +H and adjust H to minimize the output error,\n\u2022 And eventually, they retrain with ternary weights during propagations and high-precision weights during updates.\nBy comparison, we train all the way with binary weights during propagations, i.e., our training procedure could be implemented with efficient specialized hardware avoiding the forward and backward propagations multiplications, which amounts to about 2/3 of the multiplications (cf. Algorithm 1)."}, {"heading": "5 Conclusion and future works", "text": "We have introduced a novel binarization scheme for weights during forward and backward propagations called BinaryConnect. We have shown that it is possible to train DNNs with BinaryConnect on the permutation invariant MNIST, CIFAR-10 and SVHN datasets and achieve nearly state-of-the-art results. The impact of such a method on specialized hardware implementations of deep networks could be major, by removing the need for about 2/3 of the multiplications, and thus potentially allowing to speed-up by a factor of 3 at training time. With the deterministic version of BinaryConnect the impact at test time could be even more important, getting rid of the multiplications altogether and reducing by a factor of at least 16 (from 16 bits single-float precision to single bit precision) the memory requirement of deep networks, which has an impact on the memory to computation bandwidth and on the size of the models that can be run. Future works should extend those results to other models and datasets, and explore getting rid of the multiplications altogether during training, by removing their need from the weight update computation."}, {"heading": "6 Acknowledgments", "text": "We thank the reviewers for their many constructive comments. We also thank Roland Memisevic for helpful discussions. We thank the developers of Theano [42, 43], a Python library which allowed us to easily develop a fast and optimized code for GPU. We also thank the developers of Pylearn2 [44] and Lasagne, two Deep Learning libraries built on the top of Theano. We are also grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Geoffrey Hinton", "Li Deng", "George E. Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["Tara Sainath", "Abdel rahman Mohamed", "Brian Kingsbury", "Bhuvana Ramabhadran"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS\u20192012", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "Technical report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "In Proc. ACL\u20192014,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS\u20192014,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR\u20192015,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Large-scale deep unsupervised learning using graphics processors", "author": ["Rajat Raina", "Anand Madhavan", "Andrew Y. Ng"], "venue": "In ICML\u20192009,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G.S Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M.A. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": "In NIPS\u20192012,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "A highly scalable restricted Boltzmann machine FPGA implementation", "author": ["Sang Kyun Kim", "Lawrence C McAfee", "Peter Leonard McMahon", "Kunle Olukotun"], "venue": "In Field Programmable Logic and Applications,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning", "author": ["Tianshi Chen", "Zidong Du", "Ninghui Sun", "Jia Wang", "Chengyong Wu", "Yunji Chen", "Olivier Temam"], "venue": "In Proceedings of the 19th international conference on Architectural support for programming languages and operating systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun"], "venue": "In Microarchitecture (MICRO),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Rounding methods for neural networks with low resolution synaptic weights", "author": ["Lorenz K Muller", "Giacomo Indiveri"], "venue": "arXiv preprint arXiv:1504.05767,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "In ICML\u20192015,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Low precision arithmetic for deep learning", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Arxiv:1412.7024,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Hippocampal spine head sizes are highly precise", "author": ["Thomas M Bartol", "Cailey Bromer", "Justin P Kinney", "Michael A Chirillo", "Jennifer N Bourne", "Kristen M Harris", "Terrence J Sejnowski"], "venue": "bioRxiv,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Improving neural networks with dropout", "author": ["Nitish Srivastava"], "venue": "Master\u2019s thesis, U. Toronto,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann LeCun", "Rob Fergus"], "venue": "In ICML\u20192013,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Hardware complexity of modular multiplication and exponentiation", "author": ["J.P. David", "K. Kalach", "N. Tittley"], "venue": "Computers, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Large Scale Machine Learning", "author": ["R. Collobert"], "venue": "PhD thesis, Universite\u0301 de Paris VI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In AISTATS\u20192011,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS\u20192010,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/k)", "author": ["Yu Nesterov"], "venue": "Doklady AN SSSR (translated as Soviet. Math. Docl.),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1983}, {"title": "Deep learning using linear support vector machines", "author": ["Yichuan Tang"], "venue": "Workshop on Challenges in Representation Learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1998}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In ICML\u20192010,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Spatially-sparse convolutional neural networks", "author": ["Benjamin Graham"], "venue": "arXiv preprint arXiv:1409.6070,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Daniel Soudry", "Itay Hubara", "Ron Meir"], "venue": "In NIPS\u20192014,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Training binary multilayer neural networks for image classification using expectation backpropgation", "author": ["Zhiyong Cheng", "Daniel Soudry", "Zexi Mao", "Zhenzhong Lan"], "venue": "arXiv preprint arXiv:1503.03562,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Fixed-point feedforward deep neural network design using weights+", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Signal Processing Systems (SiPS),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "X1000 real-time phoneme recognition vlsi using feed-forward deep neural networks", "author": ["Jonghong Kim", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Expectation propagation for approximate bayesian inference", "author": ["Thomas P Minka"], "venue": "In UAI\u20192001,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2001}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Pylearn2: a machine learning research", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4].", "startOffset": 133, "endOffset": 139}, {"referenceID": 1, "context": "Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4].", "startOffset": 133, "endOffset": 139}, {"referenceID": 2, "context": "Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4].", "startOffset": 200, "endOffset": 206}, {"referenceID": 3, "context": "Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks, especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4].", "startOffset": 200, "endOffset": 206}, {"referenceID": 4, "context": "More recently, deep learning is making important strides in natural language processing, especially statistical machine translation [5, 6, 7].", "startOffset": 132, "endOffset": 141}, {"referenceID": 5, "context": "More recently, deep learning is making important strides in natural language processing, especially statistical machine translation [5, 6, 7].", "startOffset": 132, "endOffset": 141}, {"referenceID": 6, "context": "More recently, deep learning is making important strides in natural language processing, especially statistical machine translation [5, 6, 7].", "startOffset": 132, "endOffset": 141}, {"referenceID": 7, "context": "Interestingly, one of the key factors that enabled this major progress has been the advent of Graphics Processing Units (GPUs), with speed-ups on the order of 10 to 30-fold, starting with [8], and similar improvements with distributed training [9, 10].", "startOffset": 188, "endOffset": 191}, {"referenceID": 8, "context": "Interestingly, one of the key factors that enabled this major progress has been the advent of Graphics Processing Units (GPUs), with speed-ups on the order of 10 to 30-fold, starting with [8], and similar improvements with distributed training [9, 10].", "startOffset": 244, "endOffset": 251}, {"referenceID": 9, "context": "Interestingly, one of the key factors that enabled this major progress has been the advent of Graphics Processing Units (GPUs), with speed-ups on the order of 10 to 30-fold, starting with [8], and similar improvements with distributed training [9, 10].", "startOffset": 244, "endOffset": 251}, {"referenceID": 10, "context": "This along, with the drive to put deep learning systems on low-power devices (unlike GPUs) is greatly increasing the interest in research and development of specialized hardware for deep networks [11, 12, 13].", "startOffset": 196, "endOffset": 208}, {"referenceID": 11, "context": "This along, with the drive to put deep learning systems on low-power devices (unlike GPUs) is greatly increasing the interest in research and development of specialized hardware for deep networks [11, 12, 13].", "startOffset": 196, "endOffset": 208}, {"referenceID": 12, "context": "This along, with the drive to put deep learning systems on low-power devices (unlike GPUs) is greatly increasing the interest in research and development of specialized hardware for deep networks [11, 12, 13].", "startOffset": 196, "endOffset": 208}, {"referenceID": 13, "context": "[14] and [15] show that randomized or stochastic rounding can be used to provide unbiased discretization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[14] and [15] show that randomized or stochastic rounding can be used to provide unbiased discretization.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "[14] have shown that SGD requires weights with a precision of at least 6 to 8 bits and [16] successfully train DNNs with 12 bits dynamic fixed-point computation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[14] have shown that SGD requires weights with a precision of at least 6 to 8 bits and [16] successfully train DNNs with 12 bits dynamic fixed-point computation.", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "Besides, the estimated precision of the brain synapses varies between 6 and 12 bits [17].", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "Noisy weights actually provide a form of regularization which can help to generalize better, as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights.", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "Noisy weights actually provide a form of regularization which can help to generalize better, as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights.", "startOffset": 157, "endOffset": 165}, {"referenceID": 19, "context": "Noisy weights actually provide a form of regularization which can help to generalize better, as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights.", "startOffset": 157, "endOffset": 165}, {"referenceID": 20, "context": "Noisy weights actually provide a form of regularization which can help to generalize better, as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights.", "startOffset": 182, "endOffset": 186}, {"referenceID": 20, "context": "For instance, DropConnect [21], which is closest to BinaryConnect, is a very efficient regularizer that randomly substitutes half of the weights with zeros during propagations.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "This is a huge gain, as fixed-point adders are much less expensive both in terms of area and energy than fixed-point multiply-accumulators [22].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "It is similar to the \u201chard tanh\u201d non-linearity introduced by [23].", "startOffset": 61, "endOffset": 65}, {"referenceID": 23, "context": "It is also piece-wise linear and corresponds to a bounded form of the rectifier [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "An interesting analogy to understand BinaryConnect is the DropConnect algorithm [21].", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "Table 1: Test error rates of a (small) CNN trained on CIFAR-10 depending on optimization method and on whether the learning rate is scaled with the weights initialization coefficients from [25].", "startOffset": 189, "endOffset": 193}, {"referenceID": 25, "context": "We use Batch Normalization (BN) [26] in all of our experiments, not only because it accelerates the training by reducing internal covariate shift, but also because it reduces the overall impact of the weights scale.", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "Moreover, we use the ADAM learning rule [27] in all of our CNN experiments.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "Last but not least, we scale the weights learning rates respectively with the weights initialization coefficients from [25] when optimizing with ADAM, and with the squares of those coefficients when optimizing with SGD or Nesterov momentum [28].", "startOffset": 119, "endOffset": 123}, {"referenceID": 27, "context": "Last but not least, we scale the weights learning rates respectively with the weights initialization coefficients from [25] when optimizing with ADAM, and with the squares of those coefficients when optimizing with SGD or Nesterov momentum [28].", "startOffset": 240, "endOffset": 244}, {"referenceID": 28, "context": "47% Deep L2-SVM [30] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "35% DropConnect [21] 1.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "MNIST is a benchmark image classification dataset [33].", "startOffset": 50, "endOffset": 54}, {"referenceID": 30, "context": "The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform better than Softmax on several classification benchmarks [30, 32]).", "startOffset": 92, "endOffset": 103}, {"referenceID": 23, "context": "The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform better than Softmax on several classification benchmarks [30, 32]).", "startOffset": 92, "endOffset": 103}, {"referenceID": 2, "context": "The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform better than Softmax on several classification benchmarks [30, 32]).", "startOffset": 92, "endOffset": 103}, {"referenceID": 28, "context": "The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform better than Softmax on several classification benchmarks [30, 32]).", "startOffset": 221, "endOffset": 229}, {"referenceID": 31, "context": "We do not use any data-augmentation (which can really be a game changer for this dataset [35]).", "startOffset": 89, "endOffset": 93}, {"referenceID": 32, "context": "This architecture is greatly inspired from VGG [36].", "startOffset": 47, "endOffset": 51}, {"referenceID": 33, "context": "Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40].", "startOffset": 76, "endOffset": 92}, {"referenceID": 34, "context": "Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40].", "startOffset": 76, "endOffset": 92}, {"referenceID": 35, "context": "Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40].", "startOffset": 76, "endOffset": 92}, {"referenceID": 36, "context": "Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40].", "startOffset": 76, "endOffset": 92}, {"referenceID": 33, "context": "[37, 38] do not train their DNN with Backpropagation (BP) but with a variant called Expectation Backpropagation (EBP).", "startOffset": 0, "endOffset": 8}, {"referenceID": 34, "context": "[37, 38] do not train their DNN with Backpropagation (BP) but with a variant called Expectation Backpropagation (EBP).", "startOffset": 0, "endOffset": 8}, {"referenceID": 37, "context": "EBP is based on Expectation Propagation (EP) [41], which is a variational Bayes method used to do inference in probabilistic graphical models.", "startOffset": 45, "endOffset": 49}, {"referenceID": 35, "context": "[39, 40] retrain neural networks with ternary weights during forward and backward propagations, i.", "startOffset": 0, "endOffset": 8}, {"referenceID": 36, "context": "[39, 40] retrain neural networks with ternary weights during forward and backward propagations, i.", "startOffset": 0, "endOffset": 8}, {"referenceID": 38, "context": "We thank the developers of Theano [42, 43], a Python library which allowed us to easily develop a fast and optimized code for GPU.", "startOffset": 34, "endOffset": 42}, {"referenceID": 39, "context": "We thank the developers of Theano [42, 43], a Python library which allowed us to easily develop a fast and optimized code for GPU.", "startOffset": 34, "endOffset": 42}, {"referenceID": 40, "context": "We also thank the developers of Pylearn2 [44] and Lasagne, two Deep Learning libraries built on the top of Theano.", "startOffset": 41, "endOffset": 45}], "year": 2016, "abstractText": "Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.", "creator": "LaTeX with hyperref package"}}}