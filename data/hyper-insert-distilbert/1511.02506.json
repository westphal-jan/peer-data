{"id": "1511.02506", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Nov-2015", "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition", "abstract": "similarly in this paper work we propose the systematic structured deep neural network ( structured dnn ) as specifically a structured synthetic and logical deep learning framework. this approach can actively learn to find the best structured object ( such composed as a label dynamic sequence ) which given a correctly structured software input ( such consisting as a vector peptide sequence ) component by globally considering for the mapping relationships between the xml structures assigned rather than item by ranking item.", "histories": [["v1", "Sun, 8 Nov 2015 17:08:54 GMT  (385kb,D)", "http://arxiv.org/abs/1511.02506v1", "arXiv admin note: text overlap witharXiv:1506.01163"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1506.01163", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["yi-hsiu liao", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1511.02506"}, "pdf": {"name": "1511.02506.pdf", "metadata": {"source": "CRF", "title": "TOWARDS STRUCTURED DEEP NEURAL NETWORK FOR AUTOMATIC SPEECH RECOGNITION", "authors": ["Yi-Hsiu Liao", "Hung-yi Lee", "Lin-shan Lee"], "emails": ["r03921048@ntu.edu.tw,", "hungyilee@ntu.edu.tw,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": null, "text": "Index Terms\u2014 structured learning, deep neural network\n1. INTRODUCTION\nWith the maturity of machine learning, great efforts have been made to try to integrate more machine learning concepts into the Hidden Markov Model (HMM). Using Deep Neural Networks (DNN) [1, 2] with HMM is a good example [3\u20135]. In general, HMMs consider the phoneme structure by states and the transitions among them, but trained primarily on frame level regardless of being based on DNN [6, 7] or Gaussian Mixture Model (or subspace GMM, SGMM [8]). Under HMM framework [9], the hierarchical structure of an utterance is taken care of by the HMM and their states, the lexicon and the language model, which are respectively learned separetely from disjoint sets of knowledge sources. On the other hand, it is well known that there may exist some underlying overall structures for the utterances behind the signals which may be helpful to recognition. If we can learn such structures comprehensively from the signals of the entire utterance globally, the recognition scenario may be different.\nOn the contrary, structured learning has been substantially investigated in machine learning, which tries to learn\nthe complicated structures exhibited by the data. Conditional Random Fields (CRF) [10\u201315] and structured Support Vector Machine (SVM) [16\u201318] are good example approaches. Recently, structured SVM has been used to perform initial phoneme recognition by learning the relationships between the acoustic vector sequence and the phoneme label sequence of the whole utterance jointly rather than on the frame level or from different sets of knowledge sources [19], utilizing the nice properties of SVM [20] to classify the structured patterns of utterances with maximized margin. However, both CRF and structured SVM are linear, therefore limited in analyzing speech signals.\nIn this paper, we extend the above structured SVM approach to phoneme recognition using a structured DNN including nonlinear units in multi-layers, but similarly learning the global mapping relationships from an acoustic vector sequence to a phoneme label sequence for a whole utterance. In recent work, the front-end feature extraction DNN has been integrated with SVM [21] and Weighted Finite-State Transducers (WFST) [22], but here we further integrate the frontend DNN with structured DNN, which is completely different from the previous work.\n2. PROPOSED APPROACH \u2013 STRUCTURED DEEP NEURAL NETWORK\nThe whole picture of the concept of the structured DNN for phoneme recognition is in Fig. 1. Given an utterance with an acoustic vector sequence x and a corresponding phoneme label sequence y, we can first obtain a structured feature vector \u03a8(x,y) representing x and y and the relationships between them as in Fig. 1(a) (details of \u03a8(x,y) are given in Section 3), and then feed it into either an SVM as in Fig. 1(b) or a DNN as in Fig. 1(c) to get a score by a scoring function F1(x,y; \u03b81) or F2(x,y; \u03b82), where \u03b81 and \u03b82 are the parameter sets for the SVM and DNN respectively. Here the acoustic vector sequence x can be raw acoustic features like filter bank outputs or phoneme posteriorgram vectors generated from the DNN in Fig. 1(a). Because both x and y represent the entire utterance by a structure (sequence), and either SVM or DNN learns to map the pair of (x,y) to a score on the utterance level globally rather than on the frame level, this is structured learning optimized on the utterance level. ar X\niv :1\n51 1.\n02 50\n6v 1\n[ cs\n.C L\n] 8\nN ov\n2 01\n5"}, {"heading": "2.1. Structured Learning Concept", "text": "In structured learning, both the desired outputs y and the input objects x can be sequences, trees, lattices, or graphs, rather than simply classes or real numbers. In the context of supervised learning for phoneme recognition for utterances, we are given a set of training utterances, (x1,y1), ..., (xN ,yN ) \u2208 X \u00d7 Y, where xi is the acoustic vector sequence of the ith utterance, yi the corresponding reference phoneme label sequence, and we wish to assign correct phoneme label sequences to unknown utterance.\nWe first define a function f(x; \u03b8) = y : X \u2192 Y , mapping each acoustic vector sequence x to a phoneme label sequence y, where \u03b8 is the parameter set be learned. One way to achieve this is to assign every possible phoneme label sequence y given an acoustic vector sequence x a score by a scoring function F (x,y; \u03b8) : X \u00d7 Y \u2192 R, and take the phoneme label sequence y giving the highest score as the output of f(x; \u03b8),\nf(x; \u03b8) = arg max y\u2208Y F (x,y; \u03b8). (1)"}, {"heading": "2.2. Structured SVM", "text": "Based on the maximized margin concept of SVM, we wish to maximize not only the score of the correct label sequence, but the margin between the score of the correct label sequence and those of the nearest incorrect label sequences as shown in Figure 2. In Figure 2, yi is the correct label sequence, and all the other incorrect label sequences are in blue. The score of the correct label sequence F1(x,yi; \u03b81) is higher than the highest score among the incorrect label sequences, F1(x,yk; \u03b81), by \u2206(yi,yk), which is the difference between the scores of the two sequences yi and yk. All incorrect label sequences have scores below that of the correct sequence by at least \u2206(yi,yk), or the margin. Maximizing this margin is the learning target of structured SVM. The scoring function used in structured SVM is as below, which is linear.\nF1(x,y; \u03b81) = \u3008\u03b81,\u03a8(x,y)\u3009 , (2)\nwhere \u03a8(x,y) is the structured feature vector mentioned above and shown in Figure 1, representing the structured relationship between x and y, \u03b81 is in vector form, and \u3008\u00b7, \u00b7\u3009 represents inner product. We can then train the parameter vector \u03b81 using training instances {(xi,yi), i = 1, 2, ...,N} subject to the following formula,\nmin \u03b81,\u03bei \u2016\u03b81\u20162 + C N\u2211 i=1 \u03bei, \u03bei > 0, \u2200y \u2208 Y,\nF1(xi,yi; \u03b81)\u2212 F1(xi,y; \u03b81) + \u03bei \u2265 \u2206(yi,y), (3)\nwhere C is the cost balancing the model complexity (\u03b81) with the inequality, and \u03bei is the slack variable for the inequality, and \u2206(\u00b7, \u00b7) : Y \u00d7 Y \u2192 R+ measures the distance between two label sequences. Phone error rate is used as the distance in this paper, but other evaluation metrics are also feasible. Optimizing the function above is equivalent to maximizing the margin separating the scores between the correct label sequence and all other label sequences for each training sample xi. Formula (3) can be solved by quadratic programming and the cutting-plane algorithm [23], and is equivalent to the following formula:\nmin \u03b81 \u2016\u03b81\u20162 + C N\u2211 i=1 Li(\u03b81),\nLi(\u03b81) = max y max(0, F1(xi,y; \u03b81) + \u2206(yi,y)\n\u2212 F1(xi,yi; \u03b81)), (4)\nIn (4), Li(\u03b81) is the loss function for each example (xi,yi), and max(0, \u00b7) (the inner max operator) in Li(\u03b81) is a hinge loss function, which penalizes the model if the inequality in (3) does not hold. Formula (4) is helpful in understanding the concept of the cost function for structured DNN in Subsection 2.3.2. With the scoring function F1(x,y; \u03b81) and the trained parameter set \u03b81, we can find the label sequence y for the acoustic vector sequence x of any input testing utterance by the well known Viterbi algorithm [23]."}, {"heading": "2.3. Structured Deep Neural Network (Structured DNN)", "text": "The assumption of the linear scoring function as in (2) makes structured SVM limited. Instead, the proposed structured DNN uses a series of nonlinear transforms to build the scoring function F2(x,y; \u03b82) with L hidden layers to evaluate a single output value F2(x,y; \u03b82) as in Fig. 1(c).\nh1 = \u03c3(W0 \u00b7\u03a8(x,y)) hl = \u03c3(Wl\u22121 \u00b7 hl\u22121), 2 \u2264 l \u2264 L\nF2(x,y; \u03b82) = \u03c3(WL \u00b7 hL), (5)\nwhereWi is weight matrix (including the bias) of layer i, \u03c3(\u00b7) a nonlinear transform (sigmoid is used here), hi the output vector of hidden layer i, and the set of all DNN parameters (W0, W1, W2,..., WL) is \u03b82. Note that the last weight matrix WL is a vector, because this DNN gives only a single value as the output. Two different lost functions for learning structured DNN are defined in this work and described respectively in Subsections 2.3.1 and 2.3.2."}, {"heading": "2.3.1. Approximating phoneme accuracy (Approx. Ph. Acc)", "text": "First, the label phoneme accuracy for a label sequence y is defined as C(yi,y) = 1 \u2212\u2206(yi,y), where yi is the correct label sequence, and \u2206(yi,y) is the phoneme error rate of y given the correct label sequence yi. The parameter set \u03b82 of structured DNN can be trained by minimizing the following loss function,\nL(\u03b82) = N\u2211 i=1 \u2211 y\u2208Y [ C(yi,y)\u2212 F2(xi,y; \u03b82) ]2 . (6)\nBy minimizing (6), the DNN learns to minimize the mean square error between its output F2(xi,y; \u03b82) given xi and y and the phoneme accuracy of y, C(yi,y), over all training utterances and for each utterance all possible phoneme sequences y \u2208 Y. In other words, the score function F2(x,y; \u03b82) thus learned can be considered as an estimate of the phoneme accuracy, so the correct label sequence would tend to have the largest F2(x,y; \u03b82) among all possible sequences. In practice, considering all possible y is intractable, so only a subset of Y is considered during training, which will be described later in Subsection 2.5."}, {"heading": "2.3.2. Maximizing the margin (Max. Margin)", "text": "Inspired by the maximum margin concept of structured SVM, we replace the linear part of structured SVM by nonlinear DNN to take advantage of both DNN and maximum margin. The proposed structured DNN thus optimizes the following formula:\nmin \u03b82 \u2016\u03b82\u20162 + C N\u2211 i=1 L\u2032i(\u03b82),\nL\u2032i(\u03b82) = \u2211 y max(0, F2(xi,y; \u03b82) + \u2206(yi,y)\n\u2212 F2(xi,yi; \u03b82)) (7)\nL\u2032i(\u03b82) in (7) is parallel to Li(\u03b81) in (4), except that \u03b81 and F1(.) in (4) are replaced by \u03b82 and F2(.) in (7) respectively, while the outer max operator in (4) is replaced by summation1. Because the loss function L\u2032i(\u03b82) would be larger than zero whenever F2(xi,y; \u03b82)+\u2206(yi,y)\u2212F2(xi,yi; \u03b82) > 0, the DNN model parameters \u03b82 are penalized if any of the inequalities below do not hold.\nF2(xi,yi; \u03b82)\u2212 F2(xi,y; \u03b82) > \u2206(yi,y), i = 1, ...., N,\u2200y \u2208 Y. (8)\nTherefore, by (7), the scores between the correct label sequence and other label sequences would be separated by at least a margin which is maximized as in structured SVM, but here the scores are evaluated from a DNN with parameters learned based on the DNN framework, rather than from SVM. Note that all components in loss function L\u2032i(\u03b82) in (7) are piecewise differentiable which means we can use back propagation to find the model parameters \u03b82 when optimizing (7). According to (7), we need to traverse over all possible y for an utterance which is intractable and need some approximation as described in Subsection 2.5."}, {"heading": "2.4. Inference with Structured DNN", "text": "With the structured DNN trained as above, given the acoustic vector sequence x of an unknown utterance, we need to find the best phoneme label sequence y for it. For structured SVM in Subsection 2.2, due to the linear assumption, the learned model parameter \u03b81 contains enough information to execute the Viterbi algorithm to find the best label sequence. This is not true for structured DNN. From (1), in principle we need to search over all possible phoneme label sequences (KM for K phonemes and M acoustic vectors) for the given acoustic vector sequence and pick the one giving the highest score, which is computationally infeasible.\nInstead of searching through all possible phoneme label sequences, we first decode x using WFST to generate a lattice, and then search through the phoneme label sequences in\n1We replace the outer max operator in (4) of structured SVM with summation. In this way all the label sequences y are properly considered and this makes the DNN training more efficient.\nthe lattice which give the highest scores. Obviously, in this way the performance is bounded by the quality of the lattice."}, {"heading": "2.5. Training of Structured DNN", "text": "For each training utterance, again we haveKM possible label sequences. It is also impossible to train over all these label sequences for the training utterances. In structured SVM, due to the linear property, we are able to find training examples to produce the maximum margin. For structured DNN here, how to find and choose effective training examples is important. Besides the positive examples (reference phoneme label sequences for the training utterances), in this work negative examples (those other than reference label sequences) are chosen both by random and from the lattice decoded from WFST. For each training utterance with a lattice, the negative examples have three sources: (a) N completely random sequences, (b) N random paths on the lattice, and (c) the N-best paths on the lattice."}, {"heading": "2.6. Full-scale structured DNN (FSDNN)", "text": "The acoustic feature sequence x here can be the output of another DNN in the front-end (the DNN in Fig. 1(a)). For example, it can be generated from a DNN whose input is the filter bank output, and the output is the phoneme posteriorgram vectors. In this case, during back propagation, we can further propagate the errors of the structured DNN (the DNN in Fig. 1(c)) all the way back into the front-end DNN. In this way, we have Full-scale Structured DNN (FSDNN) in which all parameters from filter bank up to the whole utterance score are jointly learned.\nThe FSDNN we proposed can be considered as a special case (or structured version) of Convolutional Neural Network [24] [25] which works perfectly in computer vision and speech recognition [26]. The power of CNN is mainly based on shared kernel parameters which are able to discover frontend feature filters. In FSDNN, we can view the front-end DNN as the kernel in CNN because they all share the parameters in the front end. The difference between CNN and FSDNN is that CNN uses max-pooling layer, while we use \u03a8(x,y) to forward the output of the front-end DNN.\n3. STRUCTURED FEATURE VECTOR \u03a8(X,Y) FOR AN UTTERANCE\nTake the filter bank outputs or phoneme posteriorgram vectors as the acoustic vectors for an utterance of M frames, x = {xj , j = 1, 2, ...M}, and the phoneme label for xj is yj . So the task is to decode x into the label sequence y = {yj , j = 1, 2, ...M}. Since the most successful and well known solution to this problem is with HMM, we try to encode what HMM has been doing into the feature vector \u03a8(x,y) to be used here. An HMM consists of a series of\nstates, and two most important sets of parameters \u2013 the transition probabilities between states, and the observation probability distribution for each state. Such a structure is slightly complicated for the work here, so in the preliminary work we use a simplified HMM with only one state for each phoneme. With this simplification, these two sets of probabilistic parameters can be estimated for each utterance by adding up all the counts of the transition between labels (or states) and also adding up all the acoustic vectors for each label (phoneme or state). This is shown in Fig. 3(a).\nAssume K is the total number of different phonemes, we first define a K dimensional vector \u039b(yj) for yj with its k-th component being 1 and all other components being 0 if yj is the k-th phoneme. Tensor product \u2297 is helpful here, which is defined as\n\u2297 : RP \u00d7 RQ \u2192 RPQ, (a\u2297 b)i+(j\u22121)P \u2261 ai \u00d7 bj , (9)\nwhere a and b are two ordinary vectors with dimensions P andQ respectively. The right half of (9) says a\u2297b is a vector of dimension PQ, whose [i+ (j \u2212 1)P ]-th component is the i-th component of a multiplied by the j-th component of b. With this expression, the feature vector \u03a8(x,y) in Fig. 1(a) to be used for evaluating the scoring function F1(x,y; \u03b81) in (2) or F2(x,y; \u03b82) in (5) can then be configured as the concatenation of two vectors,\n\u03a8(x,y) =\n( \u2211M j=1 x\nj \u2297 \u039b(yj)\u2211M\u22121 j=1 \u039b(y j)\u2297 \u039b(yj+1)\n) , (10)\nwhere x = {x1,x2, ...,xM} and y = {y1, y2, ..., yM}. The upper half of the right hand side of (10) is to accumulate the distribution of all components of xj for each phoneme in the acoustic vector sequence x, and then locate them at different sections of components of the feature vector \u03a8(x,y) (corresponding to the observation probability distribution for each state or phoneme label estimated with the utterance). The lower half of the right hand side of (10), on the other hand, is to accumulate the transition counts between each pair of labels (phonemes or states) in the label sequence y (corresponding to state transition probabilities estimated for the utterance). Then, \u03a8(x,y) is the concatenation of the two, so it keeps the primary statistical parameters of xj for different phonemes yj for all xj in x, and the transitions between states for all yj in y. With enough training utterances (x,y) and the corresponding function \u03a8(x,y), we can then learn the scoring function F1(x,y; \u03b8) or F2(x,y; \u03b82) by training the parameters \u03b81 or \u03b82. The vector \u03a8(x,y) in (10) can be easily extended to higher order Markov assumptions (transition to the next state depending on more than one previous states). For example, by replacing the upper half of (10) with \u2211N n=1 x\nn \u2297 \u039b(yn)\u2297 \u039b(yn+1) and the lower half of (10) with \u2211N\u22121 n=1 \u039b(y\nn)\u2297 \u039b(yn+1)\u2297 \u039b(yn+2) , we have the second order Markov assumption.\nConsider a simplified example for K = 3 (only 3 allowed phonemes A, B, C) and an utterance with length M = 4 as shown in Fig. 3(b). It is then easy to find that the upper half of \u03a8(x,y) is \u22114 n=1 x\nn \u2297 \u039b(yn) = (1.2, 2.6, 2.7, 2.3, 1.5, 2.5)\n\u2032, and the lower half of \u03a8(x,y) is \u22113 n=1 \u039b(y\nn)\u2297 \u039b(yn+1) = (0, 0, 0, 1, 1, 0, 0, 1, 0)\u2032. We therefore have \u03a8(x,y) = (1.2, 2.6, 2.7, 2.3, 1.5, 2.5, 0, 0, 0, 1, 1, 0, 0, 1, 0) \u2032.\n4. EXPERIMENTAL SETUP\nInitial experiments were performed with TIMIT. We used the training set without dialect sentences for training and the core testing set (with 24 speakers and no dialect) for testing. The models were trained with a set of 48 phonemes and tested with a set of 39 phonemes, conformed to CMU/MIT standards [27]. We used an online library [28] for structured SVM, and modified the kaldi [29] code to implement structured DNN.\nOur experiment was based on Vesely\u2019s recipe in kaldi, called as baseline, which used LDA-MLLT-fMLLR features obtained from auxiliary GMM models, RBM pre-training, frame cross-entropy training and sMBR. The structured DNN was performed on top of the lattices obtained by Vesely\u2019s recipe. We used two sets of acoustic vectors, (a)LDA-MLLTfMLLR feature (40 dimensions), or input to DNN in Vesely\u2019s recipe; (b)phoneme posterior probabilities (48 dimensions) obtained from the 1943 DNN output (state posterior) from Vesely\u2019s recipe. Because 1943-dimension feature was too large for \u03a8(x,y), we reduced the dimension to 48(monophone size) by adding an extra layer(1943 \u00d7 48) to Vesely\u2019s DNN, and used one-hot mono-phone as training target to train this extra layer.\nUnless specified, we used the following parameters: 2 hidden layers, 900 neurons per layer, random initial weights for structured DNN, Vesely\u2019s DNN as initial weight for frontend DNN in FSDNN, phone error rate as \u2206(\u00b7, \u00b7), mini-batch used, momentum = 0.9, learning rate = 4 \u00d7 10\u22126, halving learning rate if the improvements of loss function was too small. Due to computation time, we only used N=1 when\nchoosing the negative examples, that was 1 totally random path, 1-best lattice path and 1 random lattice path were used to train. Test was on 10\u00d7N (10-best) lattice paths.\n5. EXPERIMENTAL RESULTS\nThe results are listed in Table 1. Rows (1) and (2) are for different acoustic features, where row (1) is actually the acoustic features used by Vesel\u2019s recipe. Column (A) are the results of structured SVM. Columns (B) and (C) are for the proposed structured DNN, respectively for the lost function of approximating the phoneme accuracy in subsection 2.3.1 in column (B) and maximizing the margin in subsection 2.3.2 in column (C). Column (D) is for the extension of column (C) to the Fullscale structured DNN (FSDNN) described in Subsection 2.6, in which the front-end DNN and structured DNN were jointly trained. Column (E) is the baseline.\nIt is clear that the phoneme posterior in row (2) is better than the feature vectors used in the Vesel\u2019s recipe in row (1), accounting for the powerful feature transform achieved by DNN. The structured DNN outperformed the structured SVM on both acoustic features (columns (B), (C) v.s. (A) on both rows (1) (2)). This is not surprising because the structured SVM learned only the linear transform; while structured DNN, learned much more complex nonlinearity. Although the results of structured DNN were obtained by rescoring the lattices of Vesel\u2019s results which is 18.90% on Phoneme Error Rate (PER), structured DNN was better than baseline in most cases with both sets of acoustic vectors (columns (B), (C) v.s. (E)). These results showed that the proposed structured DNN did learn some substantial information beyond the normal frame-level DNN.\nComparing the results in columns (B) and (C) of Table 1, we see the selection of the loss function is critical. Margin performed much better than approximating the phoneme accuracy (columns (C) v.s. (B)). A possible reason is as follows. The loss function of approximating the phoneme accuracy may be inevitably dominated by negative examples, since we used much more negative examples than positive examples in training, and the positive and negative examples were equally weighted. The loss function of maximizing the margin, on the other hand, focused on the score difference between each pair of positive and negative examples, and as a result, the positive and negative examples were weighted equally even if very different numbers.\nWhen we compare the structured DNN with the fullscale structured DNN (FSDNN) using the same loss function (columns (D) v.s. (C)), we see that propagating the errors all the way back into the front-end layer did offer good improvement, and the best result we got here is 17.78%, which beat baseline 18.90% by an 5.5% relative improvements (columns (D) v.s. (E) in row (1)). Note that the full potential of structured DNN was not well explored yey, for example, as explained in Section 3, we simply assume a single state for\na phoneme in (10), which is certainly over-simplified. Also, as mentioned in Sections 2.5, 2.4, both the inference and training were simplified for reduced computation and lack of time.\nIn order to see why FSDNN in column (D) can do better than baseline in column (E), we took a deeper look at the data, and the results of a selected example utterance was shown in Figure 4. In this figure, the phoneme accuracy (vertical scale) for the 10-best paths of the example utterance was plotted as a function of the scores obtained in the recognizer (horizontal scale), i.e., FSDNN and baseline in columns (D)(E) and row (1). There are 10 dots in each figure, each for a path among the best 10. The same color was used to mark the same path. Note that the phoneme accuracy is discrete here for a single utterance of the integer number for the phoneme errors. In Figure 4(a), the FSDNN gave the highest score to the path with the highest phoneme accuracy (the upper right blue point). In Figure 4(b), however, this blue point received only relatively low score from kaldi (top of the figure), while a path with lower phoneme accuracy had the highest score from kaldi (the right most brown point). This resulted in a lower phoneme accuracy by baseline for this utterance. When we evaluated the regression line for those figures, we found FSDNN score and phoneme accuracy are positively correlated in Figure 4(a), but the kaldi score and phoneme accuracy are negatively correlated. Although this is for just a selected example, it is easy to find many such examples with similar situation. Noting that FSDNN here was trained on the large margin criteria not considering the phoneme accuracy, but what was learned was positively correlated with the phonme accuracy.\nThe next experiment is to analyze the PER for different choices of the key hyper-parameters for the full-scale structured DNN (FSDNN), number of hidden layers L and number of neurons M in each hidden layer. Figure 5 is the result, a visualized PER map for FSDNN using acoustic vectors (1). The horizontal axis is M where M = 100, 200, ...1000, and the vertical axis is L where L = 1, 2, ...5. Therefore, the figure consists of 5\u00d710 = 50 data points. The overall performance is approximately between 17% and 19%, more or less comparable to baseline. For this task, N = 1 for training, N = 10 for inferencing. Better PER seemed to be located at several disjoint regions (4 valleys in the figure). The best\nresult is on (L,M) = (4, 800), which was the case in Table 1. The disjoint valleys may come from the relatively poor learning due to lack of training data and poor initialization. The loss function in (7) is ReLU like, which may result in a similar behavior of ReLU training (highly dependent on the initialization).\n6. CONCLUSION AND FUTURE WORK In this paper, we propose a new structured learning architecture, structured DNN, for phoneme recognition which jointly considers the structures of acoustic vector sequences and phoneme label sequences globally. Preliminary test results show that the structured DNN outperformed the previously proposed structured SVM and beat the state-of-the-art kaldi results. We will work on multiple states per phoneme in the future, and explore more possibilities of this approach.\n7. REFERENCES\n[1] Geoffrey Hinton, Simon Osindero, and Yee-Whye Teh, \u201cA fast learning algorithm for deep belief nets,\u201d Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.\n[2] Geoffrey E Hinton and Ruslan R Salakhutdinov, \u201cReducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.\n[3] George E Dahl, Dong Yu, Li Deng, and Alex Acero, \u201cContext-dependent pre-trained deep neural networks for large-vocabulary speech recognition,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 30\u201342, 2012.\n[4] Abdel-rahman Mohamed, George E Dahl, and Geoffrey Hinton, \u201cAcoustic modeling using deep belief networks,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 14\u201322, 2012.\n[5] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.\n[6] Zolta\u0301n Tu\u0308ske, Pavel Golik, Ralf Schlu\u0308ter, and Hermann Ney, \u201cAcoustic modeling with deep neural networks using raw time signal for lvcsr,\u201d in Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014.\n[7] Oriol Vinyals and Nelson Morgan, \u201cDeep vs. wide: depth on a budget for robust speech recognition.,\u201d in INTERSPEECH, 2013, pp. 114\u2013118.\n[8] Daniel Povey, Lukas Burget, Mohit Agarwal, Pinar Akyazi, Kai Feng, Arnab Ghoshal, Ondrej Glembek, Nagendra K Goel, Martin Karafia\u0301t, Ariya Rastrow, et al., \u201cSubspace gaussian mixture models for speech recognition,\u201d in Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 4330\u20134333.\n[9] Lawrence R Rabiner and Biing-Hwang Juang, Fundamentals of speech recognition, vol. 14, PTR Prentice Hall Englewood Cliffs, 1993.\n[10] John Lafferty, Andrew McCallum, and Fernando CN Pereira, \u201cConditional random fields: Probabilistic models for segmenting and labeling sequence data,\u201d 2001.\n[11] Andrew McCallum and Wei Li, \u201cEarly results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons,\u201d in Proceedings of the seventh conference on Natural language\nlearning at HLT-NAACL 2003-Volume 4. Association for Computational Linguistics, 2003, pp. 188\u2013191.\n[12] Asela Gunawardana, Milind Mahajan, Alex Acero, and John C Platt, \u201cHidden conditional random fields for phone classification.,\u201d in INTERSPEECH, 2005, pp. 1117\u20131120.\n[13] Geoffrey Zweig and Patrick Nguyen, \u201cA segmental crf approach to large vocabulary continuous speech recognition,\u201d in Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 152\u2013157.\n[14] Yun-Hsuan Sung and Daniel Jurafsky, \u201cHidden conditional random fields for phone recognition,\u201d in Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 107\u2013 112.\n[15] Dong Yu and Li Deng, \u201cDeep-structured hidden conditional random fields for phonetic recognition.,\u201d in INTERSPEECH. Citeseer, 2010, pp. 2986\u20132989.\n[16] Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun, \u201cLarge margin methods for structured and interdependent output variables,\u201d in Journal of Machine Learning Research, 2005, pp. 1453\u2013 1484.\n[17] Shi-Xiong Zhang, Mark JF Gales, et al., \u201cStructured support vector machines for noise robust continuous speech recognition.,\u201d in INTERSPEECH, 2011, pp. 989\u2013990.\n[18] Shi-Xiong Zhang and Mark JF Gales, \u201cStructured svms for automatic speech recognition,\u201d Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 3, pp. 544\u2013555, 2013.\n[19] Hao Tang, Chao-Hong Meng, and Lin-Shan Lee, \u201cAn initial attempt for phoneme recognition using structured support vector machine (svm),\u201d in Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 4926\u20134929.\n[20] Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik, \u201cA training algorithm for optimal margin classifiers,\u201d in Proceedings of the fifth annual workshop on Computational learning theory. ACM, 1992, pp. 144\u2013 152.\n[21] Shi-Xiong Zhang, Chaojun Liu, Kaisheng Yao, and Yifan Gong, \u201cDeep neural support vector machines for speech recognition,\u201d .\n[22] Yotaro Kubo, Takaaki Hori, and Atsushi Nakamura, \u201cIntegrating deep neural networks into structural classification approach based on weighted finite-state transducers.,\u201d in INTERSPEECH, 2012.\n[23] Thorsten Joachims, Thomas Finley, and ChunNam John Yu, \u201cCutting-plane training of structural svms,\u201d Machine Learning, vol. 77, no. 1, pp. 27\u201359, 2009.\n[24] Steve Lawrence, C Lee Giles, Ah Chung Tsoi, and Andrew D Back, \u201cFace recognition: A convolutional neural-network approach,\u201d Neural Networks, IEEE Transactions on, vol. 8, no. 1, pp. 98\u2013113, 1997.\n[25] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in neural information processing systems, 2012, pp. 1097\u20131105.\n[26] Yann LeCun and Yoshua Bengio, \u201cConvolutional networks for images, speech, and time series,\u201d The handbook of brain theory and neural networks, vol. 3361, no. 10, 1995.\n[27] K-F Lee and H-W Hon, \u201cSpeaker-independent phone recognition using hidden markov models,\u201d Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 37, no. 11, pp. 1641\u20131648, 1989.\n[28] Thorsten Joachims, \u201cstruct svm, support vector machine for complex outputs,\u201d 2008.\n[29] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely, \u201cThe kaldi speech recognition toolkit,\u201d in IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. Dec. 2011, IEEE Signal Processing Society, IEEE Catalog No.: CFP11SRW-USB."}], "references": [{"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Acoustic modeling using deep belief networks", "author": ["Abdel-rahman Mohamed", "George E Dahl", "Geoffrey Hinton"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 14\u201322, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Acoustic modeling with deep neural networks using raw time signal for lvcsr", "author": ["Zolt\u00e1n T\u00fcske", "Pavel Golik", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep vs. wide: depth on a budget for robust speech recognition", "author": ["Oriol Vinyals", "Nelson Morgan"], "venue": "INTERSPEECH, 2013, pp. 114\u2013118.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Subspace gaussian mixture models for speech recognition", "author": ["Daniel Povey", "Lukas Burget", "Mohit Agarwal", "Pinar Akyazi", "Kai Feng", "Arnab Ghoshal", "Ondrej Glembek", "Nagendra K Goel", "Martin Karafi\u00e1t", "Ariya Rastrow"], "venue": "Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 4330\u20134333.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando CN Pereira"], "venue": "2001.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons", "author": ["Andrew McCallum", "Wei Li"], "venue": "Proceedings of the seventh conference on Natural language  learning at HLT-NAACL 2003-Volume 4. Association for Computational Linguistics, 2003, pp. 188\u2013191.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Hidden conditional random fields for phone classification", "author": ["Asela Gunawardana", "Milind Mahajan", "Alex Acero", "John C Platt"], "venue": "INTERSPEECH, 2005, pp. 1117\u20131120.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "A segmental crf approach to large vocabulary continuous speech recognition", "author": ["Geoffrey Zweig", "Patrick Nguyen"], "venue": "Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 152\u2013157.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Hidden conditional random fields for phone recognition", "author": ["Yun-Hsuan Sung", "Daniel Jurafsky"], "venue": "Automatic Speech Recognition & Understanding, 2009. ASRU 2009. IEEE Workshop on. IEEE, 2009, pp. 107\u2013 112.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep-structured hidden conditional random fields for phonetic recognition", "author": ["Dong Yu", "Li Deng"], "venue": "IN- TERSPEECH. Citeseer, 2010, pp. 2986\u20132989.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["Ioannis Tsochantaridis", "Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun"], "venue": "Journal of Machine Learning Research, 2005, pp. 1453\u2013 1484.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Structured support vector machines for noise robust continuous speech recognition", "author": ["Shi-Xiong Zhang", "Mark JF Gales"], "venue": "INTERSPEECH, 2011, pp. 989\u2013990.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Structured svms for automatic speech recognition", "author": ["Shi-Xiong Zhang", "Mark JF Gales"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 21, no. 3, pp. 544\u2013555, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "An initial attempt for phoneme recognition using structured support vector machine (svm)", "author": ["Hao Tang", "Chao-Hong Meng", "Lin-Shan Lee"], "venue": "Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on. IEEE, 2010, pp. 4926\u20134929.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "A training algorithm for optimal margin classifiers", "author": ["Bernhard E Boser", "Isabelle M Guyon", "Vladimir N Vapnik"], "venue": "Proceedings of the fifth annual workshop on Computational learning theory. ACM, 1992, pp. 144\u2013 152.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Deep neural support vector machines for speech recognition", "author": ["Shi-Xiong Zhang", "Chaojun Liu", "Kaisheng Yao", "Yifan Gong"], "venue": ".", "citeRegEx": "21", "shortCiteRegEx": null, "year": 0}, {"title": "Integrating deep neural networks into structural classification approach based on weighted finite-state transducers", "author": ["Yotaro Kubo", "Takaaki Hori", "Atsushi Nakamura"], "venue": "INTERSPEECH, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Cutting-plane training of structural svms", "author": ["Thorsten Joachims", "Thomas Finley", "Chun- Nam John Yu"], "venue": "Machine Learning, vol. 77, no. 1, pp. 27\u201359, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Face recognition: A convolutional neural-network approach", "author": ["Steve Lawrence", "C Lee Giles", "Ah Chung Tsoi", "Andrew D Back"], "venue": "Neural Networks, IEEE Transactions on, vol. 8, no. 1, pp. 98\u2013113, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks, vol. 3361, no. 10, 1995.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Speaker-independent phone recognition using hidden markov models", "author": ["K-F Lee", "H-W Hon"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, vol. 37, no. 11, pp. 1641\u20131648, 1989.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1989}, {"title": "struct svm, support vector machine for complex outputs", "author": ["Thorsten Joachims"], "venue": "2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. Dec. 2011, IEEE Signal Processing Society, IEEE Catalog No.: CFP11SRW-USB.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Using Deep Neural Networks (DNN) [1, 2] with HMM is a good example [3\u20135].", "startOffset": 33, "endOffset": 39}, {"referenceID": 1, "context": "Using Deep Neural Networks (DNN) [1, 2] with HMM is a good example [3\u20135].", "startOffset": 33, "endOffset": 39}, {"referenceID": 2, "context": "Using Deep Neural Networks (DNN) [1, 2] with HMM is a good example [3\u20135].", "startOffset": 67, "endOffset": 72}, {"referenceID": 3, "context": "Using Deep Neural Networks (DNN) [1, 2] with HMM is a good example [3\u20135].", "startOffset": 67, "endOffset": 72}, {"referenceID": 4, "context": "Using Deep Neural Networks (DNN) [1, 2] with HMM is a good example [3\u20135].", "startOffset": 67, "endOffset": 72}, {"referenceID": 5, "context": "In general, HMMs consider the phoneme structure by states and the transitions among them, but trained primarily on frame level regardless of being based on DNN [6, 7] or Gaussian Mixture Model (or subspace GMM, SGMM [8]).", "startOffset": 160, "endOffset": 166}, {"referenceID": 6, "context": "In general, HMMs consider the phoneme structure by states and the transitions among them, but trained primarily on frame level regardless of being based on DNN [6, 7] or Gaussian Mixture Model (or subspace GMM, SGMM [8]).", "startOffset": 160, "endOffset": 166}, {"referenceID": 7, "context": "In general, HMMs consider the phoneme structure by states and the transitions among them, but trained primarily on frame level regardless of being based on DNN [6, 7] or Gaussian Mixture Model (or subspace GMM, SGMM [8]).", "startOffset": 216, "endOffset": 219}, {"referenceID": 8, "context": "Conditional Random Fields (CRF) [10\u201315] and structured Support Vector Machine (SVM) [16\u201318] are good example approaches.", "startOffset": 32, "endOffset": 39}, {"referenceID": 9, "context": "Conditional Random Fields (CRF) [10\u201315] and structured Support Vector Machine (SVM) [16\u201318] are good example approaches.", "startOffset": 32, "endOffset": 39}, {"referenceID": 10, "context": "Conditional Random Fields (CRF) [10\u201315] and structured Support Vector Machine (SVM) [16\u201318] are good example approaches.", "startOffset": 32, "endOffset": 39}, {"referenceID": 11, "context": "Conditional Random Fields (CRF) [10\u201315] and structured Support Vector Machine (SVM) [16\u201318] are good example approaches.", "startOffset": 32, "endOffset": 39}, {"referenceID": 12, "context": "Conditional Random Fields (CRF) [10\u201315] and structured Support Vector Machine (SVM) [16\u201318] are good example approaches.", "startOffset": 32, "endOffset": 39}, {"referenceID": 13, "context": "Conditional Random Fields (CRF) [10\u201315] and structured Support Vector Machine (SVM) [16\u201318] are good example approaches.", "startOffset": 32, "endOffset": 39}, {"referenceID": 14, "context": "Conditional Random Fields (CRF) [10\u201315] and structured Support Vector Machine (SVM) [16\u201318] are good example approaches.", "startOffset": 84, "endOffset": 91}, {"referenceID": 15, "context": "Conditional Random Fields (CRF) [10\u201315] and structured Support Vector Machine (SVM) [16\u201318] are good example approaches.", "startOffset": 84, "endOffset": 91}, {"referenceID": 16, "context": "Conditional Random Fields (CRF) [10\u201315] and structured Support Vector Machine (SVM) [16\u201318] are good example approaches.", "startOffset": 84, "endOffset": 91}, {"referenceID": 17, "context": "Recently, structured SVM has been used to perform initial phoneme recognition by learning the relationships between the acoustic vector sequence and the phoneme label sequence of the whole utterance jointly rather than on the frame level or from different sets of knowledge sources [19], utilizing the nice properties of SVM [20] to classify the structured patterns of utterances with maximized margin.", "startOffset": 282, "endOffset": 286}, {"referenceID": 18, "context": "Recently, structured SVM has been used to perform initial phoneme recognition by learning the relationships between the acoustic vector sequence and the phoneme label sequence of the whole utterance jointly rather than on the frame level or from different sets of knowledge sources [19], utilizing the nice properties of SVM [20] to classify the structured patterns of utterances with maximized margin.", "startOffset": 325, "endOffset": 329}, {"referenceID": 19, "context": "In recent work, the front-end feature extraction DNN has been integrated with SVM [21] and Weighted Finite-State Transducers (WFST) [22], but here we further integrate the frontend DNN with structured DNN, which is completely different from the previous work.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "In recent work, the front-end feature extraction DNN has been integrated with SVM [21] and Weighted Finite-State Transducers (WFST) [22], but here we further integrate the frontend DNN with structured DNN, which is completely different from the previous work.", "startOffset": 132, "endOffset": 136}, {"referenceID": 21, "context": "Formula (3) can be solved by quadratic programming and the cutting-plane algorithm [23], and is equivalent to the following formula:", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "With the scoring function F1(x,y; \u03b81) and the trained parameter set \u03b81, we can find the label sequence y for the acoustic vector sequence x of any input testing utterance by the well known Viterbi algorithm [23].", "startOffset": 207, "endOffset": 211}, {"referenceID": 22, "context": "The FSDNN we proposed can be considered as a special case (or structured version) of Convolutional Neural Network [24] [25] which works perfectly in computer vision and speech recognition [26].", "startOffset": 114, "endOffset": 118}, {"referenceID": 23, "context": "The FSDNN we proposed can be considered as a special case (or structured version) of Convolutional Neural Network [24] [25] which works perfectly in computer vision and speech recognition [26].", "startOffset": 119, "endOffset": 123}, {"referenceID": 24, "context": "The FSDNN we proposed can be considered as a special case (or structured version) of Convolutional Neural Network [24] [25] which works perfectly in computer vision and speech recognition [26].", "startOffset": 188, "endOffset": 192}, {"referenceID": 25, "context": "The models were trained with a set of 48 phonemes and tested with a set of 39 phonemes, conformed to CMU/MIT standards [27].", "startOffset": 119, "endOffset": 123}, {"referenceID": 26, "context": "We used an online library [28] for structured SVM, and modified the kaldi [29] code to implement structured DNN.", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "We used an online library [28] for structured SVM, and modified the kaldi [29] code to implement structured DNN.", "startOffset": 74, "endOffset": 78}], "year": 2015, "abstractText": "In this paper we propose the Structured Deep Neural Network (structured DNN) as a structured and deep learning framework. This approach can learn to find the best structured object (such as a label sequence) given a structured input (such as a vector sequence) by globally considering the mapping relationships between the structures rather than item by item. When automatic speech recognition is viewed as a special case of such a structured learning problem, where we have the acoustic vector sequence as the input and the phoneme label sequence as the output, it becomes possible to comprehensively learn utterance by utterance as a whole, rather than frame by frame. Structured Support Vector Machine (structured SVM) was proposed to perform ASR with structured learning previously, but limited by the linear nature of SVM. Here we propose structured DNN to use nonlinear transformations in multi-layers as a structured and deep learning approach. This approach was shown to beat structured SVM in preliminary experiments on TIMIT.", "creator": "LaTeX with hyperref package"}}}