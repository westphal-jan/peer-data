{"id": "1511.02916", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2015", "title": "Spectral-Spatial Classification of Hyperspectral Image Using Autoencoders", "abstract": "tagged hyperspectral tagged image ( hsi ) classification mapping is a hot topic researched in the asian remote sound sensing community. discussing this 2017 paper paper proposes demanding a new framework of spectral - spatial feature extraction for hsi classification, in which for the first time onwards the concept of bounded deep learning is officially introduced. specifically, establishing the model modelling of the autoencoder identification is carefully exploited in our framework to additionally extract various distinctive kinds unknown of features. first shall we verify exactly the eligibility of autoencoder definition by uniformly following classical sequential spectral volume information distance based classification and use autoencoders labelled with different search depth curves to classify hyperspectral fuzzy image. further in shaping the proposed framework, wherein we combine pca findings on spectral dimension enhancement and improve autoencoder identification on either the dominant other the two coupled spatial dimensions to extract spectral - spatial descriptive information for classification. as the result experimental performance results show that this same framework achieves progressively the highest hierarchical classification separation accuracy among all methods, hence and likewise outperforms classical classifiers such as svm and pca - based svm.", "histories": [["v1", "Mon, 9 Nov 2015 22:29:13 GMT  (718kb)", "http://arxiv.org/abs/1511.02916v1", "Accepted as a conference paper at ICICS 2013, an updated version. Codes published. 9 pages, 6 figures"]], "COMMENTS": "Accepted as a conference paper at ICICS 2013, an updated version. Codes published. 9 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["zhouhan lin", "yushi chen", "xing zhao", "gang wang"], "accepted": false, "id": "1511.02916"}, "pdf": {"name": "1511.02916.pdf", "metadata": {"source": "CRF", "title": "Spectral-Spatial Classification of Hyperspectral Image Using Autoencoders", "authors": ["Zhouhan Lin", "Yushi Chen", "Xing Zhao", "Gang Wang"], "emails": ["lin.zhouhan@gmail.com,", "chenyushi@hit.edu.cn,", "xintongzhaoxing@126.com", "wanggang@ntu.edu.sg"], "sections": [{"heading": null, "text": "paper proposes a new framework of spectral-spatial feature extraction for HSI classification, in which for the first time the concept of deep learning is introduced. Specifically, the model of autoencoder is exploited in our framework to extract various kinds of features. First we verify the eligibility of autoencoder by following classical spectral information based classification and use autoencoders with different depth to classify hyperspectral image. Further in the proposed framework, we combine PCA on spectral dimension and autoencoder on the other two spatial dimensions to extract spectral-spatial information for classification. The experimental results show that this framework achieves the highest classification accuracy among all methods, and outperforms classical classifiers such as SVM and PCA-based SVM.\nKeywords-autoencoders; deep learning; hyperspectral; image classification; neural networks; stacked\nautoencoders\nI. INTRODUCTION\nBy combining imaging and spectroscopy technology, hyperspectral remote sensing can get spatially and spectrally continuous data simultaneously. Hyperspectral imagery is becoming a valuable tool for monitoring the Earth\u2019s surface [1]. If successfully exploited, the hyperspectral image can yield higher classification accuracies and more detailed class taxonomies [2].\nTraditional HSI classification methods uses spectral information only, and the classification algorithms typically include parallelepiped classification, k-nearest-neighbors, maximum likelihood, minimum distance and logistic regression [3]. Since HSI classification deals with a problem with high-dimensional feature space and low number of labeled data, the majority of these above algorithms suffer a lot from \u201ccurse of dimensionality.\u201d[4] To tackle this problem, scholars of the remote sensing community try to introduce variety of feature extraction methods like PCA, ICA, sequential forward floating search and wavelet analysis [5]. However, these methods do not bring significant improvement for classification, in some cases the resulting accuracies even fall below direct-classification approaches.\nHowever, large marginal machines like SVM deals well on the above over fitting problem. In most cases, SVM based methods can obtain better classification accuracy than other widely used pattern recognition techniques on HSI data [3]. So after SVM is introduced in this field [6], feature extraction is\n*This work is supported by \"the Fundamental Research Funds for the Central Universities\" (Grant No. HIT.\nNSRIF.2013028) and National Natural Science Foundation of China( No. 61301206 )\n*Codes for the Stacked Autoencoder models mentioned in this paper are available at\nhttps://github.com/hantek/deeplearn_hsi\nseldom used, and applying SVM based method on the original data becomes the state-of-the-art method in HSI classification. However, the ways all these generally used methods are applied have one thing in common: They deal with spectral information of each pixel only. But as recently the resolution of imaging spectrometer burgeons, the spatial information seems growing more important for further improving HSI classification accuracy. Recently there are some methods proposed for incorporating spatial information, like mathematical morphology [7].\nOn the other thread, recent advantages in training multilayer neural networks have refurbished best records in a wide variety of machine learning problems including classification or regression tasks that involve processing image [8], language [9] and speech [10]. Typical deep neural network architectures include Deep Belief Networks [11], Deep Boltzmann Machines [12], Stacked Autoencoders [13] and Stacked Denoising Autoencoders [14]. The layer-wise training framework has a bunch of alternatives like Restricted Boltzmann Machines [15], Pooling Units [16], Convolutional Neural Networks [17] and Autoencoders [13].\nIn this paper, we introduce deep learning based feature extraction for HSI classification for the first time. Our work focuses applying one of the aforementioned models \u2013 autoencoders \u2013 to learn representations (i.e. features) of hyperspectral image data in an unsupervised manner. Since hidden layer activities are exploited as features of the initial data, and they are called \u201crepresentations\u201d in the neural network vocabulary, we will use the word \u201crepresentation\u201d instead of \u201cfeature\u201d in the rest of this paper. Our methods exploit single-layer autoencoder and multi-layer stacked autoencoder to extract shallow and deep representations of hyperspectral image data respectively. Further we propose a new way of extracting spectral-spatial representation for classification. These representations are then exploited to deal with HSI classification problems. The effectiveness of these methods is verified by classification accuracy.\nThe rest of this paper is organized in four sections. Section II is a description for the hyperspectral image classification task we are dealing with, and a terse introduction on the autoencoder models used in this paper. Section III details on how we extract various representations with the novel architecture. Subsequent classification is conducted by logistic regression and SVM. Section IV deals with the experimental results. Finally, Section V summarizes the observations and point out some probable future works to complete this paper.\nII. BACKGROUND"}, {"heading": "A. Hyperspectral Image Classification: Description", "text": "Classifying hyperspectral image is a little bit different with ordinary image classification. So we need to elaborate here the task we are facing.\nA typical scene of hyperspectral image covers several square kilometers of lands and has hundreds of spectral channels instead of only 3 RGB channels, as shown in Fig 1. As a result, the task of classifying hyperspectral image has unique characters. The most unique one is that, instead of giving one label to a whole image, hyperspectral image classification task is pixel-based: We arrange a label for each pixel in the scene according to the spectral information provided by its hundreds of spectral channels."}, {"heading": "B. Autoencoders (AE) and Stacked autoencoders (SAE)", "text": "An autoencoder (AE) has one visible layer of \ud835\udc51 inputs and one hidden layer of \u210e units with an activation function f. During training, it first maps the input \ud835\udc65\ud835\udf16\ud835\udc45\ud835\udc51 to the hidden layer and get the latent representation \ud835\udc66\ud835\udf16\ud835\udc45\u210e; and then \ud835\udc66 is mapped to an output layer that has the same size with input layer, which is called \u201creconstruction.\u201d The reconstruction is denoted as \ud835\udc67\ud835\udf16\ud835\udc45\ud835\udc51 (Fig 2).\nMathematically, these procedures can be shown as:\n\ud835\udc66 = \ud835\udc53(\ud835\udc4a\ud835\udc66\ud835\udc65 + \ud835\udc4f\ud835\udc66) (1)\n\ud835\udc67 = \ud835\udc53(\ud835\udc4a\ud835\udc67\ud835\udc66 + \ud835\udc4f\ud835\udc67) (2)\nwhere \ud835\udc4a\ud835\udc66,\ud835\udc4a\ud835\udc67 denotes input-to-hidden and hidden-to-output weights respectively, \ud835\udc4f\ud835\udc66, \ud835\udc4f\ud835\udc67 denotes the bias of hidden and output units, and \ud835\udc53(\u2219) denotes the activation function, which apply element-wise to its arguments. The goal of training is to minimize the \u201cerror\u201d between input and reconstructed input, i.e.,\nargmin \ud835\udc4a,\ud835\udc4f\ud835\udc66,\ud835\udc4f\ud835\udc67 [\ud835\udc50(\ud835\udc65, \ud835\udc67)] (3)\nwhere \ud835\udc67 is dependent on parameters \ud835\udc4a,\ud835\udc4f\ud835\udc66, \ud835\udc4f\ud835\udc67 while \ud835\udc65 is given. \ud835\udc50(\ud835\udc65, \ud835\udc67) stands for the \u201cerror,\u201d which can be defined in a variety of ways.\nWe introduce stacked autoencoder (SAE) to help us compute deep representations of spectral data. The SAE has multiple layers of autoencoders and yields a deep representation of input data at the output of the last layer. It is trained in a layer-wise manner. That is, after we finish training a former layer of parameters, subsequent layer is trained according to the output of its previous layer. Stacking these input-to-hidden layers sequentially constructs a stacked autoencoder.\nIII. LEARNING REPRESENTATIONS FOR HSI CLASSIFICATION\nOur methods involve feature extraction for HSI classification. We first compute representations via autoencoder and deem it as the feature of data, then construct a classifier on the neural network to finish the classification phase. This section focuses on how the varieties of features are extracted and incorporated in classification. For classical HSI classification which exploits spectral information only, we first propose two kinds of classification schemes exploiting shallow and deep representations of HSI spectra respectively. In the last part of this section, we propose a novel classification framework which exploits both spectral and spatial information."}, {"heading": "A. Classifying with shallow spectral representation", "text": "This scheme consists of two steps. First an autoencoder is used to extract single-layer spectral representation and then a SVM is constructed on top of the hidden layer of autoencoder.\nIn the autoencoder layer, the activation function in our method is set to be sigmoidal, i.e. \ud835\udc53(\ud835\udc65) = 1\n1+\ud835\udc52\u2212\ud835\udc65 .\nIts first-order and second-order derivative has special forms that brings convenience for computing: \ud835\udc53\u2032(x) = \ud835\udc53(\ud835\udc65)[1 \u2212 \ud835\udc53(\ud835\udc65)], \ud835\udc53\u2032\u2032(\ud835\udc65) = \ud835\udc53(\ud835\udc65)[1 \u2212 \ud835\udc53(\ud835\udc65)][1 \u2212 2\ud835\udc53(\ud835\udc65)]. And to reduce the number of parameters, we use tied weights while we are training the autoencoder, i.e. let \ud835\udc4a\ud835\udc66 = \ud835\udc4a\ud835\udc67 = \ud835\udc4a. So now there\u2019s 3 groups of parameters remaining to learn: \ud835\udc4a,\ud835\udc4f\ud835\udc66 and \ud835\udc4f\ud835\udc67.\nWe choose the cross entropy as the cost function here, which is in accordance with the sigmoid output. In our implementation, the cost is computed on a minibatch of inputs since we adopt minibatch update strategy for the large dataset.\n\ud835\udc50 = \u2212 1\n\ud835\udc5a \u2211\u2211[\ud835\udc65\ud835\udc56\ud835\udc58 \ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udc67\ud835\udc56\ud835\udc58) + (1 \u2212 \ud835\udc65\ud835\udc56\ud835\udc58) \ud835\udc59\ud835\udc5c\ud835\udc54(1 \u2212 \ud835\udc67\ud835\udc56\ud835\udc58)]\n\ud835\udc51\n\ud835\udc58=1\n\ud835\udc5a\n\ud835\udc56=1\n(5)\nHere \ud835\udc51 denotes the input vector size, and \ud835\udc5a denotes minibatch size. \ud835\udc65\ud835\udc56\ud835\udc58(\ud835\udc67\ud835\udc56\ud835\udc58) denotes \ud835\udc58-th element of the \ud835\udc56-th input (reconstruction) in the minibatch. The inner summation is over the input dimension, while the outer over a whole minibatch.\nOur hope turns to optimize Eq. 5 using minibatch stochastic gradient descent. We\u2019ll now derive the partial differentials of cost with respect to parameters \ud835\udc4a,\ud835\udc4f\ud835\udc66 and \ud835\udc4f\ud835\udc67. First we rewrite reconstruction in a scalar form:\n\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc5d \ud835\udc66 =\u2211\ud835\udc65\ud835\udc56\ud835\udc5e\ud835\udc4a\ud835\udc5e\ud835\udc5d\n\ud835\udc51\n\ud835\udc5e=1\n+ \ud835\udc4f\ud835\udc66\ud835\udc5d (6)\n\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc58 \ud835\udc67 =\u2211\ud835\udc4a\ud835\udc58\ud835\udc5d\ud835\udc53(\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc5d \ud835\udc66 )\n\u210e\n\ud835\udc5d=1\n+ \ud835\udc4f\ud835\udc67\ud835\udc58 (7)\n\ud835\udc67\ud835\udc56\ud835\udc58 = \ud835\udc53(\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc58 \ud835\udc67 ) = \ud835\udc53 (\u2211\ud835\udc4a\ud835\udc58\ud835\udc5d\ud835\udc53(\u2211\ud835\udc65\ud835\udc56\ud835\udc5e\ud835\udc4a\ud835\udc5e\ud835\udc5d\n\ud835\udc51\n\ud835\udc5e=1\n+ \ud835\udc4f\ud835\udc66\ud835\udc5d)\n\u210e\n\ud835\udc5d=1\n+ \ud835\udc4f\ud835\udc67\ud835\udc58) (8)\nwhere \ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc5d \ud835\udc66 , \ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc58 \ud835\udc67 denotes the net input of the \ud835\udc5d-th hidden and output unit, given the \ud835\udc56-th sample in\nthe minibatch.\nPutting them all together (Eqs. 5~8), and using the chain rule, we have partial differentials of cost (Eq. 5) over parameters \ud835\udc4a,\ud835\udc4f\ud835\udc66 and \ud835\udc4f\ud835\udc67.\n{\n\u2202c\n\u2202\ud835\udc4a\ud835\udc5f\ud835\udc60 = \u2212\n1 \ud835\udc5a \u2211{\u2211[ \ud835\udc65\ud835\udc56\ud835\udc58 \u2212 \ud835\udc67\ud835\udc56\ud835\udc58 \ud835\udc67\ud835\udc56\ud835\udc58(1 \u2212 \ud835\udc67\ud835\udc56\ud835\udc58) \ud835\udc53\u2032(\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc58 \ud835\udc67 )\ud835\udc4a\ud835\udc58\ud835\udc60\ud835\udc53 \u2032(\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc60 \ud835\udc66 )\ud835\udc65\ud835\udc56\ud835\udc5f]\n\ud835\udc51\n\ud835\udc58=1\n+ \ud835\udc53\u2032(\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc58 \ud835\udc67 )\ud835\udc53(\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc60 \ud835\udc66 )}\n\ud835\udc5a\n\ud835\udc56=1\n\u2202c\n\u2202\ud835\udc4f\ud835\udc66\ud835\udc5f = \u2212\n1 \ud835\udc5a \u2211\u2211 \ud835\udc65\ud835\udc56\ud835\udc58 \u2212 \ud835\udc67\ud835\udc56\ud835\udc58 \ud835\udc67\ud835\udc56\ud835\udc58(1 \u2212 \ud835\udc67\ud835\udc56\ud835\udc58) \ud835\udc53\u2032(\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc58 \ud835\udc67 )\ud835\udc4a\ud835\udc58\ud835\udc5f\ud835\udc53\u2032(\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc5f \ud835\udc66 )\n\ud835\udc51\n\ud835\udc58=1\n\ud835\udc5a\n\ud835\udc56=1\n\u2202c\n\u2202\ud835\udc4f\ud835\udc67\ud835\udc5f = \u2212\n1 \ud835\udc5a \u2211\u2211 \ud835\udc65\ud835\udc56\ud835\udc58 \u2212 \ud835\udc67\ud835\udc56\ud835\udc58 \ud835\udc67\ud835\udc56\ud835\udc58(1 \u2212 \ud835\udc67\ud835\udc56\ud835\udc58) \ud835\udc53\u2032(\ud835\udc5b\ud835\udc52\ud835\udc61\ud835\udc56\ud835\udc58 \ud835\udc67 )\n\ud835\udc51\n\ud835\udc58=1\n\ud835\udc5a\n\ud835\udc56=1\n(9)\nEq. 9 gives explicitly the gradients we need in conducting stochastic gradient descent, by which we learn the 3 groups of parameters.\nAfter training the network, we remove the reconstruction layer and deem the hidden activity to be the learned representation. The second step is to put an SVM with linear kernel on top of its hidden layer.\nWhat we should notice is that the two models are trained separately, i.e., the SVM is trained after all parameters of the autoencoder are determined, and training SVM does not change anything of the\nautoencoder. So this classification scheme should be called Autoencoder-based SVM, and we will call it AE-SVM for short in the rest part of the paper."}, {"heading": "B. Classifying with deep spectral representation", "text": "There exist some motivations to extract more robust deep spectral representations. First, because of the complex situation of lightening in the large scene, objects in a same class show different spectral characters in different location. For example, a lawn exposed in direct sunshine shows different spectral characters from a same lawn but eclipsed from the sunshine by a high building. Also, scattering from other peripheral ground objects tilts the spectra of the lawn and change its characters too. Other factors involve rotations of the sensor, different atmospheric scattering condition, and so on. According to these factors, the probability distribution of a certain class is hard to be one-hot and has variations over multiple directions in the feature space. These complex variations of spectra make it hopeless to analyze pixel by pixel how they are affected by their tangent pixels in the complicated real situation, thus they demand more robust and invariant features.\nIt is believed that deep architectures can potentially lead to progressively more abstract features at higher layers of representation, and more abstract features are generally invariant to most local changes of the input [17]. So, to get more generally invariant representations and tackle these problems, stacked autoencoder and its corresponding deep spectral representations are suitable for this problem.\nWe introduce stacked autoencoder to help us get deep representations of spectral data, and add a logistic regression classifier on top of the stacked autoencoder to reach the high classification accuracy. Fig 3 shows a typical instance of the deep architecture used in our paper. The first layer autoencoder maps inputs in layer 0 to a first layer representation in layer 1. It is trained in the same manner to aforementioned single layer autoencoder.\nFor logistic regression, we use soft-max as its output layer activation, and output layer size is the same to total number of classes. Since it is implemented as a single-layer neural network, it is merged with the former layers of network. Fitting the classifier is conducted over the whole architecture, but with very slight learning rates on former layer autoencoders."}, {"heading": "C. Classifying with spectral-spatial representation", "text": "Unlike other HSI spectral-spatial information extraction methods which only use the 4 or 8 tangent neighbors and simple filtering, our deep framework takes all the pixels in a flat neighbor region into consideration, and let the autoencoders learn the representation by itself. The overall flowchart of our proposed method is detailed in Fig 4.\nFirst a 7x7 neighbor region of a certain pixel is extracted from the original image. Due to the hundreds of channels along the spectral dimension, data on this initial layer always have tens of thousands of dimensions. Such a large neighbor region will result in too large an input dimension for the classifier, and contains too large amount of redundancy.\nIn the second layer, PCA is introduced to condense the spectral information, thus reducing data dimension to an acceptable scale, but reserve spatial information at the meantime. Since we mainly care about incorporating spatial information in this method, we use PCA along the spectral dimension and only retain the first 3 principle components. The PCA transformation matrix is fitted on the whole image, both for tagged and untagged pixels. This step does cast away part of spectral information, but since PCA is conducted within pixels, the spatial information remains intact.\nAfter these processing, we \u201cflatten\u201d the data in the third layer, i.e., stench it onto a 1-D vector, and feed it into a stacked autoencoder.\nThe subsequent layers include layer-wise training autoencoders and fine-tune the whole model with a final logistic regression layer. These steps are similar to the former subsection which deals with deep spectral representation and thus we would not repeat describing them here.\nIV. EXPERIMENTAL RESULTS\nIn our study, two hyperspectral images with different environmental settings are applied to validate the proposed autoencoder-based classification method. The first is a mixed vegetation site over Kennedy Space Center (KSC), Florida, which has 13 land cover categories, 224 spectral channels. Due to water absorption and existence of low signal-noise ratio channels, 176 of them are used in our experiments. It has 512 \u00d7 614 pixels, in which 5211 pixels are labeled. The second is the urban site over the city of Pavia, Italy, which has 9 land cover classes, 103 spectral channels. Its spatial size is 512 \u00d7 614, with 42469 pixels labeled.\nOur experiments are three-folded, using the aforementioned three methods respectively. All data are regularized on to an interval of [0, 1] before feeding into classifiers. We conduct our experiments on an Ubuntu 12.10 system, on an Intel i5-3230M processor, which has 2 cores of 2.6GHZ. The codes are implemented using Theano [18], a Python library for symbolic computation. The SVM and PCA are implementations from SciKit-Learn, a Python machine learning package."}, {"heading": "A. Single-layer spectral representation", "text": "This part compares our proposed AE-SVM classification scheme with both ordinary SVM and SVM with PCA plugged in as feature extraction step (PCA-SVM).\n1) Reconstructed Spectrum Since single layer autoencoder is an important building block of other methods proposed in this paper, we first inspect on how well the input is reconstructed through epochs before we give resulting classification accuracies by examine one sample on different training epochs. The examined autoencoder has 100 hidden units and trained on KSC data. It is shown that the autoencoder restitutes a really perfect reconstruction from several hundreds of iterating epochs (Fig. 5).\n2) Classification accuracy Fig. 6 shows the classification performance of 8 autoencoders with different sizes of hidden layers ranging from 20 to 140 on KSC data set and 60 to 180 on Pavia respectively. Although the hidden layer sizes range widely, the performance of the corresponding autoencoder-based SVM always outperforms direct applying SVM after an abundant number of iterations. However, if we accept PCA as a feature extraction step before we conduct SVM (PCA-SVM), then no matter how many principle components are incorporated, the performances of PCA-SVM are never better than direct SVM. Our experiments show that autoencoders help a lot in improving SVM classification performance. And in addition, unlike multilayer perceptron, the result is not sensitive to the number of hidden units."}, {"heading": "B. Classifying with deep spectral representation", "text": "We tried several stacked autoencoders with different depths and compare the results with other traditional methods. For KSC data, it has 176 spectral channels, and each hidden layer size is set to be 100, plus a logistic regression layer on top of the stacked autoencoder. So the neural networks are constructed as 176-100-\u2026-100-13. For Pavia dataset who has 103 spectral channels and 9 classes, the performance reaches its best when using 140 as its hidden layers size. The neural networks are like 103-140-\u2026-140-9.\n\u201cHidden layer numbers\u201d in Table I correspond to the number of 100 or 140-sized layers in the deep neural network. The experiments show that depth does help descending classification error rate. For comparison, we conduct 2 traditional methods, SVM and SVM with PCA plugged in as feature extraction, on the same data. It has shown a significant gain of accuracies on both of the images. (Table I)"}, {"heading": "C. Classifying with spectral-spatial representation", "text": "If we directly apply SVM on the spectral-spatial information, the error rate will be unacceptably high, as shown in Table II. Our methods confirm that this kind of representation also provides abundant information for classification. We use only the former 3 principle components of a 7x7 neighbor region to reach an error rate as low as 4.000% on KSC data and 14.355% on Pavia data.\nClassifiers in the control groups are the same as the former subsection, and are conducted on the PCA-compressed data. However, these traditional methods fail in yielding good enough accuracy; but our proposed method succeeds in finding correct features in the dataset and yields the highest accuracies.\nIn this paper, we propose a hyperspectral image classification framework using both spectral and spatial information extracted by stacked autoencoders.\nOur experiments first confirm that autoencoder-extracted representations help lowering error rate of SVM, a classical classifier previously considered as state-of-the-art in this field. It is shown that autoencoders are not sensitive to hidden unit number. What\u2019s more, the impact that the depth of representation has on classifying hyperspectral images is also inspected, experiments suggest that deeper representations always lead to better classification accuracies. For spectral-spatial information based\nclassification, the proposed deep framework performs well and succeeded in classifying hyperspectral images with highest accuracies. Our future work involves incorporating other kinds of deep learning models into this framework to further improve the classification accuracy.\nREFERENCES\n[1] R. B. Smith, \"Introduction to hyperspectral imaging,\" Aug 2006, available at http://www.microimages.com/getstart/hyprspec.htm. [2] S. Rajan , J. Ghosh and M. M. Crawford \"An active learning approach to hyperspectral data classification,\" IEEE Trans. Geosci. Remote Sens., vol. 46, no. 4, pp.1231-1242, 2008. [3] Foody, Giles M., and Ajay Mathur. \"A relative evaluation of multiclass image classification by support vector machines,\" IEEE Trans. Geosci. Remote Sens., vol. 42, no. 6, pp. 1335-1343, 2004. [4] G. Camps-Valls and L. Bruzzone, \"Kernel-based methods for hyperspectral image classification\", IEEE Trans. Geosci. Remote Sens., vol. 43, pp.1351 -1362 2005. [5] Liu Ying, Gu Yanfeng, Zhang Ye, \u201cHyperspectral Feature Extraction using Selective PCA based on Genetic Algorithm with Subgroups,\u201d Innovative Computing, Information and Control, First International Conference on, 2006. [6] Melgani, Farid, and Lorenzo Bruzzone, \"Classification of hyperspectral remote sensing images with support vector machines.\" IEEE Trans. Geosci. Remote Sens., vol. 42, no.8, pp. 1778-1790, 2004. [7] J.A. Benediktsson et. al., \"Classification and Feature Extraction of Remote Sensing Images from Urban Areas based on Morphological Approaches,\" IEEE Trans. Geosci. Remote Sens., vol. 41, no. 10, pp. 1940-1949, 2003. [8] Hinton, G., Osindero, S., and Teh, Y. \u201cA fast learning algorithm for deep belief nets,\u201d Neural Computation, vol. 18, pp. 1527-1554, 2006. [9] Yu, D., Deng, L., and Wang, S., \u201cLearning in the deep structured conditional random fields,\u201d Proc. NIPS Workshop, Dec. 2009. [10] Mohamed,A.,Dahl, G.,Hinton, G. \u201cDeep belief networks for phone recognition,\u201dProc.NIPS Workshop, Dec. 2009. [11] Salakhutdinov R and Hinton G. E., \u201cDeep Boltzmann machines\u201d, AISTATS\u20192009, pp. 448-455, 2009. [12] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \u201cGreedy layer-wise training of deep networks,\u201d Neural Information Processing Systems 19 (NIPS\u201906), pp. 153-160, 2007. [13] Vincent P., Larochelle H., Lajoie I., Bengio Y., and Manzagol P., \u201cStacked denoising autoencoders\u201d, J. Machine Learning Res., vol. 11, pp. 3371-3408, 2010. [14] Hinton G. E., \u201cA practical guide to training restricted Boltzmann machines,\u201d Technical Report UTML TR2010-003, Department of Computer Science, University of Toronto, 2010. [15] LeCun Y., Boser B., Denker J. S. et.al., \u201cBackpropagation applied to handwritten zip code recognition,\u201d Neural Computation, vol. 1, no. 4, pp. 541-551, 1989. [16] Fukushima K, \u201cNeocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position,\u201d Biological Cybernetics, vol. 36, pp. 193-202, 1980. [17] Bengio Y., Courville A. and Vincent P., \u201cRepresentation learning: A review and new perspectives,\u201d arXiv:1206.5538, 2012. [18] J. Bergstra, O. Breuleux, et. al., \u201cTheano: A CPU and GPU Math Expression Compiler,\u201d Proc. of the Python for Scientific Computing Conference (SciPy), Austin, TX, June 30 - July 3, 2010."}], "references": [{"title": "Introduction to hyperspectral imaging", "author": ["R.B. Smith"], "venue": "Aug 2006, available at http://www.microimages.com/getstart/hyprspec.htm.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "An active learning approach to hyperspectral data classification,", "author": ["S. Rajan", "J. Ghosh", "M.M. Crawford"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 46,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A relative evaluation of multiclass image classification by support vector machines", "author": ["Foody", "Giles M.", "Ajay Mathur"], "venue": "IEEE  Trans.  Geosci.  Remote Sens., vol. 42, no. 6, pp. 1335-1343, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Kernel-based methods for hyperspectral image classification", "author": ["G. Camps-Valls", "L. Bruzzone"], "venue": "IEEE Trans. Geosci. Remote Sens.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Hyperspectral Feature Extraction using Selective PCA based on Genetic Algorithm with Subgroups", "author": ["Liu Ying", "Gu Yanfeng", "Zhang Ye"], "venue": "Innovative Computing, Information and Control, First International Conference on, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Classification of hyperspectral remote sensing images with support vector machines.", "author": ["Melgani", "Farid", "Lorenzo Bruzzone"], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 42,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Classification and Feature Extraction of Remote Sensing Images from Urban Areas based on Morphological Approaches", "author": ["J.A. Benediktsson et. al."], "venue": "IEEE Trans. Geosci. Remote Sens., vol. 41, no. 10, pp. 1940-1949, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1940}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y Teh"], "venue": "Neural Computation, vol. 18, pp. 1527-1554, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning in the deep structured conditional random fields", "author": ["D. Yu", "L. Deng", "S. Wang"], "venue": "Proc. NIPS Workshop, Dec. 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep belief networks for phone recognition,\u201dProc.NIPS", "author": ["A. Mohamed", "G. Dahl", "G. Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Neural Information Processing Systems 19 (NIPS\u201906), pp. 153-160, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Stacked denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "J. Machine Learning Res.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A practical guide to training restricted Boltzmann machines", "author": ["E. Hinton G."], "venue": "Technical Report UTML TR2010-003, Department of Computer Science, University of Toronto, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "et.al. Denker J.S."], "venue": "Neural Computation, vol. 1, no. 4, pp. 541-551, 1989.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1989}, {"title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["K Fukushima"], "venue": "Biological Cybernetics, vol. 36, pp. 193-202, 1980.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1980}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "arXiv:1206.5538, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: A CPU and GPU Math Expression Compiler", "author": ["J. Bergstra", "O. Breuleux", "et. al."], "venue": "Proc. of the Python for Scientific Computing Conference (SciPy), Austin, TX, June 30 - July 3, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Hyperspectral imagery is becoming a valuable tool for monitoring the Earth\u2019s surface [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "If successfully exploited, the hyperspectral image can yield higher classification accuracies and more detailed class taxonomies [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 2, "context": "Traditional HSI classification methods uses spectral information only, and the classification algorithms typically include parallelepiped classification, k-nearest-neighbors, maximum likelihood, minimum distance and logistic regression [3].", "startOffset": 236, "endOffset": 239}, {"referenceID": 3, "context": "\u201d[4] To tackle this problem, scholars of the remote sensing community try to introduce variety of feature extraction methods like PCA, ICA, sequential forward floating search and wavelet analysis [5].", "startOffset": 1, "endOffset": 4}, {"referenceID": 4, "context": "\u201d[4] To tackle this problem, scholars of the remote sensing community try to introduce variety of feature extraction methods like PCA, ICA, sequential forward floating search and wavelet analysis [5].", "startOffset": 196, "endOffset": 199}, {"referenceID": 2, "context": "In most cases, SVM based methods can obtain better classification accuracy than other widely used pattern recognition techniques on HSI data [3].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "So after SVM is introduced in this field [6], feature extraction is", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Recently there are some methods proposed for incorporating spatial information, like mathematical morphology [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "On the other thread, recent advantages in training multilayer neural networks have refurbished best records in a wide variety of machine learning problems including classification or regression tasks that involve processing image [8], language [9] and speech [10].", "startOffset": 230, "endOffset": 233}, {"referenceID": 8, "context": "On the other thread, recent advantages in training multilayer neural networks have refurbished best records in a wide variety of machine learning problems including classification or regression tasks that involve processing image [8], language [9] and speech [10].", "startOffset": 244, "endOffset": 247}, {"referenceID": 9, "context": "On the other thread, recent advantages in training multilayer neural networks have refurbished best records in a wide variety of machine learning problems including classification or regression tasks that involve processing image [8], language [9] and speech [10].", "startOffset": 259, "endOffset": 263}, {"referenceID": 10, "context": "Typical deep neural network architectures include Deep Belief Networks [11], Deep Boltzmann Machines [12], Stacked Autoencoders [13] and Stacked Denoising Autoencoders [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "Typical deep neural network architectures include Deep Belief Networks [11], Deep Boltzmann Machines [12], Stacked Autoencoders [13] and Stacked Denoising Autoencoders [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "Typical deep neural network architectures include Deep Belief Networks [11], Deep Boltzmann Machines [12], Stacked Autoencoders [13] and Stacked Denoising Autoencoders [14].", "startOffset": 168, "endOffset": 172}, {"referenceID": 13, "context": "The layer-wise training framework has a bunch of alternatives like Restricted Boltzmann Machines [15], Pooling Units [16], Convolutional Neural Networks [17] and Autoencoders [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "The layer-wise training framework has a bunch of alternatives like Restricted Boltzmann Machines [15], Pooling Units [16], Convolutional Neural Networks [17] and Autoencoders [13].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "The layer-wise training framework has a bunch of alternatives like Restricted Boltzmann Machines [15], Pooling Units [16], Convolutional Neural Networks [17] and Autoencoders [13].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "The layer-wise training framework has a bunch of alternatives like Restricted Boltzmann Machines [15], Pooling Units [16], Convolutional Neural Networks [17] and Autoencoders [13].", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "It is believed that deep architectures can potentially lead to progressively more abstract features at higher layers of representation, and more abstract features are generally invariant to most local changes of the input [17].", "startOffset": 222, "endOffset": 226}, {"referenceID": 0, "context": "All data are regularized on to an interval of [0, 1] before feeding into classifiers.", "startOffset": 46, "endOffset": 52}, {"referenceID": 16, "context": "The codes are implemented using Theano [18], a Python library for symbolic computation.", "startOffset": 39, "endOffset": 43}], "year": 2015, "abstractText": "Hyperspectral image (HSI) classification is a hot topic in the remote sensing community. This paper proposes a new framework of spectral-spatial feature extraction for HSI classification, in which for the first time the concept of deep learning is introduced. Specifically, the model of autoencoder is exploited in our framework to extract various kinds of features. First we verify the eligibility of autoencoder by following classical spectral information based classification and use autoencoders with different depth to classify hyperspectral image. Further in the proposed framework, we combine PCA on spectral dimension and autoencoder on the other two spatial dimensions to extract spectral-spatial information for classification. The experimental results show that this framework achieves the highest classification accuracy among all methods, and outperforms classical classifiers such as SVM and PCA-based SVM. Keywords-autoencoders; deep learning; hyperspectral; image classification; neural networks; stacked autoencoders", "creator": "Microsoft\u00ae Word 2013"}}}