{"id": "1511.03225", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2015", "title": "Label Efficient Learning by Exploiting Multi-Class Output Codes", "abstract": "we essentially analyze the potentially popular multi - class algorithmic techniques of one - vs - all and error for correcting output - mode codes, roughly and later show the surprising intermediate result notice that how under varying the assumption \u03b4 that they are always successful ( at learning from labeled data ), then and indirectly under an additional mild distributional assumption, we can learn from similarly unlabeled variables data ( up sum to roughly a lower permutation set of among the labels ). the consistent key argument point above is that in single cases where they ideally work, integrating these techniques should implicitly simultaneously assume similar structure on how dramatically the classes are related. we show how to exploit this relationship both in the case where ideally the codewords are potentially well fourier separated ( presumably which includes for the one - vs - dos all case ) 1 and in the case where \u03c9 the code matrix has the property note that each identity bit of the required codewords is important for distinguishing at own least easily one sub class key from typically impossible inputs.", "histories": [["v1", "Tue, 10 Nov 2015 18:50:03 GMT  (179kb,D)", "https://arxiv.org/abs/1511.03225v1", null], ["v2", "Mon, 22 Feb 2016 02:24:20 GMT  (296kb,D)", "http://arxiv.org/abs/1511.03225v2", null], ["v3", "Fri, 29 Jul 2016 03:58:08 GMT  (272kb,D)", "http://arxiv.org/abs/1511.03225v3", null], ["v4", "Fri, 25 Nov 2016 15:52:08 GMT  (251kb,D)", "http://arxiv.org/abs/1511.03225v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maria-florina balcan", "travis dick", "yishay mansour"], "accepted": true, "id": "1511.03225"}, "pdf": {"name": "1511.03225.pdf", "metadata": {"source": "CRF", "title": "Label Efficient Learning by Exploiting Multi-class Output Codes", "authors": ["Maria Florina Balcan", "Travis Dick", "Yishay Mansour"], "emails": ["ninamf@cs.cmu.edu", "tdick@cs.cmu.edu", "mansour@tau.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Motivation: Large scale multi-class learning problems with an abundance of unlabeled data are ubiquitous in modern machine learning. For example, an in-home assistive robot needs to learn to recognize common household objects, familiar faces, facial expressions, gestures, and so on in order to be useful. Such a robot can acquire large amounts of unlabeled training data simply by observing its surroundings, but it would be prohibitively time consuming (and frustrating) to ask its owner to annotate any significant portion of this raw data. More generally, in many modern learning problems we often have easy and cheap access to large quantities of unlabeled training data (e.g., on the internet) but obtaining high-quality labeled examples is relatively expensive. More examples include text understanding, recommendation systems, or wearable computing [Thrun, 1996, Thrun and Mitchell, 1995b,a, Mitchell et al., 2015]. The scarcity of labeled data is especially pronounced in problems with many classes, since supervised learning algorithms typically require labeled examples from every class. In such settings, algorithms should strive to make the best use of unlabeled data in order to minimize the need for expensive labeled examples. Overview: We approach label-efficient learning by making the implicit assumptions of popular multi-class learning algorithms explicit and showing that they can also be exploited when learning from limited labeled data. We focus on a family of techniques called output codes that work by decomposing a given multi-class problem into a collection of binary classification tasks [Mohri et al., 2012, Dietterich and Bakiri, 1995, Langford and Beygelzimer, 2005, Beygelzimer et al., 2009]. The novelty of our results is to show that the existence of various low-error output codes constrains the distribution of unlabeled data in ways that can be exploited to reduce the label complexity of learning. We consider both the consistent setting, where the output code achieves zero error, and the agnostic setting, where the goal is to compete with the best output code. The most well known output code technique is one-vs-all learning, where we learn one binary classifier for distinguishing each class from the union of the rest. When output codes are successful at learning from labeled data, it often implies geometric structure in the underlying problem. For example, if it is possible to learn an accurate one-vs-all classifier with linear separators, it implies that no three classes can be collinear, since then it would be impossible for a single linear separator to distinguish the middle class from the union of the others. In this work exploit this implicitly assumed structure to design label-efficient algorithms for the commonly assumed cases of one-vs-all and error correcting output codes, as well as a novel boundary features condition that captures the intuition that every bit of the codewords should be significant.\nar X\niv :1\n51 1.\n03 22\n5v 4\n[ cs\n.L G\n] 2\n5 N\nov 2\n01 6\nOur results: Before discussing our results, we briefly review the output code methodology. For a problem with L classes, a domain expert designs a code matrix C \u2208 {\u00b11}L\u00d7m where each column partitions the classes into two meaningful groups. The number of columns m is chosen by the domain expert. For example, when recognizing household objects we could use the following true/false questions to define the partitions: \u201cis it made of wood?\u201d, \u201cis it sharp?\u201d, \u201cdoes it have legs?\u201d, \u201cshould I sit on it?\u201d, and so on. Each row of the code matrix describes one of the classes in terms of these partitions (or semantic features). For example, the class \u201ctable\u201d could be described by the vector (+1,\u22121,+1,\u22121), which is called the class\u2019 codeword. Once the code matrix has been designed, we train an output code by learning a binary classifier for each of the binary partitions (e.g., predicting whether an object is made of wood or not). To predict the class of a new example, we predict its codeword in {\u00b11}m and output the class with the nearest codeword under the Hamming distance. Two popular special cases of output codes are one-vs-all learning, where C is the identity matrix (with -1 in the off-diagonal entries), and error correcting output codes, where the Hamming distance between the codewords is large.\nIn each of our results we assume that there exists a consistent or low-error linear output code classifier and we impose constraints on the code matrix and the distribution that generates the data. We present algorithms and analysis techniques for a wide range of different conditions on the code matrix and data distribution to showcase the variety of implicit structures that can be exploited. For the code matrix, we consider the case when the codewords are well separated (i.e., the output code is error correcting), the case of one-vs-all (where the code matrix is the identity), and a natural boundary features condition. These conditions can loosely be compared in terms of the Hamming distance between codewords. In the case of error correcting output codes, the distance between codewords is large (at least d+ 1 when the data is d-dimensional), in one-vs-all the distance is always exactly 2, and finally in the boundary features condition the distance can be as small as 1. In the latter cases, the lower Hamming distance requirement is balanced by other structure in the code matrix. For the distribution, we either assume that the data density function satisfies a thick level set condition or that the density is upper and lower bounded on its support. Both regularity conditions are used to ensure that the geometric structure implied by the consistent output code will be recoverable based on a sample of data. Error correcting output codes: We first showcase how to exploit the implicit structure assumed by the commonly used and natural case of linear output codes where the Hamming distance between codewords is large. In practice, output codes are designed to have this property in order to be robust to prediction errors for the binary classification tasks [Dietterich and Bakiri, 1995]. We suppose that the output code makes at most \u03b2 errors when predicting codewords and has codewords with Hamming distance at least 2\u03b2 + d+ 1 in a d-dimensional problem. The key insight is that when the code words are well separated, this implies that points belonging to different classes must be geometrically separated as well. This suggests that tight clusters of data will be label-homogeneous, so we should be able to learn an accurate classifier using only a small number of label queries per cluster. The main technical challenge is to show that our clustering algorithm will not produce too many clusters (in order to keep the label complexity controlled), and that with high probability, a new sample from the distribution will have the same label as its nearest cluster. We show that when the data density satisfies a thick-level set condition (requiring that its level sets do not have bridges or cusps that are too thin), then a single-linkage clustering algorithm can be used to recover a small number of label-homogeneous clusters. One-vs-all: Next, we consider the classic one-vs-all setting for data in the unit ball. This is an interesting setting because of the popularity of one-vs-all classification and because it significantly relaxes the assumption that the codewords are well separated (in a one-vs-all classifier, the Hamming distance between codewords is exactly 2). The main challenge in this setting is that there need not be a margin between classes and a simple single-linkage style clustering might group multiple classes into the same cluster. To overcome this challenge, we show that the classes are probabilistically separated in the following sense: after projecting onto the surface of the unit ball, the level sets of the projected density are label-homogeneous. Equivalently, the high-density regions belonging to different classes must be separated by low-density regions. We exploit this structure by estimating the connected components of the level set using a robust single-linkage clustering algorithm. The boundary features condition: Finally, we introduce an interesting and natural condition on the code matrix capturing the intuition that every binary learning task should be significant. This condition has the weakest separation requirement, allowing the codewords to have a Hamming distance of only 1. This setting is our most challenging, since it allows for the classes to be very well connected to one another, which prevents clustering or level set estimation from being used to find a small number of label-homogeneous clusters. Nevertheless, we show that the implicit geometric\nstructure implied by the output code can be exploited to learn using a small number of label queries. In this case, rather than clustering the unlabeled sample, we apply a novel hyperplane-detection algorithm that uses the absence of data to learn local information about the boundaries between classes. We then use the implicit structure of the output code to extend these local boundaries into a globally accurate prediction rule. Agnostic Setting: Finally, we show that our results for the error correcting, one-vs-all, and boundary features cases can all be extended to an agnostic learning setting, where we do not assume that there exists a consistent output code classifier.\nOur results show an interesting trend: when linear output codes are able to learn from labeled data, it is possible to exploit the same underlying structure in the problem to learn using a small number of label requests. Our results hold under several natural assumptions on the output code and general conditions on the data distribution, and employ both clustering and hyperplane detection strategies to reduce the label complexity of learning."}, {"heading": "2 Related Work", "text": "Reduction to binary classification is one of the most widely used techniques in applied machine learning for attacking multi-class problems. Indeed, the one-vs-all, one-vs-one, and the error correcting output code approaches [Dietterich and Bakiri, 1995] all follow this structure [Mohri et al., 2012, Langford and Beygelzimer, 2005, Beygelzimer et al., 2009, Daniely et al., 2012, Allwein et al., 2000].\nThere is no prior work providing error bounds for output codes using unlabeled data and interaction. There has been a long line of work for providing provable bounds for semi-supervised learning [Balcan et al., 2004, Balcan and Blum, 2010, Blum and Mitchell, 1998, Chapelle et al., 2010] and active learning [Balcan et al., 2006, Dasgupta, 2011, Balcan and Urner, 2015, Hanneke, 2014]. These works provide bounds on the benefits of unlabeled data and interaction for significantly different semi-supervised and active learning methods that are based different assumptions, often focusing on binary classification, thus the results are largely incomparable. Another line of recent work considers the multi-class setting and uses unlabeled data to consistently estimate the risk of classifiers when the data is generated from a known family of models [Donmez et al., 2010, Balasubramanian et al., 2011a,b]. Their results do not immediately imply learning algorithms and they consider generative assumptions, while in contrast our work explicitly designs learning algorithms under commonly used discriminative assumptions.\nAnother work related to ours is that of Balcan et al. [2013], where labels are recovered from unlabeled data. The main tool that they use, in order to recover the labels, is the assumption that there are multiple views and an underlying ontology that are known, and restrict the possible labeling. Similarly, Steinhardt and Liang [2016] show how to use the method of moments to estimate the risk of a model from unlabeled data under the assumption that the data has three independent views. Our work is more widely applicable, since it applies when we have only a single view.\nThe output-code formalism is also used by Palatucci et al. [2009] for the purpose of zero shot learning. They demonstrate that it is possible to exploit the semantic relationships encoded in the code matrix to learn a classifier from labeled data that can predict accurately even classes that did not appear in the training set. These techniques make very similar assumptions to our work but require that the code matrix C is known and the problem that they solve is different."}, {"heading": "3 Preliminaries", "text": "We consider multiclass learning problems over an instance space X \u2282 Rd where each point is labeled by f\u2217 : X \u2192 {1, . . . , L} to one out of L classes and the probability of observing each outcome x \u2208 X is determined by a data distribution P on X . The density function of P is denoted by p : X \u2192 [0,\u221e). In all of our results we assume that there exists a consistent (but unknown) linear output-code classifier defined by a code matrix C \u2208 {\u00b11}L\u00d7m and m linear separators h1, . . . , hm. We denote class i\u2019s code word by Ci and define h(x) = (sign(h1(x)), . . . , sign(hm(x))) to be the predicted code word for point x. We let dHam(c, c\u2032) denote the Hamming distance between any codewords c, c\u2032 \u2208 {\u00b11}m. Finally, to simplify notation, we assume that the diameter of X is at most 1.\nOur goal is to learn a hypothesis f\u0302 : X \u2192 {1, . . . , L} minimizing errP (f\u0302) = PrX\u223cP (f\u0302(x) 6= f(x)) from an unlabeled sample drawn from the data distribution P together with a small set of actively queried labeled examples.\nFinally, we use the following notation throughout the paper: For any set A in a metric space (X , d), the \u03c3-interior of A is the set int\u03c3(A) = {x \u2208 A : B(x, \u03c3) \u2282 A}. The notation O\u0303(\u00b7) suppresses logarithmic terms."}, {"heading": "4 Error Correcting Output Codes", "text": "We first consider the implicit structure when there exists a consistent linear error correcting output code classifier:\nAssumption 1. There exists a code matrix C \u2208 {\u00b11}L\u00d7m and linear functions h1, . . . , hm such that: (1) there exists \u03b2 \u2265 0 such that any point x from class y satisfies dHam(h(x), Cy) \u2264 \u03b2, (2) The Hamming distance between the codewords of C is at least 2\u03b2 + d+ 1; and (3) at most d of the separators h1, . . . , hm intersect at any point.\nPart (1) of this condition is a bound on the number of linear separators that can make a mistake when the output code predicts the codeword of a new example, part (2) formalizes the requirement of having well separated codewords, and part (3) requires that the hyperplanes be in general position, which is a very mild condition that can be satisfied by adding an arbitrarily small perturbation to the linear separators.\nDespite being very natural, Assumption 1 conveniently implies that there exists a distance g > 0 such that any points that f\u2217 assigns to different classes must be at least distance g apart. To see this, fix any pair of points x and x\u2032 with f\u2217(x) 6= f\u2217(x\u2032). By the triangle inequality, we have that dHam(h(x), h(x\u2032)) \u2265 d+ 1, implying that the line segment [x, x\u2032] crosses at least d + 1 of the linear separators. Since only d linear separators can intersect at a point, the line segment must have non-zero length. Applying this argument to the closest pair of points between all pairs of classes and taking the minimum length gives the result. A formal proof is given Section 9 of the Appendix.\nLemma 1. Under Assumption 1, there exists g > 0 s.t. if points x and x\u2032 belong to different classes, then \u2016x\u2212 x\u2032\u2016 > g.\nLemma 1 suggests that we should be able to reduce the label complexity of learning by clustering the data and querying the label of each cluster, since nearby points must belong to the same class. If we use a single-linkage style clustering algorithm that merges clusters whenever their distance is smaller than g, we are guaranteed that the clusters will be label-homogeneous, and therefore we can recover nearly all of the labels by querying one label from the largest clusters. See Algorithm 1 for pseudocode.\nInput: Sample S = {x1, . . . , xn}, radius rc > 0, target error > 0 1. Let {A\u03021}Ni=1 be the connected components of the graph G with vertex set S and an edge between xi and xj if \u2016xi \u2212 xj\u2016 \u2264 rc. 2. In decreasing order of size, query the label of each A\u0302i until \u2264 4n points belong to unlabeled clusters. 3. Output f\u0302(x) = label of nearest labeled cluster to x.\nAlgorithm 1: Single-linkage learning.\nIn order to get a meaningful reduction in label complexity, we need to ensure that when we cluster a sample of data, most of the samples will belong to a small number of clusters. For this purpose, we borrow the following very general and interesting thick level set condition from Steinwart [2015]: a density function p has C-thick level sets if there exists a level \u03bb0 > 0 and a radius \u03c30 > 0 such that for every level \u03bb \u2264 \u03bb0 and radius \u03c3 < \u03c30, (1) the \u03c3-interior of {p \u2265 \u03bb} is non-empty and (2) every point in {p \u2265 \u03bb} is at most distance C\u03c3 from the \u03c3-interior. This condition elegantly characterizes a large family of distributions for which single-linkage style clustering algorithms succeed at recovering the high-density clusters and only rules out distributions whose level sets have bridges or cusps that are too thin. The thickness parameter C measures how pointed the boundary of the level sets of p can be. For example, in Rd if the level set of p is a ball then C = 1, while if the level set is a cube, then C = \u221a d.\nUsing the thick level set condition to guarantee that our clustering algorithm will not subdivide the high-density clusters of p, we obtain the following result for Algorithm 1\nTheorem 1. Suppose that Assumption 1 holds and that the data distribution has C-thick level sets. For any target error > 0, let N be the number of connected components of {p \u2265 /(2 Vol(K))}. With probability at least 1\u2212 \u03b4, running Algorithm 1 with parameter rc < g on an unlabeled sample of size n = O\u0303( 1 2 ((4C)\n2ddd+1/r2dc +N)) will query at most N labels and output a classifier with error at most .\nProof. For convenience, define \u03c3 = rc/(4C) and \u03bb = /(2 Vol(K)). Using a standard VC-bound [Vapnik and Chervonenkis, 1971] together with the fact that balls have VC-dimension d + 1, for n = O((4C)2ddd+1/( 2r2dc )) guarantees that with probability at least 1\u2212 \u03b4/2 the following holds simultaneously for every center x \u2208 Rd and radius r \u2265 0: \u2223\u2223\u2223\u2223|B(x, r) \u2229 S|/n\u2212 P (B(x, r)) \u2223\u2223\u2223\u2223 \u2264 1\n2 \u03bb\u03c3dvd, (1)\nwhere vd denotes the volume of the unit ball in Rd. Assume that this high probability event occurs. We first show that the sample S forms a 2C\u03c3-covering of the set {p \u2265 \u03bb}; that is, for every x \u2208 {p \u2265 \u03bb} we have d(x, S) \u2264 2C\u03c3. Let x be any point in {p \u2265 \u03bb}. Since p has C-thick level sets, we know that there exists a point y \u2208 int\u03c3({p \u2265 \u03bb}) such that \u2016x\u2212 y\u2016 \u2264 C\u03c3. Moreover, the ball B(y, \u03c3) is contained in {p \u2265 \u03bb}, which implies that it has probability mass at least \u03bb\u03c3dvd and by (1) we have that |B(y, r) \u2229 S|/n \u2265 12\u03bb\u03c3dvd > 0, so there must exist a point z \u2208 S \u2229 B(y, \u03c3). Now we have that d(x, S) \u2264 \u2016x \u2212 z\u2016 \u2264 \u2016x \u2212 y\u2016 + \u2016y \u2212 z\u2016 \u2264 C\u03c3 + \u03c3 \u2264 2C\u03c3, where the final inequality follows from the fact that C \u2265 1.\nNow let A1, . . . , AN be the N connected components of {p \u2265 \u03bb}. We will argue that for each i \u2208 [N ], there exists a unique cluster output by step 1 of the algorithm, say A\u0302i, such that A\u0302i contains Ai \u2229 S and for any point x \u2208 Ai, the closest output cluster is A\u0302i.\nTo see that A\u0302i contains Ai\u2229S, consider any pair of points x and x\u2032 in Ai\u2229S. Since Ai is connected, we know there is a path \u03c0 : [0, 1] \u2192 Ai such that \u03c0(0) = x and \u03c0(1) = x\u2032. Since the sample set X is a 2C\u03c3 covering of {p \u2265 \u03bb}, it is also a 2C\u03c3-covering of Ai, which implies that we can find a sequence of points y1, . . . , yM \u2208 X (possibly with repetition) such that the path \u03c0 passes through the balls B(y1, 2C\u03c3), . . . , B(yM , 2C\u03c3) in order. Since consecutive balls must touch at the point that the path \u03c0 crosses from one ball to the next, we know that \u2016yi \u2212 yi+1\u2016 \u2264 4C\u03c3 = rc, and therefore the path x\u2192 y1 \u2192 \u00b7 \u00b7 \u00b7 \u2192 yM \u2192 x\u2032 is a path in the graph G connecting x and x\u2032.\nNow consider any point x \u2208 Ai. We argued above that there exists a sample point z \u2208 A\u0302i that was within distance 2C\u03c3 from x. Now let z\u2217 be the closest sample in X to x. Then we know that \u2016x \u2212 z\u2217\u2016 \u2264 \u2016x \u2212 z\u2016 \u2264 2C\u03c3. By the triangle inequality, we have that d\u2016z \u2212 z\u2217\u2016 \u2264 \u2016z \u2212 x\u2016+ \u2016x\u2212 z\u2217\u2016 \u2264 4C\u03c3 \u2264 rc, and therefore z and z\u2217 are connected in the graph G. Since z belongs to A\u0302i, it follows that z\u2217 does too, and therefore the closest cluster to x is A\u0302i.\nIt remains to bound the error of the resulting classification rule. Since there is a margin of width g > 0 separating the classes, we know that every connected component of {p \u2265 \u03bb} must contain points belonging to exactly one class. Moreover, since we ran the algorithm with connection radius rc < g, we know that the clusters output by step 1 will contain points belonging to exactly one class. It follows that if we query the label of any point in the cluster A\u0302i then the algorithm will not error on any test point in Ai. Say that one of the connected components Ai is labeled if we query the label of the corresponding cluster A\u0302i.\nApplying Hoeffding\u2019s inequality and the union bound to all possible 2N unions of the sets A1, . . . , AN , our value of n guarantees that with probability at least 1 \u2212 \u03b4/2, the following holds simultaneously for all subsets of indices I \u2282 [N ]: \u2223\u2223\u2223\u2223 \u2223\u2223S \u2229 (\u22c3\ni\u2208I Ai )\u2223\u2223/n\u2212 P ( \u22c3 i\u2208I Ai) \u2223\u2223\u2223\u2223 \u2264 4 .\nSince the algorithm queries labels until at most 4n points belong to unlabeled clusters, we know that the number of samples belonging to the unlabeled Ai sets is at most 4n. By the above uniform convergence, it follows that their total probability mass is at most /2. Finally, since the algorithm only errors on test points in {p \u2264 \u03bb}, which has probability mass at most /2 or on unlabeled Ai sets, the error of the resulting classifier is at most .\nThe exponential dependence on the dimension in Theorem 1 is needed to ensure the sample S will be a fine covering of the level set of p w.h.p, which guarantees that Algorithm 1 will not subdivide its connected components into smaller clusters. When the data has low intrinsic dimensionality, the unlabeled sample complexity is only exponential in the intrinsic dimension. The following result shows that under the common assumption that the distribution is a doubling\nmeasure, then the unlabeled sample complexity is exponential only in the doubling dimension. Recall that a probability measure P is said to have doubling dimension D if for every point x in the support of P and every radius r > 0, we have that P (B(x, 2r)) \u2264 2DP (B(x, r)) (see, for example, [Dasgupta and Sinha, 2013]).\nTheorem 2. Suppose that Assumption 1 holds the data distribution P has doubling dimension D, and the support of P has N connected components. With probability at least 1\u2212 \u03b4, running Algorithm 1 with parameter rc < g on a sample of size n = O\u0303 ( d/r2Dc +N/ 2 ) will query at most N labels and have error at most .\nProof. Let x be any point in the support of P . Since we assumed that the diameter of X is 1, we know that X \u2282 B(x, 1) and therefore P (B(x, 1)) = 1. Applying the doubling condition lg(r) times, it follows that for any radius r > 0 we have that P (B(x, r)) \u2265 r\u2212D.\nAs in the proof of Theorem 1, for our choice of n the following holds with probability at least 1\u2212 \u03b4/2 uniformly for every center x in X and radius r \u2265 0:\n\u2223\u2223|B(x, r) \u2229 S|/n\u2212 P (B(x, r)) \u2223\u2223 \u2264 1\n2 r\u2212D.\nAssume this high probability event occurs. Since every ball of radius r centered at a point in the support of P has mass at least r\u2212D, each such ball must contain at least one sample point and it follows that the sample S forms an r\u2212D-covering of the support of P .\nThe rest of the proof now follows identically the proof of Theorem 1 with the A1, . . . , AN sets being the connected components of the support of P , since each connected component must be label-homogeneous.\nThe unlabeled sample complexity in Theorem 1 depends on the gap g between classes because we must have rc < g. Such a scale parameter must appear in our results, since Assumption 1 is scale-invariant, yet our algorithm exploits scale-dependent geometric properties of the problem. If we have a conservatively small estimate g\u0302 \u2264 g, then the conclusion of Theorem 1 and Theorem 2 continue to hold if the connection radius and unlabeled sample complexity are set using the estimate g\u0302. Nevertheless, in some cases we may not have an estimate of g, making it difficult to to apply Algorithm 1. The following result shows that if we have an estimate of the number of high-density clusters, and these clusters have roughly balanced probability mass, then we are still able to take advantage of the geometric structure even when the distance g is unknown. The idea is to construct a hierarchical clustering of S using single linkage, and then to use a small number of label queries to find a good pruning.\nInput: Sample S = {x1, . . . , xn}, t \u2208 N. 1. Let T be the hierarchical clustering of S obtained by single-linkage. 2. Query the labels of a random subset of S of size t. 3. Let {B\u0302i}Mi=1 be the coarsest pruning of T such that each B\u0302i contains labels from one class. 4. Output f\u0302(x) = label of nearest B\u0302i to x.\nAlgorithm 2: Hierarchical single-linkage learning.\nTheorem 3. Suppose Assumption 1 holds and the density p has C-thick level sets. For any 0 < \u2264 1/2, suppose that {Ai}Ni=1 are the connected components of {p \u2265 /(2 Vol(K))} and for some \u03b1 \u2265 1 we have P (Ai) \u2264 \u03b1P (Aj) for all i, j. With probability \u2265 1 \u2212 \u03b4, running Algorithm 2 with t = O\u0303(\u03b1N) on an unlabeled sample of size n = O\u0303( 1 2 (C 2ddd+1/g2d +N)) will have error \u2264 .\nProof. Define \u03bb = /(2 Vol(K)) and let A1, . . . , AN be the connected components of {p \u2265 \u03bb}. Suppose that each Ai set has probability mass at least \u03b3. Under the assumption that the probability mass of the largest Ai is at most \u03b1 times the mass of the smallest, we have that \u03b3 \u2265 (1\u2212 )/(\u03b1N), but the result holds for any arbitrary lower bound \u03b3.\nSince we query the labels of points without replacement, the set of labeled examples is an iid sample from the data density p. Whenever m \u2265 2\u03b3 ln 2N\u03b4 , with probability at least 1 \u2212 \u03b4/2, every set Ai will contain at least one labeled example, since they each have probability mass at least \u03b3. Assume this high probability event holds.\nLet g be the margin between classes that is guaranteed by Lemma 1. Whenever samples x, x\u2032 \u2208 S have \u2016x\u2212x\u2032\u2016 \u2264 g, they must belong to the same cluster B\u0302i. Applying an identical covering-style argument as in Theorem 1, we have that with probability at least 1\u2212 \u03b4/2, for every Ai set there is a cluster, say B\u0302i, such that:\n1. All samples in Ai \u2229 S are contained in B\u0302i.\n2. For every x \u2208 Ai, the nearest cluster to x is B\u0302i.\nSince every Ai set contains at least one labeled example, it follows that whenever two of these high-density clusters belong to different classes, they will contain differently labeled points and therefore will not have been merged by Algorithm 2. It follows that the label of B\u0302i must agree with the label of Ai. At this point, the error analysis follows identically as in Theorem 1.\nIn Section 7 we describe a meta-argument that can be used to extend our results into the agnostic setting, where we no longer require that the output code is consistent. Details for the error correcting case are given in Section 9.\nIn this section we showed that when there exists linear error correcting correcting output code with low error, then it is possible to reduce the label complexity of learning to the number of high-density clusters, which are the connected components of {p \u2265 }. The label complexity of our algorithms is always linear in the number of high density clusters, while the worst-case unlabeled complexity of our algorithms is exponential in the dimension (or intrinsic dimension)."}, {"heading": "5 One-Versus-All on the Unit Ball", "text": "In this section we show that even when the codewords are not well separated, we can still exploit the implicit structure of output codes to reduce the label complexity of learning by clustering the data. Specifically, we consider the implicit structure of a linear one-vs-all classifier over the unit ball:\nAssumption 2. The instance space X there exist L linear separators h1, . . . , hL such that: (1) point x belongs to class i iff hi(x) > 0, and (2) for all i, hi(x) = w>i x\u2212 bi with \u2016wi\u2016 = 1 and bi \u2265 bmin > 0.\nSee Figure 1 for an example problem satisfying this condition. Since a one-vs-all classifier is an output code where the code matrix is the identity, the Hamming distance between any pair of codewords is exactly 2. Therefore, in this setting we do not have a result similar to Lemma 1 to ensure that the classes are geometrically separated. Instead, we exploit the one-vs-all structure to show the classes are probabilistically separated and employ a robust clustering algorithm.\nAs before, we study this problem under a mild constraint on the data distribution. For each class i denote the set of points in class i by Ki = {x : \u2016x\u2016 \u2264 1, hi(x) > 0} and K = \u22c3L i=1Ki. In this section, we assume that the density p is supported on K with upper and lower bounds:\nAssumption 3. There exist constants 0 < clb \u2264 cub s.t. for x \u2208 K we have clb \u2264 p(x) \u2264 cub and otherwise p(x) = 0.\nThis distributional constraint is quite general: it only requires that we will not observe examples for which the one-vs-all classifier would be confused (i.e., where none of its linear separators claim the point) and that the density does not take extreme values. When K is compact, every continuous density supported on K satisfies Assumption 3.\nOur algorithm for this setting first projects the data onto the unit sphere Sd\u22121 = {x \u2208 Rd : \u2016x\u2016 = 1} and then applies a robust clustering algorithm to the projected data. The projection does not introduce any errors, since the label\nof an example is independent of its distance to the origin. This is because each linear separator carves out a spherical cap for its class, and no two class caps overlap. Since we assume that no class contains the origin, it follows that an examples label depends only on its projection to the sphere. We show that projecting to the sphere has the useful property that the projected density goes to zero at the boundary of the classes, which suggests that we can use a robust single-linkage style clustering algorithm to find label-homoegeneous clusters. Algorithm 3 gives pseudocode, using the notation \u03b8(u, v) = arccos(u>v) for the angle between u and v and V d(r) is the probability that a uniformly random sample from Sd\u22121 lands in a given spherical cap of angular radius r.\nInput: Sample S = {x1, . . . , xn}, radius rc > 0. 1. Define ra = rc/2 and \u03c4 = clb2cubV\nd(ra) . 2. Let vi = xi\u2016xi\u2016 be the projection of xi to the sphere. 3. Mark vi active if |{vj : \u03b8(vi, vj) \u2264 ra}| \u2265 \u03c4n and inactive otherwise for i \u2208 [n]. 4. Let A\u03021, . . . , A\u0302N be the connected components of the graph G whose vertices are the active vi with an edge\nbetween vi and vj if \u03b8(vi, vj) < rc. 5. In decreasing order of size, query the label of each A\u0302i until \u2264 4n points belong to unlabeled clusters. 6. Output f\u0302(x) = label of nearest cluster to x/\u2016x\u2016.\nAlgorithm 3: Robust single-linkage learning.\nOur first result characterizes the density of the projected data (defined relative to the uniform distribution on Sd\u22121).\nLemma 2. Suppose Assumptions 2 and 3 hold and let q : Sd\u22121 \u2192 [0,\u221e) be the density function of the data projected onto the unit sphere. Then qlb(v) \u2264 q(v) \u2264 qub(v), where\nqlb(v) = { clbdvd(1\u2212 (bi/w>i v)d) if v \u2208 Ki 0 otherwise,\nand qub(v) = cub/clb \u00b7 qlb(v), where vd is the volume of the unit ball in d dimensions.\nProof. Let X \u223c p be and set V = X/\u2016X\u20162 so that V is a sample from q. For any set A \u2282 Sd\u22121, we know that Pr(V \u2208 A) = Pr(X \u2208 cone(A)), where cone(A) = {rv : r > 0, v \u2208 A}, which gives\nPr(V \u2208 A) = Pr(X \u2208 cone(A)) = \u222b\nx\u2208cone(A) p(x) dx =\n\u222b\nv\u2208A dvd\n\u222b \u221e\nr=0\np(rv)rd\u22121 dr d\u00b5\u25e6(v),\nwhere the last inequality follows by a change of variables x to (r, v) where r = \u2016x\u20162 and v = x/\u2016x\u20162. The term rd\u22121 is the determinant of the Jacobian of the change of variables, and the term dvd, which is the surface area of Sd\u22121, appears since \u00b5\u25e6 is normalized so that \u00b5\u25e6(Sd\u22121) = 1. From this, it follows that the density function q can be written as\nq(v) = dvd\n\u222b \u221e\nr=0\np(rv)rd\u22121 dr, (2)\nsince integrating this function over any set A gives the probability that V will land in A. From our assumptions on p, we know that\np(rv) \u2265 L\u2211\ni=1\nI{rv \u2208 Ki}clb.\nMoreover, we can rewrite the indicator as I{rv \u2208 Ki} = I{ biw>i v < r \u2264 1}. Substituting this into (2) gives\nq(v) \u2265 L\u2211\ni=1\nclbdvd\n\u222b \u221e\nr=0 I{[} ] bi w>i v < r \u2264 1rd\u22121 dr\n=\nL\u2211\ni=1\nI{v \u2208 Ki}clbdvd \u222b 1\nr=bi/(w>i v)\nrd\u22121 dr\n=\nL\u2211\ni=1\nI{v \u2208 Ki}clbdvd(1\u2212 bi/(w>i v)d)\n= qlb(v)\nNote that the indicator I{v \u2208 Ki} appears in line 2 because the integral is only non-zero when bi/(w>i v) < 1, which is exactly the condition that v \u2208 Ki. The upper bound on q follows by an identical argument using the upper bound on p(rv).\nBoth bounds are defined piece-wise with one piece for each class. Restricted to class i, both the qlb(v) and qub(v) are decreasing functions of \u03b8(wi, v), which implies that their \u03bb-level sets are spherical caps. Therefore, each class contributes one large connected component to the level set of q that is roughly a spherical cap centered at the point wi and the density of q goes to zero at the boundary of each class. Our main result is as follows:\nTheorem 4. Suppose Assumptions 2 and 3 hold and that f\u2217 is consistent. There exists an rc satisfying rc = \u2126( clb/(c 2 ubbmin)) such that with probability at least 1\u2212 \u03b4, running Algorithm 3 with parameter rc on an unlabeled sample of size n = O\u0303((c4ubd/( 2c2lbb 2 min)) d) will query at most L labels and output a classifier with error at most .\nNote that if the scale parameter bmin is unknown, the conclusion of Theorem 4 continues to hold if the connection radius rc and unlabeled sample complexity n are set using a conservatively small estimate b\u0302min satisfying b\u0302min \u2264 bmin. This comes at the cost of an increased unlabeled sample complexity.\nBefore proving Theorem 4, we develop some general results for the robust linkage clustering algorithm. More generally, Algorithm 3 can be applied in any metric space (X , d) by replacing \u03b8 with the distance metric d and suitable settings for the internal parameters ra and \u03c4 . For the robust linkage approach to have low error, each class should have one large connected component in the graph G constructed by the algorithm so that: (1) with high probability a new point in class i will be nearest to that largest component, and (2) the large components of different classes are separated. Intuitively, G will have these properties if each positive region Ki has a connected high-density inner region Ai covering most of its probability mass and when it is rare to observe a point that is close to two or more classes. This notion is formalized below.\nLet S be any set in X . We say that a path \u03c0 : [0, 1]\u2192 X crosses S if the path starts and ends in different connected components of the complement of S in X and we say that the width of S is the length of the shortest path that crosses S. Definition 1. The sets A1, . . . , AL are (rc, ra, \u03c4, \u03b3)-clusterable under probability P if there exists a separating set S of width at least rc such that: (1) Each Ai is connected; (2) If x \u2208 X satisfies d(x,Ai) \u2264 rc/3 then PrX\u223cP (X \u2208 B(x, ra)) > \u03c4 + \u03b3; (3) If x \u2208 Ai then PrX\u223cP (X \u2208 B(x, rc/3)) > \u03b3; (4) Every path from Ai to Aj crosses S; and (5) If x \u2208 S then PrX\u223cP (X \u2208 B(x, ra)) < \u03c4 \u2212 \u03b3.\nNote that typically there must be a gap between the set Ai and the set S in order to satisfy the probability requirements (i.e., the set S will be smaller than X \u2212\u22c3Li=1Ai). The first three properties ensure that each set Ai will have one large connected component and the remaining two properties ensure that these connected components will be disconnected. Following an analysis similar to that of the cluster tree algorithm of Chaudhuri and Dasgupta [2010] gives the following result.\nLemma 3. Suppose that the setsA1, . . . ,AN are (rc, ra, \u03c4, \u03b3)-clusterable with respect to distribution P . For any failure probability \u03b4 > 0, let G be the graph constructed by Algorithm 3 run on a sample S of size O( 1\u03b32 (D + ln 1 \u03b4 ), where D is the VC-dimension of balls in (X , d), with parameters and rc, ra, and \u03c4 . Define Ki = {x \u2208 S : d(x,Ai) \u2264 rc/3} for each i \u2208 [N ]. With probability at least 1\u2212 \u03b4, the graph G has the following properties:\n1. Complete: For each i, all samples in Ki are active and included in the graph G.\n2. Separated: For any i 6= j, there is no path in G from Ki to Kj .\n3. Connected: For every i, the set Ki is connected in G.\n4. Extendible: For any point x \u2208 Ai, the nearest connected component of G to x contains Ki.\nProof. The proof technique used here follows a similar argument as Chaudhuri and Dasgupta [2010]. We use a standard VC bound [Vapnik and Chervonenkis, 1971] to relate the probability constraints in the clusterability definition to the empirical measure P\u0302 . For our value of n we have\nPr ( sup x,r \u2223\u2223P\u0302 (B(x, r))\u2212 P (B(x, r)) \u2223\u2223 > \u03b3 ) < \u03b4.\nThis implies that with probability at least 1 \u2212 \u03b4 for all points x we have: (1) if d(x,Ai) \u2264 rc3 for any i then P\u0302 (B(x, ra)) > \u03c4 ; (2) if x \u2208 S then P\u0302 (B(x, ra)) < \u03c4 ; and (3) if x \u2208 Ai for any i then P\u0302 (B(x, rc3 )) > 0. We now use these facts to prove that the graph G has the completeness, separation, and connectedness properties.\nCompleteness follows from the fact that every sample x \u2208 K\u0302i is within distance rc/3 of Ai and therefore P\u0302 (B(x, ra)) > \u03c4 .\nTo show separation, first observe that every sample z \u2208 S that belongs to S will be marked as inactive, since P\u0302 (B(z, ra)) < \u03c4 . Now let x \u2208 K\u0302i and x\u2032 \u2208 K\u0302j for i 6= j. Since the graph G does not contain any samples in the set S, any path in G from x to x\u2032 must have one edge that crosses S. Since the width of S is at least rc, this edge would not be included in the graph G, and therefore G does not include a path from x to x\u2032.\nTo show connectedness, let x and x\u2032 be any pair of samples in K\u0302i and let v and v\u2032 be their nearest points in Ai, respectively. By definition of K\u0302i, we know that d(x, v) < rc/3 and d(x\u2032, v\u2032) < rc/3. Since Ai is a connected set, there is a path \u03c0 : [0, 1] \u2192 Ai in Ai starting at v and ending at v\u2032. Cover the path \u03c0 with a sequence of points z1, . . . , zk such that d(zj , zj+1) < rc/3 for all j and the path \u03c0 is covered by the balls B(zj , rc/3). Further, choose z1 = v and zk = v\n\u2032. Since each point zj belongs to Ai, the empirical probability mass of the ball B(zj , rc/3) is non-zero, which implies that it must contain at least one sample point, say yj \u2208 S. We may take y1 = x and yk = x\u2032. Since every sample y1, . . . , yk is within distance rc/3 of Ai, they are all active and included in the graph G. Moreover, since d(yj , yj+1) < rc, we have that the path x = y1 \u2192 \u00b7 \u00b7 \u00b7 \u2192 yk = x\u2032 is a path connecting x and x\u2032 in G, as required.\nFinally to show extensibility, let x \u2208 Ai be any point. By the uniform convergence for balls, P (x, rc/3) has nonzero empirical probability mass and therefore contains at least one active sample, say z. Since z is within distance rc/3 of Ai, it belongs to the set Ki. Now let z\u2217 be the closest active sample to x. We must have d(x, z\u2217) \u2264 d(x, z) \u2264 rc/3 and it follows that d(z, z\u2217) \u2264 d(z, x) + d(x, z\u2217) \u2264 2rc/3 < rc. Therefore, z\u2217 also belongs to Ki, as required.\nWe now prove Theorem 4 by combining Lemmas 2 and 3:\nProof of Theorem 4. For each class i \u2208 [L], define Ai = {q(i)ub \u2265 }. We will show that the sets A1, . . . , AL are (rc, ra, \u03b3, \u03c4)-clusterable for appropriate choices of the parameters. Then Lemma 3 will guarantee that with high probability, the clustering produced by Algorithm 3 will approximate the connected components of the -level of {qub \u2265 }.\nRecall that for each class i \u2208 [L], the sets {q(i)ub \u2265 } and {q (i) lb \u2265 } are spherical caps. To simplify notation, let C(u, r) = {v \u2208 Sd\u22121 : \u03b8(v, u) \u2264 r} denote the spherical cap of angular radius r centered at u. Let \u03c1(i)ub(\u03bb) = arccos(bi(1 \u2212 \u03bb/(cubdvd))\u22121/d) denote the angular radius of {q(i)ub \u2265 }, so that {q (i) ub \u2265 } = C(wi, \u03c1 (i) ub( )), and \u03c1 (i) lb (\u03bb), defined similarly, be the angular radius of {q (i) lb \u2265 }. Define \u0303 = clbcub and suppose for the moment that we can find an activation radius ra > 0 small enough so that the following inequalities hold for all classes i = 1, . . . , L:\n5 3 ra \u2264 \u03c1(i)lb (3\u0303 4 ) \u2212\u03c1(i)ub( ) and 2ra \u2264 \u03c1 (i) ub(0)\u2212 \u03c1 (i) ub ( \u0303 4 ) .\nGiven such an activation radius, we will show that the sets A1, . . . , AL are (rc, ra, \u03c4, \u03b3)-clusterable with rc = 2ra, \u03c4 = \u0303V\nd(ra) 2 , and \u03b3 = \u0303V d(rc/3) 4 and the separating set is S = {v \u2208 Sd\u22121 : \u03b8(v, wi) \u2265 \u03c1 (i) ub(0)\u2212 ra for all i}:\n1. Connection: Each Ai set is a spherical cap and therefore connected.\n2. High-density near Ai: Let v \u2208 Sd\u22121 be such that \u03b8(v,Ai) < rc/3 and let u \u2208 C(v, ra) be any point in the spherical cap of angular radius ra about v. By the triangle inequality, we know that \u03b8(wi, u) \u2264 \u03b8(wi, v) + \u03b8(v, u) \u2264 \u03c1(i)ub( ) + 53ra \u2264 \u03c1 (i) lb ( 3\u0303 4 ). This implies that q(u) \u2265 3\u03034 for all points in C(v, ra) and therefore\nPrV\u223cq(V \u2208 C(v, ra)) \u2265 4\u03033 V d(ra) \u2265 \u03c4 + \u03b3.\n3. High-density inside Ai: Now let v \u2208 Ai. Since rc/3 < ra, the above arguments show that q(u) \u2265 4\u03033 for all points u \u2208 C(v, rc/3) and therefore PrV\u223cq(V \u2208 C(v, rc/3)) \u2265 4\u03033 V d(rc/3) \u2265 \u03b3.\n4. Separation by the set S: For each class i, the set S contains the annulus {v \u2208 Sd\u22121 : \u03c1(i)ub(0)\u2212 ra \u2264 \u03b8(wi, v) \u2264 \u03c1 (i) ub(0)} which has width ra. Any path from one Ai to another Aj must cross two such annuli, each of width ra,\nso the length of the path crossing S is at least 2ra = rc.\n5. Low density inside S: Finally, let v be any point in the set S and let u \u2208 C(v, ra). For any class i, the reverse triangle inequality gives that \u03b8(wi, v) \u2265 \u03b8(v, wi)\u2212 \u03b8(u,wi) \u2265 \u03c1(i)ub(0)\u2212 2ra \u2265 \u03c1 (i) ub( \u0303 4 ). Since this is true for all\nclasses i, we have q(v) \u2264 \u03034 and therefore PrV\u223cq(V \u2208 C(v, ra)) \u2264 \u03034V d(ra) \u2264 \u03c4 \u2212 \u03b3.\nIt follows that the sets A1, . . . , AL are (rc, ra, \u03c4, \u03b3)-clusterable and it only remains to find an activation radius ra that satisfies the above inequalities. Since the robust linkage algorithm needs to estimate the probability mass of balls to within error \u03b3 = \u0303V\nd(2ra/3) 4 , we want this activation radius to be not too small.\nTaking the first order Taylor expansion of the \u03c1(i)lb and \u03c1 (i) ub functions, we have:\n\u03c1 (i) lb (\u03bb) = arccos(bi)\u2212 bi\u221a 1\u2212 b2i 1 clbdvd \u03bb+O(\u03bb2)\n\u03c1 (i) ub(\u03bb) = arccos(bi)\u2212 bi\u221a 1\u2212 b2i 1 cubdvd \u03bb+O(\u03bb2),\nas \u03bb\u2192 0. Therefore, we have that\n\u03c1 (i) lb (3\u0303/4)\u2212 \u03c1 (i) ub( ) =\n1 4dvdcub \u00b7 bi\u221a 1\u2212 b2i +O( 2)\nand \u03c1 (i) ub(0)\u2212 \u03c1 (i) ub(\u0303/4) =\nclb 4dvdc2ub \u00b7 bi\u221a 1\u2212 b2i +O( 2),\nwhich shows that it is sufficient to set ra = 3clb20dvdc2ub \u00b7 bi\u221a 1\u2212b2i + O( 2) = \u2126( clb c2ub bmin ) as \u2192 0 and it follows that n = O( 1\u03b32 (d+ ln 1 \u03b4 ) = O\u0303((c 4 ubd/( 2c2lbb 2 min))\nd). Finally, we show that the algorithm correctly recovers the labels of the large clusters. For n = O\u0303(L/ 2), we have that\nwith probability at least 1\u2212 \u03b4 the following holds simultaneously for all 2L subsets I \u2282 [L]: \u2223\u2223P\u0302 (AI)\u2212 P (AI) \u2223\u2223 \u2264 /4, where AI = \u22c3 i\u2208I Ai. Since all samples in AI are marked as active (by Lemma 3), this implies that all but at most 4n of the active points will belong to the Ai sets. It follows that if the algorithm queries the labels of the L largest clusters, they will also contain all but 4n active samples.\nOn the other hand, whenever we query the label of one of the Ai sets, we know that we will correctly classify every test point belonging to Ai, so the error of the resulting classifier is at most the probability mass of {qub \u2264 } together with the probability mass of the Ai sets for which we did not query the label. Since the unqueried Ai sets have empirical probability mass at most /4 and we have uniform convergence for all unions of Ai sets to within error /4, it follows that the total probability mass of the unlabeled Ai sets is at most /2 and it follows that the error of the resulting classifier is at most .\nThere are two main differences between the sample complexity of Theorem 4 and the results from Section 4. First, the unlabeled sample complexity now has an \u22122d dependence, rather than only \u22122. This is because the distance between the connected components of {p \u2265 } goes to zero (in the worst case) as \u2192 0, so our algorithm must be able to detect low-density regions of small width. In contrast, Lemma 1 allowed us to establish a non-diminishing gap g > 0 between the classes when the codewords were well separated. On the other hand, the label complexity in this setting is better, scaling with L instead of N , since we are able to establish that each class will have one very large cluster containing nearly all of its data.\nTheorem 7 in the appendix gives an analysis of Algorithm 3 in the agnostic setting of Section 7."}, {"heading": "6 The Boundary Features Condition", "text": "Finally, in this section we introduce a novel condition on the code matrix called the boundary features condition that captures the intuition that every binary classification task should be significant. Assumption 4 formalizes this intuition.\nAssumption 4. There exists a code matrix C \u2208 {\u00b11}L\u00d7m, linear functions h1, . . . , hm, and a scale parameter R > 0 so that: (1) for any point x in class y, we have h(x) = Cy; (2) for each hj , there exists a class i such that negating the jth entry of Ci produces a codeword C \u2032i not in C and there exists a point x on the hyperplane hj = 0 such that every point in B(x,R) has either code word Ci or C \u2032i; and (3) any pair of points x, x\n\u2032 \u2208 X such that h(x) and h(x\u2032) are not codewords in C and h(x) 6= h(x\u2032) must have \u2016x\u2212 x\u2032\u2016 \u2265 R.\nPart (1) of this assumption requires that the output code classifier is consistent, part (2) is a condition that guarantees every linear separator hj separates at least one class i from a region of space that does not belong to any class, and part (3) requires that points with codewords not in the code matrix must either have the same codeword or be separated by distance R. Part (3) allows us to simplify our algorithm and analysis and is trivially satisfied in cases where all points in X that do not belong to any class have the same codeword, as is the case for one-vs-all classification and the problem in Figure 2.\nProblems in this setting are more challenging than those of the previous sections because they may not be amenable to clustering-based learning strategies. Whenever the Hamming distance between a pair of codewords is only 1, this implies that one of the linear separators hj forms a shared boundary between the classes, and therefore these classes may be connected by a large and high-density region. Instead, Assumption 4 guarantees that for every linear separator hj , there is some ball B(x,R) centered on hj that is half-contained in the set of points belonging to some class i and the other half belongs to the set of points that do not belong to any class. Therefore, by looking for hyperplanes that locally separate sample data from empty regions of space, we can recover the linear separator hj from the local absence of data. Define Ki = {x \u2208 X : h(x) = Ci} to be the set of points that belong to class i and K = \u22c3L i=1Ki. Under the condition that the density p is supported on K and is upper and lower bounded, we exploit this structure in an algorithm that directly learns the linear separators h1, . . . , hm.\nOur hyperplane detection algorithm works by searching for balls of radius r whose centers are sample points such that one half of the ball contains very few samples. If a half-ball contains very few sample points then it must be mostly disjoint from the set K. But since its center point belongs to the set K, this means that the hyperplane defining the half-ball is a good approximation to at least one of the true hyperplanes. See Figure 3 for examples of half-balls that would pass and fail this test. The collection H of hyperplanes produced in this way partition the space into cells. Our algorithm queries the labels of the cells containing the most sample points and classifies test points\nInput: Sample S = {x1, . . . , xn}, r > 0, \u03c4 > 0. 1. Initialize set of candidate hyperplanes H = \u2205. 2. For all samples x\u0302 \u2208 S with B(x\u0302, r) \u2282 X :\n(a) Let w\u0302 = argminw\u2208Sd\u22121 |B1/2(x\u0302, r, w) \u2229 S|. (b) If |B1/2(x\u0302, r, w\u0302)\u2229S|/n < \u03c4 , add (x\u0302, w\u0302) to H . 3. Let {C\u0302i}Ni=1 be the partitioning of X induced by H . 4. Query the label of the L cells with the most samples. 5. Output f\u0302(x) = label of Ci containing x.\nAlgorithm 4: Plane-detection algorithm. Figure 3: Examples of half-balls that would be included (green) or excluded (red) by the plane detection algorithm.\nbased on the label of their cell in the partition (and if the label is unknown, we output a random label). Pseudocode is given in Algorithm 4 using the following notation: for any center x \u2208 X , radius r \u2265 0, and direction w \u2208 Sd\u22121, let B1/2(x, r, w) = {y \u2208 B(x, r) : w>(y \u2212 x) > 0} and define p1/2(r) = 12clbrdvd.\nEach candidate hyperplane produced by Algorithm 4 is associated with a half-ball that caused it to be included in H . In fact, we can think of the pairs (x\u0302, w\u0302) in H as either encoding the linear function h\u0302(x) = w>(x\u2212 x\u0302) or the half-ball B1/2(x\u0302, r, w\u0302), where r is the scale parameter of the algorithm. Most of our arguments will deal with the half-balls directly, so we adopt the second interpretation. The analysis of Algorithm 4 has two main steps. First, we show that the face of every half-ball in the set H is a good approximation to at least one of the true hyperplanes, and that every true hyperplane is well approximated by the face of at least one half-ball in H . Second, using the fact that the half-balls in H are good approximations to the true hyperplanes, we argue that the output classifier will only be inconsistent with the true classification rule in a small margin around each of the true linear separators. Then the error of the classification rule is easily bounded by bounding the probability mass of these margins.\nTo measure the approximation quality, we say that the half-ball B1/2 = B1/2(x\u0302, r, w\u0302) is an \u03b1-approximation to the linear function h if PrX\u223cB1/2(sign(h(X)) = sign(h(x\u0302))) \u2264 \u03b1, where PrX\u223cB1/2 denotes the probability when X is sampled uniformly from the half-ball B1/2. The motivation for this definition is as follows: given any point x\u0302 \u2208 X , the half-ball B1/2(x\u0302, r, w\u0302) will be an \u03b1-approximation to hi only if x\u0302 is on one side of the decision surface of hi and all but an \u03b1-fraction of the half-ball\u2019s volume is on the other side. Intuitively, this means that the face of the half-ball must approximate the decision surface of the function hi.\nThe following Lemma shows that when Algorithm 4 is run with appropriate parameters and on a large enough sample drawn from the data distribution, then with high probability the algorithm will include at least one half-ball in H \u03b1-approximating each true hyperplane hi and every half-ball in H will be an \u03b1-approximation to at least one true hyperplane. Recall that p1/2(r) = 12clbr\ndvd is a lower bound on the probability mass of a half-ball of radius r contained in the set K.\nLemma 4. Fix any \u03b1 > 0 and confidence parameter \u03b4 > 0. Let H be the set of half-balls produced by Algorithm 4 when run with parameters r = R/2 and \u03c4 = 12\u03b1p 1/2(r) on a sample of size n = O( 1\u03b32 (ln 2 d \u03b3 + ln 1 \u03b4 )) where \u03b3 = 25\u03c4 = 1 5\u03b1p\n1/2(r). Then with probability at least 1\u2212 \u03b4, every half-ball in H will be an \u03b1-approximation to at least one true hyperplane hi, and every true hyperplane hi will be \u03b1-approximated by at least one half-ball in H .\nProof. Since the VC-dimension of both balls and half-spaces in Rd is d+1, the VC-dimension of the set of intersections of balls and up to two half-spaces is O(d ln d). Therefore, by a standard VC-bound [Vapnik and Chervonenkis, 1971], if we see an iid sample S of size n = O( 1\u03b32 (ln 2 d \u03b3 + ln 1 \u03b4 )), then with probability at least 1\u2212 \u03b4 the empirical measure of any ball intersected with up to two half-spaces will be within \u03b3 of its true probability mass. In other words, the fraction of the sample set S that lands in any ball intersected with up to two half-spaces will be within \u03b3 of the probability that a sample X drawn from P will land in the same set. For the remainder of the proof, assume that this high-probabilty event holds.\nFirst, we show that every half-ball in the set H is an \u03b1-approximation to at least one true hyperplane. Suppose\notherwise, then there is a half-ball B1/2 = B1/2(x\u0302, r, w\u0302) with (x\u0302, w\u0302) \u2208 H that is not an \u03b1 approximation to any true hyperplane hi. The center x\u0302 of the half-ball must belong to the positive region K, since it is one of the sample points. If the half-ball B1/2 is contained entirely in the set K, then the probability that a new sample X drawn from P will land in the half-ball B1/2 is p1/2(r) and therefore the fraction of samples that landed in the half-ball is at least p1/2(R/2)\u2212 \u03b3. But since p1/2(r) \u2212 \u03b3 \u2265 45\u03b1p1/2(r) > \u03c4 , this contradicts the half-ball being included in the set H . Otherwise, the half-ball contains at least one point y that does not belong to the set K (i.e., it does not belong to any class). Since x\u0302 is in the set K, there is at least one true hyperplane hi that separates x\u0302 from y. Since r = R/2 < R, every other point y\u2032 in the half-ball that does not belong to any class must have the same code word as y (since, by assumption, points outside of K that do not belong to any class must have the same code word when they are closer than R), and therefore must be on the same side of hi as y. It follows that all points in the half-ball on the same side of hi as x\u0302 (i.e., those points for which the sign of hi matches the sign of hi(x\u0302)) belong to the set K. But, since the half-ball is not an \u03b1-approximation to hi, this implies that at least an \u03b1 fraction of the half-ball\u2019s volume must belong to the set K. Therefore, the probability that a new sample x drawn from the data distribution p belongs to the half-ball can be lower bounded as follows:\nPr x\u223cp\n(x \u2208 B1/2) \u2265 clb Vol(B1/2 \u2229K) = clb Vol(B1/2) Vol(B1/2 \u2229K)\nVol(B1/2) \u2265 \u03b1p1/2(r).\nBy the uniform convergence argument, the fraction of the samples in S contained in the half-ball B1/2 is at least \u03b1p1/2(r)\u2212 \u03b3 > \u03c4 , which contradicts the half-ball being in H . In either case we arrived at a contradiction and it follows that every half-ball in H is an \u03b1-approximation to at least one true hyperplane hi.\nFinally, we show that the set H will contain at least one half-ball that is an \u03b1-approximation to each true hyperplane hi. Fix any true hyperplane hi. By assumption, there is a class ` and a point x0 on the decision surface of hi so that one half-ball of B(x0, R) with face hi is is contained in K` \u2282 K and the other half-ball is disjoint from K. Suppose WLOG that the half-ball on the negative side of hi is contained in K (the case when the half-ball on the positive side is contained in K is identical). Define \u03c1 > 0 to be the width such that the probability that a new sample X from P lands in the slice of the ball S = {x \u2208 B(x\u03020, r) : hi(x) \u2208 [\u2212\u03c1, 0]} is equal to \u03c4 \u2212 \u03b3. Note that, since the half-ball on the negative side of hi is a subset of K and \u03c4 \u2212 \u03b3 = 310\u03b1p1/2(r) < p1/2(r), such a value of \u03c1 always exists. Since \u03c4 \u2212 \u03b3 > \u03b3, the uniform convergence argument guarantees that there will be at least one sample point in the slice, say x\u0302 \u2208 S. Since x\u0302 is within distance r = R/2 of the point x0, the ball B(x\u0302, r) is contained in B(x0, R). Therefore, the ball of radius r centered at x\u0302 only contains points that either belong to class ` or no class, since only the linear separator hi passes through this ball. By construction, the half-ball B1/2(x\u0302, r, wi) (where wi is the coefficient vector defining hi(x) = w>i x\u2212 bi) with face parallel to hi intersects the set K in a slice of width at most \u03c1 and therefore has probability mass at at most \u03c4 \u2212 \u03b3. It follows that the direction w\u0302 that minimizes the number of samples in the half-ball B1/2(x\u0302, r, w\u0302) will result in the half-ball containing at most a \u03c4 fraction of the sample set, and therefore the pair (x\u0302, w\u0302) will be included in H , and this will be an \u03b1-approximation to hi.\nNaturally, if a half-ball B1/2(x\u0302, r, w\u0302) is an \u03b1-approximation to the linear function h, we expect that the decision surface of h\u0302(x) = w\u0302>(x\u2212 x\u0302) is similar to the decision surface of h. In turn, this suggests that either h\u0302(x) or \u2212h\u0302(x) should take similar function values to h(x) (since the coefficient vectors are normalized). We first give a simple probability lemma that bounds the fraction of a ball contained between two parallel hyperplanes, one passing through the ball\u2019s center. The proof of Lemma 5 is in Section 11 of the appendix.\nLemma 5. Let r > 0 be any radius and X be a random sample drawn uniformly from the ball of radius r centered at the origin. For any width 0 \u2264 \u03c1 \u2264 r/ \u221a 2, the probability that the first coordinate of X lands in [0, \u03c1] can be bounded as follows: \u221a d\n2d\u03c0\n\u03c1 r \u2264 Pr X\u223cB(r,0)\n(X1 \u2208 [0, \u03c1]) \u2264 \u221a d+ 1\n2\u03c0\n\u03c1 r .\nUsing Lemma 5, we show the following:\nLemma 6. Let the half-ballB1/2(x\u0302, r, w\u0302) be an \u03b1-approximation to the linear function h(x) = w>x\u2212b with \u2016w\u2016 = 1, x\u0302 \u2208 X , and \u03b1 < 12 . Let D be the diameter of X . If h(x\u0302) < 0 then for all x \u2208 X we have\n|h(x)\u2212 h\u0302(x)| \u2264 ( 2D + \u221a 2d\u03c0\nd\nr\n2\n)\u221a \u03b1,\nwhere h\u0302(x) = w\u0302>(x\u2212 x\u0302). Otherwise, if h(x\u0302) > 0 then the same upper bound holds for |h(x) + h\u0302(x)|. Proof. Suppose that h(x\u0302) < 0 and let X be a uniformly random sample from the half-ball B1/2 = B1/2(x\u0302, r, w\u0302). By assumption, we know that Pr(h(X) < 0) \u2264 \u03b1.\nFirst we show that \u2016w \u2212 w\u0302\u2016 is small. Since \u03b1 < 1/2 we have that w>w\u0302 > 0. To see this, notice that we must have h(x\u0302 + rw\u0302) \u2265 0, since otherwise at least half of the half-ball would be on the negative side of h. Define g(x) = w>(x\u2212 x\u0302) to be the linear function whose decision surface runs parallel to that of h but passes through the point x\u0302. Since h(x) = g(x) + h(x\u0302) \u2264 g(x), we have that \u03b1 > Pr(h(X) < 0) \u2265 Pr(g(X) < 0). Moreover, since the decision surface of g passes through the center of the half-ball B1/2 and the uniform distribution on the half-ball is radially symmetric about the point x\u0302, we have that Pr(g(X) < 0) = \u03b8(w,w\u0302)\u03c0 . It follows that \u03b8(w, w\u0302) \u2264 \u03c0\u03b1. Using this fact, we can bound \u2016w \u2212 w\u0302\u2016 as follows:\n\u2016w \u2212 w\u0302\u20162 = \u2016w\u20162 + \u2016w\u0302\u20162 \u2212 2w>w\u0302 = 2(1\u2212 w>w\u0302). Since w>w\u0302 = cos(\u03b8(w, w\u0302)) and on the interval [0, \u03c0/2], the cos(\u03b8) function is decreasing and lower bounded by 1\u2212 2\u03c0 \u03b8, we have that 2(1\u2212 w>w\u0302) \u2264 4\u03b1. Taking the square root gives that \u2016w \u2212 w\u0302\u2016 \u2264 2 \u221a \u03b1.\nNext we show that |h(x\u0302)| (the distance from x\u0302 to the decision surface of h) is not too large. The half-ball B1/2(x\u0302, r, w), whose directionw matches the coefficient vector of h is one half-ball centered at x\u0302 of radius r minimizing the fraction of its volume contained on the same side of h as x\u0302. This is because every point in the ball B(x\u0302, r) not on the same side as x\u0302 is contained in B1/2(x\u0302, r, w). Let Y be uniformly sampled from B1/2(x\u0302, r, w). By construction of the half-ball Y is sampled from, we have that Pr(h(X) < 0) \u2265 Pr(h(Y ) < 0), which gives\n\u03b1 \u2265 Pr X\u223cB1/2(x\u0302,r,w\u0302)\n( h(X) < 0 ) \u2265 Pr Y\u223cB1/2(x\u0302,r,w) ( h(Y ) < 0\n) \u2265 \u221a d\n2d\u03c0 2|h(x\u0302)| r ,\nwhich implies that\n|h(x\u0302)| \u2264 \u221a 2d\u03c0\nd\nr\u03b1\n2 .\nFinally, let x\u2032 be any point on the decision surface of h, so that h(x) = w>(x \u2212 x\u2032). Combining the above calculations we have\n|h(x)\u2212 h\u0302(x)| = |w>(x\u2212 x\u2032)\u2212 w\u0302>(x\u2212 x\u0302)| = |w>(x\u2212 x\u0302) + w>(x\u0302\u2212 x\u2032)\u2212 w\u0302>(x\u2212 x\u0302)| = |(w \u2212 w\u0302)(x\u2212 x\u0302) + w>(x\u0302\u2212 x\u2032)| \u2264 \u2016w \u2212 w\u0302\u2016\u2016x\u2212 x\u0302\u2016+ |h(x\u0302)|\n\u2264 2\u221a\u03b1D + \u221a 2d\u03c0\nd\nr\u03b1\n2\n\u2264 (2D + \u221a 2d\u03c0\nd\nr 2 ) \u221a \u03b1,\nas required. The proof of the case when h(x) > 0 follows by applying the above arguments to the function \u2212h.\nRecall that for any hyperplane h(x) = w>x\u2212 b with \u2016w\u20162 = 1, the distance from point x to the decision surface of h is |h(x)|. The above lemma implies that if B1/2(x\u0302, r, w\u0302) is an \u03b1-approximation to h, then either h\u0302 or \u2212h\u0302 will have the same sign as h for all points in X except those in a margin of width O(\u221a\u03b1) around h. Under the uniform distribution on K, the probability mass of the margins surrounding the true hyperplanes isn\u2019t large, which results in low error for the classification rule.\nTheorem 5. Suppose Assumptions 3 and 4 hold. For any desired error > 0, with probability at least 1 \u2212 \u03b4, running Algorithm 4 with parameters r \u2264 R/2 and \u03c4 = \u03b1p1/2(r)/2 for a known constant \u03b1 on on a sample of size n = O\u0303(dm2c2ubR d/(c2lb 4)) will have error at most .\nProof. By Lemma 4, for the parameter settings \u03c4 and r and the given sample size, with probability at least 1\u2212 \u03b4 every half-ball included in the set H will be an \u03b1-approximation to some true hyperplane hi, and every true hyperplane hi is \u03b1-approximated by at least one half-ball in H . Assume that this high probability event occurs.\nLet H = {(x\u03021, w\u03021), . . . , (x\u0302M , w\u0302M )} be the set of of half-balls produced by the algorithm and define h\u0302i(x) = w\u0302>i (x\u2212 x\u0302i) for i = 1, . . . ,M to be the corresponding linear functions. Algorithm 4 uses these hyperplanes to partition the space X into a collection of polygonal regions and assigns a unique class label to each cell in the partition. Notice that negating any of the h\u0302i functions does not change the partitioning of the space. Therefore, negating any subset of the h\u0302i will not change the permutation-invariant error of the resulting classifier.\nLet I1, . . . , Im be a partition of the set of indices {1, . . . ,M} such that for all j \u2208 Ii, we have that B1/2(x\u0302j , w\u0302j , r) is an \u03b1-approximation to hi. By Lemma 6, we know that for at least one g \u2208 {h\u0302j ,\u2212h\u0302j}, we have that\n|hi(x)\u2212 g(x)| \u2264 ( 2D + \u221a 2d\u03c0\nd\nr\n2\n) \u221a \u03b1\nSince negating any of the functions h\u0302j does not change the error of the resulting classifier, assume WLOG that the above holds for g = h\u0302j .\nThis implies that whenever |hi(x)| > c \u221a \u03b1, where c = 2D+ \u221a 2d\u03c0 d R 4 , then for every j \u2208 Ii, the sign of h\u0302j(x) is the same as the sign of hi(x). It follows that for points that are not within a margin of c \u221a \u03b1 of any of the true hyperplanes, every h\u0302j function with j \u2208 Ii will have the same sign as hi for all i = 1, . . . ,m. It follows that the classifier can only error on points that are within a c \u221a \u03b1 margin of one of the true hyperplanes.\nUsing Lemma 5 we can bound the probability that a sample X drawn uniformly from K lands in the c \u221a \u03b1-margin\nof hyperplane hi as follows:\nPr(X in c \u221a \u03b1-margin of hi) \u2264 2\n\u221a d+ 1\n2\u03c0\nc \u221a \u03b1\nD Ddvdcub,\nwhere D is the diameter of X . We can make this upper bound equal to /m by setting\n\u03b1 = \u03c0\n2(d+ 1)\n( D\nmcDdvdclb\n)2 = \u2126 ( 2\nm22dR2D2dv2dc 2 lb\n)\nApplying the union bound to the m hyperplanes h1, . . . , hm shows that the error of f\u0302 is at most .\nNote that if the scale parameter R is unknown, the conclusions of Theorem 5 continue to hold when the parameter r and the unlabeled sample complexity n are set using a conservatively small estimate R\u0302 satisfying R\u0302 \u2264 R.\nTheorem 8 in the appendix extends the above result to the agnostic setting considered in Section 7."}, {"heading": "7 Extensions to the Agnostic Setting", "text": "The majority of our algorithms have two phases: first, we extract a partitioning of the unlabeled data into groups that are likely label-homogeneous, and second, we query the label of the largest groups. We can extend our results for these algorithms to the agnostic setting by querying multiple labels from each group and using the majority label.\nSpecifically, suppose that the data is generated according to a distribution P over X \u00d7 [L] and there exists a labeling function f\u2217 such that Pr(x,y)\u223cP (f\u2217(x) 6= y) \u2264 \u03b7 and our assumptions hold when the unlabeled data is drawn from the marginal PX but the labels are assigned by f\u2217. That is, the true distribution over class labels disagrees with a function f\u2217 satisfying our assumptions with probability at most \u03b7. In this setting, the first phase of our algorithms, which deals with only unlabeled data, behaves exactly as in the realizable setting. The only difference is that we will need to query\nmultiple labels from each group of data to ensure that the majority label is the label predicted by f\u2217. Suppose that the training data is (x1, y1), . . . , (xn, yn) drawn from P (where the labels yi are initially unobserved). For n = O\u0303(1/\u03b72), we are guaranteed that on at most 2\u03b7n of the training points we have that yi 6= f\u2217(xi). Moreover, if we only need to guess the label of large groups of samples, say those containing at least 8\u03b7n points, then we are guaranteed that within each group at least 1/4 of the sample points will have labels that agree with f\u2217. Therefore, after querying O(log(1/\u03b4)) labeled examples from each group, the majority label will agree with f\u2217. If we use these labels in the second phase of the algorithm, we would be guaranteed that the error of our algorithm would be at most had the labels been produced by f\u2217, and therefore the error under the distribution P is at most \u03b7 + . The appendix contains agnostic versions of Theorems 1, 4, and 5.\nSimilarly, modifying Algorithm 2 to require that the each cluster in the pruning have a majority label that accounts for at least 3/4 of the cluster\u2019s data can be used to extend the corresponding results to the agnostic setting."}, {"heading": "8 Conclusion and Discussion", "text": "In this work we showed how to exploit the implicit geometric assumptions made by output code techniques under the well studied cases of one-vs-all and well separated codewords, and for a novel boundary features condition that captures the intuition that every binary learning task should be significant. We provide label-efficient learning algorithms for both the consistent and agnostic learning settings with guarantees when the data density has thick level sets or upper and lower bounds. In all cases, our algorithms show that the implicit assumptions of output code learning can be used to learn from very limited labeled data.\nIn this work we focused on linear output codes, which have been in several practical works. For example Palatucci et al. [2009] use linear output codes for neural decoding of thoughts from fMRI data, Berger [1999] used them successfully for text classification, and Crammer and Singer [2000] show that they perform well on MNIST and several UCI datasets. Many other works use non-linear output codes, and it is a very interesting research direction to extend our work to such cases.\nThe unlabeled sample complexity of our algorithms is exponential in the dimension because our algorithms require the samples to cover high-density regions. It is common for semi-supervised algorithms to require exponentially more unlabeled data than labeled, e.g. [Singh et al., 2008, Castelli and Cover, 1995]. Our results also show that the unlabeled sample complexity only scales exponentially with the intrinsic dimension, which may be significantly lower than the ambient dimension for real-world problems. An interesting direction for future work is to determine further conditions under which the unlabeled sample complexity can be drastically reduced."}, {"heading": "Acknowledgments", "text": "This work was supported in part by NSF grants CCF-1422910, CCF-1535967, IIS-1618714, a Sloan Research Fellowship, a Microsoft Research Faculty Fellowship, and a Google Research Award."}, {"heading": "9 Appendix for Error Correcting Output Codes", "text": "First, we show that the line segment [x, y] crosses the decision surface of the linear separator hk if and only if h(x) and h(y) differ on the kth entry.\nLemma 7. Let i 6= j be any pair of classes whose codewords disagree on the kth bit. Then for any points x \u2208 Ki and y \u2208 Kj , the line segment [x, y] intersects with the line hk = 0.\nProof. Without loss of generality, suppose that Cik = 1 and Cjk = \u22121. Then, from the definition of Ki and Kj , we have that hk(x) > 0 and hk(y) < 0. The function f(t) = hk((1 \u2212 t)x + ty) is continuous and satisfies f(0) = hk(x) > 0 and f(1) = hk(y) < 0. It follows that there must be some t0 \u2208 (0, 1) such that f(t0) = 0. But this implies that the point z = (1\u2212 t0)x+ t0y \u2208 [x, y] satisfies hk(z) = 0 and it follows that hk = 0 intersects with [x, y] at the point z.\nNext, we show that when the consistent linear output code makes at most \u03b2 errors when predicting the code word of a new example and the Hamming distance of the code words is at least 2\u03b2 + d+ 1, then there must be a minimum gap g > 0 between any pair of points belonging to different classes.\nLemma 1. Under Assumption 1, there exists g > 0 s.t. if points x and x\u2032 belong to different classes, then \u2016x\u2212 x\u2032\u2016 > g.\nProof. For sets A and B, let d(A,B) = mina\u2208A,b\u2208B \u2016a\u2212 b\u2016 denote the distance between them and recall that for each i = 1, . . . , L, we defined Ki = {x \u2208 X : dHam(h(x), Ci) \u2264 \u03b2} to be the set of points that belong to class i.\nFix any pair of classes i and j and suppose for contradiction that d(Ki,Kj) = 0. This implies that there are two code words c, c\u2032 \u2208 {\u00b11}m such that dHam(c, Ci) \u2264 \u03b2, dHam(c\u2032, Cj) \u2264 \u03b2, and the distance between A = {x \u2208 X : h(x) = c} and B = {x \u2208 X : h(x) = c\u2032} is 0. First, we construct a point x that belongs to A \u2229 B, where A and B denote the closure of A and B, respectively. Since d(A,B) = 0, there exists a sequence of points x1, x2, . . . \u2208 A such that d(xn, B)\u2192 0 as n\u2192\u221e. But, since A is bounded, so is the sequence (xn), and therefore by the Bolzano-Weierstrass theorem, (xn) has a convergent subsequence. Without loss of generality, suppose that (xn) itself converges to the point x. Then x is a limit point of A and therefore belongs to the closure of A. On the other hand, since the function z 7\u2192 d(z,B) is continuous, it follows that d(x,B) = limn\u2192\u221e d(xn, B) = 0 and therefore x is also in the closure of B.\nNow let k be any index such that the code words c and c\u2032 differ on the kth entry. Next, we show that hk(x) = 0. For each integer n > 0, let Cn = B(x, 1/n) be the ball of radius 1/n centered at x. Since x belongs to the closure of A and Cn is a neighborhood of x, we can find some point, say xn that belongs to the intersection A \u2229 Cn. Similarly, we can find a point yn belonging to B \u2229 Cn. Since the line segment [xn, yn] passes from A to B, Lemma 7 guarantees that there is a point zn \u2208 [xn, yn] \u2282 Cn such that hk(zn) = 0. But, by construction, the sequence zn is converging to x and, since linear functions are continuous, it follows that hk(x) = limn\u2192\u221e hk(zn) = 0.\nBut this leads to a contradiction: since the codewords c and c\u2032 must disagree on at least d + 1 entries, at least d+ 1 of the linear separators h1, . . . , hm intersect at the point x, which contradicts our assumption that at most d lines intersect at any point x \u2208 X . Therefore, we must have d(Ki,Kj) > 0. Since there are finitely many classes, taking g = mini,j d(Ki,Kj) completes the proof.\nNext, we prove a similar result to Theorem 1 that holds in the agnostic setting of Section 7.\nTheorem 6. Assume Assumption 1, errP (f\u2217) \u2264 \u03b7, and p has C-thick level sets. For 0 < \u2264 \u03b7, suppose {p \u2265 /(2 Vol(K))} has N connected components, each with probability at least 7\u03b7. With probability at least 1\u2212 \u03b4, running Algorithm 1 with parameter rc < g on an unlabeled sample of size n = O\u0303( 1 2 ((4C)\n2ddd+1/r2cd+N)) and querying t = O(lnN/\u03b4) labels per cluster will have error at most \u03b7 + after querying at most Nt labels.\nProof. Define \u03bb = /(2 Vol(K)) and let A1, . . . , AN be the connected components of {p \u2265 \u03bb}. Since Assumption 1 holds, Lemma 1 guarantees that there is a distance g > 0 such that whenever f\u2217(x) 6= f\u2217(x\u2032), we must have \u2016x\u2212 x\u2032\u2016 \u2265 g. This implies that for any \u03bb\u2032 > 0, f\u2217 must be constant on the connected components of {p \u2265 \u03bb\u2032}, since otherwise we could construct a pair of points closer than g with f\u2217(x) 6= f\u2217(x\u2032). In particular, we know that f\u2217 is constant on each of the Ai sets.\nSince the clustering produced by Algorithm 1 does not see the labeled examples, an identical covering argument to the one in the proof of Theorem 1 shows that for n = O((4C)2ddd+1/( 2r2dc )) with probability at least 1\u2212 \u03b4, for each set Ai there is a unique cluster, say A\u0302i, such that A\u0302i contains S \u2229Ai, the closest cluster to every point in Ai is A\u0302i. Assume this high probability event occurs.\nSimilarly to the proof of Theorem 1, for n = O(N 2 ln 1 \u03b4 ), we have that with probability at least 1\u2212 \u03b4, for any subset\nof indices I \u2282 [N ], we have that \u2223\u2223|S \u2229AI |/n\u2212 PX (AI) \u2223\u2223 \u2264 , where AI = \u22c3 i\u2208I Ai. Assume this high probability event occurs.\nNow let y1, . . . , yn be the (unobserved) labels corresponding to the unlabeled sample x1, . . . , xn. Since Pr(x,y)\u223cP (f\n\u2217(x) 6= y) \u2264 \u03b7, if n = O( 1\u03b72 ln 1\u03b4 ), then with probability at least 1 \u2212 \u03b4, we have that f\u2217(xi) 6= yi for at most 2\u03b7n of the sample points.\nNow, for any connected component Ai, let A\u0302i be the cluster containing Ai \u2229S. Since we have uniform convergence for all unions of the Ai sets, and PX (Ai) \u2265 7\u03b7, we know that the set Ai contains at least 6\u03b7n sample points. Therefore, even if every point whose label yi disagrees with f\u2217 belongs to A\u0302i, we know that at most a 2\u03b7n/(6\u03b7n) = 1/3 fraction of the points belonging to the cluster A\u0302i will have labels other than f\u2217(Ai). If we query the label of t = 32 ln 2N\u03b4 = O(ln N \u03b4 ) points belonging to cluster A\u0302i, then with probability at least 1\u2212 \u03b4/N the majority label will agree with f\u2217 on Ai. Applying the union bound over the connected components A1, . . . , AN gives the same guarantee for all connected components with probability at least 1\u2212 \u03b4.\nLet f\u0302 be the classifier output by Algorithm 1 and Q \u2282 [N ] be the indices of the Ai sets for which the algorithm queried the label of the corresponding cluster A\u0302i. The above arguments show that with probability at least 1\u2212 4\u03b4, we have that f\u0302(x) = f\u2217(x) for any x \u2208 \u222ai\u2208QAi and, as in Theorem 1, we know that PX ( \u22c3 i 6\u2208QAi) \u2264 /2. This gives the following bound on the error of f\u0302 : Let (x, y) \u223c P , then\nPr(f\u0302(x) 6= y) = Pr(f\u0302(x) 6= y, x \u2208 {p < \u03bb}) + Pr(f\u0302(x) 6= y, x \u2208 \u22c3\ni\u2208Q Ai)\n+ Pr(f\u0302(x) 6= y, x \u2208 \u22c3\ni 6\u2208Q\nAi)\n\u2264 Pr(x \u2208 {p < \u03bb}) + Pr(f\u2217(x) 6= y) + Pr(x \u2208 \u22c3\ni 6\u2208Q\nAi).\nBy our choice of \u03bb, the first term is at most /2, by assumption the second term is at most \u03b7, and the last term is at most /2, giving the final error bound of \u03b7 + ."}, {"heading": "10 Appendix For One-vs-all on the Unit Ball", "text": "The following result is similar to Theorem 4 and shows that Algorithm 3 continues to work in the agnostic setting of Section 7.\nTheorem 7. Suppose the data is drawn from distribution P over X \u00d7 [L] and that there exists a labeling function f\u2217 such that Pr(x,y)\u223cP (f\u2217(x) 6= y) \u2264 \u03b7 and Assumptions 2 and 3 hold when labels are assigned by f\u2217. Assume that Prx\u223cPX (f\n\u2217(x) = i) \u2265 19\u03b7 for all classes i. For any excess error , There exists an rc satisfying rc = \u2126( clb/(c 2 ubbmin)) such that with probability at least 1\u2212 \u03b4, running Algorithm 3 with parameter rc on an unlabeled sample of size n = O\u0303((c4ubd/( 2c2lbb 2 min))\nd) and querying t = O(ln N\u03b4 ) labels from each cluster will output a classifier with error at most \u03b7 + and query at most tL labels.\nProof. For small enough , we know that at least half of the probability mass of the points assigned to class i will belong to the -level set of {q(i)ub \u2265 } (in the notation of Theorem 4). Therefore, the probability mass of each of the sets\nA1, . . . , AL in the proof of Theorem 4 is at least 9\u03b7. It follows that if we see an unlabeled set of size n = O\u0303( 1\u03b72 ), then with probability at least 1\u2212 \u03b4 every Ai set will contain at least 8\u03b7n points. Since these points belong to Ai, we know that they will be active, included in the graph G, and connected to the cluster that contains samples belonging to Ai. Moreover, under the same high probability event, we know that there are at most 2\u03b7n points whose labels disagree with f\u2217. Therefore, the cluster that contains samples from Ai must have at least 8\u03b7n points, at most 2\u03b7n of which can have labels that disagree with f\u2217, so the label assigned by f\u2217 will account for at least a 3/4 fraction of the points belonging to the cluster containing Ai. It follows that if we query O(log(L/\u03b4)) labels from each Ai set then with probability at least 1\u2212 \u03b4, we will output a classification rule that agrees with f\u2217 except with probability . It follows that the error with respect to P at most \u03b7 + ."}, {"heading": "11 Appendix for Boundary Features Condition", "text": "We begin by proving the probability bounds for slices of a d-dimensional ball under the uniform distribution.\nLemma 5. Let r > 0 be any radius and X be a random sample drawn uniformly from the ball of radius r centered at the origin. For any width 0 \u2264 \u03c1 \u2264 r/ \u221a 2, the probability that the first coordinate of X lands in [0, \u03c1] can be bounded as follows: \u221a d\n2d\u03c0\n\u03c1 r \u2264 Pr X\u223cB(r,0)\n(X1 \u2208 [0, \u03c1]) \u2264 \u221a d+ 1\n2\u03c0\n\u03c1 r .\nProof. Let B be the ball of radius r centered at the origin and S = {x \u2208 B : x1 \u2208 [0, \u03c1]} be the slice of B for which the first coordinate is in the interval [0, \u03c1]. The probability that a uniformly random sample from B lands in the subset S is given by Vol(S)/Vol(B), where Vol denotes the (Lebesgue) volume of a set.\nWe bound the volume of the set S by writing the volume as a double integral over the first coordinate x1 and the remaining d\u2212 1 coordinates xR.\nVol(S) =\n\u222b \u03c1\n0\n\u222b\nRd\u22121 I{\u2016xR\u201622 \u2264 r2 \u2212 x21} dxR dx1\nNoticing that the inner integral is actually the volume of a d\u2212 1 dimensional ball of radius \u221a r2 \u2212 x21, and using the fact that for any d, the volume of a d-dimensional ball of radius r is rdvd, where vd is the volume of the d-dimensional unit ball, we have\nVol(S) = vd\u22121\n\u222b \u03c1\n0\n(r2 \u2212 x21)(d\u22121)/2 dx1.\nUpper bounding the integrand by rd\u22121 gives that Vol(S) \u2264 vd\u22121\u03c1rd\u22121. Lower bounding the integrand by (r2 \u2212 \u03c12)(d\u22121)/2 and using the fact that \u03c1 \u2264 r\u221a\n2 we have that Vol(S) \u2265 vd\u22121 1\u221a 2d\u22121 \u03c1rd\u22121. Dividing both inequalities by the\nvolume of B, which is rdvd, and using the fact that for all d we have vd\u22121 vd \u2208 [ \u221a d 2\u03c0 , \u221a d+1 2\u03c0 ] gives\n\u221a d\n2d\u03c0\n\u03c1 r \u2264 Pr X\u223cB\n(X \u2208 S) \u2264 \u221a d+ 1\n2\u03c0\n\u03c1 r ,\nas required.\nThe following is an extension of Theorem 5 to the agnostic setting described in Section 7.\nTheorem 8. Suppose the data is drawn from distribution P over X \u00d7 [L] and that there exists a labeling function f\u2217 such that Pr(x,y)\u223cP (f\u2217(x) 6= y) \u2264 \u03b7 and Assumptions 3 and 4 hold when labels are assigned by f\u2217. Moreover, assume that Prx\u223cPX (f\n\u2217(x) = i) \u2265 10\u03b7 for all classes i. For any excess error 0 < \u2264 \u03b7, with probability at least 1\u2212 \u03b4, running Algorithm 4 with parameters r \u2264 R/2 and \u03c4 = \u03b1p1/2(r)/2 for a known constant \u03b1 on on a sample of size n = O\u0303(dm2c2ubR d/(c2lb 4)) and querying t = O(ln(N/\u03b4)) labels from the L largest clusters will have error at most \u03b7 + .\nProof. In the proof of Theorem 5 we argued that with the set of hyperplanes produced by Algorithm 4 will be good approximations to the true hyperplanes. We additionally showed that the set of hyperplanes approximating one of the linear separators hi defining the output code will agree with high probability with hi except in a small margin and we bounded the probability mass of these margins around each hi by . It follows that for each class i, the probability mass of the set of points in that class not contained in these margins is at least 10\u03b7 \u2212 \u2265 9\u03b7, and it follows that if our unlabeled sample is of size at least O\u0303( 1\u03b72 ) that with probability at least 1\u2212 \u03b4, we will see at least 8\u03b7n points from each class which are not contained in the small margins. Under the same high probability event, we know that at most 2\u03b7n of the labels we query can disagree with f\u2217, which implies that the majority label within the L largest cells will be the label predicted by f\u2217 on these cells. It follows that if we query the labels of O(lnN/\u03b4) labels from each class, then with probability at least 1\u2212 \u03b4 the resulting classifier will predict labels that disagree with f\u2217 with probability at most . It follows that the error of the classifier with respect to the distribution P is at most \u03b7 + ."}], "references": [{"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["E. Allwein", "R. Schapire", "Y. Singer"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Allwein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2000}, {"title": "Unsupervised supervised learning ii: Margin-based classification without labels", "author": ["K. Balasubramanian", "P. Donmez", "G. Lebanon"], "venue": "In AISTATS,", "citeRegEx": "Balasubramanian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2011}, {"title": "Unsupervised supervised learning ii: Margin-based classification without labels", "author": ["K. Balasubramanian", "P. Donmez", "G. Lebanon"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Balasubramanian et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2011}, {"title": "A discriminative model for semi-supervised learning", "author": ["M-F. Balcan", "A. Blum"], "venue": "In Journal of the ACM,", "citeRegEx": "Balcan and Blum.,? \\Q2010\\E", "shortCiteRegEx": "Balcan and Blum.", "year": 2010}, {"title": "Active learning", "author": ["M-F. Balcan", "R. Urner"], "venue": "In Survey in the Encyclopedia of Algorithms,", "citeRegEx": "Balcan and Urner.,? \\Q2015\\E", "shortCiteRegEx": "Balcan and Urner.", "year": 2015}, {"title": "Co-training and expansion: Towards bridging theory and practice", "author": ["M-F. Balcan", "A. Blum", "K. Yang"], "venue": "In NIPS,", "citeRegEx": "Balcan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2004}, {"title": "Agnostic active learing", "author": ["M-F. Balcan", "A. Beygelzimer", "J. Lanford"], "venue": "In ICML,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Exploiting ontology structures and unlabeled data for learning", "author": ["M.-F. Balcan", "A. Blum", "Y. Mansour"], "venue": "In Proceedings of the 31st International Conference on Machine Learning (ICML),", "citeRegEx": "Balcan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2013}, {"title": "Error-correcting output coding for text classification", "author": ["A. Berger"], "venue": "In IJCAI Workshop on machine learning for information filtering,", "citeRegEx": "Berger.,? \\Q1999\\E", "shortCiteRegEx": "Berger.", "year": 1999}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["A. Beygelzimer", "J. Langford", "P. Ravikumar"], "venue": "ALT,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2009}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In COLT,", "citeRegEx": "Blum and Mitchell.,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell.", "year": 1998}, {"title": "On the exponential value of labeled samples", "author": ["V. Castelli", "T. Cover"], "venue": "In Pattern Recognition Letters,", "citeRegEx": "Castelli and Cover.,? \\Q1995\\E", "shortCiteRegEx": "Castelli and Cover.", "year": 1995}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Schlkopf", "A. Zien"], "venue": "The MIT Press, 1st edition,", "citeRegEx": "Chapelle et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2010}, {"title": "Improved output coding for classification using continuous relaxation", "author": ["K. Crammer", "Y. Singer"], "venue": "In NIPS,", "citeRegEx": "Crammer and Singer.,? \\Q2000\\E", "shortCiteRegEx": "Crammer and Singer.", "year": 2000}, {"title": "Multiclass learning approaches: A theoretical comparison with implications", "author": ["A. Daniely", "M. Schapira", "G. Shahaf"], "venue": "In NIPS,", "citeRegEx": "Daniely et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Daniely et al\\.", "year": 2012}, {"title": "Two faces of active learning", "author": ["S. Dasgupta"], "venue": "In Theoretical Computer Science,", "citeRegEx": "Dasgupta.,? \\Q2011\\E", "shortCiteRegEx": "Dasgupta.", "year": 2011}, {"title": "Randomized partition trees for exact nearest neighbor search", "author": ["S. Dasgupta", "K. Sinha"], "venue": "In COLT,", "citeRegEx": "Dasgupta and Sinha.,? \\Q2013\\E", "shortCiteRegEx": "Dasgupta and Sinha.", "year": 2013}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T.G. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri.,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri.", "year": 1995}, {"title": "Unsupervised supervised learning i: Estimating classification and regression errors without labels", "author": ["P. Donmez", "G. Lebanon", "K. Balasubramanian"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Donmez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Donmez et al\\.", "year": 2010}, {"title": "Theory of active learning", "author": ["S. Hanneke"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Hanneke.,? \\Q2014\\E", "shortCiteRegEx": "Hanneke.", "year": 2014}, {"title": "Sensitive error correcting output codes", "author": ["J. Langford", "A. Beygelzimer"], "venue": "COLT,", "citeRegEx": "Langford and Beygelzimer.,? \\Q2005\\E", "shortCiteRegEx": "Langford and Beygelzimer.", "year": 2005}, {"title": "Never-ending learning", "author": ["T. Mitchell", "W. Cohen", "E. Hruschka", "P. Talukdar", "J. Betteridge", "A. Carlson", "B. Dalvi", "M. Gardner", "B. Kisiel", "J. Krishnamurthy", "N. Lao", "K. Mazaitis", "T. Mohamed", "N. Nakashole", "E. Platanios", "A. Ritter", "M. Samadi", "B. Settles", "R. Wang", "D. Wijaya", "A. Gupta", "X. Chen", "A. Saparov", "M. Greaves", "J. Welling"], "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15),", "citeRegEx": "Mitchell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2015}, {"title": "Foundations of Machine Learning", "author": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "venue": "MIT press,", "citeRegEx": "Mohri et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mohri et al\\.", "year": 2012}, {"title": "Zero-shot learning with semantic output codes", "author": ["M. Palatucci", "D. Pomerleau", "G. Hinton", "T. Mitchell"], "venue": "In NIPS,", "citeRegEx": "Palatucci et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palatucci et al\\.", "year": 2009}, {"title": "Unlabeled data: Now it helps, now it doesn\u2019t", "author": ["A. Singh", "X. Zhu", "R. Nowak"], "venue": "In NIPS,", "citeRegEx": "Singh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2008}, {"title": "Unsupervised risk estimation with only structural assumptions", "author": ["J. Steinhardt", "P. Liang"], "venue": "In Annals of Statistics,", "citeRegEx": "Steinhardt and Liang.,? \\Q2016\\E", "shortCiteRegEx": "Steinhardt and Liang.", "year": 2016}, {"title": "Explanation-Based Neural Network Learning: A Lifelong Learning Approach", "author": ["S. Thrun"], "venue": null, "citeRegEx": "Thrun.,? \\Q1996\\E", "shortCiteRegEx": "Thrun.", "year": 1996}, {"title": "Learning one more thing", "author": ["S. Thrun", "T. Mitchell"], "venue": "In Proc. 14th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Thrun and Mitchell.,? \\Q1995\\E", "shortCiteRegEx": "Thrun and Mitchell.", "year": 1995}, {"title": "Lifelong robot learning", "author": ["Sebastian Thrun", "Tom M. Mitchell"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Thrun and Mitchell.,? \\Q1995\\E", "shortCiteRegEx": "Thrun and Mitchell.", "year": 1995}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V. Vapnik", "A. Chervonenkis"], "venue": "Theory of Probability and its Applications,", "citeRegEx": "Vapnik and Chervonenkis.,? \\Q1971\\E", "shortCiteRegEx": "Vapnik and Chervonenkis.", "year": 1971}], "referenceMentions": [{"referenceID": 17, "context": "In practice, output codes are designed to have this property in order to be robust to prediction errors for the binary classification tasks [Dietterich and Bakiri, 1995].", "startOffset": 140, "endOffset": 169}, {"referenceID": 17, "context": "Indeed, the one-vs-all, one-vs-one, and the error correcting output code approaches [Dietterich and Bakiri, 1995] all follow this structure [Mohri et al.", "startOffset": 84, "endOffset": 113}, {"referenceID": 0, "context": ", 2012, Allwein et al., 2000]. There is no prior work providing error bounds for output codes using unlabeled data and interaction. There has been a long line of work for providing provable bounds for semi-supervised learning [Balcan et al., 2004, Balcan and Blum, 2010, Blum and Mitchell, 1998, Chapelle et al., 2010] and active learning [Balcan et al., 2006, Dasgupta, 2011, Balcan and Urner, 2015, Hanneke, 2014]. These works provide bounds on the benefits of unlabeled data and interaction for significantly different semi-supervised and active learning methods that are based different assumptions, often focusing on binary classification, thus the results are largely incomparable. Another line of recent work considers the multi-class setting and uses unlabeled data to consistently estimate the risk of classifiers when the data is generated from a known family of models [Donmez et al., 2010, Balasubramanian et al., 2011a,b]. Their results do not immediately imply learning algorithms and they consider generative assumptions, while in contrast our work explicitly designs learning algorithms under commonly used discriminative assumptions. Another work related to ours is that of Balcan et al. [2013], where labels are recovered from unlabeled data.", "startOffset": 8, "endOffset": 1212}, {"referenceID": 0, "context": ", 2012, Allwein et al., 2000]. There is no prior work providing error bounds for output codes using unlabeled data and interaction. There has been a long line of work for providing provable bounds for semi-supervised learning [Balcan et al., 2004, Balcan and Blum, 2010, Blum and Mitchell, 1998, Chapelle et al., 2010] and active learning [Balcan et al., 2006, Dasgupta, 2011, Balcan and Urner, 2015, Hanneke, 2014]. These works provide bounds on the benefits of unlabeled data and interaction for significantly different semi-supervised and active learning methods that are based different assumptions, often focusing on binary classification, thus the results are largely incomparable. Another line of recent work considers the multi-class setting and uses unlabeled data to consistently estimate the risk of classifiers when the data is generated from a known family of models [Donmez et al., 2010, Balasubramanian et al., 2011a,b]. Their results do not immediately imply learning algorithms and they consider generative assumptions, while in contrast our work explicitly designs learning algorithms under commonly used discriminative assumptions. Another work related to ours is that of Balcan et al. [2013], where labels are recovered from unlabeled data. The main tool that they use, in order to recover the labels, is the assumption that there are multiple views and an underlying ontology that are known, and restrict the possible labeling. Similarly, Steinhardt and Liang [2016] show how to use the method of moments to estimate the risk of a model from unlabeled data under the assumption that the data has three independent views.", "startOffset": 8, "endOffset": 1488}, {"referenceID": 0, "context": ", 2012, Allwein et al., 2000]. There is no prior work providing error bounds for output codes using unlabeled data and interaction. There has been a long line of work for providing provable bounds for semi-supervised learning [Balcan et al., 2004, Balcan and Blum, 2010, Blum and Mitchell, 1998, Chapelle et al., 2010] and active learning [Balcan et al., 2006, Dasgupta, 2011, Balcan and Urner, 2015, Hanneke, 2014]. These works provide bounds on the benefits of unlabeled data and interaction for significantly different semi-supervised and active learning methods that are based different assumptions, often focusing on binary classification, thus the results are largely incomparable. Another line of recent work considers the multi-class setting and uses unlabeled data to consistently estimate the risk of classifiers when the data is generated from a known family of models [Donmez et al., 2010, Balasubramanian et al., 2011a,b]. Their results do not immediately imply learning algorithms and they consider generative assumptions, while in contrast our work explicitly designs learning algorithms under commonly used discriminative assumptions. Another work related to ours is that of Balcan et al. [2013], where labels are recovered from unlabeled data. The main tool that they use, in order to recover the labels, is the assumption that there are multiple views and an underlying ontology that are known, and restrict the possible labeling. Similarly, Steinhardt and Liang [2016] show how to use the method of moments to estimate the risk of a model from unlabeled data under the assumption that the data has three independent views. Our work is more widely applicable, since it applies when we have only a single view. The output-code formalism is also used by Palatucci et al. [2009] for the purpose of zero shot learning.", "startOffset": 8, "endOffset": 1794}, {"referenceID": 29, "context": "Using a standard VC-bound [Vapnik and Chervonenkis, 1971] together with the fact that balls have VC-dimension d + 1, for n = O((4C)d/( r c )) guarantees that with probability at least 1\u2212 \u03b4/2 the following holds simultaneously for every center x \u2208 R and radius r \u2265 0: \u2223\u2223\u2223\u2223|B(x, r) \u2229 S|/n\u2212 P (B(x, r)) \u2223\u2223\u2223\u2223 \u2264 1 2 \u03bb\u03c3vd, (1)", "startOffset": 26, "endOffset": 57}, {"referenceID": 16, "context": "Recall that a probability measure P is said to have doubling dimension D if for every point x in the support of P and every radius r > 0, we have that P (B(x, 2r)) \u2264 2P (B(x, r)) (see, for example, [Dasgupta and Sinha, 2013]).", "startOffset": 198, "endOffset": 224}, {"referenceID": 15, "context": "Following an analysis similar to that of the cluster tree algorithm of Chaudhuri and Dasgupta [2010] gives the following result.", "startOffset": 85, "endOffset": 101}, {"referenceID": 29, "context": "We use a standard VC bound [Vapnik and Chervonenkis, 1971] to relate the probability constraints in the clusterability definition to the empirical measure P\u0302 .", "startOffset": 27, "endOffset": 58}, {"referenceID": 15, "context": "The proof technique used here follows a similar argument as Chaudhuri and Dasgupta [2010]. We use a standard VC bound [Vapnik and Chervonenkis, 1971] to relate the probability constraints in the clusterability definition to the empirical measure P\u0302 .", "startOffset": 74, "endOffset": 90}, {"referenceID": 29, "context": "Therefore, by a standard VC-bound [Vapnik and Chervonenkis, 1971], if we see an iid sample S of size n = O( 1 \u03b32 (ln 2 d \u03b3 + ln 1 \u03b4 )), then with probability at least 1\u2212 \u03b4 the empirical measure of any ball intersected with up to two half-spaces will be within \u03b3 of its true probability mass.", "startOffset": 34, "endOffset": 65}, {"referenceID": 20, "context": "For example Palatucci et al. [2009] use linear output codes for neural decoding of thoughts from fMRI data, Berger [1999] used them successfully for text classification, and Crammer and Singer [2000] show that they perform well on MNIST and several UCI datasets.", "startOffset": 12, "endOffset": 36}, {"referenceID": 8, "context": "[2009] use linear output codes for neural decoding of thoughts from fMRI data, Berger [1999] used them successfully for text classification, and Crammer and Singer [2000] show that they perform well on MNIST and several UCI datasets.", "startOffset": 79, "endOffset": 93}, {"referenceID": 8, "context": "[2009] use linear output codes for neural decoding of thoughts from fMRI data, Berger [1999] used them successfully for text classification, and Crammer and Singer [2000] show that they perform well on MNIST and several UCI datasets.", "startOffset": 79, "endOffset": 171}], "year": 2016, "abstractText": "We present a new perspective on the popular multi-class algorithmic techniques of one-vs-all and error correcting output codes. Rather than studying the behavior of these techniques for supervised learning, we establish a connection between the success of these methods and the existence of label-efficient learning procedures. We show that in both the realizable and agnostic cases, if output codes are successful at learning from labeled data, they implicitly assume structure on how the classes are related. By making that structure explicit, we design learning algorithms to recover the classes with low label complexity. We provide results for the commonly studied cases of one-vs-all learning and when the codewords of the classes are well separated. We additionally consider the more challenging case where the codewords are not well separated, but satisfy a boundary features condition that captures the natural intuition that every bit of the codewords should be significant.", "creator": "LaTeX with hyperref package"}}}