{"id": "1511.03774", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "On the Optimal Sample Complexity for Best Arm Identification", "abstract": "we study the best arm defect identification ( assuming best - risk 1 - arm ) problem, which is defined as follows. we thus are given $ n $ stochastic bandit beta arms. the $ i $ 70 th each arm similarly has a reward distribution distribution $ d _ i $ + with an essentially unknown negative mean $ \\, mu _ i $. upon combining each second play end of the $ i $ 20 th arm, we can get a reward, sampled or i. i. r d. c from $ d _ [ i $. further we reportedly would like readers to identify the arm with largest mean with varying probability at least $ 1 - \\ \u03b8 delta $, sometimes using as few samples as possible. we also only study either an altogether important special case above where there are only potentially two independent arms, since which we also call the true sign problem. we simultaneously achieve already a little very fairly detailed understanding of modeling the smallest optimal product sample for complexity of sign, simplifying terminology and building significantly since extending a proven classical result by farrell schwartz in 1964, with a completely new proof. using \" the fairly new method lower worst bound approximation for sign, we obtain the interesting first lower bound theorem for best - value 1 - symmetric arm matrices that goes beyond the usual classic, mannor - muller tsitsiklis lower bound, by an again interesting reduction projection from sign to nearest best - likelihood 1 - arm.", "histories": [["v1", "Thu, 12 Nov 2015 04:49:46 GMT  (51kb)", "https://arxiv.org/abs/1511.03774v1", "39 pages"], ["v2", "Fri, 13 Nov 2015 05:47:39 GMT  (51kb)", "http://arxiv.org/abs/1511.03774v2", "39 pages"], ["v3", "Tue, 23 Aug 2016 18:05:29 GMT  (63kb)", "http://arxiv.org/abs/1511.03774v3", null]], "COMMENTS": "39 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["lijie chen", "jian li"], "accepted": false, "id": "1511.03774"}, "pdf": {"name": "1511.03774.pdf", "metadata": {"source": "CRF", "title": "On the Optimal Sample Complexity for Best Arm Identification", "authors": ["Lijie Chen", "Jian Li"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n03 77\n4v 3\n[ cs\n.L G\n] 2\n3 A\nug 2\n01 6"}, {"heading": "1 Introduction", "text": "The stochastic multi-armed bandit is a paradigmatic model for capturing the exploration-exploitation tradeoff in many decision-making problems in stochastic environments. While the most studied goal is to maximize the cumulative rewards (or minimize the cumulative regret) obtained by the forecaster (see e.g., [8, 5]), the pure exploration multi-armed bandit problems, in which the exploration phase and exploitation phase are separate, have also attracted significant attentions, due to their applications in several domains such as medical trials [29, 2, 10], communication network [2], crowdsourcing [30, 7]. In a pure exploration problem, the forecaster first performs a pureexploration phase, by (adaptively) drawing samples from the stochastic arms, to infer the optimal (or near optimal) solution, then keeps exploiting this solution. In this paper, we study the best arm identification (Best-1-Arm) problem, which is the most basic pure exploration problem in stochastic multi-armed bandits.\nDefinition 1.1. Best-1-Arm: We are given n arms A1, . . . , An. The ith arm Ai has a reward distribution Di with an unknown mean \u00b5i \u2208 [0, 1]. We assume that all reward distributions have 1-sub-Gaussian tails (see Definition A.1), which is a standard assumption in the stochastic multiarmed bandit literature. Upon each play of Ai, we can get a reward value sampled i.i.d. from Di. Our goal is to identify the arm with largest mean using as few samples as possible. We assume here that the largest mean is strictly larger than the second largest (i.e., \u00b5[1] > \u00b5[2]) to ensure the uniqueness of the solution, where \u00b5[i] denotes the ith largest mean.\nWe also study the following sequential testing problem, named Sign-\u03be, which is important for understanding Best-1-Arm.\nDefinition 1.2. Sign-\u03be: \u03be is a fixed constant. We are given a single arm with unknown mean \u00b5 6= \u03be. The goal is to decide whether \u00b5 > \u03be or \u00b5 < \u03be. Here, the gap of the problem is define to be \u2206 = |\u00b5\u2212 \u03be|. Again, we assume that the distribution of the arm is 1-sub-Gaussian.\nIn fact, Sign-\u03be can be viewed as a special case of Best-1-Arm where there are only two arms and we know the mean of one arm. Hence, a thorough understanding of the sample complexity of Sign-\u03be is very useful for deriving tight sample complexity bounds for Best-1-Arm.\nDefinition 1.3. For a fixed value \u03b4 \u2208 (0, 1), we say that an algorithm A for Best-1-Arm (or Sign-\u03be) is \u03b4-correct, if given any Best-1-Arm(or Sign-\u03be) instance, A returns the correct answer with probability at least 1\u2212 \u03b4.\nWe say that an algorithm A for Best-1-Arm is an (\u03b5, \u03b4)-PAC algorithm, if given any Best-1Arm instance and any confidence level \u03b4 > 0, A returns an \u03b5-optimal arm with probability at least 1\u2212 \u03b4. Here we say an arm Ai is \u03b5-optimal if \u00b5[1] \u2212 \u00b5i \u2264 \u03b5.\nThe studies of both problems have a long history dating back to 1950s [3, 28, 15]. We first discuss the Sign-\u03be problem. It is well known that for any \u03b4-correct algorithm (for constant \u03b4) A that can distinguish two Gaussian arms with means \u03be + \u2206 and \u03be \u2212\u2206 (the values of \u03be and \u2206 are known beforehand), the expected number of samples required by A is \u2126(\u2206\u22122), which is optimal (e.g., [11]). This can be seen as a lower bound for Sign-\u03be as well. However, a tighter lower bound of Sign-\u03be was in fact provided by Farrell in 1964 [15]. He showed that for any \u03b4-correct A for Sign-\u03be (where the reward distribution is in the exponential family), it holds that\nlim sup \u2206\u21920\nTA[\u2206]\n\u2206\u22122 ln ln\u2206\u22121 > 0, (1)\nwhere TA[\u2206] is the expected number of samples taken by A on an instance with gap \u2206. Farrell\u2019s result crucially relies on the Law of Iterated Logarithm (LIL), which roughly states that\nlim supt\n\u2223\u2223\u2223 \u2211t\ni=1 Xi \u2223\u2223\u2223/ \u221a 2t log log t = 1 almost surely where Xi \u223c N (0, 1) for all i. Comparing with the\n\u2126(\u2206\u22122) lower bound, the extra ln ln\u2206\u22121 factor is caused by the fact that we do not known the gap \u2206 beforehand. The above result implies that \u2206\u22122[2] ln ln\u2206 \u22121 [2] is also a lower bound for Best-1-Arm (for two arms). Bechhofer [3] formulated the Best-1-Arm problem for Gaussians in 1954. The early advances are summarized in the monograph [4]. The last decade has witnessed a resurgence of interest in the Best-1-Arm problem and its optimal sample complexity [12, 16, 23, 19]. [27] showed that for any \u03b4-correct algorithm for Best-1-Arm, it requires \u2126 (\u2211n\ni=2\u2206 \u22122 [i] ln \u03b4\n\u22121 ) samples in expectation for\nany instance. We note that the Mannor-Tsitsiklis lower bound is an instance-wise lower bound, i.e., any Best-1-Arm instance requires the stated number of samples. The current best known bound\nis O (\u2211n\ni=2 \u2206 \u22122 [i] ( ln ln\u2206\u22121[i] + ln \u03b4 \u22121 )) , due to Karnin et al. [24]. Jamieson et al. [20] obtained\na UCB-type algorithm (called lil\u2019UCB), which achieves the same sample complexity and is also efficient in practice. We refer the above bound as the KKS bound. See Table 1 for more previous upper bounds.\nGiven Farrell\u2019s \u2206\u22122[2] ln ln\u2206 \u22121 [2] lower bound, it is very attempting to believe that the KKS upper bound is optimal, (which matches the lower bound for two arms). Both [20] and [21] explicitly referred the KKS bound as \u201coptimal\u201d, and [20] stated that \u201cThe procedure cannot be improved in the sense that the number of samples required to identify the best arm is within a constant factor of\n.\na lower bound based on the law of the iterated logarithm (LIL)\u201d. However, as we will demonstrate, none of the existing lower and upper bounds are optimal and the problem is more complicated (and rich) than we expected. The KKS bound is tight only for two arms (or O(1) arms) and in the worst case sense (not instance optimal in the sense of [14, 1])."}, {"heading": "1.1 Our Contributions", "text": "We need some notations to state our results formally. Let {\u00b5[1], \u00b5[2], . . . , \u00b5[n]} be the means of the n arms, sorted in the nondecreasing order (ties are broken in an arbitrary but consistent manner). We use A[i] to denote the arm with mean \u00b5[i]. In the Best-1-Arm problem, we define the gap for the arm A[i] to be \u2206[i] = \u00b5[1] \u2212 \u00b5[i], which is an important quantity to measure the sample complexity. We use A to denote an algorithm and TA(I) to be the expected number of total arm pulls (i.e., samples) by A on the instance I."}, {"heading": "1.1.1 Upper Bounds", "text": "First, we consider the upper bounds for the Best-1-Arm problem with n arms. We provide a novel algorithm which strictly improves the KKS bound, implying that it is not optimal. In particular, for any \u03b4 < 0.1, our algorithm is \u03b4-correct for Best-1-Arm and it needs at most\nO ( \u2206\u22122[2] ln ln\u2206 \u22121 [2] + \u2211n i=2 \u2206\u22122[i] ln \u03b4 \u22121 + \u2211n i=2 \u2206\u22122[i] ln lnmin(n,\u2206 \u22121 [i] ) )\nsamples in expectation. We can see that the improvement over KKS bound is mainly due to the third term. At first glance, the ln lnn factor may seem to be an artifact of either our algorithm or analysis. However, it turns out to be a fundamental quantity in the Best-1-Arm problem, since we can also prove a \u2126( \u2211n i=2\u2206 \u22122 [i] ln lnn) lower bound (Theorem 3.1). Moreover, the first two terms in our upper bounds are also necessary (first term due to the lower bound by [15], second term due to the lower bound by [27]).\nTheorem 2.5 has a few interesting consequences we would like to point out. For example, it is not possible to construct a class of infinite instances that requires \u2126(n\u2206\u22122[2] ln ln\u2206 \u22121 [2] ) samples unless ln ln\u2206\u22121[2] = O(ln lnn). This is somewhat surprising: Consider a very basic family of instances in\nthis class: there are n\u22121 arms with mean 0.5 and 1 arm with mean 0.5+\u2206. The Mannor-Tsitsiklis lower bound \u2126 ( n\u2206\u22122 ) for this instance (even when \u2206 is known) is in fact a directed sum-type result: roughly speaking, in order to solve Best-1-Arm, we essentially need to solve n \u2212 1 independent copies of Sign-\u03be with gap \u2206. However, our upper bound in Theorem 2.5 indicates that the role of \u2206[2] is different from the others \u2206[i]s. Hence, if we want to go beyond Mannor-Tsitsiklis, 1 Best1-Arm can not be thought as n independent copies of Sign-\u03be. In fact, from the analysis of the algorithm, we can see that the first term and the rest come from very different procedures: the first term is used for \u201cestimating\u201d the gap distribution and the rest for \u201cverifying\u201d and \u201celiminating\u201d suboptimal arms.\nOur algorithm can achieve an even better upper bound O ( \u2206\u22122[2] ln ln\u2206 \u22121 [2] + \u2211n i=2\u2206 \u22122 [i] ln \u03b4 \u22121 ) for a special but important class of instances, which we call clustered instances. 2 See Section B.4. Note that this bound is almost instance optimal, since it matches the Mannor-Tsitsiklis instancewise lower bound plus the 2-arm lower bound \u2206\u22122[2] ln ln\u2206 \u22121 [2] . In fact, the aforementioned instances (n \u2212 1 arms with mean 0.5 and 1 arm with mean 0.5 + \u2206) are clustered instances. Even for such basic instances, a tight bound is not known so far! After a careful examination, we find that all previous algorithms are suboptimal on the very basic examples, while our algorithm can achieve the optimal bound O(\u2206\u22122 ln ln\u2206\u22121 + n\u2206\u22122 ln \u03b4\u22121).\nBy slightly modifying our algorithm, we can easily obtain an (\u03b5, \u03b4)-PAC algorithm for Best1-Arm, which improves several prior works. See the supplementary material for the detailed information. Technical Novelty of Our Algorithm: Now, we provide a high level idea of our algorithm. Our algorithm is inspired by the elegant ExpGapElim algorithm in [24]. In order to highlight the technical novelty of our algorithm, we provide here a very brief introduction to the ExpGapElim algorithm which runs in round. In the rth round of ExpGapElim, we first try to identify an \u03b5r-optimal arm Ar (where \u03b5r = O(2\n\u2212r)), using the classical PAC algorithm in [13]. Then, using the empirical mean of Ar as a threshold, the algorithm tries to eliminate those arms with smaller means. In fact, comparing with the previous elimination-based algorithms, such as [12, 13, 6], ExpGapElim seems to be the most aggressive one, which is the main reason that ExpGapElim improves on the previous results. However, we show that ExpGapElim may be over-aggressive, and we may benefit from delaying the elimination for some rounds, if we cannot eliminate a substantial number of arms in this round. To exploit this fact, we develop a procedure called FractionTest, which, roughly speaking, can inform us about the gap distribution and decide whether or not we should do elimination in this round."}, {"heading": "1.1.2 Lower Bounds", "text": "Lower bound of Sign-\u03be: First we briefly discuss our lower bound for the Sign-\u03be problem, which plays a crucial role in the lower bound reduction for Best-1-Arm.\nWe first emphasize that Farrell\u2019s lower bound (1) is not an instance-wise lower bound.3 In particular, the lim sup in (1) merely asserts that the existence of infinite number of instances that require \u2206\u22122 ln ln\u2206\u22122 samples (as \u2206 \u2192 0), which is not enough for our purpose (our reduction for Best-1-Arm requires a stronger quantitative lower bound). Moreover, we note that it is impossible\n1 In other words, we want the lower bound to reflect the hardness caused by not knowing \u2206is. 2 We say the instances is clustered if the cardinality of the set {\u230aln \u2206\u22121[i] \u230b} n i=2 is bounded by a constant. See\nTheorem B.22 for more details. 3 To the contrary, the bound \u2211 i \u2206 \u22122 [i] ln \u03b4\u22121 [27] is an instance-wise lower bound.\nto obtain an \u2126(\u2206\u22122 ln ln\u2206\u22122) lower bound for every instance, since we can design an algorithm that uses o(\u2206\u22122 ln ln\u2206\u22122) samples for infinite number of instances (see the supplementary material Section G).\nComparing to Farrell\u2019s lower bound, ours is a quantitative one. Let the target lower bound be F (\u2206) = c\u2206\u22122 ln ln\u2206\u22121, where c is a small universal constant. For simplicity, assume that all the reward distributions are Gaussian with \u03c3 = 1. Let TA(\u2206) = max(TA(A\u03be+\u2206), TA(A\u03be\u2212\u2206)), in which A\u03be+\u2206 and A\u03be\u2212\u2206 denote the arms with means \u03be +\u2206 and \u03be \u2212\u2206, respectively. For an algorithm A, if TA(\u2206) \u2265 F (\u2206), we say \u2206 is a \u201cslow point\u201d (or A is slow at \u2206), otherwise, it is a \u201cfast point\u201d. 4 Roughly speaking, our lower bound asserts that for any \u03b4-correct algorithm for Sign-\u03be, there must be slow points in almost all intervals [e\u2212i, e\u2212i+1), i \u2208 Z+. The precise statement and the proof can be found in supplementary material (Theorem D.1 in Section D).\nOur proof is very different from, and much simpler than the complicated proof in [15]. Furthermore, we note that [15] assumes the reward distributions are from the exponential family, while our proof only utilizes the KL divergence between different reward distributions, which is more general and applies to non-exponential family as well. Lower bound for Best-1-Arm: Now, we discuss our new lower bound for Best-1-Arm. Note that our current knowledge (in particular the lower bounds of Farrell and Mannor-Tsitsiklis) does not rule out an O (\u2211n\ni=2\u2206 \u22122 [i] ln \u03b4\u22121 +\u2206\u22122 [2] ln ln\u2206\u22121 [2]\n) algorithm for Best-1-Arm. If such a result\nexists, it is clearly a very satisfying answer. However, we show it is impossible by by presenting an \u2126 (\u2211n\ni=2 \u2206 \u22122 [i] ln lnn\n) lower bound.\nThis is the first lower bound that surpasses \u2126( \u2211n\ni=2\u2206 \u22122 [i] ln \u03b4 \u22121) [27, 25] for general Best-1Arm. The proof of the theorem is also interesting in its own right. We provide a nontrivial reduction from the Sign-\u03be problem to the Best-1-Arm problem, and utilize our previous lower bound for Sign-\u03be to obtain the desired lower bound for Best-1-Arm. More concretely, we construct a class of instances for Best-1-Arm, and show that if there is an algorithm A that can solve those instances faster than the target lower bound, we can construct an algorithm B (calling A as a subroutine) to solve a nontrivial proportion of a class of Sign-\u03be instances faster than \u2206\u22122 ln ln\u2206\u22121 time, which leads to a contradiction to our lower bound on Sign-\u03be. Note that the old lower bound in [15] cannot be used here since it does not preclude the existence of such an algorithm B for Sign-\u03be. On Instance Optimality: Instance optimality ([14, 1]) is arguably the strongest possible notion of optimality. Loosely speaking, an algorithm A is instance optimal if the running time of A on instance I is at most O(L(I)), where L(I) is the lower bound required to solve the instance for any algorithm. We propose an intriguing conjecture concerning the instance optimality of Best-1Arm. The conjecture concerns the sample complexity of every Best-1-Arm instance, and provides a concrete formula for it. Interestingly, the formula involves an entropy-like term, which we call gap entropy. The new ln lnn factor appearing in both our new upper and lower bounds is in fact a tight bound of the gap entropy. The proofs of our new results also provide strong evidence for the conjecture. The details can be found in the supplementary material."}, {"heading": "1.2 Other Related Work", "text": "Best-1-Arm: Kaufmann et al. [25] provided an \u2126( \u2211n\ni=2 \u2206 \u22122 [i] ln \u03b4 \u22121) lower bound for Best-1-\nArm, with a better constant factor than in [27]. Garivier and Kaufmann [18] obtained a complete\n4 We sometimes mention the sample complexity of an algorithm and its running time interchangeably, since for all of our algorithms the running time is at most a constant times the number of samples. Hence, sometimes when we informally speak that an algorithm must be \u201cslow\u201d, which also means it requires many samples.\nresolution of the asymptotic sample complexity of Best-1-Arm in the regime where \u03b4 \u2192 0 (treating \u2206is as fixed). However, our work focus on the regime where all \u2206is, and \u03b4 are variables that can approach to 0. In fact, when we allow \u2206i to approach to 0 and maintain \u03b4 fixed, their lower bound is not tight. 5 Sign-\u03be and A/B testing: The Sign-\u03be problem is closely related to the A/B testing problem in the literature, in which we have two arms with unknown means and the goal is to decide which one is larger. It is easy to see that a lower bound for Sign-\u03be is also a lower bound for the the A/B testing problem. Kaufmann et al. [25] studied the optimal sample complexity for the A/B testing problem. However, their focus is on the limiting behavior of the sample complexity when \u03b4 \u2192 0, while we are interested in the case where the gap \u2206 approaches to zero but \u03b4 is a constant (in their case, the ln ln\u2206\u22121 factor is absorbed by the O(ln \u03b4\u22121) factor). Best-k-Arm: One natural generalization of Best-1-Arm is the Best-k-Arm problem, which asks for the top-k arms instead of just the top-1. The Best-k-Arm problem has also been studied extensively for the last few years [22, 16, 17, 23, 6, 26, 30, 25]. Most lower and upper bounds for Best-k-Arm are variants of those for Best-1-Arm, and the bounds also depend on the gap parameters. But in this case, the gaps are typically defined to be the distance to \u00b5[k] or \u00b5[k+1]. Chen et al. [10] and Chen et al. [9] study the combinatorial pure exploration problem, which generalizes the cardinality constraint in Best-k-Arm to more general combinatorial constraints. PAC learning: The worst case sample complexity of Best-1-Arm in the PAC setting is also well studied. There is a matching lower and upper bounds obtained by \u2126(n ln \u03b4\u22121/\u03b52) in [12, 13, 27]. The worst case sample complexity for Best-k-Arm in the PAC setting has also been well studied by many authors during the last few years [22, 23, 30, 7].\n2 An Improved Upper Bound for Best-1-Arm\nIn this section, we present our new algorithm, which achieves the improved upper bound in Theorem 2.5. Due to space constraint, all proofs and some details are deferred to the supplementary material. Our final algorithm builds on several useful components. 1. Uniform Sampling: The first building block is the simple uniform sampling algorithm. Given two parameters \u03b5, \u03b4 and a set of arms S, it takes from each arm a \u2208 S 2\u03b5\u22122 ln(2 \u00b7 \u03b4\u22121) samples. Let \u00b5\u0302[a] be the empirical mean of arm a. We denote the algorithm as UniformSample(S, \u03b5, \u03b4). We have the following lemma which is a simple consequence of Hoeffding\u2019s inequality. Lemma 2.1. For each arm a \u2208 S, we have that Pr [ |\u00b5[a] \u2212 \u00b5\u0302[a]| \u2265 \u03b5 ] \u2264 \u03b4.\n2. Median Elimination: We need the median elimination algorithm developed in [13], which is a classic (\u03b5, \u03b4)-PAC algorithm for Best-1-Arm. The algorithm takes parameters \u03b5, \u03b4 > 0 and a set S of arms, and returns an \u03b5-optimal arm with probability 1\u2212 \u03b4. The algorithm runs in rounds. In each round, it samples every remaining arm a uniform number of times, and then discard half of the arms with lowest empirical mean (thus the name median elimination). It outputs the final arm that survives. We denote the procedure by MedianElim(S, \u03b5, \u03b4). We use this algorithm in a black-box manner and its performance is summarized in the following lemma.\n5 Their lower bound is of the form T \u2217(I) \u00b7 ln \u03b4\u22121 for instance I (see [18] for the definition of T \u2217). In fact, we can see T \u2217(I) is upper bounded by O( \u2211n i=2 \u2206 \u22122 [i] ) by existing upper bounds. When \u03b4 is some constant, by our new lower bound Theorem 3.1, \u2126( \u2211n\ni=2 \u2206 \u22122 [i] ln \u03b4 \u22121) is not tight.\nLemma 2.2. Let \u00b5[1] be the maximum mean value. MedianElim(S, \u03b5, \u03b4) returns an \u03b5-optimal arm (i.e., with mean at least \u00b5[1] \u2212 \u03b5) with probability at least 1 \u2212 \u03b4, using a budget of at most O(|S| log(1/\u03b4)/\u03b52) samples.\n3. Fraction test: FractionTest is a simple estimation procedure, which can be used to gain some information about the distribution of the arms. It plays a key role in the final algorithm. The algorithm takes six parameters (S, cl, cr, \u03b4, t, \u03b5), where S is the set of arms, \u03b4 is the confidence level, cl < cr are real numbers called range parameters, t \u2208 (0, 1) is the threshold, and \u03b5 is a small positive constant. Typically, cl and cr are very close. The goal of the algorithm, roughly speaking, is to distinguish whether there are still many arms in S which have small means (w.r.t. cr) or the majority of arms already have large means (w.r.t. cl).\nThe algorithm runs in ln(2 \u00b7 \u03b4\u22121)(\u03b5/3)\u22122/2 iterations. In each iteration, it samples an arm ai uniformly from S, and takes O(ln \u03b5\u22121(cr\u2212cl)\u22122) independent samples from ai. Then, we maintain a counter cnt which is initially 0, and counts the fraction of iterations in which the empirical mean of ai is smaller than (cl+cr)/2. If the fraction is larger than t, the algorithm returns True. Otherwise, it returns False.\nFor ease of notation, we define S\u2265c := {\u00b5[a] \u2265 c | a \u2208 S}, i.e., all arms in S with means at least c. Similarly, we can define S>c, S\u2264c and S<c.\nLemma 2.3. Suppose \u03b5 < 0.1 and t \u2208 (\u03b5, 1 \u2212 \u03b5). With probability 1\u2212 \u03b4, the following hold: \u2022 If FractionTest outputs True, then |S>cr | < (1\u2212 t+ \u03b5)|S| (or equivalently |S\u2264cr | > (t\u2212 \u03b5)|S|).\n\u2022 If FractionTest outputs False, then |S<cl | < (t+ \u03b5)|S| (or equivalently |S\u2265cl | > (1\u2212 t\u2212 \u03b5)|S|). Moreover, the number of samples taken by the algorithm is O(ln \u03b4\u22121\u03b5\u22122\u2206\u22122 ln \u03b5\u22121), in which \u2206 = cr \u2212 cl. If \u03b5 is a fixed constant, then the number of samples is simply O(ln \u03b4\u22121\u2206\u22122).\n4. Eliminating arms: The last ingredient is an elimination procedure, which can be used to eliminate most arms below a given threshold. The procedure takes four parameters (S, cl, cr, \u03b4) as input, where S is a set of arms, cl < cr are the range parameters, and \u03b4 is the confidence level. It outputs a subset of S and guarantees that upon termination, most of the remaining arms have means at least cl with probability 1\u2212 \u03b4.\nNow, we describe the procedure Elimination which runs in iterations. It maintains the current set Sr of arms, which is initially S. In each iteration, it first applies FractionTest(Sr, cl, cm, \u03b4r, 0.075, 0.025) on Sr, where cm = (cr+cr)/2. If FractionTest returns True, which means that there are at least 5% fraction of arms with means smaller than cm in Sr, we sample all arms in Sr uniformly by calling UniformSample(Sr, (cr \u2212 cm)/2, \u03b4r) where \u03b4r = \u03b4/(10 \u00b7 2r), and retain those with empirical means at least (cm+ cr)/2. If FractionTest returns False (meaning that 90% arms have means at least cl) the algorithm terminates and returns the remaining arms. The guarantee of Elimination is summarized in the following lemma.\nLemma 2.4. Suppose \u03b4 < 0.1. Let S\u2032 = Elimination(S, cl, cr, \u03b4). Let A1 be the best arm among S, with mean \u00b5[A1] \u2265 cr. Then with probability at least 1\u2212 \u03b4, the following statements hold 1. A1 \u2208 S\u2032 (the best arm survives);\n2. |S\u2032\u2264cl | < 0.1|S\u2032| (only a small fraction of arms have means less than cl);\n3. The number of samples is O(|S| ln \u03b4\u22121\u2206\u22122), in which \u2206 = cr \u2212 cl.\nOur Final Algorithm DistrBasedElim: Now, everything is ready to describe our algorithm DistrBasedElim for Best-1-Arm. We provide a high level description here. All detailed parameters can be found in Algorithm 1. The algorithm runs in rounds. It maintains the current set Sr of arms. Initially, S1 is the set of all arms S. In round r, the algorithm tries to eliminate a set of suboptimal arms, while makes sure the best arm is not eliminated. First, it applies the MedianElim procedure to find an \u03b5r/4-optimal arm, where \u03b5r = 2\n\u2212r. Suppose it is ar. Then, we take a number of samples from ar to estimate its mean (denote the empirical mean by \u00b5\u0302[ar]). Unlike previous algorithms in [12, 24], which eliminates either a fixed fraction of arms or those arms with mean much less than ar, we use a FractionTest to see whether there are many arms with mean much less than ar. If it returns True, we apply the Elimination procedure to eliminate those arms (for the purpose of analysis, we need to use MedianElim again, but with a tighter confidence level, to find an \u03b5r/4-optimal arm br). If it returns False, the algorithm decides that it is not judicious to do elimination in this round (since we need to spend a lot of samples, but only discard very few arms, which is wasteful), and simply sets Sr+1 to be Sr, and then proceeds to the next round.\nAlgorithm 1: DistrBasedElim(S, \u03b4) 1 h \u2190 1 2 S1 \u2190 S 3 for r = 1 to +\u221e do 4 if |Sr| = 1 then 5 Return the only arm in Sr 6 \u03b5r \u2190 2\u2212r 7 \u03b4r \u2190 \u03b4/50r2 8 ar \u2190 MedianElim(Sr, \u03b5r/4, 0.01). 9 \u00b5\u0302[ar] \u2190 UniformSample({ar}, \u03b5r/4, \u03b4r)\n10 if FractionTest(Sr, \u00b5\u0302[ar ] \u2212 1.5\u03b5r , \u00b5\u0302[ar ] \u2212 1.25\u03b5r , \u03b4r, 0.4, 0.1) then 11 \u03b4h \u2190 \u03b4/50h2 12 br \u2190 MedianElim(Sr, \u03b5r/4, \u03b4h) 13 \u00b5\u0302[br] \u2190 UniformSample({br}, \u03b5r/4, \u03b4h) 14 Sr+1 \u2190 Elimination(Sr, \u00b5\u0302[br ] \u2212 0.5\u03b5r, \u00b5\u0302[br ] \u2212 0.25\u03b5r , \u03b4h) 15 h \u2190 h+ 1 16 else 17 Sr+1 \u2190 Sr\nTheorem 2.5. For any \u03b4 < 0.1, there is a \u03b4-correct algorithm for Best-1-Arm which needs at most O ( \u2206\u22122[2] ln ln\u2206 \u22121 [2] + \u2211n i=2 \u2206 \u22122 [i] ln \u03b4 \u22121 + \u2211n i=2\u2206 \u22122 [i] ln lnmin(n,\u2206 \u22121 [i] ) ) samples in expectation.\nIt is not difficult to verify that DistrBasedElim returns the best arm with probability at least 1 \u2212 \u03b4. However, the analysis of the running time of our algorithm is much more challenging. The rough idea is to consider the number of arms in each interval U s = {a | 2\u2212s \u2264 \u2206[a] < 2\u2212s+1} and see how it changes in very round. When we execute Elimination in round r, we can eliminate a substantial fraction or arms in U1, . . . , U r. However, there are still some remaining and we need to keep track of them over the ensuing rounds. For that purpose, we need to carefully choose a potential function to amortize the costs over different iterations.\nOur algorithm can achieve an even better upper boundO ( \u2206\u22122[2] ln ln\u2206 \u22121 [2] + \u2211n i=2 \u2206 \u22122 [i] ln \u03b4 \u22121 ) for\nclustered instances. Furthermore, by slightly modifying the algorithm, we can obtain an improved (\u03b5, \u03b4)-PAC algorithm for Best-1-Arm. See the supplementary material.\n3 A New Lower Bound for Best-1-Arm\nIn this section, we provide a sketch proof of the following new lower bound for Best-1-Arm. From now on, \u03b4 is a fixed constant such that 0 < \u03b4 < 0.005. Throughout this section, we assume the distributions of all the arms are Gaussian with variance 1.\nTheorem 3.1. There exist constants c, c1 > 0 and N \u2208 N such that, for any \u03b4 < 0.005 and any \u03b4-correct algorithm A, and any n \u2265 N , there exists an n arms instance I such that TA[I] \u2265 c \u00b7\u2211ni=2\u2206\u22122[i] ln lnn. Furthermore, \u2206 \u22122 [2] ln ln\u2206 \u22121 [2] < c1 lnn \u00b7 \u2211n i=2 \u2206 \u22122 [i] ln lnn.\nThe second statement of the theorem says that \u2211n\ni=2 \u2206 \u22122 [i] ln lnn dominates the 2-arm lower\nbound \u2206\u22122[2] ln ln\u2206 \u22121 [2] (so that the theorem is not vacant). In order to prove Theorem 3.1, we need a new lower bound for Sign-\u03be to serve as the basis for our reduction to Best-1-Arm. Let A\u2032 denote an algorithm for Sign-\u03be, A\u00b5 be an arm with mean \u00b5 (i.e., with distribution N (\u00b5, 1)), and we define TA\u2032(\u2206) = max(TA\u2032(A\u03be+\u2206), TA\u2032(A\u03be\u2212\u2206)). Then we have the following new lower bound for Sign-\u03be. The proof is deferred to the supplementary material (Section D).\nLemma 3.2. For any \u03b4\u2032-correct algorithm A\u2032 for Sign-\u03be with \u03b4\u2032 \u2264 0.01, there exist constants N0 \u2208 N and c1 > 0 such that for all N \u2265 N0, |{TA\u2032(\u2206) < c1 \u00b7 \u2206\u22122 lnN | \u2206 = 2\u2212i, i \u2208 [0, N ]}| \u2264 0.1(N \u2212 1). Proof of Theorem 3.1. (sketch) Without loss of generality, we can assume N0 in Lemma 3.2 is an even integer, and N0 > 10 such that 2 \u00b74N0 \u2265 43 \u00b74N0 +N0+2. Let N = 2 \u00b74N0 . For every n \u2265 N , we pick the largest even integer m such that 2 \u00b7 4m \u2264 n. Consider the following Best-1-Arm instance Iinit with n arms: (1) There is a single arm with mean \u03be. (2) For each k \u2208 [0,m], there are 4m\u2212k arms with mean \u03be \u2212 2\u2212k. (3) There are n\u2212\u2211mk=0 4k \u2212 1 arms with mean \u03be \u2212 2.\nNow we define a class of Best-1-Arm instances {IS} where each S \u2286 {0, 1, . . . ,m}. Each IS is formed as follows: for every k \u2208 S, we add one more arm with mean \u03be \u2212 2\u2212k to Iinit; finally we remove |S| arms with mean \u03be \u2212 2 (by our choice of m there are enough such arms to remove). Obviously, there are still n arms in every instance IS.\nFor a Best-1-Arm instance I, let n(I) be the number of arms in I, and \u2206[i](I) be the gap \u2206[i] according to I. We denote H(I) = \u2211n(I)i=2 \u2206[i](I)\u22122. Now we claim that for any \u03b4-correct algorithm A for Best-1-Arm, there must exist an instance IS such that TAperm(IS) > c \u00b7 H(IS) \u00b7 lnm = \u2126(H(IS) ln lnn), for some universal constant c > 0, where Aperm first randomly permutes the arms and then simulates A.\nSuppose for contradiction that there exists a \u03b4-correct A such that TAperm(IS) \u2264 c \u00b7 H(IS) \u00b7 lnm for all S. Let U = {IS | |S| = m/2}, V = {IS | |S| = m/2 + 1} be two sets of Best-1-Arm instances. Notice that |U | = |V | = ( m+1 m/2 ) (since m is even).\nFix S \u2208 U . Consider the problem Sign-\u03be, in which the given instance is a single arm A with unknown mean \u00b5, and we would like to decide whether \u00b5 > \u03be or \u00b5 < \u03be. Now, we construct an algorithm AS for Sign-\u03be. First consider the following two algorithms for Sign-\u03be, which call Aperm as a subprocedure. (1) A1S: We first create a Best-1-Arm instance instance Inew by replacing one arm with mean \u03be \u2212 2 in IS with A. Then run Aperm on Inew. We output \u00b5 > \u03be if Aperm selects A as the best arm. Otherwise, we output \u00b5 < \u03be. (2) A2S : We first construct an artificial arm Anew\nwith mean 2\u03be \u2212 \u00b5 from A. 6 Create a Best-1-Arm instance Inew by replacing one arm with mean \u03be \u2212 2 in IS with Anew. Then run Aperm on Inew. We output \u00b5 < \u03be if Aperm selects Anew as the best arm. Otherwise, we output \u00b5 > \u03be. AS simulates A 1 S and A 2 S simultaneously: Each time it takes a sample from the input arm, and feeds it to both A1S and A 2 S . If A 1 S (A 2 S resp.) terminates first, it returns the output of A1S (A 2 S resp.). It is not hard to see that AS is 2\u03b4-correct for Sign-\u03be.\nNow we analyze the expected total number of samples taken by AS on arm A with mean \u00b5 and gap \u2206 = |\u03be \u2212 \u00b5| = 2\u2212k. Suppose k /\u2208 S. A key observation is the following: if \u00b5 < \u03be, then the instance constructed in A1S is exactly IS\u222a{k}; otherwise \u00b5 > \u03be, since 2\u03be\u2212(\u03be+\u2206) = \u03be\u2212\u2206 = \u03be\u22122\u2212k, the instance constructed in A2S is exactly IS\u222a{k}. Hence, TAS (A) \u2264 min(TA1S (A), TA2S (A)) \u2264 a k S\u222a{k} \u00b7 4k, where akS is so defined that a k S \u00b7 4k is the expected number of samples taken from an arm with gap 2\u2212k by Aperm(IS). Moreover, since TAperm(IS) \u2264 c \u00b7 H(IS) \u00b7 lnm for all S, we can show that for any S, there are at most 0.1 fraction of elements in {akS}mk=0 satisfying akS \u2265 c1 \u00b7 lnm (letting c = c1/30 will suffice). Intuitively, this implies TAS(A) \u2264 c14k lnm from for \u2206 = 2\u2212k, k \u2208 [0,m]. Indeed, by a careful counting argument, we can show that there exists an S \u2208 U such that {TAS (\u2206) < c1 \u00b7 \u2206\u22122 lnm | \u2206 = 2\u2212i, i \u2208 [0,m]} \u2265 0.4(m + 1), which is a contradiction to Lemma 3.2."}, {"heading": "4 Concluding Remarks", "text": "The most interesting open problem from this paper is to obtain an almost instance optimal algorithm for Best-1-Arm, in particular to prove (or disprove) Conjecture E.4. Note that for the clustered instances, and the instances where the gap entropy is \u2126(ln lnn), we already have such an algorithm. Our techniques may be helpful for obtaining better bounds for the Best-k-Arm problem, or even the combinatorial pure exploration problem. In an ongoing work, we already have some partial results on applying some of the ideas in this paper to obtain improved upper and lower bounds for Best-k-Arm.\n6That is, whenever the algorithm pulls Anew, we pull A to get a reward r, and return 2\u03be \u2212 r as the reward for Anew. Note although we do not know \u00b5, Anew is clearly an arm with mean 2\u03be \u2212 \u00b5."}, {"heading": "Supplementary Material for \u201cOn the Optimal Sample Complexity", "text": "for Best Arm Identification\u201d\nThe supplementary material is organized as follows:\n1. (Section A) We provide some preliminary knowledge for our later developments.\n2. (Section B) We provide all details of our new algorithm DistrBasedElim and the proof of Theorem 2.5. In Section B.4, we define the clustered instances and show our algorithm is almost instance optimal for such instances. In Section B.5, we slightly modify the algorithm and obtain an improved (\u03b5, \u03b4)-PAC algorithm for Best-1-Arm.\n3. (Section C) We provide the detailed proof of Theorem 3.1, our new lower bound for Best-1-Arm.\n4. (Section D) We provide our new lower bound for Sign-\u03be, which is the basis of our lower bound reduction in Section C.\n5. (Section E) We propose to investigate Best-1-Arm from the perspective of instance optimality, and propose a conjecture concerning the fundamental sample complexity for every instance of Best-1-Arm. We also discuss how our new results are related with the conjecture and why we think the conjecture is likely to be true.\n6. (Section F) This section contains some missing technical proofs from Section B.\n7. (Section G) We present a class of \u03b4-correct algorithms for Sign-\u03be which needs o(\u2206\u22122 ln ln\u2206\u22121) samples for infinite instances. It is useful in discussing the instance optimality in Section E.\n8. (Section H) We provide a transformation that turns an algorithm with only conditional expected sample complexity upper bound to one with asymptotically the same unconditional expected sample complexity upper bound, under mild conditions."}, {"heading": "A Preliminaries", "text": "Definition A.1. Let R > 0, we say a distribution D on R has R-sub-Gaussian tail (or D is R-subGaussian) if for the random variable X drawn from D and any t \u2208 R, we have that E[exp(tX \u2212 tE[X])] \u2264 exp(R2t2/2).\nIt is well known that the family of R-sub-Gaussian distributions contains all distributions with support on [0, R] as well as many unbounded distributions such as Gaussian distributions with variance R2. Then we recall a standard concentration inequality for R-sub-Gaussian random variables.\nLemma A.2. (Hoeffding\u2019s inequality) Let X1, . . . ,Xn be n i.i.d. random variables drawn from an R-sub Gaussian distribution D. Let \u00b5 = Ex\u223cD[x]. Then for any \u03b5 > 0, we have that\nPr [\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nXi \u2212 \u00b5 \u2223\u2223\u2223\u2223\u2223 \u2265 \u03b5 ] \u2264 2 exp ( \u2212 n\u03b5 2 2R2 ) .\nFor simplicity of exposition, we assume all reward distributions are 1-sub-Gaussian in the paper. Suppose A is an algorithm for Best-1-Arm(or Sign-\u03be). Let the given instance be I. Let E be an event and PrA,I [E ] be the probability that the event E happens when running A on instance I. When A is clear from the context, we omit the subscript A and simply write PrI [E ]. Similarly, if X is a random variable, we use EA,I [X] to denote the expectation of X when running A on instance I. Sometimes, A takes an additional confidence parameter \u03b4, and we write A\u03b4 to denote the algorithm A with the fixed confidence parameter \u03b4.\nLet \u03c4i be the random variable that denotes the number of pulls from arm i (when the algorithm and the problem instance are clear from the context) and EI [\u03c4i] be its expectation. Let \u03c4 = \u2211n i=1 \u03c4i be the total number of samples taken by A. The Kullback-Leibler (KL) divergence of any two distributions p and q is defined to be\nKL(p, q) = \u222b log ( dp\ndq (x)\n) dp(x) if q \u226a p\nwhere q \u226a p means that dp(x) = 0 whenever dq(x) = 0. For any two real numbers x, y \u2208 (0, 1), let H(x, y) = x log(x/y) + (1\u2212 x) log((1\u2212 x)/(1 \u2212 y)) be the relative entropy function.\nMany lower bounds in the bandit literature rely on certain \u201cchanges of distributions\u201d argument. The following version (Lemma 1 in [25]) is crucial to us.\nLemma A.3. (Change of distribution) [25] We use an algorithm A for a bandit problem with n arms. 7 Let I (with arm distributions {Di}i\u2208[n]) and I \u2032 (with arm distributions {D\u2032i}i\u2208[n]) be two instances. Let E be an event, 8 such that 0 < PrA,I(E) < 1. Then, we have\nn\u2211\ni=1\nEI [\u03c4i]KL(Di,D\u2032i) \u2265 H(PrA,I(E),PrA,I\u2032(E)).\nN (\u00b5, \u03c32) denotes the Gaussian distribution with mean \u00b5 and standard deviation \u03c3. We also need the following well known fact about the KL divergence between two Gaussian distributions.\nLemma A.4.\nKL(N (\u00b51, \u03c32),N (\u00b52, \u03c32)) = (\u00b51 \u2212 \u00b52)2\n2\u03c32 .\nB An Improved Upper Bound for Best-1-Arm\nIn this section we prove Theorem 2.5 by presenting an algorithm for Best-1-Arm. Our final algorithm builds on several useful components."}, {"heading": "B.1 Useful Building Blocks", "text": "1. Uniform Sampling: The first building block is the simple uniform sampling algorithm.\n7 We make no assumption on the behavior of A in this lemma. For example, A may even output incorrect answers with high probability.\n8 More rigorously, E should be in the \u03c3-algebra F\u03c4 where \u03c4 is a stopping time with respect to the filtration {Ft}t\u22650.\nAlgorithm 2: UniformSample(S, \u03b5, \u03b4)\nData: Arm set S, approximation level \u03b5, confidence level \u03b4. Result: For each arm a, output the empirical mean \u00b5\u0302[a].\n1 For each arm a \u2208 S, sample it 2\u03b5\u22122 ln(2 \u00b7 \u03b4\u22121) times. Let \u00b5\u0302[a] be the empirical mean.\nWe have the following Lemma for Algorithm 2, which is a simple consequence of Lemma A.2. Lemma B.1. For each arm a \u2208 S, we have that Pr [ |\u00b5[a] \u2212 \u00b5\u0302[a]| \u2265 \u03b5 ] \u2264 \u03b4.\n2. Median Elimination: We need the MedianElim algorithm in [13], which is a classic (\u03b5, \u03b4)-PAC algorithm for Best-1-Arm. The algorithm takes parameters \u03b5, \u03b4 > 0 and a set S of n arms, and returns an \u03b5-optimal arm with probability 1\u2212 \u03b4. The algorithm runs in rounds. In each round, it samples every remaining arm a uniform number of times, and then discard half of the arms with lowest empirical mean (thus the name median elimination). It outputs the final arm that survives. We denote the procedure by MedianElim(S, \u03b5, \u03b4). We use this algorithm in a black-box manner and its performance is summarized in the following lemma.\nLemma B.2. Let \u00b5[1] be the maximum mean value. MedianElim(S, \u03b5, \u03b4) returns an arm with mean at least \u00b5[1] \u2212 \u03b5 with probability at least 1\u2212 \u03b4, using a budget of at most O(|S| log(1/\u03b4)/\u03b52) pulls.\n3. Fraction test: FractionTest is an estimation procedure, which can be used to gain some information about the distribution of the arms. The algorithm takes six parameters (S, cl, cr, \u03b4, t, \u03b5), where S is the set of arms, \u03b4 is the confidence level, cl < cr are real numbers called range parameters, and t \u2208 (0, 1) is the threshold, and \u03b5 is a small positive constant. Typically, cl and cr are very close. The goal of the algorithm, roughly speaking, is to distinguish whether there are still many arms in S which have small means (w.r.t. cr) or the majority of arms already have large means. The precise guarantee the algorithm can achieve can be found in Lemma B.3.\nThe algorithm runs in ln(2 \u00b7 \u03b4\u22121)(\u03b5/3)\u22122/2 iterations. In each iteration, it samples an arm ai uniformly from S, and takes O(ln \u03b5\u22121(cr \u2212 cl)\u22122) independent samples from ai. Then, we look at the fraction of iterations in which the empirical mean of ai is smaller than (cl+cr)/2. If the fraction is larger than t, the algorithm returns True. Otherwise, it returns False.\nFor ease of notation, we define S\u2265c := {\u00b5[a] \u2265 c | a \u2208 S}, i.e., all arms in S with means at least c. Similarly, we can define S>c, S\u2264c and S<c.\nLemma B.3. Suppose \u03b5 < 0.1 and t \u2208 (\u03b5, 1 \u2212 \u03b5). With probability 1\u2212 \u03b4, the following hold:\n\u2022 If FractionTest outputs True, then |S>cr | < (1\u2212 t+ \u03b5)|S| (or equivalently |S\u2264cr | > (t\u2212 \u03b5)|S|).\n\u2022 If FractionTest outputs False, then |S<cl | < (t+ \u03b5)|S| (or equivalently |S\u2265cl | > (1\u2212 t\u2212 \u03b5)|S|).\nMoreover, the number of samples taken by the algorithm is O(ln \u03b4\u22121\u03b5\u22122\u2206\u22122 ln \u03b5\u22121), in which \u2206 = cr \u2212 cl. If \u03b5 is a fixed constant, then the number of samples is simply O(ln \u03b4\u22121\u2206\u22122).\nThe proof can be found in Section F. 4. Eliminating arms: The final ingredient is an elimination procedure, which can be used to eliminate most arms below a given threshold. The procedure takes four parameters (S, cl, cr, \u03b4) as input, where S is a set of arms, cl < cr are the range parameters, and \u03b4 is the confidence level. It outputs a subset of S and guarantees that upon termination, most of the remaining arms have means at least cl with probability 1\u2212 \u03b4.\nAlgorithm 3: FractionTest(S, cl, cr, \u03b4, t, \u03b5)\nData: Arm set S, range parameters cl, cr, confidence level \u03b4, threshold t, approximate parameter \u03b5.\n1 cnt \u2190 0 2 tot \u2190 ln(2 \u00b7 \u03b4\u22121)(\u03b5/3)\u22122/2 3 for i = 1 to tot do 4 Pick a random arm ai \u2208 S uniformly. 5 \u00b5\u0302[ai] \u2190 UniformSample({ai}, (cr \u2212 cl)/2, \u03b5/3) 6 if \u00b5\u0302[ai] < (cl + cr)/2 then cnt \u2190 cnt+ 1 7 if cnt/tot > t then 8 Return True 9 else\n10 Return False\nAlgorithm 4: Elimination(S, cl, cr, \u03b4)\nData: Arm set S, range parameters cl, cr, confidence level \u03b4. Result: A set of arms after elimination.\n1 S1 \u2190 S 2 cm \u2190 (cl + cr)/2 3 for r = 1 to +\u221e do 4 \u03b4r = \u03b4/(10 \u00b7 2r) 5 if FractionTest(Sr, cl, cm, \u03b4r, 0.075, 0.025) then 6 UniformSample(Sr, (cr \u2212 cm)/2, \u03b4r) 7 Sr+1 \u2190 { a \u2208 Sr | \u00b5\u0302[a] > (cm + cr)/2 }\n8 else 9 Return Sr\nNow, we describe the procedure Elimination which runs in iterations. It maintains the current set Sr of arms, which is initially S. In each iteration, it first applies FractionTest(Sr, cl, cm, \u03b4r, 0.075, 0.025) on Sr, where cm = (cr + cr)/2. If FractionTest returns True, which means that there are at least 5% fraction of arms with small means in Sr, we sample all arms in Sr uniformly by calling UniformSample(Sr, (cr \u2212 cm)/2, \u03b4r) where \u03b4r = \u03b4/(10 \u00b7 2r), and retain those with empirical means at least (cm+ cr)/2. If FractionTest returns False (meaning that 90% arms have means at least cl) the algorithm terminates and returns the remaining arms. The guarantee of Elimination is summarized in the following lemma. The proof can be found in Section F.\nLemma B.4. Suppose \u03b4 < 0.1. Let S\u2032 = Elimination(S, cl, cr, \u03b4). Let A1 be the best arm among S, with mean \u00b5[A1] \u2265 cr. Then with probability at least 1\u2212 \u03b4, the following statements hold\n1. A1 \u2208 S\u2032 (the best arm survives);\n2. |S\u2032\u2264cl | < 0.1|S\u2032| (only a small fraction of arms have means less than cl);\nAlgorithm 5: DistrBasedElim(S, \u03b4)\nData: Arm set S, confidence level \u03b4. Result: The best arm.\n1 h \u2190 1 2 S1 \u2190 S 3 for r = 1 to +\u221e do 4 if |Sr| = 1 then 5 Return the only arm in Sr 6 \u03b5r \u2190 2\u2212r 7 \u03b4r \u2190 \u03b4/50r2 8 ar \u2190 MedianElim(Sr, \u03b5r/4, 0.01). 9 \u00b5\u0302[ar] \u2190 UniformSample({ar}, \u03b5r/4, \u03b4r)\n10 if FractionTest(Sr, \u00b5\u0302[ar ] \u2212 1.5\u03b5r , \u00b5\u0302[ar ] \u2212 1.25\u03b5r , \u03b4r, 0.4, 0.1) then 11 \u03b4h \u2190 \u03b4/50h2 12 br \u2190 MedianElim(Sr, \u03b5r/4, \u03b4h) 13 \u00b5\u0302[br] \u2190 UniformSample({br}, \u03b5r/4, \u03b4h) 14 Sr+1 \u2190 Elimination(Sr, \u00b5\u0302[br ] \u2212 0.5\u03b5r, \u00b5\u0302[br ] \u2212 0.25\u03b5r , \u03b4h) 15 h \u2190 h+ 1 16 else 17 Sr+1 \u2190 Sr\n3. The number of samples is O(|S| ln \u03b4\u22121\u2206\u22122), in which \u2206 = cr \u2212 cl."}, {"heading": "B.2 Our Algorithm", "text": "Now, everything is ready to describe our algorithm DistrBasedElim for Best-1-Arm. We provide a high level description here. All detailed parameters can be found in Algorithm 5. The algorithm runs in rounds. It maintains the current set Sr of arms. Initially, S1 is the set of all arms S. In round r, the algorithm tries to eliminate a set of suboptimal arms, while makes sure the best arm is not eliminated. First, it applies the MedianElim procedure to find an \u03b5r/4-optimal arm, where \u03b5r = 2\n\u2212r. Suppose it is ar. Then, we take a number of samples from ar to estimate its mean (denote the empirical mean by \u00b5\u0302[ar ]). Unlike previous algorithms in [12, 24], which eliminates either a fixed fraction of arms or those arms with mean much less than ar, we use a FractionTest to see whether there are many arms with mean much less than ar. If it returns True, we apply the Elimination procedure to eliminate those arms (for the purpose of analysis, we need to use MedianElim again, but with a tighter confidence level, to find an \u03b5r/4-optimal arm br). If it returns False, the algorithm decides that it is not judicious to do elimination in this round (since we need to spend a lot of samples, but only discard very few arms, which is wasteful), and simply sets Sr+1 to be Sr, and then proceeds to the next round.\nWe devote the rest of the section to prove that Algorithm 5 indeed solves the Best-1-Arm problem and achieves the sample complexity stated in Theorem 2.5. To simplify the argument, we first describe some event we will condition on for the rest of the proof, and show the algorithm indeed finds the best arm under the condition.\nLemma B.5. Let EG denote the event that all procedure calls in line 9, 10, 12, 13, 14 return correctly for all rounds. EG happens with probability at least 1\u2212 \u03b4. Moreover, conditioning on EG, the algorithm outputs the correct answer.\nProof. By Lemma B.1, Lemma B.3 and Lemma B.4, we can simply bound the total error probability with a union bound over all procedure calls in all rounds:\n+\u221e\u2211\nr=1\n2\u03b4r +\n+\u221e\u2211\nh=1\n3\u03b4h \u2264 \u03b4 \u00b7 5 +\u221e\u2211\ni=1\n1/50i2 \u2264 \u03b4.\nTo prove the correctness, it suffices to show that the best arm A1 is never eliminated in line 14. Conditioning on event EG, for all r, we have \u00b5[br] \u2264 \u00b5[A1] and |\u00b5\u0302[br] \u2212 \u00b5[br ]| < \u03b5r/4, thus \u00b5\u0302[br] < \u00b5[A1] + \u03b5r/4. Clearly, this means \u00b5[A1] \u2265 \u00b5\u0302[br] \u2212 0.25\u03b5r . Then by Lemma B.4, we know that A1 has survived round r.\nNote that the correctness of MedianElim (line 8) is not included in event EG."}, {"heading": "B.3 Analysis of the running time", "text": "We use A to denote Algorithm 5. Let\nT (\u03b4, I) =\nn\u2211\ni=2\n\u2206\u22122[i] ( ln \u03b4\u22121 + ln lnmin(n,\u2206\u22121[i] ) ) +\u2206\u22122[2] ln ln\u2206 \u22121 [2]\nbe the target upper bound we want to prove. We need to prove EA\u03b4,I [\u03c4 | EG] = O(T (\u03b4, I)) for \u03b4 < 0.1. In the rest of the proof, we condition on the event EG, unless state otherwise. Let A1 denote the best arm.\nWe need some additional notations. First, for all s \u2208 N, define the sets of arms U s, U\u2265s and U\u2264s as:\nU s = {a | 2\u2212s \u2264 \u2206[a] < 2\u2212s+1}, U\u2265s = +\u221e\u22c3\nr=s\nU r, U\u2264s =\ns\u22c3\nr=1\nU r\nNote that the best arm is not in any of the above set. Let \u03b5s = 2 \u2212s. It is also convenient to use the following equivalent definitions for U\u2265s and U\u2264s:\nU\u2265s = {a | \u00b5[A1] \u2212 2\u03b5s < \u00b5[a] < \u00b5[A1]}, U\u2264s = {a | \u00b5[a] \u2264 \u00b5[A1] \u2212 \u03b5s}\nLet maxs be the maximum s such that U s is not empty.\nWe start with a lemma which concerns the ranges of \u00b5\u0302[ar] and \u00b5\u0302[br].\nLemma B.6. Conditioning on EG, the following statements hold:\n1. For any round r, \u00b5\u0302[ar] < \u00b5[A1] + \u03b5r/4. In addition, if MedianElim (line 8) returns an \u03b5r/4approximation correctly in that round, we also have \u00b5\u0302[ar ] > \u00b5[A1] \u2212 2\u03b5r/4.\n2. For any round r, if br exists (the algorithm enters line 12), then \u00b5\u0302[br] < \u00b5[A1] + \u03b5r/4 and \u00b5\u0302[br] > \u00b5[A1] \u2212 2\u03b5r/4.\nProof. Conditioning on EG, we have that |\u00b5\u0302[ar ] \u2212 \u00b5[ar]| < \u03b5r/4. Clearly \u00b5[ar] \u2264 \u00b5[A1]. Hence, \u00b5\u0302[ar ] < \u00b5[A1] + \u03b5r/4. If ar is an \u03b5r/4-approximation of A1, we have \u00b5[ar] \u2265 \u00b5[A1] \u2212 \u03b5r/4. Then, we can see \u00b5\u0302[ar] > \u00b5[A1] \u2212 2\u03b5r/4. Note that conditioning on EG, br is always an \u03b5r/4-approximation of A1. Hence the second claim follows exactly in the same way.\nThen we give an upper bound of h (updated in line 15). Note that h indicates how many times we pass the FractionTest and execute Elimination (line 14). Indeed, in the following analysis, we only need an upper bound of h during the first maxs rounds. We introduce the definition first.\nDefinition B.7. Given an instance I, conditioning on EG, we denote the maximum value of h during the first maxs rounds as hI . It is easy to see that hI \u2264 maxs.\nLemma B.8. hI = O(ln n) for any instance I.\nProof. Suppose we enter line 12 at round r. By Lemma B.6, we have \u00b5\u0302[ar] < \u00b5[A1] + \u03b5r/4 and \u00b5\u0302[br] > \u00b5[A1] \u2212 2\u03b5r/4. Hence \u00b5\u0302[br ] > \u00b5\u0302[ar] \u2212 0.75\u03b5r .\nBy Lemma B.3, we know there is at least 0.3 fraction of arms in Sr with means \u2264 \u00b5\u0302[ar ]\u22121.25\u03b5r . But by Lemma B.4, we know that after executing line 14, there are at most 0.1 fraction of arms in Sr with means \u2264 \u00b5\u0302[br] \u2212 0.5\u03b5r . By noting that \u00b5\u0302[br] \u2212 0.5\u03b5r > \u00b5\u0302[ar ] \u2212 1.25\u03b5r . we can see that |Sr| drops by at least a constant fraction whenever we enter line 14. Therefore, h can increase by 1 for at most O(lnn) times.\nRemark B.9. Conditioning on EG, for all round r \u2264 maxs, we can see h \u2264 r. Thus, h \u2264 min(hI , r), and ln \u03b4 \u22121 h = O(ln \u03b4 \u22121 + ln[min(hI , r)]).\nNow, we describe some behaviors of MedianElim and Elimination before we analyze the sample complexity.\nLemma B.10. If FractionTest (line 10) outputs True, we know that there are > 0.3 fraction of arms with means \u2264 \u00b5[A1] \u2212 \u03b5r in Sr. In other words, |U\u2264r \u2229 Sr| > 0.3|Sr|. Moreover, we have that |U\u2264r \u2229 Sr+1| \u2264 0.1|Sr+1| (i.e., we can eliminate a significant portion in this round).\nProof. By Lemma B.6, we can see that \u00b5\u0302[ar ] < \u00b5[A1]+ \u03b5r/4, \u00b5\u0302[br] > \u00b5[A1]\u2212 2\u03b5r/4. Now consider the parameters for FractionTest. Let cr = \u00b5\u0302[ar] \u2212 1.25\u03b5r . Then we have that cr < \u00b5[A1] \u2212 \u03b5r.\nBy Lemma B.3, when FractionTest outputs True, we know that there are > (0.4 \u2212 0.1) = 0.3 fraction of arms with means \u2264 cr \u2264 \u00b5[A1] \u2212 \u03b5r in Sr. Clearly, \u00b5[a] \u2264 \u00b5[A1] \u2212 \u03b5r is equivalent to a \u2208 U\u2264r for an arm a.\nNow consider the parameters for Elimination. Let cl = \u00b5\u0302[br ] \u2212 0.5\u03b5r . Then cl > \u00b5[A1] \u2212 \u03b5r. We also note that \u00b5[a] \u2264 \u00b5[A1] \u2212 \u03b5r is equivalent to a \u2208 U\u2264r for an arm a. Then by Lemma B.4, after the elimination, we have |U\u2264r \u2229 Sr+1| \u2264 |S\u2264clr+1| \u2264 0.1|Sr+1|.\nLemma B.11. Consider a round r. Suppose MedianElim (line 8) returns a correct \u03b5r/4-approximation ar. Then, the following statements hold:\n1. If FractionTest (line 10) outputs True, we know there are > 0.3 fraction of arms with means \u2264 \u00b5[A1]\u2212 \u03b5r in Sr. In other words, |U\u2264r \u2229Sr| > 0.3|Sr|. Moreover, we have that |U\u2264r \u2229Sr+1| \u2264 0.1|Sr+1|.\n2. If it outputs False, we know there are at least 0.5 fraction of arms with means at least \u00b5[A1]\u22122\u03b5r in Sr. In other words, |U\u2265r \u2229 Sr|+ 1 > 0.5|Sr|.\nProof. Since MedianElim (line 8) returns the correctly, by Lemma B.6, \u00b5\u0302[ar ] > \u00b5[A1] \u2212 2\u03b5r/4 and \u00b5\u0302[ar ] < \u00b5[A1] + \u03b5r/4. Now consider the parameters for FractionTest, let cl = \u00b5\u0302[ar] \u2212 1.5\u03b5r and cr = \u00b5\u0302[ar] \u2212 1.25\u03b5r , It is easy to see that cl > \u00b5[A1] \u2212 2\u03b5r, and cr < \u00b5[A1] \u2212 \u03b5r.\nThe first claim just follows from Lemma B.10 (note that Lemma B.10 does not require the output of MedianElim (line 8) being correct).\nBy Lemma B.3, if FractionTest outputs False, we know there are at least (1 \u2212 0.4 \u2212 0.1) = 0.5 fraction of arms with means \u2265 cl > \u00b5[A1] \u2212 2\u03b5r in Sr. For an arm a, \u00b5[a] > \u00b5[A1] \u2212 2\u03b5r is equivalent to a \u2208 U\u2265r or a is the best arm A1 itself.\nWe also need the following lemma describing the behavior of the algorithm when r > maxs.\nLemma B.12. For each round r > maxs, the algorithm terminates if MedianElim returns an \u03b5r/4-approximation correctly, which happens with probability at least 0.99.\nProof. If we already have |Sr| = 1 at the beginning of round r, then there is nothing to prove since it halts immediately. So we can assume |Sr| > 1.\nSuppose in round r, MedianElim returns a correct \u03b5r/4-approximation. Conditioning on EG, FractionTest must output True. Since if it outputs False, then by Lemma B.11, |U\u2265r \u2229 Sr| + 1 > 0.5|Sr|. But U\u2265r = \u2205 from r > maxs. So 1 > 0.5|Sr|, which implies |Sr| = 1, rendering a contradiction.\nThen, by Lemma B.11, |U\u2264r \u2229 Sr+1| \u2264 0.1|Sr+1|, which is equivalent to the fact that |U\u2265r+1 \u2229 Sr+1|+ 1 \u2265 0.9|Sr+1|. Note that |U\u2265r+1 \u2229 Sr+1| = 0 as U\u2265r+1 = \u2205. So it holds that 1 \u2265 0.9|Sr+1|, or equivalently |Sr+1| = 1. Thus it terminates right after round r.\nFinally, as MedianElim returns correctly an \u03b5r/4-approximation with probability at least 0.99, the proof is completed.\nWe analyze the expected number of samples used for each subprocedure separately. We first consider FractionTest (line 10) and UniformSample (line 9, 13) and prove the following simple lemma.\nLemma B.13. Conditioning on event EG, the expected number of samples incurred by FractionTest (line 10) and UniformSample (line 9, 13) is\nO ( \u2206\u22122[2] (ln \u03b4 \u22121 + ln ln\u2206\u22121[2] )) ) .\nProof. By Lemma B.12, for any round r > maxs, the algorithm halts w.p. at least 0.99. So we can bound the expectation of samples incurred by FractionTest and UniformSample by:\nmaxs\u2211\nr=1\nc4 \u00b7 ln \u03b4\u22121r \u03b5\u22122r + +\u221e\u2211\nr=maxs+1\nc4 \u00b7 (0.01)r\u2212maxs\u22121 ln \u03b4\u22121r \u03b5\u22122r\nHere, c4 is a constant large enough such that in round r the number of samples taken by FractionTest and UniformSample together is bounded by c4 \u00b7 ln \u03b4\u22121r \u03b5\u22122r . It is not hard to see the first sum is dominated by the last term while the second sum is dominated by the first. So the bound is O(\u2206\u22122[2] (ln \u03b4 \u22121 + ln ln\u2206\u22121[2] )) since maxs is \u0398(ln\u2206 \u22121 [2] ).\nNext, we analyze MedianElim (line 8, 12) and Elimination (line 14). In the following, we only consider samples due to these two procedures.\nLemma B.14. Conditioning on event EG, the expected number of samples incurred by MedianElim (line 8, 12) and Elimination (line 14) is\nO\n( n\u2211\ni=2\n\u2206\u22122[i] (ln \u03b4 \u22121 + ln[min(hI , ln\u2206 \u22121 [i] )])\n) .\nWe devote the rest of the section to the proof of Lemma B.14, which is more involved. We first need a lemma which provides an upper bound on the number of samples for one round.\nLemma B.15. Let c3 be a sufficiently large constant. The number of samples in round r \u2264 maxs can be bounded by\n{ c3 \u00b7 |Sr|\u03b5\u22122r if FractionTest outputs False. c3 \u00b7 |Sr|\u03b5\u22122r (ln \u03b4\u22121 + ln[min(hI , r)]) if FractionTest outputs True.\nIf r > maxs, the number of samples can be bounded by\nc3 \u00b7 |Sr|\u03b5\u22122r (ln \u03b4\u22121 + ln(hI + r \u2212maxs)).\nProof. Note that conditioning on event EG, Elimination always returns correctly. Let c3 be a constant such that MedianElim(Sr, \u03b5r/4, 0.01) takes no more than c3/3 \u00b7 |Sr|\u03b5\u22122r samples, and MedianElim(Sr, \u03b5r/4, \u03b4h) and Elimination(Sr, \u00b5\u0302[ar ]\u2212 1.5\u03b5r , \u00b5\u0302[ar ]\u2212 1.25\u03b5r , \u03b4h) both take no more than c3/3 \u00b7 |S|\u03b5\u22122r (ln \u03b4\u22121 + ln[min(hI , r)]) samples conditioning on EG. The later one is due to the fact that ln \u03b4\u22121h = O(ln \u03b4\n\u22121 + ln[min(hI , r)]) for r \u2264 maxs. If r > maxs, we have h \u2264 hI + r \u2212 maxs, then the bounds follow from a simple calculation.\nProof of Lemma B.14. We prove the lemma inductively. Let T (r,Nsma) denote the maximum expected total number of samples the algorithm takes at and after round r, when |Sr\u2229U\u2264r\u22121| \u2264 Nsma. In other words, it is an upper bound of the expected number of samples we will take further, provided that we are at the beginning of round r and there are at most Nsma \u201csmall\u201d arms left. By definition, T (1, 0) is the final upper bound for the total expected number of samples taken by the algorithm.\nLet c1 = 4c3, c2 = 60c3. 9 For ease of notation, we let ls = ln(min(hI , s)). We first consider the case where r = maxs + 1 and prove the following bound of T (r,Nsma):\nT (r,Nsma) \u2264 (ln \u03b4\u22121 + lnhI)c1 \u00b7Nsma \u00b7 \u03b5\u22122r . (2)\nClearly there is nothing to prove for the base case Nsma = 0. So we consider Nsma \u2265 1. Now, suppose the first round after r in which MedianElim (line 8) returns correctly an \u03b5r/4-approximation is r\u2032 \u2265 r. Clearly, this happens with probability at most 0.01r\u2032\u2212r (all rounds in between fail). By Lemma B.12, the algorithm terminates after round r\u2032. Moreover, we have |U\u2265r \u2229 Sr| = 0 since U\u2265r = \u2205. So Sr consists of the single best arm A1 and Nsma arms in U\u2264r\u22121. By Lemma B.15, the number of samples is bounded by\nr\u2032\u2211\ni=r\nc3(ln \u03b4 \u22121 + ln[hI + i\u2212maxs])(1 +Nsma)\u03b5\u22122i .\n9 Although these constants are chosen somewhat arbitrarily, they need to satisfy certain relations (will be clear from the proof) and it is necessary to make them explicit.\nHence, we can bound T (r,Nsma) as follows:\nT (r,Nsma) \u2264 +\u221e\u2211\nr\u2032=r\n(0.01)r \u2032\u2212r \u00b7\nr\u2032\u2211\ni=r\nc3 ( ln \u03b4\u22121 + ln[hI + i\u2212maxs])(1 +Nsma)\u03b5\u22122i\n\u22642c3Nsma +\u221e\u2211\nr\u2032=r\n(0.01)r \u2032\u2212r \u00b7\nr\u2032\u2211\ni=r\n(ln \u03b4\u22121 + ln[hI + i\u2212maxs])\u03b5\u22122i\n\u22643c3Nsma +\u221e\u2211\nr\u2032=r\n(0.01)r \u2032\u2212r \u00b7 (ln \u03b4\u22121 + ln[hI + r\u2032 \u2212maxs])\u03b5\u22122r\u2032\n\u22644c3Nsma(ln \u03b4\u22121 + lnhI)\u03b5\u22122r .\nNow, we analyze the more challenging case where r \u2264 maxs. For ease of notation, we let\nCr,s = (ln \u03b4 \u22121 + ls)\ns\u2211\ni=r\n\u03b5\u22122i .\nWhen r > s, we let Cr,s = 0. We also define\nPr = c2 \u00b7 ( +\u221e\u2211\ns=r\nCr,s|U s|+ Cr,maxs\n) .\nPr can be viewed as a potential function. Notice that when r > maxs, we have Pr = 0. We are going to show inductively that\nT (r,Nsma) \u2264 (ln \u03b4\u22121 + lr) \u00b7 c1 \u00b7Nsma \u00b7 \u03b5\u22122r + Pr. (3)\nWe note that (2) is in fact consistent with (3) (when r > maxs, we have Pr = 0 and lr = lnhI). The induction hypothesis assumes that the inequality holds for r + 1 and all Nsma. We need to prove that it also holds for r and all Nsma. Now we are at round r. Conditioning on event EG, there are three cases we need to consider. We state the following lemmas, each analyzes one case, which together imply our time bound. Their proofs are not difficult but somewhat tedious. So we defer them to Section F.\nLemma B.16. Suppose that MedianElim (line 8) returns an \u03b5r/4-approximation of the best arm A1, and FractionTest outputs True. The expected number of samples taken at and after round r is bounded by\n(ln \u03b4\u22121 + lr)c3Nsma\u03b5 \u22122 r + Pr.\nLemma B.17. Suppose that MedianElim (line 8) returns an \u03b5r/4-approximation of the best arm A1, and FractionTest outputs False. The expected number of samples taken at and after round r is bounded by Pr.\nLemma B.18. Suppose that MedianElim (line 8) returns an arm which is not an \u03b5r/4-approximation of the best arm A1. The expected number of samples taken at and after round r is bounded by\n(ln \u03b4\u22121 + lr)(c3 + 5c1)Nsma\u03b5 \u22122 r + Pr.\nNote that the bound in Lemma B.18 is larger than we need to prove (in particular, the constant is larger). So, we need to combine three cases together as follows:\nRecall that MedianElim (line 8) returns correctly an \u03b5r/4-approximation with probability p (p \u2265 0.99). By Lemma B.16, Lemma B.17 and Lemma B.18, we have that\nT (r,Nsma) \u2264 (ln \u03b4\u22121 + lr)(p \u00b7 (c3 \u00b7Nsma \u00b7 \u03b5\u22122r ) + (1\u2212 p) \u00b7 (c3 + 5c1) \u00b7Nsma\u03b5\u22122r ) + Pr \u2264 (ln \u03b4\u22121 + lr)((c3 + 0.05c1) \u00b7Nsma \u00b7 \u03b5\u22122r ) + Pr (c3 + (1\u2212 p) \u00b7 5c1 \u2264 c3 + 0.05c1) \u2264 (ln \u03b4\u22121 + lr)c1 \u00b7Nsma \u00b7 \u03b5\u22122r + Pr (c3 + 0.05c1 \u2264 c1)\nThis finishes the proof of (3). Hence, the number of samples is bounded by T (1, 0) \u2264 P1. Note that C1,s \u2264 2(ln \u03b4\u22121 + ls)\u03b5\u22122s for any s. By a simple calculation of P1, we can see that the overall sample complexity for MedianElim and Elimination is\nT (1, 0) \u2264 P1 = O ( n\u2211\ni=2\n\u2206\u22122[i] (ln \u03b4 \u22121 + ln[min(hI , ln\u2206 \u22121 [i] )])\n) .\nThis finally completes the proof of Lemma B.14.\nPutting Lemma B.14 and Lemma B.13 together, we have the following corollary.\nCorollary B.19. With probability 1\u2212 \u03b4, Algorithm 5 returns the correct answer for Best-1-Arm and take at most O(T ) sample in expectation, where\nT =\nn\u2211\ni=2\n\u2206\u22122[i] (ln \u03b4 \u22121 + ln[min(hI , ln\u2206 \u22121 [i] )]) + \u2206 \u22122 [2] ln ln\u2206 \u22121 [2] .\nThe time bound in Theorem 2.5 is an immediate consequence of the above corollary and Lemma B.8, which asserts hI = O(ln n).\nHowever, there is one subtlety: we only provide a bound on the running time conditioning on EG (i.e., E[TA | EG]). With probability at most \u03b4 (when the event EG fails), we do not have any bound on the running time (or the number of samples) of the algorithm (the algorithm may not terminate). So strictly speaking, the overall expected running time of DistrBasedElim is not bounded. In fact, several previous algorithms in the literature [13, 24, 16] have the same problem. However, it is possible to transform such an algorithm A to another A\u2032 which succeeds with probability 1\u2212 \u03b4 and whose overall expected running time is bounded as E[TA\u2032 ] = O(E[TA | EG]). We are not aware such a transformation in the literature and we provide one in Section H."}, {"heading": "B.4 Almost Instance Optimal Bound for Clustered Instances", "text": "Recall U s = {a | 2\u2212s \u2264 \u2206[a] < 2\u2212s+1}.\nDefinition B.20. We say an instance I is clustered if |{U i 6= \u2205 | 1 \u2264 i \u2264 maxs}| is bounded by a constant.\nIn this case, we can obtain an almost instance optimal algorithm for such instances. For this purpose, we only need to establish a tighter bound on hI .\nLemma B.21. hI \u2264 2 \u00b7 |{U i 6= \u2205 | 1 \u2264 i \u2264 maxs}|.\nProof. Let s be an index such that U s is not empty. Let s\u2032 be the largest index < s such that U s \u2032\nis not empty. If such index does not exist, let s\u2032 = 0. We show that during rounds s\u2032+1, s\u2032+2, . . . , s\u22121, we can only call Elimination (or equivalently, increase h) once.\nSuppose otherwise, we call Elimination in round r and r\u2032 such that s\u2032 < r < r\u2032 < s. We further assume that there is no other call to Elimination between round r and r\u2032. Then clearly Sr\u2032 = Sr+1.\nNow by Lemma B.10, we have |U\u2264r \u2229 Sr+1| \u2264 0.1|Sr+1|, but this means |Ur\u2032 \u2229 Sr\u2032 | \u2264 0.1|S\u2032r|, as Ur\u2032 = U \u2264r and Sr\u2032 = Sr+1. Again by Lemma B.10, this contradicts the fact that on round r \u2032, FractionTest outputs true. As Umaxs is not empty, we can partition the rounds 1, 2, . . . ,maxs into at most 2 \u00b7 |{U i 6= \u2205 | 1 \u2264 i \u2264 maxs}| groups. Each group is either a single round which corresponds to a non-empty set U s, or the rounds between two rounds corresponding to two adjacent non-empty sets U s \u2032\nand U s. Therefore, h can increase at most by 1 in each group, which concludes the proof.\nThe following theorem is an immediate consequence of the above lemma and Corollary B.19.\nTheorem B.22. There is an \u03b4-correct algorithm for clustered instances, with expected sample complexity\nT (\u03b4, I) = O (\u2211n\ni=2 \u2206\u22122[i] ln \u03b4 \u22121 +\u2206\u22122[2] ln ln\u2206 \u22121 [2]\n) .\nExample B.23. Consider a very simple yet important instance where there are n \u2212 1 arms with mean 0.5, and a single arm with mean 0.5 +\u2206. In fact, a careful examination of all previous algorithms (including [20, 24]) shows that they all require \u2126 ( n\u2206\u22122(ln \u03b4\u22121 + ln ln\u2206\u22121) ) samples even in this particular instance. However, our algorithm only requires O ( n\u2206\u22122 ln \u03b4\u22121 +\u2206\u22122 ln ln\u2206\u22121 ) samples. Our bound is almost instance optimal, since the first term matches the instance-wise lower bound n\u2206\u22122 ln \u03b4\u22121. This is the best bound for such instances we can hope for."}, {"heading": "B.5 An Improved PAC Algorithm", "text": "Finally, we discuss how to convert our algorithm to an (\u03b5, \u03b4)-PAC algorithm for Best-1-Arm. Our result improves several previous PAC algorithm, which we summarize in Table 2.\nTheorem B.24. For any \u03b5 < 0.5 and \u03b4 < 0.1, there exists an (\u03b5, \u03b4)-PAC algorithm for Best-1Arm, with expected sample complexity\nT (\u03b4, I) = O\n( n\u2211\ni=2\n\u2206\u22122i,\u03b5 (ln \u03b4 \u22121 + ln lnmin(n,\u2206\u22121i,\u03b5 )) + \u2206 \u22122 2,\u03b5 ln ln\u2206 \u22121 2,\u03b5\n) ,\nwhere \u2206i,\u03b5 = max(\u2206[i], \u03b5).\nProof. Given parameters \u03b5, \u03b4, we run DistrBasedElim with confidence \u03b4/2 only for the first \u2308ln \u03b5\u22121\u2309 rounds. After that, we invoke MedianElim with confidence \u03b4/2 to find an \u03b5-optimal arm among S\u2308log2 \u03b5\u22121\u2309. Clearly we are correct with probability at least 1 \u2212 \u03b4. The analysis for the sample complexity is exactly the same as the original DistrBasedElim.\nC A New Lower bound for Best-1-Arm\nIn this section, we provide a new lower bound for Best-1-Arm. In particular, we prove Theorem 3.1. From now on, \u03b4 is a fixed constant such that 0 < \u03b4 < 0.005. We use [0, N ] to denote the set of integers {0, 1, . . . , N}. Throughout this section, we assume the distributions of all the arms are Gaussian with variance 1.\nOur proof for Theorem 3.1 consists of two ingredients. The first one is a non-trivial lower bound for Sign-\u03be, which we state here but defer its proof to the next section. The second one is a novel reduction from Sign-\u03be to Best-1-Arm, which turns the lower bound for Sign-\u03be into the desired lower bound for Best-1-Arm.\nFor stating the lower bound for Sign-\u03be we introduce some notations first. Let A\u2032 denote an algorithm for Sign-\u03be, A\u00b5 be an arm with mean \u00b5 (i.e., with distribution N (\u00b5, 1)), and we define TA\u2032(\u2206) = max(TA\u2032(A\u03be+\u2206), TA\u2032(A\u03be\u2212\u2206)). Then we have the following lower bound for Sign-\u03be. The proof is deferred to Section D.\nLemma C.1. For any \u03b4\u2032-correct algorithm A\u2032 for Sign-\u03be with \u03b4\u2032 \u2264 0.01, there exist constants N0 \u2208 N and c1 > 0 such that for all N \u2265 N0:\n|{TA\u2032(\u2206) < c1 \u00b7\u2206\u22122 lnN | \u2206 = 2\u2212i, i \u2208 [0, N ]}| \u2264 0.1(N + 1).\nFor an algorithm A for Best-1-Arm, let Aperm be the algorithm which first randomly permutes the input arms, then runs A. More precisely, given an arm instance I with n arms, Aperm first chooses a random permutation \u03c0 on n elements in I uniformly, then simulates A on the instance \u03c0 \u25e6 I and returns what A returns. It is not difficult to see that the running time of Aperm only depends on the set {Di} of reward distributions of the instance, not their particular order.\nClearly, if A is a \u03b4-correct algorithm for any instance of Best-1-Arm, so is Aperm. Furthermore, we have the following simple lemma, which says we only need to prove a lower bound for Aperm.\nLemma C.2. For any instance I, there exists a permutation \u03c0 such that TA(\u03c0 \u25e6 I) \u2265 TAperm(I).\nProof. By the definition of Aperm, we can see that\nTAperm(I) = 1\nn!\n\u2211\n\u03c0\u2208Sym(n)\nTA(\u03c0 \u25e6 I),\nwhere Sym(n) is the set of all n! permutations of {1, . . . , n}.\nNow we prove theorem 3.1. The high-level idea is to construct some \u201cbalanced\u201d instances for Best-1-Arm, and show that if an algorithm A is \u201cfast\u201d on those instances, we can construct a fast algorithm for Sign-\u03be, which leads to a contradiction to Lemma C.1.\nProof of Theorem 3.1. In this proof, we assume all distributions are Gaussian random variables with \u03c3 = 1. Without loss of generality, we can assume N0 in Lemma C.1 is an even integer, and N0 > 10. So we have 2 \u00b7 4N0 \u2265 43 \u00b7 4N0 +N0 + 2. Let N = 2 \u00b7 4N0 .\nFor every n \u2265 N , we pick the largest even integer m such that 2 \u00b74m \u2264 n. Clearly m \u2265 N0 > 10 and \u2211m k=0 4\nk +m+ 2 \u2264 43 \u00b7 4m +m+ 2 \u2264 2 \u00b7 4m. Also, by the choice of m, we have 2 \u00b7 4m+2 > n, hence 4m > n8 .\nConsider the following Best-1-Arm instance Iinit with n arms:\n1. There is a single arm with mean \u03be.\n2. For each integer k \u2208 [0,m], there are 4m\u2212k arms with mean \u03be \u2212 2\u2212k. 3. There are n\u2212\u2211mk=0 4k \u2212 1 arms with mean \u03be \u2212 2.\nFor a Best-1-Arm instance I, let n(I) be the number of arms in I, and \u2206[i](I) be the gap \u2206[i] according to I. We denote H(I) = \u2211n(I)i=2 \u2206[i](I)\u22122. Now we define a class of Best-1-Arm instances {IS} where each S \u2286 {0, 1, . . . ,m}. Each IS is formed as follows: for every k \u2208 S, we add one more arm with mean \u03be \u2212 2\u2212k to Iinit; finally we remove |S| arms with mean \u03be \u2212 2 (by our choice of m there are enough such arms to remove). Obviously, there are still n arms in every instance IS.\nLet c be a universal constant to be specified later (in particular c does not dependent on n). Now we claim that for any \u03b4-correct algorithm A for Best-1-Arm, there must exist an instance IS such that\nTAperm(IS) > c \u00b7 H(IS) \u00b7 lnm = \u2126(H(IS) ln lnn). Suppose for contradiction that there exists a \u03b4-correct A such that TAperm(IS) \u2264 c \u00b7 H(IS) \u00b7 lnm for all S. Let U = {IS | |S| = m/2}, V = {IS | |S| = m/2 + 1} be two sets of Best-1-Arm instances.\nNotice that |U | = |V | = (m+1 m/2 ) (since m is even).\nFix S \u2208 U . Consider the problem Sign-\u03be, in which the given instance is a single arm A with unknown mean \u00b5, and we would like to decide whether \u00b5 > \u03be or \u00b5 < \u03be. Consider the following two algorithms for Sign-\u03be, which call Aperm as a subprocedure.\n1. A1S : We first create a Best-1-Arm instance instance Inew by replacing one arm with mean \u03be\u22122 in IS with A. Then run Aperm on Inew. We output \u00b5 > \u03be if Aperm selects A as the best arm. Otherwise, we output \u00b5 < \u03be.\n2. A2S : We first construct an artificial arm Anew with mean 2\u03be \u2212 \u00b5 from A 10 , and create a Best1-Arm instance Inew by replacing one arm with mean \u03be \u2212 2 in IS with Anew. Then run Aperm on Inew. We output \u00b5 < \u03be if Aperm selects Anew as the best arm. Otherwise, we output \u00b5 > \u03be.\nSince Aperm is \u03b4-correct for Best-1-Arm, A 1 S and A 2 S are both \u03b4-correct for Sign-\u03be.\nNow, consider the algorithm AS for Sign-\u03be which runs as follows: It simulates A 1 S and A 2 S simultaneously. Each time it takes a sample from the input arm, and feeds it to both A1S and A 2 S . If A1S (A 2 S resp.) terminates first, it returns the output of A 1 S (A 2 S resp.). In case of a tie, it returns the output of A1S .\n10That is, whenever the algorithm pulls Anew, we pull A to get a reward r, and return 2\u03be \u2212 r as the reward for Anew. Note although we do not know \u00b5, Anew is clearly an arm with mean 2\u03be \u2212 \u00b5.\nFirst, we can see that if both A1S and A 2 S are correct, then AS must be correct. Therefore, AS is 2\u03b4-correct for Sign-\u03be. Then we are going to show that there exists some particular S such that the algorithm AS runs too fast for way too many points in {\u2206 = 2\u2212i}i\u2208[0,m] for Sign-\u03be hence rendering a contradiction to Lemma C.1.\nFor a Best-1-Arm instance IS and an integer k \u2208 [0,m], we use NkS to denote the number of arms with gap 2\u2212k. Let akS \u00b7 4k be the expected number of samples taken from an arm with gap 2\u2212k by Aperm. Then we have that\nm\u2211\nk=0\nNkS(4 k \u00b7 akS) \u2264 TAperm(IS) \u2264 c \u00b7 H(IS) \u00b7 lnm.\nSince 4m\u2212k \u2264 NkS \u2264 4m\u2212k + 1, we can see that\nH(IS) = m\u2211\nk=0\nNkS \u00b7 4k + (n\u2212 m\u2211\nk=0\n4k \u2212 1\u2212 |S|) \u00b7 2\u22122 \u2264 2 m\u2211\nk=0\n4m + 1\n4 \u00b7 n.\nThus we have that\nm\u2211\nk=0\n4m\u2212k(4k \u00b7 akS) \u2264 m\u2211\nk=0\nNkS(4 k \u00b7 akS) \u2264 c \u00b7 H(IS) \u00b7 lnm \u2264 c\n( 2 m\u2211\nk=0\n4m + 1\n4 n\n) \u00b7 lnm.\nSimplifying it a bit and noting that n8 < 4 m, we get that\nm\u2211\nk=0\n4m \u00b7 akS \u2264 c \u00b7 2(m+ 2)4m lnm,\nwhich is equivalent to m\u2211\nk=0\nakS \u2264 2c \u00b7 (m+ 2) lnm \u2264 3c \u00b7 (m+ 1) lnm.\nThe last inequality holds since m \u2265 N0 > 10. Now we set c = c130 , in which c1 is the constant in Lemma C.1. Since \u2211m k=0 a k S \u2264 3c\u00b7(m+1) lnm = c1 10 \u00b7(m+1)\u00b7lnm, we can see for any S, there are at most 0.1 fraction of elements in {akS}mk=0 satisfying akS \u2265 c1 \u00b7 lnm.\nThen for S \u2208 U , let badS = {k 6\u2208 S \u2227 akS\u222a{k} \u2265 c1 lnm | k \u2208 [0,m]}. We have that\n\u2211\nS\u2208U\n|badS | \u2264 \u2211\nS\u2208V\nm\u2211\nk=0\n1{akS \u2265 c1 lnm} \u2264 m+ 1 10 |V | = m+ 1 10 |U |.\nBy an averaging argument, there exists S \u2208 U such that |badS | \u2264 m+110 . We will show for that particular S, the algorithm AS for Sign-\u03be contradicts Lemma C.1.\nNow we analyze the expected total number of samples taken by AS on arm A with mean \u00b5 and gap \u2206 = |\u03be \u2212 \u00b5| = 2\u2212k. Suppose k /\u2208 S. A key observation is the following: if \u00b5 < \u03be, then the instance constructed in A1S is exactly IS\u222a{k}; otherwise \u00b5 > \u03be, since 2\u03be\u2212 (\u03be+\u2206) = \u03be\u2212\u2206 = \u03be\u22122\u2212k, the instance constructed in A2S is exactly IS\u222a{k} (the order of arms in the constructed instance\nand IS\u222a{k} may differ, but as Aperm randomly permutes the arms beforehand, it does not matter). Hence, either TA1S (A) = akS\u222a{k} \u00b7 4k or TA2S (A) = a k S\u222a{k} \u00b7 4k. Since AS terminates as soon as either one of them terminate, we clearly have TAS(A) \u2264 min(TA1S (A), TA2S (A)) \u2264 a k S\u222a{k} \u00b7 4k for arm A with gap 2\u2212k when k 6\u2208 S. So for all k \u2208 [0,m] \\ (S \u222a badS), we can see TAS (2\u2212k) \u2264 akS\u2229{k} \u00b7 4k < c1 \u00b7 4k lnm. But this implies that\n{TAS (\u2206) < c1 \u00b7\u2206\u22122 lnm | \u2206 = 2\u2212i, i \u2208 [0,m]} m+ 1 \u2265 |[0,m] \\ (S \u222a badS)| m+ 1 \u2265 0.4,\nwhich contradicts Lemma C.1. So there must exist IS such that TAperm(IS) > c \u00b7 H(IS) \u00b7 lnm. By Lemma C.2, there exists a permutation \u03c0 on IS such that TA(\u03c0 \u25e6 IS) \u2265 c2 \u2211n i=2\u2206 \u22122 i ln lnn. This finishes the first part of the theorem. To see that \u2206\u221222 ln ln\u2206 \u22121 2 is not the dominating term, simply notice that\n\u2206\u221222 ln ln\u2206 \u22121 2 = 4 m ln(m \u00b7 ln 2) \u2264 4m lnm \u2264 1 m\nm\u2211\nk=0\nNkS \u00b7 4k lnm \u2264 2 \u00b7 ln 4 lnn\nn\u2211\ni=2\n\u2206\u22122i ln lnn.\nThis proves the second statement of the theorem."}, {"heading": "D A New Lower Bound for Sign-\u03be", "text": "In this section, we prove a new lower bound of Sign-\u03be (Theorem D.1), from which Lemma C.1 follows easily.\nWe introduce some notations first. Recall that the distributions of all the arms are Gaussian with variance 1. Fix an algorithm A for Sign-\u03be, for a random event E , let PrA,A\u00b5 [E ] (recall that A\u00b5 denotes an arm with mean \u00b5) denote the probability that E happens if we run A on arm A\u00b5. For notational simplicity, when A is clear from the context, we abbreviate it as Pr\u00b5[E ]. Similarly, we write E\u00b5[X] as a short hand notation for EA,A\u00b5 [X], which denotes the expectation of random variable X when running A on arm A\u00b5.\nWe use 1{expr} to denote the indicator function which equals 1 when expr is true and 0 otherwise, and we define F (\u2206) = \u2206\u22122 \u00b7 ln ln\u2206\u22121, which is the wanted lower bound for Sign-\u03be. Finally, for an integer N \u2208 N, a \u03b4-correct algorithm A for Sign-\u03be, and a function g : R \u2192 R, define\nC(A, g,N) =\nN\u2211\ni=1\n1 { There exists some \u2206 \u2208 [e\u2212i, e\u2212i+1) such that: TA(\u2206) < g(\u2206) } .\nIntuitively, it is the number of intervals [e\u2212i, e\u2212i+1) among the first N intervals that contains a fast point with respect to g.\nTheorem D.1. For any \u03b3 > 0, we have a constant c1 > 0 (which depends on \u03b3) such that for any 0 < \u03b4 < 0.01,\nlim N\u2192+\u221e sup A\u2032 is \u03b4-correct\nC(A\u2032, c1F,N)\nN\u03b3 = 0.\nIn other words, the fraction of the intervals containing fast points with respect to \u2126(F ) can be smaller than any inverse polynomial.\nBefore proving Lemma D.1, we show it implies Lemma C.1 as desired. We restate Lemma C.1 here for convenience.\nLemma C.1 (restated) For any \u03b4\u2032-correct algorithm A\u2032 for Sign-\u03be with \u03b4\u2032 \u2264 0.01, there exist constants N0 \u2208 N and c1 > 0 such that for all N \u2265 N0:\n|{TA\u2032(\u2206) < c1 \u00b7\u2206\u22122 lnN | \u2206 = 2\u2212i, i \u2208 [0, N ]}| \u2264 0.1(N + 1).\nProof of Lemma C.1. Let \u03b3 = 1/2. Applying Lemma D.1, we can see that there exist an integer M0 and a constant c2 such that, for any integer M \u2265 M0, it holds that\nC(A\u2032, c2F,M) \u2264 0.05 \u00b7 \u221a M,\nfor any \u03b4\u2032-correct algorithm A\u2032 for Sign-\u03be. Then for any N \u2265 M0/ ln 2, we can see that\n|{TA\u2032(\u2206) < c2 \u00b7 F (\u2206) | \u2206 = 2\u2212i, i \u2208 [0, N ]}| \u2264 C(A\u2032, c2 \u00b7 F, \u2308ln 2 \u00b7N\u2309) \u00b7 2 \u2264 0.1 \u00b7 \u221a N,\nsince e\u2308ln 2\u00b7N\u2309 \u2265 2N , and each interval [e\u2212i, e\u2212i+1) can contain at most 2 values of the form \u2206 = 2\u2212k. For \u2206 = 2\u2212i (i \u2265 \u221a N), we have that F (\u2206) = \u2206\u22122 ln ln\u2206\u22121 = \u2206\u22122 ln i \u2265 \u2206\u22122 lnN/2. Therefore, by letting c1 = c2/2, we can bound the cardinality of the set as follows\n|{TA\u2032(\u2206) < c1 \u00b7\u2206\u22122 lnN | \u2206 = 2\u2212i, i \u2208 [0, N ]}| \u2264 \u221a N + |{TA\u2032(\u2206) < c1 \u00b7\u2206\u22122 lnN | \u2206 = 2\u2212i, i \u2208 [ \u221a N,N ]}| \u2264 \u221a N + |{TA\u2032(\u2206) < c2 \u00b7 F (\u2206) | \u2206 = 2\u2212i, i \u2208 [ \u221a N,N ]}| \u22641.1 \u00b7 \u221a N.\nThis completes the proof of the lemma."}, {"heading": "D.1 Proof for Theorem D.1", "text": "The rest of this section is devoted to prove Theorem D.1. From now on, 0 < \u03b4 < 0.01 is a fixed constant. We first show a simple but convenient lemma based on Lemma A.3.\nLemma D.2. Let I1 (with reward distribution D1) and I2 (with reward distribution D2) be two instances of Sign-\u03be. Let E be a random event and \u03c4 be the total number of samples taken by A. Suppose PrD1 [E ] \u2265 12 . Then, we have\nPrD2 [E ] \u2265 1\n4 exp\n( \u22122ED1 [\u03c4 ]KL(D1,D2) ) .\nProof. Applying Lemma A.3, we have that\nED1 [\u03c4 ] \u00b7KL(D1,D2) \u2265 H(PrD1 [E ],PrD2 [E ]) \u2265 H ( 1\n2 ,PrD2 [E ]\n) \u2265 1\n2 \u00b7 ln\n( 1\n4PrD2 [E ](1\u2212 PrD2 [E ])\n) .\nHence, we can see that 4PrD2 [E ](1\u2212PrD2 [E ]) \u2265 exp(\u22122ED1 [\u03c4 ] \u00b7KL(D1,D2)), from which the lemma follows easily.\nFrom now on, suppose A is a \u03b4-correct algorithm for Sign-\u03be. We define two events:\nEU = [A outputs \u201c\u00b5 > \u03be\u201d],\nE(\u2206) = EU \u2227 [d\u2206\u22122 \u2264 \u03c4 \u2264 5TA(\u2206)], where \u03c4 is the number of samples taken by A and d is a universal constant to be specified later. The following lemma is a key tool, which can be used to partition the event EU into several disjoint parts.\nLemma D.3. For any \u2206 > 0 and d < H(0.2, 0.01)/2, we have that\nPr\u03be+\u2206[E(\u2206)] = PrA,A\u03be+\u2206 [E(\u2206)] \u2265 1\n2 .\nProof. First, we can see that Pr\u03be+\u2206[EU ] \u2265 1\u2212 \u03b4 \u2265 0.99 since A is \u03b4-correct and \u201c\u00b5 > \u03be\u201d is the right answer.\nNow, we claim Pr\u03be+\u2206[\u03c4 < d\u2206 \u22122] < 0.25. Suppose to the contrary that Pr\u03be+\u2206[\u03c4 < d\u2206 \u22122] \u2265 0.25. We can see that Pr\u03be+\u2206[EU \u2227 \u03c4 < d\u2206\u22122] \u2265 0.25 \u2212 \u03b4 > 0.2.\nConsider the following algorithm A\u2032: A\u2032 simulates A for d\u2206\u22122 steps. If A halts, A\u2032 outputs what A outputs, otherwise A\u2032 outputs nothing.\nLet EV be the event that A\u2032 outputs \u00b5 > \u03be. Clearly, we have PrA\u2032,\u03be+\u2206[EV ] > 0.2. On the other hand, PrA\u2032,\u03be\u2212\u2206[EV ] < \u03b4, since A is a \u03b4-correct algorithm. So by Lemma A.3, we have that\nEA\u2032,\u03be+\u2206[\u03c4 ]KL(N(\u03be +\u2206, \u03c3), N(\u03be \u2212\u2206, \u03c3)) = EA\u2032,\u03be+\u2206[\u03c4 ]2\u22062 \u2265 H(0.2, \u03b4) \u2265 H(0.2, 0.01).\nSince d\u2206\u22122 \u2265 EA\u2032,\u03be+\u2206[\u03c4 ], we have d \u2265 H(0.2, 0.01)/2. But this contradicts the condition of the lemma. Hence, we must have Pr\u03be+\u2206[\u03c4 < d\u2206\n\u22122] < 0.25. Finally, we can see that\nPr\u03be+\u2206[E(\u2206)] \u2265 Pr\u03be+\u2206[EU ]\u2212 Pr\u03be+\u2206[\u03c4 < d\u2206\u22122]\u2212 Pr\u03be+\u2206[\u03c4 > 5TA(\u2206)] \u2265 1\u2212 0.01 \u2212 0.25\u2212 Pr\u03be+\u2206[\u03c4 > 5EA,\u03be+\u2206[\u03c4 ]] \u2265 1\u2212 0.01 \u2212 0.25\u2212 0.2 \u2265 0.5\nwhere the first inequality follows from the union bound and the second from Markov inequality.\nLemma D.4. For any \u03b4-correct algorithm A, and any finite sequence {\u2206i}ni=1 such that\n1. the events {E(\u2206i)} are disjoint, 11 and 0 < \u2206i+1 < \u2206i for all 1 \u2264 i \u2264 n\u2212 1;\n2. there exists a constant c > 0 such that TA(\u2206i) \u2264 c \u00b7 F (\u2206i) for all 1 \u2264 i \u2264 n,\nit must hold that: n\u2211\ni=1\nexp{\u22122c \u00b7 F (\u2206i) \u00b7\u22062i } \u2264 4\u03b4.\n11 More concretely, the intervals [d\u2206\u22122i , 5TA(\u2206i)] are disjoint.\nProof. Suppose for contradiction that \u2211n\ni=1 exp{\u22122c \u00b7 F (\u2206i) \u00b7\u22062i } > 4\u03b4. Let \u03b1 = 15\u2206n. By Lemma A.4, we can see that KL(N (\u03be+\u2206i, \u03c3),N (\u03be\u2212\u03b1, \u03c3)) = 12\u03c32 (\u2206i+\u03b1)2 \u2264\n1 2(1.2\u2206i) 2 \u2264 \u22062i . By Lemma D.2, we have:\nPr\u03be\u2212\u03b1[EU ] \u2265 n\u2211\ni=1\nPr\u03be\u2212\u03b1[E(\u2206i)] \u2265 1\n4\nn\u2211\ni=1\nexp{\u22122E\u03be+\u2206i [\u03c4 ]\u22062i } \u2265 1\n4\nn\u2211\ni=1\nexp{\u22122c \u00b7 F (\u2206i) \u00b7\u22062i } > \u03b4.\nNote that we need Lemma D.3(1) (i.e., Pr\u03be+\u2206i [E(\u2206i)] \u2265 1/2 ) in order to apply Lemma D.2 for the second inequality. The above inequality means that A outputs a wrong answer for instance \u03be \u2212 \u03b1 with probability > \u03b4, which contradicts that A is \u03b4-correct.\nNow, we try to utilize Lemma D.4 on a carefully constructed sequence {\u2206i}. The construction of the sequence {\u2206i} requires quite a bit calculation. To facilitate the calculation, we provide a sufficient condition for the disjointness of the sequence, as follows.\nLemma D.5. A is any \u03b4-correct algorithm for Sign-\u03be. c > 0 is a universal constant. The sequence {\u2206i}Ni=0 satisfies the following properties:\n1. 1/e > \u22061 > \u22062 > . . . > \u2206N \u2265 \u03b1 > 0.\n2. For all i \u2208 [N ], we have that TA(\u2206i) \u2264 c \u00b7 F (\u2206i).\n3. Let Li = ln\u2206 \u22121 i . We have Li+1 \u2212 Li > 12 ln ln ln\u03b1\u22121 + c1, in which c1 = ln c+ln5\u2212ln d2 .\nThen, the events {E(\u22061), E(\u22062), . . . , E(\u2206N )} are disjoint.\nProof. We only need to show the intervals for each E(\u2206i) are disjoint. In fact, it suffices to show it holds for two adjacent events E(\u2206i) and E(\u2206i+1). Since 5TA(\u2206i) \u2264 5c \u00b7 F (\u2206i), we only need to show 5c \u00b7 F (\u2206i) < d\u2206\u22122i+1, which is equivalent to\nln c+ ln 5 + 2Li + ln lnLi < ln d+ 2Li+1.\nBy simple manipulation, this is further equivalent to Li+1 \u2212 Li > (ln c + ln 5 \u2212 ln d + ln lnLi)/2. Since \u03b1 \u2264 \u2206i, we have ln ln ln\u03b1\u22121 \u2265 ln lnLi, which concludes the proof.\nNow, everything is ready to prove Theorem D.1.\nProof of Theorem D.1. Suppose for contradiction, for any c1 > 0, the limit is not zero. This is equivalent to\nlim sup N\u2192+\u221e sup A\u2032 is \u03b4\u2212correct\nC(A\u2032, c1F,N)\nN\u03b3 > 0. (4)\nWe claim that for c1 = \u03b3 4 , the above can lead to a contradiction.\nFirst, we can see that (4) is equivalent to the existence of an infinite increasing sequence {Ni}i and a positive number \u03b2 > 0 such that\nsup A\u2032 is \u03b4\u2212correct\nC(A\u2032, c1F,Ni)\nN\u03b3i > \u03b2, for all i.\nConsider some large enough Ni in the above sequence. The above formula implies that there exists a \u03b4-correct algorithm A such that C(A, c1F,Ni) \u2265 \u03b2N\u03b3i .\nWe maintain a set S, which is initially empty. For each 2 \u2264 j \u2264 Ni, if there exists \u2206 \u2208 [e\u2212j , e\u2212j+1) such that TA(\u2206) \u2264 c1F (\u2206), then we add one such \u2206 into the set S. We have |S| \u2265 C(A, c1F,Ni)\u2212 1 \u2265 \u03b2N\u03b3i \u2212 1 (\u22121 comes from that j starts from 2). Let\nb =\n\u2308 ln c1 + ln 5\u2212 ln d+ ln lnNi\n2 + 1\n\u2309 .\nWe keep only the 1st, (1 + b)th, (1 + 2b)th, . . . elements in S, and remove the rest. With a slight abuse of notation, rename the elements in S by {\u2206i}|S|i=1, sorted in decreasing order. It is not difficult to see that 1e > \u22061 > \u22062 > . . . > \u2206|S| \u2265 e\u2212Ni > 0. By the way we choose the elements, for 1 \u2264 i < |S|, we have\nln\u2206\u22121i+1 \u2212 ln\u2206\u22121i > ln c1 + ln 5\u2212 ln d\n2 +\n1 2 ln lnN.\nRecall that we also have TA(\u2206i) \u2264 c1F (\u2206i) for all i. Hence, we can apply Lemma D.5 and conclude that all events {E(\u2206i)} are disjoint.\nWe have |S| \u2265 (\u03b2N\u03b3i \u2212 1)/b, for large enough Ni (we can choose such Ni since {Ni} approaches to infinity), it implies |S| \u2265 \u03b2N\u03b3i / ln lnNi. Then, we can get\n|S|\u2211\nj=1\nexp{\u22122c1 \u00b7 F (\u2206j) \u00b7\u2206\u22122j } = |S|\u2211\nj=1\nexp{\u2212\u03b3 \u00b7 ln ln\u2206\u22121j /2}\n=\n|S|\u2211\nj=1\n(ln\u2206\u22121j ) \u2212\u03b3/2 \u2265 |S| \u00b7N\u2212\u03b3/2i\n\u2265\u03b2N\u03b3i / ln lnNi \u00b7N \u2212\u03b3/2 i = \u03b2N \u03b3/2 i / ln lnNi.\nThe inequality in the second line holds since \u2206j \u2265 e\u2212Ni for all j \u2208 [|S|]. Since \u03b3 > 0, we can choose Ni large enough such that \u03b2N \u03b3/2 i / ln lnNi > 4\u03b4, which renders a contradiction to Lemma D.4.\nRemark D.6. It would be transparent from our proof why the ln ln\u2206\u22121 term is essential: The main reason is that \u2206 is not known beforehand (if \u2206 is known, Sign-\u03be can be solved in O(\u2206\u22122 ln \u03b4\u22121) time). Intuitively, an algorithm has to \u201cguess\u201d and \u201cverify\u201d (in some sense) the true \u2206 value. As a result, if the algorithm is \u201clucky\u201d in allocating the time for verifying the right guess of \u2206, it may stop earlier and thus be faster than F (\u2206) for some \u2206s. But as we will demonstrate, if an algorithm stops earlier on larger \u2206, it would hurt the accuracy for smaller \u2206, and there is no way to be always lucky. This is the only factor accounting for the ln ln\u2206\u22121. While Farrell\u2019s proof attributes the ln ln\u2206\u22121 factor to the Law of Iterative Logarithm (see also [20]), our proof shows that the ln ln\u2206\u22121 factor exists due to algorithmic reasons, which is a new perspective."}, {"heading": "E On Almost Instance Optimality", "text": "Instance optimality ([14, 1]) is the strongest possible notion of optimality in the theoretical computer science literature. Loosely speaking, an algorithm A is instance optimal if the running time of A\non instance I is at most O(L(I)), where L(I) is the lower bound required to solve the instance for any algorithm.\nLet us first consider Sign-\u03be (the two arms case). As we mentioned, Farrell\u2019s lower bound (1) is not an instance-wise lower bound. On the other hand, it is impossible to obtain an \u2126(\u2206\u22122 ln ln\u2206\u22122) lower bound for every instance, since we can design an algorithm that uses o(\u2206\u22122 ln ln\u2206\u22122) samples for infinite number of instances. We provide a detailed discussion in Section G. Combining this two fact, we can see that it is impossible to obtain an instance optimal algorithm even for Sign-\u03be. Hence, it appears to be more hopeless to consider instance optimality for Best-1-Arm. However, based on our current understanding, we suspect that the two arms case is the only obstruction for an instance optimal algorithm, and modulo a \u2206\u22122[2] ln ln\u2206[2] additive term, we may be able to achieve instance optimality for Best-1-Arm.\nWe propose an intriguing conjecture concerning the instance optimality of Best-1-Arm. The conjecture provides an explicit formula for the sample complexity. Interestingly, the formula involves an entropy term, which we call the gap entropy, which has not appeared in the bandit literature, to the best of our knowledge. We assume that all reward distributions are Gaussian with variance 1. In order to state the conjecture formally, we need to define what is an instance-wise lower bound. Our definition is inspired by that in [1].\nDefinition E.1. (Order-Oblivious Instance-wise Lower Bound) Suppose L(I, \u03b4) is a function which maps a Best-1-Arm instance I with n arms, and confidence parameter \u03b4 to a number. We say L(I, \u03b4) is an instance-wise lower bound for I if\nL(I, \u03b4) \u2264 inf A:A is \u03b4-correct\n1\nn! \u00b7\n\u2211\n\u03c0\u2208Sym(n)\nTA(\u03c0 \u25e6 I).\nNow, we define the entropy term.\nDefinition E.2. (Gap Entropy) Given a Best-1-Arm instance I, let\nGi = {u \u2208 [2, n] | 2\u2212i \u2264 \u2206[u] < 2\u2212i+1}, Hi = \u2211\nu\u2208Gi\n\u2206\u22122[u] , and pi = Hi/ \u2211\nj\nHj.\nWe can view {pi} as a discrete probability distribution. We define the following quantity as the gap entropy for the instance I\nEnt(I) = \u2211\nGi 6=\u2205\npi log p \u22121 i .\nNote that it is exactly the Shannon entropy for the distribution defined by {pi}.\nRemark E.3. We choose to partition the arms based on the powers of 2. There is nothing special about 2 and replacing it by any other constant only changes Ent(I) by a constant factor.\nNow, we formally state our conjecture. Let H(I) = \u2211n\ni=2 \u2206 \u22122 [i] .\nConjecture E.4. For any Best-1-Arm instance I and confidence \u03b4 \u2208 (0, c) (c is a universal small constant), let\nL(I, \u03b4) = \u0398 ( H(I)(ln \u03b4\u22121 + Ent(I)) ) .\nL(I, \u03b4) is an instance-wise lower bound for I.\nMoreover, there is a \u03b4-correct algorithm for Best-1-Arm with sample complexity\nO ( L(I, \u03b4) + \u2206\u22122[2] ln ln\u2206 \u22121 [2] ) .\nIn other words, modulo the \u2206\u22122[2] ln ln\u2206 \u22121 [2] additive term, the algorithm is instance optimal. We call such an algorithm an almost instance optimal algorithm.\nThe conjectured sample complexity consists of two terms, one matching an instance-wise lower bound L(I, \u03b4) and the other matching the optimal bound \u2206\u22122[2] ln ln\u2206 \u22121 [2] for Sign-\u03be.\n12 Hence, a resolution of the conjecture would provide a complete understanding of the sample complexity of Best-1-Arm.\nIn fact, our proofs of Theorem 3.1 and Theorem 2.5 provide strong evidences for Conjecture E.4 and we briefly discuss the connections below.\nFirst, the third additive term \u2211n\ni=2\u2206 \u22122 [i] ln lnmin(n,\u2206 \u22121 [i] ) in Theorem 2.5 might appear to be\nan artifact of the algorithm or the analysis at first glance. However, in light of Conjecture E.4, it is a natural upper bound of H(I)Ent(I), as shown in the following proposition. On one extreme, the maximum value Ent(I) can get is O(ln lnn). This can be achieved by instances in which there are log n nonempty groups Gi and they have almost the same weight Hi. On the other extreme where there is only a constant number of nonempty groups (i.e., the instance is clustered), Ent(I) = O(1), and our algorithm can achieve almost instance optimality in this case. The proof of the proposition is somewhat tedious and we defer it to the end of this section.\nProposition E.5. For any instance I, H(I)Ent(I) is upper bounded by\nO\n( n\u2211\ni=2\n\u2206\u22122[i] (1 + ln lnmin(n,\u2206 \u22121 [i] ))\n) .\nIn particular, Ent(I) = O(ln lnn). Moreover, for any clustered instance I, we have Ent(I) = O(1).\nBesides the fact that our algorithm can achieve optimal results for both extreme cases, we have more reasons to believe why Ent(I) should enter the picture. Gap Entropy Ent(I): First, we motivate Ent from the algorithmic side. Consider an eliminationbased algorithms (such as [24] or our algorithm). We must ensure that the best arm is not eliminated in any round. Recall that in the r-th round, we want to eliminate arms with gap \u2206r = \u0398(2\n\u2212r), which is done by obtaining an approximation of the best arm, then take O(\u2206\u22122r ln \u03b4 \u22121 r ) samples from each arm and eliminate the arms with smaller empirical means. Roughly speaking, we need to assign the failure probability \u03b4r carefully to each round (by union bound, we need \u2211 r \u03b4r \u2264 \u03b4). The algorithm in [24] use \u03b4r = O(\u03b4 \u00b7r\u22122). and our algorithm uses a better way to assign \u03b4r, based on the information we collected using FractionTest. However, if one can assign \u03b4rs optimally (i.e., minimize\u2211\nr Hr ln \u03b4 \u22121 r subject to \u2211 r \u03b4r \u2264 \u03b4), one could achieve the entropy bound \u2211 r Hr \u00b7 (ln \u03b4\u22121 + Ent(I))\n(by letting \u03b4r = \u03b4Hr/ \u2211\niHi). Of course, this does not lead to an algorithm directly, as we do not know His in advance.\nWe also have strong evidence from our lower bound result. In fact, it is possible to extend Theorem 3.1 in the following way. 13 We can use different Iinit (e.g. choosing a different number\n12 From our previous discussion, we know it is impossible to obtain an instance optimal algorithm for 2-arm instances, and the bound \u2206\u22122\n[2] ln ln\u2206\u22121 [2] is not improvable.\n13We omit the details in this version.\nof arms with gap 2\u2212k for each k) and show there is a similar instance IS such that A requires at least \u2126(H(IS) \u00b7 Ent(IS)) samples. Even this does not prove an lower bound for every instance, it strongly suggests \u2126(H(I) \u00b7 Ent(I)) is the right lower bound.\nNow, we provide a proof of Proposition E.5. Proof of Proposition E.5. In the following the base of log is 2. Let m denote the maximum index i such that Gi is non-empty, and Si = |Gi|. Clearly, 4i\u22121Si \u2264 Hi \u2264 4iSi.\nWe first prove the second claim, which is straightforward. By definition, in a clustered instance, the number of non-empty Gi is bounded by a constant C, hence the corresponding entropy is bounded by a constant.\nFor the first claim, we make use of the non-negativity of KL divergence . We construct another probability distribution q. Recall that KL(p, q) = \u2211 (pi log q \u22121 i \u2212pi log p\u22121i ) \u2265 0, which implies that\nEnt(I) \u2264 \u2211 pi log q\u22121i . Now, we partition all the arms into blocks. The t-th block Bt is the union of a consecutive segment of Glt , Glt+1, . . . , Grt . The blocks are constructed one by one starting from the first block B1. The t-th block Bt is constructed as follows: let lt = rt\u22121 + 1 (if t = 1, then lt = 1), and rt be the first k such that\n\u2211k i=lt Si \u2265 12 \u00b7 \u2211m i=lt Si. We terminate w hen rt = m. Suppose there are h\nblocks in total. Clearly h = O(log n). Now, we define the probability distribution q. For each i such that Gi belongs to Bt, we let qi = 6 \u03c02 \u00b7 t\u22122 \u00b7 2i\u2212rt\u22121. Note that \u2211m i=1 qi = 6 \u03c02 \u00b7 \u2211h t=1 t \u22122 \u2211rt i=lt 2i\u2212rt\u22121 \u2264 6\u03c02 \u00b7 \u2211h t=1 t\n\u22122 \u2264 1. In addition, we let qm+1 = 1\u2212 \u2211m i=1 qi. Hence, q is a well defined distribution over [1,m+ 1].\nLet C = log \u03c0 2 6 . So now we only need provide an upper bound for \u2211m i=1Hi log q \u22121 i . 14\nm\u2211\ni=1\nHi log q \u22121 i =\nh\u2211\nt=1\nrt\u2211\ni=lt\nHi(2 log t+ (rt \u2212 i+ 1) +C). (5)\nNow, consider the following quantity U .\nU =\nh\u2211\nt=1\nrt\u2211\ni=lt\nHi(C + 1 +\ni\u22121\u2211\nj=1\n4j\u2212i+1(i\u2212 j + 1) + 2 log t).\nWe claim that \u2211m\ni=1Hi log q \u22121 i \u2264 U . We first see that the proposition is an easy consequence of\nthe claim. Since \u2211i\u22121\nj=1 4 j\u2212i+1(i\u2212 j+1) = O(1), we have U = O (\u2211h t=1 \u2211rt i=lt Hi(1 + log t)) ) . Note\nthat t \u2264 min(h, i). So U can be further bounded by O(\u2211mi=1 Hi(1 + logmin(log n, i))), which is exactly O (\u2211n i=2 \u2206 \u22122 [i] (1 + ln lnmin(n,\u2206 \u22121 [i] )) ) . Now, the only remaining task is to prove the claim.\n14 For empty groups, we adopt the convention 0 log 0 = 0.\nWe first see that\nh\u2211\nt=1\nrt\u2211\ni=lt\nHi\n C + 1 + i\u22121\u2211\nj=1\n4j\u2212i+1(i\u2212 j + 1) + 2 log t\n \n\u2265 h\u2211\nt=1\nrt\u2211\ni=lt\n( Hi(C + 1 + 2 log t) + m\u2211\nk=i+1\nHk \u00b7 4i\u2212k+1(k \u2212 i+ 1) )\n\u2265 h\u2211\nt=1\nrt\u2211\ni=lt\n( Hi(C + 1 + 2 log t) + m\u2211\nk=i+1\nSk4 k\u22121 \u00b7 4i\u2212k+1(k \u2212 i+ 1)\n)\n\u2265 h\u2211\nt=1\nrt\u2211\ni=lt\n( Hi(C + 1 + 2 log t) + 4 i \u00b7 m\u2211\nk=i+1\nSk(k \u2212 i+ 1) )\n(6)\nFor each i such that lt \u2264 i < rt, by the construction of the blocks, we have Si \u2264 \u2211m\nj=rt Sj. So\nwe have 4i \u2211m\nk=rt Sk(k\u2212 i+1) \u2265 4i \u2211m k=rt\nSk(rt \u2212 i+1) \u2265 4iSi(rt \u2212 i+1) \u2265 Hi(rt \u2212 i+1). Hence, each term in (6) is no less than the corresponding term in (5). (It is trivially true for i = rt). This concludes the proof."}, {"heading": "F Missing Proofs in Section B", "text": ""}, {"heading": "F.1 Proof for Lemma B.3", "text": "Lemma B.3 (restated) Suppose \u03b5 < 0.1 and t \u2208 (\u03b5, 1 \u2212 \u03b5). With probability 1 \u2212 \u03b4, the following hold:\n\u2022 If FractionTest outputs True, then |S>cr | < (1\u2212 t+ \u03b5)|S| (or equivalently |S\u2264cr | > (t\u2212 \u03b5)|S|).\n\u2022 If FractionTest outputs False, then |S<cl | < (t+ \u03b5)|S| (or equivalently |S\u2265cl | > (1\u2212 t\u2212 \u03b5)|S|).\nMoreover the number of samples taken by the algorithm is O(ln \u03b4\u22121\u03b5\u22122\u2206\u22122 ln \u03b5\u22121), in which \u2206 = cr \u2212 cl.\nProof of Lemma B.3. Let Sa = S <cl , Sb = S >cr , Na = |Sa|, Nb = |Sb|, N = |S|. For each iteration i and the arm ai in line 4, by Lemma B.1, we have Pr[|\u00b5\u0302[ai] \u2212 \u00b5[ai]| \u2265 (cr \u2212 cl)/2] \u2264 \u03b5/3. Hence, if \u00b5[ai] < cl, then Pr[\u00b5\u0302[ai] < cl+cr 2 ] \u2265 1\u2212 \u03b5/3. Similarly, if \u00b5[ai] > cr, then Pr[\u00b5\u0302[ai] < cl+cr 2 ] \u2264 \u03b5/3.\nLet Xi be the indicator Boolean variable 1 { \u00b5\u0302[ai] < cl+cr 2 } . Clearly Xis are i.i.d. From the\nalgorithm, we can see that cnt = \u2211tot\ni=1 Xi. Let E = E[Xi]. Let E\u0302 = cnt/tot, which is the empirical\nvalue of E. By Chernoff bound, we can easily get that Pr[|E \u2212 E\u0302| \u2265 \u03b5/3] \u2264 \u03b4. In the rest of the proof, we condition on the event that |E \u2212 E\u0302| < \u03b5/3, which happens with probability at least 1\u2212 \u03b4. Suppose E\u0302 > t. Then we have E > t\u2212 \u03b5/3. It is also easy to see that:\nE \u00b7N = \u2211\na\u2208S\nPr [ \u00b5\u0302[a] <\ncl + cr 2\n] \u2264 (N \u2212Nb) \u00b7 1 + (\u03b5/3)Nb = N \u2212 (1\u2212 \u03b5/3)Nb,\nas \u03b5 < 0.1, 11\u2212\u03b5/3 < (1 + 2\u03b5/3). So, we have proved the first claim:\nNb \u2264 (1\u2212 E)N 1\u2212 \u03b5/3 \u2264 (1\u2212 t+ \u03b5/3)N 1\u2212 \u03b5/3 < (1\u2212 t+ \u03b5/3)(1 + 2\u03b5/3)N \u2264 (1\u2212 t+ \u03b5)N.\nThe second claim is completely symmetric. Suppose E\u0302 \u2264 t. Then we have E \u2264 t + \u03b5/3. We also have E \u00b7N \u2265 Na(1\u2212 \u03b5/3). So,\nNa \u2264 E \u00b7N 1\u2212 \u03b5/3 \u2264 (t+ \u03b5/3)N 1\u2212 \u03b5/3 < (t+ \u03b5/3)(1 + 2\u03b5/3)N \u2264 (t+ \u03b5)N.\nFinally, the upper bound for the number of samples can be verified by a direct calculation."}, {"heading": "F.2 Proof for Lemma B.4", "text": "Lemma B.4 (restated) Suppose \u03b4 < 0.1. Let S\u2032 = Elimination(S, cl, cr, \u03b4). Let A1 be the best arm among S, with mean \u00b5[A1] \u2265 cr. Then with probability at least 1\u2212 \u03b4, the following statements hold\n1. A1 \u2208 S\u2032 (the best arm survives);\n2. |S\u2032\u2264cl | < 0.1|S\u2032| (only a small fraction of arms have means less than cl);\n3. The number of samples is O(|S| ln \u03b4\u22121\u2206\u22122), in which \u2206 = cr \u2212 cl.\nNote that with probability at most \u03b4, there is no guarantee for any of the above statements. Before proving Lemma B.4, we first describe two events (which happen with high probability) that we condition on, in order to simplify the argument.\nLemma F.1. With probability at least 1 \u2212 \u03b4/2, it holds that in all round r, FractionTest outputs correctly, and |\u00b5\u0302[A1] \u2212 \u00b5[A1]| < cr\u2212cm2 .\nProof. Fix a round r. FractionTest outputs incorrectly with probability at most \u03b4r. By Theorem B.1, Pr(|\u00b5\u0302[A1] \u2212 \u00b5[A1]| \u2265 cr\u2212cm2 ) \u2264 \u03b4r.\nThe lemma follows from a simple union bound over all rounds: 2 \u2211+\u221e r=1 \u03b4r \u2264 2\u03b4 \u2211+\u221e r=1 0.1/2 r \u2264\n\u03b4/2.\nLemma F.2. Let Nr = |S\u2264cmr |. Then with probability at least 1 \u2212 \u03b4/2, for all rounds r in which Algorithm 4 does not terminate, Nr+1 \u2264 14Nr.\nProof. Suppose a \u2208 S\u2264cmr . By Theorem B.1, we have that Pr[|\u00b5\u0302[a] \u2212 \u00b5[a]| \u2265 cr\u2212cm2 ] \u2264 \u03b4r. So Pr[\u00b5\u0302[a] > cm+cr 2 ] \u2264 \u03b4r. Then, we can see that E[Nr+1] \u2264 \u03b4rNr. By Markov inequality, we can see that Pr(Nr+1 > 1 4Nr) \u2264 \u03b4rNr1\n4 Nr\n= 4\u03b4r.\nAgain, the lemma follows by a simple union bound: \u2211+\u221e r=1 4\u03b4r \u2264 4\u03b4 \u2211+\u221e r=1 0.1/2 r \u2264 \u03b4/2.\nProof of Lemma B.4. With probability at least 1\u2212\u03b4, both statements in Lemma F.1 and Lemma F.2 hold. Let that event be EG. Now we prove Lemma B.4 under the condition that EG holds. Now we prove all the claims one by one.\nConsider the first claim. Note that for all r, conditioning on EG, we have that |\u00b5\u0302[A1] \u2212 \u00b5[A1]| < cr\u2212cm\n2 , or equivalently \u00b5\u0302[A1] > cr \u2212 cr\u2212cm2 = cm+cr2 . Hence, A1 survives all rounds and A1 \u2208 S\u2032.\nFor the second claim, we note that, conditioning on event EG, FractionTest always outputs correctly. Suppose the algorithm terminates at round r, which means FractionTest(Sr, cl, cm, \u03b4r, 0.075, 0.025) outputs False. By Lemma B.3, we have |S<clr | < (0.075 + 0.025)|Sr | = 0.1|Sr|. Since S\u2032 = Sr, the claim clearly follows.\nNow, we prove the last claim. Again we condition on EG. Suppose the algorithm does not terminate at round r, which means FractionTest(Sr, cl, cm, \u03b4r, 0.075, 0.025) outputs True. By Lemma B.3, we know Nr = |S\u2264cmr | > (0.075 \u2212 0.025)|Sr | = 0.05|Sr|. Then, we have that\n|Sr+1| \u2264 |Sr| \u2212 (|Nr| \u2212 |Nr+1|) \u2264 |Sr| \u2212 3\n4 |Nr| \u2264 0.99|Sr|.\nSuppose the algorithm terminates at round r\u2032. Let c1 be a large enough constant (so that c1 ln \u03b4r\u2206\n\u22122|Sr| is an upper bound for the samples taken by UniformSample in round r and c1 ln \u03b4r\u2206\u22122 is an upper bound for the samples taken by FractionTest in round r). Then, the number of samples is bounded by:\nr\u2032\u2211\nr=1\nc1(\u2206 \u22122 ln \u03b4r|Sr|+\u2206\u22122 ln \u03b4r) \u2264 2c1|S|\u2206\u22122\nr\u2032\u2211\nr=1\n(ln \u03b4\u22121 + r ln 2 + ln 10) \u00b7 0.99r\u22121\n\u2264 2c1|S|\u2206\u22122 r\u2032\u2211\nr=1\n(ln \u03b4\u22121(r + 1) + ln 10) \u00b7 0.99r\u22121\n\u2264 2c1|S|\u2206\u22122 ( ln \u03b4\u22121 +\u221e\u2211\nr=1\n(r + 1) \u00b7 0.99r\u22121 + +\u221e\u2211\nr=1\nln 10 \u00b7 0.99r\u22121 )\n\u2264 2c1|S|\u2206\u22122 ( ln \u03b4\u22121 \u00b7 10100 + 100 \u00b7 ln 10 )\nSo the number of samples is O(|S| ln \u03b4\u22121\u2206\u22122), which concludes the proof of the lemma."}, {"heading": "F.3 Proofs for Lemma B.16, Lemma B.17 and Lemma B.18", "text": "For convenience, we let\nS=rr = U r \u2229 Sr, S>rr = U\u2265r+1 \u2229 Sr, Ncur = |S=rr |, Nbig = |S>rr |.\nThen we have |Sr| = Ncur +Nbig +Nsma + 1. Also, recall that ls = ln(min(hI , s)). In order to prove these three lemmas, we need an important inequality for Pr. If r \u2264 maxs, we\nhave:\nPr \u2212 Pr+1 = c2 \u00b7 ( +\u221e\u2211\ns=r\n(ln \u03b4\u22121 + ls) \u00b7 \u03b5\u22122r |U s|+ (ln \u03b4\u22121 + lmaxs) \u00b7 \u03b5\u22122r\n)\n\u2265 c2 \u00b7 \u03b5\u22122r (ln \u03b4\u22121 + lr)(|U\u2265r|+ 1) \u2265 c2 \u00b7 \u03b5\u22122r (ln \u03b4\u22121 + lr)(Ncur +Nbig + 1). (7)\nLemma B.16 (restated) When MedianElim (line 8) returns an \u03b5r/4-approximation of the best arm A1, and FractionTest outputs True. The expected number of samples taken at and after round r is bounded by\n\u03b5\u22122r (ln \u03b4 \u22121 + lr)c3Nsma + Pr.\nProof of Lemma B.16. Since by Lemma B.11, we have |U\u2264r \u2229 Sr+1| \u2264 0.1|Sr+1|, which means |U\u2265r+1 \u2229 Sr+1|+ 1 \u2265 0.9|Sr+1|. So, we have that\n|U\u2264r \u2229 Sr+1| \u2264 1\n9 (|U\u2265r+1 \u2229 Sr+1|+ 1) \u2264\n1 9 (Nbig + 1).\nTherefore, we can see the number of samples is bounded by:\nc3 \u00b7 |Sr|\u03b5\u22122r (ln \u03b4\u22121 + lr) + T ( r + 1, 1\n9 (Nbig + 1)\n) ,\nwhere the first additive term is the number of samples in this round, and is bounded by Lemma B.15. By the induction hypothesis, we have:\nT ( r + 1, 1\n9 (Nbig + 1)\n)\n\u2264(ln \u03b4\u22121 + lr+1) \u00b7 c1 \u00b7 1\n9 (Nbig + 1) \u00b7 \u03b5\u22122r+1 + Pr+1\n\u2264(ln \u03b4\u22121 + lr+1) \u00b7 c1 \u00b7 1\n9 (Nbig + 1) \u00b7 \u03b5\u22122r+1 + Pr \u2212 c2 \u00b7 (ln \u03b4\u22121 + lr)(Nbig +Ncur + 1)\u03b5\u22122r (By (7))\n\u2264(ln \u03b4\u22121 + lr) \u00b7 \u03b5\u22122r ( c1 5\n9 Nbig +\n5 9 c1 \u2212 c2Ncur \u2212 c2Nbig \u2212 c2\n) + Pr\nTherefore, we can bound the expected number of samples by:\n\u03b5\u22122r (ln \u03b4 \u22121 + lr) ( c3 \u00b7 |Sr|+ c1 5\n9 Nbig +\n5 9 c1 \u2212 c2Ncur \u2212 c2Nbig \u2212 c2\n) + Pr\n\u2264\u03b5\u22122r (ln \u03b4\u22121 + lr) ( (c3 \u2212 c2)Ncur + (c3 + 5\n9 c1 \u2212 c2)Nbig + c3Nsma +\n5 9 c1 + c3 \u2212 c2\n) + Pr\n\u2264\u03b5\u22122r (ln \u03b4\u22121 + lr)c3Nsma + Pr (59c1 + c3 \u2212 c2 < 0 , c3 + 59c1 \u2212 c2 < 0)\nIn the first inequality, we use the fact that |Sr| = Ncur +Nbig +Nsma + 1.\nLemma B.17 (restated) When MedianElim (line 8) returns an \u03b5r/4-approximation of the best arm A1, and FractionTest outputs False. The expected number of samples taken at and after round r is bounded by Pr.\nProof of Lemma B.17. By Lemma B.11, we can see |U\u2265r \u2229 Sr|+ 1 = Ncur +Nbig + 1 > 0.5|Sr|. So Nsma < 0.5|Sr |, thus\nNcur +Nbig + 1 \u2265 Nsma. (8)\nWe can see that the total number of samples is bounded by:\nc3 \u00b7 |Sr|\u03b5\u22122r + T (r + 1, Nsma +Ncur) \u2264 (ln \u03b4\u22121 + lr)c3 \u00b7 |Sr|\u03b5\u22122r + T (r + 1, Nsma +Ncur).\nIn the above, the first term is due to the number of samples in this round by Lemma B.15. The second term is an upper bound for the number of samples starting at round r+1. Since we do not eliminate any arm in this case, we have |U\u2264r \u2229 Sr+1| \u2264 Nsma +Ncur. From the induction hypothesis, we have:\nT (r + 1, Nsma +Ncur)\n\u2264(ln \u03b4\u22121 + lr+1) \u00b7 c1 \u00b7 (Nsma +Ncur) \u00b7 \u03b5\u22122r+1 + Pr+1 \u2264(ln \u03b4\u22121 + lr) \u00b7 c1 \u00b7 5(Nsma +Ncur) \u00b7 \u03b5\u22122r + Pr \u2212 c2 \u00b7 (ln \u03b4\u22121 + lr)(Nbig +Ncur + 1)\u03b5\u22122r (By (7)) \u2264(ln \u03b4\u22121 + lr)(5c1Nsma + 5c1Ncur \u2212 c2Ncur \u2212 c2Nbig \u2212 c2)\u03b5\u22122r + Pr\nPlugging it into the bound, we have the following bound for the expected number of samples:\n(ln \u03b4\u22121 + lr)c3 \u00b7 |Sr|\u03b5\u22122r + (ln \u03b4\u22121 + lr)(5c1Nsma + 5c1Ncur \u2212 c2Ncur \u2212 c2Nbig \u2212 c2) \u00b7 \u03b5\u22122r + Pr \u2264(ln \u03b4\u22121 + lr)((5c1 + c3)Ncur + c3Nbig + c3 + (5c1 + c3)Nsma \u2212 c2(Ncur +Nbig + 1)) \u00b7 \u03b5\u22122r + Pr \u2264(ln \u03b4\u22121 + lr)(c3Nbig + c3 + (5c1 + c3)Ncur \u2212 (c2 \u2212 5c1 \u2212 c3)(Ncur +Nbig + 1)) \u00b7 \u03b5\u22122r + Pr \u2264Pr (c2 \u2212 5c1 \u2212 c3 > 5c1 + c3 > c3)\nIn the second inequality, we use the fact that (5c1 + c3)Nsma \u2264 (5c1 + c3)(Ncur +Nbig + 1) due to (8).\nLemma B.18 (restated)When MedianElim (line 8) returns an arm which is not an \u03b5r/4-approximation of the best arm A1. The expected number of samples taken at and after round r is bounded by\n\u2264 (ln \u03b4\u22121 + lr)(c3 + 5c1) \u00b7Nsma\u03b5\u22122r + Pr.\nProof of Lemma B.18. In this case, we can simply bound it by:\nc3 \u00b7 |Sr|\u03b5\u22122r (ln \u03b4\u22121 + lr) + T (r + 1, Nsma +Ncur).\nThe first term is still due to the number of samples in this round by Lemma B.15. The second term is an upper bound for the samples taken starting at round r+ 1, since |U\u2264r \u2229 Sr+1| \u2264 Nsma +Ncur in any case. Then, we have that\nc3 \u00b7 |Sr|\u03b5\u22122r (ln \u03b4\u22121 + lr) + T (r + 1, Nsma +Ncur) \u2264(ln \u03b4\u22121 + lr)(c3(Nsma +Ncur +Nbig + 1)\u03b5\u22122r + 5c1(Nsma +Ncur)\u03b5\u22122r \u2212 c2\u03b5\u22122r (Nbig +Ncur + 1)) + Pr\n(By (7))\n\u2264(ln \u03b4\u22121 + lr)((c3 + 5c1 \u2212 c2)Ncur + (c3 \u2212 c2)Nbig + (c3 + 5c1)Nsma + c3 \u2212 c2)\u03b5\u22122r + Pr\nSince c3 + 5c1 \u2212 c2 \u2264 0 and c3 \u2212 c2 \u2264 0, we have the following bound for the expected number of samples:\n(ln \u03b4\u22121 + lr)(c3 + 5c1) \u00b7Nsma\u03b5\u22122r + Pr"}, {"heading": "G More About Sign-\u03be", "text": "In this section, we present a class of \u03b4-correct algorithms for Sign-\u03be which needs o(\u2206\u22122 ln ln\u2206\u22121) samples for infinite number of instances. In particular, we show the following stronger result.\nTheorem G.1. For any function T on (0, 1] such that lim sup\u2206\u2192+0 T (\u2206)\u2206 2 = +\u221e and for any fixed constant \u03b4 > 0, there exists a \u03b4-correct algorithm A for Sign-\u03be, such that\nlim inf \u2206\u2192+0\nTA(\u2206)\nT (\u2206) = 0.\nNow, we begin our description of the algorithm, which is in fact a simple variant of the ExpGapElim algorithm in [24]. Our algorithm takes an infinite sequence S = {\u039bi}+\u221ei=1 as input, which we call the reference sequence.\nDefinition G.2. We say an infinite sequence S = {\u039bi}+\u221ei=1 is a reference sequence if the following statements hold:\n1. 0 < \u039bi < 1, for all i.\n2. There exists a constant 0 < c < 1 such that for all i, \u039bi+1 \u2264 c \u00b7 \u039bi. Our algorithm TestSign takes a confidence level \u03b4 and the reference sequence {\u039bi} as input. It runs in rounds. In the rth round, the algorithm takes a number of samples (the actual number depends on r, and can be found in Algorithm 6) from the arm and let \u00b5\u0302r be the empirical mean. If \u00b5\u0302r \u2208 \u03be\u00b1\u039br/2, we decide that the gap \u2206 is smaller than the reference gap \u039br and we should proceed to the next round with a smaller reference gap. If \u00b5\u0302r is larger than \u03be + \u039br/2, we decide \u00b5 > \u03be. If \u00b5\u0302r is smaller than \u03be \u2212 \u039br/2, we decide \u00b5 < \u03be. The pseudocode can be found in algorithm 6. Algorithm 6: TestSign(A, \u03b4, {\u039bi}) Data: The single arm A with unknown mean \u00b5 6= \u03be, confidence level \u03b4, the reference\nsequence {\u039bi}. Result: Whether \u00b5 > \u03be or \u00b5 < \u03be.\n1 for r = 1 to +\u221e do 2 \u03b5r = \u039br/2 3 \u03b4r = \u03b4/10r 2 4 Pull A for tr = 2 ln(2/\u03b4r)/\u03b5 2 r times. Let \u00b5\u0302\nr denote its average reward. 5 if \u00b5\u0302r > \u03be + \u03b5r then 6 Return \u00b5 > \u03be 7 8 if \u00b5\u0302r < \u03be \u2212 \u03b5r then 9 Return \u00b5 < \u03be\n10\nThe algorithm can achieve the following guarantee. The proof is somewhat similar to the analysis of the ExpGapElim algorithm in [24] (in fact, simpler since there is only one arm).\nLemma G.3. Fix a confidence level \u03b4 > 0 and an arbitrary reference sequence S = {\u039bi}\u221ei=1. Suppose that the given instance has a gap \u2206. Let \u03ba be the smallest i such that \u039bi \u2264 \u2206. With probability at least 1\u2212\u03b4, TestSign determines whether \u00b5 > \u03be or \u00b5 < \u03be correctly and uses O((ln \u03b4\u22121+ ln\u03ba)\u039b\u22122\u03ba ) samples in total.\nProof. For any round r, by Hoeffding\u2019s inequality (Lemma A.2), we have that\nPr (|\u00b5\u0302r \u2212 \u00b5| \u2265 \u03b5r) \u2264 2 exp(\u2212\u03b52r/2 \u00b7 tr) = \u03b4r. (9)\nThen, by a union bound, with probability 1 \u2212 \u2211+\u221ei=1 \u03b4r = 1 \u2212 \u03b4 \u00b7 \u2211+\u221e i=1 1/10r 2 \u2265 1 \u2212 \u03b4, we have |\u00b5\u0302r \u2212\u00b5| < \u03b5r for all r. Denote this event by E . Then we prove that conditioning on E , Algorithm 6 is correct.\nLet k be the round the algorithm returns the answer. By the definition of \u03ba, we know that \u039b\u03ba \u2264 \u2206. Then on round \u03ba, we have |\u00b5\u0302\u03ba \u2212 \u00b5| < \u039b\u03ba/2 \u2264 \u2206/2. Thus, |\u00b5\u0302\u03ba \u2212 \u03be| \u2265 |\u00b5\u2212 \u03be| \u2212 |\u00b5\u0302\u03ba \u2212 \u00b5| > \u2206/2 \u2265 \u03b5r. Therefore, we can see that k \u2264 \u03ba, which shows that the algorithm terminates on or before round \u03ba. On round k, if we have \u00b5\u0302k > \u03be + \u03b5r, we must have \u00b5 > \u03be since |\u00b5\u0302k \u2212 \u00b5| < \u03b5r. The case \u00b5\u0302k < \u03be \u2212 \u03b5r is completely symmetric, which proves the correctness.\nNow, we analyze the number of samples. It is easy to see that the total number of samples is at most:\n\u03ba\u2211\nr=1\ntr = \u03ba\u2211\nr=1\n8 ln(2/\u03b4r)/\u039b 2 r .\nBy the definition of the reference sequence, \u039br \u2265 cr\u2212\u03ba\u039b\u03ba for 1 \u2264 r \u2264 \u03ba. Hence, we have that \u03ba\u2211\nr=1\n8(ln \u03b4\u22121 + ln 20 + ln r)/\u039b2r \u2264 \u039b\u22122\u03ba \u03ba\u2211\nr=1\nc2(\u03ba\u2212r) \u00b7 8(ln \u03b4\u22121 + ln 20 + ln r)\n= O ( (ln \u03b4\u22121 + ln\u03ba)\u039b\u22122\u03ba )\nThis finishes the proof of the theorem.\nFinally, we prove Theorem G.1.\nProof of Theorem G.1. First, we can easily construct a reference sequence {\u039bi} such that 1. 0 < \u039bi < 1 and \u039bi+1 \u2264 \u039bi/2 for all i.\n2. T (\u039bi) \u2265 i \u00b7 \u039b\u22122i for all i (this is possible since lim sup\u2206\u2192+0 T (\u2206)/\u2206\u22122 = +\u221e). With this reference sequence, we can see that with probability 1 \u2212 \u03b4, the algorithm outputs the correct answer and runs in O(ln \u03b4\u22121+ln\u03ba)\u039b\u22122\u03ba time. However, there is a subtlety here: the expected running time is not bounded since we do not have a bound with probability \u03b4 (when the good event E in Lemma G.3 does not happen). In Theorem H.5, we provide a general transformation that can produce a \u03b4-correct algorithm whose expected running time is O(ln \u03b4\u22121 + ln\u03ba)\u039b\u22122\u03ba . Let the algorithm be A.\nFor any fixed \u03b4, we can see that\nlim i\u2192+\u221e\nTA(\u039bi)\nT (\u039bi) \u2264 lim i\u2192+\u221e C(ln \u03b4\u22121 + ln i)\u039b\u22122i i\u039b\u22122i = 0,\nwhere C is some large constant. This implies that lim inf\u2206\u2192+0 TA(\u2206)/T (\u2206) = 0.\nRemark G.4. If we use the reference sequence {\u039bi = e\u2212i}, we have an \u03b4-correct algorithm for Sign-\u03be, and it takes O(\u2206\u22122(ln \u03b4\u22121 + ln ln\u2206\u22121)) samples in expectation on instance with gap \u2206.\nRemark G.5. Recall Farrell\u2019s (worse case) lower bound (1) is \u2126(\u2206\u22122 ln ln\u2206\u22121). Together with Theorem G.1, they imply that it is impossible to obtain an instance optimal algorithm for Sign-\u03be."}, {"heading": "H \u03b4-correct Algorithms and Parallel Simulation", "text": "In Corollary B.19, we show that our algorithm can output the correct answer with probability at least 1\u2212\u03b4, and the conditional expected running time is upper bounded. However, with probability \u03b4, there is no guarantee on its behavior (e.g., it could potentially run forever). 15 Hence, the expected running time may be unbounded. It is preferable to have an algorithm with a bounded expected running time, and being correct with probability at least 1\u2212\u03b4. In this section, we provide such a transformation that given an algorithm for the former kind, produces one of the later (the time bound only increases by a constant factor).\nNow, we formally define the two kinds of algorithms.\nDefinition H.1. Let A be an algorithm for some problem P. A takes an additional input \u03b4 as the confidence level. Let I be the set of all valid instances for P. We write A\u03b4 to denote the algorithm A with a fixed confidence level \u03b4.\n1. We call A an expected-T -time \u03b4-correct algorithm iff there exists \u03b40 \u2208 (0, 1) such that for any \u03b4 \u2208 (0, \u03b40) and instance I \u2208 I:\nTA\u03b4 [I] \u2264 T (\u03b4, I) and Pr[A\u03b4 returns the correct answer on I] \u2265 1\u2212 \u03b4.\n2. We call A a weakly expected-T -time \u03b4-correct algorithm iff there exists \u03b40 \u2208 (0, 1) such that for any \u03b4 \u2208 (0, \u03b40) and instance I \u2208 I, there exists an event E that\nPrA\u03b4,I [E ] \u2265 1\u2212\u03b4\u2227EA\u03b4,I [\u03c4 | E ] \u2264 T (\u03b4, I) and Pr[A\u03b4 returns the correct answer on I | E ] = 1.\nWe call the above event E a good event.\nWe need a mild assumption on the running times for our general transformation.\nDefinition H.2. We say a function T : (0, 1) \u00d7 I \u2192 R is a reasonable time bound, if there exists 0 < \u03b40 < 1 such that for all 0 < \u03b4 \u2032 < \u03b4 < \u03b40 and I \u2208 I we have that\nT (\u03b4\u2032, I) \u2264 ln \u03b4 \u2032\u22121\nln \u03b4\u22121 T (\u03b4, I).\nRemark H.3. The running times of most previous algorithms are of the form \u03b1(I) + \u03b2(I) ln \u03b4\u22121, where \u03b1 and \u03b2 only depend on I (e.g., see Table 1). Such running time bounds are obviously reasonable.\nSupposeA is a weakly expected-T -time \u03b4-correct algorithm. Our strategy to produce a expectedO(T )-time \u03b4-correct algorithm is to simulate many copies of A with different confidence levels. Now we formally define the parallel simulation method. Suppose we have a class of algorithms {Ai} (possibly infinite) for the same problem P. We want to construct a new algorithm B for problem P which simulates {Ai} in a parallel fashion. The details of the construction are specified as follows.\n15 Many previous algorithms for pure exploration multi-armed bandits belong to this kind, such as [13, 24, 16]. Some previous work has noticed the issue as well, but we are not aware of a systematic way to deal with it. For example, [23] stated that \u201cHowever, their (i.e., [13]) elimination algorithm could incur high sample complexity on the \u03b4-fraction of the runs on which mistakes are made\u2014we think it unlikely that elimination algorithms can yield an expected sample complexity bound smaller than ... \u201d.\nDefinition H.4. (Parallel Simulation) The new algorithm B simulates Ai with rate ri \u2208 N+, for all i. More specifically, B runs in rounds. In the r-th round, B simulates each algorithm Ai such that ri divides r (i.e., ri|r) for one step. If there are more than one such algorithms, B simulates them in the increasing order of their indices. If any such Ai requires a sample, B takes a fresh sample and feeds it to Ai.\nB terminates whenever any Ai terminates and B outputs what Ai outputs. We denote this new algorithm as B = SIM({Ai}, {ri}).\nNow we prove the main result of this section.\nTheorem H.5. Suppose T is a reasonable time bound. If A is a weakly expected-T -time \u03b4-correct algorithm for the problem P, then there exists an algorithm B which is expected-O(T )-time \u03b4-correct. Moreover, B can be constructed explicitly from A.\nProof. The construction of B is very simple: Let B\u03b4 = SIM({Ai}, {ri}), in which Ai = A\u03b4/2i (that is algorithm A with confidence parameter \u03b4/2i) and ri = 2\ni. Now we prove B is an expected-O(T )-time \u03b4-correct algorithm for problem P. Suppose the given instance is I. Let Ei be a good event for Ai on instance I. First, by a simple union bound, we can see that the probability that B\u03b4 outputs the correct\nanswer is at least 1\u2212\u2211+\u221ei=1 \u03b4/2i = 1\u2212 \u03b4 (B\u03b4 returns the correct answer if all Ai returns the correct answer).\nNow, assume \u03b4 < \u03b4\u20320 = min(0.1, \u03b40). Let us analyze the running time of B\u03b4 on instance I. For ease of argument, we can think that B executes in a slight different way. B does not terminate the same way as before, but keeps simulating all algorithms until all of them terminate (or run forever). The output of B is still the same as the Ai that terminates first, and the running time of B is determined by the first terminated Ai.\nPartition this probability space into disjoint events {Fi}, in which Fi is the event that all events Ej with j < i do not happen, and Ei happens. Note that {Fi} is indeed a partition of the probability space (Pr[\u222aiFi] = 1). This is simply because limi\u2192+\u221ePr[Ei] \u2265 limi\u2192+\u221e 1\u2212 \u03b4/2i = 1.\nLet \u03c4 be the running time of B. Since each Ai uses its own independent samples, we have that\nEAi,I [\u03c4 | Fi] = EAi,I [\u03c4 | Ei] \u2264 T (\u03b4/2i, I).\nMoreover, for Ai to run one step, for any j 6= i, Aj runs at most 2i\u2212j steps. Thus, the running time for B\u03b4 conditioning on Fi is bounded by:\nEB\u03b4,I [\u03c4 | Fi] \u2264 T (\u03b4/2i, I) \u00b7 +\u221e\u2211\nj=1\n2i\u2212j \u2264 T (\u03b4/2i, I) \u00b7 2i.\nFurthermore, by the independence of different Ais, we note that\nPr[Fi] \u2264 i\u22121\u220f\nk=1\n\u03b4/2k \u2264 \u03b4i\u22121.\nNow, we can bound the expected running time of B as follows:\nEB\u03b4,I [\u03c4 ] = +\u221e\u2211\ni=1\nPr[Fi] \u00b7 EA\u2032\u03b4,I [\u03c4 | Fi]\n\u2264 +\u221e\u2211\ni=1\n\u03b4i\u22121 \u00b7 T (\u03b4/2i, I) \u00b7 2i.\n\u2264 2 +\u221e\u2211\ni=1\n(2\u03b4)i\u22121 \u00b7 T (\u03b4, I) ( ln \u03b4\u22121 + i ln 2\nln \u03b4\u22121\n) .\n\u2264 2 +\u221e\u2211\ni=1\n(2\u03b4)i\u22121(1 + i) \u00b7 T (\u03b4, I).\n\u2264 2 +\u221e\u2211\ni=1\n(0.2)i\u22121(1 + i) \u00b7 T (\u03b4, I).\n\u2264 6T (\u03b4, I).\nIn the second inequality, we use the fact that T is a reasonable time bound. To summarize, we can see that B is an expected-O(T )-time \u03b4-correct algorithm."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "<lb>We study the best arm identification (Best-1-Arm) problem, which is defined as follows.<lb>We are given n stochastic bandit arms. The ith arm has a reward distribution<lb>Di with an<lb>unknown mean \u03bci. Upon each play of the ith arm, we can get a reward, sampled i.i.d. from<lb>Di. We would like to identify the arm with the largest mean with probability at least 1 \u2212 \u03b4,<lb>using as few samples as possible. We provide a nontrivial algorithm for Best-1-Arm, which<lb>improves upon several prior upper bounds on the same problem. We also study an important<lb>special case where there are only two arms, which we call the Sign-\u03be problem. We provide a<lb>new lower bound of Sign-\u03be, simplifying and significantly extending a classical result by Farrell<lb>in 1964, with a completely new proof. Using the new lower bound for Sign-\u03be, we obtain the<lb>first lower bound for Best-1-Arm that goes beyond the classic Mannor-Tsitsiklis lower bound,<lb>by an interesting reduction from Sign-\u03be to Best-1-Arm. We propose an interesting conjecture<lb>concerning the optimal sample complexity of Best-1-Arm from the perspective of instance-wise<lb>optimality.", "creator": "LaTeX with hyperref package"}}}