{"id": "1511.04024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Multimodal Skip-gram Using Convolutional Pseudowords", "abstract": "practically this work truly studies the functional representational mapping across multimodal data such having that given given a piece weight of the distributed raw data conveyed in one modality the encoded corresponding semantic description is in distinct terms of the raw data in another homogeneous modality thus is indeed immediately obtained. such a quantum representational mapping domain can broadly be independently found already in virtually a wide broader spectrum of distributed real - practice world applications including image / video images retrieval, object file recognition, program action / computational behavior task recognition, query and event understand research and prediction. to that end, we introduce a simplified learning training objective for students learning multimodal embeddings using the skip - gram computing architecture refined by naturally introducing convolutional'pseudowords :'viral embeddings entirely composed of easily the desired additive complementary combination of distributed texture word representations and 3d image features from previously convolutional neural networks traditionally projected into the multimodal sample space. indeed we additionally present some preliminary results of using the representational properties inherent of these multimedia embeddings on various sparse word similarity benchmarks.", "histories": [["v1", "Thu, 12 Nov 2015 19:32:08 GMT  (125kb)", "https://arxiv.org/abs/1511.04024v1", null], ["v2", "Sun, 29 Nov 2015 19:09:38 GMT  (203kb,D)", "http://arxiv.org/abs/1511.04024v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["zachary seymour", "yingming li", "zhongfei zhang"], "accepted": false, "id": "1511.04024"}, "pdf": {"name": "1511.04024.pdf", "metadata": {"source": "CRF", "title": "MULTIMODAL SKIP-GRAM USING CONVOLUTIONAL PSEUDOWORDS", "authors": ["Zachary Seymour", "Yingming Li"], "emails": ["zseymou1@binghamton.edu", "liymn@zju.edu.cn", "zhongfei@cs.binghamton.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Distributed representations of multimodal embeddings (Feng & Lapata, 2010) are receiving increasing attention recently in the machine learning literature, and techniques developed have found a wide spectrum of applications in the real world. These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary (Lazaridou et al., 2015; Glenberg & Robertson, 2000; Hill & Korhonen, 2014).\nAs such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2015; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.\nThe work introduced in Lazaridou et al. (2015) sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval.\nIn this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a;b), Hill & Korhonen (2014), and Lazaridou et al. (2015). Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space. In this way, we further address the grounding problem of Glenberg & Robertson (2000) by incorpo-\nar X\niv :1\n51 1.\n04 02\n4v 2\n[ cs\n.C L\n] 2\n9 N\nov 2\nrating the word-level visual modality directly into the sentence context. This model represents an advancement of the existing literature surrounding multimodal skip-gram, as well as multimodal distributional semantic models in general, by greatly simplifying the method of situating the words in the visual context and reducing the number of hyperparameters to tune by directly incorporating multimodal words into the existing objective function and hiearchical softmax formulations of the skip-gram models.\nFinally, we would also like the learned embeddings to be applicable to the problem of zero-shot learning (Socher et al., 2013; Lazaridou et al., 2014; Frome et al., 2013). By incorporating perceptual information into the skip-gram learning objective, we can leverage vocabulary terms for which no manually-annotated images were originally available. In this way, these learned representations can be used to both grow the annotation set and retrieve new annotations for a given image set."}, {"heading": "2 RELATED WORK", "text": "In the last few years, there has been a wealth of literature on multimodal representational models. As explained in Lazaridou et al. (2015), the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics. Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method.\nIn Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric.\nOther recent work has presented several methods for directly incorporating visual context in neural language models. In Xu et al. (2014), word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image). The multimodal skip-gram architecture proposed by Lazaridou et al. (2015) takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation. Our model makes this approach even more explicit, by training the word vectors to predict an additive composition of the textual and visual context and thus constructing an implicit mapping between the textual and visual modalities.\nFinally, the work introduced in Hill & Korhonen (2014) employs a similar \u201cpseudoword\u201d architecture to that proposed here. However, the visual features used are in the form of perceptual information derived from either user-generated attributes or other textual annotations of imagery. While this is shown to be useful for distinguishing classes of words (e.g., between abstract and concrete), it precludes any incorporation of visual, non-linguistic context and thus the derivation of any mapping between images and words or applications to representation-related tasks."}, {"heading": "3 ARCHITECTURE", "text": ""}, {"heading": "3.1 SKIP-GRAM FOR WORD REPRESENTATIONS", "text": "This model is primarily derived from the skip-gram model introduced by Mikolov et al. (2013c). Skip-gram learns representations of words that predict a target word\u2019s context. The model maximizes\n1\nT T\u2211 t=1  \u2211 \u2212c\u2264j\u2264c,j 6=0 log p(wt+j |wt)  (1)\nwhere w1, w2, . . . , wT are words in the training set and c the window size around the target word. The probablity p(wt+j |wt) is given by softmax, that is:\np(wt+j |wt) = e u\u2032wt+j Tuwt\u2211W w\u2032=1 e u\u2032 w\u2032 Tuwt (2)\nwhere uw and u\u2032w are the context vector and target vector representations induced for word w, respectively, and W gives the vocabulary size. To speed up this computation, a Huffman tree is constructed from the vocabulary, and then the softmax formula is replaced with the hierarchical softmax (Morin & Bengio, 2005)."}, {"heading": "3.2 SKIP-GRAM WITH MULTIMODAL PSEUDOWORDS", "text": "To ground the word representations in a visual context, we introduce a means of replacing a word in the corpus with its corresponding multimodal pseudoword. The pseudoword vector for a given word w, denoted zw, is given by\nzw = uw +Mvw (3)\nwhere uw is the multimodal word representation of w to be induced, vw is the visual data for the concept represented by w, and M is the mapping induced between the visual and multimodal space. (The sources of the textual and visual data are explained below.) Where no visual features are available for a given word, vw is set to 0. Thus, the objective function in (1) remains the same, while each word vector in the context window of the current word in (2) is replaced with its corresponding pseudoword. In this way, each target word in the corpus is trained to predict every given pseudoword in its context window.\nFor the value of vw, a key issue in this approach is selecting a canonical visual representation for a given concept (e.g., a single image labeled \u201cdog\u201d does not necessarily accurately represent the visual information of all dogs). Consequently, although the features extracted for a given image from a convolutional neural network (CNN) provided a higher-level visual representation than the raw pixel data, these features are not representative of the concept as a whole. Rather than handpicking an appropriate image for each class, we rely on the manner in which the CNN features form clusters\nbased on their corresponding visual concepts, a well-explored phenomenon.1 Thus, for each visual word, we can sample some images corresponding to the concept and extract CNN features for the images. Figure 1 shows the nature of some of these clusters; based on this intuition, we test two approaches to this problem, which we refer to as the centroid method and the hypersphere method.\nFor the centroid method2, the CNN features sampled for a given visual concept are averaged together. In this way, we form unified representation for each cluster by extracting its centroid and using this for vw. We not, however, that this somewhat limits the representational quality of vw by condensing the varied class of images to a single data pointl.\nTo this end, we introduce the hypersphere method as a means of capturing the complexity of the visual space. Rather than averaging the sampled CNN features, we instead fit a Gaussian mixture model to the cluster of CNN features, and, at each training step, sample a point from the model. This method, then, retains more of the variation between samples present in the dataset, while still only drawing on a small set of original images. The hypersphere technique can also be seen as a means of augmenting the training data without directly extracting more convolutional features. Since all points in a given cluster are assumed to be similar and to represent the same concept, a \u201cnew\u201d image can be used to form vw at each training step without necessitating an equivalently-large image dataset."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 EXPERIMENTAL DATA", "text": "For our text corpus, keeping with the existing literature, we use a preprocessed dump of Wikipedia3 containing approximately 800M tokens. For the visual data, we use the image data from ILSVRC 2012 (Russakovsky et al., 2015) and the corresponding Wordnet hierarchy (Miller, 1995) to represent a word visually if the word or any of its hyponyms has an entry in Imagenet and occurs more than 500 times in the text corpus. This yields approximately 5,100 \u201cvisual\u201d words.\nTo construct the vectors for the visual representations, we follow a similar experimental set-up as that used by Lazaridou et al. (2015). In each of the cases described above\u2014centroid and hypersphere\u2014, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in Krizhevsky et al. (2012) via the Caffe toolkit (Jia et al., 2014) to extract a 4096-dimensional vector representation of each image. We then treat the 100 vectors corresponding to each of the 5,100 visual words as clusters in the 4096-dimensional visual space."}, {"heading": "4.2 APPROXIMATING HUMAN SIMILARITY JUDGMENTS", "text": "Word Similarity Benchmarks To compare our technique to the existing literature, we evaluate our embeddings on four common benchmarks which capture several diverse aspects of word meaning: MEN (Bruni et al., 2014), Simlex-999 (Hill et al., 2014), SemSim (Silberer & Lapata, 2014), VisSim (Silberer & Lapata, 2014). MEN was designed to capture general word \u201crelatedness.\u201d Simlex999 and SemSim measure notions of semantic similarity, and VisSim ranks the same words as SemSim but in terms of visual similarity. In each case, the designers of the benchmarks provided pairs of words to human judges, who in turned provide ratings based on the metric of the benchmark. To judge our model, we calculate the cosine similarity of our embeddings for the word pairs and then calculate Spearman\u2019s \u03c1 between our list of ratings and those of the human judges. We evaluate three versions of our model on these benchmarks: pseudowords using the centroid method (PSUEDOWORDS-C), pseudowords using the hypersphere method (PSEUDOWORDS-H), and the centroid method with a randomly initialized mapping (PSEUDOWORDS-RAN), as explained below. Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings . Using the results published by Lazaridou et al. (2015) and a target word embedding of 300, we compare our results to their MMSKIP-GRAMA and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations\n1See, for example, http://cs.stanford.edu/people/karpathy/cnnembed/ 2This is the approach taken by Lazaridou et al. (2015) 3http://wacky.sslmit.unibo.it\nunder a max-margin framework.; the former constrains the dimensionality of the visual features to be the same as the word embeddings, while the latter learns an explicit mapping between the textual and visual spaces. We also include baseline results for pure-text skip-gram embeddings (SKIP-GRAM))."}, {"heading": "4.3 RESULTS", "text": "The results for the human judgment experiments are presented in Table 1. For these experiments, we tried two methods of initializing the mapping. First, Random Initialization: the visual-textual mapping matrix was randomly initialized in the same manner as the word embeddings, with the goal of allowing the mapping to be freely generated from the word context. Second, Neural Weight Initialization: to boost the performance of the multimodal embeddings, the mapping was initialized with the weights from a simple neural network trained to predict known word embeddings 4 from our convolutional image features."}, {"heading": "4.3.1 RANDOM INITIALIZATION", "text": "Interestingly, there is a degradation in the correlation from the addition of the visual features across all benchmarks. This seems to indicate that the induced mapping, when beginning with a random initialization, is yet insufficient to properly situate the convolutional features into the multimodal space. It would seem initially that, during training, while the mapping is still being learned, adding the visual context to the text vectors perhaps worsens the representational quality of the word embedding."}, {"heading": "4.3.2 NEURAL WEIGHT INITIALIZATION", "text": "On the other hand, when the mapping is quickly pretrained on existing distributed word representations, the results are greatly improved. In the cases of capturing general relatedness and pure visual similarity, the multimodal model of Lazaridou et al. (2015) performs better. However, in the case of capturing semantic word similarity, our model performs signficantly better than MMSKIP-GRAM-B (although it should be noted that these results are roughly on par with the benchmark authors (Silberer & Lapata, 2014) and a point below the non-mapping MMSKIP-GRAM-A). Although further work is needed to examine this result, the performance of the model in this case can be visualized through an example. Table 2 provides some insights on the changes made to the word embeddings as a result of the inclusion of visual information in the learning process. In the two visual instances, our model captures many of the same nuances as MMSKIP-GRAM-B over the SKIP-GRAM model: donuts are more similar to other types of food than to places where you find donuts and owls are more similar to other birds of prey than just woodland creatures. However, our model seems to capture more of the semantic idea of donuts as \u201cjunk food\u201d rather than just the visual similarity of roundness (the link established between donut and cupcake is particularly interesting). As for owl, some of the visual similarity is lost, by ranking sparrow first, with regards to the class of birds of prey, but there seems to be a recognition of the semantic relationship between the top similar words (\u201csparrow hawk\u201d is a synonym for \u201ckestrel\u201d in Wordnet, for example) as well as visual similarity via brown feathers and beaks.\nAs for the representations learned without explicit visual information, our model still seems to demonstrate the propagation of this information but in a different manner than MMSKIP-GRAM-\n4We used embeddings from the Google News dataset available at https://code.google.com/p/word2vec.\nB. The words ranked as similar to mural lose the artistic concepts of painting and portrait ranked highly by the other models; instead our model ranks \u201cfresco\u201d and \u201cbas-relief\u201d alongside sculpture, capturing instead a more complex representation of \u201cartwork executed directly on a wall.\u201d For tobacco, our model dismisses the recreational uses of tobacco captured via \u201ccigar\u201d and \u201ccigarette,\u201d while also ignoring the na\u0131\u0308ve \u201ccrop\u201d sense captured by \u201ccorn.\u201d Instead, the highly-ranked words seem to display a more robust use of tobacco in the semanic sense as a cash crop, specifically referencing other notable trade crops.5\nThe two abstract concepts, depth and chaos, reveal two very different results of the visual propagation. For depth, no evidence of the visual is apparent: unlike the relation to the sea drawn by MMSKIP-GRAM-B, our model seems to only capture depth\u2019s semantic similarity to other types of measurement (height and thickness). For chaos, there is still the loss of more imageable concepts, as in shadow, from the rankings; however, our model instead seems to capture words more semantically similar to chaos (anarchy) or synonyms of events like chaos (turmoil and pandemonium)."}, {"heading": "5 CONCLUSION", "text": "Our model performs multimodal skip-gram using pseudowords construction from convolutional image features, and then demonstrates the propagation of visual information to non-visual words but in a seemingly distinct manner from the existing models. Of particular note is that it is apparent that distributed word representations are being improved and informed by this information over pure-text skipgram, but these embeddings seem to perform best at a semantic rather than a visual level. Future work will focus on the nature of this embedding. In particular, we will investigate the applicability of the induced textual-visual mapping to the task of zero-shot image labeling (Socher et al., 2013) and image retrieval. As of yet, the nature of the mapping, beyond the qualitative improvements provided to the word embeddings, is still unclear, and future work will seek to address this."}], "references": [{"title": "Multimodal distributional semantics", "author": ["Bruni", "Elia", "Tran", "Nam-Khanh", "Baroni", "Marco"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Visual information in semantic representation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 91\u201399", "author": ["Feng", "Yansong", "Lapata", "Mirella"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Feng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2010}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome", "Andrea", "Corrado", "Greg S", "Shlens", "Jon", "Bengio", "Samy", "Dean", "Jeff", "Mikolov", "Tomas"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Symbol grounding and meaning: A comparison of high-dimensional and embodied theories of meaning", "author": ["Glenberg", "Arthur M", "Robertson", "David A"], "venue": "Journal of memory and language,", "citeRegEx": "Glenberg et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Glenberg et al\\.", "year": 2000}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably cant see what i mean", "author": ["Hill", "Felix", "Korhonen", "Anna"], "venue": "In Proceedings of EMNLP, pp. 255\u2013265,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill", "Felix", "Reichart", "Roi", "Korhonen", "Anna"], "venue": "arXiv preprint arXiv:1408.3456,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["Kiela", "Douwe", "Bottou", "L\u00e9on"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kiela et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Multimodal neural language models", "author": ["Kiros", "Ryan", "Salakhutdinov", "Ruslan", "Zemel", "Rich"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world", "author": ["Lazaridou", "Angeliki", "Bruni", "Elia", "Baroni", "Marco"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2014}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Lazaridou", "Angeliki", "Pham", "Nghia The", "Baroni", "Marco"], "venue": "arXiv preprint arXiv:1501.02598,", "citeRegEx": "Lazaridou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov", "Tomas", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Tomas", "Yih", "Wen-tau", "Zweig", "Geoffrey"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["Miller", "George A"], "venue": "Communications of the ACM,", "citeRegEx": "Miller and A.,? \\Q1995\\E", "shortCiteRegEx": "Miller and A.", "year": 1995}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In Proceedings of the international workshop on artificial intelligence and statistics,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Russakovsky", "Olga", "Deng", "Jia", "Su", "Hao", "Krause", "Jonathan", "Satheesh", "Sanjeev", "Ma", "Sean", "Huang", "Zhiheng", "Karpathy", "Andrej", "Khosla", "Aditya", "Bernstein", "Michael", "Berg", "Alexander C", "Fei-Fei", "Li"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["Silberer", "Carina", "Lapata", "Mirella"], "venue": "In Proceedings of ACL, pp", "citeRegEx": "Silberer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2014}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Socher", "Richard", "Ganjoo", "Milind", "Manning", "Christopher D", "Ng", "Andrew"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improving word representations via global visual context", "author": ["Xu", "Ran", "Lu", "Jiasen", "Xiong", "Caiming", "Yang", "Zhi", "Corso", "Jason J"], "venue": "NIPS Workshop on Learning Semantics,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "These types of vector representations are particularly desirable for the way in which they better model the grounding of perceptual or semantic concepts in human vocabulary (Lazaridou et al., 2015; Glenberg & Robertson, 2000; Hill & Korhonen, 2014).", "startOffset": 173, "endOffset": 248}, {"referenceID": 11, "context": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2015; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.", "startOffset": 96, "endOffset": 205}, {"referenceID": 8, "context": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2015; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.", "startOffset": 96, "endOffset": 205}, {"referenceID": 2, "context": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2015; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.", "startOffset": 96, "endOffset": 205}, {"referenceID": 0, "context": "As such, there has been development towards so-called multimodal distributional semantic models (Silberer & Lapata, 2014; Lazaridou et al., 2015; Kiros et al., 2014; Frome et al., 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts.", "startOffset": 96, "endOffset": 205}, {"referenceID": 0, "context": ", 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in Lazaridou et al. (2015) sought to address many of the drawbacks of these models.", "startOffset": 8, "endOffset": 189}, {"referenceID": 0, "context": ", 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in Lazaridou et al. (2015) sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval. In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a;b), Hill & Korhonen (2014), and Lazaridou et al.", "startOffset": 8, "endOffset": 1158}, {"referenceID": 0, "context": ", 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in Lazaridou et al. (2015) sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval. In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a;b), Hill & Korhonen (2014), and Lazaridou et al. (2015). Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space.", "startOffset": 8, "endOffset": 1187}, {"referenceID": 0, "context": ", 2013; Bruni et al., 2014), which leverage textual co-occurance and visual features to form multimodal representations of words or concepts. The work introduced in Lazaridou et al. (2015) sought to address many of the drawbacks of these models. In particular, by incorporating visual information into the training objective, they address the biological inaccuracy of the existing models, in that word representations grounded in visual information have been shown to more closely approximate the way humans learn language. Furthermore, incorporating visual information alongside the text corpus allows the training set to consist of both visual and non-visual words. As a result, the induced multimodal representations and multimodal mapping no longer rely on the assumption of full visual coverage of the vocabulary, so the results are able to generalize beyond the initial training set and to be applied to various representation-related tasks, such as image annotation or retrieval. In this work, we introduce a further refinement on the multimodal skip-gram architecture, building upon the approaches of Mikolov et al. (2013a;b), Hill & Korhonen (2014), and Lazaridou et al. (2015). Rather than adding a visual term to the linguistic training objective, we directly situate terms in a visual context by replacing relevant words with multimodal pseudowords, derived by composing the textual representations with convolutional features projected into the multimodal space. In this way, we further address the grounding problem of Glenberg & Robertson (2000) by incorpo-", "startOffset": 8, "endOffset": 1561}, {"referenceID": 19, "context": "Finally, we would also like the learned embeddings to be applicable to the problem of zero-shot learning (Socher et al., 2013; Lazaridou et al., 2014; Frome et al., 2013).", "startOffset": 105, "endOffset": 170}, {"referenceID": 10, "context": "Finally, we would also like the learned embeddings to be applicable to the problem of zero-shot learning (Socher et al., 2013; Lazaridou et al., 2014; Frome et al., 2013).", "startOffset": 105, "endOffset": 170}, {"referenceID": 2, "context": "Finally, we would also like the learned embeddings to be applicable to the problem of zero-shot learning (Socher et al., 2013; Lazaridou et al., 2014; Frome et al., 2013).", "startOffset": 105, "endOffset": 170}, {"referenceID": 8, "context": "As explained in Lazaridou et al. (2015), the majority of this literature focuses on constructing textual and visual representations independently and then combining them under some metrics.", "startOffset": 16, "endOffset": 40}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text.", "startOffset": 0, "endOffset": 288}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric.", "startOffset": 0, "endOffset": 526}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric. Other recent work has presented several methods for directly incorporating visual context in neural language models. In Xu et al. (2014), word context is enhanced by global visual context; i.", "startOffset": 0, "endOffset": 812}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric. Other recent work has presented several methods for directly incorporating visual context in neural language models. In Xu et al. (2014), word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image). The multimodal skip-gram architecture proposed by Lazaridou et al. (2015) takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation.", "startOffset": 0, "endOffset": 1065}, {"referenceID": 0, "context": "Bruni et al. (2014) utilize a direct approach to \u201cmixing\u201d the vector representations by concatenating the text and image vectors and applying Singular Value Decomposition. The image vectors used here, though, are constructed using the bag-of-visual-words method. In Kiela & Bottou (2014), the authors utilize a more sophisticated approach to the concatenation method by extracting visual features using state-of-the-art convolutional neural networks and the skip-gram architecture for the text. Similarly, Frome et al. (2013) also utilizes the skip-gram architecture and convolutional features; however the two modalities are then combined using a natural similarity metric. Other recent work has presented several methods for directly incorporating visual context in neural language models. In Xu et al. (2014), word context is enhanced by global visual context; i.e., a single image is used as the context for the whole sentence (conversely, the sentence acts as a caption for the image). The multimodal skip-gram architecture proposed by Lazaridou et al. (2015) takes a more fine-grained approach by incorporating word-level visual context and concurrently training words to predict other text words in the window as well as their visual representation. Our model makes this approach even more explicit, by training the word vectors to predict an additive composition of the textual and visual context and thus constructing an implicit mapping between the textual and visual modalities. Finally, the work introduced in Hill & Korhonen (2014) employs a similar \u201cpseudoword\u201d architecture to that proposed here.", "startOffset": 0, "endOffset": 1545}, {"referenceID": 12, "context": "This model is primarily derived from the skip-gram model introduced by Mikolov et al. (2013c). Skip-gram learns representations of words that predict a target word\u2019s context.", "startOffset": 71, "endOffset": 94}, {"referenceID": 17, "context": "For the visual data, we use the image data from ILSVRC 2012 (Russakovsky et al., 2015) and the corresponding Wordnet hierarchy (Miller, 1995) to represent a word visually if the word or any of its hyponyms has an entry in Imagenet and occurs more than 500 times in the text corpus.", "startOffset": 60, "endOffset": 86}, {"referenceID": 6, "context": "(2012) via the Caffe toolkit (Jia et al., 2014) to extract a 4096-dimensional vector representation of each image.", "startOffset": 29, "endOffset": 47}, {"referenceID": 8, "context": "To construct the vectors for the visual representations, we follow a similar experimental set-up as that used by Lazaridou et al. (2015). In each of the cases described above\u2014centroid and hypersphere\u2014, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in Krizhevsky et al.", "startOffset": 113, "endOffset": 137}, {"referenceID": 8, "context": "In each of the cases described above\u2014centroid and hypersphere\u2014, we randomly sample 100 images from the corresponding synsets of Imagenet for each visual word and use a pre-trained convolutional neural network as described in Krizhevsky et al. (2012) via the Caffe toolkit (Jia et al.", "startOffset": 225, "endOffset": 250}, {"referenceID": 0, "context": "Word Similarity Benchmarks To compare our technique to the existing literature, we evaluate our embeddings on four common benchmarks which capture several diverse aspects of word meaning: MEN (Bruni et al., 2014), Simlex-999 (Hill et al.", "startOffset": 192, "endOffset": 212}, {"referenceID": 4, "context": ", 2014), Simlex-999 (Hill et al., 2014), SemSim (Silberer & Lapata, 2014), VisSim (Silberer & Lapata, 2014).", "startOffset": 20, "endOffset": 39}, {"referenceID": 0, "context": "Word Similarity Benchmarks To compare our technique to the existing literature, we evaluate our embeddings on four common benchmarks which capture several diverse aspects of word meaning: MEN (Bruni et al., 2014), Simlex-999 (Hill et al., 2014), SemSim (Silberer & Lapata, 2014), VisSim (Silberer & Lapata, 2014). MEN was designed to capture general word \u201crelatedness.\u201d Simlex999 and SemSim measure notions of semantic similarity, and VisSim ranks the same words as SemSim but in terms of visual similarity. In each case, the designers of the benchmarks provided pairs of words to human judges, who in turned provide ratings based on the metric of the benchmark. To judge our model, we calculate the cosine similarity of our embeddings for the word pairs and then calculate Spearman\u2019s \u03c1 between our list of ratings and those of the human judges. We evaluate three versions of our model on these benchmarks: pseudowords using the centroid method (PSUEDOWORDS-C), pseudowords using the hypersphere method (PSEUDOWORDS-H), and the centroid method with a randomly initialized mapping (PSEUDOWORDS-RAN), as explained below. Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings . Using the results published by Lazaridou et al. (2015) and a target word embedding of 300, we compare our results to their MMSKIP-GRAMA and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations See, for example, http://cs.", "startOffset": 193, "endOffset": 1320}, {"referenceID": 0, "context": "Word Similarity Benchmarks To compare our technique to the existing literature, we evaluate our embeddings on four common benchmarks which capture several diverse aspects of word meaning: MEN (Bruni et al., 2014), Simlex-999 (Hill et al., 2014), SemSim (Silberer & Lapata, 2014), VisSim (Silberer & Lapata, 2014). MEN was designed to capture general word \u201crelatedness.\u201d Simlex999 and SemSim measure notions of semantic similarity, and VisSim ranks the same words as SemSim but in terms of visual similarity. In each case, the designers of the benchmarks provided pairs of words to human judges, who in turned provide ratings based on the metric of the benchmark. To judge our model, we calculate the cosine similarity of our embeddings for the word pairs and then calculate Spearman\u2019s \u03c1 between our list of ratings and those of the human judges. We evaluate three versions of our model on these benchmarks: pseudowords using the centroid method (PSUEDOWORDS-C), pseudowords using the hypersphere method (PSEUDOWORDS-H), and the centroid method with a randomly initialized mapping (PSEUDOWORDS-RAN), as explained below. Existing Multimodal Models We compare our results on these benchmarks against previously published results for other multimodal word embeddings . Using the results published by Lazaridou et al. (2015) and a target word embedding of 300, we compare our results to their MMSKIP-GRAMA and MMSKIP-GRAM-B, which maximize the similarity of the textual and visual representations See, for example, http://cs.stanford.edu/people/karpathy/cnnembed/ This is the approach taken by Lazaridou et al. (2015) http://wacky.", "startOffset": 193, "endOffset": 1613}, {"referenceID": 10, "context": "In the cases of capturing general relatedness and pure visual similarity, the multimodal model of Lazaridou et al. (2015) performs better.", "startOffset": 98, "endOffset": 122}], "year": 2015, "abstractText": "This work studies the representational mapping across multimodal data such that given a piece of the raw data in one modality the corresponding semantic description in terms of the raw data in another modality is immediately obtained. Such a representational mapping can be found in a wide spectrum of real-world applications including image/video retrieval, object recognition, action/behavior recognition, and event understanding and prediction. To that end, we introduce a simplified training objective for learning multimodal embeddings using the skip-gram architecture by introducing convolutional \u201cpseudowords:\u201d embeddings composed of the additive combination of distributed word representations and image features from convolutional neural networks projected into the multimodal space. We present extensive results of the representational properties of these embeddings on various word similarity benchmarks to show the promise of this approach.", "creator": "LaTeX with hyperref package"}}}