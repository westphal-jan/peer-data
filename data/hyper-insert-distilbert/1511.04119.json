{"id": "1511.04119", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Nov-2015", "title": "Action Recognition using Visual Attention", "abstract": "we propose a quantum soft attention based model for the usual task modeling of action phase recognition observed in videos. we use multi - layered recurrent neural networks ( mp rnns ) with long series short - term memory ( lstm ) units which seemingly are deep networks both in spatially and temporally. our model learns people to consistently focus words selectively poorly on processing parts of the video viewed frames together and classifies videos better after taking a few glimpses. modeling the model essentially learns which parts in the lower frames are exactly relevant for directing the main task at by hand and attaches higher attention importance to them. formally we mainly evaluate how the key model on linear ucf - cd 11 ( live youtube / action ), with hmdb - 51 videos and hollywood2 multimedia datasets and i analyze how the main model focuses its whole attention deficit depending on both the scene depth and the action being performed.", "histories": [["v1", "Thu, 12 Nov 2015 23:06:42 GMT  (5992kb,D)", "http://arxiv.org/abs/1511.04119v1", null], ["v2", "Wed, 6 Jan 2016 20:46:47 GMT  (5994kb,D)", "http://arxiv.org/abs/1511.04119v2", null], ["v3", "Sun, 14 Feb 2016 17:20:19 GMT  (5993kb,D)", "http://arxiv.org/abs/1511.04119v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["shikhar sharma", "ryan kiros", "ruslan salakhutdinov"], "accepted": false, "id": "1511.04119"}, "pdf": {"name": "1511.04119.pdf", "metadata": {"source": "CRF", "title": "ACTION RECOGNITION USING VISUAL ATTENTION", "authors": ["Shikhar Sharma", "Ryan Kiros"], "emails": ["shikhar@cs.toronto.edu", "rkiros@cs.toronto.edu", "rsalakhu@cs.toronto.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "It has been noted in visual cognition literature that humans do not focus their attention on an entire scene at once (Rensink, 2000). Instead, they focus sequentially on different parts of the scene to extract relevant information. Most traditional computer vision algorithms do not employ attention mechanisms and are indifferent to various parts of the image/video. With the recent surge of interest in deep neural networks, attention based models have been shown to achieve promising results on several challenging tasks, including caption generation (Xu et al., 2015), machine translation (Bahdanau et al., 2015), game-playing and tracking (Mnih et al., 2014), as well as image recognition (e.g. Street View House Numbers dataset (Ba et al., 2015b)). Many of these models have employed LSTM (Hochreiter & Schmidhuber, 1997) based RNNs and have shown good results in learning sequences.\nAttention models can be classified into soft attention and hard attention models. Soft attention models are deterministic and can be trained using backpropagation, whereas hard attention models are stochastic and can be trained by the REINFORCE algorithm (Williams, 1992; Mnih et al., 2014), or by maximizing a variational lower bound or using importance sampling (Ba et al., 2015b;a). Learning hard attention models can become computationally expensive as it requires sampling. In soft attention approaches, on the other hand, a differentiable mapping can be used from all the locations output to the next input. Attention based models can also potentially infer the action happening in videos by focusing only on the relevant places in each frame. For example, Fig. 1a shows four frames from the UCF-11 video sequence belonging to the \u201cgolf swinging\u201d category. The model tends to focus on the ball, the club, and the human, which allows the model to correctly recognize the activity as \u201cgolf swinging\u201d. In Fig. 1b, our model attends to the trampoline, while correctly identifying the activity as \u201ctrampoline jumping\u201d.\nIn this paper we propose a soft attention based recurrent model for action recognition. We describe how our model dynamically pools convolutional features and show that using these features for action recognition gives better results compared to average or max pooling which is used by many of the existing models (Zha et al., 2015). We further demonstrate that our model tends to recognize important elements in video frames based on the activities it detects."}, {"heading": "2 RELATED WORK", "text": "Convolutional Neural Networks (CNNs) have been highly successful in image classification and object recognition tasks (Ren et al., 2015; Wu et al., 2015). Classifying videos instead of images\nar X\niv :1\n51 1.\n04 11\n9v 1\n[ cs\n.L G\n] 1\n2 N\nov 2\n01 5\nadds a temporal dimension to the problem of image classification. Learning temporal dynamics is a difficult problem and earlier approaches have used optical flow, HOG and hand-crafted features to generate descriptors with both appearance and dynamics information encoded. LSTMs have been recently shown to perform well in the domain of speech recognition (Graves et al., 2013), machine translation (Sutskever et al., 2014), image description (Xu et al., 2015; Vinyals et al., 2015) and video description (Yao et al., 2015; Venugopalan et al., 2014). They have also started picking up momentum in action recognition (Srivastava et al., 2015; Ng et al., 2015).\nMost of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015). LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework. More recently, Yao et al. (2015) have proposed to use 3-D CNN features and an LSTM decoder in an encoder-decoder framework to generate video descriptions. Their model incorporates attention on a video level by defining a probability distribution over frames used to generate individual words. They, however, do not employ an attention mechanism on a frame level (i.e. within a single frame).\nIn general, it is rather difficult to interpret internal representations learned by deep neural networks. Attention models add a dimension of interpretability by capturing where the model is focusing its attention when performing a particular task. Karpathy et al. (2014) used a multi-resolution CNN architecture to perform action recognition in videos. They mention the concept of fovea but they fix attention to the center of the frame. A recent work of Xu et al. (2015) used both soft attention and hard attention mechanisms to generate image descriptions. Their model actually looks at the respective objects when generating their description. More recently, Jaderberg et al. (2015) have proposed a soft-attention mechanism called the Spatial Transformer module which they add between the layers of CNNs. Instead of weighting locations using a softmax layer which we do, they apply affine transformations to multiple layers of their CNN to attend to the relevant part and get state-ofthe-art results on the Street View House Numbers dataset (Netzer et al., 2011)."}, {"heading": "3 THE MODEL AND THE ATTENTION MECHANISM", "text": ""}, {"heading": "3.1 CONVOLUTIONAL FEATURES", "text": "We extract the last convolutional layer obtained by pushing the video frames through GoogLeNet model (Szegedy et al., 2015) trained on the ImageNet dataset (Deng et al., 2009). This last convolutional layer has D convolutional maps and is a feature cube of shape K \u00d7K \u00d7D (7\u00d7 7\u00d7 1024 in our experiments). Thus, at each time-step t, we extract K2 D-dimensional vectors. We refer to these vectors as feature slices in a feature cube:\nXt = [Xt,1, . . . ,Xt,K2 ], Xt,i \u2208 RD.\nEach of these K2 vertical feature slices maps to different overlapping regions in the input space and our model chooses to focus its attention on these K2 regions."}, {"heading": "3.2 THE LSTM AND THE ATTENTION MECHANISM", "text": "We use the LSTM implementation discussed in Zaremba et al. (2014) and Xu et al. (2015): itftot gt  =  \u03c3\u03c3\u03c3 tanh M (ht\u22121,xt ) , (1)\nct = ft ct\u22121 + it gt, (2) ht = ot tanh(ct), (3)\nwhere it is the input gate, ft is the forget gate, ot is the output gate, and gt is calculated as shown in Eq. 1. ct is the cell state, ht is the hidden state, and xt (see Eqs. 4, 5) represents the input to the LSTM at time-step t. M : Ra \u2192 Rb is an affine transformation consisting of trainable parameters with a = d+D and b = 4d, where d is the dimensionality of all of it, ft, ot, gt, ct, and ht.\nAt each time-step t, our model predicts lt+1, a softmax overK\u00d7K locations, and yt, a softmax over the label classes with an additional hidden layer with tanh activations (see Fig. 2b). The location softmax is defined as follows:\nlt,i = p(Lt = i|ht\u22121) = exp(W>i ht\u22121)\u2211K\u00d7K\nj=1 exp(W > j ht\u22121)\ni \u2208 1 . . .K2, (4)\nwhere Wi are the weights mapping to the ith element of the location softmax and Lt is a random variable which can take 1-of-K2 values. This softmax can be thought of as the probability with which our model believes the corresponding region in the input frame is important. After calculating these probabilities, the soft attention mechanism (Bahdanau et al., 2015) computes the expected value of the input at the next time-step xt by taking expectation over the feature slices at different regions (see Fig. 2a):\nxt = Ep(Lt|ht\u22121)[Xt] = K2\u2211 i=1 lt,iXt,i, (5)\nwhere Xt is the feature cube and Xt,i is the ith slice of the feature cube at time-step t. Note that in the hard attention based models, we would sample Lt from a softmax distribution of Eq. 4. The input xt would then be the feature slice at the sampled location instead of taking expectation over all the slices. Thus, hard attention based models are not differentiable and have to resort to some form of sampling.\nWe initialize the cell state and the hidden state of the LSTM as:\nc0 = finit,c  1 T T\u2211 t=1  1 K2 K2\u2211 i=1 Xt,i  and h0 = finit,h  1 T T\u2211 t=1  1 K2 K2\u2211 i=1 Xt,i  , (6) where finit,c and finit,h are two multilayer perceptrons and T is the number of time-steps in the model. These values are used to calculate the first location softmax l1 which determines the initial input x1. In our experiments, we use multi-layered deep LSTMs, as shown in Fig. 2b."}, {"heading": "3.3 LOSS FUNCTION AND THE ATTENTION PENALTY", "text": "We use cross-entropy loss coupled with the doubly stochastic penalty introduced in Xu et al. (2015). We impose an additional constraint over the location softmax, so that \u2211T t=1 lt,i \u2248 1. This is the attention regularization which forces the model to look at each region of the frame at some point in time. The loss function is defined as follows:\nL = \u2212 T\u2211\nt=1 C\u2211 i=1 yt,i log y\u0302t,i + \u03bb K2\u2211 i=1 (1\u2212 T\u2211 t=1 lt,i) 2 + \u03b3 \u2211 i \u2211 j \u03b82i,j , (7)\nwhere yt is the one hot label vector, y\u0302t is the vector of class probabilities at time-step t, T is the total number of time-steps, C is the number of output classes, \u03bb is the attention penalty coefficient, \u03b3 is the weight decay coefficient, and \u03b8 represents all the model parameters. Details about the architecture and hyper-parameters are given in Section 4.2."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 DATASETS", "text": "We have used UCF-11, HMDB-51 and Hollywood2 datasets in our experiments. UCF-11 is the YouTube Action dataset consisting of 1600 videos and 11 actions - basketball shooting, biking/cycling, diving, golf swinging, horse back riding, soccer juggling, swinging, tennis swinging, trampoline jumping, volleyball spiking, and walking with a dog. The clips have a frame rate of 29.97 fps and each video has only one action associated with it. We use 975 videos for training and 625 videos for testing.\nHMDB-51 Human Motion Database dataset provides three train-test splits each consisting of 5100 videos. These clips are labeled with 51 classes of human actions like Clap, Drink, Hug, Jump, Somersault, Throw and many others. Each video has only one action associated with it. The training set for each split has 3570 videos (70 per category) and the test set has 1530 videos (30 per category). The clips have a frame rate of 30 fps.\nHollywood2 Human Actions dataset consists of 1707 video clips collected from movies. These clips are labeled with 12 classes of human actions - AnswerPhone, DriveCar, Eat, FightPerson, GetOutCar, HandShake, HugPerson, Kiss, Run, SitUp, SitDown and StandUp. Some videos have multiple actions associated with them. The training set has 823 videos and the testing set has 884 videos.\nAll the videos in the datasets were resized to 224 \u00d7 224 resolution and fed to a GoogLeNet model trained on the ImageNet dataset. The last convolutional layer of size 7 \u00d7 7 \u00d7 1024 was used as an input to our model."}, {"heading": "4.2 TRAINING DETAILS AND EVALUATION", "text": "In all of our experiments, model architecture and various other hyper-parameters were set using cross-validation. In particular, for all datasets we trained 3-layer LSTM models, where the dimensionality of the LSTM hidden state, cell state, and the hidden layer were set to 512 for both UCF-11 and Hollywood2 and 1024 for HMDB-51. We also experimented with models having one LSTM layer to five LSTM layers, but did not observe any significant improvements in model performance. For the attention penalty coefficient we experimented with values 0, 1, 10. While reporting results, we have set the weight decay penalty to 10\u22125 and use dropout (Srivastava et al.,\n2014) of 0.5 at all non-recurrent connections. All models were trained using Adam optimization algorithm (Kingma & Ba, 2015) for 15 epochs over the entire datasets. However, we found that Adam usually converged after 3 epochs. Our implementation is based in Theano (Bastien et al., 2012) which also handles the gradient computation and our code is available at https: //github.com/kracwarlock/action-recognition-visual-attention.\nFor both training and testing our model takes 30 frames at a time sampled at fixed fps rates. We split each video into groups of 30 frames starting with the first frame, selecting 30 frames according to the fps rate, and then moving ahead with a stride of 1. Each video thus gets split into multiple 30-length samples. At test time, we compute class predictions for each time step and then average those predictions over 30 frames. To obtain a prediction for the entire video clip, we average the predictions from all 30 frame blocks in the video."}, {"heading": "4.2.1 BASELINES", "text": "The softmax regression model uses the complete 7\u00d77\u00d71024 feature cube as its input to predict the label at each time-step t, while all other models use only a 1024-dimensional feature slice as their input. The average pooled and max pooled LSTM models use the same architecture as our model except that they do not have any attention mechanism and thus do not produce a location softmax. The inputs at each time-step for these models are obtained by doing average or max pooling over the 7\u00d7 7\u00d7 1024 cube to get 1024 dimensional slices, whereas our soft attention model dynamically weights the slices by the location softmax (see Eq. 5)."}, {"heading": "4.3 QUANTITATIVE ANALYSIS", "text": "Table 1 reports accuracies on both UCF-11 and HMDB-51 datasets and mean average precision (mAP) on Hollywood2. Even though the softmax regression baseline is given the complete 7\u00d7 7\u00d7 1024 cube as its input, it performs worse than our model for all three datasets and worse than all\nmodels in the case of HMDB-51 and Hollywood2. The results from Table 1 demonstrate that our attention model performs better than both average and max pooled LSTMs.\nWe next experimented with doubly stochastic penalty term \u03bb (see Eq. 7). Figure 3a shows that with no attention regularization term, \u03bb = 0, the model tends to select a few specific locations and stay fixed on them. Setting \u03bb = 1 encourages the model further explore different gaze locations. The model with \u03bb = 10 looks everywhere (see Fig. 3c), in which case its behavior tends to become similar to the average pooling case. Values in between these correspond to dynamic weighted averaging of the slices. The models with \u03bb = 0 and \u03bb = 1 perform better than the models with \u03bb = 10.\nIn Table 2, we compare the performance of our model with other state-of-the-art action recognition models. We have divided the table into three sections. Models in the first section use only RGB data while models in the second section use both RGB and optical flow data. The model in the third section uses both RGB, optical flow, as well as object responses of the videos on some ImageNet categories. Our model performs competitively against deep learning models in its category (models using RGB features only), while providing some insight into where the neural network is looking."}, {"heading": "4.4 QUALITATIVE ANALYSIS", "text": "Figure 4 shows some test examples of where our model attends to on UCF-11 dataset. In Fig. 4a, we see that the model was able to focus on parts of the cycle, while correctly recognizing the activity as \u201ccycling\u201d. Similarly, in Fig. 4b, the model attends to the dogs and classifies the activity as \u201cwalking with a dog\u201d.\nWe can also better understand failures of the model using the attention mechanism. For example, Fig. 5a shows that the model mostly attends to the background like the light blue floor of the court. The model incorrectly classifies the example as \u201cdiving\u201d. However, using a different manually specified glimpse, as shown in Fig. 5b, the model classifies the same example as \u201cvolleyball spiking\u201d. It is quite interesting to see that we can better understand the success and failure cases of this deep attention model by visualizing where it attends to.1\nThe model does not always need to attend to the foreground. In many cases the camera is far away and it may be difficult to make out what the humans are doing or what the objects in the frames are. In these cases the model tends to look at the background and tries to infer the activity from the information in the background. For example, the model can look at the basketball court in the background and predict the action being performed. Thus, depending on the video both foreground and background might be important for activity recognition. Some examples are shown in Fig. 6, where the model appears to look everywhere.\n1All the figures are from our best performing models with \u03bb = 0 unless otherwise mentioned.\nIt is also interesting to observe that in some cases, the model is able to attend to important objects in the video frames and attempts to track them to some extent in order to correctly identify the performed activity. In Fig. 7b, the video is sampled at 30fps and subsequent frames are almost identical. In this case the model stays focused on the golf ball, club, and the human. However, when we change the sampling rate to 6fps, as shown in Fig. 7a, we find that the video frames change quickly. The model now remains focused on the ball before it disappears. After the person hits the ball, we see that the model tries to look at other places, possibly to track the ball and the golf club.\nWe next examined the model\u2019s performance on the HMDB-51 dataset.2 In Fig. 8a the model attempts to focus on the person performing push-ups to recognize \u201cPushup\u201d activity. In Fig. 8c the model classifies the example of \u201cKickBall\u201d incorrectly as \u201cSomersault\u201d despite attending to the location where the action is happening. In some cases, however, the model fails to even attend to the relevant location (see Fig. 8d). For Hollywood2, Fig. 8b shows an example of a short clip belonging to the \u201cKiss\u201d action. It appears that the model correctly anticipates that a kiss is going to take place and attempts to focus on the region between the man and the woman.\nIn our final set of experiments, we have tried to examine some failure cases of our attention mechanism. As an example, Fig. 9 shows a test video clip of \u201csoccer juggling\u201d (top row). Our model focuses on the white boundaries of the field (second row), while incorrectly recognizing the activity as \u201ctennis swinging\u201d. To see whether we can potentially correct the model\u2019s mistake by forcing it to look at the relevant locations, we took a trained model and initialized the location softmax weights to uniform random numbers between the minimum and maximum in the original model. The model\u2019s glimpse in this case is shown in the third row of Fig. 9. We next optimized only the softmax weights, or the location variables, for this specific example of \u201csoccer juggling\u201d to find the glimpse for which the model would predict it correctly. All the other model parameters were kept fixed. Note that this only changes the sequences of glimpses, or where the model attends to, and not the model itself. It is interesting to see that in order to classify this video clip correctly, the glimpse the model learns (the fourth row of Fig. 9) tends to focus on the soccer player\u2019s legs."}, {"heading": "5 CONCLUSION", "text": "In this paper we developed recurrent soft attention based models for action recognition and analyzed where they focus their attention. Our proposed model tends to recognize important elements in video frames based on the action that is being performed. We also showed that our model performs better than baselines which do not use any attention mechanism. Soft attention models, though impressive, are still computationally expensive since they still require all the features to perform dynamic pooling. In the future, we plan to explore hard attention models as well as hybrid soft and hard attention approaches which can reduce the computational cost of our model, so that we can potentially scale to larger datasets like the Sports-1M dataset. These models can also be extended to the multi-resolution setting, in which the attention mechanism could also choose to focus on the earlier convolutional layers in order to attend to the lower-level features in the video frames.\nAcknowledgments: This work was supported by IARPA and Raytheon BBN Contract No. D11PC20071. We would like to thank Nitish Srivastava for valuable discussions and Yukun Zhu for his assistance with the CNN packages.\n2More examples of our model\u2019s attention are available in Appendix A and at http://www.cs.toronto.edu/\u02dcshikhar/projects/action-recognition-attention."}, {"heading": "A ADDITIONAL EXAMPLES", "text": "We present some more correctly classified examples from UCF-11, HMDB-51 and Hollywood2 in Fig. 10 and incorrectly classified examples in Fig. 11."}], "references": [{"title": "Learning wake-sleep recurrent attention models", "author": ["J. Ba", "R. Grosse", "R. Salakhutdinov", "B. Frey"], "venue": "In NIPS,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "ICLR,", "citeRegEx": "Ba et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "F.-F"], "venue": "In CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Modeling video evolution for action recognition", "author": ["B. Fernando", "E. Gavves", "J. Oramas", "A. Ghodrati", "T. Tuytelaars"], "venue": "In CVPR,", "citeRegEx": "Fernando et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fernando et al\\.", "year": 2015}, {"title": "Hybrid speech recognition with deep bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "Mohamed", "A.-r"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding,", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "What do 15,000 object categories tell us about classifying and localizing actions", "author": ["M. Jain", "J.C. v. Gemert", "C.G.M. Snoek"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2015}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "Li", "F.-F"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "ICLR,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Beyond gaussian pyramid: Multi-skip feature stacking for action recognition", "author": ["Lan", "Z.-Z", "M. Lin", "X. Li", "A.G. Hauptmann", "B. Raj"], "venue": "CoRR, abs/1411.6660,", "citeRegEx": "Lan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lan et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["Ng", "J.Y.-H", "M.J. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici"], "venue": "In CVPR,", "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "Action recognition with stacked fisher vectors", "author": ["X. Peng", "C. Zou", "Y. Qiao", "Q. Peng"], "venue": "In ECCV,", "citeRegEx": "Peng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2014}, {"title": "Object detection networks on convolutional feature", "author": ["S. Ren", "K. He", "R.B. Girshick", "X. Zhang", "J. Sun"], "venue": "maps. CoRR,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "The dynamic representation of scenes", "author": ["R.A. Rensink"], "venue": "Visual Cognition,", "citeRegEx": "Rensink,? \\Q2000\\E", "shortCiteRegEx": "Rensink", "year": 2000}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In NIPS", "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Unsupervised learning of video representations using LSTMs", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "DL-SFA: deeply-learned slow feature analysis for action recognition", "author": ["L. Sun", "K. Jia", "Chan", "T.-H", "Y. Fang", "G. Wang", "S. Yan"], "venue": "In CVPR,", "citeRegEx": "Sun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V.V. Le"], "venue": "In NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R.J. Mooney", "K. Saenko"], "venue": "CoRR, abs/1412.4729,", "citeRegEx": "Venugopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "Deep image: Scaling up image recognition", "author": ["R. Wu", "S. Yan", "Y. Shan", "Q. Dang", "G. Sun"], "venue": "CoRR, abs/1501.02876,", "citeRegEx": "Wu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "CoRR, abs/1502.08029,", "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "CoRR, abs/1409.2329,", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Exploiting image-trained CNN architectures for unconstrained video classification", "author": ["S. Zha", "F. Luisier", "W. Andrews", "N. Srivastava", "R. Salakhutdinov"], "venue": "CoRR, abs/1503.04144,", "citeRegEx": "Zha et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zha et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 17, "context": "1 INTRODUCTION It has been noted in visual cognition literature that humans do not focus their attention on an entire scene at once (Rensink, 2000).", "startOffset": 132, "endOffset": 147}, {"referenceID": 27, "context": "With the recent surge of interest in deep neural networks, attention based models have been shown to achieve promising results on several challenging tasks, including caption generation (Xu et al., 2015), machine translation (Bahdanau et al.", "startOffset": 186, "endOffset": 203}, {"referenceID": 2, "context": ", 2015), machine translation (Bahdanau et al., 2015), game-playing and tracking (Mnih et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 12, "context": ", 2015), game-playing and tracking (Mnih et al., 2014), as well as image recognition (e.", "startOffset": 35, "endOffset": 54}, {"referenceID": 25, "context": "Soft attention models are deterministic and can be trained using backpropagation, whereas hard attention models are stochastic and can be trained by the REINFORCE algorithm (Williams, 1992; Mnih et al., 2014), or by maximizing a variational lower bound or using importance sampling (Ba et al.", "startOffset": 173, "endOffset": 208}, {"referenceID": 12, "context": "Soft attention models are deterministic and can be trained using backpropagation, whereas hard attention models are stochastic and can be trained by the REINFORCE algorithm (Williams, 1992; Mnih et al., 2014), or by maximizing a variational lower bound or using importance sampling (Ba et al.", "startOffset": 173, "endOffset": 208}, {"referenceID": 30, "context": "We describe how our model dynamically pools convolutional features and show that using these features for action recognition gives better results compared to average or max pooling which is used by many of the existing models (Zha et al., 2015).", "startOffset": 226, "endOffset": 244}, {"referenceID": 16, "context": "2 RELATED WORK Convolutional Neural Networks (CNNs) have been highly successful in image classification and object recognition tasks (Ren et al., 2015; Wu et al., 2015).", "startOffset": 133, "endOffset": 168}, {"referenceID": 26, "context": "2 RELATED WORK Convolutional Neural Networks (CNNs) have been highly successful in image classification and object recognition tasks (Ren et al., 2015; Wu et al., 2015).", "startOffset": 133, "endOffset": 168}, {"referenceID": 6, "context": "LSTMs have been recently shown to perform well in the domain of speech recognition (Graves et al., 2013), machine translation (Sutskever et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 22, "context": ", 2013), machine translation (Sutskever et al., 2014), image description (Xu et al.", "startOffset": 29, "endOffset": 53}, {"referenceID": 27, "context": ", 2014), image description (Xu et al., 2015; Vinyals et al., 2015) and video description (Yao et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 24, "context": ", 2014), image description (Xu et al., 2015; Vinyals et al., 2015) and video description (Yao et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 28, "context": ", 2015) and video description (Yao et al., 2015; Venugopalan et al., 2014).", "startOffset": 30, "endOffset": 74}, {"referenceID": 23, "context": ", 2015) and video description (Yao et al., 2015; Venugopalan et al., 2014).", "startOffset": 30, "endOffset": 74}, {"referenceID": 20, "context": "They have also started picking up momentum in action recognition (Srivastava et al., 2015; Ng et al., 2015).", "startOffset": 65, "endOffset": 107}, {"referenceID": 14, "context": "They have also started picking up momentum in action recognition (Srivastava et al., 2015; Ng et al., 2015).", "startOffset": 65, "endOffset": 107}, {"referenceID": 4, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015).", "startOffset": 167, "endOffset": 206}, {"referenceID": 14, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015).", "startOffset": 167, "endOffset": 206}, {"referenceID": 20, "context": "LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework.", "startOffset": 98, "endOffset": 123}, {"referenceID": 13, "context": "Instead of weighting locations using a softmax layer which we do, they apply affine transformations to multiple layers of their CNN to attend to the relevant part and get state-ofthe-art results on the Street View House Numbers dataset (Netzer et al., 2011).", "startOffset": 236, "endOffset": 257}, {"referenceID": 4, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015). LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework. More recently, Yao et al. (2015) have proposed to use 3-D CNN features and an LSTM decoder in an encoder-decoder framework to generate video descriptions.", "startOffset": 168, "endOffset": 412}, {"referenceID": 4, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015). LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework. More recently, Yao et al. (2015) have proposed to use 3-D CNN features and an LSTM decoder in an encoder-decoder framework to generate video descriptions. Their model incorporates attention on a video level by defining a probability distribution over frames used to generate individual words. They, however, do not employ an attention mechanism on a frame level (i.e. within a single frame). In general, it is rather difficult to interpret internal representations learned by deep neural networks. Attention models add a dimension of interpretability by capturing where the model is focusing its attention when performing a particular task. Karpathy et al. (2014) used a multi-resolution CNN architecture to perform action recognition in videos.", "startOffset": 168, "endOffset": 1043}, {"referenceID": 4, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015). LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework. More recently, Yao et al. (2015) have proposed to use 3-D CNN features and an LSTM decoder in an encoder-decoder framework to generate video descriptions. Their model incorporates attention on a video level by defining a probability distribution over frames used to generate individual words. They, however, do not employ an attention mechanism on a frame level (i.e. within a single frame). In general, it is rather difficult to interpret internal representations learned by deep neural networks. Attention models add a dimension of interpretability by capturing where the model is focusing its attention when performing a particular task. Karpathy et al. (2014) used a multi-resolution CNN architecture to perform action recognition in videos. They mention the concept of fovea but they fix attention to the center of the frame. A recent work of Xu et al. (2015) used both soft attention and hard attention mechanisms to generate image descriptions.", "startOffset": 168, "endOffset": 1244}, {"referenceID": 4, "context": "Most of the existing approaches also tend to have CNNs underlying the LSTMs and classify sequences directly or do temporal pooling of features prior to classification (Donahue et al., 2015; Ng et al., 2015). LSTMs have also been used to learn an effective representation of videos in unsupervised settings (Srivastava et al., 2015) by using them in an encoder-decoder framework. More recently, Yao et al. (2015) have proposed to use 3-D CNN features and an LSTM decoder in an encoder-decoder framework to generate video descriptions. Their model incorporates attention on a video level by defining a probability distribution over frames used to generate individual words. They, however, do not employ an attention mechanism on a frame level (i.e. within a single frame). In general, it is rather difficult to interpret internal representations learned by deep neural networks. Attention models add a dimension of interpretability by capturing where the model is focusing its attention when performing a particular task. Karpathy et al. (2014) used a multi-resolution CNN architecture to perform action recognition in videos. They mention the concept of fovea but they fix attention to the center of the frame. A recent work of Xu et al. (2015) used both soft attention and hard attention mechanisms to generate image descriptions. Their model actually looks at the respective objects when generating their description. More recently, Jaderberg et al. (2015) have proposed a soft-attention mechanism called the Spatial Transformer module which they add between the layers of CNNs.", "startOffset": 168, "endOffset": 1458}, {"referenceID": 3, "context": ", 2015) trained on the ImageNet dataset (Deng et al., 2009).", "startOffset": 40, "endOffset": 59}, {"referenceID": 28, "context": "2 THE LSTM AND THE ATTENTION MECHANISM We use the LSTM implementation discussed in Zaremba et al. (2014) and Xu et al.", "startOffset": 83, "endOffset": 105}, {"referenceID": 27, "context": "(2014) and Xu et al. (2015): \uf8ec\uf8ed it ft ot gt \uf8f7\uf8f8 = \uf8ec\uf8ed \u03c3\u03c3\u03c3 tanh \uf8f7\uf8f8M (ht\u22121, xt ) , (1)", "startOffset": 11, "endOffset": 28}, {"referenceID": 2, "context": "After calculating these probabilities, the soft attention mechanism (Bahdanau et al., 2015) computes the expected value of the input at the next time-step xt by taking expectation over the feature slices at different regions (see Fig.", "startOffset": 68, "endOffset": 91}, {"referenceID": 27, "context": "We use cross-entropy loss coupled with the doubly stochastic penalty introduced in Xu et al. (2015). We impose an additional constraint over the location softmax, so that \u2211T t=1 lt,i \u2248 1.", "startOffset": 83, "endOffset": 100}, {"referenceID": 20, "context": "9 Composite LSTM Model (Srivastava et al., 2015) 44.", "startOffset": 23, "endOffset": 48}, {"referenceID": 21, "context": "0 DL-SFA (Sun et al., 2014) 48.", "startOffset": 9, "endOffset": 27}, {"referenceID": 5, "context": "4 VideoDarwin (Fernando et al., 2015) 63.", "startOffset": 14, "endOffset": 37}, {"referenceID": 11, "context": "7 Multi-skIp Feature Stacking (Lan et al., 2014) 65.", "startOffset": 30, "endOffset": 48}, {"referenceID": 15, "context": "0 Traditional+Stacked Fisher Vectors (Peng et al., 2014) 66.", "startOffset": 37, "endOffset": 56}, {"referenceID": 8, "context": "8 Objects+Traditional+Stacked Fisher Vectors (Jain et al., 2015) 71.", "startOffset": 45, "endOffset": 64}], "year": 2015, "abstractText": "We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.", "creator": "LaTeX with hyperref package"}}}