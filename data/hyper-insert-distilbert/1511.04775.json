{"id": "1511.04775", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Nov-2015", "title": "Expressive Recommender Systems through Normalized Nonnegative Models", "abstract": "we introduce normalized nonnegative models ( nnm ) equations for explorative comparison data curve analysis. nnms are partial convexifications of models from probability sampling theory. importantly we here demonstrate exclusively their matching value at the example of one item recommendation. we also show that we nnm - based recommender systems satisfy three core criteria because that and all explicit recommender treatment systems should ideally continuously satisfy : their high predictive stat power, computational tractability, consistent and expressive representations of users and items. expressive user and item representations perception are some important dimensions in practice only to systematically succinctly summarize roughly the pool arrangement of textual customers vocabulary and the matching pool consisting of items. in our nnms, literal user representations but are obviously expressive instances because actually each sampled user's preference perception can be highly regarded potentially as entirely normalized mixture of preferences of other stereotypical dictionary users. thus the interpretability of item and user model representations allow like us to arrange properties properties of items ( e. for g., diverse genres of movies titles or topics of documents ) interact or distribute users ( e. g., biological personality mental traits ) hierarchically.", "histories": [["v1", "Sun, 15 Nov 2015 22:39:58 GMT  (227kb,D)", "http://arxiv.org/abs/1511.04775v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["cyril j stark"], "accepted": true, "id": "1511.04775"}, "pdf": {"name": "1511.04775.pdf", "metadata": {"source": "CRF", "title": "Expressive recommender systems through normalized nonnegative models", "authors": ["Cyril J. Stark"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Recommender systems are algorithms designed to recommend items to users. Good recommender systems address the following three partially conflicting objectives: (1) predictive power (despite very sparse and noisy data), (2) computational tractability (despite quickly growing numbers of users and items), (3) interpretability (to allow for feedback, market analysis and visual representations). Here, to address those difficulties, we are going to adopt the system-state-measurement paradigm in the form of a class of models which we call normalized nonnegative models (NNM).\nAdopting the system-state-measurement paradigm amounts to making a clear distinction between the \u2018state of a system\u2019 and the \u2018measurement device\u2019 used to probe that system. The success of this paradigm in science and engineering motivates its application in item recommendation and beyond. In the study of recommendation the system is that abstract part of our thinking that determines whether we like or dislike an item. The state of that system varies from person to person; it forms the description of the individual preferences. The measurements that we perform on the system are questions of the form \u201cDo you like the movie Jurassic Park?\u201d. Each measurement probes our taste (e.g., movie taste) and measuring sufficiently many diverse questions allows to get an idea of the preferences/opinion of a person. In the natural sciences and engineering the system is oftentimes described in terms a sample space, the state of the system is probability distribution on that sample space and a measurement is a random variable. Hence, ideally, to adopt that picture, we need to compute the following building blocks: (1) an effective sample space, (2) a probability distribution for each user to describe that user\u2019s taste, (3) a random variable for each item to describe questions like \u201cHow do you rate the movie Ex Machina?\u201d. To arrive at NNMs we simply convexify the third of these building blocks, i.e., the space of random variables. This convex relaxation will allow us to compute NNMs through alternating convex optimization. In this manner, approximate inference of NNMs becomes computationally tractable.\nThe main strength of NNMs are highly interpretable user and item representations. The way we represent users allows us to regard users as normalized mixtures of a small number of user stereotypes. We provide\n\u2217Massachusetts Institute of Technology, 77 Massachusetts Avenue, 6-304, Cambridge MA 02139-4307, USA\nar X\niv :1\n51 1.\n04 77\n5v 1\n[ cs\n.L G\n] 1\n5 N\nov 2\n01 5\nstrategies to characterize those stereotypical users in words so that those stereotypes can be understood intuitively by people who are unfamiliar with data analysis. Hence, NNMs allow everybody to interpret users\u2019 behaviors as mixtures of well-characterized stereotypical behaviors. On the other hand, the way we represent items allows us to infer hierarchical orderings of item categories like movie genres or topics of documents. This is how we address the criterion interpretability. Of course, topic models like those based on latent Dirichlet allocation (LDA) have also been used to derive said expressive descriptions of users and items. We explain this in much more detail when discussing related work towards the end of this paper.\nFinally, we evaluate the last remaining criterion predictive power in numerical experiments. We show that in mean-average-error, NNMs outperform methods like SVD++ [1] on MovieLens datasets. This indicates that the high level of interpretability of NNMs comes not at the price of sacrificing predictive power.\nThroughout the paper we introduce NNMs through their application in item recommendation. But we hope that our presentation will be clear enough to convince the reader that the scope of NNMs is not limited to recommendation\u2014in the very same sense that the scope of probability theory is not limited to one particular branch of science."}, {"heading": "2 Notation", "text": "For n \u2208 N, we set [n] = {1, ..., n}. Throughout, u \u2208 [U ] labels users, i \u2208 [I] labels items and z \u2208 [Z] denote possible ratings (e.g., z \u2208 [5] in case of 5-star ratings). By R \u2208 [Z]U\u00d7I we denote the complete rating matrix, i.e., Rui \u2208 [Z] is the rating that user u provides for item i. In practice, we only know a subset of the entries of R. We use \u0393 \u2286 [U ] \u00d7 [I] to mark the known entries, i.e., (u, i) \u2208 \u0393 if Rui is known a priori. We use \u2206 = {~p \u2208 RD+ |\u2016~p\u20161 = 1} to denote the probability simplex. A finite probability space is described in terms of a sample space \u2126 = {\u03c91, ..., \u03c9D}, probability distributions ~p \u2208 \u2206 and random variables E\u0302 with some alphabet [Z]. Recall that random variables are functions E\u0302 : \u2126\u2192 [Z]."}, {"heading": "3 Probability theory recap", "text": "According to Kolmogorov, a random experiment with finite sample space is described by the following triple:\n\u2022 A sample space \u2126 = {\u03c91, ..., \u03c9D}. The elements \u03c9j denote elementary events. \u2022 A probability distribution ~p \u2208 RD+ with \u2211 j(~p)j = 1, i.e., ~p is an element of the probability simplex \u2206.\n\u2022 A random variable E\u0302, i.e., a function E\u0302 : \u2126\u2192 {1, ..., Z} for some alphabet size Z \u2208 N.\nWe denote by P[E\u0302 = z] the probability of the event {\u03c9 \u2208 \u2126|E\u0302(\u03c9) = z}. Therefore,\nP[E\u0302 = z] = P[E\u0302\u22121(z)] = \u2211\n\u03c9\u2208E\u0302\u22121(z)\np\u03c9 (1)\nwhere E\u0302\u22121(z) \u2286 \u2126 denotes the pre-image of z under the map E\u0302. The expression (1) can be rewritten using indicator vectors. For that purpose we define ~Ez by(\n~Ez ) j = { 1, if \u03c9j \u2208 E\u0302\u22121(z) 0, otherwise.\n(2)\nIt follows that P[E\u0302 = z] = ~ETz ~p ,i.e., probabilities for measuring specific outcomes of random variables can be expressed in terms of inner products between two D-dimensional vectors. By (2), \u2211 z ~Ez = (1, ..., 1) T .\nExamples. For an unbiased coin, ~p = (1/2, 1/2)T , ~Eheads = (1, 0) T and ~Etails = (0, 1) T . Consequently, P[E\u0302 = heads] = (1/2, 1/2)(1, 0)T = 1/2. For a biased 4-sided coin, we may have ~p = (1/4, 1/4, 1/8, 3/8)T and ~Ez such that ( ~Ez)j := \u03b4zj . It follows for example that P[E\u0302 = 1 or 4] = ~pT ( ~E1 + ~E4) = 5/8."}, {"heading": "4 Normalized nonnegative models", "text": "We adopt the system-state-measurement paradigm. The system we are interested in is the part of our mind that determines the outcome to the question \u201cDo you like item i?\u201d (i \u2208 [I]). For each user u \u2208 [U ] this system is in some state described by a distribution ~pu \u2208 \u2206 on some unknown sample space \u2126 = {\u03c91, ..., \u03c9D} representing the system. Each question \u201cDo you like item i?\u201d is modeled in terms of a random variable E\u0302i with alphabet [Z] (Z = 5 for 5-star ratings). We denote by Pu[E\u0302i = z] the probability for user u to rate item i with z \u2208 [Z]. Thus,\nPu[E\u0302i = z] = Pu[E\u0302\u22121i (z)] = ~E T iz~pu (3)\nwhere ~pu models the state of user u and where ~Eiz \u2208 {0, 1}D is defined by\n( ~Eiz ) j = { 1, if \u03c9j \u2208 E\u0302\u22121i (z) 0, otherwise.\n(4)\nBy (4), \u2211 z ~Eiz = (1, ..., 1)\nT . We denote by M0 the set of all valid descriptions ( ~E1, ..., ~EZ) of a random variable E\u0302, i.e.,\nM0 = { ( ~E1, ..., ~EZ) \u2208 {0, 1}D\u00d7Z \u2223\u2223\u2223\u2211 z ~Eiz = (1, ..., 1) T } .\nAllowing for stochastic mixtures of elements of M0 we arrive at the convex relaxation M = { ( ~E1, ..., ~EZ) \u2208 RD\u00d7Z+ \u2223\u2223\u2223\u2211 z ~Eiz = (1, ..., 1) T }\nof M0. In the remainder, the tuple of vectors ( (~pu)u\u2208[U ], ( ~Eiz)i\u2208[I],z\u2208[Z] ) denotes a normalized nonnegative model (NNM) if ~pu \u2208 \u2206 for all users u \u2208 [U ] and if ( ~Eiz)z\u2208[Z] \u2208 M for all items i \u2208 [I]. A NNM captures observations well if R \u2248 ( argmaxz{ ~ETiz~pu}z\u2208[Z] ) u\u2208[U ],i\u2208[I] ."}, {"heading": "4.1 Non-categorical", "text": "In the previous description of the random variables E\u0302i we did not make any assumptions on the nature of the outcomes z \u2208 [Z]. Hence, the outcomes z \u2208 [Z] are categorical, i.e., the outcomes are not ordered (e.g., (z = 3) < (z = 4)) and the numerical values z \u2208 [Z] carry no meaning. This is a feature of NNMs as it makes them applicable in a very wide range of settings.\nIn item recommendation, systems that allow for categorical information are particularly convenient to make use of side information such as the gender of users. But even if we only focus on the user-item matrix, systems that allow for categorical information may generally have an advantage over other systems because 5-star ratings do not come with a scale. For example, we cannot claim that we prefer a 2-star-item over a 1-star-item to the same extent as we prefer a 3-star-item over a 2-star-item.\nOn the other hand, star-ratings are clearly ordered as, for example, a 4-star-rating is better than a 3- star-rating. Consequently, we lose information if we treat ratings in a purely categorical manner. Therefore, to avoid the little-data-problem we may want to interpret a rating z \u2208 [Z] of an item i by user u as an approximation of P[u likes i], i.e.,\nz/Z \u2248 P[u likes i]. (5)\nTo model this interpretation of users\u2019 ratings, we regard the random variables E\u0302i (i \u2208 [I]) as binary random variables whose outcomes are interpreted as \u2018I like item i\u2019 and \u2018I dislike item i\u2019, respectively.\nNNMs allow for the modeling of categorical variables because we assigned individual vectors ( ~Eiz)z\u2208[Z] to each of the outcomes z \u2208 [Z]. This can be done in general matrix factorization (e.g., SVD++ [1]), and the potential possibilities motivate a thorough analysis of the modeling of categorical variables in terms of general matrix factorization."}, {"heading": "5 Interpretability", "text": "One of the reasons for the popularity of models from probability theory is their interpretability. To make this more precise, we imagine flipping a coin. The natural probabilistic description of a single coin flip is in terms of a sample space \u2126 = {\u03c9heads, \u03c9tails} where \u03c9heads is the event heads and \u03c9tails is the event tails. We denote by ~\u03b4heads and ~\u03b4tails the deterministic distributions located at \u03c9heads and \u03c9tails, respectively. (For\nexample, ( ~\u03b4heads ) \u03c9 = 1 if \u03c9 = \u03c9heads and ( ~\u03b4heads ) \u03c9 = 0 otherwise.) Hence, we can easily and clearly describe the states ~\u03b4heads and ~\u03b4tails in terms of words: ~\u03b4heads is the state that always returns heads, and ~\u03b4tails is the state that always returns tails. It is through those concise descriptions that we get an intuitive understanding of the states ~\u03b4heads and ~\u03b4tails.\nEvery possible state ~p = (pheads, ptails) of the coin is a probabilistic mixture of ~\u03b4heads and ~\u03b4tails. Since we have an intuitive understanding of what mixtures are, we can thus lift our intuitive understanding of ~\u03b4heads and ~\u03b4tails to arrive at an intuitive understanding of general states ~p = (pheads, ptails). We think that this is one of the main strengths of probabilistic models and we think that this is one of the main reasons why those models are so appealing not only to scientists and engineers but also to people with less mathematical training.\nIn NNMs the states of users are distributions ~pu = \u2211D \u03c9=1 pu,\u03c9\n~\u03b4\u03c9 where (~\u03b4\u03c9)k = 1 if \u03c9 = k and (~\u03b4\u03c9)k = 0 otherwise. Hence, we could get a good understanding of the users\u2019 states if we found ways to describe the elementary preferences ~\u03b4\u03c9 in an intuitive manner. We call those elementary preferences ~\u03b4\u03c9 stereotypes because each user\u2019s preference is a mixture of those stereotypes. We next describe approaches to acquire said intuitive description of stereotypes."}, {"heading": "5.1 Understanding stereotypes through tags", "text": "Oftentimes, we not only have access to users\u2019 ratings of items but we also have access to side information about items in terms of tags. For example the MovieLens 1M dataset provides genre tags for movies; each movie gets assigned to (sometimes multiple) genres like Action, Adventure, Animation, etc. Note that we do have an intuitive understanding of those genres\u2014just as we have an intuitive understanding of the coin-states ~\u03b4heads and ~\u03b4tails. Thus, at the example of genre tags we explain next, how side information can be used to get an intuitive characterization of stereotypes.\nWe assume that each movie i is assigned to some genres gi1, ..., g i ni \u2208 {g1, ..., gG}. To characterize a stereotype ~\u03b4\u03c9 we want to determine how much the hypothetical user ~\u03b4\u03c9 likes movies of genre g1, movies of genre g2, etc. We make this precise in terms of the following game to characterize stereotypes ~\u03b4\u03c9. The game involves a referee and two players Alice and Bob. For some \u03c9 \u2208 \u2126, Alice\u2019s user vector is assumed to be ~\u03b4\u03c9. We proceed as follows.\n1. Fix a genre g \u2208 {g1, ..., gG}. Let i1, ..., im denote all the movies that have been tagged with g. 2. Bob is given access to Alice\u2019s vector ~\u03b4\u03c9, to g, to all genre tags ( gi1, ..., g i ni ) i\u2208[I] and to all item vectors(\n~Eiz ) i\u2208[I],z\u2208[Z] (z \u2208 [2]; z = 1 means \u2018like\u2019 and z = 2 means \u2018dislike\u2019).\n3. The referee draws uniformly at random a movie i\u2217 from {i1, ..., im}.\n4. By looking up R, the referee checks whether Alice likes or dislikes i\u2217. We denote her answer by z\u2217 \u2208 {like,dislike}.\n5. Bob guesses z\u2217. He wins the game if he guesses z\u2217 correctly. Otherwise, he loses.\nBefore we describe how to make use of this game for the characterization of stereotypical users, we describe Bob\u2019s strategy. First we note that Bob needs to estimate the probability for z\u2217 = 1 and z\u2217 = 2,\nrespectively. Conditioned on the event \u2018Referee draws i\u2019 we have that P[z\u2217 = 1 | i] = ~ETi1~p. The probability that the referee draws i is 1/m because there are in total m movies associated with g. Therefore,\nP[z\u2217 = 1] = \u2211\ni\u2208{i1,...,im}\nP[z\u2217 = 1 | i] P[i] = ~ETg ~p.\nwhere ~Eg := 1\nm \u2211 i\u2208{i1,...,im} ~Ei1.\nHence, Bob computes ~ETg ~\u03b4\u03c9 = Eg,\u03c9. If Eg,\u03c9 \u2265 1/2 he guesses z\u2217 = 1. Otherwise, he guesses z\u2217 = 2.\nHow can this game be used for the characterization of stereotypes? The number Eg,\u03c9 specifies the probability that the stereotypical user ~\u03b4\u03c9 likes a random movie from genre g. For instance, if Eg,\u03c9 \u2248 1 for g \u2208 [G] then we know that the stereotypical user ~\u03b4\u03c9 very much likes movies from genre g. We repeat above game for all g \u2208 {g1, ..., gG} and for all \u03c9 \u2208 \u2126. We arrive at numbers (Eg,\u03c9)g\u2208[G],\u03c9\u2208\u2126. For each \u03c9, the tuple (Eg,\u03c9)g\u2208[G] provides a characterization of the preferences of the stereotypical user ~\u03b4\u03c9. The characterization (Eg,\u03c9)g\u2208[G] is convenient because Eg,\u03c9 specifies the probability for ~\u03b4\u03c9 to like a movie from genre g, and because those genres g are understood intuitively."}, {"heading": "5.2 Understanding stereotypes without tags", "text": "In the previous section we proposed a method for characterizing stereotypes ~\u03b4\u03c9. That method is applicable whenever items come along with interpretable tags. What can we do if no such tags are available? Assume we only have access to users\u2019 ratings of items. In those cases we suggest to proceed by characterizing each of the stereotypical users through a list of items they like. For a stereotype ~\u03b4\u03c9, those items can be found by firstly, collecting all items with the property that ( ~EiZ ) \u03c9 \u2248 1 (e.g., vectors associated to 5 stars). Denote those highly rated movies by \u03b3\u03c91 , ..., \u03b3 \u03c9 M . Then, in a second step, we select from the set {\u03b3\u03c91 , ..., \u03b3\u03c9M} those items that are popular (i.e., known to many people). We arrive at items \u03b3\u03c9m1 , ..., \u03b3 \u03c9 mJ . To characterize the stereotype ~\u03b4\u03c9, we report the list \u03b3 \u03c9 m1 , ..., \u03b3 \u03c9 mJ ."}, {"heading": "5.3 Stereotypes in general matrix factorization models", "text": "Let ~ru denote user vectors computed in a general matrix factorization model. Computing the convex hull of the cone spanned by the set {~ru}u\u2208[U ] we could in principle determine user vectors ~ri1 , ..., ~riT with the property that for all ~ru there exist coefficients \u03bb1, ..., \u03bbT \u2265 0 such that \u2211T k=1 \u03bbk~rik = ~ru. Therefore, as in NNMs, we can still express every user vector as mixture of other user vectors. There are, however, at least two major problems with this approach. Firstly, computing the convex conic hull of the span of {~ru}u\u2208[U ] is computationally not tractable; even for small numbers of users. Secondly, we expect the number T of extremal rays R~ri1 , ...,R~riT of the convex conic hull to be very large\u2014independently of D. Therefore, users\u2019 states are difficult to interpret because they are the mixture of a very large number of stereotypes. We would like to stress that for each dimension D, NNMs are efficient as they only use the least possible number of D stereotypes. For example, if we only needed D\u22121 stereotypes then all vectors could be restricted to a sub-space of RD and a (D\u2212 1)-dimensional NNM could be used instead of the D-dimensional NNM."}, {"heading": "6 Hierarchical structure of tags", "text": "Assume the considered items are tagged. For instance, as before, if the items are movies then these tags could specify which genre each movie belongs to. We denote by {it1, ..., itmt} all items that have been tagged\nwith a tag \u03c4t from the set of all tags {\u03c4t}t\u2208[T ]. As before, we describe tags \u03c4t (t \u2208 [T ]) in terms of vectors\n~E\u03c4t := 1\nmt \u2211 i\u2208{i1,...,imt} ~EiZ , (6)\nso that ~pTu ~E\u03c4t is the probability that user u likes a randomly chosen item i \u2208 {it1, ..., itmt}. As we are going to see next, (6) enables us to order tags in a hierarchical manner.\nWe note that if the tag vectors ~E\u03c4t were binary vectors (i.e., \u2208 {0, 1}D), then we would say \u03c4t \u2286 \u03c4t\u2032 whenever the support of ~E\u03c4t is contained in the support of\n~E\u03c4t\u2032 . This is a meaningful definition of \u2018\u2286\u2019 for tags because if \u03c4t \u2286 \u03c4t\u2032 then \u2018~\u03b4\u03c9 likes \u03c4t\u2019 implies \u2018~\u03b4\u03c9 likes \u03c4t\u2032 \u2019.\nGenerally, in NNMs, tag vectors ~E\u03c4t are not binary and therefore, the definition of \u03c4t \u2286 \u03c4t\u2032 needs to make sense for non-binary vectors. To find a new definition of \u2018\u2286\u2019 we note that in the previous binary setting, \u03c4t \u2286 \u03c4t\u2032 if and only if\n~ET\u03c4t\u2032 ~E\u03c4t = \u2211 \u03c9\u2208\u2126 ( ~E\u03c4t ) \u03c9 =: \u2016 ~E\u03c4t\u20161. (7)\nCondition (7) can never be satisfied if the components of ~E\u03c4t and ~E\u03c4t\u2032 are < 1. This can be the case in NNMs. However, the operational meaning of the condition (7) is preserved under the relaxation\n~ET\u03c4t\u2032 ~E\u03c4t \u2265 (1\u2212 \u03b5)\u2016 ~E\u03c4t\u20161 (8)\nif \u03b5 > 0 is small. That is because if the relaxed condition (8) is satisfied then we still have that most of the weight of ~E\u03c4t is contained in the approximate support of ~E\u03c4t\u2032 .\nTherefore, we say \u03c4t \u2286\u03b5 \u03c4t\u2032 if condition (8) is satisfied, and we say \u03c4t =\u03b5 \u03c4t\u2032 if both \u03c4t \u2286\u03b5 \u03c4t\u2032 and \u03c4t\u2032 \u2287\u03b5 \u03c4t. The collection of all relations \u2018\u2286\u03b5\u2019 between tags can be represented graphically in terms of a graph. For that purpose we interpret the set of tags {\u03c4t}t\u2208[T ] as the vertex set V of a graph G = (V,E). G contains directed edges defined through the rule\n(\u03c4t \u2192 \u03c4t\u2032) if \u03c4t\u2032 \u2286\u03b5 \u03c4t.\nFor every choice of \u03b5 \u2208 [0, 1], the graph G induces an approximate hierarchical ordering of tags; see figure 2."}, {"heading": "7 Computation of normalized nonnegative models", "text": "Presumably, the simplest approach for computing a NNM proceeds via alternating constrained optimization to solve\nmin ~pu\u2208\u2206,(~Eiz)z\u2208[Z]\u2208M \u2211 (u,i)\u2208\u0393 ( ~ETiz~pu \u2212Rui/Z )2 , (9)\ni.e., the algorithm switches back and forth between optimizing (~pu)u\u2208[U ] (keeping ( ~Eiz)i\u2208[I],z\u2208[Z] fixed) and optimizing ( ~Eiz)i\u2208[I],z\u2208[Z] (keeping (~pu)u\u2208[U ] fixed). Each of these tasks can be computed efficiently and in parallel. Moreover, this approach is guaranteed to converge to a local minimum.\nIn the context of recommendation, training data is typically subject to a large selection bias: a majority of the ratings are high ratings. This significantly impacts the model we fit to the data. For example, if all the known entries (marked by \u0393) of the rating matrix R were equal to Z then a 1-dimensional model (user vectors = 1, item vectors = Z) would lead to zero training error. It is commonly believed that (u, i) \u2208 \u0393c implies typically that u does not like i. Thus, during the first 2 iterations, we set the missing entries of R equal to zero. We believe that this preprocessing leads to an initialization for alternating constrained optimization that captures more accurately the ground truth. We arrive at Algorithm 1 to fit NNMs to measured data.\nAlgorithm 1 Alternating constrained least squares for NNM\n1: Fix D (e.g., by cross validation). 2: For all u, initialize ~pu = ~ei where i \u2208 [D] is chosen uniformly at random and where (~ei)j = \u03b4ij . 3: For all items i, solve the (linearly constrained) nonnegative least squares problem\nmin(~Eiz)z\u2208[Z]\u2208M \u2211 u:(u,i)\u2208\u0393 ( ~ETiz~pu \u2212Rui/Z )2 .\n4: For all users u, solve the (linearly constrained) nonnegative least squares problem min~pu\u2208\u2206 \u2211 i:(u,i)\u2208\u0393 ( ~ETiz~pu \u2212Rui/Z )2 . 5: Repeat steps 3 and 4 until a stopping criteria is satisfied; e.g., until a maximum number of iterations is reached. For the first 2 iterations we pretend we knew all of R by setting unknown entries equal to zero."}, {"heading": "8 Computational tractability", "text": "All of the steps in Algorithm 1 can be parallelized straightforwardly. The main bottleneck are large number of summands in the objective functions of user and item updates. With standard methods this becomes a potential issue during the first two iterations (i.e., preprocessing). There are at least two loopholes. The easiest solution is in terms of sampling a fixed number of unknown entries and replacing only those with zeros. Here, the number of sampled entries should be comparable to the number of known entries so that we can compensate for the selection bias towards positive ratings.\nAlternatively, we can run Algorithm 1 for a subset of all users and items. We denote these users and items by {un}Nn=1 and {im}Mm=1, respectively. If N and M equal a couple of thousands then we can easily run Algorithm 1; see section \u2018Experiments\u2019. How can we compute representations of the remaining users and items? To determine user vectors for u /\u2208 {un}Nn=1 we simply solve step 4 of Algorithm 1 to determine ~pu. We can proceed analogously to determine representations for i /\u2208 {im}Mm=1. These single small optimization problems can be solved quickly and therefore, online. The resulting scheme is a two-phase procedure where we compute the representations of the \u2018anchor users\u2019 {un}Nn=1 and \u2018anchor items\u2019 {im}Mm=1 offline, and where we compute or update all other user and item representations online. Of course, this requires selecting the sets {un}Nn=1 and {im}Mm=1 so that the anchor items are popular (\u2192 u knows some of them), and so that the anchor users rate lots of items (\u2192 soon after release, i is rated by a couple of those users)."}, {"heading": "9 Experiments", "text": "We evaluate predictive power and interpretability of NNMs at the example of the omnipresent MovieLens (ML) 100K and 1M datasets.1 Due to lack of space we moved the details about the configurations of all algorithms and results for the MovieLens 100K dataset to the appendix. To use the data economically, we employed interpretation (5) of the rating matrix R. We computed NNMs in Matlab using cvx [2] calling SDPT3 [3].\nInterpretability. We computed figure 1 to illustrate the interpretability of NNMs through user stereotypes. We computed figure 2 to evaluate interpretability of NNMs through emergent hierarchies of tags.\nFigure 1 (left) corresponds to a 2-dimensional NNM. The two towers characterize the two user stereotypes. The second stereotype does not much care about the particular genre; she rates many movies highly and is open to everything. The first stereotype is generally more skeptical but rates movies from the genres \u2018Documentaries\u2019 and \u2018Film-Noir\u2019 highly; she dislikes the genre \u2018Horror\u2019. Interestingly, when increasing the dimension from 2 to 3, we leave those two stereotypes approximately unchanged\u2014we simply add a new stereotype; see figure 1 (right). The newly emergent stereotype has preferences for the genres \u2018Action\u2019, \u2018Adventure\u2019, \u2018Children\u2019s\u2019, \u2018Fantasy\u2019 and dislikes both Documentaries and movies from the Horror genre. All probabilities are large due to the selection bias towards high ratings. Filling in missing entries with low ratings affects the size of those probabilities but leaves the structure of the towers approximately unchanged.\n1http://grouplens.org/datasets/movielens/\nFigure 2 serves as an example for how expressive computed tag-hierarchies are. The hierarchy from figure 2 visualizes the pattern of \u2018\u2286\u03b5\u2019-relations between movie genre for the value \u03b5 = 1/3. To decrease the complexity of figure 2, we excluded the genres \u2018Film-Noir\u2019 and \u2018War\u2019 from the figure. Movies from these genres are rated highly by a majority of users and thus, all genres are connected to these genres.\nEvaluation of predictive power. We evaluate mean-squared-error (MAE) and root-mean-squared-error (RMSE) through 5-fold cross validation with 0.8-to-0.2 data splitting. All results were computed using 16 iterations. Figure 3 (left) shows how MAE depends on D, and Figure 3 (right) shows that the proposed algorithm converges smoothly. In appendix A we argue why MAE is less sensitive to outliers and therefore, we think that MAE should be preferred over RMSE. Table 1 compares NNMs with popular recommender systems (see appendix B). We notice that Algorithm 1 outperforms SVD++ [1] in MAE. Note that neither SVD++ nor NNMs were trained to minimize MAE. We evaluated the previously known methods by using the LibRec library [4]."}, {"heading": "10 Related work", "text": "In recommendation we are interested in the co-occurence of pairs (u, i) of users u and items i. Depending on the application, (u, i) may be interpreted as \u2018u likes item i\u2019, \u2018u attends event i\u2019, etc. In aspect models, pLSA [8, 9] (and similarly in their extension LDA [10]) we model (u, i) as random variable with distribution\nP[u, i] = K\u2211 k=1 P[u|k] P[i|k]. (10)\nHence, P[u, i] is expressed as a inner product of two vectors: (P[u|k])k\u2208K and (P[i|k])k\u2208K . This is reminiscent of (3) where we expressed the probability of the event \u2018u rates i with z stars\u2019 in terms of the inner product between ~Eiz and ~pu. Therefore, one half of the inner product (3) agrees with the identity (10) as both (P[u|k])k\u2208K and ~pu are probability distributions. However, aspect models and NNMs disagree on the other half (i.e., ~Eiz) because ~Eiz is constrained through the existence of ~Ei1, ..., ~Eiz\u22121, ~Eiz+1, .., ~EiZ such that ( ~Ei1, ..., ~EiZ) \u2208 M. More importantly however, the difference between aspect models and NNMs lies in the different interpretations of (P[u|k])k\u2208K and (P[i|k])k\u2208K on the one hand and ~pu and ~Eiz on the other hand.\nSimilarly, more general NMF-based models [11] agree with NNM-based models in that they make prediction in terms of inner products of nonnegative vectors but they differ from NNM-based models through different interpretations and regularizations of those nonnegative vectors.\nThese new interpretations of ~pu and ~Eiz allow us to deal with situations where z \u2208 [Z] is a categorical random variable and we can extract hierarchical structures from NNMs in a straightforward manner. Aspect models can deal with both of these tasks too. However, we think that dealing with these tasks in terms of aspect models is less natural then dealing with these tasks with NNMs. For instance, to model multiple outcomes like (z \u2208 [5]) we need to first decide on a particular graphical model (see section 2.3 in [8]). Moreover, to extract hierarchical classifications of topics, we need to imagine generative processes like the nested Chinese restaurant process [12], or we need to employ nested hierarchical Dirichlet processes [13].\nOn the other hand, probabilistic matrix factorization (PMF, [14]) leads to another interesting and related class of models where we assume that entries Rui of the rating matrix are independent Gaussian random variables with mean ~UTu ~Vi and variance \u03c3. PMF-based models also describe ratings as samples of an underlying random variable and the distribution of that random variable is parameterize symmetrically in terms of vectors ~Uu assigned to users and ~Vi assigned to items. However, for PMF we need to assume that the random variables Rui are normally distributed. In NNM-based models, we do not need to assume anything about the distribution of Rui once the underlying dimension D has been fixed by cross-validation. PMF would allow for the extraction of hierarchical orderings as discussed here but PMF does not allow for the interpretation of data through stereotypes because PMF corresponds to a specific infinite-dimensional NNM (PMF refers to continuous distributions).\nThe evaluation of NNMs in the extreme multi-label setting [15, 16, 17, 18] is still outstanding."}, {"heading": "11 Conclusion", "text": "We introduced NNMs at the example of item recommendation. We discussed in which way these models meet the criteria predictive power, computational tractability and interpretability that are ideally met by recommender systems. The main strength of NNMs is their high level of interpretability. This quality can be used to characterize users\u2019 behavior in an interpretable manner, and this quality can be used to derive hierarchical orderings of properties of items and users. Fortunately, as indicated by numerical experiments, these features of NNMs do not come at the price of sacrificing neither predictive power nor computational tractability. Hence, we believe that NNMs will prove valuable in recommendation and beyond."}, {"heading": "12 Acknowledgments", "text": "I thank Patrick Pletscher and Sharon Wulff for interesting and fruitful discussions. I acknowledge funding by the ARO grant Contract Number W911NF-12-0486, and I acknowledge funding by the Swiss National Science Foundation through a postdoctoral fellowship."}, {"heading": "A MAE vs RMSE", "text": "Assume the considered items are movies. Then, it can happen that a user u likes movie i but rates it badly because of the particular reason that one of the actors is a member of scientology. Hence, the reason for the poor rating is independent of user u\u2019s movie taste; rather it is a consequence of user u being atheist. It seems very unlikely that information of that kind is captured by the few numbers we use to describe a person\u2019s movie taste. Moreover, in practice, we would presumably not have enough data about movie i to infer that one of the actors is a member of scientology. Hence, even extremely good models for capturing movie tastes will make a few predictions that are entirely wrong\u2014nothing else should be expected. In conclusion, when measuring the quality of a model in terms of an error metric we may want to consider error metrics that are not too sensitive to a few outliers (i.e., predictions that are entirely wrong). RMSE is a scaled l2-distance and MAE is a scaled l1-distance. Hence, RMSE is much more sensitive to outliers than MAE. For that reason MAE appears to be more meaningful than RMSE. Moreover, there is an unavoidable tradeoff between enforcing low MAE and low RMSE\u2014a recommender system cannot be near-optimal in both error measures."}, {"heading": "B Configurations of algorithms", "text": "Table 2 specifies the configurations of algorithms as called by the Java LibRec library [4]."}], "references": [{"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Yehuda Koren"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "CVX: Matlab software for disciplined convex programming, version 2.1", "author": ["Michael Grant", "Stephen Boyd"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "SDPT3 a MATLAB software package for semidefinite programming, version 1.3", "author": ["Kim-Chuan Toh", "Michael J Todd", "Reha H T\u00fct\u00fcnc\u00fc"], "venue": "Optimization methods and software,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Grouplens: an open architecture for collaborative filtering of netnews", "author": ["Paul Resnick", "Neophytos Iacovou", "Mitesh Suchak", "Peter Bergstrom", "John Riedl"], "venue": "In Proceedings of the 1994 ACM conference on Computer supported cooperative work,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["Steffen Rendle", "Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Daniel D Lee", "H Sebastian Seung"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Latent class models for collaborative filtering", "author": ["Thomas Hofmann", "Jan Puzicha"], "venue": "In IJCAI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Recommender systems with social regularization", "author": ["Hao Ma", "Dengyong Zhou", "Chao Liu", "Michael R Lyu", "Irwin King"], "venue": "In Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["David M Blei", "Thomas L Griffiths", "Michael I Jordan"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Nested hierarchical dirichlet processes", "author": ["John Paisley", "Chingyue Wang", "David M Blei", "Michael Jordan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Probabilistic matrix factorization", "author": ["Andriy Mnih", "Ruslan Salakhutdinov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["Rahul Agrawal", "Archit Gupta", "Yashoteja Prabhu", "Manik Varma"], "venue": "In Proceedings of the 22nd international conference on World Wide Web,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Multi-label prediction via compressed sensing", "author": ["Daniel Hsu", "Sham Kakade", "John Langford", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Extracting shared subspace for multi-label classification", "author": ["Shuiwang Ji", "Lei Tang", "Shipeng Yu", "Jieping Ye"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "We show that in mean-average-error, NNMs outperform methods like SVD++ [1] on MovieLens datasets.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": ", z \u2208 [5] in case of 5-star ratings).", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": ", SVD++ [1]), and the potential possibilities motivate a thorough analysis of the modeling of categorical variables in terms of general matrix factorization.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": ", g i ni ) i\u2208[I] and to all item vectors ( ~ Eiz ) i\u2208[I],z\u2208[Z] (z \u2208 [2]; z = 1 means \u2018like\u2019 and z = 2 means \u2018dislike\u2019).", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "For every choice of \u03b5 \u2208 [0, 1], the graph G induces an approximate hierarchical ordering of tags; see figure 2.", "startOffset": 24, "endOffset": 30}, {"referenceID": 1, "context": "We computed NNMs in Matlab using cvx [2] calling SDPT3 [3].", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "We computed NNMs in Matlab using cvx [2] calling SDPT3 [3].", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "We notice that Algorithm 1 outperforms SVD++ [1] in MAE.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "MAE (100K) RMSE (100K) MAE (1M) RMSE (1M) UserKNN [5] 0.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "905 ItemKNN [6] 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "876 NMF [7] 0.", "startOffset": 8, "endOffset": 11}, {"referenceID": 0, "context": "920 SVD++ [1] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "In aspect models, pLSA [8, 9] (and similarly in their extension LDA [10]) we model (u, i) as random variable with distribution", "startOffset": 23, "endOffset": 29}, {"referenceID": 7, "context": "In aspect models, pLSA [8, 9] (and similarly in their extension LDA [10]) we model (u, i) as random variable with distribution", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "In aspect models, pLSA [8, 9] (and similarly in their extension LDA [10]) we model (u, i) as random variable with distribution", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "Similarly, more general NMF-based models [11] agree with NNM-based models in that they make prediction in terms of inner products of nonnegative vectors but they differ from NNM-based models through different interpretations and regularizations of those nonnegative vectors.", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "For instance, to model multiple outcomes like (z \u2208 [5]) we need to first decide on a particular graphical model (see section 2.", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "3 in [8]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "Moreover, to extract hierarchical classifications of topics, we need to imagine generative processes like the nested Chinese restaurant process [12], or we need to employ nested hierarchical Dirichlet processes [13].", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "Moreover, to extract hierarchical classifications of topics, we need to imagine generative processes like the nested Chinese restaurant process [12], or we need to employ nested hierarchical Dirichlet processes [13].", "startOffset": 211, "endOffset": 215}, {"referenceID": 12, "context": "On the other hand, probabilistic matrix factorization (PMF, [14]) leads to another interesting and related class of models where we assume that entries Rui of the rating matrix are independent Gaussian random variables with mean ~ U u ~ Vi and variance \u03c3.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "The evaluation of NNMs in the extreme multi-label setting [15, 16, 17, 18] is still outstanding.", "startOffset": 58, "endOffset": 74}, {"referenceID": 14, "context": "The evaluation of NNMs in the extreme multi-label setting [15, 16, 17, 18] is still outstanding.", "startOffset": 58, "endOffset": 74}, {"referenceID": 15, "context": "The evaluation of NNMs in the extreme multi-label setting [15, 16, 17, 18] is still outstanding.", "startOffset": 58, "endOffset": 74}], "year": 2015, "abstractText": "We introduce normalized nonnegative models (NNM) for explorative data analysis. NNMs are partial convexifications of models from probability theory. We demonstrate their value at the example of item recommendation. We show that NNM-based recommender systems satisfy three criteria that all recommender systems should ideally satisfy: high predictive power, computational tractability, and expressive representations of users and items. Expressive user and item representations are important in practice to succinctly summarize the pool of customers and the pool of items. In NNMs, user representations are expressive because each user\u2019s preference can be regarded as normalized mixture of preferences of stereotypical users. The interpretability of item and user representations allow us to arrange properties of items (e.g., genres of movies or topics of documents) or users (e.g., personality traits) hierarchically.", "creator": "LaTeX with hyperref package"}}}