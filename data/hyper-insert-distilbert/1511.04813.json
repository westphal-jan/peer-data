{"id": "1511.04813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2015", "title": "Budget Online Multiple Kernel Learning", "abstract": "modeling online learning with multiple kernels has gained increasing interests gained in recent years and found many applications. unusual for classification computing tasks, traditional online multiple discrete kernel classification ( initial omkc ), which learns out a kernel based distribution classifier precisely by seeking the simplest optimal linear orthogonal combination of a pool of single mirror kernel classifiers in an experimental online fashion, achieves superior accuracy requirements and enjoys increasingly great tool flexibility beyond compared strategies with traditional single - sensor kernel statistical classifiers. even despite being systematically studied extensively, existing ranked omkc allocation algorithms continuously suffer from high numerical computational cost due significantly to detecting their unbounded numbers of support vectors. to overcome but this drawback, simply we present a new novel framework of optimization budget versus online multiple kernel learning ( bomkl ) and propose a generic new sparse timed passive aggressive learning to then perform effective budget online cpu learning. specifically, we potentially adopt a simple adaptive yet particularly effective conditional bernoulli sampling to quietly decide if an incoming instance bus should only be added to the current trend set matrix of support vectors. both by limiting the number space of support vectors, our proposed method can already significantly negatively accelerate competitive omkc tasks while maintaining satisfactory accuracy that mean is comparable to that precision of the existing omkc processing algorithms. we thus theoretically prove that implementing our new method achieves an expected optimal slower regret bound than in market expectation, and by empirically easily found that the older proposed filter algorithm simultaneously outperforms previously various traditional omkc algorithms and can easily truly scale up to large - array scale raw datasets.", "histories": [["v1", "Mon, 16 Nov 2015 03:40:50 GMT  (46kb)", "http://arxiv.org/abs/1511.04813v1", null], ["v2", "Wed, 18 Nov 2015 08:08:43 GMT  (35kb)", "http://arxiv.org/abs/1511.04813v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jing lu", "steven c h hoi", "doyen sahoo", "peilin zhao"], "accepted": false, "id": "1511.04813"}, "pdf": {"name": "1511.04813.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Peilin Zhao"], "emails": ["chhoi@smu.edu.sg", "jing.lu.2014@phdis.smu.edu.sg", "doyensahoo.2014@phdis.smu.edu.sg", "zhaop@i2r.a-star.edu.sg"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n04 81\n3v 1\n[ cs\n.L G\n] 1\n6 N\nov 2"}, {"heading": "1 INTRODUCTION", "text": "Online Multiple Kernel Learning has been successfully used in many real-world applications including classification (Hoi et al., 2013; Jin et al., 2010), regression (Sahoo et al., 2014), similarity learning for multimedia search(Xia et al., 2014), and structured prediction (Martins et al., 2010). In contrast to traditional online kernel methods (Kivinen et al., 2001) where a single kernel function is often chosen either manually or via some intensive cross-validation process, online multiple kernel learning algorithms learn multiple kernel classifiers and their combination simultaneously. This enables multiple kernel learning algorithms to efficiently learn very complicated patterns in an online fashion without the need of choosing the best kernel function prior to the online learning task.\nDespite being extensively studied, conventional online multiple kernel learning methods suffer from the curse of kernelization in large-scale applications, that is, the number of support vectors (SV\u2019s) often grows linearly with the number of instances received in the online learning process. This not only results in increasing computational cost, but also the growth of memory need which consequently leads to memory overflow in large-scale online applications.\nRecent years have witnessed a variety of studies that attempt to make online kernel learning (with a single kernel) scalable by bounding the number of SV\u2019s during the online learning process, known as budget online learning. Examples include Randomized Budget Perceptron (RBP) (Cavallanti et al., 2007), Forgetron (Dekel et al., 2005), Projectron (Orabona et al., 2008; 2009), Budget Passive Aggressive (BPA) learning (Wang & Vucetic, 2010), Bounded Online Gradient Descent (BOGD) (Zhao et al., 2012; Wang et al., 2012), among others. Although budget online learning has been actively studied, very few existing work systematically addresses the similar problem in online multiple kernel learning which is more challenging than online learning with a single kernel.\n\u2217Corresponding author: http://stevenhoi.org/, SIS, SMU, 80 Stamford Road, Singapore 178902\nOne straightforward approach to make online multiple kernel learning scalable is to apply the existing budget online learning algorithms to replace the current online learner for each kernel, and allocate a uniform budget for each kernel due to the lack of prior on the quality of individual kernels. Such a naive approach is simple and easy to implement, but falls short in many aspects. First of all, some efficient budget onilne learning algorithms are too simple to achieve satisfactory accuracy (e.g., RBP(Cavallanti et al., 2007)), while some other algorithms, despite being more effective, are often computationally very intensive for multiple kernel learning (e.g., Projectron (Orabona et al., 2008)). Second, because of the varied performances of different kernel functions in practice, using a simple uniform budget allocation is clearly not optimal as one of key successful principles in online the multiple kernel learning process is to focus more in learning with the better quality kernels.\nIn this paper, we investigate a novel framework of Budget Online Multiple Kernel Classification (BOMKC) and propose a new algorithm termed online Sparse Passive Aggressive learning (SPA) by extending the popular online Passive Aggressive (PA) technique (Crammer et al., 2006). The key idea of the proposed method is to explore a simple yet effective stochastic sampling strategy for adding SV\u2019s, where the probability of an incoming training instance to become a SV is determined by two factors: (i) the loss suffered by the classifier on this instance; and (ii) the historical accuracy of this individual kernel classifier. By limiting the number of SV\u2019s, the proposed BOMKC with SPA significantly accelerates the learning process of OMKC while maintaining satisfactory accuracy which is fairly comparable to that of existing non-budget OMKC algorithms. We theoretically prove that the proposed new algorithm not only bounds the number of SV\u2019s but also achieves an optimal regret bound in expectation. Finally, we conduct an extensive set of empirical studies which shows that the proposed algorithm outperforms a variety of existing OMKC algorithms and easily scales to large-scale datasets with million instances.\nThe rest of this paper is organized as follows. Section 2 formulates the problem of Budget OMKC learning and presents the SPA algorithm for BOMKC. Section 3 gives theoretical analysis of the proposed algorithm and achieves the optimal mistake bound in expectation. Section 4 presents the results of our empirical studies, and finally Section 5 concludes this paper."}, {"heading": "2 BUDGET ONLINE MULTIPLE KERNEL LEARNING", "text": "In this section, we first formulate the problem setting for Online Multiple Kernel Classification (OMKC), and then present the proposed Budget Online Multiple Kernel Classification (BOMKC) with Sparse Passive Aggressive (SPA) learning for classification tasks."}, {"heading": "2.1 PROBLEM SETTING AND PRELIMINARIES", "text": "In a typical binary classification task, our goal is to learn a function f : Rd \u2192 R from a sequence of training examples {(x1, y1), . . . , (xT , yT )}, where the feature vector xt \u2208 X \u2282 Rd and the class label yt \u2208 Y = {+1,\u22121}. We use y\u0302 = sign(f(x)) to predict the class label, and |f(x)| to measure the classification confidence.\nConsider a collection of m kernel functions K = {\u03bai : Rd \u00d7 Rd \u2192 R, i = 1, . . . ,m}. Each can be a predefined parametric or nonparametric function. MKC aims to learn a kernel-based prediction model by identifying the best linear combination of the m kernels whose weights are denoted by \u03b8 = (\u03b81, . . . , \u03b8m). The learning task can be cast into the following optimization:\nmin \u03b8\u2208\u2206 min f\u2208HK(\u03b8)\n1 2 \u2016f\u20162HK(\u03b8) + C\nT \u2211\nt=1\n\u2113(f(xt), yt)\nwhere \u2206 = {\u03b8 \u2208 Rm+ |\u03b8T 1m = 1}, K(\u03b8)(\u00b7, \u00b7) = \u2211m i=1 \u03b8i\u03bai(\u00b7, \u00b7), and \u2113(f(xt), yt) is a convex loss function that penalizes the deviation of estimating f(xt) from observed labels yt. To simplify the discussion, we denote \u2113t(f) = \u2113(f ; (xt, yt)) throughout this paper.\nThe above convex optimization problem of regular batch MKL have been solved by different optimization schemes (Sonnenburg et al., 2006; Xu et al., 2008; Go\u0308nen & Alpayd\u0131n, 2011). Despite being studied extensively, it still suffers from the common drawbacks of batch learning methods, i.e., not scalable to large-scale applications, expensive retraining cost when data arrives sequentially and not able to adapt to fast-evolving patterns.\nTo address the challenges faced by batch MKC methods, several algorithms attempt to solve the MKC problem in an online manner (Hoi et al., 2013; Sahoo et al., 2014) whose updating scheme usually consists of two steps. First learn a set of effective single kernel classifiers f it \u2208 H\u03bai , i = 1, ...,m using existing online kernel learning algorithms (such as kernel Passive Aggressive and kernel Perceptron). Second, learn an effective classifier ft(x) by combining these single kernel classifiers:\nft(x) =\nm \u2211\ni=1\n\u03b8itf i t (x) (1)\nwhere \u03b8it \u2208 [0, 1] is the combination weight of the classifier with respect to \u03bai at time t. The weights can be updated during the learning process by adopting the Hedge algorithm.\nCompared with batch learning methods, these Online Multiple Kernel Classification algorithms enjoy better scalability to large-scale applications and are applicable to sequential data. However, whenever a new instance is misclassified, it will be added to the SV set. The unbounded number of SV\u2019s will eventually lead to memory explosion, making it difficult to scale up in practice."}, {"heading": "2.2 SPARSE PASSIVE AGGRESSIVE LEARNING FOR ONLINE MULTIPLE KERNEL LEARNING", "text": "To overcome the critical limitation of existing OMKC algorithms, we aim to explore novel techniques to build scalable and efficient OMKC algorithms. Similar to the existing OMKC algorithms, the proposed algorithm adopts a two-step updating scheme: (i) update the single kernel classifiers individually; (ii) update the weights for combining the classifiers. Following, we will present the details of our proposed SPA algorithm.\nOur update strategy of each single kernel classifier is generalized from the PA algorithm (Crammer et al., 2006), At the t-step, the online hypothesis will be updated:\nft+1 = min f\u2208H\u03ba\n1 2 \u2016f \u2212 ft\u20162H\u03ba + \u03b7\u2113t(f)\nwhere \u03b7 > 0 and the hinge loss is adopted \u2113t(f) = [1\u2212 ytf(xt)]+. This optimization involves two objectives: the first is to keep the new function close to the old one, while the second is to minimize the loss of the new function on the current example.\nTo limit the number of SV\u2019s in each single kernel classifier, we propose a simple yet effective sampling rule which decides if an incoming instance should be a support vector by performing a Bernoulli trial as follows:\nPr(Zit = 1) = \u03c1 i t, \u03c1 i t =\nmin(\u03b1, \u2113t(f i t ))\n\u03b2\nwhere Zit \u2208 {0, 1} is a random variable such that Zit = 1 indicates whether a new SV should be added to update the classifier at the t-th step, and \u03b2 \u2265 \u03b1 > 0 are parameters to adjust the ratio of support vectors with some given budget. The above sampling rule has two key concerns: (i) The probability of making the t-th step update is always less than \u03b1/\u03b2, which avoids assigning too high probability on a noisy instance. (ii) An example suffering higher loss has a higher probability of being assigned to the support vector set.\nIn the above, the first is to guarantee that the ratio of support vectors to total received instances is always bounded in expectation, and the second is to maximize the learning accuracy by adding informative support vectors or equivalently avoid making unnecessary updates.\nAfter obtaining the random variable Zit , we will need to develop an effective strategy for updating the classifier. Following the PA learning principle, we propose the following updating method:\nf it+1 = min f\u2208H\u03ba\nP it (f) := 1\n2 \u2016f \u2212 f it\u20162H\u03ba + Zit \u03c1it \u03b7\u2113t(f) (2)\nNote when \u03c1t = 0, then Zt = 0 and we set Zt/\u03c1t = 0. We adopt the above update since it is an unbiased estimation of that of PA update. We can derive a closed-form solution as following.\nf it+1(\u00b7) = f it (\u00b7) + \u03c4tyt\u03ba(xt, \u00b7), \u03c4t = min( \u03b7Zit \u03c1it , \u2113t(f i t ) \u03ba(xt,xt) )\nThe remaining problem is to learn the appropriate combination weights for the set of single kernel classifiers. The simplest way is a uniform combination, i.e. \u03b8i = 1\nm , which will be used in our\ninitialization step. During the online learning process, the Hedge algorithm is used to update the weights according to the performance of each classifier.\nIn addition, since we would like to focus on the best kernel, the update probability of a component classifier should depend on the historical accuracy, which is reflected by the current combination weight. Finally, we summarize the proposed Sparse Passive Aggressive algorithm in Algorithm 1\nAlgorithm 1 Sparse Passive Aggressive learning for Budget OMKC \u201cBOMKC(SPA)\u201d INPUT: Kernels: ki(\u00b7, \u00b7) : X \u00d7 X \u2192 R, i = 1, . . . ,m; Aggressiveness parameter \u03b7 > 0, and parameters \u03b2 \u2265 \u03b1 > 0; Discount parameter \u03b3 \u2208 (0, 1) and smoothing parameters \u03b4t \u2208 (0, 1). Initialization: f i1 = 0, w i 1 = \u03b8 i 1 = 1 m , i = 1, ...m\nfor t = 1, 2, . . . , T do\nReceive an instance: xt and predict y\u0302t = sign ( \u2211m i=1 \u03b8 i t \u00b7 f it (xt) ) Receive the class label: yt for i = 1, 2, . . . ,m do\nSuffer loss: \u2113t(f it ), i = 1, ...m Compute pit = (1\u2212 \u03b4t) wit\nmaxj w j t + \u03b4t; Bernoulli Sampling cit \u2208 {0, 1} by Pr(cit = 1) = pit if cit = 1 then\nCompute \u03c1it = min(\u03b1,\u2113t(f\ni t ))\n\u03b2 ; Sample Zit \u2208 {0, 1} by: Pr(Zit = 1) = \u03c1it\nUpdate the classifier as Proposition 1 end if Update weight wit = w i t\u03b3 \u2113t(f i t )\nend for Scale the weights \u03b8it+1 =\nwit\u2211 m j=1 w j t , i = 1, ...,m\nend for"}, {"heading": "3 THEORETICAL ANALYSIS", "text": "In this section, we will provide detailed theoretical analysis to the loss bound of our proposed SPA algorithm. We first propose a lemma that bounds the regret of a single kernel classifier learnt by the SPA update strategy and then utilize this lemma for the finally analysis of our proposed SPA algorithm for OMKC problem. We denote f\u2217 = argminf\u2208\u03ba \u2211T t=1 \u2113t(f) as the optimal classifier in H\u03ba space with the assumption of the foresight to all the instances.\nLemma 1 Let (x1, y1), . . . , (xT , yT ) be a sequence of examples where xt \u2208 Rd, yt \u2208 Y = {\u22121,+1} for all t. If we assume \u03ba(x,x) = 1 and the hinge loss function \u2113t(\u00b7) is 1-Lipschitz, then for any \u03b2 \u2265 \u03b1 > 0, and \u03b7 > 0, when using the proposed SPA update strategy to generate a sequence of single kernel classifiers in space \u03ba, we have\nE[\nT \u2211\nt=1\n(\u2113t(ft)\u2212 \u2113t(f\u2217))] < 1 2\u03b7 \u2016f\u2217\u20162H\u03ba + \u03b7\u03b2 min(\u03b1, \u221a \u03b2\u03b7) T\nwhere \u03b7 is the aggressiveness parameter. When setting \u03b7 = \u2016f\u2217\u2016H\u03ba \u221a \u03b1 2\u03b2T and \u03b13 \u2264 \u03b2 2T \u2016f\u2217\u20162H\u03ba , we will have\nE[ T \u2211\nt=1\n(\u2113t(ft)\u2212 \u2113t(f\u2217))] < \u2016f\u2217\u2016H\u03ba \u221a 2\u03b2T/\u03b1\nRemark 1. The theorem indicates that the expected regret of any single classifier in H\u03ba by the SPA algorithm can be bounded by \u2016f\u2217\u2016H\u03ba \u221a\n2\u03b2T/\u03b1 in expectation. In practice, \u03b2/\u03b1 is usually a small constance. Namely, the proposed algorithm achieves a strong regret bound in expectation.\nRemark 2. The expected regret has a close relationship with the two parameters \u03b1 and \u03b2. As indicated above, to avoid assigning too high probability on a noisy instance, the parameter \u03b1 can not be too large. Assuming \u03b1 \u2264 \u221a \u03b2\u03b7 (which is accessible in the practical parameter setting), the expected regret bound is proportion to the ratio \u03b2/\u03b1. This consists with the intuition that larger chances of adding SV\u2019s leads to smaller loss. Further more, for \u03b1 > \u221a \u03b2\u03b7, the expected regret bound is less tight than the above case, which consists with the analysis before that too large \u03b1 involves large number of noisy instances and might be harmful.\nWe have demonstrated that the loss of each single kernel online classifier is up bounded by the loss of its batch counterpart and an online regret term ( \u221a T ). While in reality, the performance of single kernel classifiers varies significantly and we have no foresight to the winner. Following, we will show that the final multiple kernel classifier in our proposed algorithm based on single kernel SPA classifiers achieves almost the same performance as that of the best online single kernel classifier.\nTheorem 1 Let (x1, y1), . . . , (xT , yT ) be a sequence of examples where xt \u2208 Rd, yt \u2208 Y = {\u22121,+1} for all t. If we assume \u03bai(x,x) = 1 and the hinge loss function \u2113t(\u00b7) is 1-Lipschitz, then for any \u03b2 \u2265 \u03b1 > 0, and \u03b7 > 0, \u03b3 \u2208 (0, 1), any the multiple kernel classifier generated by our proposed SPA algorithm satisfies the following inequality\nE[\nT \u2211\nt=1\n\u2113t(ft)] \u2264 mini(\n\u2211T t=1 \u2113t(f i \u2217) + 1 2\u03b7\u03b4\u2016f i\u2217\u20162H\u03ba)\u00b5 ln(1/\u03b3)\n1\u2212 \u03b3 + \u00b5\u03b7\u03b2 ln(1/\u03b3) min(\u03b1, \u221a \u03b2\u03b7)(1 \u2212 \u03b3)\u03b4 T + \u00b5 lnm\n1\u2212 \u03b3 where \u03b7 is the aggressiveness parameter, \u00b5 is a constant that only depends on \u03b3 and the upper bound of \u2113t. When setting \u03b7 = \u2016f i\u2217\u2016H\u03ba \u221a \u03b1 2\u03b2T and \u03b1 3 \u2264 \u03b22T \u2016f i\u2217\u20162H\u03ba , we have\nE[\nT \u2211\nt=1\n\u2113t(ft)] \u2264 \u00b5 lnm 1\u2212 \u03b3 + \u00b5 ln(1/\u03b3) 1\u2212 \u03b3 mini {\nT \u2211\nt=1\n\u2113t(f i \u2217) +\n1 \u03b4 \u2016f i\u2217\u2016H\u03ba \u221a 2\u03b2T/\u03b1 }\nThe proof to the above theorem can be found in the appendix.\nRemark. This theorem suggests that even without any foresight of which kernel would achieve the highest accuracy, the performance of our proposed multiple kernel classifier is still comparable to the best single kernel classifier. Compared with the optimal batch classifier with foresight to all the training instances, the regret of our proposed algorithm is O( \u221a T ).\nNext, we would bound the number of SV\u2019s of each single kernel classifier f iT in expectation.\nTheorem 2 Let (x1, y1), . . . , (xT , yT ) be a sequence of examples where xt \u2208 Rd, yt \u2208 Y = {\u22121,+1} for all t. If we assume \u03ba(x,x) = 1 and the hinge loss function \u2113t(\u00b7) is 1-Lipschitz, \u03b2 \u2265 \u03b1 > 0, and \u03b7 > 0, for any i = 1, ...m the proposed SPA algorithm satisfies the following inequality\nE[\nT \u2211\nt=1\nZit ]\u2264 min { \u03b1\n\u03b2 T,\n1 \u03b2 [\nT \u2211\nt=1\n\u2113t(f i \u2217) +\n1\n2\u03b7 \u2016f i\u2217\u20162H\u03bai +\n\u03b7\u03b2\nmin(\u03b1, \u221a \u03b2\u03b7) T ]\n}\nEspecially, when \u03b7 = \u2016f i\u2217\u2016H\u03bai \u221a \u03b1 2\u03b2T and \u03b1 3 \u2264 \u03b22T \u2016f i\u2217\u20162H\u03ba , we have\nE[ T \u2211\nt=1\nZit ] \u2264 min { \u03b1\n\u03b2 T,\n1 \u03b2 [\nT \u2211\nt=1\n\u2113t(f i \u2217) + \u2016f i\u2217\u2016H\u03bai\n\u221a\n2\u03b2T/\u03b1]\n}\nRemark. First, this theorem indicates the expected number of support vectors is less than \u03b1T/\u03b2. Thus, by setting \u03b2 \u2265 \u03b1T/B (1 < B \u2264 T ), we guarantee the expected number of support vectors of the final classifier is bounded by a budget B. Second, this theorem also implies that, by setting \u03b2 \u2265 [\n\u2211T t=1 \u2113t(f i \u2217) + \u2016f i\u2217\u2016H\u03bai\n\u221a\n2\u03b2T/\u03b1]/B (1 < B \u2264 T ), the expected number of support vectors is always less than B, no matter how is the value of \u03b1. In practice, as the Hedge algorithm trends to converge to a single best kernel, the total number of support vectors of all classifiers is only slightly larger than that used by the best classifier."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we conduct extensive experiments to evaluate the empirical performance of the proposed SPA multiple kernel algorithm for online binary classification tasks."}, {"heading": "4.1 EXPERIMENTAL TESTBED", "text": "Table 1 summarizes details of the binary classification datasets used in our experiments. All of them are commonly used benchmark datasets and are publicly available from LIBSVM1 and KDDCUP competition site. These datasets are chosen fairly randomly to cover a variety of different sizes.\n1http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/"}, {"heading": "4.2 KERNELS", "text": "In our experiments, we examine BOMKC by exploring a set of 16 predefined kernels, including 3 polynomial kernels \u03ba(xi,xj) = (x\u22a4i xj) p with the degree parameter p = 1, 2, 3, 13 Gaussian kernels \u03ba(xi,xj) = exp(\u2212 ||xi\u2212xj || 2 2 2\u03c32 ) with the kernel width parameter \u03c3 = [2 \u22126, 2\u22125, ..., 26]."}, {"heading": "4.3 COMPARED ALGORITHMS", "text": "First, we include an important baseline algorithm showing the best performance that could be achieved by a single kernel classifier assuming the foresight of optimal choice of the kernel function before the training instances arrives. Specially, we search for the best single kernel classifier from the set of our predefined 16 kernels using one random permutation of all the training examples and then apply the \u201cPerceptron\u201d algorithm (Rosenblatt, 1958) with the best kernel function.\nOur second group of compared algorithms are the Online Multiple Kernel Classification algorithms (Hoi et al., 2013) which achieved state-of-the-art performance on many benchmark datasets. Three variants of this algorithm are included:\n\u2022 \u201cOMKC(U)\u201d: the OMKC algorithm with a naive uniform combination; \u2022 \u201cOMKC(DD)\u201d: the OMKC algorithm with deterministic combination and update; \u2022 \u201cOMKC(SD)\u201d: OMKC with stochastic update and deterministic combination;\nFinally, to test the efficiency and effectiveness of our budget strategy, we also compare with multiple kernel classification algorithms whose component classifiers are updated by budget kernel learning algorithms including:\n\u2022 \u201cRBP\u201d: the Random Budget Perceptron algorithm (Cavallanti et al., 2007); \u2022 \u201cForgetron\u201d: the Forgetron algorithm that discards the oldest SV (Dekel et al., 2005); \u2022 \u201cBOGD\u201d: the Budget Online Gradient Descent algorithm (Zhao et al., 2012); \u2022 \u201cBPAS\u201d: the Budget Passive-aggressive algorithm (simple)(Wang & Vucetic, 2010).\nMultiple Kernel classification is a more challenging problem compared with single kernel classification, which requires high efficient component classifiers. Consequently, we did not compare with slow budget algorithm such as Projectron."}, {"heading": "4.4 PARAMETER SETTINGS", "text": "To make a fair comparison, we adopt the same experimental setup for all the algorithms. The weight discount parameter \u03b3 is fixed to 0.99 for all multiple kernel algorithms on all datasets. The smoothing parameter \u03b4 for all stochastic update algorithms is fixed to 0.001. The learning rate parameters in all algorithms (SPA, the BOGD and BPAS) are all fixed to 0.1. For the proposed SPA algorithm, we simply fix the random sampling parameter \u03b1 = 1 and \u03b2 = 3 for the first 8 datasets and will discuss the parameter sensitivity later. For a fair comparison between the budget algorithms, we set a uniform limit B to the SV size for in all the component classifiers so that the total number of SV\u2019s used by all component classifiers 16B is nearly equal to the SV size of the SPA algorithm. To test the scalability of our proposed algorithm, we also include the experiment on million scale dateset, we set the \u03b2 = 300 for efficiency.\nAll experiments were repeated 10 times on different random permutations of instances and all the results were obtained by averaging over 10 runs. All the algorithms were implemented in C++ on a Windows machine with 3.2 GHz CPU. We report the online mistake rates along the training process, the total number of SV\u2019s used by all component single kernel classifiers and the running time."}, {"heading": "4.5 EVALUATION OF ONLINE LEARNING PERFORMANCE WITH COMPARISON TO NON-BUDGET OMKL ALGORITHMS", "text": "The first experiment is to evaluate the performance of SPA for binary classification tasks with comparison to non-budget OMKL algorithms. We did not report the results on the 3 largest datasets since it is nearly impossible for non-budget algorithms to process large scale data in limited time and memory. Table 2 summarizes the experimental results. We can draw the following observations.\nFirst of all, we compare the accuracy of the three algorithms with deterministic update and full SV\u2019s (Perceptron, OMKC(U), OMKC(DD)). Obviously, the mistake rate of OMKC(DD) is usually much lower than that of OMKC(U), which indicates that the OMKC(DD) algorithm is able to learn the best combination to build an effective multiple kernel classifier. Further, to our surprise, even with the unrealistic assumption that we have foresight to all the instances and can choose the best kernel before learning, the Perceptron algorithm still can not achieves the lowest error rate. We conjecture that there might be two explanations to this observation. First, our optimal kernel is searched in one random permutation, which might not be the optimal kernel for other permutations of instances. While, OMKC(DD) always learns the best combination of the kernel functions. Second, in some datasets no single kernel have significant advantage over others, while an optimal weighted combination might performs better than any single kernel classifier. This further validates the significance of studying multiple kernel learning algorithms.\nSecond, we find that although using fewer SV\u2019s, the stochastic update algorithms OMKC(SD) can even achieve lower mistake rate compared with deterministic update OMKC(DD), which consists with the previous observations (Hoi et al., 2013). This indicates that not all SV\u2019s are essential for an accurate classifier. This support our main claim that when the added SV\u2019s are wisely selected, the accuracy may not decrease much while the efficiency can be significantly upgraded.\nThird, when comparing the time cost, we find that the non-budget OMKC algorithms are significantly slower than our proposed SPA algorithm, which is more serious in larger datasets. Actually, it is even impossible for non-budget algorithms to complete the learning process in realistic time and space in the three largest datasets. This is due to two reasons: first, similar to the problem faced by single kernel methods, the prediction cost grows rapidly with unlimited number of SV\u2019s added; second, there is a big pool of kernel classifiers need to be updated and combined. This observation further validates the importance of studying efficient and scalable online multiple kernel methods.\nFinally, we found that apart from the obvious advantage in time cost, our proposed SPA algorithm achieves the highest accuracy in all datasets even when adopting only a small factor of SV\u2019s. This validates the effectiveness of our proposed algorithm in wisely selecting the most informative SV\u2019s."}, {"heading": "4.6 EVALUATION OF ONLINE LEARNING PERFORMANCE WITH COMPARISON TO DIFFERENT BUDGET OMKL ALGORITHMS", "text": "The next experiment is to test the accuracy and efficiency of our proposed SPA algorithm with comparison to other budget maintenance strategies. Table 3 summarizes the experimental results.\nFrom the results, it is obvious to see that our SPA algorithm achieves the best accuracy among all the budget algorithms in most of the cases. In addition, the time cost of the proposed SPA algorithm is always the lowest. This is due to the advantage in our algorithm design: SPA can find the optimal kernel and concentrate the effort in this kernel. Thus, poor kernels will receive few SV\u2019s and thus the prediction time is greatly reduced. When adopting the same number of SVs as other budget algorithms, instead of paying equal attention to all components, SPA focuses on the best kernel and thus achieves the highest accuracy.\n4.7 PARAMETER SENSITIVITY OF \u03b1 AND \u03b2\nThe proposed SPA algorithm has two critical parameters \u03b1 and \u03b2 which could considerably affect the accuracy, support vector size and time cost. Our second experiment is to examine how different parameters of \u03b1 and \u03b2 affects the learning performance so as to give insights for how to choose them in practice. Figure 1 evaluate the performance of the SPA algorithm on the \u201ca9a\u201d dataset with varied \u03b1 and varied \u03b2. Several observations can be drawn from the experimental results.\nFirst of all, when \u03b2 is fixed, increasing \u03b1 generally results in (i) larger support vector size, (ii) higher time cost, but (iii) better classification accuracy, especially when \u03b1 is small. However, when \u03b1 is large enough (e.g., \u03b1 > 1.5), increasing \u03b1 has very minor impact to the performance. This is because the number of instances whose hinge loss above \u03b1 is relatively small. We also note that the accuracy decreases slightly when \u03b1 is too large. This might be because some (potentially noisy) instances with large loss are given a high chance of being assigned as SV\u2019s, which may harm the classifier due to noise. Thus, on this dataset (\u201ca9a\u201d), it is easy to find a good \u03b1 in the range of [1,2].\nSecond, when \u03b1 is fixed, increasing \u03b2 will result in (i) smaller support vector size, (ii) smaller time cost, but (iii) worse classification accuracy. On one hand, \u03b2 cannot be too small as it will lead to too many support vectors and thus suffer very high time cost. On the other than, \u03b2 cannot be too large as\nit will considerably decrease the classification accuracy. We shall choose \u03b2 that yields a sufficiently accurate classifier while minimizing the support vector size and training time cost. For example, for this particular dataset, choosing \u03b2 in the range of [3,6] achieves a good trade-off between accuracy and efficiency/sparsity."}, {"heading": "5 CONCLUSIONS", "text": "This paper proposed a new framework of Budget Online Budget Multiple Kernel Learning with a novel Sparse Passive Aggressive learning algorithm. In contrast to the existing Online Multiple Kernel Classification (OMKC) algorithms that usually suffer from extremely high time cost due to their unbounded numbers of support vectors in the online learning process, our proposed algorithm adopts a simple but effective SV sampling strategy, making it applicable to large scale applications. We theoretically demonstrated that the SPA algorithm enjoyed an optimal regret bound in expectation. In addition, The experiment results with comparison to both the existing non-budget OMKL algorithms and budget OMKL algorithms showed that the proposed method achieved a significant speedup and yielded more accurate classifiers than existing methods, validating the efficacy, efficiency and scalability of the proposed technique."}, {"heading": "APPENDIX A: PROOF FOR LEMMA 1", "text": "Proof: Firstly, the Pt(f) defined in the equality\nft+1 = min f\u2208H\u03ba\nPt(f) := 1\n2 \u2016f \u2212 ft\u20162H\u03ba + Zt \u03c1t \u03b7\u2113t(f)\nis 1-strongly convex. Further, ft+1 is the optimal solution of minf Pt(f), we thus have the following inequality according the definition of strongly convex\n1 2 \u2016f \u2212 ft\u20162H\u03ba + 1 \u03c1t Zt\u03b7\u2113t(f) \u2265 1 2 \u2016ft+1 \u2212 ft\u20162H\u03ba + 1 \u03c1t Zt\u03b7\u2113t(ft+1) + 1 2 \u2016f \u2212 ft+1\u20162H\u03ba\nwhere the inequality used \u2207Pt(ft+1) = 0. After rearranging the above inequality, we get 1\n\u03c1t Zt\u03b7\u2113t(ft+1)\u2212\n1 \u03c1t Zt\u03b7\u2113t(f) \u2264 1 2 \u2016f \u2212 ft\u20162H\u03ba \u2212 1 2 \u2016f \u2212 ft+1\u20162H\u03ba \u2212 1 2 \u2016ft+1 \u2212 ft\u20162H\u03ba\nSecondly, since \u2113t(f) is 1-Lipshitz with respect to f\n\u2113t(ft)\u2212 \u2113t(ft+1) \u2264 \u2016ft \u2212 ft+1\u2016H\u03ba . Combining the above two inequalities, we get\n1 \u03c1t Zt\u03b7[\u2113t(ft)\u2212 \u2113t(f)] \u2264 1 2 \u2016f \u2212 ft\u20162H\u03ba \u2212 1 2 \u2016f \u2212 ft+1\u20162H\u03ba \u2212 1 2 \u2016ft+1 \u2212 ft\u20162H\u03ba + 1 \u03c1t Zt\u03b7\u2016ft \u2212 ft+1\u2016H\u03ba\nSumming the above inequalities over all t leads to\nT \u2211\nt=1\n1 \u03c1t Zt\u03b7 [\u2113t(ft)\u2212 \u2113t(f)] \u2264 1 2 \u2016f \u2212 f1\u20162H\u03ba +\nT \u2211\nt=1\n[\n\u22121 2 \u2016ft+1 \u2212 ft\u20162H\u03ba + 1 \u03c1t Zt\u03b7\u2016ft \u2212 ft+1\u2016H\u03ba\n]\nWe now take expectation on the left side. Note, by definition of the algorithm, EtZt = \u03c1t, where we used Et to indicate conditional expectation give all the random variables Z1, . . . , Zt\u22121. Assuming \u03c1t > 0, we have\nE [ 1\n\u03c1t Zt\u03b7 [\u2113t(ft)\u2212 \u2113t(f)]\n] = E [ 1\n\u03c1t EtZt\u03b7 [\u2113t(ft)\u2212 \u2113t(f)]\n]\n= \u03b7E [\u2113t(ft)\u2212 \u2113t(f)] (3)\nNote that in some iterations, \u03c1t = 0, in that case, we have \u2113t(ft) = 0, thus:\n\u03b7[\u2113t(ft)\u2212 \u2113t(f)] \u2264 0 (4)\nAs mentioned before, \u03c1t = 0 indicates Zt = 0 and Zt/\u03c1t = 0, we get\n1 \u03c1t Zt\u03b7[\u2113t(ft)\u2212 \u2113t(f)] = 0 (5)\nCombining (3), (4) and (5) and summarizing over all t leads to\n\u03b7E\nT \u2211\nt=1\n[\u2113t(ft)\u2212 \u2113t(f)] \u2264 E T \u2211\nt=1\n1 \u03c1t Zt\u03b7 [\u2113t(ft)\u2212 \u2113t(f)]\nWe now take expectation on the right side of (3)\nE\n[\n1 2 \u2016f \u2212 f1\u20162H\u03ba\n]\n+ E\n[\nT \u2211\nt=1\n[\n\u22121 2 \u2016ft+1 \u2212 ft\u20162H\u03ba + 1 \u03c1t Zt\u03b7\u2016ft \u2212 ft+1\u2016H\u03ba\n]\n]\n\u2264 1 2 \u2016f\u20162H\u03ba +\nT \u2211\nt=1\nE\n[\n\u22121 2 \u03c4 2t + 1 \u03c1t Zt\u03b7\u03c4t\n]\n(6)\nGiven all the random variables Z1, . . . , Zt\u22121, we now calculate the conditional expectation of the variable Mt = \u2212 12\u03c42t + 1\u03c1tZt\u03b7\u03c4t: In probability \u03c1t, Zt = 1 and \u03c4t = \u03c4 \u2032 t = min( \u03b7 \u03c1t , \u2113t(ft)). We have Mt (Zt=1) = \u2212 12 \u03c4 \u20322t + 1\u03c1t \u03b7\u03c4 \u2032 t . And in probability 1\u2212\u03c1t, Zt = 0 and \u03c4t = 0. We have Mt (Zt=0) = 0. Considering the two cases, the conditional expectation is:\nEt[Mt] = \u03c1tMt (Zt=1) + (1 \u2212 \u03c1t)Mt (Zt=0) = \u03c1t [ \u22121 2 \u03c4 \u20322t + 1 \u03c1t \u03b7\u03c4 \u2032t ] < \u03b7\u03c4 \u2032t\nIn the case when \u03b1 \u2264 \u2113t and \u03c1t = \u03b1\u03b2 , \u03c4 \u2032t = min( \u03b7\u03b2 \u03b1 , \u2113t(ft)) \u2264 \u03b7\u03b2\u03b1 , thus \u03b7\u03c4 \u2032t \u2264 \u03b72\u03b2 \u03b1 .\nAnd in the case \u03b1 > \u2113t and \u03c1t = \u2113t\u03b2 , \u03c4 \u2032 t = min( \u03b7\u03b2 \u2113t(ft) , \u2113t(ft)) \u2264 \u221a \u03b7\u03b2. Thus, \u03b7\u03c4 \u2032t \u2264 \u03b7 2\u03b2\u221a \u03b2\u03b7 .\nConsidering both of the cases leads to\nEt[Mt] < \u03b72\u03b2\nmin(\u03b1, \u221a \u03b2\u03b7)\nSumming the above inequality over all t and combining with (6), we get\n\u03b7E\nT \u2211\nt=1\n[\u2113t(ft)\u2212 \u2113t(f)] < 1\n2 \u2016f\u20162H\u03ba +\n\u03b72\u03b2\nmin(\u03b1, \u221a \u03b2\u03b7) T\nSetting f = f\u2217, and multiplying the above inequality with 1/\u03b7 will conclude the lemma."}, {"heading": "APPENDIX B: PROOF FOR THEOREM 1", "text": "Proof: In the following proof, we first generalize the loss bound of the Hedge algorithm (Freund & Schapire, 1995) to a different situation where 1) stochastic update and stochastic combination are adopted and 2) the hinge loss function we adopt is bounded \u2113t(f) \u2208 [0, L] and L \u2265 1. Using the convexity, we have\n\u03b3\u2113 i t \u2264 1\u2212 1\u2212 \u03b3\n\u00b5 \u2113it\nwhere \u00b5 > 1 satisfies the equality when \u2113it = L and this constant only depends on L and \u03b3. We then get\nm \u2211\ni=1\nwit+1 =\nm \u2211\ni=1\nwit\u03b3 \u2113it \u2264\nm \u2211\ni=1\nwit\n(\n1\u2212 1\u2212 \u03b3 \u00b5 \u2113it\n)\n= (\nm \u2211\ni=1\nwit)\n(\n1\u2212 1\u2212 \u03b3 \u00b5\nm \u2211\ni=1\n\u03b8it\u2113 i t\n)\nApplying the above formula repeatedly for t = 1, ...T and using 1 + x \u2264 ex for all real number x yield,\nm \u2211\ni=1\nwiT+1 \u2264 exp ( \u22121\u2212 \u03b3 \u00b5 T \u2211\nt=1\nm \u2211\nt=1\n\u03b8it\u2113 i t\n)\nwe may write the above as following, T \u2211\nt=1\nm \u2211\nt=1\n\u03b8it\u2113 i t \u2264\n\u2212\u00b5 ln( \u2211m i=1 w i T+1)\n1\u2212 \u03b3 (7)\nObviously, all the weights are positive, thus m \u2211\ni=1\nwiT+1 \u2265 wiT+1 = wi1\u03b3 \u2211 T t=1 \u2113 i t\nPlugging this into (7) and replace wi1 with 1/m yields T \u2211\nt=1\nm \u2211\nt=1\n\u03b8it\u2113 i t \u2264\n\u2212\u00b5 ln(wi1\u03b3 \u2211T t=1 \u2113 i t)\n1\u2212 \u03b3 = \u00b5 lnm\u2212 \u00b5 ln \u03b3\u2211Tt=1 \u2113it 1\u2212 \u03b3 (8)\nUsing the convexity of loss function, we have T \u2211\nt=1\n\u2113t(ft) =\nT \u2211\nt=1\n\u2113t(\nm \u2211\ni=1\n\u03b8itf i t ) \u2264\nT \u2211\nt=1\nm \u2211\ni=1\n\u03b8it\u2113 i t\nThus T \u2211\nt=1\n\u2113t(ft) \u2264 \u00b5 lnm\u2212 \u00b5 ln \u03b3\u2211Tt=1 \u2113it\n1\u2212 \u03b3 (9)\nNow we need to bound the accumulate loss of a single classifier, \u2211T t=1 \u2113 i t. In Lemma 1 we have proven the regret bound of one single kernel classifier learnt by SPA assuming E[Zt] = \u03c1t. Here, we slightly modify this assumption for our proposed SPA multiple kernel classifier, i.e. E[Zt] = \u03c1t \u2217pit, where \u03b4 < pit < 1. Consequently, the conclusion of Lemma 1 becomes,\n\u03b4E[\nT \u2211\nt=1\n(\u2113t(ft)\u2212 \u2113t(f\u2217))] < 1\n2\u03b7 \u2016f\u2217\u20162H\u03ba +\n\u03b7\u03b2\nmin(\u03b1, \u221a \u03b2\u03b7) T\nCombining with (9) concludes this proof."}, {"heading": "APPENDIX C: PROOF FOR THEOREM 2", "text": "Proof: Since Et[Zt] = \u03c1t, where Et is the conditional expectation, we have\nE[\nT \u2211\nt=1\nZt] = E[\nT \u2211\nt=1\nEtZt] = E[\nT \u2211\nt=1\n\u03c1t] = E[\nT \u2211\nt=1\nmin( \u03b1 \u03b2 , \u2113t(ft) \u03b2 )] \u2264 min(\u03b1 \u03b2 T, 1 \u03b2 E\nT \u2211\nt=1\n\u2113t(ft))\n\u2264 min { \u03b1\n\u03b2 T,\n1 \u03b2 [\nT \u2211\nt=1\n\u2113t(f\u2217) + 1\n2\u03b7 \u2016f\u2217\u20162H\u03ba +\n\u03b7\u03b2\nmin(\u03b1, \u221a \u03b2\u03b7) T ]\n}\nwhich concludes the first part of the theorem. The second part of the theorem is trivial to be derived."}], "references": [{"title": "Tracking the best hyperplane with a simple budget perceptron", "author": ["Cavallanti", "Giovanni", "Cesa-Bianchi", "Nicol\u00f2", "Gentile", "Claudio"], "venue": "Machine Learning,", "citeRegEx": "Cavallanti et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cavallanti et al\\.", "year": 2007}, {"title": "Online passiveaggressive algorithms", "author": ["Crammer", "Koby", "Dekel", "Ofer", "Keshet", "Joseph", "Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Crammer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2006}, {"title": "The forgetron: A kernel-based perceptron on a fixed budget", "author": ["Dekel", "Ofer", "Shalev-Shwartz", "Shai", "Singer", "Yoram"], "venue": "In NIPS,", "citeRegEx": "Dekel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2005}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Freund", "Yoav", "Schapire", "Robert E"], "venue": "In Computational learning theory,", "citeRegEx": "Freund et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1995}, {"title": "Multiple kernel learning algorithms", "author": ["G\u00f6nen", "Mehmet", "Alpayd\u0131n", "Ethem"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "G\u00f6nen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "G\u00f6nen et al\\.", "year": 2011}, {"title": "Online multiple kernel classification", "author": ["Hoi", "Steven CH", "Jin", "Rong", "Zhao", "Peilin", "Yang", "Tianbao"], "venue": "Machine Learning,", "citeRegEx": "Hoi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoi et al\\.", "year": 2013}, {"title": "Online multiple kernel learning: Algorithms and mistake bounds", "author": ["Jin", "Rong", "Hoi", "Steven CH", "Yang", "Tianbao"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Jin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2010}, {"title": "Online learning with kernels", "author": ["Kivinen", "Jyrki", "Smola", "Alex J", "Williamson", "Robert C"], "venue": "In NIPS, pp", "citeRegEx": "Kivinen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 2001}, {"title": "Online multiple kernel learning for structured prediction", "author": ["Martins", "Andr\u00e9 FT", "Figueiredo", "Mario AT", "Aguiar", "Pedro MQ", "Smith", "Noah A", "Xing", "Eric P"], "venue": "arXiv preprint arXiv:1010.2770,", "citeRegEx": "Martins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2010}, {"title": "The projectron: a bounded kernel-based perceptron", "author": ["Orabona", "Francesco", "Keshet", "Joseph", "Caputo", "Barbara"], "venue": "In ICML, pp", "citeRegEx": "Orabona et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2008}, {"title": "Bounded kernel-based online learning", "author": ["Orabona", "Francesco", "Keshet", "Joseph", "Caputo", "Barbara"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Orabona et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2009}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "Rosenblatt,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt", "year": 1958}, {"title": "Online multiple kernel regression", "author": ["Sahoo", "Doyen", "Hoi", "Steven CH", "Li", "Bin"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Sahoo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sahoo et al\\.", "year": 2014}, {"title": "Large scale multiple kernel learning", "author": ["Sonnenburg", "S\u00f6ren", "R\u00e4tsch", "Gunnar", "Sch\u00e4fer", "Christin", "Sch\u00f6lkopf", "Bernhard"], "venue": "JMLR, 7:1531\u20131565,", "citeRegEx": "Sonnenburg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2006}, {"title": "Online passive-aggressive algorithms on a budget", "author": ["Wang", "Zhuang", "Vucetic", "Slobodan"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Breaking the curse of kernelization: Budgeted stochastic gradient descent for large-scale svm training", "author": ["Wang", "Zhuang", "Crammer", "Koby", "Vucetic", "Slobodan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Online multiple kernel similarity learning for visual search", "author": ["Xia", "Hao", "Hoi", "Steven CH", "Jin", "Rong", "Zhao", "Peilin"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Xia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2014}, {"title": "An extended level method for efficient multiple kernel learning", "author": ["Xu", "Zenglin", "Jin", "Rong", "King", "Irwin", "Lyu", "Michael R"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2008}, {"title": "Fast bounded online gradient descent algorithms for scalable kernel-based online learning", "author": ["Zhao", "Peilin", "Wang", "Jialei", "Wu", "Pengcheng", "Jin", "Rong", "Hoi", "Steven CH"], "venue": "In ICML,", "citeRegEx": "Zhao et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "1 INTRODUCTION Online Multiple Kernel Learning has been successfully used in many real-world applications including classification (Hoi et al., 2013; Jin et al., 2010), regression (Sahoo et al.", "startOffset": 131, "endOffset": 167}, {"referenceID": 6, "context": "1 INTRODUCTION Online Multiple Kernel Learning has been successfully used in many real-world applications including classification (Hoi et al., 2013; Jin et al., 2010), regression (Sahoo et al.", "startOffset": 131, "endOffset": 167}, {"referenceID": 12, "context": ", 2010), regression (Sahoo et al., 2014), similarity learning for multimedia search(Xia et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 16, "context": ", 2014), similarity learning for multimedia search(Xia et al., 2014), and structured prediction (Martins et al.", "startOffset": 50, "endOffset": 68}, {"referenceID": 8, "context": ", 2014), and structured prediction (Martins et al., 2010).", "startOffset": 35, "endOffset": 57}, {"referenceID": 7, "context": "In contrast to traditional online kernel methods (Kivinen et al., 2001) where a single kernel function is often chosen either manually or via some intensive cross-validation process, online multiple kernel learning algorithms learn multiple kernel classifiers and their combination simultaneously.", "startOffset": 49, "endOffset": 71}, {"referenceID": 0, "context": "Examples include Randomized Budget Perceptron (RBP) (Cavallanti et al., 2007), Forgetron (Dekel et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 2, "context": ", 2007), Forgetron (Dekel et al., 2005), Projectron (Orabona et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 9, "context": ", 2005), Projectron (Orabona et al., 2008; 2009), Budget Passive Aggressive (BPA) learning (Wang & Vucetic, 2010), Bounded Online Gradient Descent (BOGD) (Zhao et al.", "startOffset": 20, "endOffset": 48}, {"referenceID": 18, "context": ", 2008; 2009), Budget Passive Aggressive (BPA) learning (Wang & Vucetic, 2010), Bounded Online Gradient Descent (BOGD) (Zhao et al., 2012; Wang et al., 2012), among others.", "startOffset": 119, "endOffset": 157}, {"referenceID": 15, "context": ", 2008; 2009), Budget Passive Aggressive (BPA) learning (Wang & Vucetic, 2010), Bounded Online Gradient Descent (BOGD) (Zhao et al., 2012; Wang et al., 2012), among others.", "startOffset": 119, "endOffset": 157}, {"referenceID": 0, "context": ", RBP(Cavallanti et al., 2007)), while some other algorithms, despite being more effective, are often computationally very intensive for multiple kernel learning (e.", "startOffset": 5, "endOffset": 30}, {"referenceID": 9, "context": ", Projectron (Orabona et al., 2008)).", "startOffset": 13, "endOffset": 35}, {"referenceID": 1, "context": "In this paper, we investigate a novel framework of Budget Online Multiple Kernel Classification (BOMKC) and propose a new algorithm termed online Sparse Passive Aggressive learning (SPA) by extending the popular online Passive Aggressive (PA) technique (Crammer et al., 2006).", "startOffset": 253, "endOffset": 275}, {"referenceID": 13, "context": "The above convex optimization problem of regular batch MKL have been solved by different optimization schemes (Sonnenburg et al., 2006; Xu et al., 2008; G\u00f6nen & Alpayd\u0131n, 2011).", "startOffset": 110, "endOffset": 176}, {"referenceID": 17, "context": "The above convex optimization problem of regular batch MKL have been solved by different optimization schemes (Sonnenburg et al., 2006; Xu et al., 2008; G\u00f6nen & Alpayd\u0131n, 2011).", "startOffset": 110, "endOffset": 176}, {"referenceID": 5, "context": "To address the challenges faced by batch MKC methods, several algorithms attempt to solve the MKC problem in an online manner (Hoi et al., 2013; Sahoo et al., 2014) whose updating scheme usually consists of two steps.", "startOffset": 126, "endOffset": 164}, {"referenceID": 12, "context": "To address the challenges faced by batch MKC methods, several algorithms attempt to solve the MKC problem in an online manner (Hoi et al., 2013; Sahoo et al., 2014) whose updating scheme usually consists of two steps.", "startOffset": 126, "endOffset": 164}, {"referenceID": 1, "context": "Our update strategy of each single kernel classifier is generalized from the PA algorithm (Crammer et al., 2006), At the t-step, the online hypothesis will be updated: ft+1 = min f\u2208H\u03ba 1 2 \u2016f \u2212 ft\u20162H\u03ba + \u03b7lt(f) where \u03b7 > 0 and the hinge loss is adopted lt(f) = [1\u2212 ytf(xt)]+.", "startOffset": 90, "endOffset": 112}, {"referenceID": 11, "context": "Specially, we search for the best single kernel classifier from the set of our predefined 16 kernels using one random permutation of all the training examples and then apply the \u201cPerceptron\u201d algorithm (Rosenblatt, 1958) with the best kernel function.", "startOffset": 201, "endOffset": 219}, {"referenceID": 5, "context": "Our second group of compared algorithms are the Online Multiple Kernel Classification algorithms (Hoi et al., 2013) which achieved state-of-the-art performance on many benchmark datasets.", "startOffset": 97, "endOffset": 115}, {"referenceID": 0, "context": "Three variants of this algorithm are included: \u2022 \u201cOMKC(U)\u201d: the OMKC algorithm with a naive uniform combination; \u2022 \u201cOMKC(DD)\u201d: the OMKC algorithm with deterministic combination and update; \u2022 \u201cOMKC(SD)\u201d: OMKC with stochastic update and deterministic combination; Finally, to test the efficiency and effectiveness of our budget strategy, we also compare with multiple kernel classification algorithms whose component classifiers are updated by budget kernel learning algorithms including: \u2022 \u201cRBP\u201d: the Random Budget Perceptron algorithm (Cavallanti et al., 2007); \u2022 \u201cForgetron\u201d: the Forgetron algorithm that discards the oldest SV (Dekel et al.", "startOffset": 535, "endOffset": 560}, {"referenceID": 2, "context": ", 2007); \u2022 \u201cForgetron\u201d: the Forgetron algorithm that discards the oldest SV (Dekel et al., 2005); \u2022 \u201cBOGD\u201d: the Budget Online Gradient Descent algorithm (Zhao et al.", "startOffset": 76, "endOffset": 96}, {"referenceID": 18, "context": ", 2005); \u2022 \u201cBOGD\u201d: the Budget Online Gradient Descent algorithm (Zhao et al., 2012); \u2022 \u201cBPAS\u201d: the Budget Passive-aggressive algorithm (simple)(Wang & Vucetic, 2010).", "startOffset": 64, "endOffset": 83}, {"referenceID": 5, "context": "Second, we find that although using fewer SV\u2019s, the stochastic update algorithms OMKC(SD) can even achieve lower mistake rate compared with deterministic update OMKC(DD), which consists with the previous observations (Hoi et al., 2013).", "startOffset": 217, "endOffset": 235}], "year": 2017, "abstractText": "Online learning with multiple kernels has gained increasing interests in recent years and found many applications. For classification tasks, Online Multiple Kernel Classification (OMKC), which learns a kernel based classifier by seeking the optimal linear combination of a pool of single kernel classifiers in an online fashion, achieves superior accuracy and enjoys great flexibility compared with traditional single-kernel classifiers. Despite being studied extensively, existing OMKC algorithms suffer from high computational cost due to their unbounded numbers of support vectors. To overcome this drawback, we present a novel framework of Budget Online Multiple Kernel Learning (BOMKL) and propose a new Sparse Passive Aggressive learning to perform effective budget online learning. Specifically, we adopt a simple yet effective Bernoulli sampling to decide if an incoming instance should be added to the current set of support vectors. By limiting the number of support vectors, our method can significantly accelerate OMKC while maintaining satisfactory accuracy that is comparable to that of the existing OMKC algorithms. We theoretically prove that our new method achieves an optimal regret bound in expectation, and empirically found that the proposed algorithm outperforms various OMKC algorithms and can easily scale up to large-scale datasets.", "creator": "LaTeX with hyperref package"}}}