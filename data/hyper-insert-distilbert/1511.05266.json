{"id": "1511.05266", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2015", "title": "Semi-supervised Collaborative Ranking with Push at Top", "abstract": "existing collaborative asset ranking based recommender inspection systems tend to best perform relative best when there is enough precisely observed ratings for filtering each suspected user sample and the missed observation information is made completely at alt random. organizations under this setting recommender review systems can relatively properly formally suggest a general list of sample recommendations according ultimately to the potential user interests. however, when the observed ratings terms are extremely delicately sparse ( e. g. in implementing the informal case of cold - start users where indeed no rating data is available ), and information are technically not sampled based uniformly at exact random, traditional existing trusted ranking methods should fail to effectively use leverage sensitive side action information to precisely transduct all the knowledge from existing ratings to eliminate unobserved sample ones. we propose a formal semi - supervised collaborative ranking model, dubbed \\ \u30fb texttt { s $ ^ 2 $ cor }, to improve the quality of cold - start recommendation. \\ = texttt { \u00d7 s $ ^ ) 2 $ cor } mitigates repeating the aforementioned sparsity quality issue by deliberately leveraging side information about both naturally observed and likely missing consumption ratings separated by collaboratively learning specifically the ranking graph model. moreover this functionality enables it simplest to deal exclusively with documenting the case of missing observation data not at or random, collectively but helps to also effectively incorporate the least available side information expressed in transduction. we experimentally efficiently evaluated upon our proposed algorithm on adapting a number of challenging real - world datasets scenarios and tools compared extensively against previously state - of - the - art quantitative models critical for cold - start recommendation. we report these significantly the higher quality recommendations with our underlying algorithm compared, to much the contemporary state - of - the - art.", "histories": [["v1", "Tue, 17 Nov 2015 04:02:26 GMT  (25kb)", "http://arxiv.org/abs/1511.05266v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["iman barjasteh", "rana forsati", "abdol-hossein esfahanian", "hayder radha"], "accepted": false, "id": "1511.05266"}, "pdf": {"name": "1511.05266.pdf", "metadata": {"source": "CRF", "title": "Semi-supervised Collaborative Ranking with Push at Top", "authors": ["Iman Barjasteh", "Rana Forsati", "Abdol-Hossein Esfahanian", "Hayder Radha"], "emails": ["forsati@cse.msu.edu,", "esfahanian@cse.msu.edu,", "barjaste@msu.edu", "radha@msu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 1.\n05 26\n6v 1\n[ cs"}, {"heading": "1 Introduction", "text": "Due to the popularity and exponential growth of e-commerce and online streaming websites, a compelling demand has been created for efficient recommender systems to guide users\n\u2217These authors contributed equally to this work.\ntoward items of their interests (e.g. products, books, movies) [1]. In collaborative methods, either filtering or ranking, by relying on the low-rank assumption on the users\u2019 preferences, both users and items are mapped into a latent feature space based on partially observed ratings that are later used to make predictions. In collaborative filtering (CF) methods such as matrix factorization [15], where the aim is to accurately predict the ratings, the latent features are extracted in a way to minimize the prediction error measured in terms of popular performance measures such as root mean square error (RMSE). In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.\nRecent studies have demonstrated that CR models lead to significantly higher ranking accuracy over their traditional CF counterparts that optimize rating prediction. This is important considering the fact that what we really care in recommendation is not the actual values of ratings, but the order of items to be recommended to a specific user. Therefore, the error measures such as RMSE are often hopelessly insufficient, as their place equal emphasis on all the ratings. Among ranking models, the methods that mainly concentrate on the top of the list have received a considerable amount of attention, due to the higher probability of examining the top portion of the list of recommendations by users. Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].\nAlthough CR models for recommender systems has been studied extensively and some progress has been made, however, the state of affairs remains unsettled: the issue of handling cold-start items in ranking models and coping with not missing at random assumption of ratings are elusive open issues. First, in many real world applications, the rating data are very sparse (e.g., the density of the data is around 1% for many publicly available datasets) or for a subset of users or items the rating data is entirely missing (knows as cold-start user and cold-start item problem, respectively) [28]. Second, collaborative filtering and ranking models rely on the critical assumption that the missing ratings are sampled uniformly at random. However, in many real applications of recommender systems, this assumption is not believed to hold, as invariably some users are more active than others and some items are rated by many people while others are rarely rated [34]. These issues have been investigated in factorization based methods, nonetheless, it is not straightforward to adapt them to CR models and are left open [10]. Motivated by these challenges, we ask the following fundamental question in the context of collaborative ranking models:"}, {"heading": "Is it possible to effectively learn a collaborative ranking model in the presence of cold-start items/users that is robust to the sampling of observed ratings?", "text": "In this paper, we give an affirmative answer to the above question. In particular, we introduce a semi-supervised collaborative ranking model, dubbed S2COR , by leveraging side information about both observed and missing ratings in collaboratively learning the ranking\nmodel. In the learned model, unrated items are conservatively pushed after the relevant and before the irrelevant items in the ranked list of items for each individual user. This crucial difference greatly boosts the performance and limits the bias caused by learning only from sparse non-random observed ratings. We also introduce a graph regularization method to exploit the side information about users to overcome the cold-start users problem. In summary, the key features of S2COR are:\n\u2022 Inspired by recent developments in ranking at top [26, 3, 17], the proposed model is a collaborative ranking model that primarily focuses on the top of the recommendation list for each user. Moreover, in stark contrast to pairwise ranking models which have quadratic dependency on the number of items, the proposed ranking model has a linear dependency on the number of items, making it suitable for large-scale recommendation.\n\u2022 It leverages side information about items with both observed and missing ratings while collaboratively learning the ranking model, which enables it to effectively incorporate the available side information in knowledge transduction.\n\u2022 By incorporating the unrated items in ranking, it limits the bias caused by learning solely based on the observed ratings and consequently deals with the not missing at random issue of ratings.\n\u2022 It is also able to leverage the similarity information between users based on a graph regularization method to make high quality recommendations for users with few ratings or cold-start users without an rating information.\nTo build the intuition on how incorporating missing ratings in S2COR is beneficial in handling cold-start problem and mitigating data sparsity issue, we note that in many real world applications the available feedback on items is extremely sparse, and therefore the ranking models fail to effectively leverage the available side information in transdcuting the knowledge from existing ratings to unobserved ones. This problem becomes especially eminent in cases where surrogate ranking models such as pairwise models are used due to their computational virtues, where the unobserved ratings do not play any role in learning the model. As a result, by leveraging rich sources of information about all items, one can potentially bridge the gap between existing items and new items to overcome the cold-start problem.\nTurning to the non-random sampling issue of observed ratings, we note that the nonrandomness is observing the ratings creates a bias in learning the model that negatively impacts the future predictions and may degrade the resulting recommendation accuracy if ignored. Therefore, the nature of missing ratings has to be modeled precisely as to obtain correct results. To reduce the effect of bias, the proposed ranking model takes a conservative approach and pushes the items with unknown ratings to the middle of ranked list, i.e., after the relevant and before the irrelevant items. This is equivalent to assuming a prior about the unknown ratings which is believed to perform well as investigated in [12]. However, unlike [12], the proposed ranking idea is free of deciding an explicit value for missing ratings\nwhich makes it more valuable from a practical point of view. We conduct thorough experiments on real datasets and compare our results with the state-of-the-art models for cold-start recommendation to demonstrate the effectiveness of our proposed algorithm in recommendation at the top of the list and mitigating the data sparsity issue. Our results indicate that our algorithm outperforms other algorithms and provides recommendation with higher quality compared to the state-of-the-art methods.\nOrganization. This paper is organized as follows. We briefly review related work in Section 2. We establish the notation and formally define the problem in Section 3. In Section 4, we propose the semi-supervised collaborative ranking model with a push at the top of the list. Section 5 discusses efficient convex and non-convex optimization algorithms for optimization. Section 6 generalizes the proposed algorithm to leverage similarity information about users. We empirically evaluate the proposed method in Section 7, and conclude in Section 8."}, {"heading": "2 Related Work", "text": "There is now a vast body of literature on ranking models for recommendation, coping with non-random missing ratings and an even bigger body of literature on handling cold-start problem by leveraging side information; we restrict our literature review here to papers that are most directly related.\nCollaborative ranking for recommendation. The last few years have seen a resurgence in collaborative ranking centered around the technique of exploiting low-rank structures, an approach we take as well. Several approaches to CR have recently been proposed that are mainly inspired by the analogy between query-document relations in IR and useritem relations in recommender systems. The PMF-based approach [4] uses the latent representations produced by matrix factorization as user-item features and learns a ranking model on these features. CofiRank [37] learns latent representations that minimize a ranking-based loss instead of the squared error. ListRankMF [30] aims at minimizing the cross entropy between the predict item permutation probability and true item permutation probability. In [16] a method for Local Collaborative Ranking (LCR) where ideas of local low-rank matrix approximation were applied to the pairwise ranking loss minimization framework is introduced. In [35] a framework that allows for pointwise as well as listwise training with respect to various ranking metrics is proposed. Finally, [10] proposed a CR model build on the recent developments in ranking methods [3, 26] that focus on accuracy at top and proposed CR methods with p-push and infinite push norms. Incorporating side information in their model which is left as open issue was the main motivation of the current work.\nCold-start recommendation with side information. Due in part to its importance, there has been an active line of work to address difficulties associated with cold-start users and items, where a common theme among them is to exploit auxiliary information about\nusers or items besides the rating data that are usually available [31]. A feature based regression ranking model for predicting the values (rates) of user-item matrix in cold-start scenarios by leveraging all information available for users and items is proposed in [24]. The kernelized matrix factorization approach studied in [38], which incorporates the auxiliary information into the MF. In [27] joint factorization of the user-item and item-feature matrices by using the same item latent feature matrix in both decompositions is utilized. The FBSM model is introduced in [29], which learns factorized bilinear similarity model for new items, given the rating information as well as the features of these items. Recently, [5] proposed a decoupling approach to transduct knowledge from side information to rating prediction which is able to handle both cold-start items and users problems in factorization based models.\nRecommendation with not missing at random ratings. Substantial evidence for violations of the missing at random condition in recommender systems is reported in [20] and it has been showed that incorporating an explicit model of the missing data mechanism can lead to significant improvements in prediction performance.The first study of the effect of non-random missing data on collaborative ranking is presented in [19]. In [32] an EM algorithm to optimize in turn the factorization and the estimation of missing values. Recently, in [12] a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values is introduced. However their algorithm requires a careful setting of the prior rating to be practical."}, {"heading": "3 Preliminaries", "text": "In this section we establish the notation used throughout the paper and formally describe our problem setting.\nScalars are denoted by lower case letters and vectors by bold face lower case letters such as u. We use bold face upper case letters such as M to denote matrices. The Frobenius\nnorm of a matrix M \u2208 Rn\u00d7m is denoted by \u2016M\u2016F, i.e, \u2016M\u2016F = \u221a\u2211n\ni=1 \u2211m j=1 |Mij |2 and its\n(i, j)th entry is denoted by Ai,j. The trace norm of a matrix is denoted by \u2016M\u2016\u2217 which is defined as the sum of its singular values. The transpose of a vector and a matrix denoted by u\u22a4 and U\u22a4, respectively. We use [n] to denote the set on integers {1, 2, \u00b7 \u00b7 \u00b7 , n}. The set of non-negative real numbers is denoted by R+. The indicator function is denoted by I[\u00b7]. For a vector u \u2208 Rp we use \u2016u\u20161 = \u2211p i=1 |ui|, \u2016u\u20162 = ( \u2211p i=1 |ui|2) 1/2 , and \u2016u\u2016\u221e = max1\u2264i\u2264p ui to denote its \u21131, \u21132, and \u2113\u221e norms, respectively. The dot product between two vectors u and u\u2032 is denoted by either \u3008u,u\u2032\u3009 or u\u22a4u\u2032.\nIn collaborative filtering we assume that there is a set of n users U = {u1, \u00b7 \u00b7 \u00b7 , un} and a set of m items I = {i1, \u00b7 \u00b7 \u00b7 , im} where each user ui expresses opinions about a set of items. The rating information is summarized in an n \u00d7 m matrix R \u2208 {\u22121,+1, ?}n\u00d7m, 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 m where the rows correspond to the users and the columns correspond to the items and (p, q)th entry is the rate given by user up to the item iq. We note that the rating\nmatrix is partially observed and it is sparse in most cases. We are mainly interested in recommending a set of items for an active user such that the user has not rated these items before."}, {"heading": "4 Transductive Collaborating Ranking", "text": "We now turn our attention to the main thrust of the paper where we present our transductive collaborative ranking algorithm with accuracy at top by exploiting the features of unrated data. We begin with the basic formulation and then extend it to incorporate the unrated items. The pseudo-code of the resulting learning algorithm is provided in Algorithm 1."}, {"heading": "4.1 A basic formulation", "text": "We consider a ranking problem, where, given a set of users U and known user feedback on a set of items I, the goal is to generate rankings of unobserved items, adapted to each of the users\u2019 preferences. Here we consider the bipartite setting in which items are either relevant (positive) or irrelevant (negative). Many ranking methods have been developed for bipartite ranking, and most of them are essentially based on pairwise ranking. These algorithms reduce the ranking problem into a binary classification problem by treating each relevant/irrelevant instance pair as a single object to be classified [18].\nAs mentioned above, most research has concentrated on the rating prediction problem in CF where the aim is to accurately predict the ratings for the unrated items for each user. However, most applications that use CF typically aim to recommend only a small ranked set of items to each user. Thus rather than concentrating on rating prediction we instead approach this problem from the ranking viewpoint where the goal is to rank the unrated items in the order of relevance to the user. Moreover, it is desirable to concentrate aggressively on top portion of the ranked list to include mostly relevant items and push irrelevant items down from the top. Specifically, we propose an algorithm that maximizes the number of relevant items which are pushed to the absolute top of the list by utilizing the P-Norm Push ranking measure which is specially designed for this purpose [26] .\nFor simplicity of exposition, let us first consider the ranking model for a single user u. Let X+ = {x+1 , \u00b7 \u00b7 \u00b7 ,x+n+} and X\u2212 = {x\u22121 , \u00b7 \u00b7 \u00b7 ,x\u2212n\u2212} be the set of feature vectors of n+ relevant and n\u2212 irrelevant items to user u, respectively. We consider linear ranking functions where each item features vector x \u2208 Rd is mapped to a score w\u22a4x . The goal is to find parameters w for each user such that the ranking function best captures past feedback from the user. The goal of ranking is to maximize the number of relevant items ranked above the highestranking irrelevant item. We cast this idea for each user u individually into the following optimization problem:\nmin w\u2208Rd\n1\nn+\nn+\u2211\ni=1\nI [ \u3008w,x+i \u3009 \u2264 max\n1\u2264j\u2264n\u2212 \u3008w,x\u2212j \u3009\n] (1)\nAlgorithm 1 S2COR\n1: input: \u03bb \u2208 R+: the regularization parameter, and {\u03b7t}t\u22651: the sequence of scalar step sizes 2: Initialize W0 \u2208 Rn\u00d7d 3: Choose an appropriate step size 4: for t = 1, . . . , T do 5: Compute the sub-gradient of Gt \u2208 \u2202L(Wt) using Eq. (11) 6: [Ut,\u03a3t,Vt] \u2190 SVD(Wt\u22121 \u2212 1\u03b7t\u22121Gt)) 7: Wt \u2190 Ut [ \u03a3\u2212 \u03bb\n\u03b7t\u22121 I ] + V\u22a4t\n8: end for 9: output:\nwhere I[\u00b7] is the indicator function which returns 1 when the input is true and 0 otherwise, n+ and n\u2212 are the the number of relevant and irrelevant items to user u, respectively.\nLet us now derive the general form of our objective. We hypothesize that most users base their decisions about items based on a number of latent features about the items. In order to uncover these latent feature dimensions, we impose a low-rank constraint on the set of parameters for all users. To this end, let W = [w1,w2, \u00b7 \u00b7 \u00b7 ,wn]\u22a4 \u2208 Rn\u00d7d denote the matrix of all parameter vectors for n users. Let I+i \u2286 {1, 2, . . . , m} and I\u2212i \u2286 {1, 2, . . . , m} be the set of relevant and irrelevant items of ith user, respectively. The overall objective for all users is formulated as follows:\nF(W) = \u03bb\u2016W\u2016\u2217\n+\nn\u2211\ni=1\n  1 |I+i | \u2211\nj\u2208I+i\nI [ \u3008wi,xj\u3009 \u2264 max\nk\u2208I\u2212i\n\u3008wi,xk\u3009 ] , (2)\nwhere \u2016 \u00b7 \u2016\u2217 is the trace norm (also known as nuclear norm) which is the sum of the singular values of the input matrix.\nThe objective in Eq. (2) is composed of two terms. The first term is the regularization term and is introduced to capture the factor model intuition discussed above. The premise behind a factor model is that there is only a small number of factors influencing the preferences, and that a user\u2019s preference vector is determined by how each factor applies to that user. Therefore, the parameter vectors of all users must lie in a low-dimensional subspace. Trace-norm regularization is a widely-used and successful approach for collaborative filtering and matrix completion. The trace-norm regularization is well-known to be a convex surrogate to the matrix rank, and has repeatedly shown good performance in practice [33, 9]. The second term is introduced to push the relevant items of each user to the top of the list when ranked based on the parameter vector of the user and features of items.\nThe above optimization problem is intractable due to the non-convex indicator function. To design practical learning algorithms, we replace the indicator function in (2) with its convex surrogate. To this end, define the convex loss function \u2113 : R 7\u2192 R+ as \u2113(x) =\n[1 \u2212 x]+. This is the widely used hinge loss in SVM classification (see e.g., [7]) 1. This loss function reflects the amount by which the constraints are not satisfied. By replacing the non-convex indicator function with this convex surrogate leads to the following tractable convex optimization problem:\nF(W) = \u03bb\u2016W\u2016\u2217\n+\nn\u2211\ni=1\n  1 |I+i | \u2211\nj\u2208I+i\n\u2113 ( \u3008wi,xj\u3009 \u2212 \u2016X\u2212i wi\u2016\u221e\n)   (3)\nwhere X\u2212i = [x1, . . . ,xn\u2212 i ]\u22a4 is the matrix of features of n\u2212i irrelevant items in I\u2212i and \u2016 \u00b7 \u2016\u221e is the max norm of a vector."}, {"heading": "4.2 Semi-supervised collaborative ranking", "text": "In this part, we extend the proposed ranking idea to learn both from rated as well as unrated items. The motivation of incorporating unrated items comes from the following key observations. First, we note that commonly there is a small set of rated (either relevant or irrelevant) items for each user and a large number of unrated items. As it can be seen from Eq. (2), the unrated items do not play any role in learning the model for each user as the learning is only based on the pair of rated items. When the feature information for items is available, it would be very helpful if one can leverage such unrated items in the learning-torank process to effectively leverage the available side information. By leveraging both types of rated and unrated items, we can compensate for the lack of rating data. Second, the non-randomness in observing the observed ratings creates a bias in learning the model that may degrade the resulting recommendation accuracy. Therefore, finding a precise model to reduce the effect of bias introduced by non-random missing ratings seems essential.\nTo address these two issues, we extend the basic formulation in Eq. (2) to incorporate items with missing ratings in ranking of items for individual users. A conservative solution is to push the items with unknown ratings to the middle of ranked list, i.e., after the relevant and before the irrelevant items. To do so, let I\u25e6i = I \\ ( I+i \u222a I\u2212i ) denote the set of items unrated for user i \u2208 U . We introduce two extra terms in the objective in Eq. (2) to push the unrated items Ii\u25e6 below the relevant items and above the irrelevant items, which yilelds\n1We note that other convex loss functions such as exponential loss \u2113(x) = exp(\u2212x), and logistic loss \u2113(x) = log(1 + exp(\u2212x)) also can be used as the surrogates of indicator function, but for the simplicity of derivation we only consider the hinge loss here.\nthe following objective:\nL(w) = 1|I+i | \u2211\ni\u2208I+i\n\u2113 ( \u3008w,xi\u3009 \u2264 max\nj\u2208I\u2212i\n\u3008w,xj\u3009 )\n+ 1 |I+i | \u2211\ni\u2208I+ i\n\u2113 ( \u3008w,xi\u3009 \u2264 max\nj\u2208I\u25e6i\n\u3008w,xj\u3009 )\n+ 1 |I\u25e6i | \u2211\ni\u2208I\u25e6i\n\u2113 ( \u3008w,xi\u3009 \u2264 max\nj\u2208I\u2212i\n\u3008w,xj\u3009 )\n(4)\nEquipped with the objective of individual users, we now turn to the final collaborating ranking objective as:\nF(W) = \u03bb\u2016W\u2016\u2217\n+\nn\u2211\ni=1\n  1 |I+i | \u2211\nj\u2208I+i\n\u2113 ( \u3008wi,xj\u3009 \u2212 \u2016X\u2212i wi\u2016\u221e\n)  \n+ n\u2211\ni=1\n  1 |I+i | \u2211\nj\u2208I+i\n\u2113 (\u3008wi,xj\u3009 \u2212 \u2016X\u25e6iwi\u2016\u221e)\n \n+\nn\u2211\ni=1\n  1 |I\u25e6i | \u2211\nj\u2208I\u25e6i\n\u2113 ( \u3008wi,xj\u3009 \u2212 \u2016X\u2212i wi\u2016\u221e )  ,\n(5)\nwhere X\u25e6i = [x1, . . . ,xn\u25e6i ] \u22a4 is the matrix of n\u25e6i unrated items in I\u25e6i .\nRemark 4.1 We emphasize that beyond the accuracy considerations of push norm at top of the list, the push norm has a clear advantage to the pairwise ranking models from a computational point of view. In particular, the push norm has a linear O(m) dependency on the number of items which is quadratic O(m2) for pairwise ranking models."}, {"heading": "5 The Optimization", "text": "We now turn to solving the optimization problem in (5). We start by discussing a gradient descent method with shrinkage operator followed by its accelerated version, and then propose a non-convex formulation with alternative minimization for more effective optimization of objective in S2COR."}, {"heading": "5.1 Gradient descent with shrinkage operator", "text": "Due to the presence of trace norm of the parameters matrix, this objective function falls into the general category of composite optimization, which can be solved by stochastic gradient\nor gradient descent methods. In this part we propose a projected gradient decent method to solve the optimization problem. First we write the objective as:\nmin W\u2208Rn\u00d7d\nF(W) = \u03bb\u2016W\u2016\u2217 + L(W), (6)\nwhere L(W) = \u2211ni=1 L(wi). A simple way to solving the above optimization problem is gradient descent algorithm [23], which needs to evaluate the gradient of objective at each iteration. To deal with the non-smooth trace norm \u2016W\u2016\u2217 in the objective, we first note that the optimization problem in Eq. (6) can be reformulated under the framework of proximal regularization or composite gradient mapping [23]. By taking advantage of the composite structure it is possible to retain the same convergence rates of the gradient method for the smooth optimization problems. In particular, the optimization problem in (6) can be solved iteratively by:\nWt = argmin W\nL(Wt\u22121) + tr ( (W \u2212Wt\u22121)\u22a4\u2207L(Wt\u22121) )\n+ \u03b7t 2 \u2016W \u2212Wt\u22121\u20162F + \u03bb\u2016W\u2016\u2217,\n(7)\nwhere {\u03b7t}t\u22651 is a sequence of scalar step sizes and tr(\u00b7) is the trace of input matrix.\nBy ignoring the constant terms, Eq. (7) can also be rewritten as:\n\u03b7t 2 \u2225\u2225\u2225\u2225W \u2212 ( Wt\u22121 \u2212 1 \u03b7t \u2207L(Wt\u22121) )\u2225\u2225\u2225\u2225 2\nF\n+ \u03bb\u2016W\u2016\u2217. (8)\nWe use the singular value shrinkage operator introduced in [8] to find the optimal solution to Eq. (8). To this end, consider the singular value decomposition (SVD) of a matrix M \u2208 Rn\u00d7d of rank r as M = U\u03a3V\u2217, \u03a3 = diag({\u03c3i}1\u2264i\u2264r), where U and V are respectively n\u00d7 r and d\u00d7 r matrices with orthonormal columns, and the singular values \u03c3i are positive. For a scalar \u03c4 \u2208 R+, define the singular value shrinkage operator P\u03c4 as:\nP\u03c4 (M) := UP\u03c4 (\u03a3)V \u2217, P\u03c4 (\u03a3) = diag([\u03c3i \u2212 \u03c4 ]+}), (9)\nwhere [x]+ is the positive part of x, namely, [x]+ = max(0, x). The shrinkage operator basically applies a soft-thresholding rule to the singular values of M, effectively shrinking these towards zero.\nTheorem 5.1 (Theorem 2.1, [8]) For each \u03c4 \u2265 0 and W \u2208 Rn\u00d7d, the singular value shrinkage operator (9) obeys\nP\u03c4 (W) = argmin X\n{ 1\n2 \u2016X\u2212W\u20162F + \u03c4\u2016X\u2016\u2217\n} . (10)\nThe above theorem shows that the singular value shrinkage operator of a matrix is the solution to Eq. (10). Equipped with this result, the optimization problem in Eq. (8) can be\nsolved by first computing the SVD of updated Wt\u22121 and then applying soft thresholding on the singular values as:\nWt = P \u03bb \u03b7t\u22121\n( Wt\u22121 \u2212 1\n\u03b7t\u22121 \u2207L(Wt\u22121)\n)\nNow we need to evaluate the gradient of L(W) at Wt\u22121. The convex function L(W) is not differentiable, so we use its subgradient in updating the solutions which can be computed for ith parameter vector wi, as follows:\ngi = \u2202L/\u2202wi\n= \u2211\nj\u2208I+i\nI [ \u3008wi,xj\u3009 \u2212 \u2016X\u2212i wi\u2016\u221e \u2264 1 ] ( \u2202\u2016X\u2212i wi\u2016\u221e \u2212 xj )\n+ \u2211\nj\u2208I+i\nI [\u3008wi,xj\u3009 \u2212 \u2016X\u25e6iwi\u2016\u221e \u2264 1] (\u2202\u2016X\u25e6iwi\u2016\u221e \u2212 xj)\n+ \u2211\nj\u2208I\u25e6i\nI [ \u3008wi,xj\u3009 \u2212 \u2016X\u2212i wi\u2016\u221e \u2264 1 ] ( \u2202\u2016X\u2212i wi\u2016\u221e \u2212 xj ) ,\n(11)\nwhere \u2202\u2016X\u2212i wi\u2016\u221e is the subdifferential of the function \u2016X\u2212i wi\u2016\u221e at point wi. Since the subdifferential of the maximum of functions is the convex hull of the union of subdifferentials of the active functions at point w [22], we have:\n\u2202\u2016X\u2212i wi\u2016\u221e = \u2202 max 1\u2264j\u2208I\u2212i \u3008wi,xj\u3009\n= conv { xj|\u3008wi,xj\u3009 = \u2016X\u2212i wi\u2016\u221e, j \u2208 I\u2212j } .\nThen, G = [g1, g2, . . . , gn] \u22a4 \u2208 Rn\u00d7d is a subgradient at W, i.e. G \u2208 \u2202L(W).\nRemark 5.2 We note that here, for the ease of exposition, we only consider the nondifferentiable convex hinge loss as the surrogate of non-convex due to its computational virtues. However, by using other smooth convex surrogate losses such as smoothed hinge loss or exponential loss, one can apply the accelerated gradient descent methods [22] to solve the optimization problem which results in significantly faster convergence rate compared to the naive gradient descent (i.e, O(1/ \u221a \u01eb) convergence rate for accelerated method compared to the O(1/\u01eb2) rate for gradient descent for non-smooth optimization, where \u01eb is the target accuracy)."}, {"heading": "5.2 Efficient optimization by dropping convexity", "text": "The main computational cost in each iteration of the S2COR algorithm lies in computing the SVD decomposition of Wk. An alternative cheaper way to solving the optimization problem in Eq. (6) is as follows. For a fixed rank of the target parameter matrix W, say k, one\ncan decompose it as W = UV. From the equivalence relation between trace norm and the Frobenius of its components in decomposition,\n\u2016W\u2016\u2217 = min U\u2208Rn\u00d7k ,V\u2208Rm\u00d7k\nW=UV\u22a4\n1\n2\n( \u2016U\u20162F + \u2016V\u20162F )\nwe can write the objective in terms of these low rank components as:\nmin U\u2208Rn\u00d7k ,V\u2208Rm\u00d7k\n\u03bb\n2\n( \u2016U\u20162F + \u2016V\u20162F ) + L(UV), (12)\nThese factored optimization problem does not have the explicit trace norm regularization. However, the new formulation is non-convex and potentially subject to stationary points that are not globally optimal. However, despite its non-convexity, the formulation in Eq. (12) is competitive as compared to trace-norm minimization, while scalability is much better. In particular, the objective is not jointly convex in both U and V but it is convex in each of them fixing the other one. Therefore, to find a local solution one can stick to the standard gradient descent method to find a solution in an iterative manner as follows:\nUt+1 \u2190 (1\u2212 \u03bb\u03b7t)Ut \u2212 \u03b7t\u2207UL|U=Ut,V=Vt , Vt+1 \u2190 (1\u2212 \u03bb\u03b7t)Vt \u2212 \u03b7t\u2207VL|U=Ut,V=Vt .\nRemark 5.3 It is remarkable that the large number of users or items may cause computational problems in solving the optimization problem using GD method. The reason is essentially the fact that computing the gradient at each iteration requires to go through all the users and compute the gradient for pair of items. To alleviate this problem one can utilize stochastic gradient method [21] to solve the optimization problem. The main idea is to choose a fixed subset of pairs for gradient computation instead of all pairs at each iteration or a sample a user at random for gradient computation instead of including all users. We note that this strategy generates unbiased estimates of the true gradient and makes each iteration of algorithm computationally more efficient compared to the full gradient counterpart."}, {"heading": "6 Regularization by Exploiting Similarity of Users", "text": "In many recommender systems, in addition to handling cold-start items, it would be beneficial to make high quality recommendations to new users. The new user cold start issue represents a serious problem in recommender systems as it can lead to the loss of new users who decide to stop using the system due to the lack of accuracy in the recommendations received in that first stage in which they have not yet cast a significant number of ratings with which to feed the recommender system\u2019s collaborative filtering core.\nThe above formulation estimates a parameter vector separately for each user regardless of potential similarities across users. In this section we generalize the proposed ranking model\nAlgorithm 2 S2COR+\n1: input: \u03b3, \u03bb \u2208 R+: the regularization parameters, {\u03b7t}t\u22651: the sequence of scalar step sizes, and S \u2208 Rn\u00d7n: the similarity matrix of usets 2: Initialize W0 \u2208 Rn\u00d7d 3: Choose an appropriate step size 4: for t = 1, . . . , T do 5: Compute the sub-gradient of Gt \u2208 \u2202L(Wt) using Eq. (11) 6: Compute G\u0302t = Gt + 2\u03b3XX\n\u22a4WL 7: [Ut,\u03a3t,Vt] \u2190 SVD(Wt\u22121 \u2212 1\u03b7t\u22121 G\u0302t)) 8: Wt \u2190 Ut [ \u03a3\u2212 \u03bb\n\u03b7t\u22121 I ] + V\u22a4t\n9: end for 10: output:\nto the setting where the similarity information between users is available. Let S \u2208 Rn\u00d7n be the similarity matrix of users which is inferred from the side information about users such as social relations between them or explicit users\u2019 features. Let D be the diagonal matrix with Dii = \u2211n j=1Wij, and L = D \u2212 S be the Laplacian matrix. It is natural to require the similar users in the matrix S have similarity evaluation on rating the items [6]. Thus, the new regularization on the parameters of users W using the similarity matrix S can be achieved by minimizing:\n1\n2\nm\u2211\ni=1\n( n\u2211\nj,k=1\nSjk (\u3008wj,xi\u3009 \u2212 \u3008wk,xi\u3009)2 )\n= 1\n2\nm\u2211\ni=1\n( x\u22a4i wj( \u2211\nk\nSjk)w \u22a4 j xi \u2212\n\u2211\nj,k\nx\u22a4i wjSjkw \u22a4 k xi\n)\n= \u2211\ni\n( \u2211\nj\nx\u22a4i wjSjjw \u22a4 j xi \u2212\n\u2211\nj,k\nx\u22a4i wjSijw \u22a4 k xi\n)\n= \u2211\ni\nx\u22a4i ( WDW\u22a4 \u2212WSW\u22a4 ) xi\n= \u2211\ni\nx\u22a4i WLW \u22a4xi = tr\n( X\u22a4WLW\u22a4X ) .\nBy plugging the above regularization term in the Eq. (6), we obtain the following optimization problem:\nF(W) = \u03bb\u2016W\u2016\u2217 + L(W) + \u03b3 tr ( X\u22a4WLW\u22a4X ) ,\nThe above optimization problem can also be solved using the optimization procedure discussed before. The only difference is the gradient computation step which now has an extra term due the introduction of user\u2019s similarity regularization. Specifically, we have the new gradient G\u0302t computed as:\nG\u0302t = Gt + 2\u03b3XX \u22a4WL,\nwhich replaces the Gt in Algorithm 1. The resulting algorithm, dubbed S 2 COR+, is detailed in Algorithm 2."}, {"heading": "7 Experiments", "text": "In this section, we conduct exhaustive experiments to demonstrate the merits and advantages of the proposed algorithm. We conduct our experiments on three well-known datasets MovieLens, Amazon and CiteULike. We investigate how the proposed S2COR performs in comparison to the state-of-the-art methods. In the following, first we introduce the datasets that we use in our experiments and then the metrics that we employ to evaluate the results, followed by our detailed experimental results on the real datasets.\nIn the following subsections, we intend to answer these key questions:\n\u2022 Ranking versus rating: How does learning optimization for a ranking based loss function affect the performance of recommending versus the square loss function?\n\u2022 Employing missing ratings: How does employing the missing ratings could help in making more accurate recommendations?\n\u2022 Dealing with cold-start items: How does the proposed algorithm, with incorporating side information of items, perform in comparison to the state-of-the-art algorithms to deal with cold-start items?"}, {"heading": "7.1 Datasets", "text": "We use the following well known datasets to evaluate the performance of S2COR:\n\u2022 ML-IMDB. We used ML-IMDB which is a dataset extracted from the IMDB and the MovieLens 1M datasets by mapping the MovieLens and IMDB and collecting the movies that have plots and keywords. The rating values are 10 discrete numbers ranging from 1 to 10 and the rating were made binary by treating all the ratings greater than 5 as +1 and below 5 as \u22121.\n\u2022 Amazon. We used the dataset of best-selling books and their ratings in Amazon. Each book has a one or two paragraphs of textual description, which has been used to have a set of features of the books. Ratings can be integers numbers from 1 to 5. The ratings were also made binary by treating all the ratings greater or equal to 3 as +1 and below 3 as \u22121.\nFor all above datasets, the description about the items were tokenized and after removing the stop words, the rest of the words were stemmed. Then those words that have been appeared in less than 20 items and more that 20% of the items were also removed [29]. At the end, the TF-IDF was applied on the remaining words and the TF-IDF scores represented the features of the items. The statistics of the datasets are given in Table 1. As it is shown in Table 1, all these three datasets have high dimensional feature space."}, {"heading": "7.2 Metrics", "text": "We adopt the widely used metrics, Discounted Cumulative Gain at n and Recall at n, for assessing the performance of our and baseline algorithms. For each user u, given an item i, let sk be the relevance score of the item ranked at position k, where sk = 1 if the item is relevant to the user u and sk = 0 otherwise. Now, given the list of top-n item recommendations for user u, Discounted Cumulative Gain at n, is defined as:\nDCGu@n = s1 + n\u2211\nk=2\nsk log2(k)\nIf we divide the DCGu@n by its maximum value, we get the NDCGu@n value. Given the list of top-n item recommendations for each user u, Recall at n will count the number of relevant items appeared in the recommendation list divided by the size of the list. Recall at n is defined as:\nRECu@n = |{relevant items to u} \u2229 {top-n recommended items}|\n|{top-n recommended items}| DCG@n, NDCGu@n and REC@n will be computed for each user and then will be averaged over all users."}, {"heading": "7.3 Methodology", "text": "Given the partially observed rating matrix, we transformed the observed ratings of all datasets from a multi-level relevance scale to a two-level scale (+1,\u22121) while 0 is considered for unobserved ratings. We randomly selected 60% of the observed ratings for training and 20% for validation set and consider the remaining 20% of the ratings as our test set. To better evaluate the results, we performed a 3-fold-cross validation and reported the average value for our results."}, {"heading": "7.4 Baseline Algorithms", "text": "The proposed S2COR algorithm is compared to the following algorithms:\n\u2022 Matrix Factorization (MF) [33]: Is a matrix completion method that factorizes the incomplete observed matrix and completes the matrix using the unveiled latent features.\n\u2022 Matrix Factorization with Side Information (KPMF) [38]: Is a matrix completion based algorithm, which incorporates external side information of the users or items into the matrix factorization process. It imposes a Gaussian Process prior over all rows of the matrix, and the learned model explicitly captures the underlying correlation among the rows.\n\u2022 Decoupled Completion and Transduction (DCT) [5]: Is a matrix factorization based algorithm that decouples the completion and transduction stages and exploits the similarity information among users and items to complete the (rating) matrix.\n\u2022 Feature Based Factorized Bilinear Similarity Model (FBS) [29]: This algorithm uses bilinear model to capture pairwise dependencies between the features. In particular, this model accounts for the interactions between the different item features.\n\u2022 Collaborative User-specific Feature-based Similarity Models (CUFSM): By using the history of ratings for users, it learns personalized user model across the dataset. This method is one of the best performing collaborative latent factor based model [13].\n\u2022 Regression based Latent Factor Model (RLF):2 This method incorporates the features of items in factorization process by transforming the features to the latent space using linear regression [2]. If the learning method is Markov Chain Monte Carlo, we name it RLF-MCMC.\n\u2022 Cosine Similarity Based Recommender (CSR): Using the similarity between features of items, the preference score of a user on an item will be estimated.\nWe would like to mention that as baseline algorithms we only consider state-of-the art methods that are able to exploit the side information about items.\n2The implementation of this method is available in LibFM library [25].\n7.5 S2COR vs. rating\nMany different algorithms are trying to provide recommendations to users such that the predicted rating values be very close to the actual rates that users would provide. These algorithms try to minimize the error between the predicted values and actual rating values by minimizing Mean Squared Error (MAE) or Root Mean Square Error (RMSE) or etc. Then, due to the fact that users tend to only care about the top of their recommendation list, predicting a ranking of items of interest instead of ratings became the main focus of recent works [10]. In this section we compare the results of S2COR with those stateof-the-art algorithms that try to predict ratings for unrated items. Among the state-ofthe-art algorithms, we chose a diverse set of algorithms, which are Matrix Factorization, Matrix Factorization with Side Information, Decoupled Completion and Transduction and Regression Based Latent Factor Model. Table 2 shows the NDCG value of top 10 items of recommendation list. It shows that S2COR outperformed all other rating prediction based algorithms in terms of NDCG measure. The results confirm the effectiveness of providing the ranks of items rather than their ratings."}, {"heading": "7.6 Robustness to not missing at random ratings", "text": "In this section we compare the effect of incorporating the unobserved ratings in our learning in comparison with excluding them from our learning. Most of the methods in the literature ignore the unobserved ratings and train their model only base on observed ratings. By incorporating the unrated items in ranking, our method can limit the bias caused by learning solely based on the observed ratings and consequently deals with the not missing at random issue of ratings. Table 3 shows results of comparing these two scenarios for S2COR on MLIMDB. In order to see the difference between these two scenarios, we considered 70% of the ratings for training and 30% for test to have more ground truth for our testing. Table 3 shows the NDCG@5, 10,15 and 20 for both scenarios and it shows that incorporating the unobserved ratings causes to improve the accuracy of recommendation list. Hence, the NDCG values for top 5, 10, 15 and 20 items improved when unrated items were included as part of the training process."}, {"heading": "7.7 Dealing with cold-start items", "text": "We now turn to evaluating the effectiveness of S2COR for cold-start recommendation. To do so, we randomly selected 60% of the items as our training items and 20% for validation set and considered the remaining 20% of the items as our test set. In this scenario, baseline algorithms that are used for comparison are CSR, FBS, CUFSM and RLF. For the experiments, we used ML-IMDB, Amazon and CiteULike datasets. Table 4 shows the measurement results of applying mentioned algorithms on these datasets. For each test, the parameters\u2019 values producing the best ranking on the validation set were selected to be used and reported. As it can be seen from the results in Table 4, the proposed S2COR algorithm outperformed all other baseline algorithms and provided a recommendations with higher quality in comparison to other methods. We can also see from the results of Table 4 that for the ML-IMDB dataset, the improvement in terms of REC@10 is significant compared to other datasets. Since the density of this dataset is much higher than other two datasets, this observation indicates that our method is more effective in utilizing side information compared to other methods. These results demonstrate the effectiveness of S2COR in comparison with other state-of-the-art algorithms. S2COR was able to outperform other state-of-the-art algorithms by considering the missing data and focusing on top of the recommendation list for cold-start items."}, {"heading": "8 Conclusions", "text": "In this paper we introduced a semi-supervised collaborative ranking model by leveraging side information about both observed and missing ratings in collaboratively learning the ranking model. In the learned model, unrated items are conservatively pushed after the relevant and before the relevant items in the ranked list of items for each individual user. This crucial difference greatly boosts the performance and limits the bias caused by learning only from sparse non-random observed ratings. The proposed algorithm is compared with seven baseline algorithms on three real world datasets that demonstrated the effectiveness of proposed algorithm in addressing cold-start problem and mitigating the data sparsity problem, while being robust to sampling of missing ratings.\nThis work leaves few interesting directions as future work. First, we would like to investigate the performance of the proposed S2COR algorithm when side information about\nusers is also available using the graph regularization idea discussed in Section 6. Second, we would like to empirically evaluate the performance of the optimization method derived by dropping the convexity in future. Also, we have largely ignored the case of differentiable smooth surrogate convex loss functions in this work and it would be interesting to consider smooth alternatives and apply the accelerated optimization methods for faster convergence. Moreover, the scalability analysis of of proposed algorithm on large datasets using stochastic optimization methods is also worthy of investigation. Finally, we believe there are still many open questions relating to non-random nature of rating information in many real applications."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 17(6):734\u2013749", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Regression-based latent factor models", "author": ["D. Agarwal", "B.-C. Chen"], "venue": "SIGKDD, pages 19\u201328. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "The infinite push: A new support vector ranking algorithm that directly optimizes accuracy at the absolute top of the list", "author": ["S. Agarwal"], "venue": "SDM, pages 839\u2013850. SIAM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Collaborative ranking", "author": ["S. Balakrishnan", "S. Chopra"], "venue": "ACM WSDM, pages 143\u2013152. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Cold-start item and user recommendation with decoupled completion and transduction", "author": ["I. Barjasteh", "R. Forsati", "F. Masrour", "A.-H. Esfahanian", "H. Radha"], "venue": "Proceedings of the 9th ACM Conference on Recommender Systems, pages 91\u201398. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "The Journal of Machine Learning Research, 7:2399\u20132434", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J. Burges"], "venue": "Data mining and knowledge discovery, 2(2):121\u2013167", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization, 20(4):1956\u20131982", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "The power of convex relaxation: Near-optimal matrix completion", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "Information Theory, IEEE Transactions on, 56(5):2053\u20132080", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative ranking with a push at the top", "author": ["K. Christakopoulou", "A. Banerjee"], "venue": "WWW, pages 205\u2013215. International WorldWide Web Conferences Steering Committee", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Performance of recommender algorithms on top-n recommendation tasks", "author": ["P. Cremonesi", "Y. Koren", "R. Turrin"], "venue": "ACM RecSys, pages 39\u201346. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamic matrix factorization with priors on unknown values", "author": ["R. Devooght", "N. Kourtellis", "A. Mantrach"], "venue": "ACM SIGKDD, pages 189\u2013198. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "User-specific feature-based similarity models for top-n recommendation of new items", "author": ["A. Elbadrawy", "G. Karypis"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 6(3):33", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Ir evaluation methods for retrieving highly relevant documents", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM SIGIR, pages 41\u201348. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, (8):30\u201337", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Local collaborative ranking", "author": ["J. Lee", "S. Bengio", "S. Kim", "G. Lebanon", "Y. Singer"], "venue": "Proceedings of the 23rd international conference on World wide web, pages 85\u201396. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Top rank optimization in linear time", "author": ["N. Li", "R. Jin", "Z.-H. Zhou"], "venue": "Advances in Neural Information Processing Systems, pages 1502\u20131510", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning to rank for information retrieval", "author": ["T.-Y. Liu"], "venue": "Foundations and Trends in Information Retrieval, 3(3):225\u2013331", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative prediction and ranking with non-random missing data", "author": ["B.M. Marlin", "R.S. Zemel"], "venue": "RecSys, pages 5\u201312. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative filtering and the missing at random assumption", "author": ["B.M. Marlin", "R.S. Zemel", "S.T. Roweis", "M. Slaney"], "venue": "UAI, pages 267\u2013275", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization, 19(4):1574\u20131609", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Introductory lectures on convex optimization", "author": ["Y. Nesterov"], "venue": "volume 87. Springer Science & Business Media", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Gradient methods for minimizing composite functions", "author": ["Y. Nesterov"], "venue": "Mathematical Programming, 140(1):125\u2013161", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Pairwise preference regression for cold-start recommendation", "author": ["S.-T. Park", "W. Chu"], "venue": "RecSys, pages 21\u201328. ACM", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Factorization machines with libfm", "author": ["S. Rendle"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 3(3):57", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "The p-norm push: A simple convex ranking algorithm that concentrates at the top of the list", "author": ["C. Rudin"], "venue": "The Journal of Machine Learning Research, 10:2233\u20132271", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Item cold-start recommendations: learning local collective embeddings", "author": ["M. Saveski", "A. Mantrach"], "venue": "RecSys, pages 89\u201396. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Methods and metrics for cold-start recommendations", "author": ["A.I. Schein", "A. Popescul", "L.H. Ungar", "D.M. Pennock"], "venue": "SIGIR, pages 253\u2013260. ACM", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Feature-based factorized bilinear similarity model for cold-start top-n item recommendation", "author": ["M. Sharma", "J. Zhou", "J. Hu", "G. Karypis"], "venue": "SDM", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "List-wise learning to rank with matrix factorization for collaborative filtering", "author": ["Y. Shi", "M. Larson", "A. Hanjalic"], "venue": "ACM RecSys, pages 269\u2013272. ACM", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative filtering beyond the user-item matrix: A survey of the state of the art and future challenges", "author": ["Y. Shi", "M. Larson", "A. Hanjalic"], "venue": "ACM Computing Surveys (CSUR), 47(1):3", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "One-class matrix completion with low-density factorizations", "author": ["V. Sindhwani", "S.S. Bucak", "J. Hu", "A. Mojsilovic"], "venue": "ICDM, pages 1055\u20131060. IEEE", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T.S. Jaakkola"], "venue": "Advances in neural information processing systems, pages 1329\u20131336", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Training and testing of recommender systems on data missing not at random", "author": ["H. Steck"], "venue": "KDD, pages 713\u2013722. ACM", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Gaussian ranking by matrix factorization", "author": ["H. Steck"], "venue": "Proceedings of the 9th ACM Conference on Recommender Systems, pages 115\u2013122. ACM", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative ranking with 17 parameters", "author": ["M. Volkovs", "R.S. Zemel"], "venue": "Advances in Neural Information Processing Systems, pages 2294\u20132302", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Maximum margin matrix factorization for collaborative ranking", "author": ["M. Weimer", "A. Karatzoglou", "Q.V. Le", "A. Smola"], "venue": "NIPS", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernelized probabilistic matrix factorization: Exploiting graphs and side information", "author": ["T. Zhou", "H. Shan", "A. Banerjee", "G. Sapiro"], "venue": "SDM, volume 12, pages 403\u2013414. SIAM", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "products, books, movies) [1].", "startOffset": 25, "endOffset": 28}, {"referenceID": 14, "context": "In collaborative filtering (CF) methods such as matrix factorization [15], where the aim is to accurately predict the ratings, the latent features are extracted in a way to minimize the prediction error measured in terms of popular performance measures such as root mean square error (RMSE).", "startOffset": 69, "endOffset": 73}, {"referenceID": 14, "context": "In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.", "startOffset": 62, "endOffset": 78}, {"referenceID": 9, "context": "In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.", "startOffset": 62, "endOffset": 78}, {"referenceID": 35, "context": "In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.", "startOffset": 62, "endOffset": 78}, {"referenceID": 10, "context": "In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.", "startOffset": 62, "endOffset": 78}, {"referenceID": 13, "context": "In spark contrast to CF, in collaborating ranking (CR) models [15, 10, 36, 11], where the goal is to rank the unrated items in the order of relevance to the user, the popular ranking measures such as as discounted cumulative gain (DCG), normalized discounted cumulative gain (NDCG), and average precision (AP) [14] are often employed to collaboratively learn a ranking model for the latent features.", "startOffset": 310, "endOffset": 314}, {"referenceID": 25, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 82, "endOffset": 97}, {"referenceID": 2, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 82, "endOffset": 97}, {"referenceID": 9, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 82, "endOffset": 97}, {"referenceID": 15, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 82, "endOffset": 97}, {"referenceID": 34, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 195, "endOffset": 203}, {"referenceID": 9, "context": "Therefore, the introduction of ranking metrics such as push norm or infinite norm [26, 3, 10, 16], sparked a widespread interest in CR models and has been proven to be more effective in practice [35, 10].", "startOffset": 195, "endOffset": 203}, {"referenceID": 27, "context": ", the density of the data is around 1% for many publicly available datasets) or for a subset of users or items the rating data is entirely missing (knows as cold-start user and cold-start item problem, respectively) [28].", "startOffset": 216, "endOffset": 220}, {"referenceID": 33, "context": "However, in many real applications of recommender systems, this assumption is not believed to hold, as invariably some users are more active than others and some items are rated by many people while others are rarely rated [34].", "startOffset": 223, "endOffset": 227}, {"referenceID": 9, "context": "These issues have been investigated in factorization based methods, nonetheless, it is not straightforward to adapt them to CR models and are left open [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 25, "context": "In summary, the key features of SCOR are: \u2022 Inspired by recent developments in ranking at top [26, 3, 17], the proposed model is a collaborative ranking model that primarily focuses on the top of the recommendation list for each user.", "startOffset": 94, "endOffset": 105}, {"referenceID": 2, "context": "In summary, the key features of SCOR are: \u2022 Inspired by recent developments in ranking at top [26, 3, 17], the proposed model is a collaborative ranking model that primarily focuses on the top of the recommendation list for each user.", "startOffset": 94, "endOffset": 105}, {"referenceID": 16, "context": "In summary, the key features of SCOR are: \u2022 Inspired by recent developments in ranking at top [26, 3, 17], the proposed model is a collaborative ranking model that primarily focuses on the top of the recommendation list for each user.", "startOffset": 94, "endOffset": 105}, {"referenceID": 11, "context": "This is equivalent to assuming a prior about the unknown ratings which is believed to perform well as investigated in [12].", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "However, unlike [12], the proposed ranking idea is free of deciding an explicit value for missing ratings", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "The PMF-based approach [4] uses the latent representations produced by matrix factorization as user-item features and learns a ranking model on these features.", "startOffset": 23, "endOffset": 26}, {"referenceID": 36, "context": "CofiRank [37] learns latent representations that minimize a ranking-based loss instead of the squared error.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "ListRankMF [30] aims at minimizing the cross entropy between the predict item permutation probability and true item permutation probability.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "In [16] a method for Local Collaborative Ranking (LCR) where ideas of local low-rank matrix approximation were applied to the pairwise ranking loss minimization framework is introduced.", "startOffset": 3, "endOffset": 7}, {"referenceID": 34, "context": "In [35] a framework that allows for pointwise as well as listwise training with respect to various ranking metrics is proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "Finally, [10] proposed a CR model build on the recent developments in ranking methods [3, 26] that focus on accuracy at top and proposed CR methods with p-push and infinite push norms.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "Finally, [10] proposed a CR model build on the recent developments in ranking methods [3, 26] that focus on accuracy at top and proposed CR methods with p-push and infinite push norms.", "startOffset": 86, "endOffset": 93}, {"referenceID": 25, "context": "Finally, [10] proposed a CR model build on the recent developments in ranking methods [3, 26] that focus on accuracy at top and proposed CR methods with p-push and infinite push norms.", "startOffset": 86, "endOffset": 93}, {"referenceID": 30, "context": "users or items besides the rating data that are usually available [31].", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "A feature based regression ranking model for predicting the values (rates) of user-item matrix in cold-start scenarios by leveraging all information available for users and items is proposed in [24].", "startOffset": 194, "endOffset": 198}, {"referenceID": 37, "context": "The kernelized matrix factorization approach studied in [38], which incorporates the auxiliary information into the MF.", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "In [27] joint factorization of the user-item and item-feature matrices by using the same item latent feature matrix in both decompositions is utilized.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "The FBSM model is introduced in [29], which learns factorized bilinear similarity model for new items, given the rating information as well as the features of these items.", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "Recently, [5] proposed a decoupling approach to transduct knowledge from side information to rating prediction which is able to handle both cold-start items and users problems in factorization based models.", "startOffset": 10, "endOffset": 13}, {"referenceID": 19, "context": "Substantial evidence for violations of the missing at random condition in recommender systems is reported in [20] and it has been showed that incorporating an explicit model of the missing data mechanism can lead to significant improvements in prediction performance.", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "The first study of the effect of non-random missing data on collaborative ranking is presented in [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 31, "context": "In [32] an EM algorithm to optimize in turn the factorization and the estimation of missing values.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Recently, in [12] a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values is introduced.", "startOffset": 13, "endOffset": 17}, {"referenceID": 17, "context": "These algorithms reduce the ranking problem into a binary classification problem by treating each relevant/irrelevant instance pair as a single object to be classified [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 25, "context": "Specifically, we propose an algorithm that maximizes the number of relevant items which are pushed to the absolute top of the list by utilizing the P-Norm Push ranking measure which is specially designed for this purpose [26] .", "startOffset": 221, "endOffset": 225}, {"referenceID": 32, "context": "The trace-norm regularization is well-known to be a convex surrogate to the matrix rank, and has repeatedly shown good performance in practice [33, 9].", "startOffset": 143, "endOffset": 150}, {"referenceID": 8, "context": "The trace-norm regularization is well-known to be a convex surrogate to the matrix rank, and has repeatedly shown good performance in practice [33, 9].", "startOffset": 143, "endOffset": 150}, {"referenceID": 6, "context": ", [7]) .", "startOffset": 2, "endOffset": 5}, {"referenceID": 22, "context": "A simple way to solving the above optimization problem is gradient descent algorithm [23], which needs to evaluate the gradient of objective at each iteration.", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "(6) can be reformulated under the framework of proximal regularization or composite gradient mapping [23].", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "We use the singular value shrinkage operator introduced in [8] to find the optimal solution to Eq.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "1, [8]) For each \u03c4 \u2265 0 and W \u2208 R, the singular value shrinkage operator (9) obeys", "startOffset": 3, "endOffset": 6}, {"referenceID": 21, "context": "Since the subdifferential of the maximum of functions is the convex hull of the union of subdifferentials of the active functions at point w [22], we have:", "startOffset": 141, "endOffset": 145}, {"referenceID": 21, "context": "However, by using other smooth convex surrogate losses such as smoothed hinge loss or exponential loss, one can apply the accelerated gradient descent methods [22] to solve the optimization problem which results in significantly faster convergence rate compared to the naive gradient descent (i.", "startOffset": 159, "endOffset": 163}, {"referenceID": 20, "context": "To alleviate this problem one can utilize stochastic gradient method [21] to solve the optimization problem.", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "It is natural to require the similar users in the matrix S have similarity evaluation on rating the items [6].", "startOffset": 106, "endOffset": 109}, {"referenceID": 28, "context": "Then those words that have been appeared in less than 20 items and more that 20% of the items were also removed [29].", "startOffset": 112, "endOffset": 116}, {"referenceID": 32, "context": "4 Baseline Algorithms The proposed SCOR algorithm is compared to the following algorithms: \u2022 Matrix Factorization (MF) [33]: Is a matrix completion method that factorizes the incomplete observed matrix and completes the matrix using the unveiled latent features.", "startOffset": 119, "endOffset": 123}, {"referenceID": 37, "context": "\u2022 Matrix Factorization with Side Information (KPMF) [38]: Is a matrix completion based algorithm, which incorporates external side information of the users or items into the matrix factorization process.", "startOffset": 52, "endOffset": 56}, {"referenceID": 4, "context": "\u2022 Decoupled Completion and Transduction (DCT) [5]: Is a matrix factorization based algorithm that decouples the completion and transduction stages and exploits the similarity information among users and items to complete the (rating) matrix.", "startOffset": 46, "endOffset": 49}, {"referenceID": 28, "context": "\u2022 Feature Based Factorized Bilinear Similarity Model (FBS) [29]: This algorithm uses bilinear model to capture pairwise dependencies between the features.", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "This method is one of the best performing collaborative latent factor based model [13].", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "\u2022 Regression based Latent Factor Model (RLF): This method incorporates the features of items in factorization process by transforming the features to the latent space using linear regression [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 24, "context": "The implementation of this method is available in LibFM library [25].", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "Then, due to the fact that users tend to only care about the top of their recommendation list, predicting a ranking of items of interest instead of ratings became the main focus of recent works [10].", "startOffset": 194, "endOffset": 198}], "year": 2015, "abstractText": "Existing collaborative ranking based recommender systems tend to perform best when there is enough observed ratings for each user and the observation is made completely at random. Under this setting recommender systems can properly suggest a list of recommendations according to the user interests. However, when the observed ratings are extremely sparse (e.g. in the case of cold-start users where no rating data is available), and are not sampled uniformly at random, existing ranking methods fail to effectively leverage side information to transduct the knowledge from existing ratings to unobserved ones. We propose a semi-supervised collaborative ranking model, dubbed SCOR, to improve the quality of cold-start recommendation. SCOR mitigates the sparsity issue by leveraging side information about both observed and missing ratings by collaboratively learning the ranking model. This enables it to deal with the case of missing data not at random, but to also effectively incorporate the available side information in transduction. We experimentally evaluated our proposed algorithm on a number of challenging real-world datasets and compared against state-of-the-art models for cold-start recommendation. We report significantly higher quality recommendations with our algorithm compared to the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}