{"id": "1511.06052", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Overcoming Language Variation in Sentiment Analysis with Social Attention", "abstract": "variation in language acquisition is ubiquitous, and is particularly evident in newer communication forms of writing literature such as social media. fortunately, verbal variation is truly not quite random, but prevalence is usually linked to related social information factors. beginning by exploiting linguistic stimuli homophily - - - the tendency model of several socially correctly linked speaking individuals prefer to use language patterns similarly - - - it often is essentially possible to build models that are not more robust to variation. early in discovering this innovative paper, ideally we focus on social network communities, tasks which mostly make it possible to generalize sociolinguistic personality properties from authors included in deciding the training set referenced to authors in preparing the test sets, without consciously requiring demographic statistical author metadata. we detect communities via introducing standard fuzzy graph clustering algorithms, and and then exploit these communities spontaneously by manually learning community - specific morphological projections of various word embeddings. these meaningful projections then capture shifts encoded in their word meaning in sufficiently different social groups ; by modeling them, thereafter we are economically able worldwide to broadly improve the overall accuracy of generic twitter sentiment model analysis by a significant margin improving over competitive prior statistical work.", "histories": [["v1", "Thu, 19 Nov 2015 03:54:15 GMT  (1173kb,D)", "http://arxiv.org/abs/1511.06052v1", null], ["v2", "Wed, 23 Dec 2015 15:36:48 GMT  (1180kb,D)", "http://arxiv.org/abs/1511.06052v2", null], ["v3", "Wed, 28 Dec 2016 22:07:53 GMT  (184kb,D)", "http://arxiv.org/abs/1511.06052v3", "Conditionally accepted to Transactions of the ACL, 2016"], ["v4", "Sat, 26 Aug 2017 15:11:01 GMT  (184kb,D)", "http://arxiv.org/abs/1511.06052v4", "Published in Transactions of the Association for Computational Linguistics (TACL), 2017. Please cite the TACL version:this https URL"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.SI", "authors": ["yi yang", "jacob eisenstein"], "accepted": true, "id": "1511.06052"}, "pdf": {"name": "1511.06052.pdf", "metadata": {"source": "CRF", "title": "Putting Things in Context: Community-specific Embedding Projections for Sentiment Analysis", "authors": ["Yi Yang"], "emails": ["yiyang+jacobe@gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "Words can mean different things to different people. Fortunately, these differences are rarely idiosyncratic, but are usually linked to social factors, such as age (Rosenthal and McKeown, 2011), gender (Eckert and McConnell-Ginet, 2003), race (Green, 2002), geography (Trudgill, 1974), and more ineffable characteristics such as political and cultural attitudes (Fischer, 1958; Labov, 1963). In natural language processing (NLP), the recent surge of interest in social media has brought variation\nto the fore, with papers presenting computational techniques for characterizing variation in the lexicon (Eisenstein et al., 2010; Gouws et al., 2011), orthography (Eisenstein, 2015), and syntax (Stewart, 2014; Johannsen et al., 2015). However, aside from attempts to normalize spelling variants (Sproat et al., 2001; Aw et al., 2006; Han and Baldwin, 2011; Yang and Eisenstein, 2013), there have been few attempts to address the problems posed by variation for the accuracy of NLP systems.\nOne recent exception is the work on Hovy (2015), who shows that the accuracy of sentiment analysis and topic classification can be improved by the inclusion of coarse-grained author demographics such as age and gender. However, such demographic information is not directly available in most datasets, and it is not yet clear whether predicted age and gender offers any improvements. On the other end of the spectrum are attempts to create personalized language technologies, as are often employed in information retrieval (Shen et al., 2005), recommender systems (Basilico and Hofmann, 2004), and language modeling (Federico, 1996). But personalization requires annotated data for each individual user\nar X\niv :1\n51 1.\n06 05\n2v 1\n[ cs\n.C L\n] 1\n9 N\nov 2\n\u2014 something that may be possible in interactive settings such as information retrieval, but is not typically feasible in natural language processing.\nWe propose a middle ground between group-level demographic characteristics and personalization, by exploiting social network structure. The sociological theory of homophily asserts that individuals are usually similar to their friends (McPherson et al., 2001). This property has been demonstrated both for language (Puniyani et al., 2010; Bryden et al., 2013) as well as for the demographic properties targeted by Hovy (2015), which are more likely to be shared by friends than by random pairs of individuals (Thelwall, 2009; Al Zamal et al., 2012). Social network information is available in a wide range of contexts, from social media (Huberman et al., 2008) to political speech (Thomas et al., 2006) to historical texts (Winterer, 2012). Thus, social network homophily has the potential to provide a general and effective way to account for linguistic variation in NLP.\nFigure 1 gives a schematic of the motivation for our approach. The word \u2018sick\u2019 typically has a negative sentiment, e.g., \u2018I would like to believe he\u2019s sick rather than just mean and evil\u2019.1 However, in some communities the word can have a positive sentiment, e.g., the lyric \u2018this sick beat\u2019, recently trademarked by the musician Taylor Swift. Given labeled examples of \u2018sick\u2019 in use by individuals in a social network, we assume that the word will have a similar sentiment meaning for their near neighbors \u2014 an assumption of linguistic homophily that is the basis for this research. Note that this differs from the assumption of label homophily, which entails that neighbors in the network will hold similar opinions, and will therefore produce similar document-level labels (Thomas et al., 2006; Tan et al., 2011; Hu et al., 2013). Linguistic homophily is a more generalizable claim, which could in principle be applied to any language processing task where author network information is available.\nTo scale this idea to datasets with tens of thousands of unique authors, we compress the social network, using algorithms for community detection (Fortunato, 2010). Community detection algorithms place each social network node into one of a\n1Charles Rangel, describing Dick Cheney\nfinite number of communities; the goal is typically to maximize modularity, which rewards link density within each community, and penalizes link density across communities. A community detection algorithm applied to Figure 1 would likely identify the two triads as separate communities, as they are each completely connected, with only one edge between them. Applying this idea to language, we assume that linguistic meaning is relatively consistent within communities, and potentially distinct across communities \u2014 an assumption we call linguistic modularity.\nTo exploit the community structure of the author social network, we induce community-specific projection matrices, which project word embeddings into community-specific spaces. We apply this idea to Twitter sentiment classification, gathering social network metadata for Twitter users in the SemEval Twitter sentiment analysis tasks (Nakov et al., 2013; Rosenthal et al., 2015). Results are positive; building on the competitive NLSE system of Astudillo et al. (2015), we achieve significant improvements on the 2014 and 2015 tasks."}, {"heading": "2 Community Detection", "text": "Community detection in networks aims at identifying clusters or/and their hierarchical organization, by only using the information encoded in the network topology (as shown in Figure 2). This problem is of considerable interest to many areas, with applications to food webs, citation networks, social networks, and the web; in the NLP context, community\ndetection has previously been used for word sense induction, by identifying communities of senses in a co-occurrence graph (Jurgens, 2011). In practice, community detection is typically treated as an optimization problem, where the objective function captures the intuition of a network community as set of nodes with better internal connectivity than external connectivity. Specifically, many of the best-known community detection algorithms seek to maximize modularity, which compares the density of edges within communities to the density of edges between communities (Newman, 2006).\nIn an undirected network over n nodes, let us write aij \u2208 {0, 1} to indicate the presence of an edge between nodes i and j, with m , 12 \u2211n i \u2211n j aij , the total number of edges. Then the modularity can be defined as,\nQ = 1\n2m n\u2211 i=1,j=1 (aij \u2212 E[aij ])\u03b4(ci, cj), (1)\nwith ci \u2208 {1, 2, . . . ,K} indicating the community of node i, and \u03b4(ci, cj) indicating whether i and j are placed in the same community. The term E[aij ] represents the expected value of aij under a random \u201crewiring\u201d of the network that preserves the degree of nodes i and j. The absolute value of the modularity is strictly less than one, where positive modularity indicates that the edge density within communities is more than would be expected by chance. In the limiting cases of K = 1 and K = N , the modularity is zero. The problem of finding a community partitioning that maximizes the modularity is NP-complete, regardless of whether the number of communities is specified (Brandes et al., 2008). We therefore consider two well-known approximate algorithms."}, {"heading": "2.1 Community Detection Algorithms", "text": "Fast-Greedy Fast-Greedy is a hierarchical approach that detects communities in a bottom-up fashion (Clauset et al., 2004). Initially, every node belongs to a separate community, and communities are merged iteratively such that each merge is locally optimal. Specifically, in each iteration, the algorithm scans over every edge that joins two separate communities in the current configuration, and computes the increase in the current value of modularity \u2206Q\nby merging the two communities together. The edge that yields the largest positive \u2206Qwould be adopted to merge the two communities at the end of this iteration. The algorithm stops when it is not possible to increase the modularity any more, and results in the community structure as well as the dendrogram describing the community structure. By utilizing efficient data structures, the running time of Fast-Greedy on a network with n nodes andm edges is O(md log n), where d is the depth of the dendrogram.\nMulti-Level Multi-Level is also a hierarchical approach that optimizes modularity in a greedy manner (Blondel et al., 2008). Initially, the algorithm starts with each node in its own community. Different from Fast-Greedy, which makes the decision to merge communities until the end of each iterations, Multi-Level re-assigns communities in a local, faster way. In particular, in each step, the algorithm sweeps over all communities in the current configuration sequentially, where each community would be merged with one of its neighbor community (i.e., there exists edges between nodes of the two communities), as long as the resulting merged community corresponds to the largest positive increase of modularity. The process stops when there is only a single community left or when the modularity cannot be increased any more. This procedure resembles how we perform Brown clustering (Brown et al., 1992) for NLP problems, where the clustering starts with each word in its own cluster, and the algorithm merges the two word clusters that maximizes the quality of the resulting clustering in each iteration. The distinction is that we are able to merge any two word clusters in Brown clustering, while we can only merge two neighbor communities in MultiLevel algorithm. The complexity of Multi-Level is approximating O(m), as most of the computation is concentrated on the first iteration."}, {"heading": "2.2 Linguistic Modularity in Social Network Communities", "text": "The hypothesis of linguistic homophily, as presented in the introduction, is that socially connected individuals are likely to be more linguistically similar than a randomly selected pair of individuals. Having focused on community detection as a summary\nof social network structures, we now refine this hypothesis to linguistic modularity: linguistic similarity should be higher for pairs of individuals who are in the same community than for pairs of individuals in different communities.\nTo formalize linguistic modularity, we follow the traditional social network definition of modularity closely. Suppose we are given a social network G, on which we induce a community structure C = {ci}. We now create an alternative network G(L), representing linguistic similarity. InG(L), each node has an edge to the K most linguistically similar nodes, where linguistic similarity is measured by TF-IDF weighted cosine similarity. We can then compute modularity of G(L) with respect to the community structure C, which was computed from the original network G. This measure will quantify whether the communities identified from the social network structure tend to include linguisticallysimilar pairs of nodes. Linguistic modularity inherits all the properties of traditional social network modularity: the absolute value is less than one; the modularity is zero for trivial community structures of a single community or one community per node; the expected modularity is zero for a randomlychosen community structure."}, {"heading": "3 Testing Linguistic Modularity", "text": "We evaluate the linguistic modularity of the FastGreedy and Multi-Level community detection algorithms on a social network constructed from the SemEval Twitter Sentiment Analysis evaluation. Specifically, we start with the users in the SemEval datasets, and then use the Twitter API to crawl their friend links as undirected edges (in Twitter terminology, a individual\u2019s \u201cfriends\u201d are accounts that the individual follows). The statistics of this network are shown in the first line of Table 1. As shown, the average degree for each node is less than three, and more than 25% of authors are isolates, meaning that they have no social network connections. For these users, the social network can provide no useful information.\nOne approach to increase the density of the social network would be to crawl additional nodes by snowball sampling (Goodman, 1961): specifically, we would expand the set of nodes to include all\nfriends of nodes in the original SemEval network. However, there are more than seven million such individuals, which would be too many to crawl, given the limitations of the Twitter API. We therefore focus on adding nodes who will do the most to densify the author network, adding all individuals who are followed by at least 100 SemEval authors if they have less than ten thousand friends. We call the resulting network SEMEVAL+, and its statistics are shown in the second line of Table 1; the proportion of isolated nodes has fallen to 5%. Note that all data was acquired in April 2015; by this time, the authorship information of 11.4% of the tweets in our SemEval datasets was no longer available.\nWhy the follower network? Huberman et al. (2008) argue that Twitter\u2019s follower network is not a true social network because many users have an unreasonably large number of followers and friends, exceeding a supposed cognitive upper limit on the size of human social networks known as Dunbar\u2019s number (Dunbar, 1992; Dunbar, 1998). Huberman et al. (2008) instead advocate for a mention network, in which edges are drawn between each pair of individual who mention each others\u2019 usernames in their posts. Later work showed that the mention network has strong linguistic properties: it is better correlated with each author\u2019s distribution over latent topics as induced by latent Dirichlet allocation (Blei et al., 2003) than the follower network (Puniyani et al., 2010). However, the mention network is considerably sparser than the follower network, and contains a much larger number of isolated individuals, as many authors simply do not mention other usernames in their tweets \u2014 and many others are correspondingly never mentioned. Thus, while the mention network is an intriguing resource for future work, we do not employ it here."}, {"heading": "3.1 Quantifying linguistic modularity", "text": "We measure linguistic modularity as described in subsection 2.2 on the SEMEVAL and SEMEVAL+ networks, using the Fast-Greedy and Multi-Level community detection algorithms. Linguistic information was computed from the most recent 3,200 tweets from each author, and the linguistic network G(L) was constructed by building edges to the five nearest neighbors. We merge all the communities with less than ten users into a single community, which mostly consists of isolates. The number of communities is selected automatically by the algorithm in all cases.\nResults are presented in Table 2. All results are significantly higher than zero by a bootstrap resampling test, p 0.001, indicating that the hypothesis of linguistic modularity holds for this data and for both community detection algorithms. The linguistic modularity is substantially higher for SEMEVAL+, indicating that crawling additional users has improved the quality of the detected communities from a linguistic standpoint. As an upper bound, we also computed the linguistic modularity from a text-based K-means clustering on the same authors, with the same numbers of clusters (5, 7, and 19), obtaining values of Q(L) between 0.43 and 0.47. Recall that the social network community detection algorithms form communities without considering the text, so it is unsurprising that the modularity values for K-means are higher. The fact that we are able to obtain linguistic modularities that are significantly higher than zero on purely social network data shows that social network communities can provide useful complementary information for language processing tasks. Finally, we note that the results do not demonstrate a clear winner between Fast-Greedy and Multi-Level, so we consider both approaches in the supervised sentiment analysis task that follows."}, {"heading": "4 Community-specific Word Embeddings", "text": "In this section, we propose to leverage social network community structures for improving NLP tasks. We build on the successful non-linear subspace embedding (NLSE) model, which learns a task-specific projection matrix to apply to pretrained word embeddings (Astudillo et al., 2015),\nhj = Sigmoid(Swj), (2)\nwhere wj \u2208 Rd is a pre-trained word embedding for word j, and S \u2208 Rk\u00d7d is the task-specific projection matrix, with d and k indicating the sizes of the pre-trained and projected word embeddings respectively. The advantage of this approach is that the pretrained word embeddings can be learned from large amounts of unlabeled text, while supervised training is needed only to learn the s \u00d7 d matrix S, and the final classification weights. By simply adding the projected embeddings across all words in a Tweet, Astudillo et al. (2015) obtain a Tweet representation that performs very well on sentiment analysis: a Softmax classifier applied on top of this representation outperforms all alternatives on the SemEval 2014 and 2015 Twitter sentiment analysis tasks.\nWe incorporate social network communities into the NLSE model, in a method we call COMMSEM, for Commity-Specific Embeddings. Specifically, we induce additional projection matrices per community, so that for document i in community ci, we have,\nhi,j = Sigmoid( [ S(0) + S(ci) ] wj), (3)\nwhere S(0) is the community independent projection matrix, S(ci) is the community-specific projection matrix associated with the community ci, and hi,j is the embedding of word j in community ci. Intuitively, S(0) encourages COMMSEM to take advantage of the statistical power given by modeling all the training data, and S(ci) provides the flexibility to capture community-specific information. To avoid overfitting, we penalize the community-specific projections with L2 regularization. The overall setup is illustrated in Figure 3.\nBecause the community detection problem is NPcomplete, both Fast-Greedy and Multi-Level are heuristic approximations, and the specific local optima that they obtain depends on the initialization\nand ordering of nodes in the graph. By running these algorithms over multiple orderings, we can obtain multiple community structures, and by averaging over these structures, we may obtain more reliable results. This idea can be easily incorporated in COMMSEM. Suppose we obtain L distinct community structures by running the community detection algorithms multiple times. Then we can compute multi-community word embeddings for instance i as,\nhi,j = Sigmoid ([ S(0) +\nL\u2211 k=1 S(c (k) i )\n] wj ) , (4)\nwhere c(k)i is the community for instance i in community structure k."}, {"heading": "5 Experiments", "text": "We test the utility of community-specific projections on the 2013\u20132015 SemEval Twitter sentiment analysis tasks, in which the system classifies whether a given message is of positive, negative, or neutral sentiment. Following Astudillo et al. (2015), we train and tune our system using only the development data, which is the SemEval Twitter 2013 training data, and evaluate on the SemEval Twitter test sets. The statistics of these datasets are shown in Table 3. The datasets adopted in this work are from (Astudillo et al., 2015), which consist of entire tweets of the original test sets, but only a subset of development tweets. Furthermore, we fail to identify the authors of about 11.4% of the tweets in the datasets, as the tweets were not available by the time we started to crawl social relations.\nExperimental Settings We employ the best performing pre-trained word embeddings used by Astudillo et al. (2015), which are trained with a corpus of 52 million tweets. The embeddings are learned using the structured skip-gram model (Ling et al.,\n2015), and the embedding dimension is set as 600. We also pre-process the SemEval Twitter messages using the same pre-processing procedure for the embedding training corpus. We report the same evaluation metric as the SemEval challenge: the Average F1 score of positive and negative classes. For the social network, we use only the SEMEVAL+ network, since the linguistic modularity was considerably higher for communities extracted from this network than from the original SEMEVAL network (Table 2).\nCompetitive systems We consider two competitive Twitter sentiment classification methods based on NLSE, the sub-space projection method described in section 4. The pooled setting ignores the community structure information, and only estimates one NLSE model with all the training data. The per-community setting leverages the community information by training multiple independent NLSE models, one per community, and then apply each model to the test set messages from the same community. Aside from NLSE, we also compare against the three top-performing systems in the SemEval 2015 Twitter sentiment analysis challenge (Rosenthal et al., 2015): WEBIS (Hagen et al., 2015), UNITN (Severyn and Moschitti, 2015) and LSISLIF (Hamdan et al., 2015). Our implementation of NLSE model performs slightly worse than the results reported in the original paper (Astudillo et al., 2015), so we also republish these results. Our implementation of COMMSEM is built on top of our re-implementation of NLSE, so any improvements to the NLSE implementation may also improve COMMSEM.\nParameter tuning We adopt the same training and validation data splits as Astudillo et al. (2015), where 80% of the development data is used for pa-\nrameter learning and 20% for model selection.2 We choose the subspace size from {5, 10, 15}, and the L2 regularization penalty from {1.0, 3.0, 5.0, 8.0, 10.0, 20.0} for COMMSEM after preliminary search. As described in section 4, it is possible to apply COMMSEM to multiple overlapping community structures by running the community detection algorithms multiple times. We compare a range of numbers of community structures, {1, 3, 5, 10}.\nResults Evaluation results are presented in Table 4. COMMSEM achieves similar results as pooled NLSE on the SemEval Twiter 2013 test set, but it significantly outperforms pooled NLSE on the 2014 and 2015 test sets, p < .05 (\u03c72 \u2248 4.67 and 4.12, dof = 1) by McNemar\u2019s test. One way in which the 2014 and 2015 test sets are more difficult than the 2013 test set is that they are drawn from different time periods than the training data. NLSE performs considerably worse when trained separately within each community, probably due to the smaller amount of available training data. Based on dev set tuning, the optimal number of community structures used by COMMSEM is one for Fast-Greedy community detection algorithm, and three for MultiLevel community detection algorithm. Multi-Level is more sensitive to initialization and node ordering than Fast-Greedy, and the resulting community structures contain more additional useful information for COMMSEM."}, {"heading": "5.1 Analysis", "text": "We investigate what information has been captured by the community-specific embeddings. We simply compute the Euclidean distance between the community-specific embedding hi,j and the embedding obtained from the shared projection h0,j = Sigmoid(S(0)wj). The top ten words that with highest Euclidean distances corresponding to each community are presented in Table 5. We exclude the special community merged from small communities as well as one community with only 62 tweets in the training data. It is interesting to note that the metric helps identify many community-specific sentiment words, which are mostly related to positive sentiment for community 1 and 3 and negative sentiment\n2We obtained these splits through communication with the authors.\nfor community 2. Although the ratios of positive and negative tweets posted by users in these communities are generally similar, the words are dominated by one particular sentiment polarity in each community, which implies that users in one community may prefer to use sentiment words differently with respect to only one sentiment."}, {"heading": "6 Related Work", "text": "Domain adaptation and personalization Community-specific NLP can be viewed as a form of domain adaptation, where the domains are determined automatically by a graph clustering algorithm. Our approach is then a form of supervised domain adaptation, since we assume some labeled data within each community (S\u00f8gaard, 2013). Early approaches to supervised domain adaptation focused on adapting the classifier weights across communities, using enhanced feature spaces (Daume\u0301 III, 2007) or Bayesian priors (Chelba and Acero, 2006; Finkel and Manning, 2009).3 In contrast, our goal is to learn a transformation of the input representation for each community. In this way, our work is more similar in spirit to unsupervised domain adaptation, which typically works by transforming the input feature space so as to overcome domain differences (Blitzer et al., 2006; Ben-David et al., 2010; Chen et al., 2012; Glorot et al., 2011). In our case, the transformation is learned discriminatively; an interesting possibility for future work would be the exploration of new training objectives that combine domain generality (Ben-David et al., 2010) with the label log-likelihood as considered here.\nPersonalization has been an active research topic in many other areas, such as speech recognition (speaker adaptation) and information retrieval (personalized search). Standard techniques for these tasks include linear transformation of model parameters (Leggetter and Woodland, 1995) and collaborative filtering (Breese et al., 1998). These methods have been adopted to personalized sentiment analysis recently (Tang et al., 2015; Al Boni et al., 2015). Supervised personalization typically requires labeled training examples for every individual user,\n3We adopted Easy Adaptation approach (Daume\u0301 III, 2007) in our preliminary experiments, and observed marginal improvements.\nwhich is generally not possible in NLP scenarios. We therefore leverage social network community structures to generalize across individual users.\nSentiment analysis with social relations Previous work on incorporating social relations for sentiment classification mainly relies on the label consistency assumption, where the existence of social connections between users is considered as a clue that the sentiment polarities of all messages from the users should be similar. Speriosu et al. (2011) construct a heterogeneous network with tweets, users, and n-grams as nodes, and each node is associated with a sentiment label distribution obtained from a maximum entropy classifier or sentiment lexicons. The label distributions of tweets is then refined by performing label propagation over social relations. Hu et al. (2013) model social relations using the graph Laplacian of the adjacency graph represen-\ntation of the social network, which they employ as a source of regularization, so that socially-similar users are encouraged to have similar labels. Tan et al. (2011) leverage a similar intuition, using a factor graph based approach in which the labels of targets belonging to socially connected users are treated as factors in a joint probabilistic model. Our work is based on a different intuition: rather than assuming labels will tend to be similar for socially-connected users, we assume that similar usage of language. These assumptions are complementary; if both hold for a specific setting, then label consistency and linguistic consistency could in principle be applied to improve performance.\nCustomized word representations Our approach is based on community-specific projections of word embeddings. Similarly, Bamman et al. (2014) learn geographically-specific word embeddings, with the\ngoal of capturing geographical differences in meaning. Kulkarni et al. (2015) are interested in the changing meaning of words over time, and learn separate embeddings for different temporal epochs. Yang and Eisenstein (2015) learn multiple feature embeddings for a set of domain attributes, such as author, genre, and temporal epoch. All of these approaches differ from our work in that they learn multiple embeddings, rather than learning multiple projections of a single underlying embedding. This enables our approach to exploit very large, unlabeled data to learn the original word embeddings, and then to learn the much smaller community-specific projections using labeled data."}, {"heading": "7 Conclusion", "text": "This paper presents a new method for learning to overcome language variation, leveraging the tendency of individuals with similar linguistic patterns to share social network connections \u2014 the phenomenon of linguistic homophily. By learning separate projections of word embeddings for each community, our approach is able to capture subtle shifts in meaning for individual words across social communities. We have formulated this model by building on prior work which learns task-specific projections of word embeddings; we go further by learning projections that are both task-specific and community-specific. A key question for future work is whether the task and community dimensions can be decoupled: can we learn a community-based projection that is useful across multiple tasks? Answering this question requires ground truth annotations for multiple tasks for a set of socially linked authors. This could perhaps be obtained by adding a layer of annotations on top of the SemEval Twitter Sentiment dataset, or by identifying some form of \u201cfound\u201d annotations such as retweets or favorites. We plan on pursuing this direction in future work. Another direction for future research is to try to generalize linguistic homophily from its implementation in social network communities, using some form of continuous representation for individual author nodes, which should capture the node\u2019s social properties; very recent work by Li et al. (2015) suggests one possible approach."}, {"heading": "8 Acknowledgments", "text": "We thank Duen Horng \u201cPolo\u201d Chau for discussions about community detection and Ramon Astudillo for sharing data and helping us to reproduce the NLSE results. This research was supported by the National Science Foundation under award RI1452443, by the National Institutes of Health under award number R01GM112697-01, and by the Air Force Office of Scientific Research. The content is solely the responsibility of the authors and does not necessarily represent the official views of these sponsors."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Variation in language is ubiquitous, and is particularly evident in newer forms of writing such as social media. Fortunately, variation is not random, but is usually linked to social factors. By exploiting linguistic homophily \u2014 the tendency of socially linked individuals to use language similarly \u2014 it is possible to build models that are more robust to variation. In this paper, we focus on social network communities, which make it possible to generalize sociolinguistic properties from authors in the training set to authors in the test sets, without requiring demographic author metadata. We detect communities via standard graph clustering algorithms, and then exploit these communities by learning community-specific projections of word embeddings. These projections capture shifts in word meaning in different social groups; by modeling them, we are able to improve the overall accuracy of Twitter sentiment analysis by a significant margin over competitive prior work.", "creator": "LaTeX with hyperref package"}}}