{"id": "1511.06393", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Fixed Point Quantization of Deep Convolutional Networks", "abstract": "in additional recent years increasingly complex physical architectures for maintaining deep convolution networks ( using dcns ) too have publicly been variously proposed promising to boost the performance premium on image recognition tasks. however, the corresponding gains involved in performance have come at a cumulative cost margin of substantial polynomial increase value in compute resources, utilizing the computational model improved size and processing unit speed of the corresponding network optimal for training technique and conceptual evaluation. fixed point implementation of operating these networks there has discovered the evident potential to theoretically alleviate some of explaining the burden of these substantial additional complexities. in this paper, we then propose a complementary quantizer value design for fixed point implementation architecture for dcns. we recently then formulate essentially an optimization memory problem technique to thoroughly identify acceptable optimal fixed point bit - width band allocation across dcn layers. we help perform comparison experiments rely on using a successful recently earlier proposed dcn architecture modification for cifar - 10 benchmark that generates test error of less magnitude than below 7 %. we currently evaluate the effectiveness of modifying our proposed fixed stationary point bit - width allocation for this desired dcn. our experiments both show that in comparison procedures to equal bit - width settings, the existing fixed point dcns with reverse optimized bit width allocation can offer & function gt ; however 20 % reduction proceeds in the prediction model scaled size approach without making any loss limitation in dynamic performance. we also additionally demonstrate concern that fine tuning can further enhance the accuracy level of standard fixed point dcns beyond that of realizing the original floating point scaling model. interested in doing so, we report a new state - of - the - art fixed point performance criterion of 6. 78 % human error - rate correction on full cifar - alpha 10 benchmark.", "histories": [["v1", "Thu, 19 Nov 2015 21:37:06 GMT  (516kb,D)", "https://arxiv.org/abs/1511.06393v1", null], ["v2", "Thu, 7 Jan 2016 22:20:06 GMT  (528kb,D)", "http://arxiv.org/abs/1511.06393v2", null], ["v3", "Thu, 2 Jun 2016 06:21:42 GMT  (524kb,D)", "http://arxiv.org/abs/1511.06393v3", "ICML 2016"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["darryl dexu lin", "sachin s talathi", "v sreekanth annapureddy"], "accepted": true, "id": "1511.06393"}, "pdf": {"name": "1511.06393.pdf", "metadata": {"source": "META", "title": "Fixed Point Quantization of Deep Convolutional Networks", "authors": ["Darryl D. Lin", "Sachin S. Talathi", "Sreekanth Annapureddy"], "emails": ["DARRYL.DLIN@GMAIL.COM", "TALATHI@GMAIL.COM", "SREEKANTHAV@GMAIL.COM"], "sections": [{"heading": "1. Introduction", "text": "Recent advances in the development of deep convolution networks (DCNs) have led to significant progress in solving non-trivial machine learning problems involving image recognition (Krizhevsky et al., 2012) and speech recognition (Deng et al., 2013). Over the last two years several\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nadvances in the design of DCNs (Zeiler & Fergus, 2014; Simonyan & Zisserman, 2014; Szegedy et al., 2014; Chatfield et al., 2014; He et al., 2014; Ioffe & Szegedy, 2015) have not only led to a further boost in achieved accuracy on image recognition tasks but also have played a crucial role as a feature generator for other machine learning tasks such as object detection (Krizhevsky et al., 2012) and localization (Sermanet et al., 2013), semantic segmentation (Girshick et al., 2014) and image retrieval (Krizhevsky et al., 2012; Razavian et al., 2014). These advances have come with an added cost of computational complexity, resulting from DCN designs involving any combinations of: increasing the number of layers in the DCN (Szegedy et al., 2014; Simonyan & Zisserman, 2014; Chatfield et al., 2014), increasing the number of filters per convolution layer (Zeiler & Fergus, 2014), decreasing stride per convolution layer (Sermanet et al., 2013; Simonyan & Zisserman, 2014) and hybrid architectures that combine various DCN layers (Szegedy et al., 2014; He et al., 2014; Ioffe & Szegedy, 2015).\nWhile increasing computational complexity has afforded improvements in the state-of-the-art performance, the added burden of training and testing makes these networks impractical for real world applications that involve real time processing and for deployment on mobile devices or embedded hardware with limited power budget. One approach to alleviate this burden is to increase the computational power of the hardware used to deploy these networks. An alternative approach that may be cost efficient for large scale deployment is to implement DCNs in fixed point, which may offer advantages in reducing memory bandwidth, lowering power consumption and computation time as well as the storage requirements for the DCNs.\nIn general, there are two approaches to designing a fixed point DCN: (1) convert a pre-trained floating point DCN model into a fixed point model without training, and (2) train a DCN model with fixed point constraint. While\nar X\niv :1\n51 1.\n06 39\n3v 3\n[ cs\n.L G\n] 2\nJ un\nthe second approach may produce networks with superior accuracy numbers (Rastegari et al., 2016; Lin & Talathi, 2016), it requires tight integration between the network design, training and implementation, which is not always feasible. In this paper, we will mainly focus on the former approach. In many real-world applications a pre-trained DCN is used as a feature extractor, followed by a domain specific classifier or a regressor. In these applications, the user does not have access to the original training data and the training framework. For these types of use cases, our proposed algorithm will offer an optimized method to convert any off-the-shelf pre-trained DCN model for efficient run time in fixed point.\nThe paper is organized as follows: In Section 2, we present a literature survey of the related works. In Section 3, we develop quantizer design for fixed point DCNs. In Section 4 we formulate an optimization problem to identify optimal fixed point bit-width allocation per layer of DCNs to maximize the achieved reduction in complexity relative to the loss in the classification accuracy of the DCN model. Results from our experiments are reported in Section 5 followed by conclusions in the last section."}, {"heading": "2. Related work", "text": "Fixed point implementation of DCNs has been explored in earlier works (Courbariaux et al., 2014; Gupta et al., 2015). These works primarily focused on training DCNs using low precision fixed-point arithmetic. More recently, Lin et al. (2015) showed that deep neural networks can be effectively trained using only binary weights, which in some cases can even improve classification accuracy relative to the floating point baseline.\nThe works above all focused on the approach of designing the fixed point network during training. The works of Kyuyeon & Sung (2014); Sajid et al. (2015) more closely resemble our work. In Kyuyeon & Sung (2014), the authors propose a floating point to fixed point conversion algorithm for fully-connected networks. The authors used an exhaustive search strategy to identify optimal fixed point bit-width for the entire network. In a follow-up paper (Sajid et al., 2015), the authors applied their proposed algorithm to DCN models where they analyzed the quantization sensitivity of the network for each layer and then manually decide the quantization bit-widths. Other works that are somewhat closely related are Vanhoucke et al. (2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed. Gong et al. (2014) on the other hand applied codebook based on scalar and vector quantization methods in order to reduce the model size.\nIn the spirit of Sajid et al. (2015), we also focus on optimizing DCN models that are pre-trained with floating point precision. However, as opposed to exhaustive search method adopted by Sajid et al. (2015), our objective is to convert the pre-trained DCN model into a fixed-point model using an optimization strategy based on signal-toquantization-noise-ratio (SQNR). In doing so, we aim to improve upon the inference speed of the network and reduce storage requirements. The benefit of our approach as opposed to the brute force method is that it is grounded in a theoretical framework and offers an analytical solution for bit-width choice per layer to optimize the SQNR for the network. This offers an easier path to generalize to networks with significantly large number of layers such as the one recently proposed by He et al. (2015).\nOther approaches to handle complexity of deep networks include: (a) leveraging high complexity networks to boost performance of low complexity networks, as proposed in Hinton et al. (2014), (b) compressing neural networks using hashing (Chen et al., 2015), and (c) combining pruning and quantization during training to reduce the model size without affecting the accuracy (Han et al., 2015). These methods are complementary to our proposed approach and the resulting networks with reduced complexity can be easily converted to fixed point using our proposed method. In fact, the DCN model that we performed experiments with and report results on, (see Section 5.2), was trained under the dark knowledge framework by using the inception network (Ioffe & Szegedy, 2015) trained on ImageNet as the master network."}, {"heading": "3. Floating point to fixed point conversion", "text": "In this section, we will propose an algorithm to convert a floating point DCN to fixed point. For a given layer of DCN the goal of conversion is to represent the input activations, the output activations, and the parameters of that layer in fixed point. This can be seen as a process of quantization."}, {"heading": "3.1. Optimal uniform quantizer", "text": "There are three inter-dependent parameters to determine for the fixed point representation of a floating point DCN: bitwidth, step-size (resolution), and dynamic range. These are related as:\nRange \u2248 Stepsize \u00b7 2Bitwidth (1)\nGiven a fixed bit-width, the trade-off is between having large enough range to reduce the chance of overflow and small enough resolution to reduce the quantization error. The problem of striking the best trade-off between overflow error and quantization error has been extensively studied in the literature. Table 1 below shows the step sizes of the\noptimal symmetric uniform quantizer for uniform, Gaussian, Laplacian and Gamma distributions. The quantizers are optimal in the sense of minimizing the SQNR.\nFor example, suppose the input is Gaussian distributed with zero mean and unit variance. If we need a uniform quantizer with bit-width of 1 (i.e. 2 levels), the best approach is to place the quantized values at -0.798 and 0.798. In other words, the step size is 1.596. If we need a quantizer with bit-width of 2 (i.e. 4 levels), the best approach is to place the quantized values at -1.494, -0.498, 0.498, and 1.494. In other words, the step size is 0.996.\nIn practice, however, even though a symmetric quantizer is optimal for a symmetric input distribution, sometimes it is desirable to have 0 as one of the quantized values because of the potential savings in model storage and computational complexity. This means that for a quantizer with 4 levels, the quantized values could be -0.996, 0.0, 0.996, and 1.992.\nAssuming an optimal uniform quantizer with ideal input, the resulting SQNR as a function of the bit-width is shown in Figure 1. It can be observed that the quantization efficiency decreases as the Kurtosis of the input distribution increases.\nAnother take-away from this figure is that there is an ap-\nproximately linear relationship between the bit-width and resulting SQNR:\n\u03b3dB \u2248 \u03ba \u00b7 \u03b2 (2)\nwhere \u03b3dB = 10 log10(\u03b3) is the SQNR in dB, \u03ba is the quantization efficiency, and \u03b2 is the bit-width. Note that the slopes of the lines in Figure 1 depict the optimal quantization efficiency for ideal distributions. The quantization efficiency for uniform distribution is the well-known value of 6dB/bit (Shi & Sun, 2008), while the quantization efficiency for Gaussian distribution is about 5dB/bit (You, 2010) . Actual quantization efficiency for non-ideal inputs can be significantly lower. Our experiments show that the SQNR resulting from uniform quantization of the actual weights and activations in the DCN is between 2 to 4dB/bit."}, {"heading": "3.2. Empirical distributions in a pre-trained DCN", "text": "Figure 2(a) and Figure 2(b) depict the empirical distributions of weights and activations, respectively for the convolutional layers of the DCN we designed for CIFAR-10 benchmark (see Section 5). Note that the activations plotted here are before applying the activation functions.\nGiven the similarity of these distributions to the Gaussian distribution, in all our analysis we have assumed Gaussian distribution for both weights and activations. However, we also note that the distribution of weights and activations for some layers are less Gaussian-like. It will therefore be of interest to experiment with step-sizes for other distributions (see Table 1), which is beyond the scope of present work."}, {"heading": "3.3. Model conversion", "text": "Any floating point DCN model can be converted to fixed point by following these steps:\n\u2022 Run a forward pass in floating point using a large set of typical inputs and record the activations.\n\u2022 Collect the statistics of weights, biases and activations for each layer. \u2022 Determine the fixed point formats of the weights, biases and activations for each layer.\nNote that determining the fixed point format is equivalent to determining the resolution, which in turn means identifying the number of fractional bits it requires to represent the number. The following equations can be used to compute the number of fractional bits:\n\u2022 Determine the effective standard deviation of the quantity being quantized: \u03be. \u2022 Calculate step size via Table 1: s = \u03be \u00b7 Stepsize(\u03b2). \u2022 Compute number of fractional bits: n = \u2212dlog2 se.\nIn these equations,\n\u2022 \u03be is the effective standard deviation of the quantity being quantized, an indication of the width of the distribution we want to quantize. For example, if the quantized quantities follow an ideal zero mean Gaussian distribution, then \u03be = \u03c3, where \u03c3 is the true standard deviation of quantized values. If the actual distribution has longer tails than Gaussian, which is often the case as shown in Figure 2(a) and 2(b), then \u03be > \u03c3. In our experiments in Section 5, we set \u03be = 3\u03c3. \u2022 Stepsize(\u03b2) is the optimal step size corresponding to quantization bit-width of \u03b2, as listed in Table 1. \u2022 n is the number of fractional bits in the fixed point representation. Equivalently, 2\u2212n is the resolution of the fixed point representation and a quantized version of s. Note that d\u00b7e is one choice of a rounding function and is not unique."}, {"heading": "4. Bit-width optimization across a deep network", "text": "In the absence of model fine-tuning, converting a floating point deep network into a fixed point deep network is essentially a process of introducing quantization noise into the neural network. It is well understood in fields like audio processing or digital communications that as the quantization noise increases, the system performance degrades. The effect of quantization can be accurately captured in a single quantity, the SQNR.\nIn deep learning, there is not a well-formulated relationship between SQNR and classification accuracy. However, it is reasonable to assume that in general higher quantization noise level leads to worse classification performance. Given that SQNR can be approximated theoretically and analyzed layer by layer, we focus on developing a theoretical framework to optimize for the SQNR. We then conduct empirical investigations into how the proposed optimization for SQNR affect classification accuracy of the DCN."}, {"heading": "4.1. Impact of quantization on SQNR", "text": "In this section, we will derive the relationship between the quantization of the weight, bias and activation values respectively, and the resulting output SQNR."}, {"heading": "4.1.1. QUANTIZATION OF INDIVIDUAL VALUES", "text": "Quantization of individual values in a DCN, whether it is an activation or weight value, readily follows the quantizer discussion in Section 3.1. For instance, for weight value w, the quantized version w\u0303 can be written as:\nw\u0303 = w + nw, (3)\nwhere nw is the quantization noise. As illustrated in Figure 1, if w approximately follows a uniform, Gaussian, Laplacian or Gamma distribution, the SQNR, \u03b3w, as a result of the quantization process can be written as:\n10 log(\u03b3w) = 10 log E[w2]\nE[n2w] \u2248 \u03ba \u00b7 \u03b2, (4)\nwhere \u03ba is the quantization efficiency and \u03b2 is the quantizer bit-width."}, {"heading": "4.1.2. QUANTIZATION OF BOTH ACTIVATIONS AND WEIGHTS", "text": "Consider the case where weight w is multiplied by activation a, where both w and a are quantized with quantization noise nw and na, respectively. The product can be approximated, for small nw and na, as follows:\nw\u0303 \u00b7 a\u0303 = (w + nw) \u00b7 (a+ na) = w \u00b7 a+ w \u00b7 na + nw \u00b7 a+ nw \u00b7 na \u223c= w \u00b7 a+ w \u00b7 na + nw \u00b7 a.\n(5)\nThe last equality holds if |na| << |a| and |nw| << |w|. A very important observation is that the SQNR of the product, w \u00b7 a, as a result of quantization, satisfies\n1\n\u03b3w\u00b7a =\n1\n\u03b3w +\n1\n\u03b3a . (6)\nThis is characteristic of a linear system. The defining benefit of this realization is that introducing quantization noise to weights and activations independently is equivalent to adding the total noise after the product operation in a normalized system. This property will be used in later analysis."}, {"heading": "4.1.3. FORWARD PASS THROUGH ONE LAYER", "text": "In a DCN with multiple layers, computation of the ith activation in layer l+1 of the DCN can be expressed as follows:\na (l+1) i = N\u2211 j=1 w (l+1) i,j a (l) j + b (l+1) i , (7)\nwhere (l) represents the lth layer, N represents number of additions, wi,j represents the weight and bi represents the bias.\nIgnoring the bias term for the time being, since a(i+1)i is simply a sum of terms like w(l+1)i,j a (l) j , which when quantized all have the same SQNR \u03b3w(l+1)\u00b7a(l) . Assuming the product terms w(l+1)i,j a (l) j are independent, it follows that the value of a(i+1)i , before further quantization, has inverse SQNR that equals\n1\n\u03b3 w\n(l+1) i,j a (l) j\n= 1\n\u03b3 w\n(l+1) i,j\n+ 1\n\u03b3 a (l) j\n= 1\n\u03b3w(l+1) +\n1\n\u03b3a(l) (8)\nAfter a(l+1)i is quantized to the assigned bit-width, the resulting inverse SQNR then becomes 1\u03b3\na(l+1) + 1\u03b3 w(l+1) +\n1 \u03b3 a(l) . We are not considering the biases in this analysis because, assuming that the biases are quantized at the same bit-width as the weights, the SQNR is dominated by the product term w(l+1)i,j a (l) j . Note that Equation 8 matches rather well with experiments as shown in Section 5.3, even though the independence assumption ofw(l+1)i,j a (l) j does not always hold."}, {"heading": "4.1.4. FORWARD PASS THROUGH ENTIRE NETWORK", "text": "Equation 8 can be generalized to all the layers in a DCN (although we have found empirically that the approximation applies better for convolutional layers than fully-connected layers). Consider layer L in a deep network, where all the activations and weights are quantized. Extending Equation 8, we obtain the SQNR (\u03b3output) at the output of layer L as:\n1\n\u03b3output =\n1\n\u03b3a(0) +\n1\n\u03b3w(1) +\n1 \u03b3a(1) +\u00b7 \u00b7 \u00b7+ 1 \u03b3w(L) + 1 \u03b3a(L) (9)\nIn other word, the SQNR at the output of a layer in DCN is the Harmonic Mean of the SQNRs of all preceding quantization steps. This simple relationship reveals some very interesting insights:\n\u2022 All the quantization steps contribute equally to the overall SQNR of the output, regardless if it\u2019s the quantization of weights, activations, or input, and irrespective of where it happens (at the top or bottom of the network). \u2022 Since the output SQNR is the harmonic mean, the network performance will be dominated by the worst quantization step. For example, if the activations of a particular layer has a much smaller bit-width than other layers, it will be the bottleneck of network performance, because based on Equation 9, \u03b3output \u2264 \u03b3a(l) for all l.\n\u2022 Depth makes quantization more challenging, but not exceedingly so. The rest being the same, doubling the depth of a DCN will half the output SQNR (3dB loss). But this loss can be readily recovered by adding 1 bit to the bit-width of all weights and activations, assuming the quantization efficiency is more than 3dB/bit. However, this theoretical prediction will need to be empirically verified in future works."}, {"heading": "4.1.5. EFFECTS OF OTHER NETWORK COMPONENTS", "text": "\u2022 Batch normalization: Batch normalization (Ioffe & Szegedy, 2015) improves the speed of training a deep network by normalizing layer inputs. After the network is trained, the batch normalization layer is a fixed linear transformation and can be absorbed into the neighboring convolutional layer or fully-connected layer. Therefore, the quantization effect due to batch normalization does not need to be explicitly modeled.\n\u2022 ReLU: In Equation 7, for simplicity we omitted the activation function applied to a(l)j . When the activation function is ReLU and the quantization noise is small, all the positive values at the input to the activation function will have the same SQNR at the output, and the negative values become zero (effectively reducing the number of additions, N ). In other words,\na (l+1) i = \u2211N j=1 w (l+1) i,j g(a (l) j ) + b (l+1) i\n= \u2211M j=1 w (l+1) i,j a (l) j + b (l+1) i ,\n(10)\nwhere g(\u00b7) is the ReLU function and M \u2264 N is the number of a(l)j \u2019s that are positive.\nIn this case, the ReLU function has little impact on the SQNR of a(l+1)i . ReLU only starts to affect SQNR calculation when the perturbation caused by quantization is sufficiently large to alter the sign of a(l)j . Therefore, our analysis may become increasingly inaccurate as the bit-width becomes too small (quantization noise too large).\n\u2022 Non-ReLU activations: Other nonlinear activation functions such as tanh, sigmoid, PReLU functions are much harder to model and analyze. However, in Section 5.1 we will see that applying the analysis in this section to a network with PReLU activation functions still yields useful enhancements."}, {"heading": "4.2. Cross-layer bit-width optimization", "text": "From Equation 9, it is seen that trade-offs can be made between quantizers of different layers to produce the same\n\u03b3output. That is to say, we can choose to use smaller bitwidths for some layers by increasing bit-widths for other layers. For example, this may be desirable because of the following reasons:\n\u2022 Some layers may require a large number of computations (multiply-accumulate operations). Reducing the bit-widths for these layers would reduce the overall network computation load. \u2022 Some layers may contain a large number of network parameters (weights). Reducing the weight bit-widths for these layers would reduce the overall model size.\nInterestingly, such objectives can be formulated as an optimization problem and solved exactly. Suppose our goal is to reduce model size while maintaining a minimum SQNR at the DCN output. We can use \u03c1i as the scaling factor at quantization step i, which in this case represents the number of parameters being quantized in the quantization step. The problem can be written as:\nmin\u03b3i \u2211 i \u03c1i\n( 10 log \u03b3i\n\u03ba\n) , s.t. \u2211 i 1\n\u03b3i \u2264 1 \u03b3min (11)\nwhere 10 log \u03b3i is the SQNR in dB domain, and (10 log \u03b3i)/\u03ba is the bit-width in the ith quantization step according to Equation 2. \u03b3min is the minimum output SQNR required to achieve a certain level of accuracy. The optimization constraint follows from Equation 9 that the output SQNR is the harmonic mean of the SQNR of intermediate quantization steps.\nSubstituting by \u03bbi = 1\n\u03b3i and removing the constant scalars\nfrom the objective function, the problem can be reformulated as:\nmin\u03bbi \u2212 \u2211 i \u03c1i log \u03bbi, s.t. \u2211 i \u03bbi \u2264 C (12)\nwhere the constant C = 1\n\u03b3min . This is a classic convex\noptimization problem with the water-filling solution (Boyd & Vandenberghe, 2004):\n\u03c1i \u03bbi = constant.\nOr equivalently,\n\u03c1i\u03b3i = constant (13)\nRecognizing that 10 log \u03b3i = \u03ba\u03b2i based on Equation 2, the solution can be rewritten as:\n10 log \u03c1i \u03ba + \u03b2i = constant (14)\nIn other words, the difference between the optimal bitwidths of two quantization steps is inversely proportional to the difference of \u03c1\u2019s in dB, scaled by the quantization efficiency.\n\u03b2i \u2212 \u03b2j = 10 log(\u03c1j/\u03c1i)\n\u03ba (15)\nThis is a surprisingly simple and insightful relationship. For example, assuming \u03ba = 3dB/bit, the bit-widths \u03b2i and \u03b2j would differ by 1 bit if \u03c1j is 3dB (or 2x) larger than \u03c1i. More specifically, for model size reduction, layers with more parameters should use relatively lower bit-width, as it leads to better model compression under the overall SQNR constraint."}, {"heading": "5. Experiments", "text": "In this section we study the effect of reduced bit-width for both weights and activations versus traditional 32- bit single-precision floating point approach (16-bit halfprecision floating point is expected to produce comparable results as single-precision in most cases). In particular, we will implement the fixed point quantization algorithm described in Section 3 and investigate the effectiveness of the bit-width optimization algorithm in Section 4. In addition, using the quantized fixed point network as the starting point, we will further fine-tune the fixed point network within the restricted alphabets of weights and activations."}, {"heading": "5.1. Bit-width optimization for CIFAR-10 classification", "text": "We evaluate our proposed cross-layer bit-width optimization algorithm on the CIFAR-10 benchmark using the algorithm prescribed in Section 4.2.\nIn Table 2, we compute the number of parameters in each layer of our CIFAR-10 network. Consider the objective of minimizing the overall model size. Provided that the quantization efficiency \u03ba = 3dB/bit, our derivation in Section 4.2 shows that the optimal bit-width of layer conv0 and conv1 would differ by 10 log(0.295/0.007)/\u03ba = 5bits. Similarly, assuming the bit-width for layer conv0 is \u03b20, the subsequent convolutional layers should have bit-width values as indicated in Table 3.\nIn our experiment in this section, we will ignore the fullyconnected layer and assume a fixed bit-width of 16 for both weights and activations. This is because fully-connected layers have different SQNR characteristics and need to be optimized separately. Although the fully-connected lay-\ners can often be quantized more aggressively than convolutional layers, since the number of parameters of fc0 is very small in this experiment, we will set the bit-width to a large value to eliminate the impact of quantizing fc0 from the analysis, knowing that the large bit-width of fc0 has very little impact on the overall model size. We will also set the activation bit-widths of all the layers to a large value of 16 because they do not affect the model size.\nFigure 3 displays the model size vs. error rate in a scatter plot, we can clearly see the advantage of crosslayer bit-width optimization. When the model size is large (bit-width is high), the error rate saturates at around 6.9%. When the model size reduces below approximately 25Mbits, the error rate starts to increase quickly as the model size decreases. In this region, cross-layer bit-width optimization offers > 20% reduction in model size for the same performance."}, {"heading": "5.2. Bit-width optimization for ImageNet classification", "text": "In Section 5.1, we performed cross-layer bit-width optimization for our CIFAR-10 network with the objective of minimizing the overall model size while maintaining accuracy. Here we carry out a similar exercise for an AlexNetlike DCN that is trained on ImageNet-1000. The DCN architecture is described in Table 4.\nFor setting the bit-width of convolutional layers of this DCN, we follow the steps in Section 5.1 with the assumption that the bit-width for layer conv1 is \u03b21. The resulting\nbit-width allocation for all convolutional layers is summarized in Table 5.\nFor fully-connected layers we first keep the network as floating point and quantize the weights of fully-connected layers only. We then reduce bit-width of fully-connected layers until the classification accuracy starts to degrade. We found that the minimum bit-width for the fully-connected layers before performance degradation occurs is 6.\nFigure 4 depicts the convolutional layer model size vs. top5 error rate tradeoff for both optimized bit-width and equal bit-width scenarios. Similar to our CIFAR-10 network, there is a clear benefit of cross-layer bit-width optimization in terms of model size reduction. In some regions the saving can be upto 1Mbits.\nHowever, unlike our CIFAR-10 network where the convo-\nlutional layers make up most of the model size, in AlexNetlike DCN the fully-connected layers dominate in terms of number of parameters. With bit-width of 6, the overall size of fully-connected layers is more than 100Mbits. This means that the saving of 1Mbits brings in less than 1% of overall model size reduction! This is in clear contrast to the 20% model size reduction we reported for our CIFAR-10 network. Therefore, it is worth nothing that the proposed cross-layer layer bit-width optimization algorithm is most effective when the network size is dominated by convolutional layers, and is less effective otherwise.\nThe experiments on our CIFAR-10 network and AlexNetlike network demonstrate that the proposed cross-layer bitwidth optimization offers clear advantage over the equal bit-width scheme. As summarized in Table 3 and Table 5, a simple offline computation of the inter-layer bit-width relationship is all that is required to perform the optimization.\nHowever, in the absence of customized design, the implementation of the optimized bit-widths can be limited by the software or hardware platform on which the DCN operates. Often, the optimized bit-width needs to be rounded up to the next supported bit-width, which may in turn impact the network classification accuracy and model size."}, {"heading": "5.3. Validation for SQNR prediction", "text": "To verify that our SQNR calculation presented in Section 4.1 is valid, we will conduct a small experiment. More specifically, we will focus on the optimized networks in Figure 3 and compare the measured SQNR per layer to the SQNR predictions according to Equation 4 and 8.\nTable 6 contains the comparison between the theoretical SQNR and the measured SQNR (in dB) for layers conv1 to conv5 for two of the optimized networks. We observe that while the two SQNR values do not match numerically, they follow similar decreasing trend as the activations propagate deeper into the network. It should be noted that our theoretical SQNR predictions are based purely on the weight and activation bit-widths of each layer as well as the quantization efficiency \u03ba. The theory does not rely on any in-\nformation related to the network parameters or the data it is tested on."}, {"heading": "5.4. Model fine-tuning", "text": "Although our focus is fixed point implementation without training, our quantizer design can also be used as a starting point for further model fine-tuning when the training model and training parameters are available.\nTable 7 contains the classification error rate (in %) for the CIFAR-10 network after fine-tuning the model for 30 epochs. We experiment with different weight and activation bit-width combinations, ranging from floating point to 4-bit, 8-bit, and 16-bit fixed point. It is shown that even the (4b, 4b) bit-width combination works well (8.30% error rate) when the network is fine-tuned after quantization. In addition, the (float, 8b) setting generates an error rate of 6.78%, which is the new state-of-the-art result even though the activations are only 8-bit fixed point values. This may be attributed to the regularization effect of the added quantization noise (Lin et al., 2015; Luo & Yang, 2014)."}, {"heading": "6. Conclusions", "text": "Fixed point implementation of deep networks is important for real world embedded applications, which involve real time processing with limited power budget. In this paper, we develop a principled approach to converting a pretrained floating point DCN model to its fixed point equivalent. We show that the naive method of quantizing all the layers in the DCN with uniform bit-width value results in DCN networks with subpar performance in terms of error rates relative to our proposed approach of SQNR based optimization of bit-widths. Specifically, we present results for a floating point DCN trained CIFAR-10 benchmark, which on conversion to its fixed point counter-part results in >20 % reduction in model size without any loss in accuracy. We note that our proposed algorithm facilitates easy conversion of any off-the-shelf DCN model for efficient real world on-device application. Finally, through fine-tuning experiments we demonstrate that our quantizer design methodology is a useful starting point for further model fine-tuning after the floating-point-to-fixed-point conversion."}, {"heading": "Acknowledgements", "text": "We would like to acknowledge fruitful discussions and valuable feedback from our colleagues: David Julian, Anthony Sarah, Daniel Fontijne, Somdeb Majumdar, Aniket Vartak, Blythe Towal, and Mark Staskauskas."}], "references": [{"title": "Convex Optimization", "author": ["Boyd", "Stephen", "Vandenberghe", "Lieven"], "venue": null, "citeRegEx": "Boyd et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boyd et al\\.", "year": 2004}, {"title": "Return of the devil in the details: Delving deep into convolutional nets", "author": ["K. Chatfield", "S. Simonyan", "V. Vedaldi", "A. Zisserman"], "venue": null, "citeRegEx": "Chatfield et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chatfield et al\\.", "year": 2014}, {"title": "Compressing neural networks with the hashing trick", "author": ["W. Chen", "J.T. Wilson", "S. Tyree", "K.Q. Weinberger", "Y. Chen"], "venue": "In ICML,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Low precision arithmetic for deep learning", "author": ["M. Courbariaux", "Y. Bengio", "J. David"], "venue": null, "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "New types of deep neural network learning for speech recognition and related applications: an overview", "author": ["L. Deng", "G.E", "Hinton", "B. Kingsbury"], "venue": "In IEEE International Conference on Acoustic, Speech and Signal Processing,", "citeRegEx": "Deng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2013}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "In CVPR,", "citeRegEx": "Girshick et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Girshick et al\\.", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Y. Gong", "L. Liu", "M. Yang", "L. Bourdev"], "venue": null, "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": null, "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "A deep neural network compression pipeline: Pruning, quantization", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": "Huffman encoding", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2014\\E", "shortCiteRegEx": "He et al\\.", "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling knowledge in a neural network", "author": ["G. Hinton", "O. Vinyals", "J. Dean"], "venue": "In Proc. Deep Learning and Representation Learning NIPS Workshop,", "citeRegEx": "Hinton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0 and -1", "author": ["H. Kyuyeon", "W. Sung"], "venue": "In IEEE Workshop on Signal Processing Systems (SiPS),", "citeRegEx": "Kyuyeon and Sung,? \\Q2014\\E", "shortCiteRegEx": "Kyuyeon and Sung", "year": 2014}, {"title": "Overcoming challenges in fixed point training of deep convolutional networks", "author": ["D.D. Lin", "S.S. Talathi"], "venue": "In ICML Workshop,", "citeRegEx": "Lin and Talathi,? \\Q2016\\E", "shortCiteRegEx": "Lin and Talathi", "year": 2016}, {"title": "Neural networks with few multiplications", "author": ["Z. Lin", "M. Courbariaux", "R. Memisevic", "Y. Bengio"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "XNOR-Net: ImageNet classification using binary convolutional neural networks", "author": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": null, "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "CNN features off-the-shelf: An astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "Carlsson"], "venue": "In CVPR,", "citeRegEx": "Razavian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Razavian et al\\.", "year": 2014}, {"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["A. Sajid", "H. Kyuyeon", "W. Sung"], "venue": "In IEEE International Conference on Acoustic, Speech and Signal Processing,", "citeRegEx": "Sajid et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sajid et al\\.", "year": 2015}, {"title": "OverFeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": null, "citeRegEx": "Sermanet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2013}, {"title": "Image and Video Compression for Multimedia Engineering: Fundamentals, Algorithms, and Standards", "author": ["Shi", "Yun Q", "Sun", "Huifang"], "venue": null, "citeRegEx": "Shi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2008}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": null, "citeRegEx": "Simonyan and Zisserman,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2014}, {"title": "Improving the speed of neural networks on CPUs", "author": ["V. Vanhoucke", "A. Senior", "M. Mao"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "Recent advances in the development of deep convolution networks (DCNs) have led to significant progress in solving non-trivial machine learning problems involving image recognition (Krizhevsky et al., 2012) and speech recognition (Deng et al.", "startOffset": 181, "endOffset": 206}, {"referenceID": 4, "context": ", 2012) and speech recognition (Deng et al., 2013).", "startOffset": 31, "endOffset": 50}, {"referenceID": 1, "context": "advances in the design of DCNs (Zeiler & Fergus, 2014; Simonyan & Zisserman, 2014; Szegedy et al., 2014; Chatfield et al., 2014; He et al., 2014; Ioffe & Szegedy, 2015) have not only led to a further boost in achieved accuracy on image recognition tasks but also have played a crucial role as a feature generator for other machine learning tasks such as object detection (Krizhevsky et al.", "startOffset": 31, "endOffset": 168}, {"referenceID": 9, "context": "advances in the design of DCNs (Zeiler & Fergus, 2014; Simonyan & Zisserman, 2014; Szegedy et al., 2014; Chatfield et al., 2014; He et al., 2014; Ioffe & Szegedy, 2015) have not only led to a further boost in achieved accuracy on image recognition tasks but also have played a crucial role as a feature generator for other machine learning tasks such as object detection (Krizhevsky et al.", "startOffset": 31, "endOffset": 168}, {"referenceID": 13, "context": ", 2014; Ioffe & Szegedy, 2015) have not only led to a further boost in achieved accuracy on image recognition tasks but also have played a crucial role as a feature generator for other machine learning tasks such as object detection (Krizhevsky et al., 2012) and localization (Sermanet et al.", "startOffset": 233, "endOffset": 258}, {"referenceID": 20, "context": ", 2012) and localization (Sermanet et al., 2013), semantic segmentation (Girshick et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 5, "context": ", 2013), semantic segmentation (Girshick et al., 2014) and image retrieval (Krizhevsky et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 13, "context": ", 2014) and image retrieval (Krizhevsky et al., 2012; Razavian et al., 2014).", "startOffset": 28, "endOffset": 76}, {"referenceID": 18, "context": ", 2014) and image retrieval (Krizhevsky et al., 2012; Razavian et al., 2014).", "startOffset": 28, "endOffset": 76}, {"referenceID": 1, "context": "These advances have come with an added cost of computational complexity, resulting from DCN designs involving any combinations of: increasing the number of layers in the DCN (Szegedy et al., 2014; Simonyan & Zisserman, 2014; Chatfield et al., 2014), increasing the number of filters per convolution layer (Zeiler & Fergus, 2014), decreasing stride per convolution layer (Sermanet et al.", "startOffset": 174, "endOffset": 248}, {"referenceID": 20, "context": ", 2014), increasing the number of filters per convolution layer (Zeiler & Fergus, 2014), decreasing stride per convolution layer (Sermanet et al., 2013; Simonyan & Zisserman, 2014) and hybrid architectures that combine various DCN layers (Szegedy et al.", "startOffset": 129, "endOffset": 180}, {"referenceID": 9, "context": ", 2013; Simonyan & Zisserman, 2014) and hybrid architectures that combine various DCN layers (Szegedy et al., 2014; He et al., 2014; Ioffe & Szegedy, 2015).", "startOffset": 93, "endOffset": 155}, {"referenceID": 17, "context": "the second approach may produce networks with superior accuracy numbers (Rastegari et al., 2016; Lin & Talathi, 2016), it requires tight integration between the network design, training and implementation, which is not always feasible.", "startOffset": 72, "endOffset": 117}, {"referenceID": 3, "context": "Fixed point implementation of DCNs has been explored in earlier works (Courbariaux et al., 2014; Gupta et al., 2015).", "startOffset": 70, "endOffset": 116}, {"referenceID": 7, "context": "Fixed point implementation of DCNs has been explored in earlier works (Courbariaux et al., 2014; Gupta et al., 2015).", "startOffset": 70, "endOffset": 116}, {"referenceID": 3, "context": "Fixed point implementation of DCNs has been explored in earlier works (Courbariaux et al., 2014; Gupta et al., 2015). These works primarily focused on training DCNs using low precision fixed-point arithmetic. More recently, Lin et al. (2015) showed that deep neural networks can be effectively trained using only binary weights, which in some cases can even improve classification accuracy relative to the floating point baseline.", "startOffset": 71, "endOffset": 242}, {"referenceID": 19, "context": "In a follow-up paper (Sajid et al., 2015), the authors applied their proposed algorithm to DCN models where they analyzed the quantization sensitivity of the network for each layer and then manually decide the quantization bit-widths.", "startOffset": 21, "endOffset": 41}, {"referenceID": 16, "context": "The works of Kyuyeon & Sung (2014); Sajid et al. (2015) more closely resemble our work.", "startOffset": 36, "endOffset": 56}, {"referenceID": 16, "context": "The works of Kyuyeon & Sung (2014); Sajid et al. (2015) more closely resemble our work. In Kyuyeon & Sung (2014), the authors propose a floating point to fixed point conversion algorithm for fully-connected networks.", "startOffset": 36, "endOffset": 113}, {"referenceID": 16, "context": "The works of Kyuyeon & Sung (2014); Sajid et al. (2015) more closely resemble our work. In Kyuyeon & Sung (2014), the authors propose a floating point to fixed point conversion algorithm for fully-connected networks. The authors used an exhaustive search strategy to identify optimal fixed point bit-width for the entire network. In a follow-up paper (Sajid et al., 2015), the authors applied their proposed algorithm to DCN models where they analyzed the quantization sensitivity of the network for each layer and then manually decide the quantization bit-widths. Other works that are somewhat closely related are Vanhoucke et al. (2011); Gong et al.", "startOffset": 36, "endOffset": 639}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed.", "startOffset": 8, "endOffset": 52}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed. Gong et al. (2014) on the other hand applied codebook based on scalar and vector quantization methods in order to reduce the model size.", "startOffset": 8, "endOffset": 205}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed. Gong et al. (2014) on the other hand applied codebook based on scalar and vector quantization methods in order to reduce the model size. In the spirit of Sajid et al. (2015), we also focus on optimizing DCN models that are pre-trained with floating point precision.", "startOffset": 8, "endOffset": 360}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed. Gong et al. (2014) on the other hand applied codebook based on scalar and vector quantization methods in order to reduce the model size. In the spirit of Sajid et al. (2015), we also focus on optimizing DCN models that are pre-trained with floating point precision. However, as opposed to exhaustive search method adopted by Sajid et al. (2015), our objective is to convert the pre-trained DCN model into a fixed-point model using an optimization strategy based on signal-toquantization-noise-ratio (SQNR).", "startOffset": 8, "endOffset": 531}, {"referenceID": 6, "context": "(2011); Gong et al. (2014). Vanhoucke et al. (2011) quantized the weights and activations of pre-trained deep networks using 8-bit fixed-point representation to improve inference speed. Gong et al. (2014) on the other hand applied codebook based on scalar and vector quantization methods in order to reduce the model size. In the spirit of Sajid et al. (2015), we also focus on optimizing DCN models that are pre-trained with floating point precision. However, as opposed to exhaustive search method adopted by Sajid et al. (2015), our objective is to convert the pre-trained DCN model into a fixed-point model using an optimization strategy based on signal-toquantization-noise-ratio (SQNR). In doing so, we aim to improve upon the inference speed of the network and reduce storage requirements. The benefit of our approach as opposed to the brute force method is that it is grounded in a theoretical framework and offers an analytical solution for bit-width choice per layer to optimize the SQNR for the network. This offers an easier path to generalize to networks with significantly large number of layers such as the one recently proposed by He et al. (2015).", "startOffset": 8, "endOffset": 1164}, {"referenceID": 2, "context": "(2014), (b) compressing neural networks using hashing (Chen et al., 2015), and (c) combining pruning and quantization during training to reduce the model size without affecting the accuracy (Han et al.", "startOffset": 54, "endOffset": 73}, {"referenceID": 8, "context": ", 2015), and (c) combining pruning and quantization during training to reduce the model size without affecting the accuracy (Han et al., 2015).", "startOffset": 124, "endOffset": 142}, {"referenceID": 9, "context": "Other approaches to handle complexity of deep networks include: (a) leveraging high complexity networks to boost performance of low complexity networks, as proposed in Hinton et al. (2014), (b) compressing neural networks using hashing (Chen et al.", "startOffset": 168, "endOffset": 189}, {"referenceID": 16, "context": "This may be attributed to the regularization effect of the added quantization noise (Lin et al., 2015; Luo & Yang, 2014).", "startOffset": 84, "endOffset": 120}], "year": 2016, "abstractText": "In recent years increasingly complex architectures for deep convolution networks (DCNs) have been proposed to boost the performance on image recognition tasks. However, the gains in performance have come at a cost of substantial increase in computation and model storage resources. Fixed point implementation of DCNs has the potential to alleviate some of these complexities and facilitate potential deployment on embedded hardware. In this paper, we propose a quantizer design for fixed point implementation of DCNs. We formulate and solve an optimization problem to identify optimal fixed point bit-width allocation across DCN layers. Our experiments show that in comparison to equal bitwidth settings, the fixed point DCNs with optimized bit width allocation offer> 20% reduction in the model size without any loss in accuracy on CIFAR-10 benchmark. We also demonstrate that fine-tuning can further enhance the accuracy of fixed point DCNs beyond that of the original floating point model. In doing so, we report a new state-of-the-art fixed point performance of 6.78% error-rate on CIFAR-10 benchmark.", "creator": "LaTeX with hyperref package"}}}