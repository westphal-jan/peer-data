{"id": "1511.06406", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Denoising Criterion for Variational Auto-Encoding Framework", "abstract": "denoising autoencoders ( dae ) often are trained to quickly reconstruct their own clean input driver with noisy noise signal injected slightly at the appropriate input error level, while external variational autoencoders ( the vae ) drivers are initially trained with noise injected in filling their stochastic hidden layer, with a regularizer parameter that encourages this noise above injection. in this paper, technically we show that algorithms injecting noise trapped both in damaged input and in the stochastic error hidden layer can partly be therefore advantageous too and we propose requiring a strongly modified variational binary lower bound functions as being an presumably improved objective criteria function in this setup. if error noise is well injected in defective input, then the null standard vae lower bound involves marginalizing from the encoder against conditional optimal distribution over the input noise, obviously which makes optimization the appropriate training outcome criterion intractable. therefore instead, we not propose a modified training criteria criterion which supposedly corresponds to a tighter bound, essentially when experimental noise mixture is injected differently in input. experimentally, we explicitly find furthermore that the wrongly proposed denoising selected variational autoencoder ( dvae ) formula yields better average log - likelihood algorithms than matching the dummy vae criterion and thus the higher importance weighted auto - measured encoder on the underlying mnist and frey with face datasets.", "histories": [["v1", "Thu, 19 Nov 2015 21:56:21 GMT  (3409kb,D)", "http://arxiv.org/abs/1511.06406v1", "ICLR conference submission"], ["v2", "Mon, 4 Jan 2016 15:12:46 GMT  (3592kb,D)", "http://arxiv.org/abs/1511.06406v2", "ICLR conference submission"]], "COMMENTS": "ICLR conference submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel jiwoong im", "sungjin ahn", "roland memisevic", "yoshua bengio"], "accepted": true, "id": "1511.06406"}, "pdf": {"name": "1511.06406.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Daniel Jiwoong Im", "Sungjin Ahn", "Roland Memisevic", "Yoshua Bengio"], "emails": ["imdaniel@iro.umontreal.ca", "ahnsungj@iro.umontreal.ca", "memisevr@iro.umontreal.ca", "<findme>@iro.umontreal.ca"], "sections": [{"heading": "1 INTRODUCTION", "text": "Variational inference (Jordan et al., 1999) has been a core component of approximate Bayesian inference along with the Markov chain Monte Carlo (MCMC) method (Neal, 1993). It has been popular to many researchers and practitioners because the problem of learning an intractable posterior distribution is formulated as an optimization problem which has many advantages compared to MCMC; (i) we can easily take advantage of many advanced optimization tools (Kingma & Ba, 2014b; Duchi et al., 2011; Zeiler, 2012), (ii) the training by optimization is usually faster than the MCMC sampling, and (iii) unlike MCMC where it is difficult to decide when to finish the sampling, the stopping criterion in variational inference is clear.\nOne remarkable recent advance in variational inference is to use the inference network (also known as the recognition network) as the approximate posterior distribution (Kingma & Welling, 2014; Rezende & Mohamed, 2014; Dayan et al., 1995; Bornschein & Bengio, 2014). Unlike the traditional variational inference where different variational parameters are required for each latent variable, in the inference network, the approximate posterior distribution for each latent variable is conditioned on an observation and the parameters are shared among the latent variables. Combined with advances in training techniques such as the re-parameterization trick and the REINFORCE (Williams, 1992; Mnih & Gregor, 2014), it became possible to train variational inference models efficiently for large-scale datasets.\nDespite these advances, it is still a major problem to obtain a class of variational distributions which is flexible enough to accurately model the true posterior distribution. For instance, in the variational autoencoder (VAE), in order to achieve efficient training, each dimension of the latent variable is assumed to be independent each other (i.e., there are factorized) and modeled by a univariate Gaussian distribution whose parameters (i.e., the mean and the variance) are obtained by a nonlinear projection of the input using a neural network. Even when VAE performs well in practice for a rather\n\u2020CIFAR Senior Fellow\nar X\niv :1\n51 1.\n06 40\n6v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\nsimple problems such as generating small and simple images (e.g., MNIST), it is still required to relax this strong restriction on the variational distributions in order to apply it to more complex realworld problems. Recently, there have been efforts in this direction. Salimans et al. (2015) integrated MCMC steps into the variational inference such that the variational distribution becomes closer to the target distribution as it takes more MCMC steps inside each iteration of the variational inference. Similar ideas but applying a sequence of invertible non-linear transformations rather than MCMC are also proposed by Dinh et al. (2015) and Rezende & Mohamed (2015).\nOn the other hand, the denoising criterion, where the input is corrupted by adding some noise and the model is asked to recover the original input, has been studied extensively for deterministic generative models (Seung, 1998; Vincent et al., 2008; Bengio et al., 2013). The study showed that the denoising criterion plays an important role in achieving good generalization performance (Vincent et al., 2008) because it makes the nearby data points in the low dimensional manifold to be robust against the presence of small noise in the high dimensional observation space (Seung, 1998; Vincent et al., 2008; Rifai, 2011; Alain & Bengio, 2014; Im et al., 2016). Therefore, it is natural to ask if the denoising criterion (where we add the noise to the inputs) can also be advantageous for the variational autoencoding framework where the noise is added to the latent variables, not the inputs, and if so, how can we formulate the problem for efficient training. Although it has not been considerably studied how to combine these, there has been some evidences of its usefulness1. For example, Rezende & Mohamed (2014) pointed out that injecting additional noise to the recognition model is crucial to achieve the reported accuracy for unseen data, advocating that in practice denoising can help the regularization of probabilistic generative models as well.\nIn this paper, motivated by the DAE and the VAE, we study the denoising criterion for variational inference based on recognition networks, which we call the variational auto-encoding framework throughout. Our main contributions are as follows. We introduce a new class of approximate distributions where the recognition network is obtained by marginalizing the input noise over a corruption distribution, and thus provides capacity to obtain a more flexible approximate distribution class such as the mixture of Gaussian. Because applying this approximate distribution to the standard VAE objective makes the training intractable, we propose a new objective, called the denoising variational lower bound, and show that, given a sensible corruption function, this is (i) tighter than the standard variational lower bound on noisy inputs, (ii) efficient to train, and (iii) easily applicable to many existing models such as the variational autoencoder, the importance reweighted autoencoder (IWAE) (Burda et al., 2015), and the neural variational inference and learning (NVIL) (Mnih & Gregor, 2014). In the experiments, we empirically demonstrate that the proposed denoising criterion for variational auto-encoding framework helps to improve the performance in both the variational autoencoders and the importance weighted autoencoders (IWAE) on the binarized MNIST dataset and the Frey Face dataset."}, {"heading": "2 BACKGROUND", "text": "Variational inference is an approximate inference method where the goal is to approximate the intractable posterior distribution p(z|x), by a tractable approximate distribution q\u03c6(z). Here, x is the observation and z \u2208 RD is the model parameters or latent variables. To keep it tractable, the approximate distributions are limited to a restricted family of distributions q\u03c6 \u2208 Q parameterized by variational parameters \u03c6. For example, in the mean-field variational inference (Jordan et al., 1999), the distributions in Q treat all dependent variables as independent, i.e., q(z) = \u220f qd(zd).\nThe basic idea of obtaining the optimal approximate distribution q\u03c6\u2217 \u2208 Q is to find the variational parameter \u03c6\u2217 that minimizes the Kullback-Leibler (KL) divergence between the approximate distribution q\u03c6 and the target distribution p. Although the KL divergence itself involves the intractable target distribution, instead of directly minimizing the KL divergence, we can bypass it by decomposing the marginal log-likelihood as follows:\nlog p(x) = Eq\u03c6(z) [ log p(x, z)\nq\u03c6(z)\n] + KL(q\u03c6(z)||p(z|x)). (1)\n1In practice, it turned out to be useful to augment the dataset by adding some random noise to the inputs. However, in denoising criterion, unlike the augmenting, the model tries to recover the original data, not the corrupted one.\nThat is, observing that the marginal log-likelihood log p(x) is independent of the variational distribution q\u03c6 and the KL term is non-negative, instead of minimizing the KL divergence, we can maximize the first term, called the variational lower bound, which is the same as minimizing the KL divergence term. Thus, in variational inference, we transform the problem of learning a distribution to an optimization problem of maximizing the variational lower bound with respect to the variational parameter \u03c6."}, {"heading": "2.1 VARIATIONAL AUTOENCODERS", "text": "The variational autoencoder (VAE) (Kingma & Welling, 2014) is a particular type of variational inference framework which is closely related to our focus in this work. With the VAE, the posterior distribution is defined as p\u03b8(z|x) \u221d p\u03b8(x|z)p(z). Specifically, we define a prior p(z) on the latent variable z \u2208 RD, which is usually set to an isotropic Gaussian distribution N (0, \u03c3ID). Then, we use a parameterized distribution to define the observation model p\u03b8(x|z). A typical choice for the parameterized distribution is to use a neural network where the input is z and output a parametric distribution over x, such as the Gaussian or Bernoulli, depending on the data type. Then, \u03b8 becomes the weights of the neural network. We call this network p\u03b8(x|z) the generative network. Due to the complex nonlinearity of the neural network, the posterior distribution p\u03b8(z|x) is intractable. One interesting aspect of VAE is that the approximate distribution q is conditioned on the observation x, resulting in a form q\u03c6(z|x). Similar to the generative network, we use a neural network for q\u03c6(z|x) with x and z as its input and output, respectively. The variational parameter \u03c6, which is also the weights of the neural network, is shared among all observations. We call this network q\u03c6(z|x) the inference network, recognition network. The objective of VAE is to maximize the following variational lower bound with respect to the parameters \u03b8 and \u03c6.\nlog p\u03b8(x) \u2265 Eq\u03c6(z|x) [ log p\u03b8(x, z)\nq\u03c6(z|x)\n] (2)\n= Eq\u03c6(z|x) [log p\u03b8(x|z)]\u2212KL(q\u03c6(z|x)||p(z)). (3)\nNote that in Eqn. (3), we can interpret the first term as a reconstruction accuracy through an autoencoder with noise injected in the hidden layer that is the output of the inference network, and the second term as a regularizer which enforces the approximate posterior to be close to the prior and maximizes the entropy of the injected noise.\nThe earlier approaches to train this type of model were based on the variational EM algorithm: in the E-step, fixing \u03b8, we update \u03c6 such that the approximate distribution q\u03c6(z|x) close to the true posterior distribution p\u03b8(z|x), and then in the M-step, fixing \u03c6, we update \u03b8 to increase the marginal log-likelihood. However, with the VAE it is possible to apply the backpropagation on the variational parameter \u03c6 by using the re-parameterization trick (Kingma & Welling, 2014), considering z as a function of i.i.d. noise and of the output of the encoder (such as the mean and variance of the Gaussian). Armed with the gradient on these parameters, the gradient on the generative network parameters \u03b8 can readily be computed by back-propagation, and thus we can jointly update both \u03c6 and \u03b8 using efficient optimization algorithms such as the stochastic gradient descent.\nAlthough our exposition in the following proceeds mainly with the VAE model for simplicity, the proposed method can be applied to a more general class of variational inference methods which use the inference network q\u03c6(z|x) for the approximate distribution. This includes other recent models such as the importance weighted autoencoders (IWAE), the neural variational inference and learning (NVIL), and DRAW (Gregor et al., 2015)."}, {"heading": "3 DENOISING CRITERION IN VARIATIONAL FRAMEWORK", "text": "With the denoising autoencoder criterion (Seung, 1998; Vincent et al., 2008), the input is corrupted according to some noise distribution, and the model needs to learn to reconstruct the original input or maximize the log-probability of the clean input x, given the corrupted input x\u0303. Before applying the denoising criterion to the variational autoencoder, we shall investigate a synthesized inference formulation of VAE in order to comprehend the consequences of the denoising criterion.\nProposition 1. Let q\u03c6(z|x\u0303) be a Gaussian distribution such that q\u03c6(z|x\u0303) = N (z|\u00b5\u03c6(x\u0303), \u03c3\u03c6(x\u0303)) where \u00b5\u03c6(x\u0303) and \u03c3\u03c6(x\u0303) are non-linear functions of x\u0303. Let p(x\u0303|x) be a known corruption distribution around x. Then,\nEp(x\u0303|x) [q\u03c6(z|x\u0303)] = \u222b x\u0303 q\u03c6(z|x\u0303)p(x\u0303|x)dx\u0303 (4)\nis a mixture of Gaussian.\nDepending on whether the distribution is over a continuous or discrete variables, the integral in Equation 4 can be replaced by a summation. It is instructive to consider the distribution over discrete domain to see that Equation 4 has a form of mixture of Gaussian - that is to say, each time we sample x\u0303 \u223c p(x\u0303|x) and substitute into q(z|x\u0303), we get different Gaussian distributions. Example 1. Let x \u2208 {0, 1}D be a D-dimension observation, and consider a Bernoulli corruption distribution p\u03c0(x\u0303|x) = Ber(\u03c0) around the input x. Then,\nEp\u03c0(x\u0303|x) [q\u03c6(z|x\u0303)] = K\u2211 i=1 q\u03c6(z|x\u0303i)p\u03c0(x\u0303i|x) (5)\nhas the form of a finite mixture of Gaussian and the number of mixture component K is 2D.\nAs mentioned in the previous section, usually a feedforward neural network is used for the inference network. In the case of the Bernoulli distribution as a corrupting distribution and q\u03c6(z|x\u0303) is a Gaussian distribution, we will have 2D Gaussian mixture components and all of them share the parameter \u03c6. Example 2. Consider a Gaussian corruption model p(x\u0303|x) = N(x|0, \u03c3I). Let q\u03c6(z|x\u0303) be a Gaussian inference network. Then,\nEp(x\u0303|x) [q\u03c6(z|x\u0303)] = \u222b x\u0303 q\u03c6(z|x\u0303)p(x\u0303|x)dx\u0303. (6)\n1. If q\u03c6(z|\u03c6>x\u0303) = N (z|\u00b5 = \u03c6>x\u0303, \u03c3 = \u03c32I) such that the mean parameter is a linear model of weight vector \u03c6 and input x\u0303, then the Equation 6 is a Gaussian distribution.\n2. If q\u03c6(z|x\u0303) = N (z|\u00b5(x\u0303), \u03c3(x\u0303)) where \u00b5(x\u0303) and \u03c3(x\u0303) are non-linear functions of x\u0303, then the Equation 6 is an infinite mixture of Gaussian.\nIn practice, there will be infinitely many number of Gaussian mixture components as in the second case, all of whose parameters are predicted by a single neural network. In other words, the inference neural network will learn which Gaussian distribution is needed for the given input x\u03032.\nWe can see this corruption procedure as adding a stochastic layer to the bottom of the inference network. For example, we can define a corruption network p\u03c0(x\u0303|x) which is a neural network where the input is x and the output is stochastic units (e.g., Gaussian or Bernoulli distributions). Then, it is also possible to learn the parameter \u03c0 of the corruption network by backpropagation using the reparameterization trick. Note that a similar idea is explored in IWAE (Burda et al., 2015). However, our method is different in a sense that we use the denoising variational lower bound described below."}, {"heading": "3.1 THE DENOISING VARIATIONAL LOWER BOUND", "text": "Previously, we described that integrating the denoising criterion into the variational auto-encoding framework is equivalent to having a stochastic layer at the bottom of the inference network, and then estimating the variational lower bound becomes intractable because log q\u03a6(z|x) requires integrating out the noise x\u0303 for a corruption distribution. Before introducing the denoising variational lower bound, let us examine the variational lower bound when an additional stochastic layer is added to the inference network and integrate over the stochastic variables. Lemma 1. Consider an approximate posterior distribution of the following form:\nq\u03a6(z|x) = \u222b z\u2032 q\u03d5(z|z\u2032)q\u03c8(z\u2032|x)dz\u2032,\n2Note that the mixture components are encoded in a vector form.\nhere, we use \u03a6 = {\u03d5,\u03c8}. Then, given p\u03b8(x, z) = p\u03b8(x|z)p(z), we obtain the following inequality: log p(x) \u2265 Eq\u03a6(z|x)Eq\u03c8(z\u2032|x) [ log p\u03b8(x, z)\nq\u03d5(z|z\u2032)\n] \u2265 Eq\u03a6(z|x) [ log p\u03b8(x, z)\nq\u03a6(z|x)\n] .\nRefer to the Appendix for the proof. Note that q\u03c8(z\u2032|x) can be either parametric or non-parametric distribution. We can further show that this generalizes to multiple stochastic layers in the inference network.\nTheorem 1. Consider an approximate posterior distribution of the following form q\u03a6(z|x) = \u222b z1\u00b7\u00b7\u00b7zL\u22121 q\u03c6L(z|zL\u22121) \u00b7 \u00b7 \u00b7 q\u03c61(z1|x)dz1 \u00b7 \u00b7 \u00b7 dzL\u22121\nThen, given p\u03b8(x, z) = p\u03b8(x|z)p(z), we obtain the following inequality:\nlog p(x) \u2265 Eq\u03a6(z|x)Eq\u03c61 (z1|x) \u00b7 \u00b7 \u00b7Eq\u03c6L (z|zL\u22121)\n[ log\np\u03b8(x, z)\u220fL\u22121 i=1 q\u03c6i(z i + 1|zi)\n] \u2265 Eq\u03a6(z|x) [ log p\u03b8(x, z)\nq\u03a6(z|x)\n] ,\nwhere z = zL.\nThe proof is presented in the Appendix. Theorem 1 illustrates that adding more stochastic layers gives tighter lower bound.\nWe now use Lemma 1 to derive the denoising variational lower bound. For the approximate distribution q\u03c6,p\u0303(z|x) = \u222b q\u03c6(z|x\u0303)p(x\u0303|x)dx\u0303, we can write the standard variational lower bound as follows:\nlog p(x) \u2265 Eq\u03c6,p\u0303(z|x) [ log p\u03b8(x, z)\nq\u03c6,p\u0303(z|x)\n] = Eq\u03c6,p\u0303(z|x) [ log\np\u03b8(x, z)\nEp(x\u0303|x)[q\u03c6(z|x\u0303)]\n] def = Lcvae. (7)\nApplying Lemma 1 to Equation 7, we can pull out the expectation in the denominator outside of the log and obtain the denoising variational lower bound:\nLdvae def = Ep(x\u0303|x)Eq\u03c6,p\u0303(z|x)\n[ log p\u03b8(x, z)\nq\u03c6(z|x\u0303)\n] . (8)\nBy the Lemma 1, we finally have the following:\nlog p(x) \u2265 Ldvae \u2265 Lcvae. (9)\nIt is important to note that the above does not necessarily mean that Ldvae \u2265 Lvae because the approximate posterior distribution in Lcvae is not the same as the one in Lvae. That is, q\u03c6,p\u0303(z|x) in Lcvae depends on a corruption distribution while q\u03c6(z|x) in Lvae does not. Note that q\u03c6,p\u0303(z|x) has the capacity to cover a much broader class of distributions than q\u03c6(z|x). The distributions that q\u03c6(z|x) can cover is an instance of the distributions that q\u03c6,p\u0303(z|x) can cover. This makes it possible for Ldvae to be a tighter lower bound of log p(x) than Lvae. For examples, suppose that p(z|x) consists of multiple modes for all point x. Then, q\u03c6,p\u0303(z|x) has the potential of modeling more than a single mode, whereas it is impossible to model multiple modes of p(z|x) from q\u03c6(z|x) regardless of which lower bound of log p(x) is used as the objective function. Note, however, that it is also possible to make the Lcvae a looser lower bound than Lvae by choosing a corruption distribution p(x\u0303|x) such that it completely distorts the input x in such a way to lose all useful information required for the reconstruction, resulting in Lcvae < Lvae. Thus, it is important to choose a sensible corruption distribution for Ldvae."}, {"heading": "3.2 TRAINING PROCEDURE", "text": "One may consider a simple way of training VAE with the denoising criterion, which is similar to how the vanilla denoising autoencoder is trained: (i) sample a corrupted input x\u0303(m) \u223c p(x\u0303|x), (ii) sample z(l) \u223c q(z|x\u0303(m)), and (iii) sample reconstructed images from the generative network\np\u03b8(x|z(l)). This procedure is very similar to the regular VAE except that the input is corrupted by a noise distribution at every update.\nThe above procedure can be seen as a special case of optimizing the following objective which can be easily approximated by Monte Carlo sampling.\nEq(z|x\u0303)Ep(x\u0303|x) [ log p\u03b8(x, z)\nq\u03c6(z|x\u0303) ] ' 1 MK K\u2211 k=1 M\u2211 m=1 log p\u03c6(x, z (k|m)) q\u03c6(z(k|m)|x\u0303(m)) (10)\nwhere x\u0303(m) \u223c p(x\u0303|x) and z(k|m) \u223c q\u03c6(z|x\u0303(m)). By Jensen\u2019s inequality, we can see that this is also a lower bound of the marginal log-likelihood:\nEp(x\u0303|x)Eq(z|x\u0303) [ log p\u03b8(x, z)\nq\u03c6(z|x\u0303)\n] \u2264 logEp(x\u0303|x)Eq(z|x\u0303) [ p\u03b8(x, z)\nq\u03c6(z|x\u0303)\n] = log p(x). (11)\nIn the experiment, we refer this algorithm by DVAE\u2205. Note that although this is a lower bound of the marginal log-likelihood, it is not clear if this can be a tighter bound than Lcvae, which is satisfied for the case of Ldvae. Thus, our main algorithm optimizes the Ldvae objective by the following:\nEp(x\u0303|x)Eq\u03c6,p\u0303(z|x) [ log p\u03b8(x, z)\nq\u03c6(z|x\u0303) ] ' 1 KML K\u2211 l=1 M\u2211 m=1 L\u2211 l=1 log p\u03b8(x, z (k|l)) q\u03c6(z(k|l)|x\u0303(m)) (12)\nwhere z(k|l) \u223c q\u03c6,p\u0303(z|x\u0303(l)) and x\u0303(m), x\u0303(l) \u223c p(x\u0303|x). Note that the z(k|l) is not constructed from x\u0303(m) but from another noisy input x\u0303(l). As a result, the recognition network needs to learn to be robust to the presence of the mismatch between x\u0303(m) and x\u0303(l). We call this algorithm as DVAE (or DIWAE when the same technique is applied to IWAE objective). To compute the gradients, we use the re-parameterization trick for both DVAE-\u2205 and DVAE. Due to the Monte Carlo approximation, we need to M samples of x\u0303 and L samples of z for each update. Although Equation 12 requires sampling x(m) \u223c p(x\u0303|x) and z(l) \u223c q\u03c6,p\u0303(z|x), these sample are drawn independently, thus easily parallelizable. Moreover, the experimental results demonstrate that a single sample per update for each of x\u0303 and z are good enough in practice. Note that although we presented the training procedure for DVAE as a demonstration, this proposed procedure can also be additionally applied to other variational methods based on inference networks."}, {"heading": "4 EXPERIMENTS", "text": "We conducted empirical studies of DVAE under the denoising variational lower bound as discussed in Section 3. To assess whether adding a denoising criterion to the variational auto-encoding models enhances the performance or not, we tested on the denoising criterion on VAE and IWAE throughout the experiments. We also performed experiments with both the classical denoising training technique (DVAE\u2205) and the proposed denoising training technique (DVAE) to apprehend the performance gain. As mentioned in Section 3.1, since the choice of the corruption distribution is crucial, we compare on different corruption distributions of various noise levels.\nWe consider two datasets, the binarized MNIST dataset and the Frey face dataset. The MNIST dataset contains 60,000 images for training and 10,000 images for test and each of the images is 28 \u00d7 28 pixels for handwritten digits from 0 to 9 (LeCun et al., 1998). Out of the 60,000 training examples, we used 10,000 examples as validation set to tune the hyper-parameters of our model. We use the binarized version of MNIST, where each pixel of an image is sampled from {0, 1} according to its pixel intensity value. The Frey Face3 dataset consists of 2000 images of Brendan Frey\u2019s face. We split the images into 1572 training data, 295 validation data, and 200 test data. We normalized the images such that each pixel value ranges between [0, 1].\nThroughout the experiments, we used the same neural network architectures for VAE and IWAE. Also, a single stochastic layer with 50 latent variables is used for both VAE and IWAE. For the generation network, we used a neural network of two hidden layers each of which has 200 units. For the inference network, we tested two architectures, one with a single hidden layer and the other with\n3Available at http://www.cs.nyu.edu/\u223croweis/data.html.\ntwo hidden layers. We then used 200 hidden units for both of them. We used softmax activations for VAE and tanh activations for IWAE following the same configuration of the original papers of Kingma & Welling (2014) and Burda et al. (2015). For binarized MNIST dataset, the last layer of the generative network was sigmoid and the usual cross-entropy term was used. For the Frey Face dataset where the input value is real numbers, we used Gaussian stochastic units for the output layer of the generation network.\nFor all our results, we ran 10-fold experiments. We optimized all our models with ADAM (Kingma & Ba, 2014a). We set the batch size to 100 and the learning rate was selected from a discrete range chosen based on the validation set. We used 1 sample of z per update for VAE and 5 samples for IWAE. Note that using 1 sample for IWAE is equivalent to VAE. The reported results were only trained with training set, not including the validation set.\nFollowing common practices of choosing a noise distribution, we deployed the salt and pepper noise to the binary MNIST dataset and Gaussian noise to the real-valued Frey Face dataset. Table 1 presents the negative variational lower bounds with respect to different corruption levels on the MNIST dataset. Similarly, Table 2 presents the negative variational lower bound using unnormalized generation networks, with respect to different corruption levels on the Frey Face dataset. Note that when the corruption level is set to zero, DVAE and DIWAE are identical to VAE and IWAE, respectively.\nIn the following, we analyze the results by answering questions on the experiments."}, {"heading": "Q: Does adding the denoising criterion improve the performance of variational autoencoders?", "text": "Yes. All of the methods with denoising criterion surpassed the performance of vanilla VAE and vanilla IWAE as shown in Table 1 and Table 2. But, it is dependent on the choice of proper corruption level; for a large amount of noise, as we expected, it tends to perform worse than the vanilla VAE and IWAE."}, {"heading": "Q: How sensitive is the model for the type and the level of the noise?", "text": "It seems that both of the models are not very sensitive with respect to the two types of noises: Gaussian and salt and pepper. They are more sensitive to the level of the noise rather than the type. Based on the experiments, the optimal corruption level lies in between (0, 5] since all of the results in that range are better than the one with 0% noise. It is natural to see this result considering that, when the noise level is excessive, (i) the model will lose information required to reconstruct the original input and that (ii) there will be large gap between the distributions of the (corrupted) training dataset and the test dataset."}, {"heading": "Q: How do the sample sizes M and L affect to the result?", "text": "In Figure 1, we show the results on different configurations of M and L. As shown, increasing the sample size helps to converge faster in terms of the number of epochs. Note, however, that increasing sample size requires more computation. Thus, in practice using M = L = 1 seems a reasonable choice. The converged values of VAE are 95.50, 95.22, and 95.10 for M = L = 1, 5, and 10 respectively, and 88.62, 88.45, and 88.42 for IWAE."}, {"heading": "Q: How does the DVAE perform compared to the DVAE\u2205?", "text": "For proper noise distributions, both training criteria provide better performance than the vanilla VAE and IWAE even with single sample per each updates. Also, we can see that the DIWAE outperforms the DIWAE in most cases. It is also interesting to see that the performance gap between the DIWAE and the DIWAE\u2205 is significantly larger than that between the DVAE and the DVAE\u2205. This considerably large gain of DIWAE compared to DVAE raises a question; whether the gain is due to the IWAE objective or due to having a larger sample size K where K = 5 for DIWAE while K = 1 for DVAE. Thus, in order to verify this, we trained VAE with K = 5 and with the tanh activation function for a inference network with two hidden layers. The Table 3 below shows the results:\nBased on Table 3, the improvement due to denoising criterion is large for VAE with K = 5 as is in IWAE with K = 5. This leads us to suspect that the performance gain is achieved by the combination of both the denoising criterion and the IWAE objective function."}, {"heading": "Q: Data augmentation v.s. data corruption?", "text": "Here, we consider specific data augmentation where our data lies in between 0 and 1, x \u2208 (0, 1)D like MNIST. We consider a new binary data point x\u2032 \u2208 {0, 1}D where the previous data is treated as a probability of each pixel value turning on, i.e. p(x\u2032) = x. Then, we augment the data by sampling the data from x at every iteration. Although, this setting is not realistic, but we were curious whether the performance of this data augmentation compare to denoising criterion. The performance of such data augmentation on MNIST gave 93.88\u00b1 0.08 and 92.51\u00b1 0.07 for VAE and IWAE. Comparing these negative log-likelihood with the performance of DVAE and DIWAE, which were 95.45\u00b1 0.11 and 88.76 \u00b1 0.11, data augmentation VAE outperformed DVAE but data augmentation IWAE was worse than DIWAE.\nQ: Can we propose a more sensible noise distribution?\nFor all the experiment results, we have used a simple corruption distribution using a global corruption rate (the parameter of the Bernoulli distribution or the variance of the Gaussian distribution) to all pixels in the images. To see if a more sensible corruption can lead to an improvement, we also tested another corruption distribution by obtaining a mean image. Here, we obtained the mean image by averaging all training images and then used the pixel intensity of the mean image as the corruption rate so that each pixel has different corruption rate which statistically encodes at some extent the pixel-wise noise from the entire dataset. However, we could not observe a noticeable improvement from this compared to the version with the global corruption rate, although we believed that this is a better way of designing the corrupting distribution. One interesting direction is to use a parameterized corruption distribution and learn the parameter. This will be advantageous because we can use our denoising variational lower bound which it is tighter than the classical variational lower bound on noisy inputs. We leave this for the future work."}, {"heading": "5 CONCLUSIONS", "text": "In this paper, we studied the denoising criterion for a general class of variational inference models where the approximate posterior distribution is conditioned on the input x. The main result of our paper was to introduce the denoising variational lower bound which, provided a sensible corruption function, can be tighter than the standard variational lower bound on noisy inputs. We claimed that this training criterion makes it possible to learn more flexible and robust approximate posterior distributions such as the mixture of Gaussian than the standard training method without corruption. In the experiments, we empirically observed that the proposed method can consistently help to improve the performance for the variational autoencoder and the importance weighted auto-encoder. Although we observed considerable improvements for our experiments with simple corruption distributions, how to obtain the sensible corruption distribution is still an important open question. We think that learning with a parametrized corruption distribution or obtaining a better heuristic procedure will be important for the method to be applied more broadly.\nAcknowledgments: We thank the developers of Theano (Bergstra et al., 2010) for their great work. We thank NSERC, Compute Canada, CIFAR and Samsung for their support."}], "references": [{"title": "What regularized auto-encoders learn from the data generating distribution", "author": ["Alain", "Guillaume", "Bengio", "Yoshua"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Alain et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alain et al\\.", "year": 2014}, {"title": "Generalized denoising autoencoders as generative models", "author": ["Bengio", "Yoshua", "Yao", "Li", "Alain", "Guillaume", "Vincent", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["Bergstra", "James", "Breuleux", "Olivier", "Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Desjardins", "Guillaume", "Turian", "Joseph", "Warde-Farley", "David", "Bengio", "Yoshua"], "venue": "In Proc. SciPy,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Reweighted wake-sleep", "author": ["Bornschein", "J\u00f6rg", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1406.2751,", "citeRegEx": "Bornschein et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bornschein et al\\.", "year": 2014}, {"title": "Importance weighted auto-encoder", "author": ["Burda", "Yuri", "Grosse", "Roger", "Salakhutdinov", "Ruslan"], "venue": "In http://arxiv.org/pdf/1509.00519v1.pdf,", "citeRegEx": "Burda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2015}, {"title": "The helmholtz machine", "author": ["Dayan", "Peter", "Hinton", "Geoffrey E", "Neal", "Radford M", "Zemel", "Richard S"], "venue": "Neural computation,", "citeRegEx": "Dayan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1995}, {"title": "Nice: Non-linear independent component estimation", "author": ["Dinh", "Laurent", "Krueger", "David", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representation Workshop,", "citeRegEx": "Dinh et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dinh et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi", "John", "Hazan", "Elad", "Singer", "Yoram"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Gregor", "Karol", "Danihelka", "Ivo", "Graves", "Alex", "Rezende", "Danilo Jimenez", "Wierstra", "Daan"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Conservativeness of untied auto-encoders", "author": ["Im", "Daniel Jiwoong", "Belghazi", "Mohamed Ishmael Diwan", "Memisevic", "Roland"], "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Im et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Im et al\\.", "year": 2016}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Micheal", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawarence K"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Auto-encoding varational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the Neural Information Processing Systems (NIPS),", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "Leon", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Probabilistic inference using markov chain monte carlo methods", "author": ["R.M. Neal"], "venue": "Technical Report CRG-TR-93-1,", "citeRegEx": "Neal,? \\Q1993\\E", "shortCiteRegEx": "Neal", "year": 1993}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir"], "venue": "In Proceedings of the International Conference of Machine Learning (ICML),", "citeRegEx": "Rezende et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2015}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["Rifai", "Salah"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "Rifai and Salah.,? \\Q2011\\E", "shortCiteRegEx": "Rifai and Salah.", "year": 2011}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Salimans", "Tim", "Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the International Conference of Machine Learning (ICML),", "citeRegEx": "Salimans et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2015}, {"title": "Learning continuous attractors in recurrent networks", "author": ["Seung", "H.Sebastian"], "venue": "In Proceedings of the Neural Information Processing Systems (NIPS),", "citeRegEx": "Seung and H.Sebastian.,? \\Q1998\\E", "shortCiteRegEx": "Seung and H.Sebastian.", "year": 1998}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "P.A. Manzagol"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "Variational inference (Jordan et al., 1999) has been a core component of approximate Bayesian inference along with the Markov chain Monte Carlo (MCMC) method (Neal, 1993).", "startOffset": 22, "endOffset": 43}, {"referenceID": 16, "context": ", 1999) has been a core component of approximate Bayesian inference along with the Markov chain Monte Carlo (MCMC) method (Neal, 1993).", "startOffset": 122, "endOffset": 134}, {"referenceID": 7, "context": "It has been popular to many researchers and practitioners because the problem of learning an intractable posterior distribution is formulated as an optimization problem which has many advantages compared to MCMC; (i) we can easily take advantage of many advanced optimization tools (Kingma & Ba, 2014b; Duchi et al., 2011; Zeiler, 2012), (ii) the training by optimization is usually faster than the MCMC sampling, and (iii) unlike MCMC where it is difficult to decide when to finish the sampling, the stopping criterion in variational inference is clear.", "startOffset": 282, "endOffset": 336}, {"referenceID": 5, "context": "One remarkable recent advance in variational inference is to use the inference network (also known as the recognition network) as the approximate posterior distribution (Kingma & Welling, 2014; Rezende & Mohamed, 2014; Dayan et al., 1995; Bornschein & Bengio, 2014).", "startOffset": 169, "endOffset": 265}, {"referenceID": 22, "context": "On the other hand, the denoising criterion, where the input is corrupted by adding some noise and the model is asked to recover the original input, has been studied extensively for deterministic generative models (Seung, 1998; Vincent et al., 2008; Bengio et al., 2013).", "startOffset": 213, "endOffset": 269}, {"referenceID": 1, "context": "On the other hand, the denoising criterion, where the input is corrupted by adding some noise and the model is asked to recover the original input, has been studied extensively for deterministic generative models (Seung, 1998; Vincent et al., 2008; Bengio et al., 2013).", "startOffset": 213, "endOffset": 269}, {"referenceID": 22, "context": "The study showed that the denoising criterion plays an important role in achieving good generalization performance (Vincent et al., 2008) because it makes the nearby data points in the low dimensional manifold to be robust against the presence of small noise in the high dimensional observation space (Seung, 1998; Vincent et al.", "startOffset": 115, "endOffset": 137}, {"referenceID": 22, "context": ", 2008) because it makes the nearby data points in the low dimensional manifold to be robust against the presence of small noise in the high dimensional observation space (Seung, 1998; Vincent et al., 2008; Rifai, 2011; Alain & Bengio, 2014; Im et al., 2016).", "startOffset": 171, "endOffset": 258}, {"referenceID": 9, "context": ", 2008) because it makes the nearby data points in the low dimensional manifold to be robust against the presence of small noise in the high dimensional observation space (Seung, 1998; Vincent et al., 2008; Rifai, 2011; Alain & Bengio, 2014; Im et al., 2016).", "startOffset": 171, "endOffset": 258}, {"referenceID": 4, "context": "Because applying this approximate distribution to the standard VAE objective makes the training intractable, we propose a new objective, called the denoising variational lower bound, and show that, given a sensible corruption function, this is (i) tighter than the standard variational lower bound on noisy inputs, (ii) efficient to train, and (iii) easily applicable to many existing models such as the variational autoencoder, the importance reweighted autoencoder (IWAE) (Burda et al., 2015), and the neural variational inference and learning (NVIL) (Mnih & Gregor, 2014).", "startOffset": 474, "endOffset": 494}, {"referenceID": 16, "context": "Salimans et al. (2015) integrated MCMC steps into the variational inference such that the variational distribution becomes closer to the target distribution as it takes more MCMC steps inside each iteration of the variational inference.", "startOffset": 0, "endOffset": 23}, {"referenceID": 4, "context": "Similar ideas but applying a sequence of invertible non-linear transformations rather than MCMC are also proposed by Dinh et al. (2015) and Rezende & Mohamed (2015).", "startOffset": 117, "endOffset": 136}, {"referenceID": 4, "context": "Similar ideas but applying a sequence of invertible non-linear transformations rather than MCMC are also proposed by Dinh et al. (2015) and Rezende & Mohamed (2015). On the other hand, the denoising criterion, where the input is corrupted by adding some noise and the model is asked to recover the original input, has been studied extensively for deterministic generative models (Seung, 1998; Vincent et al.", "startOffset": 117, "endOffset": 165}, {"referenceID": 1, "context": ", 2008; Bengio et al., 2013). The study showed that the denoising criterion plays an important role in achieving good generalization performance (Vincent et al., 2008) because it makes the nearby data points in the low dimensional manifold to be robust against the presence of small noise in the high dimensional observation space (Seung, 1998; Vincent et al., 2008; Rifai, 2011; Alain & Bengio, 2014; Im et al., 2016). Therefore, it is natural to ask if the denoising criterion (where we add the noise to the inputs) can also be advantageous for the variational autoencoding framework where the noise is added to the latent variables, not the inputs, and if so, how can we formulate the problem for efficient training. Although it has not been considerably studied how to combine these, there has been some evidences of its usefulness1. For example, Rezende & Mohamed (2014) pointed out that injecting additional noise to the recognition model is crucial to achieve the reported accuracy for unseen data, advocating that in practice denoising can help the regularization of probabilistic generative models as well.", "startOffset": 8, "endOffset": 876}, {"referenceID": 10, "context": "For example, in the mean-field variational inference (Jordan et al., 1999), the distributions in Q treat all dependent variables as independent, i.", "startOffset": 53, "endOffset": 74}, {"referenceID": 8, "context": "This includes other recent models such as the importance weighted autoencoders (IWAE), the neural variational inference and learning (NVIL), and DRAW (Gregor et al., 2015).", "startOffset": 150, "endOffset": 171}, {"referenceID": 22, "context": "With the denoising autoencoder criterion (Seung, 1998; Vincent et al., 2008), the input is corrupted according to some noise distribution, and the model needs to learn to reconstruct the original input or maximize the log-probability of the clean input x, given the corrupted input x\u0303.", "startOffset": 41, "endOffset": 76}, {"referenceID": 4, "context": "Note that a similar idea is explored in IWAE (Burda et al., 2015).", "startOffset": 45, "endOffset": 65}, {"referenceID": 14, "context": "The MNIST dataset contains 60,000 images for training and 10,000 images for test and each of the images is 28 \u00d7 28 pixels for handwritten digits from 0 to 9 (LeCun et al., 1998).", "startOffset": 157, "endOffset": 177}, {"referenceID": 4, "context": "We used softmax activations for VAE and tanh activations for IWAE following the same configuration of the original papers of Kingma & Welling (2014) and Burda et al. (2015). For binarized MNIST dataset, the last layer of the generative network was sigmoid and the usual cross-entropy term was used.", "startOffset": 153, "endOffset": 173}, {"referenceID": 2, "context": "Acknowledgments: We thank the developers of Theano (Bergstra et al., 2010) for their great work.", "startOffset": 51, "endOffset": 74}], "year": 2015, "abstractText": "Denoising autoencoders (DAE) are trained to reconstruct their clean inputs with noise injected at the input level, while variational autoencoders (VAE) are trained with noise injected in their stochastic hidden layer, with a regularizer that encourages this noise injection. In this paper, we show that injecting noise both in input and in the stochastic hidden layer can be advantageous and we propose a modified variational lower bound as an improved objective function in this setup. When input is corrupted, then the standard VAE lower bound involves marginalizing the encoder conditional distribution over the input noise, which makes the training criterion intractable. Instead, we propose a modified training criterion which corresponds to a tighter bound when input is corrupted. Experimentally, we find that the proposed denoising variational autoencoder (DVAE) yields better average loglikelihood than the VAE and the importance weighted autoencoder on the MNIST and Frey Face datasets.", "creator": "LaTeX with hyperref package"}}}