{"id": "1511.06412", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "QBDC: Query by dropout committee for training deep supervised architecture", "abstract": "while the current trend is appearing to increase by the search depth of neural science networks seeming to increasingly increase effectively their performance, the size flexibility of their training database footprint has to grow accordingly. nowadays we notice today an emergence movement of tremendous learning databases, who although closely providing labels to build from a secure training tag set solution still remains a very unexpectedly expensive intensive task. we particularly tackle the problem of selecting additionally the diverse samples to thus be automatically labelled uniquely in an accurate online semantic fashion. in this paper, we present employ an active learning strategy based indirectly on query by committee and dropout matching technique to randomly train a convolutional neural information network ( cnn ). we derive into a commmittee of partial cnns resulting from consistent batchwise dropout runs on the respective initial cnn. we therefore evaluate utilizing our random active learning processing strategy for documenting cnn observations on mnist benchmark, ultimately showing programs in particular that routinely selecting each less than r 30 % vocabulary from among the annotated database is enough to instantly get similar error rate improvements as using the full training set on mnist. we work also studied investigating the robustness of our method approaches against adversarial examples.", "histories": [["v1", "Thu, 19 Nov 2015 22:03:14 GMT  (915kb,D)", "http://arxiv.org/abs/1511.06412v1", null], ["v2", "Thu, 26 Nov 2015 14:19:01 GMT  (915kb,D)", "http://arxiv.org/abs/1511.06412v2", "Submitted to ICLR2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["melanie ducoffe", "frederic precioso"], "accepted": false, "id": "1511.06412"}, "pdf": {"name": "1511.06412.pdf", "metadata": {"source": "CRF", "title": "QBDC: QUERY BY DROPOUT COMMITTEE FOR TRAINING DEEP SUPERVISED ARCHITECTURE", "authors": ["Melanie Ducoffe", "Frederic Precioso"], "emails": ["precioso}@i3s.unice.fr"], "sections": [{"heading": "1 INTRODUCTION", "text": "Larger deep architectures fed with more data provide better results: This widely acknowledged idea has been confirmed all along the recent years when analyzing for instance the results at Imagenet Large Scale Visual Recognition Challenge Russakovsky et al. (2015). Indeed, in 2012, the winner was the SuperVision team Krizhevsky et al. (2012) using a deep convolutional neural network with 60 million parameters and making a real breakthrough in the image classification task. The huge step forward from SuperVision team has deeply impacted the following contributions to ILSVRC after 2012. In 2014, Simonyan et al. Simonyan & Zisserman (2014) proposed also to use a CNN architecture from 11 up to 19 layers with 133 up to 144 million of parameters. In 2015, the winning team for the image classification context was GoogLeNet. They proposed a non-conventional CNN architecture allowing them to increase significantly both the depth and the width of the CNN while containing computational cost.\nThe relation between the depth of the architecture, the required amount of training data and the final accuracy of the decision has not only been observed experimentally but it has also been explained in various papers. In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem.\nConsidering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques. Even on smaller datasets, such as MNIST, the accuracy was improved when methods have been including a training set extension process also based on image transformations LeCun et al. (1998).\nDespite these techniques provide a better sampling of the target manifold, despite the \u201cexpressive power of deep architectures\u201d Bengio & Delalleau (2011) and despite their generalization power Bengio et al. (2013), the work of Szegedy et al. Szegedy et al. (2013) demonstrated this was not enough.\nar X\niv :1\n51 1.\n06 41\n2v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5\nWhen considering the difficulty and the cost to gather relevant annotations for Challenges such as ImageNet Russakovsky et al. (2015), the need for methods requiring smaller training sets is increasing.\nTo this purpose, semi-supervised techniques have seen a recently increase into the recent publications. In deep learning especially, research works are mixing unsupervised pretraining on the full training set with supervised classification on only a subset of it. While these techniques has proven their efficiency with state of the art results, it remains that their performance depends, on some level, on the choice of the annotated subset.\nOur work focuses on the selection of a better subset to be annotated for training, exploiting the theory of committee decisions. We propose an adaptation of Query-By-Committee strategy (QBC) to deep learning. Indeed the huge number of parameters in a deep architecture prevents us from training a committee of deep networks. Instead, we train a full Convolutional Neural Network (CNN) by selecting the training samples with a committee of partial CNNs based on batchwise dropout on the current full CNN, reducing the computational cost of the standard QBC technique."}, {"heading": "2 RELATED WORK", "text": "Several learning strategies have been proposed to tackle the problem of selecting the best training samples in order to optimize the training phase computational cost: Query-By-Committee, Semi-supervised methods, active learning strategies, and more recently active dropout deep architectures."}, {"heading": "2.1 QUERY BY COMMITTEE", "text": "The first algorithm based on Query By Committee (QBC) strategy has been proposed by Seung et al. Seung et al. (1992). They proved two important results: first, the generalization error (denoted g in the original article) of a linear classifier for random training samples behaves as the inverse power law, g(\u03b1) \u223c 1/\u03b1 where \u03b1 = P/N with P the number of training samples considered and N the dimension of the input space; second, the generalization error of a linear classifier for training samples selectect through a query by committee strategy, scales like g(\u03b1) \u223c e\u2212\u03b1I with the constant decay is given by the information gain I . Later Freund et al. Freund et al. (1997) proved that this property hold for a more general class of learning problems.\nIn this paper, we consider a batch active learning based on Query-By-Committee."}, {"heading": "2.2 SEMI SUPERVISED LEARNING", "text": "Recently some works have been focusing on how to combine both unsupervised and supervised learning at the same time from a restricted annotated training set. Those semi supervised technique adapted for deep architectures are leaveraging the size of the annotated database while being competitive with convolutional networks trained on the full annotated training set like done in Rasmus et al. (2015) or Kingma et al. (2014). However those methods are mostly based on generative networks Kingma & Welling (2013), which are at the present time adapted to a restricted are of inputs like images (Kingma et al. (2014)) or texts( Graves (2013))."}, {"heading": "2.3 ACTIVE LEARNING FOR DEEP ARCHITECTURES", "text": "Active learning has been first adapted recently to deep architectures for sentiment classification. In Zhou et al. (2010), EZhou et al. proposed an Active Deep Network (ADN) to select the most relevant reviews to constitute the annotated training set for their semi supervised tasks. First they pretrain a Restricted Boltzmann Machine and use active learning on top of it : they ask the labels of a random unnatotated set of examples and selects the ones which gets the least confidence on their label."}, {"heading": "2.4 ACTIVE DEEP DROPOUT", "text": "Simultaneous and independent work by Gammelsaeter (2015) also considers doing query by committee by applying dropout on a standart Multi Layer Perceptron (MLP) to form a committee. To outperform the MLP accuracy, the first layers came from a pretrained DBN whose weights are reused each time the MLP is reinitialized after increasing the labelled training set. While their algorithm resembles ours, it differs in three main aspects wich make the difference on performances :\n\u2022 The author uses dropout to build the committee, making it not usable for convolutional architecture. \u2022 The committee adds only one sample at a time, creating unbalanced weight among the training samples."}, {"heading": "3 METHOD", "text": ""}, {"heading": "3.1 SELECTION CRITERION", "text": "It consists in building a committee of deep architectures which selects among a subset of unlabelled samples, then ask for the most relevant ones to be labelled and add them to the training set. Between two selections of new samples, a deep network is train on the labelled training set using backpropagation until early stopping. For a better understanding of our method, we denote by full network the deep architecture trained on the labelled training set and partial network a model of the committee.\nWe start with a fixed architecture and learn its weights and biases so as to minimize the error of prediction on an independent validation set. We train initially the full network using a random annotated subset of the training samples. It has experimentally been shown Saxe et al. (2011) that CNN initialized with random weights already hold some capability of discriminating classes (up to a certain accuracy of course) so that the initial annotated subset might be left empty, however we have chosen to start from an initial random subset of the training set to converge faster to relevant samples and benefit from the sample selection of our committee based approach from the first epochs. After several epochs, the performance of this initial network does not increase anymore on the validation set. In order to improve the accuracy, we then inject a new minibatch of samples. Note that adding a minibatch of examples differ from Gammelsaeter (2015) in which samples are added to the training set one at a time. Furthermore, adding minibatches of samples fosters gpu parallelization for training deep architectures, plus balances every sample in the training set when applying mean gradient descent.\nWe hence select a random subset of K samples from the rest of the training set. Let us notice than we are not requesting here the labels of the samples(Zhou et al. (2010)). A committee of deep networks assigns then a score to each sample from the selected minibatch, this score being related to the impact of the considered sample on the training of the deep architecture. If we denote by batch size the number of samples in a minibatch, we then request the labels of the batch size samples having the greatest score among the K samples. The score of a sample correspond to the number of models in the committee whose prediction differs from the majority prediction among the committee members. The quality of the score is related to both the size of the committee and the correlation between the models Melville & Mooney (2004)."}, {"heading": "3.2 COMMITTEE DROPOUT WISE BUILT", "text": "The goal of the committee is to be representative of the version space where the current trained network lies. Indeed it shows what a network, predicting correctly the annotated training set, could predict on samples from an uncovered part of the input distribution. At first, as most part of the input distribution is not covered by the version space, most of the examples will be informative and the variance of their scores will be relatively low. We can expect an increase of the variance as the knowledge of the manifold will improve with the size of the annotated training set, thus saving a lot of the intervention of the human annotator. We illustrate our technique in figure 3.1\nLet us now detail how to form the committee. In the early papers describing active learning through committee selection, convergence and better result against randomness have been proven, first for perceptron-like learning functions Seung et al. (1992) and later for more general classes of learning problems Freund et al. (1997). However, for those results to hold, each model of our committee has to lie in the current version space defined by the annotated training set (which is the set of networks built from the same \u2019architecture\u2019 and making no mistake on the current training set). Note already that this condition is not always feasible or suitable in the case of neural networks:\n1. Indeed, we should apply backpropagation on each of the models of the committee to be effective on the training set. However, we are dealing with deep architectures, thus training several networks is not always computationally reasonnable.\n2. When training a network, we are applying early stopping to avoid overfitting. Thus the current version space is not exactly defined on the training set but rather on a subset of it. When training a deep network, making no mistake on the training set is not always possible so we cannot assure to form a committee defined on the exact same version space than the full network\u2019s.\nTo initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network.\nThe batchwise dropout Graham et al. (2015) is a version of dropout where we use a unique bernouilli mask to discard neurons for each sample in the minibatch. Thus the batchwise dropout reduce quadratically in the percentage of kept neurons the number of parameters in the architecture. Moreover dropout, when applied on convolutional layers remove neurons independently given the spatial locations. Whereas batchwise dropout is spatially dependant, switching on or off filters so to discard neurons obtained through the same filter. It is thus preserving the consistency in a CNN architecture. This technique is required to extend QBDC on CNN architecture which may be one of the reasons for the difference in the accuracy with Gammelsaeter (2015) that solely applied the dropout committee on MLP architectures.\nIn Graham et al. (2015), a batchwise dropout committee has already been envisaged for future work in order to fasten the testing while averaging the prediction on the full network Baldi & Sadowski (2014). The main advantage is to obtain a committee which the committee has the same architecture than the full network, with members having some zero constraints on several connections. However because we are not using dropout while training the full network, some members of the committee may have poor accuracy on the current training set, not being representative of the current version space. In order to increase the accuracy of the committee, our idea is to retrain the last layer of every partial CNN via backpropagation.\nLet consider the full network NF trained by backpropagation. Let design by PF = softmax(aF ) the output softmax with aF the vector of pre-softmax activations. Similarly, let Ci be a partial network built with batchwise dropout on NF and Pi = softmax(ai) the output softmax with ai the vector of pre-softmax activations. Suppose that NF is of depth M , we train every model Ci on its last layer Wi,M so that its ouput Pi is similar to the true class of a sample ytrue prediction. Every network in the process is trained to optimize the loss function cross entropy H as described in the following equations:\nLF : L(W \u2208 NF ) = H(ytrue prediction, PF ) (1)\nLi : L(Wi,M ) = H(ytrue prediction, Pi) (2)\nEquation 1 refers to the standard cost function when training a network for supervised classification. On the other side, the training for the committee in equation 2 differs in that we modify only the last layers. Thus we obtain an ensemble\nof smaller architectures, uncorrelated owing to the random selection of the nodes during dropout. The process is presented in the algorithm 1\nAlgorithm 1: Query by committee Require : L0 set of initial labelled training examples (can be empty) Require : U0 set of initial labelled training examples Require : n committee size Require : pd dropout probability Require : K number of samples analyzed by the committee Parameter : hyperparameters value, batch size, architecture A\n1 j=0 2 while stop criterion do 3 Train a network N jF on L\nj built given the architecture A 4 Construct a committee Ci of N classifiers\nCi = DropoutSampling(N jF , pd, n)\n5 for i \u2208 [| 1, n |] do 6 Apply backpropagation given the loss function in equation 2 7 end 8 Select a random subset of K examples: SUj \u2282 U j 9 for each xu \u2208 SUj do\n10 \u2200 xu Calculate the utility xu based on the current committee utility Ci 11 Rank the examples in SUi based on their utility 12 Select the batch size examples in SUj with the highest rank: Sj 13 Ask an oracle to label examples in Sj 14 U j+1 \u2190 U j \\ Sj 15 Lj+1 \u2190 Lj \u222a Sj 16 end 17 end 18 return N jF"}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 PERFORMANCE ON BENCHMARK DATASETS", "text": "In this section, we analyze the performance of our active learning strategy on the benchmark MNIST. 1\nMNIST: We train a convolutional neural network with rectifier linear units for the full network NF . The first two layers are convolutional layers with 20 and 40 filters of size (3,3) plus 2 by 2 pooling with no overlapping. We added on top of it two fully connected layers with 100 neurons in each layer. We used RMSPProp Tieleman & Hinton (2012) with a learning rate of 0.001 and a decay rate of 0.9 with minibatches of size 200. The training set and the validation set are made of 50000 and 10000 samples respectively. Eventually when it comes to the hyperparameters of the active learning we picked up a committee of size 3 and init the annotated training set with 10 minibatches. To build the committee we apply a dropout rate of 0.5. The experiments have been repeated 5 times using different random seed and we provide the mean error and the minimal error (best accuracy over the 5 runs). We compare the performance with a random selection in the exact same state of hyperparameters.\nAs we can see in figure 4.1 , the error rate when using active learning is always lower than when applying a random selection , except for the first adds of samples. Indeed this peak results from a few networks having their accuracy worsening, while the other experiments increase their accuracy. We make the assumption than this peak result from selection of outliers or adversarial examples.\nAnother observation is the time of convergence which is faster in the QBDC case : if we look at the slopes of both curve, they tend to diminish after 40 minibatches for both random and QBDC selection, with an approximate 2% of differences between their results. Although the average score when training on the standart full architecture (a\n1Code will be publicly available on the github account: https://github.com/mducoffe/ICLR 2016\nnetwork trained on the full labelled training set) is of 1.38%, which is higher than the state of the art on MNIST for CNN (0.95 %, LeCun et al. (1998)), the error rate on the QBDC full architecture (a network trained using only the samples selected with QBDC) is competitive with the state of the art : we get an average error rate of 1.10 % while using less than 30 % of the training set, as shown in table 1\nTable 1: Average and best error rates obtain using QBDC and random selection on MNIST using less than 30% of the training set\nMETHOD MEAN error rate MIN error rate\nQBDC 1.10 0.99 RANDOM 2.13 1.78"}, {"heading": "4.2 DROPOUT SHORT NOTE", "text": "To improve the initial performance when instantiating a committee, training the full network with dropout Srivastava et al. (2014) appears like an intuitive solution. Indeed when applying natural dropout proposed by Hinton et al. in Srivastava et al. (2014), every possible model of the committee will be considered while training the full network. However, dropout is a technique designed for regularizing large networks with a certain amount of data. Consequently, when confronting to smaller network or restricted training set such as in MNIST, QBDC with dropout is not able to learn the features correctly, misleading the committee in its choices of new queries.\nIn the figure 4.2, QDBC with dropout using the exact same state of hyperparameters and a dropout rate of 0.5 is not generalizing during the first epochs. We must wait after 30 minibatches (when the training database is sufficiently large) than the full network has learnt enough knowledge so that the committee starts selecting relevant samples. After 40 minibatches, the behavior of both error rate for QDBC with and without dropout training tends to get similar, with a better accuracy of 0.4 % for QBDC as shown in table 2."}, {"heading": "4.3 ADVERSARY SENSITIVE", "text": "As any other machine learning, deep networks, especially CNNs are sensitive to adversarial examples, which are samples statistically diverging from the natural input distribution. Thus, every model of the committee may still be fooled by an adversarial example and select it to be labelled and added at the training examples. Moreover, as pointed\nout in Goodfellow et al. (2014), model averaging or dropout are not helping the network to be more robust from adversarial examples so that the committee decision strategy itself is still not preserving QBDC from considering and selecting adversarial examples.\nBecause we reduce the set of samples on which we train the full network, a natural question arising is: whether considering only a subset of examples may alter the fitting to the input distribution and thus weakening the full network against adversarial attacks.\nTo compare the robustness of the standart full classifier ( the classifier trained on the full training set) and the QBDC full network (a network trained using only the samples selected with QBDC, we compare the number of adversarial examples found for different level of noise on 1000 samples on the test set of MNIST (over the 5 runs). To generate efficiently approximate adversary we used the fast gradient sign method proposed by Goodfellow et al. in (Goodfellow et al. (2014), see 3). We kept the exact same set of hyperparameters from the previous experiments.\nIn figure 4.3, we notice than our method tends to make the QBDC full network slightly more sensitive to adversarial example. Indeed for low noise value (epsilon = 0.1), we found 4% more adversarial examples on the QBDC full network. However these decrease is lower compared to the amount of training samples discarded to train a QBDC full network (70%)."}, {"heading": "5 DISCUSSION", "text": "Our QDBC method has demonstrated that using an automatic selection of the samples can reduce drastically the size of the training set while preserving the accuracy for a fully supervised task. Preliminary results on ongoing experiments are highlighting the usefulness of our method on more complicated distribution such as CIFAR10. Eventually a drawback of our method, but shared by other active learning technique when applied to neural networks, is the need to retrain the full network each time the committee adds a minibatch of new examples. We checked whether the gain of accuracy when retraining the network was low enough so to forget this step and spare time, Unfortunately the network training by this way is overfitting. As a first tendency we have noticed that up to some extreme case (committee of size 1 or 2) adding or retriveving a few members in the commitee is not modifying in average the accuracy.\nAs a future work, we would like also to analyze the samples selected by a committee and check for their relevancy."}, {"heading": "6 CONCLUSION", "text": "In this paper, we have presented our own Query-By-Dropout-Committee (QBDC) for deep architectures. QBDC allows to train supervised deep networks on reduced annotated datasets by iteratively select the most relevant training samples. We have proved the efficiency of our approach: using only 30% of the training set of MNIST we got close to state of the art performance while not degrading much the accuracy when confronting to adversarial examples. These promising results will be extended to other machine learning techniques such as semi-supervised classification (ladder network) or regularization techniques such as batch normalization to handle huge datasets like ImageNet Challenge."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank the developpers of the frameworks Theano Bastien et al. (2012), Blocks and Fuel van Merrie\u0308nboer et al. (2015) which we used for the experi- ments."}], "references": [{"title": "The dropout learning algorithm", "author": ["Baldi", "Pierre", "Sadowski", "Peter"], "venue": "Artificial intelligence,", "citeRegEx": "Baldi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baldi et al\\.", "year": 2014}, {"title": "Theano: new features and speed improvements", "author": ["Bastien", "Fr\u00e9d\u00e9ric", "Lamblin", "Pascal", "Pascanu", "Razvan", "Bergstra", "James", "Goodfellow", "Ian J", "Bergeron", "Arnaud", "Bouchard", "Nicolas", "Bengio", "Yoshua"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "On the expressive power of deep architectures", "author": ["Bengio", "Yoshua", "Delalleau", "Olivier"], "venue": "URL http://dx.doi.org/10.1007/978-3-642-24412-4_3", "citeRegEx": "Bengio et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2011}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "pp. 153\u2013160,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Selective sampling using the query by committee", "author": ["Freund", "Yoav", "Seung", "H. Sebastian", "Shamir", "Eli", "Tishby", "Naftali"], "venue": "ISSN 0885-6125. doi: 10.1023/A:1007330508534. URL http://dx.doi.org/10.1023/A:1007330508534", "citeRegEx": "Freund et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Freund et al\\.", "year": 1997}, {"title": "A committee of one. Master\u2019s thesis, NTNU", "author": ["Gammelsaeter", "Martin"], "venue": "Norwegian University of Science and Technology,", "citeRegEx": "Gammelsaeter and Martin.,? \\Q2015\\E", "shortCiteRegEx": "Gammelsaeter and Martin.", "year": 2015}, {"title": "Explaining and harnessing adversarial examples", "author": ["Goodfellow", "Ian J", "Shlens", "Jonathon", "Szegedy", "Christian"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Efficient batchwise dropout training using submatrices", "author": ["Graham", "Ben", "Reizenstein", "Jeremy", "Robinson", "Leigh"], "venue": "arXiv preprint arXiv:1502.02478,", "citeRegEx": "Graham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Graham et al\\.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks", "author": ["Graves", "Alex"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves and Alex.,? \\Q2013\\E", "shortCiteRegEx": "Graves and Alex.", "year": 2013}, {"title": "Some improvements on deep convolutional neural network based image classification", "author": ["Howard", "Andrew G"], "venue": "CoRR, abs/1312.5402,", "citeRegEx": "Howard and G.,? \\Q2013\\E", "shortCiteRegEx": "Howard and G.", "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "Kingma et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2013}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo Jimenez", "Welling", "Max"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Diverse ensembles for active learning", "author": ["Melville", "Prem", "Mooney", "Raymond J"], "venue": "In Proceedings of 21st International Conference on Machine Learning (ICML-2004),", "citeRegEx": "Melville et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Melville et al\\.", "year": 2004}, {"title": "Semi-supervised learning with ladder network", "author": ["Rasmus", "Antti", "Valpola", "Harri", "Honkala", "Mikko", "Berglund", "Mathias", "Raiko", "Tapani"], "venue": "arXiv preprint arXiv:1507.02672,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "On random weights and unsupervised feature learning", "author": ["Saxe", "Andrew", "Koh", "Pang W", "Chen", "Zhenghao", "Bhand", "Maneesh", "Suresh", "Bipin", "Ng", "Andrew Y"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Saxe et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2011}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "In Proceedings of the Fifth Annual Workshop on Computational Learning Theory,", "citeRegEx": "Seung et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Seung et al\\.", "year": 1992}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Intriguing properties of neural networks", "author": ["Szegedy", "Christian", "Zaremba", "Wojciech", "Sutskever", "Ilya", "Bruna", "Joan", "Erhan", "Dumitru", "Goodfellow", "Ian", "Fergus", "Rob"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Dumoulin", "Vincent", "Serdyuk", "Dmitriy", "Warde-Farley", "David", "Chorowski", "Jan", "Bengio", "Yoshua"], "venue": "CoRR, abs/1506.00619,", "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Active deep networks for semi-supervised sentiment classification", "author": ["Zhou", "Shusen", "Chen", "Qingcai", "Wang", "Xiaolong"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "Indeed, in 2012, the winner was the SuperVision team Krizhevsky et al. (2012) using a deep convolutional neural network with 60 million parameters and making a real breakthrough in the image classification task.", "startOffset": 53, "endOffset": 78}, {"referenceID": 10, "context": "Indeed, in 2012, the winner was the SuperVision team Krizhevsky et al. (2012) using a deep convolutional neural network with 60 million parameters and making a real breakthrough in the image classification task. The huge step forward from SuperVision team has deeply impacted the following contributions to ILSVRC after 2012. In 2014, Simonyan et al. Simonyan & Zisserman (2014) proposed also to use a CNN architecture from 11 up to 19 layers with 133 up to 144 million of parameters.", "startOffset": 53, "endOffset": 379}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques.", "startOffset": 15, "endOffset": 1236}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques.", "startOffset": 15, "endOffset": 1281}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques. Even on smaller datasets, such as MNIST, the accuracy was improved when methods have been including a training set extension process also based on image transformations LeCun et al. (1998). Despite these techniques provide a better sampling of the target manifold, despite the \u201cexpressive power of deep architectures\u201d Bengio & Delalleau (2011) and despite their generalization power Bengio et al.", "startOffset": 15, "endOffset": 1584}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques. Even on smaller datasets, such as MNIST, the accuracy was improved when methods have been including a training set extension process also based on image transformations LeCun et al. (1998). Despite these techniques provide a better sampling of the target manifold, despite the \u201cexpressive power of deep architectures\u201d Bengio & Delalleau (2011) and despite their generalization power Bengio et al.", "startOffset": 15, "endOffset": 1739}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques. Even on smaller datasets, such as MNIST, the accuracy was improved when methods have been including a training set extension process also based on image transformations LeCun et al. (1998). Despite these techniques provide a better sampling of the target manifold, despite the \u201cexpressive power of deep architectures\u201d Bengio & Delalleau (2011) and despite their generalization power Bengio et al. (2013), the work of Szegedy et al.", "startOffset": 15, "endOffset": 1799}, {"referenceID": 2, "context": "In their paper Bengio et al. (2007), Bengio et al. explain clearly that complex decisions can be seen as highly-varying functions and that the decision making algorithm which intends to comprehend all these variations must be composed of many non-linearities. This specificity of deep architectures also impacts the representation compactness of highly-varying functions. In this same paper, they illustrate how deep architectures outperform shallow ones in terms of number of computational units required and thus training samples, in order to represent a given function. Their examples highlight even the differences in representation compactness between deep architectures, with respect to the problem. Considering the huge amount of parameters of the aforementioned deep architectures to be learnt in order to address ImageNet Challenge, one can understand that the training set has to be huge too. Furthermore, in order to cover the dispersion of the input distribution, strategies to extend the training set have appeared. For instance, when deep networks have made an entrance since 2012 ImageNet challenge, they have been going along with image transformation pre-processing to enlarge the training set Krizhevsky et al. (2012). In 2013, for instance, Howard Howard (2013) started from the SuperVision team winning approach of 2012 and increase the accuracy by 20% with such techniques. Even on smaller datasets, such as MNIST, the accuracy was improved when methods have been including a training set extension process also based on image transformations LeCun et al. (1998). Despite these techniques provide a better sampling of the target manifold, despite the \u201cexpressive power of deep architectures\u201d Bengio & Delalleau (2011) and despite their generalization power Bengio et al. (2013), the work of Szegedy et al. Szegedy et al. (2013) demonstrated this was not enough.", "startOffset": 15, "endOffset": 1849}, {"referenceID": 17, "context": "The first algorithm based on Query By Committee (QBC) strategy has been proposed by Seung et al. Seung et al. (1992). They proved two important results: first, the generalization error (denoted g in the original article) of a linear classifier for random training samples behaves as the inverse power law, g(\u03b1) \u223c 1/\u03b1 where \u03b1 = P/N with P the number of training samples considered and N the dimension of the input space; second, the generalization error of a linear classifier for training samples selectect through a query by committee strategy, scales like g(\u03b1) \u223c e\u2212\u03b1I with the constant decay is given by the information gain I .", "startOffset": 84, "endOffset": 117}, {"referenceID": 5, "context": "Later Freund et al. Freund et al. (1997) proved that this property hold for a more general class of learning problems.", "startOffset": 6, "endOffset": 41}, {"referenceID": 14, "context": "Those semi supervised technique adapted for deep architectures are leaveraging the size of the annotated database while being competitive with convolutional networks trained on the full annotated training set like done in Rasmus et al. (2015) or Kingma et al.", "startOffset": 222, "endOffset": 243}, {"referenceID": 11, "context": "(2015) or Kingma et al. (2014). However those methods are mostly based on generative networks Kingma & Welling (2013), which are at the present time adapted to a restricted are of inputs like images (Kingma et al.", "startOffset": 10, "endOffset": 31}, {"referenceID": 11, "context": "(2015) or Kingma et al. (2014). However those methods are mostly based on generative networks Kingma & Welling (2013), which are at the present time adapted to a restricted are of inputs like images (Kingma et al.", "startOffset": 10, "endOffset": 118}, {"referenceID": 11, "context": "(2015) or Kingma et al. (2014). However those methods are mostly based on generative networks Kingma & Welling (2013), which are at the present time adapted to a restricted are of inputs like images (Kingma et al. (2014)) or texts( Graves (2013)).", "startOffset": 10, "endOffset": 221}, {"referenceID": 11, "context": "(2015) or Kingma et al. (2014). However those methods are mostly based on generative networks Kingma & Welling (2013), which are at the present time adapted to a restricted are of inputs like images (Kingma et al. (2014)) or texts( Graves (2013)).", "startOffset": 10, "endOffset": 246}, {"referenceID": 24, "context": "In Zhou et al. (2010), EZhou et al.", "startOffset": 3, "endOffset": 22}, {"referenceID": 17, "context": "It has experimentally been shown Saxe et al. (2011) that CNN initialized with random weights already hold some capability of discriminating classes (up to a certain accuracy of course) so that the initial annotated subset might be left empty, however we have chosen to start from an initial random subset of the training set to converge faster to relevant samples and benefit from the sample selection of our committee based approach from the first epochs.", "startOffset": 33, "endOffset": 52}, {"referenceID": 17, "context": "It has experimentally been shown Saxe et al. (2011) that CNN initialized with random weights already hold some capability of discriminating classes (up to a certain accuracy of course) so that the initial annotated subset might be left empty, however we have chosen to start from an initial random subset of the training set to converge faster to relevant samples and benefit from the sample selection of our committee based approach from the first epochs. After several epochs, the performance of this initial network does not increase anymore on the validation set. In order to improve the accuracy, we then inject a new minibatch of samples. Note that adding a minibatch of examples differ from Gammelsaeter (2015) in which samples are added to the training set one at a time.", "startOffset": 33, "endOffset": 718}, {"referenceID": 17, "context": "It has experimentally been shown Saxe et al. (2011) that CNN initialized with random weights already hold some capability of discriminating classes (up to a certain accuracy of course) so that the initial annotated subset might be left empty, however we have chosen to start from an initial random subset of the training set to converge faster to relevant samples and benefit from the sample selection of our committee based approach from the first epochs. After several epochs, the performance of this initial network does not increase anymore on the validation set. In order to improve the accuracy, we then inject a new minibatch of samples. Note that adding a minibatch of examples differ from Gammelsaeter (2015) in which samples are added to the training set one at a time. Furthermore, adding minibatches of samples fosters gpu parallelization for training deep architectures, plus balances every sample in the training set when applying mean gradient descent. We hence select a random subset of K samples from the rest of the training set. Let us notice than we are not requesting here the labels of the samples(Zhou et al. (2010)).", "startOffset": 33, "endOffset": 1139}, {"referenceID": 17, "context": "It has experimentally been shown Saxe et al. (2011) that CNN initialized with random weights already hold some capability of discriminating classes (up to a certain accuracy of course) so that the initial annotated subset might be left empty, however we have chosen to start from an initial random subset of the training set to converge faster to relevant samples and benefit from the sample selection of our committee based approach from the first epochs. After several epochs, the performance of this initial network does not increase anymore on the validation set. In order to improve the accuracy, we then inject a new minibatch of samples. Note that adding a minibatch of examples differ from Gammelsaeter (2015) in which samples are added to the training set one at a time. Furthermore, adding minibatches of samples fosters gpu parallelization for training deep architectures, plus balances every sample in the training set when applying mean gradient descent. We hence select a random subset of K samples from the rest of the training set. Let us notice than we are not requesting here the labels of the samples(Zhou et al. (2010)). A committee of deep networks assigns then a score to each sample from the selected minibatch, this score being related to the impact of the considered sample on the training of the deep architecture. If we denote by batch size the number of samples in a minibatch, we then request the labels of the batch size samples having the greatest score among the K samples. The score of a sample correspond to the number of models in the committee whose prediction differs from the majority prediction among the committee members. The quality of the score is related to both the size of the committee and the correlation between the models Melville & Mooney (2004).", "startOffset": 33, "endOffset": 1797}, {"referenceID": 17, "context": "In the early papers describing active learning through committee selection, convergence and better result against randomness have been proven, first for perceptron-like learning functions Seung et al. (1992) and later for more general classes of learning problems Freund et al.", "startOffset": 188, "endOffset": 208}, {"referenceID": 5, "context": "(1992) and later for more general classes of learning problems Freund et al. (1997). However, for those results to hold, each model of our committee has to lie in the current version space defined by the annotated training set (which is the set of networks built from the same \u2019architecture\u2019 and making no mistake on the current training set).", "startOffset": 63, "endOffset": 84}, {"referenceID": 8, "context": "To initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network.", "startOffset": 119, "endOffset": 140}, {"referenceID": 8, "context": "To initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network. The batchwise dropout Graham et al. (2015) is a version of dropout where we use a unique bernouilli mask to discard neurons for each sample in the minibatch.", "startOffset": 119, "endOffset": 204}, {"referenceID": 8, "context": "To initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network. The batchwise dropout Graham et al. (2015) is a version of dropout where we use a unique bernouilli mask to discard neurons for each sample in the minibatch. Thus the batchwise dropout reduce quadratically in the percentage of kept neurons the number of parameters in the architecture. Moreover dropout, when applied on convolutional layers remove neurons independently given the spatial locations. Whereas batchwise dropout is spatially dependant, switching on or off filters so to discard neurons obtained through the same filter. It is thus preserving the consistency in a CNN architecture. This technique is required to extend QBDC on CNN architecture which may be one of the reasons for the difference in the accuracy with Gammelsaeter (2015) that solely applied the dropout committee on MLP architectures.", "startOffset": 119, "endOffset": 909}, {"referenceID": 8, "context": "To initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network. The batchwise dropout Graham et al. (2015) is a version of dropout where we use a unique bernouilli mask to discard neurons for each sample in the minibatch. Thus the batchwise dropout reduce quadratically in the percentage of kept neurons the number of parameters in the architecture. Moreover dropout, when applied on convolutional layers remove neurons independently given the spatial locations. Whereas batchwise dropout is spatially dependant, switching on or off filters so to discard neurons obtained through the same filter. It is thus preserving the consistency in a CNN architecture. This technique is required to extend QBDC on CNN architecture which may be one of the reasons for the difference in the accuracy with Gammelsaeter (2015) that solely applied the dropout committee on MLP architectures. In Graham et al. (2015), a batchwise dropout committee has already been envisaged for future work in order to fasten the testing while averaging the prediction on the full network Baldi & Sadowski (2014).", "startOffset": 119, "endOffset": 997}, {"referenceID": 8, "context": "To initiate a partial CNN while getting rid of the computation cost due to backpropagation, we apply batchwise dropout Graham et al. (2015) on our full network. The batchwise dropout Graham et al. (2015) is a version of dropout where we use a unique bernouilli mask to discard neurons for each sample in the minibatch. Thus the batchwise dropout reduce quadratically in the percentage of kept neurons the number of parameters in the architecture. Moreover dropout, when applied on convolutional layers remove neurons independently given the spatial locations. Whereas batchwise dropout is spatially dependant, switching on or off filters so to discard neurons obtained through the same filter. It is thus preserving the consistency in a CNN architecture. This technique is required to extend QBDC on CNN architecture which may be one of the reasons for the difference in the accuracy with Gammelsaeter (2015) that solely applied the dropout committee on MLP architectures. In Graham et al. (2015), a batchwise dropout committee has already been envisaged for future work in order to fasten the testing while averaging the prediction on the full network Baldi & Sadowski (2014). The main advantage is to obtain a committee which the committee has the same architecture than the full network, with members having some zero constraints on several connections.", "startOffset": 119, "endOffset": 1177}, {"referenceID": 14, "context": "95 %, LeCun et al. (1998)), the error rate on the QBDC full architecture (a network trained using only the samples selected with QBDC) is competitive with the state of the art : we get an average error rate of 1.", "startOffset": 6, "endOffset": 26}, {"referenceID": 20, "context": "To improve the initial performance when instantiating a committee, training the full network with dropout Srivastava et al. (2014) appears like an intuitive solution.", "startOffset": 106, "endOffset": 131}, {"referenceID": 20, "context": "To improve the initial performance when instantiating a committee, training the full network with dropout Srivastava et al. (2014) appears like an intuitive solution. Indeed when applying natural dropout proposed by Hinton et al. in Srivastava et al. (2014), every possible model of the committee will be considered while training the full network.", "startOffset": 106, "endOffset": 258}, {"referenceID": 7, "context": "out in Goodfellow et al. (2014), model averaging or dropout are not helping the network to be more robust from adversarial examples so that the committee decision strategy itself is still not preserving QBDC from considering and selecting adversarial examples.", "startOffset": 7, "endOffset": 32}, {"referenceID": 7, "context": "To generate efficiently approximate adversary we used the fast gradient sign method proposed by Goodfellow et al. in (Goodfellow et al. (2014), see 3).", "startOffset": 96, "endOffset": 143}, {"referenceID": 7, "context": "Table 3: formula of the fast gradient sign method Goodfellow et al. (2014) \u2022 \u0398 the parameters of the model \u2022 x the input of the model \u2022 y the targets associated with x \u2022 J(\u0398, x, y) the cost used to train the network \u2022 \u03b7 the max norm constrained perturbation \u03b7 = sign(\u2206xJ(\u0398, x, y))", "startOffset": 50, "endOffset": 75}, {"referenceID": 1, "context": "We would like to thank the developpers of the frameworks Theano Bastien et al. (2012), Blocks and Fuel van Merri\u00ebnboer et al.", "startOffset": 64, "endOffset": 86}, {"referenceID": 1, "context": "We would like to thank the developpers of the frameworks Theano Bastien et al. (2012), Blocks and Fuel van Merri\u00ebnboer et al. (2015) which we used for the experi- ments.", "startOffset": 64, "endOffset": 133}], "year": 2017, "abstractText": "While the current trend is to increase the depth of neural networks to increase their performance, the size of their training database has to grow accordingly. We notice an emergence of tremendous databases, although providing labels to build a training set still remains a very expensive task. We tackle the problem of selecting the samples to be labelled in an online fashion. In this paper, we present an active learning strategy based on query by committee and dropout technique to train a Convolutional Neural Network (CNN). We derive a commmittee of partial CNNs resulting from batchwise dropout runs on the initial CNN. We evaluate our active learning strategy for CNN on MNIST benchmark, showing in particular that selecting less than 30 % from the annotated database is enough to get similar error rate as using the full training set on MNIST. We also studied the robustness of our method against adversarial examples.", "creator": "LaTeX with hyperref package"}}}