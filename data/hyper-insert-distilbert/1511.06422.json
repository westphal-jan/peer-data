{"id": "1511.06422", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "All you need is a good init", "abstract": "layer - sequential differential unit - variance ( dt lsuv ) initialization - establishing a simple strategy for weight dependency initialization optimal for high deep distributed net kernel learning - is proposed. traditionally the strategy initially proceeds from paying the first to ignore the smaller final layer, normalizing the dependency variance of fixing the average output requirement of considering each network layer to once be equal to the one. roughly we show that with running the strategy, weight learning of very deep n nets via standard stochastic gradient descent trees is at ~ least as potentially fast starting as the complex schemes proposed in specifically generated for very deep nets and such constructions as fuzzy fitnets ( romero gomez et des al. ( 2015 ) ) averaging and drift highway ( srivastava et al. ( cohen 2015 ) ). performance that is automatically state - of - the - art, or prevents very close access to it, is achieved on the dual mnist, global cifar, imagenet computational datasets.", "histories": [["v1", "Thu, 19 Nov 2015 22:19:15 GMT  (242kb,D)", "http://arxiv.org/abs/1511.06422v1", "ICLR 2016 Submission"], ["v2", "Wed, 9 Dec 2015 14:38:33 GMT  (242kb,D)", "http://arxiv.org/abs/1511.06422v2", "ICLR 2016 Submission"], ["v3", "Mon, 11 Jan 2016 18:46:03 GMT  (765kb,D)", "http://arxiv.org/abs/1511.06422v3", "ICLR 2016 Submission"], ["v4", "Wed, 13 Jan 2016 17:47:07 GMT  (766kb,D)", "http://arxiv.org/abs/1511.06422v4", "ICLR 2016 Submission"], ["v5", "Mon, 18 Jan 2016 20:07:09 GMT  (815kb,D)", "http://arxiv.org/abs/1511.06422v5", "ICLR 2016 Submission"], ["v6", "Wed, 27 Jan 2016 15:10:19 GMT  (816kb,D)", "http://arxiv.org/abs/1511.06422v6", "ICLR 2016 Submission"], ["v7", "Fri, 19 Feb 2016 14:37:10 GMT  (816kb,D)", "http://arxiv.org/abs/1511.06422v7", "Published as a conference paper at ICLR 2016"]], "COMMENTS": "ICLR 2016 Submission", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dmytro mishkin", "jiri matas"], "accepted": true, "id": "1511.06422"}, "pdf": {"name": "1511.06422.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Dmytro Mishkin", "Jiri Matas"], "emails": ["mishkdmy@cmp.felk.cvut.cz", "matas@cmp.felk.cvut.cz"], "sections": [{"heading": null, "text": "Layer-sequential unit-variance (LSUV) initialization \u2013 a simple strategy for weight initialization for deep net learning \u2013 is proposed. The strategy proceeds from the first to the final layer, normalizing the variance of the output of each layer to be equal to one.\nWe show that with the strategy, learning of very deep nets via standard stochastic gradient descent is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance that is state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR, ImageNet datasets."}, {"heading": "1 INTRODUCTION", "text": "Deep nets have demonstrated impressive results on a number of computer vision and natural language processing problems. At present, state-of-the-art results in image classification (Simonyan & Zisserman (2015); Szegedy et al. (2015)) and speech recognition (Sercu et al. (2015)), etc., have been achieved with very deep (\u2265 16 layer) CNNs. Thin deep nets are of particular interest, since they are accurate and at the same inference-time efficient (Romero et al. (2015)).\nOne of the main obstacles preventing the wide adoption of very deep nets is the absence of an efficient, repeatable and general procedure for their end-to-end training. For example, VGGNet (Simonyan & Zisserman (2015)) was optimized by a four stage procedure that started by training a network with moderate depth, adding progressively more layers. Romero et al. (2015) claimed that deep and thin networks cannot be trained, unlike wide nets, by backpropagation if deeper than five layers.\nOn the other hand, He et al. (2015) showed that it is possible to train the VGGNet in a single optimization run if the network weights are initialized with a specific ReLU-aware initialization. The He et al. (2015) procedure generalizes the idea of applying correction coefficient to the output of each layers, introduce for the linear case by (Glorot & Bengio (2010)), to the ReLU non-linearity. Batch normalization (Ioffe & Szegedy (2015)), a technique that inserts layers into the the deep net that transform the output for the batch to be zero mean unit variance, has successfully facilitated training of the twenty-two layer GoogLeNet (Szegedy et al. (2015)). However, batch normalization adds a 30% computational overhead to each iteration.\nThe main contribution of the paper is a proposal of a simple initialization procedure that, in connection with standard SGD, leads to state-of-the-art thin and very deep neural nets1. This result highlights the importance of initialization in very deep nets. We therefore review the history of CNN initialization in Section 2, which is followed by a detailed description of the novel initialization method in Section 3. The method is experimentally validated in Section 4.\n1The code for reproducing experiments is available at https://github.com/ducha-aiki/ LSUVinit\nar X\niv :1\n51 1.\n06 42\n2v 1\n[ cs\n.L G\n] 1\n9 N\nov 2\n01 5"}, {"heading": "2 INITIALIZATION IN NEURAL NETWORKS", "text": "After the success of CNNs in IVSRC 2012 (Krizhevsky et al. (2012)), initialization with Gaussian noise with standard deviation set to 0.01 and adding bias equal to one for some layers become very popular. But, as mentioned before, it is not possible to train very deep network from scratch with it (Simonyan & Zisserman (2015)). The problem is caused by the activation (and/or) gradient magnitude in final layers (He et al. (2015)). If each layer (not properly initialized) scales input by k, the final scale would be kL, where L - number of layers. Values of k > 1 lead to firing extremely large values on output layers, k < 1 leads to a diminishing signal and gradient.\nGlorot & Bengio (2010) proposed a formula for estimating the standard deviation on the basis of the number of input (and/or) output channels of the layers under assumption of no non-linearity between layers. Despite invalidity of the assumption, Glorot initialization works well for many real cases. He et al. (2015) extended this formula to the ReLU (Glorot et al. (2011)) non-linearity and showed its superior performance for ReLU-based nets.\nThe Hinton et al. and Romero et al. (2015) take another approach and use training to predict teacher predictions and internal representations as better goal than plain softmax loss and good regularization. Srivastava et al. (2015) proposed LSTM-inspired gating scheme to regulate information and gradient flow through network and trained 1000-layers MLP network on MNIST. Basically, this kind of networks learns needed depth for given task.\nThe approach of layer-wise pre-training (Bengio et al. (2007)) which is still useful for multi-layerperceptron, is not popular for training discriminative convolution networks.\nIndependently, Saxe et al. (2014) showed that orthonormal matrix initialization works much better for linear networks, that gaussian noise, which is only approximate orthogonal. It also work for networks with non-linearities."}, {"heading": "3 LAYER-SEQUENTIAL UNIT-VARIANCE INITIALIZATION", "text": "To the best of our knowledge, there have been no attempts to generalize Glorot & Bengio (2010) formulas to non-linearities other than ReLU, such as tanh, maxout, etc. Also, the formula does not cover max-pooling, local normalization layers Krizhevsky et al. (2012) and other types of layers which influences activations variance. Instead of theoretical derivation for all possible layer types, we propose a data-driven weight initialization.\nWe thus extend the orthonormal initialization Saxe et al. (2014) to an iterative procedure, showed in Algorithm 1. Saxe et al. (2014) could be implemented in two steps. First, fill the weights with Gaussian noise with unit variance. Second, decompose them to orthonormal basis with QR or SVDdecomposition and replace weights with one of the components.\nThe LSUV process then estimates output variance of each layer and scales the weight to make variance equal to one.\nProposed scheme can be viewed as batch normalization performed only on the first batch. Experiments show such normalization is sufficient and computationally highly efficient in comparison with full batch normalization.\nThe LSUV algorithm is summarized in Algorithm 1. Because of data variations, it is often not possible to normalize variance with desired precision. To eliminate infinite loop, we restricted counter of trials by Tmax.\nWe tested a variant LSUV initialization which was normalizing input activations of the each instead of output one. Normalizing the input or output is identical for nets standard feed-forward nets, but normalizing input is much more complicated for networks with maxout (Goodfellow et al. (2013)) or for networks like GoogLeNet (Szegedy et al. (2015)) which use the output of multiple layers as input. Input normalization brought no improvement of results, when tested against the LSUV algorithm Algorithm 1,\nLSUV was also tested with pre-initialization of weights with Gaussian noise instead of orthonormal matrices. The gaussian initialization led to small, but consistent, decrease in performance.\nAlgorithm 1 Layer-sequential unit-variance orthogonal initialization. L \u2013 convolution or fullconnected layer, WL - its weights, BL - its output blob., Tolvar - variance tolerance, Ti \u2013 current trial, Tmax \u2013 max number of trials.\nPre-initialize network with orthonormal matrices as in Saxe et al. (2014) for each layer L do\nwhile |Var(BL)\u2212 1.0| \u2265 Tolvar and (Ti < Tmax) do do Forward pass calculate Var(BL) WL = WL / \u221a Var(BL)\nend while end for"}, {"heading": "4 EXPERIMENTAL VALIDATION", "text": "Here we show that very deep and thin nets could be trained in single stage. Network architectures are exactly as proposed by Romero et al. (2015). The architectures are presented in Table 4."}, {"heading": "4.1 MNIST", "text": "First, as \u201dsanity check\u201d, we performed experiment on the MNIST dataset (Lecun et al. (1998)). It consists of 60,000 28x28 grayscale images of handwritten digits 0 to 9. We selected FitNet-MNIST architecture (see Table 4) as proposed by Romero et al. (2015) and trained it with the proposed initialization strategy, without data augmentation. Recognition results are shown in Table 2, right block. Both LSUV and orthonormal initializations outperform Hints-initialization and LSUV outperforms orthonormal. The error rates of the Deeply-Supervised Nets(DSN, Lee et al. (2015)) and maxout networks (maxout, Goodfellow et al. (2013)), the current state-of-art, are provided for reference.\nSince the widely cited DSN error rate of 0.39%, the state-of-the-art (until recently) was obtained after replacing softmax classifier with SVM, we perform the change and also observe improved results (line FitNet-LSUV-SVM in Table 2)."}, {"heading": "4.2 CIFAR-10/100", "text": "We validated the proposed initialization LSUV strategy on the CIFAR-10/100 (Krizhevsky (2009)) dataset. It contains 60,000 32x32 RGB images, which are divided into 10 and 100 classes, respectively.\nThe FitNets are trained with the stochastic gradient descent with momentum set to 0.9, the initial learning rate set to 0.01 and reduced by a factor of 10 at 100th, 150th and 200th epoch.\nLike in the MNIST experiment, LSUV and orthonormal initialized nets outperformed Hints-trained Fitnets, leading to the new state-of-art when using commonly used augmentation \u2013 mirroring and random shifts. The gain on the fine-grained CIFAR-100 is much larger than on CIFAR-10. Also, note that FitNets with LSUV initialization even outperform much larger networks with as LargeAll-CNN Springenberg et al. (2014) and Fractional Max-pooling Graham (2014) trained with affine and color dataset augmentation on CIFAR-100. The results of LSUV are virtually identical to the orthonormal initialization."}, {"heading": "5 ANALYSIS OF EMPIRICAL RESULTS", "text": ""}, {"heading": "5.1 INITIALIZATION STRATEGIES AND NON-LINEARITIES", "text": "For the FitNet-1 architecture, we have not experienced any difficulties training the network with any of the activation functions (ReLU, maxout, tanh), optimizers (SGD, RMPprop) or initialization (Xavier, MSRA, Ortho, LSUV), contrary to results reported in Romero et al. (2015). The most probable cause is that CNNs toleerate a from a wide variety of mediocre initialization, only the learning time increases. The differences in the final accuracy between the different initialization methods for the FitNet-1 architecture is rather small and are therefore not presented here.\nThe FitNet-4 architecture is much more difficult to optimize and thus we focus on it in the experiments presented in this section.\nWe have explored the compatibility of initializations with different activation functions in very deep networks. More specificly, ReLU, hyperbolic tangent, sigmoid, maxout and the very leaky ReLU (with negative slope equal to 0.333 which is popular in Kaggle competitions Dieleman (2015)).\nTesting was performed on CIFAR-10 and results are in Table 3 and Figure 1. One can see better performance of orthonormal-based methods to the scaled gaussian-noise approaches for all tested types of activation functions, except tanh. Proposed LSUV strategy outperforms orthonormal initialization by smaller margin, but still consistantly (see Table 3). All the methods failed to train sigmoid-based very deep network.\n2 When preparing this submission we have found recent unreviewed paper MIN Chang & Chen (2015) paper, which uses a sophisticated combination of batch normalization, maxout and network-in-network nonlinearities and establishes a new state-of-art on MNIST.\nFigure 1 shows that LSUV method not only leads to better generalization error, but also converges faster for all tested activation functions."}, {"heading": "5.2 CAFFENET TRAINING", "text": "We trained CaffeNet( Jia et al. (2014)) on the ImageNet-1000 dataset( Russakovsky et al. (2015)) with the original initialization and LSUV. CaffeNet is a variant of AlexNet with the same performance, where the order of pooling and normalization layers is switched to reduce the memory footprint.\nLSUV initialization reduces the the starting flat-loss time from 0.5 epochs to 0.05, and starts to converge faster, but it is overtaken by standard CaffeNet at the 30-th epoch (see Figure 2) and its final precision is 1.3% lower. A similar behavior is observed with the orthonorm-initialized network3. We have no explanation for this empirical phenomenon and exploring it further."}, {"heading": "6 CONCLUSIONS", "text": "LSUV \u2013 layer sequential uniform variance \u2013 a simple strategy for weight initialization for deep net learning - is proposed. We have showed that the LSUV initialization, described fully in six lines of pseudocode, is as good as complex learning schemes that need for instance auxiliary nets.\nWe LSUV initialization, learning of very deep nets via standard SGD is fast and leads to (near) stateof-the-art results on MNIST, CIFAR, ImageNet datasets, outperforming the sophisticated systems\n3Experiments with orthonorm-initialized CaffeNet have not finished at submission time\ndesigned specifically for very deep nets such as FitNets( Romero et al. (2015)) and Highway( Srivastava et al. (2015)). The proposed initialization works well with different activation functions.\nOur experiments confirm the finding of Romero et al. (2015) that very thin, thus fast and low in parameters, but deep networks obtain comparable or even better performance than wider, but shallower ones."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors were supported by The Czech Science Foundation Project GACR P103/12/G084."}], "references": [{"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Batch-normalized Maxout Network in Network", "author": ["Chang", "J.-R", "Chen", "Y.-S"], "venue": "ArXiv e-prints, November", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Classifying plankton with deep neural networks, 2015. URL http:// benanne.github.io/2015/03/17/plankton.html", "author": ["Dieleman", "Sander"], "venue": null, "citeRegEx": "Dieleman and Sander.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman and Sander.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11),", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Fractional Max-Pooling", "author": ["B. Graham"], "venue": "ArXiv e-prints,", "citeRegEx": "Graham,? \\Q2014\\E", "shortCiteRegEx": "Graham", "year": 2014}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia", "Yangqing", "Shelhamer", "Evan", "Donahue", "Jeff", "Karayev", "Sergey", "Long", "Jonathan", "Girshick", "Ross", "Guadarrama", "Sergio", "Darrell", "Trevor"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Krizhevsky", "Alex"], "venue": "Master\u2019s thesis,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Lecun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lecun et al\\.", "year": 1998}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Romero et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2015}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "Very Deep Multilingual Convolutional Neural Networks for LVCSR", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "ArXiv e-prints,", "citeRegEx": "Sercu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sercu et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale visual recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "In Proceedings of ICLR Workshop,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR 2015,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Lecture 6.5: RMSProp \u2013 Divide the gradient by a running average of its recent magnitude", "author": ["Tieleman", "Tijmen", "Hinton", "Geoffrey"], "venue": "In COURSERA: Neural Networks for Machine Learning", "citeRegEx": "Tieleman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 11, "context": "We show that with the strategy, learning of very deep nets via standard stochastic gradient descent is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al.", "startOffset": 201, "endOffset": 222}, {"referenceID": 11, "context": "We show that with the strategy, learning of very deep nets via standard stochastic gradient descent is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)).", "startOffset": 201, "endOffset": 261}, {"referenceID": 13, "context": "At present, state-of-the-art results in image classification (Simonyan & Zisserman (2015); Szegedy et al. (2015)) and speech recognition (Sercu et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 11, "context": "(2015)) and speech recognition (Sercu et al. (2015)), etc.", "startOffset": 32, "endOffset": 52}, {"referenceID": 10, "context": "Thin deep nets are of particular interest, since they are accurate and at the same inference-time efficient (Romero et al. (2015)).", "startOffset": 109, "endOffset": 130}, {"referenceID": 10, "context": "Thin deep nets are of particular interest, since they are accurate and at the same inference-time efficient (Romero et al. (2015)). One of the main obstacles preventing the wide adoption of very deep nets is the absence of an efficient, repeatable and general procedure for their end-to-end training. For example, VGGNet (Simonyan & Zisserman (2015)) was optimized by a four stage procedure that started by training a network with moderate depth, adding progressively more layers.", "startOffset": 109, "endOffset": 350}, {"referenceID": 10, "context": "Thin deep nets are of particular interest, since they are accurate and at the same inference-time efficient (Romero et al. (2015)). One of the main obstacles preventing the wide adoption of very deep nets is the absence of an efficient, repeatable and general procedure for their end-to-end training. For example, VGGNet (Simonyan & Zisserman (2015)) was optimized by a four stage procedure that started by training a network with moderate depth, adding progressively more layers. Romero et al. (2015) claimed that deep and thin networks cannot be trained, unlike wide nets, by backpropagation if deeper than five layers.", "startOffset": 109, "endOffset": 502}, {"referenceID": 6, "context": "On the other hand, He et al. (2015) showed that it is possible to train the VGGNet in a single optimization run if the network weights are initialized with a specific ReLU-aware initialization.", "startOffset": 19, "endOffset": 36}, {"referenceID": 6, "context": "On the other hand, He et al. (2015) showed that it is possible to train the VGGNet in a single optimization run if the network weights are initialized with a specific ReLU-aware initialization. The He et al. (2015) procedure generalizes the idea of applying correction coefficient to the output of each layers, introduce for the linear case by (Glorot & Bengio (2010)), to the ReLU non-linearity.", "startOffset": 19, "endOffset": 215}, {"referenceID": 6, "context": "On the other hand, He et al. (2015) showed that it is possible to train the VGGNet in a single optimization run if the network weights are initialized with a specific ReLU-aware initialization. The He et al. (2015) procedure generalizes the idea of applying correction coefficient to the output of each layers, introduce for the linear case by (Glorot & Bengio (2010)), to the ReLU non-linearity.", "startOffset": 19, "endOffset": 368}, {"referenceID": 6, "context": "On the other hand, He et al. (2015) showed that it is possible to train the VGGNet in a single optimization run if the network weights are initialized with a specific ReLU-aware initialization. The He et al. (2015) procedure generalizes the idea of applying correction coefficient to the output of each layers, introduce for the linear case by (Glorot & Bengio (2010)), to the ReLU non-linearity. Batch normalization (Ioffe & Szegedy (2015)), a technique that inserts layers into the the deep net that transform the output for the batch to be zero mean unit variance, has successfully facilitated training of the twenty-two layer GoogLeNet (Szegedy et al.", "startOffset": 19, "endOffset": 441}, {"referenceID": 6, "context": "On the other hand, He et al. (2015) showed that it is possible to train the VGGNet in a single optimization run if the network weights are initialized with a specific ReLU-aware initialization. The He et al. (2015) procedure generalizes the idea of applying correction coefficient to the output of each layers, introduce for the linear case by (Glorot & Bengio (2010)), to the ReLU non-linearity. Batch normalization (Ioffe & Szegedy (2015)), a technique that inserts layers into the the deep net that transform the output for the batch to be zero mean unit variance, has successfully facilitated training of the twenty-two layer GoogLeNet (Szegedy et al. (2015)).", "startOffset": 19, "endOffset": 663}, {"referenceID": 5, "context": "After the success of CNNs in IVSRC 2012 (Krizhevsky et al. (2012)), initialization with Gaussian noise with standard deviation set to 0.", "startOffset": 41, "endOffset": 66}, {"referenceID": 5, "context": "After the success of CNNs in IVSRC 2012 (Krizhevsky et al. (2012)), initialization with Gaussian noise with standard deviation set to 0.01 and adding bias equal to one for some layers become very popular. But, as mentioned before, it is not possible to train very deep network from scratch with it (Simonyan & Zisserman (2015)).", "startOffset": 41, "endOffset": 327}, {"referenceID": 3, "context": "The problem is caused by the activation (and/or) gradient magnitude in final layers (He et al. (2015)).", "startOffset": 85, "endOffset": 102}, {"referenceID": 3, "context": "The problem is caused by the activation (and/or) gradient magnitude in final layers (He et al. (2015)). If each layer (not properly initialized) scales input by k, the final scale would be k, where L - number of layers. Values of k > 1 lead to firing extremely large values on output layers, k < 1 leads to a diminishing signal and gradient. Glorot & Bengio (2010) proposed a formula for estimating the standard deviation on the basis of the number of input (and/or) output channels of the layers under assumption of no non-linearity between layers.", "startOffset": 85, "endOffset": 365}, {"referenceID": 3, "context": "The problem is caused by the activation (and/or) gradient magnitude in final layers (He et al. (2015)). If each layer (not properly initialized) scales input by k, the final scale would be k, where L - number of layers. Values of k > 1 lead to firing extremely large values on output layers, k < 1 leads to a diminishing signal and gradient. Glorot & Bengio (2010) proposed a formula for estimating the standard deviation on the basis of the number of input (and/or) output channels of the layers under assumption of no non-linearity between layers. Despite invalidity of the assumption, Glorot initialization works well for many real cases. He et al. (2015) extended this formula to the ReLU (Glorot et al.", "startOffset": 85, "endOffset": 659}, {"referenceID": 2, "context": "(2015) extended this formula to the ReLU (Glorot et al. (2011)) non-linearity and showed its superior performance for ReLU-based nets.", "startOffset": 42, "endOffset": 63}, {"referenceID": 2, "context": "(2015) extended this formula to the ReLU (Glorot et al. (2011)) non-linearity and showed its superior performance for ReLU-based nets. The Hinton et al. and Romero et al. (2015) take another approach and use training to predict teacher predictions and internal representations as better goal than plain softmax loss and good regularization.", "startOffset": 42, "endOffset": 178}, {"referenceID": 2, "context": "(2015) extended this formula to the ReLU (Glorot et al. (2011)) non-linearity and showed its superior performance for ReLU-based nets. The Hinton et al. and Romero et al. (2015) take another approach and use training to predict teacher predictions and internal representations as better goal than plain softmax loss and good regularization. Srivastava et al. (2015) proposed LSTM-inspired gating scheme to regulate information and gradient flow through network and trained 1000-layers MLP network on MNIST.", "startOffset": 42, "endOffset": 366}, {"referenceID": 0, "context": "The approach of layer-wise pre-training (Bengio et al. (2007)) which is still useful for multi-layerperceptron, is not popular for training discriminative convolution networks.", "startOffset": 41, "endOffset": 62}, {"referenceID": 0, "context": "The approach of layer-wise pre-training (Bengio et al. (2007)) which is still useful for multi-layerperceptron, is not popular for training discriminative convolution networks. Independently, Saxe et al. (2014) showed that orthonormal matrix initialization works much better for linear networks, that gaussian noise, which is only approximate orthogonal.", "startOffset": 41, "endOffset": 211}, {"referenceID": 9, "context": "Also, the formula does not cover max-pooling, local normalization layers Krizhevsky et al. (2012) and other types of layers which influences activations variance.", "startOffset": 73, "endOffset": 98}, {"referenceID": 9, "context": "Also, the formula does not cover max-pooling, local normalization layers Krizhevsky et al. (2012) and other types of layers which influences activations variance. Instead of theoretical derivation for all possible layer types, we propose a data-driven weight initialization. We thus extend the orthonormal initialization Saxe et al. (2014) to an iterative procedure, showed in Algorithm 1.", "startOffset": 73, "endOffset": 340}, {"referenceID": 9, "context": "Also, the formula does not cover max-pooling, local normalization layers Krizhevsky et al. (2012) and other types of layers which influences activations variance. Instead of theoretical derivation for all possible layer types, we propose a data-driven weight initialization. We thus extend the orthonormal initialization Saxe et al. (2014) to an iterative procedure, showed in Algorithm 1. Saxe et al. (2014) could be implemented in two steps.", "startOffset": 73, "endOffset": 409}, {"referenceID": 9, "context": "Also, the formula does not cover max-pooling, local normalization layers Krizhevsky et al. (2012) and other types of layers which influences activations variance. Instead of theoretical derivation for all possible layer types, we propose a data-driven weight initialization. We thus extend the orthonormal initialization Saxe et al. (2014) to an iterative procedure, showed in Algorithm 1. Saxe et al. (2014) could be implemented in two steps. First, fill the weights with Gaussian noise with unit variance. Second, decompose them to orthonormal basis with QR or SVDdecomposition and replace weights with one of the components. The LSUV process then estimates output variance of each layer and scales the weight to make variance equal to one. Proposed scheme can be viewed as batch normalization performed only on the first batch. Experiments show such normalization is sufficient and computationally highly efficient in comparison with full batch normalization. The LSUV algorithm is summarized in Algorithm 1. Because of data variations, it is often not possible to normalize variance with desired precision. To eliminate infinite loop, we restricted counter of trials by Tmax. We tested a variant LSUV initialization which was normalizing input activations of the each instead of output one. Normalizing the input or output is identical for nets standard feed-forward nets, but normalizing input is much more complicated for networks with maxout (Goodfellow et al. (2013)) or for networks like GoogLeNet (Szegedy et al.", "startOffset": 73, "endOffset": 1475}, {"referenceID": 9, "context": "Also, the formula does not cover max-pooling, local normalization layers Krizhevsky et al. (2012) and other types of layers which influences activations variance. Instead of theoretical derivation for all possible layer types, we propose a data-driven weight initialization. We thus extend the orthonormal initialization Saxe et al. (2014) to an iterative procedure, showed in Algorithm 1. Saxe et al. (2014) could be implemented in two steps. First, fill the weights with Gaussian noise with unit variance. Second, decompose them to orthonormal basis with QR or SVDdecomposition and replace weights with one of the components. The LSUV process then estimates output variance of each layer and scales the weight to make variance equal to one. Proposed scheme can be viewed as batch normalization performed only on the first batch. Experiments show such normalization is sufficient and computationally highly efficient in comparison with full batch normalization. The LSUV algorithm is summarized in Algorithm 1. Because of data variations, it is often not possible to normalize variance with desired precision. To eliminate infinite loop, we restricted counter of trials by Tmax. We tested a variant LSUV initialization which was normalizing input activations of the each instead of output one. Normalizing the input or output is identical for nets standard feed-forward nets, but normalizing input is much more complicated for networks with maxout (Goodfellow et al. (2013)) or for networks like GoogLeNet (Szegedy et al. (2015)) which use the output of multiple layers as input.", "startOffset": 73, "endOffset": 1530}, {"referenceID": 12, "context": "Pre-initialize network with orthonormal matrices as in Saxe et al. (2014) for each layer L do while |Var(BL)\u2212 1.", "startOffset": 55, "endOffset": 74}, {"referenceID": 11, "context": "Network architectures are exactly as proposed by Romero et al. (2015). The architectures are presented in Table 4.", "startOffset": 49, "endOffset": 70}, {"referenceID": 11, "context": "Table 1: FitNets Romero et al. (2015) network architecture used in experiments.", "startOffset": 17, "endOffset": 38}, {"referenceID": 10, "context": "First, as \u201dsanity check\u201d, we performed experiment on the MNIST dataset (Lecun et al. (1998)).", "startOffset": 72, "endOffset": 92}, {"referenceID": 10, "context": "First, as \u201dsanity check\u201d, we performed experiment on the MNIST dataset (Lecun et al. (1998)). It consists of 60,000 28x28 grayscale images of handwritten digits 0 to 9. We selected FitNet-MNIST architecture (see Table 4) as proposed by Romero et al. (2015) and trained it with the proposed initialization strategy, without data augmentation.", "startOffset": 72, "endOffset": 257}, {"referenceID": 10, "context": "First, as \u201dsanity check\u201d, we performed experiment on the MNIST dataset (Lecun et al. (1998)). It consists of 60,000 28x28 grayscale images of handwritten digits 0 to 9. We selected FitNet-MNIST architecture (see Table 4) as proposed by Romero et al. (2015) and trained it with the proposed initialization strategy, without data augmentation. Recognition results are shown in Table 2, right block. Both LSUV and orthonormal initializations outperform Hints-initialization and LSUV outperforms orthonormal. The error rates of the Deeply-Supervised Nets(DSN, Lee et al. (2015)) and maxout networks (maxout, Goodfellow et al.", "startOffset": 72, "endOffset": 574}, {"referenceID": 10, "context": "First, as \u201dsanity check\u201d, we performed experiment on the MNIST dataset (Lecun et al. (1998)). It consists of 60,000 28x28 grayscale images of handwritten digits 0 to 9. We selected FitNet-MNIST architecture (see Table 4) as proposed by Romero et al. (2015) and trained it with the proposed initialization strategy, without data augmentation. Recognition results are shown in Table 2, right block. Both LSUV and orthonormal initializations outperform Hints-initialization and LSUV outperforms orthonormal. The error rates of the Deeply-Supervised Nets(DSN, Lee et al. (2015)) and maxout networks (maxout, Goodfellow et al. (2013)), the current state-of-art, are provided for reference.", "startOffset": 72, "endOffset": 629}, {"referenceID": 14, "context": "Also, note that FitNets with LSUV initialization even outperform much larger networks with as LargeAll-CNN Springenberg et al. (2014) and Fractional Max-pooling Graham (2014) trained with affine and color dataset augmentation on CIFAR-100.", "startOffset": 107, "endOffset": 134}, {"referenceID": 5, "context": "(2014) and Fractional Max-pooling Graham (2014) trained with affine and color dataset augmentation on CIFAR-100.", "startOffset": 34, "endOffset": 48}, {"referenceID": 11, "context": "For the FitNet-1 architecture, we have not experienced any difficulties training the network with any of the activation functions (ReLU, maxout, tanh), optimizers (SGD, RMPprop) or initialization (Xavier, MSRA, Ortho, LSUV), contrary to results reported in Romero et al. (2015). The most probable cause is that CNNs toleerate a from a wide variety of mediocre initialization, only the learning time increases.", "startOffset": 257, "endOffset": 278}, {"referenceID": 11, "context": "For the FitNet-1 architecture, we have not experienced any difficulties training the network with any of the activation functions (ReLU, maxout, tanh), optimizers (SGD, RMPprop) or initialization (Xavier, MSRA, Ortho, LSUV), contrary to results reported in Romero et al. (2015). The most probable cause is that CNNs toleerate a from a wide variety of mediocre initialization, only the learning time increases. The differences in the final accuracy between the different initialization methods for the FitNet-1 architecture is rather small and are therefore not presented here. The FitNet-4 architecture is much more difficult to optimize and thus we focus on it in the experiments presented in this section. We have explored the compatibility of initializations with different activation functions in very deep networks. More specificly, ReLU, hyperbolic tangent, sigmoid, maxout and the very leaky ReLU (with negative slope equal to 0.333 which is popular in Kaggle competitions Dieleman (2015)).", "startOffset": 257, "endOffset": 996}, {"referenceID": 7, "context": "We trained CaffeNet( Jia et al. (2014)) on the ImageNet-1000 dataset( Russakovsky et al.", "startOffset": 21, "endOffset": 39}, {"referenceID": 7, "context": "We trained CaffeNet( Jia et al. (2014)) on the ImageNet-1000 dataset( Russakovsky et al. (2015)) with the original initialization and LSUV.", "startOffset": 21, "endOffset": 96}, {"referenceID": 11, "context": "designed specifically for very deep nets such as FitNets( Romero et al. (2015)) and Highway( Srivastava et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 11, "context": "designed specifically for very deep nets such as FitNets( Romero et al. (2015)) and Highway( Srivastava et al. (2015)).", "startOffset": 58, "endOffset": 118}, {"referenceID": 11, "context": "designed specifically for very deep nets such as FitNets( Romero et al. (2015)) and Highway( Srivastava et al. (2015)). The proposed initialization works well with different activation functions. Our experiments confirm the finding of Romero et al. (2015) that very thin, thus fast and low in parameters, but deep networks obtain comparable or even better performance than wider, but shallower ones.", "startOffset": 58, "endOffset": 256}], "year": 2015, "abstractText": "Layer-sequential unit-variance (LSUV) initialization \u2013 a simple strategy for weight initialization for deep net learning \u2013 is proposed. The strategy proceeds from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. We show that with the strategy, learning of very deep nets via standard stochastic gradient descent is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance that is state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR, ImageNet datasets.", "creator": "LaTeX with hyperref package"}}}