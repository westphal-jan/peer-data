{"id": "1511.06448", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2015", "title": "Learning Representations from EEG with Deep Recurrent-Convolutional Neural Networks", "abstract": "pursuing one of the challenges in concurrently modeling persistent cognitive scale events starting from homogeneous electroencephalogram ( eeg ) biomedical data production is finding physical representations that uniquely are primarily invariant to inter - object and intra - subject differences, primarily as exceptionally well as to inherent transmission noise perceptions associated with such data. herein, we propose starting a practical novel conceptual approach for visually learning such tensor representations obtained from any multi - user channel eeg time - series, and demonstrate its advantages in the context of distributed mental load spatial classification task. first, now we presently transform eeg activities globally into achieving a sequence definition of tagged topology - preserving multi - spectral images, as opposed ideally to assuming standard concurrent eeg analysis techniques that ignore such encoded spatial information. next, we train as a modified deep gradient recurrent - structured convolutional network library inspired by state - of - the - art video network classification to learn robust representations from the sequence dimensions of images. thus the proposed approach technology is roughly designed therefore to preserve the spatial, spectral, and temporal window structure of eeg which leads generally to errors finding reflective features that are less sensitive to variations and include distortions within simply each dimension. empirical evaluation on the resulting cognitive load classification / task tool demonstrated significant resolution improvements seen in maximum classification accuracy over with current state - system of - the - art design approaches aimed in this field.", "histories": [["v1", "Thu, 19 Nov 2015 23:29:55 GMT  (9942kb,D)", "http://arxiv.org/abs/1511.06448v1", null], ["v2", "Thu, 7 Jan 2016 22:04:23 GMT  (17736kb,D)", "http://arxiv.org/abs/1511.06448v2", null], ["v3", "Mon, 29 Feb 2016 21:33:45 GMT  (25079kb,D)", "http://arxiv.org/abs/1511.06448v3", "To be published as a conference paper at ICLR 2016"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["pouya bashivan", "irina rish", "mohammed yeasin", "noel codella"], "accepted": true, "id": "1511.06448"}, "pdf": {"name": "1511.06448.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Pouya Bashivan", "Irina Rish"], "emails": ["pbshivan@memphis.edu", "rish@us.ibm.com", "myeasin@memphis.edu", "nccodell@us.ibm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep neural networks have recently achieved great success in recognition tasks within a wide range of applications including images, videos, speech, and text (Krizhevsky et al., 2012; Graves et al., 2013; Karpathy & Toderici, 2014; Zhang & LeCun, 2015; Hermann et al., 2015). Convolutional neural networks (ConvNets) lie at the core of best current architectures working with images and video data, primarily due to their ability to extract representations that are robust to partial translation and deformation of input patterns (LeCun et al., 1998). On the other hand, recurrent neural networks have delivered state-of-the-art performance in many applications involving dynamics in temporal sequences, such as, for example, handwriting and speech recognition (Graves et al., 2013; 2008). In addition, combination of these two network types have recently been used for video classification (Ng & Hausknecht, 2015).\nDespite numerous successful applications of deep neural networks to large-scale image, video and text data, they remain relatively unexplored in neuroimaging domain. Perhaps one of the main reasons here is that the number of samples in most neuroimaging datasets is limited, thus making such data less adequate for training large-scale networks with millions of parameters. (As it is often demonstrated, the advantages of deep neural networks over traditional machine-learning techniques become more apparent when the dataset size becomes very large.) Nevertheless, deep belief network and ConvNets have been used to learn representations from functional Magnetic Resonance Imaging (fMRI) and Electroencephalogram (EEG) in some previous work with moderate dataset sizes (Plis\nar X\niv :1\n51 1.\n06 44\n8v 1\n[ cs\n.L G\n] 1\n9 N\net al., 2014; Mirowski et al., 2009). Plis et al. (2014) showed that adding several Restricted Boltzman Machine layers to a deep belief network and using supervised pretraining results into networks that can learn increasingly complex representation of the data and achieve considerable increase in classification accuracy. In other works, convolutional and recurrent neural networks have been used to extract representations from EEG time series (Cecotti & Gra\u0308ser, 2011; Guler et al., 2005). For instance, ConvNets were used to encode the 2-D structure of EEG feature maps (Mirowski et al., 2009). These studies demonstrated the potential benefit of adopting down-scaled deep neural networks on neuroimaging data even in the absence of large-scale data sets, as compared to those available for images, video, and text modalities.\nHerein, we explore the capabilities of deep neural nets for modeling cognitive events from EEG data. EEG is a widely used noninvasive neuroimaging modality which operates by measuring changes in electrical voltage on the scalp induced from cortical activity. Using the classical blind-source separation analogy, EEG data can be thought of as a multi-channel \u201dspeech\u201d signal obtained from several \u201dmicrophones\u201d (associated with EEG electrodes) that record signals from multiple \u201dspeakers\u201d (that correspond to activity in cortical regions). State-of-the-art mental state recognition using EEG consists of manual feature selection from continuous time series and applying supervised learning algorithms to learn the discriminative manifold between the states (Lotte & Congedo, 2007; Subasi & Ismail Gursoy, 2010). A key challenge in correctly recognizing mental states from observed brain activity is constructing a model that is robust to translation and deformation of signal in space, frequency, and time, due to inter- and intra-subject differences, as well as signal acquisition protocols. Much of the variations originate from slight individual differences in cortical mapping and/or functioning, giving rise to observed differences in spatial, spectral, and temporal patterns. Moreover, EEG caps which are used to place the electrodes on top of predetermined cortical regions can be another the source of spatial variations in observed responses due to imperfect fitting of the cap on heads of different sizes and shapes. An example illustrating potentially high inter- and intra-subject variability in EEG data is given in Appendix.\nHerein, we propose a novel approach to learning representations from EEG data that relies on deep learning and appears to be more robust to inter- and intra-subject differences, as well as to measurement-related noise. Our approach is fundamentally different from the previous attempts to learn high-level representations from EEG using deep neural networks. Specifically, rather than representing low-level EEG features as a vector, we transform the data into a multi-dimensional tensor which retains the structure of the data throughout the learning process. In other words, we obtain a sequence of topology-preserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Once such EEG \u201dmovie\u201d is obtained, we train deep recurrent-convolutional neural network architectures, inspired by state-of-the-art video classification, to learn robust representations from the sequence of images, or frames. More specifically, we use ConvNet-based neural nets to extract spatial and spectral invariant representations from each frame data, and adopt LSTM network to extract temporal patterns in the frame sequence. Overall, the proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG data, which extracts features that are more robust to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements over current state-of-the-art approaches in this field, reducing the classification error from 15.3% (state-of-art on this application) to 8.9%."}, {"heading": "2 OUR APPROACH", "text": ""}, {"heading": "2.1 MAKING IMAGES FROM EEG TIME-SERIES", "text": "Electroencephalogram includes multiple time series corresponding to measurements across different spatial locations over the cortex. Similarly to speech signals, the most salient features reside in frequency domain, usually studied using spectrogram of the signal. However, as already noted, EEG signal has an additional spatial dimension. Fast Fourier Transform (FFT) is performed on the time series for each trial to estimate the power spectrum of the signal. Oscillatory cortical activity related to memory operations primarily exists in three frequency bands of theta (4-7Hz), alpha (8- 13Hz), and beta (13-30Hz) (Bashivan et al., 2014; Jensen & Tesche, 2002). Sum of squared absolute values within each of the three frequency bands was computed and used as separate measurements for each electrode.\nAggregating spectral measurements for all electrodes to form a feature vector is the standard approach in EEG data analysis. However, this approach is clearly losing the inherent structure of the data in space, frequency, and time. Instead, we propose to transform the measurements into a 2-D image to preserve the spatial structure and use multiple color channels to represent the spectral dimension. Finally, we use the sequence of images derived from consecutive time windows to account for temporal evolutions in brain activity.\nThe EEG electrodes are distributed over the scalp in a three-dimensional space. In order to transform the spatially distributed activity maps as 2-D images, we need to first project the location of electrodes from a 3-dimensional space onto a 2-D surface. However, such transformation should also preserve the relative distance between neighboring electrodes. For this purpose, we used the Azimuthal Equidistant Projection (AEP) borrowed from mapping application (Snyder, 1987). Applying AEP to 3-D electrode locations, we obtain 2-D projected locations of electrodes. Width and height of the image represent the spatial distribution of activities over the cortex. We apply CloughTocher scheme (Alfeld, 1984) for interpolating the scattered power measurements over the scalp and for estimating the values in-between the electrodes over a 32\u00d7 32 mesh. This procedure is repeated for each frequency band of interest, resulting in three topographical activity maps corresponding to each frequency band. The three spatial maps are then merged together to form an image with three (color) channels. This three-channel image is given as an input to a deep convolutional network, as discussed in the following section. Figure 1 illustrates an overview of our multi-step approach to mental state classification from EEG data, where the novelty resides in transforming raw EEG into sequence of images, or frames (EEG \u201dmovie\u201d), combined with recurrent convolutional network architecture applied on top of such transformed EEG data. Note that our approach is general enough to be used in any EEG-based classification task, and a specific problem of mental load classification presented later only serves as an example demonstrating potential advantages of the proposed approach."}, {"heading": "2.2 ARCHITECTURE", "text": "We adopted a recurrent-convolutional neural network to deal with the inherent structure of EEG data. ConvNets were used to deal with variations in space and frequency domains due to their ability to learn good two-dimensional representation of the data. Wherever needed, the extracted representations were fed into another layer to account for temporal variations in the data. We evaluated various types of layers used for extracting temporal patterns, including convolutional and recurrent layers. Essentially, we evaluated the following two primary approaches to the cognitive state classification problem: 1) single-frame approach, where a single image was constructed from spectral\nmeasurements over the complete trial duration. The constructed image was then used as input to the ConvNet. 2) Multi-frame approach in which we divided each trial into 0.5 second windows and constructed the images over each time window. The sequence of images was then used as input data to the recurrent-convolutional network."}, {"heading": "2.2.1 CONVNET ARCHITECTURE", "text": "We adopted an architecture mimicking the VGG network used in Imagenet classification challenge (Simonyan & Zisserman, 2015). This network enjoys a highly scalable architecture which uses stacked convolutional layers with small receptive fields. All convolutional layers use small receptive fields of size 3\u00d7 3 and stride of 1 pixel with ReLU activation function. The convolution layer inputs are padded with 1 pixel to preserve the spatial resolution after convolution. Multiple convolution layers are stacked together which are followed by maxpool layer. Max-pooling is performed over a 2 \u00d7 2 window with stride of 2 pixels. Number of kernels within each convolution layer increases by a factor of two for layers located in deeper stacks. Stacking of multiple convolution layers leads to effective receptive field of higher dimensions while requiring much less parameters (Simonyan & Zisserman, 2015)."}, {"heading": "2.2.2 SINGLE-FRAME APPROACH", "text": "For this approach the single EEG image was generated by applying FFT on the whole trial duration. Purpose of this approach was to find the optimized ConvNet configuration, we first studied a simplified version of the problem by computing the average activity over the complete duration of trial. For this purpose, we computed all power features over the whole duration of trial. Following this procedure, EEG recording for each trial was reduced to a single multi-channel image. We evaluated ConvNet configurations of various depths, as described in Table 1. The convolutional layer parameters here are denoted as conv<receptive field size>-<number of channels>. Essentially, configuration A involves only two ConvNets (Conv3-32) stacked together, followed by maxpool layer; configuration B adds on top of architecture A two more ConvNets (Conv3-64), followed by another maxpool; then configuration C adds one more ConvNet layer (Conv3-128) followed by maxpool; configuration D differs from C by using 4 rather than 2 Conv3-32 ConvNet layers at the beginning. Finally, a fully-connected layer with 512 nodes (FC-512) is added on top of all these architectures, followed by softmax as the last layer.\nTable 1: Evaluated ConvNet configurations for single-frame approach. The convolutional layer parameters are denoted as conv<receptive field size>-<number of channels>.\nConvNet Configurations\nA B C D input (32 \u00d7 32 3-channel image)\nConv3-32 Conv3-32 Conv3-32 Conv3-32 Conv3-32 Conv3-32 Conv3-32 Conv3-32 Conv3-32\nConv3-32 maxpool\nConv3-64 Conv3-64 Conv3-64 Conv3-64 Conv3-64 Conv3-64\nmaxpool Conv3-128 Conv3-128\nmaxpool\nFC-512\nsoftmax"}, {"heading": "2.2.3 MULTI-FRAME APPROACH", "text": "We adopted the best performing ConvNet architecture from single frame approach for each frame. In order to reduce the number of parameters in the network, all ConvNets share parameters across frames. Outputs of all ConvNets are reshaped as sequential frames and used to investigate temporal sequence in maps. We evaluated three approaches to extract temporal information from sequence of activity maps (Figure 2); 1) Max-pooling over time; 2) Temporal convolution; 3) LSTM. Finally the outputs from the last layer are fed to a fully connected layer with 512 hidden units followed by a four-way softmax layer. We kept the number of neurons in the fully connected layer relatively low to control the total number of parameters in the network. 50% dropout was used on the last two fully connected layers.\nMax-pooling: This model performs max-pooling over ConvNet outputs across time frames. While representations found from this model preserve spatial location, they are nonetheless order invariant.\nTemporal convolution: This model applies a 1-D convolution to ConvNet outputs across time frames. It consists of 32 kernels of size 3 with stride of 1 frame. Kernels capture distinct temporal patterns across multiple frames.\nLong Short-Term Memory (LSTM): Recurrent neural networks take input in the shape of a sequence x = (x1, ..., xT ) and compute hidden vector sequence h = (h1, ..., hT ) and output vector y = (y1, ..., yT ) by iterating the following equations from t = 1 to T :\nht = H(Wxhxt +Whhht\u22121 + bh) (1) yt =Whyht + by, (2)\nwhere the W terms denote weight matrices, b terms denote bias vectors, and H is the hidden layer function.\nGiven the dynamic nature of neural responses and therefore EEG, using recurrent neural networks (RNN) is a reasonable choice for modeling brain activitys temporal evolution. Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) is a RNN with improved memory. It uses memory cells with an internal memory and gated inputs/outputs which have shown to be more efficient in capturing long-term dependencies. The hidden layer function for LSTM is computed by\nthe following set of equations:\nit = \u03c3(Wxixt +Whiht\u22121 +Wcict\u22121 + bi) (3) ft = \u03c3(Wxfxt +Whfht\u22121 +Wcfct\u22121 + bf ) (4)\nct = ftct\u22121 + it tanh(Wxcxt +Whcht\u22121 + bc) (5) ot = \u03c3(Wxoxt +Whoht\u22121 +Wcoct + bo) (6)\nht = ot tanh(ct), (7)\nwhere \u03c3 is the logistic sigmoid function, and the components of the LSTM model, referred to as input gate, forget gate, output gate and cell activation vectors are denoted, respectively, as i, f , o, and c (see (Hochreiter & Schmidhuber, 1997) for details).\nWe experimented with up to two LSTM layers and various number of memory cells in each layer and obtained the best results with one layer consisting of 128 cells. Only the prediction made by LSTM after seeing the complete sequence of frames was propagated up to the fully connected layer.\nWe adopted LSTM to capture temporal evolution in sequences of ConvNet activations. Since brain activity is a temporally dynamic process, variations between frames may contain additional information about the underlying mental state."}, {"heading": "2.3 TRAINING", "text": "Training is carried out by optimizing the cross-entropy loss function. Weight sharing in ConvNets results in vastly different gradients in different layers and for this reason a smaller learning rate is usually used when applying SGD. We trained the recurrent-convolutional network with Adam algorithm (Kingma & Ba, 2015) with a learning factor of 103, and decay rate of first and second moments as 0.9 and 0.999 respectively. Batch size was set to 20. Adam has been shown to achieve competitively fast convergence rates when used for training ConvNets as well as multi-layer neural networks. In addition, VGG architecture requires fewer epochs to converge due to implicit regularization imposed by greater depth and smaller convolution filter sizes. Figure 3 shows the validation loss with number of epochs over the training set. We found that the network parameters converge after about 600 iterations (5 epochs)."}, {"heading": "3 BASELINE METHODS", "text": "We compared our approach against various classifiers commonly used in the field, including Support-Vector Machines (SVM), Random Forest, sparse Logistic Regression, and Deep Belief\nNetworks (DBN). Here we briefly describe some of the details and parameter settings used in those methods.\nSVM: SVM hyperparameters consisting of regularization penalty parameter (C) and inverse of RBF kernels standard deviation (\u03b3 = 1/\u03c3) were selected by grid-search through cross-validation on training set (C = {0.01, 0.1, 1, 10, 100}, \u03b3 = {0.1, 0.2, ..., 1, 2, ..., 10}). Random Forest: Random forest is an ensemble method consisting of a group of independent random decision trees. Each tree is grown using a randomly selected subset of features. For each input, outputs of all trees are computed, and the class with majority of votes is selected. The number of estimators for the random forest was varied within the set of {5, 10, 20, 50, 100, 500, 1000}. Logistic Regression: l1-regularization was used to introduce sparsity in the logistic regression model. Optimal regularization parameter C was selected via cross-validation on training set, in which the logarithmic range of [10\u22122, 103] was searched.\nDeep Belief Network: We used a three-layer Deep Belief Network (DBN). The first layer was a Gaussian-Binary Restricted Boltzman Machine (RBM) and the other two layers were Binary RBMs. The output of the final level was fed into a two-way softmax layer for predicting the class label. Parameters of each layer of DBN were greedily pre-trained to improve learning by shifting the initial random parameter values toward a good local minimum (Bengio et al., 2007). We used the following empirically selected numbers of neurons in the three layers that demonstrated good performance: 512, 512, and 128. The last layer was connected to a softmax layer with 4 units. The network was fine-tuned using batch stochastic gradient descent with l1-regularization to reduce the overfitting during training."}, {"heading": "4 EXPERIMENTS ON AN EEG DATASET", "text": "EEG was recorded as fifteen participants (eight female) performed a standard working memory experiment. Details of procedures for data recording and cleaning are reported in our previous publication (Bashivan et al., 2014). In brief, continuous EEG was recorded from 64 electrodes placed over the scalp at standard 10-10 locations with a sampling frequency of 500 Hz. Electrodes are placed at distances of 10% along the medial-lateral contours. Data for two of the subjects was excluded from the dataset because of excessive noise and artifacts in their recorded data. During the experiment, an array of English characters was shown for 0.5 second (SET) and participants were instructed to memorize the characters. A TEST character was shown three seconds later and participants indicated whether the test character was among the first array or not by press of a button. Each participant repeated the experiment for 240 times. The number of characters in the SET for each trial was randomly chosen to be 2, 4, 6, or 8. The number of characters in the SET determined the amount of memory load induced on the participant. Recorded brain activity during the period which individuals retained the information in their memory (3.5 seconds) was used to recognize the amount of mental workload. Figure 4 demonstrates the time course of the working memory experiment.\nContinuous EEG was sliced offline to equal lengths of 3.5 seconds corresponding to each trial. A total of 3120 trials were recorded. Only data corresponding to correctly responded trials were included in the data set which reduced the data set size to 2670 trials.\nFor evaluating the performance of each classifier we followed the leave-subject-out cross validation approach. In each of the 13 folds, all trials belonging to one of the subjects were used as the test set. A number of samples equal to the test set were then randomly extracted from rest of data for validation set and the remaining samples were used as training set."}, {"heading": "5 RESULTS", "text": "We examined the EEG dataset from two approaches. In the first approach (single-frame) we extracted the power features by applying FFT on the complete duration of each trial leading to single 3-channel image corresponding to each trial. The second approach included dividing each trial to multiple time windows and extracting power features for each window separately leading to conservation of temporal information rather than averaging them out into single slice of activity map."}, {"heading": "5.1 SINGLE-FRAME CLASSIFICATION", "text": "We first present our results on classification using a single frame derived by extracting features over the complete trial duration and applying ConvNets. The purpose of this part was to empirically seek the best performing ConvNet architecture working on images generated from complete EEG time series. We evaluated various configurations with different number of convolution and maxpool layers. We followed the VGG architecture for selection of number of filters in each layer and grouping convolution layers with small receptive fields.\nTable 1 presented earlier summarized the architectures we considered. Table 2 shows the number of parameters used by each type of architecture, and the corresponding error achieved on the test set. We can see that increasing the number of layers to seven slightly improved the achievable error rates on the test set. The best result was obtained with architecture D containing 7 convolution layers. Most of the network parameters lie in the last two layers (fully connected and softmax) containing approximately 1 million parameters. In VGG style network, the number of filters in each layer is selected in a way that size of the output remains the same after each stack (filter size \u00d7 number of kernels).\nTable 2: The number of parameters in convolutional layers for evaluated single-frame architectures, and the test errors achieved by each architecture.\nArchitecture Number of parameters Test Error (%)\nA 10k 13.05 B 65.5k 13.17 C 139.4k 13.91 D 158k 12.39"}, {"heading": "5.2 MULTI-FRAME CLASSIFICATION", "text": "For the multi-frame classification, we used ConvNet with architecture D from previous step and applied it on each frame. We explored the four different approaches to aggregate temporal features from multiple frames (Figure 2). Using temporal convolution and LSTM significantly improved the classification accuracy (see Table 3). A closer look at the accuracies derived for each individual, reveals that while both methods are achieving close to perfect classification accuracies for eight of participants, most of the differences originated from differences in accuracy for the remaining five\nindividuals (Table 4). This observation motivated us to use a combination of temporal convolution and LSTM structures together in single structure which led to our best results on the dataset.\nWhile our approach does not directly operate on raw EEG time-series, we drastically reduced the amount of required data by manually extracting power features from EEG. In addition, discovering complex temporal relationships such as those related to spectral properties in time-series using neural networks, is still an open question which has not been fully addressed.\nSince our approach transforms the EEG data into sequence of EEG images, it can be applied on EEG data acquired with different hardware (e.g. with different number of electrodes). The preprocessing step used in our approach transforms the EEG time-series acquired from various sources into comparable EEG frames. In this way, various EEG datasets could be merged together. The only information needed to complete this transform would be the spatial coordinates of electrodes for each setup."}, {"heading": "6 CONCLUSIONS", "text": "This work is motivated by the high-level goal of finding robust representations from EEG data, that would be invariant to inter- and intra-subject differences and to inherent noise associated with EEG data collection. We propose a novel methodology for learning representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. Our approach is fundamentally different from the previous attempts to learn high-level representations from EEG using deep neural networks. Specifically, rather than representing low-level EEG features as a vector, we transform the data into a sequence of topology-preserving multi-spectral images (EEG \u201dmovie\u201d), as opposed to standard EEG analysis techniques that ignore such spatial information. We then train deep recurrent-convolutional networks inspired by state-of-the-art video classification to learn robust representations from the sequence of images. The proposed approach demonstrates significant improvements in classification accuracy over the state-of-the-art results."}], "references": [{"title": "A trivariate cloughtocher scheme for tetrahedral data", "author": ["Alfeld", "Peter"], "venue": "Computer Aided Geometric Design,", "citeRegEx": "Alfeld and Peter.,? \\Q1984\\E", "shortCiteRegEx": "Alfeld and Peter.", "year": 1984}, {"title": "Spectrotemporal dynamics of the EEG during working memory encoding and maintenance predicts individual behavioral capacity", "author": ["Bashivan", "Pouya", "Bidelman", "Gavin M", "Yeasin", "Mohammed"], "venue": "European Journal of Neuroscience,", "citeRegEx": "Bashivan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bashivan et al\\.", "year": 2014}, {"title": "Greedy layer-wise training of deep networks", "author": ["Bengio", "Yoshua", "Lamblin", "Pascal", "Popovici", "Dan", "Larochelle", "Hugo"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Convolutional neural networks for P300 detection with application to brain-computer interfaces", "author": ["Cecotti", "Hubert", "Gr\u00e4ser", "Axel"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Cecotti et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cecotti et al\\.", "year": 2011}, {"title": "Unconstrained online handwriting recognition with recurrent neural networks", "author": ["Graves", "Alex", "S Fern\u00e1ndez", "Liwicki", "Marcus"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Graves et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2008}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Graves", "Alex", "Mohamed", "Abdel-Rahman", "Hinton", "Geoffrey E"], "venue": "Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Recurrent neural networks employing Lyapunov exponents for EEG signals classification", "author": ["N Guler", "E Ubeyli", "I. Guler"], "venue": "Expert Systems with Applications,", "citeRegEx": "Guler et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Guler et al\\.", "year": 2005}, {"title": "Teaching Machines to Read and Comprehend", "author": ["Hermann", "Karm Moritz", "Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "arXiv, pp", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Frontal theta activity in humans increases with memory load in a working memory", "author": ["Jensen", "Ole", "Tesche", "Claudia D"], "venue": "task. Neuroscience,", "citeRegEx": "Jensen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 2002}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["Karpathy", "Andrej", "G. Toderici"], "venue": "Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Karpathy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2014}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy Lei"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks. Advances in neural information processing", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "systems, pp. 1097\u20131105,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun", "Yann", "Bottou", "L\u00e9on", "Bengio", "Yoshua", "Haffner", "Patrick"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "A Review of Classification Algorithms for EEG-based Brain-Computer Interfaces", "author": ["F Lotte", "M. Congedo"], "venue": "Journal of neural engineering,", "citeRegEx": "Lotte and Congedo,? \\Q2007\\E", "shortCiteRegEx": "Lotte and Congedo", "year": 2007}, {"title": "Classification of patterns of EEG synchronization for seizure prediction", "author": ["Mirowski", "Piotr", "Madhavan", "Deepak", "LeCun", "Yann", "Kuzniecky", "Ruben"], "venue": "Clinical Neurophysiology,", "citeRegEx": "Mirowski et al\\.,? \\Q1927\\E", "shortCiteRegEx": "Mirowski et al\\.", "year": 1927}, {"title": "Beyond Short Snippets: Deep Networks for Video Classification", "author": ["Ng", "Jyh", "M. Hausknecht"], "venue": "In CVPR,", "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "Deep learning for neuroimaging: a validation study", "author": ["Plis", "Sergey M", "Hjelm", "Devon R", "Salakhutdinov", "Ruslan", "Allen", "Elena a", "Bockholt", "Henry J", "Long", "Jeffrey D", "Johnson", "Hans J", "Paulsen", "Jane S", "Turner", "Jessica a", "Calhoun", "Vince D"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "Plis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Plis et al\\.", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K Simonyan", "A. Zisserman"], "venue": "In ICLR, pp", "citeRegEx": "Simonyan and Zisserman,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman", "year": 2015}, {"title": "Map projections\u2013A working manual, volume 1395", "author": ["Snyder", "John Parr"], "venue": "US Government Printing Office,", "citeRegEx": "Snyder and Parr.,? \\Q1987\\E", "shortCiteRegEx": "Snyder and Parr.", "year": 1987}, {"title": "EEG signal classification using PCA, ICA, LDA and support vector machines", "author": ["Subasi", "Abdulhamit", "M. Ismail Gursoy"], "venue": "Expert Systems with Applications,", "citeRegEx": "Subasi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Subasi et al\\.", "year": 2010}, {"title": "Text Understanding from Scratch", "author": ["Zhang", "Xiang", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1502.01710,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "Deep neural networks have recently achieved great success in recognition tasks within a wide range of applications including images, videos, speech, and text (Krizhevsky et al., 2012; Graves et al., 2013; Karpathy & Toderici, 2014; Zhang & LeCun, 2015; Hermann et al., 2015).", "startOffset": 158, "endOffset": 274}, {"referenceID": 5, "context": "Deep neural networks have recently achieved great success in recognition tasks within a wide range of applications including images, videos, speech, and text (Krizhevsky et al., 2012; Graves et al., 2013; Karpathy & Toderici, 2014; Zhang & LeCun, 2015; Hermann et al., 2015).", "startOffset": 158, "endOffset": 274}, {"referenceID": 7, "context": "Deep neural networks have recently achieved great success in recognition tasks within a wide range of applications including images, videos, speech, and text (Krizhevsky et al., 2012; Graves et al., 2013; Karpathy & Toderici, 2014; Zhang & LeCun, 2015; Hermann et al., 2015).", "startOffset": 158, "endOffset": 274}, {"referenceID": 13, "context": "Convolutional neural networks (ConvNets) lie at the core of best current architectures working with images and video data, primarily due to their ability to extract representations that are robust to partial translation and deformation of input patterns (LeCun et al., 1998).", "startOffset": 254, "endOffset": 274}, {"referenceID": 5, "context": "On the other hand, recurrent neural networks have delivered state-of-the-art performance in many applications involving dynamics in temporal sequences, such as, for example, handwriting and speech recognition (Graves et al., 2013; 2008).", "startOffset": 209, "endOffset": 236}, {"referenceID": 6, "context": "In other works, convolutional and recurrent neural networks have been used to extract representations from EEG time series (Cecotti & Gr\u00e4ser, 2011; Guler et al., 2005).", "startOffset": 123, "endOffset": 167}, {"referenceID": 14, "context": ", 2014; Mirowski et al., 2009). Plis et al. (2014) showed that adding several Restricted Boltzman Machine layers to a deep belief network and using supervised pretraining results into networks that can learn increasingly complex representation of the data and achieve considerable increase in classification accuracy.", "startOffset": 8, "endOffset": 51}, {"referenceID": 1, "context": "Oscillatory cortical activity related to memory operations primarily exists in three frequency bands of theta (4-7Hz), alpha (813Hz), and beta (13-30Hz) (Bashivan et al., 2014; Jensen & Tesche, 2002).", "startOffset": 153, "endOffset": 199}, {"referenceID": 2, "context": "Parameters of each layer of DBN were greedily pre-trained to improve learning by shifting the initial random parameter values toward a good local minimum (Bengio et al., 2007).", "startOffset": 154, "endOffset": 175}, {"referenceID": 1, "context": "Details of procedures for data recording and cleaning are reported in our previous publication (Bashivan et al., 2014).", "startOffset": 95, "endOffset": 118}], "year": 2017, "abstractText": "One of the challenges in modeling cognitive events from electroencephalogram (EEG) data is finding representations that are invariant to interand intra-subject differences, as well as to inherent noise associated with such data. Herein, we propose a novel approach for learning such representations from multi-channel EEG time-series, and demonstrate its advantages in the context of mental load classification task. First, we transform EEG activities into a sequence of topologypreserving multi-spectral images, as opposed to standard EEG analysis techniques that ignore such spatial information. Next, we train a deep recurrent-convolutional network inspired by state-of-the-art video classification to learn robust representations from the sequence of images. The proposed approach is designed to preserve the spatial, spectral, and temporal structure of EEG which leads to finding features that are less sensitive to variations and distortions within each dimension. Empirical evaluation on the cognitive load classification task demonstrated significant improvements in classification accuracy over current state-of-the-art approaches in this field.", "creator": "LaTeX with hyperref package"}}}