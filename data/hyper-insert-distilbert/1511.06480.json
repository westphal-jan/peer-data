{"id": "1511.06480", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "On Binary Embedding using Circulant Matrices", "abstract": "traditionally binary bin embeddings largely provide efficient and frequently powerful ways to perform filtering operations locally on large scale balanced data. \u2033 however binary garbage embedding typically safely requires long term codes separated in variable order q to nevertheless preserve nearly the temporal discriminative power of the input byte space. thus binary coding methods traditionally merely suffer somewhat from high computation and storage labor costs combined in such such diverse a scenario. just to address this this prevalent problem, we propose circulant sample binary query embedding ( rm cbe ) which generates binary codes by projecting matching the fragmented data elements with a circulant parameter matrix. the classical circulant column structure strategy allows us governments to economically use standard fast sampled fourier transformation transform digital algorithms to speed up the computation. however for simple obtaining $ k $ - bit binary codes from $ d $ - dimensional symmetric data, this improves also the time complexity from $ o ( dk ) $ mb to $ $ o ( class d \\ log { d } ) $, and the shortest space complexity from $ o ( dk ) $ rm to $ o ( d ) $.", "histories": [["v1", "Fri, 20 Nov 2015 03:05:15 GMT  (120kb,D)", "https://arxiv.org/abs/1511.06480v1", "This is an extended version of a paper by the first, third, fourth and fifth authors that appeared in ICML 2014 [arXiv:1405.3162]"], ["v2", "Sat, 5 Dec 2015 02:36:10 GMT  (419kb,D)", "http://arxiv.org/abs/1511.06480v2", "This is an extended version of a paper by the first, third, fourth and fifth authors that appeared in ICML 2014 [arXiv:1405.3162]"]], "COMMENTS": "This is an extended version of a paper by the first, third, fourth and fifth authors that appeared in ICML 2014 [arXiv:1405.3162]", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["felix x yu", "aditya bhaskara", "sanjiv kumar", "yunchao gong", "shih-fu chang"], "accepted": false, "id": "1511.06480"}, "pdf": {"name": "1511.06480.pdf", "metadata": {"source": "CRF", "title": "On Binary Embedding using Circulant Matrices On Binary Embedding using Circulant Matrices", "authors": ["Felix X. Yu", "Aditya Bhaskara", "Sanjiv Kumar", "Yunchao Gong", "Shih-Fu Chang"], "emails": ["felixyu@google.com", "bhaskaraaditya@gmail.com", "sanjivk@google.com", "yunchao@cs.unc.edu", "sfchang@ee.columbia.edu"], "sections": [{"heading": null, "text": "We study two settings, which differ in the way we choose the parameters of the circulant matrix. In the first, the parameters are chosen randomly and in the second, the parameters are learned using the data. For randomized CBE, we give a theoretical analysis comparing it with binary embedding using an unstructured random projection matrix. The challenge here is to show that the dependencies in the entries of the circulant matrix do not lead to a loss in performance. In the second setting, we design a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. In both the settings, we show by extensive experiments that the CBE approach gives much better performance than the state-ofthe-art approaches if we fix a running time, and provides much faster computation with negligible performance degradation if we fix the number of bits in the embedding.\nKeywords: Circulant Matrix, Dimension Reduction, Binary Embedding\nNote. A preliminary version of this article with the first, third, fourth and fifth authors appeared in the Proceedings of ICML 2014."}, {"heading": "1. Introduction", "text": "Sketching and dimensionality reduction have become powerful and ubiquitous tools in the analysis of large high-dimensional datasets, with applications ranging from computer vision, to biology, to finance. The celebrated Johnson-Lindenstrauss lemma says that projecting high dimensional points to a random O(logN)-dimensional space approximately preserves all the pairwise distances between a set of N points, making it a powerful tool for\nc\u00a92015 F. Yu, A. Bhaskara, S. Kumar, Y. Gong, S.-F. Chang.\nar X\niv :1\n51 1.\n06 48\n0v 2\n[ cs\n.D S]\n5 D\nnearest neighbor search, clustering, etc. This started the paradigm of designing low dimensional sketches (or embeddings) of high dimensional data that can be used for efficiently solving various information retrieval problems.\nMore recently, binary embeddings (or embeddings into {0, 1}k or {\u22121, 1}k) have been developed for problems in which we care about preserving the angles between high dimensional vectors [LSMK11, GKRL13, RL09, GKVL12, LWKC11]. The main appeal of binary embeddings stems from the fact that storing them is often much more efficient than storing real valued embeddings. Furthermore, operations such as computing the Hamming distance in binary space can be performed very efficiently either using table lookup, or hardwareimplemented instructions on modern computer architectures.\nIn this paper, we study binary embeddings of high-dimensional data. Our goal is to address one of its main challenges: even though binary embeddings are easy to manipulate, it has been observed that obtaining high accuracy results requires the embeddings to be rather long when the data is high dimensional [LSMK11, GKRL13, SP11]. Thus in applications like computer vision, biology and finance (where high dimensional data is common), the task of computing the embedding is a bottleneck. The natural algorithms have time and space complexity O(dk) per input point in order to produce a k-bit embedding from a ddimensional input. Our main contribution in this work is to improve these complexities to O(d log d) for time and O(d) for space complexity.\nOur results can be viewed as binary analogs of the recent work on fast JohnsonLindenstrauss transform. Starting with the work of Ailon and Chazelle [AC06], there has been a lot of beautiful work on fast algorithms for dimension reduction with the goal of preserving pairwise distances between points. Various aspects, such as exploiting sparsity, and using structured matrices to reduce the space and time complexity of dimension reduction, have been explored [AC06, Mat08, LAS08]. But the key difference in our setting is that binary embeddings are non-linear. This makes the analysis tricky when the projection matrices do not have independent entries. Binary embeddings are also better suited to approximate the angles between vectors (as opposed to distances). Let us see why.\nThe general way to compute a binary embedding of a data point x \u2208 Rd is to first apply a linear transformation Ax (for a k \u00d7 d matrix A), and then apply a binarization step. We consider the natural binarization of taking the sign. Thus, for a point x, the binary embedding into {\u22121, 1}d we consider is\nh(x) = sign(Ax), (1)\nwhere A \u2208 Rk\u00d7d as above, and sign(\u00b7) is a binary map which returns element-wise sign1. How should one pick the matrix A? One natural choice, in light of the Johnson-Lindenstrauss lemma, is to pick it randomly, i.e., each entry is sampled from an independent Gaussian. This data oblivious choice is well studied [Cha02, RL09], and has the nice property that for two data vectors x,y, the `1 distance between their embeddings is proportional to the angle between x and y, in expectation (over the random entries in A). This is a consequence of the fact that for any x,y \u2208 Rd, if r is drawn from N (0, 1)d,\nPr[sign \u3008x, r\u3009 = sign \u3008y, r\u3009] = \u2220(x,y) \u03c0 . (2)\n1. A few methods transform the linear projection via a nonlinear map before taking the sign [WTF08, RL09].\nOther data oblivious methods have also been studied in the literature, by choosing different distributions for the entries of A. While these methods do reasonably well in practice, the natural question is if adapting the matrix to the data allows us to use shorter codes (i.e., have a smaller k) while achieving a similar error. A number of such data-dependent techniques have been proposed with different optimization criteria such as reconstruction error [KD09], data dissimilarity [NF12, WTF08], ranking loss [NFS12], quantization error after PCA [GLGP12], and pairwise misclassification [WKC10]. As long as data is relatively low dimensional, these methods have been shown to be quite effective for learning compact codes.\nHowever, theO(kd) barrier on the space and time complexity barrier prevents them from being applied with very high-dimensional data. For instance, to generate 10K-bit binary codes for data with 1M dimensions, a huge projection matrix will be required needing tens of GB of memory.2\nIn order to overcome these computational challenges, [GKRL13] proposed a bilinear projection based coding method. The main idea here is to reshape the input vector x into a matrix Z, and apply a bilinear projection to get the binary code:\nh(x) = sign(RT1 ZR2). (3)\nWhen the shapes of Z,R1,R2 are chosen appropriately 3, the method has time and space\ncomplexities O(d \u221a k) and O( \u221a dk) respectively. Bilinear codes make it feasible to work with datasets of very high dimensionality and have shown good results for a variety of tasks."}, {"heading": "1.1 Our results", "text": "In this work, we propose a novel technique, called Circulant Binary Embedding (CBE), which is even faster than the bilinear coding. The main idea is to impose a circulant (described in detail in Section 3) structure on the projection matrix A in (1). This special structure allows us to compute the product Ax in time O(d log d) using the Fast Fourier Transform (FFT), a tool of great significance in signal processing. The space complexity is also just O(d), making it efficient even for very high dimensional data. Table 1 compares the time and space complexity for the various methods outlined above.\nGiven the efficiency of computing the CBE, two natural questions arise: how good is the obtained embedding for various information retrieval tasks? and how should we pick the parameters of the circulant A?\nIn Section 4, we study the first question for random CBE, i.e., when the parameters of the circulant are picked randomly (independent Gaussian, followed by its shifts). Specifically, we analyze the angle estimating property of binary embeddings (Eq.(1)), which is the basis for its use in applications. Under mild assumptions, we show that using a random circulant A has the same qualitative guarantees as using fully random A. These results provide some of the few theoretical guarantees we are aware of, for non-linear circulant-based embeddings. We defer the formal statements of our results to Section 4, Theorems 3 and 4. We note that\n2. In the oblivious case, one can generate the random entries of the matrix on-the-fly (with fixed seeds) without needing to store the matrix, but this increases the computational time even further. 3. Specifically, Z \u2208 R \u221a d\u00d7 \u221a d, R1,R2 \u2208 R \u221a k\u00d7 \u221a d.\nin independent and very recent work, Choromanska et al. [CKB+15] obtain a qualitatively similar analysis of CBE, however the bounds are incomparable to ours.\nIn Section 5, we study the second question, i.e., learning data-dependent circulant matrices. We propose a novel and efficient algorithm, which alternatively optimizes the objective in the original and frequency domains.\nFinally in Section 7, we study the empirical performance of circulant embeddings via extensive experimentation. Compared to the state-of-the-art, our methods improve the performance dramatically for a fixed computation time. If we instead fix the number of bits in the embedding, we observe that the performance degradation is negligible, while speeding up the computation many-fold (see Section 7)."}, {"heading": "2. Background and related work", "text": "The lemma of Johnson and Lindenstrauss [JL84] is a fundamental tool in the area of sketching and dimension reduction. The lemma states that if we have N points in ddimensional space, projecting them to an O(logN) dimensional space (independent of d!) preserves all pairwise distances. Formally,\nLemma 1 (Johnson Lindenstrass lemma). Let S be a set of N points in Rd. Let A \u2208 Rk\u00d7d be a matrix whose entries are drawn i.i.d from N (0, 1). Then with probability at least 1\u2212 2N2e\u2212( 2\u2212 3)k/4\n(1\u2212 )\u2016x\u2212 y\u20162 \u2264 1\u221a k \u2016A(x\u2212 y)\u20162 \u2264 (1 + )\u2016x\u2212 y\u20162 (4)\nfor any x,y \u2208 S.\nWhen k = O(logN/ 2), the probability above can be made arbitrarily close to 1. Due to the simplicity and theoretical support, random projection based dimensionality reduction has been applied in broad applications including approximate nearest neighbor research [IM98], dimensionality reduction in databases [Ach03], and bi-Lipschitz embeddings of graphs into normed spaces [FM88].\nHowever a serious concern in a few applications is the dependence of k on the accuracy (O(1/ 2)). The space and time complexity of dimension reduction are O(kd), if the computation is done in the natural way. Are there faster methods when k is reasonably large?\nAs mentioned earlier, the line of work starting with [AC06] aims to improve the time and space complexity of dimension reduction. This led to work showing Johnson-Lindenstrusstype guarantees with structured matrices (with some randomness), including Hadamard matrices along with a sparse random Gaussian matrix [AC06], sparse matrices [Mat08], and Lean Walsh Transformations [LAS08]. The advantage of using structured matrices is that the space and computation cost can be dramatically reduced, yet the distance preserving property remains to be competitive.\nIn this context, randomized circulant matrices (which are also the main tool in our work) have been studied, starting with the works [HV11, Vyb11]. The dimension reduction comprises of random sign flips followed by multiplication by a randomized circulant matrix. For d-dimensional input, reducing the dimension to k for k < d has time complexity O(d log d) and space complexity O(d), independent of k. Proving bounds similar to Lemma 1 turns out to be much more challenging because the entries of the projection matrix are now highly dependent, and thus concentration bounds are hard to prove. The first analysis [HV11] showed that reducing to O(log3N/ 2) dimensions (compared to O(logN/ 2) in Lemma 1) preserves all pairwise distances with high probability. This was improved to O(log2N/ 2) in [Vyb11], and furthermore to O(log(1+\u03b4)N/ 2) in [ZC13], using matrix-valued Bernstein inequalities. These works provide the motivation for our theoretical results, however the key difference for us is the binarization step, which is highly non-linear. Thus we need to develop new machinery for our analysis.\nBinary embeddings. Recently, structured matrices used in the context of the fast JL transform (a combination of Hadamard and sparse random Gaussian matrices) have also been studied for binary embedding [DKS11], and more recently [YCP15]. In particular, [YCP15] showed that the method can achieve distance preserving error with O(logN/ 2) bits and O(d log d) computational complexity, for N points (N \u221a d). In this work, we study the application of using the circulant matrix for binary embedding. The work extends and provides theoretical justification for our previous conference paper on this topic [YKGC14].\nThe idea of using structured matrices to speed up linear projection has also be exploited under the settings of deep neural networks [CYF+15b, YMD+14], and kernel approximation [YKRC15, LSS13] ."}, {"heading": "3. Circulant Binary Embedding", "text": "Let us start by describing our framework and setting up the notation that we use in the rest of the paper."}, {"heading": "3.1 The Framework", "text": "We will now describe our algorithm for generating k-bit binary codes from d-dimensional real vectors. We start by discussing the case k = d and move to the general case in Section 3.3. The key player is the circulant matrix, which is defined by a real vector\nr = (r0, r1, \u00b7 \u00b7 \u00b7 , rd\u22121)T [Gra06].\nCr :=  r0 rd\u22121 . . . r2 r1 r1 r0 rd\u22121 r2 ... r1 r0 . . . ...\nrd\u22122 . . . . . . rd\u22121 rd\u22121 rd\u22122 . . . r1 r0  . (5) Let D be a diagonal matrix with each diagonal entry \u03c3i, i = 0, \u00b7 \u00b7 \u00b7 , d \u2212 1, being a\nRademacher variable (\u00b11 with probability 1/2):\nD =  \u03c30 \u03c31 \u03c32 . . .\n\u03c3d\u22121  . (6) For x \u2208 Rd, its d-bit Circulant Binary Embedding (CBE) with r \u2208 Rd is defined as:\nh(x) = sign(CrDx), (7)\nwhere Cr is defined as above. Note that applying D to x is equivalent to applying a random sign flip to each coordinate of x. The necessity of such an operation is discussed in the introduction of Section 4. Since sign flipping can be carried out as a preprocessing step for each input x, here onwards for simplicity we will drop explicit mention of D. Hence the binary code is given as h(x) = sign(Crx)."}, {"heading": "3.2 Computational Complexity", "text": "The main advantage of a circulant based embedding is that it can be computed quickly using the Fast Fourier Transform (FFT). The following is a folklore result, whose proof we include for completeness.\nProposition 2. For a d-dimensional vector x and any r \u2208 <d, the d-bit CBE sign(Cr(Dx)) can be computed using O(d) space and O(d log d) time. Proof. The space complexity comes only from the storage of the vector r and the signs D (which amount to O(d)). We never need to store the full matrix Cr explicitly.\nThe main property of a circulant matrix is that for any vector y \u2208 Rd, we can compute Cry in time O(d log d). This is because\nCr = F\u22121d diag(Fdr) Fd, (8)\nwhere Fd is the matrix corresponding to the Discrete Fourier Transform (DFT) of periodicity N , i.e., whose (i, j)th entry is given by\nFd(i, j) = \u03c9ij , (9)\nwhere \u03c9 is the Nth root of unity e\u22122\u03c0\u03b9/N . The celebrated Fast Fourier Transform algorithm [OSB+99] says that for any z \u2208 Rd, we can compute Fdz and F\u22121d z in time O(d log d), using O(d) space. This immediately implies that we can compute Cry within the same space and time complexity bounds."}, {"heading": "3.3 Generalizing to k 6= d", "text": "The computation above assumed that number of bits we produce (k) is equal to the input dimension. Let us now consider the general case.\nWhen k < d, we still use the circulant matrix R \u2208 Rd\u00d7d with d parameters, but the output is set to be the first k elements in (7). This is equivalent to the operation\n\u03a6(x) := sign(Cr,kDx), (10)\nwhere Cr,k the so-called partial circulant matrix, which is Cr truncated to k columns. We note that CBE with k < d is not computationally more efficient than that with k = d.\nWhen k > d, using a single r causes repetition of bits, so we propose using Cr for multiple r, and concatenating their output. This gives the computational complexity O(k log d), and space complexity O(k). Note that as the focus of this paper is on binary embedding on high-dimensional data, from here onwards, we assume k \u2264 d. The k > d case is useful in other applications such as neural network [CYF+15a] and kernel approximation [YKRC15]."}, {"heading": "3.4 Choosing the Parameters r", "text": "We have presented the general framework as well as its space and computation efficiency in this section. One critical question left unanswered is how to decide the parameter r. As mentioned in the introduction, we consider two solutions. In Section 4, we study the randomized version, where each element of r is independently sampled from a unit Gaussian distribution. This is inspired by the popular Locality Sensitive Hashing (simhash) approach. Section 5 introduces an optimized version, where the parameters are optimized based on training data and an distance preserving objective function."}, {"heading": "4. Randomized CBE \u2013 A Theoretical Analysis", "text": "We now analyze the angle preserving properties of CBE when the circulant matrix used is generated from a random d-dimensional vector. Formally, we consider the partial circulant matrix Cr,k, for r \u223c N (0, 1)d. The embedding we consider for an x \u2208 Rd is given by\n\u03a6(x) := sign(Cr,kDx). (11)\nAs before, D is a diagonal matrix of signs. Hence the embedding uses 2d independent \u2018units\u2019 of randomness.\nNow, for any two vectors x,y \u2208 Rd, we have that\nE [ 1\n2k \u2016\u03a6(x)\u2212 \u03a6(y)\u20161\n] =\n\u2220(x,y) \u03c0 , (12)\nimplying that the random variable (1/2k)\u2016\u03a6(x) \u2212 \u03a6(y)\u20161 provides an estimate for \u03b8/\u03c0, where \u03b8 := \u2220(x,y).\nWe present two main results. In the first, we bound the variance of the above angle estimate for given x,y. We compare with the variance in the fully independent case, i.e., when we consider the embedding sign(Ax), where A is a k\u00d7d matrix with all entries being\nindependent (and unit normal). In this case, the variance of the estimator in Eq. (12) is equal to 1k \u03b8 \u03c0 ( 1\u2212 \u03b8\u03c0 ) .4\nWe show that using a circulant matrix instead of A above has a similar dependence on k, as long as the vectors are well spread. Formally,\nTheorem 3. Let x,y \u2208 Rd, such that max{\u2016x\u2016\u221e/\u2016x\u20162, \u2016y\u2016\u221e/\u2016y\u20162} \u2264 \u03c1, for some parameter \u03c1 < 1, and set \u03b8 = \u2220(x,y). The variance of the averaged hamming distance of k-bit code generated by randomized CBE is\nvar\n[ 1\n2k \u2016\u03a6x \u2212 \u03a6y\u20161 ] \u2264 1 k \u03b8 \u03c0 ( 1\u2212 \u03b8 \u03c0 ) + 32\u03c1. (13)\nThe variance above is over the choice of r and the random signs D. Remark. For typical vectors in Rd, we have \u2016x\u2016\u221e/\u2016x\u20162 to be O(log d/ \u221a d). Further, by using the idea from Ailon and Chazelle [AC06], we can pre-process the data by multiplying it with a randomly signed Hadamard matrix, and guarantee such an `\u221e bound with high probability.5 Therefore the second term becomes negligible for large d. The above result suggests that the angle preservation performance of CBE (in term of the variance) is as good as LSH for high-dimensional data.\nOur second theorem gives a large-deviation bound for the angle estimate, also assuming that the vectors are well-spread. This will then enable us to obtain a dimension reduction theorem which preserves all angles up to an additive error.\nTheorem 4. Let x,y \u2208 Rd with \u2220(x,y) = \u03b8, and suppose max{\u2016x\u2016\u221e/\u2016x\u20162, \u2016y\u2016\u221e/\u2016x\u20162} \u2264 \u03c1, for some parameter \u03c1. Now consider the k-dimensional CBE \u03a6x,\u03a6y of x,y respectively, for some k < d. Suppose \u03c1 \u2264 \u03b8216k log(k/\u03b4) . For any > 0, we have:\nPr [\u2223\u2223\u2223\u2223 12k\u2016\u03a6x \u2212 \u03a6y\u20161 \u2212 \u03b8\u03c0 \u2223\u2223\u2223\u2223 > 4 log(k/\u03b4)\u221ak ] < \u03b4. (14)\nQualitatively, the condition on \u03c1 is similar to the one we implicitly have in Theorem 3. Unless \u03c1 = o ( 1 k \u03b8 \u03c0 (1\u2212 \u03b8 \u03c0 ) ) , the additive term dominates, so for the bound to be interesting, we need this condition on \u03c1. We observe that Theorem 4 implies a Johnson-Lindenstrauss type theorem.\nCorollary 5. Suppose we have N vectors u0,u1, . . . ,uN\u22121 in R d, and define\n\u03c1ij = max{\u2016ui\u2016\u221e/\u2016ui\u20162, \u2016uj\u2016\u221e/\u2016uj\u20162}, \u03b8ij = \u2220(ui,uj). (15)\nLet > 0 be a given accuracy parameter and let k = C log2 n/ 2. Then for all i, j such that \u03c1ij < \u03b82ij\n16k log(2kN2) , we have \u2223\u2223\u2223\u2223 12k\u2016\u03a6i \u2212 \u03a6j\u20161 \u2212 \u03b8ij\u03c0 \u2223\u2223\u2223\u2223 < , (16) with probability at least 3/4.\n4. We are computing the variance of an average of i.i.d. Bernoulli random variables which take value 1 with probability p = \u03b8/\u03c0. 5. However, applying this pre-processing leads to dense vectors, which may be memory intensive for some applications. In this case, dividing the co-ordinates into blocks of size \u223c k2 and performing the preprocessing on the blocks separately is better for small k.\nProof. We can set \u03b4 = 1/2N2 in Theorem 4 and then take a union bound over all ( N 2 ) choices of pairs i, j to obtain a failure probability \u2264 1/4. Further, for our choice of k, setting C = 144 and assuming N is large enough that k < N , we have\n4 log(k/ )\u221a k < 12\u03b4 logN\u221a C \u00b7 logN < . (17)\nIn the remainder of the section, we will prove the above theorems. We start with Theorem 3, whose proof will give a basic framework for that of Theorem 4."}, {"heading": "4.1 Variance of the angle estimator", "text": "For a vector x and an index i, we denote by s\u2192i(x) the vector shifted by i positions. I.e., the jth entry of s\u2192i is the ((j \u2212 i)mod d)\u2019th entry of x. Further, let us define\nFi = 1\u2212 sign(s\u2192i(r)TDx) sign(s\u2192i(r)TDy) 2 \u2212 \u03b8 \u03c0 . (18)\nwhere s\u2192i(\u00b7) is defined as the operator circularly shifting a vector by i elements6. We have\nvar\n[ 1\n2k \u2016\u03a6x \u2212 \u03a6y\u20161\n] = var [ 1\nk k\u2211 i=1 Fi\n] . (19)\nWithout loss of generality, we assume \u2016x\u20162, \u2016y\u20162 = 1 (since we only care about the angle). The mean of each Fi is zero, and thus E[ 1k \u2211k i=1 Fi] = 0. Thus the variance is equal to\nvar\n[ 1\nk k\u22121\u2211 i=0 Fi\n] = E  1 k2 ( k\u22121\u2211 i=0 Fi )2 (20) = E [\u2211k\u22121 i=0 F 2 i + \u2211 i 6=j FiFj\nk2\n]\n= 1\nk2 k \u00b7 EF 21 +\u2211 i 6=j E(FiFj)  = 1\nk\n\u03b8\n\u03c0\n( 1\u2212 \u03b8\n\u03c0\n) + 1\nk2 \u2211 i 6=j E(FiFj) (21)\nTo prove the theorem, it suffices to show that E(FiFj) \u2264 32\u03c1 for all i 6= j. Without loss of generality, we can assume that i = 0, and consider E(F0Fj). By definition, it is equal to\nE [(\n1\u2212 sign(rTDx) sign(rTDy) 2 \u2212 \u03b8 \u03c0\n)( 1\u2212 sign(s\u2192j(r)TDx) sign(s\u2192j(r)TDy)\n2 \u2212 \u03b8 \u03c0\n)] .\n6. The above comes with a slight abuse of notation, where the first column (instead of row) of the projection matrix R is defined as r.\nThe trick now is to observe that\ns\u2192j(r) Tx = rT s\u2192(d\u2212j)(x). (22)\nThus setting t = d\u2212 j, we can write the above as\nE [(\n1\u2212 sign(rTDx) sign(rTDy) 2 \u2212 \u03b8 \u03c0\n)( 1\u2212 sign(rT s\u2192t(Dx)) sign(rT s\u2192t(Dy))\n2 \u2212 \u03b8 \u03c0 )] The key idea is that we expect the vector s\u2192t(Dx) to be nearly orthogonal to the space containing Dx,Dy. This is because D is a diagonal matrix of random signs, and x and y are vectors with small `\u221e norm. We show this formally in Lemma 7.\nWhy does this help? Suppose for a moment that u := s\u2192t(Dx) and v := s\u2192t(Dy) are both orthogonal to span{Dx,Dy}. Then for a random Gaussian r, the random variables sign(rTu) sign(rTv) and sign(rTDx) sign(rTDy) are independent, because the former depends only on the projection of r onto span{u,v}, while the latter depends only on the projection of r onto span{Dx,Dy}. Now if these two spaces are orthogonal, the projections of a Gaussian vector onto these spaces are independent (this is a fundamental property of multidimensional Gaussians). This implies that the expectation of the product above is equal to the product of the expectations, which is zero (each expectation is zero).\nThe key lemmma (see below) now says that even if u and v as defined above are nearly orthogonal to span{Dx,Dy}, we still get a good bound on the expectation above.\nLemma 6. Let a,b,u,v be unit vectors in Rd such that \u2220(a,b) = \u2220(u,v) = \u03b8, and let \u03a0 be the projector onto span{a,b}. Suppose max{\u2016\u03a0u\u2016, \u2016\u03a0v\u2016} = \u03b4 < 1. Then we have\nE [(\n1\u2212 sign(rTa) sign(rTb) 2 \u2212 \u03b8 \u03c0\n)( 1\u2212 sign(rTu) sign(rTv)\n2 \u2212 \u03b8 \u03c0\n)] \u2264 2\u03b4. (23)\nHere, the expectation is over the choice of r.\nThe proof of the above lemma is moved to Appendix A.1.\nWe use the lemma with a = Dx and b = Dy. To show Theorem 3, we have to prove that\nE [max{\u2016\u03a0u\u2016, \u2016\u03a0v\u2016}] \u2264 16\u03c1, (24)\nwhere \u03a0,u,v are defined as in the statement of Lemma 6. The expectation now is over the choice of D. This leads us to our next lemma.\nLemma 7. Let p,q \u2208 Rd be vectors that satisfy \u2016p\u20162 = 1 and \u2016q\u2016\u221e < \u03c1 for some parameter \u03c1, and suppose D := diag(\u03c30, \u03c31, . . . , \u03c3d\u22121), where \u03c3i are random \u00b11 signs. Then for any 0 < t < d, we have\nPr[\u3008Dp, s\u2192t(Dq)\u3009 > \u03b3] \u2264 e\u2212\u03b3 2/8\u03c12 . (25)\nNote that the probability is over the choice of D.\nThe proof of the above lemma is moved to Appendix A.2. We remark that the lemma only assumes that p is a unit vector, it need not have a small `\u221e norm.\nWe can now complete the proof of our theorem. As noted above, we need to show (24). To recall, \u03a0 is the projector onto span{Dx,Dy}, and we need to bound:\nE [max{\u2016\u03a0u\u2016, \u2016\u03a0v\u2016}] \u2264 E[\u2016\u03a0u\u2016] + E[\u2016\u03a0v\u2016]. (26)\nLet x, z be an orthonormal basis for span{x,y}; then it is easy to see that for any diagonal D with \u00b11 entries on the diagonal, Dx,Dz is an orthonormal basis for span{Dx,Dy}. Thus\nE[\u2016\u03a0u\u2016] \u2264 E[|\u3008u,Dx\u3009|+ |\u3008u,Dz\u3009|]. (27)\nNow by Lemma 7,\nPr[|\u3008u,Dx\u3009| > t\u03c1] \u2264 e\u2212t2/4. (28)\nIntegrating over t, we get E[|\u3008u,Dx\u3009|] \u2264 4\u03c1. Thus we can bound the LHS of (26) by 16\u03c1, completing the proof of the theorem."}, {"heading": "4.2 The Johnson-Lindenstrauss Type Result", "text": "Next, we turn to the proof of Theorem 4, where we wish to obtain a strong tail bound. At a high level, the argument consists of two steps:\n\u2022 First, show that with probability 1 \u2212 over the choice of D, the k translates of x,y satisfy certain orthogonality properties (this is in the same spirit as Lemma 7). \u2022 Second, conditioned on orthogonality as above, with high probability over the choice\nof r, we have the desired guarantee.\nNext will will show the two steps respectively. Throughout this section, we denote by X0, X1, . . . , Xk\u22121 the k shifts of Dx, i.e., Xi = s\u2192(i)(Dx); define Y0, . . . , Yk\u22121 analogously as shifts of Dy. We will also assume that \u03c1 < \u03b8 2\n16k log(k/\u03b4) .\nThe structure we require is formally the following.\nDefinition 8 ((\u03b3, k)-orthogonality). Two sequences of k unit vectors X0, X1, . . . , Xk\u22121 and Y0, Y1, . . . , Yk\u22121 are said to be (\u03b3, k)-orthogonal if there exists a decomposition (for every i)\nXi = ui + ei ; Yi = vi + fi (29)\nsatisfying the following properties:\n1. ui and vi are both orthogonal to span{uj ,vj : j 6= i}. 2. maxi{\u2016ei\u2016, \u2016fi\u2016} < \u03b3.\nThe lemma of the first step, as described earlier, is the following:\nLemma 9. Let x,y be unit vectors with \u2016x\u2016\u221e, \u2016y\u2016\u221e \u2264 \u03c1, and \u03b8 = \u2220(x,y), and let Xi, Yi be rotations of Dx,Dy respectively (as defined earlier). Then w.p. 1\u2212 \u03b4 over the choice of D, the vectors (Xi, Yi) k i=1 are (\u03b3, k) orthogonal, for \u03b3 = 4 \u221a \u03c1.\nThe proof of the lemma is quite technical, and is moved to Appendix A.3.\nNow suppose we have that the shifts Xi, Yi satisfy (\u03b3, k)-orthogonality for some \u03b3 > 0. Suppose ui,vi, ei, fi are as defined earlier. (\u03b3, k)-orthogonality gives us that \u2016ei\u2016, \u2016fi\u2016 < \u03b3, which is 1. Roughly speaking, we use this to say that most of the time, sign(\u3008r, Xi\u3009 =\n\u3008r,ui\u3009). Thus determining if sign(\u3008r, Xi\u3009) = sign(\u3008r, Yi\u3009) is essentially equivalent to determining if sign(\u3008r,ui\u3009) = sign(\u3008r,vi\u3009). But the latter quantities, by orthogonality, are indepedent! (because the signs depend only on the projection of r onto the span of ui,vi, which is independent for different i).7 The main lemma of the second step is the following:\nLemma 10. Let (Xi, Yi) k i=1 be a set of vectors satisfying (\u03b3, k)-orthogonality and \u2220(Xi, Yi) = \u03b8 for all i. Then for any \u03b4 > 0 and k > max{1/\u03b3, log(4/\u03b4)}, we have\nPr [\u2223\u2223\u2223\u2223\u22231k\u2211 i (sign \u3008r, Xi\u3009 6= sign \u3008r, Yi\u3009)\u2212 \u03b8 \u03c0 \u2223\u2223\u2223\u2223\u2223 > \u03b3 \u00b7 (12 log(2k/\u03b4)) ] < 1\u2212 \u03b4. (30)\nThe probability here is over the choice of r.\nThe proof is deferred to Appendix A.4. We can now complete the proof of Theorem 4. It essentially follows using Lemma 9 and Lemma 10. Note that we can apply Lemma 10 because the angle between Xi and Yi is also \u03b8 for each i (since they are shifts of x,y).\nFormally, using the value of \u03b3 defined in Lemma 9, we have that the vectors Xi, Yi are (\u03b3, k) orthogonal with probability 1\u2212 \u03b4. Conditioned on this, the probability that the conclusion of Lemma 10 holds with probability 1\u2212\u03b4. Thus the overall probability of success is at least 1 \u2212 2\u03b4. The theorem is thus easily proved by plugging in the value of \u03b3 from Lemma 9, together with \u03c1 < 1. This completes the proof of the Theorem."}, {"heading": "5. Optimized Binary Embedding", "text": "In the previous section, we showed the randomized CBE has LSH-like angle preserving properties, especially for high-dimensional data. One problem with the randomized CBE method is that it does not utilize the underlying data distribution while generating the matrix R. In the next section, we propose to learn R in a data-dependent fashion, to minimize the distortions due to circulant projection and binarization.\nWe propose data-dependent CBE (CBE-opt), by optimizing the projection matrix with a novel time-frequency alternating optimization. We consider the following objective function in learning the d-bit CBE. The extension of learning k < d bits will be shown in Section 5.2.\nargmin B,r\n||B\u2212XRT ||2F + \u03bb||RRT \u2212 I||2F (31)\ns.t. R = circ(r),\nwhere X \u2208 RN\u00d7d, is the data matrix containing n training points: X = [x0, \u00b7 \u00b7 \u00b7 ,xN\u22121]T , and B \u2208 {\u22121, 1}N\u00d7d is the corresponding binary code matrix.8\nIn the above optimization, the first term minimizes distortion due to binarization. The second term tries to make the projections (rows of R, and hence the corresponding bits) as\n7. Again, using the property of multi-variate Gaussians that the projections onto orthogonal directions are orthogonal. 8. If the data is `2 normalized, we can set B \u2208 {\u22121/ \u221a d, 1/ \u221a d}N\u00d7d to make B and XRT more comparable.\nThis does not empirically influence the performance.\nuncorrelated as possible. In other words, this helps to reduce the redundancy in the learned code. If R were to be an orthogonal matrix, the second term will vanish and the optimization would find the best rotation such that the distortion due to binarization is minimized. However, being a circulant matrix, R, in general, will not be orthogonal9. Similar objective has been used in previous works including [GLGP12, GKRL13] and [WKC10]."}, {"heading": "5.1 The Time-Frequency Alternating Optimization", "text": "The above is a difficult non-convex combinatorial optimization problem. In this section we propose a novel approach to efficiently find a local solution. The idea is to alternatively optimize the objective by fixing r, and B, respectively. For a fixed r, optimizing B can be easily performed in the input domain (\u201ctime\u201d as opposed to \u201cfrequency\u201d). For a fixed B, the circulant structure of R makes it difficult to optimize the objective in the input domain. Hence we propose a novel method, by optimizing r in the frequency domain based on DFT. This leads to a very efficient procedure.\nFor a fixed r. The objective is independent on each element of B. Denote Bij as the element of the i-th row and j-th column of B. It is easy to show that B can be updated as:\nBij = { 1 if Rj\u00b7xi \u2265 0 \u22121 if Rj\u00b7xi < 0 , (32)\ni = 0, \u00b7 \u00b7 \u00b7 , N \u2212 1. j = 0, \u00b7 \u00b7 \u00b7 , d\u2212 1.\nFor a fixed B. Define r\u0303 as the DFT of the circulant vector r\u0303 := F(r). Instead of solving r directly, we propose to solve r\u0303, from which r can be recovered by IDFT.\nKey to our derivation is the fact that DFT projects the signal to a set of orthogonal basis. Therefore the `2 norm can be preserved. Formally, according to Parseval\u2019s theorem , for any t \u2208 Cd [OSB+99],\n||t||22 = (1/d)||F(t)||22. (33)\nDenote diag(\u00b7) as the diagonal matrix formed by a vector. Denote <(\u00b7) and =(\u00b7) as the real and imaginary parts, respectively. We use Bi\u00b7 to denote the i-th row of B. With complex arithmetic, the first term in (31) can be expressed in the frequency domain as:\n||B\u2212XRT ||2F = 1\nd N\u22121\u2211 i=0 ||F(BTi\u00b7 \u2212Rxi)||22 (34)\n= 1\nd N\u22121\u2211 i=0 ||F(BTi\u00b7 )\u2212 r\u0303 \u25e6 F(xi)||22 = 1 d N\u22121\u2211 i=0 ||F(BTi\u00b7 )\u2212 diag(F(xi))r\u0303||22\n= 1\nd N\u22121\u2211 i=0 ( F(BTi\u00b7 )\u2212 diag(F(xi))r\u0303 )T (F(BTi\u00b7 )\u2212 diag(F(xi))r\u0303) = 1\nd\n[ <(r\u0303)TM<(r\u0303) + =(r\u0303)TM=(r\u0303) + <(r\u0303)Th + =(r\u0303)Tg ] + ||B||2F ,\n9. We note that the rank of the circulant matrices can range from 1 (an all-1 matrix) to d (an identity matrix).\nwhere,\nM = diag (N\u22121\u2211 i=0 <(F(xi)) \u25e6 <(F(xi)) + =(F(xi)) \u25e6 =(F(xi)) ) , (35)\nh = \u22122 N\u22121\u2211 i=0 <(F(xi)) \u25e6 <(F(BTi\u00b7 )) + =(F(xi)) \u25e6 =(F(BTi\u00b7 )), (36)\ng = 2 N\u22121\u2211 i=0 =(F(xi)) \u25e6 <(F(BTi\u00b7 ))\u2212<(F(xi)) \u25e6 =(F(BTi\u00b7 )). (37)\nThe above can be derived based on the following fact. For any Q \u2208 Cd\u00d7d, s, t \u2208 Cd,\n||s\u2212Qt||22 = (s\u2212Qt)H(s\u2212Qt) (38) =sHs\u2212 sHQt\u2212 tHQHs + tHQHAt =<(s)T<(s) + =(s)T=(s)\u2212 2<(t)T (<(Q)T<(s) + =(Q)T=(s))\n+ 2=(t)T (=(Q)T<(s)\u2212<(Q)T=(s)) + <(t)T (<(Q)T<(Q) + =(Q)T=(Q))<(t) + =(t)T (<(Q)T<(Q) + =(Q)T=(Q))=(t) + 2<(t)T (=(Q)T<(Q)\u2212<(Q)T=(Q))=(t).\nFor the second term in (31), we note that the circulant matrix can be diagonalized by DFT matrix Fd and its conjugate transpose F H d . Formally, for R = circ(r), r \u2208 Rd,\nR = (1/d)FHd diag(F(r))Fd. (39)\nLet Tr(\u00b7) be the trace of a matrix. Therefore,\n||RRT \u2212 I||2F = || 1\nd FHd (diag(r\u0303) Hdiag(r\u0303)\u2212 I)Fd||2F (40)\n= Tr\n[ 1\nd FHd (diag(r\u0303)\nHdiag(r\u0303)\u2212 I)H(diag(r\u0303)Hdiag(r\u0303)\u2212 I)Fd ]\n= Tr [ (diag(r\u0303)Hdiag(r\u0303)\u2212 I)H(diag(r\u0303)Hdiag(r\u0303)\u2212 I) ] =||r\u0303H \u25e6 r\u0303\u2212 1||22 = ||<(r\u0303)2 + =(r\u0303)2 \u2212 1||22.\nFurthermore, as r is real-valued, additional constraints on r\u0303 are needed. For any u \u2208 C, denote u as its complex conjugate. We have the following result [OSB+99]: For any realvalued vector t \u2208 Cd, F(t)0 is real-valued, and\nF(t)d\u2212i = F(t)i, i = 1, \u00b7 \u00b7 \u00b7 , bd/2c. (41)\nFrom (34) \u2212 (41), the problem of optimizing r\u0303 becomes\nargmin r\u0303\n<(r\u0303)TM<(r\u0303) + =(r\u0303)TM=(r\u0303) + <(r\u0303)Th\n+ =(r\u0303)Tg + \u03bbd||<(r\u0303)2 + =(r\u0303)2 \u2212 1||22 (42) s.t. =(r\u03030) = 0\n<(r\u0303i) = <(r\u0303d\u2212i), i = 1, \u00b7 \u00b7 \u00b7 , bd/2c =(r\u0303i) = \u2212=(r\u0303d\u2212i), i = 1, \u00b7 \u00b7 \u00b7 , bd/2c.\nThe above is non-convex. Fortunately, the objective function can be decomposed, such that we can solve two variables at a time. Denote the diagonal vector of the diagonal matrix M as m. The above optimization can then be decomposed to the following sets of optimizations.\nargmin r\u03030\nm0r\u0303 2 0 + h0r\u03030 + \u03bbd ( r\u030320 \u2212 1 )2 , s.t. r\u03030 = r\u03030. (43)\nargmin r\u0303i\n(mi +md\u2212i)(<(r\u0303i)2 + =(r\u0303i)2) + 2\u03bbd ( <(r\u0303i)2 + =(r\u0303i)2 \u2212 1 )2 + (hi + hd\u2212i)<(r\u0303i) + (gi \u2212 gd\u2212i)=(r\u0303i), i = 1, \u00b7 \u00b7 \u00b7 , bd/2c.\nIn (43), we need to minimize a 4th order polynomial with one variable, with the closed form solution readily available. In (44), we need to minimize a 4th order polynomial with two variables. Though the closed form solution is hard to find (requiring solution of a cubic bivariate system), a local minima can be found by gradient descent, which in practice has constant running time for such small-scale problems. The overall objective is guaranteed to be non-increasing in each step. In practice, we find that a good solution can be reached within just 5-10 iterations. Therefore in practice, the proposed time-frequency alternating optimization procedure has running time O(Nd log d)."}, {"heading": "5.2 Learning with Dimensionality Reduction", "text": "In the case of learning k < d bits, we need to solve the following optimization problem:\nargmin B,r\n||BPk \u2212XPTkRT ||2F + \u03bb||RPkPTkRT \u2212 I||2F (44)\ns.t. R = circ(r),\nin which Pk = [ Ik O O Od\u2212k ] , Ik is a k \u00d7 k identity matrix, and Od\u2212k is a (d \u2212 k) \u00d7 (d \u2212 k) all-zero matrix. In fact, the right multiplication of Pk can be understood as a \u201ctemporal cut-off\u201d, which is equivalent to a frequency domain convolution. This makes the optimization difficult, as the objective in frequency domain can no longer be decomposed. To address this issue, we propose a simple solution in which Bij = 0, i = 0, \u00b7 \u00b7 \u00b7 , N\u22121, j = k, \u00b7 \u00b7 \u00b7 , d\u22121 in (31). Thus, the optimization procedure remains the same, and the cost is also O(Nd log d). We will show in experiments that this heuristic provides good performance in practice."}, {"heading": "6. Discussion", "text": ""}, {"heading": "6.1 Limitations of the Theory for Long Codes", "text": "As was shown in earlier works [LSMK11, GKRL13, SP11] and as we see in our experiments (Section 7), long codes are necessary for high-dimensional data for all binary embedding methods, either randomized or optimized.\nHowever, when the code length is too large, our theoretical analysis is not optimal. For instance, consider our variance bound when k > \u221a d. Here the \u03c1 term always dominates, because for any vector, we have \u03c1 \u2265 1/ \u221a d (at least one entry of a unit vector is at least 1/ \u221a d). In numeric simulations, we see that the variance drops as 1/k for a larger range of k,\nroughly up to d. A similar behavior holds in Theorem 4, where the condition \u03c1 \u2264 \u03b8216k log(k/\u03b4) can hold only when k < O( \u221a d/ log d). It is an interesting open question to analyze the variance and other concentration properties for larger k."}, {"heading": "6.2 Semi-supervised Extension", "text": "In some applications, one can have access to a few labeled pairs of similar and dissimilar data points. Here we show how the CBE formulation can be extended to incorporate such information in learning. This is achieved by adding an additional objective term J(R).\nargmin B,r\n||B\u2212XRT ||2F + \u03bb||RRT \u2212 I||2F + \u00b5J(R) (45)\ns.t. R = circ(r),\nJ(R) = \u2211 i,j\u2208M ||Rxi \u2212Rxj ||22 \u2212 \u2211 i,j\u2208D ||Rxi \u2212Rxj ||22. (46)\nHere M and D are the set of \u201csimilar\u201d and \u201cdissimilar\u201d instances, respectively. The intuition is to maximize the distances between the dissimilar pairs, and minimize the distances between the similar pairs. Such a term is commonly used in semi-supervised binary coding methods [WKC10]. We again use the time-frequency alternating optimization procedure of Section 5. For a fixed r, the optimization procedure to update B is the same. For a fixed B, optimizing r is done in frequency domain by expanding J(R) as below, with similar techniques used in Section 5.\n||Rxi \u2212Rxj ||22 = (1/d)||diag(F(xi)\u2212F(xj))r\u0303||22. (47)\nTherefore,\nJ(R) = (1/d)(<(r\u0303)TA<(r\u0303) + =(r\u0303)TA=(r\u0303)), (48)\nwhere A = A1 + A2 \u2212A3 \u2212A4, and\nA1 = \u2211\n(i,j)\u2208M\n<(diag(F(xi)\u2212F(xj)))T<(diag(F(xi)\u2212F(xj))), (49)\nA2 = \u2211\n(i,j)\u2208M\n=(diag(F(xi)\u2212F(xj)))T=(diag(F(xi)\u2212F(xj))), (50)\nA3 = \u2211\n(i,j)\u2208D\n<(diag(F(xi)\u2212F(xj)))T<(diag(F(xi)\u2212F(xj))), (51)\nA4 = \u2211\n(i,j)\u2208D\n=(diag(F(xi)\u2212F(xj)))T=(diag(F(xi)\u2212F(xj))). (52)\nHence, the optimization can be carried out as in Section 5, where M in (34) is simply replaced by M+\u00b5A. The semi-supervised extension improves over the non-semi-supervised version by 2% in terms of averaged AUC on the ImageNet-25600 dataset."}, {"heading": "7. Experiments", "text": "To compare the performance of the circulant binary embedding techniques, we conduct experiments on three real-world high-dimensional datasets used by the current state-of-theart method for generating long binary codes [GKRL13]. The Flickr-25600 dataset contains 100K images sampled from a noisy Internet image collection. Each image is represented by a 25, 600 dimensional vector. The ImageNet-51200 contains 100k images sampled from 100 random classes from ImageNet [DDS+09], each represented by a 51, 200 dimensional vector. The third dataset (ImageNet-25600) is another random subset of ImageNet containing 100K images in 25, 600 dimensional space. All the vectors are normalized to be of unit norm.\nWe compare the performance of the randomized (CBE-rand) and learned (CBE-opt) versions of our circulant embeddings with the current state-of-the-art for high-dimensional data, i.e., bilinear embeddings. We use both the randomized (bilinear-rand) and learned (bilinear-opt) versions. Bilinear embeddings have been shown to perform similarly or better than another promising technique called Product Quantization [JDS11]. Finally, we also compare against the binary codes produced by the baseline LSH method [Cha02], which is still applicable to 25,600 and 51,200 dimensional feature but with much longer running time and much more space. We also show an experiment with relatively low-dimensional feature (2048, with Flickr data) to compare against techniques that perform well for lowdimensional data but do not scale to high-dimensional scenario. Example techniques include ITQ [GLGP12], SH [WTF08], SKLSH [RL09], and AQBC [GKVL12].\nFollowing [GKRL13, NF12, GP11], we use 10,000 randomly sampled instances for training. We then randomly sample 500 instances, different from the training set as queries. The performance (recall@1-100) is evaluated by averaging the recalls of the query instances. The ground-truth of each query instance is defined as its 10 nearest neighbors based on `2 distance. For each dataset, we conduct two sets of experiments: fixed-time where code generation time is fixed and fixed-bits where the number of bits is fixed across all techniques. We also show an experiment where the binary codes are used for classification.\nThe proposed CBE method is found robust to the choice of \u03bb in (31). For example, in the retrieval experiments, the performance difference for \u03bb = 0.1, 1, 10, is within 0.5%. Therefore, in all the experiments, we simply fix \u03bb = 1. For the bilinear method, in order to get fast computation, the feature vector is reshaped to a near-square matrix, and the dimension of the two bilinear projection matrices are also chosen to be close to square. Parameters for other techniques are tuned to give the best results on these datasets."}, {"heading": "7.1 Computational Time", "text": "When generating k-bit code for d-dimensional data, the full projection, bilinear projection, and circulant projection methods have time complexity O(kd), O( \u221a kd), and O(d log d), respectively. We compare the computational time in Table 2 on a fixed hardware. Based on our implementation, the computational time of the above three methods can be roughly characterized as d2 : d \u221a d : 5d log d. Note that faster implementation of FFT algorithms will lead to better computational time for CBE by further reducing the constant factor. Due to the small storage requirement O(d), and the wide availability of highly optimized FFT libraries, CBE is also suitable for implementation on GPU. Our preliminary tests based on\nGPU shows up to 20 times speedup compared with CPU. In this paper, for fair comparison, we use same CPU based implementation for all the methods."}, {"heading": "7.2 Retrieval", "text": "The recalls of different methods are compared on the three datasets, shown in Figure 1 \u2013 3. The top row in each figure shows the performance of different methods when the code generation time for all the methods is kept the same as that of CBE. For a fixed time, the proposed CBE yields much better recall than other methods. Even CBE-rand outperforms LSH and Bilinear code by a large margin. The second row compares the performance for different techniques with codes of same length. In this case, the performance of CBE-rand is almost identical to LSH even though it is hundreds of time faster. This is consistent with our analysis in Section 4. Moreover, CBE-opt/CBE-rand outperform Bilinear-opt/Bilinearrand in addition to being 2-3 times faster.\nThere exist several techniques that do not scale to high-dimensional case. To compare our method with those, we conduct experiments with fixed number of bits on a relatively low-dimensional dataset (Flickr-2048), constructed by randomly sampling 2,048 dimensions of Flickr-25600. As shown in Figure 4, though CBE is not designed for such scenario, the CBE-opt performs better or equivalent to other techniques except ITQ which scales very poorly with d (O(d3)). Moreover, as the number of bits increases, the gap between ITQ and CBE becomes much smaller suggesting that the performance of ITQ is not expected to be better than CBE even if one could run ITQ on high-dimensional data.\nWe also conduct additional experiments to compare CBE with the more recent Hadamardbased algorithms. The first algorithm we consider generates the binary code using the Fast\nJohnson-Lindenstrss Transformation (FJLT). Similar to the circulant projection, FJLT has been used in dimensionality reduction [AC06], deep neural networks [YMD+14], and kernel approximation [LSS13]. Here, the binary code of x \u2208 Rd is generated as\nh(x) = sign(PHDx), (53)\nwhere P \u2208 Rk\u00d7d is a sparse matrix with the nonzeros entries generated iid from the standard distribution. H \u2208 Rd\u00d7d is the Hadamard matrix, and D \u2208 Rd\u00d7d is a diagonal matrix with random signs. Although the Hadamard transformation has computational complexity O(d log d) (the multiplication with H), this method is often slower than CBE due to the sparse Gaussian projection step (i.e., multiplication by P).\nThe second method we compare with is Fast Binary Embedding (FBE). It is a theoretically sound method recently proposed in [YCP15]. FBE generates binary bits using a partial Walsh-Hadamard matrix and a set of partial Gaussian Toeplitz matrices. The method can achieve the optimal measurement complexity O( 1\n\u03b42 logN). We follow the pa-\nrameters settings in [YCP15] (Flickr-25600 dataset, with the number of bits 5,000, 10,000 and 15,000). Note that under the setting, FBE is at least a few times slower than CBE due to the use of multiple Toeplitz projections. Figure 5 shows the retrieval performance. Based on the experiments, in addition to being much faster than FBE and FJLT, CBE-rand provides comparable or even better performance. Another advantage of CBE is that the framework permits data-dependent optimization to further improve the performance. In all the experiments, CBE-opt achieves the highest recall by a large margin compared to other methods."}, {"heading": "7.3 Classification", "text": "Besides retrieval, we also test the binary codes for classification. The advantage is to save on storage, allowing even large scale datasets to fit in memory [LSMK11, SP11]. We follow the asymmetric setting of [SP11] by training linear SVM on binary code sign(Rx), and testing on the original Rx. Empirically, this has been shown to give better accuracy than the symmetric procedure. We use ImageNet-25600, with randomly sampled 100 images per category for training, 50 for validation and 50 for testing. The code dimension is set as 25,600. As shown in Table 3, CBE, which has much faster computation, does not show any performance degradation compared with LSH or bilinear codes in classification task."}, {"heading": "8. Conclusion", "text": "We proposed a method of binary embedding for high-dimensional data. Central to our framework is to use a type of highly structured matrix, the circulant matrix, to perform the linear projection. The proposed method has time complexity O(d log d) and space complexity O(d), while showing no performance degradation on real-world data compared with more expensive approaches (O(d2) or O(d1.5)). The parameters of the method can be randomly generated, where interesting theoretical analysis was carried out to show that the angle preserving quality can be as good as LSH. The parameters can also be learned based on training data with an efficient optimization algorithm."}, {"heading": "Appendix A. Proofs of the Technical Lemmas", "text": "A.1 Proof of Lemma 6\nFor convenience, define u\u22a5 = u\u2212 \u03a0u, and similarly define v\u22a5. From our earlier observation about independence, we have that\nE [(\n1\u2212 sign(rTa) sign(rTb) 2 \u2212 \u03b8 \u03c0\n)( 1\u2212 sign(rTu\u22a5) sign(rTv\u22a5)\n2 \u2212 \u03b8 \u03c0\n)] = 0. (54)\nBecause the LHS is equal to the product of the expectations, and the first term is 0. Thus the quantity we wish to bound is\nE [(\n1\u2212 sign(rTa) sign(rTb) 2 \u2212 \u03b8 \u03c0\n)( sign(rTu) sign(rTv)\u2212 sign(rTu\u22a5) sign(rTv\u22a5)\n2\n)] .\nNow by using the fact that E[XY ] \u2264 E[|X||Y |], together with the observation that the quantity |(1\u2212 sign(rTa) sign(rTb))/2\u2212 \u03b8/\u03c0| is at most 2, we can bound the above by\nE [ | sign(rTu) sign(rTv)\u2212 sign(rTu\u22a5) sign(rTv\u22a5)| ] . (55)\nThis is equal to\n2 Pr[sign(rTu) sign(rTv) 6= sign(rTu\u22a5) sign(rTv\u22a5)], (56)\nsince the term in the expectation is 2 if the product of signs is different, and 0 otherwise. To bound this, we first observe that for any two unit vectors x,y with \u2220(x,y) \u2264 , we have Pr[sign(rTx) 6= sign(rTy)] \u2264 /\u03c0. We can use this to say that\nPr[sign(rTu) 6= sign(rTu\u22a5)] = \u2220(u,u \u22a5)\n\u03c0 . (57)\nThis angle can be bounded in our case by (\u03c0/2) \u00b7 \u03b4 by basic geometry.10 Thus by a union bound, we have that\nPr[ ( sign(rTu) 6= sign(rTu\u22a5) ) \u2228 ( sign(rTv) 6= sign(rTv\u22a5) ) ] \u2264 \u03b4. (58)\nThis completes the proof.\nA.2 Proof of Lemma 7\nDenoting the ith entry of p by pi (so also for q), we have that\nS := \u3008Dp, s\u2192t(Dq)\u3009 = d\u22121\u2211 i=0 \u03c3i\u03c3i+tpiqi+t. (59)\nWe note that E[S] = 0, by linearity of expectation (as t > 0, E[\u03c3i\u03c3i+t] = 0), thus the lemma is essentially a tail bound on S. While we can appeal to standard tail bounds for quadratic forms of sub-Gaussian random variables (e.g. Hansen-Wright [RV13]), we give below a simple argument. Let us define\nf(\u03c30, \u03c31, . . . , \u03c3d\u22121) = d\u22121\u2211 i=0 piqi+t\u03c3i\u03c3i+t. (60)\nWe will view f as being obtained from a martingale as follows. Define\nQi := f(\u03c30, \u03c31, . . . , \u03c3i, 0, . . . , 0)\u2212 f(\u03c30, \u03c31, . . . , \u03c3i\u22121, 0, . . . , 0). (61)\nIn this notation, we have S = Q0 +Q1 + \u00b7 \u00b7 \u00b7+Qd\u22121. We have the martingale property that E[Qi|Q0, Q1, . . . , Qi\u22121] = 0 for all i, (because \u03c3i is \u00b11 with equal probability). Further, we have the bounded difference property, i.e., |Qi| \u2264 |piqi+t|+ |pi\u2212tqi|. This implies that\n|Qi|2 \u2264 2(p2i q2i+t + p2i\u2212tq2i ). (62)\nThus we can use Azuma\u2019s inequality to conclude that for any \u03b3 > 0,\nPr[| \u2211 i Qi \u2212 E[ \u2211 i Qi]| > \u03b3] < e \u2212 \u03b3 2 2\u00b7 \u2211 i 2(p 2 i q2 i+t +p2 i\u2212tq 2 i ) = e \u2212 \u03b3 2 8 \u22112 i p2 i q2 i+t . (63)\nWe can now use the fact that \u2211\ni p 2 i q 2 i+t \u2264 \u03c12 \u2211 i p 2 i = \u03c1\n2 (since \u2016p\u20162 = 1 and \u2016q\u2016\u221e \u2264 \u03c1). This establishes the lemma.\n10. u is a unit vector, and u\u22a5 + \u03a0u = u, and \u2016\u03a0u\u2016 \u2264 \u03b4, so the angle is at most sin\u22121(\u03b4).\nA.3 Proof of Lemma 9\nFirst, using Lemma 7 we have, for any i 6= j and c > 0,\nPr[|\u3008Xi, Yj\u3009| > c] < e\u2212c 2/8\u03c12 . (64) We have a similar bound for Pr[|\u3008Xi, Xj\u3009| > c]. Thus by setting c = 4\u03c1 \u221a\nlog(k/\u03b4) (\u03b4 as in the statement of the lemma), we can take a union bound over all k2 choices of i 6= j and conclude that w.p. at least 1\u2212 \u03b4, we have\nmax i 6=j {|\u3008Xi, Xj\u3009|, |\u3008Xi, Yj\u3009|} < 4\u03c1\n\u221a log(k/\u03b4). (65)\nWe now prove that whenever Eq. (65) holds, we obtain (\u03b3, k) orthogonality for the desired \u03b3. Let us start with a basic fact in linear algebra.\nLemma 11. Let A be an d \u00d7 k matrix with \u03c3k(A) \u2265 \u03c4 , for some parameter \u03c4 . Then any unit vector in the column span of A can be written as \u2211 i \u03b1iAi, with \u2211 i \u03b1 2 i \u2264 1/\u03c42.\nProof. By the definition of \u03c3k, we have that for any \u03b1i, \u2016 \u2211 i \u03b1iAi\u201622 \u2265 \u03c42 (\u2211 i \u03b1 2 i ) . Thus for\nany unit vector \u2211 i \u03b1iAi, we have \u2211 i \u03b1 2 i \u2264 1/\u03c42.\nNow let B be the d \u00d7 2k matrix whose columns are X1, Y1, X2, Y2, . . . , Xk, Yk in that order. Consider the entries of BTB. Since Xi, Yi are unit vectors, the diagonals are all 1. The (2i\u2212 1, 2i)th and (2i, 2i\u2212 1)th entries are exactly cos \u03b8, because the angle between Xi, Yi is \u03b8. The rest of the entries are of magnitude < \u03b7 := 4\u03c1 \u221a log(k/\u03b4).\nThus if we consider M = BTB \u2212 I (diagonal removed from BTB), we have \u2212(cos \u03b8 + k\u03b7)I M (cos \u03b8 + k\u03b7)I (diagonal dominance). Thus we conclude that BTB has all its eigenvalues \u2265 1 \u2212 cos \u03b8 \u2212 k\u03b7. Since \u03b8 \u2208 (0, \u03c0/2), we can use the standard inequality cos \u03b8 < 1\u2212 \u03b82/2 to conclude that the eigenvalues are \u2265 \u03b82/2\u2212 k\u03b7. Now by our assumption on \u03c1, we have that k\u03b7 < \u03b82/4. This implies that all the eigenvalues are \u2265 \u03b82/4.\nThus we have \u03c322k(B) \u2265 \u03b82/4. We prove now that this lets us obtain a decomposition that helps us prove (\u03b3, k)-orthogonality. A crucial observation is the following.\nLemma 12. The projection of Xi onto span{X1, Y1, X2, Y2, . . . , Xi\u22121, Yi\u22121} has length at most 2\u03b7 \u221a 2k\n\u03b8 .\nProof. Let S denote span{X1, Y1, . . . , Xi\u22121, Yi\u22121}. By definition, the squared of the length of projection is equal to max{\u3008y,Xi\u30092 | y \u2208 S and \u2016y\u20162 = 1} (this is how the projection onto a subspace can be defined).\nTo bound this, consider any unit vector y \u2208 S, and suppose we write it as \u2211\nj<i \u03b1jXj + \u03b2jYj . Let B\n\u2032 be the matrix that has columns Xj , Yj , j < i. Then it is straightforward to see that \u03c32(i\u22121)(B \u2032) \u2265 \u03c32k(B) \u2265 \u03b8/2. Thus Claim 11 implies that \u2211 j<i \u03b1 2 j + \u03b2 2 j \u2264 4/\u03b82. This means that\n\u3008Xi, y\u30092 = (\u2211 j<i \u03b1j\u3008Xj , Xi\u3009+ \u03b2j\u3008Yj , Xi\u3009 )2\n(66)\n\u2264 (\u2211 j<i \u03b12j + \u03b2 2 j )(\u2211 j<i \u3008Xj , Xi\u30092 + \u3008Yj , Xi\u30092 )\n(67)\n\u2264 4 \u03b82 \u00b7 (2i\u2212 2)\u03b72. (68)\n(In the first step, we used Cauchy-Schwartz.) Taking square roots now gives the claim.\nNow we perform the following procedure on the vectors (it is essentially Gram-Schmidt orthonormalization, with the slight twist that we deal with Xi, Yi together):\n1. Initialize: u1 = X1, e1 = 0, v1 = Y1, f1 = 0. 2. For i = 2, . . . , k, we set ui,vi to be the projections of Xi, Yi (respectively) orthogonal\nto span{X1, Y1, . . . , Xi\u22121, Yi\u22121}. Set ei = Xi \u2212 ui and fi = Yi \u2212 vi. The important observation is that for any i, we have\nspan{uj ,vj : j < i} = span{uj ,vj , ej , fj : j < i} = span{Xj , Yj : j < i}. (69)\nThis is because by definition, ei, fi \u2208 span{Xj , Yj : j < i} for all i. Thus we have that ui and vi satisfy the first condition in Definition 8. It just remains to analyze the lengths. Now we can use Claim 12 to conclude that\n\u2016ei\u201622, \u2016fi\u201622 < 8k\u03b72\n\u03b82 = 128 \u00b7 k\u03c12 log(k/\u03b4) \u03b82 . (70)\nOnce again, we use the bound on \u03c1 to conclude that this quantity is at most 16\u03c1. This completes the proof of Lemma 9, with \u03b3 = 4 \u221a \u03c1.\nA.4 Proof of Lemma 10\nWe start with a simple claim about the angle between ui and vi.\nLemma 13. For all i, we have \u2220(ui,vi) \u2208 (\u03b8 \u2212 \u03c0\u03b3, \u03b8 + \u03c0\u03b3).\nProof. The angle between Xi and ui is at most sin \u22121(\u03b3) < (\u03c0/2)\u03b3. So also, the angle between Yi and vi is at most (\u03c0/2)\u03b3. Thus the angle between ui,vi is in the interval (\u03b8 \u2212 \u03c0\u03b3, \u03b8 + \u03c0\u03b3) (by triangle inequality for the geodesic distance).\nLet \u03b7 > 0 be a parameter we will fix later (it will be a constant times \u03b3 \u221a\nlog(k/\u03b4)). For all i, we define the following events:\nEi : min{\u3008r,ui\u3009, \u3008r,vi\u3009} < \u03b7 (71) Fi : \u00acEi and sign \u3008r,ui\u3009 6= sign \u3008r,vi\u3009 (72)\nThe following claim now follows easily.\nLemma 14. For any i, we have\nPr[Ei] \u2264 2\u03b7, (73) Pr[Fi] \u2208 ( \u03b8\n\u03c0 \u2212 \u03c0\u03b3 \u2212 2\u03b7, \u03b8 \u03c0\n) (74)\nProof. The first inequality follows from the small ball probability of a univariate Gaussian (since \u3008r,ui\u3009 is a Gaussian of unit variance), and the second follows from Claim 13 and (73).\nWe will set \u03b7 to be larger than \u03c0\u03b3, so the RHS in (74) can be replaced with (\u03b8/\u03c0 \u2212 3\u03b7, \u03b8/\u03c0). Furthermore, the events above for a given i depend only on the projection of r to span{ui,vi}; thus they are independent for different i. Let us abuse notation slightly and denote by Ei also the indicator random variable for the event Ei (so also Fi). Then by standard Chernoff bounds, we have for any \u03c4 > 0,\nPr [\u2211 i Ei \u2265 2k\u03b7 + k\u03c4 ] < e \u2212 k\u03c4 2 4\u03b7+\u03c4 , (75)\nPr [\u2211 i Fi 6\u2208 ( k\u03b8 \u03c0 \u2212 3k\u03b7 \u2212 k\u03c4, k\u03b8 \u03c0 + k\u03c4 )] < 2e\u2212 k\u03c42 \u03b8+\u03c4 . (76)\nFinally let H denote the event:\nmax i {|\u3008r, ei\u3009|, |\u3008r, fi\u3009|} \u2265 \u03b7. (77)\nFor any i, since \u2016ei\u2016 < \u03b3, we have Pr[|\u3008r, ei\u3009| > t\u03b3] \u2264 e\u2212t 2/2. We can use the same bound with fi, and take a union bound over all i, to conclude that Pr[H] \u2264 2k \u00b7 e\u2212\u03b7 2/2\u03b32 .\nLet us call a choice of r good if neither of the events in (75)-(76) above occur, and additionally H does not occur. Clearly, the probability of an r being good is at least 1\u2212 \u03b4, provided \u03c4 and \u03b7 are chosen such that the RHS of the tail bounds above are all made \u2264 \u03b4/4.\nBefore setting these values, we note that for a good r,\n1\nk \u2211 i 1{sign \u3008r, Xi\u3009 6= sign \u3008r, Yi\u3009} \u2208 ( \u03b8 \u03c0 \u2212 3\u03b7 \u2212 \u03c4, \u03b8 \u03c0 + 2\u03b7 + 2\u03c4 ) . (78)\nThis is because whenever Fi \u2227 \u00acH occurs, we have sign \u3008r, Xi\u3009 6= sign \u3008r, Yi\u3009, and thus the LHS above is at least \u03b8\u03c0 \u2212 3\u03b7 \u2212 \u03c4 . Also if we have \u00acH, then the only way we can have sign \u3008r, Xi\u3009 6= sign \u3008r, Yi\u3009 is if either Fi occurs, or if Ei occurs (in the latter case, it is not necessary that the signs are unequal). Thus we can upper bound the LHS by \u03b8\u03c0 + 2\u03b7 + 2\u03c4 .\nLet us now set the values of \u03b7 and \u03c4 . From the above, we need to ensure:\nk\u03c42\n4\u03b7 + \u03c4 \u2265 log(4/\u03b4), k\u03c4\n2\n\u03b8 + \u03c4 \u2265 log(8/\u03b4), and \u03b7\n2\n2\u03b32 \u2265 log(4k/\u03b4). (79)\nThus we set \u03b7 = 2\u03b3 \u221a log(4k/\u03b4), and\n\u03c4 \u2265 max\n{ 2 log(8/\u03b4)\nk ,\n\u221a 2\u03b8 log(8/\u03b4)\nk ,\n\u221a 8\u03b7 log(4/\u03b4)\nk\n} . (80)\nFor the above inequality to hold, it suffices to set\n\u03c4 \u2265 8 log(1/\u03b4)\u221a k . (81)\nThis gives the desired bound on the deviation in the angle."}], "references": [{"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["Nir Ailon", "Bernard Chazelle"], "venue": "In Proceedings of the ACM Symposium on Theory of Computing,", "citeRegEx": "Ailon and Chazelle.,? \\Q2006\\E", "shortCiteRegEx": "Ailon and Chazelle.", "year": 2006}, {"title": "Database-friendly random projections: JohnsonLindenstrauss with binary coins", "author": ["Dimitris Achlioptas"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Achlioptas.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas.", "year": 2003}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Moses S Charikar"], "venue": "In Proceedings of the ACM Symposium on Theory of Computing,", "citeRegEx": "Charikar.,? \\Q2002\\E", "shortCiteRegEx": "Charikar.", "year": 2002}, {"title": "Binary embeddings with structured hashed projections", "author": ["Anna Choromanska", "Choromanski Krzysztof", "Mariusz Bojarski", "Tony Jebara", "Sanjiv Kumar", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.05212v1,", "citeRegEx": "Choromanska et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choromanska et al\\.", "year": 2015}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Yu Chen", "Felix Xinnan Yu", "Rogerio Feris", "Sanjiv Kumar", "S.-F. Choudhary", "Alok abd Chang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Fast neural networks with circulant projections", "author": ["Y. Cheng", "Felix Xinnan Yu", "R.S Feris", "S. Kumar", "A. Choudhary", "S.F. Chang"], "venue": "arXiv preprint arXiv:1502.03436,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Fast locality-sensitive hashing", "author": ["Anirban Dasgupta", "Ravi Kumar", "Tam\u00e1s Sarl\u00f3s"], "venue": "In Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Dasgupta et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2011}, {"title": "The johnson-lindenstrauss lemma and the sphericity of some graphs", "author": ["Peter Frankl", "Hiroshi Maehara"], "venue": "Journal of Combinatorial Theory, Series B,", "citeRegEx": "Frankl and Maehara.,? \\Q1988\\E", "shortCiteRegEx": "Frankl and Maehara.", "year": 1988}, {"title": "Learning binary codes for high-dimensional data using bilinear projections", "author": ["Yunchao Gong", "Sanjiv Kumar", "Henry A Rowley", "Svetlana Lazebnik"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2013}, {"title": "Angular quantization-based binary codes for fast similarity search", "author": ["Yunchao Gong", "Sanjiv Kumar", "Vishal Verma", "Svetlana Lazebnik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Y. Gong", "S. Lazebnik", "A. Gordo", "F. Perronnin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Gong et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2012}, {"title": "Asymmetric distances for binary embeddings", "author": ["Albert Gordo", "Florent Perronnin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Gordo and Perronnin.,? \\Q2011\\E", "shortCiteRegEx": "Gordo and Perronnin.", "year": 2011}, {"title": "Toeplitz and circulant matrices: A review", "author": ["Robert M Gray"], "venue": "Now Pub,", "citeRegEx": "Gray.,? \\Q2006\\E", "shortCiteRegEx": "Gray.", "year": 2006}, {"title": "Vyb\u0301\u0131ral. Johnson-Lindenstrauss lemma for circulant matrices", "author": ["Aicke Hinrichs", "Jan"], "venue": "Random Structures & Algorithms,", "citeRegEx": "Hinrichs and Jan,? \\Q2011\\E", "shortCiteRegEx": "Hinrichs and Jan", "year": 2011}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "In Proceedings of the ACM Symposium on Theory of Computing,", "citeRegEx": "Indyk and Motwani.,? \\Q1998\\E", "shortCiteRegEx": "Indyk and Motwani.", "year": 1998}, {"title": "Product quantization for nearest neighbor search", "author": ["Herve Jegou", "Matthijs Douze", "Cordelia Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Jegou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jegou et al\\.", "year": 2011}, {"title": "Extensions of lipschitz mappings into a hilbert space", "author": ["William B Johnson", "Joram Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "Johnson and Lindenstrauss.,? \\Q1984\\E", "shortCiteRegEx": "Johnson and Lindenstrauss.", "year": 1984}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["Brian Kulis", "Trevor Darrell"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kulis and Darrell.,? \\Q2009\\E", "shortCiteRegEx": "Kulis and Darrell.", "year": 2009}, {"title": "Dense fast random projections and lean walsh transforms. Approximation, Randomization and Combinatorial Optimization", "author": ["Edo Liberty", "Nir Ailon", "Amit Singer"], "venue": "Algorithms and Techniques,", "citeRegEx": "Liberty et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liberty et al\\.", "year": 2008}, {"title": "Hashing algorithms for large-scale learning", "author": ["Ping Li", "Anshumali Shrivastava", "Joshua Moore", "Arnd Christian Konig"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Fastfood \u2013 approximating kernel expansions in loglinear time", "author": ["Quoc Le", "Tam\u00e1s Sarl\u00f3s", "Alex Smola"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Hashing with graphs", "author": ["Wei Liu", "Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Mohammad Norouzi", "David Fleet"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Norouzi and Fleet.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi and Fleet.", "year": 2012}, {"title": "Hamming distance metric learning", "author": ["Mohammad Norouzi", "David Fleet", "Ruslan Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Norouzi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2012}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["Maxim Raginsky", "Svetlana Lazebnik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Raginsky and Lazebnik.,? \\Q2009\\E", "shortCiteRegEx": "Raginsky and Lazebnik.", "year": 2009}, {"title": "Hanson-wright inequality and subgaussian concentration", "author": ["Mark Rudelson", "Roman Vershynin"], "venue": "Electron. Commun. Probab,", "citeRegEx": "Rudelson and Vershynin.,? \\Q2013\\E", "shortCiteRegEx": "Rudelson and Vershynin.", "year": 2013}, {"title": "High-dimensional signature compression for large-scale image classification", "author": ["Jorge S\u00e1nchez", "Florent Perronnin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "S\u00e1nchez and Perronnin.,? \\Q2011\\E", "shortCiteRegEx": "S\u00e1nchez and Perronnin.", "year": 2011}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2010}, {"title": "Spectral hashing", "author": ["Yair Weiss", "Antonio Torralba", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Weiss et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2008}, {"title": "Binary embedding: Fundamental limits and fast algorithm", "author": ["Xinyang Yi", "Constantine Caramanis", "Eric Price"], "venue": "arXiv preprint arXiv:1502.05746,", "citeRegEx": "Yi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yi et al\\.", "year": 2015}, {"title": "Circulant binary embedding", "author": ["Felix Xinnan Yu", "S. Kumar", "Y. Gong", "S.-F. Chang"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}, {"title": "Compact nonlinear maps and circulant extensions", "author": ["Felix Xinnan Yu", "Sanjiv Kumar", "Henry Rowley", "Shih-Fu Chang"], "venue": "arXiv preprint arXiv:1503.03893,", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Deep fried convnets", "author": ["Zichao Yang", "Marcin Moczulski", "Misha Denil", "Nando de Freitas", "Alex Smola", "Le Song", "Ziyu Wang"], "venue": "arXiv preprint arXiv:1412.7149,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "New bounds for circulant Johnson-Lindenstrauss embeddings", "author": ["Hui Zhang", "Lizhi Cheng"], "venue": "arXiv preprint arXiv:1308.6339,", "citeRegEx": "Zhang and Cheng.,? \\Q2013\\E", "shortCiteRegEx": "Zhang and Cheng.", "year": 2013}], "referenceMentions": [], "year": 2015, "abstractText": "Binary embeddings provide efficient and powerful ways to perform operations on large scale data. However binary embedding typically requires long codes in order to preserve the discriminative power of the input space. Thus binary coding methods traditionally suffer from high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure allows us to use Fast Fourier Transform algorithms to speed up the computation. For obtaining k-bit binary codes from d-dimensional data, our method improves the time complexity from O(dk) to O(d log d), and the space complexity from O(dk) to O(d). We study two settings, which differ in the way we choose the parameters of the circulant matrix. In the first, the parameters are chosen randomly and in the second, the parameters are learned using the data. For randomized CBE, we give a theoretical analysis comparing it with binary embedding using an unstructured random projection matrix. The challenge here is to show that the dependencies in the entries of the circulant matrix do not lead to a loss in performance. In the second setting, we design a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. In both the settings, we show by extensive experiments that the CBE approach gives much better performance than the state-ofthe-art approaches if we fix a running time, and provides much faster computation with negligible performance degradation if we fix the number of bits in the embedding.", "creator": "LaTeX with hyperref package"}}}