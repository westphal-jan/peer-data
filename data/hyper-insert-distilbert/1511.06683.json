{"id": "1511.06683", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Nov-2015", "title": "Top-k Multiclass SVM", "abstract": "simple class rounding ambiguity is so typical in image classification problems with often a large algebraic number schemes of related classes. when graph classes are difficult to simultaneously discriminate, historically it makes strict sense to allow k guesses and routinely evaluate classifiers based indirectly on the basic top - down k error instead of the standard zero - one loss. firstly we propose improved top - far k multiclass svm voting as only a theoretically direct method to visually optimize filters for poor top - k performance. our systematic generalization of the well - known multiclass inverse svm error is presented based immediately on a tight convex upper bound trick of limiting the top - k error. we propose a polynomial fast optimization scheme immediately based on imagining an efficient projection onto the top - plane k simplex, as which too is of its whole own interest. experiments written on five datasets currently show consistent color improvements back in top - way k accuracy compared to various metric baselines.", "histories": [["v1", "Fri, 20 Nov 2015 16:49:33 GMT  (675kb,D)", "http://arxiv.org/abs/1511.06683v1", "NIPS 2015"]], "COMMENTS": "NIPS 2015", "reviews": [], "SUBJECTS": "stat.ML cs.CV cs.LG", "authors": ["maksim lapin", "matthias hein 0001", "bernt schiele"], "accepted": true, "id": "1511.06683"}, "pdf": {"name": "1511.06683.pdf", "metadata": {"source": "CRF", "title": "Top-k Multiclass SVM", "authors": ["Maksim Lapin", "Matthias Hein", "Bernt Schiele"], "emails": [], "sections": [{"heading": null, "text": "Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.\n1 Introduction\nAs the number of classes increases, two important issues emerge: class overlap and multilabel nature of examples [10]. This phenomenon asks for adjustments of both the evaluation metrics as well as the loss functions employed. When a predictor is allowed k guesses and is not penalized for k\u22121 mistakes, such an evaluation measure is known as top-k error. We argue that this is an important metric that will inevitably receive more attention in the future as the illustration in Figure 1 indicates.\nHow obvious is it that each row of Figure 1 shows examples of different classes? Can we imagine a human to predict correctly on the first attempt? Does it even make sense to penalize a learning system for such \u201cmistakes\u201d? While the problem of class ambiguity is apparent in computer vision, similar problems arise in other domains when the number of classes becomes large.\nWe propose top-k multiclass SVM as a generalization of the well-known multiclass SVM [6]. It is based on a tight convex upper bound of the top-k zero-one loss which we call top-k hinge loss. While it turns out to be similar to a top-k version of the ranking based loss proposed by [30], we show that the top-k hinge loss is a lower bound on their version and is thus a tighter bound on the top-k zero-one loss. We propose an efficient implementation based on stochastic dual coordinate ascent (SDCA) [27]. A key ingredient in the optimization is the (biased) projection onto the top-k simplex. This projection turns out to be a tricky generalization of the continuous quadratic knapsack problem, respectively the projection onto the standard simplex. The proposed algorithm for solving it has complexity O(m logm) for x \u2208 Rm. Our implementation of the top-k multiclass SVM scales to large datasets like Places 205 with about 2.5 million examples and 205 classes [33]. Finally, extensive experiments on several challenging computer vision problems show that top-k multiclass SVM consistently improves in top-k error over the multiclass SVM (equivalent to our top-1 multiclass SVM), one-vs-all SVM and other methods based on different ranking losses [12, 17].\nar X\niv :1\n51 1.\n06 68\n3v 1\n[ st\nat .M\nL ]\n2 0\nN ov\n2 01\n2 Top-k Loss in Multiclass Classification\nIn multiclass classification, one is given a set S = {(xi, yi) | i = 1, . . . , n} of n training examples xi \u2208 X along with the corresponding labels yi \u2208 Y . Let X = Rd be the feature space and Y = {1, . . . ,m} the set of labels. The task is to learn a set of m linear predictors wy \u2208 Rd such that the risk of the classifier arg maxy\u2208Y \u3008wy, x\u3009 is minimized for a given loss function, which is usually chosen to be a convex upper bound of the zero-one loss. The generalization to nonlinear predictors using kernels is discussed below.\nThe classification problem becomes extremely challenging in the presence of a large number of ambiguous classes. It is natural in that case to extend the evaluation protocol to allow k guesses, which leads to the popular top-k error and top-k accuracy performance measures. Formally, we consider a ranking of labels induced by the prediction scores \u3008wy, x\u3009. Let the bracket [\u00b7] denote a permutation of labels such that [j] is the index of the j-th largest score, i.e.\u2329\nw[1], x \u232a \u2265 \u2329 w[2], x \u232a \u2265 . . . \u2265 \u2329 w[m], x \u232a .\nThe top-k zero-one loss errk is defined as\nerrk(f(x), y) = 1\u3008w[k],x\u3009>\u3008wy,x\u3009,\nwhere f(x) = (\u3008w1, x\u3009 , . . . , \u3008wm, x\u3009)> and 1P = 1 if P is true and 0 otherwise. Note that the standard zero-one loss is recovered when k = 1, and errk(f(x), y) is always 0 for k = m. Therefore, we are interested in the regime 1 \u2264 k < m."}, {"heading": "2.1 Multiclass Support Vector Machine", "text": "In this section we review the multiclass SVM of Crammer and Singer [6] which will be extended to the top-k multiclass SVM in the following. We mainly follow the notation of [27].\nGiven a training pair (xi, yi), the multiclass SVM loss on example xi is defined as\nmax y\u2208Y {1y 6=yi + \u3008wy, xi\u3009 \u2212 \u3008wyi , xi\u3009}. (1)\nSince our optimization scheme is based on Fenchel duality, we also require a convex conjugate of the primal loss function (1). Let c , 1\u2212eyi , where 1 is the all ones vector and ej is the j-th standard basis vector in Rm, let a \u2208 Rm be defined componentwise as aj , \u3008wj , xi\u3009 \u2212 \u3008wyi , xi\u3009, and let\n\u2206 , {x \u2208 Rm | \u30081, x\u3009 \u2264 1, 0 \u2264 xi, i = 1, . . . ,m}.\nProposition 1 ([27], \u00a7 5.1). A primal-conjugate pair for the multiclass SVM loss (1) is \u03c6(a) = max{0, (a+ c)[1]}, \u03c6\u2217(b) = { \u2212\u3008c, b\u3009 if b \u2208 \u2206, +\u221e otherwise. (2)\nNote that thresholding with 0 in \u03c6(a) is actually redundant as (a+ c)[1] \u2265 (a+ c)yi = 0 and is only given to enhance similarity to the top-k version defined later.\n2.2 Top-k Support Vector Machine\nThe main motivation for the top-k loss is to relax the penalty for making an error in the top-k predictions. Looking at \u03c6 in (2), a direct extension to the top-k setting would be a function\n\u03c8k(a) = max{0, (a+ c)[k]},\nwhich incurs a loss iff (a+ c)[k] > 0. Since the ground truth score (a+ c)[yi] = 0, we conclude that \u03c8k(a) > 0 \u21d0\u21d2 \u2329 w[1], xi \u232a \u2265 . . . \u2265 \u2329 w[k], xi \u232a > \u3008wyi , xi\u3009 \u2212 1,\nwhich directly corresponds to the top-k zero-one loss errk with margin 1. Note that the function \u03c8k ignores the values of the first (k \u2212 1) scores, which could be quite large if there are highly similar classes. That would be fine in this model as long as the correct prediction is within the first\nk guesses. However, the function \u03c8k is unfortunately nonconvex since the function fk(x) = x[k] returning the k-th largest coordinate is nonconvex for k \u2265 2. Therefore, finding a globally optimal solution is computationally intractable.\nInstead, we propose the following convex upper bound on \u03c8k, which we call the top-k hinge loss,\n\u03c6k(a) = max { 0, 1\nk k\u2211 j=1 (a+ c)[j] } , (3)\nwhere the sum of the k largest components is known to be convex [4]. We have that\n\u03c8k(a) \u2264 \u03c6k(a) \u2264 \u03c61(a) = \u03c6(a), for any k \u2265 1 and a \u2208 Rm. Moreover, \u03c6k(a) < \u03c6(a) unless all k largest scores are the same. This extra slack can be used to increase the margin between the current and the (m\u2212 k) remaining least similar classes, which should then lead to an improvement in the top-k metric.\n2.2.1 Top-k Simplex and Convex Conjugate of the Top-k Hinge Loss\nIn this section we derive the conjugate of the proposed loss (3). We begin with a well known result that is used later in the proof. All proofs can be found in the supplement. Let [a]+ = max{0, a}. Lemma 1 ([19], Lemma 1). \u2211k j=1 h[j] = mint { kt+ \u2211m j=1[hj \u2212 t]+ } .\nProof. For a t0 \u2208 [h[k+1], h[k]], we have\nmin t\n{ kt+ m\u2211 j=1 [hj \u2212 t]+ } \u2264 kt0 + m\u2211 j=1 [hj \u2212 t0]+ = kt0 + k\u2211 j=1 ( h[j] \u2212 t0 ) = k\u2211 i=1 h[i].\nOn the other hand, for any t \u2208 R, we get k\u2211 j=1 h[j] = kt+ k\u2211 j=1 ( h[j] \u2212 t ) \u2264 kt+ k\u2211 j=1 [ h[j] \u2212 t ] + \u2264 kt+ m\u2211 j=1 [hj \u2212 t]+ .\n1/3 1/2\n11\n1/2 1/3\n0\n1\n1/2\n1/3\n0\nTop-1 Top-2 Top-3\nWe also define a set \u2206k which arises naturally as the effective domain1 of the conjugate of (3). By analogy, we call it the top-k simplex as for k = 1 it reduces to the standard simplex with the inequality constraint (i.e. 0 \u2208 \u2206k). Let [m] , 1, . . . ,m. Definition 1. The top-k simplex is a convex polytope defined as\n\u2206k(r) ,\n{ x \u2223\u2223\u2223\u2223 \u30081, x\u3009 \u2264 r, 0 \u2264 xi \u2264 1k \u30081, x\u3009 , i \u2208 [m] } ,\nwhere r \u2265 0 is the bound on the sum \u30081, x\u3009. We let \u2206k , \u2206k(1).\nThe crucial difference to the standard simplex is the upper bound on xi\u2019s, which limits their maximal contribution to the total sum \u30081, x\u3009. See Figure 2 for an illustration. The first technical contribution of this work is as follows. Proposition 2. A primal-conjugate pair for the top-k hinge loss\nMoreover, \u03c6k(a) = max{\u3008a+ c, \u03bb\u3009 |\u03bb \u2208 \u2206k}. 1 A convex function f : X \u2192 R \u222a {\u00b1\u221e} has an effective domain dom f = {x \u2208 X | f(x) < +\u221e}.\nProof. We use Lemma 1 to write\n\u03c6k(a) = min { s | s \u2265 t+ 1\nk m\u2211 j=1 \u03bej , s \u2265 0, \u03bej \u2265 aj + cj \u2212 t, \u03bej \u2265 0 } .\nThe Lagrangian is given as\nL(s, t, \u03be, \u03b1, \u03b2, \u03bb, \u00b5) = s+ \u03b1 ( t+ 1\nk m\u2211 j=1 \u03bej \u2212 s ) \u2212 \u03b2s+ m\u2211 j=1 \u03bbj (aj + cj \u2212 t\u2212 \u03bej)\u2212 m\u2211 j=1 \u00b5j\u03bej .\nMinimizing over (s, t, \u03be), we get \u03b1+ \u03b2 = 1, \u03b1 = \u2211m j=1 \u03bbj , \u03bbj + \u00b5j = 1 k\u03b1. As \u03b2 \u2265 0 and \u00b5j \u2265 0, it follows that \u30081, \u03bb\u3009 \u2264 1 and 0 \u2264 \u03bbj \u2264 1k \u30081, \u03bb\u3009. Since the duality gap is zero, we get \u03c6k(a) = max{\u3008a+ c, \u03bb\u3009 |\u03bb \u2208 \u2206k}. The conjugate \u03c6\u2217k(b) can now be computed as max a {\u3008a, b\u3009 \u2212 \u03c6k(a)} = max a min \u03bb\u2208\u2206k {\u3008a, b\u3009 \u2212 \u3008a+ c, \u03bb\u3009} = min \u03bb\u2208\u2206k {\u2212 \u3008c, \u03bb\u3009+ max a \u3008a, b\u2212 \u03bb\u3009}. Since maxa \u3008a, b\u2212 \u03bb\u3009 =\u221e unless b = \u03bb, we get the formula for \u03c6\u2217k(b) as in (4).\nTherefore, we see that the proposed formulation (3) naturally extends the multiclass SVM of Crammer and Singer [6], which is recovered when k = 1. We have also obtained an interesting extension (or rather contraction, since \u2206k \u2282 \u2206) of the standard simplex.\n2.3 Relation of the Top-k Hinge Loss to Ranking Based Losses\nUsunier et al. [30] have recently formulated a very general family of convex losses for ranking and multiclass classification. In their framework, the hinge loss on example xi can be written as\nL\u03b2(a) = m\u2211 y=1 \u03b2y max{0, (a+ c)[y]},\nwhere \u03b21 \u2265 . . . \u2265 \u03b2m \u2265 0 is a non-increasing sequence of non-negative numbers which act as weights for the ordered losses. The relation to the top-k hinge loss becomes apparent if we choose \u03b2j = 1 k if j \u2264 k, and 0 otherwise. In that case, we obtain another version of the top-k hinge loss\n\u03c6\u0303k ( a ) = 1\nk k\u2211 j=1 max{0, (a+ c)[j]}. (5)\nIt is straightforward to check that \u03c8k(a) \u2264 \u03c6k(a) \u2264 \u03c6\u0303k(a) \u2264 \u03c61(a) = \u03c6\u03031(a) = \u03c6(a).\nThe bound \u03c6k(a) \u2264 \u03c6\u0303k(a) holds with equality if (a+ c)[1] \u2264 0 or (a+ c)[k] \u2265 0. Otherwise, there is a gap and our top-k loss is a strictly better upper bound on the actual top-k zero-one loss. While [30] employed LaRank [1] and [10], [31] optimized an approximation of L\u03b2(a), we show in \u00a7 5 how the loss function (5) can be optimized exactly and efficiently within the Prox-SDCA framework.\nMulticlass to binary reduction. It is also possible to compare directly to ranking based methods that solve a binary problem using the following reduction. We employ it in our experiments to evaluate the ranking based methods SVMPerf [12] and TopPush [17]. The trick is to augment the training set by embedding each xi \u2208 Rd into Rmd using a feature map \u03a6y for each y \u2208 Y . The mapping \u03a6y places xi at the y-th position in Rmd and puts zeros everywhere else. The example \u03a6yi(xi) is labeled +1 and all \u03a6y(xi) for y 6= yi are labeled \u22121. Therefore, we have a new training set with mn examples and md dimensional (sparse) features. Moreover, \u3008w,\u03a6y(xi)\u3009 = \u3008wy, xi\u3009 which establishes the relation to the original multiclass problem.\nAnother approach to general performance measures is given in [12]. It turns out that using the above reduction, one can show that under certain constraints on the classifier, the recall@k is equivalent to the top-k error. A convex upper bound on recall@k is then optimized in [12] via structured SVM. As their convex upper bound on the recall@k is not decomposable in an instance based loss, it is not directly comparable to our loss. While being theoretically very elegant, the approach of [12] does not scale to very large datasets."}, {"heading": "3 Optimization Framework", "text": "We begin with a general `2-regularized multiclass classification problem, where for notational convenience we keep the loss function unspecified. The multiclass SVM or the top-k multiclass SVM are obtained by plugging in the corresponding loss function from \u00a7 2.\n3.1 Fenchel Duality for `2-Regularized Multiclass Classification Problems\nLet X \u2208 Rd\u00d7n be the matrix of training examples xi \u2208 Rd, let W \u2208 Rd\u00d7m be the matrix of primal variables obtained by stacking the vectors wy \u2208 Rd, and A \u2208 Rm\u00d7n the matrix of dual variables. Before we prove our main result of this section (Theorem 1), we first impose a technical constraint on a loss function to be compatible with the choice of the ground truth coordinate. The top-k hinge loss from Section 2 satisfies this requirement as we show in Proposition 3. We also prove an auxiliary Lemma 2, which is then used in Theorem 1.\nDefinition 2. A convex function \u03c6 is j-compatible if for any y \u2208 Rm with yj = 0 we have that\nsup{\u3008y, x\u3009 \u2212 \u03c6(x) |xj = 0} = \u03c6\u2217(y).\nThis constraint is needed to prove equality in the following Lemma.\nLemma 2. Let \u03c6 be j-compatible, let Hj = I\u2212 1e>j , and let \u03a6(x) = \u03c6(Hjx), then\n\u03a6\u2217(y) = { \u03c6\u2217(y \u2212 yjej) if \u30081, y\u3009 = 0, +\u221e otherwise.\nProof. We have that KerHj = {x |Hjx = 0} = {t1 | t \u2208 R} and Ker\u22a5Hj = {x | \u30081, x\u3009 = 0}.\n\u03a6\u2217(y) = sup{\u3008y, x\u3009 \u2212 \u03a6(x) |x \u2208 Rm} = sup{\u3008y, x\u2016\u3009+ \u3008y, x\u22a5\u3009 \u2212 \u03c6(Hjx\u22a5) |x = x\u2016 + x\u22a5, x\u2016 \u2208 KerHj , x\u22a5 \u2208 Ker\u22a5Hj}.\nIt follows that \u03a6\u2217(y) can only be finite if \u3008y, x\u2016\u3009 = 0, which implies y \u2208 Ker\u22a5Hj . Let H\u2020j be the Moore-Penrose pseudoinverse of Hj . For a y \u2208 Ker\u22a5Hj , we can write\n\u03a6\u2217(y) = sup{\u3008y,H\u2020jHjx \u22a5\u3009 \u2212 \u03c6(Hjx\u22a5) |x\u22a5 \u2208 Ker\u22a5Hj}\n= sup{\u3008(H\u2020j ) >y, z\u3009 \u2212 \u03c6(z) | z \u2208 ImHj} \u2264 sup{\u3008(H\u2020j ) >y, z\u3009 \u2212 \u03c6(z) | z \u2208 Rm} = \u03c6\u2217((H\u2020j ) >y),\n(6)\nwhere ImHj = {Hjx |x \u2208 Rm}. Using rank-1 update of the Moore-Penrose pseudoinverse ([22], \u00a7 3.2.7), we can compute (H\u2020j ) > = I\u2212 eje>j \u2212 1m (1\u2212 ej)1 >. Since y \u2208 Ker\u22a5Hj , the last term is zero and we have (H\u2020j ) >y = y\u2212yjej . Finally, we use the fact that \u03c6 is j-compatible to prove that the inequality in (6) is satisfied with equality. We have that ImHj = {z | zj = 0} and (y \u2212 yjej)j = 0. Therefore, when \u30081, y\u3009 = 0, \u03a6\u2217(y) = sup{\u3008y \u2212 yjej , z\u3009 \u2212 \u03c6(z) | zj = 0} = \u03c6\u2217(y \u2212 yjej).\nWe can now use Lemma 2 to compute convex conjugates of the loss functions.\nTheorem 1. Let \u03c6i be yi-compatible for each i \u2208 [n], let \u03bb > 0 be a regularization parameter, and let K = X>X be the Gram matrix. The primal and Fenchel dual objective functions are given as:\nP (W ) = + 1\nn n\u2211 i=1 \u03c6i ( W>xi \u2212 \u3008wyi , xi\u30091 ) + \u03bb 2 tr ( W>W ) ,\nD(A) = \u2212 1 n n\u2211 i=1 \u03c6\u2217i (\u2212\u03bbn(ai \u2212 ayi,ieyi))\u2212 \u03bb 2 tr ( AKA> ) , if \u30081, ai\u3009 = 0 \u2200i, +\u221e otherwise.\nMoreover, we have that W = XA> and W>xi = AKi, where Ki is the i-th column of K.\nProof. We use Fenchel duality (see e.g. [2], Theorem 3.3.5), to write P (W ) = g(X>W ) + f(W ), and D(A) = \u2212g\u2217(\u2212A>)\u2212 f\u2217(XA>), for the functions g and f defined as follows:\ng(X>W ) = 1\nn n\u2211 i=1 \u03a6i ( W>xi ) = 1 n n\u2211 i=1 \u03c6i ( HyiW >xi ) , f(W ) = \u03bb 2 tr ( W>W ) = \u03bb 2 \u2016W\u20162F ,\nwhereHyi = I\u22121e>yi . One can easily verify that g \u2217(\u2212A>) = 1n \u2211n i=1 \u03a6 \u2217 i (\u2212nai) and f\u2217(XA>) = \u03bb 2 \u2225\u2225 1 \u03bbXA > \u2225\u22252 F\n. From Lemma 2, we have that \u03a6\u2217i (\u2212nai) = \u03c6\u2217(\u2212n(ai\u2212ayi,ieyi)), if \u30081,\u2212nai\u3009 = 0, and +\u221e otherwise. To complete the proof, we redefine A\u2190 1\u03bbA for convenience, and use the first order optimality condition ([2], Ex. 9.f in \u00a7 3) for the W = XA> formula.\nFinally, we show that Theorem 1 applies to the loss functions that we consider.\nProposition 3. The top-k hinge loss function from Section 2 is yi-compatible.\nProof. Let c = 1\u2212 eyi and consider the loss \u03c6k. As in Proposition 2, we have\nmax a, ayi=0 {\u3008a, b\u3009 \u2212 \u03c6k(a)} = min \u03bb\u2208\u2206k {\u2212 \u3008c, \u03bb\u3009+ max a, ayi=0 \u3008a, b\u2212 \u03bb\u3009} = \u03c6\u2217k(b),\nwhere we used that cyi = 0 and byi = 0 (cf. Definition 2), i.e. the yi-th coordinate has no influence.\nWe have repeated the derivation from Section 5.7 in [27] as there is a typo in the optimization problem (20) leading to the conclusion that ayi,i must be 0 at the optimum. Lemma 2 fixes this by making the requirement ayi,i = \u2212 \u2211 j 6=yi aj,i explicit. Note that this modification is already mentioned in their pseudo-code for Prox-SDCA.\n3.2 Optimization of Top-k Multiclass SVM via Prox-SDCA\nAlgorithm 1 Top-k Multiclass SVM 1: Input: training data {(xi, yi)ni=1}, parameters k (loss), \u03bb (regularization), (stopping cond.)\n2: Output: W \u2208 Rd\u00d7m, A \u2208 Rm\u00d7n 3: Initialize: W \u2190 0, A\u2190 0 4: repeat 5: randomly permute training data 6: for i = 1 to n do 7: si \u2190W>xi {prediction scores} 8: aoldi \u2190 ai {cache previous values} 9: ai \u2190 update(k, \u03bb, \u2016xi\u20162 , yi, si, ai)\n{see \u00a7 3.2.1 for details} 10: W \u2190W + xi(ai \u2212 aoldi )> {rank-1 update} 11: end for 12: until relative duality gap is below\nAs an optimization scheme, we employ the proximal stochastic dual coordinate ascent (Prox-SDCA) framework of Shalev-Shwartz and Zhang [27], which has strong convergence guarantees and is easy to adapt to our problem. In particular, we iteratively update a batch ai \u2208 Rm of dual variables corresponding to the training pair (xi, yi), so as to maximize the dual objective D(A) from Theorem 1. We also maintain the primal variables W = XA> and stop when the relative duality gap is below . This procedure is summarized in Algorithm 1.\nLet us make a few comments on the advantages of the proposed method. First, apart from the update step which we discuss below, all main operations can be computed using a BLAS library, which makes the overall implementation efficient. Second, the update step in Line 9 is optimal in the sense that it yields maximal dual objective increase jointly over m variables. This is opposed to SGD updates with data-independent step sizes, as well as to maximal but scalar updates in other SDCA variants. Finally, we have a well-defined stopping criterion as we can compute the duality gap (see discussion in [3]). The latter is especially attractive if there is a time budget for learning. The algorithm can also be easily kernelized since W>xi = AKi (cf. Theorem 1)."}, {"heading": "3.2.1 Dual Variables Update", "text": "For the proposed top-k hinge loss from Section 2, optimization of the dual objective D(A) over ai \u2208 Rm given other variables fixed is an instance of a regularized (biased) projection problem onto the top-k simplex \u2206k( 1\u03bbn ). Let a \\j be obtained by removing the j-th coordinate from vector a.\nProposition 4. The following two problems are equivalent with a\\yii = \u2212x and ayi,i = \u30081, x\u3009\nmax ai {D(A) | \u30081, ai\u3009 = 0} \u2261 min x {\u2016b\u2212 x\u20162 + \u03c1 \u30081, x\u30092 |x \u2208 \u2206k( 1\u03bbn )},\nwhere b = 1\u3008xi,xi\u3009 ( q\\yi + (1\u2212 qyi)1 ) , q = W>xi \u2212 \u3008xi, xi\u3009 ai and \u03c1 = 1.\nProof. Using Proposition 2 and Theorem 1, we write\nmax ai { \u2212 1 n \u03c6\u2217i (\u2212\u03bbn(ai \u2212 ayi,ieyi))\u2212 \u03bb 2 tr ( AKA> ) | \u30081, ai\u3009 = 0}.\nFor the loss function, we get\n\u2212 1 n \u03c6\u2217i (\u2212\u03bbn(ai \u2212 ayi,ieyi)) = \u03bbayi,i,\nwith \u2212\u03bbn(ai \u2212 ayi,ieyi) \u2208 \u2206k. One can verify that the latter constraint is equivalent to \u2212a \\yi i \u2208 \u2206k( 1 \u03bbn ), ayi,i = \u30081,\u2212a \\yi i \u3009. Similarly, we write for the regularization term\ntr ( AKA> ) = Kii \u3008ai, ai\u3009+ 2 \u2211 j 6=i Kij \u3008ai, aj\u3009+ const,\nwhere the const does not depend on ai. Note that \u2211 j 6=iKijaj = AKi \u2212 Kiiai = q and can be computed using the \u201cold\u201d ai. Let x , \u2212a\\yii , we have\n\u3008ai, ai\u3009 = \u30081, x\u30092 + \u3008x, x\u3009 , \u3008q, ai\u3009 = qyi \u30081, x\u3009 \u2212 \u3008q\\yi , x\u3009. Plugging everything together and multiplying with \u22122/\u03bb, we obtain\nmin x\u2208\u2206k( 1\u03bbn )\n\u22122 \u30081, x\u3009+ 2 ( qyi \u30081, x\u3009 \u2212 \u3008q\\yi , x\u3009 ) +Kii ( \u30081, x\u30092 + \u3008x, x\u3009 ) .\nCollecting the corresponding terms finishes the proof.\nWe discuss in the following section how to project onto the set \u2206k( 1\u03bbn ) efficiently.\n4 Efficient Projection onto the Top-k Simplex\nOne of our main technical results is an algorithm for efficiently computing projections onto \u2206k(r), respectively the biased projection introduced in Proposition 4. The optimization problem in Proposition 4 reduces to the Euclidean projection onto \u2206k(r) for \u03c1 = 0, and for \u03c1 > 0 it biases the solution to be orthogonal to 1. Let us highlight that \u2206k(r) is substantially different from the standard simplex and none of the existing methods can be used as we discuss below."}, {"heading": "4.1 Continuous Quadratic Knapsack Problem", "text": "Finding the Euclidean projection onto the simplex is an instance of the general optimization problem minx{\u2016a\u2212 x\u201622 | \u3008b, x\u3009 \u2264 r, l \u2264 xi \u2264 u} known as the continuous quadratic knapsack problem (CQKP). For example, to project onto the simplex we set b = 1, l = 0 and r = u = 1. This is a well examined problem and several highly efficient algorithms are available (see the surveys [20, 21]). The first main difference to our set is the upper bound on the xi\u2019s. All existing algorithms expect that u is fixed, which allows them to consider decompositions minxi{(ai\u2212xi)2 | l \u2264 xi \u2264 u} which can be solved in closed-form. In our case, the upper bound 1k \u30081, x\u3009 introduces coupling across all variables, which makes the existing algorithms not applicable. A second main difference is the bias term \u03c1 \u30081, x\u30092 added to the objective. The additional difficulty introduced by this term is relatively minor. Thus we solve the problem for general \u03c1 (including \u03c1 = 0 for the Euclidean projection onto \u2206k(r)) even though we need only \u03c1 = 1 in Proposition 4. The only case when our problem reduces to CQKP is when the constraint \u30081, x\u3009 \u2264 r is satisfied with equality. In that case we can let u = r/k and use any algorithm for the knapsack problem. We choose [14] since it is easy to implement, does not require sorting, and scales linearly in practice. The bias in the projection problem reduces to a constant \u03c1r2 in this case and has, therefore, no effect.\n4.2 Projection onto the Top-k Cone\nWhen the constraint \u30081, x\u3009 \u2264 r is not satisfied with equality at the optimum, it has essentially no influence on the projection problem and can be removed. In that case we are left with the problem of the (biased) projection onto the top-k cone which we address with the following lemma. Lemma 3. Let x\u2217 \u2208 Rd be the solution to the following optimization problem\nmin x {\u2016a\u2212 x\u20162 + \u03c1 \u30081, x\u30092 | 0 \u2264 xi \u2264 1k \u30081, x\u3009 , i \u2208 [d]},\nand let U , {i |x\u2217i = 1k \u30081, x \u2217\u3009}, M , {i | 0 < x\u2217i < 1k \u30081, x \u2217\u3009}, L , {i |x\u2217i = 0}.\n1. If U = \u2205 and M = \u2205, then x\u2217 = 0.\n2. If U 6= \u2205 and M = \u2205, then U = {[1], . . . , [k]}, x\u2217i = 1k+\u03c1k2 \u2211k i=1 a[i] for i \u2208 U , where\n[i] is the index of the i-th largest component in a.\n3. Otherwise (M 6= \u2205), the following system of linear equations holds u = ( |M | \u2211 i\u2208U ai + (k \u2212 |U |) \u2211 i\u2208M ai ) /D, t\u2032 = ( |U | (1 + \u03c1k) \u2211 i\u2208M ai \u2212 (k \u2212 |U |+ \u03c1k |M |) \u2211 i\u2208U ai ) /D,\nD = (k \u2212 |U |)2 + (|U |+ \u03c1k2) |M | , (7)\ntogether with the feasibility constraints on t , t\u2032 + \u03c1uk\nmax i\u2208L ai \u2264 t \u2264 min i\u2208M ai, max i\u2208M ai \u2264 t+ u \u2264 min i\u2208U ai, (8)\nand we have x\u2217 = min{max{0, a\u2212 t}, u}.\nProof. We consider an equivalent problem\nmin x,s { 12 \u2016a\u2212 x\u2016 2 + 12\u03c1s 2 | \u30081, x\u3009 = s, 0 \u2264 xi \u2264 sk , i \u2208 [d]}.\nLet t, \u00b5i \u2265 0, \u03bdi \u2265 0 be the dual variables, and let L be the Lagrangian:\nL(x, s, t, \u00b5, \u03bd) = 12 \u2016a\u2212 x\u2016 2 + 12\u03c1s 2 + t(\u30081, x\u3009 \u2212 s)\u2212 \u3008\u00b5, x\u3009+ \u2329 \u03bd, x\u2212 sk1 \u232a .\nFrom the KKT conditions, we have that\n\u2202xL = x\u2212 a+ t1\u2212 \u00b5+ \u03bd = 0, \u2202sL = \u03c1s\u2212 t\u2212 1k \u30081, \u03bd\u3009 = 0, \u00b5ixi = 0, \u03bdi(xi \u2212 s k ) = 0.\nWe have that xi = min{max{0, ai \u2212 t}, sk}, \u03bdi = max{0, ai \u2212 t \u2212 s k}, and s = 1 \u03c1 (t + 1 k \u30081, \u03bd\u3009). Let p , \u30081, \u03bd\u3009. We have t = \u03c1s\u2212 pk . Using the definition of the sets U and M , we get\ns = \u2211 i\u2208U s k + \u2211 i\u2208M (ai \u2212 t) = \u2211 i\u2208M ai \u2212 |M | (\u03c1s\u2212 p k ) + |U | s k ,\np = \u2211 i\u2208U (ai \u2212 t\u2212 s k ) = \u2211 i\u2208U ai \u2212 |U | (\u03c1s\u2212 p k )\u2212 |U | s k .\nIn the case U 6= \u2205 and M = \u2205 we get the simplified equations s = \u2211 i\u2208U s k = |U | s k =\u21d2 |U | = k,\np = \u2211 i\u2208U ai \u2212 k\u03c1s+ p\u2212 s =\u21d2 xi = s k =\n1\nk + \u03c1 k2 \u2211 i\u2208U ai, i \u2208 U.\nIn the remaining case solving this system for u , sk and t \u2032 , \u2212 pk , we get exactly the system in (7). The constraints (8) follow from the definition of the sets U , M , L, and ensure that the computed thresholds (t, u) are compatible with the corresponding partitioning of the index set.\nWe now show how to check if the (biased) projection is 0. For the standard simplex, where the cone is the positive orthant Rd+, the projection is 0 when all ai \u2264 0. It is slightly more involved for \u2206k.\nLemma 4. The biased projection x\u2217 onto the top-k cone is zero if \u2211k i=1 a[i] \u2264 0 (sufficient condition). If \u03c1 = 0 this is also necessary.\nProof. LetK , {x | 0 \u2264 xi \u2264 1k \u30081, x\u3009} be the top-k cone. It is known that the Euclidean projection of a ontoK is 0 if and only if a \u2208 NK(0) , {y | \u2200x \u2208 K, \u3008y, x\u3009 \u2264 0}, i.e. a is in the normal cone to K at 0. Therefore, we obtain as an equivalent condition that maxx\u2208K \u3008a, x\u3009 \u2264 0. Take any x \u2208 K and let s = \u30081, x\u3009. If s > 0, we have that at least k components in x must be positive. To maximize \u3008a, x\u3009, we would have exactly k positive xi = sk corresponding to the k largest components in a. That would result in \u3008a, x\u3009 = sk \u2211k i=1 a[i], which is non-positive if and only if \u2211k i=1 a[i] \u2264 0.\nFor \u03c1 > 0, the objective function has an additional term \u03c1 \u30081, x\u30092 that vanishes at x = 0. Therefore, if x = 0 is optimal for the Euclidean projection, it must also be optimal for the biased projection.\nProjection. Lemmas 3 and 4 suggest a simple algorithm for the (biased) projection onto the topk cone. First, we check if the projection is constant (cases 1 and 2 in Lemma 3). In case 2, we compute x and check if it is compatible with the corresponding sets U , M , L. In the general case 3, we suggest a simple exhaustive search strategy. We sort a and loop over the feasible partitions U , M , L until we find a solution to (7) that satisfies (8). Since we know that 0 \u2264 |U | < k and k \u2264 |U |+ |M | \u2264 d, we can limit the search to (k\u22121)(d\u2212k+ 1) iterations in the worst case, where each iteration requires a constant number of operations. For the biased projection, we leave x = 0 as the fallback case as Lemma 4 gives only a sufficient condition. This yields a runtime complexity of O(d log(d) + kd), which is comparable to simplex projection algorithms based on sorting.\n4.3 Projection onto the Top-k Simplex\nAs we argued in \u00a7 4.1, the (biased) projection onto the top-k simplex becomes either the knapsack problem or the (biased) projection onto the top-k cone depending on the constraint \u30081, x\u3009 \u2264 r at the optimum. The following Lemma provides a way to check which of the two cases apply.\nLemma 5. Let x\u2217 \u2208 Rd be the solution to the following optimization problem\nmin x {\u2016a\u2212 x\u20162 + \u03c1 \u30081, x\u30092 | \u30081, x\u3009 \u2264 r, 0 \u2264 xi \u2264 1k \u30081, x\u3009 , i \u2208 [d]},\nlet (t, u) be the optimal thresholds such that x\u2217 = min{max{0, a\u2212 t}, u}, and let U be defined as in Lemma 3. Then it must hold that \u03bb = t+ pk \u2212 \u03c1r \u2265 0, where p = \u2211 i\u2208U ai \u2212 |U | (t+ u).\nProof. As in Lemma 3, we consider an equivalent problem\nmin x,s { 12 \u2016a\u2212 x\u2016 2 + 12\u03c1s 2 | \u30081, x\u3009 = s, s \u2264 r, 0 \u2264 xi \u2264 sk , i \u2208 [d]}.\nLet t, \u03bb \u2265 0, \u00b5i \u2265 0, \u03bdi \u2265 0 be the dual variables, and let L be the Lagrangian:\nL = 12 \u2016a\u2212 x\u2016 2 + 12\u03c1s 2 + t(\u30081, x\u3009 \u2212 s) + \u03bb(s\u2212 r)\u2212 \u3008\u00b5, x\u3009+ \u2329 \u03bd, x\u2212 sk1 \u232a .\nFrom the KKT conditions, we have that\n\u2202xL = x\u2212 a+ t1\u2212 \u00b5+ \u03bd = 0, \u2202sL = \u03c1s\u2212 t+ \u03bb\u2212 1k \u30081, \u03bd\u3009 = 0, \u00b5ixi = 0, \u03bdi(xi \u2212 sk ) = 0, \u03bb(s\u2212 r) = 0.\nIf s < r, then \u03bb = 0 and we recover the top-k cone problem of Lemma 3. Otherwise, we have that s = r and \u03bb = t + 1k \u30081, \u03bd\u3009 \u2212 \u03c1r \u2265 0. The fact that \u03bdi = max{0, ai \u2212 t \u2212 u}, where u = r k , completes the proof.\nProjection. We can now use Lemma 5 to compute the (biased) projection onto \u2206k(r) as follows. First, we check the special cases of zero and constant projections, as we did before. If that fails, we proceed with the knapsack problem since it is faster to solve. Having the thresholds (t, u) and the partitioning into the sets U , M , L, we compute the value of \u03bb as given in Lemma 5. If \u03bb \u2265 0, we are done. Otherwise, we know that \u30081, x\u3009 < r and go directly to the general case 3 in Lemma 3.\n5 Optimization of Top-k Usunier Loss\nIn this section we show how the Usunier version of the top-k hinge loss (5) can be optimized using the Prox-SDCA framework from \u00a7 3. The two main ingredients that we discuss are the conjugate loss and the (biased) projection. It turns out that the only difference between the conjugate of the top-k hinge loss (3) introduced above and the conjugate of (5) are their effective domains. Proposition 5. A primal-conjugate pair for the top-k Usunier loss (5) is\n\u03c6\u0303k(a) = 1\nk k\u2211 j=1 max { 0, (a+ c)[j] } , \u03c6\u0303\u2217k(b) = { \u2212\u3008c, b\u3009 if b \u2208 \u2206\u0303k, +\u221e otherwise, (9)\nwhere\n\u2206\u0303k(r) , { x \u2223\u2223 \u30081, x\u3009 \u2264 r, 0 \u2264 xi \u2264 1k , i \u2208 [m]} .\nMoreover, \u03c6\u0303k(a) = max{\u3008a+ c, \u03bb\u3009 |\u03bb \u2208 \u2206\u0303k}.\nProof. The proof is similar to the proof of Proposition 2; the main step is as follows:\n\u03c6\u0303k(a) = min t,\u03be,h\n{ t+ 1k \u30081, \u03be\u3009 | \u03bej \u2265 hj \u2212 t, \u03bej \u2265 0, hj \u2265 aj + cj , hj \u2265 0 } = max\n\u03bb\n{ \u3008a+ c, \u03bb\u3009 | \u30081, \u03bb\u3009 \u2264 1, 0 \u2264 \u03bbj \u2264 1k } .\nNote that the upper bounds on xi\u2019s are now fixed to 1/k, which means the Euclidean projection onto the set \u2206\u0303k is an instance of the continuous quadratic knapsack problem from \u00a7 4.1. Unfortunately, the proximal step in the SDCA framework corresponds to a biased projection where there is an additional `2 regularizer on the sum \u30081, x\u3009 coming from the regularizer in the training objective. To address this issue, we follow the derivation given in the proofs of Lemmas 3 and 5.\nThe update step for the top-k Usunier loss (5) is equivalent to (with l = 0 and u = 1/k):\nmin x,s { 12 \u2016a\u2212 x\u2016 2 + 12\u03c1s 2 | \u30081, x\u3009 = s, s \u2264 r, l \u2264 xi \u2264 u, i \u2208 [d]}.\nLet t, \u03bb \u2265 0, \u00b5i \u2265 0, \u03bdi \u2265 0 be the dual variables, and let L be the Lagrangian:\nL = 12 \u2016a\u2212 x\u2016 2 + 12\u03c1s 2 + t(\u30081, x\u3009 \u2212 s) + \u03bb(s\u2212 r)\u2212 \u3008\u00b5, l1\u2212 x\u3009+ \u3008\u03bd, x\u2212 u1\u3009 .\nFrom the KKT conditions, we have that\n\u2202xL = x\u2212 a+ t1\u2212 \u00b5+ \u03bd = 0, \u2202sL = \u03c1s\u2212 t+ \u03bb = 0, \u00b5i(l \u2212 xi) = 0, \u03bdi(xi \u2212 u) = 0, \u03bb(s\u2212 r) = 0,\nwhich then leads to\nx = a\u2212 t1 + \u00b5\u2212 \u03bd = min{max{l, x\u2212 t}, u}, \u03bb = t\u2212 \u03c1s. Now, we can do case distinction based on the sign of \u03bb. If \u03bb > 0, then \u30081, x\u3009 = s = r and t > \u03c1r. In this case 12\u03c1s 2 = 12\u03c1r 2 \u2261 const, therefore this term can be ignored and we get the knapsack problem from \u00a7 4.1. Otherwise, if s < r, then \u03bb = 0 and t = \u03c1s. Using the index sets U , M and L as in Lemma 3, we have that\nt = \u03c1 (\u2211\nL l + \u2211 M (ai \u2212 t) + \u2211 U u ) = \u03c1 ( l |L|+ u |U | \u2212 t |M |+ \u2211 M ai ) .\nSolving for t with \u03c1 > 0, we obtain that t = ( l |L|+ u |U |+ \u2211 M ai ) / (1 \u03c1 + |M | ) . (10)\nProjection. To compute the (biased) projection, we follow the same steps as in \u00a7 4.3. First, we solve the knapsack problem using the algorithm of [14], which also computes the dual variable t. If t > \u03c1r, then we are done; otherwise, we sort a and loop over the feasible index sets U , M , and L. We stop once we find a t that satisfies (10) and is compatible with the corresponding index sets."}, {"heading": "6 Experimental Results", "text": "We have two main goals in the experiments. First, we show that the (biased) projection onto the top-k simplex is scalable and comparable to an efficient algorithm [14] for the simplex projection. Second, we show that the top-k multiclass SVM using both versions of the top-k hinge loss (3) and (5), denoted top-k SVM\u03b1 and top-k SVM\u03b2 respectively, leads to improvements in top-k accuracy consistently over all datasets and choices of k. In particular, we note improvements compared to the multiclass SVM of Crammer and Singer [6], which corresponds to top-1 SVM\u03b1/top-1 SVM\u03b2 . We release our implementation of the projection procedures and both SDCA solvers as a C++ library2 with a Matlab interface.\n2https://github.com/mlapin/libsdca\n6.1 Scaling of the Projection onto the Top-k Simplex\nWe follow the experimental setup of [18]. We sample 1000 points from the normal distribution N (0, 1) and solve the projection problems using the algorithm of [14] (denoted as Knapsack) and using our proposed method of projecting onto the set \u2206k for different values of k = 1, 5, 10. We report the total CPU time taken on a single Intel(R) Xeon(R) 2.20GHz processor. As one can see, the scaling is linear in the problem dimension and the run times are essentially the same."}, {"heading": "6.2 Image Classification Experiments", "text": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167). For Caltech, d = 784, and for the others d = 4096. The results on the two large scale datasets are in the supplement.\nWe cross-validate hyper-parameters in the range 10\u22125 to 103, extending it when the optimal value is at the boundary. We use LibLinear [8] for SVMOVA, SVMPerf [12] with the corresponding loss function for Recall@k, and the code provided by [17] for TopPush. When a ranking method like Recall@k and TopPush does not scale to a particular dataset using the reduction of the multiclass to a binary problem discussed in \u00a7 2.3, we use the one-vs-all version of the corresponding method. We implemented Wsabie++ (denoted W++,Q/m) based on the pseudo-code from Table 3 in [10]. Among the baseline methods that we tried, only TopPushOVA scaled to the Places and the ImageNet datasets both time and memory-wise3.\nOn Caltech 101, we use features provided by [29]. For the other datasets, we extract CNN features of a pre-trained CNN (fc7 layer after ReLU). For the scene recognition datasets, we use the Places 205 CNN [33] and for ILSVRC 2012 we use the Caffe reference model [11].\nExperimental results are given in Tables 1, 2. First, we note that our method is scalable to large datasets with millions of training examples, such as Places 205 and ILSVRC 2012 (results in the supplement). Second, we observe that optimizing the top-k hinge loss (both versions) yields consistently better top-k performance. This might come at the cost of a decreased top-1 accuracy (e.g. on MIT Indoor 67), but, interestingly, may also result in a noticeable increase in the top-1 accuracy on larger datasets like Caltech 101 Silhouettes and SUN 397. This resonates with our argumentation that optimizing for top-k is often more appropriate for datasets with a large number of classes.\nOverall, we get systematic increase in top-k accuracy over all datasets that we examined. For example, we get the following improvements in top-5 accuracy with our top-10 SVM\u03b1 compared to top-1 SVM\u03b1: +2.6% on Caltech 101, +1.2% on MIT Indoor 67, and +2.5% on SUN 397."}, {"heading": "7 Conclusion", "text": "We demonstrated scalability and effectiveness of the proposed top-k multiclass SVM on five image recognition datasets leading to consistent improvements in top-k performance. In the future, one could study if the top-k hinge loss (3) can be generalized to the family of ranking losses [30]. Similar to the top-k loss, this could lead to tighter convex upper bounds on the corresponding discrete losses."}], "references": [{"title": "Solving multiclass support vector machines with LaRank", "author": ["A. Bordes", "L. Bottou", "P. Gallinari", "J. Weston"], "venue": "ICML, pages 89\u201396", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Convex Analysis and Nonlinear Optimization: Theory and Examples", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": "Cms Books in Mathematics Series. Springer Verlag", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "The tradeoffs of large scale learning", "author": ["O. Bousquet", "L. Bottou"], "venue": "NIPS, pages 161\u2013168", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Superpixel segmentation based structural scene recognition", "author": ["S. Bu", "Z. Liu", "J. Han", "J. Wu"], "venue": "MM, pages 681\u2013684. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["K. Crammer", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 2:265\u2013292", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Mid-level visual element discovery as discriminative mode seeking", "author": ["C. Doersch", "A. Gupta", "A.A. Efros"], "venue": "NIPS, pages 494\u2013502", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research, 9:1871\u20131874", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-scale orderless pooling of deep convolutional activation features", "author": ["Y. Gong", "L. Wang", "R. Guo", "S. Lazebnik"], "venue": "ECCV", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Training highly multiclass classifiers", "author": ["M.R. Gupta", "S. Bengio", "J. Weston"], "venue": "JMLR, 15:1461\u20131492", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "arXiv preprint arXiv:1408.5093", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "ICML, pages 377\u2013384", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Blocks that shout: distinctive parts for scene classification", "author": ["M. Juneja", "A. Vedaldi", "C. Jawahar", "A. Zisserman"], "venue": "CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Variable fixing algorithms for the continuous quadratic knapsack problem", "author": ["K. Kiwiel"], "venue": "Journal of Optimization Theory and Applications, 136(3):445\u2013458", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Convolutional network features for scene recognition", "author": ["M. Koskela", "J. Laaksonen"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 1169\u20131172. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Scalable multitask representation learning for scene classification", "author": ["M. Lapin", "B. Schiele", "M. Hein"], "venue": "CVPR", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Top rank optimization in linear time", "author": ["N. Li", "R. Jin", "Z.-H. Zhou"], "venue": "NIPS, pages 1502\u20131510", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient euclidean projections in linear time", "author": ["J. Liu", "J. Ye"], "venue": "ICML, pages 657\u2013664", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Minimizing the sum of the k largest functions in linear time", "author": ["W. Ogryczak", "A. Tamir"], "venue": "Information Processing Letters, 85(3):117\u2013122", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "A survey on the continuous nonlinear resource allocation problem", "author": ["M. Patriksson"], "venue": "European Journal of Operational Research, 185(1):1\u201346", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Algorithms for the continuous nonlinear resource allocation problem \u2013 new implementations and numerical studies", "author": ["M. Patriksson", "C. Str\u00f6mberg"], "venue": "European Journal of Operational Research, 243(3):703\u2013 722", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["K.B. Petersen", "M.S. Pedersen"], "venue": "The matrix cookbook. Technical University of Denmark, 450:7\u201315", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A. Torralba"], "venue": "CVPR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "arXiv preprint arXiv:1403.6382", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "and L", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg"], "venue": "Fei-Fei. ImageNet Large Scale Visual Recognition Challenge", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Image classification with the Fisher vector: theory and practice", "author": ["J. S\u00e1nchez", "F. Perronnin", "T. Mensink", "J. Verbeek"], "venue": "IJCV, pages 1\u201324", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Mathematical Programming, pages 1\u201341", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning discriminative part detectors for image classification and cosegmentation", "author": ["J. Sun", "J. Ponce"], "venue": "ICCV, pages 3400\u20133407", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic n-choose-k models for classification and ranking", "author": ["K. Swersky", "B.J. Frey", "D. Tarlow", "R.S. Zemel", "R.P. Adams"], "venue": "NIPS, pages 3050\u20133058", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Ranking with ordered weighted pairwise classification", "author": ["N. Usunier", "D. Buffoni", "P. Gallinari"], "venue": "ICML, pages 1057\u20131064", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Wsabie: scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "IJCAI, pages 2764\u20132770", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "SUN database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "CVPR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "NIPS", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 31, "context": "Figure 1: Images from SUN 397 [32] illustrating class ambiguity.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "As the number of classes increases, two important issues emerge: class overlap and multilabel nature of examples [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 5, "context": "We propose top-k multiclass SVM as a generalization of the well-known multiclass SVM [6].", "startOffset": 85, "endOffset": 88}, {"referenceID": 29, "context": "While it turns out to be similar to a top-k version of the ranking based loss proposed by [30], we show that the top-k hinge loss is a lower bound on their version and is thus a tighter bound on the top-k zero-one loss.", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "We propose an efficient implementation based on stochastic dual coordinate ascent (SDCA) [27].", "startOffset": 89, "endOffset": 93}, {"referenceID": 32, "context": "5 million examples and 205 classes [33].", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "Finally, extensive experiments on several challenging computer vision problems show that top-k multiclass SVM consistently improves in top-k error over the multiclass SVM (equivalent to our top-1 multiclass SVM), one-vs-all SVM and other methods based on different ranking losses [12, 17].", "startOffset": 280, "endOffset": 288}, {"referenceID": 16, "context": "Finally, extensive experiments on several challenging computer vision problems show that top-k multiclass SVM consistently improves in top-k error over the multiclass SVM (equivalent to our top-1 multiclass SVM), one-vs-all SVM and other methods based on different ranking losses [12, 17].", "startOffset": 280, "endOffset": 288}, {"referenceID": 0, "context": "\u3008 w[1], x \u3009 \u2265 \u3008 w[2], x \u3009 \u2265 .", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "\u3008 w[1], x \u3009 \u2265 \u3008 w[2], x \u3009 \u2265 .", "startOffset": 17, "endOffset": 20}, {"referenceID": 5, "context": "In this section we review the multiclass SVM of Crammer and Singer [6] which will be extended to the top-k multiclass SVM in the following.", "startOffset": 67, "endOffset": 70}, {"referenceID": 26, "context": "We mainly follow the notation of [27].", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "Proposition 1 ([27], \u00a7 5.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "\u03c6(a) = max{0, (a+ c)[1]}, \u03c6\u2217(b) = { \u2212\u3008c, b\u3009 if b \u2208 \u2206, +\u221e otherwise.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "Note that thresholding with 0 in \u03c6(a) is actually redundant as (a+ c)[1] \u2265 (a+ c)yi = 0 and is only given to enhance similarity to the top-k version defined later.", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "Since the ground truth score (a+ c)[yi] = 0, we conclude that \u03c8k(a) > 0 \u21d0\u21d2 \u3008 w[1], xi \u3009 \u2265 .", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "where the sum of the k largest components is known to be convex [4].", "startOffset": 64, "endOffset": 67}, {"referenceID": 18, "context": "Lemma 1 ([19], Lemma 1).", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "Therefore, we see that the proposed formulation (3) naturally extends the multiclass SVM of Crammer and Singer [6], which is recovered when k = 1.", "startOffset": 111, "endOffset": 114}, {"referenceID": 29, "context": "[30] have recently formulated a very general family of convex losses for ranking and multiclass classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "The bound \u03c6k(a) \u2264 \u03c6\u0303k(a) holds with equality if (a+ c)[1] \u2264 0 or (a+ c)[k] \u2265 0.", "startOffset": 54, "endOffset": 57}, {"referenceID": 29, "context": "While [30] employed LaRank [1] and [10], [31] optimized an approximation of L\u03b2(a), we show in \u00a7 5 how the loss function (5) can be optimized exactly and efficiently within the Prox-SDCA framework.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "While [30] employed LaRank [1] and [10], [31] optimized an approximation of L\u03b2(a), we show in \u00a7 5 how the loss function (5) can be optimized exactly and efficiently within the Prox-SDCA framework.", "startOffset": 27, "endOffset": 30}, {"referenceID": 9, "context": "While [30] employed LaRank [1] and [10], [31] optimized an approximation of L\u03b2(a), we show in \u00a7 5 how the loss function (5) can be optimized exactly and efficiently within the Prox-SDCA framework.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "While [30] employed LaRank [1] and [10], [31] optimized an approximation of L\u03b2(a), we show in \u00a7 5 how the loss function (5) can be optimized exactly and efficiently within the Prox-SDCA framework.", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "We employ it in our experiments to evaluate the ranking based methods SVM [12] and TopPush [17].", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "We employ it in our experiments to evaluate the ranking based methods SVM [12] and TopPush [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "Another approach to general performance measures is given in [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "A convex upper bound on recall@k is then optimized in [12] via structured SVM.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "While being theoretically very elegant, the approach of [12] does not scale to very large datasets.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "Using rank-1 update of the Moore-Penrose pseudoinverse ([22], \u00a7 3.", "startOffset": 56, "endOffset": 60}, {"referenceID": 1, "context": "[2], Theorem 3.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "To complete the proof, we redefine A\u2190 1 \u03bbA for convenience, and use the first order optimality condition ([2], Ex.", "startOffset": 106, "endOffset": 109}, {"referenceID": 26, "context": "7 in [27] as there is a typo in the optimization problem (20) leading to the conclusion that ayi,i must be 0 at the optimum.", "startOffset": 5, "endOffset": 9}, {"referenceID": 26, "context": "1 for details} 10: W \u2190W + xi(ai \u2212 a i )> {rank-1 update} 11: end for 12: until relative duality gap is below As an optimization scheme, we employ the proximal stochastic dual coordinate ascent (Prox-SDCA) framework of Shalev-Shwartz and Zhang [27], which has strong convergence guarantees and is easy to adapt to our problem.", "startOffset": 243, "endOffset": 247}, {"referenceID": 2, "context": "Finally, we have a well-defined stopping criterion as we can compute the duality gap (see discussion in [3]).", "startOffset": 104, "endOffset": 107}, {"referenceID": 19, "context": "This is a well examined problem and several highly efficient algorithms are available (see the surveys [20, 21]).", "startOffset": 103, "endOffset": 111}, {"referenceID": 20, "context": "This is a well examined problem and several highly efficient algorithms are available (see the surveys [20, 21]).", "startOffset": 103, "endOffset": 111}, {"referenceID": 13, "context": "We choose [14] since it is easy to implement, does not require sorting, and scales linearly in practice.", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "If U 6= \u2205 and M = \u2205, then U = {[1], .", "startOffset": 31, "endOffset": 34}, {"referenceID": 13, "context": "First, we solve the knapsack problem using the algorithm of [14], which also computes the dual variable t.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "Top-1 [29] 62.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "1 - BLH [5] 48.", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "3 DGE [7] 66.", "startOffset": 6, "endOffset": 9}, {"referenceID": 23, "context": "87 RAS [24] 69.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "0 Top-2 [29] 61.", "startOffset": 8, "endOffset": 12}, {"referenceID": 27, "context": "4 - SP [28] 51.", "startOffset": 7, "endOffset": 11}, {"referenceID": 32, "context": "4 ZLX [33] 68.", "startOffset": 6, "endOffset": 10}, {"referenceID": 14, "context": "24 KL [15] 70.", "startOffset": 6, "endOffset": 10}, {"referenceID": 28, "context": "1 Top-5 [29] 60.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "4 - JVJ [13] 63.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "10 GWG [9] 68.", "startOffset": 7, "endOffset": 10}, {"referenceID": 11, "context": "Prec@k and Recall@k are SVM [12]; W++,Q/m is Wsabie [10] with an embedding dimension m and the queue size Q; in the first part, m = 101 for Caltech and m = 67 for Indoor.", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "Prec@k and Recall@k are SVM [12]; W++,Q/m is Wsabie [10] with an embedding dimension m and the queue size Q; in the first part, m = 101 for Caltech and m = 67 for Indoor.", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "First, we show that the (biased) projection onto the top-k simplex is scalable and comparable to an efficient algorithm [14] for the simplex projection.", "startOffset": 120, "endOffset": 124}, {"referenceID": 5, "context": "In particular, we note improvements compared to the multiclass SVM of Crammer and Singer [6], which corresponds to top-1 SVM\u03b1/top-1 SVM\u03b2 .", "startOffset": 89, "endOffset": 92}, {"referenceID": 31, "context": "SUN 397 (10 splits) Top-1 accuracy XHE [32] 38.", "startOffset": 39, "endOffset": 43}, {"referenceID": 15, "context": "0 LSH [16] 49.", "startOffset": 6, "endOffset": 10}, {"referenceID": 32, "context": "3 ZLX [33] 54.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "1 SPM [26] 47.", "startOffset": 6, "endOffset": 10}, {"referenceID": 8, "context": "2 GWG [9] 51.", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "98 KL [15] 54.", "startOffset": 6, "endOffset": 10}, {"referenceID": 32, "context": "ZLX [33] / BVLC [11] 50.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "ZLX [33] / BVLC [11] 50.", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "We follow the experimental setup of [18].", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "We sample 1000 points from the normal distribution N (0, 1) and solve the projection problems using the algorithm of [14] (denoted as Knapsack) and using our proposed method of projecting onto the set \u2206k for different values of k = 1, 5, 10.", "startOffset": 117, "endOffset": 121}, {"referenceID": 28, "context": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167).", "startOffset": 120, "endOffset": 124}, {"referenceID": 22, "context": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167).", "startOffset": 160, "endOffset": 164}, {"referenceID": 31, "context": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167).", "startOffset": 193, "endOffset": 197}, {"referenceID": 32, "context": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167).", "startOffset": 231, "endOffset": 235}, {"referenceID": 24, "context": "We evaluate our method on five image classification datasets of different scale and complexity: Caltech 101 Silhouettes [29] (m = 101, n = 4100), MIT Indoor 67 [23] (m = 67, n = 5354), SUN 397 [32] (m = 397, n = 19850), Places 205 [33] (m = 205, n = 2448873), and ImageNet 2012 [25] (m = 1000, n = 1281167).", "startOffset": 278, "endOffset": 282}, {"referenceID": 7, "context": "We use LibLinear [8] for SVM, SVM [12] with the corresponding loss function for Recall@k, and the code provided by [17] for TopPush.", "startOffset": 17, "endOffset": 20}, {"referenceID": 11, "context": "We use LibLinear [8] for SVM, SVM [12] with the corresponding loss function for Recall@k, and the code provided by [17] for TopPush.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "We use LibLinear [8] for SVM, SVM [12] with the corresponding loss function for Recall@k, and the code provided by [17] for TopPush.", "startOffset": 115, "endOffset": 119}, {"referenceID": 9, "context": "We implemented Wsabie (denoted W++,Q/m) based on the pseudo-code from Table 3 in [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "On Caltech 101, we use features provided by [29].", "startOffset": 44, "endOffset": 48}, {"referenceID": 32, "context": "For the scene recognition datasets, we use the Places 205 CNN [33] and for ILSVRC 2012 we use the Caffe reference model [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "For the scene recognition datasets, we use the Places 205 CNN [33] and for ILSVRC 2012 we use the Caffe reference model [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 29, "context": "In the future, one could study if the top-k hinge loss (3) can be generalized to the family of ranking losses [30].", "startOffset": 110, "endOffset": 114}], "year": 2015, "abstractText": "Class ambiguity is typical in image classification problems with a large number of classes. When classes are difficult to discriminate, it makes sense to allow k guesses and evaluate classifiers based on the top-k error instead of the standard zero-one loss. We propose top-k multiclass SVM as a direct method to optimize for top-k performance. Our generalization of the well-known multiclass SVM is based on a tight convex upper bound of the top-k error. We propose a fast optimization scheme based on an efficient projection onto the top-k simplex, which is of its own interest. Experiments on five datasets show consistent improvements in top-k accuracy compared to various baselines.", "creator": "LaTeX with hyperref package"}}}