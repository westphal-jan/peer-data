{"id": "1511.06961", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2015", "title": "On the Linear Algebraic Structure of Distributed Word Representations", "abstract": "successful in constructing this this work, though we leverage the linear group algebraic structure typical of reasonably distributed nonlinear word representations to explicitly automatically broadly extend knowledge bases already and so allow a computing machine to thoroughly learn new descriptive facts about the world. our goal is to extract equally structured speech facts starting from corpora in a simpler query manner, without applying classifiers symbolic or semantic patterns, and using only the mathematical co - occurrence statistics of words. simultaneously we demonstrate that the linear algebraic structure of differential word embeddings can be used to reduce data analytic requirements for additional methods of explicitly learning complicated facts., in particular, we just demonstrate by that phrase words seemingly belonging to, a common speech category, or pairs of same words satisfying a certain relation, form a predicted low - rank subspace in shaping the projected constraint space. we compute a basis for transforming this low - rank subspace using singular binary value decomposition ( like svd ), two then use this basis to discover new facts differently and to fit vectors for less particularly frequent sequence words are which since we do must not certain yet have vectors for.", "histories": [["v1", "Sun, 22 Nov 2015 04:28:39 GMT  (355kb,D)", "http://arxiv.org/abs/1511.06961v1", "55 pages"]], "COMMENTS": "55 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lisa seung-yeon lee"], "accepted": false, "id": "1511.06961"}, "pdf": {"name": "1511.06961.pdf", "metadata": {"source": "META", "title": "On the Linear Algebraic Structure of Distributed Word Representations", "authors": ["Lisa Seung-Yeon Lee", "Lisa Lee"], "emails": [], "sections": [{"heading": null, "text": "On the Linear Algebraic Structure of Distributed Word Representations\nLisa Seung-Yeon Lee\nAdvised by Professor Sanjeev Arora\nMay \nA thesis submitted to the Princeton University Department of Mathematics in partial fulfillment of the requirements for the degree of Bachelor of Arts.\nar X\niv :1\n51 1.\n06 96\n1v 1\n[ cs\n.C L\n] 2\n2 N\nov 2\n01 5"}, {"heading": "Abstract", "text": "In this work, we leverage the linear algebraic structure of distributed word representations to automatically extend knowledge bases and allow a machine to learn new facts about the world. Our goal is to extract structured facts from corpora in a simpler manner, without applying classifiers or patterns, and using only the co-occurrence statistics of words. We demonstrate that the linear algebraic structure of word embeddings can be used to reduce data requirements for methods of learning facts. In particular, we demonstrate that words belonging to a common category, or pairs of words satisfying a certain relation, form a low-rank subspace in the projected space. We compute a basis for this low-rank subspace using singular value decomposition (SVD), then use this basis to discover new facts and to fit vectors for less frequent words which we do not yet have vectors for.\nThis thesis represents my own work in accordance with university regulations.\nLisa Lee"}, {"heading": "Contents", "text": " Introduction  . Distributed word representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Extending existing knowledge bases . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Overview of the paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Word embeddings  . Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Methods for learning word vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n.. Skip-gram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .. GloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .. Squared Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n. Justification for why the word embedding methods work . . . . . . . . . . . . . . . . .  .. Justification for explicit, high-dimensional word embeddings . . . . . . . . . .  .. Justification for low-dimensional embeddings . . . . . . . . . . . . . . . . . . .  .. Motivation behind the Squared Norm objective . . . . . . . . . . . . . . . . . . \n Methods  . Training the word vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Preprocessing the corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Categories and relations  . Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Obtaining training examples of categories and relations . . . . . . . . . . . . . . . . .  . Experiments in the subsequent chapters . . . . . . . . . . . . . . . . . . . . . . . . . . \n Low-rank subspaces of categories and relations  . Computing a low-rank basis using SVD . . . . . . . . . . . . . . . . . . . . . . . . . . \n.. Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Extending a knowledge base  . Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Learning new words in a category . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n.. Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n. Learning new word pairs in a relation . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .. Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .. Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .. Varying levels of difficulty for different relations . . . . . . . . . . . . . . . . .  . Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Learning vectors for less frequent words  . Learning a new vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n.. Motivation behind the optimization objective . . . . . . . . . . . . . . . . . . .  . Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n.. Order and cosine score of the learned vector . . . . . . . . . . . . . . . . . . . .  .. Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Using an external knowledge source to reduce false-positive rate  . Analogy queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Wordnet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Conclusion "}, {"heading": "Acknowledgements", "text": "First and foremost, I would like to thank Professor Sanjeev Arora for his patient guidance, advice, and support during the planning and development of this research work. I also would like to express my very great appreciation to Dr. Yingyu Liang, without whom I could not have completed this research project; thank you so much for the time and effort you took to help me run experiments and understand concepts, and for suggesting new ideas and directions to take when I felt stuck. I would also like to deeply thank Tengyu Ma for his valuable ideas and suggestions during this research project.\nI am also greatly indebted to Ming-Yee Tsang, who helped me format my thesis, and debug the complicated regular expressions code for preprocessing the huge Wikipedia corpus (see Section .). Many thanks to Victor Luu, Irene Lo, Eliott Joo, Christina Funk, and Ante Qu for proofreading my thesis and providing invaluable feedback.\nChanning, thank you so much for keeping me company while I was writing this thesis, for impromptu coffee runs in the middle of the night so that I wouldn\u2019t fall asleep, for always motivating and encouraging me, and a myriad other things.\nI would also like to thank my wonderful friends and famiLee for their support. Mimi, thank you for being the silliest, kindest, coolest, most patient sister and best friend that you are to me. Joonhee, you will always be my cute little brother no matter how old you are. Thank you for all the fun Naruto missions we went on when you were little, for all your hilarious jokes, and for playing LoL/Pokemon/Minecraft with me. Happy, I love you too. Too bad you can\u2019t read this. Woof woof. Thank you Laon for always being by my side since I was five. Thank you umma and abba for raising me and my siblings (and Happy), and for always encouraging me to try my best.\nBilly, thanks for always challenging me to think more rigorously, and for all the fun musicmaking we did (yay Brahms, Rachmaninoff, Saint-Saens, Chopin, Piazzolla, Hisaishi, Pokemon). I am also extremely grateful to have found such a loving community in Manna Christian Fellowship during my freshman year.\nLast but not least, thank you God for giving me these four precious years at Princeton University to meet all these wonderful people and to study math."}, {"heading": "Chapter ", "text": ""}, {"heading": "Introduction", "text": ". Distributed word representations\nDistributed representations of words in a vector space represent each word with a real-valued vector, called a word vector. They are also known as word embeddings because they embed an entire vocabulary into a relatively low-dimensional linear space whose dimensions are latent continuous features. One of the earliest ideas of distributed representations dates back to  [], and has since been applied to statistical language modeling with considerable success. These word vectors have shown to improve performance in a variety of natural language processing tasks including automatic speech recognition [], information retrieval [], document classification [], and parsing [].\nThe word vectors are trained over large corpora typically in a totally unsupervised manner, using the co-occurrence statistics of words. Past methods to obtain word embeddings include matrix factorization methods [], variants of neural networks [, , , , , , ], and energybased models [, ]. The learned word vectors explicitly capture many linguistic regularities and patterns, such as semantic and syntactic attributes of words. Therefore, words that appear in similar contexts, or belong to a common \u201ccategory\u201d (e.g., country names, composer names, or university names), tend to form a cluster in the projected space.\nRecently, Mikolov et al. [] demonstrated that word embeddings created by a recurrent neural net (RNN) and by a related energy-based model called wordvec exhibit an additional linear structure which captures the relation between pairs of words, and allows one to solve analogy queries such as \u201cman:woman::king:??\u201d using simple vector arithmetics. More specifically, \u201cqueen\u201d happens to be the word whose vector vqueen is the closest approximation to the vector vwoman \u2212 vman + vking. Other subsequent works [, , ] produced word vectors that can be used to solve analogy queries in the same way. It remains a mystery as to why these radically different embedding methods, including highly non-linear ones, produce vectors exhibiting similar linear structure. A summary of current justifications for this phenomenon is provided in Section .. A corpus (plural corpora) is a large and structured set of unlabeled texts. We say two words co-occur in a corpus if they appear together within a certain (fixed) distance in the text.\nOn the Linear Structure of Word Embeddings Chapter . Introduction | \n. Extending existing knowledge bases\nIn this work, we aim to leverage the linear algebraic structure of word embeddings to extend knowledge bases and learn new facts. Knowledge bases such as Wordnet [] or Freebase [] are a key source for providing structured information about general human knowledge. Building such knowledge bases, however, is an extremely slow and labor-intensive process. Consequently, there has been much interest in finding methods for automatically learning new facts and extending knowledge bases, e.g., by applying patterns or classifiers on large corpora [, , ]. Carlson et al.\u2019s NELL (Never-Ending Language Learning) system [], for instance, extracts structured facts from the web to build a knowledge base, using over  different classifiers and extraction methods in combination with a large-scale semi-supervised multi-task learning algorithm.\nOur goal is to extract structured facts from corpora in a simpler manner, without applying classifiers or patterns, and using only the co-occurrence statistics of words. More specifically, we use the word co-occurrence statistics to produce word vectors, and then leverage their linear structure to learn new facts, such as new words belonging to a known category, or new pairs of words satisfying a known relation (see Chapter ). Our methods can supplement other methods for extending knowledge bases to reduce false positive rate, or narrow down the search space for discovering new facts.\n. Overview of the paper\nIn this paper, we will demonstrate that the linear algebraic structure of word embeddings can be used to reduce data requirements for methods of learning facts. In Chapter , we present a few methods for learning word vectors, and provide intuition as to why the embedding methods work. Chapter  describes how the word vectors used in our experiments were trained. Chapter  introduces the notion of categories and relations, which can be used to represent facts about the world in a knowledge base. In Chapter , we explore the linear algebraic structure of word embeddings. In particular, we demonstrate that words belonging to a common category, or pairs of words satisfying a certain relation, form a low-rank subspace in the projected space. We compute a basis for this low-rank subspace using singular value decomposition (SVD), then use this basis to discover new facts (Chapter ) and to fit vectors for less frequent words which we do not yet have vectors for (Chapter ). We also demonstrate that, using an external knowledge source such as Wordnet [], one can improve accuracy on analogy queries of the form \u201ca:b::c:??\u201d (Chapter ).\nA knowledge base is a collection of information that represents facts about the world."}, {"heading": "Chapter ", "text": ""}, {"heading": "Word embeddings", "text": "In this chapter, we introduce the reader to recent word embedding methods, and provide justifications for why these methods work.\nIn Section ., we present three different methods for producing word vectors that exhibit the desired linear properties: Mikolov et al.\u2019s skip-gram with negative sampling (SGNS) method [], Pennington et al.\u2019s GloVe method [], and Arora et al.\u2019s Squared Norm (SN) objective []. In Section ., we provide a summary of current justifications for why these methods work. For further details and evaluations of these methods, see [, , ].\nThe three methods presented in Section . achieve similar, state-of-the-art performance on analogy query tasks. In our experiments, we use the SN objective (.) to train the word vectors because it is perhaps the simplest method thus far for fitting word embeddings, and it is also the only method out of the three which provably finds the near-optimum fit\uf732 (see Arora et al. []).\n. Notation\nWe first introduce some notation. Let D be the set of distinct words that appear in a corpus C; then we say D is a set of vocabulary words that appear in C. We can enumerate the sequence of words (or tokens) in C as w1,w2, . . . ,w|C|, where |C| is the total number of tokens in C. Let k \u2208N be fixed; then the context window of size k around a word wi \u2208 C is the multiset consisting of the k tokens appearing before and after wi in the corpus,\nwindowk(wi) := {wi\u2212k ,wi\u2212k+1, . . . ,wi\u22121} \u222a {wi+1,wi+2, . . . ,wi+k}.\nTypically, the context window size k is chosen to be a fixed number between 5 and 10. For a vocabulary word w \u2208 D, let \u03ba(w) := \u22c3\ni\u2208{1,...,|C|}: wi=w\nwindowk(wi)\nbe the set of all tokens appearing in some context window around w. Each distinct word \u03c7 \u2208 \u03ba(w) is called a context word for w. Let Dcontext be the set of all context words; that is, Dcontext is the That is, allowing one to solve analogy queries using linear algebraic vector arithmetics. (according to their generative model for text corpora described in their paper []) This is just one way of defining context, and other types of contexts can be considered; see [].\nOn the Linear Structure of Word Embeddings Chapter . Word embeddings | \nset of all distinct words in \u222aw\u2208D\u03ba(w). Note that, because of the way we define context, we have Dcontext = D and the distinction between a word and a context word is arbitrary, i.e., we are free to interchange the two roles.\nFor two words w,w\u2032 \u2208 D, let\nXww\u2032 := |{wi \u2208 \u03ba(w) : wi = w\u2032}|\ni.e., Xww\u2032 is the number of times word w\u2032 appears in any context window around w. Then Xw :=\u2211 w\u2032 Xww\u2032 = |\u03ba(w)|, and p(w\u2032 | w) := Xww\u2032 Xw is the empirical probability that word w\u2032 appears in some\ncontext window around w (i.e., w\u2032 is a context word for w). Also, p(w) := Xw\u2211 w\u2032 Xw\u2032\nis the empirical probability that a randomly selected word of the corpus is w. The matrix X whose rows and columns are indexed by the words inD, and whose entries are Xww\u2032 , is called the word co-occurrence matrix of C.\nIn this paper, let d \u2208N be the dimension of the word vectors vw \u2208Rd . The word co-occurrence statistics are used to train the word vectors vw \u2208Rd for words w \u2208 D.\n. Methods for learning word vectors\nBelow, we present a few methods for obtaining word embeddings which allow one to solve analogy queries using linear algebraic vector arithmetics. Other methods include large-dimensional embeddings that explicitly encode co-occurrence statistics [] (see Section .) and noise-contrastive estimation [].\n.. Skip-gram\nFor a word w \u2208 D and a context \u03c7 \u2208 Dcontext, we say the pair (w,\u03c7) is observed in the corpus and write (w,\u03c7) \u2208 C, if \u03c7 appears in some context window around w (i.e., \u03c7 \u2208 \u03ba(w)). In Mikolov et al.\u2019s skip-gram with negative sampling (SGNG) model [, ], the probability that a word-context pair (w,\u03c7) is observed in the corpus is parametrized by\np(w,\u03c7) = 1 1 + exp ( \u2212vw \u00b7 v\u03c7 ) , where vw,v\u03c7 \u2208 Rd are the vectors for w and \u03c7 respectively. SGNS tries to maximize p(w,\u03c7) for observed (w,\u03c7) pairs in the corpus C, while minimizing p(w,\u03c7) for randomly sampled \u201cnegative\u201d samples (w,\u03c7). Their optimization objective is the log-likelihood,\nargmax {vw :w\u2208D}  \u220f (w,\u03c7)\u2208C p(w,\u03c7)   \u220f (w,\u03c7)<C (1\u2212 p(w,\u03c7))  =argmax {vw :w\u2208D} \u2211 (w,\u03c7)\u2208C logp(w,\u03c7) + \u2211 (w,\u03c7)<C log(1\u2212 p(w,\u03c7)) ,\nThe dimension d of the word vectors is a parameter that can be chosen, and is typically much smaller than the number of vocabulary words |D| or the size of the corpus |C|. In the three methods presented in Section ., a dimension between 50 and 300 is used. We assume that the randomly generated negative samples (w,\u03c7) are not observed in the corpus.\nOn the Linear Structure of Word Embeddings Chapter . Word embeddings | \nwhere argmaxx f (x) := {x : f (x) \u2265 f (y) \u2200y} is the set of arguments for which the given function f attains its maximum value.\n.. GloVe\nPennington et al.\u2019s GloVe (\u201cGlobal Vectors for Word Representation\u201d) method [] fits, for each word w \u2208 D, two low-dimensional vectors vw, v\u0303w \u2208 Rd and scalars bw, b\u0303w \u2208 R so as to minimize the cost function \u2211\nw,w\u2032 f (Xww\u2032 )\n( vw \u00b7 v\u0303w\u2032 + bw + b\u0303w\u2032 \u2212 logXww\u2032 )2 , (.)\nwhere f (x) = min {(\nx xmax\n)\u03b1 ,1 } for some constants xmax and \u03b1. In their experiments, they use \u03b1 = 0.75\nand xmax = 100. The purpose of the weighing function f is twofold:\n\u2022 f (x) is non-decreasing, so that rare co-occurrences (which tend to have greater noise) are not overweighted.\n\u2022 f (x) is relatively small for large values of x, so that frequent co-occurrences are not overweighted.\nFor the motivation behind the GloVe objective (.), we refer the reader to their paper [].\n.. Squared Norm\nArora et al.\u2019s Squared Norm (SN) method [] fits a scalar Z \u2208 R and, for each word w \u2208 D, a vector vw \u2208Rd so as to minimize the objective\u2211\nw,w\u2032 f (Xww\u2032 )\n( log(Xw,w\u2032 )\u2212 \u2016vw + vw\u2032\u20162 \u2212Z )2 , (.)\nwhere f is the same weighting function as in the GloVe objective (.). For the motivation behind the SN objective (.), see Section ...\n. Justification for why the word embedding methods work\nOnce the word vectors have been produced, one can answer analogy queries of the form \u201ca:b::c:??\u201d by finding the word d\u0303 whose vector is closest to vb \u2212 va + vc according to cosine similarity, i.e.,\nd\u0303 = argmax d vd \u00b7 (vb \u2212 va + vc)\n= argmin d (va \u2212 vb \u2212 vc) \u00b7 vd\n= argmin d\n2(va \u2212 vb \u2212 vc) \u00b7 vd + \u2016va \u2212 vb \u2212 vc\u20162 + \u2016vd\u20162\n= argmin d \u2016va \u2212 vb \u2212 vc + vd\u20162 (.)\nIn this paper, \u2016\u00b7\u2016 refers to Euclidean norm, or \u2016\u00b7\u20162.\nOn the Linear Structure of Word Embeddings Chapter . Word embeddings | \nwhere the vectors vw for each word w have been normalized so that \u2016vw\u2016 = 1. It remains a mystery as to why such vastly different embedding methods, including highly nonlinear ones, produce vectors exhibiting similar linear structure, and achieve fairly similar accuracy on analogy queries. In the rest of this chapter, we provide a summary of the current justifications for this phenomenon.\n.. Justification for explicit, high-dimensional word embeddings\nLevy and Goldberg [] and Pennington et al. [] provide a statistical intuition as to why the answer to the analogy \u201cman:woman::king:??\u201d must be \u201cqueen\u201d. The reason is that most contexts \u03c7 \u2208 Dcontext satisfy\np(\u03c7 |man) p(\u03c7 |woman) \u2248 p(\u03c7 | king) p(\u03c7 | queen)\nwhere p(\u03c7 | w) is the conditional probability that \u03c7 appears in some context window around word w. For example, both ratios are around  for most contexts (e.g., \u201csleep\u201d, \u201cthe\u201d, \u201cfood\u201d), but the ratio deviates from when \u03c7 is not gender-neutral (e.g., \u201cdress\u201d, \u201che\u201d, \u201cshe\u201d, \u201cElizabeth\u201d, \u201cHenry\u201d). Therefore, a reasonable answer to the analogy query \u201ca:b::c:??\u201d is the word d that minimizes\u2211\n\u03c7\u2208Dcontext\n( log\np(\u03c7 | a) p(\u03c7 | b) \u2212 log p(\u03c7 | c) p(\u03c7 | d)\n)2 . (.)\nHence, Levy and Goldberg [] proposed a very high-dimensional embedding that explicitly encodes correlation statistics between words and contexts: The vector for word w \u2208 D is given by vw \u2208 R|Dcontext |, where vw is indexed by all possible contexts \u03c7 \u2208 Dcontext, and the entry (vw)\u03c7 in coordinate \u03c7 is equal to\nPMI(w,\u03c7) := log p(w,\u03c7) p(w)p(\u03c7) = log p(\u03c7 | w) p(\u03c7) . (.)\nNote that with this word embedding, the objective (.) is equivalent to (.):\nmin d \u2016va \u2212 vb \u2212 vc + vd\u201622 = min d \u2211 \u03c7 ( log p(\u03c7 | a) p(\u03c7) \u2212 log p(\u03c7 | b) p(\u03c7) \u2212 log p(\u03c7 | c) p(\u03c7) + log p(\u03c7 | d) p(\u03c7) )2 = min\nd \u2211 \u03c7 ( log p(\u03c7 | a) p(\u03c7 | b) \u2212 log p(\u03c7 | c) p(\u03c7 | d) )2 .\nAnd indeed, Levy and Goldberg [] show that the explicit word embeddings solve analogies via linear algebraic queries empirically.\n.. Justification for low-dimensional embeddings\nHowever, the above justification only applies to large-dimensional embeddings that explicitly encode correlation statistics between words and contexts. Recently, Arora et al. [] gave a justification for why low-dimensional embeddings are successful in solving analogy queries. More specifically, they postulate that the PMI matrix and the word vectors vw \u2208Rd satisfy the following properties: The second to last equality follows because \u2016vd\u20162 = 1 is a constant. The PMI (pointwise mutual information) matrix is a |D| \u00d7 |Dcontext|matrix whose rows are indexed by words w \u2208 D and columns are indexed by contexts \u03c7 \u2208 Dcontext, and whose entries are PMI(w,\u03c7) as defined in (.).\nOn the Linear Structure of Word Embeddings Chapter . Word embeddings | \nProperty A. The PMI matrix can be approximated by a positive semidefinite matrix of fairly low rank, which is closer to logn than to n. This yields natural word embeddings vw that are implicit in the co-occurrence data itself: PMI(w,w\u2032) \u2248 vw \u00b7 vw\u2032 .\nProperty B. The word vectors vw are approximately isotropic meaning the expectation Ew[vwvTw ] is approximately like the identity matrix I , in that every one of its eigenvalues lies in [1,1 + \u03b4] for some small \u03b4 > 0.\nThey provide a plausible generative model for text corpora using log-linear distributions, under which both properties hold. Using this generative model, they prove that, up to a constant logZ and some small error,\nlogp(w,w\u2032) \u221d \u2016vw + vw\u2032\u20162 \u2212 2logZ (.) logp(w) \u221d \u2016vw\u20162 \u2212 logZ (.)\nwhere p(w,w\u2032) is the probability that the words w and w\u2032 appear together in the corpus, and p(w) is the prior of seeing word w in the corpus. Therefore,\nPMI(w,w\u2032) := logp(w,\u03c7)\u2212 logp(w)\u2212 logp(\u03c7) \u221d \u2016vw + vw\u2032\u20162 \u2212 \u2016vw\u20162 \u2212 \u2016vw\u2032\u20162 by (.) and (.) = 2vw \u00b7 vw\u2032 \u221d vw \u00b7 vw\u2032 . (.)\nProperty B implies that, for every vector v \u2208Rd ,\n\u2016v\u20162 \u2248 vT Ew[vwvTw ]v (up to error 1 + \u03b4) = Ew[(v \u00b7 vw)2] ,\nand so the query (.) for solving \u201ca:b::c:??\u201d becomes\nargmin d \u2016va \u2212 vb \u2212 vc + vd\u20162 \u2248 argmin d Ew(va \u00b7 vw \u2212 vb \u00b7 vw \u2212 vc \u00b7 vw + vd \u00b7 vw)2\n= argmin d\nEw(PMI(a,w)\u2212PMI(b,w)\u2212PMI(c,w) + PMI(d,w))2 by (.)\n= argmin d Ew\n( log\np(\u03c7 | a) p(\u03c7) \u2212 log p(\u03c7 | b) p(\u03c7) \u2212 log p(\u03c7 | c) p(\u03c7) + log p(\u03c7 | d) p(\u03c7) )2 = argmin\nd Ew\n( log\np(w | a) p(w | b) \u2212 log p(w | c) p(w | d)\n)2 ,\nwhich is close to (.), with word w acting as context \u03c7. The generative model directly models how words are produced as the corpus is being generated, and captures latent semantic structure in the text. For details about their generative model, see []. For an overview of log-linear models, which are very widely used in natural language processing, see [].\nOn the Linear Structure of Word Embeddings Chapter . Word embeddings | \n.. Motivation behind the Squared Norm objective\nNote that by (.), we have logp(w,w\u2032) \u221d \u2016vw+vw\u2032\u201622+C where C := \u22122logZ is an unknown constant. Then since the empirical probability p(w,w\u2032) is given by\np(w,w\u2032) = p(w\u2032 | w)p(w)\n= Xww\u2032\nXw Xw\u2211 w\u0302Xw\u0302\n\u221d Xww\u2032 ,\nthis gives us the Squared Norm (SN) objective (.)."}, {"heading": "Chapter ", "text": ""}, {"heading": "Methods", "text": ". Training the word vectors\nOur training corpus C is an english Wikipedia corpus consisting of roughly . billion tokens, which was preprocessed before training so that multi-word named-entities (e.g., \u201cprinceton university\u201d) are treated as a single word (e.g., \u201cprinceton_university\u201d); see Section ..\nWe use a fixed context window of size 10 to compute the co-occurrence data. Since it is too memory-expensive to store the co-occurrence count between all distinct words that appear in C, and because the co-occurrence data for infrequent words are too noisy to generate good word vectors with, we fix a minimum threshold m0 \u2208N, and only consider the set D of vocabulary words that appear at least m0 times in the training corpus. In our experiments, we set m0 = 1000, and the resulting vocabulary size is |D| = 60,265. Words which appear fewer than  times in the corpus are not included in the vocabulary D, and hence, we do not learn vectors for these words. In Chapter , we provide a way of learning vectors for less frequent words with only a small amount of co-occurrence data.\nTo train the word vectors VD := {vw : w \u2208 D}, we optimize the Squared Norm (SN) objective (.) using Adagrad []. We use d = 300 as the dimension of the word vectors vw \u2208 Rd , which is also the dimension used in [, , ] for their experiments on analogy query tasks. After training, we normalize the learned word vectors vw \u2208 VD so that \u2016vw\u2016 = 1.\n. Preprocessing the corpus\nWord embeddings are inherently limited by their inability to represent multi-word, idiomatic phrases whose meanings are not simple compositions of the individual words. For instance, \u201ckevin_bacon\u201d is the name of an individual, and so it is not a natural combination of the meanings of \u201ckevin\u201d and \u201cbacon\u201d. Many facts about the world are concerned with multi-word entities, and hence, it is important to learn vectors for these entities.\nhttp://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz Words which appear less than m0 = 1000 times are ignored in the sense that () co-occurrence counts for these words are not computed, and () these words are ignored when computing co-occurrence counts for other words. They do not affect or change the context window around any word (i.e., they are not deleted from the corpus).\nOn the Linear Structure of Word Embeddings Chapter . Methods | \nTo allow training of vectors for multi-word named-entities, we preprocessed the corpus C before training in the following manner: We used the named-entity recognition library in the Natural Language Toolkit (NLTK) [] to identify strings \u03be which correspond to multi-word named-entities in the corpus, and replaced each space in \u03be with an underscore to make \u03be a single word. (For example, we replaced every instance of the string \u201cprinceton university\u201d in the corpus with \u201cprinceton_university\u201d.) After preprocessing, we train vectors for multi-word named-entities in the same way as with other words.\nThe preprocessing decreased the size of D (i.e., the number of vocabulary words that appear at leastm0 = 1000 times in the corpus) from 68,430 to 60,265, but increased the number of words that appear at least 100 times in the corpus from 296,376 to 344,112.\nNamed-entity recognition (NER) is the task of labeling sequences of words in a text which are the names of things, such as the names of persons, organizations, and locations. NER is beyond the scope of this paper, and we refer the reader to []."}, {"heading": "Chapter ", "text": ""}, {"heading": "Categories and relations", "text": "In this chapter, we introduce the notion of categories and relations, which can be used to represent facts about the world in a knowledge base. In Figures . and ., we list the categories and relations that are used for experiments in the subsequent chapters.\n. Notation\nLet D be the set of vocabulary words that appear in a corpus C. Words w \u2208 D belong to certain categories: for example, the word princeton belongs to the categories university and municipality, while the word christianity belongs to the category religion. Moreover, a pair of words sometimes satisfy a certain relation: e.g., the word pair (united_states, dollar) satisfies the relation currency_used.\nGiven a category c, let Dc \u2286 D be the set of words that belong to c, and let\nVc := {vw : w \u2208 Dc}.\nSimilarly, given a relation r, let Dr \u2286 D \u00d7D be the set of word pairs that satisfy the relation r, and let\nVr := {va \u2212 vb : (a,b) \u2208 Dr }.\nThe vector subspace spanned by Vc is called the subspace of category c, and similarly, the vector subspace spanned by Vr is called the subspace of relation r. Moreover, we say a relation r is welldefined if there exists categories cA and cB such that, for all (a,b) \u2208 Dr , a belongs to cA and b belongs to cB.\nFor example, if c = country, thenDc contains words such as germany, japan, and united_states. If r = capital_city, then Dr contains pairs such as (germany, berlin), (japan, tokyo), and (united_states, washington_dc). Also, the relation r = capital_city is well-defined, with cA = country and cB = city. See Figure .(a)-(h) for other examples of well-defined relations. Freebase [] is an example of a knowledge base whose data items are organized using categories and relations.\nOn the Linear Structure of Word Embeddings Chapter . Categories and relations | \n. Obtaining training examples of categories and relations\nWe obtained training examples of different categories and relations from various sources including Freebase [] and the GloVe demo code []. Figures . and . list the category and relation files that are used for experiments in the subsequent chapters. Note that some words in the category and relation files, such as maremma_sheepdog in the category animal, are not in the set D of vocabulary words because they appear fewer than m0 = 1000 times in the corpus C. Since we do not have learned vectors for these words, we remove them from the category and relation files in the experiments.\n. Experiments in the subsequent chapters\nIn Chapter , we will show that the subspace of a category c, or the subspace of a well-defined relation r, can be approximated well by a relatively low-dimensional subspace, whose basis vectors can be computed using SVD. In Chapter , we use this basis to discover new words belonging to a category, or new word pairs satisfying a well-defined relation. In Chapter , we also use this basis to learn vectors for words with substantially less co-occurrence data.\nChapter  explores the idea of using external knowledge sources such as Wordnet [] to improve accuracy on analogy queries, and uses the relation files in Figure . as a benchmark.\nOn the Linear Structure of Word Embeddings Chapter . Categories and relations | \nmaremma_sheepdog pheasant rabbit english_springer_spaniel laika\n(a) animal ( words)\nbeijing shanghai tokyo seoul pyeongyang\n(b) asian_city ( words)\nshaun_livingston baron_davis larry_bird lebron_james joakim_noah\n(c) basketball_player ( words)\nraclette castelrosso beal_organic lincolnshire_poacher mascarpone\n(d) cheese ( words)\nkiev budapest speyer zagreb da_lat\n(e) city ( words)\ndebussy beethoven mahler dvorak ravel\n(f) classical_composer ( words)\nsake_screwdriver la_cucaracha manhattan beton wine_cooler\n(g) cocktail ( words)\nkievan_rus hungary bishopric_of_speyer kingdom_of_croatiaslavonia french_indochina\n(h) country ( words)\npeso rial dollar cedi euro\n(i) currency ( words)\npi_day lag_baomer lao_new_year canada_day child_health_day\n(j) holiday ( words)\nhiligaynon pangasinan moldovan saami_ume judeotunisian_arabic\n(k) languages ( words)\njanuary february march april may\n(l) month ( words)\nmellotron shvi xiqin friction_drum soprano_cornet\n(m) musical_instrument ( words)\ntswana_music rap_opera african_reggae glam_punk barcarolle\n(n) music_genre ( words)\nroanoke_logperch amanita_parvipantherina platyzoa troides_haliphron solo_man\n(o) organism_group ( words)\narnold_schwarzenegger joe_pickett alan_greenspan john_maynard_keynes karl_marx\n(p) politician ( words)\nbuddhism_in_japan anglican kirant_mundhum saura congregational_church\n(q) religion ( words)\nextreme_ironing cammag skeleton speed_skating water_aerobics\n(r) sport ( words)\njohn_rider_house brevard_zoo butaritari time_and_tide_museum museo_liverino\n(s) tourist_attraction ( words)\nprinceton harvard yale oxford cambridge\n(t) university ( words)\nFigure .: For each category file, we list the first words and the total number of words it contains.\nOn the Linear Structure of Word Embeddings Chapter . Categories and relations | \nalgeria dinar angola kwanza argentina peso armenia dram brazil real bulgaria lev cambodia riel canada dollar croatia kuna\n(a) country-currency ( pairs)\nvalentine february halloween october thanksgiving november christmas december\n(b) holiday-month ( pairs)\nhanukkah judaism passover judaism passover samaritanism sukkot judaism shavuot judaism purim judaism diwali sikhism diwali jainism diwali hinduism\n(c) holiday-religion ( pairs)\nspanish spain welsh wales french france polish poland dutch netherlands japanese japan chinese china german germany korean korea\n(d) country-language ( pairs)\nathens greece baghdad iraq bangkok thailand beijing china berlin germany bern switzerland cairo egypt canberra australia hanoi vietnam\n(e) country-capital ( pairs)\nchicago illinois houston texas fort_worth texas kansas_city missouri philadelphia pennsylvania phoenix arizona knoxville tennessee san_jose california newark california\n(f) city-in-state ( pairs)\nkievan_rus kiev hungary budapest bishopric_of_speyer speyer kingdom_of_croatiaslavonia zagreb french_indochina da_lat republic_of_afghanistan kabul moravian_serbia krusevac somali_democratic_republic mogadishu guatemala guatemala_city\n(g) capital_country ( pairs)\nchile peso iran rial persia rial ghana cedi san_marino euro tonga paanga gambia dalasi grand_duchy_of_tuscany florin indonesia rupiah\n(h) currency_used ( pairs)\nFigure .: A list of facts-based relation files. For each relation file, we list the first  word pairs and the total number of word pairs it contains. Note that (a) is a smaller subset of (h), and (e) is a smaller subset of (g), the former containing only the more \u201cwell-known\u201d pairs.\nOn the Linear Structure of Word Embeddings Chapter . Categories and relations | \namazing amazingly apparent apparently calm calmly cheerful cheerfully complete completely efficient efficiently fortunate fortunately free freely furious furiously\n(i) gram-adj-adv ( pairs)\nacceptable unacceptable aware unaware certain uncertain clear unclear comfortable uncomfortable competitive uncompetitive consistent inconsistent convincing unconvincing convenient inconvenient\n(j) gram-opposite ( pairs)\nbad worse big bigger bright brighter cheap cheaper cold colder cool cooler deep deeper easy easier fast faster\n(k) gram-comparative ( pairs)\nbad worst big biggest bright brightest cold coldest cool coolest dark darkest easy easiest fast fastest good best\n(l) gram-superlative ( pairs)\ncode coding dance dancing debug debugging decrease decreasing describe describing discover discovering enhance enhancing fly flying generate generating\n(m) gram-present-participle ( pairs)\nalbania albanian argentina argentinean australia australian austria austrian belarus belorussian brazil brazilian bulgaria bulgarian cambodia cambodian chile chilean\n(n) gram-nationality-adj ( pairs)\ndancing danced decreasing decreased describing described enhancing enhanced falling fell feeding fed flying flew generating generated going went\n(o) gram-past-tense ( pairs)\nbanana bananas bird birds bottle bottles building buildings car cars cat cats child children cloud clouds color colors\n(p) gram-plural ( pairs)\ndecrease decreases describe describes eat eats enhance enhances estimate estimates find finds generate generates go goes implement implements\n(q) gram-plural-verbs ( pairs)\nFigure .: A list of grammar-based relation files. For each relation file, we list the first  word pairs and the total number of word pairs it contains.\nOn the Linear Structure of Word Embeddings Chapter . Categories and relations | \ntriangle three quadrangle four pentagon five hexagonal six heptagon seven octagon eight january one february two march three\n(r) associated-number ( pairs)\ntub bathroom stove kitchen nurse hospital waiter restaurant bed bedroom desk classroom priest church aircraft sky submarine water\n(s) associated-position ( pairs)\nwarm hot cool cold cold freezing good great bad terrible pretty beautiful interested obsessed scared terrified tasty delicious\n(t) common-very ( pairs)\ndead alive black white sick healthy rich poor fat skinny young old old new bright dim smooth rough\n(u) graded-antonym ( pairs)\nfire hot ice cold genius smart idiot stupid\n(v) has-characteristic ( pairs)\near hear eye see tongue taste nose smell mouth eat feet walk broom clean stove cook hammer hit\n(w) has-function ( pairs)\nbird feather dog fur cat fur hamster fur alligator leather snake skin fish scale human skin\n(x) has-skin ( pairs)\ncat kitten dog puppy cow calf rooster chick human baby bear cub fish minow stone pebble mountain hill\n(y) noun-baby ( pairs)\neyes see ears hear nose smell tongue taste skin feel\n(z) senses ( pairs)\nFigure .: A list of semantics-based relation files. For each relation file, we list the first  word pairs and the total number of word pairs it contains."}, {"heading": "Chapter ", "text": ""}, {"heading": "Low-rank subspaces of categories and", "text": "relations\nWe demonstrate that the subspace of a category c, or the subspace of a well-defined relation r, can be approximated well by a relatively low-dimensional subspace in the projected space. More specifically, we use singular value decomposition (SVD) to approximate a low-rank basis Uk = {u1, . . . ,uk} for the subspace of category c or relation r (see Algorithm .). Section .. outlines an experiment where we use cross-validation to show that Uk captures most of the vectors in Vc = {vw : w \u2208 Dc} or Vr = {va \u2212 vb : (a,b) \u2208 Dr } (see Figure .). Moreover, we show that the first basis vector u1 captures the most information about a category c or a relation r (see Figures . and .).\n. Computing a low-rank basis using SVD\nThe function GET_BASIS (Algorithm .) uses SVD to compute a rank-k basis for the subspace spanned by a set of vectors V . So GET_BASIS(Vc, k) and GET_BASIS(Vr , k) return a rank-k basis for the subspace of category c and for the subspace of relation r, respectively.\nIn the following experiment, we use cross-validation to assess how much the low-rank basis Uk = {u1, . . . ,uk} returned by GET_BASIS captures the subspace of a category c or a relation r. We demonstrate that the first basis vector u1 (corresponding to the largest singular value \u03c31) captures the most information. In particular, we show that the first basis vector u1 captures significantly more mass of the subspace than any other basis vector (see Figure .), and that the only i \u2208 {1, . . . , k} satisfying\nv \u00b7ui > 0 \u2200v \u2208 Vc (or \u2200v \u2208 Vr )\nis i = 1 (see Figures . and .).\n.. Experiment\nLet c be a category where we have a set Sc \u2286 Dc of words that belong to the category c, with size |Sc | > 50. For each rank k \u2208 {1,2, . . . ,25}, we perform the following experiment over T = 50 repeated trials: The same experiment is done for a relation r, by replacing Vc with Vr , and Sc \u2286 Dc with Sr \u2286 Dr .\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \nfunction GET_BASIS(V ,k): Returns a rank-k basis for the subspace spanned by V .\nInputs:\n\u2022 V = {v1,v2, . . . , vn}, a set of vectors in Rd . Let |V | := n denote the number of vectors in V .\n\u2022 k \u2208N, the rank of the basis (where k d)\nStep . Let X be a d \u00d7 |V |matrix whose column vectors are v \u2208 V . Using SVD, factorize X as\nX =U\u03a3W T ,\nwhere U \u2208 Rd\u00d7d and W \u2208 R|V |\u00d7|V | are orthogonal matrices, and \u03a3 \u2208 Rd\u00d7|V | is the diagonal matrix of the singular values of X in descending order.\nStep . Let Uk be the d \u00d7 k submatrix obtained by taking the first k columns u1, . . . ,uk of U (which correspond to the k largest singular values of X). Since U is orthogonal, the columns of Uk form a rank-k orthonormal basis.\nStep . Scale u1 by \u00b11 so that \u2211 v\u2208V v \u00b7u1 > 0.\nStep . Return Uk .\nend function\nAlgorithm .: GET_BASIS(V ,k) returns a rank-k basis, Uk \u2208 Rd\u00d7k , for the subspace spanned by a set of vectors V .\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \nStep . Randomly partition Sc into a training set S1 and a testing set S2, where the training set size is |S1| = 0.7|Sc |. For each i \u2208 {1,2}, let Vi := {vw : w \u2208 Si}.\nStep . Compute a rank-k basis Uk for the subspace spanned by V1, using Algorithm .:\nUk \u2190GET_BASIS(V1, k).\nStep . To measure how much a vector v \u2208Rd is captured by the span of Uk \u2208Rd\u00d7k , define\n\u03c6(Uk ,v) := \u2016UTk v\u2016 \u2016v\u2016 .\nNow, test how much Uk captures the vectors V2 of the testing set, by computing the capture rate\n\u03c6(Uk ,V2) := 1 |V2| \u2211 v\u2208V2 \u03c6(Uk ,v). (.)\nIf \u03c6(Uk ,V2) is large, i.e., the vectors in V2 have a large projection onto Uk , then it would indicate that Uk is a good low-rank approximation for the subspace of category c.\nStep . Look at the distribution of the values in {ui \u00b7 v}v\u2208V2 for each i \u2208 {1, . . . , k}.\n. Results For each rank k \u2208 {1,2, . . .25}, we plot the average capture rate \u03c6\u0304k := 1T \u2211T t=1\u03c6(U (t) k ,V (t) 2 ) over T = 50 repeated trials in Figure .. Notice that when k = 1, \u03c6\u0304k is already between 0.420 and 0.673. For k \u2265 2 on the other hand, the increase from \u03c6\u0304k\u22121 to \u03c6\u0304k is relatively small.\nWe found that the values {v \u00b7 u1}v\u2208V2 all have the same sign , whereas for i \u2265 2, the values {v \u00b7 ui}v\u2208V2 are more evenly distributed around . Figure . shows the distribution {v \u00b7 ui}v\u2208V2 for i = 1 and i = 2; the distribution {v \u00b7ui}v\u2208V2 for i \u2265 2 is similar to the distribution for i = 2.\n. Conclusion\nOur results show that the subspace of many categories and relations are low-dimensional. Moreover, we demonstrated that the first basis u1 is the \u201cdefining\u201d vector that encodes the most general information about a category c (or a relation r): Letting v = vw for any w \u2208 Dc (or v = va \u2212vb for any (a,b) \u2208 Dr ), the first coordinate v \u00b7u1 has the largest magnitude, and the sign of v \u00b7u1 is always positive. All other subsequent basis vectors ui for i \u2265 2 encode more \u201cspecific\u201d information pertaining to individual words in Dc (or word pairs in Dr ).\nIt remains to be discovered what specific features are captured by these basis vectors for various categories and relations. For example, if Uk is a basis for the subspace of category c = country,\nFor each trial t \u2208 {1, . . . ,T }, \u03c6(U (t)k ,V (t) 2 ) is the capture rate attained in trial t, where V (t) 2 = {vw : w \u2208 S (t) 2 } is the set of\nvectors for words in the testing set S (t) 2 (which is randomly generated in Step ), and U (t) k is the rank-k basis computed in Step . Recall that, in Step  of Algorithm ., we scale u1 by \u00b11 so that \u2211 v\u2208V1 v \u00b7u1 > 0.\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \nthen perhaps having a positive second coordinate vw \u00b7u2 in the basis indicates that w is a developed country, and having a negative fourth coordinate vw \u00b7 u4 indicates that country w is located in Europe. We leave this to future work.\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0 5 10 15 20 25 k\n\u03c6k\ncategory\nanimal\nmusic_genre\nmusical_instrument\norganism_classification\npolitician\nreligion\nsport\ntourist_attraction\ncountry\ncapital_city\ncurrency\nlanguage\n0.5\n0.6\n0.7\n0.8\n0 5 10 15 20 25 k\n\u03c6k\nrelation\ncapital_country\ncurrency_used\ncountry_language\ncountry\u2212capital2\ncity\u2212in\u2212state\nFigure .: Results from the experiment in Section .., where we use cross-validation over T = 50 repeated trials to assess how much the low-rank basis Uk returned by GET_BASIS (Algorithm .) captures the subspace of a category c (or a relation r). In each trial t \u2208 {1, . . . ,T }, we randomly split Vc (or Vr ) into a training set V (t) 1 and a testing set V (t) 2 , then compute a rank-k basis U (t) k for the subspace spanned by V (t)1 . For each rank k \u2208 {1,2, . . .25}, we plot the average capture rate \u03c6\u0304k := 1 T \u2211T t=1\u03c6(U (t) k ,V (t) 2 ) (.), where a higher capture rate means that the vectors in V2 have a large projection onto Uk . When rank is k = 1, \u03c6\u0304k is already between 0.420 and 0.673. For ranks k \u2265 2 on the other hand, the increase from \u03c6\u0304k\u22121 to \u03c6\u0304k is relatively small. This shows that the first basis vector u1 is the \u201cdefining\u201d vector that encodes the most information about a category c (or a relation r).\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \n\u25cf u2\nu1\n\u22121.0 \u22120.5 0.0 0.5 1.0\nanimal\n\u25cfu2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\ncheese\n\u25cf u2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\ncocktail\n\u25cf u2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\ncountry\n\u25cf\u25cf u2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\ncurrency\nu2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\nlanguages\nu2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\nmusical_instrument\nu2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\nmusic_genre\nu2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\norganism_group\nu2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\npolitician\n\u25cf\u25cf \u25cfu2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\nreligion\nu2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\nsport\n\u25cf \u25cf\u25cf\u25cf \u25cfu2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\ntourist_attraction\nFigure .: For various categories c from Figure ., we randomly split Vc into a training set V1 and a testing set V2, then compute a rank-k basis Uk = {u1,u2, . . . ,uk} for the subspace spanned by V1. Here, we provide a boxplot of the distribution of {v \u00b7u1}v\u2208V2 (denoted by u) and the distribution of {v \u00b7 u2}v\u2208V2 (denoted by u). Note that for each category c, we have v \u00b7 u1 > 0 for all v \u2208 V2, whereas {v \u00b7u2}v\u2208V2 is more evenly distributed around . The distribution of {v \u00b7ui}v\u2208V2 for i \u2265 2 is similar to the distribution for i = 2, so we omit them here.\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \n\u25cf\u25cf \u25cf \u25cf \u25cf\u25cf \u25cf\u25cf \u25cf u2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\ncapital_country\n\u25cf \u25cf\u25cf u2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\ncity\u2212in\u2212state\n\u25cf u2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\ncountry\u2212capital2\nu2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\ncountry_language\n\u25cf\u25cf \u25cf\u25cf \u25cf \u25cf u2 u1\n\u22121.0 \u22120.5 0.0 0.5 1.0\ncurrency_used\nFigure .: For various relations r from Figure ., we randomly split Vr into a training set V1 and a testing set V2, then compute a rank-k basis Uk = {u1,u2, . . . ,uk} for the subspace spanned by V1. Here, we provide a boxplot of the distribution of {v \u00b7 u1}v\u2208V2 (denoted by u) and the distribution of {v \u00b7 u2}v\u2208V2 (denoted by u). Note that for eachrelation r, we have v \u00b7 u1 > 0 for all v \u2208 V2 (except for one outlier in r = capital_country, and one outlier in r = city-in-state), whereas {v \u00b7u2}v\u2208V2 is more evenly distributed around . The distribution of {v \u00b7 ui}v\u2208V2 for i \u2265 2 is similar to the distribution for i = 2, so we omit them here."}, {"heading": "Chapter ", "text": ""}, {"heading": "Extending a knowledge base", "text": "Let KB denote the current knowledge base, which consists of facts about the world that the machine currently knows. We assume that KB is incomplete, and that there are new facts to be learned. In other words, there exists categories c such that KB only knows a subset Sc \u2282 Dc of words that belong to c, and similarly, there exists relations r such that KB only knows a subset Sr \u2282 Dr of word pairs that satisfy r. Our goal is to discover new facts outside KB, such as words in D\\Sc that also belong to the category c, or word pairs in (D\u00d7D)\\Sr that also satisfy the relation r.\nIn Section ., we provide an algorithm called EXTEND_CATEGORY for discovering words in D\\Sc that also belong to c (see Algorithm .). Table . lists the top  words returned by EXTEND_CATEGORY for various categories, and shows that the algorithm performs very well. Then in Section ., we present an algorithm called EXTEND_RELATION for discovering new word pairs in (D \u00d7D)\\Sr that also satisfy r (see Algorithm .). Its accuracy for returning correct word pairs is provided in Figure ..\nWe demonstrate that the low-dimensional subspace of categories and relations can be used to discover new facts with fairly low false-positive rates. The performance of EXTEND_RELATION (Algorithm .) is especially surprising, given the simplicity of the algorithm and the fact that it returns plausible word pairs out of all |D|(|D| \u2212 1) \u2248 3.6e+09 possible word pairs in D\u00d7D.\n. Motivation\nIn Socher et. al\u2019s paper [], a neural tensor network (NTN) model is used to learn semantic word vectors that can capture relationships between two words. More specifically, their NTN model computes a score of how plausible a word pair (a,b) satisfies a certain relationship r by the following function:\ng(va, r,vb) = b \u00b7 f ( vTa W [1:m] r vb +\u03b8r [ va vb ] + cr ) (.)\nwhere va,vb \u2208Rd are the vector representations of the words a,b respectively, f = tanh is a sigmoid function, andW [1:m]r \u2208Rd\u00d7d\u00d7m is a tensor. The remaining parameters for relation r are the standard form of a neural network: \u03b8r \u2208Rm\u00d72d and b,cr \u2208Rm.\nWith this model, however, we see a potential danger of overfitting the data because the number of parameters in the term vTa W [1:m] r vb in (.) is quadratic in d. Hence, our motivation is to employ\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \nfunction EXTEND_CATEGORY(Sc, k, \u03b4): Returns a set of words in D\\Sc.\nInputs:\n\u2022 A subset Sc \u2282 Dc of words that belong to some category c\n\u2022 k \u2208N, the rank of the basis for the subspace of category c\n\u2022 \u03b4 \u2208 [0,1], a threshold value\nStep . Compute a rank-k basis Uk \u2208Rd\u00d7k for the subspace of category c using Algorithm .:\nUk \u2190 GET_BASIS({vw : w \u2208 Sc}, k).\nLet u1 be the first (column) basis vector of Uk .\nStep . Return the set of words w \u2208 D\\Sc whose vectors have (i) a positive first coordinate vw \u00b7u1 in the basis Uk , and (ii) a large enough projection \u2016vwUk\u2016 > \u03b4 in the subspace of category c:\n{w \u2208 D\\Sc : vw \u00b7u1 > 0, \u2016vwUk\u2016 > \u03b4} .\nend function\nAlgorithm .: EXTEND_CATEGORY(Sc, k,\u03b4) returns a set of words in D\\Sc which are likely to belong to category c.\nthe linear structure of the word embeddings to fit a more general model. The low-dimensional subspace of categories, as demonstrated in Chapter , gives rise to a simple algorithm that allows one to discover new facts that are not in KB.\n. Learning new words in a category\nLet c be a category such that KB only knows a subset Sc \u2282 Dc of words that belong to c. We provide a method called EXTEND_CATEGORY (Algorithm .) for discovering new words in D\\Sc that also belong to c.\n.. Performance\nWe tested EXTEND_CATEGORY(Sc, k,\u03b4) on various categories c from Figure ., using rank k = 10 and threshold \u03b4 = 0.6. Table . lists the top  words returned by the algorithm, where the words w \u2208 D\\Sc are ordered in descending magnitude of the projection \u2016vwUk\u2016 onto the subspace Uk of category c. The algorithm makes a few mistakes, e.g., it returns london as a tourist_attraction, and aren as a basketball_player. But overall, the algorithm seems to work very well and returns correct words that belong to the category.\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n(a) c = classical_composer\nword projection schumann . beethoven . stravinsky . liszt . schubert .\n(b) c = sport\nword projection biking . volleyball . skiing . softball . soccer .\n(c) c = university\nword projection cambridge_university . university_of_california . new_york_university . stanford_university . yale_university .\n(d) c = basketball_player\nword projection dwyane_wade . aren . kobe_bryant . chris_bosh . tim_duncan .\n(e) c = religion\nword projection christianity . hinduism . taoism . buddhist . judaism .\n(f) c = tourist_attraction\nword projection metropolitan_museum_of_art . museum_of_modern_art . london . national_gallery . tate_gallery .\n(g) c = holiday\nword projection diwali . christmas . passover . new_year . rosh_hashanah .\n(h) c = month\nword projection august . april . october . february . november .\n(i) c = animal\nword projection horses . moose . elk . raccoon . goats .\n(j) c = asian_city\nword projection taipei . taichung . kaohsiung . osaka . tianjin .\nTable .: Given a set Sc \u2282 Dc of words belonging to a category c, EXTEND_CATEGORY (Algorithm .) returns new words in D\\Sc which are also likely to belong to c. Here, we list the top  words returned by EXTEND_CATEGORY(Sc, k,\u03b4) for various categories c in Figure ., using rank k = 10 and threshold \u03b4 = 0.6. The words w \u2208 D\\Sc are ordered in descending magnitude of the projection \u2016vwUk\u2016 onto the category subspace. The algorithm makes a few mistakes, e.g., it returns london as a tourist_attraction, and aren as a basketball_player. But overall, the algorithm seems to work very well, and returns correct words that belong to the category.\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n. Learning new word pairs in a relation\nLet r be a well-defined relation such that KB only knows a subset Sr \u2282 Dr of word pairs that satisfy r. We now present a method called EXTEND_RELATION (Algorithm .) for discovering new word pairs in (D\u00d7D)\\Sr that also satisfy r.\n.. Experiment\nWe tested EXTEND_RELATION on three well-defined relations from Figure .: capital_city, city_in_state, and currency_used. For each of the relations r, let Sr \u2286 Dr be the set of word pairs contained in the corresponding relation file (see Figure .(f)-(h)). We used cross-validation to assess the accuracy rate of Algorithm ., as follows. For each rank k \u2208 {1,2, . . . ,9} and threshold \u03b4 \u2208 {0.4,0.45, . . . ,0.75}, we repeated the following experiment over T = 50 trials:\nStep . Randomly partition Sr into a training set S1 and a testing set S2, where the training set size is |S1| = 0.3|Sr |. Let A := {a : (a,b) \u2208 S2} and B := {b : (a,b) \u2208 S2}.\nStep . Use Algorithm . to find a set S of word pairs in (D \u00d7D)\\S1 which are likely to satisfy relation r:\nS \u2190 EXTEND_RELATION(S1, {k,k,k}, {\u03b4,\u03b4,\u03b4}).\nStep . Let S \u2032 := {(a,b) \u2208 S : a \u2208 A or b \u2208 B}. For each answer (a,b) \u2208 S \u2032 , we count it as correct if (a,b) \u2208 S2, and incorrect otherwise. So the resulting accuracy of EXTEND_RELATION using parameters k and \u03b4 is\nacc(k,\u03b4) := # correct answers # total answers = |S \u2032 \u2229 S2| |S \u2032 | .\n.. Performance\nFor each rank k \u2208 {1,2, . . . ,9} and threshold \u03b4 \u2208 {0.4,0.45,0.5, . . . ,0.75}, we plot the average accuracy 1 T \u2211T t=1 acc\n(t)(k,\u03b4) over T = 50 trials in Figure .a. The table in Figure .b lists the parameters k and \u03b4 that attained the highest average accuracy. The algorithm achieved significantly higher accuracy for r = capital_city than for the other two relations; we discuss why in Section ... However, the performance of EXTEND_RELATION is still quite remarkable, considering the fact that it returns reasonable word pairs out of the (|D| 2 ) \u2248 1.8e+09 possible word pairs in D\u00d7D.\nAs the threshold \u03b4 is increased, the algorithm filters out more words in Step  and word pairs in Step  of the algorithm, resulting in a smaller number of word pairs returned by the algorithm. Figure . illustrates this effect for r = capital_city.\nNote that the algorithm\u2019s accuracy can be further improved by fine-tuning the parameters. In our experiment (Section ..), we used the same rank k for kA, kB, and kr , and also used the same threshold \u03b4 for \u03b4A, \u03b4B, and \u03b4r ; but one can vary each of these parameters separately to achieve better performance.\nWe ignore the answers (a,b) \u2208 S such that a < A and b < B, because we do not have an automated way of determining whether it is correct or incorrect. One could check each of these answers manually using an external knowledge source (e.g., Google search), but doing so would be very time-consuming. where acc(t)(k,\u03b4) is the accuracy obtained in trial t.\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \nfunction EXTEND_RELATION(Sr , {kA, kB, kr }, {\u03b4A,\u03b4B,\u03b4r }): Returns a set of word pairs in (D\u00d7D)\\Sr .\nInputs:\n\u2022 A subset Sr \u2282 Dr of word pairs that satisfy some well-defined relation r. Let A := {a : (a,b) \u2208 Sr } and B := {b : (a,b) \u2208 Sr }. Let cA be the category such that, for all a \u2208 A, a belongs to category cA. Similarly, let cB be the category such that, for all b \u2208 B, b belongs to category cB.\n\u2022 kA, kB, kr \u2208N, the rank of the basis for the subspaces of cA, cB, and r respectively\n\u2022 \u03b4A,\u03b4B,\u03b4r \u2208 [0,1], threshold values\nStep . Use Algorithm . to get a set of words SA \u2286 D\\A whose vectors have a large enough projection (\u2265 \u03b4A) in the rank-kA subspace of category cA, and a set of words SB \u2286 D\\B whose vectors have a large enough projection (\u2265 \u03b4B) in the rank-kB subspace of category cB:\nSA\u2190 EXTEND_CATEGORY(A,kA,\u03b4A) SB\u2190 EXTEND_CATEGORY(B,kB,\u03b4B).\nStep . Compute a rank-kr basis for the subspace of relation r using Algorithm .:\nUkr \u2190 GET_BASIS({va \u2212 vb : (a,b) \u2208 Sr }, kr ).\nLet u1 be the first (column) basis vector of Ukr .\nStep . Return the set of word pairs (a,b) \u2208 SA \u00d7 SB whose difference vectors have (i) a positive first coordinate (va \u2212 vb) \u00b7 u1 in the basis Ukr , and (ii) a large enough projection \u2016(va \u2212 vb)Ukr \u2016 > \u03b4r in the subspace of relation r:\n{(a,b) \u2208 SA \u00d7 SB : (va \u2212 vb) \u00b7u1 > 0, \u2016(va \u2212 vb)Uk\u2016 > \u03b4r } .\nend function\nAlgorithm .: EXTEND_RELATION(Sr , {kA, kB, kr }, {\u03b4A,\u03b4B,\u03b4r }) returns a set of word pairs (a,b) \u2208 (D\u00d7D)\\Sr which are likely to satisfy relation r.\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n.. Varying levels of difficulty for different relations\nWe provide two explanations as to why EXTEND_RELATION underperforms on relations such as city_in_state and currency_used:\n. For r = capital_city, there is a one-to-one mapping between the sets Ar := {a : (a,b) \u2208 Sr } and Br := {b : (a,b) \u2208 Sr }, whereas the same does not hold for r = city_in_state or r = currency_used (see Figure .). This causes the algorithm to return many false-positive answers for the relations city_in_state and currency_used, as we illustrate with an example below.\nConsider the relation r = currency_used. In the set Sr of word pairs contained in the relation file for r (see Figure .(h)), there are  country-currency pairs (a,b) \u2208 Sr where b = franc. For  of these pairs {(a,franc) \u2208 Sr }, the country word a belongs to the category c = african_country. Because the low-rank basis Ukr for relation r (computed in Step  of Algorithm .) tries to capture the vectors in the set {va \u2212 vfranc : (a, franc) \u2208 Sr }, and the vectors of words belonging to the category c = african_country are clustered together, the algorithm returns many false-positive pairs consisting of an African country and the currency franc. For example, in many trial runs, the algorithm returns incorrect pairs such as (kenya, franc), (uganda, franc), and (sudan, franc). False-positive answers such as these cause the algorithm\u2019s accuracy to drop.\n. Some relations are just inherently more difficult than others to represent using word vectors. For example, Figure . shows that solving analogy queries of the form \u201ca:b::c:??\u201d for pairs (a,b), (c,d) in the relation file for country-currency is more difficult than for pairs (a,b), (c,d) satisfying the relation country-capital. This may explain why EXTEND_RELATION performs worse on currency_used than on capital_city.\n. Conclusion\nWe have demonstrated that the low-dimensional subspace of categories and relations can be used to discover new facts with fairly low false-positive rates. The performance of EXTEND_RELATION (Algorithm .) is especially surprising, given the simplicity of the algorithm and the fact that it returns plausible word pairs out of all possible word pairs inD\u00d7D. The algorithms EXTEND_CATEGORY (Algorithm .) and EXTEND_RELATION (Algorithm .) are computationally efficient, are shown to drastically narrow down the search space for discovering new facts, and can be used to supplement other methods for extending knowledge bases.\nIn other words, given a word a \u2208 Ar , there exists a unique word b \u2208 Br such that (a,b) satisfies the relation r; and conversely, given a word b \u2208 Br , there exists a unique word a \u2208 Ar such that (a,b) satisfies r. country-currency is a smaller subset of the relation file for currency_used, and country-capital is a smaller subset of capital_city; see Figure ..\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n0.2\n0.4\n0.6\n0.8\n0.4 0.5 0.6 0.7 \u03b4\nac cu\nra cy\ncapital_country\n0.2\n0.4\n0.6\n0.8\n0.4 0.5 0.6 0.7 \u03b4\nac cu\nra cy\ncity\u2212in\u2212state\n0.2\n0.4\n0.6\n0.8\n0.4 0.5 0.6 0.7 \u03b4\nac cu\nra cy\nrank\u22121\nrank\u22122\nrank\u22123\nrank\u22124\nrank\u22125\nrank\u22126\nrank\u22127\nrank\u22128\nrank\u22129\ncurrency_used\n(a) For each rank k \u2208 {1,2, . . . ,9} and threshold \u03b4 \u2208 {0.4,0.45, . . . ,0.75}, we plot the average accuracy 1 T \u2211T t=1 acc (t)(k,\u03b4) over T = 50 trials, where for each trial t, acc(t)(k,\u03b4) = # correct answers# total answers is the accuracy of the answers returned by EXTEND_RELATION(S (t) 1 , {k,k,k}, {\u03b4,\u03b4,\u03b4}). (Here, each S (t) 1 \u2282 Dr is a random subset of the word pairs contained in the relation file of r (see Figure .(f)-(h)). S(t)1 is generated randomly, at each trial t, in Step  of the experiment in Section ... ).\nr maxk,\u03b4 acc(k,\u03b4) rank k threshold \u03b4 capital_city .  . city_in_state .  . currency_used .  .\n(b) For each relation r, we list the parameters k and \u03b4 that achieved the highest average accuracy in (a).\nFigure .: Given a set Sr \u2282 Dr of word pairs satisfying a relation r, EXTEND_RELATION (Algorithm .) returns new word pairs in (D\u00d7D)\\Sr which are also likely to satisfy relation r. We use crossvalidation to assess the accuracy rate of EXTEND_RELATION on three well-defined relations from Figure .(f)-(h). Note that EXTEND_RELATION performs very well on r = capital_city, achieving accuracy as high as 0.909. We discuss why the accuracy is lower for the other two relations in Section ... For details about the experiment, see Section ...\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n(a) r = capital_city\ncA = country cB = city\njapan\ngermany\ncanada\ntokyo\nberlin\nottawa\n(b) r = city_in_state\ncA = city\ncB = us_state trenton\nnewark\nsan_francisco\nlos_angeles\nmountain_view\nnew_jersey\ncalifornia\n(c) r = currency_used\ncA = country cB = currency\nalgeria\nserbia_montenegro\ngermany\nfrance\ndinar\neuro\nfranc\nFigure .: For r = capital_city, there is a one-to-one mapping between Ar := {a : (a,b) \u2208 Sr } and Br := {b : (a,b) \u2208 Sr }, since a country has exactly one capital city. The same does not hold for (b) or (c), however: (b) Each U.S. state contains multiple cities, and some cities in different U.S. states have identical names (e.g., both California and New Jersey have a city named Newark). (c) A country can have multiple currencies (either concurrently or over history), and the same currency can be used in multiple countries. This causes EXTEND_RELATION to return a higher number of false-positive (incorrect) answers for (b) and (c); see Section .. for a detailed explanation.\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n0\n25\n50\n75\n0.4 0.5 0.6 0.7 \u03b4\nrank\u22122\n0\n25\n50\n75\n0.4 0.5 0.6 0.7 \u03b4\nrank\u22127\n0\n25\n50\n75\n0.4 0.5 0.6 0.7 \u03b4\nrank\u22128\nFigure .: Let r = capital_city, and let S1 \u2282 Dr be a random subset of the word pairs contained in the relation file of r (see Figure .(g)). We plot the number of correct (blue) and incorrect (red) word pairs returned, respectively, by calling EXTEND_RELATION(S1, {k,k,k}, {\u03b4, \u03b4, \u03b4}) for varying threshold values \u03b4 (see Algorithm .). We only provide plots for the ranks k \u2208 {2,7,8}; the plots for other ranks are similar. As the threshold \u03b4 is increased, the algorithm filters out more word pairs in Steps  and  of the algorithm, resulting in a smaller number of word pairs returned by the algorithm."}, {"heading": "Chapter ", "text": "Learning vectors for less frequent words\nSince the size of the co-occurrence data is quadratic in the size of the vocabulary D, and since the co-occurrence data for infrequent words are too noisy to generate good word vectors with, we restrict the vocabulary D to only the words that appear at least m0 = 1000 times in the corpus. Hence, we do not have vectors for words that appear fewer than 1000 times in the corpus. These words include the famous composer claude_debussy ( times), Malaysian currency ringgit ( times), the famous actor adam_sandler ( times), and the historical event boston_massacre ( times). In order to continually extend the knowledge base KB, it becomes necessary to learn vectors for these less frequent words.\nWe demonstrate that, using the low-dimensional subspace of categories, one can substantially reduce the amount of co-occurrence data needed to learn vectors for words. In particular, we present an algorithm called LEARN_VECTOR (Algorithm .) for learning vectors of words with only a small amount of co-occurrence data. We test the algorithm on seven words w\u0302 (listed in Table .) which we already have vectors for, and compare the \u201ctrue\u201d vector vw\u0302 \u2208 VD to the vector v\u0302 returned by LEARN_VECTOR. The algorithm\u2019s performance is given in Figure .. In general, the algorithm achieves very good performance while using only a small fraction of the total amount of co-occurrence data in the Wikipedia corpus C.\nOne can extend this method to learn vectors for any words \u2013 even words that do not appear at all in the Wikipedia corpus \u2013 using web-scraping tools, such as Google search, to obtain additional co-occurrence data.\n. Learning a new vector\nLet w\u0302 be a word such that (i) we know the category c that w\u0302 belongs in, and (ii) we have a small corpus \u0393 (where |\u0393 | |C|) containing co-occurrence data for w\u0302. Then we provide a method LEARN_VECTOR for learning its word vector v\u0302 \u2208Rd (see Algorithm .). There are a total of 283,847 words that occur between  and  times in the corpus, which are not included in D.\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \nfunction LEARN_VECTOR(w\u0302, c,\u0393 , k,\u03b7,\u03bb): Returns a learned vector v\u0302 \u2208Rd for word w\u0302.\nInputs:\n\u2022 w\u0302, a word\n\u2022 c, the category that w\u0302 belongs in. Let Vc := {vw : w \u2208 Dc} be the vectors of words belonging to c.\n\u2022 \u0393 , a small corpus containing co-occurrence data for w\u0302\n\u2022 k \u2208N, the rank of the basis for the category subspace\n\u2022 \u03b7 \u2208 (0,1], the learning rate for Adagrad\n\u2022 \u03bb > 0, the weight of the regularization term in the objective (.)\nStep . Compute a rank-k basis Uk \u2208Rd\u00d7k for the subspace of category c using Algorithm .:\nUk \u2190 GET_BASIS(Vc, k).\nStep . Consider only the set D \u2032 of vocabulary words that appear at least m\u20320 = 10 times in \u0393 . For each word w \u2208 D \u2032 , compute Yw\u0302w, the number of times word w appears in any context window around w\u0302 in \u0393 .\nStep . Use Adagrad with learning rate \u03b7 to train parameters v\u0302 \u2208 Rd , b \u2208 Rk , and Z \u2208 R so as to minimize the objective\u2211\nw\u2208D \u2032\u2229D g(Yw\u0302w)\n( ||v\u0302 + vw ||2 \u2212 log(Yw\u0302w) +Z )2 +\u03bb\u2016v\u0302 \u2212Ukb\u20162, (.)\nwhere {vw}w\u2208D are the already-learned word vectors, and g(x) := min {( x 10 )0.75 ,1 } . Note\nthat v\u0302, b, and Z are initialized randomly.\nStep . Normalize v\u0302 so that \u2016v\u0302\u2016 = 1.\nStep . Return v\u0302, the learned word vector for w\u0302.\nend function\nAlgorithm .: LEARN_VECTOR(w\u0302, c,\u0393 , k,\u03b7,\u03bb) returns a vector v\u0302 \u2208 Rd for word w\u0302 learned using the objective (.).\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \nw\u0302 c k Xw\u0302 california us_state  .e+ christianity religion  .e+ germany country  .e+ hinduism religion  .e+ japan country  .e+ massachusetts us_state  .e+ princeton university  .e+\nTable .: In the experiment, we train vectors for the words w\u0302 by minimizing the objective (.). c is the category that w\u0302 belongs in. For the words california, germany, and japan, we used k = 10 as the rank of the basis of the category subspace; for the remaining four words, we used rank k = 3. Xw\u0302 := \u2211 w\u2208CXw\u0302w measures the amount of co-occurrence data for w\u0302 in the original corpus C.\n.. Motivation behind the optimization objective\nThe objective (.) is nearly identical to the Squared Norm (SN) objective (.), except for a few differences: (i) we have an additional regularization term \u03bb\u2016v\u0302\u2212Ukb\u20162, (ii) the co-occurrence counts Yw\u0302w are taken from the smaller corpus \u0393 , and (iii) the summation is taken over words w \u2208 D \u2032 \u2229D. Note that b has a closed-form solution, since to minimize \u2016v\u0302 \u2212Ukb\u20162, one can just take b to be the projection of v\u0302 onto Uk . The regularization term \u03bb\u2016v\u0302 \u2212Ukb\u20162 serves as a prior knowledge, forcing the new vector v\u0302 to be trained near the subspace Uk of category c, but also allowing v\u0302 to lie outside the subspace. The hope is that the regularization term reduces the amount of co-occurrence data needed to fit v\u0302. Note that in general, the regularization weight \u03bb should be decreasing in the size of \u0393 , since less prior knowledge is needed with more data.\n. Experiment\nFor our experiment, we chose seven words w\u0302 (listed in Table .) which we already have vectors for, fitted a vector v\u0302 using LEARN_VECTOR (Algorithm .), and compared v\u0302 to the true vector vw\u0302 \u2208 VD (see Figure .). We withheld the true vector vw\u0302 \u2208 VD from training, by taking w\u0302 out of the summation over D \u2032 \u2229D in the objective (.). For the words california, germany, and japan, we used k = 10 as the rank of the basis of the category subspace; for the remaining four words, we used rank k = 3.\nFor each word w\u0302, we use Yw\u0302(\u0393 ) := \u2211 w\u2208D \u2032 Yw\u0302w to quantify the amount of co-occurrence data for word w\u0302 in a corpus \u0393 with vocabulary set D \u2032 , and evaluate the algorithm in Section . for varying values of Yw\u0302(\u0393 ). More specifically, we extracted six subcorpora \u03931, . . . ,\u03936 from the original corpus C, where \u03931 \u2282 \u03932 \u2282 \u00b7 \u00b7 \u00b7 \u2282 \u03936 \u2282 C and Yw\u0302(\u03931) < Yw\u0302(\u03932) < \u00b7 \u00b7 \u00b7 < Yw\u0302(\u03936) Yw\u0302(C) = Xw\u0302. For each i \u2208 {1, . . . ,6}, we ran the algorithm with various learning rates \u03b7i and regularization weights \u03bbi ; Table . lists the parameter values that resulted in the best performance for each corpus size and each word.\n. Performance\nWe evaluate the performance of LEARN_VECTOR by considering the order and the cosine score of the learned vector v\u0302 returned by the algorithm, defined below.\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \n.. Order and cosine score of the learned vector\nLet w\u0302 be one of the seven words we trained a vector for, v\u0302 the learned vector returned by the algorithm, and vw\u0302 \u2208 VD the \u201ctrue\u201d vector for word w\u0302. Number the vectors v1,v2, . . . , v|D| \u2208 VD in order of decreasing cosine similarity from v\u0302, so that v1 \u00b7 v\u0302 > v2 \u00b7 v\u0302 > \u00b7 \u00b7 \u00b7 > v|D| \u00b7 v\u0302. Then the order of v\u0302 is the number k \u2208N such that vw\u0302 = vk , i.e., the true vector vw\u0302 has the kth largest cosine similarity from v\u0302 out of all the words in D. The cosine score of v\u0302 is the cosine similarity between the true vector vw\u0302 and the vector v\u0302 returned by the algorithm, i.e., vw\u0302 \u00b7 v\u0302.\n.. Evaluation\nIn Figure ., we provide a plot of the order and cosine score of v\u0302 for varying values of ln(Yw\u0302), where Yw\u0302 := \u2211 w\u2208D \u2032 Yw\u0302w is the amount of co-occurrence data for w\u0302 in the training corpus \u0393 with vocabulary set D \u2032 . Note that in general, the order of v\u0302 is decreasing, and the cosine score of v\u0302 is increasing, in the amount of co-occurrence data Yw\u0302. Moreover, the algorithm seems to achieve a higher cosine score by using a smaller rank k for the basis of the category subspace: The words for which rank k = 10 was used (california, germany, and japan) have lower cosine scores than the words for which rank k = 3 was used.\nWe provide an explanation as to why using a smaller rank k improves the algorithm\u2019s performance. For a category c, let Uk be a rank-k basis for the subspace of c. The regularization term \u03bb\u2016v \u2212Ukb\u2016 in the objective (.) serves to train v\u0302 near the subspace Uk , which by definition only captures the general notion of category c. Recall from Chapter  that the first basis vector u1 of Uk is the defining vector that encodes the most information about c, while the subsequent basis vectors ui for i \u2265 2 capture more specific information about individual words in c. By using a smaller rank k, we throw away the more \u201cnoisy\u201d vectors ui for i \u2265 k \u2265 1, allowing Uk to capture the generation notion of category c better. This allows the regularization term to train the \u201ccategory\u201d component of v\u0302 more accurately. Note that the other component, which is specific to word w\u0302 and lies outside\nthe category subspace, is trained by the term g(Yww\u2032 ) ( ||v + vw\u2032 ||2 \u2212 log(Yww\u2032 ) +Z )2 in the objective (.). To illustrate our point, we trained two vectors for the same word w\u0302, one using a low rank k and the other using a high rank k, and compared their order and cosine score (see Figure .). For both w\u0302 = massachusetts and w\u0302 = hinduism, the vector learned using the lower rank resulted in a lower order and a much higher cosine score. This demonstrates that using a smaller rank k results in better performance for LEARN_VECTOR.\nTo compare the amount of co-occurrence data for w\u0302 in a subcorpus \u0393 to the amount of cooccurrence data for w\u0302 in the Wikipedia corpus C, we look at the fraction Yw\u0302(\u0393 )/Xw\u0302, which is listed in Table .. Note that the algorithm achieves very good performance while using only a small fraction of the total amount of co-occurrence data in the Wikipedia corpus C. For example, by using only Yw\u0302(\u0393 )/Xw\u0302 = 1.846e\u221202 of the total amount of co-occurrence data for the word w = christianity, the algorithm is able to learn a vector whose order is  and cosine score is 0.84.\nLastly, note that the performance depends heavily on the parameter values chosen, and can be further improved by fine-tuning the parameters.\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \n0\n10\n20\n30\n6 7 8 9 ln(Yw\u0302)\nor de\nr\n0.6\n0.7\n0.8\n6 7 8 9 ln(Yw\u0302)\nco si\nne s\nco re\nword\ncalifornia\nchristianity\ngermany\nhinduism\njapan\nmassachusetts\nprinceton\nFigure .: The order and cosine score of the learned vector v\u0302 returned by LEARN_VECTOR (Algorithm .) for word w\u0302, using varying amounts of co-occurrence data Yw\u0302. (See Section .. for the definitions of order and cosine score.) Note that in general, the order of v\u0302 is decreasing, and the cosine score of v\u0302 is increasing, in the amount of co-occurrence data Yw\u0302. (The occasional decrease in the cosine score is due to random initialization of the vector v\u0302.) Also, observe that the words for which rank k = 10 was used (california, germany, and japan) have lower cosine scores than the words for which rank k = 3 was used. The algorithm achieves very good performance while using only a small fraction of the total amount of co-occurrence data in the Wikipedia corpus C: For example, by using only Yw\u0302(\u0393 )/Xw\u0302 = 1.846e\u221202 of the total amount of co-occurrence data for the word w = christianity, the algorithm is able to learn a vector whose order is  and cosine score is 0.84.\n. Conclusion\nWe have demonstrated that, in principle, one can learn vectors with substantially less data by using the low-dimensional subspace of categories. An interesting experiment to try is the following: Use Algorithm . to learn vectors for rare words, and see if new facts can be discovered using these vectors. We leave this to future work.\nMoreover, one can extend this method to learn vectors for any words \u2013 even words that do not appear at all in the Wikipedia corpus \u2013 using web-scraping tools, such as Google search, to obtain additional co-occurrence data. However, the corpora obtained from Google search may be drawn from a different distribution than the wikipedia corpus, and hence skew the data in a certain way. We leave this to future work.\nOne weakness of LEARN_VECTOR is that it requires having prior knowledge of what category a word w\u0302 belongs in. If our prior knowledge is wrong, then the fitted vector for w\u0302 may be very bad. One could come up with an automatic method classifying which category w belongs to.\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \n(a) w = california\ni lnYw(\u0393i ) Yw(\u0393i )/Xw \u03b7i \u03bbi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(b) w = christianity\ni lnYw(\u0393i ) Yw(\u0393i )/Xw \u03b7i \u03bbi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(c) w = germany\ni lnYw(\u0393i ) Yw(\u0393i )/Xw \u03b7i \u03bbi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(d) w = hinduism\ni lnYw(\u0393i ) Yw(\u0393i )/Xw \u03b7i \u03bbi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(e) w = japan\ni lnYw(\u0393i ) Yw(\u0393i )/Xw \u03b7i \u03bbi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(f) w = massachusetts\ni lnYw(\u0393i ) Yw(\u0393i )/Xw \u03b7i \u03bbi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(g) w = princeton\ni lnYw(\u0393i ) Yw(\u0393i )/Xw \u03b7i \u03bbi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\nTable .: For each word w\u0302, we extracted six subcorpora \u03931, . . . ,\u03936 from the original corpus C, where \u03931 \u2282 \u03932 \u2282 \u00b7 \u00b7 \u00b7 \u2282 \u03936. We use Yw\u0302(\u0393 ) := \u2211 w\u2208D \u2032 Yw\u0302w to quantify the amount of co-occurrence data for word w\u0302 in a corpus \u0393 with vocabulary set D \u2032 . For each i \u2208 {1, . . . ,6}, we trained the algorithm on the subcorpus \u0393i for various learning rates \u03b7i and regularization weights \u03bbi . In the tables above, we list the parameter values \u03b7i , \u03bbi that resulted in the best performance (shown in Figure .). To compare the amount of co-occurrence data for w\u0302 in \u0393 to the amount of co-occurrence data for w\u0302 in C, we look at the fraction Yw\u0302(\u0393 )/Xw\u0302.\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \n0\n10\n20\n30\n6 7 8 9 ln(Yw\u0302)\nor de\nr\n0.5\n0.6\n0.7\n0.8\n6 7 8 9 ln(Yw\u0302)\nco si\nne s\nco re\nrank\u221210\nrank\u22123\n(a) w\u0302 = massachusetts\n0\n50\n100\n150\n6 7 8 9 ln(Yw\u0302)\nor de\nr\n0.4\n0.5\n0.6\n0.7\n0.8\n6 7 8 9 ln(Yw\u0302)\nco si\nne s\nco re\nrank\u221215\nrank\u22123\n(b) w\u0302 = hinduism\nFigure .: Using LEARN_VECTOR (Algorithm .), we trained two vectors for the same word w\u0302, one using a low rank k (blue) and the other using a high rank k (red), and compared their order and cosine score. (For each rank k, we tried various learning rates \u03b7 and regularization weights \u03bb to try to optimize performance; here, we provide the best performance found for each k.) For both w\u0302 = massachusetts and w\u0302 = hinduism, the vector learned using the lower rank resulted in a lower order and a much higher cosine score. This demonstrates that using a smaller rank k results in better performance for LEARN_VECTOR."}, {"heading": "Chapter ", "text": "Using an external knowledge source to reduce false-positive rate\nOne can use external knowledge sources such as a dictionary or Wordnet [] to filter false-positive answers and improve accuracy on analogy queries. In this section, we focus on analogy queries \u201ca:b::c:??\u201d where there exists categories cA, cB such that both a and c belong in cA, and both b and the correct answer d belong in cB. In other words, (a,b) and (c,d) both satisfy a common relation r that is well-defined.\nSOLVE_QUERY (Algorithm .) is a generic method for returning the top N answers to an analogy query \u201ca:b::c:??\u201d. In Section ., we present two ways for filtering the candidate list of answers to reduce false-positive rate: POS filter and LEX filter. Figures ., ., and . compare the accuracy of SOLVE_QUERY with and without these filters on  different relations r. We show that the POS filter always increases the accuracy (either slightly or significantly, depending on the relation r), unless the accuracy is already 100% without filter. On the other hand, the performance for the LEX filter varies widely, depending on the nature of the relation r. When used on appropriate relations r, such as facts-based relations (see Figure .(a)-(h)), the LEX filter can improve the accuracy by as much as +19.2% than without filter, and +17.7% than with POS filter (see Figure .(a)).\n. Analogy queries\nRecall that word embeddings allow one to solve analogy queries of the form \u201ca:b::c:??\u201d using simple vector arithmetics. More specifically, for two word pairs (a,b), (c,d) satisfying a common relation r, their word vectors satisfy\nva \u2212 vb \u2248 vc \u2212 vd .\nHence, a method to solve the analogy query \u201ca:b::c:??\u201d is to find the word d \u2208 D whose vector vd is closest to vb \u2212 va + vc.\nGiven a set of words \u2206 \u2286 D and a numberN \u2208 {1, . . . , |\u2206|}, SOLVE_QUERY (Algorithm .) returns the top N answers in \u2206 for the analogy query \u201ca:b::c:??\u201d. More specifically, it returns N words in \u2206 corresponding to the top N vectors in V\u2206 := {vw : w \u2208 \u2206} that are closest to the vector vb \u2212 va + vc. It is also used in other works (e.g. [, , ]) to evaluate a method\u2019s performance on analogy tasks.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \nfunction SOLVE_QUERY({a,b,c},\u2206,N ): Returns a list of N words in \u2206.\nInputs:\n\u2022 Words a,b,c for which we want to solve the analogy query \u201ca:b::c:??\u201d\n\u2022 \u2206 \u2286 D, a set of candidate answers\n\u2022 N \u2208 {1,2, . . . , |\u2206|}, the number of answers to return.\nStep . Let V\u2206 := {vw : w \u2208 \u2206}. Number the vectors v1,v2, . . . , v|\u2206| \u2208 V\u2206 in order of decreasing cosine similarity from the vector vb \u2212 va + vc. Let S := {v1,v2, . . . ,vN }.\nStep . Return the list {w \u2208 \u2206 : vw \u2208 S}.\nend function\nAlgorithm .: SOLVE_QUERY({a,b,c},\u2206,N ) returns top N answers from \u2206 for the analogy query \u201ca:b::c:??\u201d.\nWe say SOLVE_QUERY returns the correct answer if the correct answer d is in the set returned by the algorithm.\n. Wordnet\nIn Wordnet, each word is labeled with POS (\u201cpart-of-speech\u201d) and LEX (\u201clexicographic\u201d) tags. The POS tag indicates the syntactic category of a word, such as noun, verb, adjective, and adverb. The LEX tag is more specific: See Table . for a complete list of the LEX tags in Wordnet. For any word w \u2208 D, let pos(w) and lex(w) be the set of POS and LEX tags for w, respectively. For example, for the currency word w = euro,\npos(euro) = {noun}, lex(euro) = {noun.quantity}.\nDefine the following sets:\nDpos(w) := {w\u2032 \u2208 D : pos(w\u2032)\u2229pos(w) , \u2205}, Dlex(w) := {w\u2032 \u2208 D : lex(w\u2032)\u2229 lex(w) , \u2205}.\nIn other words, Dpos(w) is the set of words that share a common POS tag with w, and similarly, Dlex(w) is the set of words that share a common LEX tag with w. For example, Dpos(euro) contains all the noun words in D, and Dlex(euro) contains words such as dollar and kilometer which have the LEX tag noun.quantity.\nConsider the analogy query \u201ca:b::c:??\u201d where there exists categories cA and cB such that both a and c belong in cA, and both b and the correct answer d belong in cB. If we assume that every\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \nword in category cB share a common POS (or LEX) tag, then we can use Wordnet to filter out words in D which cannot belong in cB. More specifically, we only search among the words in Dpos(b) (or Dlex(b)) for the correct answer d. Note that LEX is a stronger filter than POS, in the sense that Dlex(b) \u2282 Dpos(b).\n. Experiment\nWe tested SOLVE_QUERY (Algorithm .) on  relations from Figure . in the following manner. For each relation r, let Sr \u2282 Dr be the set of word pairs contained in the correponding relation file (see Figure .). For each N \u2208 {1,5,10,25,50}, we performed the following experiment:\nStep . Initialize n = npos = nlex = 0.\nStep . For each (a,b) \u2208 Sr , and for each (c,d) \u2208 Sr such that (c,d) , (a,b), solve the analogy query \u201ca:b::c:??\u201d using Algorithm .:\nS\u2190 SOLVE_QUERY({a,b,c},D,N ) Spos\u2190 SOLVE_QUERY({a,b,c},Dpos(b),N ) Slex\u2190 SOLVE_QUERY({a,b,c},Dlex(b),N ).\nWe say S, Spos, and Slex are the answers returned by the algorithm without filter, with POS filter, and with LEX filter, respectively.\n\u2022 If d \u2208 S, then increment n by . \u2022 If d \u2208 Spos, then increment npos by . \u2022 If d \u2208 Slex, then increment nlex by .\nIn other words, we test the algorithm on the analogy queries \u201ca:b::c:??\u201d and \u201cc:d::a:??\u201d for every possible pairs (a,b), (c,d) \u2208 Sr . The total number of times the algorithm returns the correct answer without filter, with POS filter, and with LEX filter are n, npos, and nlex, respectively. Since the total number of analogy queries tested on is |Sr |(|Sr |\u22121), the accuracy of the algorithm without filter, with POS filter, and with LEX filter are given by n|Sr |(|Sr |\u22121) , npos |Sr |(|Sr |\u22121) , and nlex|Sr |(|Sr |\u22121) , respectively.\n. Performance\nFigures ., ., and . compare the accuracy of SOLVE_QUERY with and without filters on  different relations r, which are taken from Figure .. We ignore the performance on relation (n) gram-nationality-adj in Figure ., due to the fact that Wordnet does not have an entry for the word \u201cargentinean\u201d which is included in the test bed.\nDepending on the relation r, the POS filter always increases the accuracy, either slightly (see (a), (c), (j), (l), (m), (o)-(z)) or significantly (see (i)), unless the accuracy is already 100% without filter (see (b), (d), (e), (f), (k)).\nSo for all analogy queries \u201ca:b::c:??\u201d where either b or the correct answer d is the word argentinean, both POS and LEX filter out the correct answer from D, causing the algorithm to get the analogy query wrong and therefore lower its accuracy slightly.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \n# LEX tag Description  adj.all all adjective clusters  adj.pert relational adjectives (pertainyms)  adv.all all adverbs  noun.Tops unique beginner for nouns  noun.act nouns denoting acts or actions  noun.animal nouns denoting animals  noun.artifact nouns denoting man-made objects  noun.attribute nouns denoting attributes of people and objects  noun.body nouns denoting body parts  noun.cognition nouns denoting cognitive processes and contents  noun.communication nouns denoting communicative processes and contents  noun.event nouns denoting natural events  noun.feeling nouns denoting feelings and emotions  noun.food nouns denoting foods and drinks  noun.group nouns denoting groupings of people or objects  noun.location nouns denoting spatial position  noun.motive nouns denoting goals  noun.object nouns denoting natural objects (not man-made)  noun.person nouns denoting people  noun.phenomenon nouns denoting natural phenomena  noun.plant nouns denoting plants  noun.possession nouns denoting possession and transfer of possession  noun.process nouns denoting natural processes  noun.quantity nouns denoting quantities and units of measure  noun.relation nouns denoting relations between people or things or ideas  noun.shape nouns denoting two and three dimensional shapes  noun.state nouns denoting stable states of affairs  noun.substance nouns denoting substances  noun.time nouns denoting time and temporal relations  verb.body verbs of grooming, dressing and bodily care  verb.change verbs of size, temperature change, intensifying, etc.  verb.cognition verbs of thinking, judging, analyzing, doubting  verb.communication verbs of telling, asking, ordering, singing  verb.competition verbs of fighting, athletic activities  verb.consumption verbs of eating and drinking  verb.contact verbs of touching, hitting, tying, digging  verb.creation verbs of sewing, baking, painting, performing  verb.emotion verbs of feeling  verb.motion verbs of walking, flying, swimming  verb.perception verbs of seeing, hearing, feeling  verb.possession verbs of buying, selling, owning  verb.social verbs of political and social activities and events  verb.stative verbs of being, having, spatial relations  verb.weather verbs of raining, snowing, thawing, thundering  adj.ppl participial adjectives\nTable .: List of all LEX tags in Wordnet.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \nOn the other hand, the performance for the LEX filter varies widely depending on the nature of the relation r, due to the fact that LEX is a stronger filter than POS. For relations where words belonging to cB share a common LEX tag (e.g., (a), (c), (i), (j), (l), (r)-(v), (z)), LEX improves the accuracy significantly by filtering out false-positive answers. On the contrary, for relations where words belonging to cB have different LEX tags (e.g., (m), (o)-(q), (w), (y)), LEX filters out the correct answer from D, and hence worsens the accuracy significantly. When used on appropriate relations r, such as facts-based relations (see Figure .(a)-(h)), the LEX filter can improve the accuracy by as much as +19.2% than without filter, and +17.7% than with POS filter (see the plot in Figure .(a) for N = 50).\n. Conclusion\nWe have shown that external knowledge sources such as Wordnet can be used to improve accuracy on analogy queries, sometimes significantly, by filtering out false-positive answers. As an extension of the idea, we can apply the Wordnet filter to EXTEND_CATEGORY (Algorithm .) for learning new words in a category, or EXTEND_RELATION (Algorithm .) for learning new word pairs in a relation, to decrease the false-positive rate and improve its performance. We leave this to future work.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(a) country\u2212currency\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(b) holiday\u2212month\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(c) holiday\u2212religion\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(d) country\u2212language\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(e) country\u2212capital2\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(f) city\u2212in\u2212state\nFigure .: Accuracy of the algorithm without filter (green), with POS filter (blue), and with LEX filter (red) on the facts-based relation files from Figure .(a)-(f). For (a) and (c), the LEX filter improves the accuracy significantly, while the POS filter improves the accuracy only slightly. For all other relation files, the algorithm already achieves an accuracy of  without filter.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(i) gram1\u2212adj\u2212adv\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(j) gram2\u2212opposite\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(k) gram3\u2212comparative\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(l) gram4\u2212superlative\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(m) gram5\u2212present\u2212particle\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(n) gram6\u2212nationality\u2212adj\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(o) gram7\u2212past\u2212tense\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(p) gram8\u2212plural\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(q) gram9\u2212plural\u2212verbs\nFigure .: Accuracy of the algorithm without filter (green), with POS filter (blue), and with LEX filter (red) on the grammar-based relation files from Figure .(i)-(q). For (n), both the LEX filter and the POS filter worsen the accuracy slightly, due to the fact that Wordnet does not have an entry for the word \u201cargentinean\u201d. For all other relation files, POS improves the accuracy by a modest amount. LEX improves the accuracy for (i), (j), and (l), but performs very poorly for (m), (o)-(q), due to the fact that words belonging to cB have different LEX tags, causing LEX to filter out the correct answer.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(r) associated\u2212number\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(s) associated\u2212position\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(t) common\u2212very\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(u) graded\u2212antonym\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(v) has\u2212characteristic\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(w) has\u2212function\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(x) has\u2212skin\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(y) noun\u2212baby\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(z) senses\nFigure .: Accuracy of the algorithm without filter (green), with POS filter (blue), and with LEX filter (red) on the semantics-based relation files from Figure .(r)-(z). LEX filter worsens the accuracy for (w) and (y), but improves the accuracy significantly on all other relation files. POS filter consistently improves the accuracy on all relation files."}, {"heading": "Chapter ", "text": ""}, {"heading": "Conclusion", "text": "We have demonstrated that the linear algebraic structure of word embeddings can be used to reduce data requirements for methods of learning facts. In particular, we demonstrated that categories and relations form a low-rank subspace Uk = {u1, . . . ,uk} in the projected space (Chapter ), and this subspace can be used to discover new facts with fairly low false-positive rates () and learn new vectors for words with substantially less co-occurrence data (Chapter ).\nIn Chapter , we demonstrated that the first basis vector u1 of a low-rank subspace encodes the most general information about a category c (or a relation r), whereas the subsequent basis vectors ui for i \u2265 2 encode more \u201cspecific\u201d information pertaining to individual words in Dc (or word pairs in Dr ). It remains to be discovered what specific features are captured by these basis vectors for various categories and relations. For example, if Uk is a basis for the subspace of category c = country, then perhaps having a positive second coordinate vw \u00b7 u2 in the basis indicates that w is a developed country, and having a negative fourth coordinate vw \u00b7 u4 indicates that country w is located in Europe. We leave this to future work.\nIn Chapter , we used the low-dimensional subspace of categories and relations to discover new facts with fairly low false-positive rates. The performance of EXTEND_RELATION (Algorithm .) is fairly surprising, given the simplicity of the algorithm and the fact that it returns plausible word pairs out of all possible word pairs in D\u00d7D. At the very least, EXTEND_RELATION has shown to drastically narrow down the search space for discovering new word pairs satisfying a given relation, and allows for sharper classification than simple clustering. It can supplement other methods for extending knowledge bases to improve efficiency and attain even higher accuracy rates. One could also combine EXTEND_RELATION with POS and LEX filters described in Chapter , or explore other ways of utilizing external knowledge sources (e.g., a dictionary) to filter falsepositive answers.\nIn Chapter  we demonstrated that, in principle, one can learn vectors with substantially less data by using the low-dimensional subspace of categories. An interesting experiment to try is the following: Use LEARN_VECTOR to learn vectors for rare words in the Wikipedia corpus, and see if new facts can be discovered using these vectors. It would also be interesting to try variants of the objective (.), perhaps by adding the regularization term \u03bb\u2016v\u0302 \u2212Ukb\u20162 to other existing objectives such as GloVe [].\nMoreover, one can extend this method to learn vectors for any words \u2013 even words that do not appear at all in the Wikipedia corpus \u2013 using web-scraping tools, such as Google search, to obtain\nOn the Linear Structure of Word Embeddings Chapter . Conclusion | \nadditional co-occurrence data. However, the corpora obtained from Google search may be drawn from a different distribution than the Wikipedia corpus, and hence could skew the data in a certain way. We leave this to future work."}], "references": [], "referenceMentions": [], "year": 2015, "abstractText": "In this work, we leverage the linear algebraic structure of distributed word representations to automatically extend knowledge bases and allow a machine to learn new facts about the world. Our goal is to extract structured facts from corpora in a simpler manner, without applying classifiers or patterns, and using only the co-occurrence statistics of words. We demonstrate that the linear algebraic structure of word embeddings can be used to reduce data requirements for methods of learning facts. In particular, we demonstrate that words belonging to a common category, or pairs of words satisfying a certain relation, form a low-rank subspace in the projected space. We compute a basis for this low-rank subspace using singular value decomposition (SVD), then use this basis to discover new facts and to fit vectors for less frequent words which we do not yet have vectors for. This thesis represents my own work in accordance with university regulations.", "creator": "LaTeX with hyperref package"}}}