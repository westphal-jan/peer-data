{"id": "1511.07147", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "A PAC Approach to Application-Specific Algorithm Selection", "abstract": "establishing the suitable best algorithm system for a computational problem generally depends indirectly on the \" relevant inputs, \" a concept formulated that entirely depends on the application access domain and often defies formal articulation. recently while noteworthy there recently is a large literature specializing on empirical approaches to selecting the best algorithm models for a given application domain, sometimes there has been surprisingly remarkably little sophisticated theoretical feasibility analysis way of handling the naive problem.", "histories": [["v1", "Mon, 23 Nov 2015 09:30:19 GMT  (507kb,D)", "http://arxiv.org/abs/1511.07147v1", "28 pages, 2 figures"], ["v2", "Fri, 2 Sep 2016 21:06:20 GMT  (520kb,D)", "http://arxiv.org/abs/1511.07147v2", "28 pages, 2 figures"]], "COMMENTS": "28 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["rishi gupta", "tim roughgarden"], "accepted": false, "id": "1511.07147"}, "pdf": {"name": "1511.07147.pdf", "metadata": {"source": "CRF", "title": "A PAC Approach to Application-Specific Algorithm Selection\u2217", "authors": ["Rishi Gupta", "Tim Roughgarden"], "emails": ["rishig@cs.stanford.edu", "tim@cs.stanford.edu"], "sections": [{"heading": null, "text": "This paper adapts concepts from statistical and online learning theory to reason about application-specific algorithm selection. Our models capture several state-of-the-art empirical and theoretical approaches to the problem, ranging from self-improving algorithms to empirical performance models, and our results identify conditions under which these approaches are guaranteed to perform well. We present one framework that models algorithm selection as a statistical learning problem, and our work here shows that dimension notions from statistical learning theory, historically used to measure the complexity of classes of binary- and real-valued functions, are relevant in a much broader algorithmic context. We also study the online version of the algorithm selection problem, and give possibility and impossibility results for the existence of no-regret learning algorithms."}, {"heading": "1 Introduction", "text": "Rigorously comparing algorithms is hard. The most basic reason for this is that two different algorithms for a computational problem generally have incomparable performance: one algorithm is better on some inputs, but worse on the others. How can a theory advocate one of the algorithms over the other? The simplest and most common solution in the theoretical analysis of algorithms is to summarize the performance of an algorithm using a single number, such as its worst-case performance or its average-case performance with respect to an input distribution. This approach effectively advocates using the algorithm with the best summarizing value (e.g., the smallest worstcase running time).\nSolving a problem \u201cin practice\u201d generally means identifying an algorithm that works well for most or all instances of interest. When the \u201cinstances of interest\u201d are easy to specify formally in advance \u2014 say, planar graphs \u2014 the traditional analysis approaches often give accurate performance predictions and identify useful algorithms. However, instances of interest commonly possess domain-specific features that defy formal articulation. Solving a problem in practice can require selecting an algorithm that is optimized for the specific application domain, even though the special\n\u2217A preliminary version of this paper appeared in the Proceedings of the 7th Innovations in Theoretical Computer Science Conference, January 2016.\nar X\niv :1\n51 1.\n07 14\n7v 1\n[ cs\n.L G\n] 2\n3 N\nov 2\nstructure of its instances is not well understood. While there is a large literature, spanning numerous communities, on empirical approaches to algorithm selection (e.g. [Fin98, HXHL14, HRG+01, HJY+10, KGM12, LNS09]), there has been surprisingly little theoretical analysis of the problem. One possible explanation is that worst-case analysis, which is the dominant algorithm analysis paradigm in theoretical computer science, is deliberately application-agnostic.\nThis paper demonstrates that application-specific algorithm selection can be usefully modeled as a learning problem. Our models are straightforward to understand, but also expressive enough to capture several existing approaches in the theoretical computer science and AI communities, ranging from the design and analysis of self-improving algorithms [ACCL06] to the application of empirical performance models [HXHL14].\nWe present one framework that models algorithm selection as a statistical learning problem. We prove that many useful families of algorithms, including broad classes of greedy and local search heuristics, have small pseudo-dimension and hence low generalization error. Previously, the pseudo-dimension (and the VC dimension, fat shattering dimension, etc.) has been used almost exclusively to quantify the complexity of classes of prediction functions (e.g. [AB99]).1 Our results demonstrate that this concept is useful and relevant in a much broader algorithmic context. It also offers a novel approach to formalizing the oft-mentioned but rarely-defined \u201csimplicity\u201d of a family of algorithms.\nWe also study regret-minimization in the online version of the algorithm selection problem. We show that the \u201cnon-Lipschitz\u201d behavior of natural algorithm classes precludes learning algorithms that have no regret in the worst case, and prove positive results under smoothed analysis-type assumptions.\nPaper Organization Section 2 outlines a number of concrete problems that motivate the present work, ranging from greedy heuristics to SAT solvers, and from self-improving algorithms to parameter tuning. The reader interested solely in the technical development can skip this section with little loss. Section 3 models the task of determining the best application-specific algorithm as a PAC learning problem, and brings the machinery of statistical learning theory to bear on a wide class of problems, including greedy heuristic selection, sorting, and gradient descent step size selection. A time-limited reader can glean the gist of our contributions from Sections 3.1\u20133.3.3. Section 4 considers the problem of learning an application-specific algorithm online, with the goal of minimizing regret. Sections 4.2 and 4.3 present negative and positive results for worst-case and smoothed instances, respectively. Section 5 concludes with a number of open research directions."}, {"heading": "2 Motivating Scenarios", "text": "Our learning framework sheds light on several well-known approaches, spanning disparate application domains, to the problem of learning a good algorithm from data. To motivate and provide interpretations of our results, we describe several of these in detail.\n1A few exceptions: [Lon01] parameterizes the performance of the randomized rounding of packing and covering linear programs by the pseudo-dimension of a set derived from the constraint matrix, and [MM14, MR15] use dimension notions from learning theory to bound the sample complexity of learning approximately revenue-maximizing truthful auctions."}, {"heading": "2.1 Example #1: Greedy Heuristic Selection", "text": "One of the most common and also most challenging motivations for algorithm selection is presented by computationally difficult optimization problems. When the available computing resources are inadequate to solve such a problem exactly, heuristic algorithms must be used. For most hard problems, our understanding of when different heuristics work well remains primitive. For concreteness, we describe one current and high-stakes example of this issue, which also aligns well with our model and results in Section 3.3. The computing and operations research literature has many similar examples.\nIn 2016 the FCC is slated to run a novel double auction to buy back licenses for spectrum from certain television broadcasters and resell them to telecommunication companies for wireless broadband use. The auction is expected to generate over $20 billion dollars for the US government [CBO14]. The \u201creverse\u201d (i.e., buyback) phase of the auction must determine which stations to buy out (and what to pay them). The auction is tasked with buying out sufficiently many stations so that the remaining stations (who keep their licenses) can be \u201crepacked\u201d into a small number of channels, leaving a target number of channels free to be repurposed for wireless broadband. To first order, the feasible repackings are determined by interference constraints between stations. Computing a repacking therefore resembles familiar hard combinatorial problems like the independent set and graph coloring problems. In the currently proposed auction format [MS14], the plan is to use a greedy heuristic to compute the order in which stations are removed from the reverse auction (removal means the station keeps its license). The proposed approach is to favor stations with high value, and discriminate against stations that interfere with a large number of other stations.2 There are many ways of combining these two criteria, and no obvious reason to favor one specific implementation over another. The currently proposed implementation for the FCC auction has been justified through trial-and-error experiments using synthetic instances that are thought to be representative [MS14]. One interpretation of our results in Section 3.3 is as a post hoc justification of this exhaustive approach for sufficiently simple classes of algorithms, including the greedy heuristics proposed for this FCC auction."}, {"heading": "2.2 Example #2: Self-Improving Algorithms", "text": "The area of self-improving algorithms was initiated by [ACCL06], who considered sorting and clustering problems. Subsequent work [CS08, CMS10, CMS12] studied several problems in lowdimensional geometry, including the maxima and convex hull problems. For a given problem, the goal is to design an algorithm that, given a sequence of i.i.d. samples from an unknown distribution over instances, converges to the optimal algorithm for that distribution. In addition, the algorithm should use only a small amount of auxiliary space. For example, for sorting independently distributed array entries, the algorithm in [ACCL06] solves each instance (on n numbers) in O(n log n) time, uses space O(n1+c) (where c > 0 is an arbitrarily small constant), and after a polynomial number of samples has expected running time within a constant factor of that of an information-theoretically optimal algorithm for the unknown input distribution. Section 3.4 reinterprets self-improving algorithms via our general framework.\n2Analogously, greedy heuristics for the maximum-weight independent set problem favor vertices with higher weights and with lower degrees [STY03]. Greedy heuristics for welfare maximization in combinatorial auctions prefer bidders with higher values and smaller demanded bundles [LOS02]."}, {"heading": "2.3 Example #3: Parameter Tuning in Optimization and Machine Learning", "text": "Many \u201calgorithms\u201d used in practice are really meta-algorithms, with a large number of free parameters that need to be instantiated by the user. For instance, implementing even in the most basic version of gradient descent requires choosing a step size and error tolerance. For a more extreme version, CPLEX, a widely-used commercial linear and integer programming solver, comes with a 221-page parameter reference manual describing 135 parameters [XHHL11].\nAn analogous problem in machine learning is \u201chyperparameter optimization,\u201d where the goal is to tune the parameters of a learning algorithm so that it learns (from training data) a model with high accuracy on test data, and in particular a model that does not overfit the training data. A simple example is regularized regression, such as ridge regression, where a single parameter governs the trade-off between the accuracy of the learned model on training data and its \u201ccomplexity.\u201d More sophisticated learning algorithms can have many more parameters.\nFiguring out the \u201cright\u201d parameter values is notoriously challenging in practice. The CPLEX manual simply advises that \u201cyou may need to experiment with them.\u201d In machine learning, parameters are often set by discretizing and then applying brute-force search (a.k.a. \u201cgrid search\u201d), perhaps with random subsampling (\u201crandom search\u201d) [BB12]. When this is computationally infeasible, variants of gradient descent are often used to explore the parameter space, with no guarantee of converging to a global optimum.\nThe results in Section 3.6 can be interpreted as a sample complexity analysis of grid search for the problem of choosing the step size in gradient descent to minimize the expected number of iterations needed for convergence. We view this as a first step towards reasoning more generally about the problem of learning good parameters for machine learning algorithms."}, {"heading": "2.4 Example #4: Empirical Performance Models for SAT Algorithms", "text": "The examples above already motivate selecting an algorithm for a problem based on characteristics of the application domain. A more ambitious and refined approach is to select an algorithm on a per-instance (instead of a per-domain) basis. While it\u2019s impossible to memorize the best algorithm for every possible instance, one might hope to use coarse features of a problem instance as a guide to which algorithm is likely to work well.\nFor example, [XHHL08] applied this idea to the satisfiability (SAT) problem. Their algorithm portfolio consisted of a small number (precisely, 7) of state-of-the-art SAT solvers with incomparable and widely varying running times across different instances. The authors identified a number of instance features, ranging from simple features like input size and clause/variable ratio, to complex features like Knuth\u2019s estimate of search tree size [Knu75] and the rate of progress of local search probes.3 The next step involved building an \u201cempirical performance model\u201d (EPM) for each of the 7 algorithms in the portfolio \u2014 a mapping from instance feature vectors to running time predictions. They then computed their EPMs using labeled training data and a suitable regression model. With the EPMs in hand, it is clear how to perform per-instance algorithm selection: given an instance, compute its features, use the EPMs to predict the running time of each algorithm in the portfolio, and run the algorithm with the smallest predicted running time. Using these ideas (and several optimizations), their \u201cSATzilla\u201d algorithm won numerous medals at the 2007 SAT\n3It is important, of course, that computing the features of an instance is an easier problem than solving it.\nCompetition.4 Section 3.5 outlines how to extend our PAC learning framework to reason about EPMs and feature-based algorithm selection."}, {"heading": "3 PAC Learning an Application-Specific Algorithm", "text": "This section casts the problem of selecting the best algorithm for a poorly understood application domain as one of learning the optimal algorithm with respect to an unknown instance distribution. Section 3.1 formally defines the basic model, Section 3.2 reviews relevant preliminaries from statistical learning theory, Section 3.3 bounds the pseudo-dimension of many classes of greedy and local search heuristics, Section 3.4 re-interprets the theory of self-improving algorithms via our framework, Section 3.5 extends the basic model to capture empirical performance models and feature-based algorithm selection, and Section 3.6 studies step size selection in gradient descent."}, {"heading": "3.1 The Basic Model", "text": "Our basic model consists of the following ingredients.\n1. A fixed computational or optimization problem \u03a0. For example, \u03a0 could be computing a maximum-weight independent set of a graph (Section 2.1), or sorting n elements (Section 2.2).\n2. An unknown distribution D over instances x \u2208 \u03a0.\n3. A set A of algorithms for \u03a0; see Sections 3.3 and 3.4 for concrete examples.\n4. A performance measure cost : A\u00d7\u03a0\u2192 [0, H] indicating the performance of a given algorithm on a given instance. Two common choices for cost are the running time of an algorithm, and, for optimization problems, the objective function value of the solution produced by an algorithm.\nThe \u201capplication-specific information\u201d is encoded by the unknown input distribution D, and the corresponding \u201capplication-specific optimal algorithm\u201d AD is the algorithm that minimizes or maximizes (as appropriate) Ex\u2208D[cost(A, x)] over A \u2208 A. The error of an algorithm A \u2208 A for a distribution D is \u2223\u2223Ex\u223cD[cost(A, x)]\u2212Ex\u223cD[cost(AD, x)]\u2223\u2223.\nIn our basic model, the goal is:\nLearn the application-specific optimal algorithm from data (i.e., samples from D).\nMore precisely, the learning algorithm is given m i.i.d. samples x1, . . . , xm \u2208 \u03a0 from D, and (perhaps implicitly) the corresponding performance cost(A, xi) of each algorithm A \u2208 A on each input xi. The learning algorithm uses this information to suggest an algorithm A\u0302 \u2208 A to use on future inputs drawn from D. We seek learning algorithms that almost always output an algorithm of A that performs almost as well as the optimal algorithm in A for D.\nDefinition 3.1 A learning algorithm L ( , \u03b4)-learns the optimal algorithm in A from m samples if, for every distribution D over \u03a0, with probability at least 1\u2212 \u03b4 over m samples x1, . . . , xm \u223c D, L outputs an algorithm A\u0302 \u2208 A with error at most .\n4See [XHHL12] for details on the latest generation of their solver."}, {"heading": "3.2 Pseudo-Dimension and Uniform Convergence", "text": "PAC learning an optimal algorithm, in the sense of Definition 3.1, reduces to bounding the \u201ccomplexity\u201d of the class A of algorithms. We next review the relevant definitions from statistical learning theory.\nLet H denote a set of real-valued functions defined on the set X. A finite subset S = {x1, . . . , xm} of X is (pseudo-)shattered by H if there exist real-valued witnesses r1, . . . , rm such that, for each of the 2m subsets T of S, there exists a function h \u2208 H such that h(xi) > ri if and only if i \u2208 T (for i = 1, 2, . . . ,m). The pseudo-dimension of H is the cardinality of the largest subset shattered by H (or +\u221e, if arbitrarily large finite subsets are shattered by H). The pseudo-dimension is a natural extension of the VC dimension from binary-valued to real-valued functions.5\nTo bound the sample complexity of accurately estimating the expectation of all functions in H, with respect to an arbitrary probability distribution D on X, it is enough to bound the pseudodimension of H.\nTheorem 3.2 (E.g. [Hau92, Corollary 2]) Let H be a class of functions with domain X and range in [0, H], and suppose H has pseudo-dimension dH. For every distribution D over X, every > 0, and every \u03b4 \u2208 (0, 1], if\nm \u2265 c ( H )2( dH ln ( H ) + ln ( 1\n\u03b4\n)) (1)\nfor a suitable constant c (independent of all other parameters), then with probability at least 1\u2212 \u03b4 over m samples x1, . . . , xm \u223c D,\u2223\u2223\u2223\u2223\u2223 ( 1 m m\u2211 i=1 h(xi) ) \u2212Ex\u223cD[h(x)]\n\u2223\u2223\u2223\u2223\u2223 < for every h \u2208 H.\nWe can identify each algorithm A \u2208 A with the real-valued function x 7\u2192 cost(A, x). Regarding the class A of algorithms as a set of real-valued functions defined on \u03a0, we can discuss its pseudodimension, as defined above. We need one more definition before we can apply our machinery to learn algorithms from A.\nDefinition 3.3 (Empirical Risk Minimization (ERM)) Fix an optimization problem \u03a0, a performance measure cost, and a set of algorithms A. An algorithm L is an ERM algorithm if, given any finite subset S of \u03a0, L returns an (arbitrary) algorithm from A with the best average performance on S.\nFor example, for any \u03a0, cost, and finite A, there is a trivial ERM algorithm which simply computes the average performance of each algorithm on S by brute force, and returns the best one. The next corollary follows easily from Definition 3.1, Theorem 3.2, and Definition 3.3.\n5The fat shattering dimension is another common extension of VC dimension to real-valued functions. It is a weaker condition, in that the fat shattering dimension of H is always at most the pseudo-dimension of H, that is still sufficient for sample complexity bounds. Most of our arguments give the same upper bounds on pseudo-dimension and fat shattering dimension, so we present the stronger statements.\nCorollary 3.4 Fix parameters > 0, \u03b4 \u2208 (0, 1], a set of problem instances \u03a0, and a performance measure cost. Let A be a set of algorithms that has pseudo-dimension d with respect to \u03a0. Then any ERM algorithm (2 , \u03b4)-learns the optimal algorithm in A from m samples, where m is defined as in (1).\nCorollary 3.4 is only interesting if interesting classes of algorithms A have small pseudodimension. In the simple case where A is finite, as in our example of an algorithm portfolio for SAT (Sections 2.4 and 3.5.2), the pseudo-dimension of A is trivially at most log2 |A|. The next few sections demonstrate the much less obvious fact that natural infinite classes of algorithms also have small pseudo-dimension.6"}, {"heading": "3.3 Application: Greedy Heuristics and Extensions", "text": "The goal of this section is to bound the pseudo-dimension of many classes of greedy heuristics including, as a special case, the family of heuristics relevant for the FCC double auction described in Section 2.1. It will be evident that analogous computations are possible for many other classes of heuristics, and we provide several extensions in Section 3.3.4 to illustrate this point. Throughout this section, the performance measure cost is the objective function value of the solution produced by a heuristic on an instance, where we assume without loss of generality a maximization objective."}, {"heading": "3.3.1 Definitions and Examples", "text": "Our general definitions are motivated by greedy heuristics for (NP -hard) problems like the following; the reader will have no difficulty coming up with additional natural examples.\n1. Knapsack. The input is n items with values v1, . . . , vn, sizes s1, . . . , sn, and a knapsack capacity C. The goal is to compute a subset of S \u2286 {1, 2, . . . , n} with maximum total value\u2211 i\u2208S vi, subject to having total size \u2211\ni\u2208S si at most C. Two natural greedy heuristics are to greedily pack items (subject to feasibility) in order of nonincreasing value vi, or in order of nonincreasing density vi/si (or to take the better of the two, see Section 3.3.4).\n2. Maximum-Weight Independent Set (MWIS). The input is an undirected graph G = (V,E) and a non-negative weight wv for each vertex v \u2208 V . The goal is to compute the independent set \u2014 a subset of mutually non-adjacent vertices \u2014 with maximum total weight. Two natural greedy heuristics are to greedily choose vertices (subject to feasibility) in order of nonincreasing weight wv, or nonincreasing density wv/(1 + deg(v)). (The intuition for the denominator is that choosing v \u201cuses up\u201d 1+deg(v) vertices \u2014 v and all of its neighbors.) The latter heuristic also has a (superior) adaptive variant, where the degree deg(v) is computed in the subgraph induced by the vertices not yet blocked from consideration, rather than in the original graph.7\n3. Machine Scheduling. This is a family of optimization problems, where n jobs with various attributes (processing time, weight, deadline, etc.) need to be assigned to m machines,\n6The present work focuses on the sample complexity rather than the computational aspects of learning, so outside of a few remarks we won\u2019t say much about the existence or efficiency of ERM in our examples. A priori, an infinite class of algorithms may not admit any ERM algorithm at all, though all of the examples in this section do have ERM algorithms under mild assumptions.\n7An equivalent description is: whenever a vertex v is added to the independent set, delete v and its neighbors from the graph, and recurse on the remaining graph.\nperhaps subject to some constraints (precedence constraints, deadlines, etc.), to optimize some objective (makespan, weighted sum of completion times, number of late jobs, etc.). A typical greedy heuristic for such a problem considers jobs in some order according to a score derived from the job parameters (e.g., weight divided by processing time), subject to feasibility, and always assigns the current job to the machine that currently has the lightest load (again, subject to feasibility).\nIn general, we consider object assignment problems, where the input is a set of n objects with various attributes, and the feasible solutions consist of assignments of the objects to a finite set R, subject to feasibility constraints. The attributes of an object are represented as an element \u03be of an abstract set. For example, in the Knapsack problem \u03be encodes the value and size of an object; in the MWIS problem, \u03be encodes the weight and (original or residual) degree of a vertex. In the Knapsack and MWIS problems, R = {0, 1}, indicating whether or not a given object is selected. In machine scheduling problems, R could be {1, 2, . . . ,m}, indicating the machine to which a job is assigned, or a richer set that also keeps track of the job ordering on each machine.\nBy a greedy heuristic, we mean algorithms of the following form (cf., the \u201cpriority algorithms\u201d of [BNR03]):\n1. While there remain unassigned objects:\n(a) Use a scoring rule \u03c3 (see below) to compute a score \u03c3(\u03bei) for each unassigned object i, as a function of its current attributes \u03bei.\n(b) For the unassigned object i with the highest score, use an assignment rule to assign i a value from R and, if necessary, update the attributes of the other unassigned objects.8\nFor concreteness, assume that ties are always resolved lexicographically.\nA scoring rule assigns a real number to an object as a function of its attributes. Assignment rules that do not modify objects\u2019 attributes yield non-adaptive greedy heuristics, which use only the original attributes of each object (like vi or vi/si in the Knapsack problem, for instance). In this case, objects\u2019 scores can be computed in advance of the main loop of the greedy heuristic. Assignment rules that modify object attributes yield adaptive greedy heuristics, such as the adaptive MWIS heuristic described above.\nIn a single-parameter family of scoring rules, there is a scoring rule of the form \u03c3(\u03c1, \u03be) for each parameter value \u03c1 in some interval I \u2286 R. Moreover, \u03c3 is assumed to be continuous in \u03c1 for each fixed value of \u03be. Natural examples include Knapsack scoring rules of the form vi/s \u03c1 i and MWIS scoring rules of the form wv/(1 + deg(v)) \u03c1 for \u03c1 \u2208 [0, 1] or \u03c1 \u2208 [0,\u221e). A single-parameter family of scoring rules is \u03ba-crossing if, for each distinct pair of attributes \u03be, \u03be\u2032, there are at most \u03ba values of \u03c1 for which \u03c3(\u03c1, \u03be) = \u03c3(\u03c1, \u03be\u2032). For example, all of the scoring rules mentioned above are 1-crossing rules.\nFor an example assignment rule, in the Knapsack and MWIS problems, the rule simply assigns i to \u201c1\u201d if it is feasible to do so, and to \u201c0\u201d otherwise. A typical machine scheduling assignment rule assigns the current job to the machine with the lightest load. In the adaptive greedy heuristic for the MWIS problem, whenever the assignment rule assigns \u201c1\u201d to a vertex v, it updates the residual degrees of other unassigned vertices (two hops away) accordingly.\n8We assume that there is always as least one choice of assignment that respects the feasibility constraints; this holds for all of our motivating examples.\nWe call an assignment rule \u03b2-bounded if every object i is guaranteed to take on at most \u03b2 distinct attribute values. For example, an assignment rule that never modifies an object\u2019s attributes is 1- bounded. The assignment rule in the adaptive MWIS algorithm is n-bounded, since it only modifies the degree of a vertex (which lies in {0, 1, 2 . . . , n\u2212 1}).\nCoupling a single-parameter family of \u03ba-crossing scoring rules with a fixed \u03b2-bounded assignment rule yields a (\u03ba, \u03b2)-single-parameter family of greedy heuristics. All of our running examples of greedy heuristics are (1, 1)-single-parameter families, except for the adaptive MWIS heuristic, which is a (1, n)-single-parameter family."}, {"heading": "3.3.2 Upper Bound on Pseudo-Dimension", "text": "We next show that every (\u03ba, \u03b2)-single-parameter family of greedy heuristics has small pseudodimension. This result applies to all of the concrete examples mentioned above, and it is easy to come up with other examples (for the problems already discussed, and for additional problems).\nTheorem 3.5 (Pseudo-Dimension of Greedy Algorithms) If A denotes a (\u03ba, \u03b2)-single-parameter family of greedy heuristics for an object assignment problem with n objects, then the pseudodimension of A is O(log(\u03ba\u03b2n)).\nIn particular, all of our running examples are classes of heuristics with pseudo-dimension O(log n).\nProof: Recall from the definitions (Section 3.2) that we need to upper bound the size of every set that is shatterable using the greedy heuristics in A. For us, a set is a fixed set of s inputs (each with n objects) S = x1, . . . , xs. For a potential witness r1, . . . , rs \u2208 R, every algorithm A \u2208 A induces a binary labeling of each sample xi, according to whether cost(A, xi) is strictly more than or at most ri. We proceed to bound from above the number of distinct binary labellings of S induced by the algorithms of A, for any potential witness.\nConsider ranging over algorithms A \u2208 A \u2014 equivalently, over parameter values \u03c1 \u2208 I. The trajectory of a greedy heuristic A \u2208 A is uniquely determined by the outcome of the comparisons between the current scores of the unassigned objects in each iteration of the algorithm. Since the family uses a \u03ba-crossing scoring rule, for every pair i, j of distinct objects and possible attributes \u03bei, \u03bej , there are at most \u03ba values of \u03c1 for which there is a tie between the score of i (with attributes \u03bei) and that of j (with attributes \u03bej). Since \u03c3 is continuous in \u03c1, the relative order of the score of i (with \u03bei) and j (with \u03bej) remains the same in the open interval between two successive values of \u03c1 at which their scores are tied. The upshot is that we can partition I into at most \u03ba+ 1 intervals such that the outcome of the comparison between i (with attributes \u03bei) and j (with attributes \u03bej) is constant on each interval.9\nNext, the s instances of S contain a total of sn objects. Each of these objects has some initial attributes. Because the assignment rule is \u03b2-bounded, there are at most sn\u03b2 object-attribute pairs (i, \u03bei) that could possibly arise in the execution of any algorithm from A on any instance of S. This implies that, ranging across all algorithms of A on all inputs in S, comparisons are only ever made between at most (sn\u03b2)2 pairs of object-attribute pairs (i.e., between an object i with current attributes \u03bei and an object j with current attributes \u03bej). We call these the relevant comparisons.\n9This argument assumes that \u03bei 6= \u03bej . If \u03bei = \u03bej , then because we break ties between equal scores lexicographically, the outcome of the comparison between \u03c3(\u03bei) and \u03c3(\u03bej) is in fact constant on the entire interval I of parameter values.\nFor each relevant comparison, we can partition I into at most \u03ba + 1 subintervals such that the comparison outcome is constant (in \u03c1) in each subinterval. Intersecting the partitions of all of the at most (sn\u03b2)2 relevant comparisons splits I into at most (sn\u03b2)2\u03ba + 1 subintervals such that every relevant comparison is constant in each subinterval. That is, all of the algorithms of A that correspond to the parameter values \u03c1 in such a subinterval execute identically on every input in S. The number of binary labellings of S induced by algorithms of A is trivially at most the number of such subintervals. Our upper bound (sn\u03b2)2\u03ba+ 1 on the number of subintervals exceeds 2s, the requisite number of labellings to shatter S, only if s = O(log(\u03ba\u03b2n)).\nTheorem 3.5 and Corollary 3.4 imply that, if \u03ba and \u03b2 are bounded above by a polynomial in n, then an ERM algorithm would ( , \u03b4)-learn the optimal algorithm in A from only m = O\u0303(H2\n2 )\nsamples,10 where H is the largest objective function value of a feasible solution output by an algorithm of A on an instance of \u03a0.11\nWe note that Theorem 3.5 gives a quantifiable sense in which natural greedy algorithms are indeed \u201csimple algorithms.\u201d Not all classes of algorithms have such a small pseudo-dimension; see also the next section for further discussion.12\nRemark 3.6 (Non-Lipschitzness) We noted in Section 3.2 that the pseudo-dimension of a finite set A is always at most log2 |A|. This suggests a simple discretization approach to learning the best algorithm from A: take a finite \u201c -net\u201d of A and learn the best algorithm in the finite net. (Indeed, Section 3.6 uses precisely this approach.) The issue is that without some kind of Lipschitz condition \u2014 stating that \u201cnearby\u201d algorithms in A have approximately the same performance on all instances \u2014 there\u2019s no reason to believe that the best algorithm in the net is almost as good as the best algorithm from all of A. Two different greedy heuristics \u2014 two MWIS greedy algorithms with arbitrarily close \u03c1-values, say \u2014 can have completely different executions on an instance. This lack of a Lipschitz property explains why we take care in Theorem 3.5 to bound the pseudo-dimension of the full infinite set of greedy heuristics."}, {"heading": "3.3.3 Computational Considerations", "text": "The proof of Theorem 3.5 also demonstrates the presence of an efficient ERM algorithm: the O((sn\u03b2)2) relevant comparisons are easy to identify, the corresponding subintervals induced by each are easy to compute (under mild assumptions on the scoring rule), and brute-force search can be used to pick the best of the resulting O((sn\u03b2)2\u03ba) algorithms (an arbitrary one from each subinterval). This algorithm runs in polynomial time as long as \u03b2 and \u03ba are polynomial in n, and every algorithm of A runs in polynomial time.\nFor example, for the family of Knapsack scoring rules described above, implementing this ERM algorithm reduces to comparing the outputs of O(n2m) different greedy heuristics (on each of the m sampled inputs), with m = O(log n). For the adaptive MWIS heuristics, where \u03b2 = n, it is enough to compare the sample performance of O(n4m) different greedy algorithms, with m = O(log n).\n10The notation O\u0303(\u00b7) suppresses logarithmic factors. 11Alternatively, the dependence of m on H can be removed if learning error H (rather than ) can be tolerated \u2014 for example, if the optimal objective function value is expected to be proportional to H anyways. 12When the performance measure cost is solution quality, as in this section, one cannot identify \u201csimplicity\u201d with \u201clow pseudo-dimension\u201d without caveats: strictly speaking, the set A containing only the optimal algorithm for the problem has pseudo-dimension 1. When the problem \u03a0 is NP -hard and A consists only of polynomial-time algorithms (and assuming P 6= NP ), the pseudo-dimension is a potentially relevant complexity measure for the heuristics in A."}, {"heading": "3.3.4 Extensions: Multiple Algorithms, Multiple Parameters, and Local Search", "text": "Theorem 3.5 is robust and its proof is easily modified to accommodate various extensions. For a first example, consider algorithms than run q different members of a single-parameter greedy heuristic family and return the best of the q feasible solutions obtained.13 Extending the proof of Theorem 3.5 yields a pseudo-dimension bound of O(q log(\u03ba\u03b2n)) for the class of all such algorithms.\nFor a second example, consider families of greedy heuristics parameterized by d real-valued parameters \u03c11, . . . , \u03c1d. Here, an analog of Theorem 3.5 holds with the crossing number \u03ba replaced by a more complicated parameter \u2014 essentially, the number of connected components of the cozero set of the difference of two scoring functions (with \u03be, \u03be\u2032 fixed and variables \u03c11, . . . , \u03c1d). This number can often be bounded (by a function exponential in d) in natural cases, for example using Be\u0301zout\u2019s theorem.\nFor a final extension, we sketch how to adapt the definitions and results of this section from greedy to local search heuristics. The input is again an object assignment problem (see Section 3.3.1), along with an initial feasible solution (i.e., an assignment of objects to R, subject to feasibility constraints). By a k-swap local search heuristic, we mean algorithms of the following form:\n1. Start with arbitrary feasible solution.\n2. While the current solution is not locally optimal:\n(a) Use a scoring rule \u03c3 to compute a score \u03c3({\u03bei : i \u2208 K}) for each set of objects K of size k, where \u03bei is the current attribute of object i. (b) For the set K with the highest score, use an assignment rule to re-assign each i \u2208 K to a value from R. If necessary, update the attributes of the appropriate objects. (Again, assume that ties are resolved lexicographically.)\nWe assume that the assignment rule maintains feasibility, so that we have a feasible assignment at the end of each execution of the loop. We also assume that the scoring and assignment rules ensure that the algorithm terminates, e.g. via the existence of a global objective function that decreases at every iteration (or by incorporating timeouts).\nA canonical example of a k-swap local search heuristic is the k-OPT heuristic for the traveling salesman problem (TSP)14 (see e.g. [JM97]). We can view TSP as an object assignment problem, where the objects are edges and R = {0, 1}; the feasibility constraint is that the edges assigned to 1 should form a tour. Recall that a local move in k-OPT consists of swapping out k edges from the current tour and swapping in k edges to obtain a new tour. (So in our terminology, k-OPT is a 2k-swap local search heuristic.) Another well-known example is the local search algorithms for the p-median problem studied in [AGK+04], which are parameterized by the number of medians that can be removed and added in each local move. Analogous local search algorithms make sense for the MWIS problem as well.\nScoring and assignment rules are now defined on subsets of k objects, rather than individual objects. A single-parameter family of scoring rules is now called \u03ba-crossing if, for every subset K of at most k objects and each distinct pair of attribute sets \u03beK and \u03be \u2032 K , there are at most \u03ba values of \u03c1\n13For example, the classical 1 2 -approximation for Knapsack has this form (with q = 2).\n14Given a complete undirected graph with a cost cuv for each edge (u, v), compute a tour (visiting each vertex exactly once) that minimizes the sum of the edge costs.\nfor which \u03c3(\u03c1, \u03beK) = \u03c3(\u03c1, \u03be \u2032 K). An assignment rule is now \u03b2-bounded if for every subset K of at most k objects, ranging over all possible trajectories of the local search heuristic, the attribute set of K takes on at most \u03b2 distinct values. For example, in MWIS, suppose we allow two vertices u, v to be removed and two vertices y, z to be added in a single local move, and we use the single-parameter scoring rule family\n\u03c3\u03c1(u, v, y, z) = wu\n(1 + deg(u))\u03c1 + wv (1 + deg(v))\u03c1 \u2212 wy (1 + deg(y))\u03c1 \u2212 wz (1 + deg(z))\u03c1 .\nHere deg(v) could refer to the degree of vertex v in original graph, to the number of neighbors of v that do not have any neighbors other than v in the current independent set, etc. In any case, since a generalized Dirichlet polynomial with t terms has at most t\u2212 1 zeroes (see e.g. [Jam06, Corollary 3.2]), this is a 3-crossing family. The natural assignment rule is n4-bounded.15\nBy replacing the number n of objects by the number O(nk) of subsets of at most k objects in the proof of Theorem 3.5, we obtain the following.\nTheorem 3.7 (Pseudo-Dimension of Local Search Algorithms) If A denotes a (\u03ba, \u03b2)-singleparameter family of k-swap local search heuristics for an object assignment problem with n objects, then the pseudo-dimension of A is O(k log(\u03ba\u03b2n))."}, {"heading": "3.4 Application: Self-Improving Algorithms Revisited", "text": "We next give a new interpretation of the self-improving sorting algorithm of [ACCL06]. Namely, we show that the main result in [ACCL06] effectively identifies a set of sorting algorithms that simultaneously has low representation error (for independently distributed array elements) and small pseudo-dimension (and hence low generalization error). Other constructions of self-improving algorithms [ACCL06, CS08, CMS10, CMS12] can be likewise reinterpreted. In contrast to Section 3.3, here our performance measure cost is the running time of an algorithm A on an input x, which we want to minimize.\nConsider the problem of sorting n real numbers in the comparison model. By a bucket-based sorting algorithm, we mean an algorithm A for which there are \u201cbucket boundaries\u201d b1 < b2 < \u00b7 \u00b7 \u00b7 < b` such that A first distributes the n input elements into their rightful buckets, and then sorts each bucket separately, concatenating the results. The degrees of freedom when defining such an algorithm are: (i) the choice of the bucket boundaries; (ii) the method used to distribute input elements to the buckets; and (iii) the method used to sort each bucket.\nThe key steps in the analysis in [ACCL06] can be reinterpreted as proving that this set of bucket-based sorting algorithms has low representation error, in the following sense.\nTheorem 3.8 ([ACCL06, Theorem 2.1]) Suppose that each array element ai is drawn independently from a distribution Di. Then there exists a bucket-based sorting algorithm with expected running time at most a constant factor times that of the optimal sorting algorithm for D1\u00d7\u00b7 \u00b7 \u00b7\u00d7Dn.\nThe proof in [ACCL06] establishes Theorem 3.8 even when the number ` of buckets is only n, each bucket is sorted using InsertionSort, and each element ai is distributed independently to its rightful\n15In general, arbitrary local search algorithms can be made \u03b2-bounded through time-outs: if such an algorithm always halts within T iterations, then the corresponding assignment rule is T -bounded.\nbucket using a search tree stored in O(nc) bits, where c > 0 is an arbitrary constant (and the running time depends on 1c ).\n16 Let Ac denote the set of all such bucket-based sorting algorithms. Theorem 3.8 reduces the task of learning a near-optimal sorting algorithm to the problem of ( , \u03b4)-learning the optimal algorithm from Ac. Since Ac is a finite set, it admits an ERM algorithm, and Corollary 3.4 reduces this learning problem to bounding the pseudo-dimension of Ac. We next prove such a bound, which effectively says that bucket-based sorting algorithms are \u201crelatively simple\u201d algorithms.17\nTheorem 3.9 (Pseudo-Dimension of Bucket-Based Sorting Algorithms) The pseudo-dimension of Ac is O(n1+c).\nProof: Recall from the definitions (Section 3.2) that we need to upper bound the size of every set that is shatterable using the bucket-based sorting algorithms in Ac. For us, a set is a fixed set of s inputs (i.e., arrays of length n), S = x1, . . . , xs. For a potential witness r1, . . . , rs \u2208 R, every algorithm A \u2208 Ac induces a binary labeling of each sample xi, according to whether cost(A, xi) is strictly more than or at most ri. We proceed to bound from above the number of distinct binary labellings of S induced by the algorithms of Ac, for any potential witness.\nBy definition, an algorithm from Ac is fully specified by: (i) a choice of n bucket boundaries b1 < \u00b7 \u00b7 \u00b7 < bn; and (ii) for each i = 1, 2, . . . , n, a choice of a search tree Ti of size at most O(nc) for placing xi in the correct bucket. Call two algorithms A,A\n\u2032 \u2208 Ac equivalent if their sets of bucket boundaries b1, . . . , bn and b \u2032 1, . . . , b \u2032 n induce the same partition of the sn array elements of the inputs in S \u2014 that is, if xij < bk if and only xij < b \u2032 k (for all i, j, k). The number of equivalence classes of\nthis equivalence relation is at most ( sn+n n ) \u2264 (sn+n)n. Within an equivalence class, two algorithms that use structurally identical search trees will have identical performance on all s of the samples. Since the search trees of every algorithm of Ac are described by at most O(n1+c) bits, ranging over the algorithms of a single equivalence class generates at most 2O(n\n1+c) distinct binary labellings of the s sample inputs. Ranging over all algorithms thus generates at most (sn+n)n2O(n\n1+c) labellings. This exceeds 2s, the requisite number of labellings to shatter S, only if s = O(n1+c).\nTheorem 3.9 and Corollary 3.4 imply that m = O\u0303(H 2 2 n1+c) samples are enough to ( , \u03b4)-learn the optimal algorithm in Ac, where H can be taken as the ratio between the maximum and minimum running time of any algorithm in Ac on any instance.18 Since the minimum running time is \u2126(n) and we can assume that the maximum running time is O(n log n) \u2014 if an algorithm exceeds this bound, we can abort it and safely run MergeSort instead \u2014 we obtain a sample complexity bound of O\u0303(n1+c).19\nRemark 3.10 (Comparison to [ACCL06]) The sample complexity bound implicit in [ACCL06] for learning a near-optimal sorting algorithm is O\u0303(nc), a linear factor better than the O\u0303(n1+c) bound\n16For small c, each search tree Ti is so small that some searches will go unresolved; such unsuccessful searches are handled by a standard binary search over the buckets.\n17Not all sorting algorithms are simple in the sense of having polynomial pseudo-dimension. For example, the space lower bound in [ACCL06, Lemma 2.1] can be adapted to show that no class of sorting algorithms with polynomial pseudo-dimension (or fat shattering dimension) has low representation error in the sense of Theorem 3.8 for general distributions over sorting instances, where the array entries need not be independent.\n18We again use O\u0303(\u00b7) to suppress logarithmic factors. 19In the notation of Theorem 3.2, we are taking H = \u0398(n logn), = \u0398(n), and using the fact that all quantities are \u2126(n) to conclude that all running times are correctly estimated up to a constant factor. The results implicit in [ACCL06] are likewise for relative error.\nimplied by Theorem 3.9. There is good reason for this: the pseudo-dimension bound of Theorem 3.9 implies that an even harder problem has sample complexity O\u0303(n1+c), namely that of learning a near-optimal bucket-based sorting algorithm with respect to an arbitrary distribution over inputs, even with correlated array elements.20 The bound of O\u0303(nc) in [ACCL06] applies only to the problem of learning a near-optimal bucket-based sorting algorithm for an unknown input distribution with independent array entries \u2014 the savings comes from the fact that all n near-optimal search trees T1, . . . , Tn can be learned in parallel."}, {"heading": "3.5 Application: Feature-Based Algorithm Selection", "text": "Previous sections studied the problem of selecting a single algorithm for use in an application domain \u2014 of using training data to make an informed commitment to a single algorithm from a class A, which is then used on all future instances. A more refined and ambitious approach is to select an algorithm based both on previous experience and on the current instance to be solved. This approach assumes, as in the scenario in Section 2.4, that it is feasible to quickly compute some features of an instance and then to select an algorithm as a function of these features.\nThroughout this section, we augment the basic model of Section 3.1 with:\n5. A set F of possible instance feature values, and a map f : X \u2192 F that computes the features of a given instance.21\nFor instance, if X is the set of SAT instances, then f(x) might encode the clause/variable ratio of the instance x, Knuth\u2019s estimate of the search tree size [Knu75], and so on.\nSection 3.5.1 describes how our work on the basic model (Sections 3.1\u20133.4) extends to an augmented model with instance features. This extension yields good results if F is finite and small. Section 3.5.2 considers the case of rich feature sets, as in the SAT application described in Section 2.4. We show that the basic model can be easily augmented to capture state-of-the-art empirical approaches to feature-based algorithm selection."}, {"heading": "3.5.1 The Case of Few Features: Estimating Selection Maps", "text": "When the set F of possible instance feature values is finite, the guarantees for the basic model can be extended with at most a linear (in |F|) degradation in the pseudo-dimension.22 To explain, we add an additional ingredient to the model.\n6. A set G of algorithm selection maps, with each g \u2208 G a function from F to A.\nAn algorithm selection map recommends an algorithm as a function of the features of an instance. We can view an algorithm selection map g as a real-valued function defined on the instance space X, with g(x) defined as cost(g(f(x)), x). That is, g(x) is the running time on x of the algorithm g(f(x)) advocated by g, given that x has features f(x). The basic model studied earlier\n20When array elements are not independent, however, Theorem 3.8 fails and the best bucket-based sorting algorithm might be more than a constant-factor worse than the optimal sorting algorithm.\n21Defining a good feature set is a notoriously challenging and important problem, but it is beyond the scope of our model \u2014 we take the set F and map f as given.\n22For example, [XHHL08] first predicts whether or not a given SAT instance is satisfiable or not, and then uses a \u201cconditional\u201d empirical performance model to choose a SAT solver. This can be viewed as an example with |F| = 2, corresponding to the feature values \u201clooks satisfiable\u201d and \u201clooks unsatisfiable.\u201d\nis the special case where G is the set of constant functions, which are in correspondence with the algorithms of A.\nCorollary 3.4 reduces bounding the sample complexity of ( , \u03b4)-learning the best algorithm selection map of G to bounding the pseudo-dimension of the set of real-valued functions induced by G. When G is finite, there is a trivial upper bound of log2 |G|. The pseudo-dimension is also small whenever F is small and the set A of algorithms has small pseudo-dimension.\nTheorem 3.11 (Pseudo-Dimension of Algorithm Selection Maps) If G is a set of algorithm selection maps from a finite set F to a set A of algorithms with pseudo-dimension d, then G has pseudo-dimension O(|F|d).\nProof: Fix a set S \u2286 X of size s and a potential witness r1, . . . , rs. For a value \u03d5 \u2208 F , consider the subset S\u03d5 of instances x \u2208 S with f(x) = \u03d5. Using the bound on the growth function given by Sauer\u2019s Lemma (see e.g. [SB14]), we see that ranging over all A \u2208 A generates at most |S\u03d5|d different binary labellings (w.r.t. the ri\u2019s) of S\u03d5. Thus, ranging over all g \u2208 G generates at most\u220f\n\u03d5\u2208F |S\u03d5|d \u2264 s|F|d\nbinary labellings of S. This is at least 2s, the requisite number of labellings to shatter S, only if s = O(|F|d).\nRemark 3.12 An alternative approach is to just separately learn the best algorithm for each feature value \u03d5 \u2208 F . This straightforward idea yields sample complexity upper bounds similar to those implied by Theorem 3.11, provided two extra assumptions hold. First, that G is the set of all maps from F to A (otherwise, this approach might output a map not in the allowed set G); second, that all feature values of F appear with approximately equal probability in the unknown distribution D (otherwise, there is a corresponding blow-up in sample complexity)."}, {"heading": "3.5.2 Feature-Based Performance Prediction", "text": "The bound in Theorem 3.11 is meaningless when F is very large (or infinite). In this case, there is no hope of observing one or more instances x with f(x) = \u03d5 for every possible \u03d5, and one must learn a model that predicts the performance of an algorithm as a function of the features of an instance.\nWe focus on the case where A is small enough that it is feasible to learn a separate performance prediction model for each algorithm A \u2208 A (though see Remark 3.15). This is exactly the approach taken in the motivating example of empirical performance models (EPMs) for SAT described in Section 2.4. In this case, we augment the basic model to include a family of performance predictors.\n6. A set P of performance predictors, with each p \u2208 P a function from F to R.\nPerformance predictors play the same role as the EPMs used in [XHHL08]. The goal is to learn, for each algorithm A \u2208 A, among all permitted predictors p \u2208 P, the one that minimizes some loss function. Like the performance measure cost, we take this loss function as given. The most commonly used loss function is squared error; in this case, for each A \u2208 A we aim to compute the function that minimizes\nEx\u223cD [ (cost(A, x)\u2212 p(f(x)))2 ]\nover p \u2208 P.23 For a fixed algorithm A, this is a standard regression problem, with domain F , realvalued labels, and a distribution on F \u00d7R induced by D via x 7\u2192 (f(x),cost(A, x)). Bounding the sample complexity of this learning problem reduces to bounding the pseudo-dimension of P (see e.g. [AB99]). We conclude the section by noting two choices of P that are common in empirical work and that have modest pseudo-dimension. For both, suppose the features are real-valued, with F \u2286 Rd.\nFor the first example, suppose the set P is the class of linear predictors, with each p \u2208 P having the form p(f(x)) = aT f(x) for some coefficient vector a \u2208 Rd.24 The following is well known (see e.g. [AB99]).\nProposition 3.13 (Pseudo-Dimension of Linear Predictors) If F contains real-valued d-dimensional features and P is the set of linear predictors, then the pseudo-dimension of P is at most d.\nIf all functions in P map all possible \u03d5 to [0, H], then Proposition 3.13 and Corollary 3.4 imply a sample complexity bound of O\u0303(H 4\n2 d) for ( , \u03b4)-learning the predictor with minimum expected\nsquare error. Similar results hold, with worse dependence on d, if P is a set of low-degree polynomials [AB99].\nFor another example, suppose P` is the set of regression trees with at most ` nodes, where each internal node performs an inequality test on a coordinate of the feature vector \u03d5 (and leaves are labelled with performance estimates).25 This class also has low pseudo-dimension26, and hence the problem of learning a near-optimal predictor has correspondingly small sample complexity.\nTheorem 3.14 (Pseudo-Dimension of Regression Trees) Suppose F contains real-valued ddimensional features and let P` be the set of regression trees with at most ` nodes, where each node performs an inequality test on one of the features. Then, the pseudo-dimension of P` is O(` log(`d)).\nProof: Suppose S = {x1, . . . , xs} \u2282 F is shattered by P`, and let r1, . . . , rs be its real-valued witnesses. We count the number of binary labellings P` can induce on S. For convenience, relabel the samples so that the ri\u2019s are nondecreasing.\nFirst, there are O(4`) binary trees with at most ` internal nodes. Fix such a binary tree. Each node can partition x1, . . . , xs in at most d(s + 1) different ways, since there are d ways to pick a coordinate, and each coordinate induces up to s + 1 different splits of x1, . . . , xs. At each leaf, there are at most s+ 1 equivalence classes of predictions, corresponding to the intervals (\u2212\u221e, r1), [r1, r2), . . . , [rs,\u221e).\nPutting it together, there are O(4` \u00b7(d(s+1))` \u00b7(s+1)`+1) different binary labellings of S, where we use the fact that every tree in P` has at most ` nodes and ` + 1 leaves. This exceeds 2\ns, the requisite number of labellings to shatter S, only if s = O(` log(`d)).\n23Note that the expected loss incurred by the best predictor depends on the choices of the predictor set P, the feature set F , and map f . Again, these choices are outside our model.\n24A linear model might sound unreasonably simple for the task of predicting the running time of an algorithm, but significant complexity can be included in the feature map f(x). For example, each coordinate of f(x) could be a nonlinear combination of several \u201cbasic features\u201d of x. Indeed, linear models often exhibit surprisingly good empirical performance, given a judicious choice of a feature set [LNS09].\n25Regression trees, and random forests thereof, have emerged as a popular class of predictors in empirical work on application-specific algorithm selection [HXHL14].\n26We suspect this fact is known, but have been unable to locate a suitable reference.\nRemark 3.15 (Extension to Large A) We can also extend our approach to scenarios with a large or infinite set A of possible algorithms. This extension is relevant to state-of-the-art empirical approaches to the auto-tuning of algorithms with many parameters, such as mathematical programming solvers [HXHL14]; see also the discussion in Section 2.3. (Instantiating all of the parameters yields a fixed algorithm; ranging over all possible parameter values yields the set A.) In parallel with our formalism for accommodating a large number of possible features, we now assume that there is a set F \u2032 of possible \u201calgorithm feature values\u201d and a mapping f \u2032 that computes the features of a given algorithm. A performance predictor is now a map from F \u00d7 F \u2032 to R, taking as input the features of an algorithm A and of an instance x, and returning as output an estimate of A\u2019s performance on x. If P is the set of linear predictors, for example, then by Proposition 3.13 its pseudo-dimension is d+ d\u2032, where d and d\u2032 denote the dimensions of F and F \u2032, respectively."}, {"heading": "3.6 Application: Choosing the Step Size in Gradient Descent", "text": "For our last PAC example, we give sample complexity results for the problem of choosing the best step size in gradient descent. When gradient descent is used in practice, the step size is generally taken much larger than the upper limits suggested by theoretical guarantees, and often converges in many fewer iterations than with the step size suggested by theory. This motivates the problem of learning the step size from examples. We view this as a baby step towards reasoning more generally about the problem of learning good parameters for machine learning algorithms.\nIn this section, we look at a setting where the approximation quality is fixed for all algorithms, and the performance measure cost is the running time of the algorithm. Unlike the applications we\u2019ve seen so far, the parameter space here will indeed satisfy a Lipschitz-like condition, and we will be able to follow the discretization approach suggested by Remark 3.6."}, {"heading": "3.6.1 Gradient Descent Preliminaries", "text": "Recall the basic gradient descent algorithm for minimizing a function f given an initial point z0 over Rn:\n1. Initialize z := z0.\n2. While \u2016\u2207f(z)\u20162 > \u03bd:\n(a) z := z \u2212 \u03c1 \u00b7 \u2207f(z).\nWe take the error tolerance \u03bd as given and focus on the more interesting parameter, the step size \u03c1. Bigger values of \u03c1 have the potential to make more progress in each step, but run the risk of overshooting a minimum of f .\nWe instantiate the basic model (Section 3.1) to study the problem of learning the best step size. There is an unknown distribution D over instances, where an instance x \u2208 \u03a0 consists of a function f and an initial point z0. Each algorithm A\u03c1 of A is the basic gradient descent algorithm above, with some choice \u03c1 of a step size drawn from some fixed interval [\u03c1`, \u03c1u] \u2282 (0,\u221e). The performance measure cost(A, x) is the number of iterations (i.e., steps) taken by the algorithm for the instance x.\nTo obtain positive results, we need to restrict the allowable functions f (see Appendix A). First, we assume that every function f is convex and L-smooth for a known L. A function f is L-smooth\nif it is everywhere differentiable, and \u2016\u2207f(z1)\u2212\u2207f(z2)\u2016 \u2264 L\u2016z1 \u2212 z2\u2016 for all z1 and z2 (all norms in this section are in `2). Since gradient descent is translation invariant, and f is convex, we can assume for convenience that the (uniquely attained) minimum value of f is 0, with f(0) = 0.\nSecond, we assume that the magnitudes of the initial points are bounded, with \u2016z0\u2016 \u2264 Z for some known constant Z > \u03bd.\nThird, we assume that there is a known constant c \u2208 (0, 1) such that \u2016z\u2212\u03c1\u2207f(z)\u2016 \u2264 (1\u2212c)\u2016z\u2016 for all \u03c1 \u2208 [\u03c1`, \u03c1u]. In other words, the norm of any point z \u2014 equivalently, the distance to the global minimum \u2014 decreases by some minimum factor after each gradient descent step. We refer to this as the guaranteed progress condition. This is satisfied (for instance) by L-smooth, m-strongly convex functions27 which is a well studied regime (see e.g. [BV04]). The standard analysis of gradient descent implies that c \u2265 \u03c1m for \u03c1 \u2264 2/(m+ L) over this class of functions.\nUnder these restrictions, we will be able to compute a nearly optimal \u03c1 given a reasonable number of samples from D.\nOther Notation All norms in this section are `2-norms. Unless otherwise stated, \u03c1 means \u03c1 restricted to [\u03c1`, \u03c1u], and z means z such that \u2016z\u2016 \u2264 Z. We let g(z, \u03c1) := z \u2212 \u03c1\u2207f(z) be the result of taking a single gradient descent step, and gj(z, \u03c1) be the result of taking j gradient descent steps.\nTypical textbook treatments of gradient descent assume \u03c1 < 2/L or \u03c1 \u2264 2/(m+L), which give various convergence and runtime guarantees. The learning results of this section apply for any \u03c1, but this natural threshold will still appear in our analysis and results. Let D(\u03c1) := max{1, L\u03c1\u2212 1} denote how far \u03c1 is from 2/L.\nLastly, let H = log(\u03bd/Z)/ log(1\u2212 c). It is easy to check that cost(A\u03c1, x) \u2264 H for all \u03c1 and x."}, {"heading": "3.6.2 A Lipschitz-like Bound on cost(A\u03c1, x) as a Function of \u03c1.", "text": "This will be the bulk of the argument. Our first lemma shows that for fixed \u03c1, the gradient descent step g is a Lipschitz function of z, even when \u03c1 is larger than 2/L. One might hope that the guaranteed progress condition would be enough to show that (say) g is a contraction, but the Lipschitzness of g actually comes from the L-smoothness. (It is not too hard to come up with non-smooth functions that make guaranteed progress, and where g is arbitrarily non-Lipschitz.)\nLemma 3.16 \u2016g(w, \u03c1)\u2212 g(y, \u03c1)\u2016 \u2264 D(\u03c1)\u2016w \u2212 y\u2016.\nProof: For notational simplicity, let \u03b1 = \u2016w \u2212 y\u2016 and \u03b2 = \u2016\u2207f(w)\u2212\u2207f(y)\u2016. Now,\n\u2016g(w, \u03c1)\u2212 g(y, \u03c1)\u20162 = \u2016(w \u2212 y)\u2212 \u03c1(\u2207f(w)\u2212\u2207f(y))\u20162\n= \u03b12 + \u03c12\u03b22 \u2212 2\u03c1\u3008\u03b1, \u03b2\u3009 \u2264 \u03b12 + \u03c12\u03b22 \u2212 2\u03c1\u03b22/L = \u03b12 + \u03b22\u03c1(\u03c1\u2212 2/L).\nThe only inequality above is a restatement of a property of L-smooth functions called the cocoercivity of the gradient, namely that \u3008\u03b1, \u03b2\u3009 \u2265 \u03b22/L.\n27A (continuously differentiable) function f is m-strongly convex if f(y) \u2265 f(w) +\u2207f(w)T (y\u2212w) + m 2 \u2016y\u2212w\u20162 for all w, y \u2208 Rn. The usual notion of convexity is the same as 0-strong convexity. Note that the definition of L-smooth implies m \u2264 L.\nNow, if \u03c1 \u2264 2/L, then \u03c1(\u03c1 \u2212 2/L) \u2264 0, and we\u2019re done. Otherwise, L-smoothness implies \u03b2 \u2264 L\u03b1, so the above is \u2264 \u03b12(1 + L\u03c1(L\u03c1\u2212 2)), which is the desired result.\nThe next lemma bounds how far two gradient descent paths can drift from each other, if they start at the same point. The main thing to note is that the right hand side goes to 0 as \u03b7 becomes close to \u03c1.\nLemma 3.17 For any z, j, and \u03c1 \u2264 \u03b7,\n\u2016gj(z, \u03c1)\u2212 gj(z, \u03b7)\u2016 \u2264 (\u03b7 \u2212 \u03c1)D(\u03c1) jLZ\nc .\nProof: We first bound \u2016g(w, \u03c1)\u2212 g(y, \u03b7)\u2016, for any w and y. We have\ng(w, \u03c1)\u2212 g(y, \u03b7) = [w \u2212 \u03c1\u2207f(w)]\u2212 [y \u2212 \u03b7\u2207f(y)] = g(w, \u03c1)\u2212 [g(y, \u03c1)\u2212 (\u03b7 \u2212 \u03c1)\u2207f(y)]\nby definition of g. The triangle inequality and Lemma 3.16 then give\n\u2016g(w, \u03c1)\u2212 g(y, \u03b7)\u2016 = \u2016g(w, \u03c1)\u2212 g(y, \u03c1) + (\u03b7 \u2212 \u03c1)\u2207f(y)\u2016 \u2264 D(\u03c1) \u2016w \u2212 y\u2016+ (\u03b7 \u2212 \u03c1)\u2016\u2207f(y)\u2016.\nPlugging in w = gj(z, \u03c1) and y = gj(z, \u03b7), we have\n\u2016gj+1(z, \u03c1)\u2212 gj+1(z, \u03b7)\u2016 \u2264 D(\u03c1) \u2016gj(z, \u03c1)\u2212 gj(z, \u03b7)\u2016+ (\u03b7 \u2212 \u03c1)\u2016\u2207f(gj(z, \u03b7))\u2016\nfor all j. Now, \u2016\u2207f(gj(z, \u03b7))\u2016 \u2264 L\u2016gj(z, \u03b7)\u2016 \u2264 L\u2016z\u2016(1 \u2212 c)j \u2264 LZ(1 \u2212 c)j , where the first inequality is from L-smoothness, and the second is from the guaranteed progress condition. Letting rj = \u2016gj(z, \u03c1)\u2212gj(z, \u03b7)\u2016, we now have the simple recurrence r0 = 0, and rj+1 \u2264 D(\u03c1) rj +(\u03b7\u2212\u03c1)LZ(1\u2212 c)j . One can check via induction that\nrj+1 \u2264 D(\u03c1)j(\u03b7 \u2212 \u03c1)LZ j\u2211 i=0 (1\u2212 c)iD(\u03c1)\u2212i\nfor all j. Rounding D(\u03c1)\u2212i up to 1 and doing the summation gives the desired result.\nFinally, we show that cost(A\u03c1, x) is essentially Lipschitz in \u03c1. The \u201cessentially\u201d is necessary, since cost is integer-valued.\nLemma 3.18 |cost(A\u03c1, x)\u2212 cost(A\u03b7, x)| \u2264 1 for all x, \u03c1, and \u03b7 with 0 \u2264 \u03b7 \u2212 \u03c1 \u2264 \u03bdc 2 LZD(\u03c1) \u2212H .\nProof: WLOG, let cost(A\u03b7, x) \u2264 cost(A\u03c1, x). Let j = cost(A\u03b7, x), and recall that j \u2264 H. By Lemma 3.17, \u2016gj(z, \u03c1)\u2212 gj(z, \u03b7)\u2016 \u2264 \u03bdc. Hence, by the triangle inequality,\n\u2016gj(z, \u03c1)\u2016 \u2264 \u03bdc+ \u2016gj(z, \u03b7)\u2016 \u2264 \u03bdc+ \u03bd.\nNow, by the guaranteed progress condition, \u2016w\u2016 \u2212 \u2016g(w, p)\u2016 \u2265 c\u2016w\u2016 for all w. Since we only run a gradient descent step on w if \u2016w\u2016 > \u03bd, each step of gradient descent run by any algorithm in A drops the magnitude of w by at least \u03bdc.\nSetting w = gj(z, \u03c1), we see that either \u2016gj(z, \u03c1)\u2016 \u2264 \u03bd, and cost(A\u03c1, x) = j, or that \u2016gj+1(z, \u03c1)\u2016 \u2264 (\u03bdc+ \u03bd)\u2212 \u03bdc = \u03bd, and cost(A\u03c1, x) = j + 1, as desired."}, {"heading": "3.6.3 Learning the Best Step Size", "text": "We can now apply the discretization approach suggested by Remark 3.6. Let K = \u03bdc 2 LZD(\u03c1u) \u2212H . Note that since D is an increasing function, K is less than or equal to the \u03bdc 2\nLZD(\u03c1) \u2212H of Lemma 3.18\nfor every \u03c1. Let N be a minimal K-net. Note that |N | \u2264 \u03c1u/K + 1. We tie everything together in the theorem below.\nTheorem 3.19 (Learnability of Step Size in Gradient Descent)28 There is a learning algorithm that (1.1, \u03b4)-learns the optimal algorithm in A using m = O\u0303 ( log(Z/\u03bd)\nc\n)3 samples from D.29\nProof: The pseudo-dimension of AN = {A\u03c1 : \u03c1 \u2208 N} is at most log |N |, since AN is a finite set. Since AN is finite, it also trivially admits an ERM algorithm LN , and Corollary 3.4 implies that LN (0.1, \u03b4)-learns the optimal algorithm in AN using m = O\u0303(H2 log |N |) samples.\nNow, Lemma 3.18 implies that for any \u03c1, there is an \u03b7 \u2208 N such that the expected cost of A\u03b7 minus the expected cost of A\u03c1 is at most 1, over any distribution D. Hence LN also (1.1, \u03b4)-learns the optimal algorithm in A.\nAssuming L\u03c1u is roughly constant, the approximations H = log(\u03bd/Z)/ log(1\u2212 c) \u2264 log(Z/\u03bd)c and log |N | = O\u0303(H) give the desired result."}, {"heading": "4 Online Learning of Application-Specific Algorithms", "text": "This section studies the problem of learning the best application-specific algorithm online, with instances arriving one-by-one.30 The goal is choose an algorithm at each time step, before seeing the next instance, so that the average performance is close to that of the best fixed algorithm in hindsight. This contrasts with the statistical (or \u201cbatch\u201d) learning setup used in Section 3, where the goal was to identify a single algorithm from a batch of training instances that generalizes well to future instances from the same distribution. For many of the motivating examples in Section 2, both the statistical and online learning approaches are relevant. The distribution-free online learning formalism of this section may be particularly appropriate when instances cannot be modeled as i.i.d. draws from an unknown distribution."}, {"heading": "4.1 The Online Learning Model", "text": "Our online learning model shares with the basic model of Section 3.1 a computational or optimization problem \u03a0 (e.g., MWIS), a set A of algorithms for \u03a0 (e.g., a single-parameter family of greedy heuristics), and a performance measure cost : A\u00d7 \u03a0\u2192 [0, 1] (e.g., the total weight of the returned solution).31 Rather than modeling the specifics of an application domain via an unknown distribution D over instances, however, we use an unknown instance sequence x1, . . . , xT .32\n28One can also directly get a pseudo-dimension-like bound as in the previous examples, using a generalization known as the \u03b3-fat shattering dimension. In particular, A has 1.01-fat shattering dimension log |N | = log(Z/\u03bd)\nc .\n29We again use O\u0303(\u00b7) to suppress logarithmic factors. 30The online model is obviously relevant when training data arrives over time. Also, even with offline data sets that are very large, it can be computationally necessary to process training data in a one-pass, online fashion. 31One could also have cost take values in [0, H] rather than [0, 1], to parallel the PAC setting; we set H = 1 here since the dependence on H will not be interesting. 32For simplicity, we assume that the time horizon T is known. This assumption can be removed by standard doubling techniques (e.g. [CL06]).\nA learning algorithm now outputs a sequence A1, . . . , AT of algorithms, rather than a single algorithm. Each algorithm Ai is chosen (perhaps probabilistically) with knowledge only of the previous instances x1, . . . , xi\u22121. The standard goal in online learning is to choose A1, . . . , AT to minimize the worst-case (over x1, . . . , xT ) regret, defined as the average performance loss relative to the best algorithm A \u2208 A in hindsight:33\n1\nT ( sup A\u2208A T\u2211 t=1 cost(A, xi)\u2212 T\u2211 t=1 cost(Ai, xi) ) . (2)\nA no-regret learning algorithm has expected (over its coin tosses) regret o(1), as T \u2192\u221e, for every instance sequence. The design and analysis of no-regret online learning algorithms is a mature field (see e.g. [CL06]). For example, many no-regret online learning algorithms are known for the case of a finite set |A| (e.g., the \u201cmultiplicative weights\u201d algorithms)."}, {"heading": "4.2 An Impossibility Result for Worst-Case Instances", "text": "This section proves an impossibility result for no-regret online learning algorithms for the problem of application-specific algorithm selection. We show this for the running example in Section 3.3: maximum-weight independent set (MWIS) heuristics34 that, for some parameter \u03c1 \u2208 [0, 1], process the vertices in order of nonincreasing value of wv/(1 + deg(v))\n\u03c1. Let A denote the set of all such MWIS algorithms. Note that A is an infinite set and hence the standard no-regret results (for a finite number of actions) do not immediately apply. In online learning, infinite sets of options are normally controlled through a Lipschitz condition, stating that \u201cnearby\u201d actions always yield approximately the same performance; our set A does not possess such a Lipschitz property (recall Remark 3.6). The next section shows that these issues are not mere technicalities \u2014 there is enough complexity in the set A of MWIS heuristics to preclude a no-regret learning algorithm."}, {"heading": "4.2.1 A Hard Example for MWIS", "text": "We show a distribution over sequences of MWIS instances for which every algorithm (randomized or not) has expected regret 1 \u2212 o(1). By o(1) here (and in the rest of this section) we mean a function that is constant in T , but which tends to 0 as the number of vertices n tends to infinity. Recall that cost(A\u03c1, x) is the the total weight of the returned independent set, and we are trying to maximize this quantity. The key construction is the following:\nLemma 4.1 For any constants 0 < r < s < 1, there exists a MWIS instance x on at most n vertices such that cost(A\u03c1, x) = 1 when \u03c1 \u2208 (r, s), and cost(A\u03c1, x) = o(1) when \u03c1 < r or \u03c1 > s.\nProof: Let A,B, and C be 3 sets of vertices of sizes m2 \u2212 2,m3 \u2212 1, and m2 +m+ 1 respectively, such that their sum m3 +2m2 +m is between n/2 and n. Let (A,B) be a complete bipartite graph. Let (B,C) also be a bipartite graph, with each vertex of B connected to exactly one vertex of C, and each vertex of C connected to exactly m\u2212 1 vertices of B. See Figure 1.\n33Without loss of generality, we assume cost corresponds to a maximization objective. 34Recall that we defined two different versions of the MWIS heuristic; an adaptive version and a non-adaptive version. All the results of Section 4 apply to both, and so unless otherwise stated assume MWIS can refer to either version.\nNow, set the weight of every vertex in (A,B,C) to (tmr, t, tm\u2212s) respectively, for t = (m3\u22121)\u22121. We summarize the situation below, and make some straightforward calculations:\nsize weight deg weight/(deg+1)\u03c1 size \u00d7 weight A m2 \u2212 2 tmr m3 \u2212 1 tmr\u22123\u03c1 o(1) B m3 \u2212 1 t m2 \u2212 1 tm\u22122\u03c1 1 C m2 +m+ 1 tm\u2212s m\u2212 1 tm\u2212s\u2212\u03c1 o(1)\nWe now calculate the cost of A\u03c1 on this instance. If \u03c1 < r, the algorithm A\u03c1 first chooses a vertex in A, which immediately removes all of B, leaving at most A and C in the independent set. The total weight of A and C is o(1), so cost(A\u03c1) is o(1).\nIf \u03c1 > s, the algorithm first chooses a vertex in C, which removes a small chunk of B. In the non-adaptive setting, A\u03c1 simply continues choosing vertices of C until B is gone. In the adaptive setting, the degrees of the remaining elements of B never change, but the degrees of A decrease as we pick more and more elements of C. We eventually pick a vertex of A, which immediately removes the rest of B. In either case, the returned independent set has no elements from B, and hence has cost o(1).\nIf \u03c1 \u2208 (r, s), the algorithm first picks a vertex of B, immediately removing all of A, and one element of C. The remaining graph comprises m\u2212 2 isolated vertices of B (which get added to the independent set), and m2 +m stars with centers in C and leaves in B. It is easy to see that both the adaptive and the non-adaptive versions of the heuristic return exactly B.\nWe are now ready to state the result of this section.\nTheorem 4.2 (Impossibility of Worst-Case Online Learning) There is a distribution on MWIS input sequences over which every algorithm has expected regret 1\u2212 o(1).\nProof: Let tj = (rj , sj) be a distribution over sequences of intervals with sj \u2212 rj = n\u2212j , t0 = (0, 1), and with tj chosen uniformly at random from within tj\u22121. Let xj be an MWIS instance on up to n vertices such that cost(A\u03c1, x) = 1 for \u03c1 \u2208 (rj , sj), and cost(A\u03c1, x) = o(1) for \u03c1 < rj and \u03c1 > sj (Lemma 4.1).\nThe adversary presents the instances x1, x2, . . . , xT , in that order. For any \u03c1 \u2208 tT , cost(A\u03c1, xj) = 1 for all j. However, at every t, no algorithm can have a better than 1/n chance of picking a \u03c1t for which cost(A\u03c1t , xt) = \u0398(1), even given x1, x2, . . . , xt\u22121 and full knowledge of how the sequence is generated."}, {"heading": "4.3 A Smoothed Analysis", "text": "Despite the negative result above, we can show a \u201clow-regret\u201d learning algorithm for MWIS under a slight restriction on how the instances xt are chosen. By low-regret we mean that the regret can be made polynomially small as a function of the number of vertices n. Though not a true no-regret algorithm, which would require regret tending to 0 as a function of T , note the vast improvement between this and the nearly full regret of the worst case lower bound. As in the previous section, we use o(1) to denote the class of functions that is constant in T but that tends to 0 as n tends to infinity.\nWe take the approach suggested by smoothed analysis [ST09], described below. Fix a parameter 0 < \u03c3 < 1. We allow each xt to have an arbitrary graph on n vertices, but we replace each weight wv with a probability distribution \u2206t,v, which has maximum density \u03c3 \u22121, and support in [0, 1]. A simple example of such a distribution for \u03c3 = 0.1 would be the uniform distribution over [0.6, 0.65] \u222a [0.82, 0.87]. To get the instance xt, we draw each weight from its distribution \u2206t,v. We call such an instance a \u03c3-smooth MWIS instance.\nFor small \u03c3, this is quite a weak restriction. Note that as \u03c3 \u2192 0, we recover the worst case setting of the previous section. It is not hard to imagine extending Theorem 4.2 to a setting with exponentially small \u03c3 as well, so we imagine \u03c3 to be polynomial in n. An example of such a smoothing with polynomial \u03c3 is if we start with an arbitrary MWIS instance, keep the first O(log n) bits of every weight, and set the remaining lower order bits at random.\nThe result of this section is a polynomial time low-regret learning algorithm for sequences of \u03c3-smooth MWIS instances. Our strategy will be to take a finite net N \u2282 [0, 1] such that every algorithm A\u03c1 has identical performance to some {A\u03b7 : \u03b7 \u2208 N}, for any instance xt, with probability 1\u2212 o(1) over the randomness in the weights of the xt. We can then use any off the shelf no-regret algorithm to output a sequence of algorithms from the finite set {A\u03b7 : \u03b7 \u2208 N}, and show the desired regret bound."}, {"heading": "4.3.1 A Low-Regret Algorithm for \u03c3-Smooth MWIS", "text": "We start with some definitions. For a fixed x, let \u03c4 \u2032(x) be the set of transition points, namely,35\n\u03c4 \u2032(x) := {\u03c1 : A\u03c1\u2212\u03c9(x) 6= A\u03c1+\u03c9(x) for arbitrarily small \u03c9}.\nIt is easy to see \u03c4 \u2032(x) \u2282 \u03c4(x), where\n\u03c4(x) := {\u03c1 : wv1/k \u03c1 1 = wv2/k \u03c1 2 for some v1, v2, k1, k2 \u2208 [n]; k1, k2 \u2265 2}.\nWith probability 1, the wv are all distinct and non-zero, so we can rewrite \u03c4 as\n\u03c4(x) := { \u03c1(v1, v2, k1, k2) =\nln(wv1)\u2212 ln(wv2) ln(k1)\u2212 ln(k2)\n: v1, v2, k1, k2 \u2208 [n]; k1, k2 \u2265 2; k1 6= k2 } ,\nwhere ln is the natural logarithm function. The main technical task is showing that no two elements of \u03c4(x1) \u222a \u00b7 \u00b7 \u00b7 \u222a \u03c4(xm) are within q of each other, for a sufficiently large q and sufficiently large m, and with high enough probability over the randomness in the weights of the xt.\nWe first make a few straightforward computations. The following brings the noise into log space.\n35The corner cases \u03c1 = 0 and \u03c1 = 1 require straightforward but wordy special handling in this statement and in several others in this section. We omit these details to keep the argument free of clutter.\nLemma 4.3 If X is a random variable over (0, 1] with density at most \u03b4, then ln(X) also has density at most \u03b4.\nProof: Let Y = ln(X), let f(x) be the density of X at x, and let g(y) be the density of Y at y. Note that X = eY , and let v(y) = ey. Then g(y) = f(v(y)) \u00b7 v\u2032(y) \u2264 f(v(y)) \u2264 \u03b4 for all y.\nSince | ln(k1) \u2212 ln(k2)| \u2264 lnn, Lemma 4.3 and our definition of \u03c3-smoothness implies the following.\nCorollary 4.4 For any \u03c3-smooth MWIS instance x, and any v1, v2, k1, k2 \u2208 [n], k1, k2 \u2265 2, k1 6= k2, the density of \u03c1(v1, v2, k1, k2) is bounded by \u03c3 \u22121 lnn.\nThe next observation is a straightforward application of the union bound, but will be useful later.\nObservation 4.5 Given R variables, each with density bounded by \u03b4, the probability that no two of them are within q of each other is at least 1\u2212 qR2\u03b4 for any q > 0.\nLastly, we formally state the existence of no-regret algorithms (e.g., \u201cmultiplicative weights\u201d) for the case of finite |A|.\nFact 4.6 (E.g. [LW94]) For a finite set of algorithms A, there exists an online learning algorithm Lmw that, for every > 0, has expected regret after seeing O((ln |A|)/ 2) instances. If the time cost of evaluating cost(A, x) is bounded by B, then this algorithm runs in O(B|A|) time per instance.\nWe can now state our main theorem.\nTheorem 4.7 (Online Learning of Smooth MWIS) There is a poly(n, \u03c3) time algorithm with expected regret o(1) for \u03c3-smooth MWIS.\nProof: Consider the first m instances of our sequence, x1, . . . , xm, for some m = \u03c9(ln(n/\u03c3)) and m = poly(n, \u03c3). Each \u03c4(xt) has at most n\n4 elements, so the union \u03c4 = \u03c4(x1) \u222a \u00b7 \u00b7 \u00b7 \u222a \u03c4(xm) has at most mn4 elements. Let Eq be the event that no two elements of \u03c4 are within q of each other. By Corollary 4.4 and Observation 4.5, Eq holds with probability 1\u2212 q \u00b7 (mn4)2 \u00b7 \u03c3\u22121 lnn.\nLet q = 1/(n \u00b7 (mn4)2 \u00b7 \u03c3\u22121 lnn). Then Eq holds with probability at least 1 \u2212 1/n. Let AN = {Ai : i \u2208 {0, q, 2q, . . . , b1/qcq, 1}} be a \u201cq-net\u201d. Our algorithm L is simply Lmw of Fact 4.6 on AN . We now analyze its expected regret.\nIf Eq does hold, then for every A \u2208 A, there is an A\u2032 \u2208 AN such that cost(A, xt) = cost(A\u2032, xt) for x1, . . . , xm. In other words, the best A \u2208 AN does no worse than the best A \u2208 A, and the\nregret of L is simply the regret due to Lmw. By Fact 4.6, the expected regret = O (\u221a ln |AN | m ) =\nO\n(\u221a ln(mn/\u03c3)\nm\n) (over the randomness of Lmw), and so by our assumptions on m, = o(1).\nIf Eq does not hold, our regret is at most 1, since cost is between 0 and 1. Hence L has expected regret at most O( \u00b7 (1 \u2212 1/n) + 1 \u00b7 (1/n)) = o(1) over the m instances. For arbitrarily large sequences T , we simply run L on x1, . . . , xm, then run L on xm+1, . . . , x2m, and so on, giving the desired result."}, {"heading": "5 Conclusions and Future Directions", "text": "Empirical work on application-dependent algorithm selection has far outpaced theoretical analysis of the problem, and this paper has taken an initial step toward redressing this imbalance. We formulated the problem as one of learning the best algorithm or algorithm sequence from a class with respect to an unknown input distribution or input sequence. Many state-of-the-art empirical approaches to algorithm selection map naturally to instances of our learning frameworks. We demonstrated that many well-studied classes of algorithms have small pseudo-dimension, and thus it is possible to learn a near-optimal algorithm from a relatively modest amount of data. We proved that worst-case guarantees for no-regret online learning algorithms are impossible, but that good online learning algorithms exist in a natural smoothed model.\nOur work suggests numerous wide-open research directions worthy of further study. For example:\n1. Which computational problems admit a class of algorithms that simultaneously has low representation error and small pseudo-dimension (like in Section 3.4)?\n2. When is it possible to learn a near-optimal algorithm using only a polynomial amount of computation?\n3. For what settings is there is a learning algorithm better than brute-force search? Alternatively, are there (conditional) lower bounds stating that brute-force search is necessary for learning?36\n4. Are there any non-trivial relationships between statistical learning measures of the complexity of an algorithm class and more traditional complexity measures?\n5. How should instance features be chosen to minimize the representation error of the induced family of algorithm selection maps (cf., Section 3.5.1)?"}, {"heading": "Acknowledgements", "text": "We are grateful for the comments provided by the anonymous ITCS reviewers."}, {"heading": "A A Bad Example for Gradient Descent", "text": "We depict a family F of functions for which the class A of gradient descent algorithms from Section 3.6 has arbitrarily high pseudo-dimension. For each member f \u2208 F , f : R2 \u2192 R, and we parameterize fI \u2208 F by finite subsets I \u2282 [0, 1]. We now draw an aerial view of fI .\nThe \u201csquiggle\u201d s(I) intersects the relevant axis at exactly I (to be concrete, let s(I) be the monic polynomial with roots at I). We fix the initial point z0 to be at the tail of the arrow for all instances, and fix \u03c1` and \u03c1u so that the first step of gradient descent takes z0 from the red incline into the middle of the black and blue area. Let xI be the instance corresponding to fI with starting point z0. If for a certain \u03c1 and I, g(z0, \u03c1) lands in the flat, black area, gradient descent stops immediately and cost(A\u03c1, xI) = 1. If g(z0, \u03c1) instead lands in the sloped, blue area, cost(A\u03c1, xI) 1.\nIt should be clear that F can shatter any finite subset of (\u03c1`, \u03c1u), and hence has arbitrarily large pseudo-dimension. One can also make slight modifications to ensure that all the functions in F are continuously differentiable and L-smooth."}], "references": [{"title": "Neural Network Learning: Theoretical Foundations", "author": ["M. Anthony", "P.L. Bartlett"], "venue": null, "citeRegEx": "Anthony and Bartlett.,? \\Q1999\\E", "shortCiteRegEx": "Anthony and Bartlett.", "year": 1999}, {"title": "Self-improving algorithms", "author": ["N. Ailon", "B. Chazelle", "S. Comandur", "D. Liu"], "venue": "In Symposium on Discrete Algorithms (SODA), pages 261\u2013270,", "citeRegEx": "Ailon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2006}, {"title": "Local search heuristics for k-median and facility location problems", "author": ["Vijay Arya", "Naveen Garg", "Rohit Khandekar", "Adam Meyerson", "Kamesh Munagala", "Vinayaka Pandit"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Arya et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Arya et al\\.", "year": 2004}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bergstra and Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Convex optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Congressional Budget Office: The budget and economic outlook: 2015 to 2025", "author": ["S. U"], "venue": null, "citeRegEx": "U.,? \\Q2014\\E", "shortCiteRegEx": "U.", "year": 2014}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Self-improving algorithms for convex hulls", "author": ["Kenneth L. Clarkson", "Wolfgang Mulzer", "C. Seshadhri"], "venue": "In Symposium on Discrete Algorithms (SODA),", "citeRegEx": "Clarkson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2010}, {"title": "Self-improving algorithms for coordinate-wise maxima", "author": ["Kenneth L. Clarkson", "Wolfgang Mulzer", "C. Seshadhri"], "venue": "In Symposium on Computational Geometry (SoCG),", "citeRegEx": "Clarkson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Clarkson et al\\.", "year": 2012}, {"title": "Self-improving algorithms for Delaunay triangulations", "author": ["K.L. Clarkson", "C. Seshadhri"], "venue": "In Symposium on Computational Geometry (SoCG),", "citeRegEx": "Clarkson and Seshadhri.,? \\Q2008\\E", "shortCiteRegEx": "Clarkson and Seshadhri.", "year": 2008}, {"title": "How to solve it automatically: Selection among problem solving methods", "author": ["Eugene Fink"], "venue": "In International Conference on Artificial Intelligence Planning Systems,", "citeRegEx": "Fink.,? \\Q1998\\E", "shortCiteRegEx": "Fink.", "year": 1998}, {"title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "author": ["D. Haussler"], "venue": "Information and Computation,", "citeRegEx": "Haussler.,? \\Q1992\\E", "shortCiteRegEx": "Haussler.", "year": 1992}, {"title": "Predicting execution time of computer programs using sparse polynomial regression", "author": ["Ling Huang", "Jinzhu Jia", "Bin Yu", "Byung-Gon Chun", "Petros Maniatis", "Mayur Naik"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "A Bayesian approach to tackling hard computational problems", "author": ["Eric Horvitz", "Yongshao Ruan", "Carla P. Gomes", "Henry A. Kautz", "Bart Selman", "David Maxwell Chickering"], "venue": "In Conference in Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Horvitz et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Horvitz et al\\.", "year": 2001}, {"title": "Algorithm runtime prediction: Methods & evaluation", "author": ["Frank Hutter", "Lin Xu", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "Artificial Intelligence,", "citeRegEx": "Hutter et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2014}, {"title": "Counting zeros of generalized polynomials: Descartes rule of signs and Laguerres extensions", "author": ["G.J.O. Jameson"], "venue": "Mathematical Gazette,", "citeRegEx": "Jameson.,? \\Q2006\\E", "shortCiteRegEx": "Jameson.", "year": 2006}, {"title": "The traveling salesman problem: A case study in local optimization", "author": ["David S Johnson", "Lyle A McGeoch"], "venue": "Local search in combinatorial optimization,", "citeRegEx": "Johnson and McGeoch.,? \\Q1997\\E", "shortCiteRegEx": "Johnson and McGeoch.", "year": 1997}, {"title": "An evaluation of machine learning in algorithm selection for search problems", "author": ["Lars Kotthoff", "Ian P. Gent", "Ian Miguel"], "venue": "AI Communications,", "citeRegEx": "Kotthoff et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kotthoff et al\\.", "year": 2012}, {"title": "Estimating the efficiency of backtrack programs", "author": ["D.E. Knuth"], "venue": "Mathematics of Computation,", "citeRegEx": "Knuth.,? \\Q1975\\E", "shortCiteRegEx": "Knuth.", "year": 1975}, {"title": "Empirical hardness models: Methodology and a case study on combinatorial auctions", "author": ["Kevin Leyton-Brown", "Eugene Nudelman", "Yoav Shoham"], "venue": "Journal of the ACM,", "citeRegEx": "Leyton.Brown et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Leyton.Brown et al\\.", "year": 2009}, {"title": "Using the pseudo-dimension to analyze approximation algorithms for integer programming", "author": ["Philip M. Long"], "venue": "In International Workshop on Algorithms and Data Structures (WADS),", "citeRegEx": "Long.,? \\Q2001\\E", "shortCiteRegEx": "Long.", "year": 2001}, {"title": "Truth revelation in approximately efficient combinatorial auctions", "author": ["Daniel Lehmann", "Liadan Ita O\u0107allaghan", "Yoav Shoham"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Lehmann et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2002}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K Warmuth"], "venue": "Information and computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Learning theory and algorithms for revenue optimization in second price auctions with reserve", "author": ["Mehryar Mohri", "Andres Munoz Medina"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Mohri and Medina.,? \\Q2014\\E", "shortCiteRegEx": "Mohri and Medina.", "year": 2014}, {"title": "The pseudo-dimension of near-optimal auctions", "author": ["Jamie Morgenstern", "Tim Roughgarden"], "venue": "CoRR, abs/1506.03684,", "citeRegEx": "Morgenstern and Roughgarden.,? \\Q2015\\E", "shortCiteRegEx": "Morgenstern and Roughgarden.", "year": 2015}, {"title": "Deferred-acceptance auctions and radio spectrum reallocation", "author": ["P. Milgrom", "I. Segal"], "venue": null, "citeRegEx": "Milgrom and Segal.,? \\Q2014\\E", "shortCiteRegEx": "Milgrom and Segal.", "year": 2014}, {"title": "Understanding Machine Learning: From Theory to Algorithms", "author": ["S. Shalev-Shwartz", "S. Ben-David"], "venue": null, "citeRegEx": "Shalev.Shwartz and Ben.David.,? \\Q2014\\E", "shortCiteRegEx": "Shalev.Shwartz and Ben.David.", "year": 2014}, {"title": "Smoothed analysis: an attempt to explain the behavior of algorithms in practice", "author": ["Daniel A Spielman", "Shang-Hua Teng"], "venue": "Communications of the ACM,", "citeRegEx": "Spielman and Teng.,? \\Q2009\\E", "shortCiteRegEx": "Spielman and Teng.", "year": 2009}, {"title": "A note on greedy algorithms for the maximum weighted independent set problem", "author": ["Shuichi Sakai", "Mitsunori Togasaki", "Koichi Yamazaki"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Sakai et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sakai et al\\.", "year": 2003}, {"title": "SATzilla: Portfoliobased algorithm selection for SAT", "author": ["Lin Xu", "Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "J. Artificial Intelligence Research (JAIR),", "citeRegEx": "Xu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2008}, {"title": "Hydra-mip: Automated algorithm configuration and selection for mixed integer programming. In RCRA workshop on experimental evaluation of algorithms for solving problems with combinatorial explosion at the international joint conference on artificial intelligence", "author": ["Lin Xu", "Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "SATzilla2012: Improved algorithm selection based on cost-sensitive classification models", "author": ["Lin Xu", "Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "In International Conference on Theory and Applications of Satisfiability Testing (SAT),", "citeRegEx": "Xu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2012}], "referenceMentions": [], "year": 2017, "abstractText": "The best algorithm for a computational problem generally depends on the \u201crelevant inputs,\u201d a concept that depends on the application domain and often defies formal articulation. While there is a large literature on empirical approaches to selecting the best algorithm for a given application domain, there has been surprisingly little theoretical analysis of the problem. This paper adapts concepts from statistical and online learning theory to reason about application-specific algorithm selection. Our models capture several state-of-the-art empirical and theoretical approaches to the problem, ranging from self-improving algorithms to empirical performance models, and our results identify conditions under which these approaches are guaranteed to perform well. We present one framework that models algorithm selection as a statistical learning problem, and our work here shows that dimension notions from statistical learning theory, historically used to measure the complexity of classes of binaryand real-valued functions, are relevant in a much broader algorithmic context. We also study the online version of the algorithm selection problem, and give possibility and impossibility results for the existence of no-regret learning algorithms.", "creator": "LaTeX with hyperref package"}}}