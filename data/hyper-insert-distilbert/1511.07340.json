{"id": "1511.07340", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2015", "title": "Modular Autoencoders for Ensemble Feature Extraction", "abstract": "these we suddenly introduce the concept of having a modular intermediate autoencoder ( mathematical mae ), capable of now learning a systematic set of typically diverse but universally complementary variable representations from unlabelled data, elements that can but later be actually used for supervised digital tasks. the learning of the representations is then controlled by achieving a special trade off parameter, and sometimes we should show variants on six benchmark sized datasets the optimum sensitivity lies between two extremes : producing a set consist of economically smaller, fully independent autoencoders each with quite low capacity, versus forming a separate single upstream monolithic encoding, outperforming an appropriate encoding baseline. in the present recent paper approaches we explore the special case modelling of linear mae, and derive assume an svd - based algorithm which periodically converges several degree orders is of magnitude faster often than the gradient descent.", "histories": [["v1", "Mon, 23 Nov 2015 17:51:18 GMT  (253kb,D)", "http://arxiv.org/abs/1511.07340v1", "18 pages, 8 figures, to appear in a special issue of The Journal Of Machine Learning Research (vol.44, Dec 2015)"]], "COMMENTS": "18 pages, 8 figures, to appear in a special issue of The Journal Of Machine Learning Research (vol.44, Dec 2015)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["henry w j reeve", "gavin brown"], "accepted": false, "id": "1511.07340"}, "pdf": {"name": "1511.07340.pdf", "metadata": {"source": "CRF", "title": "Modular Autoencoders for Ensemble Feature Extraction", "authors": ["Henry W J Reeve", "Gavin Brown", "Afshin Rostamizadeh"], "emails": ["henrywjreeve@gmail.com", "gavin.brown@manchester.ac.uk"], "sections": [{"heading": null, "text": "Keywords: Modularity, Autoencoders, Diversity, Unsupervised, Ensembles"}, {"heading": "1. Introduction", "text": "In a wide variety of Machine Learning problems we wish to extract information from high dimensional data sets such as images or documents. Dealing with high dimensional data creates both computational and statistical challenges. One approach to overcoming these challenges is to extract a small set of highly informative features. These features may then be fed into a task dependent learning algorithm. In representation learning these features are learnt directly from the data (Bengio et al., 2013).\nWe consider a modular approach to representation learning. Rather than extracting a single set of features, we extract multiple sets of features. Each of these sets of features is then fed into a separate learning module. These modules may then be trained independently, which addresses both computational challenges, by being easily distributable, and statistical challenges, since each module is tuned to just a small set of features. The outputs of the different classifiers are then combined, giving rise to a classifier ensemble.\nEnsemble methods combine the outputs of a multiplicity of models in order to obtain an enriched hypothesis space whilst controlling variance (Friedman et al., 2001). In this work we shall apply ensemble methods to representation learning in order to extract several subsets of features for an effective classifier ensemble. Successful ensemble learning results from a fruitful trade-off between accuracy and diversity within the ensemble. Diversity\nc\u00a92015 Henry Reeve and Gavin Brown.\nar X\niv :1\n51 1.\n07 34\n0v 1\n[ cs\n.L G\n] 2\nis typically encouraged, either through some form of randomisation, or by encouraging diversity through supervised training (Brown et al., 2005).\nWe investigate an unsupervised approach to learning a set of diverse but complementary representations from unlabelled data. As such, we move away from the recent trend towards coupled dimensionality reduction in which the tasks of feature extraction and supervised learning are performed in unison Go\u0308nen (2014); Storcheus et al. (2015). Whilst coupled dimensionality reduction has been shown to improve accuracy for certain classification tasks Go\u0308nen (2014), the unsupervised approach allows us to use unlabelled data to learn a transferable representation which may be used on multiple tasks without the need for retraining Bengio et al. (2013).\nWe show that one can improve the performance of a classifier ensemble by first learning a diverse collection of modular feature extractors in a purely unsupervised way (see Section 4) and then training a set of classifiers independently. Features are extracted using a Modular Autoencoder trained to simultaneously minimise reconstruction error and maximise diversity amongst reconstructions (see Section 2). Though the MAE framework is entirely general to any activation function, in the present paper we focus on the linear case and provide an efficient learning algorithm that converges several orders of magnitude faster than gradient descent (see Section 3). The training scheme involves a hyper-parameter \u03bb. We provide an upper bound on \u03bb, enabling a meaningful trade off between reconstruction error and diversity (see Section 2.2)."}, {"heading": "2. Modular Autoencoders", "text": "A Modular Autoencoder consists of an ensemble W = {(Ai,Bi)}Mi=1 consisting of M autoencoder modules (Ai,Bi), where each module consists of an encoder map Bi : RD \u2192 RH from a D-dimensional feature space RD to an H-dimensional representation space RH , and a decoder map Ai : RD \u2192 RH . For reasons of brevity we focus on the linear case, where Ai \u2208MD\u00d7H (R) and Bi \u2208MH\u00d7D (R) are matrices. See Figure 1.\nIn order to train our Modular Autoencoders W we introduce the following loss function\nL\u03bb (W,x) := 1\nM M\u2211 i=1 reconstruction error\ufe37 \ufe38\ufe38 \ufe37 ||AiBix\u2212 x||2 \u2212\u03bb \u00b7 1 M M\u2211 i=1 diversity\ufe37 \ufe38\ufe38 \ufe37\u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223AiBix\u2212 1M M\u2211 j=1 AjBjx \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2 , (1)\nfor feature vectors x \u2208 RD. The loss function L\u03bb (W,x) is inspired by (but not identical to) the Negative Correlation Learning approach of by Liu and Yao for training supervised ensembles of neural networks (Liu and Yao, 1999)1. The first term corresponds to the squared reconstruction error typically minimised by Autoencoders (Bengio et al., 2013). The second term encourages the reconstructions to be diverse, with a view to capturing different factors of variation within the training data. The hyper-parameter \u03bb, known as the diversity parameter, controls the degree of emphasis placed upon these two terms. We discuss its properties in Sections 2.1 and 2.2. Given a data set D \u2282 RD we train a Modular Autoencoder to minimise the error E\u03bb (W,D), the loss function L\u03bb (W,x) averaged across the data x \u2208 D."}, {"heading": "2.1 Between two extremes", "text": "To understand the role of the diversity parameter \u03bb we first look at the two extremes of \u03bb = 0 and \u03bb = 1. If \u03bb = 0 then no emphasis is placed upon diversity. Consequently L0 (W,x) is precisely the average squared error of the individual modules (Ai,Bi). Since there is no interaction term, minimising L0 (W,x) over the training data is equivalent to training each of the auto-encoder modules independently, to minimise squared error. Hence, in the linear case E0 (W,D) is minimised by taking each Bi to be the projection onto the first H principal components of the data covariance (Baldi and Hornik, 1989).\nIf \u03bb = 1 then, by the Ambiguity Decomposition (Krogh et al., 1995),\nL1 (W,x) = \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 1M M\u2211 i=1 AiBix\u2212 x \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2 .\nHence, minimising L1 (W) is equivalent to minimising squared error for a single large Autoencoder (A,B) with an M \u00b7H-dimensional hidden layer, where B = [BT1 , \u00b7 \u00b7 \u00b7 ,BTM ]T and A = M\u22121[A1, \u00b7 \u00b7 \u00b7 ,AM ].\nConsequently, moving \u03bb between 0 and 1 corresponds to moving from training each of our autoencoder modules independently through to training the entire network as a single monolithic autoencoder."}, {"heading": "2.2 Bounds on the diversity parameter", "text": "The diversity parameter \u03bb may be set by optimising the performance of a task-specific system using the extracted sets of features on a validation set. Theorem 1 shows that the search region may be restricted to the closed unit interval [0, 1].\nTheorem 1 Suppose we have a data set D. The following dichotomy holds:\n\u2022 If \u03bb \u2264 1 then inf E\u03bb (W,D) \u2265 0.\n\u2022 If \u03bb > 1 then inf E\u03bb (W,D) = \u2212\u221e."}, {"heading": "In both cases the infimums range over possible parametrisations for the ensemble W.", "text": "Moreover, if the diversity parameter \u03bb > 1 there exist ensembles W with arbitrarily low error E\u03bb (W,D) and arbitrarily high average reconstruction error.\n1. See the Appendix for details.\nTheorem 1 is a special case of Theorem 3, which is proved the appendix."}, {"heading": "3. An Efficient Algorithm for Training Linear Modular Autoencoders", "text": "One method to minimise the error E\u03bb (W,D) would be to apply some form of gradient descent. However, Linear Modular Autoencoders we can make use of the Singular Value Decomposition to obtain a fast iterative algorithm for minimising the error E\u03bb (W,D) (see Algorithm 1).\nAlgorithm 1 Backfitting for Linear Modular Autoencoders\nInputs: D \u00d7 N data matrix X, diversity parameter \u03bb, number of hidden nodes per module H, number of modules M , maximal number of epochs T, Randomly generate {(Ai,Bi)}Mi=1 and set \u03a3\u2190XXT , for t = 1 to T do\nfor i = 1 to M do Zi \u2190M\u22121 \u2211 j 6=iAjBj\n\u03a6\u2190 (ID \u2212 \u03bb \u00b7Zi) \u03a3 (ID \u2212 \u03bb \u00b7Zi)T , where ID denotes the D \u00d7D identity matrix. Ai \u2190 [u1, \u00b7 \u00b7 \u00b7 ,uH ], where {u1, \u00b7 \u00b7 \u00b7 ,uH} are the top eigenvectors of \u03a6. Bi \u2190 (1\u2212 \u03bb \u00b7 (M \u2212 1)/M)\u22121 \u00b7AiT (ID \u2212 \u03bb \u00b7Zi)\nend for end for return Decoder-Encoder pairs (Ai,Bi) M i=1\nAlgorithm 1 is a simple greedy procedure reminiscent of the back-fitting algorithm for additive models (Friedman et al., 2001). Each module is optimised in turn, leaving the parameters for the other modules fixed. The error E\u03bb (W,D) decreases every epoch until a critical point is reached.\nTheorem 2 Suppose that \u03a3 = XXT is of full rank. Let (Wt)Tt=1 be a sequence of parameters obtained by Algorithm 1. For every epoch t = {1, \u00b7 \u00b7 \u00b7 , T}, we have E\u03bb (Wt+1,D) < E\u03bb (Wt,D), unless Wt is a critical point for E\u03bb (\u00b7,D), in which case E\u03bb (Wt+1,D) \u2264 E\u03bb (Wt,D).\nTheorem 2 justifies the procedure in Algorithm 1. The proof is given in Appendix B. We compared Algorithm 1 with (batch) gradient descent on an artificial data set consisting of 1000 data points randomly generated from a Gaussian mixture data set consisting of equally weighted spherical Gaussians with standard deviation 0.25 and a mean drawn from a standard multivariate normal distribution. We measured the time for the cost to stop falling by at least = 10\u22125 per epoch for both Algorithm 1 and (batch) gradient descent. The procedure was repeated ten times. The two algorithms performed similarly in terms of minimum cost attained, with Algorithm 1 attaining slightly lower costs on average. However, as we can see from Table 1, Algorithm 1 converged several orders of magnitude faster than gradient descent."}, {"heading": "4. Empirical results", "text": "In this section we demonstrate the efficacy of Modular Autoencoders for extracting useful sets features for classification tasks. In particular, we demonstrate empirically that we can improve the performance of a classifier ensemble by first learning a diverse collection of modular feature extractors in an unsupervised way.\nOur methodology is as follows. We take a training data set D = {(xn, yn)}Nn=1 consisting of pairs of feature vectors xn and class labels yn. The data set D is pre-processed so each of the features have zero mean. We first train a Modular Autoencoder W = (Ai,Bi). For each module i we take Ci to be the 1-nearest neighbour classifier with the data set Di = {(Bixn, yn)}Nn=1. The combined prediction of the ensemble on a test point x is defined by taking a modal average of the class predictions {Ci(Bix)}Mi=1.\nWe use a collection of six image data sets from Larochelle et al. (2007), Basic, Rotations, Background Images and Background Noise variants of MNIST as well as Rectangles and Convex. In each case we use a Modular Autoencoder consisting of ten modules (M = 10), each consisting of ten hidden nodes (H = 10). The five-fold cross-validated test error is shown as a function of the diversity parameter \u03bb. We contrast with a natural baseline approach Bagging Autoencoders (BAE) in which we proceed as described, but the modules (Ai,Bi) are trained independently on bootstrapped samples from the data. In all cases, as the diversity parameter increases from zero the test error for features extracted using Modular Autoencoders falls well below the level attained by Bagging Autoencoders. As \u03bb\u2192 1 the ensemble error begins to rise, sometimes sharply."}, {"heading": "5. Understanding Modular Autoencoders", "text": "In this section we analyse the role of encouraging diversity in an unsupervised way with Modular Autoencoders and the impact this has upon supervised classification."}, {"heading": "5.1 A more complex decision boundary", "text": "We begin by considering a simple two-dimensional example consisting of a Gaussian mixture with three clusters. In this setting we use a Linear Modular Autoencoder consisting of two modules, each with a single hidden node, so each of the feature extractors is simply a projection onto a line. We use a linear Softmax classifier on each of the extracted features. The probabilistic outputs of the individual classifiers are then combined by taking the mean average. The predicted label is defined to be the one with the highest probability. Once again we observe the same trend as we saw in Section 4 - encouraging diversity leads to a substantial drop in the test error of our ensemble, with a test error of 21.3\u00b1 1.3% for \u03bb = 0 and 12.8\u00b1 1.0% for \u03bb = 0.5.\nTo see why this is the case we contrast the features extracted when \u03bb = 0 with those extracted when \u03bb = 0.5. Figure 3 shows the projection of the class densities onto the two extracted features when \u03bb = 0. No emphasis is placed upon diversity the two modules are trained independently to maximise reconstruction. Hence, the features extract identical information and there is no ensemble gain. Figure 5 shows the resultant decision boundary; a simple linear decision boundary based upon a single one-dimensional classification.\nIn contrast, when \u03bb = 0.5 the two features yield diverse and complementary information. As we can see from Figure 4, one feature separates class 1 from classes 2 and 3, and the other separates class 3 from classes 1 and 2. As we can see from the right of Figure 5, the resulting decision boundary accurately reflects the true class boundaries, despite being based upon two independently trained one-dimensional classifiers. This leads to the reduction in test error for \u03bb = 0.5.\nIn general, Modular Autoencoders trained with the loss function defined in (1) extract diverse and complementary sets of features, whilst reflecting the main factors of variation within the data. Simple classifiers may be trained independently based upon these sets of features, so that the combined ensemble system gives rise to a complex decision boundary."}, {"heading": "5.2 Diversity of feature extractors", "text": "In this Section we give further insight into the effect of diversity upon Modular Autoencoders. We return to the empirical framework of Section 4. Figure 6 plots two values test error for features extracted with Linear Modular Autoencoders. We plot both the average individual error of the classifiers (without ensembling the outputs) and the test error of the ensemble. In every case the average individual error rises as the diversity parameter moves\naway from zero. Nonetheless, the ensemble error falls as the diversity parameter increases (at least initially).\nTo see why the ensemble error falls whilst the average individual error rises we consider the metric structure of the different sets of extracted features. To compare the metric structure captured by different feature extractors, both with one another, and with the original feature space, we use the concept of distance correlation introduced by (Sze\u0301kely et al., 2007).\nGiven a feature extractor map F (such as x 7\u2192 Bix) we compute D (F,D), the distance correlation based upon the pairs {(F (x), x) : x \u2208 D}. The quantity D (F,D) tells us how faithfully the extracted feature space for a feature map F captures the metric structure of the original feature space. For each of our data sets we compute the average value of D (F,D) across the different feature extractors. To reduce computational cost we restrict ourselves to a thousand examples of both train and test data, Dred. Figure 7 shows how the average value of M\u22121 \u2211M i=1D ( Bi,Dred ) varies as a function of the diversity parameter. As we increase the diversity parameter \u03bb we also reduce the emphasis on reconstruction accuracy. Hence, increasing \u03bb reduces the degree to which the extracted features accurately\nreflect the metric structure of the original feature space. This explains the fall in individual classification accuracy we observed in Figure 6.\nGiven feature extractor maps F and G (such as x 7\u2192 Bix and x 7\u2192 Bjx), on a data set D we compute C (F,G,D), the distance correlation based upon the pairs {(F (x), G(x)) : x \u2208 D}. The quantity C (F,G,D) gives us a measure of the correlation between the metric structures induced by F and G. Again, to reduce computational cost we restrict ourselves to a thousand examples of both train and test data, Dred. To measure the degree of diversity between our different sets of extracted features we compute the average pairwise correlation C ( Bi,Bj ,Dred ) , averaged across all pairs of distinct feature maps Bi,Bj with i 6= j. Again we restrict ourselves to a thousand out-of-sample examples. Figure 8 shows how the degree of metric correlation between the different sets of extracted features falls as we increase the diversity parameter \u03bb. Increasing \u03bb places an increasing level of emphasis on a diversity of reconstructions. This diversity results in the different classifiers making different errors from one another enabling the improved ensemble performance we observed in Section 4."}, {"heading": "6. Discussion", "text": "We have introduced a modular approach to representation learning where an ensemble of auto-encoder modules is learnt so as to achieve a diversity of reconstructions, as well\nas maintaining low reconstruction error for each individual module. We demonstrated empirically, using six benchmark data sets, that we can improve the performance of a classifier ensemble by first learning a diverse collection of modular feature extractors in an unsupervised way. We explored Linear Modular Autoencoders and derived an SVDbased algorithm which converges three orders of magnitude faster than gradient descent. In forthcoming work we extend this concept beyond the realm of auto-encoders and into a broader framework of modular manifold learning."}, {"heading": "Acknowledgments", "text": "The research leading to these results has received funding from EPSRC Centre for Doctoral Training grant EP/I028099/1, the EPSRC Anyscale project EP/L000725/1 and from the AXLE project funded by the European Union\u2019s Seventh Framework Programme (FP7/20072013) under grant agreement no 318633. We would also like to thank Kostas Sechidis, Nikolaos Nikolaou, Sarah Nogueira and Charlie Reynolds for their useful comments, and the anonymous referee for suggesting several useful references."}, {"heading": "Appendix A. Modular Regression Networks", "text": "We shall consider the more general framework of Modular Regression Networks (MRN) which encompasses Modular Autoencoders (MAE).\nA Modular Regression Network F = {Fi}Mi=1 is an ensemble system consisting of M mappings Fi : RD \u2192 RQ. The MRN F is trained using the following loss,\nL\u03bb (F ,x,y) := 1\nM M\u2211 i=1 error\ufe37 \ufe38\ufe38 \ufe37 ||Fi (x)\u2212 y||2\u2212\u03bb \u00b7 1 M M\u2211 i=1 diversity\ufe37 \ufe38\ufe38 \ufe37\u2223\u2223\u2223\u2223Fi(x)\u2212 F (x)\u2223\u2223\u2223\u22232, (2) where x is a feature vector, y a corresponding output, and F denotes the arithmetic average F := 1M \u2211M i=1 Fi. Given a data set D = {(xn,yn)} N n=1 we let E\u03bb (F ,D) denote the loss L\u03bb (F ,x,y) averaged over (x,y) \u2208 D. The MRN F is trained to minimise E\u03bb (F ,D).\nA.1 Investigating the loss function\nProposition 1 Given \u03bb \u2208 [0,\u221e) and an MRN F , for each example (x,y) we have\nL\u03bb (F ,x,y) = (1\u2212 \u03bb) \u00b7 1\nM \u2211 i ||Fi (x)\u2212 y||2 + \u03bb \u00b7 \u2223\u2223\u2223\u2223F (x)\u2212 y\u2223\u2223\u2223\u22232 .\nProof The result may be deduced from the Ambiguity Decomposition (Krogh et al., 1995).\nThe following proposition relates MRNs to Negative Correlation Learning (Liu and Yao, 1999).\nProposition 2 Given an MRN F and (x,y) \u2208 D we have\n\u2202L\u03bb (F ,x,y) \u2202Fi = 2 M \u00b7 ( (Fi (x)\u2212 y)\u2212 \u03bb \u00b7 ( Fi (x)\u2212 F (x) )) .\nProof This follows from the definitions of L\u03bb (F ,x,y) and F (x).\nIn Negative Correlation Learning each network Fi is updated in parallel with rule\n\u03b8i \u2190 \u03b8i \u2212 \u03b1 \u00b7 \u2202Fi \u2202\u03b8i\n( (Fi (x)\u2212 y)\u2212 \u03bb \u00b7 ( Fi (x)\u2212 F (x) )) ,\nfor each example (x,y) \u2208 D in turn, where \u03b8i denotes the parameters of Fi and \u03b1 denotes the learning rate (Liu and Yao, 1999, Equation 4). By Proposition 2 this is equivalent to training a MRN F to minimise E\u03bb (F ,D) with stochastic gradient descent, using the learning rate M/2 \u00b7 \u03b1.\nA.2 An upper bound on the diversity parameter\nWe now focus on a particular class of networks. Suppose that there exists a vector-valued function \u03d5 (x; \u03c1), parametrised by \u03c1. We assume that \u03d5 is sufficiently expressive that for\neach possible feature vector x \u2208 RD there exists a choice of parameters \u03c1 with \u03d5 (x; \u03c1) 6= 0. Suppose that for each i there exists a weight matrix W i and parameter vector \u03c1i such that Fi (x) = W i\u03d5 ( x; \u03c1i ) . We refer to such networks as Modular Linear Top Layer Networks (MLT). This is the natural choice in the context of regression and includes Modular Autoencoders with linear outputs.\nTheorem 3 Suppose we have a MLT F and a dataset D. The following dichotomy holds:\n\u2022 If \u03bb \u2264 1 then inf E\u03bb (F ,D) \u2265 0.\n\u2022 If \u03bb > 1 then inf E\u03bb (F ,D) = \u2212\u221e."}, {"heading": "In both cases the infimums range over possible parametrisations for the MRN F .", "text": "Moreover, if \u03bb > 1 there exists parametrisations of F with arbitrarily low error E\u03bb (F ,D) and arbitrarily high squared loss for the ensemble output F and average squared loss for the individual regression networks F .\nProof It follows from Proposition 1 that whenever \u03bb \u2264 1, L\u03bb (F ,x,y) \u2265 0 for all choices of F and all (x,y) \u2208 D. This implies the consequent in the case where \u03bb \u2264 1.\nWe now address the implications of \u03bb > 1. Take (x\u0303, y\u0303) \u2208 D. By the (MLT) assumption we may find parameters \u03c1 so that \u03d5 (x\u0303; \u03c1) 6= 0. Without loss of generality we may assume that 0 6= c = \u03d51 (x\u0303; \u03c1), where \u03d51 (x; \u03c1) denotes the first coordinate of \u03d5 (x; \u03c1). We shall leave \u03c1 fixed and obtain a sequence (Fq)q\u2208N, where for each q we have F qi (x) = W\n(i,q)\u03d5 (x; \u03c1) by choosing W (i,q). First take W (i,q) = 0 for all i = 3, \u00b7 \u00b7 \u00b7 ,M , so\nF (x) = 1\nM (F1(x) + F2(x)) .\nIn addition we choose W (1,q) kl = W (2,q) kl = 0 for all k > 1 or l > 1. Finally we take W (1,q) 11 = c \u00b7 ( q2 + q ) and W (2,q) 11 = \u2212c \u00b7 q2. It follows that for each (x,y) \u2208 D we have,\nF1(x) = W (1,q)\u03d5 (x, \u03c1) = ( c(q2 + q)\u03d51 (x; \u03c1) , 0, \u00b7 \u00b7 \u00b7 , 0 ) F1(x) = W (2,q)\u03d5 (x, \u03c1) = ( \u2212cq2\u03d51 (x; \u03c1) , 0, \u00b7 \u00b7 \u00b7 , 0 ) ,\nF (x) = ( M\u22121cq\u03d51 (x; \u03c1) , 0, \u00b7 \u00b7 \u00b7 , 0 ) .\nNoting that \u03d51 (x; \u03c1) = c 6= 0 and (x\u0303, y\u0303) \u2208 D we see that,\n1\nN N\u2211 n=1 ( F (xn)\u2212 yn )2 = \u2126(q2). (3)\nOn the other hand we have,\n1\nN N\u2211 n=1 (F1(xn)\u2212 yn)2 = \u2126(q4),\nand clearly for all i,\n1\nN N\u2211 n=1 (F1(xn)\u2212 yn)2 > 0.\nHence,\n1\nN N\u2211 n=1 1 M M\u2211 i=1 (F1(xn)\u2212 yn)2 = \u2126(q4).\nCombining with Equation (3) this gives\nE\u03bb (Fq,D) = (1\u2212 \u03bb) \u00b7 \u2126(q4) + \u03bb \u00b7 \u2126(q2).\nSince \u03bb > 1 this implies\nE\u03bb (Fq,D) = \u2212\u2126(q4). (4)\nBy Equations 3 and 4 we see that for any Q1, Q2 > 1, by choosing q sufficiently large we have\nE\u03bb (Fq,D) < \u2212Q1,\nand\n1\nN N\u2211 n=1 1 M M\u2211 i=1 (Fi(xn)\u2212 yn)2 \u2265 1 N N\u2211 n=1 ( F (xn)\u2212 yn )2 > Q2,\nThis proves the second item.\nThe impact of Theorem 3 is that whenever \u03bb > 1, minimising E\u03bb will result in one or more parameters diverging. Moreover, the resultant solutions may be arbitrarily bad in terms of training error, leading to very poor choices of parameters.\nTheorem 4 Suppose we have a MLT F on a data set D. Suppose we choose i \u2208 {1, \u00b7 \u00b7 \u00b7 ,M} and fix Fj for all j 6= i. The following dichotomy holds:\n\u2022 If \u03bb < MM\u22121 then inf E\u03bb (F ,D) > \u2212\u221e.\n\u2022 If \u03bb > MM\u22121 then inf E\u03bb (F ,D) = \u2212\u221e.\nIn both cases the infimums range over possible parameterisations for the function fi, with fj fixed for j 6= i.\nProof We fix Fj for j 6= i. For each pair (x,y) \u2208 D we consider L\u03bb (F) for large ||Fi(x)||. By Proposition 1 we have\nL\u03bb (F ,x,y) = (1\u2212 \u03bb) \u00b7 1 M \u2126 ( ||Fi(x)||2 ) + \u03bb \u00b7 \u2126 ( || 1 M Fi(x)||2 ) = ( 1\u2212 \u03bb \u00b7 M \u2212 1\nM\n) \u00b7 \u2126 ( ||Fi(x)||2 ) .\nHence, if \u03bb < MM\u22121 we see that L\u03bb (F ,x,y) is bounded from below for each example (x,y), for all choices of Fi. This implies the first case.\nIn addition, the fact that \u03d5(x1, \u03c1) 6= 0 for some choice of parameters \u03c1 means that we may choose a sequence of parameters such that ||Fi(x)|| \u2192 \u221e for one or more examples (x,y) \u2208 D. Hence, if \u03bb > MM\u22121 , we may choose weights so that L\u03bb (F ,x,y) \u2192 \u2212\u221e for some examples (x,y) \u2208 D. The above asymptotic formula also implies that L\u03bb (F ,x,y) is uniformly bounded from above when \u03bb > MM\u22121 . Thus, we have inf E\u03bb (F ,D) = \u2212\u221e."}, {"heading": "Appendix B. Derivation of the Linear Modular Autoencoder Training", "text": "Algorithm\nIn what follows we fix D,N , and H < D and define\nCD,H := { (A,B) : A \u2208 RD\u00d7H ,B \u2208 RH\u00d7D } .\nWe data set D \u2282 RD, with D features and N examples, and let X denote the D\u00d7N matrix given by X = [x1, \u00b7 \u00b7 \u00b7 ,xN ].\nGiven any \u03bb \u2208 [0,\u221e) we define our error function by\nE\u03bb (W,D) = 1\nN N\u2211 n=1  1 M M\u2211 j=1 ||xn \u2212AjBjxn||2 \u2212 \u03bb \u00b7 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223AjBjxn \u2212 1M M\u2211 k=1 AkBkxn \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2 \n= 1\nN \u00b7M M\u2211 i=1 ||X \u2212AjBjX||2 \u2212 \u03bb \u00b7 \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223AjBjX \u2212 1M M\u2211 k=1 AkBkX \u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223 2  ,\nwhere W = ((Ai,Bi))Mi=1 \u2208 (CD,P ) M , and ||\u00b7|| denotes the Frobenius matrix norm.\nProposition 3 Suppose we take X so that \u03a3 = XXT has full rank D and choose \u03bb < M/(M \u2212 1). We pick some i \u2208 {1, \u00b7 \u00b7 \u00b7 ,M}, and fix Aj ,Bj for each j 6= i. Then we find (Ai,Bi) which minimises E\u03bb (W,D) by\n1. Taking Ai to be the matrix whose columns consist of the D unit eigenvectors with largest eigenvalues for the matrixID \u2212 \u03bb\nM \u2211 j 6=i AjBj\n\u03a3 ID \u2212 \u03bb\nM \u2211 j 6=i AjBj T , 2. Choosing Bi so that\nBi =\n( 1\u2212 \u03bb \u00b7 M \u2212 1\nM\n)\u22121 \u00b7ATi ID \u2212 \u03bb M \u2211 j 6=i AjBj  .\nMoreover, for any other decoder-encoder pair ( A\u0303i, B\u0303i ) which also minimises E\u03bb (W,D)\n(with the remaining pairs Aj ,Bj fixed) we have A\u0303iB\u0303i = AiBi.\nProposition 3 implies the following proposition from Section 3.\nTheorem 2 Suppose that \u03a3 is of full rank. Let (Wt)Tt=1 be a sequence of parameters obtained by Algorithm 1. For every epoch t = {1, \u00b7 \u00b7 \u00b7 , T}, we have E\u03bb (Wt+1,D) < E\u03bb (Wt,D), unless Wt is a critical point for E\u03bb (\u00b7,D), in which case E\u03bb (Wt+1,D) \u2264 E\u03bb (Wt,D).\nProof By Proposition 3, each update in Algorithm 1 modifies a decoder-encoder pair (Ai,Bi) so as to minimise E\u03bb (W,D), subject to the condition that (Aj ,Bj) remain fixed for j 6= i. Hence, E\u03bb (Wt+1,D) \u2264 E\u03bb (Wt,D).\nNow suppose E\u03bb (Wt+1,D) = E\u03bb (Wt,D) for some t. Note that E\u03bb (Wt+1,D) is a function of C = {Ci}Mi=1 where Ci = AiBi for i = 1, \u00b7 \u00b7 \u00b7 ,M . We shall show that Ct is a critical point in terms for E\u03bb. Since E\u03bb (Wt+1,D) = E\u03bb (Wt,D) we must have Ct+1i = Cti for i = 1, \u00b7 \u00b7 \u00b7 ,M . Indeed, Proposition 3 implies that Algorithm 1 only modifies Ci when E\u03bb (W,D) is reduced (although the individual matrices Ai and Bi may be modified). Since Ct+1i = C t i we may infer that C t i attains the minimum value of E\u03bb (W,D) over the set of parameters such that Cj = C t j for all j 6= i. Hence, at the point Ct we have \u2202E\u03bb/\u2202Ci = 0 for each i = 1, \u00b7 \u00b7 \u00b7 ,M . Thus, \u2202E\u03bb/\u2202Ai = 0 and \u2202E\u03bb/\u2202Bi = 0, for each i, by the chain rule.\nTo prove Proposition 3 we require two intermediary lemmas. The first is a theorem concerning Rank Restricted Linear Regression.\nTheorem 5 Suppose we have D\u00d7N data matrices X,Y . We define a function E : CD,H \u2192 R by\nE (A,B) = ||Y \u2212ABX||2 .\nSuppose that the matrix XXT is invertible and define \u03a3 := (Y XT )(XXT )\u22121(XY T ). Let U denote the N \u00d7 D matrix who\u2019s columns are the D unit eigenvectors of \u03a3 with largest eigen-values. Then the minimum for E is attained by taking,\nA = U B = UT (Y XT )(XXT )\u22121.\nProof See Baldi and Hornik (1989, Fact 4).\nNote that the minimal solution is not unique. Indeed if A,B attain the minimum, then so does AC, C\u22121B for any invertible H \u00d7H matrix C.\nLemma 6 Suppose we have D \u00d7 N matrices X and Y1, \u00b7 \u00b7 \u00b7 ,YQ, and scalars \u03b11, \u00b7 \u00b7 \u00b7 , \u03b1Q such that \u2211Q q=1 \u03b1q > 0. Then we have\narg min (A,B)\u2208CD,H  Q\u2211 q=1 \u03b1q||Yq \u2212ABX||2 \n= arg min (A,B)\u2208CD,H ||  Q\u2211 q=1 \u03b1\u0303qY \u2212ABX||2  ,\nwhere \u03b1\u0303q = \u03b1q/ (\u2211Q q\u2032=1 \u03b1q\u2032 ) .\nProof We use the fact that under the Frobenius matrix norm, ||M ||2 = tr(MMT ) for matrices M , where tr denotes the trace operator. Note also that the trace operator is linear and invariant under matrix transpositions. Hence, we have\nQ\u2211 q=1 \u03b1q ||Yq \u2212ABX||2\n= Q\u2211 q=1 \u03b1q \u00b7 tr ( (Yq \u2212ABX)(Yq \u2212ABX)T ) =\nQ\u2211 q=1 \u03b1q \u00b7 tr ( YqY T q \u2212 2(AB)XY Tq + (AB)XX(AB)T )\n= Q\u2211 q=1 \u03b1q ||Yq||2 \u2212 tr 2(AB)X  Q\u2211 q=1 \u03b1qYq T + tr  Q\u2211 q=1 \u03b1q  (AB)XXT (AB)T  .\nNote that we may add constant terms (ie. terms not depending on A or B) and multiply by positive scalars without changing the minimising argument. Hence, dividing by \u2211Q q=1 \u03b1q > 0 and adding a constant we see that the minimiser of the above expression is equal to the minimiser of tr ( (AB)XXT (AB)T ) + tr\n2(AB)X  Q\u2211 q=1 \u03b1\u0303qYq T + tr   Q\u2211 q=1 \u03b1\u0303qYq  Q\u2211 q=1 \u03b1\u0303qYq T  .\nMoreover, by the linearity of the trace operator this expression is equal to\u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223  Q\u2211 q=1 \u03b1\u0303qYq \u2212ABX \u2223\u2223\u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\u2223 2 .\nThis proves the lemma.\nProof [Proposition 3] We begin observing that if we fix Aj ,Bj for j 6= i, then minimising E\u03bb (W,D) is equivalent to minimising\n||X \u2212AiBiX||2 \u2212 \u03bb (\n1\u2212 1 M )2 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 1M \u2212 1 \u00b7 S\u2212i \u2212AiBiX \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u22232\u2212\n\u03bb\nM2 \u2211 j 6=i ||(MAjBjX \u2212 S\u2212i)\u2212AiBiX||2 ,\nwhere S\u2212i = \u2211\nj 6=iAjBjX. This holds as the above expression differs from E\u03bb (W,D) only by a multiplicative factor of NM and some constant terms which do not depend upon Ai,Bi.\nBy Lemma 6, minimising the above expression in terms of Ai,Bi is equivalent to minimising\n||Y \u2212AiBiX||, (5)\nwith\nY = ( 1\u2212 \u03bb (( 1\u2212 1\nM )2 + M \u2212 1 M2 ))\u22121\n\u00b7 X \u2212 \u03bb \u00b7 (1\u2212 1\nM )2 1 M \u2212 1 \u00b7 S\u2212i + 1 M2 \u2211 j 6=i (MAjBjX \u2212 S\u2212i)  . Here we use the fact that \u03bb < M/(M \u2212 1), so\n1\u2212 \u03bb (( 1\u2212 1\nM )2 + M \u2212 1 M2 ) = 1\u2212 \u03bb \u00b7 M \u2212 1 M > 0.\nWe may simplify our expression for Y as follows,\nY = ( 1\u2212 \u03bb \u00b7 M \u2212 1\nM\n)\u22121 \u00b7 ID \u2212 \u03bb M \u2211 j 6=i AjBj X. By Theorem 5, we may minimise the expression in 5 by taking Ai to be the matrix whose columns consist of the D unit eigenvectors with largest eigenvalues for the matrixID \u2212 \u03bb\nM \u2211 j 6=i AjBj\n(XXT ) ID \u2212 \u03bb\nM \u2211 j 6=i AjBj T , and setting\nBi =\n( 1\u2212 \u03bb \u00b7 M \u2212 1\nM\n)\u22121 \u00b7ATi ID \u2212 \u03bb M \u2211 j 6=i AjBj  . This completes the proof of the proposition."}], "references": [{"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["Pierre Baldi", "Kurt Hornik"], "venue": "Neural networks,", "citeRegEx": "Baldi and Hornik.,? \\Q1989\\E", "shortCiteRegEx": "Baldi and Hornik.", "year": 1989}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pierre Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Diversity creation methods: a survey and categorisation", "author": ["Gavin Brown", "Jeremy Wyatt", "Rachel Harris", "Xin Yao"], "venue": "Information Fusion,", "citeRegEx": "Brown et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Brown et al\\.", "year": 2005}, {"title": "The elements of statistical learning, volume 1. Springer series in statistics", "author": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2001}, {"title": "Coupled dimensionality reduction and classification for supervised and semi-supervised multilabel learning", "author": ["Mehmet G\u00f6nen"], "venue": "Pattern recognition letters,", "citeRegEx": "G\u00f6nen.,? \\Q2014\\E", "shortCiteRegEx": "G\u00f6nen.", "year": 2014}, {"title": "Neural network ensembles, cross validation, and active learning", "author": ["Anders Krogh", "Jesper Vedelsby"], "venue": null, "citeRegEx": "Krogh and Vedelsby,? \\Q1995\\E", "shortCiteRegEx": "Krogh and Vedelsby", "year": 1995}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["Hugo Larochelle", "Dumitru Erhan", "Aaron Courville", "James Bergstra", "Yoshua Bengio"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Larochelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Ensemble learning via negative correlation", "author": ["Yong Liu", "Xin Yao"], "venue": "Neural Networks,", "citeRegEx": "Liu and Yao.,? \\Q1999\\E", "shortCiteRegEx": "Liu and Yao.", "year": 1999}, {"title": "Foundations of coupled nonlinear dimensionality reduction", "author": ["Dmitry Storcheus", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "arXiv preprint arXiv:1509.08880,", "citeRegEx": "Storcheus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Storcheus et al\\.", "year": 2015}, {"title": "Measuring and testing dependence by correlation of distances", "author": ["G\u00e1bor J Sz\u00e9kely", "Maria L Rizzo", "Nail K Bakirov"], "venue": "The Annals of Statistics,", "citeRegEx": "Sz\u00e9kely et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sz\u00e9kely et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "In representation learning these features are learnt directly from the data (Bengio et al., 2013).", "startOffset": 76, "endOffset": 97}, {"referenceID": 3, "context": "Ensemble methods combine the outputs of a multiplicity of models in order to obtain an enriched hypothesis space whilst controlling variance (Friedman et al., 2001).", "startOffset": 141, "endOffset": 164}, {"referenceID": 2, "context": "is typically encouraged, either through some form of randomisation, or by encouraging diversity through supervised training (Brown et al., 2005).", "startOffset": 124, "endOffset": 144}, {"referenceID": 1, "context": "is typically encouraged, either through some form of randomisation, or by encouraging diversity through supervised training (Brown et al., 2005). We investigate an unsupervised approach to learning a set of diverse but complementary representations from unlabelled data. As such, we move away from the recent trend towards coupled dimensionality reduction in which the tasks of feature extraction and supervised learning are performed in unison G\u00f6nen (2014); Storcheus et al.", "startOffset": 125, "endOffset": 458}, {"referenceID": 1, "context": "is typically encouraged, either through some form of randomisation, or by encouraging diversity through supervised training (Brown et al., 2005). We investigate an unsupervised approach to learning a set of diverse but complementary representations from unlabelled data. As such, we move away from the recent trend towards coupled dimensionality reduction in which the tasks of feature extraction and supervised learning are performed in unison G\u00f6nen (2014); Storcheus et al. (2015). Whilst coupled dimensionality reduction has been shown to improve accuracy for certain classification tasks G\u00f6nen (2014), the unsupervised approach allows us to use unlabelled data to learn a transferable representation which may be used on multiple tasks without the need for retraining Bengio et al.", "startOffset": 125, "endOffset": 483}, {"referenceID": 1, "context": "is typically encouraged, either through some form of randomisation, or by encouraging diversity through supervised training (Brown et al., 2005). We investigate an unsupervised approach to learning a set of diverse but complementary representations from unlabelled data. As such, we move away from the recent trend towards coupled dimensionality reduction in which the tasks of feature extraction and supervised learning are performed in unison G\u00f6nen (2014); Storcheus et al. (2015). Whilst coupled dimensionality reduction has been shown to improve accuracy for certain classification tasks G\u00f6nen (2014), the unsupervised approach allows us to use unlabelled data to learn a transferable representation which may be used on multiple tasks without the need for retraining Bengio et al.", "startOffset": 125, "endOffset": 605}, {"referenceID": 1, "context": "Whilst coupled dimensionality reduction has been shown to improve accuracy for certain classification tasks G\u00f6nen (2014), the unsupervised approach allows us to use unlabelled data to learn a transferable representation which may be used on multiple tasks without the need for retraining Bengio et al. (2013). We show that one can improve the performance of a classifier ensemble by first learning a diverse collection of modular feature extractors in a purely unsupervised way (see Section 4) and then training a set of classifiers independently.", "startOffset": 288, "endOffset": 309}, {"referenceID": 7, "context": "The loss function L\u03bb (W,x) is inspired by (but not identical to) the Negative Correlation Learning approach of by Liu and Yao for training supervised ensembles of neural networks (Liu and Yao, 1999)1.", "startOffset": 179, "endOffset": 198}, {"referenceID": 1, "context": "The first term corresponds to the squared reconstruction error typically minimised by Autoencoders (Bengio et al., 2013).", "startOffset": 99, "endOffset": 120}, {"referenceID": 0, "context": "Hence, in the linear case E0 (W,D) is minimised by taking each Bi to be the projection onto the first H principal components of the data covariance (Baldi and Hornik, 1989).", "startOffset": 148, "endOffset": 172}, {"referenceID": 3, "context": "Algorithm 1 is a simple greedy procedure reminiscent of the back-fitting algorithm for additive models (Friedman et al., 2001).", "startOffset": 103, "endOffset": 126}, {"referenceID": 6, "context": "We use a collection of six image data sets from Larochelle et al. (2007), Basic, Rotations, Background Images and Background Noise variants of MNIST as well as Rectangles and Convex.", "startOffset": 48, "endOffset": 73}, {"referenceID": 9, "context": "To compare the metric structure captured by different feature extractors, both with one another, and with the original feature space, we use the concept of distance correlation introduced by (Sz\u00e9kely et al., 2007).", "startOffset": 191, "endOffset": 213}, {"referenceID": 7, "context": "The following proposition relates MRNs to Negative Correlation Learning (Liu and Yao, 1999).", "startOffset": 72, "endOffset": 91}], "year": 2015, "abstractText": "We introduce the concept of a Modular Autoencoder (MAE), capable of learning a set of diverse but complementary representations from unlabelled data, that can later be used for supervised tasks. The learning of the representations is controlled by a trade off parameter, and we show on six benchmark datasets the optimum lies between two extremes: a set of smaller, independent autoencoders each with low capacity, versus a single monolithic encoding, outperforming an appropriate baseline. In the present paper we explore the special case of linear MAE, and derive an SVD-based algorithm which converges several orders of magnitude faster than gradient descent.", "creator": "LaTeX with hyperref package"}}}