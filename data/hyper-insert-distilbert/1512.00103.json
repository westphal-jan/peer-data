{"id": "1512.00103", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2015", "title": "Multilingual Language Processing From Bytes", "abstract": "we describe an architectural lstm - based computation model which we call generalized byte - response to - span ( bts ) algorithm that uniquely reads text as bytes < and outputs span while annotations those of the equivalent form [ label start, length, label ] sequences where start positions, letter lengths, and labels \\ are also separate entries in our coding vocabulary. and because we cannot operate virtually on unicode bytes rather than language - specific words data or key characters, naturally we can vastly analyze text in many possible languages stored with a single model. significantly due to filling the small computing vocabulary tab size, constructing these 2d multilingual models are hence very compact, but produce predictable results similar to or rather better recognition than the state - domain of - the - art in generic part - of - speech variable tagging methods and named entity recognition that they use it only the provided document training pattern datasets ( no external data abstraction sources ). our digital models are easily learning \" from scratch \" in that discipline they are do basically not rely on any elements of applying the standard arithmetic pipeline in natural language processing.", "histories": [["v1", "Tue, 1 Dec 2015 00:23:44 GMT  (49kb,D)", "http://arxiv.org/abs/1512.00103v1", null], ["v2", "Sat, 2 Apr 2016 16:26:23 GMT  (92kb,D)", "http://arxiv.org/abs/1512.00103v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["dan gillick", "cliff brunk", "oriol vinyals", "amarnag subramanya"], "accepted": true, "id": "1512.00103"}, "pdf": {"name": "1512.00103.pdf", "metadata": {"source": "CRF", "title": "Multilingual Language Processing From Bytes", "authors": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "emails": ["asubram}@google.com"], "sections": [{"heading": "1 Introduction", "text": "The long-term trajectory of research in Natural Language Processing has seen the replacement of rules and specific linguistic knowledge with machine learned components. Perhaps the most standardized way that knowledge is still injected into largely statistical systems is through the processing pipeline: Some set of basic language-specific tokens are identified in a first step. Sequences of tokens are segmented into sentences in a second step. The resulting sentences are fed one at a time for syntactic analysis: Part-of-Speech (POS) tagging and parsing. Next, the predicted syntactic structure is typically used as features in semantic analysis, Named Entity Recognition (NER), Semantic Role Labeling,\netc. While each step of the pipeline now relies more on data and models than on hand-curated rules, the pipeline structure itself encodes one particular understanding of how meaning attaches to raw strings.\nOne motivation for our work is to try removing this structural dependence. Rather than rely on the intermediate representations invented for specific subtasks (for example, Penn Treebank tokenization), we are allowing the model to learn whatever internal structure is most conducive to producing the annotations of interest. To this end, we describe a Recurrent Neural Network (RNN) model that reads raw input string segments, one byte at a time, and produces output span annotations corresponding to specific byte regions in the input1. This is truly language annotation from scratch (see Collobert et al. (2011) and Zhang and LeCun (2015)).\nTwo key innovations facilitate this approach. First, Long Short Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997) allow us to replace the traditional independence assumptions in text processing with structural constraints on memory. While we have long known that long-term dependencies are important in language, we had no mechanism other than conditional independence to keep sparsity in check. The memory in an LSTM, however, is not constrained by any explicit assumptions of independence. Rather, its ability to learn patterns is limited only by the structure of the network and the size of the memory (and of course the amount of training data).\n1Our span annotation model can be applied to any sequence labeling task; it is not immediately applicable to predicting more complex structures like trees.\nar X\niv :1\n51 2.\n00 10\n3v 1\n[ cs\n.C L\n] 1\nD ec\n2 01\nSecond, sequence-to-sequence models (Sutskever et al., 2014), allow for flexible input/output dynamics. Traditional models, including feedforward neural networks, read fixed-length inputs and generate fixed-length outputs by following a fixed set of computational steps. Instead, we can now read an entire segment of text before producing an arbitrary number of outputs, allowing the model to learn a function best suited to the task.\nWe leverage these two ideas with a basic strategy: Decompose inputs and outputs into their component pieces, then read and predict them as sequences. Rather than read words, we are reading a sequence of unicode bytes2; rather than producing a label for each word, we are producing triples [start, length, label], that correspond to the spans of interest, as a sequence of three separate predictions (see Figure 1). This forces the model to learn how the components of words and labels interact so all the structure typically imposed by the NLP pipeline (as well as the rules of unicode) are left to the LSTM to model.\nDecomposed inputs and outputs have a few important benefits. First, they reduce the size of the vocabulary relative to word-level inputs, so the re-\n2We use the variable length UTF-8 encodings to keep the vocabulary as small as possible.\nsulting models are extremely compact (on the order of a million parameters). Second, because unicode is essentially a universal language, we can train models to analyze many languages at once. In fact, by stacking LSTMs, we are able to learn representations that appear to generalize across languages, improving performance significantly (without using any additional parameters) over models trained on a single language. This is the first account, to our knowledge, of a multilingual model that achieves good results across many languages, thus bypassing all the language-specific engineering usually required to build models in different languages3. We describe results similar to or better than the stateof-the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources).\nThe rest of this paper is organized as follows. Section 2 discusses related work; Section 3 describes our model; Section 4 gives training details including a new variety of dropout (Hinton et al., 2012); Section 5 gives inference details; Section 6 presents\n3These multilingual models are able to handle code-mixed text, an important practical problem that\u2019s received relatively little attention. However, we do not have any annotated data that contains code switching, so we cannot report any results.\nresults on POS tagging and NER across many languages; Finally, we summarize our contributions in section 7."}, {"heading": "2 Related Work", "text": "One important feature of our work is the use of byte inputs. Character-level inputs have been used with some success for tasks like NER (Klein et al., 2003), parallel text alignment (Church, 1993), and authorship attribution (Peng et al., 2003) as an effective way to deal with n-gram sparsity while still capturing some aspects of word choice and morphology. Such approaches often combine character and word features and have been especially useful for handling languages with large character sets (Nakagawa, 2004). However, there is almost no work that explicitly uses bytes \u2013 one exception uses byte n-grams to identify source code authorship (Frantzeskou et al., 2006) \u2013 but there is nothing, to the best of our knowledge, that exploits bytes as a cross-lingual representation of language. Work on multilingual parsing using Neural Networks that share some subset of the parameters across languages (Duong et al., 2015) seems to benefit the low-resource languages; however, we are sharing all the parameters among all languages.\nRecent work has shown that modeling the sequence of characters in each token with an LSTM can more effectively handle rare and unknown words than independent word embeddings (Ling et al., 2015; Ballesteros et al., 2015). Similarly, language modeling, especially for morphologically complex languages, benefits from a Convolutional Neural Network (CNN) over characters to generate word embeddings (Kim et al., 2015). Rather than decompose words into characters, Rohan and Denero (2015) encode rare words with Huffman codes, allowing a neural translation model to learn something about word subcomponents. In contrast to this line of research, our work has no explicit notion of tokens and operates on bytes rather than characters.\nOur work is philosophically similar to Collobert et al.\u2019s (2011) experiments with \u201calmost from scratch\u201d language processing. They avoid taskspecific feature engineering, instead relying on a multilayer feedforward (or convolutional) Neural Network to combine word embeddings to produce\nfeatures useful for each task. In the Results section, below, we compare NER performance on the same dataset they used. The \u201calmost\u201d in the title actually refers to the use of preprocessed (lowercased) tokens as input instead of raw sequences of letters. Our byte-level models can be seen as a realization of their comment: \u201cA completely from scratch approach would presumably not know anything about words at all and would work from letters only.\u201d Recent work with convolutional neural networks that read character-level inputs (Zhang and LeCun, 2015) shows some interesting results on a variety of classification tasks, but because their models need very large training sets, they do not present comparisons to established baselines on standard tasks.\nFinally, recent work on Automatic Speech Recognition (ASR) uses a similar sequence-to-sequence LSTM framework to produce letter sequences directly from acoustic frame sequences (Chan et al., 2015; Bahdanau et al., 2015). Just as we are discarding the usual intermediate representations used for text processing, their model makes no use of phonetic alignments, clustered triphones, or pronunciation dictionaries. This line of work \u2013 discarding intermediate representations in speech \u2013 was pioneered by Graves and Jaitly (2014) and earlier, by Eyben et al. (2009)."}, {"heading": "3 Model", "text": "Our model is based on the sequence-to-sequence model used for machine translation (Sutskever et al., 2014), an adaptation of an LSTM that encodes a variable length input as a fixed-length vector, then decodes it into a variable number of outputs.\nGenerally, the sequence-to-sequence LSTM is trained to estimate the conditional probability P (y1, ..., yT \u2032 |x1, ..., xT ) where (x1, ..., xT ) is an input sequence and (y1, ..., yT \u2032) is the corresponding output sequence whose length T \u2032 may differ from T . The encoding step computes a fixed-dimensional representation v of the input (x1, ..., xT ) given by the hidden state of the LSTM after reading the last input xT . The decoding step computes the output probability P (y1, ..., yT \u2032) with the standard LSTM formulation for language modeling, except that the initial hidden state is set to v:\nP (y1, ..., yT \u2032 |x1, ..., xT ) = T \u2032\u220f t=1 P (yt|v, y1, ..., yt\u22121)\n(1) Sutskever et al. used a separate LSTM for the encoding and decoding tasks. While this separation permits training the encoder and decoder LSTMs separately, say for multitask learning or pre-training, we found our results were no worse if we used a single set of LSTM parameters for both encoder and decoder."}, {"heading": "3.1 Vocabulary", "text": "The primary difference between our model and the translation model is our novel choice of vocabulary. The set of inputs include all 256 possible bytes, a special Generate Output (GO) symbol, and a special DROP symbol used for regularization, which we will discuss below. The set of outputs include all possible span start positions (byte 0..k), all possible span lengths (0..k), all span labels (PER, LOC, ORG, MISC for the NER task), as well as a special STOP symbol. A complete span annotation includes a start, a length, and a label, but as shown in Figure 1, the model is trained to produce this triple as three separate outputs. This keeps the vocabulary size small and in practice, gives better performance (and faster convergence) than if we use the crossproduct space of the triples.\nMore precisely, the prediction at time t is conditioned on the full input and all previous predictions (via the chain rule). By splitting each span annotation into a sequence [start, length, label], we are making no independence assumption; instead we are relying on the model to maintain a memory state that captures the important dependencies.\nEach output distribution P (yt|v, y1, ..., yt\u22121) is given by a softmax over all possible items in the output vocabulary, so at a given time step, the model is free to predict any start, any length, and any label (including STOP). In practice, because the training data always has these complete triples in a fixed order, we seldom see malformed or incomplete spans (the decoder simply ignores such spans). During training, the true label yt\u22121 is fed as input to the model at step t (see Figure 1), and during inference, the argmax prediction is used instead. Note also that\nthe training procedure tries to maximize the probability in Equation 1 (summed over all the training examples). While this does not quite match our task objectives (F1 over labels, for example), it is a reasonable proxy."}, {"heading": "3.2 Independent segments", "text": "Ideally, we would like our input segments to cover full documents so that our predictions are conditioned on as much relevant information as possible. However, this is impractical for a few reasons. From a training perspective, a Recurrent Neural Network is unrolled to resemble a deep feedforward network, with each layer corresponding to a time step. It is well-known that running backpropagation over a very deep network is hard because it becomes increasingly difficult to estimate the contribution of each layer to the gradient, and further, RNNs have trouble generalizing to different length inputs (Erhan et al., 2009).\nSo instead of document-sized input segments, we make a segment-independence assumption: We choose some fixed length k and train the model on segments of length k. This has the added benefit of limiting the range of the start and length label components. It can also allow for more efficient batched inference since each segment is decoded independently. Finally, we can generate a large number of training segments by sliding a window of size k one byte at a time through a document.\nThe arbitrary choice of k creates an additional issue: what to do with span annotations that are not completely contained by a segment? The simplest solution \u2013 only including span annotations that are completely contained by the segment \u2013 turned out to work well. Note that the sliding window approach to training data generation results in segments that can begin and end mid-word, and indeed, mid-character. For both tasks described below, we set the segment size k = 60."}, {"heading": "3.3 Sequence ordering", "text": "Our model differs from the translation model in one more important way. Sutskever et al. found that feeding the input words in reverse order and generating the output words in forward order gave significantly better translations, especially for long sentences. In theory, the predictions are conditioned on\nthe entire input, but as a practical matter, the learning problem is easier when relevant information is ordered appropriately since long dependencies are harder to learn than short ones.\nBecause the byte order is more meaningful in the forward direction (the first byte of a multibyte character specifies the length, for example), we found somewhat better performance with forward order than reverse order (less than 1% absolute). But unlike translation, where the outputs have a complex order determined by the syntax of the language, our span annotations are more like an unordered set. We tried sorting them by end position in both forward and backward order, and found a small improvement (again, less than 1% absolute) using the backward ordering (assuming the input is given in the forward order). This result validates the translation ordering experiments: the modeling problem is easier when the sequence-to-sequence LSTM is used more like a stack than a queue."}, {"heading": "3.4 Model shape", "text": "We experimented with a few different architectures and found no significant improvements in using more than 320 units for the embedding dimension and LSTM memory and 4 stacked LSTMs (see Table 4). This observation holds for both models trained on a single language and models trained on many languages. Because the vocabulary is so small, the total number of parameters is dominated by the size of the recurrent matrices. All the results reported below use the same architecture (unless otherwise noted) and thus have roughly 900k parameters."}, {"heading": "4 Training", "text": "We trained our models with Stochastic Gradient Descent (SGD) on mini-batches of size 128, using an initial learning rate of 0.3. For all other hyperparameter choices, including random initialization, learning rate decay, and gradient clipping, we follow Sutskever et al. (2014). Each model is trained on a single CPU over a period of a few days, at which point, development set results have stabilized. Distributed training on GPUs would likely speed up training to just a few hours."}, {"heading": "4.1 Dropout and byte-dropout", "text": "Neural Network models are often trained using dropout (Hinton et al., 2012), which tends to improve generalization by limiting correlations among hidden units. During training, dropout randomly zeroes some fraction of the elements in the embedding layer and the model state just before the softmax layer (Zaremba et al., 2014).\nWe were able to further improve generalization with a technique we are calling byte-dropout: We randomly replace some fraction of the input bytes in each segment with a special DROP symbol (without changing the corresponding span annotations). Intuitively, this results in a more robust model, perhaps by forcing it to use longer-range dependencies rather than memorizing particular local sequences.\nIt is worth noting that noise is often added at training time to images in image classification and speech in speech recognition where the added noise does not fundamentally alter the input, but rather blurs it. By using a byte representation of language, we are now capable of achieving something like blurring with text. Indeed, if we removed 20% of the characters in a sentence, humans would be able to infer words and meaning reasonably well."}, {"heading": "5 Inference", "text": "We perform inference on a segment by (greedily) computing the most likely output at each time step and feeding it to the next time step. Experiments with beam search show no meaningful improvements (less than 0.2% absolute). Because we assume that each segment is independent, we need to choose how to break up the input into segments and how to stitch together the results.\nThe simplest approach is to divide up the input into segments with no overlapping bytes. Because the model is trained to ignore incomplete spans, this approach misses all spans that cross segment boundaries, which, depending on the choice of k, can be a significant number. We avoid the missed-span problem by choosing segments that overlap such that each span is fully contained by at least one segment.\nFor our experiments, we create segments with a fixed overlap (k/2 = 30). This means that with the exception of the first segment in a document, the model reads 60 bytes of input, but we only keep pre-\ndictions about the last 30 bytes."}, {"heading": "6 Results", "text": "Here we describe experiments on two datasets that include annotations across a variety of languages. The multilingual datasets allow us to highlight the advantages of using byte-level inputs: First, we can train a single compact model that can handle many languages at once. Second, we demonstrate some cross-lingual abstraction that improves performance of a single multilingual model over each singlelanguage model. In the experiments, we refer to the LSTM setup described above as Byte-to-Span or BTS.\nMost state-of-the-art results in POS tagging and NER leverage unlabeled data to improve a supervised baseline. For example, word clusters or word embeddings estimated from a large corpus are often used to help deal with sparsity. Because our LSTM models are reading bytes, it is not obvious how to insert information like a word cluster identity. Recent results with sequence-to-sequence autoencoding (Dai and Le, 2015) seem promising in this regard, but here we limit our experiments to use just annotated data.\nEach task specifies separate data for training, development, and testing. We used the development data for tuning the dropout and byte-dropout parameters (since these likely depend on the amount of available training data), but did not tune the remaining hyperparameters. In total, our training set for POS Tagging across 13 languages included 2.87 million tokens and our training set for NER across 4 languages included 0.88 million tokens. Recall, though, that our training examples are 60-byte segments obtained by sliding a window through the training data, shifting by 1 byte each time. This results in 25.3 million and 6.0 million training segments for the two tasks."}, {"heading": "6.1 Part-of-Speech Tagging", "text": "Our part-of-speech tagging experiments use Version 1.1 of the Universal Dependency data4, a collection of treebanks across many languages annotated with a universal tagset (Petrov et al., 2011). The most relevant recent results (Ling et al., 2015) use dif-\n4http://universaldependencies.github.io/docs/\nferent datasets, with different finer-grained tagsets in each language, so our results aren\u2019t immediately comparable. However, we provide baseline results (for each language separately) using a Conditional Random Field (Lafferty et al., 2001) with an extensive collection of features comparable to those used in the Stanford POS tagger (Manning, 2011). For our experiments, we chose the 13 languages that had at least 50k tokens of training data. We did not subsample the training data, though the amount of data varies widely across languages, but rather shuffled all training examples together. These languages represent a broad range of linguistic phenomena and character sets so it was not obvious at the outset that a single multilingual model would work.\nTable 1 compares the CRF with and without externally trained cluster features with BTS trained on all languages as well as each language separately. The single BTS model improves on average over the CRF models trained using the same data, though clearly there is some benefit in using external resources. Note that BTS is particularly strong in Finnish, surpassing even CRF+ by nearly 1.5% (absolute), probably because the byte representation generalizes better to agglutinative languages than word-based models, a finding validated by Ling et al. (2015). In addition, the baseline CRF models, including the (compressed) cluster tables, require about 50 MB per language, while BTS is under 10 MB. BTS trained on all the languages improves on average over BTS trained on each individual language, suggesting that it is learning some language independent representation."}, {"heading": "6.2 Named Entity Recognition", "text": "Our main motivation for showing POS tagging results was to demonstrate how effective a single BTS model can be across a wide range of languages. The NER task is a more interesting test case because, as discussed in the introduction, it usually relies on a pipeline of processing. We use the 2002 and 2003 ConLL shared task datasets5 for multilingual NER because they contain data in 4 languages (English, German, Spanish, and Dutch) with consistent annotations of named entities (PER, LOC, ORG, and MISC). In addition, the shared task competition\n5http://www.cnts.ua.ac.be/conll200{2,3}/ner\nproduced strong baseline numbers for comparison. However, most published results use extra information beyond the provided training data which makes fair comparison with our model more difficult.\nThe best competition results for English and German (Florian et al., 2003) used a large gazetteer and the output of two additional NER classifiers trained on richer datasets. Since 2003, better results have been reported using additional semi-supervised techniques (Ando and Zhang, 2005) and more recently, Passos et al. (2014) claimed the best English results (90.90% F1) using features derived from word-embeddings. The 1st place submission in 2002 (Carreras et al., 2002) comment that without extra resources for Spanish, their results drop by about 2% (absolute).\nPerhaps the most relevant comparison is the overall 2nd place submission in 2003 (Klein et al., 2003). They use only the provided data and report results with character-based models which provide a useful comparison point to our byte-based LSTM. The performance of a character HMM alone is much worse than their best result (83.2% vs 92.3% on the English development data), which includes a variety of word and POS-tag features that describe the context\n(as well as some post-processing rules). For English (assuming just ASCII strings), the character HMM uses the same inputs as BTS, but is hindered by some combination of the independence assumption and smaller capacity.\nCollobert et al.\u2019s (2011) convolutional model (discussed above) gives 81.47% F1 on the English test set when trained on only the gold data. However, by using carefully selected word-embeddings trained on external data, they are able to increase F1 to 88.67%. Huang et al. (2015) improve on Collobert\u2019s results by using a bidirectional LSTM with a CRF layer where the inputs are features describing the words in each sentence. Either by virtue of the more powerful model, or because of more expressive features, they report 84.26% F1 on the same test set and 90.10% when they add pretrained word embedding features.\nThere is relatively little work on multilingual NER, and most research is focused on building systems that are unsupervised in the sense that they use resources like Wikipedia and Freebase rather than manually annotated data. Nothman et al. (2013) use Wikipedia anchor links and disambiguation pages joined with Freebase types to create a huge amount of somewhat noisy training data and are able to achieve good results on many languages (with some extra heuristics). These results are also included in Table 2.\nWhile BTS does not improve on the state-ofthe-art in English, its performance is better than the best previous results that use only the provided training data. BTS improves significantly on the best known results in German, Spanish, and Dutch even though these leverage external data. In addition, the BTS models trained separately on each language are worse than the single model (with the same number of parameters as each single-language model) trained on all languages combined. This suggests that the model is learning some languageindependent representation of the task.\nOne interesting shortcoming of the BTS model is that it is not obvious how to tune it to increase recall. In a standard classifier framework, we could simply increase the prediction threshold to increase precision and decrease the prediction threshold to increase recall. However, because we only produce annotations for spans (non-spans are not annotated),\nwe can adjust a threshold on total span probability (the product of the start, length, and label probabilities) to increase precision, but there is no clear way to increase recall. The untuned model tends to prefer precision over recall already, so some heuristic for increasing recall might improve our overall F1 results."}, {"heading": "6.3 Dropout and Stacked LSTMs", "text": "There are many modeling options and hyperparameters that significantly impact the performance of Neural Networks. Here we show the results of a few experiments that were particularly relevant to the performance obtained above.\nFirst, Table 3 shows how dropout and bytedropout improve performance for both tasks. Without any kind of dropout, the training process starts to overfit (development data perplexity starts increasing) relatively quickly. For POS tagging, we set dropout and byte-dropout to 0.2, while for NER, we set both to 0.3. This significantly reduces the overfitting problem. Curiously, even after perplexity begins to increase on the development data, the metrics we care about, POS tagging accuracy and NER F1 continue to improve. This may be related\nto the fact that during training, we feed the gold label from the previous time step rather than the predicted label; average perplexity may not reflect how robust the model is to incorrect upstream predictions. Recent results with scheduled sampling (Bengio et al., 2015), feeding either the gold or predicted label during training according to some schedule, would likely improve our results."}, {"heading": "1 76.15 77.59", "text": ""}, {"heading": "2 79.40 79.73", "text": ""}, {"heading": "3 81.44 81.93", "text": ""}, {"heading": "4 82.13 82.18", "text": "Second, Table 4 shows how performance improves as we increase the size of the model in two ways: the number of units in the model\u2019s state (width) and the number of stacked LSTMs (depth). Increasing the width of the model improves performance less than increasing the depth, and once we use 4 stacked LSTMs, the added benefit of a much wider model has disappeared. This result suggests that rather than learning to partition the space of inputs according to the source language, the model is learning some lanugage-independent representation at the deeper levels."}, {"heading": "7 Conclusions", "text": "We have described a model that uses a sequence-tosequence LSTM framework that reads a segment of text one byte at a time and then produces span annotations over the inputs. This work makes a number of novel contributions:\n1. We use the bytes in variable length unicode encodings as inputs. This makes the model vocabulary very small and also allows us to train a multilingual model that improves over singlelanguage models without using additional parameters. We introduce byte-dropout, an analog to added noise in speech or blurring in images, which significantly improves generalization.\n2. The model produces span annotations, where each is a sequence of three outputs: a start position, a length, and a label. This decomposition keeps the output vocabulary small and marks a significant departure from the typical BeginInside-Outside (BIO) scheme used for labeling sequences.\n3. The models are much more compact than traditional word-based systems and they are standalone \u2013 no processing pipeline is needed."}, {"heading": "Acknowledgments", "text": "Many thanks to Fernando Pereira and Dan Ramage for their insights about this project from the outset. Thanks also to Cree Howard for creating Figure 1."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "End-to-end attentionbased large vocabulary speech recognition", "author": ["Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1508.04395", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A Smith"], "venue": "arXiv preprint arXiv:1508.00657", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks. arXiv preprint arXiv:1506.03099", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Named entity extraction using adaboost", "author": ["Llu\u0131\u0301s M\u00e0rques", "Llu\u0131\u0301s Padr\u00f3"], "venue": "In Proceedings of CoNLL-2002,", "citeRegEx": "Carreras et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2002}, {"title": "Listen, attend and spell", "author": ["Chan et al.2015] William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211", "citeRegEx": "Chan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "Variable-length word encodings for neural translation models", "author": ["Chitnis", "DeNero2015] Rohan Chitnis", "John DeNero"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chitnis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chitnis et al\\.", "year": 2015}, {"title": "Char align: a program for aligning parallel texts at the character level", "author": ["Kenneth Ward Church"], "venue": "In Proceedings of the 31st annual meeting on Association for Computational Linguistics,", "citeRegEx": "Church.,? \\Q1993\\E", "shortCiteRegEx": "Church.", "year": 1993}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Semi-supervised sequence learning. arXiv preprint arXiv:1511.01432", "author": ["Dai", "Le2015] Andrew M Dai", "Quoc V Le"], "venue": null, "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser", "author": ["Duong et al.2015] Long Duong", "Trevor Cohn", "Steven Bird", "Paul Cook"], "venue": "In 53rd Annual Meeting of the Association", "citeRegEx": "Duong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2015}, {"title": "The difficulty of training deep architectures and the effect of unsupervised pre-training", "author": ["Erhan et al.2009] Dumitru Erhan", "Pierre-Antoine Manzagol", "Yoshua Bengio", "Samy Bengio", "Pascal Vincent"], "venue": "In International Conference on artificial intelligence and", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "From speech to letters-using a novel neural network architecture for grapheme based asr", "author": ["Eyben et al.2009] Florian Eyben", "Martin W\u00f6llmer", "Bj\u00f6rn Schuller", "Alex Graves"], "venue": "In Automatic Speech Recognition & Understanding,", "citeRegEx": "Eyben et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Eyben et al\\.", "year": 2009}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Effective identification of source code authors using byte-level information", "author": ["Efstathios Stamatatos", "Stefanos Gritzalis", "Sokratis Katsikas"], "venue": "In Proceedings of the 28th international conference on Software engineer-", "citeRegEx": "Frantzeskou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Frantzeskou et al\\.", "year": 2006}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Graves", "Jaitly2014] Alex Graves", "Navdeep Jaitly"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Named entity recognition with character-level models", "author": ["Klein et al.2003] Dan Klein", "Joseph Smarr", "Huy Nguyen", "Christopher D Manning"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Part-ofspeech tagging from 97% to 100%: is it time for some linguistics", "author": ["Christopher D Manning"], "venue": "In Computational Linguistics and Intelligent Text Processing,", "citeRegEx": "Manning.,? \\Q2011\\E", "shortCiteRegEx": "Manning.", "year": 2011}, {"title": "Chinese and japanese word segmentation using word-level and character-level information", "author": ["Tetsuji Nakagawa"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Nakagawa.,? \\Q2004\\E", "shortCiteRegEx": "Nakagawa.", "year": 2004}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Nothman et al.2013] Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R Curran"], "venue": "Artificial Intelligence,", "citeRegEx": "Nothman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "arXiv preprint arXiv:1404.5367", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Language independent authorship attribution using character level language models", "author": ["Peng et al.2003] Fuchun Peng", "Dale Schuurmans", "Shaojun Wang", "Vlado Keselj"], "venue": "In Proceedings of the tenth conference on European chapter of the Association", "citeRegEx": "Peng et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2003}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2011] Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": "arXiv preprint arXiv:1104.2086", "citeRegEx": "Petrov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2011}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Text understanding from scratch. arXiv preprint arXiv:1502.01710", "author": ["Zhang", "LeCun2015] Xiang Zhang", "Yann LeCun"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "This is truly language annotation from scratch (see Collobert et al. (2011) and Zhang and LeCun (2015)).", "startOffset": 52, "endOffset": 76}, {"referenceID": 8, "context": "This is truly language annotation from scratch (see Collobert et al. (2011) and Zhang and LeCun (2015)).", "startOffset": 52, "endOffset": 103}, {"referenceID": 29, "context": "Second, sequence-to-sequence models (Sutskever et al., 2014), allow for flexible input/output dynamics.", "startOffset": 36, "endOffset": 60}, {"referenceID": 16, "context": "Section 2 discusses related work; Section 3 describes our model; Section 4 gives training details including a new variety of dropout (Hinton et al., 2012); Section 5 gives inference details; Section 6 presents", "startOffset": 133, "endOffset": 154}, {"referenceID": 20, "context": "Character-level inputs have been used with some success for tasks like NER (Klein et al., 2003), parallel text alignment (Church, 1993), and authorship attribution (Peng et al.", "startOffset": 75, "endOffset": 95}, {"referenceID": 7, "context": ", 2003), parallel text alignment (Church, 1993), and authorship attribution (Peng et al.", "startOffset": 33, "endOffset": 47}, {"referenceID": 27, "context": ", 2003), parallel text alignment (Church, 1993), and authorship attribution (Peng et al., 2003) as an effective way to deal with n-gram sparsity while still capturing some aspects of word choice and morphology.", "startOffset": 76, "endOffset": 95}, {"referenceID": 24, "context": "Such approaches often combine character and word features and have been especially useful for handling languages with large character sets (Nakagawa, 2004).", "startOffset": 139, "endOffset": 155}, {"referenceID": 14, "context": "uses byte n-grams to identify source code authorship (Frantzeskou et al., 2006) \u2013 but there is nothing, to the best of our knowledge, that exploits bytes as a cross-lingual representation of language.", "startOffset": 53, "endOffset": 79}, {"referenceID": 10, "context": "share some subset of the parameters across languages (Duong et al., 2015) seems to benefit the low-resource languages; however, we are sharing all the parameters among all languages.", "startOffset": 53, "endOffset": 73}, {"referenceID": 22, "context": "Recent work has shown that modeling the sequence of characters in each token with an LSTM can more effectively handle rare and unknown words than independent word embeddings (Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 174, "endOffset": 219}, {"referenceID": 2, "context": "Recent work has shown that modeling the sequence of characters in each token with an LSTM can more effectively handle rare and unknown words than independent word embeddings (Ling et al., 2015; Ballesteros et al., 2015).", "startOffset": 174, "endOffset": 219}, {"referenceID": 19, "context": "Similarly, language modeling, especially for morphologically complex languages, benefits from a Convolutional Neural Network (CNN) over characters to generate word embeddings (Kim et al., 2015).", "startOffset": 175, "endOffset": 193}, {"referenceID": 2, "context": ", 2015; Ballesteros et al., 2015). Similarly, language modeling, especially for morphologically complex languages, benefits from a Convolutional Neural Network (CNN) over characters to generate word embeddings (Kim et al., 2015). Rather than decompose words into characters, Rohan and Denero (2015) encode rare words with Huffman codes, allowing a neural translation model to learn something about word subcomponents.", "startOffset": 8, "endOffset": 299}, {"referenceID": 8, "context": "Our work is philosophically similar to Collobert et al.\u2019s (2011) experiments with \u201calmost from scratch\u201d language processing.", "startOffset": 39, "endOffset": 65}, {"referenceID": 5, "context": "rectly from acoustic frame sequences (Chan et al., 2015; Bahdanau et al., 2015).", "startOffset": 37, "endOffset": 79}, {"referenceID": 1, "context": "rectly from acoustic frame sequences (Chan et al., 2015; Bahdanau et al., 2015).", "startOffset": 37, "endOffset": 79}, {"referenceID": 12, "context": "This line of work \u2013 discarding intermediate representations in speech \u2013 was pioneered by Graves and Jaitly (2014) and earlier, by Eyben et al. (2009).", "startOffset": 130, "endOffset": 150}, {"referenceID": 29, "context": "Our model is based on the sequence-to-sequence model used for machine translation (Sutskever et al., 2014), an adaptation of an LSTM that encodes a variable length input as a fixed-length vector, then decodes it into a variable number of outputs.", "startOffset": 82, "endOffset": 106}, {"referenceID": 11, "context": "very deep network is hard because it becomes increasingly difficult to estimate the contribution of each layer to the gradient, and further, RNNs have trouble generalizing to different length inputs (Erhan et al., 2009).", "startOffset": 199, "endOffset": 219}, {"referenceID": 29, "context": "parameter choices, including random initialization, learning rate decay, and gradient clipping, we follow Sutskever et al. (2014). Each model is trained on a single CPU over a period of a few days, at which point, development set results have stabilized.", "startOffset": 106, "endOffset": 130}, {"referenceID": 16, "context": "Neural Network models are often trained using dropout (Hinton et al., 2012), which tends to improve generalization by limiting correlations among hidden units.", "startOffset": 54, "endOffset": 75}, {"referenceID": 30, "context": "During training, dropout randomly zeroes some fraction of the elements in the embedding layer and the model state just before the softmax layer (Zaremba et al., 2014).", "startOffset": 144, "endOffset": 166}, {"referenceID": 28, "context": "1 of the Universal Dependency data4, a collection of treebanks across many languages annotated with a universal tagset (Petrov et al., 2011).", "startOffset": 119, "endOffset": 140}, {"referenceID": 22, "context": "The most relevant recent results (Ling et al., 2015) use dif-", "startOffset": 33, "endOffset": 52}, {"referenceID": 21, "context": "However, we provide baseline results (for each language separately) using a Conditional Random Field (Lafferty et al., 2001) with an extensive collection of features comparable to those used in the Stanford POS tagger (Manning, 2011).", "startOffset": 101, "endOffset": 124}, {"referenceID": 23, "context": ", 2001) with an extensive collection of features comparable to those used in the Stanford POS tagger (Manning, 2011).", "startOffset": 101, "endOffset": 116}, {"referenceID": 13, "context": "The best competition results for English and German (Florian et al., 2003) used a large gazetteer and the output of two additional NER classifiers trained on richer datasets.", "startOffset": 52, "endOffset": 74}, {"referenceID": 4, "context": "The 1st place submission in 2002 (Carreras et al., 2002) comment that without extra resources for Spanish, their results drop by about 2% (absolute).", "startOffset": 33, "endOffset": 56}, {"referenceID": 25, "context": "techniques (Ando and Zhang, 2005) and more recently, Passos et al. (2014) claimed the best English results (90.", "startOffset": 53, "endOffset": 74}, {"referenceID": 20, "context": "all 2nd place submission in 2003 (Klein et al., 2003).", "startOffset": 33, "endOffset": 53}, {"referenceID": 25, "context": "Nothman et al. (2013) use", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Recent results with scheduled sampling (Bengio et al., 2015), feeding either the gold or predicted label during training according to some schedule, would likely improve our results.", "startOffset": 39, "endOffset": 60}], "year": 2017, "abstractText": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of-the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning \u201cfrom scratch\u201d in that they do not rely on any elements of the standard pipeline in Natural Language Processing.", "creator": "LaTeX with hyperref package"}}}