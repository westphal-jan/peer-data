{"id": "1512.01370", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2015", "title": "Locally Adaptive Translation for Knowledge Graph Embedding", "abstract": "initial knowledge graph embedding essentially aims primarily to represent entities and relations in a large - variable scale knowledge graph as graph elements implemented in a virtual continuous vector processing space. existing methods, e. not g., transe nl and r transh, therefore learn embedding representation by defining a global margin - region based maximum loss transport function compliant over changing the data. however, eventually the predicted optimal global loss loss function specification is theoretically determined substantially during experiments whose parameters accurately are examined among a standard closed priority set of candidates. moreover, embeddings found over two knowledge graphs with different distinct entities boundaries and relations share uniquely the typical same set selection of generalized candidate loss transfer functions, ignoring the locality of both key graphs. fundamentally this leads purely to the limited performance of embedding related applications. in this paper, informally we propose a locally adaptive source translation method for knowledge graph semantic embedding, called loop transa, to entirely find the optimal loss function by actually adaptively determining its margin over different knowledge vertex graphs. performance experiments modelling on two benchmark graph data yield sets both demonstrate likely the superiority behaviour of the naive proposed method, as compared to the - computational state - complexity of - systems the -, art ones.", "histories": [["v1", "Fri, 4 Dec 2015 11:09:55 GMT  (73kb,D)", "http://arxiv.org/abs/1512.01370v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["yantao jia", "yuanzhuo wang", "hailun lin", "xiaolong jin", "xueqi cheng"], "accepted": true, "id": "1512.01370"}, "pdf": {"name": "1512.01370.pdf", "metadata": {"source": "CRF", "title": "Locally Adaptive Translation for Knowledge Graph Embedding", "authors": ["Yantao Jia", "Yuanzhuo Wang", "Hailun Lin", "Xiaolong Jin", "Xueqi Cheng"], "emails": [], "sections": [{"heading": null, "text": "Keywords:locally adaptive translation, knowledge graph embedding, optimal margin"}, {"heading": "Introduction", "text": "A knowledge graph is actually a graph with entities of different types as nodes and various relations among them as edges. Typical examples include Freebase (Bollacker et al. 2008), WordNet (Miller 1995), OpenKN (Jia et al. 2014), to name a few. In the past decade, the great success of constructing these large-scale knowledge graphs have advanced many realistic applications, such as, link prediction (Liu et al. 2014) and document understanding (Wu et al. 2012). However, since knowledge graphs usually contain millions of vertices and edges, any inference and computation over them may not be easy. For example, when using Freebase for link prediction, we need to deal with 68 million of vertices and one billion of edges. In addition, knowledge graphs usually adopt symbolic and logical representation only. This is not enough when handling applications with intensive numerical computation.\nRecently, many research work have been conducted to embed a knowledge graph into a continuous vector space,\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\ncalled knowledge graph embedding, to tackle these problems. For example, TransE (Bordes et al. 2013) represents entities as points and relations as translation from head entities to tail entities in the vector space. TransH (Wang et al. 2014) models relations as translation on a hyperplane and represents a relation as two vectors, i.e., the norm vector of the hyperplane and the translation vector on the hyperplane. These methods finally learn the representations of entities and relations by minimizing a global margin-based loss function.\nOn one hand, existing embedding methods over one knowledge graph determine the optimal form of loss function during experiments. A critical problem is that the determination is made over a limited number of candidates. For example, the optimal loss function used in TransE (Bordes et al. 2013) on Freebase is determined with its margin among the 3-element set {1, 2, 10}. It is unclear that why the loss function is examined by only testing a closed set of values of its parameters in the literature. Obviously, the margin is nonnegative and the full grid search of finding its optimum is almost impossible. On the other hand, existing embedding methods over two different knowledge graphs find their individual optimal loss functions over the same set of candidates. For instance, in TransH (Wang et al. 2014), the loss functions on Freebase and WordNet share the same candidate margins, i.e., {0.25, 0.5, 1, 2}. Since different knowledge graphs contain different entities and relations, this compatible setting ignores the individual locality of knowledge graphs and seems not convincing in theory.\nIn this paper, we propose a translation based embedding method, called TransA, to address the above two issues. For different knowledge graphs, TransA adaptively finds the optimal loss function according to the structure of knowledge graphs, and no closed set of candidates is needed in advance. It not only makes the translation based embedding more tractable in practice, but promotes the performance of embedding related applications, such as link prediction and triple classification. Specifically, the contributions of the paper are two-fold.\n\u2022 We experimentally prove that knowledge graphs with different localities may correspond to different optimal loss functions, which differ in the setting of margins. Then we study how margin affects the performance of embedding by deducing a relation between them.\nar X\niv :1\n51 2.\n01 37\n0v 1\n[ cs\n.A I]\n4 D\nec 2\n01 5\n\u2022 We further propose the locally adaptive translation method (TransA) for knowledge graph embedding. It finds the optimal loss function by adaptively determining the margins over different knowledge graphs. Finally, experiments on two standard benchmarks validate the effectiveness and efficiency of TransA."}, {"heading": "Related Work", "text": "Existing knowledge embedding methods aim to represent entities and relations of knowledge graphs as vectors in a continuous vector space, where they usually define a loss function to evaluate the representations. Different methods differ in the definition of loss functions with respect to the triple (h, r, t) in knowledge graph, where h and t denote the head and tail entities, and r represents their relationship. The loss function implies some type of transformation on h and t. Among translation based methods, TransE (Bordes et al. 2013) assumes h + r = t when (h, r, t) is a golden triple, which indicated that t should be the nearest neighbor of h + r. Unstructured method (Bordes et al. 2012; Bordes et al. 2014) is a naive version of TransE by setting r = 0. To deal with relations with different mapping properties, TransH (Wang et al. 2014) was established to project entities into a relation-specific hyperplane and the relation becomes a translating operation on hyperplane. Another direction of embedding is the energy based method, which assigns low energies to plausible triples of a knowledge graph and employs neural network for learning. For example, Structured Embedding (SE) (Bordes et al. 2011) defines two relation-specific matrices for head entity and tail entity, and establishes the embedding by a neural network architecture. Single Layer Model (SLM) is a naive baseline of NTN (Socher et al. 2013) by concatenating h and t as an input layer to a non-linear hidden layer. Other energy based methods include Semantic Matching Energy (SME) (Bordes et al. 2012), Latent Factor Model (LFM) (Jenatton et al. 2012; Sutskever, Tenenbaum, and Salakhutdinov 2009) and Neural Tensor Network (NTN) (Socher et al. 2013). Besides, matrix factorization based methods were also presented in recent studies, such as RESCAL (Nickel, Tresp, and Kriegel 2012). However, these methods find the optimal loss function of embedding whose parameters are selected during experiments. We intend to propose an adaptive translation method to find the optimal loss function in this paper."}, {"heading": "Loss Function Analysis", "text": "In this section, we firstly study the loss functions over different knowledge graphs and find their difference in the setting of margins. Then in order to figure out how margin affects the performance of embedding, we derive a relation by virtue of the notion of stability for learning algorithms (Bousquet and Elisseeff 2002).\nMargin setting over different knowledge graphs As mentioned before, a knowledge graph is composed of heterogeneous entities and relations. Knowledge graphs with different types of entities and relations are different and\nexhibit different locality with regard to the types of their elements. In this sense, existing knowledge embedding methods using a common margin-based loss function cannot satisfactorily represent the locality of knowledge graphs.\nTo verify this, we construct knowledge graphs with different locality. We simply partition a knowledge graph into different subgraphs in a uniform manner. Each subgraph contains different types of relations and their corresponding entities. Moreover, different subgraphs have the identical number of relations for the sake of balance of the number of entities. We claim that over different subgraphs, the optimal margin-based loss function may be different in terms of the margin. To validate this point, we perform the embedding method TransE (Bordes et al. 2013) on the data set FB15K from the knowledge graph, Freebase. And we partition FB15K into five subsets with equal size of relations. For example, one subset, named Subset1, contains 13,666 entities and 269 relations. Another subset, named Subset2, has 13603 entities and 269 relations. We conduct link prediction task over these five subsets and use mean rank (i.e., mean rank of correct entities) to evaluate the results shown in Table 1.\nIt follows from Table 1 that the settings of loss functions are different over the two data sets, Subset1 and Subset2. More precisely, they take different values of margin as 3 and 2, respectively. Meanwhile, for the whole data set FB15K, it has been shown in (Bordes et al. 2013) that the best performance is achieved when the optimal loss function takes the margin 1. This suggests that knowledge embedding of different knowledge graphs with a global setting of loss function can not well represent the locality of knowledge graphs, and it is indispensable to propose a locality sensitive loss function with different margins."}, {"heading": "How margin affects the performance", "text": "So far we have experimentally found that the performance of a knowledge embedding method is relevant to the setting of margins in the margin-based loss function. This raises a theoretical question that how the setting of margin affects the performance when the margin increases. To answer this question, we present a relation between the error of performance and the margin.\nBefore obtaining the relation, let us firstly give some notations. Denote the embedding method by A. Since the embedding method is to learn the appropriate representations of entities and relations in the knowledge graph, it can be viewed as a learning algorithm. Suppose that for the embed-\nding method and the knowledge graph, the training data set S = {(h1, r1, t1) , . . . , (hi, ri, ti) , . . . , (hn, rn, tn)}\nof size n is a set of triplets in the knowledge graph and the range of (hi, ri, ti) is denoted by Z . Denote Si the set obtained from S by replacing the i-th sample with a new pair of head and tail entities drawn in the knowledge graph. Namely, Si = {S\\ (hi, ri, ti) \u222a (h\u2032i, ri, t\u2032i)}, where (h\u2032i, ri, t\u2032i) is obtained by substituting (h\u2032i, t \u2032 i) for (hi, ti).\nSince A can be regarded as a learning algorithm, we define R(A, S) as its true risk or generalization error, which is a random variable on the training set S and defined as\nR(A, S) = Ez[L(A, z)]. Here Ez[\u00b7] denotes the expectation when z = (h, r, t) is a triple in knowledge graph and L(A, z) is the loss function of the learning algorithm A with respect z. It is well known that the true risk R(A, S) cannot be computed directly and is usually estimated by the empirical risk defined as\nRemp(A, S) = 1\nn n\u2211 k=1 L(A, zk),\nwhere zk = (hk, rk, tk) is the k-th element of S. After defining the two risks, the performance of learning algorithm A can be evaluated by the difference between R(A, S) and Remp(A, S), and we need to find the relation between the difference and the margin. To this end, we define the Uniform-Replace-One stability motivated by (Bousquet and Elisseeff 2002). Definition 1 The learning algorithm A has UniformReplace-One stability \u03b2 with respect to the loss function L if for all S \u2208 Zn and i \u2208 {1, 2, . . . , n}, the following inequality holds \u2016L(AS , \u00b7)\u2212 L(ASi , \u00b7)\u2016\u221e \u2264 \u03b2. Here AS means that the learning algorithm A is trained on the data set S, \u2016L(AS , \u00b7)\u2016\u221e is the maximum norm, and is equal to maxzk{L(AS , zk)} for k = 1, 2, . . . , n. The loss function L of existing embedding methods takes the form L(AS , z) = fr(h, t) +M \u2212 fr(h\u2032, t\u2032), (1) where (h, r, t) , (h\u2032, r, t\u2032) \u2208 S, fr(h, t) is a nonnegative score function.M is the margin separating positive and negative triples and is usually set to be a global constant in a knowledge graph, e.g. M = 1 in FB15K (Bordes et al. 2011). Moreover,L(AS , z) is defined as a nonnegative function, namely, if fr(h, t) + M \u2212 fr(h\u2032, t\u2032) < 0, then we set L(AS , z) = 0. According to the loss function of embedding method A, we have the following lemma. Lemma 1 The Uniform-Replace-One stability \u03b2 of the embedding methods with respect to the given loss function L(AS , z) is equal to 2f\u0302r, where f\u0302r = maxh,t fr(h, t) is the maximum over the triples (h, r, t) \u2208 S. Proof. By the definition of \u03b2 and the loss function defined in Equation (1), we deduce \u2016L(AS , \u00b7)\u2212 L(ASi , \u00b7)\u2016\u221e =|fr(h, t) +M \u2212 fr(h\u2032, t\u2032)\u2212 fr(h, t)\u2212M + fr(h\u2032\u2032, t\u2032\u2032)| =|fr(h\u2032\u2032, t\u2032\u2032)\u2212 fr(h\u2032, t\u2032)| \u2264 |fr(h\u2032\u2032, t\u2032\u2032)|+ |fr(h\u2032, t\u2032)| \u22642 max\nh,t fr(h, t).\nSetting f\u0302r = maxh,t fr(h, t) completes the proof. Now it is ready to reveal the relationship between the margin and the difference between the two risks of performance.\nTheorem 1 For the embedding method A with UniformReplace-One stability \u03b2 with respect to the given loss function L, we have the following inequality with probability at least 1\u2212 \u03b4,\nR(A, S) \u2264 Remp(A, S) +\n\u221a (M + f\u0302r)2\n2n\u03b4 +\n6f\u0302r(M + f\u0302r)\n\u03b4 ,\n(2) where f\u0302r = max\nh,t fr(h, t) is defined in Lemma 1.\nBefore proving Theorem 1, we first present the following Lemma verified in (Bousquet and Elisseeff 2002).\nLemma 2 For any algorithm A and loss function L(AS , z) such that 0 \u2264 L(AS , z) \u2264 L\u0302, set zi = (hi, ri, ti) \u2208 S, we have for any different i, j \u2208 {1, 2, . . . , n} that\nES [ (R(A, S)\u2212Remp(A, S))2 ] \u2264 L\u0302 2\n2n + 3L\u0302ES\u222az\u2032i [|L(AS , zi)\u2212 L(ASi , zi)|]\nProof of Theorem 1: First, for the given loss function L(AS , z), we deduce that\nL(AS , z) \u2264M + fr(h, t) \u2264M + f\u0302r.\nThen from the definition of Uniform-Replace-One stability, we find that ES\u222az\u2032i [|L(AS , zi) \u2212 L(ASi , zi)|] \u2264 \u03b2. Hence, by Lemma 2 and Lemma 1, we obtain that\nES [ (R(A, S)\u2212Remp(A, S))2 ] \u2264 (M + f\u0302r) 2\n2n + 6(M + f\u0302r)f\u0302r\nBy Chebyshev\u2019s inequality, we derive that\nProb((R(A, S)\u2212Remp(A, S)) \u2265 )\n\u2264 ES\n[ (R(A, S)\u2212Remp(A, S))2 ] 2\n\u2264\n( (M + f\u0302r) 2\n2n + 6(M + f\u0302r)f\u0302r\n) .\nLet the right hand side of the above inequality be \u03b4, then we have with probability at least 1\u2212 \u03b4 that\nR(A, S) \u2264 Remp(A, S) +\n\u221a (M + f\u0302r)2\n2n\u03b4 +\n6f\u0302r(M + f\u0302r)\n\u03b4 .\nThis completes the proof. From Theorem 1, it can be seen that a large margin would lead to over-fitting during the learning process. This is reasonable since a large margin means more incorrect triples (h\u2032, r, t\u2032) involve in the computation of the loss function\n(1) such that fr(h, t) + M \u2212 fr(h\u2032, t\u2032) \u2265 0. However, setting M as small as possible is also inadvisable since it is a strict constraint that excludes incorrect triples whose values fr(h\n\u2032, t\u2032) are slightly larger than fr(h, t). Therefore, it demands a strategy to choose moderately small values of M in practice so as to make the error of performance as small as possible."}, {"heading": "Locally Adaptive Translation Method", "text": "In the previous section, to obtain better performance of embedding, it has proved that it is necessary to find an appropriate loss function in terms of margin over different knowledge graphs. In this section, we propose a locally adaptive translation method, called TransA, to adaptively choose the optimal margin.\nBecause the classical knowledge graph is fully made up of two disjoint sets, i.e., the entity set and relation set, it makes sense that the optimal margin, denoted by Mopt, is composed of two parts, namely, entity-specific marginMent, and relation-specific margin Mrel. Furthermore, it is natural to linearly combine the two specific margins via a parameter \u00b5 which controls the trade-off between them. Therefore, the optimal margin of embedding satisfies\nMopt = \u00b5Ment + (1\u2212 \u00b5)Mrel, (3)\nwhere 0 \u2264 \u00b5 \u2264 1. To demonstrate that the margin Mopt is optimal, it is sufficient to find the optimal entity-specific margin and the optimal relation-specific margin, and we will elaborate this in the following."}, {"heading": "Entity-specific margin", "text": "To define the optimal entity-specific margin Ment, it has been verified by (Fan et al. 2015) that for a specific head entity h (or tail entity t), the best performance is achieved when the embedding of entities brings the positive tail entities (or head entities) close to each other, and moves the negative ones with a margin. The positive entities have the same relation with h (or t), and the negative entities have different relations with h (or t). In this sense, the optimal margin Ment is actually equal to the distance between two concentric spheres in the vector space, illustrated in Figure 1. The positive entities (illustrated as \u201c\u00a9\u201d) are constrained within the internal sphere, while the negative entities (illustrated as \u201c \u201d) lie outside the external sphere. The interpretation of Ment is motivated by the work of metric learning, such as (Weinberger and Saul 2009; Do et al. 2012). Notice that the above analysis applies to both the head entity h and the tail entity t, thus we simply use entity h to stand for both cases in the rest of the paper.\nMore formally, for a specific entity h and one related relation r, the sets of positive and negative entities with respect to r are defined as Pr = {t|(h, r, t) \u2208 \u2206} and Nr = {t|(h, r, t) 6\u2208 \u2206, (h, r\u2032, t) \u2208 \u2206,\u2203r\u2032 \u2208 R}, where \u2206 is the set of correct triples, R is the set of relations in the knowledge graph. In other words, the set of negative entities Nr contains those which have relations with h of type other than r. Remark that due to the multi-relational property of the knowledge graph, it may be true that Pr \u2286 Nr. Then\nthe optimal margin Ment is equal to the distance between the external sphere and the internal sphere, which push the elements in Nr away from h and keep the elements in Pr close to each other. Let nrh be the number of relations of the entity h, i.e., the number of different types of relations with h as one end. Set Rh be the set of relations related to h. More formally, we define Ment as follows. Definition 2 (Entity-specific margin) For a given entity h, for all t \u2208 Pr and t\u2032 \u2208 Nr,\nMent =\n\u2211 r\u2208Rh min t,t\u2032 \u03c3(\u2016h\u2212 t\u2032\u2016 \u2212 \u2016h\u2212 t\u2016)\nnrh ,\nwhere\n\u03c3(x) = { x when x \u2265 0; \u2212x otherwise.\nreturns the absolute value of x. It can be seen from Definition 2 that for each relation r, the value mint,t\u2032 \u03c3(\u2016h \u2212 t\u2032\u2016 \u2212 \u2016h \u2212 t\u2016) obtains the minimum when it takes the nearest negative entity and the farthest positive entity with respect to h. In Figure 1, the nearest negative entity is marked as the black rectangle, and the farthest positive entity is marked as the red circle. This definition is kind of similar to the margin defined in Support Vector Machine (Vapnik 2013; Boser, Guyon, and Vapnik 1992), where the margin of two classes with respect to the classification hyperplane is equal to the minimum absolute difference of the distances of any two different-class instances projected to the norm vector of the hyperplane. In particular, when Nr = \u2205, we set Ment = 0, which is reasonable since all positive entities are within the internal sphere. Remark that the reason we consider the term nrh as a denominator is to discriminate relations with different mapping properties, i.e., 1-to-1, 1-to-N, N-to-1, N-to-N. Besides, in order to apply the optimal margin Ment to large data set, we similarly adopt the active set method proposed in (Weinberger and Saul 2008) to speed up the calculation. We check a very small fraction of negative entities typically lying nearby the farthest positive entity for several rounds. Averaging the margins obtained in each round leads to the final entity-specific margin."}, {"heading": "Relation-specific margin", "text": "The optimal relation-specific margin Mrel from the relation aspect is found by considering the proximity of re-\nlations concerning a given entity. For a specific entity h and one of its related relation r, if Nr 6= \u2205, set Rh,r = {r1, r2, . . . , rnhr\u22121} be the set of relations the entity h has except r. Different relations in Rh,r have different degrees of similarity with the relation r. To measure this similarity, we consider the length of relation-specific embedding vectors. We classify the relations in Rh,r into two parts according to whether its length is larger than \u2016r\u2016. For relations ri, rj \u2208 Rh,r with lengths larger than \u2016r\u2016, we assume that ri is more similar with r than rj if \u2016ri\u2016\u2212\u2016r\u2016 \u2264 \u2016rj\u2016\u2212\u2016r\u2016. Then similar to the analysis of entity-specific margin, the optimal relation-specific margin is equal to the distance between two concentric spheres in the vector space. The internal sphere constraints relation r and those with length smaller than \u2016r\u2016, while the relations with length greater than \u2016r\u2016 lie outside the external sphere. More formally, we define the optimal relation-specific margin as follows. Definition 3 (Relation-specific margin) For a given entity h and one of its related relations r, if \u2203 ri \u2208 Rh,r such that \u2016ri\u2016 \u2265 \u2016r\u2016, then\nMrel = min ri\u2208Rh,r\n(\u2016ri\u2016 \u2212 \u2016r\u2016). (4)\nIt follows from the definition ofMrel that for ri \u2208 Rh,r with \u2016ri\u2016 \u2265 \u2016r\u2016, we have \u2016ri\u2016 \u2212 \u2016r\u2016 \u2265 Mrel. In other words, \u2016ri\u2016 \u2265 Mrel + \u2016r\u2016 holds for those ri \u2208 Rh,r. This means that Mrel is determined by the most similar relation(s) with smallest difference of length, and it pushes other dissimilar relations whose lengths are much larger than \u2016r\u2016 away from r. In this sense, Mrel is optimal. In Figure 2, we illustrate the calculation of Mrel with Mrel = \u2016r1\u2016 \u2212 \u2016r\u2016. It can be seen that any of the other relations ri (i = 2, 3, 4, 5) has length larger than r1. In particular, if Rh,r = \u2205 or no ri \u2208 Rh,r exists such that \u2016ri\u2016 \u2265 \u2016r\u2016, then we setMrel = 0 by convention. This makes sense since relations with length smaller than \u2016r\u2016 are inherently constrained within the internal sphere of radius \u2016r\u2016.\nGiven a triple (h, r, t), we define the margin-varying objective function as follows.\u2211 (h,r,t)\u2208\u2206 \u2211 (h\u2032,r,t\u2032)\u2208\u2206\u2032 max(0, fr(h, t) +Mopt \u2212 fr(h\u2032, t\u2032)),\nwhere max(x, y) returns the maximum between x and y, Mopt = \u00b5Ment + (1 \u2212 \u00b5)Mrel is the optimal margin defined by Equation (3), \u2206 is the set of correct triples and \u2206\u2032\nis the set of incorrect triples. Following the construction of incorrect triples (Bordes et al. 2013), we replace the head or tail entities in the correct triple (h, r, t) \u2208 \u2206 to obtain (h\u2032, r, t\u2032) \u2208 \u2206\u2032.\nIn fact, the optimal margin defined in Equation (3) from the entity and relation aspects provides a way to characterize the locality of knowledge graph in that it considers the structure information in the graph. For one thing, the entityspecific margin models the local distance between negative and positive entities. For another, the relation-specific margin quantifies the proximity of relations. Moreover, when the entities and relations change, the optimal margin varies accordingly, without fixing its value among some predefined candidates. This is way the method is called locally adaptive.\nThe learning process of TransA employs the widely used stochastic gradient descent (SGD) method. To combat overfitting, we follow the initialization of entity and relation embeddings as in (Bordes et al. 2013). In this paper, without loss of generality, we assume that given a triple (h, r, t), the score function fr(h, t) takes the form as\nfr(h, t) = \u2016h+ r \u2212 t\u2016, (5)\nwhere \u2016 \u00b7 \u2016 represents the L1-norm or L2-norm of the vector h + r \u2212 t, and h, r, t \u2208 Rd, d is the dimension of the embedded vector space. Remark that although we suppose that entity and relation embedding are in the same vector space Rd, it is not difficult to extend the method to that in different vector spaces similar to the work (Lin et al. 2015)."}, {"heading": "Experiments", "text": "We will conduct experiments on two tasks: link prediction (Bordes et al. 2013) and triple classification (Wang et al. 2014). The data sets we use are publicly available from two widely used knowledge graphs, WordNet (Miller 1995) and Freebase (Bollacker et al. 2008). For the data sets from WordNet, we employ WN18 used in (Bordes et al. 2014) and WN11 used in (Socher et al. 2013). For the data sets of Freebase, we employ FB15K used also in (Bordes et al. 2014) and FB13 used in (Socher et al. 2013). The statistics of these data sets are listed in Table 2."}, {"heading": "Link prediction", "text": "Link prediction aims to predict the missing entities h or t for a triple (h, r, t). Namely, it predicts t given (h, r) or predict h given (r, t). Similar to the setting in (Bordes et al. 2011; Bordes et al. 2013; Zhao, Jia, and Wang 2014), the task returns a list of candidate entities from the knowledge graph instead of one best answer, and we conduct the link prediction task on the two data sets WN18 and FB15K. Following the procedure used in (Bordes et al. 2013), we also adopt the evaluation measure, namely, mean rank (i.e., mean rank of\ncorrect entities). It is clear that a good predictor has lower mean rank. In the test stage, for each test triple (h, r, t), we replace the head or tail entity by all entities in the knowledge graph, and rank the entities in the decreasing order with respect to the scores calculated by fr. To distinguish the corrupted triples which are also correct ones, we also filter out the corrupted triples before the ranking of candidate entities. This operation is denoted as \u201cfilter\u201d and \u201craw\u201d otherwise.\nThe baseline methods include classical embedding methods, such as TransE (Bordes et al. 2013), TransH (Wang et al. 2014), and others shown in Table 3. Since the data sets we used are the same as our baselines, we compare our results with them reported in (Wang et al. 2014). The learning rate \u03bb during the SGD process is selected among {0.1, 0.01, 0.001}, the embedding dimension d in {20, 50, 100}, the batch size B among {20, 120, 480, 1440, 4800}, and the parameter \u00b5 in Equation (3) in [0, 1]. Notice that the optimal margin for TransA is not predefined but computed by Equation (3). All parameters are determined on the validation set. The optimal settings are: \u03bb = 0.001, d = 100, B = 1440, \u00b5 = 0.5 and taking L1 as dissimilarity on WN18; \u03bb = 0.001, d = 50, B = 4800, \u00b5 = 0.5 and taking L1 as dissimilarity on FB15K.\nExperiment results are shown in Table 3. It can be seen that on both data sets, TransA obtains the lowest mean rank. Furthermore, on WN18, among the baselines, Unstructured and TransH(unif) perform the best, but TransA decreases the mean rank by about 150 compared with both of them. On FB15K, among the baselines, TransH(unif) is the best baseline. TransA decreases its mean rank by 30 \u223c 50. Notice that the decreases on WN18 and FB15K are different, because the number of relations in WN18 is quite small and the relation-specific margin is very small too. In this case, the optimal margin is almost equal to the entity-specific margin. While on FB15K, the number of relations is 1345, and the optimal margin is the combination of the entity-specific margin and the relation-specific margin."}, {"heading": "Triple classification", "text": "Triple classification, studied in (Socher et al. 2013; Wang et al. 2014), is to confirm whether a triple (h, r, t) is correct or not, namely, a binary classification problem on the triple. The data sets we use in this task are WN11 and FB13 used in (Socher et al. 2013) and FB15K used in (Wang et\nal. 2014). Following the evaluation in NTN (Socher et al. 2013), the evaluation needs negative labels. The data sets WN11 and FB13 already have negative triples, which are obtained by corrupting golden ones. For FB15K, we follow the same way to construct negative triples as (Socher et al. 2013). The classification is evaluated as follows. For a triple (h, r, t), if the dissimilarity score obtained by fr is less than a relation-specific threshold, then the triple is classified to be positive, and negative otherwise. The threshold is determined by maximizing accuracy on the validation data set.\nThe baseline methods include classical embedding methods, such as TransE (Bordes et al. 2013), TransH (Wang et al. 2014), and others shown in Table 4. The learning rate \u03bb during the stochastic gradient descent process is selected among {0.1, 0.01, 0.001}, the embedding dimension d in {20, 50, 100, 200, 220, 300}, the parameter \u00b5 in Equation (3) in [0, 1], and batch size B among {20, 120, 480, 1440, 4800}. The optimal margin for TransA is not predefined but computed according to Equation (3). All parameters are determined on the validation set. The optimal setting are: \u03bb = 0.001, d = 220, B = 120, \u00b5 = 0.5 and takingL1 as dissimilarity on WN11; \u03bb = 0.001, d = 50, B = 480, \u00b5 = 0.5 and taking L1 as dissimilarity on FB13.\nExperiment results are shown in Table 4. On WN11, TransA outperforms the other methods. On FB13, the method NTN is shown more powerful. This is consistent with the results in previous literature (Wang et al. 2014). On FB15K, TransA also performs the best. Since FB13 is much denser to FB15K, NTN is more expressive on dense graph. On sparse graph, TransA is superior to other state-of-the-art embedding methods."}, {"heading": "Conclusion", "text": "In this paper, we tackled the knowledge embedding problem and proposed a locally adaptive translation method, called TransA, to adaptively learn the representation of entities and relations in a knowledge graph. We firstly presented the necessity of choosing an appropriate margin in the marginbased loss function. Then we defined the optimal margin from the entity and relation aspects, and integrated the margin into the commonly used loss function for knowledge embedding. Experimental results validate the effectiveness of the proposed method."}, {"heading": "Acknowledgement", "text": "This work is supported by National Grand Fundamental Research 973 Program of China (No. 2012CB316303,\n2014CB340401), National Natural Science Foundation of China (No. 61402442, 61572469, 61572473, 61303244), Beijing Nova Program (No. Z121101002512063), and Beijing Natural Science Foundation (No. 4154086)."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Bollacker,? \\Q2008\\E", "shortCiteRegEx": "Bollacker", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes"], "venue": "In Conference on Artificial Intelligence,", "citeRegEx": "Bordes,? \\Q2011\\E", "shortCiteRegEx": "Bordes", "year": 2011}, {"title": "Joint learning of words and meaning representations for open-text semantic parsing", "author": ["Bordes"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Bordes,? \\Q2012\\E", "shortCiteRegEx": "Bordes", "year": 2012}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes,? \\Q2013\\E", "shortCiteRegEx": "Bordes", "year": 2013}, {"title": "A semantic matching energy function for learning with multi-relational data. Machine Learning 94(2):233\u2013259", "author": ["Bordes"], "venue": null, "citeRegEx": "Bordes,? \\Q2014\\E", "shortCiteRegEx": "Bordes", "year": 2014}, {"title": "V", "author": ["B.E. Boser", "I.M. Guyon", "Vapnik"], "venue": "N.", "citeRegEx": "Boser. Guyon. and Vapnik 1992", "shortCiteRegEx": null, "year": 1992}, {"title": "and Elisseeff", "author": ["O. Bousquet"], "venue": "A.", "citeRegEx": "Bousquet and Elisseeff 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "A metric learning perspective of svm: on the relation of lmnn and svm", "author": ["Do"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Do,? \\Q2012\\E", "shortCiteRegEx": "Do", "year": 2012}, {"title": "T", "author": ["M. Fan", "Q. Zhou", "Zheng"], "venue": "F.; and Grishman, R.", "citeRegEx": "Fan et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "G", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "Obozinski"], "venue": "R.", "citeRegEx": "Jenatton et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Openkn: An open knowledge computational engine for network big data", "author": ["Jia"], "venue": "In Advances in Social Networks Analysis and Mining (ASONAM),", "citeRegEx": "Jia,? \\Q2014\\E", "shortCiteRegEx": "Jia", "year": 2014}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin"], "venue": "Proceedings of AAAI", "citeRegEx": "Lin,? \\Q2015\\E", "shortCiteRegEx": "Lin", "year": 2015}, {"title": "Lsdh: a hashing approach for large-scale link prediction in microblogs", "author": ["Liu"], "venue": "In Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Liu,? \\Q2014\\E", "shortCiteRegEx": "Liu", "year": 2014}, {"title": "G", "author": ["Miller"], "venue": "A.", "citeRegEx": "Miller 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["Tresp Nickel", "M. Kriegel 2012] Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "C", "author": ["R. Socher", "D. Chen", "Manning"], "venue": "D.; and Ng, A.", "citeRegEx": "Socher et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "R", "author": ["I. Sutskever", "J.B. Tenenbaum", "Salakhutdinov"], "venue": "R.", "citeRegEx": "Sutskever. Tenenbaum. and Salakhutdinov 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "L", "author": ["K.Q. Weinberger", "Saul"], "venue": "K.", "citeRegEx": "Weinberger and Saul 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "L", "author": ["K.Q. Weinberger", "Saul"], "venue": "K.", "citeRegEx": "Weinberger and Saul 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "K", "author": ["W. Wu", "H. Li", "H. Wang", "Zhu"], "venue": "Q.", "citeRegEx": "Wu et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Content-structural relation inference in knowledge base", "author": ["Jia Zhao", "Z. Wang 2014] Zhao", "Y. Jia", "Y. Wang"], "venue": "In Twenty-Eighth AAAI Conference on Artificial Intelligence", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [], "year": 2015, "abstractText": "Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, e.g., TransE and TransH, learn embedding representation by defining a global margin-based loss function over the data. However, the optimal loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidate loss functions, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this paper, we propose a locally adaptive translation method for knowledge graph embedding, called TransA, to find the optimal loss function by adaptively determining its margin over different knowledge graphs. Experiments on two benchmark data sets demonstrate the superiority of the proposed method, as compared to the-state-of-the-art ones.", "creator": "LaTeX with hyperref package"}}}