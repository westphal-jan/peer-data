{"id": "1512.01752", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Dec-2015", "title": "Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation", "abstract": "traditional graph - based semi - algorithm supervised learning ( ssl ) communication approaches, even though both widely applied, are not suited for massive empirical data distribution and large global label scenarios themselves since often they scale linearly upwards with the number ranges of specified edges $ |. e | $ and distinct item labels $ \u222a m $. to indirectly deal accurately with the combined large cross label size problem, recent works propose capturing sketch - based methods compared to approximate scaling the distribution on labels per node thereby to achieving a space mass reduction measure from $ o ( m ) $ to $ o ( \\ @ log m ) $, under certain desired conditions. while in addressing this paper, while we present a novel streaming interactive graph - sketch based ssl software approximation kit that captures the sparsity of fitting the label distribution and ultimately ensures the statistical algorithm propagates labels accurately, and further readily reduces the space cost complexity per node r to $ c o ( v 1 ) $. books we discussed also specifically provide a distributed version implementations of the classical algorithm variant that scales well to large data group sizes. experiments shown on pure real - world quantitative datasets demonstrate that the new method paradigm achieves better scaling performance ultimately than previously existing state - or of - the - art mapping algorithms with hugely significant gap reduction in user memory utilization footprint. we did also study substantially different feedback graph construction mechanisms for semantic natural language intelligence applications... and ourselves propose considering a robust graph augmentation process strategy trained using state - of - past the - art unsupervised deep learning architectures that would yields further environmentally significant quality value gains.", "histories": [["v1", "Sun, 6 Dec 2015 06:58:57 GMT  (464kb,D)", "https://arxiv.org/abs/1512.01752v1", "10 pages"], ["v2", "Mon, 16 May 2016 19:40:37 GMT  (461kb,D)", "http://arxiv.org/abs/1512.01752v2", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["sujith ravi", "qiming diao"], "accepted": false, "id": "1512.01752"}, "pdf": {"name": "1512.01752.pdf", "metadata": {"source": "CRF", "title": "Large Scale Distributed Semi-Supervised Learning Using Streaming Approximation", "authors": ["Sujith Ravi", "Qiming Diao"], "emails": ["sravi@google.com", "qiming.ustc@gmail.com"], "sections": [{"heading": null, "text": "Traditional graph-based semi-supervised learning (SSL) approaches are not suited for massive data and large label scenarios since they scale linearly with the number of edges |E| and distinct labels m. To deal with the large label size problem, recent works propose sketch-based methods to approximate the label distribution per node thereby achieving a space reduction from O(m) to O(logm), under certain conditions. In this paper, we present a novel streaming graphbased SSL approximation that effectively captures the sparsity of the label distribution and further reduces the space complexity per node to O(1). We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. Finally, we propose a robust graph augmentation strategy using unsupervised deep learning architectures that yields further significant quality gains for SSL in natural language applications."}, {"heading": "1 Introduction", "text": "Semi-supervised learning (SSL) methods use small amounts of labeled data along with large amounts of unlabeled data to train prediction systems. Such approaches have gained widespread usage in recent years\n1Work done during an internship at Google.\nAppearing in Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain. JMLR: W&CP volume 51. Copyright 2016 by the authors.\nand have been rapidly supplanting supervised systems in many scenarios owing to the abundant amounts of unlabeled data available on the Web and other domains. Annotating and creating labeled training data for many predictions tasks is quite challenging because it is often an expensive and labor-intensive process. On the other hand, unlabeled data is readily available and can be leveraged by SSL approaches to improve the performance of supervised prediction systems.\nThere are several surveys that cover various SSL methods in the literature [25, 37, 8, 6]. The majority of SSL algorithms are computationally expensive; for example, transductive SVM [16]. Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes. These methods exploit the idea of constructing and smoothing a graph in which data (both labeled and unlabeled) is represented by nodes and edges link vertices that are related to each other. Edge weights are defined using a similarity function on node pairs and govern how strongly the labels of the nodes connected by the edge should agree. Graph-based methods based on label propagation [38, 29] work by using class label information associated with each labeled \u201cseed\u201d node, and propagating these labels over the graph in a principled, iterative manner. These methods often converge quickly and their time and space complexity scales linearly with the number of edges |E| and number of labels m. Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].\nSeveral classification and knowledge expansion type of problems involve a large number of labels in realworld scenarios. For instance, entity-relation classification over the widely used Freebase taxonomy requires learning over thousands of labels which can grow further by orders when extending to open-domain ex-\nar X\niv :1\n51 2.\n01 75\n2v 2\n[ cs\n.L G\n] 1\n6 M\nay 2\ntraction from the Web or social media; scenarios involving complex overlapping classes [7]; or fine-grained classification at large scale for natural language and computer vision applications [28, 13]. Unfortunately, existing graph-based SSL methods cannot deal with large m and |E| sizes. Typically individual nodes are initialized with sparse label distributions, but they become dense in later iterations as they propagate through the graph. Talukdar and Cohen [28] recently proposed a method that seeks to overcome the label scale problem by using a Count-Min Sketch [10] to approximate labels and their scores for each node. This reduces the memory complexity to O(logm) from O(m). They also report improved running times when using the sketch-based approach. However, in realworld applications, the number of actual labels k associated with each node is typically sparse even though the overall label space may be huge; i.e., k m. Cleverly leveraging sparsity in such scenarios can yield huge benefits in terms of efficiency and scalability. While the sketching technique from [28] approximates the label space succinctly, it does not utilize the sparsity (a naturally occurring phenomenon in real data) to full benefit during learning.\nContributions: In this paper, we propose a new graph propagation algorithm for general purpose semisupervised learning with applications for NLP and other areas. We show how the new algorithm can be run efficiently even when the label size m is huge. At its core, we use an approximation that effectively captures the sparsity of the label distribution and ensures the algorithm propagates the labels accurately. This reduces the space complexity per node from O(m) to O(k), where k m and a constant (say, 5 or 10 in practice), so O(1) which scales better than previous methods. We show how to efficiently parallelize the algorithm by proposing a distributed version that scales well for large graph sizes. We also propose an efficient linear-time graph construction strategy that can effectively combine information from multiple signals which can vary between sparse or dense representations. In particular, we show that for graphs where nodes represent textual information (e.g., entity name or type), it is possible to robustly learn latent semantic embeddings associated with these nodes using only raw text and state-of-the-art deep learning techniques. Augmenting the original graph with such embeddings followed by graph SSL yields significant improvements in quality. We demonstrate the power of the new method by evaluating on different knowledge expansion tasks using existing benchmark datasets. Our results show that, when compared with existing state-of-the-art systems for these tasks, our method performs better in terms of space complexity and qualitative performance."}, {"heading": "2 Graph-based Semi-Supervised Learning", "text": "Preliminary: The goal is to produce a soft assignment of labels to each node in a graph G = (V,E,W ), where V is the set of nodes, E the set of edges and W the edge weight matrix.2 Every edge (v, u) /\u2208 E is assigned a weight wvu = 0. Among the |V | = n number of nodes, |Vl| = nl of them are labeled while |Vu| = nu are unlabeled. We use diagonal matrix S to record the seeds, in which svv = 1 if the node v is seed. L represents the output label set whose size |L| = m can be large in the real world. Y is a n \u2217 m matrix which records the training label distribution for the seeds where Yvl = 0 for v \u2208 Vu, and Y\u0302 is an n \u2217 m label distribution assignment matrix for all nodes. In general, our method is a graph-based semi-supervised learning algorithm, which learns Y\u0302 by propagating the information of Y on graph G."}, {"heading": "2.1 Graph SSL Optimization", "text": "We learn a label distribution Y\u0302 by minimizing the convex objective function:\nC(Y\u0302) = \u00b51 \u2211 v\u2208Vl svv||Y\u0302v \u2212Yv||22\n+ \u00b52 \u2211\nv\u2208V,u\u2208N (v)\nwvu||Y\u0302v \u2212 Y\u0302u||2\n+ \u00b53 \u2211 v\u2208V ||Y\u0302v \u2212U||22\ns.t. L\u2211 l=1 Y\u0302vl = 1, \u2200v (1)\nwhere N (v) is the (incoming) neighbor node set of the node v, and U is the (uniform) prior distribution over all labels. The above objective function models that: 1) the label distribution should be close to the gold label assignment for all the seeds; 2) the label distribution of a pair of neighbors should be similar measured by their affinity score in the edge weight matrix; 3) the label distribution should be close to the prior U, which is a uniform distribution. The setting of the hyperparameters \u00b5i will be discussed in Section 5.1.\nThe optimization criterion is inspired from [5] and similar to some existing approaches such as Adsorption [3] and MAD [29] but uses a slightly different objective function, notably the matrices have different constructions. In Section 5, we also compare our vanilla version against some of these baselines for completeness.\nThe objective function in Equation 1 permits an efficient iterative optimization technique that is repeated\n2The graph G can be directed or undirected depending on the task. Following most existing works in the literature, we use undirected edges for E in our experiments.\nuntil convergence. We utilize the Jacobi iterative algorithm which defines the approximate solution at the (i+ 1)th iteration, given the solution of the (i)th iteration as follows:\nY\u0302 (i) vl =\n1\nMvl (\u00b51svvYvl + \u00b52 \u2211 u\u2208N (v) wvuY\u0302 (i\u22121) ul + \u00b53Ul)\nMvl = \u00b51svv + \u00b52 \u2211\nu\u2208N (v)\nwvu + \u00b53\n(2)\nwhere i is the iteration index and Ul = 1 m which is the uniform distribution on label l. The iterative procedure starts with Y\u0302 (0) vl which is initialized with seed label weight Yvl if v \u2208 Vl, else with uniform distribution 1m . In each iteration i, Y\u0302 (i) vl aggregates the label distribution Y\u0302 (i\u22121) u at iteration i\u22121 from all its neighbors u \u2208 N (v). More details for deriving the update equation can be found in [5].\nWe use the name EXPANDER to refer to this vanilla method that optimizes Equation 1.\n2.2 DIST-EXPANDER: Scaling To Large Data\nIn many applications, semi-supervised learning becomes challenging when the graphs become huge. To scale to really large data sizes, we propose DISTEXPANDER, a distributed version of the algorithm that is directly suited towards parallelization across many machines. We turn to Pregel [20] and its open source version Giraph [2] as the underlying framework for our distributed algorithm. These systems follow a Bulk Synchronous Parallel (BSP) model of computation that proceeds in rounds. In every round, every machine does some local processing and then sends arbitrary messages to other machines. Semantically, we think of the communication graph as fixed, and in each round each node performs some local computation and then sends messages to its neighbors.\nThe specific systems like Pregel and Giraph build infrastructure that ensures that the overall system is fault tolerant, efficient, and fast. The programmer\u2019s job is simply to specify the code that each vertex will run at every round. Previously, some works have explored using MapReduce framework to scale to large graphs [31]. But unlike these methods, the Pregelbased model is far more efficient and better suited for graph algorithms that fit the iterative optimization scheme for SSL algorithms. Pregel keeps vertices and edges on the machine that performs computation, and uses network transfers only for messages. MapReduce, however, is essentially functional, so expressing a graph algorithm as a chained MapReduce requires passing the entire state of the graph from one stage to the next\u2014in general requiring much more communication and associated serialization overhead which results in significant network cost (refer [20]\nAlgorithm 1 DIST-EXPANDER Algorithm\n1: Input: A graph G = (V,E,W ), where V = Vl \u222a Vu Vl = seed/labeled nodes, Vu = unlabeled nodes 2: Output: A label distribution Y\u0302v = Y\u0302v1Y\u0302v2...Y\u0302vm for every node v \u2208 V minimizing the overall objective function (1). Here, Y\u0302vl represents the weight of label l assigned to the node v. 3: Let L be the set of all possible labels, |L| = m. 4: Initialize Y\u0302 0vl with seed label weights if v \u2208 Vl, else 1m . 5: (Graph Creation) Initialize each node v with its neigh-\nbors N (v) = {u : (v, u) \u2208 E}. 6: Partition the graph into p disjoint partitions V1, ..., Vp, where \u22c3\ni Vi = V . 7: for i = 1 to max iter do 8: Process individual partitions Vp in parallel. 9: for every node v \u2208 Vp do\n10: (Message Passing) Send previous label distribution Y\u0302i\u22121v to all neighbors u \u2208 N (v). 11: (Label Update) Receive a message Mu from\nits neighbor u with corresponding label weights Y\u0302i\u22121u . Process each message M1...M|N (v)| and update current label distribution Y\u0302 iv iteratively using Equation (2).\n12: end for 13: end for\nfor a detailed comparison). In addition, the need to coordinate the steps of a chained MapReduce adds programming complexity that is avoided by DISTEXPANDER iterations over rounds/steps. Furthermore, we use a version of Pregel that allows spilling to disk instead of storing the entire computation state in RAM unlike [20]. Algorithm 1 describes the details."}, {"heading": "3 Streaming Algorithm for Scaling To Large Label Spaces", "text": "Graph-based SSL methods usually scale linearly with the label size m, and requireO(m) space for each node. Talukdar and Cohen [28] proposed to deal with the issue of large label spaces by employing a Count-Min Sketch approximation to store the label distribution of each node. However, we argue that it is not necessary to approximate the whole label distribution for each node, especially for large label sets, because the label distribution of each node is typically sparse and only the top ranking ones are useful. Moreover, the Count-Min Sketch can even be harmful for the top ranking labels because of its approximation. The authors also mention other related works that attempt to induce sparsity using regularization techniques [32, 18] but for a very different purpose [11]. In contrast, our work tackles the exact same problem as [28] to scale graph-based SSL for large label settings. The method presented here does not attempt to enforce sparsity and instead focuses on efficiently storing and updating label distributions during semi-supervised learning with a streaming approximation. In addition, we also compare (in Section 5) against other relevant graph-\nbased SSL baselines [30, 1] that use heuristics to discard poorly scored labels and retain only top ranking labels per node out of a large label set.\nEXPANDER-S Method: We propose a streaming sparsity approximation algorithm for semi-supervised learning that achieves constant space complexity and huge memory savings over the current state-of-the-art approach (MAD-SKETCH) in addition to significant runtime improvements over the exact version. The method processes messages from neighbors efficiently in a streaming fashion and records a sparse set of top ranking labels for each node and approximate estimate for the remaining. In general, the idea is similar to finding frequent items from data streams, where the item is the label and the streams are messages from neighbors in our case. Our Pregel-based approach (Algorithm 1) provides a natural framework to implement this idea of processing message streams. We replace the update Step 11 in the algorithm with the new version thereby allowing us to scale to both large label spaces and data using the same framework.\nPreliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24]. The general idea is that a data stream containing N elements is split into multiple epochs with 1 elements in each epoch. Thus there are N epochs in total, and each such epoch has an ID starting from 1. The algorithm processes elements in each epoch sequentially and maintains a list of tuples of the form (e, f,\u2206), where e is an item, f is its reported frequency, and \u2206 is the maximum error of the frequency estimation. In current epoch t, when an item e comes in, it increments the frequency count f , if the item e is contained in the list of tuples. Otherwise, it creates a new tuple (e, 1, t \u2212 1). Then, after each epoch, the algorithm filters out the items whose maximum frequency is small. Specifically, if the epoch t ended, the algorithm deletes all tuples that satisfies the condition f + \u2206 \u2264 t. This ensures that rare items are not retained at the end.\nNeighbor label distributions as weighted streams: Intuitively, in our setting, each item is a label and each neighbor is an epoch. For a given node v, the neighbors pass label probability streams to node v, where each neighbor u \u2208 N (v) is an epoch and the size of epochs is |N (v)|. We maintain a list of tuples of the form (l, f,\u2206), in which the l is the label index, f is the weighted probability value, and \u2206 is the maximum error of the weighted probability estimation. For the current neighbor ut (say, it is the t-th neighbor of v, t \u2264 |N (v)|), the node v receives the label distribution Y\u0302utl with edge weight wvut . The al-\ngorithm then does two things: if the label l is currently in the tuple list, it increments the probability value f by adding wvut Y\u0302utl. If not, it creates new tuple of the\nform (l, wvut Y\u0302utl, \u03b4 \u2211t\u22121 i=1 wvui). Here, we use \u03b4 as a probability threshold (e.g., can be set as uniform distribution 1m ), because the value in an item frequency stream is naturally 1 while ours is a probability weight. Moreover, each epoch t, which is neighbor ut in our task, is weighted by the edge weight wvut unlike previous settings [21]. Then, after we receive the message from the t-th neighbor, we filter out the labels whose maximum probability is small. We delete label l, if f + \u2206 \u2264 \u03b4 \u2211t i=1 wvui .\nMemory-bounded update: With the given streaming sparsity algorithm, we can ensure that no low weighted-probability labels are retained after receiving messages from all neighbors. However, in many cases, we want the number of retained labels to be bounded by k, i.e retain the top-k label based on the probability. In this case, for a node v, each of its neighbors u \u2208 N (v) just contains its top-k labels, i.e. Y\u0302u= Y\u0302ul1 , Y\u0302ul2 , \u00b7 \u00b7 \u00b7 , Y\u0302ulk . Moreover, we use \u03b4u = 1.0\u2212 \u2211k i=1 Y\u0302uli m\u2212k to record the average probability mass of the remaining labels. We then apply the previous streaming sparsity algorithm. The only difference is that when a label l does not exists in the current tuple list, it creates a new tuple of the form (l, wvut Y\u0302utl, \u2211t\u22121 i=1 wvui\u03b4ui). Intuitively, instead of setting a fixed global \u03b4 as threshold, we vary the threshold \u03b4ui based on the sparsity of the previous seen neighbors. In each epoch, after receiving messages Y\u0302ut from the current (t-th) neighbor, we scan the current tuple list. For each tuple (l, f,\u2206), we increments its probability value f by adding \u03b4ut , if label l is not within the top-k label list of the current t-th neighbor. Then, we filter out label l, if f + \u2206 \u2264 \u2211t i=1 wvui\u03b4ui . Finally, after receiving messages from all neighbors, we rank all remaining tuples based on the value f + \u2206 within each tuple (l, f,\u2206). This value represents the maximum weighted-probability estimation. Then we just pick the top-k labels and record only their probabilities for the current node v.\nLemma 1 For any node u \u2208 V , let y be the unnormalized true label weights and y\u0302 be the estimate given by the streaming sparsity approximation version of EXPANDER algorithm at any given iteration. Let N be the total number of label entries received from all neighbors of u before aggregation, d = |N (u)| be the degree of node u and k be the constant number of (non-zero) entries retained in y\u0302 where N \u2264 k \u00b7 d, then (1) the approximation error of the proposed sparsity approximation is bounded in each iteration by y\u0302l \u2264 yl \u2264 y\u0302l + \u03b4 \u00b7 Nk for all labels l, (2) the space used by the algorithm at each node is O(k) = O(1).\nThe proof for the first part of the statement can be derived following a similar analysis as [21] using label weights instead of frequency. At the end of each iteration, the algorithm ensures that labels with low weights are not retained and for the remaining ones, its estimate is close to the exact label weight within an additive factor. The second part of the statement follows direclty from the fact that each node retains atmost k labels in every iteration. The detailed proof is not included here.\nNext, we study various graph construction choices and demonstrate how augmenting the input graph using external information can be beneficial for learning."}, {"heading": "4 Graph Construction", "text": "The main ingredient for graph-based SSL approaches is the input graph itself. We demonstrate that the choice of graph construction mechanism has an important effect on the quality of SSL output. Depending on the edge link information as well as choice of vertex representation, there are multiple ways to create an input graph for SSL\u2014(a) Generic graphs which represent observed neighborhood or link information connecting vertices (e.g., connections in a social network), (b) graphs constructed from sparse feature representations for each vertex (e.g., a bipartite Freebase graph connecting entity nodes with cell value nodes that capture properties of the entity occurring in a schema or table), (c) graphs constructed from dense representations for each vertex, i.e., use dense feature characteristics per node to define neighborhood (discussed in more detail in the next section), and (d) augmented graphs that use a mixture of the above.\nFigure 1 shows an illustration of the various graph types. We focus on (b), (c) and (d) here since these are more applicable to natural language scenarios. Sparse instance-feature graphs (b) are typically provided as input for most SSL tasks in NLP. Next, we propose a method to automatically construct a graph (c) for text applications using semantic embeddings and use this to produce an augmented graph (d) that captures both sparse and dense per-vertex characteristics."}, {"heading": "4.1 Graph Augmentation with Dense Semantic Representations", "text": "In the past, graph-based SSL methods have been widely applied to several NLP problems. In many scenarios, the nodes (and labels) represent textual information (e.g., query, document, entity name/type, etc.) and could be augmented with semantic information from the real world. Recently, some researchers have explored strategies to enhance the input graphs [19] using external sources such as the Web or a knowledge base. However, these methods require access to structured information from a knowledge base or access to Web search results corresponding to a large number of targeted queries from the particular domain. Unlike these methods, we propose a more robust strategy for graph augmentation that follows a two-step approach using only a large corpus of raw text. First, we learn a dense vector representation that captures the underlying semantics associated with each (text) node. We resort to recent state-of-the-art deep learning algorithms to efficiently learn word and phrase semantic embeddings in a dense low-dimensional space from a large text corpus using unsupervised methods.\nWe follow the recent work of Mikolov et al. [22, 23] to compute continuous vector representations of words (or phrases) from very large datasets. The method takes a text corpus as input and learns a vector representation for every word (or phrase) in the vocabulary. We use the continuous skip-gram model [22] combined with a hierarchical softmax layer in which each word in a sentence is used as an input to a log-linear classifier which tries to maximize classification of another word within the same sentence using the current word. More details about the deep learning architecture and training procedure can be found in [22]. Moreover, these models can be efficiently parallelized and scale to huge datasets using a distributed training framework [12]. We obtain a 1000-dimensional vector representation (for each word) trained on 100 billion tokens of newswire text.3 For some settings (example dataset in Section 5), nodes represent entity names (word collocations and not bag-of-words). We can train the embedding model to take this into account by treating entity mentions (e.g., within Wikipedia or news article text) as special words and applying the same procedure as earlier to produce embedding vectors for entities. Next, for each node v = w1w2...wn, we query the pre-trained vectors E to obtain its corresponding embedding vemb from words in the node text.\nvemb = { E(v), if v \u2208 E 1 n \u2211 i E(wi), otherwise\n(3)\n3It is also possible to use pre-trained embedding vectors: https://code.google.com/p/word2vec/\nFollowing this, we compute a similarity function over pairs of nodes using the embedding vectors, where simemb(u, v) = uemb \u00b7 vemb. We filter out node pairs with low similarity values < \u03b8sim and add an edge in the original graph G = (V,E) for every remaining pair.\nUnfortunately, the above strategy requires O(|V |2) similarity computations which is infeasible in practice. To address this challenge, we resort to locality sensitive hashing (LSH) [9], a random projection method used to efficiently approximate nearest neighbor lookups when data size and dimensionality is large. We use the node embedding vectors vemb and perform LSH to significantly reduce unnecessary pairwise computations that would yield low similarity values.4"}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experiment Setup", "text": "Data: We use two real-world datasets (publicly available from Freebase) for evaluation in this section.\nData Name Nodes Edges Labels Avg, Deg.\nFreebase-Entity 301, 638 1, 155, 001 192 3.83 Freebase-Relation 9, 367, 013 16, 817, 110 7, 664 1.80\nFreebase-Entity (referred as FB-E) is the exact same dataset and setup used in previous works [28, 30]. This dataset consists of cell value nodes and property nodes which are entities and Table properties in Freebase. An edge indicates that an entity appears in a table cell. The second dataset is Freebase-Relation (referred as FB-R). This dataset comprises entity1-relation-entity2 triples from Freebase, which consists of more than 7000 relations and more than 8M triples. We extract two kinds of nodes from these triples, entity-pair nodes (e.g., <Barack Obama, Hawaii>) and entity nodes (e.g., Barack Obama). The former one is labeled with the relation type (e.g., PlaceOfBirth). An edge is created if two nodes have an entity in common. Graph-based SSL systems: We compare different graph-based SSL methods: EXPANDER, both the vanilla method and the version that runs on the graph with semantic augmentation (as detailed in Section 4.1), and EXPANDER-S, the streaming approximation algorithm introduced in Section 3.\nFor baseline comparison, we consider two state-of-art existing works MAD [29] and MAD-SKETCH [28]. Talukdar and Pereira [30] show that MAD outperforms traditional graph-based SSL algorithms. MADSKETCH further approximates the label distribution on each node using Count-Min Sketch to reduce the space and time complexity. To ensure a fair comparison, we obtained the MAD code directly from the authors and ran the exact same code on the\n4For LSH, we use \u03b8sim=0.6, number of hash tables D=12, width W=10 in our experiments.\nsame machine as EXPANDER for all experiments reported here. We obtained the same MRR performance (0.28) for MAD on the Freebase-Entity dataset (10 seeds/label) as reported by [28]. Parameters: For the SSL objective function parameters, we set \u00b51 = 1, \u00b52 = 0.01 and \u00b53 = 0.01. We tried multiple settings for MAD and MAD-SKETCH algorithms and replicated the best reported performance metrics from [28] using these values, so the baseline results are comparable to their system. Evaluation: Precision@K (referred as P@K) and Mean Reciprocal Rank (MRR) are used as evaluation metrics for all experiments, where higher is better. P@K measures the accuracy of the top ranking labels (i.e., atleast one of the gold labels was found among the top K) returned by each method. MRR is calculated as 1|Q| \u2211 v\u2208Q 1 rankv\n, where Q \u2286 V is the test node set, and rankv is the rank of the gold label among the label distribution Y\u0302v.\nFor experiments, we use the same procedure as reported in literature [28], running each algorithm for 10 iterations per round (verified to be sufficient for convergence on these datasets) and then taking the average performance over 3 rounds."}, {"heading": "5.2 Graph SSL Results", "text": "First, we quantitatively compare the graph-based SSL methods in terms of MRR and Precision@K without considering the space and time complexity. Table 1 shows the results with 5 seeds/label and 10 seeds/label on the Freebase-Entity dataset.\nFrom the results, we have several findings: (1) Both EXPANDER-based methods outperform MAD consistently in terms of MRR and Precison@K. (2) Our algorithm on the enhanced graph using semantic embeddings (last row) produces significant gains over the original graph, which indicates that densifying the graph with additional information provides a useful technique for improving SSL in such scenarios."}, {"heading": "5.3 Streaming Sparsity versus Sketch", "text": "In this section, we compare the MAD-SKETCH and EXPANDER-S algorithms against the vanilla versions. The former one uses Count-Min Sketch to approximate the whole label distribution per node, while the latter uses streaming approximation to capture the sparsity of the label distribution. For FreebaseEntity dataset, we run these two methods with 5 seeds/label.5 The Freebase-Relation dataset is too big to run on a single machine, so we sample a smaller dataset FB-R2 6 from it. For this new dataset, we only\n5We observed similar findings with larger seed sizes. 6We create FB-R2 by randomly picking 1000 labels and keeping only entity-pair nodes which belong to these labels and their corresponding edges (4.5M nodes, 7.4M edges).\ncompare the approximation methods MAD-SKETCH and our EXPANDER-S, by picking 20 seeds/label and taking average over 3 rounds. We just test MAD-SKETCH (w=20,d=3), since the setting MADSKETCH (w=109,d=3) runs out-of-memory using a single machine. We use protocol buffers (an efficient data serialization scheme) to store the data for EXPANDER-S. For the space, we report the memory taken by the whole process. For EXPANDER-S, as described in section 3, each node stores at most k labels, so the MRR and precision@K where K > k are not available, and we refer it as NA.\nTables 2, 3 show results on Freebase-Entity and the smaller Freebase-Relation (FB-R2) datasets, respectively. We make the following observations: (1) MAD-SKETCH (w=109,d=3) can obtain similar performance compared with MAD, while it can achieve about 5.2\u00d7 speedup, and 3.54\u00d7 space reduction. However, when the sketch size is small (e.g. w=20,d=3), the algorithm loses quality in terms of both MRR and Precision@K. For applications involving large label sizes, due to space limitations, we can only allocate a limited memory size for each node, yet we should still be able to retain the accurate or relevant labels within the available memory. On FB-R2 data, we observe that MAD-SKETCH (w=109,d=3) is not executable, and the MAD-SKETCH (w=20,d=3) yields poor results. (2) Comparing the EXPANDER and EXPANDER-S, the latter one obtains similar performance in terms of Precision@K, while it achieves 3.28\u00d7 speedup for k = 5 and 2.16\u00d7 space reduction. Compared with the MAD-SKETCH, the speedup is not as steep mainly because the algorithm needs to go through the tuple list and filter out the ones below the threshold to ensure that we retain the \u201cgood\u201d labels. However, we can easily execute EXPANDER-\nS on the subset of Freebase-Relation (FB-R2), due to low space requirements (\u223c12\u00d7 lower than even MADSKETCH). Moreover, it outperforms MAD-SKETCH (w=20,d=3) in terms of Precision@K.\nFrequency Thresholding vs. Streaming Sparsity: We also compare our streaming approximation algorithm (EXPANDER-S) against a simple frequency-based thresholding technique (FREQTHRESH) used often by online sparse learning algorithms (zero out small weights after each update Step 11 in Algorithm 1). However, this becomes computationally inefficient O(degree \u2217 k) in our case especially for nodes with high degree, which is prohibitive since it requires us to aggregate label distributions from all neighbors before pruning.7 In both cases, we can still maintain constant space complexity per-node by retaining only top-K labels after the update step. Table 4 shows that the streaming sparsity approximation produces significantly better quality results (precision at 5, 10) than frequency thresholding in addition to being far more computationally efficient."}, {"heading": "5.4 Graph SSL with Large Data, Label Sizes", "text": "In this section, we evaluate the space and time efficiency of our distributed algorithm DISTEXPANDER (described in Section 2.2) coupled with\n7We set threshold \u03b4 = 0.001 for FREQ-THRESH based on experiments on a small heldout dataset.\nthe new streaming approximation update (Section 3). To focus on large data settings, we only use FreebaseRelation data set in subsequent experiments. Following previous work [28], we use the identity of each node as a label. In other words, the label size can potentially be as large as the size of the nodes |V |. First, we test how the computation time scales with the number of nodes, by fixing label size. We follow a straightforward strategy: randomly sample different number of edges (and corresponding nodes) from the original graph. For each graph size, we randomly sample 1000 nodes as seeds, and set their node identities as labels. We then run vanilla EXPANDER, EXPANDER-S (k = 5) and DIST-EXPANDER-S (k = 5) on each graph. The last one is a distributed version which partitions the graph and runs on 100 machines. We show the running time for different node sizes in Figure 2. EXPANDER runs out-of-memory when the node size\ngoes up to 1M, and the running time slows down significantly when graph size increases. EXPANDER-S (k=5) can handle all five data sets on a single machine and while the running time is better than EXPANDER, it starts to slow down noticeably on larger graphs with 7M nodes. DIST-EXPANDER-S scales quite well with the node size, and yields a 50-fold speedup when compared with EXPANDER-S when the node size is \u223c7M.\nFigure 3 illustrates how the memory usage scales with label size for our distributed version. For this scenario, we use the entire Freebase-Relation dataset and vary label size by randomly choosing different number of seed nodes as labels. We find that the overall\nspace cost is consistently around 35GB, because our streaming algorithm captures a sparse constant space approximation of the label distribution and does not run out-of-memory even for large label sizes. Note that the distributed version consumes more than 30GB primarily because there will be redundant information recorded when partitioning the graph (including data replication for system fault-tolerance).\nFinally, we test how the distributed sparsity approximation algorithm scales on massive graphs with billions of nodes and edges. Since the Freebase graph is not sufficiently large for this setting, we construct graphs of varying sizes from a different dataset (segmented video sequences tagged with 1000 labels). Figure 4 illustrates that the streaming distributed algorithm scales very efficiently for such scenarios and runs quite fast. Each computing iteration for DISTEXPANDER-S runs to completion in just 2.3 seconds on a 17.8M node/26.7M edge graph and roughly 9 minutes on a much larger 2.6B node/6.5B edge graph."}, {"heading": "6 Conclusion", "text": "Existing graph-based SSL algorithms usually require O(m) space per node and do not scale to scenarios involving large label sizes m and massive graphs. We propose a novel streaming algorithm that effectively and accurately captures the sparsity of the label distribution. The algorithm operates efficiently in a streaming fashion and reduces the space complexity per node to O(1) in addition to yielding high quality performance. Moreover, we extend the method with a distributed algorithm that scales elegantly to large data and label sizes (for example, billions of nodes/edges and millions of labels). We also show that graph augmentation using unsupervised learning techniques can provide a robust strategy to yield performance gains for SSL problems involving natural language."}, {"heading": "Acknowledgements", "text": "We thank Partha Talukdar for useful pointers to MAD code and Kevin Murphy for providing us access to the Freebase-Relation dataset."}], "references": [{"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["R. Agrawal", "A. Gupta", "Y. Prabhu", "M. Varma"], "venue": "Proceedings of the International World Wide Web Conference,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Video suggestion and discovery for Youtube: Taking random walks through the view graph", "author": ["S. Baluja", "R. Seth", "D. Sivakumar", "Y. Jing", "J. Yagnik", "S. Kumar", "D. Ravichandran", "M. Aly"], "venue": "Proceedings of the 17th International Conference on World Wide Web, WWW \u201908, pages 895\u2013904,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "On manifold regularization", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Proceeding of the Conference on Artificial Intelligence and Statistics (AIS- TATS),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Label propagation and quadratic criterion", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "O. Chapelle, B. Sch\u00f6lkopf, and A. Zien, editors, Semi-Supervised Learning, pages 193\u2013216. MIT Press,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised learning for natural language processing", "author": ["J. Blitzer", "X.J. Zhu"], "venue": "ACL-HLT Tutorial, June", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Toward an architecture for never-ending language learning", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R.H. Jr.", "T.M. Mitchell"], "venue": "In AAAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Sch\u00f6lkopf", "A. Zien", "editors"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M. Charikar"], "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing, pages 380\u2013388,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "An improved data stream summary: The count-min sketch and its applications", "author": ["G. Cormode", "S. Muthukrishnan"], "venue": "Journal of Algorithms, 55(1):58\u2013 75,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Graph-based lexicon expansion with sparsity-inducing penalties", "author": ["D. Das", "N.A. Smith"], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 677\u2013687. Association for Computational Linguistics,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M. Ranzato", "A.W. Senior", "P.A. Tucker", "K. Yang", "A.Y. Ng"], "venue": "Proceedings of NIPS, pages 1232\u20131240,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR09,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Streaming pointwise mutual information", "author": ["B.V. Durme", "A. Lall"], "venue": "Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 1892\u20131900.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Streaming for large scale NLP: Language modeling", "author": ["A. Goyal", "III H. Daum\u00e9", "S. Venkatasubramanian"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "Proceedings of the Sixteenth International Conference on Machine Learning, pages 200\u2013209,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Transductive learning via spectral graph partitioning", "author": ["T. Joachims"], "venue": "Proceedings of ICML, pages 290\u2013297,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Sparsity and persistence: mixed norms provide simple signal models with dependent coefficients", "author": ["M. Kowalski", "B. Torr\u00e9sani"], "venue": "Signal, Image and Video Processing, 3(3):251\u2013264, Sept.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Class label enhancement via related instances", "author": ["Z. Kozareva", "K. Voevodski", "S.-H. Teng"], "venue": "Proceedings of EMNLP, pages 118\u2013128,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Pregel: a system for large-scale graph processing", "author": ["G. Malewicz", "M.H. Austern", "A.J. Bik", "J.C. Dehnert", "I. Horn", "N. Leiser", "G. Czajkowski"], "venue": "Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, pages 135\u2013146,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximate frequency counts over data streams", "author": ["G.S. Manku", "R. Motwani"], "venue": "Proceedings of the 28th International Conference on Very Large Data Bases, VLDB \u201902, pages 346\u2013357,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of Workshop at ICLR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proceedings of NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Exponential reservoir sampling for streaming language models", "author": ["M. Osborne", "A. Lall", "B.V. Durme"], "venue": "Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics, ACL \u20192014, pages 687\u2013692,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with labeled and unlabeled data", "author": ["M. Seeger"], "venue": "Technical report,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2001}, {"title": "Entropic graph regularization in non-parametric semi-supervised classification", "author": ["A. Subramanya", "J.A. Bilmes"], "venue": "Proceedings of NIPS, pages 1803\u20131811,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient graph-based semi-supervised learning of structured tagging models", "author": ["A. Subramanya", "S. Petrov", "F. Pereira"], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP \u201910, pages 167\u2013 176,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Scaling graph-based semi supervised learning to large number of labels using count-min sketch", "author": ["P. Talukdar", "W. Cohen"], "venue": "Proceedings of AISTATS, pages 940\u2013947,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "New regularized algorithms for transductive learning", "author": ["P.P. Talukdar", "K. Crammer"], "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD \u201909, pages 442\u2013457,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Experiments in graph-based semi-supervised learning methods for class-instance acquisition", "author": ["P.P. Talukdar", "F. Pereira"], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 1473\u20131481,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Weakly-supervised acquisition of labeled class instances using graph random walks", "author": ["P.P. Talukdar", "J. Reisinger", "M. Pasca", "D. Ravichandran", "R. Bhagat", "F. Pereira"], "venue": "EMNLP, pages 582\u2013590,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society (Series B), 58:267\u2013288,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "On information regularization", "author": ["A.C. Tommi", "T. Jaakkola"], "venue": "Proceedings of the 19th UAI,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Balanced label propagation for partitioning massive graphs", "author": ["J. Ugander", "L. Backstrom"], "venue": "Proceedings of the Sixth ACM International Conference on Web Search and Data Mining, pages 507\u2013516,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient online locality sensitive hashing via reservoir counting", "author": ["B. Van Durme", "A. Lall"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT \u201911, pages 18\u201323,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Label propagation from imagenet to 3d point clouds", "author": ["Y. Wang", "R. Ji", "S.-F. Chang"], "venue": "Proceedings of CVPR, pages 3135\u20133142. IEEE,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Semi-supervised learning literature survey", "author": ["X. Zhu"], "venue": "Technical Report 1530, Computer Sciences, University of Wisconsin-Madison,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Semisupervised learning using gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "Proceedings of ICML, pages 912\u2013919,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 23, "context": "There are several surveys that cover various SSL methods in the literature [25, 37, 8, 6].", "startOffset": 75, "endOffset": 89}, {"referenceID": 35, "context": "There are several surveys that cover various SSL methods in the literature [25, 37, 8, 6].", "startOffset": 75, "endOffset": 89}, {"referenceID": 6, "context": "There are several surveys that cover various SSL methods in the literature [25, 37, 8, 6].", "startOffset": 75, "endOffset": 89}, {"referenceID": 4, "context": "There are several surveys that cover various SSL methods in the literature [25, 37, 8, 6].", "startOffset": 75, "endOffset": 89}, {"referenceID": 14, "context": "The majority of SSL algorithms are computationally expensive; for example, transductive SVM [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 36, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 15, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 31, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 2, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 24, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 28, "context": "Graph-based SSL algorithms [38, 17, 33, 4, 26, 30] are a subclass of SSL techniques that have received a lot of attention recently, as they scale much better to large problems and data sizes.", "startOffset": 27, "endOffset": 50}, {"referenceID": 36, "context": "Graph-based methods based on label propagation [38, 29] work by using class label information associated with each labeled \u201cseed\u201d node, and propagating these labels over the graph in a principled, iterative manner.", "startOffset": 47, "endOffset": 55}, {"referenceID": 27, "context": "Graph-based methods based on label propagation [38, 29] work by using class label information associated with each labeled \u201cseed\u201d node, and propagating these labels over the graph in a principled, iterative manner.", "startOffset": 47, "endOffset": 55}, {"referenceID": 34, "context": "Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].", "startOffset": 73, "endOffset": 77}, {"referenceID": 32, "context": "Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 28, "context": "Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].", "startOffset": 249, "endOffset": 261}, {"referenceID": 25, "context": "Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].", "startOffset": 249, "endOffset": 261}, {"referenceID": 17, "context": "Successful applications include a wide range of tasks in computer vision [36], information retrieval (IR) and social networks [34] and natural language processing (NLP); for example, class instance acquisition and relation prediction, to name a few [30, 27, 19].", "startOffset": 249, "endOffset": 261}, {"referenceID": 5, "context": "traction from the Web or social media; scenarios involving complex overlapping classes [7]; or fine-grained classification at large scale for natural language and computer vision applications [28, 13].", "startOffset": 87, "endOffset": 90}, {"referenceID": 26, "context": "traction from the Web or social media; scenarios involving complex overlapping classes [7]; or fine-grained classification at large scale for natural language and computer vision applications [28, 13].", "startOffset": 192, "endOffset": 200}, {"referenceID": 11, "context": "traction from the Web or social media; scenarios involving complex overlapping classes [7]; or fine-grained classification at large scale for natural language and computer vision applications [28, 13].", "startOffset": 192, "endOffset": 200}, {"referenceID": 26, "context": "Talukdar and Cohen [28] recently proposed a method that seeks to overcome the label scale problem by using a Count-Min Sketch [10] to approximate labels and their scores for each node.", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "Talukdar and Cohen [28] recently proposed a method that seeks to overcome the label scale problem by using a Count-Min Sketch [10] to approximate labels and their scores for each node.", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "While the sketching technique from [28] approximates the label space succinctly, it does not utilize the sparsity (a naturally occurring phenomenon in real data) to full benefit during learning.", "startOffset": 35, "endOffset": 39}, {"referenceID": 3, "context": "The optimization criterion is inspired from [5] and similar to some existing approaches such as Adsorption [3] and MAD [29] but uses a slightly different objective function, notably the matrices have different constructions.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "The optimization criterion is inspired from [5] and similar to some existing approaches such as Adsorption [3] and MAD [29] but uses a slightly different objective function, notably the matrices have different constructions.", "startOffset": 107, "endOffset": 110}, {"referenceID": 27, "context": "The optimization criterion is inspired from [5] and similar to some existing approaches such as Adsorption [3] and MAD [29] but uses a slightly different objective function, notably the matrices have different constructions.", "startOffset": 119, "endOffset": 123}, {"referenceID": 3, "context": "More details for deriving the update equation can be found in [5].", "startOffset": 62, "endOffset": 65}, {"referenceID": 18, "context": "We turn to Pregel [20] and its open source version Giraph [2] as the underlying framework for our distributed algorithm.", "startOffset": 18, "endOffset": 22}, {"referenceID": 29, "context": "Previously, some works have explored using MapReduce framework to scale to large graphs [31].", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "MapReduce, however, is essentially functional, so expressing a graph algorithm as a chained MapReduce requires passing the entire state of the graph from one stage to the next\u2014in general requiring much more communication and associated serialization overhead which results in significant network cost (refer [20] Algorithm 1 DIST-EXPANDER Algorithm 1: Input: A graph G = (V,E,W ), where V = Vl \u222a Vu Vl = seed/labeled nodes, Vu = unlabeled nodes 2: Output: A label distribution \u0176v = \u0176v1\u0176v2.", "startOffset": 308, "endOffset": 312}, {"referenceID": 18, "context": "Furthermore, we use a version of Pregel that allows spilling to disk instead of storing the entire computation state in RAM unlike [20].", "startOffset": 131, "endOffset": 135}, {"referenceID": 26, "context": "Talukdar and Cohen [28] proposed to deal with the issue of large label spaces by employing a Count-Min Sketch approximation to store the label distribution of each node.", "startOffset": 19, "endOffset": 23}, {"referenceID": 30, "context": "The authors also mention other related works that attempt to induce sparsity using regularization techniques [32, 18] but for a very different purpose [11].", "startOffset": 109, "endOffset": 117}, {"referenceID": 16, "context": "The authors also mention other related works that attempt to induce sparsity using regularization techniques [32, 18] but for a very different purpose [11].", "startOffset": 109, "endOffset": 117}, {"referenceID": 9, "context": "The authors also mention other related works that attempt to induce sparsity using regularization techniques [32, 18] but for a very different purpose [11].", "startOffset": 151, "endOffset": 155}, {"referenceID": 26, "context": "In contrast, our work tackles the exact same problem as [28] to scale graph-based SSL for large label settings.", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "based SSL baselines [30, 1] that use heuristics to discard poorly scored labels and retain only top ranking labels per node out of a large label set.", "startOffset": 20, "endOffset": 27}, {"referenceID": 0, "context": "based SSL baselines [30, 1] that use heuristics to discard poorly scored labels and retain only top ranking labels per node out of a large label set.", "startOffset": 20, "endOffset": 27}, {"referenceID": 19, "context": "Preliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24].", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "Preliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24].", "startOffset": 233, "endOffset": 249}, {"referenceID": 12, "context": "Preliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24].", "startOffset": 233, "endOffset": 249}, {"referenceID": 33, "context": "Preliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24].", "startOffset": 233, "endOffset": 249}, {"referenceID": 22, "context": "Preliminary: Manku and Motwani [21] presented an algorithm for computing frequency counts exceeding a user-specified threshold over data streams, and others have applied this algorithm to handle large amounts of data in NLP problems [15, 14, 35, 24].", "startOffset": 233, "endOffset": 249}, {"referenceID": 19, "context": "Moreover, each epoch t, which is neighbor ut in our task, is weighted by the edge weight wvut unlike previous settings [21].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "The proof for the first part of the statement can be derived following a similar analysis as [21] using label weights instead of frequency.", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "Recently, some researchers have explored strategies to enhance the input graphs [19] using external sources such as the Web or a knowledge base.", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "[22, 23] to compute continuous vector representations of words (or phrases) from very large datasets.", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[22, 23] to compute continuous vector representations of words (or phrases) from very large datasets.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "We use the continuous skip-gram model [22] combined with a hierarchical softmax layer in which each word in a sentence is used as an input to a log-linear classifier which tries to maximize classification of another word within the same sentence using the current word.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "More details about the deep learning architecture and training procedure can be found in [22].", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "Moreover, these models can be efficiently parallelized and scale to huge datasets using a distributed training framework [12].", "startOffset": 121, "endOffset": 125}, {"referenceID": 7, "context": "To address this challenge, we resort to locality sensitive hashing (LSH) [9], a random projection method used to efficiently approximate nearest neighbor lookups when data size and dimensionality is large.", "startOffset": 73, "endOffset": 76}, {"referenceID": 26, "context": "Freebase-Entity (referred as FB-E) is the exact same dataset and setup used in previous works [28, 30].", "startOffset": 94, "endOffset": 102}, {"referenceID": 28, "context": "Freebase-Entity (referred as FB-E) is the exact same dataset and setup used in previous works [28, 30].", "startOffset": 94, "endOffset": 102}, {"referenceID": 27, "context": "For baseline comparison, we consider two state-of-art existing works MAD [29] and MAD-SKETCH [28].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "For baseline comparison, we consider two state-of-art existing works MAD [29] and MAD-SKETCH [28].", "startOffset": 93, "endOffset": 97}, {"referenceID": 28, "context": "Talukdar and Pereira [30] show that MAD outperforms traditional graph-based SSL algorithms.", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "28) for MAD on the Freebase-Entity dataset (10 seeds/label) as reported by [28].", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "We tried multiple settings for MAD and MAD-SKETCH algorithms and replicated the best reported performance metrics from [28] using these values, so the baseline results are comparable to their system.", "startOffset": 119, "endOffset": 123}, {"referenceID": 26, "context": "For experiments, we use the same procedure as reported in literature [28], running each algorithm for 10 iterations per round (verified to be sufficient for convergence on these datasets) and then taking the average performance over 3 rounds.", "startOffset": 69, "endOffset": 73}, {"referenceID": 26, "context": "Following previous work [28], we use the identity of each node as a label.", "startOffset": 24, "endOffset": 28}], "year": 2016, "abstractText": "Traditional graph-based semi-supervised learning (SSL) approaches are not suited for massive data and large label scenarios since they scale linearly with the number of edges |E| and distinct labels m. To deal with the large label size problem, recent works propose sketch-based methods to approximate the label distribution per node thereby achieving a space reduction from O(m) to O(logm), under certain conditions. In this paper, we present a novel streaming graphbased SSL approximation that effectively captures the sparsity of the label distribution and further reduces the space complexity per node to O(1). We also provide a distributed version of the algorithm that scales well to large data sizes. Experiments on real-world datasets demonstrate that the new method achieves better performance than existing state-of-the-art algorithms with significant reduction in memory footprint. Finally, we propose a robust graph augmentation strategy using unsupervised deep learning architectures that yields further significant quality gains for SSL in natural language applications.", "creator": "LaTeX with hyperref package"}}}