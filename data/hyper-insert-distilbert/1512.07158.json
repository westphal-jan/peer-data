{"id": "1512.07158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2015", "title": "Feature Selection for Classification under Anonymity Constraint", "abstract": "well over the and last continuous decade, proliferation preferences of various online platforms users and their ensuing increasing adoption by billions states of users have quickly heightened the privacy risk of monitoring a rogue user researcher enormously. in fact, security researchers have shown that sparse microdata containing information about online activities of apparently a participating user population although considered anonymous, likely can still be used to disclose the identity of the target user by cross - referencing the data with other data sources. to ensure preserve preserve the privacy desirable of a user, purposes in constructing existing works like several methods ( k - anonymity, + l - diversity, differential privacy ) are proposed \u2014 that ensure analyzing a relevant dataset beforehand which also is always meant initially to share or publish bears for small possible identity disclosure risk. however, the majority of these methods modify either the stolen data in actual isolation, without effectively considering finding their utility covered in subsequent knowledge discovery verification tasks, which makes extending these user datasets less highly informative. in analyzing this work, essentially we consider labeled anonymous data that simply are generally used commonly for classification, and scientists propose creating two relevant methods \u2014 for feature based selection considering identifying two critical goals : first, on the unique reduced label feature set the reduced data group has small, disclosure risk, and second, the utility of observing the data is finally preserved for instance performing a classification recognition task. experimental analyses results on various real - quality world datasets themselves show that the proposed method is effective and useful in regulatory practice.", "histories": [["v1", "Tue, 22 Dec 2015 17:06:01 GMT  (47kb)", "http://arxiv.org/abs/1512.07158v1", null], ["v2", "Sat, 13 Feb 2016 03:05:36 GMT  (54kb)", "http://arxiv.org/abs/1512.07158v2", null], ["v3", "Fri, 19 Feb 2016 02:01:57 GMT  (54kb)", "http://arxiv.org/abs/1512.07158v3", null], ["v4", "Thu, 17 Mar 2016 02:30:33 GMT  (55kb)", "http://arxiv.org/abs/1512.07158v4", null], ["v5", "Thu, 1 Dec 2016 01:05:59 GMT  (35kb)", "http://arxiv.org/abs/1512.07158v5", null], ["v6", "Tue, 31 Jan 2017 15:47:47 GMT  (202kb)", "http://arxiv.org/abs/1512.07158v6", "Transactions on Data Privacy 2017"], ["v7", "Mon, 6 Feb 2017 01:14:37 GMT  (201kb)", "http://arxiv.org/abs/1512.07158v7", "Transactions on Data Privacy 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.CR", "authors": ["baichuan zhang", "noman mohammed", "vachik dave", "mohammad al hasan"], "accepted": false, "id": "1512.07158"}, "pdf": {"name": "1512.07158.pdf", "metadata": {"source": "CRF", "title": "Feature Selection for Classification under Anonymity Constraint", "authors": ["Baichuan Zhang", "Mohammad Al Hasan"], "emails": ["bz3@umail.iu.edu", "vsdave@umail.iu.edu", "alhasan@cs.iupui.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n07 15\n8v 1\n[ cs\n.L G\n] 2\n2 D\nec 2"}, {"heading": "1. INTRODUCTION", "text": "Over the last decade, with the proliferation of various online platforms, such as web search, eCommerce, social networking, micro-messaging, streaming entertainment and cloud storage, the digital footprint of today\u2019s Internet user has grown at an unprecedented rate. At the same time, the availability of sophisticated computing paradigm and advanced machine learning algorithms have enabled the platform owners to mine and analyze tera-bytes of digital footprint data for building various predictive analytics and personalization products. For example, search engines and social network platforms use search keywords for providing sponsored advertisements that are personalized for a user\u2019s information need; e-commerce platforms use visitor\u2019s search history for bolstering their merchandising\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\neffort; streaming entertainment providers use people\u2019s rating data for building future product or service recommendation. However, the impressive personalization of services of various online platforms enlighten us as much, as they do make us feel insecure, which stems from knowing the fact that individual\u2019s online behaviors are stored within these companies, and an individual, more often, is not aware of the specific information about themselves that are being stored.\nThe key reason for a web user\u2019s insecurity over the digital footprint data (also known as microdata) is that such data contain sensitive information. For instance, a person\u2019s online search about a disease medication may insinuate that she may be suffering from that disease; a fact that she rather not want to disclose. Similarly, people\u2019s choice of movies, their recent purchases, etc. reveal enormous information regarding their background, preference and lifestyle. Arguably microdata exclude biographical information, but due to the sheer size of our digital footprint the identity of a person can still be recovered from these data by cross-correlation with other data sources or by using publicly available background information. In this sense, these apparently non-sensitive attributes can serve as a quasi-identifier. For an example, Narayanan et al. [22] have identified a Netflix subscriber from his anonymous movie rating by using Internet Movie Database (IMBD) as the source of background information. For the case of Netflix, anonymous microdata was released publicly for facilitating Netflix prize competition, however even if the data is not released, there is always a concern that people\u2019s digital footprint data can be abused within the company by employees or by external hackers, who have malicious intents.\nFor the case of microdata, the identity disclosure risk is high due to some key properties of such a dataset\u2014highdimensionality and sparsity. Sparsity stands for the fact that for a given record there is rarely any record that is similar to the given record considering full multi-dimensional space. It is also shown that in high-dimensional data, the ratio of distance to the nearest neighbor and the farthest neighbor is almost one, i.e., all the points are far from each other [30]. Due to this fact, privacy is difficult to achieve on such datasets. A simple privacy metric that quantifies the disclosure risk of a given data instance is k-anonymity [23] which requires that for any data instance, there are at least k \u2212 1 distinct data instances that share the same feature vector. For high dimensional data such an occurrence is highly unlikely even for a reasonable value of k (say 5), which aggravates the disclosure risk even further. Typically, value based generalization or attribute based generalization are applied so that k-anonymity is achieved, but Aggrawal has proved both theoretically and\nexperimentally that for high dimensional data k-anonymity is not a viable solution even for a k value of 2 [1]. He has also shown that as data dimensionality increases, entire discriminatory information in the data is lost during the process of k-anonymization, which severely limits the data utility. Evidently, finding a good balance between a user\u2019s privacy and the utility of high dimensional microdata is an unsolved problem\u2014which is the primary focus of this paper.\nA key observation of a real-life high dimensional dataset is that it exhibits high clustering tendency in many sub-spaces of the data, even though over the full dimension the dataset is very sparse. Thus an alternative technique for protecting identity disclosure on such data can be finding a subset of features, such that when projecting on these set of features an acceptable level of anonymity can be achieved. One can view this as column suppression instead of more commonly used row suppression for achieving k-anonymity [23]. Now for the case of feature selection for a given k, there may exist many sub-spaces for which a given dataset satisfies k-anonymity, but our objective is to obtain a set of features such that projecting on this set offers the maximum utility of the dataset in terms of a supervised classification task.\nConsider the toy dataset that is shown in Table 1. Each row represents a person, and each column (except the first and the last) represents a keyword. If a cell entry is \u20181\u2019 then the keyword at the corresponding column is associated with the person at the corresponding row. Reader may think this table as a tabular representation of search log of an eCommerce platform, where the \u20181\u2019 under ei column stands for the fact that the corresponding user has searched using the keyword xj within a given period of time, and \u20180\u2019 represents otherwise. The last column represents whether this user has made a purchase over the same time period. The platform owner wants to solve a classification problem to predict which of the users are more likely to make a purchase.\nSay, the platform owner wants to protect the identity of its site visitor by making the dataset k-anonymous. Now, for this toy dataset, if he chooses k = 2, this dataset is not kanonymous. For instance, the feature vector of e3, 10011 is unique in this dataset. However, the dataset is k-anonymous for the same k under the subspace spanned by {x3, x4, x5}. It is also k-anonymous (again for the same k) under the subspace spanned by {x1, x2, x5} (See Table 2). Among these two choices, the latter subspace is probably a better choice, as the features in this set are better discriminator than the features in the former set with respect to the class-label. For feature set {x1, x2, x5}, if we associate the value \u2018101\u2019 with the +1 label, and the value \u2018111\u2019 with the -1 label, we make only 1 mistake out of 6. On the other hand for feature set {x3, x4, x5}, no good correlation exists between the feature values and the class labels.\nThe research problem in the above task is the selection of optimal feature set for utility preserving entity anonymization, where the utility is considered with respect to the classification performance. In existing works, k-anonymity is achieved by suppression or generalization of cell values, whereas we consider to achieve the same by selecting a subset of features, yet ensuring that the selected features retain the classification utility of the dataset. This is a challenging task, as it is well-known that privacy is always at odd with the utility of a knowledge-based system [14, 15], so finding the right balance is a difficult task. Besides, feature selection itself, without considering the privacy constraint, is an NP-Hard\nproblem [12]."}, {"heading": "1.1 Our Contributions", "text": "In this work, we consider the task of feature selection under privacy constraint. Given a classification dataset with binary features, our proposed solution finds a subset of features such that after projecting each instance on these subsets each entity in the dataset satisfies a privacy constraint, called k-anonymous by Selection (AS), which we propose in this work. AS is an adapted version of k-anonymity, which strikes the correct balance between disclosure risk and dataset utility, and it is particularly suitable for high dimensional binary data. We also propose two algorithms: Maximal and Greedy. The first is a maximal itemset mining based method and the second is a greedy incremental approach, both respecting the user-defined AS constraints.\nThe algorithms that we propose are particularly intended for high dimensional sparse microdata where the features are binary. The nature of such data is different from a typical dataset that is considered in many of the existing works on privacy preserving data disclosure mechanism. The first difference is that existing works consider two kinds of attributes, sensitive and nonsensitive, whereas for our dataset all attributes are sensitive, and any subset of attributes can be used by an attacker to de-anonymize one or more entities in the dataset using probabilistic inference methodologies. Second, in this work, we only consider binary attributes, which enable us to provide efficient algorithms and an interesting anonymization protocol. Nevertheless, as shown in [22], binary attributes are sufficient for an attacker to de-anonymize a person using high dimensional microdata. Besides, such binary attributes are adequate when modeling online behavior of a person , such as \u2018like\u2019 in Facebook, \u2018bought\u2019 in Amazon, and \u2018click\u2019 in Google advertisement. Also, collecting explicit user feedback in terms of frequency data (say, the number of times a search keyword is used ) may be costly in many online platforms. The contributions of our work are summarized as below:\n1. We propose a novel method for entity anonymization using feature selection over a set of binary attributes from a two-class classification dataset. For this, we design a new anonymization metric, named k-anonymity by selection (AS), which is particularly suitable for highdimensional binary microdata.\n2. We propose two methods for solving the above task and show experimental results to validate the effectiveness of these methods.\n3. We show the utility of the proposed methods with four real-life applications; specifically, we show how the privacyaware feature selection affects the performance of these applications."}, {"heading": "2. PRIVACY BASICS", "text": "Given a dataset (not necessarily binary) D, where each row corresponds to a person, and each column contains nonpublic information about that person; examples include disease, medication, sexual orientation, etc. In the context of online behavior, the search keywords, or purchase history of a person may be such information. Privacy preserving data publishing methodologies make it difficult for an attacker to de-anonymize the identity of a person who is in the dataset.\nFor de-anonymization, an attacker generally uses a set of attributes that act almost like a key and it uniquely identifies some individual in the datasets. These attributes are called quasi-identifiers. k-Anonymity is a well-known privacy metric.\nDefinition 2.1 (k-Anonymity). A dataset D satisfies k-anonymity if for any row entity e \u2208 D there exist at least k\u2212 1 other entities that have the same values as e for every possible quasi-\nidentifiers.\nThe database in Table 1 is not 2-anonymous, as the row entity e3 is unique considering the entire attribute-set {x1, x2, x3, x4, x5} as quasi-identifier. On the other hand, It is 2- anonymous for both the datasets (one with Feature Set-1 and the other with Feature Set-2) in Table 2. For numerical or categorical attributes, a process, called generalization and/or suppression (row or cell value) are used for achieving k-anonymity. Generalization partitions the values of an attribute into disjoint buckets and identify each bucket with a value. Suppression either hides the entire row or some of its cell values, so that the anonymity of that entity can be maintained. Generalization and suppression make anonymous group, where all the entities in that group have the same value for every possible quasi-identifier, and for a dataset to be kanonymous, the size of each of such groups is at least k. In this work we consider binary attributes, for which value based generalization is not possible.\nThere are some security attacks against which k-anonymity is vulnerable. For example, k-Anonymity does not provide statistical guaranty about anonymity which can be obtained by using differential privacy\u2014a method which adds statistical noise to minimize the chance of identification while maximizing the query accuracy. For binary data as only two feature values (true, and false) are allowed, none of the above methods are much useful.\nA key concern in data anonymization is the loss of utility. In generalization based k-anonymization the loss occurs due to the decrement of data variance or due to the loss of discernibility. Suppression of a row is also a loss as in this case the entire row entity is not discernible for any of the remaining entities in the dataset. However, both generalization and suppression based techniques don\u2019t consider the utility of data for a classification task after the anonymization process. For classification data, Iyengar [13] has proposed a metric called CM (Classification Metric) which is defined as below.\nDefinition 2.2 (Classification Metric [13]). Classification metric (CM) is a utility metric for classification dataset, which assigns a penalty of 1 for each suppressed entity, and for each non-suppressed entity it assigns a penalty of 1 if those entities belong to the minority class within its anonymous group. CM value is equal to the sum of penalties over all the entities.\nIyengar\u2019s method solves k-anonymization through generalization and suppression while minimizing the CM metric value using a genetic algorithm which provides no optimality guaranty."}, {"heading": "3. PROBLEM STATEMENT", "text": "Given a classification dataset with binary attributes, our objective is to find a subset of attributes which increase the non-disclosure protection of the row entities, and at the same time maintain the classification utility of the dataset without\nUser x1 x2 x3 x4 x5 Class\ne1 1 0 1 0 1 +1 e2 1 0 1 0 1 \u22121 e3 1 0 0 1 1 +1 e4 1 0 1 0 1 +1 e5 1 1 1 0 1 \u22121 e6 1 1 0 1 1 \u22121\nsuppressing any of the row entities. In this section we will provide a formal definition of the problem.\nWe define a database D(E, I) as a binary relation between a set of entities (E) and a set of features (I); thus, D \u2286 E\u00d7I , where E = {e1, e2, \u00b7 \u00b7 \u00b7 , en} and I = {x1, x2, \u00b7 \u00b7 \u00b7 , xd}; n and d are the number of entities and the number of features, respectively. The database D can also be represented as an n \u00d7 d binary data matrix, where the rows correspond to the entities, and the columns correspond to the features. For an entity ei \u2208 E, and a feature xj \u2208 I , if \u3008ei, xj\u3009 \u2208 D, the corresponding data matrix entry D(ei, xj) = 1, otherwise D(ei, xj) = 0. Thus each row of D is a binary vector of size d in which the 1 entries correspond to the set of features with which the corresponding row entity is associated. In a classification dataset, besides the attributes, the entities are also associated to a class label which is a category value. In this task we assume a binary class label {C1, C2}. A typical supervised learning task is to use the features I to predict the class label of an entity.\nWe say that an entity ei \u2208 E contains a set of features X = {x1, x2, \u00b7 \u00b7 \u00b7 , xl}, if D(ei, xk) = 1 for all k = 1, 2, \u00b7 \u00b7 \u00b7 , l; set X is also called containment set of the entity ei.\nDefinition 3.1 (Containment Set). Given a binary dataset, D(E, I), the containment set of a row entity e \u2208 E, represented as CSD(e), is the set of attributes X \u2286 I such that \u2200x \u2208 I,D(e, x) = 1\nWhen the dataset D is clear from the context we will simply write CS(e) instead of CSD(e) to represent the containment set of e.\nDefinition 3.2 (k-Anonymity by Selection). In a binary dataset D(E, I) and for a given positive integer k, an entity e \u2208 E satisfies k-anonymity by selection if there exists a set of entities F \u2286 E, such that e /\u2208 F \u2227|F | \u2265 k\u22121\u2227\u2200f \u2208 F,CSD(f) \u2287 CSD(e). In other words, their exist at least k \u2212 1 other entities in D such that their containment set is the same or a superset of CSD(e).\nBy definition, if an entity satisfies k-anonymity by selection, it satisfies the same for all integer values from 1 upto k. We\nuse the term AS(e) to denote the largest k for which the entity e satisfies k-anonymous by selection.\nDefinition 3.3 (k-Anonymous by Selection Group). For a binary dataset D(E, I), if e \u2208 E satisfies the k-anonymity by selection, k-anonymous by selection group with respect to e exists and this is F \u222a {e}, where F is the largest possible set as is defined in Definition 3.2.\nDefinition 3.4 (k-Anonymous by Selection Dataset). A binary dataset D(E, I) is k-anonymous by selection if every entity e \u2208 E satisfies k-anonymity by selection.\nWe extend the term AS over a dataset as well, thus AS(D) is the largest k for which the dataset D is anonymous. Example: For the dataset in Table 1, CS(e1) = {x1, x3, x5}. Entity e1 satisfies 4-anonymity by selection, because for each of the following three entities e2, e4, and e5, their containment set is the same or a superset of CS(e1). But, the entity e6 only satisfies 1-anonymity by selection, as besides itself no other entity contains CS(e6) = {x1, x2, x4, x5}. 4-anonymous by selection group of e2 exists, and it is {e1, e2, e4, e5}, but 5-anonymous by selection group for the same entity does not exist. The dataset in Table 1 is 1-anonymous by selection because there exist one entity, namely e6 such that the highest k-value for which e6 satisfies k-anonymity by selection is 1; alternatively AS(D) = 1\nk-Anonymity by selection is the privacy metric that we use in this work. The argument for this metric is that if a large number of other entities contain the same or super feature subset which an entity e contains, the disclosure protection of the entity e is strong, and vice versa. Thus a higher value of k stands for a higher level of privacy for e. k-Anonymity by selection (AS) is similar to k-anonymity for binary feature set except that for AS only the \u20181\u2019 value of feature set is considered as a privacy risk. For example, entity e1 satisfies 4-anonymity by selection, but it satisfies only 3-anonymity because except e1, only two other entities e2 and e4 share the same values for the quasi-identifier set {x1, x2, x3, x4, x5}.\nFor disclosure protection in a high dimensional microdata with binary attributes k-anonymity by selection is an adequate privacy metric, because the absence of an attribute in a containment set of an entity reveals negligible information about the entity. For example, if the dataset is about the search keywords that a set of users have used over a given time, for a person having a 0 value under a keyword reveals no sensitive information about that person. Also note that, in traditional datasets, only a few attributes which belong to non-sensitive group are assumed to be quasi-identifier, so a privacy metric, like k-anonymity works well for such dataset. But, in our case dataset is high dimensional, and quasi-identifiers can be any subset of attributes; in such a scenario, k-anonymity is severely restrictive and is not much useful for performing feature selection task as only a few features pass this criteria . On the other hand, AS based privacy metrics enables selection of sufficient features for retaining the classification utility of the dataset.\nFeature selection [12] for a classification task is to select a subset of highly predictive variables so that classification accuracy possibly improves which happens due to the fact that contradictory or noisy attributes are generally ignored during the feature selection step. For a dataset D(E, I), and a feature-set S \u2286 I , following relational algebra notations, we use \u03a0S(D) to denote the projection of database D over the feature set S. Now, given a user-defined integer number\nk, our goal is to perform an optimal feature selection on the dataset D to obtain \u03a0S(D) which satisfies two objectives: first, \u03a0S(D) is k-anonymous by selection, i.e., AS(\u03a0S(D)) \u2265 k; second, \u03a0S(D) maintains the predictive performance of the classification task as much as possible. For denoting the predictive performance of a dataset (or a projected dataset) we define a classification utility function f . The higher the value of f , the better the dataset for the classification. We consider f to be a filter based feature selection criteria which is independent of the classification model that we use.\nThe formal research task of this work is as below. Given a binary dataset D(E, I), and an integer number k, find S \u2282 I so that f(\u03a0S(D)) is maximized under the constraint that AS(\u03a0S(D)) \u2265 k. Mathematically,\nmaximize S\u2286I f(\u03a0S(D)) subject to AS(\u03a0S(D)) \u2265 k (1)\nSince the general task of feature selection even for the case of linearly separable positive and negative examples is NPHard [2], the problem 1 is also NP-Hard, because by setting k = 0, we can reduce the feature selection problem to this problem. Given the intractability of this problem, in this paper, we propose two effective local optimal solutions for this problem."}, {"heading": "4. METHODS", "text": "In this section, we describe two algorithms, namely Maximal and Greedy that we propose for the task of feature selection under privacy constraint. Maximal is a maximal itemset mining based feature selection method, and Greedy is a greedy method with privacy constraint based filtering."}, {"heading": "4.1 Maximal Itemset based Approach", "text": "A key observation regarding k-anonymity by selection (AS) of a dataset is that this criteria satisfies the downward-closure property under feature selection. The following lemma holds:\nLemma 4.1. Say D(E, I) is a binary dataset and X \u2282 I and Y \u2282 I are two feature subsets. If X \u2286 Y , then AS(\u03a0X(D)) \u2265 AS(\u03a0Y (D)). Proof: Let\u2019s prove by contradiction. Suppose X \u2286 Y and AS(\u03a0X(D)) < AS(\u03a0Y (D)). Then from the definition of AS, there exists at least one entity e \u2208 E for which AS(\u03a0X(e)) < AS(\u03a0Y (e)). Now, let\u2019s assume AX and AY are the set of entities which make the anonymous by selection group for the entity e in \u03a0X(D) and \u03a0Y (D), respectively. Since AS(\u03a0X(e)) < AS(\u03a0Y (e)), |AX | < |AY |; so there exists an entity p \u2208 AY \\ AX , for which CS\u03a0Y (D)(p) \u2287 CS\u03a0Y (D)(e) and CS\u03a0X(D)(p) 6\u2287 CS\u03a0X (D)(e); But this is impossible, because X \u2286 Y , if CS\u03a0Y (D)(p) \u2287 CS\u03a0Y (D)(e) holds, then CS\u03a0X(D)(p) \u2287 CS\u03a0XD(e) must be true. Thus, the lemma is proved by contradiction. .\nLet\u2019s call the collection of feature subsets which satisfy the AS threshold for a given k, the feasible set and represent it with Fk. Thus, Fk = {X | X \u2286 I \u2227 AS(\u03a0X(D)) \u2265 k}. A subset of features X \u2208 Fk is called maximal if it has no supersets which is feasible. Let Mk be the set of all maximal subset of features. Then Mk = {X | X \u2208 Fk\u2227 6 \u2203Y \u2283 X, such that Y \u2208 Fk}. As we can observe given an integer k, if there exists a maximal feature set Z that satisfies the AS constraint, then any feature set X \u2286 Z, also satisfies the same AS constraint, i.e., k \u2264 AS(\u03a0Z(D)) \u2264 AS(\u03a0X(D)) if X \u2286 Z \u2208 Mk based on the Lemma 4.1.\nExample: For the dataset in Table 1, the 2-anonymous by selection feasible feature set 1 F2 = {x1, x2, x3, x4, x5, x1x2, x1x3, x1x4, x1x5, x2x5, x3x5, x4x5, x1x2x5, x1x3x5, x1x4x5} andM2 = {x1x2x5, x1x3x5, x1x4x5}. In this dataset, the feature-set x2x3 /\u2208 F2 because in \u03a0x2x3(D), CS(e5) = {x2, x3} and the size of the k-anonymous by selection group of e5 is 1; thusAS(\u03a0x2x3(D)) = 1 < 2. On the other hand for feature-set x1x2x5, the projected dataset \u03a0x1x2x5(D) has two k-anonymous by selection groups, which are {e1, e2, e3, e4, e5, e6} and {e5, e6}; since each group contains at least two entities, AS(\u03a0x1x2x5(D)) = 2\nLemma 4.2. Say, D(E, I) is a binary dataset, and T is its transaction representation where each entity e \u2208 E is a transaction consisting of the containment set CSD(e). Frequent itemset of the dataset T with minimum support threshold k are the feasible feature set Fk for the optimization problem 1. Proof: Say, X is a frequent itemset in the transaction T for support threshold k. Then, the support-set of X in T are the transactions (or entities) which contain X. Since, X is frequent, the support-set of X consists of at least k entities. In the projected dataset \u03a0X(D), all these entities make a kanonymous by selection group, thus satisfying k-anonymity by selection. For each of the remaining entities (say, e), e\u2019s containment set contains some subset of X (say Y ) in \u03a0X(D). Since, X is a frequent itemset and Y \u2282 X, Y is also frequent with a support-set that has at least k entities. Then e also belongs to a k-anonymous by selection group. Thus, each of the entities in D belongs to some k-anonymous by selection group(s) which yields: X is frequent \u21d2 X \u2208 Fk. Hence proved.\nA consequence of Lemma 4.1 is that for a given dataset D, k and a feature set S, if AS(\u03a0S(D)) \u2265 k, then any subset of S (say, R) satisfies AS(\u03a0R(D)) \u2265 k. This is identical to the downward closure property of frequent itemset mining. Also, Lemma 4.2 confirms that any itemset that is frequent in the transaction representation of D for a minimum support threshold k is a feasible solution for 1. Hence, Apriori [30] like algorithm for itemset mining can be used for effectively enumerating all the feature subsets of D which satisfies the required k-anonymity by selection constraint.\n4.1.1 Maximal Feasible Feature Set Generation For large dataset, the feasible feature set Fk which consists\nof feasible solutions for the optimization problem 1 can be very large. One way to control its size is by choosing appropriate k; if k increases, |Fk| decreases, and vice-versa, but choosing a large k negatively impacts the classification utility of the dataset, thus reducing the optimal value of Problem (1). A brute force method for finding the optimal feature set S is to enumerate all the feature subset in F and find the one that is the best given the utility criteria f . However, this can be very slow. So, Maximal generates all possible maximal feature sets Mk instead of generating Fk and search for the best feature subset within Mk. The idea of enumerating Mk instead of Fk comes from the assumption that with more features the classification performance will increase; thus, the size of the feature set is its utility function value; i.e., f(\u03a0S(D)) = |S|, and in that case the largest set in Mk is the solution to the problem 1.\nAn obvious advantage of working only with the maximal feature set is that for many datasets, |Mk| << |Fk|, thus\n1To enhance the readability, we write the feature set as string; for example, the set {x1, x2} is written as x1x2.\nfinding solution within Mk instead of Fk leads to significant savings in computation time. Just like the case for frequent itemset mining, maximal frequent itemset mining algorithm can also be used for finding Mk. Any off-the-shelf software can be used for this. Maximal uses the LCM-Miner package provided in [25] which, at present, is the fastest method for finding maximal frequent itemsets.\n4.1.2 Classification Utility Function The simple utility function f(\u03a0S(D)) = |S| has a few lim-\nitations. First, the ties are very commonplace, as there are many maximal feature sets that have the same size. Second, and more importantly, this function does not take into account the class labels of the instances so it cannot find a feature set that maximizes the separation between the positive and negative instances. So, we consider another utility function, named as HamDist, which does not succumb much to the tie situation. It also considers the class label for choosing features that provide good separation between the positive and negative classes.\nDefinition 4.1 (Hamming Distance). For a given binary database D(E, I), and a subset of features, S \u2286 I, the Hamming distance between \u03a0S(a) and \u03a0S(b) is defined as below:\ndH(\u03a0S(a),\u03a0S(b)) =\n|S| \u2211\nj=1\n1{asj 6= bsj ,\u2200sj \u2208 S} (2)\nwhere 1{X} is the indicator function, and asj and bsj are the sjth feature value under S for the entities a and b, respectively.\nWe can partition the entities in D(E, I) into two disjoint subsets, E1 and E2; entities in E1 have a class label value of C1, and entities in E2 have a class label value of C2.\nDefinition 4.2 (HamDist). Given a dataset D(E = E1 \u222a E2, I) where the partition E1 and E2 are based on class labels, The classification utility function HamDist for a feature subset S \u2286 I is the average Hamming distance between all pair of entities a and b such that a \u2208 E1 and b \u2208 E2.\nHamDist(S) = 1\n| E1 || E2 |\n\u2211\na\u2208E1,b\u2208E2\ndH(\u03a0S(a),\u03a0S(b)) (3)\nExample: For the dataset in Table 1, for its projection on x1x2x5 (see, Table 2), distance of e1 from the negative entities are 0+1+1 = 2, and the same for the other positive entities, e3 and e4 also. So, HamDist(x1x2x5) = ((0 + 1 + 1) + (0 + 1 + 1) + (0 + 1 + 1))/9 = 6/9. From the same table we can also see that HamDist(x3x4x5) = 8/9.\nAs we can observe from Equations 2 and 3, the utility function HamDist(S) reflects the discriminative power between classes given the feature set S. The larger the value of HamDist(S), the better the quality of selected feature set S to distinguish between classes. Another separation metric similar to HamDist is Distinguish Count, which is defined as below.\nDefinition 4.3 (DistCnt). For D(E1\u222aE2, I), and S \u2286 I, the Distinguish Count (DistCnt) is the number of pairs from E1 and E2 which can be distinguished using at least one feature in S. Mathematically,\nDistCnt(S) = 1\n| E1 || E2 |\n\u2211\na\u2208E1,b\u2208E2\n1{\u03a0S(a) 6= \u03a0S(b)} (4)\nDistCnt can also be used instead of HamDist in the Maximal algorithm. Note that, we can also use CM criterion (see, Definition 2.2) instead of HamDist; however, experimental results show that CM performs much poorly in terms of AUC. Besides, both HamDist and DistCnt functions have some good properties (will be discussed in Section 4.2) which CM does not have.\nThe Maximal algorithm utilizes classification utility metrics (HamDist or DistCnt) for selecting the best feature set from the maximal set Mk. For some datasets, the size of Mk can be large and selecting the best feature set by applying the utility metric on each element of Mk can be time-consuming. Then, we can find the best feature set among the largest sized element in Mk. Another option is to consider the maximal feature sets in Mk in the decreasing order of their size in such a way that at most r of the maximal feature sets from set Mk are chosen as candidate for which the utility metric computation is performed. In this work we use this second option by setting r = 20 for all our experiments.\nAlgorithm 1 Maximal Itemset Mining Based Feature Selection Method Input: D(E, I), k, r Output: S 1: Calculate maximal feature set Mk which contains the\nfeature-set that satisfies k-anonymity by selection for the given k 2: Select best feature-set S based on the HamDist criteria by considering r largest-sized feature set in Mk. 3: return S"}, {"heading": "4.1.3 Maximal Itemset based Method (Pseudo-code)", "text": "The pseudo-code of the entire process for Maximal is given\nin Algorithms 1. Maximal takes integer number k and the number of maximal patterns r as input and returns the final feature set S which satisfies k-anonymity by selection. Line 1 uses the package used in [25] to generate all the maximal feature sets that satisfy k-anonymity by selection for the given k value. Line 2 groups maximal feasible feature sets according to its size and selects top r maximal feature sets with the largest size and build the candidate feature sets. Then the algorithm computes the feature selection criteria HamDist of each feature set in the candidate feature sets and returns the best feature set that has the maximum value for this criteria.\nThe complexity of the above algorithm, predominantly depends on the complexity of the maximal itemset mining step (Line 1), which depends on the input value k. For larger k, the privacy is stronger and it reduces Mk making the algorithm run faster, but the classification utility of the dataset may suffer. On the other hand, for smaller k, Mk can be large making the algorithm slower, but it better retains the classification utility of the dataset."}, {"heading": "4.2 Greedy with Modular and Sub-Modular Objective Functions", "text": "A potential limitation of Maximal is that for dense dataset this method can be slow. So, we propose a second method, called Greedy which runs much faster as it greedily adds a new feature to an existing feasible feature-set. For greedy criteria, Greedy can use different separation functions which discriminate between positive and negative instances. In this work we use HamDist (See Definition 4.2) and DistCnt (See Defi-\nnition 4.3). Thus Greedy solves the Problem (1) by replacing f by either of the two functions. Because of the monotone property of these functions, Greedy ensures that as we add more features, the objective function value of (1) monotonically increases. The process stops once no more features are available to add to the existing feature set while ensuring the desired AS value of the projected dataset.\n4.2.1 Submodularity, and Modularity\nDefinition 4.4 (Submodular Set Function). Given a finite ground set U , a monotone function f that maps subsets of U to a real number f : 2U \u2192 R is called submodular if\nf(S \u222a {u}) \u2212 f(S) \u2265 f(T \u222a {u}) \u2212 f(T ),\u2200S \u2286 T \u2286 U, u \u2208 U"}, {"heading": "If the above condition is satisfied with equality, the function is called modular.", "text": "Theorem 4.3. HamDist is monotone, submodular, and modular.\nProof. For a Dataset D(E, I), S \u2286 I , and T \u2286 I are two arbitrary feature-sets, such that S \u2286 T . E = E1 \u222a E2, where the partition is based on class label. Consider the pair (a, b), such that a \u2208 E1 and b \u2208 E2. Let, w(\u00b7) is a function that sums the Hamming distance over all such pairs (a, b) for a given feature subset S. Thus, w(S) = \u2211\na\u2208E1,b\u2208E2 dH(\u03a0S(a),\u03a0S(b)),\nwhere the function dH is the Hamming distance between a and b as is defined in Equation 2. Similarly we can define w(T ), for the feature subset T . Using Equation 2, dH(\u03a0S(a),\u03a0S(b)) is the summation over each of the features in S. Since S \u2286 T , dH(\u03a0T (a),\u03a0T (b)) includes the sum values for the variables in S and possibly includes the sum value of other variables, which is non-negative. Summing over all a, b pairs yields w(S) \u2264 w(T ). So, HamDist is monotone. Now, for a feature u /\u2208 T ,\nw(S \u222a {u}) = \u2211\na\u2208E1,b\u2208E2\ndH ( \u03a0S\u222a{u}(a),\u03a0S\u222a{u}(b) )\n= \u2211\na\u2208E1,b\u2208E2\n\u2211\nsj\u2208S\u222a{u}\n1{asj 6= bsj}(using Eq. 2)\n= \u2211\na\u2208E1,b\u2208E2\n\n\n\u2211\nsj\u2208S\n1{asj 6= bsj}+ 1{au 6= bu}\n\n\n= w(S) + w({u})\nSimilarly, w(T \u222a {u}) = w(T ) + w({u}). Then, we have w({u}) = w(S \u222a {u}) \u2212 w(S) = w(T \u222a {u}) \u2212 w(T ). Dividing both sides by 1/(|E1| \u00b7 |E2|) yields HamDist(S \u222a {u}) \u2212 HamDist(S) = HamDist(T \u222a {u}) \u2212 HamDist(T ). Hence proved with the equality.\nTheorem 4.4. DistCnt is monotone, and submodular.\nProof. Given, a dataset D(E, I) where E is partitioned as E1\u222aE2 based on class label. Now, consider a bipartite graph, where vertices in one partition (say, V1) corresponds to features in I , and the vertices of other partition (say, V2) corresponds to a distinct pair of entities (a, b) such that a \u2208 E1, and b \u2208 E2; thus, |V2| = |E1| \u00b7 |E2|. If for a feature x \u2208 V1, we have ax 6= bx, an edge exists between the corresponding vertices x \u2208 V1 and (a, b) \u2208 V2. Say, S \u2286 V1 and T \u2286 V1 and\nS \u2286 T . For a set of vertices, \u0393(\u00b7) represents their neighborlist. Since, the size of neighbor-list of a vertex-set is monotone and submodular, for u /\u2208 T , we have |\u0393(S)| \u2264 |\u0393(T )|, and |\u0393(S \u222a {u})| \u2212 |\u0393(S)| \u2265 |\u0393(T \u222a {u})| \u2212 |\u0393(T )|. By construction, for a feature set, S, \u0393(S) contains the entity-pairs for which at least one feature-value out of S is different. Thus, DistCnt function is |\u0393(\u00b7)| |V2| and it is submodular.\nAlgorithm 2 Greedy Algorithm for HamDist\nInput: D(E, I), k Output: S 1: Sort the features in non-increasing order based on their\nhamDist, denoted as Fsorted 2: S = \u2205 3: for each feature x \u2208 Fsorted do 4: if AS(\u03a0S\u222a{x}(D)) \u2265 k then 5: S = S \u222a {x} 6: else 7: break 8: end if 9: end for 10: return S\nTheorem 4.5. For monotone submodular function f , let S be a set of size k obtained by selecting elements one at a time, each time choosing an element provides the largest marginal increase in the function value. Let S\u2217 be a set that maximizes the value of f over all k-element sets. Then f(S) \u2265 (1 \u2212 1 e )f(S\u2217); in other words, S provides (1 \u2212 1 e )-approximation. For modular function f(S) = f(S\u2217) [21].\n4.2.2 Greedy Method (Pseudo-code) Using the above theorem we can design two greedy algo-\nrithms, one for modular function HamDist, and the other for submodular function DistCnt. The pseudo-code of these algorithms are shown in Algorithm 2 and Algorithm 3. Both the methods take binary dataset D and integer value k as input and generates the selected feature set S as output. Initially S = \u2205. For modular function, the marginal gain of an added feature can be pre-computed, so Algorithm 2 first sorts the features in non-increasing order of their HamDist value, and greedily adds features until it encounters a feature such that its addition does not satisfy the AS constraint. For submodular function DistCnt, margin gain cannot be precomputed, so Algorithm 3 selects the new feature by iterating over all the features and finding the best one (Line 5 -11). The terminating condition of this method is also identical to the Algorithm 2. Since the number of features is finite, both the methods always terminate with a valid S which satisfies AS(\u03a0S(D)) \u2265 k.\nCompared to Maximal, both the greedy methods are faster. With respect to the number of features (d), Algorithm 2 runs in O(d lg d) time and Algorithm 3 runs in O(d2) time. Also, using Theorem 4.5, Algorithm 2 returns the optimal size |S| feature-set, and Algorithm 3 returns S, for which the objective function value is (1 \u2212 1/e) optimal over all possible size-|S| feature sets."}, {"heading": "5. EXPERIMENTS AND RESULTS", "text": "In order to evaluate the result of our proposed methods we perform various experiments. Our main objective in these\nAlgorithm 3 Greedy Algorithm for DistCnt\nInput: D(E, I), k Output: S 1: T = \u2205 2: repeat 3: S = T 4: \u2206Hmax = 0.0 5: for u \u2208 I \\ S do 6: Compute \u2206H = DistCnt(S \u222a {u}) \u2212DistCnt(S) 7: if \u2206H > \u2206Hmax then 8: \u2206Hmax = \u2206H 9: um = u 10: end if 11: end for 12: T = S \u222a {um} 13: until AS(\u03a0T (D)) \u2265 k 14: return S\nexperiments is to validate how the performance of the privacy preserving classification task varies as we change the value of AS\u2014user-defined privacy threshold metric. We also compare the performance of our proposed utility preserving anonymization methods with other existing anonymization methods. We use four real-world datasets for our experiments. All four datasets consist of entities that are labeled with 2 classes. The number of entities, the number of features, the distribution of the two classes (#positive and # negative), and the dataset density (fraction of non-zero cell values) are shown in Table 3."}, {"heading": "5.1 Privacy-preserving Classification Tasks", "text": "Below, we discuss the datasets and the privacy preserving classification tasks that we solve using our proposed methods.\nEntity Disambiguation (ED). The objective of this classification task is to identify whether the name reference at a row in the data matrix maps to multiple real-life persons or not. Such an exercise is quite common in National Security Agency (NSA) for disambiguating multiple suspects from their digital footprints. Privacy of the people in such a dataset is important as many innocent persons can also be listed in such a dataset as a suspect. Given a set of keywords that are associated with a name reference, we build a binary data matrix for solving the ED task. We use Arnetminer2 academic publication data. In this dataset, each row is a name reference of one or multiple researchers, and each column is a research keyword within computer science research umbrella. A \u20181\u2019 in a cell represents that the name reference in the corresponding row has used the keyword in her (or their) published works. In this way we simulate a NSA dataset, where each row is a name reference and each column is a set of keywords that can be associated with that name. In our dataset, there are\n2http://arnetminer.org\n148 rows which are labeled such that half of the people in this dataset are pure entity (a negative case), and the rest of them are multi-entity (a positive case). The dataset contains 552 attributes (keywords).\nTo solve the entity disambiguation problem we first perform topic modeling over the keywords and then compute the distribution of entity u\u2019s keywords across different topics. Our hypothesis is that for a pure entity the topic distribution will be concentrated on a few related topics, but for an impure entity (which is composed of multiple real-life persons) the topic distribution will be distributed over many non-related topics. We use this idea to build a simple classifier which uses an entropy-based score E(u) for an entity u. E(u) is defined as below:\nE(u) = \u2212\n|T | \u2211\nk=1\nP (u | Tk) logP (u | Tk) (5)\nwhere P (u | Tk) is the probability of u belonging to topic Tk, and |T | represents the pre-defined number of topics for topic modeling. Clearly, for a pure entity the entropy-based score E(u) is relatively smaller than the same for a non-pure entity. We use this score as our predicted value and compute AUC (area under ROC curve) to report the performance of the classifier. No other classification tools are used for this classification task. Adult. The Adult benchmark dataset is based on census data and has been widely used in earlier works on k-anonymization [13]. For our experiments, we use eight of the original attributes; these are age, work class, education, marital status, occupation, race, gender, and hours-per-week. The classification task is to determine whether a person earns over 50K a year or not. Among all of the attributes, gender is originally binary. For the other attributes, we make them as binary for our purpose. For example, for marital attribute, we consider never-married (1) versus others (0). For race attribute, we consider white (1) versus others (0). For the numerical attributes, we cut them into different categories and consider a binary attribute for each category. In this way, we have a total of 19 attributes for the Adult dataset. As we can see privacy of the individuals in such a dataset is quite important as many people consider their personal data, such as race, gender, marital status and so on as sensitive attributes and they are not willing to release them to public. German The German benchmark dataset is based on census data and is widely used in privacy preserving data mining community. For our work, we use all the original attributes, which are discussed in the UCI ML repository. The classification task is to determine whether a customer has good credit behavior or not. Attribute pre-processing step is similar to the Adult data. After data processing, we have a total of 84 features for German dataset. Again, privacy is critical in such a dataset as most people consider their financial information as sensitive attributes. Email The last dataset, namely Email dataset is a collection of approximately 1099 personal email messages distributed in 10 different directories. Each directory contains both legitimate and spam messages. To respect privacy issue, each token including word, number, and punctuation symbol is encrypted by a unique number. The classification task is to distinguish the spam email with nonspam email. We use this data to mimic microdata (such as twitter or Facebook messages) classification. Privacy is important in such a dataset as keyword based features in a micro-message can potentially identify a\nperson. In the dataset, each row is an emaill message, and each column denotes a token. A \u20181\u2019 in a cell represents that the row reference contains the token in the email message."}, {"heading": "5.2 Experimental Setting", "text": "For our experiments, we vary the k value of the proposed anonymity by selection (AS) metric and run Maximal and different variants of Greedy independently for building projected classification datasets for which AS value is at least k. We use the names HamDist and DistCnt for the two variants of Greedy (Algorithm 2 and 3), which optimize Hamming distance and Distinguish count greedy criteria, respectively. As we mentioned earlier, k-anonymity based method imposes strong restriction which severely affects the utility of the dataset. To demonstrate that, instead of using AS, we utilize k-Anonymity as our privacy criteria for different variants of Greedy. We call these competing methods k-Anonymity HamDist, and k-Anonymity DistCnt. We also use three other methods for comparing their performance with the performance of our proposed methods. We call these competing methods Non-Private Greedy, CM Greedy [13], and RF [6]. We discuss these methods below.\nThe Non-private Greedy method is an unconstrained counterpart of our Greedy algorithm, which employs the greedy criterion but does not enforce the AS constraint. For a fair comparison with Greedy, for Non-private Greedy we use the same number of features as is obtained in our methods. Since, the number of features in our greedy methods HamDist and DistCnt can be different, for Non-private Greedy we use the higher value among these two numbers. Thus, for a given number of features, the Non-private Greedy is the least restrictive and the performance difference between this method and our proposed methods is owing to the utility loss for enforcing privacy. RF is a Randomization Flipping based kanonymization technique presented in [6], which randomly flips the feature value such that each instance in the dataset satisfies the k-anonymity privacy constraint. RF uses clustering such that after random flipping operation, each cluster has at least k entities with the same feature values with respect to the entire feature set. Thus, RF ensures the privacy without making any explicit effort for maintaining the utility of the dataset. CM greedy represents another greedy based method which uses Classification Metric utility criterion proposed in [13] (See definition 2.2). Iyenger [13] uses a genetic algorithm for the classification task, but for a fair comparison we use CM criterion in the Greedy algorithm and with the selected features we use identical setup for classification.\nFor all the algorithms and all the datasets (except ED) we use the LibSVM to perform SVM classification using L2 loss with 5-fold cross validation. The only parameter for libSVM is regularization-loss trade-off C which we tune using a small validation set. For each of the algorithms, we report AUC and the number of selected features. For RF method, it selects all the features, so for this method we report the percentage of cell values for which the bit is flipped. Since RF is a randomized method, for each dataset we run RF 5 times and report the average AUC. We use different k-anonymity by selection (AS) values in our experiments. For practical k-anonymization, k value between 5 and 10 is suggested in earlier works [24]; we use three different k values, which are 5, 8 and 11. For each result table in the following sections, we also highlight the best results in terms of AUC among all methods under same k setting. We run all the experiments\non a 2.1 GHz Machine with 4GB memory running Linux operating system."}, {"heading": "5.3 Name Entity Disambiguation", "text": "In Table 4 we report the AUC value of anonymized name entity disambiguation task using various privacy methods (in rows) for different k values (in columns). For better comparison, our proposed methods, competing methods, and nonprivate methods are grouped by the horizontal lines: our proposed methods are in the top group, the competing methods are in the middle group, and non-private methods are in the bottom group. For each method, we also report the count of selected features within parenthesis. Since RF method uses the full set of features; for this method the value in the parenthesis is the percent of cell values that have been flipped. We also report the AUC performance using full feature set (last row), since no privacy restriction is imposed this value is independent of k.\nAs we see, for most of the methods increasing k decreases the number of selected features, which translates to poorer classification performance; this validates the privacy-utility trade-off. However, for a given k, our proposed methods perform better than the competing methods in terms of AUC metric for all different k values. For instance, for k = 5, the AUC result of RF and CM Greedy are only 0.75, and 0.68, whereas different versions of proposed Greedy obtain AUC values between 0.81 and 0.88. Among the competing methods, CM Greedy performs the worst (0.55 for all k) and kAnonymity DistCnt performs the best (0.79 for k=3); yet all perform much poorer than our proposed methods. A reason for this may be the competing methods are too restrictive, as we can see that they are able to select only 2 to 3 features for various k values, whereas our proposed methods that use k-anonymity by selection are able to select between 11 and 61 features, which help our methods to retain classification utility. For this data, RF performs better than other competing methods, yet poorer than all of our methods. We observe that the performance of RF is largely dependent on the percentage of flips in the cell-value; if this percentage is large, the performance is poor. As k increases, with more privacy requirement, the percentage of flips increases, and the AUC drops.\nFor a sparse dataset like the one that we use for entity disambiguation, feature selection helps classification performance. In this dataset using full set of features (no privacy), we obtain only 0.87 AUC value, whereas using less than 10% of features we can achieve comparable or better accuracy using our proposed methods (when k=5). Even for k = 11, our methods retain substantial part of the classification utility of\nthe dataset and obtain AUC value of 0.81 (see second row), which is very close to 0.87. Also, note that ourHamDist algorithm guaranties that for the given feature count this method obtains the optimal (in terms of HamDist objective function, not necessarily AUC) set of features, which is reflected in the comparison between Non-privacy-greedy and HamDist. Overall, all our proposed methods maintain the classification utility of the dataset better than all the competing methods."}, {"heading": "5.4 Adult Data", "text": "The performance of various methods for the Adult dataset is shown in Table 5, where the rows and columns are organized identically as in previous table. Adult dataset is lowdimensional and dense (27.9% values are non-zero). Achieving privacy on such a dataset is comparatively easier, so existing methods for anonymization work fairly well on this. As we can observe, RF performs the best among all the methods. The good performance of RF is owing to the very small percentage of flips which ranges from 0.60% to 1.50% for various k values. Basically, RF can achieve k-anonymity on this dataset with very small number of flips, which helps it maintain the classification utility of the dataset. For the same reason, k-Anonymity HamDist and k-Anonymity DistCnt methods are also able to retain many dimensions (8 out of 19 for k=5) of this dataset, and perform fairly well. On the other hand, different versions of Greedy and Maximal retain between 8 and 10 dimensions and achieve an AUC between 0.74 and 0.78, which is close to 0.80 (the AUC value for RF). Also, note that, the AUC using the full set of features (no privacy) is 0.82, so the utility loss due to the privacy is not substantial for this dataset. As a remark, our method is particularly suitable for high dimensional sparse data for which anonymization using traditional methods is difficult."}, {"heading": "5.5 Spam Email Filtering", "text": "In Table 6, we compare AUC value of different methods for spam email filtering task. This is a very high dimensional data with 24604 features. As we can observe, our proposed methods, especially DistCnt and HamDist perform better than the competing methods. For example, for k = 5, the classification AUC of RF is 0.87 with flip rate 1.30%, but using less than 0.045% of features DistCnt obtains an AUC value of 0.95, which is equal to the AUC value using the full feature set. Again, k-Anonymity based methods show worse performance as they select less number of features due to stronger restriction of this privacy metric. For instance, for k = 5, HamDist selects 11 features, but k-Anonymity HamDist selects only 4 features. Due to this, classification results using k-Anonymity constraint is worse compared to those using our proposed AS as privacy metric. Among our methods, both DistCnt and Maximal are the best as they consistently hold the classification performance for all different k settings."}, {"heading": "5.6 German Data", "text": "Finally, the performance of various methods for German dataset is shown in Table 7. As we can see, our proposed methods, especially HamDist performs better than the competing approaches. For example, under k = 8, it obtains 0.69 AUC with 6 features selected. However, RF performs worse in terms of AUC. For instance, RF obtains 0.58 AUC under k = 8. The bad performance of RF is owing to the large percentage of flips ranging from 11% to 16% for various k values. The significant flip rate deteriorates the data utility and leads to bad classification performance. Moreover, the AUC performance of RF drops from 0.65 to 0.58 as we increase the k values; in contrast, the AUC results of our proposed methods remain stable as we increase the k value. Among all the competing methods, CM has the worst AUC, for different k, this value is around 0.55\u2014similar to a random predictor! Methods that use k-Anonymity as privacy metric also perform worse than our proposed methods again due to the fact that they are able to select less number of features than our proposed methods."}, {"heading": "6. RELATED WORK", "text": "In terms of privacy model, several privacy metrics have been widely used in order to quantify the privacy risk of published data instances, such as k-anonymity [23], t-closeness [17], \u2113diversity [19], and differential privacy [7]. Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18,\n27] and frequent pattern mining [8]. In fact, the majority of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions. A few other works [4, 5] consider output privacy by ensuring that the output of a data mining task does not reveal sensitive information.\nIn recent years differential privacy metric has become more popular in privacy research literatures. Fan [10] proposed a practical data analytics framework that generates differentially private aggregates which can be used to perform data mining and recommendation tasks. [9] adopted differential privacy and showed that differentially private aggregates of web browsing activities can be released in real-time. Nevertheless, k-anonymity privacy metric, due to its simplicity and flexibility, has been studied extensively over the years. Samarati [23] has proposed formal methods of k-anonymity using suppression and generalization techniques. She also introduced the concept of minimal generalization. Meyerson et al. [20] proved that two definition of k-optimality is NPhard: first, to find the minimum number of cell value that needs to be suppressed; second, to find the minimum number of attributes that needs to be suppressed. Henceforth, a large number of works have explored the approximation of anonymization [3, 16, 20, 23]. However, None of these works consider the utility of the dataset along with the privacy requirements.\nIt is well known that the utility of a dataset is at odd with the privacy requirements. Samarati [23] proposes one of the earliest metric for privacy, which is called generalization height. Average size of anonymous groups [19] and discernibility [3] are other privacy metrics. Kifer et al. [14] propose methods that inject utility in the form of data distribution information into k-anonymous and \u2113-diverse tables. However, the above works do not consider a classification dataset. Iyengar [13] proposes a utility metric called CM which is explicitly designed for a classification dataset. It assigns a generalization penalty over the rows of the dataset, but its performance is poor as we have shown in this work."}, {"heading": "7. CONCLUSION", "text": "In this paper, we propose a novel method for entity anonymization using feature selection. We define a new anonymity metric called k-anonymity by selection which is particularly suitable for high dimensional microdata. We also propose two feature selection methods along with two classification utility metrics. These metrics satisfy submodular properties thus enable effective greedy algorithms. In experiment section we show that both the proposed methods select good quality features on a variety of datasets for retaining the classification utility yet satisfy the user defined anonymity constraint."}, {"heading": "8. REFERENCES", "text": "[1] C. C. Aggarwal. On k-anonymity and the curse of dimensionality. VLDB, 2005.\n[2] E. Amaldi and V. Kann. On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems. Theor. Comput. Sci., 1998.\n[3] R. J. Bayardo and R. Agrawal. Data privacy through optimal k-anonymization. ICDE\u201905, 2005.\n[4] R. Bhaskar, S. Laxman, A. Smith, and A. Thakurta. Discovering frequent patterns in sensitive data. KDD\u201910, pages 503\u2013512, 2010.\n[5] L. Bonomi and L. Xiong. Mining frequent patterns with differential privacy. Proc. VLDB Endow., 2013.\n[6] J.-W. Byun, A. Kamra, E. Bertino, and N. Li. Efficient k-anonymization using clustering techniques. In DASFAA, 2007.\n[7] C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Foundation and Trends in Theoretical Computer Science, 2014.\n[8] A. Evfimievski, R. Srikant, R. Agrawal, and J. Gehrke. Privacy preserving mining of association rules. KDD\u201902, pages 217\u2013228, 2002.\n[9] L. Fan, L. Bonomi, L. Xiong, and V. Sunderam. Monitoring web browsing behavior with differential privacy. WWW. ACM, 2014.\n[10] L. Fan and H. Jin. A practical framework for privacy-preserving data analytics. WWW, 2015.\n[11] S. E. Fienberg and J. Jin. Privacy-preserving data sharing in high dimensional regression and classification settings. Journal of Privacy and Confidentiality, 4(1):10, 2012.\n[12] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. J. Mach. Learn. Res., 2003.\n[13] V. S. Iyengar. Transforming data to satisfy privacy constraints. KDD, 2002.\n[14] D. Kifer and J. Gehrke. Injecting utility into anonymized datasets. In SIGMOD, 2006.\n[15] D. Kifer and A. Machanavajjhala. No free lunch in data privacy. SIGMOD\u201911, pages 193\u2013204, 2011.\n[16] K. LeFevre, D. J. DeWitt, and R. Ramakrishnan. Incognito: Efficient full-domain k-anonymity. SIGMOD\u201905, pages 49\u201360, 2005.\n[17] N. Li and T. Li. t-closeness: Privacy beyond k-anonymity and l-diversity. ICDE\u201907, 2007.\n[18] X. Lin, C. Clifton, M. Zhu, E. M. Modeling, X. Lin,\nC. Clifton, and M. Zhu. Privacy-preserving clustering with distributed em mixture modeling. Knowledge and Information Systems, pages 68\u201381, 2005.\n[19] A. Machanavajjhala, D. Kifer, J. Gehrke, and M. Venkitasubramaniam. L-diversity: Privacy beyond k-anonymity. ACM TKDD, 2007.\n[20] A. Meyerson and R. Williams. On the complexity of optimal k-anonymity. ACM PODS, 2004.\n[21] G. C. Michele Conforti. Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem. Discrete Applied Mathematics, 1984.\n[22] A. Narayanan and V. Shmatikov. Robust de-anonymization of large sparse datasets. In IEEE Symposium on Security and Privacy, 2008.\n[23] P. Samarati. Protecting respondents\u2019 identities in microdata release. IEEE TKDE, 2001.\n[24] L. Sweeney. K-anonymity: A model for protecting privacy. Int. J. Uncertain. Fuzziness Knowl.-Based Syst., 10(5):557\u2013570, Oct. 2002.\n[25] T. Uno, M. Kiyomi, and H. Arimura. LCM ver. 2: Efficient mining algorithms for frequent/closed/maximal itemsets. FIMI-ICDM, 2004.\n[26] J. Vaidya and C. Clifton. Privacy preserving association rule mining in vertically partitioned data. KDD\u201902, pages 639\u2013644, 2002.\n[27] J. Vaidya and C. Clifton. Privacy-preserving k-means clustering over vertically partitioned data. KDD, 2003.\n[28] J. Vaidya, C. Clifton, M. Kantarcioglu, and A. S. Patterson. Privacy-preserving decision trees over vertically partitioned data. ACM TKDD, 2008.\n[29] J. Vaidya, M. Kantarc, and C. Clifton. Privacy-preserving naive bayes classification. The VLDB Journal, 17(4):879\u2013898, July 2008.\n[30] M. J. Zaki and J. Wagner Meira. Data Mining and Analysis: Fundamental Concepts and Algorithms. Cambridge University Press, 2014."}], "references": [{"title": "On k-anonymity and the curse of dimensionality", "author": ["C.C. Aggarwal"], "venue": "VLDB", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems", "author": ["E. Amaldi", "V. Kann"], "venue": "Theor. Comput. Sci.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Data privacy through optimal k-anonymization", "author": ["R.J. Bayardo", "R. Agrawal"], "venue": "ICDE\u201905", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Discovering frequent patterns in sensitive data", "author": ["R. Bhaskar", "S. Laxman", "A. Smith", "A. Thakurta"], "venue": "KDD\u201910, pages 503\u2013512", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Mining frequent patterns with differential privacy", "author": ["L. Bonomi", "L. Xiong"], "venue": "Proc. VLDB Endow.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient k-anonymization using clustering techniques", "author": ["J.-W. Byun", "A. Kamra", "E. Bertino", "N. Li"], "venue": "DASFAA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "The algorithmic foundations of differential privacy", "author": ["C. Dwork", "A. Roth"], "venue": "Foundation and Trends in Theoretical Computer Science", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Privacy preserving mining of association rules", "author": ["A. Evfimievski", "R. Srikant", "R. Agrawal", "J. Gehrke"], "venue": "KDD\u201902, pages 217\u2013228", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Monitoring web browsing behavior with differential privacy", "author": ["L. Fan", "L. Bonomi", "L. Xiong", "V. Sunderam"], "venue": "WWW. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "A practical framework for privacy-preserving data analytics", "author": ["L. Fan", "H. Jin"], "venue": "WWW", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Privacy-preserving data sharing in high dimensional regression and classification settings", "author": ["S.E. Fienberg", "J. Jin"], "venue": "Journal of Privacy and Confidentiality, 4(1):10", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Mach. Learn. Res.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Transforming data to satisfy privacy constraints", "author": ["V.S. Iyengar"], "venue": "KDD", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Injecting utility into anonymized datasets", "author": ["D. Kifer", "J. Gehrke"], "venue": "SIGMOD", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "No free lunch in data privacy", "author": ["D. Kifer", "A. Machanavajjhala"], "venue": "SIGMOD\u201911, pages 193\u2013204", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Incognito: Efficient full-domain k-anonymity", "author": ["K. LeFevre", "D.J. DeWitt", "R. Ramakrishnan"], "venue": "SIGMOD\u201905, pages 49\u201360", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "t-closeness: Privacy beyond k-anonymity and l-diversity", "author": ["N. Li", "T. Li"], "venue": "ICDE\u201907", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "C", "author": ["X. Lin", "C. Clifton", "M. Zhu", "E.M. Modeling", "X. Lin"], "venue": "Clifton, and M. Zhu. Privacy-preserving clustering with distributed em mixture modeling. Knowledge and Information Systems, pages 68\u201381", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "L-diversity: Privacy beyond k-anonymity", "author": ["A. Machanavajjhala", "D. Kifer", "J. Gehrke", "M. Venkitasubramaniam"], "venue": "ACM TKDD", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "On the complexity of optimal k-anonymity", "author": ["A. Meyerson", "R. Williams"], "venue": "ACM PODS", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Submodular set functions", "author": ["G.C. Michele Conforti"], "venue": "matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem. Discrete Applied Mathematics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1984}, {"title": "Robust de-anonymization of large sparse datasets", "author": ["A. Narayanan", "V. Shmatikov"], "venue": "IEEE Symposium on Security and Privacy", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Protecting respondents\u2019 identities in microdata release", "author": ["P. Samarati"], "venue": "IEEE TKDE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "K-anonymity: A model for protecting privacy", "author": ["L. Sweeney"], "venue": "Int. J. Uncertain. Fuzziness Knowl.-Based Syst.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "LCM ver", "author": ["T. Uno", "M. Kiyomi", "H. Arimura"], "venue": "2: Efficient mining algorithms for frequent/closed/maximal itemsets. FIMI-ICDM", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Privacy preserving association rule mining in vertically partitioned data", "author": ["J. Vaidya", "C. Clifton"], "venue": "KDD\u201902, pages 639\u2013644", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Privacy-preserving k-means clustering over vertically partitioned data", "author": ["J. Vaidya", "C. Clifton"], "venue": "KDD", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "Privacy-preserving decision trees over vertically partitioned data", "author": ["J. Vaidya", "C. Clifton", "M. Kantarcioglu", "A.S. Patterson"], "venue": "ACM TKDD", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Privacy-preserving naive bayes classification", "author": ["J. Vaidya", "M. Kantarc", "C. Clifton"], "venue": "The VLDB Journal,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Data Mining and Analysis: Fundamental Concepts and Algorithms", "author": ["M.J. Zaki", "J. Wagner Meira"], "venue": "Cambridge University Press", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "[22] have identified a Netflix subscriber from his anonymous movie rating by using Internet Movie Database (IMBD) as the source of background information.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": ", all the points are far from each other [30].", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "given data instance is k-anonymity [23] which requires that for any data instance, there are at least k \u2212 1 distinct data instances that share the same feature vector.", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "is not a viable solution even for a k value of 2 [1].", "startOffset": 49, "endOffset": 52}, {"referenceID": 22, "context": "One can view this as column suppression instead of more commonly used row suppression for achieving k-anonymity [23].", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "it is well-known that privacy is always at odd with the utility of a knowledge-based system [14, 15], so finding the right balance is a difficult task.", "startOffset": 92, "endOffset": 100}, {"referenceID": 14, "context": "it is well-known that privacy is always at odd with the utility of a knowledge-based system [14, 15], so finding the right balance is a difficult task.", "startOffset": 92, "endOffset": 100}, {"referenceID": 11, "context": "Besides, feature selection itself, without considering the privacy constraint, is an NP-Hard problem [12].", "startOffset": 101, "endOffset": 105}, {"referenceID": 21, "context": "Nevertheless, as shown in [22], binary attributes are sufficient for an attacker to de-anonymize", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "For classification data, Iyengar [13] has proposed a metric called CM (Classification Metric) which is defined as below.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "2 (Classification Metric [13]).", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "Feature selection [12] for a classification task is to select a subset of highly predictive variables so that classification accuracy possibly improves which happens due to the fact that contradictory or noisy attributes are generally ignored during the feature selection step.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "Since the general task of feature selection even for the case of linearly separable positive and negative examples is NPHard [2], the problem 1 is also NP-Hard, because by setting k = 0, we can reduce the feature selection problem to this problem.", "startOffset": 125, "endOffset": 128}, {"referenceID": 29, "context": "Hence, Apriori [30] like algorithm for itemset mining can be used for effectively enumerating all the feature subsets of D which satisfies the required k-anonymity by selection constraint.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "Maximal uses the LCM-Miner package provided in [25] which, at present, is the fastest method for finding maximal frequent itemsets.", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "Line 1 uses the package used in [25] to generate all the maximal feature sets that satisfy k-anonymity by selection for the given k value.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "For modular function f(S) = f(S) [21].", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "The Adult benchmark dataset is based on census data and has been widely used in earlier works on k-anonymization [13].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "We call these competing methods Non-Private Greedy, CM Greedy [13], and RF [6].", "startOffset": 62, "endOffset": 66}, {"referenceID": 5, "context": "We call these competing methods Non-Private Greedy, CM Greedy [13], and RF [6].", "startOffset": 75, "endOffset": 78}, {"referenceID": 5, "context": "RF is a Randomization Flipping based kanonymization technique presented in [6], which randomly flips the feature value such that each instance in the dataset satisfies the k-anonymity privacy constraint.", "startOffset": 75, "endOffset": 78}, {"referenceID": 12, "context": "CM greedy represents another greedy based method which uses Classification Metric utility criterion proposed in [13] (See definition 2.", "startOffset": 112, "endOffset": 116}, {"referenceID": 12, "context": "Iyenger [13] uses a genetic algorithm for the classification task, but for a fair comparison we use CM criterion in the Greedy algorithm and with the selected features we use identical setup for classification.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "For practical k-anonymization, k value between 5 and 10 is suggested in earlier works [24]; we use three different k values, which are 5, 8 and 11.", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "CM Greedy [13] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "68 (2) RF [6] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "CM Greedy [13] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "75 (5) RF [6] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "CM Greedy [13] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "86 (3) RF [6] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "CM Greedy [13] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "57 (2) RF [6] 0.", "startOffset": 10, "endOffset": 13}, {"referenceID": 22, "context": "In terms of privacy model, several privacy metrics have been widely used in order to quantify the privacy risk of published data instances, such as k-anonymity [23], t-closeness [17], ldiversity [19], and differential privacy [7].", "startOffset": 160, "endOffset": 164}, {"referenceID": 16, "context": "In terms of privacy model, several privacy metrics have been widely used in order to quantify the privacy risk of published data instances, such as k-anonymity [23], t-closeness [17], ldiversity [19], and differential privacy [7].", "startOffset": 178, "endOffset": 182}, {"referenceID": 18, "context": "In terms of privacy model, several privacy metrics have been widely used in order to quantify the privacy risk of published data instances, such as k-anonymity [23], t-closeness [17], ldiversity [19], and differential privacy [7].", "startOffset": 195, "endOffset": 199}, {"referenceID": 6, "context": "In terms of privacy model, several privacy metrics have been widely used in order to quantify the privacy risk of published data instances, such as k-anonymity [23], t-closeness [17], ldiversity [19], and differential privacy [7].", "startOffset": 226, "endOffset": 229}, {"referenceID": 10, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 161, "endOffset": 169}, {"referenceID": 28, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 161, "endOffset": 169}, {"referenceID": 10, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 182, "endOffset": 186}, {"referenceID": 17, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 199, "endOffset": 207}, {"referenceID": 26, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 199, "endOffset": 207}, {"referenceID": 7, "context": "Existing works on privacy preserving data mining solve a specific data mining problem given a privacy constraint over the data instances, such as classification [11, 29], regression [11], clustering [18, 27] and frequent pattern mining [8].", "startOffset": 236, "endOffset": 239}, {"referenceID": 10, "context": "of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions.", "startOffset": 19, "endOffset": 30}, {"referenceID": 25, "context": "of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions.", "startOffset": 19, "endOffset": 30}, {"referenceID": 26, "context": "of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions.", "startOffset": 19, "endOffset": 30}, {"referenceID": 27, "context": "of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions.", "startOffset": 19, "endOffset": 30}, {"referenceID": 28, "context": "of the above works [11, 26\u201329] consider distributed privacy where the dataset is partitioned among multiple participants owning different portions of the data, and the goal is to mine shared insights over the global data without compromising the privacy of the local portions.", "startOffset": 19, "endOffset": 30}, {"referenceID": 3, "context": "A few other works [4, 5] consider output privacy by ensuring that the output of a data mining task does not reveal sensitive information.", "startOffset": 18, "endOffset": 24}, {"referenceID": 4, "context": "A few other works [4, 5] consider output privacy by ensuring that the output of a data mining task does not reveal sensitive information.", "startOffset": 18, "endOffset": 24}, {"referenceID": 9, "context": "Fan [10] proposed a practical data analytics framework that generates differentially private aggregates which can be used to perform data mining and recommendation tasks.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "[9] adopted differential", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "Samarati [23] has proposed formal methods of k-anonymity using suppression and generalization techniques.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "[20] proved that two definition of k-optimality is NPhard: first, to find the minimum number of cell value that needs to be suppressed; second, to find the minimum number of attributes that needs to be suppressed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "anonymization [3, 16, 20, 23].", "startOffset": 14, "endOffset": 29}, {"referenceID": 15, "context": "anonymization [3, 16, 20, 23].", "startOffset": 14, "endOffset": 29}, {"referenceID": 19, "context": "anonymization [3, 16, 20, 23].", "startOffset": 14, "endOffset": 29}, {"referenceID": 22, "context": "anonymization [3, 16, 20, 23].", "startOffset": 14, "endOffset": 29}, {"referenceID": 22, "context": "Samarati [23] proposes one of the earliest metric for privacy, which is called generalization height.", "startOffset": 9, "endOffset": 13}, {"referenceID": 18, "context": "Average size of anonymous groups [19] and discernibility [3] are other privacy metrics.", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "Average size of anonymous groups [19] and discernibility [3] are other privacy metrics.", "startOffset": 57, "endOffset": 60}, {"referenceID": 13, "context": "[14] propose methods that inject utility in the form of data distribution information into k-anonymous and l-diverse tables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "gar [13] proposes a utility metric called CM which is explicitly designed for a classification dataset.", "startOffset": 4, "endOffset": 8}], "year": 2017, "abstractText": "Over the last decade, proliferation of various online platforms and their increasing adoption by billions of users have heightened the privacy risk of a user enormously. In fact, security researchers have shown that sparse microdata containing information about online activities of a user although anonymous, can still be used to disclose the identity of the user by cross-referencing the data with other data sources. To preserve the privacy of a user, in existing works several methods (k-anonymity, l-diversity, differential privacy) are proposed that ensure a dataset which is meant to share or publish bears small identity disclosure risk. However, the majority of these methods modify the data in isolation, without considering their utility in subsequent knowledge discovery tasks, which makes these datasets less informative. In this work, we consider labeled data that are generally used for classification, and propose two methods for feature selection considering two goals: first, on the reduced feature set the data has small disclosure risk, and second, the utility of the data is preserved for performing a classification task. Experimental results on various real-world datasets show that the method is effective and useful in practice.", "creator": "LaTeX with hyperref package"}}}