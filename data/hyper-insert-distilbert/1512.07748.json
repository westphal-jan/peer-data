{"id": "1512.07748", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Dec-2015", "title": "Real-Time Audio-to-Score Alignment of Music Performances Containing Errors and Arbitrary Repeats and Skips", "abstract": "moreover this paper discusses real - time alignment of audio signals best of any music performance sensitivity to the resulting corresponding score ( a. k. marked a. score or following ) estimate which can perfectly handle specific tempo modulation changes, errors and arbitrary arithmetic repeats \u2026 and / or skips ( repeats / skips ) in regular performances. this type of timing score : following computation is particularly useful in automatic accompaniment for practices seminars and rehearsals, locations where errors ( and overlapping repeats / skips are often made. simple extensions of realizing the general algorithms previously proposed first in the literature are not applicable in achieving these situations for scores of practical length ensembles due allegedly to especially the engineering problem of large dimensional computational structural complexity. to cope with this problem, we independently present two hidden tree markov models of low monophonic sound performance with repetitive errors and perform arbitrary individual repeats / skips, quickly and immediately derive efficient multiple score - following generation algorithms constructed with an accuracy assumption finding that the prior probability distributions of specific score positions before samples and after repeats / skips are independent measured from each other. independently we confirmed predicted real - time operation convergence of the experimental algorithms with simple music scores of considerable practical calculation length ( accuracy around 10000 notes ) on firing a modern laptop lcd and showcased their tracking capability ability to repeat the input performance within ca 0. 34 7 s according on average precision after repeats / elimination skips in separate clarinet performance data. further improvements and extension for polyphonic signals programs are also discussed.", "histories": [["v1", "Thu, 24 Dec 2015 08:21:48 GMT  (592kb,D)", "http://arxiv.org/abs/1512.07748v1", "12 pages, 8 figures, version accepted in IEEE/ACM Transactions on Audio, Speech, and Language Processing"]], "COMMENTS": "12 pages, 8 figures, version accepted in IEEE/ACM Transactions on Audio, Speech, and Language Processing", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.MM", "authors": ["tomohiko nakamura", "eita nakamura", "shigeki sagayama"], "accepted": false, "id": "1512.07748"}, "pdf": {"name": "1512.07748.pdf", "metadata": {"source": "CRF", "title": "Real-Time Audio-to-Score Alignment of Music Performances Containing Errors and Arbitrary Repeats and Skips", "authors": ["Tomohiko Nakamura", "Shigeki Sagayama"], "emails": ["Nakamura@ipc.i.u-tokyo.ac.jp).", "(enakamura@am.kuis.kyoto-u.ac.jp).", "(sagayama@meiji.ac.jp)."], "sections": [{"heading": null, "text": "Keywords\u2014Score following, audio-to-score alignment, arbitrary repeats and skips, fast Viterbi algorithm, hidden Markov model, music signal processing\nI. INTRODUCTION Real-time alignment of an audio signal of a music performance to a given score, also known as score following, has been gathering attention since its first appearance in 1984 [1], [2]. Score following is a basic technique for realtime musical applications such as automatic accompaniment, automatic score page-turning [3] and automatic captioning to music videos. The technique is particularly essential for automatic accompaniment, which synchronizes an accompaniment to a performer on the fly, referring to performance and accompaniment scores. Automatic accompaniment enables live\nCitation information: DOI 10.1109/TASLP.2015.2507862, IEEE/ACM Transactions on Audio, Speech, and Language Processing.\n(c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications standards/ publications/rights/index.html for more information.\nT. Nakamura is with the Department of Information Physics and Computing, Graduate School of Information Science and Technology, the University of Tokyo, Tokyo 113-8656, Japan (Tomohiko Nakamura@ipc.i.u-tokyo.ac.jp).\nE. Nakamura is with the Graduate School of Informatics, Kyoto University, Kyoto 606-8501, Japan (enakamura@am.kuis.kyoto-u.ac.jp).\nS. Sagayama is a Professor Emeritus of University of Tokyo, Tokyo, 113- 8656, Japan and currently with the School of Interdisciplinary Mathematical Sciences, Meiji University, Tokyo 164-8525, Japan (sagayama@meiji.ac.jp).\nperformance of ensemble music by one or a few performers. Many studies of score following have been carried out (see [4] for a review and [5]\u2013[13] for recent progress).\nAutomatic accompaniment is particularly useful for practices, rehearsals and personal enjoyment of ensemble music. In these situations, performers often make errors. Moreover, performers may want to start playing from the middle of a score and generally make repeats and/or skips (repeats/skips). Since errors and repeats/skips are hard to predict, a scorefollowing algorithm capable of handling arbitrary errors and repeats/skips is necessary to realize an automatic accompaniment system effective in those situations. Our aim is to develop such an algorithm.\nTreatment of errors in score following is discussed in some studies [4], [5], [13], [14]. However, a detailed discussion and a systematic evaluation of the effectiveness of the methods for audio score following have not been given in the literature. Score-following algorithms that can follow repeats/skips have been proposed in [5], [11], [15]. The targets of these algorithms are predetermined repeats/skips from and to specific score positions, and treatment of arbitrary repeats/skips is not discussed nor guaranteed. In fact, as we will show in this paper, simple extensions of these algorithms have the problem of large computational cost and cannot work in real time for long scores of practical length. Unless the problem is solved, score-following systems can only work with limited scores with very short length or we must give up following arbitrary repeats/skips as most of the current systems do, both of which sacrifice the vast potential application of score following. Therefore, it is essential to reduce the computational complexity to follow arbitrary repeats/skips.\nThe authors have presented a new type of hidden Markov model (HMM) that describes musical instrument digital interface (MIDI) performances with errors and arbitrary repeats/skips, and derived a computationally efficient algorithm for the HMM [13]. It reduces the computational complexity with an assumption to simplify a probability distribution of score positions before and after repeats/skips. While a similar model would be applicable to the audio case, further discussions are required since audio inputs (frame-wise discrete in time and continuous in features) significantly differ with MIDI inputs (continuous in time and discrete in pitches) in nature.\nThe main contribution of this paper is to present real-time algorithms that can follow monophonic audio performances containing arbitrary repeats/skips and errors. Although monophonic score following has been addressed since [1], [2],\nar X\niv :1\n51 2.\n07 74\n8v 1\n[ cs\n.S D\n] 2\n4 D\nec 2\n01 5\narbitrary repeats/skips have never been discussed despite the practical importance of their treatment as the above mentioned. Because polyphonic score following is still an active field of research and the extension of the present method for polyphonic performances requires many additional issues discussed in Sec. V, we confine ourselves to monophonic performances.\nWe develop a model of music performances containing errors and arbitrary repeats/skips with an HMM. We first discuss how various types of errors can be incorporated into the model (Sec. II). Next, we extend the model to incorporate arbitrary repeats/skips. In order to solve the problem of large computational cost for following arbitrary repeats/skips, two HMMs with refined topologies are presented. We derive efficient score-following algorithms with reduced computational complexity based on both HMMs (Sec. III). We demonstrate that both algorithms can work in real time with scores of practical length on a modern laptop computer and are effective in following performances with errors and arbitrary repeats/skips through evaluations using clarinet performances during practice (Sec. IV). We discuss possible improvements and extensions of the proposed algorithms for polyphonic inputs (Sec. V). Part of this study (Sec. III and a part of Sec. IV) was reported in our previous conference paper [12]."}, {"heading": "II. SCORE FOLLOWING FOR PERFORMANCES WITH ERRORS", "text": "A. Variety in Audio Performance and Statistical Approach Score following is generally challenging since audio signals of music performances widely vary even if the same score is used. Four typical sources of variety in monophonic audio performance are listed below. (a) Acoustic variations: Spectral features of audio perfor-\nmances depend on musical instruments and are not stationary. In addition, audio performances usually include noise caused by the surrounding environment and musical instruments (e.g. resonance, background noise, breath noise and other acoustics). (b) Temporal fluctuations: The tempo of the performance and onset times and durations of performed notes deviate from those indicated in scores due to performer\u2019s skills, physical limitations of musical instruments and musical expressions. For example, performances during practice are often rendered in slow tempo to avoid errors. (c) Performance errors: Performers may make errors due to lack of performance skills or mis-readings of the score. Errors are categorized into pitch errors (substitution errors), dropping notes (deletion errors), adding extra notes (insertion errors) [1]. Besides, performers may make pauses between notes, for example, to turn a page of the score and to check the next note. (d) Repeats/skips: Performers may repeat and/or skip phrases in particular during practice. Furthermore, the performers generally add or delete a repeated section.\nThese four sources of variety in monophonic audio performance make score following difficult and motivate us to study it. In particular, it is essential to adapt automatic accompaniment systems to the variety in order to keep synchronization\nto live performances. Although it is out of the scope of this paper, there are other sources of variety in music performance such as ornaments [6], [13], [16], [17] and improvisation [18], [19].\nRecent score-following systems commonly use probabilistic models such as HMM to capture the variety of audio performances, and their effectiveness has been well confirmed [4] (and references in the Introduction). They are particularly advantageous to capture continuous variations of audio features and to handle errors which are hard to predict. Therefore, we take the statistical approach in this study."}, {"heading": "B. Performance HMM", "text": "We represent the performance score with N musical events, each of which is a note or a rest. A performer reads the score from event to event and keeps making a sound corresponding to an event. This process of performance can be modeled with a hierarchical HMM with two levels [20], [21], which we call the performance HMM. The top level describes the progression of performed events, and the bottom level expresses temporal structure of the audio signal in a performed event.\nEvents correspond to states (top states) of the top-level HMM (top HMM), and the performance is described as transitions between the top states. Let z(top)t = 0, \u00b7 \u00b7 \u00b7 , N \u2212 1 denote the random variable describing the top state at the tth frame (t = 0, \u00b7 \u00b7 \u00b7 , T \u22121), and let i and j label a top state. The top HMM is parameterized by state transition probabilities aj,i and initial probabilities \u03c0i:\naj,i := P (z (top) t = i|z (top) t\u22121 = j), (1)\n\u03c0i := P (z (top) 0 = i), (2) which satisfy \u2211N\u22121 i=0 \u03c0i = 1 and \u2211N\u22121 i=0 aj,i = 1 for all j.\nEach top state is itself an HMM (bottom HMM), whose states (bottom states) correspond to subevents in an event, for\nexample, sustain of an instrumental sound, pauses between notes, etc. Let L denote the number of bottom states in the top state, z(bot)t = 0, \u00b7 \u00b7 \u00b7 , L \u2212 1 denote the random variable describing the bottom state at the tth frame, and let l and l\u2032 label a bottom state. The state transitions of the bottom HMM are characterized by three kinds of probabilities. The initial probability \u03c0(i)l describes the probability of a transition to bottom state l when top state i is entered, the exiting probability e(i)l describes the probability of exiting top state i from bottom state l, and the transition probability a(i)l\u2032,l := P (z\n(bot) t = l|z (bot) t\u22121 = l \u2032) represents the transition from bottom state l\u2032 to bottom state l in top state i. These probabilities satisfy \u2211L\u22121 l\u2032=0 \u03c0 (i) l\u2032 = 1 and \u2211L\u22121 l\u2032=0 a (i) l,l\u2032 + e (i) l = 1 for all l and i. Thus, the performance is modeled as a sequence of T pairs of random variables {(z(top)t , z (bot) t )}T\u22121t=0 (Fig. 1). For example, if the pair zt := (z (top) t , z (bot) t ) equals to (i, l), the score position at frame t is at bottom state l of top state i. Observed audio features are described as being stochastically generated from a bottom state. Given an audio feature yt := [yt,0, yt,1, \u00b7 \u00b7 \u00b7 , yt,D\u22121]> at frame t as a D-dimensional real vector, the emission probability of state (i, l) is defined as\nb (i) l (yt) := P (yt|zt = (i, l)). (3)"}, {"heading": "C. Emission Probability and Substitution Error", "text": "From here to Sec. II-E, we consider the performance HMM with L = 1 for simplicity, but the case for L > 1 can be treated similarly. To extract pitch information from the input signal, we need a suitable feature representation. In the comparison of some audio features in [7], [22], the magnitude of a constant-Q transform (CQT) [23] with a quality factor set to one semitone yielded the best result of score following for monophonic audio input. Furthermore, normalizing magnitudes of CQTs such that \u2211D\u22121 d=0 yd = 1 makes them insusceptible to dynamic variations. Although one may think that the normalization makes it difficult to discriminate pauses from notes, the difference in spectral shape between pauses and notes can help the discrimination: The CQT of a pitched sound have clear peaks at its fundamental frequency and harmonics, whereas the CQTs at pauses are relatively flat. We use normalized magnitudes of CQTs (normalized CQTs) as audio features.\nLet k be the pitch index and K be the set of possible pitches. For convenience, we indicate the pitches A0 to C8 in the range of a standard piano as k = 21 to k = 108 and silence as k = \u22121, and K = {21, 22, \u00b7 \u00b7 \u00b7 , 108} \u222a {\u22121}. We assume that normalized CQTs corresponding to pitch k follow a D-dimensional normal distribution with mean \u00b5k and covariance matrix \u03a3k, denoted by N (yt|\u00b5k,\u03a3k). The emission probability b(i)0 (yt) of bottom state 0 of top state i is given as\nb (i) 0 (yt) = \u2211 k\u2208K w (i) k,0N (yt|\u00b5k,\u03a3k). (4)\nHere w(i)k,0 \u2208 [0, 1] is a mixture weight of pitch k of bottom state 0 of top state i, which satisfies \u2211 k\u2208K w (i) k,0 = 1 for all i.\nWhen substitution errors are not made, w(i)k,0 = 0 unless k = pi, where pi \u2208 K denotes the pitch of event i (pi = \u22121 for a rest). On the other hand, to describe a performance with substitution errors, we have small positive values of w(i)k,0 for k 6= pi since a substitution error is represented by an emission of an audio feature with an incorrect pitch."}, {"heading": "D. Transition Probability and Deletion and Insertion Errors", "text": "Transition probabilities in the top level aj,i represent the frequency of the transitions between the events. If performances do not contain insertion and deletion errors, aj,i = 0 unless i = j + 1. We can express an insertion error and a deletion error with a self transition and a transition to the second next top state, which correspond to aj,j and aj,j+2.\nThe self-transition probability a(i)0,0 of bottom state 0 of top state i describes the expected duration of the corresponding event di, which is computed as a product of the note value of the event and the score-notated tempo:\ndi = \u221e\u2211 k=1 k(a (i) 0,0) k\u22121(1\u2212 a(i)0,0) = 1 1\u2212 a(i)0,0 . (5)\nIf di is shorter than a processing time interval, we put a (i) 0,0 = 0. This probabilistic representation of the event duration describes the temporal fluctuations of music performance."}, {"heading": "E. Pauses between Notes", "text": "Pauses between notes can be introduced into the performance HMM by adding an extra bottom state with index 1, which we call a pause state (Fig. 2). The occurrence of the pause is expressed as a transition to the pause state, which corresponds to a(i)0,1. The duration of the extra pause is represented by the self-transition probability of the pause state a (i) 1,1, which can be set similarly to Eq. (5). We put a (i) 1,0 = 0 and \u03c0 (i) 1 = 0 for all i. We assume that b (i) 1 (yt) = N (yt|\u00b5\u22121,\u03a3\u22121)."}, {"heading": "F. Estimation of Score Positions", "text": "For the convenience of estimating score positions, we convert the performance HMM into an equivalent standard HMM. Its state corresponds to a bottom state of the performance HMM and is labeled with (i, l). The standard HMM is\nparameterized by emission probabilities b\u0303(i,l)(yt), initial probabilities \u03c0\u0303(i,l), and transition probabilities a\u0303(j,l\u2032),(i,l), defined by b\u0303(i,l)(yt) := b (i) l (yt), \u03c0\u0303(i,l) := \u03c0i\u03c0 (i) l , and\na\u0303(j,l\u2032),(i,l) :=\n{ a (i) l\u2032,l + e (i) l\u2032 ai,i\u03c0 (i) l (i = j)\ne (j) l\u2032 aj,i\u03c0 (i) l (i 6= j)\n, (6)\nGiven observed normalized CQTs up to the tth frame y0:t = {y\u03c4}t\u03c4=0, the score position at frame t is estimated with the standard HMM by solving\nargmax zt P (zt|y0:t) = argmax zt P (y0:t, zt), (7)\nwhere\nP (y0:t, zt) = \u2211 z0:t\u22121 ( t\u220f \u03c4=1 b\u0303z\u03c4 (y\u03c4 )a\u0303z\u03c4\u22121,z\u03c4 ) b\u0303z0(y0)\u03c0\u0303z0 . (8)\nHere z0:t\u22121 denotes {z\u03c4}t\u22121\u03c4=0. Eq. (7) is derived from the Bayes\u2019 theorem.\nThis maximization problem can be solved efficiently with the forward algorithm. It computes the forward variable \u03b1t,zt := P (y0:t, zt) in a recursive manner:\n\u03b1t,(i,l) =  b\u0303(i,l)(yt) \u2211 j=0,\u00b7\u00b7\u00b7 ,N\u22121 l\u2032=0,\u00b7\u00b7\u00b7 ,L\u22121 \u03b1t\u22121,(j,l\u2032)a\u0303(j,l\u2032),(i,l) (t \u2265 1),\nb\u0303(i,l)(y0)\u03c0\u0303(i,l) (t = 0). (9)\nSince a\u0303(j,l\u2032),(i,l) = 0 unless 0 \u2264 i\u2212 j \u2264 2, the complexity of computing \u03b1t,(i,l) is of O(LN) at each time step.\nIII. INCORPORATING ARBITRARY REPEATS/SKIPS AND FAST SCORE-FOLLOWING ALGORITHMS\nA. Incorporating Arbitrary Repeats/Skips and Computational Complexity for Inference\nSo far, the top HMM is left-to-right and its states are connected only to their neighboring states. However, all top states must be connected to describe arbitrary repeats/skips, i.e. aj,i > 0 for all j and i. The model is a generalization of the performance models in previous studies [5], [11], [15].\nAssuming L = 1 for simplicity and dropping the subscripts l, l\u2032 from the parameters of the standard HMM and the forward variables as a\u0303j,i := a\u0303(j,0),(i,0), b\u0303i(yt) := b\u0303(i,0)(yt), \u03c0\u0303i := \u03c0\u0303(i,0) and \u03b1t,i := \u03b1t,(i,0), Eq. (9) can be rewritten as\n\u03b1t,i = b\u0303i(yt) N\u22121\u2211 j=0 \u03b1t\u22121,j a\u0303j,i (t \u2265 1),\nb\u0303i(y0)\u03c0\u0303i (t = 0).\n(10)\nEq. (10) for t \u2265 1 contains a summation over N states for each i, and the complexity is of O(N2). As we will experimentally show in Sec. IV-A, this complexity is too large to run in real time with scores of practical length on a modern laptop. Therefore, it is crucial to reduce the complexity. It is noteworthy that a similar large complexity can emerge even if only specific repeats/skips are allowed (e.g. transitions between\nthe first notes of bars in a score), since the number of such specific transitions often increases in proportion to N .\nOne may think that pruning techniques can be used to reduce the computational complexity. However, pruning is ineffective here since repeats/skips seldom occur, and it is necessary to take all transitions into account. Computing all transitions has a benefit also in following performances without repeats/skips. When an estimation error of score position occurs, a score follower may fail to track the performance and become lost. It often happens that a score follower with a pruning technique (e.g. with a limited search window) cannot recover from being lost. By contrast, if a score follower searches all transitions, it can return to find the correct score position after a while if the performer continues the performance."}, {"heading": "B. Reduction of Computational Complexity by Factorizing Probabilities of Repeats/Skips", "text": "One method to reduce the computational complexity while computing all transitions is to introduce some constraints on the transition probabilities. In [13], reduction of the computational complexity is achieved with an assumption that the probability of score positions where performers stop before repeats/skips (stop positions) is the same regardless of where they resume performing after repeats/skips (resumption positions).\nWe shall introduce this assumption to the performance HMM. The transition probability of a repeat/skip from event j to event i is then written as a product of two probabilities sj and ri. sj is the probability of stopping at event j before a repeat/skip, and ri is the probability of resuming a performance at event i after a repeat/skip. The transition probability of the top HMM is then written as\naj,i = a (nbh) j,i + sjri. (11)\nwhere a(nbh)j,i is a band matrix satisfying a (nbh) j,i = 0 unless 0 \u2264 i \u2212 j \u2264 2. The parameter a(nbh)j,i characterizes transitions within neighboring states and is determined according to the normalization constraint of aj,i, which is written as 1 = \u2211 i aj,i = \u2211 i a (nbh) j,i + sj \u2211 i ri for all j. Without loss\nof generality, we can assume \u2211 i ri = 1 and then we have\u2211\ni a (nbh) j,i = 1\u2212 sj .\nLet us denote the set of neighboring states of top state i by nbh(i) := {j; j = 0, \u00b7 \u00b7 \u00b7 , N\u22121, 0 \u2264 i\u2212j \u2264 2}. The transition probability of the standard HMM a\u0303j,i for j /\u2208 nbh(i) is written as\na\u0303j,i = e (j) 0 sjri\u03c0 (i) 0 . (12)\nWith Eqs. (12) and (10), we have \u03b1t,i =b\u0303i(yt) { \u2211 j\u2208nbh(i) \u03b1t\u22121,j a\u0303j,i\n+ ri\u03c0 (i) 0 (N\u22121\u2211 j=0 \u03b1t\u22121,je (j) 0 sj \u2212 \u2211 j\u2208nbh(i) \u03b1t\u22121,je (j) 0 sj )} .\n(13)\nSince the first summation in the parentheses of the second term is independent of i, it is sufficient to calculate it once at each time step. This term and the rest of Eq. (13) are of O(N), and hence the total computational complexity is O(N). The space complexity is also reduced: The transition probability matrix in the top level is now parameterized by 4(N \u2212 1) parameters (sj , ri and a (nbh) i,j ). It has N(N \u2212 1) parameters originally.\nThis result can be generalized for the performance HMM with L > 1. The standard HMM has LN states and updating \u03b1t,(i,l) at each time step is of O((LN)2) according to Eq. (9). If we introduce the above assumption, the transition probability of the standard HMM a\u0303(j,l\u2032),(i,l) can also be divided into a component dependent only on i, l and a component dependent only on j, l\u2032. Therefore, the total computational complexity is reduced to O(LN) (see Appendix B for details). Importantly, this reduction method can be used regardless of the topology of the bottom HMMs, and it is compatible with the pause states and applicable to performance HMMs with more complex structure of bottom HMMs (e.g. [6], [20], [24], [25]).\nA similar reduction method is valid for the Viterbi algorithm and the backward algorithm. The method can be applied to any HMM and similar dynamic programming techniques as well, and it can be useful for applications other than score following, (e.g. timbre editing of music signals [26])."}, {"heading": "C. Explicit Description of Silent Breaks at Repeats/Skips", "text": "We can achieve a similar reduction of the computational complexity by using another assumption on arbitrary repeats/skips. Performers frequently make silent breaks at repeats/skips to get ready for resuming the performance. In fact, 59 of 63 repeats/skips accompanied the breaks longer than 500 ms in actual performances used in Sec. IV-C1.\nLet us represent the silent breaks by introducing an additional state (the break state) as the N th top state. The duration of the breaks is described with the self-transition probability of the bottom state of the break state a(N)0,0 , and its value is determined similarly to Eq. (5). Repeats/skips are represented as two-step transitions via the break state (Fig. 3). Stopping (resuming) a performance is expressed as transitions to (from) the break state whose probability is denoted by sj (ri, respectively). We note that the top states excluding the break state are connected only to neighboring top states, and thus a\u0303j,i = 0 if j /\u2208 nbh(i) for all i, j 6= N . On the other hand, the break state is connected to all top states except itself. We put a\u0303N,N = 0. The transition probability of the standard HMM\nfrom or to the break state is written as\na\u0303j,N =e (j) 0 sj\u03c0 (N) 0 (j 6= N), (14)\na\u0303N,i =\n{ e (N) 0 ri\u03c0 (i) 0 (i 6= N),\na (N) 0,0 (i = N)\n(15)\nwhere e(N)0 (= 1 \u2212 a (N) 0,0 ) and \u03c0 (N) 0 (= 1) denote the exiting probability and the initial probability of state (N, 0). For this model, Eq. (10) for t \u2265 1 can be written as\n\u03b1t,i =  b\u0303i(yt) ( \u2211 j\u2208nbh(i) \u03b1t\u22121,j a\u0303j,i + \u03b1t\u22121,N a\u0303N,i ) (i 6= N) b\u0303N (yt) N\u22121\u2211 j=0 \u03b1t\u22121,j a\u0303j,N (i = N). (16) We see that updating \u03b1t,i involves summation of at most four terms for each i 6= N and N terms for i = N . The total complexity is thus O(N) for each time step. This reduction method can also be extended to the case of L > 1 (see Appendix C).\nIt is noteworthy that the performance HMM with the break state is related to the performance HMM presented in Sec. III-B. If we assume that transitions go through the break state in no time, the two-step transition from top state j to top state i via the break state is reduced to the direct transition from top state j to top state i, and its probability is written as a product of sj and ri. In other words, the difference between these models is whether breaks are explicitly described. Since it is difficult to quantify its effect on the performance of score following analytically, we will evaluate the effect through an experiment in Sec. IV-C2."}, {"heading": "IV. EXPERIMENTAL EVALUATION OF THE PROPOSED SCORE-FOLLOWING ALGORITHMS", "text": ""}, {"heading": "A. Processing Time", "text": "We measured processing times in order to evaluate the reduction of the computational complexity with the proposed algorithms. The processing time depends on the number of events N and virtually not on other score content and signal content. We used synthetic scores with 10 to 106 events1 and a random signal of two seconds length with a sampling rate of 16 kHz as an audio input. Normalized CQTs were computed with a frame length of 128 ms and a hopsize of 20 ms. Their center frequencies ranged from 55 to 7040 Hz at a semitone interval, and the quality factor was set to 16, which approximately corresponds to one semitone. Algorithms were implemented in C++ on a computer with 3.30 GHz CPU (Intel(R) Core(TM) i3-2120 CPU) and 8 GB memory running Debian.\nProcessing times averaged over 100 frames with standard errors are shown in Fig. 4 for the algorithms proposed in Sec. III-C (break algorithm) with and without the pause states (L = 2 and L = 1) and the algorithm that calculates \u03b1t,i\n1Practical scores contain O(103) to O(104) notes. For instance, there are around 2200 events in the clarinet part of the first movement in the Mozart\u2019s Clarinet Quintet.\naccording to Eq. (10) (baseline algorithm). (The results for the algorithm proposed in Sec. III-B (no-break algorithm) did not significantly differ with the results for the break algorithms.) It can be confirmed that the average processing times increased asymptotically in proportion to N2 (N ) with the baseline algorithm (the break algorithms, respectively). The result shows that the proposed algorithms significantly suppress the increase of processing times. The processing times for N \u2265 1000 were larger than the hopsize with the baseline algorithm, and the algorithm can work in real time with scores with only up to O(102) events, which is the size of short music pieces. By contrast, the average processing times were below the hopsize for N \u2264 10000 (N \u2264 50000) with the break algorithm with (without, respectively) the pause states. Therefore, the proposed algorithms with and without the pause states can work in real time with scores with up to O(103) events and O(104) events, respectively. Note that processing times depend on the computing power, but their relative values remain almost the same and the proposed algorithms are always effective in reducing the computational complexity."}, {"heading": "B. Score-Following Accuracy for Performances with Errors", "text": "1) Data Preparation: To evaluate the score-following accuracy for performances with errors, we conducted an experiment using the Bach10 dataset [27]. It consists of audio recordings of ten four-part chorales by J. S. Bach. The soprano, alto, tenor and bass parts of each piece were separately recorded and performed by the violin, clarinet, saxophone and bassoon, respectively. Their durations ranged from 25 to 41 seconds.\nSince the performances did not contain errors, we simulated errors by randomly inserting, dropping and substituting notes in each score, which correspond to deletion, insertion and substitution errors in the performance, respectively. Their probability values were obtained from the MIDI piano performances during practice in [13]: 0.0034 for deletion errors and 0.0245 for insertion errors. For simplicity, substitution errors\nwere restricted to three types typical in clarinet performances, namely errors in semitone, whole-tone and perfect 12th. The first two errors are often caused by fingering errors and mis-readings of the score, and the last error is caused by overblowing on a clarinet. The probability values of the three pitch errors were 0.0145, 0.0224 and 0.0047 in the simulation, where the probability of the perfect 12th pitch error was substituted by that of the octave pitch errors obtained in [13].\n2) Experimental Conditions: We conducted a preliminary experiment and set the parameter for performance errors as follows: ai,i+2 = 1.0\u00d7 10\u221250 for deletion errors, ai,i = 0 for insertion errors, and a(i)1,1 = 0.999 and a (i) 0,1 = 1.0\u00d710\u2212100 for pauses between notes. Although the mixture weight w(i)k,0 can be learned from audio signals at each k and i in principle, it is difficult to obtain them independently for the lack of enormous performance data. To reduce the number of parameters, we considered only the most important three substitution errors described in the previous section. The mixture weights w(i)k,0 for the errors were designed in proportion to their frequencies used in the simulation:\nw (i) k,0 =  1\u2212 C (k = pi) C \u00d7 0.175 (k = pi \u00b1 1) C \u00d7 0.270 (k = pi \u00b1 2) C \u00d7 0.055 (k = pi \u00b1 19) 0 (otherwise)\n(17)\nfor all pi 6= \u22121, where C is the probability of pitch errors. The value of C was optimized in a preliminary experiment and we set C = 1.0\u00d7 10\u221250. For pi = \u22121, we put w(i)k,0 = 0 unless k = \u22121. The probabilities of stopping and resuming a performance sj , ri were set uniformly in i, j: s0 = s1 = \u00b7 \u00b7 \u00b7 = sN\u22121 = 1.0 \u00d7 10\u2212x for some positive x and r0 = r1 = \u00b7 \u00b7 \u00b7 = rN\u22121 = 1/N . Since the value of a(N)0,0 did not significantly change the result in a preliminary experiment, we fixed a(N)0,0 = 0.996.\nThe accuracy of score following generally depends on the parameters of the emission probabilities. It has been reported that learning them from audio performances improves the accuracy [10], [22], and thus we learned the parameters \u00b5k and \u03a3k from audio signals. The parameters can be learned from every musical instrument if necessary data is available and we can form a detailed model for a specific instrument. Alternatively, we can use a set of data consisting of several musical instruments to form a \u201cgeneral model\u201d that can be applied for a wider class of instruments. Such a learning method is applicable for any instruments in principle, and it can be even more effective for musical instruments with complex signals, for which physical modeling or manual spectrum-template construction is more difficult. In general, there is a tradeoff between the generalization capability and the adaptation ability. Here, we learned the parameters with performance data of several musical instruments and used them to measure the accuracy of score following.\nThe learning data consisted of performances played by the violin and clarinet in RWC musical instrument database [28]. To reduce overfitting, we assumed that \u03a3k is diagonal and\nintroduced a lower bound, or a flooring value F , on the diagonal elements of \u03a3k. The introduction of F is called the flooring method and generally used for speech recognition (e.g. see [29]). We conducted a preliminary experiment and found the optimal F = 1.0\u00d7 10\u22124. The initial probabilities were set as \u03c0i = 0 for i 6= 0 and \u03c00 = 1.\nWe compared the proposed algorithms with Antescofo [6], which is one of the most known score-following systems applied to various musical pieces and used in the most severe artistic situations. Antescofo was not developed to cope with repeats/skips in monophonic performances, and is without special treatments for repeats/skips. It had the best accuracy in the music information retrieval evaluation exchange (MIREX 2006) [30], which is the most famous evaluation contest in this field. Since Antescofo ended score following when the last note in the score was estimated, estimated score positions were assumed to be the last note from the time when Antescofo ended score following. The overall accuracy of score following was measured by piecewise precision rate (PPR), defined as the piecewise average rate of onsets correctly detected within \u2206 ms error. The PPR has been used with \u2206 = 2000 ms in MIREX [30], [31].\n3) Results: Tab. I summarizes average PPRs and standard errors with \u2206 = 300 ms for every musical instrument. The results for the no-break algorithm did not significantly differ with the results for the break algorithm when sj = 0.0. We found that the proposed algorithms provided similar accuracies for the saxophone and bassoon data, which were not contained in the learning data, compared to the clarinet and violin data. The PPRs obtained with the proposed algorithms were similar to those obtained with Antescofo in all data.\nFig. 5 illustrates average PPRs and standard errors with \u2206 = 300 ms. As described in Sec. III-A, computing all transitions help that the score follower returns to recover from being lost. The benefit can be confirmed from that the proposed\nalgorithms with sj = 1.0 \u00d7 10\u22121000 provided around 0.05 higher accuracy than Antescofo, which searches only local transitions. On the other hand, sjs larger than 1.0 \u00d7 10\u2212500 caused the frequent overdetection of repeats/skips and the accuracy became lower than sj = 0. A similar tendency was observed in PPR with \u2206 = 500 and 2000 ms.\nLarge values of sj deteriorated the score-following accuracy of the present algorithms as shown in Fig. 5. This is because the larger sj , the more frequently the algorithms may misestimate insertion/deletion/substitution errors as repeats/skips. We indeed confirmed that the number of misdetected repeats/skips increased with larger sj .\nThere was around 0.1 difference in PPR between the algorithms when sj is large. We found that the total number of misdetected repeats/skips by the no-break algorithm was around 1.2 times larger than that of the break algorithm for sj \u2265 10\u221210. Since the break algorithm assumes that repeats/skips always accompany breaks and simulated errors did not accompany pauses, the results suggest that the explicit description of the breaks reduced misestimations of the errors as repeats/skips."}, {"heading": "C. Score-Following Accuracy for Performances with Errors and Repeats/Skips", "text": "1) Performance Data During Practice: We collected 16 audio recordings of clarinet performances with a time range of 31 to 213 s (totally 28 min 48 s). We requested an amateur clarinetist to freely practice seven music pieces containing classical and popular music pieces and nursery rhymes, partially from RWC music database [28]. His performances were recorded with a vibration microphone attached to the clarinet.\nThe performances were aligned to the notes in the scores by one of the authors. The total number of performed notes was 2672, and Tab. II lists the count of errors and repeats/skips. Tab. III summarizes differences in score times before and after repeats/skips in the performance data, and we see that they contain repeats/skips between remote score positions. Here, only breaks and pauses between notes longer than 500 ms were counted since it is difficult to accurately annotate offsets of performed notes and short silent breaks and pauses between notes. All transitions with j /\u2208 nbh(i) were counted\nTABLE III. STATISTICS OF DIFFERENCES IN SCORE TIMES BEFORE AND AFTER REPEATS/SKIPS IN THE PERFORMANCE DATA. \u201cQU.\u201d IS AN ABBREVIATION FOR QUARTILE.\nScore time Min. 1st Qu. Median Mean 3rd Qu. Max. In second \u221284.83 \u221215.5 \u22127.75 \u22128.775 \u22121.875 45.750 In event \u2212331 \u221244 \u221225 \u221223.35 \u22124 178\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n10 -10\n0\n10 -10\n1\n10 -10\n2\n10 -10\n3\n10 -10\n4 0.0\nP ie\nce w\nis e\np re\nci si\no n\nr a\nte\nBreak No-Break\nAntescofo\nFig. 6. Average piecewise precision rates with standard errors with respect to sj . The algorithms are same as in Fig. 5.\nas repeats/skips, where i and j denote stop and resumption positions.\n2) Results: The parameters were same as in Sec. IV-B2. To measure how well the algorithms followed repeats/skips, we calculated a detection rate of repeats/skips and the time interval between a repeat/skip and its detection, which we call following time. A repeat/skip was defined to be detected if there was a correctly estimated frame until the next repeat/skip or the end of the audio recording.\nFig. 6 illustrates average PPRs with standard errors for \u2206 = 300 ms. Both proposed algorithms outperformed Antescofo at all sis, clearly showing that the proposed algorithms are effective in following performances with errors and repeats/skips. A similar tendency was observed in PPR with \u2206 = 100, 500 and 2000 ms. We also measured the effect of adding the pause states in the proposed algorithms with si = 1.0\u00d7 10\u2212100, and found that it increased PPRs by 0.05 on average.\nTab. IV summarizes the detection rates of repeats/skips, and Fig. 7 illustrates averages of following times over all detected repeats/skips (average following times) and standard errors in second. Since the standard error for Antescofo was too large to display in the figure, only the average value is shown. Both proposed algorithms clearly outperformed Antescofo in the detection rate and the following time. For example, compared to Antescofo, both proposed algorithms with sj = 1.0\u00d710\u2212100 detected 14 times more repeats/skips and caught up with them 20 times faster in second. These results show that the proposed models are effective for repeats/skips.\nThe break algorithm (the no-break algorithm) with sj = 1.0 \u00d7 10\u2212100 detected 56 (57) repeats/skips, but failed to\ndetect seven (six, respectively) repeats/skips. These failures were caused by the existence of sections and phrases similar to each other in the scores (e.g. choruses in popular music) and considerably short performances between repeats/skips. For example, nine performances between repeats/skips were below five seconds.\nMost of the repeats/skips accompanied silent breaks, but the break algorithm provided similar results to the no-break algorithm. This is because the top states associated with rests can play the same role of the break state since these top states were connected to all top states.\nFurthermore, we measured following times and detection rates for performances played by other musical instruments. The audio recordings in the Bach10 dataset did not contain repeats/skips, and we synthesized performances containing repeats/skips by randomly jumped between breaks in each recording with a probability of 0.1 and inserting silent breaks at repeats/skips. The durations of the breaks were sampled uniformly from 0.5 to 30 seconds and each synthesized performance was forced to contain at least one repeat/skip. After the synthesis, errors were simulated in the same way as in Sec. IV-B1. Tab. V summarizes detection rates of repeats/skips for every musical instrument. The proposed algorithms with sj = 1.0 \u00d7 10\u22121000 outperformed Antescofo in the detection rate, and we found similar tendency in the PPR and the follow-\ning time as shown in Fig. 8 (a) and (b), respectively. These results show that the proposed algorithms are also effective in following performances with errors and repeats/skips for various musical instruments.\nA demonstration video of an automatic accompaniment system using the break algorithm without the pause states is available at https://www.youtube.com/watch?v=fW6VKiC4k34 on Youtube [32]. In the video, the break algorithm successfully follows the performances during practice and catches up the performances after repeats/skips within a few seconds."}, {"heading": "V. DISCUSSIONS", "text": "A. Improvement of the Proposed algorithms We now discuss possible extensions of the proposed algorithms. The stop and resumption positions are not completely random, and their distributions have certain tendencies in actual performances [13]. For example, performers frequently resume from the first beats of bars and the beginning of phrases, which reflects performers\u2019 understanding of musical structures. These tendencies can be incorporated in sj , ri in our performance HMMs, and the accuracy and following times of the proposed algorithms would improve [13].\nAnother method to improve the proposed algorithms is to refine the model of the durations of performed events. For this purpose, we can assign multiple bottom states to\nmodel the duration [20], [24], [25] or explicitly introduce its probability distribution [6]. This refinement is compatible with the proposed methods to reduce the computational cost since they can be used regardless of the topology of the bottom HMMs.\nThe proposed algorithms successfully followed clarinet performances against tempo changes in the experiment and the demonstration video in Sec. IV-C. However, the accuracy may deteriorate for the performances with large tempo changes. To suppress the deterioration, it would be effective to adequately change di on the fly, referring to estimated tempos."}, {"heading": "B. Extension to Polyphonic Music", "text": "Although we have confined ourselves to monophonic performances, let us briefly discuss the polyphonic case. We can construct a performance HMM for polyphonic scores similarly to the monophonic case. By associating top states with musical events (chords, notes and rests) in a polyphonic score, the top HMM can be used without any change, and insertions and deletions of chords, pauses between chords and repeats/skips can be incorporated in the same way. Importantly, the present methods to reduce the computational complexity can be applied to the polyphonic case since it is independent of details of the bottom HMMs. On the other hand, we need to extend the bottom HMMs to include chords. Especially, errors may occur at every note in a chord, and there are a combinatorially large number of possible forms of errors for a large chord. Although we could prepare spectral templates for all possible forms of played chords and use a mixture distribution similarly to Eq. (4) in principle, it requires large computational cost in estimating score positions. However, the influence of note-wise errors in spectral differences is generally less significant for a large chord, and a bold approximation of neglecting note-wise errors would work relatively well for such\na case, which can serve as a practical method to avoid the large computational cost.\nThere are other issues for polyphonic performances. For example, notes in a chord are indicated to be performed simultaneously in the score, but they can be actually performed at different times. Also, relative energy of notes in a chord depends on the performer. Their treatment requires additional discussions and experiments, and the extension to polyphonic performances is now under investigation."}, {"heading": "VI. CONCLUSION", "text": "We discussed score following of monophonic music performances with errors and arbitrary repeats/skips by constructing a stochastic model of music performance. We incorporated possible errors in audio performances into the model. In order to solve the problem of large computational cost for following arbitrary repeats/skips, we presented two HMMs that describe a probability of repeats/skips with a probability of stop positions and a probability of resumption positions, and derived computationally efficient algorithms. We demonstrated real-time working of the algorithms with scores of practical length (O(103) to O(104) events). Experimental evaluations using clarinet performance data showed that the algorithms outperformed Antescofo in the accuracy of score following and the tracking ability of repeats/skips. In addition, we briefly discussed methods to improve the proposed algorithms and extend them for polyphonic inputs."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank Yuu Mizuno and Kosuke Suzuki for participating in the early stage of this work, Naoya Ito for playing the clarinet, and Hirokazu Kameoka for useful discussions. This research was supported in part by JSPS Research Fellowships for Young Scientists No. 15J0992 (T. N.), and JSPS Grant-inAid No. 15K16054 (E. N.) and No. 26240025 (S. S.)."}, {"heading": "A. List of important parameters", "text": "Important parameters of the proposed models are listed in Tab. VI."}, {"heading": "B. Derivation of the No-Break Algorithm for L > 1", "text": "We now derive an efficient algorithm of computing \u03b1t,(i,l) for the performance HMM without the break state in the case of L > 1. Assuming that the transition probability of repeats/skips is described as a product of sj and ri, the transition probability of the standard HMM a\u0303(j,l\u2032),(i,l) for j /\u2208 nbh(i) can be written as\na\u0303(j,l\u2032),(i,l) = e (j) l\u2032 sjri\u03c0 (i) l , (18)\nand Eq. (9) for t \u2265 1 is rewritten as\n\u03b1t,(i,l) =b\u0303(i,l)(yt) ( \u2211\nj\u2208nbh(i) l\u2032=0,\u00b7\u00b7\u00b7 ,L\u22121\n\u03b1t\u22121,(i,l\u2032)a\u0303(j,l\u2032),(i,l)\n+ ri\u03c0 (i) l \u2211 j /\u2208nbh(i)\nl\u2032=0,\u00b7\u00b7\u00b7 ,L\u22121\n\u03b1t\u22121,(j,l\u2032)e (j) l\u2032 sj\n) . (19)\nThe first summation in the parentheses of Eq. (19) is of O(L). The second summation can be converted into\u2211\nj /\u2208nbh(i) l\u2032=0,\u00b7\u00b7\u00b7 ,L\u22121\n\u03b1t\u22121,(j,l\u2032)e (j) l\u2032 sj\n= \u2211\nj=0,\u00b7\u00b7\u00b7 ,N\u22121 l\u2032=0,\u00b7\u00b7\u00b7 ,L\u22121\n\u03b1t\u22121,(j,l\u2032)e (j) l\u2032 sj \u2212 \u2211 j\u2208nbh(i)\nl\u2032=0,\u00b7\u00b7\u00b7 ,L\u22121\n\u03b1t\u22121,(j,l\u2032)e (j) l\u2032 sj .\n(20)\nThe first summation of the right-hand side of Eq. (20) is independent of i and thus it is sufficient to compute it once at each time step. Hence, the total computational complexity at each time step is of O(LN)."}, {"heading": "C. Derivation of the Break Algorithm for L > 1", "text": "Let us consider the performance HMM with the break state and with L bottom states in each top state. In the same way as Sec. III-C, silent breaks at repeats/skips can be introduced as top state N (the break state) and arbitrary repeats/skips are described with two-step transitions via the break state. Since the transition probability of the standard HMM a\u0303(j,l\u2032),(i,l) is zero unless j \u2208 nbh(i) \u222a {N}, Eq. (9) for t \u2265 1 and i 6= N can be rewritten as\n\u03b1t,(i,l) =b\u0303(i,l)(yt) ( \u2211\nj\u2208nbh(i) l\u2032=0,\u00b7\u00b7\u00b7 ,L\u22121\n\u03b1t\u22121,(j,l\u2032)a\u0303(j,l\u2032),(i,l)\n+ L\u22121\u2211 l\u2032=0 \u03b1t\u22121,(N,l\u2032)a\u0303(N,l\u2032),(i,l) ) , (21)\nThe second term in the parentheses of Eq. (21) for each i 6= N is of a constant computational complexity. On the other hand, Eq. (9) for t \u2265 1 and i = N is converted into\n\u03b1t,(N,l) =b\u0303(N,l)(yt) \u2211\nj=0,\u00b7\u00b7\u00b7 ,N\u22121 l\u2032=0,\u00b7\u00b7\u00b7 ,L\u22121\n\u03b1t\u22121,(N,l)a\u0303(j,l\u2032),(N,l). (22)\nThis computation is of O(LN) and hence the total computational complexity is of O(LN) at each time step.\nTomohiko Nakamura He received his B.E and M.S. degrees from the University of Tokyo, Japan, in 2011 and 2013, respectively. He is currently a Ph. D. student at the University of Tokyo and a research fellow of Japan Society for the Promotion of Science (JSPS). His research interests involve audio signal processing and statistical machine learning. He received International Award from the Society of Instrument and Control Engineers (SICE) Annual Conference 2011, SICE Best Paper Award (Takeda Award) in 2015, and Yamashita SIG Research Award\nfrom the Information Processing Society of Japan (IPSJ) in 2015.\nEita Nakamura He received a Ph. D. in physics from the University of Tokyo in 2012. After having been a post-doctoral researcher at the National Institute of Informatics and Meiji University, he is currently a post-doctoral researcher at the Speech and Audio Processing Group at Kyoto University. His research interests include music information processing and statistical machine learning.\nShigeki Sagayama He received the B.E., M.S., and Ph.D. degrees from the University of Tokyo, Tokyo, Japan, in 1972, 1974, and 1998, respectively, all in mathematical engineering and information physics. He joined Nippon Telegraph and Telephone Public Corporation (currently, NTT) in 1974 and started his career in speech analysis, synthesis, and recognition at NTT Labs in Musashino, Japan. From 1990, he was Head of the Speech Processing Department, ATR Interpreting Telephony Laboratories, Kyoto, Japan where he was in charge of an automatic\nspeech translation project. In 1993, he was responsible for speech recognition, synthesis, and dialog systems at NTT Human Interface Laboratories, Yokosuka, Japan. In 1998, he became a Professor of the Graduate School of Information Science, Japan Advanced Institute of Science and Technology (JAIST), Ishikawa. In 2000, he was appointed Professor at the Graduate School of Information Science and Technology (formerly, Graduate School of Engineering), the University of Tokyo. After his retirement from the University of Tokyo, he is a Professor of Meiji University from 2014. His major research interests include the processing and recognition of speech, music, acoustic signals, handwriting, and images. He was the leader of anthropomorphic spoken dialog agent project (Galatea Project) from 2000 to 2003. Prof. Sagayama received the National Invention Award from the Institute of Invention of Japan in 1991, the Director General\u2019s Award for Research Achievement from the Science and Technology Agency of Japan in 1996, and other academic awards including Paper Awards from the Institute of Electronics, Information and Communications Engineers, Japan (IEICEJ) in 1996 and from the Information Processing Society of Japan (IPSJ) in 1995. He is a member of the Acoustical Society of Japan, IEICEJ, and IPSJ."}], "references": [{"title": "An on-line algorithm for real-time accompaniment", "author": ["R.B. Dannenberg"], "venue": "Proc. Int. Computer Music Conf., pp. 193\u2013198, 1984.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1984}, {"title": "The synthetic performer in the context of live performance", "author": ["B. Vercoe"], "venue": "Proc. Int. Computer Music Conf., pp. 199\u2013200, 1984.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1984}, {"title": "Automatic page turning for musicians via real-time machine listening", "author": ["A. Arzt", "G. Widmer", "S. Dixon"], "venue": "Proc. European Conf. Artificial Intelligence, pp. 241\u2013245, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Score following: State of the art and new developments", "author": ["N. Orio", "S. Lemouton", "D. Schwarz", "N. Schnell"], "venue": "Proc. New Interfaces for Musical Expression, pp. 36\u201341, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling form for on-line following of musical performances", "author": ["B. Pardo", "W. Birmingham"], "venue": "Proc. AAAI, vol. 2, pp. 1018\u20131023, 2005.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "A coupled duration-focused architecture for real-time musicto-score alignment", "author": ["A. Cont"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 974\u2013987, June 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A comparative study of tonal acoustic features for a symbolic level music-to-score alignment", "author": ["C. Joder", "S. Essid", "G. Richard"], "venue": "Proc. IEEE Workshop Applications Signal Process. Audio Acoust., pp. 409\u2013412, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "A state space model for online polyphonic audio-score alignment", "author": ["Z. Duan", "B. Pardo"], "venue": "Proc. Int. Conf. Acoust. Speech Signal Process., pp. 197\u2013200, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Realtime audio-to-score alignment using particle filter for coplayer music robots", "author": ["T. Otsuka", "K. Nakadai", "T. Takahashi", "T. Ogata", "H.G. Okuno"], "venue": "EURASIP J. Applied Signal Process., vol. 2011, no. 384651, pp. 1\u201313, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "A conditional random field framework for robust and scalable audio-to-score matching", "author": ["C. Joder", "S. Essid", "G. Richard"], "venue": "IEEE Trans. Acoust., Speech, and Language Process., vol. 19, no. 8, pp. 2385\u2013 2397, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A unified approach to real time audioto-score and audio-to-audio alignment using sequential Montecarlo inference techniques", "author": ["N. Montecchio", "A. Cont"], "venue": "Proc. Int. Conf. Acoust. Speech Signal Process., pp. 193\u2013196, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Acoustic score following to musical performance with errors and arbitrary repeats and skips for automatic accopaniment", "author": ["T. Nakamura", "E. Nakamura", "S. Sagayama"], "venue": "Proc. Sound and Music Computing Conf., pp. 299\u2013304, Aug. 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Outerproduct hidden Markov model and polyphonic MIDI score following", "author": ["E. Nakamura", "T. Nakamura", "Y. Saito", "N. Ono", "S. Sagayama"], "venue": "J. New Music Res., vol. 43, no. 2, pp. 183\u2013201, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Robust polyphonic MIDI score following with hidden Markov models", "author": ["D. Schwarz", "N. Orio", "N. Schnell"], "venue": "Proc. Int. Computer Music Conf., 2004.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "A Piano Duo Performance Support System to Motivate Children\u2019s Practice at Home", "author": ["C. Oshima", "K. Nishimoto", "M. Suzuki"], "venue": "Trans. Info. Process. Soc. Japan, vol. 46, no. 1, pp. 157\u2013170, 2005. in Japanese.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Merged-output hidden Markov model for score following of MIDI performance with ornaments, desynchronized voices, repeats and skips", "author": ["E. Nakamura", "Y. Saito", "N. Ono", "S. Sagayama"], "venue": "Proc. Joint Conf. of 40th Int. Computer Music Conf. and 11th Sound and Music Computing Conf., pp. 1185\u20131192, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A stochastic temporal model of polyphonic MIDI performance with ornaments", "author": ["E. Nakamura", "N. Ono", "S. Sagayama", "K. Watanabe"], "venue": "preparation. [arXiv:1404.2314].", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2314}, {"title": "Handling repeats and jumps in score-performance synchronization", "author": ["C. Fremerey", "M. M\u00fcller", "M. Clausen"], "venue": "Proc. Int. Symposium Music Info. Retrieval, pp. 243\u2013248, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Aligning semi-improvised music audio with its lead sheet", "author": ["Z. Duan", "B. Pardo"], "venue": "Proc. Int. Symposium Music Info. Retrieval, pp. 513\u2013 518, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Score following using spectral analysis and hidden Markov models", "author": ["N. Orio", "F. D\u00e9chelle"], "venue": "Proc. Int. Computer Music Conf., vol. 1001, pp. 1708\u20131710, 2001.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Realtime audio to score alignment for polyphonic music instruments, using sparse non-negative constraints and hierarchical HMMs", "author": ["A. Cont"], "venue": "Proc. Int. Conf. Acoust. Speech Signal Process., vol. 5, pp. 245\u2013248, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning optimal features for polyphonic audio-to-score alignment", "author": ["C. Joder", "S. Essid", "G. Richard"], "venue": "IEEE Trans. Acoust., Speech, and Language Process., vol. 21, pp. 2118\u20132128, Oct 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "An efficient algorithm for the calculation of a constant Q transform", "author": ["J. Brown", "M. Puckette"], "venue": "J. Acoust. Soc. Am., vol. 92, pp. 2698\u20132701, 1992.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "Score-performance matching using HMMs", "author": ["P. Cano", "A. Loscos", "J. Bonada"], "venue": "Proc. Int. Computer Music Conf., pp. 441\u2013444, 1999.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Automatic segmentation of acoustic musical signals using hidden Markov models", "author": ["C. Raphael"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 21, no. 4, pp. 360\u2013370, 1999.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1999}, {"title": "Timbre replacement of harmonic and drum components for music audio signals", "author": ["T. Nakamura", "H. Kameoka", "K. Yoshii", "M. Goto"], "venue": "Proc. Int. Conf. Acoust. Speech Signal Process., pp. 7520\u20137524, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Soundprism: An online system for scoreinformed source separation of music audio", "author": ["Z. Duan", "B. Pardo"], "venue": "IEEE J. Sel. Topics. Signal Process., vol. 5, no. 6, pp. 1205\u20131215, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluation of realtime audio-to-score alignment", "author": ["A. Cont", "D. Schwarz", "N. Schnell", "C. Raphael"], "venue": "Proc. Int. Symposium Music Info. Retrieval, 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Real-time alignment of an audio signal of a music performance to a given score, also known as score following, has been gathering attention since its first appearance in 1984 [1], [2].", "startOffset": 175, "endOffset": 178}, {"referenceID": 1, "context": "Real-time alignment of an audio signal of a music performance to a given score, also known as score following, has been gathering attention since its first appearance in 1984 [1], [2].", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "Score following is a basic technique for realtime musical applications such as automatic accompaniment, automatic score page-turning [3] and automatic captioning to music videos.", "startOffset": 133, "endOffset": 136}, {"referenceID": 3, "context": "Many studies of score following have been carried out (see [4] for a review and [5]\u2013[13] for recent progress).", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "Many studies of score following have been carried out (see [4] for a review and [5]\u2013[13] for recent progress).", "startOffset": 80, "endOffset": 83}, {"referenceID": 12, "context": "Many studies of score following have been carried out (see [4] for a review and [5]\u2013[13] for recent progress).", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "Treatment of errors in score following is discussed in some studies [4], [5], [13], [14].", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "Treatment of errors in score following is discussed in some studies [4], [5], [13], [14].", "startOffset": 73, "endOffset": 76}, {"referenceID": 12, "context": "Treatment of errors in score following is discussed in some studies [4], [5], [13], [14].", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "Treatment of errors in score following is discussed in some studies [4], [5], [13], [14].", "startOffset": 84, "endOffset": 88}, {"referenceID": 4, "context": "Score-following algorithms that can follow repeats/skips have been proposed in [5], [11], [15].", "startOffset": 79, "endOffset": 82}, {"referenceID": 10, "context": "Score-following algorithms that can follow repeats/skips have been proposed in [5], [11], [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "Score-following algorithms that can follow repeats/skips have been proposed in [5], [11], [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "The authors have presented a new type of hidden Markov model (HMM) that describes musical instrument digital interface (MIDI) performances with errors and arbitrary repeats/skips, and derived a computationally efficient algorithm for the HMM [13].", "startOffset": 242, "endOffset": 246}, {"referenceID": 0, "context": "Although monophonic score following has been addressed since [1], [2], ar X iv :1 51 2.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "Although monophonic score following has been addressed since [1], [2], ar X iv :1 51 2.", "startOffset": 66, "endOffset": 69}, {"referenceID": 11, "context": "IV) was reported in our previous conference paper [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "Errors are categorized into pitch errors (substitution errors), dropping notes (deletion errors), adding extra notes (insertion errors) [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "Although it is out of the scope of this paper, there are other sources of variety in music performance such as ornaments [6], [13], [16], [17] and improvisation [18], [19].", "startOffset": 121, "endOffset": 124}, {"referenceID": 12, "context": "Although it is out of the scope of this paper, there are other sources of variety in music performance such as ornaments [6], [13], [16], [17] and improvisation [18], [19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "Although it is out of the scope of this paper, there are other sources of variety in music performance such as ornaments [6], [13], [16], [17] and improvisation [18], [19].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "Although it is out of the scope of this paper, there are other sources of variety in music performance such as ornaments [6], [13], [16], [17] and improvisation [18], [19].", "startOffset": 138, "endOffset": 142}, {"referenceID": 17, "context": "Although it is out of the scope of this paper, there are other sources of variety in music performance such as ornaments [6], [13], [16], [17] and improvisation [18], [19].", "startOffset": 161, "endOffset": 165}, {"referenceID": 18, "context": "Although it is out of the scope of this paper, there are other sources of variety in music performance such as ornaments [6], [13], [16], [17] and improvisation [18], [19].", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "Recent score-following systems commonly use probabilistic models such as HMM to capture the variety of audio performances, and their effectiveness has been well confirmed [4] (and references in the Introduction).", "startOffset": 171, "endOffset": 174}, {"referenceID": 19, "context": "This process of performance can be modeled with a hierarchical HMM with two levels [20], [21], which we call the performance HMM.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "This process of performance can be modeled with a hierarchical HMM with two levels [20], [21], which we call the performance HMM.", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "In the comparison of some audio features in [7], [22], the magnitude of a constant-Q transform (CQT) [23] with a quality factor set to one semitone yielded the best result of score following for monophonic audio input.", "startOffset": 44, "endOffset": 47}, {"referenceID": 21, "context": "In the comparison of some audio features in [7], [22], the magnitude of a constant-Q transform (CQT) [23] with a quality factor set to one semitone yielded the best result of score following for monophonic audio input.", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "In the comparison of some audio features in [7], [22], the magnitude of a constant-Q transform (CQT) [23] with a quality factor set to one semitone yielded the best result of score following for monophonic audio input.", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "Here w k,0 \u2208 [0, 1] is a mixture weight of pitch k of bottom state 0 of top state i, which satisfies \u2211 k\u2208K w (i) k,0 = 1 for all i.", "startOffset": 13, "endOffset": 19}, {"referenceID": 4, "context": "The model is a generalization of the performance models in previous studies [5], [11], [15].", "startOffset": 76, "endOffset": 79}, {"referenceID": 10, "context": "The model is a generalization of the performance models in previous studies [5], [11], [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "The model is a generalization of the performance models in previous studies [5], [11], [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "In [13], reduction of the computational complexity is achieved with an assumption that the probability of score positions where performers stop before repeats/skips (stop positions) is the same regardless of where they resume performing after repeats/skips (resumption positions).", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "[6], [20], [24], [25]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[6], [20], [24], [25]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 23, "context": "[6], [20], [24], [25]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "[6], [20], [24], [25]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 25, "context": "timbre editing of music signals [26]).", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "\u201cBaseline\u201d represents a simple extension of the algorithms proposed in previous studies [5], [11], [15].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "\u201cBaseline\u201d represents a simple extension of the algorithms proposed in previous studies [5], [11], [15].", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "\u201cBaseline\u201d represents a simple extension of the algorithms proposed in previous studies [5], [11], [15].", "startOffset": 99, "endOffset": 103}, {"referenceID": 26, "context": "1) Data Preparation: To evaluate the score-following accuracy for performances with errors, we conducted an experiment using the Bach10 dataset [27].", "startOffset": 144, "endOffset": 148}, {"referenceID": 12, "context": "Their probability values were obtained from the MIDI piano performances during practice in [13]: 0.", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "0047 in the simulation, where the probability of the perfect 12th pitch error was substituted by that of the octave pitch errors obtained in [13].", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": "It has been reported that learning them from audio performances improves the accuracy [10], [22], and thus we learned the parameters \u03bck and \u03a3k from audio signals.", "startOffset": 86, "endOffset": 90}, {"referenceID": 21, "context": "It has been reported that learning them from audio performances improves the accuracy [10], [22], and thus we learned the parameters \u03bck and \u03a3k from audio signals.", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "The break algorithm (\u201cBreak\u201d) and the no-break algorithm (\u201cNo-Break\u201d) without the pause states are compared to Antescofo [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 5, "context": "We compared the proposed algorithms with Antescofo [6], which is one of the most known score-following systems applied to various musical pieces and used in the most severe artistic situations.", "startOffset": 51, "endOffset": 54}, {"referenceID": 27, "context": "The PPR has been used with \u2206 = 2000 ms in MIREX [30], [31].", "startOffset": 54, "endOffset": 58}, {"referenceID": 5, "context": "\u201cPROPOSED (sj = 0)\u201d (\u201cANTESCOFO\u201d) DENOTES THE break algorithm WITH sj = 0 (ANTESCOFO [6], RESPECTIVELY).", "startOffset": 85, "endOffset": 88}, {"referenceID": 12, "context": "The stop and resumption positions are not completely random, and their distributions have certain tendencies in actual performances [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "These tendencies can be incorporated in sj , ri in our performance HMMs, and the accuracy and following times of the proposed algorithms would improve [13].", "startOffset": 151, "endOffset": 155}, {"referenceID": 19, "context": "For this purpose, we can assign multiple bottom states to model the duration [20], [24], [25] or explicitly introduce its probability distribution [6].", "startOffset": 77, "endOffset": 81}, {"referenceID": 23, "context": "For this purpose, we can assign multiple bottom states to model the duration [20], [24], [25] or explicitly introduce its probability distribution [6].", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "For this purpose, we can assign multiple bottom states to model the duration [20], [24], [25] or explicitly introduce its probability distribution [6].", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "For this purpose, we can assign multiple bottom states to model the duration [20], [24], [25] or explicitly introduce its probability distribution [6].", "startOffset": 147, "endOffset": 150}], "year": 2015, "abstractText": "This paper discusses real-time alignment of audio signals of music performance to the corresponding score (a.k.a. score following) which can handle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips) in performances. This type of score following is particularly useful in automatic accompaniment for practices and rehearsals, where errors and repeats/skips are often made. Simple extensions of the algorithms previously proposed in the literature are not applicable in these situations for scores of practical length due to the problem of large computational complexity. To cope with this problem, we present two hidden Markov models of monophonic performance with errors and arbitrary repeats/skips, and derive efficient score-following algorithms with an assumption that the prior probability distributions of score positions before and after repeats/skips are independent from each other. We confirmed real-time operation of the algorithms with music scores of practical length (around 10000 notes) on a modern laptop and their tracking ability to the input performance within 0.7 s on average after repeats/skips in clarinet performance data. Further improvements and extension for polyphonic signals are also discussed. Keywords\u2014Score following, audio-to-score alignment, arbitrary repeats and skips, fast Viterbi algorithm, hidden Markov model, music signal processing", "creator": "LaTeX with hyperref package"}}}