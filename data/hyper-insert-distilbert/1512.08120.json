{"id": "1512.08120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Dec-2015", "title": "Regularized Orthogonal Tensor Decompositions for Multi-Relational Learning", "abstract": "multi - relational performance learning has usually received extremely lots focus of attention recently from researchers in various research engineering communities. most existing research methods either would suffer results from superlinear per - third iteration cost, or to are considered sensitive as to the various given ranks. to address effectively both issues, we propose a scalable infinite core likelihood tensor trace norm uniformly regularized stable orthogonal iteration decomposition ( roid ) method for full results or almost incomplete tensor algebra analytics, results which can be generalized as a full graph via laplacian regularized error version by using dedicated auxiliary information streams or simply a sparse sequential higher - order stationary orthogonal iteration ( shooi ) version. we require first induce the posterior equivalence relation of the schatten p - sparse norm ( nt 0 & lt ; ( p & db lt ; \\ infty ) type of determining a feasible low multi - linear rank orthogonal tensor block and derive its core residual tensor. yet then we achieve through a less much smaller matrix trace norm uniform minimization coefficient problem. finally,... we develop two efficient algorithm augmented generalized lagrange multiplier algorithms to solve our problems incompatible with convergence achievement guarantees. extremely extensive experiments using absolutely both real and synthetic datasets, even though with only occasionally a few validation observations, verified both the complexity efficiency characteristics and practical effectiveness of our methods.", "histories": [["v1", "Sat, 26 Dec 2015 15:26:05 GMT  (768kb)", "https://arxiv.org/abs/1512.08120v1", "18 pages, 10 figures"], ["v2", "Sat, 16 Jan 2016 15:32:15 GMT  (700kb)", "http://arxiv.org/abs/1512.08120v2", "18 pages, 10 figures"]], "COMMENTS": "18 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["fanhua shang", "james cheng", "hong cheng"], "accepted": false, "id": "1512.08120"}, "pdf": {"name": "1512.08120.pdf", "metadata": {"source": "CRF", "title": "Regularized Orthogonal Tensor Decompositions for Multi-Relational Learning", "authors": ["Fanhua Shang", "Hong Cheng"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n08 12\n0v 2\n[ cs\n.L G\n] 1\n6 Ja\nn 20\n16 JOURNAL OF LATEX CLASS FILES, VOL. 13, NO. 9, SEPTEMBER 2014 1 Regularized Orthogonal Tensor Decompositions for Multi-Relational Learning Fanhua Shang, Member, IEEE, James Cheng, and Hong Cheng\nAbstract\u2014Multi-relational learning has received lots of attention from researchers in various research communities. Most existing methods either suffer from superlinear per-iteration cost, or are sensitive to the given ranks. To address both issues, we propose a scalable core tensor trace norm Regularized Orthogonal Iteration Decomposition (ROID) method for full or incomplete tensor analytics, which can be generalized as a graph Laplacian regularized version by using auxiliary information or a sparse higher-order orthogonal iteration (SHOOI) version. We first induce the equivalence relation of the Schatten p-norm (0<p<\u221e) of a low multi-linear rank tensor and its core tensor. Then we achieve a much smaller matrix trace norm minimization problem. Finally, we develop two efficient augmented Lagrange multiplier algorithms to solve our problems with convergence guarantees. Extensive experiments using both real and synthetic datasets, even though with only a few observations, verified both the efficiency and effectiveness of our methods.\nIndex Terms\u2014Multi-relational learning, tensor completion and decomposition, link prediction, low multi-linear rank, graph Laplacian\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "R ELATIONAL learning is becoming increasingly impor-tant because of the high value hidden in relational data and also of its many applications in various domains such as social networks, the semantic web, bioinformatics, and the linked data cloud [1]. A class of relational learning methods focus mostly on the problem of modeling a single relation type, such as relational learning from latent attributes [2], [3], which models relations between objects as resulting from intrinsic latent attributes of these objects. But in reality, relational data typically involve multiple types of relations between objects or attributes, which can themselves be similar. For example, in social networks [4], relationships between individuals may be personal, familial, or professional. This type of relational data learning is often referred to as multi-relational learning (MRL), which needs to model large-scale sparse relational databases efficiently [5].\nPeople usually make use of the semantic web\u2019s RDF formalism to represent relational data, where relations are modeled as triples of the form (subject, relation, object), and a relation either denotes the relationship between two entries or between an entity and an attribute value. Considering the multiple types of relationships, it is a more natural way stacking the matrices of observed relationships into one big sparse three-order tensor. Fig. 1 shows an illustration of this modeling method. In recent years, tensors have become ubiquitous such as multi-channel images and videos, and become popular due to the ability to discover complex and interesting latent structures and correlations of data [6], [7], [8], [9]. Recently there is a growing interest\n\u2022 F. Shang (Corresponding author) and J. Cheng are with the Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong. E-mail: {fhshang, jcheng}@cse.cuhk.edu.hk.\n\u2022 H. Cheng is with the Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong. E-mail: hcheng@se.cuhk.edu.hk.\nManuscript received December 6, 2014; revised July 3, 2015.\nin tensor methods for link prediction tasks, partially due to their natural representation of multi-relational data.\nTensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15]. For example, Bader et al. [16] proposed a three-way component decomposition model for analyzing intrinsically asymmetric relationships. In addition, Nickel et al. [1] incorporated collective learning into the tensor factorization, which is designed to account for the inherent structure of relational data. Two of the most popular tensor factorizations are the Tucker decomposition [10] and the CANDECOMP/PARAFAC (CP) decomposition [11]. To address incomplete tensor estimation, two weighted alternating leastsquares methods [8], [17] were proposed. However, these methods require the ability to reliably estimate the rank of the involved tensor [18], [19].\nRecently, the low rank tensor recovery problem has been intensively studied. Liu et al. [20] first extended the trace norm (also known as the nuclear norm [21] or the Schatten 1-norm [19]) regularization for partially observed lowmultilinear rank tensor recovery. Then the tensor recovery problem is transformed into a convex combination of trace norm minimization of the matrix unfolding along each mode. More recently, in Liu et al.\u2019s subsequent paper [22], they proposed three efficient algorithms to solve the lowmulti-linear rank tensor completion problem. Some similar algorithms\ncan also be found in [18], [23], [24], [25]. In addition, there are some theoretical developments that guarantee the reconstruction of a low rank tensor from partial measurements by solving trace norm minimization under some reasonable conditions [25], [26], [27], [28]. However, the tensor trace norm minimization problems have to be solved iteratively and involve multiple singular value decompositions (SVDs) in each iteration. Therefore, existing algorithms suffer from high computational cost, making them impractical for realworld applications [19], [29].\nTo address both of the issues mentioned above, i.e., the robustness of given ranks and the computational efficiency, we propose a scalable core tensor trace norm Regularized Orthogonal Iteration Decomposition (ROID) method for full or incomplete tensor analytics. We first induce the equivalence relation of the Schatten p-norm (0< p <\u221e) of a low multi-linear rank tensor and its core tensor. We use the trace norm of the core tensor to replace that of the whole tensor, and then achieve a much smaller scale matrix trace norm minimization problem. In particular, our ROID method is generalized as a graph Laplacian regularized version by using auxiliary information from the relationships or a sparse higher-order orthogonal iteration (SHOOI) version. Finally, we develop two efficient augmented Lagrange multiplier (ALM) algorithms for our problems. Moreover, we theoretically analyze the convergence property of our algorithms. Our experimental results on real-world datasets verified both the efficiency and effectiveness of our methods.\nThe rest of the paper is organized as follows. We review preliminaries and related work in Section 2. In Section 3, we propose two novel core tensor trace norm regularized tensor decomposition models, and develop two efficient ALM algorithms and extend one algorithm to solve the SHOOI problem in Section 4. We provide the theoretical analysis of our algorithms in Section 5. We report the experimental results in Section 6. In Section 7, we conclude this paper and point out some potential extensions for future work."}, {"heading": "2 NOTATIONS AND PROBLEM FORMULATIONS", "text": "A third-order tensor is denoted by a calligraphic letter, e.g., X \u2208RI1\u00d7I2\u00d7I3 , and its entries are denoted as xi1i2i3 , where in \u2208 {1, . . . , In} for 1 \u2264 n \u2264 3. Fibers are the higher-order analogue of matrix rows and columns. The mode-n fibers of a third-order tensor are x:i2i3 , xi1:i3 and xi1i2:, respectively.\nThe mode-n unfolding, also known as matricization, of a third-order tensor X \u2208 RI1\u00d7I2\u00d7I3 is denoted by X(n)\u2208R\nIn\u00d7\u03a0j 6=nIj and arranges the mode-n fibers to be the columns of the resulting matrix X(n) such that the moden fiber becomes the row index and all other two modes become column indices. The tensor element (i1, i2, i3) is mapped to the matrix element (in, j), where\nj = 1 + 3\u2211\nk=1,k 6=n\n(ik \u2212 1)Jk with Jk = k\u22121\u220f\nm=1,m 6=n\nIm.\nThe inner product of two same-sized tensors A \u2208 R I1\u00d7I2\u00d7I3 and B \u2208 RI1\u00d7I2\u00d7I3 is the sum of the product of their entries, \u3008A, B\u3009 = \u2211\ni1,i2,i3 ai1i2i3bi1i2i3 . The Frobenius\nnorm of a third-order tensor X is defined as:\n\u2016X\u2016F := \u221a \u3008X ,X\u3009 =\n\u221a\u221a\u221a\u221a I1\u2211\ni1=1\nI2\u2211\ni2=1\nI3\u2211\ni3=1\nx2i1i2i3 .\nThe 1-mode product of a tensor X \u2208RI1\u00d7I2\u00d7I3 with a matrix U \u2208RJ\u00d7I1 , denoted by X\u00d71U \u2208R J\u00d7I2\u00d7I3 , is defined as:\n(X\u00d71U)ji2i3 = I1\u2211\ni1=1\nxi1i2i3uji1 ."}, {"heading": "2.1 Tensor Trace Norm", "text": "With an exact analogue to the definition of the matrix rank, the rank of a tensor X is defined as the smallest number of rank-one tensors that generate X as their sum. However, there is no straightforward way to determine the rank of a tensor. In fact, the problem is NP-hard [6], [30]. Fortunately, the multi-linear rank (also called the Tucker rank in [27], [31]) of a tensor X is easy to compute, and consists of the ranks of all mode-n unfoldings.\nDefinition 1. The multi-linear rank of a third-order tensor X is the tuple of the ranks of the mode-n unfoldings,\nmulti-linear rank= [ rank(X(1)), rank(X(2)), rank(X(3)) ] .\nIn order to keep problems simple, the (weighted) sum of the ranks of all unfoldings along each mode is used to take the place of the multi-linear rank of the tensor, and is relaxed into the following definition.\nDefinition 2. The Schatten p-norm (0<p<\u221e) of a third-order tensor X is the average of the Schatten p-norms of all unfoldings X(n), i.e.,\n\u2016X\u2016Sp = 1\n3\n3\u2211\nn=1\n\u2016X(n)\u2016Sp\nwhere \u2016X(n)\u2016Sp =( \u2211 i\u03c3 p i ) 1/p denotes the Schatten p-norm of the unfolding X(n), and \u03c3i is the i-th singular value of X(n). When p=1, the Schatten 1-norm is the well-known trace norm, \u2016X\u2016\u2217.\nFor some imbalance sparse tensor decomposition problems (e.g., the size of the YouTube data used in Section 6.2 is 4, 117\u00d74, 117\u00d75), the trace norm of the tensor can be incorporated by some pre-specified weights \u03b1n \u2265 0, n = 1, 2, 3, which satisfy \u2211 n\u03b1n = 1."}, {"heading": "2.2 Weighted Tensor Decompositions", "text": "We will introduce two of the most often used tensor decomposition models for MRL problems. In [8], Acar et al. presented a weighted CANDECOMP/PARAFAC (WCP) decomposition model for sparse third-order tensors:\nmin A,B,C\n1\n2\n\u2211\ni,j,k\nwijk ( tijk \u2212 R\u2211\nr=1\nairbjrckr\n)2 (1)\nwhere R is a positive integer, W denotes a non-negative indicator tensor of the same size as an incomplete tensor T : wijk = 1 if tijk is observed andwijk = 0 otherwise, andA \u2208 R I1\u00d7R, B \u2208 RI2\u00d7R, C \u2208 RI3\u00d7R are referred to as the factor matrices which are the combination of the vectors from the rank-one components (e.g., A = [a:,1, a:,2, . . . , a:,R]).\nIn [17], the weighted Tucker decomposition (WTucker) model is formulated as follows:\nmin G,U,V,W\n1 2 \u2016W \u2217 (T \u2212 G\u00d71U\u00d72V \u00d73W )\u2016 2 F (2)\nwhere \u2217 denotes the Hadamard (element-wise) product, U \u2208 R I1\u00d7R1 , V \u2208RI2\u00d7R2 , W \u2208 RI3\u00d7R3 , and G \u2208RR1\u00d7R2\u00d7R3 is a\ncore tensor with the given multi-linear rank (R1, R2, R3). Since the decomposition rank Rn (n= 1, 2, 3) is in general much smaller than In, in this sense, the storage of the Tucker decomposition form can be significantly smaller than that of the original tensor. Moreover, unlike the rank of the tensor R, the multi-linear rank (R1, R2, R3) is clearly computable. If the factor matrices of the Tucker decomposition are constrained orthogonal, the classical decomposition methods are referred to as the higher-order singular value decomposition (HOSVD) [32] or higher-order orthogonal iteration (HOOI) [33], where the latter leads to the estimation of best rank-(R1, R2, R3) approximations while the truncation of HOSVD may achieve a good rank-(R1, R2, R3) approximation but in general not the best possible one [33]. Hence, we are particularly interested in extending the HOOI method for sparse MRL problems.\nIn addition, several extensions of both tensor decomposition models are developed for tensor estimation problems, such as [34], [35], [36]. However, for all those methods, a suitable rank value needs to be given, and it has been shown that both WTucker andWCP models are usually sensitive to the given ranks due to their least-squares formulations [18], [19], and they have poor performance when the data have a high rank [22]."}, {"heading": "2.3 Problem Formulations", "text": "For multi-relational prediction, the sparse tensor trace norm minimization problem is formulated as follows:\nmin X\n3\u2211\nn=1\n\u03b1n\u2016X(n)\u2016\u2217, s.t., X\u2126 = T\u2126 (3)\nwhere \u03b1n\u2019s are pre-specified weights, and \u2126 is the set of indices of observed entries. Liu et al. [22] proposed three efficient algorithms (e.g., the HaLRTC algorithm) to solve (3). In addition, there are some similar convex tensor completion algorithms in [18], [23], [24]. Tomioka and Suzuki [25] proposed a latent trace norm minimization model,\nmin Xn\n1\n\u03bb\nN\u2211\nn=1\n\u2016Xn,(n)\u2016\u2217 + 1\n2 \u2016P\u2126(\nN\u2211\nn=1\nXn)\u2212 P\u2126(T )\u2016 2 F (4)\nwhere P\u2126 is the projection operator: P\u2126(T )ijk = Tijk if (i, j, k) \u2208 \u2126 and P\u2126(T )ijk = 0 otherwise, and \u03bb > 0 is a regularization parameter.\nMore recently, it has been shown that the tensor trace norm minimization models mentioned above can be substantially suboptimal [27], [37]. However, if the order of the involved tensor is no more than three, the models (3) and (4) often perform better than the more balanced (square) matrix model in [27]. Indeed each unfolding X(n) shares the same entries, and thus cannot be optimized independently. Therefore, we must apply variable splitting and introduce multiple additional equal-sized variables to all unfoldings of X . Moreover, existing algorithms involve multiple SVDs in each iteration and suffer from high computational cost O(I4), where the assumed size of the tensor is I \u00d7 I \u00d7 I ."}, {"heading": "3 CORE TENSOR TRACE NORM REGULARIZED TENSOR DECOMPOSITION", "text": "To address the poor scalability of existing low multi-linear rank tensor recovery algorithms, we present two scalable\ncore tensor trace norm (or together with graph Laplacian) regularized orthogonal decomposition models, and then achieve three smaller-scale matrix trace norm minimization problems. Then in Section 4, we will develop some efficient algorithms for solving the problems."}, {"heading": "3.1 Core Tensor Trace Norm Minimization Models", "text": "Assume that X \u2208RI1\u00d7I2\u00d7I3 is a multi-relational tensor with multi-linear rank (r1, r2, r3), X can be decomposed as:\nX = G\u00d71U\u00d72V \u00d73W (5)\nwhere U \u2208 RI1\u00d7r1 , V \u2208 RI2\u00d7r2 and W \u2208 RI3\u00d7r3 are the column-wise orthonormal matrices, and can be thought of as the principal components in each mode. The entries of the core tensor G \u2208Rr1\u00d7r2\u00d7r3 show the level of interaction between the different components. For rn (n = 1, 2, 3), we recommend a matrix rank estimation approach recently developed in [38] to compute some good values (r\u20321, r \u2032 2, r \u2032 3) for the multi-linear rank of the involved tensor. Then we can give some relatively large integers (d1, d2, d3) satisfying dn \u2265 r \u2032 n and dn \u2265 rn, n = 1, 2, 3.\nTheorem 1. Let X \u2208 RI1\u00d7I2\u00d7I3 with multi-linear rank (r1, r2, r3) and G \u2208 R\nd1\u00d7d2\u00d7d3 satisfy X = G\u00d71U\u00d72V \u00d73W , and UTU = Id1 , V TV = Id2 and W TW = Id3 , then\n\u2016X\u2016Sp = \u2016G\u2016Sp\nwhere \u2016X\u2016Sp and \u2016G\u2016Sp denote the Schatten p-norm of X and its core tensor G, respectively.\nThe proof of Theorem 1 is given in APPENDIX A. Since the trace norm (i.e., the Schatten 1-norm) is the tightest convex surrogate to the rank function [21], [39], we mainly consider the trace norm case in this paper. According to the equivalence relation of the trace norm of a low multi-linear rank tensor and its core tensor, the tensor completion model (3) is formulated into the following form:\nmin G,U,V,W,X\n1 \u03bb \u2016G\u2016\u2217 + 1 2 \u2016X \u2212 G\u00d71U\u00d72V \u00d73W\u2016 2 F ,\ns.t.,X\u2126=T\u2126, U TU=Id1 , V TV =Id2 ,W TW =Id3 .\n(6)\nWhen all entries of T are observed, the model (6) degenerates to the following core tensor trace norm regularized tensor decomposition problem [29]:\nmin G,U,V,W\n1 \u03bb \u2016G\u2016\u2217 + 1 2 \u2016T \u2212 G\u00d71U\u00d72V \u00d73W\u2016 2 F ,\ns.t., UTU=Id1 , V TV =Id2 ,W TW =Id3 .\n(7)\nIt is clear that the core tensor G of size (d1, d2, d3) has much smaller size than the whole tensor T , i.e., dn \u226a In for all n \u2208 {1, 2, 3}. Therefore, our core tensor trace norm regularized orthogonal tensor decomposition models (6) and (7) can alleviate the SVD computational burden of much larger unfoldings in both models (3) and (4). Besides, the core tensor trace norm term promotes low multi-linear rank tensor decompositions, and enhances the robustness of the multi-linear rank selection, while those traditional tensor decomposition methods are usually sensitive to the given multi-linear rank [22], [29]."}, {"heading": "3.2 Sparse HOOI Model", "text": "When \u03bb \u2192 \u221e, the model (6) degenerates to the following sparse tensor HOOI (SHOOI) problem,\nmin G,U,V,W,Z\n1 2 \u2016W \u2217 (Z \u2212 T )\u20162F ,\ns.t.,Z=G\u00d71U\u00d72V\u00d73W,U TU=Id1, V TV=Id2,W TW=Id3 .\n(8)\nIn a sense, the SHOOI model (8) is a special case of our ROIDmethod (see the SupplementaryMaterials for detailed discussion). When all entries of T are observed, the SHOOI model (8) becomes a traditional HOOI problem in [33]."}, {"heading": "3.3 Graph Regularized Model", "text": "Inspired by the work in [40], [41], [42], we also exploit the auxiliary information given as link-affinity matrices in a graph regularized ROID (GROID) model:\nmin G,U,V,W,X\n1 \u03bb \u2016G\u2016\u2217 + 1 2 \u2016X \u2212 G\u00d71U\u00d72V \u00d73W\u2016 2 F\n+ \u00b5\n2 [Tr(UTL1U) + Tr(V TL2V ) + Tr(W TL3W )],\ns.t., X\u2126 = T\u2126, U TU = Id1 , V TV = Id2 ,W TW = Id3\n(9)\nwhere \u00b5 \u2265 0 is a regularization constant, Tr(\u00b7) denotes the matrix trace, Ln is the graph Laplacian matrix, i.e., Ln = Dn\u2212Wn, Wn is the weight matrix for the object set On or different relations, and Dn is the diagonal matrix whose entries are column sums of Wn, i.e., (Dn)ii = \u2211 j(Wn)ij ."}, {"heading": "4 OPTIMIZATION ALGORITHMS", "text": "In this section, we propose an efficient method of augmented Lagrange multipliers (ALM) to solve our ROID problem (6), and then extend the proposed algorithm to solve (7)-(9). As a variant of the standard ALM, the alternating direction method of multipliers (ADMM) has received much attention recently due to the tremendous demand from large-scale machine learning applications [43], [44]. Similar to (3), the proposed problem (6) is difficult to solve due to the interdependent tensor trace norm term \u2016G\u2016\u2217. Therefore, we first introduce three much smaller auxiliary variables Gn \u2208R\ndn\u00d7\u03a0j 6=ndj into (6), and then reformulate it into the following equivalent form:\nmin G,U,V,W,{Gn},X\n3\u2211\nn=1\n\u2016Gn\u2016\u2217 3\u03bb + 1 2 \u2016X \u2212 G\u00d71U\u00d72V\u00d73W \u2016 2 F ,\ns.t.,X\u2126=T\u2126,G(n)=Gn, U T U=Id1 , V T V=Id2 ,W T W=Id3 .\n(10)\nThe partial augmented Lagrangian function of (10) is\nL\u03c1({Gn}, G, U, V, W, X , {Yn}) = 3\u2211\nn=1\n( \u2016Gn\u2016\u2217 3\u03bb + \u3008Yn, G(n) \u2212Gn\u3009+ \u03c1 2 \u2016G(n) \u2212Gn\u2016 2 F )\n+ 1\n2 \u2016X \u2212 G\u00d71U\u00d72V \u00d73W\u2016\n2 F\n(11)\nwhere Yn \u2208 R dn\u00d7\u03a0j 6=ndj (n = 1, 2, 3) are the matrices of Lagrange multipliers (or dual variables), and \u03c1> 0 is called the penalty parameter. Our ADMM iterative scheme for solving (10) is derived by successively minimizing L\u03c1 over ({Gn},G, U, V,W,X ), and then updating (Y1, Y2, Y3).\n4.1 Updating {Gk+11 , G k+1 2 , G k+1 3 } By keeping all the other variables fixed, Gk+1n is updated by solving the following problem,\nmin Gn \u2016Gn\u2016\u2217 3\u03bb + \u03c1k 2 \u2016Gk(n) \u2212Gn + Y k n /\u03c1 k\u20162F . (12)\nFor solving (12), we give the shrinkage operator [45] below.\nDefinition 3. For any matrix M \u2208 Rm\u00d7n, the singular vector thresholding (SVT) operator is defined as:\nSVT\u00b5(M) := Udiag(max{\u03c3 \u2212 \u00b5, 0})V T\nwhere max{\u00b7, \u00b7} should be understood element-wise, U \u2208Rm\u00d7r , V \u2208 Rn\u00d7r and \u03c3 = (\u03c31, \u03c32, . . . , \u03c3r) T \u2208 Rr\u00d71 are obtained by SVD of M , i.e., M = Udiag(\u03c3)V T .\nTherefore, a closed-form solution to (12) is given by:\nGk+1n = SVT1/(3\u03bb\u03c1k)(G k (n) + Y k n /\u03c1 k). (13)\nIt it clear that only some smaller size matrices Mn = Gk(n)+ Y k n /\u03c1\nk \u2208 Rdn\u00d7\u03a0j 6=ndj (dn \u226a In, n = 1, 2, 3) in (13) need to perform SVD. Therefore, our shrinkage operator has a significantly lower computational complexity O( \u2211 nd 2 n\u03a0j 6=ndj) while the computational complexity of\nthose algorithms for solving (3) and (4) is O( \u2211\nnI 2 n\u03a0j 6=nIj)\nfor each iteration. Hence, our algorithm has a much lower complexity than those as in [18], [22], [23], [24], [25].\n4.2 Updating {Uk+1, V k+1,W k+1,Gk+1}\nThe optimization problem (10) with respect to U , V , W and G is formulated as follows:\nmin G,U,V,W\n3\u2211\nn=1\n\u03c1k\n2 \u2016G(n) \u2212G\nk+1 n + Y k n /\u03c1 k\u20162F\n+ 1\n2 \u2016X k \u2212 G\u00d71U\u00d72V \u00d73W\u2016 2 F ,\ns.t., UTU = Id1 , V TV = Id2 , W TW = Id3 .\n(14)\nUnlike the HOOI algorithm in [33], we propose a new orthogonal iteration scheme to update thematricesU , V and W for the optimization of (14). Moreover, the conventional HOOI can be seen as a special case of (14) when \u03c1k=0. For any estimate of these matrices, the optimal solution with respect to G is given by the following theorem.\nTheorem 2. For given matrices U , V and W , the optimal core tensor G of the optimization problem (14) is given by\nG = 1\n1 + 3\u03c1k\n( A+ \u03c1kB ) (15)\nwhere A=X k\u00d71U T\u00d72V T\u00d73W T, B= \u22113 n=1refold(G k+1 n \u2212Y k n/\u03c1\nk) and refold(\u00b7) denotes the refolding of the matrix into a tensor.\nPlease see APPENDIX B for the proof of Theorem 2. Moreover, we propose an orthogonal iteration scheme for solving U , V and W , which is an alternating orthogonal procrustes scheme to solve the rank-(d1, d2, d3) problem. Analogous with Theorem 4.2 in [33], we first state that the minimization problem (14) can be formulated as follows:\nTheorem 3. Assume a real third-order tensor X k, then the minimization problem (14) is equivalent to the maximization (over these matrices U , V and W having orthonormal columns) of the following function\ng(U, V, W ) = \u2016A+ \u03c1kB\u20162F . (16)\nThe detailed proof of Theorem 3 is given in APPENDIX C. According to the theorem, an orthogonal iteration scheme is proposed to successively solve U , V and W by fixing the other variables. Imagine that the matrices V and W are fixed and that the optimization problem (16) is merely a quadratic function of the unknown matrix U . Consisting of orthonormal columns, we have\nmax U, UTU=Id1\n\u2016Mk1\u00d71U T+\u03c1kB\u20162F =\u2016(M k 1) T (1)U+\u03c1 kBT(1)\u2016 2 F (17)\nwhereMk1=X k\u00d72(V k)T\u00d73(W k)T . This is actually the wellknown orthogonal procrustes problem [46]. Hence, we have\nUk+1 = ORT ( (Mk1)(1)B T (1) ) (18)\nwhere ORT(A) := U\u0302 V\u0302 T , and U\u0302 and V\u0302 are the left singular vector and right singular vector matrices obtained by the tight SVD of the matrix A. Repeating the above procedure for V and W , we have\nV k+1 = ORT ( (Mk2)(2)B T (2) ) ,\nW k+1 = ORT ( (Mk3)(3)B T (3) ) (19)\nwhere Mk2 = X k\u00d71 (U k+1)T \u00d73 (W k)T and Mk3 = X k\u00d71 (Uk+1)T \u00d72(V\nk+1)T . For the updated matrices Uk+1, V k+1 andW k+1, then G\nis updated by\nGk+1 = \u03c1k\n\u22113 n=1 refold(G k+1 n \u2212 Y k n /\u03c1 k)\n1 + 3\u03c1k\n+ Mk3\u00d73(W k+1)T\n1 + 3\u03c1k .\n(20)\n4.3 Updating X k+1\nThe optimization problem (10) with respect to X is formulated as follows:\nmin X\n\u2016X \u2212 Gk+1\u00d71U k+1\u00d72V k+1\u00d73W k+1\u20162F ,\ns.t., X\u2126 = T\u2126. (21)\nBy introducing a Lagrangian multiplier Y \u2208 RI1\u00d7I2\u00d7I3 for X\u2126=T\u2126, the Lagrangian function of (21) is given by\nH(X ,Y) =\u2016X \u2212 Gk+1\u00d71U k+1\u00d72V k+1\u00d73W k+1\u20162F\n+ \u3008Y, P\u2126(X )\u2212 P\u2126(T )\u3009.\nLetting \u2207(X ,Y)H=0, we then obtain the following KarushKuhn-Tucker (KKT) conditions:\n2(X \u2212 Gk+1\u00d71U k+1\u00d72V k+1\u00d73W k+1) + P\u2126(Y) = 0,\nX\u2126 \u2212 T\u2126 = 0.\nBy deriving simply the KKT conditions, we have the optimal solution as follows:\nX k+1=P\u2126(T ) +P \u22a5 \u2126 (G k+1\u00d71U k+1\u00d72V k+1\u00d73W k+1) (22)\nwhere P\u22a5\u2126 is the complementary operator of P\u2126. Based on the above analysis, we develop an efficient ADMM algorithm for solving (10), as outlined in Algorithm 1. Moreover, Algorithm 1 can be extended to solve (7) and the SHOOI problem (8) (the details can be found in the Supplementary Materials). For instance, with the tensor\nof Lagrange multipliers Yk, the iterations of ADMM for solving (8) go as follows:\nmin G,U,V,W\n1 2 \u2016Zk \u2212 G\u00d71U\u00d72V \u00d73W + Y k/\u03c1k\u20162F ,\ns.t., UTU = Id1 , V TV = Id2 , W TW = Id3 ,\n(23)\nmin Z\n\u03c1k\n2 \u2016Z\u2212Gk+1\u00d71U k+1\u00d72V k+1\u00d73W k+1+Yk/\u03c1k\u20162F + 1\n2 \u2016W \u2217 (Z \u2212 T )\u20162F .\n(24)\nTo monitor convergence of Algorithm 1, the adaptively adjusting strategy of the penalty parameter \u03c1k in [43] is introduced. The necessary optimality conditions for (10) are primal feasibility\nX \u2217\u2126 = T\u2126, G \u2217 n = G \u2217 (n), n = 1, 2, 3,\n(U\u2217)TU\u2217 = Id1 , (V \u2217)TV \u2217 = Id2 , (W \u2217)TW \u2217 = Id3 (25)\nand dual feasibility\n0 \u2208 \u2202\u2016G\u2217n\u2016\u2217/(3\u03bb)\u2212 Y \u2217 n ,\nG\u2217\u2212X \u2217\u00d71(U \u2217)T\u00d72(V \u2217)T\u00d73(W \u2217)T+\n3\u2211\nn=1\nrefold(Y \u2217n )=0 (26)\nwhere ({G\u2217n},G \u2217, U\u2217, V \u2217,W \u2217,X \u2217) is a KKT point of (10). By the optimal conditions of (12) and (14) and Y k+1n = Y k n + \u03c1k(Gk+1(n) \u2212G k+1 n ), we have\n0 \u2208 \u2202\u2016Gk+1n \u2016\u2217/(3\u03bb)\u2212 Y k+1 n + \u03c1 k(Gk+1(n) \u2212 G k (n)),\nGk+1\u2212X k+1\u00d71(U k+1)T\u00d72(V k+1)T\u00d73(W k+1)T+\n3\u2211\nn=1\nrefold(Yk+1n )\n+(X k+1\u2212X k)\u00d71(U k+1)T\u00d72(V k+1)T\u00d73(W k+1)T=0.\nLet rk+1 :=max(\u2016Gk+1(n)\u2212G k+1 n \u2016F , n=1, 2, 3) be the primal\nresidual and sk+1 := max(\u03c1k\u2016Gk+1(n) \u2212G k (n)\u2016F , \u03c1 k\u2016(X k+1 \u2212 X k)\u00d71(U k+1)T\u00d72(V k+1)T\u00d73(W k+1)T \u2016F ) be the dual residual at iteration (k+1), we require the primal and dual residuals at the (k+1)-iteration to be small such that they satisfy the optimal conditions in (25) and (26). Following [43], an efficient strategy is to let \u03c1 = \u03c10 (the initialization in Algorithm 1) and update \u03c1k iteratively by:\n\u03c1k+1 =    \u03b3\u03c1k, rk > 10sk, \u03c1k/\u03b3, sk > 10rk,\n\u03c1k, otherwise,\n(27)\nwhere \u03b3 > 1."}, {"heading": "4.4 Extension for GROID", "text": "Algorithm 1 can be extended to solve our GROID problem (9), where the main difference is that the subproblem with respect to U , V , W and G is formulated as follows:\nmin G,U,V,W\n3\u2211\nn=1\n\u03c1k\n2 \u2016G(n) \u2212G\nk+1 n + Y k n /\u03c1 k\u20162F\n+ 1\n2 \u2016X k \u2212 G\u00d71U\u00d72V \u00d73W\u2016 2 F +\n\u00b5 2 h(U, V,W ),\ns.t., UTU = Id1 , V TV = Id2 , W TW = Id3\n(28)\nwhere h(U, V,W ) :=Tr(UTL1U)+Tr(V TL2V )+Tr(W TL3W ). Similar to Algorithm 1, U , V and W can be solved by minimizing the following cost function,\nAlgorithm 1 ADMM for ROID problem (10)\nInput: T\u2126, multi-linear rank (d1, d2, d3), \u03bb and tol. 1: while not converged do 2: Update Gk+1n by (13). 3: Update Uk+1, V k+1 and W k+1 by (18) and (19). 4: Update Gk+1 and X k+1 by (20) and (22). 5: Update the multipliers Y k+1n by\nY k+1n = Y k n + \u03c1 k(Gk+1(n) \u2212G k+1 n ), n = 1, 2, 3.\n6: Update \u03c1k+1 by (27). 7: Check the convergence condition,\nmax(\u2016Gk+1(n) \u2212G k+1 n \u2016F /\u2016T \u2016F , n = 1, 2, 3) < tol.\n8: end while Output: Gk+1, Uk+1, V k+1 and W k+1.\nF(U, V,W ) = \u2212 g(U, V,W )\n2(1+3\u03c1k) +\n\u00b5 2 h(U, V,W ).\nLet \u2207F(U, V k,W k) be the derivative of the function F(U, V k,W k), and \u2207F(U, V k,W k) be Lipschitz continuous with the constant \u03c4k , i.e., \u2016\u2207F(U, V k,W k) \u2212 \u2207F(U\u0302 , V k,W k)\u2016F \u2264 \u03c4 k\u2016U \u2212 U\u0302\u2016F , \u2200U, U\u0302 \u2208 R I1\u00d7d1 . To update Uk+1, an approximate procedure is given by the following linearization technique\nF(U, V k,W k) = \u2212 g(U,V k,W k)\n2(1+3\u03c1k) +\n\u00b5 2 h(U, V k,W k)\n\u2248F(Uk, V k,W k)+\u3008\u2207F(Uk, V k,W k), U\u2212Uk\u3009+ \u03c4k\n2 \u2016U\u2212Uk\u20162F\n=F(Uk, V k,W k)\u2212 \u3008U,F1(U\nk)+F2(U k)+Q+\u03c4kUk\u3009\n1 + 3\u03c1k +c\nwhere \u03c4k > 0 is the Lipschitz constant of \u2207F(U, V k,W k), c is a constant, Q = \u03c1k(Mk1)(1)B T (1), F1(U\nk) = 1 2 [(M k 1)(1)(M k 1) T (1) \u2212 \u00b5(1 + 3\u03c1 k)L1]U k, and F2(U\nk) = 1 2 [(M k 1)(1)(M k 1) T (1)\u2212\u00b5(1+3\u03c1 k)L1] TUk. Thus, the minimization of this problem is transformed into the following maximization\nmax U\n\u3008U, F1(U k) + F2(U k) +Q+ \u03c4kUk\u3009,\ns.t., UTU = Id1 . (29)\nFollowing [46], the solution of (29) is given by\nUk+1 = ORT(F1(U k) + F2(U k) +Q+ \u03c4kUk). (30)\nIn addition, V and W can be updated by the similar approximation procedure. Similar to Algorithm 1, we can propose an efficient ADMM algorithm (called Algorithm 2) to solve the graph regularized problem (9)."}, {"heading": "5 ALGORITHM ANALYSIS", "text": "In this section, we provide the convergence analysis and the complexity analysis for our algorithms."}, {"heading": "5.1 Convergence Analysis", "text": "With the low multi-linear rank tensor decomposition in (6), the problem (10) is non-convex and so we can only consider local convergence [43]. As in [47], [48], we show below a necessary condition for local convergence.\nLemma 1. Let {Z k} = {({Gkn},G k, Uk, V k,W k,X k, {Y kn })} be a sequence generated by Algorithm 1. If the sequences {Y kn }\n(n=1, 2, 3) are bounded and satisfy \u2211\u221e\nk=0 \u2016Y k+1 n \u2212Y k n \u2016<\u221e,\nthen Z k+1\u2212Z k\u21920, and {Z k} is bounded.\nProof. First, we prove that the Lagrangian function of (10) is bounded. By (22), we obtain\n\u2016X k+1 \u2212 Gk+1\u00d71U k+1\u00d72V k+1\u00d73W k+1\u20162F\n=\u2016P\u2126(T \u2212 G k+1\u00d71U k+1\u00d72V k+1\u00d73W k+1)\u20162F \u2264\u2016P\u2126(T \u2212 G k+1\u00d71U k+1\u00d72V k+1\u00d73W k+1)\u20162F\n+ \u2016P\u22a5\u2126 (X k \u2212 Gk+1\u00d71U k+1\u00d72V k+1\u00d73W k+1)\u20162F\n=\u2016X k \u2212 Gk+1\u00d71U k+1\u00d72V k+1\u00d73W k+1\u20162F .\nThus, we have\nL\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k+1, {Y kn })\n\u2264L\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k, {Y kn }).\nSimilarly, by the iteration procedure, we have\nL\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k+1, {Y kn })\n\u2264L\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k, {Y kn }) \u2264L\u03c1k({G k+1 n },G k, Uk, V k,W k,X k, {Y kn }) \u2264L\u03c1k({G k n},G k, Uk, V k,W k,X k, {Y kn }) =L\u03c1k\u22121({G k n},G k, Uk, V k,W k,X k, {Y k\u22121n })\n+ \u03b8k\n3\u2211\nn=1\n\u2016Y kn \u2212 Y k\u22121 n \u2016 2 F\nwhere \u03b8k = (\u03c1 k\u22121 + \u03c1k)/(2(\u03c1k\u22121)2). According to\u2211\u221e\nk=0\u2016Y k+1 n \u2212 Y k n \u2016 < \u221e, we have that L\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k+1, {Y kn }) is upper-bounded. By (11), we have\nL\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k+1, {Y kn })\n\u2265 1\n2\n3\u2211\nn=1\n\u2016Y k+1n \u2016 2 F \u2212 \u2016Y k n \u2016 2 F\n\u03c1k .\nThen L\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k+1, {Y kn }) is lower-bounded due to the boundedness of {Y kn }. That is, {L\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k+1, {Y kn })} is bounded. Furthermore, by Y k+1n =Y k n + \u03c1 k(Gk+1(n) \u2212G k+1 n ), n=1, 2, 3, then {L\u03c1k+1(Z k+1)} is also bounded.\nL\u03c1(\u00b7) is strong convex with respect to Gn (n = 1, 2, 3) and X , respectively. For any Gn and \u25b3Gn, we have\nL\u03c1(Gn +\u25b3Gn, \u00b7)\u2212 L\u03c1(Gn, \u00b7)\n\u2265\u3008\u2202GnL\u03c1(Gn, \u00b7),\u25b3Gn\u3009+ \u03c1\n2 \u2016\u25b3Gn\u2016\n2 F\n(31)\nwhere L\u03c1(Gn, \u00b7) denotes that all variables except Gn are fixed. Moreover, G\u2217n is a minimizer of L\u03c1(Gn, \u00b7) if\n\u3008\u2202GnL\u03c1(G \u2217 n, \u00b7), \u25b3Gn\u3009 \u2265 0. (32)\nNote that Gk+1n is a minimizer of L\u03c1(Gn, \u00b7) at the k-th iteration. Then, combining (31) and (32), we have\nL\u03c1k (G k n, \u00b7)\u2212 L\u03c1k(G k+1 n , \u00b7) \u2265\n\u03c1k\n2 \u2016Gkn \u2212G k+1 n \u2016 2 F . (33)\nSimilarly, we obtain\nL\u03c1k(X k, \u00b7)\u2212 L\u03c1k(X\nk+1, \u00b7) \u2265 1\n2 \u2016X k \u2212X k+1\u20162F . (34)\nUsing (14), we have\nL\u03c1k({G k+1 n },G k, Uk, V k,W k,X k, {Y kn })\u2212 L\u03c1k({G k+1 n },\nGk+1, Uk+1, V k+1,W k+1,X k, {Y kn }) \u2265 0. (35)\nBy (33)-(35) and Y k+1n = Y k n + \u03c1 k(Gk+1(n) \u2212G k+1 n ), we have\nL\u03c1k(Z k)\u2212 L\u03c1k+1(Z k+1)\n=L\u03c1k(Z k)\u2212 L\u03c1k ({G k+1 n },G k, Uk, V k,W k,X k, {Y kn })\n+ L\u03c1k({G k+1 n },G k, Uk, V k,W k,X k, {Y kn }) \u2212 L\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k, {Y kn }) + L\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k, {Y kn }) \u2212 L\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k+1, {Y kn }) + L\u03c1k({G k+1 n },G k+1, Uk+1, V k+1,W k+1,X k+1, {Y kn }) \u2212 L\u03c1k(Z k+1) + L\u03c1k(Z k+1)\u2212 L\u03c1k+1(Z k+1)\n\u2265 \u03c1k\n2\n3\u2211\nn=1\n\u2016Gk+1n \u2212G k n\u2016 2 F \u2212 \u03b8k+1\n3\u2211\nn=1\n\u2016Y k+1n \u2212Y k n \u2016 2 F\n+ 1\n2 \u2016X k+1\u2212X k\u20162F .\nBy taking summation of the above inequality from 1 to \u221e, and by the boundedness of {L\u03c1k(Z\nk)}, then \u221e\u2211\nk=1\n( \u03c1k\n2\n3\u2211\nn=1\n\u2016Gk+1n \u2212G k n\u2016 2 F +\n1 2 \u2016X k+1 \u2212X k\u20162F\n)\n\u2212 \u221e\u2211\nk=1\n( \u03b8k+1 3\u2211\nn=1\n\u2016Y k+1n \u2212 Y k n \u2016 2 F\n) < \u221e\nand by \u2211\u221e\nk=1\u2016Y k+1 n \u2212Y k n \u2016 2 F < \u221e, we have \u2211\u221e k=1\u2016X\nk+1\u2212 X k\u20162F <\u221e and \u2211\u221e k=1 \u22113 n=1\u2016G k+1 n \u2212G k n\u2016 2 F <\u221e, i.e., X\nk+1\u2212 X k\u21920 and Gk+1n \u2212G k n\u21920. By (14), we have G\nk+1\u2212Gk\u21920, Uk+1\u2212Uk \u2192 0, V k+1\u2212V k \u2192 0, and W k+1\u2212W k \u2192 0, i.e., Z k+1\u2212Z k\u21920.\nFurthermore, we have 3\u2211\nn=1\n\u2016Gkn\u2016\u2217/(3\u03bb) + 1\n2 \u2016X k \u2212 Gk\u00d71U k\u00d72V k\u00d73W k\u20162F\n=L\u03c1k\u22121({G k n},G k, Uk, V k,W k,X k, {Y k\u22121n })\n\u2212 1\n2\u03c1k\u22121\n3\u2211\nn=1\n( \u2016Y kn \u2016 2 F \u2212 \u2016Y k\u22121 n \u2016 2 F )\nis upper-bounded due to the boundedness of {Y kn } for all n \u2208 {1, 2, 3} and L\u03c1k\u22121({G k n},G\nk, Uk, V k,W k,X k, {Y k\u22121n }). According to Y kn = Y k\u22121 n +\u03c1 k\u22121(Gk(n)\u2212G k n) and (27), thus {Gkn}, {G k} and {X k} are all bounded, i.e., {Z k} is bounded.\nTheorem 4. Let {({Gkn},G k, Uk, V k,W k,X k, {Y kn })} be a sequence generated by Algorithm 1. Then any accumulation point of {({Gkn},G\nk, Uk, V k,W k,X k)} satisfies the KKT conditions for (10).\nThe proof of Theorem 4 is given in APPENDIX D. Moreover, the convergence of Algorithm 2 can also be guaranteed. We first give the following lemma [49].\nLemma 2. Let F : Rm\u00d7n \u2192 R be a continuously differentiable function with Lipschitz continuous gradient and Lipschitz constant L(F ). Then, for any \u03c4 \u2265 L(F ),\nF (X) \u2264 F (Y ) + \u3008\u2207F (X), X \u2212 Y \u3009+ \u03c4\n2 \u2016X \u2212 Y \u20162F ,\n\u2200X, Y \u2208Rm\u00d7n.\nTheorem 5. Let {Z k}= {({Gkn},G k, Uk, V k,W k,X k, {Y kn })} be a sequence generated by Algorithm 2. If the sequences {Y kn }\n(n=1, 2, 3) are all bounded, and satisfy \u2211\u221e\nk=0\u2016Y k+1 n \u2212Y k n \u2016<\u221e,\nthen any accumulation point of {({Gkn},G k, Uk, V k,W k,X k)} satisfies the KKT conditions for (9).\nProof. By Lemma 2, we have\nL\u03c1k({G k+1 n },G k, Uk, V k,W k,X k, {Y kn })\u2212 L\u03c1k({G k+1 n },\nGk+1, Uk+1, V k+1,W k+1,X k, {Y kn }) \u2265 0.\nAccording to Lemma 1, we have Z k+1\u2212Z k\u21920, and {Z k} is bounded. Moreover, by using the same proof procedure as for Theorem 4, we can obtain the conclusion."}, {"heading": "5.2 Complexity Analysis", "text": "We discuss the time complexity of our ROID and GROID methods. For solving both (6) and (9), the main running time of our algorithms is taken for performing SVDs and some multiplications. The time complexity of performing SVDs in (13), (18) and (19) is O( \u2211 nd 2 n\u03a0j 6=ndj+ \u2211 nd 2 nIn). The time complexity of some multiplication operators in (18), (19) and (22) is O((2d1 +d2+d3)\u03a0jIj + \u2211 nIn\u03a0jdj). Thus, the total time complexity of both ROID and GROID is O((2d1 + d2 + d3)\u03a0jIj) (dn \u226a In). Our algorithms are essentially the Gauss-Seidel-type schemes of ADMM, and the update strategy of the Jacobi version as in [29] can be easily implemented, and well suited for parallel computing."}, {"heading": "6 EXPERIMENTAL RESULTS", "text": "In this section, we evaluate the effectiveness and efficiency of our ROID method for low multi-linear rank tensor completion on synthetic data and multi-relational learning on real-world data such as a network data set and three popular multi-relational data sets. Except for large-scale multi-relational prediction, all the other experiments were performed on an Intel(R) Core (TM) i5-4570 (3.20 GHz) PC running Windows 7 with 8GB main memory."}, {"heading": "6.1 Results on Synthetic Data", "text": "Following [22], we generated low multi-linear rank thirdorder tensors T \u2208RI1\u00d7I2\u00d7I3 , which we used as the ground truth data. The generated tensor data follows the Tucker model, i.e., T = C\u00d71U1\u00d72U2\u00d73U3, where C \u2208 R\nr\u00d7r\u00d7r is the core tensor whose entries are generated as independent and identically distributed (i.i.d.) numbers from a uniform distribution in [0, 1], and the entries of Un \u2208 R\nIn\u00d7r are random samples drawn from a uniform distribution in the range [-0.5, 0.5]. With this construction, the multi-linear rank of third-order tensors T equals (r, r, r) almost surely."}, {"heading": "6.1.1 Algorithm Settings", "text": "We compare our ROID method with the following state-ofthe-art tensor estimation algorithms:\n1) WTucker1 [17]: In the implementation of WTucker, we set R1 = R2 = R3 = \u230a1.25r\u230b for solving the weighted Tucker (WTucker) decomposition problem (2). 2) WCP2 [8]: We set the tensor rank R = 40 to solve the weighted CP (WCP) decomposition problem (1),\n1. http://www.lair.irb.hr/ikopriva/marko-filipovi.html 2. http://www.sandia.gov/\u223ctgkolda/TensorToolbox/\nand the maximal number of iterations, maxiter = 100, for WCP and WTucker, both of which are solved by nonlinear conjugate gradient methods. 3) HaLRTC3 [22]: The value of the weights \u03b1n is set to be 1/3, n = 1, 2, 3 for solving (3) by using the highly accurate LRTC (HaLRTC) algorithm. Other parameters of HaLRTC are set to their default values. 4) Latent4 [25]: The regularization parameter \u03bb is set to 106\nfor solving the latent trace norm minimization (Latent) problem (4). Moreover, we set the tolerance value tol = 10\u22125 andmaxiter = 500 for HaLRTC, Latent and ROID.\nWe also apply Algorithm 1 to solve the SHOOI problem (8). Note that HaLRTC, Latent, SHOOI and ROID all apply the ADMM algorithm to solve their problems. For our ROID method, we set the regularization parameter \u03bb = 102 and d1 = d2 = d3 = \u230a1.5r\u230b. The relative square error (RSE) of the recovered tensor X for all these algorithms is defined by RSE :=\u2016X \u2212 T \u2016F/\u2016T \u2016F ."}, {"heading": "6.1.2 Numerical Results on Sparse Tensors", "text": "To evaluate the robustness of our ROIDmethod with respect to multi-linear rank parameter changes, we first conduct some experiments on synthetic tensors of size 100\u00d7100\u00d7100 or 200\u00d7 200\u00d7 200, and illustrate the RSE results of all these tensor decomposition methods with 10% sampling ratio, where the rank parameter of ROID, SHOOI, WTucker and WCP is chosen from {10, 15, . . . , 40}. The average RSE results of 10 independent runs are shown in Fig. 2, from which we can see that when the number of the given rank increases, the RSE of all these tensor decomposition methods (except WCP) gradually increase, especially for SHOOI. More specifically, SHOOI gives extremely accurate solutions for exact multi-linear rank tensor completion problems. However, as the number of the given rank increases, the RSE of SHOOI increases dramatically. In contrast, ROID under all these settings consistently outperforms WTucker and WCP in terms of RSE, and performs more robust than SHOOI. This confirms that our ROIDmodel with core tensor trace norm regularization is reasonable, and can provide a good estimation of the observed tensor even though from only a few observations. Note that the RSE of both convex algorithms, HaLRTC and Latent, on tensors of size 100\u00d7100\u00d7100 are 0.5796 and 0.3375, respectively.\n3. http://pages.cs.wisc.edu/\u223cji-liu/ 4. http://ttic.uchicago.edu/\u223cryotat/softwares/tensor/\nWe also report the recovery results of WTucker, WCP, HaLRTC, Latent and our ROID method with different fractions of observed entries and tensor multi-linear ranks on synthetic tensors of size 200\u00d7 200\u00d7 200 in Fig. 3, where the sampling ratio varies from 5% to 20% with increment 2.5%, and the multi-linear ranks rn, n = 1, 2, 3 are chosen from 10 to 40 with increment 5. We can observe that in all these settings, our ROID method consistently outperforms the other methods in terms of RSE. HaLRTC is able to yield very accurate solutions using adequate large sampling ratio (e.g. 0.2); however, when the fraction of observed entries is low (e.g. 0.05), or the underlying tensor multilinear ranks are high (e.g. 40), the performance of HaLRTC (and also the other convex tensor trace norm minimization method, Latent) is poor. The main reason is that WTucker and our ROID method are all multiple structured methods similar to the matrix case [50], and need only O(dN+ dNI) observations to exactly recover an Nth-order low multilinear rank tensor X with high probability, while O(rIN\u22121) observations are required for recovering the true tensor by both convex tensor trace norm minimization methods, HaLRTC and Latent, as stated in [27], [28], [51].\nMoreover, we conduct some experiments to evaluate the robustness of our ROID method with respect to the regularization parameter \u03bb, and report the results of Latent and our ROID method on synthetic tensors of size 100\u00d7100\u00d7100 or 200\u00d7200\u00d7200 in Fig. 4(a), where \u03bb is tuned from the grid {101, 102, . . . , 106} and the sampling ratio is set to 10%. Note that the solid and dashed lines denote the results of ROID with d1 = d2 = d3 = r and d1 = d2 = d3 = \u230a1.5r\u230b, respectively. It is clear that as \u03bb increases, both Latent and our ROID method with exact multi-linear rank give much better performance for tensor completion problems. As suggested in [25], setting \u03bb \u2192 \u221e gives more accurate solutions for the noiseless problem. In practical applications, this parameter of ROID is set to \u03bb = 100 for the following noisy problems. Moreover, our ROID method under all settings significantly outperforms Latent in terms of RSE.\nFinally, we present the running time of our ROID method and the other methods with varying sizes of thirdorder tensors, as shown in Fig. 4(b), from which we can see that the running time of WTcuker, WCP, Latent and HaLRTC dramatically grows with the increase of tensor size whereas the running time of our SHOOI and ROID methods only increase slightly. In addition, WTcuker, WCP, Latent and HaLRTC could not yield experimental results on the two largest synthetic tensor completion problems with sizes of\n800 and 1000, because they ran out of memory. Our ROID method is more than 10 times faster than WTcuker and WCP, more than 25 times faster than HaLRTC, and more than 150 times faster than Latent when the size of input tensors is 600\u00d7600\u00d7600. This shows that our ROIDmethod has good scalability and can address large-scale problems. Notice that because Latent converges too slowly, we do not consider it in the following experiments. Moreover, Table 1 summarizes the time complexities of major computations in the two related weighted tensor decomposition algorithms and the two convex trace norm minimization algorithms, where the assumed sizes of the tensor and the core tensor are I \u00d7 I \u00d7 I and d \u00d7 d \u00d7 d, respectively. From Table 4, we can see that although WTucker and WCP have the computational complexity similar to our ROIDmethod, they are much slower in practice than ROID due to their PolakRibiere nonlinear conjugate gradient algorithms with a timeconsuming line search scheme [52]."}, {"heading": "6.1.3 Numerical Results on Full Tensors", "text": "To further evaluate the performances of our method for full tensor decomposition, we compare our ROID method with the low multi-linear rank approximation (LMLRA) method [10], [53] and HOOI [33], [53] on noisy tensors, i.e., T = C\u00d71U1\u00d72U2\u00d73U3+nf \u2217E , where nf denotes the noise factor and E denotes the standard Gaussian random noise. Fig. 5(a) illustrates the RSE results of LMLRA, HOOI and ROID on 200\u00d7200\u00d7200 noisy tensors with different noise factors. We can observe that ROID performs more robust and stable against noise than the other methods. Moreover, we also report the running time on tensors of different sizes in Fig. 5(b), from which we can see that our ROID method is more than 10 times faster than the other methods. In addition, LMLRA andHOOI could not generate experimental results on the largest problem with size of 1000\u00d71000\u00d71000, because they ran out of memory."}, {"heading": "6.2 Results on Network Data", "text": "In this part, we examine our ROID and graph regularized (called GROID) methods on real-world network data sets, such as the YouTube data set5 [54]. YouTube is currently the most popular video sharing web site, which allows users to interact with each other in various forms such as contacts, subscriptions, sharing favorite videos, etc. In total, this data set contains 848,003 users, with 15,088 users sharing all of the information types, and includes 5-dimension of interactions: contact network, co-contact network, co-subscription network, co-subscribed network, and favorite network. Additional information about the data can be found in [54]. We run these experiments on a machine with 6-core Intel Xeon 2.4GHz CPU and 64GB memory.\nWe address the multi-relational prediction problem as a tensor completion problem. For the graph regularized weighted CP (GWCP) decomposition [40], graph regularized weighted Tucker (GWTucker) decomposition [40], and our ROID and GROID methods, we set the tensor rank R = 45 and the multi-linear rank d1 = d2 = 40 and d3 = 5, and the regularization parameter \u03bb = 100. For HaLRTC [22] and our ROID and GROID methods, the weights \u03b1n are set to \u03b11=\u03b12=0.4998 and \u03b13=0.0004. The tolerance value of all these methods is fixed at tol = 10\u22124.\nAs the other methods could not yield the experimental results on the whole YouTube data set, we first chose the users who have more than 10 interactions as a subset, which consists of 4,117 users and five types of interactions, i.e., 4, 117\u00d74, 117\u00d75. We randomly select 10% or 20% entries as the training set, and the remainder as the testing data. We report the average prediction accuracy (the score Area Under the receiver operating characteristic Curve, AUC)\n5. http://leitang.net/heterogeneous network.html\nand the average running time (seconds) over 10 independent runs in Figs. 6 and 7(a), where the number of users is gradually increased. Moreover, we evaluate the robustness of our ROID method with respect to given multi-linear ranks, as shown in Fig. 7(b), where the given ranks of GWTucker, GWCP and our ROID and GROID methods are chosen in the range [30, 35, . . . , 70]. We can observe that the three trace norm minimization algorithms, i.e., our ROID and GROIDmethods andHaLRTC, significantly outperform GWTucker and GWCP in terms of prediction accuracy. Moreover, ROID and GROID run remarkably faster than the other methods, and are more robust than GWTucker and GWCP with respect to given multi-linear ranks. The running time of ROID and GROID increase slightly when the number of users increases. In contrast, the running time of the other methods increases dramatically, and they could not complete the computation within 48 hours on the two largest problem sizes of 8,000 or 15,088 users. This shows that ROID and GROID have very good scalability and can address large-scale problems. GROID performs significantly better than all the other methods in terms of prediction accuracy due to the use of auxiliary information."}, {"heading": "6.3 Results on Multi-Relational Data", "text": "Finally, we examine how well our ROID method performs on the three popular multi-relational datasets, which have previously been used by Kemp et al. [55] for link prediction, including the Kinship, Nations and UMLS data sets. The Kinship data set consists of kinship relationships (such as \u201cfather\u201d or \u201cwife\u201d relations) among the members of the ALyawarra tribe in Central Australia [56]. The data set contains 104 tribe members and 26 types of kinship (binary) relations, formfitting a three-order tensor of size 104\u00d7104\u00d726. The Nations data set consists of international relations among different countries in the world [57]. The data set contains 14 countries and 56 types of (binary) relations (such as \u201cTreaties\u201d or \u201cMilitary Alliance\u201d), and is a three-order tensor of size 14\u00d714\u00d756. The UMLS data set is collected from the Unified Medical Language System by McCray et al. [58]. This data set includes a semantic network with 135 concepts and 49 binary predicates (such as \u201caffects\u201d or \u201ccauses\u201d), and is a three-order tensor of size 135\u00d7135\u00d749.\nWe compare our ROID method with several state-of-theart approaches including WCP, the nonparametric Bayesian\nmehtod, IRM6 [55], the hidden variable discovery method, MRC [2] and RESCAL7 [1] and HaLRTC [22] on these three data sets. Since WCP, WTucker and SHOOI yield very similar results on all three data sets, we only report the results of WCP. Then, we use the area under the precision-recall curve (AUC) as the evaluation metric to test the relation prediction performance as in [1], [4]. In order to obtain comparable results to IRM, MRC and RESCAL, we follow their experimental requirements and preform 10-fold cross validation. For our ROID method, we set the multi-linear rank d1=d2=d3=35 for both the Kinship and UMLS data sets, and d1 = d2 =14 and d3 =10 for the Nations data set, and the regularization parameter \u03bb = 100.\nWe illustrate the experimental results of all these six methods on these three data sets, as shown in Fig. 8, from\n6. http://www.psy.cmu.edu/\u223cckemp/code/irm.html 7. http://www.mit.edu/\u223cmnick/\nwhich we can see that our ROID method and HaLRTC consistently outperform the other four methods. The reason is that HaLRTC and our ROID method can more efficiently explore the impact of different relations to improve the accuracy of relation prediction. AsWCP, RESCAL, HaLRTC, and our ROID method have similar AUC results, we also report the RSE results of these four methods in Table 2. It is clear that our ROID method consistently performs better than the other three methods in terms of recovery accuracy. Moreover, we demonstrate the relation-based clustering capabilities of our ROID method on the Nations data set. We apply the K-means clustering method with K = 3 to the matrix Xm, and illustrate the results on four types of relationships in Fig. 9, from which we can see that similar results as in [1], [55] are obtained."}, {"heading": "6.4 Running Time and Robustness Analysis", "text": "Moreover, we present the comparison of the running time of related methods on three multi-relational data sets. In [1], it has been shown that WCP and RESCAL are much faster than MRC as well as IRM. Therefore, we only report the running time of WCP, RESCAL, HaLRTC and our ROID method on these three data sets with different ranks, as listed in Table 3. It is clear that our ROID method is much faster thanWCP and RESCAL, and is more suitable for largescale multi-relational data. RESCAL and WCP usually scale worse with regard to the ranks than our ROID method. In other words, with the increase of the given tensor ranks, the running time of RESCAL and WCP dramatically grows whereas that of our ROID method only changes slightly.\nWe also evaluate the robustness of our ROID method against its parameters: the given tensor ranks and the regularization parameter \u03bb on these three real-world data sets, as shown in Fig. 10, from which we can see that our ROID method is robust against its parameter variations, especially on the Kinship and UMLS date sets. Note that three rank parameters d1, d2 and d3 are set to the lesser of the given multi-linear rank and the corresponding size of\nthe tensor. The regularization parameter \u03bb is tuned from the grid {100, 101, . . . , 106}."}, {"heading": "7 CONCLUSIONS AND FUTURE WORK", "text": "In this paper we proposed a scalable ROID method and its graph regularized version for full or incomplete tensor analytics, such as multi-relational learning. First, we induced the equivalence relation of the Schatten p-norm (0<p<\u221e) of a low multi-linear rank tensor and its core tensor. Then we presented a novel orthogonal tensor decomposition model with core tensor trace norm regularization. We also introduced a regularization version using graph Laplacians induced from the relationships and a sparse higher-order orthogonal iteration version. Finally, we developed two efficient ADMM algorithms to solve our problems. Moreover, we analyzed theoretically the local convergence of our algorithms. The convincing experimental results for real-world problems verified both the efficiency and effectiveness of our methods, especially from only a few observations.\nMoreover, our ROID method can be extended to various higher-order tensor recovery and completion problems, such as higher-order robust principal component analysis (RPCA) [31] and robust tensor completion. For future work, we are interested in exploring ways to regularize our model with other auxiliary information, such as semantic information contained in social network [59]. Due to the unitary invariant property of norms as stated in Theorem 1 and the superiority of the Schatten-p quasi-norm (0< p < 1) to the trace norm, it would be an interesting research direction in future to investigate the more general core tensor Schatten quasi-norm regularization."}, {"heading": "APPENDIX A PROOF OF THEOREM 1:", "text": "Before giving the proof of Theorem 1, we will first present some properties of matrices and tensors in the following.\nDefinition 4. Let A and B be two matrices of size m \u00d7 n and p\u00d7 q, respectively. The Kronecker product of both matrices A and B is an mp\u00d7 nq matrix given by:\nA\u2297B = [aijB]mp\u00d7nq.\nProperty 1. Let A \u2208 Rm\u00d7p, C \u2208 Rp\u00d7q , and B \u2208 Rn\u00d7q , then\n\u2016ACBT \u2016Sp = \u2016C\u2016Sp\nwhere both A and B are column-orthonormal, i.e., ATA=Ip and BTB=Iq .\nProof. Let us denote the SVD of C by C = U\u03a3V T , then ACBT = (AU)\u03a3(BV )T . Since (AU)T (AU) = Ip and (BV )T (BV ) = Iq , (AU)\u03a3(BV )\nT is actually an SVD of ACBT . According to the definition of the Schatten p-norm, we have \u2016C\u2016Sp =(Tr(\u03a3 p)) 1/p =\u2016ACBT \u2016Sp .\nProperty 2. Let A \u2208 Rm\u00d7n, B \u2208 Rp\u00d7q , and C and D are two matrices of compatible sizes, then we have the following results: 1) (A\u2297B)\u2297 C = A\u2297 (B \u2297 C). 2) (A\u2297B)(C \u2297D) = (AC)\u2297 (BD). 3) (A\u2297B)T = AT \u2297BT .\nProperty 3. LetX = G\u00d71U\u00d72V \u00d73W , whereX \u2208 R I1\u00d7I2\u00d7I3 and G \u2208 Rd1\u00d7d2\u00d7d3 , then\nX(1) = UG(1)(W \u2297 V ) T , X(2) = V G(2)(W \u2297 U) T ,\nX(3) = WG(3)(V \u2297 U) T .\nProof. Let P1 = W\u2297V, P2 = W\u2297U, P3 = V \u2297U . According to Property 2, we have\nPT1 P1 =(W \u2297 V ) T (W \u2297 V ) = (WT \u2297 V T )(W \u2297 V ),\n=(WTW )\u2297 (V TV ) = I3 \u2297 I2 = I\u03031\nwhere In \u2208 R dn\u00d7dn , n = 1, 2, 3, are all identity matrices, I\u03031\u2208R J1\u00d7J1 is also an identity matrix, and J1 = \u03a0j 6=1dj .\nSimilarly, we also have PT2 P2= I\u03032 and P T 3 P3= I\u03033, where\nboth I\u03032 and I\u03033 are identity matrices. By Property 3, we have\n\u2016X(1)\u2016Sp = \u2016UG(1)(W \u2297 V ) T \u2016Sp .\nAccording to Property 1 and PT1 P1 = I\u03031 , we have\n\u2016X(1)\u2016Sp = \u2016UG(1)(W \u2297 V ) T \u2016Sp = \u2016G(1)\u2016Sp .\nSimilarly, we have \u2016X(2)\u2016Sp = \u2016G(2)\u2016Sp and \u2016X(3)\u2016Sp = \u2016G(3)\u2016Sp . Hence, we have \u2016X\u2016Sp =\u2016G\u2016Sp ."}, {"heading": "APPENDIX B PROOF OF THEOREM 2:", "text": "Proof. The optimization problem (14) with respect to G is written by\nmin G\nh(G) = 3\u2211\nn=1\n\u03c1k\n2\n\u2225\u2225\u2225G(n) \u2212Gk+1n + Y kn /\u03c1k \u2225\u2225\u2225 2\nF\n+ 1\n2\n\u2225\u2225\u2225X k \u2212 G\u00d71U\u00d72V \u00d73W \u2225\u2225\u2225 2\nF .\n(36)\nThe above problem (36) is a smooth convex optimization problem, thus we can obtain the derivative of the function h in the following form:\n\u2202h \u2202G =\n( G \u2212 X k\u00d71(U) T \u00d72(V ) T \u00d73(W ) T )\n+ 3\u2211\nn=1\n\u03c1k ( G \u2212 refold(Gk+1n \u2212 Y k n /\u03c1 k) )\n= (3\u03c1k + 1)G \u2212 \u03c1k 3\u2211\nn=1\nrefold(Gk+1n \u2212 Y k n /\u03c1 k)\n\u2212X k\u00d71(U) T \u00d72(V ) T \u00d73(W ) T .\nLet \u2202h\u2202G=0, the optimal solution of (36) is given by\nG = 1\n1 + 3\u03c1k X k\u00d71(U) T \u00d72(V ) T \u00d73(W ) T\n+ \u03c1k\n1 + 3\u03c1k\n3\u2211\nn=1\nrefold(Gk+1n \u2212 Y k n /\u03c1 k)."}, {"heading": "APPENDIX C PROOF OF THEOREM 3:", "text": "Proof. Let\nf(G, U, V,W ) = \u03c1k\n2\n3\u2211\nn=1\n\u2225\u2225\u2225G(n) \u2212Gk+1n + Y kn /\u03c1k \u2225\u2225\u2225 2\nF\n+ 1\n2\n\u2225\u2225\u2225X k \u2212 G\u00d71U\u00d72V \u00d73W \u2225\u2225\u2225 2\nF ,\n(37)\nA = X k\u00d71(U) T \u00d72(V ) T \u00d73(W ) T ,\nand B = 3\u2211\nn=1\nrefold(Gk+1n \u2212 Y k n /\u03c1 k).\nThen the closed-form solution of (37) with respect to G can be obtained by (15), and it can be rewritten as\nG = 1\n1 + 3\u03c1k A+\n\u03c1k\n1 + 3\u03c1k B. (38)\nUsing (38) and according to the definitions of the tensors A and B, we have\n\u3008X k,G\u00d71U\u00d72V\u00d73W \u3009= \u2329 A,\n1\n1+3\u03c1k (A+\u03c1kB)\n\u232a\n= 1\n1 + 3\u03c1k \u2016A\u20162F +\n\u03c1k\n1 + 3\u03c1k \u3008A, B\u3009,\n(39)\n\u3008G, B\u3009 = \u2329 B,\n1\n1 + 3\u03c1k (A+ \u03c1kB)\n\u232a\n= \u03c1k\n1 + 3\u03c1k \u2016B\u20162F +\n1\n1 + 3\u03c1k \u3008A, B\u3009.\n(40)\nHence,\nf(G, U, V,W ) = \u03c1k\n2\n3\u2211\nn=1\n\u2016Gk+1n \u2212Y k n /\u03c1 k\u20162F + 1\n2 \u2016X k\u20162F\n\u2212 \u3008X k, G\u00d71U\u00d72V\u00d73W \u3009+ 3\u03c1k+1\n2 \u2016G\u20162F\u2212\u03c1 k\u3008G, B\u3009. (41)\nSubstituting (38), (39) and (40) into (41), then the cost function (41) is rewritten in the following form,\nf(G, U, V,W )= \u03c1k\n2\n3\u2211\nn=1\n\u2016Gk+1n \u2212Y k n /\u03c1 k\u20162F + 1\n2 \u2016X k\u20162F\n\u2212 \u3008A+ \u03c1kB, G\u3009+ 1 + 3\u03c1k\n2 \u2016G\u20162F\n=c1 \u2212 1\n2(1 + 3\u03c1k) \u2016A+ \u03c1kB\u20162F\nwhere c1 = \u2016X k\u20162F/2+(\u03c1\nk/2) \u22113\nn=1\u2016G k+1 n \u2212Y k n /\u03c1 k\u20162F is a constant, and g(U, V,W ) := \u2016A+ \u03c1kB\u20162F ."}, {"heading": "APPENDIX D PROOF OF THEOREM 4:", "text": "Proof. By (12), (14) and (21), and Y k+1n = Y k n + \u03c1 k(Gk+1(n) \u2212 Gk+1n ), we have\n0 \u2208 \u2202\u2016Gk+1n \u2016\u2217/(3\u03bb)\u2212 Y k+1 n + \u03c1 k(Gk+1(n) \u2212 G k (n)),\n3\u2211\nn=1\nrefold(Y k+1n )+G k+1\u2212X k+1\u00d71(U k+1)T\u00d72(V k+1)T\u00d73(W k+1)T\n+ (X k+1\u2212X k)\u00d71(U k+1)T\u00d72(V k+1)T\u00d73(W k+1)T=0,\nX k+1\u2126 =T\u2126, X k+1 \u2126C =(G k+1\u00d71U k+1\u00d72V k+1\u00d73W k+1)\u2126C , (Uk+1)TUk+1=Id1 , (V k+1)TV k+1=Id2 , (W\nk+1)TW k+1=Id3 (42)\nwhere \u2126C is the complement of \u2126. By Lemma 1, we have that the sequence {Z k} is bounded, and Z k+1 \u2212 Z k \u2192 0, such asX k+1\u2212X k \u2192 0, Gk+1\u2212Gk \u2192 0,Gk+1n \u2212G k n \u2192 0 and Y k+1n \u2212 Y k n \u2192 0, n = 1, 2, 3. By the Bolzano-Weierstrass theorem, the bounded sequence {Z k} must have a convergent subsequence {Z kj}, and the limit point is denoted by Z \u221e = limj=\u221e Z\nkj . Moreover, {Z k} lies in a compact set, thus Z \u221e is an accumulation point of {Z k}, where Z \u221e = ({G\u221en },G\n\u221e, U\u221e, V\u221e,W\u221e,X\u221e, {Y \u221en }). Moreover, according to Y k+1n \u2212 Y k n \u2192 0 and Y k+1 n =\nY kn +\u03c1 k(Gk+1(n) \u2212G k+1 n ), we have that G k+1 (n) \u2212G k+1 n \u2192 0, n = 1, 2, 3. By (42), we have\n0 \u2208 \u2202\u2016G\u221en \u2016\u2217/(3\u03bb)\u2212 Y \u221e n , G \u221e (n) = G \u221e n , n = 1, 2, 3,\n3\u2211\nn=1\nrefold(Y\u221en )+G \u221e\u2212X\u221e\u00d71(U \u221e)T\u00d72(V \u221e)T\u00d73(W \u221e)T=0,\nX\u221e\u2126 = T\u2126, X \u221e \u2126C = (G \u221e\u00d71U \u221e\u00d72V \u221e\u00d73W \u221e)\u2126C , (U\u221e)TU\u221e=Id1 , (V \u221e)TV \u221e=Id2 , (W\n\u221e)TW\u221e=Id3 . (43)\nIt is easy to see that (43) is the KKT conditions for (10), that is, the accumulation point ({G\u221en },G\n\u221e, U\u221e, V \u221e,W\u221e,X\u221e) satisfies the KKT conditions of (10). This completes the proof of Theorem 4."}], "references": [{"title": "A three-way model for collective learning on multi-relational data,", "author": ["M. Nickel", "V. Tresp", "H. Kriegel"], "venue": "in Proc. 28th Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Statistical predicate invention,", "author": ["S. Kok", "P. Domingos"], "venue": "Proc. 24th Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Relational learning via collective matrix factorization,", "author": ["A. Singh", "G. Gordon"], "venue": "Proc. 14th ACM Int. Conf. Know. Disc. Data Min. (KDD),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "A latent factor model for highly multi-relational data,", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G. Obozinski"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning)", "author": ["L. Getoor", "B. Taskar"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Tensor decompositions and applications,", "author": ["T. Kolda", "B. Bader"], "venue": "SIAM Review,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "MultiVis: Content-based social network exploration through multiway visual analysis,", "author": ["J. Sun", "S. Papadimitriou", "C. Lin", "N. Cao", "S. Liu", "andW. Qian"], "venue": "in Proc. SIAM Int. Conf. Data Min. (SDM),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor factorization and completion,", "author": ["Y. Xu", "W. Yin"], "venue": "SIAM J. Imaging Sci.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Some mathematical notes on three-mode factor", "author": ["L. Tucker"], "venue": "analysis,\u201d Psychometrika,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1966}, {"title": "Foundations of the PARAFAC procedure: Models and conditions for an explanatory multi-modal factor analysis,", "author": ["R. Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1970}, {"title": "PARAFAC: parallel factor analysis,", "author": ["R. Harshman", "M. Lundy"], "venue": "Comput. Stat. Data An.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Generalized coupled tensor factorization,", "author": ["Y. Yilmaz", "A. Cemgil", "U. Simsekli"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Unsupervised multiway data analysis: A literature survey,", "author": ["E. Acar", "B. Yener"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Factor matrix trace norm minimization for low-rank tensor completion,", "author": ["Y. Liu", "F. Shang", "H. Cheng", "J. Cheng", "H. Tong"], "venue": "in Proc. SIAM Int. Conf. Data Min. (SDM),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Temporal analysis of semantic graphs using ASALSAN,", "author": ["B. Bader", "R. Harshman", "T. Kolda"], "venue": "in Proc. 7th IEEE Int. Conf. Data Min. (ICDM),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Tucker factorization with missing data with application to low-n-rank tensor completion,\u201dMultidim", "author": ["M. Filipovic", "A. Jukic"], "venue": "Syst. Sign. Process.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Tensor completion and lown-rank tensor recovery via convex optimization,", "author": ["S. Gandy", "B. Recht", "I. Yamada"], "venue": "Inverse Probl.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Generalized higher-order orthogonal iteration for tensor decomposition and completion,", "author": ["Y. Liu", "F. Shang", "W. Fan", "J. Cheng", "H. Cheng"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Tensor completion for estimating missing values in visual data,", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "in Proc. of IEEE Int. Conf. Comput. Vis. (ICCV),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "A rank minimization heuristic with application to minimum order system approximation,", "author": ["M. Fazel", "H. Hindi", "S.P. Boyd"], "venue": "in Proc. IEEE Amer. Control Conf.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Tensor completion for estimating missing values in visual data,", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Learning with tensors: A framework based on covex optimization and spectral regularization,", "author": ["M. Signoretto", "Q. Dinh", "L. Lathauwer", "J. Suykens"], "venue": "Mach. Learn., vol. 94,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Convex tensor decomposition via structured Schatten norm regularization,", "author": ["R. Tomioka", "T. Suzuki"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}, {"title": "Provable models for robust low-rank tensor completion,", "author": ["B. Huang", "C. Mu", "D. Goldfarb", "J. Wright"], "venue": "Pac. J. Optim.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Square deal: Lower bounds and improved relaxations for tensor recovery,", "author": ["C. Mu", "B. Huang", "J. Wright", "D. Goldfarb"], "venue": "in Proc. 31st Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Generalized higher order orthogonal iteration for tensor learning and decomposition,", "author": ["Y. Liu", "F. Shang", "W. Fan", "J. Cheng", "H. Cheng"], "venue": "IEEE Trans. Neural Netw. Learn. Syst.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Generalized higher-order tensor decomposition via parallel ADMM,", "author": ["F. Shang", "Y. Liu", "J. Cheng"], "venue": "in Proc. 28th AAAI Conf. Artif. Intell. (AAAI),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Most tensor problems are NP hard,", "author": ["C. Hillar", "L. Lim"], "venue": "J. ACM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Robust low-rank tesnor recovery: Models and algorithms,", "author": ["D. Goldfarb", "Z. Qin"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "A multilinear singular value decomposition,", "author": ["L. Lathauwer", "B. Moor", "J. Vandewalle"], "venue": "SIAM J. Matrix Anal. Appl.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2000}, {"title": "Simultaneous tensor decomposition and completion using factor priors,", "author": ["Y. Chen", "C. Hsu", "H. Liao"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "A new convex relaxation for tensor completion,", "author": ["B. Romera-Paredes", "M. Pontil"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Rank estimation in missing data matrix problems,", "author": ["C. Juli\u00e0", "A.D. Sappa", "F. Lumbreras", "J. Serrat", "A. L\u00f3pez"], "venue": "J. Math. Imaging Vis.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "The power of convex relaxation: Nearoptimal matrix completion,", "author": ["E. Cand\u00e8s", "T. Tao"], "venue": "IEEE Trans. Inf. Theory, vol. 56,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Tensor facotiration using auxiliary information,", "author": ["A. Narita", "K. Hayashi", "R. Tomioka", "H. Kashima"], "venue": "Data Min. Knowl. Disc.,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2012}, {"title": "Collaborative filtering: weighted nonnegative matrix factorization incorporating user and item graphs,", "author": ["Q. Gu", "J. Zhou", "C. Ding"], "venue": "in Proc. SIAM Int. Conf. Data Min. (SDM),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2010}, {"title": "Graph dual regularization nonnegative matrix factorization for co-clustering,", "author": ["F. Shang", "L. Jiao", "F. Wang"], "venue": "Pattern Recogn.,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers,", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "Stochastic alternating direction method of multipliers,", "author": ["H. Ouyang", "N. He", "L. Tran", "A. Gray"], "venue": "in Proc. 30th Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2013}, {"title": "A singular value thresholding algorithm for matrix completion,", "author": ["J. Cai", "E. Cand\u00e8s", "Z. Shen"], "venue": "SIAM J. Optim., vol. 20,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2010}, {"title": "Matrix procrustes problems,", "author": ["H. Nick"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1995}, {"title": "An alternating direction algorithm for matrix completion with nonnegative factors,", "author": ["Y. Xu", "W. Yin", "Z. Wen", "Y. Zhang"], "venue": "Front. Math. Chin.,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}, {"title": "Efficient kernel learning from side information using ADMM,", "author": ["E. Hu", "J. Kwok"], "venue": "in Proc. 23rd Int. Joint Conf. Artif. Intell. (IJCAI),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2013}, {"title": "Nonlinear Programming", "author": ["D. Bertsekas"], "venue": "Athena Scientific, Belmont: The 2nd edition,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1999}, {"title": "Structured low-rank matrix factorization: Optimality, algorithm, and applications to image processing,", "author": ["B. Haeffele", "E. Young", "R. Vidal"], "venue": "in Proc. 31st Int. Conf. Mach. Learn. (ICML),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Statistical performance of convex tensor decomposition,", "author": ["R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima"], "venue": "in Proc. Adv. Neural Inf. Process. Syst. (NIPS),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2011}, {"title": "A scaled conjugate gradient algorithm for fast supervised learning,", "author": ["M.F. M\u00f8ller"], "venue": "Neural Netw.,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1993}, {"title": "Uncovering groups via heterogeneous interaction analysis,", "author": ["L. Tang", "X. Wang", "H. Liu"], "venue": "in Proc. 9th IEEE Int. Conf. Data Min. (ICDM),", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2009}, {"title": "Learning systems of concepts with an infinite relational model,", "author": ["C. Kemp", "J. Tenenbaum", "T. Griffiths", "T. Yamada", "N. Ueda"], "venue": "in Proc. 21st AAAI Conf. Artif. Intell. (AAAI),", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2006}, {"title": "The Detection of Patterns in Alyawarra Nonverbal Behavior", "author": ["W. Denham"], "venue": "University of Washington: PhD thesis,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 1973}, {"title": "Dimensionality of nations project: Attributes of nations and behavior of nation dyads,", "author": ["R. Rummel"], "venue": "ICPSR Data File,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1999}, {"title": "An upper level ontology for the biomedical domain,", "author": ["A. McGray"], "venue": "Comp. Funct. Genom.,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "R ELATIONAL learning is becoming increasingly important because of the high value hidden in relational data and also of its many applications in various domains such as social networks, the semantic web, bioinformatics, and the linked data cloud [1].", "startOffset": 246, "endOffset": 249}, {"referenceID": 1, "context": "A class of relational learning methods focus mostly on the problem of modeling a single relation type, such as relational learning from latent attributes [2], [3], which models relations between objects as resulting from intrinsic latent attributes of these objects.", "startOffset": 154, "endOffset": 157}, {"referenceID": 2, "context": "A class of relational learning methods focus mostly on the problem of modeling a single relation type, such as relational learning from latent attributes [2], [3], which models relations between objects as resulting from intrinsic latent attributes of these objects.", "startOffset": 159, "endOffset": 162}, {"referenceID": 3, "context": "For example, in social networks [4], relationships between individuals may be personal, familial, or professional.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "This type of relational data learning is often referred to as multi-relational learning (MRL), which needs to model large-scale sparse relational databases efficiently [5].", "startOffset": 168, "endOffset": 171}, {"referenceID": 5, "context": "In recent years, tensors have become ubiquitous such as multi-channel images and videos, and become popular due to the ability to discover complex and interesting latent structures and correlations of data [6], [7], [8], [9].", "startOffset": 206, "endOffset": 209}, {"referenceID": 6, "context": "In recent years, tensors have become ubiquitous such as multi-channel images and videos, and become popular due to the ability to discover complex and interesting latent structures and correlations of data [6], [7], [8], [9].", "startOffset": 211, "endOffset": 214}, {"referenceID": 7, "context": "In recent years, tensors have become ubiquitous such as multi-channel images and videos, and become popular due to the ability to discover complex and interesting latent structures and correlations of data [6], [7], [8], [9].", "startOffset": 221, "endOffset": 224}, {"referenceID": 8, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 33, "endOffset": 37}, {"referenceID": 11, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 103, "endOffset": 107}, {"referenceID": 13, "context": "Tensor decomposition [10], [11], [12], [13] is a popular tool for multi-relational prediction problems [14], [15].", "startOffset": 109, "endOffset": 113}, {"referenceID": 14, "context": "[16] proposed a three-way component decomposition model for analyzing intrinsically asymmetric relationships.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] incorporated collective learning into the tensor factorization, which is designed to account for the inherent structure of relational data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Two of the most popular tensor factorizations are the Tucker decomposition [10] and the CANDECOMP/PARAFAC (CP) decomposition [11].", "startOffset": 75, "endOffset": 79}, {"referenceID": 9, "context": "Two of the most popular tensor factorizations are the Tucker decomposition [10] and the CANDECOMP/PARAFAC (CP) decomposition [11].", "startOffset": 125, "endOffset": 129}, {"referenceID": 15, "context": "To address incomplete tensor estimation, two weighted alternating leastsquares methods [8], [17] were proposed.", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "However, these methods require the ability to reliably estimate the rank of the involved tensor [18], [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 17, "context": "However, these methods require the ability to reliably estimate the rank of the involved tensor [18], [19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "[20] first extended the trace norm (also known as the nuclear norm [21] or the Schatten 1-norm [19]) regularization for partially observed lowmultilinear rank tensor recovery.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] first extended the trace norm (also known as the nuclear norm [21] or the Schatten 1-norm [19]) regularization for partially observed lowmultilinear rank tensor recovery.", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "[20] first extended the trace norm (also known as the nuclear norm [21] or the Schatten 1-norm [19]) regularization for partially observed lowmultilinear rank tensor recovery.", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "\u2019s subsequent paper [22], they proposed three efficient algorithms to solve the lowmulti-linear rank tensor completion problem.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "can also be found in [18], [23], [24], [25].", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "can also be found in [18], [23], [24], [25].", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "can also be found in [18], [23], [24], [25].", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "In addition, there are some theoretical developments that guarantee the reconstruction of a low rank tensor from partial measurements by solving trace norm minimization under some reasonable conditions [25], [26], [27], [28].", "startOffset": 202, "endOffset": 206}, {"referenceID": 23, "context": "In addition, there are some theoretical developments that guarantee the reconstruction of a low rank tensor from partial measurements by solving trace norm minimization under some reasonable conditions [25], [26], [27], [28].", "startOffset": 208, "endOffset": 212}, {"referenceID": 24, "context": "In addition, there are some theoretical developments that guarantee the reconstruction of a low rank tensor from partial measurements by solving trace norm minimization under some reasonable conditions [25], [26], [27], [28].", "startOffset": 214, "endOffset": 218}, {"referenceID": 25, "context": "In addition, there are some theoretical developments that guarantee the reconstruction of a low rank tensor from partial measurements by solving trace norm minimization under some reasonable conditions [25], [26], [27], [28].", "startOffset": 220, "endOffset": 224}, {"referenceID": 17, "context": "Therefore, existing algorithms suffer from high computational cost, making them impractical for realworld applications [19], [29].", "startOffset": 119, "endOffset": 123}, {"referenceID": 26, "context": "Therefore, existing algorithms suffer from high computational cost, making them impractical for realworld applications [19], [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 5, "context": "In fact, the problem is NP-hard [6], [30].", "startOffset": 32, "endOffset": 35}, {"referenceID": 27, "context": "In fact, the problem is NP-hard [6], [30].", "startOffset": 37, "endOffset": 41}, {"referenceID": 24, "context": "Fortunately, the multi-linear rank (also called the Tucker rank in [27], [31]) of a tensor X is easy to compute, and consists of the ranks of all mode-n unfoldings.", "startOffset": 67, "endOffset": 71}, {"referenceID": 28, "context": "Fortunately, the multi-linear rank (also called the Tucker rank in [27], [31]) of a tensor X is easy to compute, and consists of the ranks of all mode-n unfoldings.", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "In [17], the weighted Tucker decomposition (WTucker) model is formulated as follows:", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "If the factor matrices of the Tucker decomposition are constrained orthogonal, the classical decomposition methods are referred to as the higher-order singular value decomposition (HOSVD) [32] or higher-order orthogonal iteration (HOOI) [33], where the latter leads to the estimation of best rank-(R1, R2, R3) approximations while the truncation of HOSVD may achieve a good rank-(R1, R2, R3) approximation but in general not the best possible one [33].", "startOffset": 188, "endOffset": 192}, {"referenceID": 30, "context": "In addition, several extensions of both tensor decomposition models are developed for tensor estimation problems, such as [34], [35], [36].", "startOffset": 128, "endOffset": 132}, {"referenceID": 16, "context": "However, for all those methods, a suitable rank value needs to be given, and it has been shown that both WTucker andWCP models are usually sensitive to the given ranks due to their least-squares formulations [18], [19], and they have poor performance when the data have a high rank [22].", "startOffset": 208, "endOffset": 212}, {"referenceID": 17, "context": "However, for all those methods, a suitable rank value needs to be given, and it has been shown that both WTucker andWCP models are usually sensitive to the given ranks due to their least-squares formulations [18], [19], and they have poor performance when the data have a high rank [22].", "startOffset": 214, "endOffset": 218}, {"referenceID": 20, "context": "However, for all those methods, a suitable rank value needs to be given, and it has been shown that both WTucker andWCP models are usually sensitive to the given ranks due to their least-squares formulations [18], [19], and they have poor performance when the data have a high rank [22].", "startOffset": 282, "endOffset": 286}, {"referenceID": 20, "context": "[22] proposed three efficient algorithms (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In addition, there are some similar convex tensor completion algorithms in [18], [23], [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "In addition, there are some similar convex tensor completion algorithms in [18], [23], [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "Tomioka and Suzuki [25] proposed a latent trace norm minimization model,", "startOffset": 19, "endOffset": 23}, {"referenceID": 24, "context": "More recently, it has been shown that the tensor trace norm minimization models mentioned above can be substantially suboptimal [27], [37].", "startOffset": 128, "endOffset": 132}, {"referenceID": 31, "context": "More recently, it has been shown that the tensor trace norm minimization models mentioned above can be substantially suboptimal [27], [37].", "startOffset": 134, "endOffset": 138}, {"referenceID": 24, "context": "However, if the order of the involved tensor is no more than three, the models (3) and (4) often perform better than the more balanced (square) matrix model in [27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 32, "context": "For rn (n = 1, 2, 3), we recommend a matrix rank estimation approach recently developed in [38] to compute some good values (r 1, r \u2032 2, r \u2032 3) for the multi-linear rank of the involved tensor.", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": ", the Schatten 1-norm) is the tightest convex surrogate to the rank function [21], [39], we mainly consider the trace norm case in this paper.", "startOffset": 77, "endOffset": 81}, {"referenceID": 33, "context": ", the Schatten 1-norm) is the tightest convex surrogate to the rank function [21], [39], we mainly consider the trace norm case in this paper.", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "When all entries of T are observed, the model (6) degenerates to the following core tensor trace norm regularized tensor decomposition problem [29]:", "startOffset": 143, "endOffset": 147}, {"referenceID": 20, "context": "Besides, the core tensor trace norm term promotes low multi-linear rank tensor decompositions, and enhances the robustness of the multi-linear rank selection, while those traditional tensor decomposition methods are usually sensitive to the given multi-linear rank [22], [29].", "startOffset": 265, "endOffset": 269}, {"referenceID": 26, "context": "Besides, the core tensor trace norm term promotes low multi-linear rank tensor decompositions, and enhances the robustness of the multi-linear rank selection, while those traditional tensor decomposition methods are usually sensitive to the given multi-linear rank [22], [29].", "startOffset": 271, "endOffset": 275}, {"referenceID": 34, "context": "Inspired by the work in [40], [41], [42], we also exploit the auxiliary information given as link-affinity matrices in a graph regularized ROID (GROID) model:", "startOffset": 24, "endOffset": 28}, {"referenceID": 35, "context": "Inspired by the work in [40], [41], [42], we also exploit the auxiliary information given as link-affinity matrices in a graph regularized ROID (GROID) model:", "startOffset": 30, "endOffset": 34}, {"referenceID": 36, "context": "Inspired by the work in [40], [41], [42], we also exploit the auxiliary information given as link-affinity matrices in a graph regularized ROID (GROID) model:", "startOffset": 36, "endOffset": 40}, {"referenceID": 37, "context": "As a variant of the standard ALM, the alternating direction method of multipliers (ADMM) has received much attention recently due to the tremendous demand from large-scale machine learning applications [43], [44].", "startOffset": 202, "endOffset": 206}, {"referenceID": 38, "context": "As a variant of the standard ALM, the alternating direction method of multipliers (ADMM) has received much attention recently due to the tremendous demand from large-scale machine learning applications [43], [44].", "startOffset": 208, "endOffset": 212}, {"referenceID": 39, "context": "For solving (12), we give the shrinkage operator [45] below.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Hence, our algorithm has a much lower complexity than those as in [18], [22], [23], [24], [25].", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "Hence, our algorithm has a much lower complexity than those as in [18], [22], [23], [24], [25].", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "Hence, our algorithm has a much lower complexity than those as in [18], [22], [23], [24], [25].", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "Hence, our algorithm has a much lower complexity than those as in [18], [22], [23], [24], [25].", "startOffset": 90, "endOffset": 94}, {"referenceID": 40, "context": "This is actually the wellknown orthogonal procrustes problem [46].", "startOffset": 61, "endOffset": 65}, {"referenceID": 37, "context": "To monitor convergence of Algorithm 1, the adaptively adjusting strategy of the penalty parameter \u03c1 in [43] is introduced.", "startOffset": 103, "endOffset": 107}, {"referenceID": 37, "context": "Following [43], an efficient strategy is to let \u03c1 = \u03c1 (the initialization in Algorithm 1) and update \u03c1 iteratively by:", "startOffset": 10, "endOffset": 14}, {"referenceID": 40, "context": "Following [46], the solution of (29) is given by U = ORT(F1(U ) + F2(U ) +Q+ \u03c4U).", "startOffset": 10, "endOffset": 14}, {"referenceID": 37, "context": "1 Convergence Analysis With the low multi-linear rank tensor decomposition in (6), the problem (10) is non-convex and so we can only consider local convergence [43].", "startOffset": 160, "endOffset": 164}, {"referenceID": 41, "context": "As in [47], [48], we show below a necessary condition for local convergence.", "startOffset": 6, "endOffset": 10}, {"referenceID": 42, "context": "As in [47], [48], we show below a necessary condition for local convergence.", "startOffset": 12, "endOffset": 16}, {"referenceID": 43, "context": "We first give the following lemma [49].", "startOffset": 34, "endOffset": 38}, {"referenceID": 26, "context": "Our algorithms are essentially the Gauss-Seidel-type schemes of ADMM, and the update strategy of the Jacobi version as in [29] can be easily implemented, and well suited for parallel computing.", "startOffset": 122, "endOffset": 126}, {"referenceID": 20, "context": "1 Results on Synthetic Data Following [22], we generated low multi-linear rank thirdorder tensors T \u2208R123 , which we used as the ground truth data.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": ") numbers from a uniform distribution in [0, 1], and the entries of Un \u2208 R In\u00d7r are random samples drawn from a uniform distribution in the range [-0.", "startOffset": 41, "endOffset": 47}, {"referenceID": 15, "context": "1 Algorithm Settings We compare our ROID method with the following state-ofthe-art tensor estimation algorithms: 1) WTucker [17]: In the implementation of WTucker, we set R1 = R2 = R3 = \u230a1.", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "3) HaLRTC [22]: The value of the weights \u03b1n is set to be 1/3, n = 1, 2, 3 for solving (3) by using the highly accurate LRTC (HaLRTC) algorithm.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "4) Latent [25]: The regularization parameter \u03bb is set to 10 for solving the latent trace norm minimization (Latent) problem (4).", "startOffset": 10, "endOffset": 14}, {"referenceID": 44, "context": "The main reason is that WTucker and our ROID method are all multiple structured methods similar to the matrix case [50], and need only O(d+ dNI) observations to exactly recover an Nth-order low multilinear rank tensor X with high probability, while O(rI) observations are required for recovering the true tensor by both convex tensor trace norm minimization methods, HaLRTC and Latent, as stated in [27], [28], [51].", "startOffset": 115, "endOffset": 119}, {"referenceID": 24, "context": "The main reason is that WTucker and our ROID method are all multiple structured methods similar to the matrix case [50], and need only O(d+ dNI) observations to exactly recover an Nth-order low multilinear rank tensor X with high probability, while O(rI) observations are required for recovering the true tensor by both convex tensor trace norm minimization methods, HaLRTC and Latent, as stated in [27], [28], [51].", "startOffset": 399, "endOffset": 403}, {"referenceID": 25, "context": "The main reason is that WTucker and our ROID method are all multiple structured methods similar to the matrix case [50], and need only O(d+ dNI) observations to exactly recover an Nth-order low multilinear rank tensor X with high probability, while O(rI) observations are required for recovering the true tensor by both convex tensor trace norm minimization methods, HaLRTC and Latent, as stated in [27], [28], [51].", "startOffset": 405, "endOffset": 409}, {"referenceID": 45, "context": "The main reason is that WTucker and our ROID method are all multiple structured methods similar to the matrix case [50], and need only O(d+ dNI) observations to exactly recover an Nth-order low multilinear rank tensor X with high probability, while O(rI) observations are required for recovering the true tensor by both convex tensor trace norm minimization methods, HaLRTC and Latent, as stated in [27], [28], [51].", "startOffset": 411, "endOffset": 415}, {"referenceID": 22, "context": "As suggested in [25], setting \u03bb \u2192 \u221e gives more accurate solutions for the noiseless problem.", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "Algorithms Complexity WCP [8] O(8dI3) WTucker [17] O(8dI3) HaLRTC [22], Latent [25] O(3I4) ROID and SHOOI O(4dI3)", "startOffset": 46, "endOffset": 50}, {"referenceID": 20, "context": "Algorithms Complexity WCP [8] O(8dI3) WTucker [17] O(8dI3) HaLRTC [22], Latent [25] O(3I4) ROID and SHOOI O(4dI3)", "startOffset": 66, "endOffset": 70}, {"referenceID": 22, "context": "Algorithms Complexity WCP [8] O(8dI3) WTucker [17] O(8dI3) HaLRTC [22], Latent [25] O(3I4) ROID and SHOOI O(4dI3)", "startOffset": 79, "endOffset": 83}, {"referenceID": 46, "context": "From Table 4, we can see that although WTucker and WCP have the computational complexity similar to our ROIDmethod, they are much slower in practice than ROID due to their PolakRibiere nonlinear conjugate gradient algorithms with a timeconsuming line search scheme [52].", "startOffset": 265, "endOffset": 269}, {"referenceID": 8, "context": "To further evaluate the performances of our method for full tensor decomposition, we compare our ROID method with the low multi-linear rank approximation (LMLRA) method [10], [53] and HOOI [33], [53] on noisy tensors, i.", "startOffset": 169, "endOffset": 173}, {"referenceID": 47, "context": "2 Results on Network Data In this part, we examine our ROID and graph regularized (called GROID) methods on real-world network data sets, such as the YouTube data set [54].", "startOffset": 167, "endOffset": 171}, {"referenceID": 47, "context": "Additional information about the data can be found in [54].", "startOffset": 54, "endOffset": 58}, {"referenceID": 34, "context": "For the graph regularized weighted CP (GWCP) decomposition [40], graph regularized weighted Tucker (GWTucker) decomposition [40], and our ROID and GROID methods, we set the tensor rank R = 45 and the multi-linear rank d1 = d2 = 40 and d3 = 5, and the regularization parameter \u03bb = 100.", "startOffset": 59, "endOffset": 63}, {"referenceID": 34, "context": "For the graph regularized weighted CP (GWCP) decomposition [40], graph regularized weighted Tucker (GWTucker) decomposition [40], and our ROID and GROID methods, we set the tensor rank R = 45 and the multi-linear rank d1 = d2 = 40 and d3 = 5, and the regularization parameter \u03bb = 100.", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "For HaLRTC [22] and our ROID and GROID methods, the weights \u03b1n are set to \u03b11=\u03b12=0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 48, "context": "[55] for link prediction, including the Kinship, Nations and UMLS data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "The Kinship data set consists of kinship relationships (such as \u201cfather\u201d or \u201cwife\u201d relations) among the members of the ALyawarra tribe in Central Australia [56].", "startOffset": 156, "endOffset": 160}, {"referenceID": 50, "context": "The Nations data set consists of international relations among different countries in the world [57].", "startOffset": 96, "endOffset": 100}, {"referenceID": 51, "context": "[58].", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "mehtod, IRM [55], the hidden variable discovery method, MRC [2] and RESCAL [1] and HaLRTC [22] on these three data sets.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "mehtod, IRM [55], the hidden variable discovery method, MRC [2] and RESCAL [1] and HaLRTC [22] on these three data sets.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "mehtod, IRM [55], the hidden variable discovery method, MRC [2] and RESCAL [1] and HaLRTC [22] on these three data sets.", "startOffset": 75, "endOffset": 78}, {"referenceID": 20, "context": "mehtod, IRM [55], the hidden variable discovery method, MRC [2] and RESCAL [1] and HaLRTC [22] on these three data sets.", "startOffset": 90, "endOffset": 94}, {"referenceID": 0, "context": "Then, we use the area under the precision-recall curve (AUC) as the evaluation metric to test the relation prediction performance as in [1], [4].", "startOffset": 136, "endOffset": 139}, {"referenceID": 3, "context": "Then, we use the area under the precision-recall curve (AUC) as the evaluation metric to test the relation prediction performance as in [1], [4].", "startOffset": 141, "endOffset": 144}, {"referenceID": 0, "context": "9, from which we can see that similar results as in [1], [55] are obtained.", "startOffset": 52, "endOffset": 55}, {"referenceID": 48, "context": "9, from which we can see that similar results as in [1], [55] are obtained.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "In [1], it has been shown that WCP and RESCAL are much faster than MRC as well as IRM.", "startOffset": 3, "endOffset": 6}, {"referenceID": 28, "context": "Moreover, our ROID method can be extended to various higher-order tensor recovery and completion problems, such as higher-order robust principal component analysis (RPCA) [31] and robust tensor completion.", "startOffset": 171, "endOffset": 175}], "year": 2016, "abstractText": "Multi-relational learning has received lots of attention from researchers in various research communities. Most existing methods either suffer from superlinear per-iteration cost, or are sensitive to the given ranks. To address both issues, we propose a scalable core tensor trace norm Regularized Orthogonal Iteration Decomposition (ROID) method for full or incomplete tensor analytics, which can be generalized as a graph Laplacian regularized version by using auxiliary information or a sparse higher-order orthogonal iteration (SHOOI) version. We first induce the equivalence relation of the Schatten p-norm (0<p<\u221e) of a low multi-linear rank tensor and its core tensor. Then we achieve a much smaller matrix trace norm minimization problem. Finally, we develop two efficient augmented Lagrange multiplier algorithms to solve our problems with convergence guarantees. Extensive experiments using both real and synthetic datasets, even though with only a few observations, verified both the efficiency and effectiveness of our methods.", "creator": "LaTeX with hyperref package"}}}