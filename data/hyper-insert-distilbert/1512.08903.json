{"id": "1512.08903", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Dec-2015", "title": "Online Keyword Spotting with a Character-Level Recurrent Neural Network", "abstract": "in compiling this paper, consider we propose a fast context - aware keyword image spotting decomposition model employing mostly a character - depth level distributed recurrent neural path network ( rnn ) for spoken term preference detection in doing continuous timed speech. the correct rnn is then end - process to - end trained with coherent connectionist temporal classification ( ctc ) mechanisms to equally generate the maximum probabilities of localized character constraints and word - boundary labels. there currently is technically no need for the dynamic phonetic transcription, senone texture modeling, or system modeling dictionary layout in training and clinical testing. also, interactive keywords search can actually easily be immediately added continuously and modified by editing the text template based keyword list without immediately retraining the native rnn. moreover, the current unidirectional rnn diagram processes an exceptionally infinitely long amplitude input frequency audio streams consistently without pre - delayed segmentation problems and keywords are detected normally with low - speed latency conditions before the utterance is formally finished. better experimental reading results show that the proposed keyword spotter scheme significantly outperforms the deep neural network ( h dnn ) hierarchy and hidden verb markov model ( gr hmm ) speech based coherent keyword - filler model results even with performing less computations.", "histories": [["v1", "Wed, 30 Dec 2015 10:32:12 GMT  (405kb)", "http://arxiv.org/abs/1512.08903v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["kyuyeon hwang", "minjae lee", "wonyong sung"], "accepted": false, "id": "1512.08903"}, "pdf": {"name": "1512.08903.pdf", "metadata": {"source": "CRF", "title": "Online Keyword Spotting with a Character-Level Recurrent Neural Network", "authors": ["Kyuyeon Hwang"], "emails": ["kyuyeon.hwang@gmail.com;", "mjlee@dsp.snu.ac.kr;", "wysung@snu.ac.kr)."], "sections": [{"heading": null, "text": "ar X\niv :1\n51 2.\n08 90\n3v 1\n[ cs\n.C L\n] 3\n0 D\nIn this paper, we propose a context-aware keyword spotting model employing a character-level recurrent neural network (RNN) for spoken term detection in continuous speech. The RNN is end-toend trained with connectionist temporal classification (CTC) to generate the probabilities of character and word-boundary labels. There is no need for the phonetic transcription, senone modeling, or system dictionary in training and testing. Also, keywords can easily be added and modified by editing the text based keyword list without retraining the RNN. Moreover, the unidirectional RNN processes an infinitely long input audio streams without pre-segmentation and keywords are detected with low-latency before the utterance is finished. Experimental results show that the proposed keyword spotter significantly outperforms the deep neural network (DNN) and hidden Markov model (HMM) based keyword-filler model even with less computations.\nIndex Terms\nKeyword spotting (KWS), spoken term detection (STD), online, continuous speech, recurrent neural\nnetwork (RNN), connectionist temporal classification (CTC).\nI. INTRODUCTION\nKeyword spotting, or spoken term detection, is a technique for detecting pre-defined keywords in continuous speech. This technique is used for numerous applications including voice activation of smartphones, voice command for TVs and automobiles, and audio search. Compared to large\nK. Hwang, M. Lee, and W. Sung are with the Department of Electrical and Computer Engineering, Seoul National University,\nSeoul, 08826 Korea (e-mail: kyuyeon.hwang@gmail.com; mjlee@dsp.snu.ac.kr; wysung@snu.ac.kr).\n2 vocabulary continuous speech recognition (LVCSR), keyword spotting is usually designed with a simple structure, which is suitable for real-time low-power systems.\nThe traditional hidden Markov model (HMM) based keyword spotter [1] does not include a language model to limit the complexity of the decoding stage. Therefore, the algorithm only makes use of the pronunciation without knowing the context of the utterances. This is especially problematic when the phonetic description of a certain keyword is included in other longer spoken words, as there is no way to distinguish whether the detected phonetic sequence is actually belonging to the matching keyword or a part of other words. For example, the keyword \u201choney\u201d can be detected when \u201choneymoon\u201d is spoken.\nTo remedy this issue, a recurrent neural network (RNN) based context-aware keyword spotter is proposed in this paper. The RNN is end-to-end trained with connectionist temporal classification (CTC) [2] to directly transcribe the input speech to a sequence of character labels and a wordboundary label. The word-boundary label plays a key role in filtering out the case where a certain keyword string is included in other words. The soft output of the RNN is fed into a simple decoding network for computing posterior probabilities of keywords.\nThe proposed keyword spotter has several advantages over the previous approaches as follows:\n\u2022 The front-end RNN is unidirectional and trained by online CTC [3] to process infinitely\nlong audio streams with low latency. Therefore, there is no need for pre-segmenting the utterance with an additional voice activity detector [4]. On the other hand, in the previous approaches with bidirectional RNNs [5], [6], keywords are detected after listening each (pre-segmented) utterance to the end. \u2022 The RNN is end-to-end trained. Unlike the previous phoneme based approaches [1], [6],\nthere is no need for phoneme or senone modeling in both learning and testing. Also, the RNN learns vocabulary and weak language models, which allows it to detect word boundaries and greatly improve the accuracy in continuous speech. \u2022 New keywords can be added without retraining the RNN by modifying the back-end that\nis simpler than that of the HMM keyword-filler model [1]. This is not possible with the previous end-to-end approaches [5], [7].\nExperimental results show that the proposed CTC keyword spotter greatly outperforms the deep neural network (DNN) [8] and HMM [1] hybrid keyword-filler model on the Wall Street Journal (WSJ) [9] Nov\u201992 20K evaluation set especially when the length of the keyword is short.\n3"}, {"heading": "II. END-TO-END KEYWORD SPOTTING MODEL", "text": "The proposed keyword spotter consists of a front-end RNN and a back-end decoder. At each frame, the RNN converts an input audio feature to probabilities of characters. Then, the decoder computes probabilities of the keywords based on the recent character-level probabilities."}, {"heading": "A. Character-Level Unidirectional RNN", "text": "The front-end RNN is a deep unidirectional long short-term memory (LSTM) [10] network with forget gates [11] and peephole connections [12]. Specifically, the RNN consists of three LSTM layers and a softmax output layer [13], and sequence-to-sequence trained with connectionist temporal classification (CTC) [2]. The network is similar to the one employed for the endto-end speech recognition [14]. However, in our case, the network is unidirectional and trained with online CTC [3] on very long speech streams, instead of bidirectional LSTM networks with sequence-wise standard CTC training. This enables the RNN to process infinitely long input speech.\nCTC [2] is one of the most successful sequence-to-sequence learning algorithms for RNNs. Recently, CTC based end-to-end trained RNNs have been reported to show impressive performance in automatic speech recognition [14], [15], which are comparable to state-of-the-art DNN-HMM models. In those end-to-end models, RNNs learn how to directly transcribe the input speech to the corresponding character-level transcription without intermediate senone or phoneme modeling. Moreover, the RNNs can attain weak language models from the training speech, which diminishes the need for external lexicon models or word dictionaries. Although the language modeling capability is not sufficient for the application to LVCSR, this improves the accuracy of keyword spotting very much.\nFor the proposed keyword spotter, we employ the online CTC algorithm [3] to train continuously running unidirectional RNNs, which can process an infinitely long input speech without pre-segmentation. For this, the training is performed on the infinitely long training streams that are generated by randomly concatenating training sequences. The online CTC algorithm allows the RNN to learn sequences that are longer than the unroll amount. Therefore, a fixed amount of unroll can be used for training sequences with various lengths. This enables efficient RNN training with multiple parallel training streams on a GPU with synchronized forward and backward steps [16] using the online backpropagation through time algorithm [17]. The\nnetworks are unrolled 2,048 times and weight updates are performed every 1,024 forward steps with stochastic gradient descent (SGD).\nAs the input to the RNN, 40-dimensional log-filterbank features plus energy and their delta and double-delta (total 123-dimensional vector) are used. The feature vectors are extracted using a 25 ms Hamming window with 10 ms period and element-wisely normalized to zero mean and unit standard deviation based on the statistics obtained from the training set. The output is the probabilities for 30 labels including 26 alphabets, 2 special characters (\u2019 and .), the wordboundary label ( ), and the CTC blank label (-). The CTC blank label is generated when there is no specific output to emit.\nAll speaker independent training utterances in WSJ corpus [9] except verbalized punctuation versions are used for training. Also, odd transcriptions are filtered out, which makes the final training set contain 167 hours of speech. The WSJ Nov\u201993 20K development set is used as the validation set for annealing and early stopping."}, {"heading": "B. Decoder Models", "text": "The output of the CTC-trained network is a vector that contains the probabilities of labels (i.e., characters, the word-boundary label, and the CTC blank label). The goal of decoding is to convert the character-level output sequence to the sequence of keyword-level probabilities.\nTo spot keywords with the soft output of the RNN, two back-end decoders are considered in this paper. As shown in Fig. 1a, the first one is similar to the HMM keyword spotter with a\nfiller model [1]. Instead of HMM states, character-level nodes are used for filler and keyword networks. The keyword network starts and ends with the word-boundary labels, \u201c \u201d. At every frame, the posterior probability of the filler model and the keyword model are compared. When the difference of the two log-posterior probabilities is below a threshold, the keyword is detected. For discriminative keyword spotting, multiple keyword networks can be employed. The definition of the keyword and filler probabilities will be described later in this Section.\nThe second decoding network is depicted in Fig. 1b, which is similar to the first one, but the filler network is removed. This is equivalent to setting the posterior probability of the filler model to one and inserting a word-boundary node before the keyword network. A keyword is detected when the negative log-posterior probability is below a threshold. The experiments in Section III show that there is virtually no performance difference between the two decoding networks. That is, the filler network is not needed for the CTC keyword spotter.\nActual state transition between two label nodes exactly follows the original formulation of the standard CTC. As depicted in Fig. 2, each node in Fig. 1a and Fig. 1b consists of a character label or the word-boundary label followed by the CTC blank label. Note that transitions between the same CTC labels are not allowed. At every frame, each state updates its value by multiplying the net incoming probability by the corresponding RNN output, where the net probability is the sum of all incoming probabilities. However, when the filler model is employed, the incoming probabilities may have different label histories. In this case, the exact net probability cannot be obtained since the probabilities from different paths cannot be summed. Therefore, we approximate the sum operation to the max operation to find the best alignment (with timing) instead of the best path (without timing). Experimental results show that this approximation does\n6 not degrade the decoding accuracy when applied to the keyword-only model.\nThe probability of the filler model is defined as the maximum posterior probability among all CTC states inside the filler model. On the other hand, the probability of the keyword network is the summed (maximum, if the sum-to-max approximation is applied) posterior probability of the two CTC states inside the last word-boundary node (red and blue circles in Fig. 1a and Fig. 1b). Precision-recall trade-off is adjusted by the threshold value between these two probabilities. Setting a large threshold increases the recall at the cost of the reduced precision, and vice versa."}, {"heading": "III. EXPERIMENTS", "text": ""}, {"heading": "A. Baseline DNN-HMM Keyword-Filler Model", "text": "As a baseline, the performance of the DNN-HMM keyword spotter is also obtained, where the Gaussian mixture model (GMM) of the keyword-filler model in [1] is replaced by a DNN acoustic model [8]. The DNN has 4 layers with 1,024 logistic sigmoid units each and a softmax layer with 2,717 units, which corresponds to 2,717 tied triphone states. The input of the DNN-HMM keyword spotter is 17 consecutive 39-dimensional Mel-frequency cepstral coefficient (MFCC) feature vectors (13 cepstral coefficients plus their delta and double-delta values) extracted at every 10 ms with 25 ms Hamming window. It is observed in our DNN-HMM experiments that the MFCC feature performs better than the log-filterbank features. The baseline model is trained on the same 167 hours of training set that is used for the proposed model."}, {"heading": "B. Experimental Setup", "text": "The experiments are performed on the 42-minute audio stream that is generated by concate-\nnating all 330 utterances in the WSJ Nov\u201992 20K evaluation set.\nFor the DNN-HMM keyword spotter with multiple keywords, the threshold of each keyword is given to be proportional to the duration of the keyword pronounced. For the proposed CTC keyword spotter, on the other hand, the threshold is proportional to the number of characters in the keyword.\nThe models are evaluated on the following keyword sets:\n\u2022 Set A: percent, hundred, thousand, million, people, president, average, foreign, international,\nnuclear\n\u2022 Set B: that, this, they, but, and, or, with, from, will, not\nSet A consists of multisyllabic words. On the other hand, Set B is a set of monosyllabic words, which are extremely difficult to detect without understanding the underlying linguistic structures."}, {"heading": "C. Evaluation", "text": "1) Detection Latency: The proposed keyword spotter employs unidirectional RNNs for lowlatency online spoken term detection. When unidirectional networks are trained with CTC, the network learns the output delay that is required for making use of sufficient amount of information from the input. Fig. 3 shows the input waveform and the posterior probabilities of the keywords from the keyword-only decoder. In this example, the keywords are detected within 200 ms after they are completely spoken. This is comparable to the human reaction time to speech stimuli, which is also roughly 200 ms as reported in [18].\n2) Comparison of the Decoding Models: The precision-recall plots for Set A and Set B are shown in Fig. 4, with various decoding models where the network size is fixed to 3x768 (i.e., 3 LSTM layers with 768 cells each). The experiments are performed with the baseline DNN-HMM model, the CTC keyword-filler model, and the CTC keyword-only model. Also, we observe the effect of approximating the sum operations to the max operations for the CTC keyword-only model. As shown in the figures, the proposed CTC based keyword spotters outperform the DNN-HMM based one especially on the Set B. This indicates that the subsidiary information such as word boundaries, the linguistic structure, and context is very helpful for detecting the\nmonosyllabic keywords. Note that there is no advantage of employing the filler model for the CTC keyword spotter. Moreover, the sum-to-max approximation does not degrade the detection accuracy.\n3) Comparison of Various Network Sizes: We now compare the proposed keyword spotters with the networks sizes of 3x128 and 3x768 as in Fig. 5. The decoder model is the keyword-\n9 only model without employing the sum-to-max approximation. The maximum F1 scores for the baseline DNN-HMM, small CTC (3x128), and large CTC (3x768) models are 0.936, 0.964, and 0.980 for Set A, and 0.517, 0.806, and 0.847, respectively, for Set B. Note that the number of parameters in the front-end networks for the baseline, small CTC, and large CTC models are 6.61 M, 0.40 M, and 12.21 M, respectively. Considering that the number of parameters is approximately proportional to the number of arithmetic operations per frame in the front-end networks, the proposed keyword spotter with the network size of 3x128 outperforms the baseline DNN-HMM model with only 6% of computations. Moreover, the back-end decoder network is much simpler in the proposed keyword-only model since the number of state transitions is significantly smaller than those of the triphone-based HMM keyword-filler model."}, {"heading": "IV. CONCLUDING REMARKS", "text": "Throughout the paper, an end-to-end RNN-based context-aware dictionary-free keyword spotter is proposed. Due to the weak language model embedded in the front-end RNN, the proposed keyword spotter shows remarkable performance improvement over the baseline DNN-HMM keyword spotter even with significantly less computations. Furthermore, the back-end of the proposed one is much simpler than that of the DNN-HMM model. Therefore, the proposed CTC keyword spotter is suitable for small-footprint low-latency applications with continuous speech input."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported in part by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF) grants funded by the Korea government (MSIP) (No. 2015R1A2A1A10056051).\n10"}], "references": [{"title": "A hidden Markov model based keyword recognition system", "author": ["R.C. Rose", "D.B. Paul"], "venue": "Acoustics, Speech, and Signal Processing, 1990. ICASSP-90., 1990 International Conference on. IEEE, 1990, pp. 129\u2013132.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Online sequence training of recurrent neural networks with connectionist temporal classification", "author": ["K. Hwang", "W. Sung"], "venue": "arXiv preprint arXiv:1511.06841, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "A statistical model-based voice activity detection", "author": ["J. Sohn", "N.S. Kim", "W. Sung"], "venue": "Signal Processing Letters, IEEE, vol. 6, no. 1, pp. 1\u20133, 1999.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "An application of recurrent neural networks to discriminative keyword spotting", "author": ["S. Fern\u00e1ndez", "A. Graves", "J. Schmidhuber"], "venue": "Artificial Neural Networks\u2013ICANN 2007. Springer, 2007, pp. 220\u2013229.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Bidirectional LSTM networks for context-sensitive keyword detection in a cognitive virtual agent framework", "author": ["M. W\u00f6llmer", "F. Eyben", "A. Graves", "B. Schuller", "G. Rigoll"], "venue": "Cognitive Computation, vol. 2, no. 3, pp. 180\u2013190, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Small-footprint keyword spotting using deep neural networks", "author": ["G. Chen", "C. Parada", "G. Heigold"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 4087\u20134091.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "The design for the Wall Street Journal-based CSR corpus", "author": ["D.B. Paul", "J.M. Baker"], "venue": "Proceedings of the workshop on Speech and Natural Language. Association for Computational Linguistics, 1992, pp. 357\u2013362.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural computation, vol. 12, no. 10, pp. 2451\u20132471, 2000.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["F.A. Gers", "N.N. Schraudolph", "J. Schmidhuber"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 115\u2013143, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["J.S. Bridle"], "venue": "Neurocomputing. Springer, 1990, pp. 227\u2013236.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764\u20131772.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in English and Mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos", "E. Elsen", "J. Engel", "L. Fan", "C. Fougner", "T. Han", "A. Hannun", "B. Jun", "P. LeGresley", "L. Lin", "S. Narang", "A. Ng", "S. Ozair", "R. Prenger", "J. Raiman", "S. Satheesh", "D. Seetapun", "S. Sengupta", "Y. Wang", "Z. Wang", "C. Wang", "B. Xiao", "D. Yogatama", "J. Zhan", "Z. Zhu"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "author": ["K. Hwang", "W. Sung"], "venue": "arXiv preprint arXiv:1503.02852, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["R.J. Williams", "J. Peng"], "venue": "Neural Computation, vol. 2, no. 4, pp. 490\u2013501, 1990.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1990}, {"title": "Simple reaction-times to speech and non-speech stimuli", "author": ["D. Fry"], "venue": "Cortex, vol. 11, no. 4, pp. 355\u2013360, 1975.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1975}], "referenceMentions": [{"referenceID": 0, "context": "The traditional hidden Markov model (HMM) based keyword spotter [1] does not include a language model to limit the complexity of the decoding stage.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "The RNN is end-to-end trained with connectionist temporal classification (CTC) [2] to directly transcribe the input speech to a sequence of character labels and a wordboundary label.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "The proposed keyword spotter has several advantages over the previous approaches as follows: \u2022 The front-end RNN is unidirectional and trained by online CTC [3] to process infinitely long audio streams with low latency.", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Therefore, there is no need for pre-segmenting the utterance with an additional voice activity detector [4].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "On the other hand, in the previous approaches with bidirectional RNNs [5], [6], keywords are detected after listening each (pre-segmented) utterance to the end.", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "On the other hand, in the previous approaches with bidirectional RNNs [5], [6], keywords are detected after listening each (pre-segmented) utterance to the end.", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "Unlike the previous phoneme based approaches [1], [6], there is no need for phoneme or senone modeling in both learning and testing.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "Unlike the previous phoneme based approaches [1], [6], there is no need for phoneme or senone modeling in both learning and testing.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "\u2022 New keywords can be added without retraining the RNN by modifying the back-end that is simpler than that of the HMM keyword-filler model [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "This is not possible with the previous end-to-end approaches [5], [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "This is not possible with the previous end-to-end approaches [5], [7].", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "Experimental results show that the proposed CTC keyword spotter greatly outperforms the deep neural network (DNN) [8] and HMM [1] hybrid keyword-filler model on the Wall Street Journal (WSJ) [9] Nov\u201992 20K evaluation set especially when the length of the keyword is short.", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "Experimental results show that the proposed CTC keyword spotter greatly outperforms the deep neural network (DNN) [8] and HMM [1] hybrid keyword-filler model on the Wall Street Journal (WSJ) [9] Nov\u201992 20K evaluation set especially when the length of the keyword is short.", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "Experimental results show that the proposed CTC keyword spotter greatly outperforms the deep neural network (DNN) [8] and HMM [1] hybrid keyword-filler model on the Wall Street Journal (WSJ) [9] Nov\u201992 20K evaluation set especially when the length of the keyword is short.", "startOffset": 191, "endOffset": 194}, {"referenceID": 9, "context": "The front-end RNN is a deep unidirectional long short-term memory (LSTM) [10] network with forget gates [11] and peephole connections [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "The front-end RNN is a deep unidirectional long short-term memory (LSTM) [10] network with forget gates [11] and peephole connections [12].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "The front-end RNN is a deep unidirectional long short-term memory (LSTM) [10] network with forget gates [11] and peephole connections [12].", "startOffset": 134, "endOffset": 138}, {"referenceID": 12, "context": "Specifically, the RNN consists of three LSTM layers and a softmax output layer [13], and sequence-to-sequence trained with connectionist temporal classification (CTC) [2].", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "Specifically, the RNN consists of three LSTM layers and a softmax output layer [13], and sequence-to-sequence trained with connectionist temporal classification (CTC) [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 13, "context": "The network is similar to the one employed for the endto-end speech recognition [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "However, in our case, the network is unidirectional and trained with online CTC [3] on very long speech streams, instead of bidirectional LSTM networks with sequence-wise standard CTC training.", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "CTC [2] is one of the most successful sequence-to-sequence learning algorithms for RNNs.", "startOffset": 4, "endOffset": 7}, {"referenceID": 13, "context": "Recently, CTC based end-to-end trained RNNs have been reported to show impressive performance in automatic speech recognition [14], [15], which are comparable to state-of-the-art DNN-HMM models.", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "Recently, CTC based end-to-end trained RNNs have been reported to show impressive performance in automatic speech recognition [14], [15], which are comparable to state-of-the-art DNN-HMM models.", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "For the proposed keyword spotter, we employ the online CTC algorithm [3] to train continuously running unidirectional RNNs, which can process an infinitely long input speech without pre-segmentation.", "startOffset": 69, "endOffset": 72}, {"referenceID": 15, "context": "This enables efficient RNN training with multiple parallel training streams on a GPU with synchronized forward and backward steps [16] using the online backpropagation through time algorithm [17].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "This enables efficient RNN training with multiple parallel training streams on a GPU with synchronized forward and backward steps [16] using the online backpropagation through time algorithm [17].", "startOffset": 191, "endOffset": 195}, {"referenceID": 8, "context": "All speaker independent training utterances in WSJ corpus [9] except verbalized punctuation versions are used for training.", "startOffset": 58, "endOffset": 61}, {"referenceID": 0, "context": "filler model [1].", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "As a baseline, the performance of the DNN-HMM keyword spotter is also obtained, where the Gaussian mixture model (GMM) of the keyword-filler model in [1] is replaced by a DNN acoustic model [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "As a baseline, the performance of the DNN-HMM keyword spotter is also obtained, where the Gaussian mixture model (GMM) of the keyword-filler model in [1] is replaced by a DNN acoustic model [8].", "startOffset": 190, "endOffset": 193}, {"referenceID": 17, "context": "This is comparable to the human reaction time to speech stimuli, which is also roughly 200 ms as reported in [18].", "startOffset": 109, "endOffset": 113}], "year": 2015, "abstractText": "In this paper, we propose a context-aware keyword spotting model employing a character-level recurrent neural network (RNN) for spoken term detection in continuous speech. The RNN is end-toend trained with connectionist temporal classification (CTC) to generate the probabilities of character and word-boundary labels. There is no need for the phonetic transcription, senone modeling, or system dictionary in training and testing. Also, keywords can easily be added and modified by editing the text based keyword list without retraining the RNN. Moreover, the unidirectional RNN processes an infinitely long input audio streams without pre-segmentation and keywords are detected with low-latency before the utterance is finished. Experimental results show that the proposed keyword spotter significantly outperforms the deep neural network (DNN) and hidden Markov model (HMM) based keyword-filler model even with less computations.", "creator": "LaTeX with hyperref package"}}}