{"id": "1512.09300", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2015", "title": "Autoencoding beyond pixels using a learned similarity metric", "abstract": "initially we present an intermediate autoencoder that leverages the response power values of universally learned multiple representations, to collectively better confidently measure dynamic similarities in the data space. by combining every a single variational autoencoder ( + vae ) with instance a generative adaptive adversarial memory network ( mod gan ) we possibly can use learned feature representations in fitting the gan discriminator as basis pairs for calculating the subsequent vae reconstruction objective. thereby, we then replace element - wise errors with explicit feature - wise errors that thereby better capture the relevant data distance distribution characteristics while ultimately offering invariance towards e. g. translation. we apply our preferred method description to detect images of faces 2d and show conversely that our objective method outperforms vaes associated with classic element - wise similarity measures in objective terms of visual barrier fidelity. likewise moreover, we completely show that our method learns an expected embedding interval in which only high - level intrinsic abstract and visual threshold features ( e. more g. wearing glasses ) can be modified using simple arithmetic.", "histories": [["v1", "Thu, 31 Dec 2015 14:53:39 GMT  (1033kb,D)", "http://arxiv.org/abs/1512.09300v1", null], ["v2", "Wed, 10 Feb 2016 21:18:27 GMT  (1305kb,D)", "http://arxiv.org/abs/1512.09300v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["anders boesen lindbo larsen", "s\u00f8ren kaae s\u00f8nderby", "hugo larochelle", "ole winther"], "accepted": true, "id": "1512.09300"}, "pdf": {"name": "1512.09300.pdf", "metadata": {"source": "META", "title": "Autoencoding beyond pixels using a learned similarity metric", "authors": ["Anders Boesen", "Lindbo Larsen", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "emails": ["ABLL@DTU.DK", "SKAAESONDERBY@GMAIL.DK", "OLWI@DTU.DK"], "sections": [{"heading": "1. Introduction", "text": "Deep architectures have allowed a wide range of discriminative models to scale to large and diverse datasets. However, generative models still have problems with complex data distributions such as images and sound. In this work, we show that currently used similarity metrics impose a hurdle for learning good generative models and that we can improve a generative model by employing a learned similarity measure.\nWhen learning models such as the variational autoencoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014), the choice of similarity metric is central as it provides the main part of the training signal via the reconstruction error objective. For this task, the element-wise Euclidean dis-\nPreliminary work. Comments and suggestions are most welcome!\nx\nz\nx\u0303 REAL / GEN\nencoder\ndiscriminator\ndecoder/generator\nGAN AE\nx\nFigure 1. Overview of our network. We combine a VAE with a GAN by collapsing the decoder and the generator into one.\ntance is a commonly used metric. This metric is simple but not very suitable for image data, as it does not model the properties of human visual perception. E.g. a small image translation might result in a large pixel-wise error whereas a human would barely notice the change. Therefore, we argue in favor of measuring image similarity using a higherlevel and sufficiently invariant representation of the images. Rather than hand-engineering a suitable measure to accommodate the problems of element-wise metrics, we want to learn a function for the task. The question is how to learn such a similairty measure? We find that by jointly training a VAE and a GAN we can use the GAN discriminator to measure sample similairty. We achieve this by combining a VAE with a GAN (Goodfellow et al., 2014) as shown in Fig. 1. We collapse the VAE decoder and the GAN generator into one by letting them share parameters and training them together. For the VAE training objective, we replace the typical element-wise reconstruction metric with a feature-wise metric expressed in the discriminator."}, {"heading": "1.1. Contributions", "text": "\u2022 We combine VAEs and GANs into an unsupervised generative model that simultaneously learns to encode, generate and compare dataset samples.\n\u2022 We show that a model trained with learned error measures generates better image samples.\nar X\niv :1\n51 2.\n09 30\n0v 1\n[ cs\n.L G\n] 3\n1 D\nec 2\n\u2022 We demonstrate that unsupervised training result in a latent image representation with disentangled factors of variation (Bengio et al., 2013)."}, {"heading": "2. Autoencoding with learned similarity", "text": "In this section we provide background on VAEs and GANs and introduce a slight modification of GANs that makes combining VAEs and GANs more straightforward. Furthermore, we replace the expected log likelihood in the VAE with a reconstruction error expressed by the GAN.\nVariational autoencoder A VAE consists of two networks that encode a data sample x to a latent representation z and decode the latent representation back to data space, respectively:\nz \u223c Enc(x) = q(z|x) , x\u0303 \u223c Dec(z) = p(x|z) . (1)\nThe VAE regularizes the encoder by imposing a prior over the distribution of the encoding p(z). Typically z \u223c N (0, I) is chosen. The VAE loss is minus the sum of the expected log likelihood (the reconstruction error) and a prior regularization term:\nLVAE = \u2212Eq(z|x) [ log\np(x|z)p(z) q(z|x) ] = Lpixelllike + Lprior\n(2)\nwith\nLpixelllike =\u2212 Eq(z|x) [log p(x|z)] (3) Lprior =DKL(q(z|x)\u2016p(z)) , (4)\nwhere DKL is the Kullback-Leibler divergence.\nGenerative adversarial network A GAN consists of two networks: the generator network Gen(z) maps latents z to data space while the discriminator network assigns probability y = Dis(x) \u2208 [0, 1] that x is an actual training sample and probability 1\u2212 y that x is generated by our model through x = Gen(z) with z \u223c p(z). The GAN objective is to find the binary classifier that gives the best possible discrimination between true and generated data and simultaneously encouraging Gen to fit the true data distribution. We thus aim to maximize/minimize the binary cross entropy:\nLGAN = log(Dis(x)) + log(1\u2212Dis(Gen(z))) , (5)\nwith respect to Dis /Gen with x being a training sample and z \u223c p(z). We generalize GANs by replacing the deterministic generator network with a sample from the VAE decoder:\nLGAN = log(Dis(x)) + log(1\u2212Dis(x\u0303)) (6)\nwith x\u0303 \u223c Dec(z) and z \u223c p(z). This parameter sharing step allows us to combine the VAE encoder/decoder framework with GAN."}, {"heading": "2.1. Beyond element-wise reconstruction error", "text": "Since element-wise reconstruction errors are not adequate for images and other signals with invariances, we propose replacing the VAE reconstruction (expected log likelihood) error term from Eq. 3 with a reconstruction error expressed in the GAN discriminator. To achieve this let Disl(x) denote the hidden representation of the lth layer of the discriminator. We introduce a Gaussian observation model for Disl(x) with mean Disl(x\u0303) and identity covariance:\np(Disl(x)|z) = N (Disl(x)|Disl(x\u0303), I) , (7)\nwhere x\u0303 \u223c Dec(z) is the the sample from the decoder of x. We can now the replace the VAE error Eq. 3 with\nLDislllike = \u2212Eq(z|x) [log p(Disl(x)|z)] (8)\nWe train our combined model with the triple criterion\nL = Lprior + LDislllike + LGAN . (9)\nNotably, we optimize the VAE wrt. LGAN which we regard as a style error in addition to the reconstruction error which can be interpreted as a content error using the terminology from Gatys et al. (2015).\nIn practice, we have observed the devil in the details during development and training of this model. We therefore provide a list of practical considerations in this section. We refer to Fig. 2 and Alg. 1 for overviews of the training procedure.\nLimiting error signals to relevant networks Using the loss function in Eq. 9, we train both a VAE and a GAN simultaneously. This is possible because we do not update all network parameters wrt. the combined loss. In particular, Dis should not try to minimize LDislllike as this would collapse the discriminator to 0. We also observe better results by not backpropagating the error signal from LGAN to Enc.\nWeighting VAE vs. GAN As Dec receives an error signal from both LDislllike and LGAN, we use a parameter \u03b3 to weight the ability to reconstruct vs. fooling the discriminator. This can also be interpreted as weighting style and content. Rather than applying \u03b3 to the entire model (Eq. 9), we perform the weighting only when updating the parameters of Dec:\n\u03b8Dec +\u2190 \u2212\u2207\u03b8Dec(\u03b3LDislllike \u2212 LGAN) (10)\nx\nz\nx\u0303\nEnc\nDis\nDec\nx\nLprior\nLDislllike\nLGAN\nzp\nxp\np(z)\nFigure 2. Flow through the combined VAE/GAN model during training. Gray lines represent terms in the training objective.\nAlgorithm 1 Training the VAE/GAN model \u03b8Enc,\u03b8Dec,\u03b8Dis \u2190 initialize network parameters repeat X \u2190 random mini-batch from dataset Z \u2190 Enc(X) Lprior \u2190 DKL(q(Z|X)\u2016p(Z)) X\u0303 \u2190 Dec(Z) LDislllike \u2190 \u2212Eq(Z|X) [p(Disl(X)|Z)] Zp \u2190 samples from prior N (0, I) Xp \u2190 Dec(Zp) LGAN \u2190 \u2016 log(Dis(X)) + log(1\u2212Dis(X\u0303))\n+ log(1\u2212Dis(Xp))\u20161 // Update parameters according to gradients \u03b8Enc\n+\u2190 \u2212\u2207\u03b8Enc(Lprior + LDislllike ) \u03b8Dec\n+\u2190 \u2212\u2207\u03b8Dec(\u03b3LDislllike \u2212 LGAN) \u03b8Dis\n+\u2190 \u2212\u2207\u03b8DisLGAN until deadline\nSampling from p(z) We observe better results by sampling directly from our prior p(z) and using these in addition to Enc(x) when training the GAN. We suspect that our regularization of the latent space does not quite achieve z \u223c N (0, I) which is problematic when we draw samples from N (0, I) at test time."}, {"heading": "3. Experiments", "text": "Measuring the quality of generative models is challenging as current evaluation methods do not scale to natural images. Because the log-likelihood is intractable, it is often estimated using a Parzen window approach (Breuleux et al., 2011), but even for small images of 6\u00d76 pixels this is shown to be biased in favor of methods that overfit (Theis et al., 2015). In this work, we use images of size 64x64 and perform a qualitative assessment only since current evaluation methods are not realiable for our problem size. We leave it as future work to evaluate the method with more\ncomparable measures.\nWe investigate the performance of different generative models:\n\u2022 Plain VAE with an element-wise Gaussian recognition model. This corresponds to a reconstruction error with an Euclidean distance.\n\u2022 VAE with a learned distance (VAEDisl ). We first train a GAN and use the discriminator network as a learned similarity measure. We select a single layer l at which we measure the similariy according to Disl. l is chosen such that the comparison is performed after 3 downsamplings of each a factor of 2.\n\u2022 The combined VAE/GAN model. This model is similar to VAEDisl but we also optimize Dec wrt. LGAN. \u2022 GAN. This modes has recently been shown capable of generating high-quality images (Denton et al., 2015; Radford et al., 2015).\nAll models share the same architectures for Enc, Dec and Dis respectively. For all our experiments, we use convolutional architectures and use backward convolution (aka. fractional striding) with stride 2 to upscale images in Dec. Backward convolution is achieved by flipping the convolution direction such that striding causes upsampling. We omit further details concerning network construction and training and refer to our implementation available online1."}, {"heading": "3.1. Face images", "text": "We apply our methods to face images from the CelebA dataset2 (Liu et al., 2015). This dataset consists of 202,599 images annotated with 40 binary attributes such as eyeglasses, bangs, pale skin etc. We scale and crop the images to 64\u00d764 pixels and use only the images (not the attributes) for unsupervised training.\nAfter training, we draw samples from p(z) and propagate these through Dec to generate new images which are shown in Fig. 3. The plain VAE is able draw the frontal part of the face sharply, but off-center the images get blurry. This is because the dataset aligns faces using frontal landmarks. When we move too far away from the aligned parts, the recognition model breaks down because pixel correspondence cannot be assumed. VAEDisl produces sharper images even off-center because the reconstruction error is lifted beyond pixels. However, we see severe noisy artefacts which we believe are caused by the harsh downsampling scheme. In comparison, VAE/GAN and pure GAN produce sharper images with more natural textures and face\n1http://github.com/andersbll/ autoencoding_beyond_pixels\n2We use the aligned and cropped images.\nparts.\nAdditionally, we make the VAEs reconstruct images taken from a separate test set. Reconstruction is not possible with the GAN model as it lacks an encoder network. The results are shown in Fig. 4 and our conclusions are similar to what we observed for the random samples. Note that VAEDisl generates noisy blue patterns in some of the reconstructions. We suspect the GAN-based similarity measure can collapse to 0 in certain cases (such as the pattern we observe), which encourages Dec to generate such patterns."}, {"heading": "3.1.1. VISUAL ATTRIBUTE VECTORS", "text": "Inspired by attempts at learning embeddings in which semantic concepts can be expressed using simple arithmetic (Mikolov et al., 2013), we inspect the latent space of a trained AE/GAN model. The idea is to find directions in the latent space corresponding to specific visual features in image space.\nWe use the binary attributes of the dataset to extract visual attribute vectors. For all images we use the encoder to calculate latent vector representation. For each attribute, we compute the mean vector for images with the attribute and\nthe mean vector for images without the attribute. We then compute the visual attribute vector as the difference between the two mean vectors. This is a very simple method for calculating visual attribute vectors that will have problems with highly correlated visual attributes such as heavy makeup and wearing lipstick. In Fig. 5, we show face images as well as the reconstructions after adding different visual attribute vectors to the latent representations. Though not perfect, we clearly see that the attribute vectors capture semantic concept like eyeglasses, bangs, etc. E.g. when bangs are added to the faces, both the hair color and the hair texture matches the original face. We also see that being a man is highly correlated with having a mustache which caused by attribute correlations in the dataset."}, {"heading": "4. Related work", "text": "Element-wise distance measures are notoriously inadequate for complex data distributions like images. In the computer vision community, preprocessing images is a prevalent solution to improve robustness to certain perturbations. Examples of preprocessing are contrast normalization, working with gradient images or pixel statistics gathered in histograms. We view these operations as a form of metric engineering to account for the shortcomings of simple element-wise distance measures. A more detailed discussion on the subject is provided by Wang & Bovik (2009).\nNeural networks have been applied to metric learning in form of the Siamese architecture (Bromley et al., 1993; Chopra et al., 2005). The learned distance metric is minimized for similar samples and maximized for dissimilar samples using a max margin cost. However, since Siamese networks are trained in a supervised setup, we cannot apply them directly to our problem.\nFor autoencoders (AE), several attempts at improving on element-wise distances have been proposed within the last year. Ridgeway et al. (2015) apply the structural similarity index as reconstruction metric for grey-scale images. Yan et al. (2015) let the AE output two additional images to learn shape and edge structures more explicitly. Mansimov et al. (2015) append a GAN-based sharpening step to their decoder network. While these AE extensions yield sharper images, they are not built capture high-level structure compared to a deep learning approach.\nIn contrast to AEs that model the relationship between a dataset sample and a latent representation directly, GANs learn to generate samples indirectly. By optimizing the GAN generator to produce samples that imitate the dataset according to the GAN discriminator, GANs avoid elementwise similarity measures by construction. This is a likely explanation for their ability to produce high-quality images\nas demonstrated by Denton et al. (2015); Radford et al. (2015).\nLately, convolutional networks with upsampling have shown useful for generating images from a latent representation. This has sparked interest in learning image embeddings where semantic relationships can be expressed using simple arithmetic \u2013 similar to the suprising results of the word2vec model by Mikolov et al. (2013). First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al. (2015); Reed et al. (2015) have demonstrated encoder-decoder architectures with disentangled feature representations, but their training schemes rely on supervised information. Radford et al. (2015) inspect the latent space of a GAN after training and find directions corresponding to eyeglasses and smiles. As they rely on pure GANs, however, they cannot encode images making it challenging to explore the latent space.\nOur idea of a learned similarity metric is partly motivated by the neural artistic style network of Gatys et al. (2015) who demonstrate the representational power of deep convolutional features. They obtain impressive results by optimizing an image to have similar features as a subject image and similar feature correlations as a style image in a pretrained convolutional network. In our VAE/GAN model, one could view LDislllike as content and LGAN as style. Our style term, though, is not computed from feature correlations but is the error signal from trying to fool the GAN discriminator."}, {"heading": "5. Discussion", "text": "The problems with element-wise distance metrics are well known in the literature and many attempts have been made at going beyond pixels \u2013 typically using hand-engineered measures. Much in the spirit of deep learning, we argue that the similarity measure is yet another component which can be replaced by a learned model capable of capturing high-level structure relevant to the data distribution. In this work, our main contribution is an unsupervised scheme for learning and applying such a distance measure. With the learned distance measure we are able to train an image encoder-decoder network generating images of unprecedented visual fidelity as shown by our experiments. Moreover, we show that our network is able to disentangle factors of variation in the input data distribution and discover visual attributes in the high-level representation of the latent space. In principle, this lets us employ a large set of unlabeled images for training and use a small set of labeled images to discover features in latent space.\nWe regard our method as an extension of the VAE framework. Though, it must be noted that the high quality of our generated images is due to the combined training of Dec as a both a VAE decoder and a GAN generator. This makes our method more of a hybrid between VAE and GAN, and alternatively, one could view our method more as an extension of GAN where p(z) is constrained by an additional network.\nIt is not obvious that the discriminator network of a GAN provides a useful similarity measure as it is trained for a different task, namely being able to tell generated faces from\nreal faces. However, convolutional features are often surprisingly good for transfer learning, and as we show, good enough in our case to improve on element-wise distances for images. It would be interesting to see if better features in the distance measure would improve the model, e.g. by employing a similarity measure provided by a Siamese network trained on faces. Though in practice, Siamese networks are not a good fit with our method as they require labeled data. Alternatively one could investigate the effect of using a pretrained feedforward network for measuring similarity.\nWhile we believe our preliminary results are convincing, we acknowledge the problem of not having a good measure for evaluating our generative model comparatively. Further work is needed in this direction to verify the applicability of our method beyond generating visually pleasing images."}, {"heading": "Acknowledgements", "text": "We would like to thank S\u00f8ren Hauberg, Casper Kaae S\u00f8nderby and Lars Maal\u00f8e for insightful discussions, Nvidia for donating GPUs used in experiments, and the authors of DeepPy3 and CUDArray (Larsen, 2014) for the software frameworks used to implement our model."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pierre"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Quickly generating representative samples from an rbmderived process", "author": ["Breuleux", "Olivier", "Bengio", "Yoshua", "Vincent", "Pascal"], "venue": "Neural Computation,", "citeRegEx": "Breuleux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Breuleux et al\\.", "year": 2011}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["S. Chopra", "R. Hadsell", "Y. LeCun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Learning to generate chairs with convolutional neural networks", "author": ["Dosovitskiy", "Alexey", "Springenberg", "Jost Tobias", "Brox", "Thomas"], "venue": "In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Dosovitskiy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dosovitskiy et al\\.", "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["Gatys", "Leon A", "Ecker", "Alexander S", "Bethge", "Matthias"], "venue": "CoRR, abs/1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Deep convolutional inverse graphics", "author": ["Kulkarni", "Tejas D", "Whitney", "Will", "Kohli", "Pushmeet", "Tenenbaum", "Joshua B"], "venue": "network. CoRR,", "citeRegEx": "Kulkarni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "CUDArray: CUDA-based NumPy", "author": ["Larsen", "Anders Boesen Lindbo"], "venue": "Technical Report DTU Compute 2014-21,", "citeRegEx": "Larsen and Lindbo.,? \\Q2014\\E", "shortCiteRegEx": "Larsen and Lindbo.", "year": 2014}, {"title": "Deep learning face attributes in the wild", "author": ["Liu", "Ziwei", "Luo", "Ping", "Wang", "Xiaogang", "Tang", "Xiaoou"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Generating images from captions with attention", "author": ["Mansimov", "Elman", "Parisotto", "Emilio", "Ba", "Lei Jimmy", "Salakhutdinov", "Ruslan"], "venue": "CoRR, abs/1511.02793,", "citeRegEx": "Mansimov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mansimov et al\\.", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "CoRR, abs/1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Learning to generate images with perceptual similarity", "author": ["Ridgeway", "Karl", "Snell", "Jake", "Roads", "Brett", "Zemel", "Richard S", "Mozer", "Michael C"], "venue": "metrics. CoRR,", "citeRegEx": "Ridgeway et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ridgeway et al\\.", "year": 2015}, {"title": "A note on the evaluation of generative models", "author": ["L. Theis", "A. van den Oord", "M. Bethge"], "venue": "CoRR, abs/1511.01844,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "Mean squared error: Love it or leave it? a new look at signal fidelity measures", "author": ["Wang", "Zhou", "A.C. Bovik"], "venue": "Signal Processing Magazine,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Attribute2Image: Conditional Image Generation from Visual Attributes", "author": ["X. Yan", "J. Yang", "K. Sohn", "H. Lee"], "venue": "CoRR, abs/1512.00570,", "citeRegEx": "Yan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "When learning models such as the variational autoencoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014), the choice of similarity metric is central as it provides the main part of the training signal via the reconstruction error objective.", "startOffset": 63, "endOffset": 109}, {"referenceID": 0, "context": "\u2022 We demonstrate that unsupervised training result in a latent image representation with disentangled factors of variation (Bengio et al., 2013).", "startOffset": 123, "endOffset": 144}, {"referenceID": 4, "context": "LGAN which we regard as a style error in addition to the reconstruction error which can be interpreted as a content error using the terminology from Gatys et al. (2015).", "startOffset": 149, "endOffset": 169}, {"referenceID": 1, "context": "Because the log-likelihood is intractable, it is often estimated using a Parzen window approach (Breuleux et al., 2011), but even for small images of 6\u00d76 pixels this is shown to be biased in favor of methods that overfit (Theis et al.", "startOffset": 96, "endOffset": 119}, {"referenceID": 13, "context": ", 2011), but even for small images of 6\u00d76 pixels this is shown to be biased in favor of methods that overfit (Theis et al., 2015).", "startOffset": 109, "endOffset": 129}, {"referenceID": 10, "context": "This modes has recently been shown capable of generating high-quality images (Denton et al., 2015; Radford et al., 2015).", "startOffset": 77, "endOffset": 120}, {"referenceID": 8, "context": "We apply our methods to face images from the CelebA dataset2 (Liu et al., 2015).", "startOffset": 61, "endOffset": 79}, {"referenceID": 2, "context": "Neural networks have been applied to metric learning in form of the Siamese architecture (Bromley et al., 1993; Chopra et al., 2005).", "startOffset": 89, "endOffset": 132}, {"referenceID": 11, "context": "Ridgeway et al. (2015) apply the structural similarity index as reconstruction metric for grey-scale images.", "startOffset": 0, "endOffset": 23}, {"referenceID": 11, "context": "Ridgeway et al. (2015) apply the structural similarity index as reconstruction metric for grey-scale images. Yan et al. (2015) let the AE output two additional images to learn shape and edge structures more explicitly.", "startOffset": 0, "endOffset": 127}, {"referenceID": 9, "context": "Mansimov et al. (2015) append a GAN-based sharpening step to their decoder network.", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "(2015); Radford et al. (2015).", "startOffset": 8, "endOffset": 30}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair.", "startOffset": 7, "endOffset": 33}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al.", "startOffset": 7, "endOffset": 191}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al. (2015); Reed et al.", "startOffset": 7, "endOffset": 210}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al. (2015); Reed et al. (2015) have demonstrated encoder-decoder architectures with disentangled feature representations, but their training schemes rely on supervised information.", "startOffset": 7, "endOffset": 230}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al. (2015); Reed et al. (2015) have demonstrated encoder-decoder architectures with disentangled feature representations, but their training schemes rely on supervised information. Radford et al. (2015) inspect the latent space of a GAN after training and find directions corresponding to eyeglasses and smiles.", "startOffset": 7, "endOffset": 402}, {"referenceID": 3, "context": "First, Dosovitskiy et al. (2015) used supervised training to train convolutional network to generate chairs given highlevel information about the desired chair. Later, Kulkarni et al. (2015); Yan et al. (2015); Reed et al. (2015) have demonstrated encoder-decoder architectures with disentangled feature representations, but their training schemes rely on supervised information. Radford et al. (2015) inspect the latent space of a GAN after training and find directions corresponding to eyeglasses and smiles. As they rely on pure GANs, however, they cannot encode images making it challenging to explore the latent space. Our idea of a learned similarity metric is partly motivated by the neural artistic style network of Gatys et al. (2015) who demonstrate the representational power of deep convolutional features.", "startOffset": 7, "endOffset": 744}], "year": 2015, "abstractText": "We present an autoencoder that leverages the power of learned representations to better measure similarities in data space. By combining a variational autoencoder (VAE) with a generative adversarial network (GAN) we can use learned feature representations in the GAN discriminator as basis for the VAE reconstruction objective. Thereby, we replace element-wise errors with feature-wise errors that better capture the data distribution while offering invariance towards e.g. translation. We apply our method to images of faces and show that our method outperforms VAEs with element-wise similarity measures in terms of visual fidelity. Moreover, we show that our method learns an embedding in which high-level abstract visual features (e.g. wearing glasses) can be modified using simple arithmetic.", "creator": "LaTeX with hyperref package"}}}