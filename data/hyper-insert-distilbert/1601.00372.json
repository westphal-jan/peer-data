{"id": "1601.00372", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jan-2016", "title": "Mutual Information and Diverse Decoding Improve Neural Machine Translation", "abstract": "sequence - to - sequence mediated neural translation models learn semantic content and syntactic processing relations between sentence encoding pairs alike by rapidly optimizing the likelihood reduction of rendering the similarity target model given nearest the source, i. that e., $ p ( y | x ) $, an objective that ignores other potentially useful sources sources of information. we introduce us an intuitive alternative objective cost function for neural mt that radically maximizes the mutual recognition information variance between the sampled source and target sample sentences, consequently modeling the inherent bi - temporal directional dependency consequences of similar sources tags and targets. we implement the model formulation with a new simple distributed re - ranking method, yielding and also introduce a decoding algorithm strategy that increases our diversity in filtering the n - best list produced generated by the first pass. applied science to mimic the wmt german / english and french / english tasks, both mechanisms, offer it a fully consistent performance boost aiming on developing both standard unix lstm modeling and microsoft attention - based neural mt model architectures. drawing the result far is estimated the globally best published performance for examining a small single ( non - ensemble ) reactive neural block mt system, as well therefore as illustrates the potential application testing of our diverse decoding algorithm to address other nlp re - gen ranking tasks.", "histories": [["v1", "Mon, 4 Jan 2016 03:04:05 GMT  (106kb,D)", "https://arxiv.org/abs/1601.00372v1", null], ["v2", "Tue, 22 Mar 2016 21:15:30 GMT  (109kb,D)", "http://arxiv.org/abs/1601.00372v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["jiwei li", "dan jurafsky"], "accepted": false, "id": "1601.00372"}, "pdf": {"name": "1601.00372.pdf", "metadata": {"source": "CRF", "title": "Mutual Information and Diverse Decoding Improve Neural Machine Translation", "authors": ["Jiwei Li"], "emails": ["jiweil@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": null, "text": "Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., p(y|x), an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, the proposed models offers a consistent performance boost on both standard LSTM and attention-based neural MT architectures."}, {"heading": "1 Introduction", "text": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches. SEQ2SEQ models require minimal domain knowledge, can be trained end-to-end, have a much smaller memory footprint than the large phrase tables needed for phrase-based SMT, and achieve state-of-the-art performance in large-scale tasks like English to French\n(Luong et al., 2015b) and English to German (Luong et al., 2015a; Jean et al., 2014) translation.\nSEQ2SEQ models are implemented as an encoder-decoder network, in which a source sequence input x is mapped (encoded) to a continuous vector representation from which a target output y will be generated (decoded). The framework is optimized through maximizing the log-likelihood of observing the paired output y given x:\nLoss = \u2212 log p(y|x) (1)\nWhile standard SEQ2SEQ models thus capture the unidirectional dependency from source to target, i.e., p(y|x), they ignore p(x|y), the dependency from the target to the source, which has long been an important feature in phrase-based translation (Och and Ney, 2002; Shen et al., 2010). Phrase based systems that combine p(x|y), p(y|x) and other features like sentence length yield significant performance boost.\nWe propose to incorporate this bi-directional dependency and model the maximum mutual information (MMI) between source and target into SEQ2SEQ models. As Li et al. (2015) recently showed in the context of conversational response generation, the MMI based objective function is equivalent to linearly combining p(x|y) and p(y|x). With a tuning weight \u03bb, such a loss function can be written as :\ny\u0302 = argmax y\nlog p(x, y)\np(x)p(y)\u03bb\n= argmax y\n(1\u2212 \u03bb) log p(y|x) + \u03bb log p(x|y)\n(2) But as also discussed in Li et al. (2015), direct decoding from (2) is infeasible because computing p(x|y) cannot be done until the target has been\nar X\niv :1\n60 1.\n00 37\n2v 2\n[ cs\n.C L\n] 2\n2 M\nar 2\n01 6\ncomputed1. To avoid this enormous search space, we propose to use a reranking approach to approximate the mutual information between source and target in neural machine translation models. We separately trained two SEQ2SEQ models, one for p(y|x) and one for p(x|y). The p(y|x) model is used to generate N-best lists from the source sentence x. The lists are followed by a reranking process using the second term of the objective function, p(x|y).\nBecause reranking approaches are dependent on having a diverse N-best list to rerank, we also propose a diversity-promoting decoding model tailored to neural MT systems. We tested the mutual information objective function and the diversitypromoting decoding model on English\u2192French, English\u2192German and German\u2192English translation tasks, using both standard LSTM settings and the more advanced attention-model based settings that have recently shown to result in higher performance.\nThe next section presents related work, followed by a background section 3 introducing LSTM/Attention machine translation models. Our proposed model will be described in detail in Sections 4, with datasets and experimental results in Section 6 followed by conclusions."}, {"heading": "2 Related Work", "text": "This paper draws on three prior lines of research: SEQ2SEQ models, modeling mutual information, and promoting translation diversity.\nSEQ2SEQ Models SEQ2SEQ models map source sequences to vector space representations, from which a target sequence is then generated. They yield good performance in a variety of NLP generation tasks including conversational response generation (Vinyals and Le, 2015; Serban et al., 2015a; Li et al., 2015), and parsing (Vinyals et al., 2014; ?).\nA neural machine translation system uses distributed representations to model the conditional\n1As demonstrated in (Li et al., 2015)\nlog p(x, y)\np(x)p(y)\u03bb = log p(y|x)\u2212 \u03bb log p(y) (3)\nEqu. 2 can be immediately achieved by applying bayesian rules\nlog p(y) = log p(y|x) + log p(x)\u2212 log p(x|y)\nprobability of targets given sources, using two components, an encoder and a decoder. Kalchbrenner and Blunsom (2013) used an encoding model akin to convolutional networks for encoding and standard hidden unit recurrent nets for decoding. Similar convolutional networks are used in (Meng et al., 2015) for encoding. Sutskever et al. (2014; Luong et al. (2015a) employed a stacking LSTM model for both encoding and decoding. Bahdanau et al. (2014), Jean et al. (2014) adopted bi-directional recurrent nets for the encoder.\nMaximum Mutual Information Maximum Mutual Information (MMI) was introduced in speech recognition (Bahl et al., 1986) as a way of measuring the mutual dependence between inputs (acoustic feature vectors) and outputs (words) and improving discriminative training (Woodland and Povey, 2002). Li et al. (2015) show that MMI could solve an important problem in SEQ2SEQ conversational response generation. Prior SEQ2SEQ models tended to generate highly generic, dull responses (e.g., I don\u2019t know) regardless of the inputs (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2015b). Li et al. (2015) show that modeling the mutual dependency between messages and response promotes the diversity of response outputs.\nOur goal, distinct from these previous uses of MMI, is to see whether the mutual information objective improves translation by bidirectionally modeling source-target dependencies. In that sense, our work is designed to incorporate into SEQ2SEQ models features that have proved useful in phrasebased MT, like the reverse translation probability or sentence length (Och and Ney, 2002; Shen et al., 2010; Devlin et al., 2014).\nGenerating Diverse Translations Various algorithms have been proposed for generated diverse translations in phrase-based MT, including compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al., 2013), or multiple systems (Cer et al., 2013). Gimpel et al. (2013; Batra et al. (2012), produce diverse N-best lists by adding a dissimilarity function based on N-gram overlaps, distancing the current translation from already-generated ones by choosing translations that have higher scores but distinct from previous ones. While we draw on these intuitions, these existing diversity promoting algorithms are tailored to\nphrase-based translation frameworks and not easily transplanted to neural MT decoding which requires batched computation."}, {"heading": "3 Background: Neural Machine Translation", "text": "Neural machine translation models map source x = {x1, x2, ...xNx} to a continuous vector representation, from which target output y = {y1, y2, ..., yNy} is to be generated."}, {"heading": "3.1 LSTM Models", "text": "A long-short term memory model (Hochreiter and Schmidhuber, 1997) associates each time step with an input gate, a memory gate and an output gate, denoted respectively as it, ft and ot. Let et denote the vector for the current word wt, ht the vector computed by the LSTM model at time t by combining et and ht\u22121., ct the cell state vector at time t, and \u03c3 the sigmoid function. The vector representation ht for each time step t is given by:\nit = \u03c3(Wi \u00b7 [ht\u22121, et]) (4) ft = \u03c3(Wf \u00b7 [ht\u22121, et]) (5) ot = \u03c3(Wo \u00b7 [ht\u22121, et]) (6) lt = tanh(Wl \u00b7 [ht\u22121, et]) (7) ct = ft \u00b7 ct\u22121 + it \u00b7 lt (8)\nhst = ot \u00b7 tanh(ct) (9)\nwhere Wi, Wf , Wo, Wl \u2208 RK\u00d72K . The LSTM defines a distribution over outputs y and sequentially predicts tokens using a softmax function:\np(y|x) = nT\u220f t=1 exp(f(ht\u22121, eyt))\u2211 w\u2032 exp(f(ht\u22121, ew\u2032))\nwhere f(ht\u22121, eyt) denotes the activation function between ht\u22121 and ewt , where ht\u22121 is the representation output from the LSTM at time t\u2212 1. Each sentence concludes with a special end-of-sentence symbol EOS. Commonly, the input and output each use different LSTMs with separate sets of compositional parameters to capture different compositional patterns. During decoding, the algorithm terminates when an EOS token is predicted."}, {"heading": "3.2 Attention Models", "text": "Attention models adopt a look-back strategy that links the current decoding stage with input time steps to represent which portions of the input are most responsible for the current decoding state (Xu\net al., 2015; Luong et al., 2015b; Bahdanau et al., 2014).\nLet H = {h\u03021, h\u03022, ..., h\u0302Nx} be the collection of hidden vectors outputted from LSTMs during encoding. Each element in H contains information about the input sequences, focusing on the parts surrounding each specific token. Let ht\u22121 be the LSTM outputs for decoding at time t \u2212 1. Attention models link the current-step decoding information, i.e., ht with each of the representations at decoding step h\u0302t\u2032 using a weight variable at. at can be constructed from different scoring functions such as the dot product between the two vectors, i.e., hTt\u22121 \u00b7 h\u0302t, a general model akin to tensor operation i.e., hTt\u22121 \u00b7 W \u00b7 h\u0302t, and the concatenation model by concatenating the two vectors i.e., UT \u00b7tanh(W \u00b7 [ht\u22121, h\u0302t]). The behavior of different attention scoring functions have been extensively studied in Luong et al. (2015a). For all experiments in this paper, we adopt the general strategy where the relevance score between the current step of the decoding representation and the encoding representation is given by:\nvt\u2032 = h T t\u22121 \u00b7W \u00b7 h\u0302t ai = exp(vt\u2217)\u2211 t\u2217 exp(vt\u2217)\n(10)\nThe attention vector is created by averaging weights over all input time-steps:\nmt = \u2211\nt\u2032\u2208[1,NS ]\naih\u0302t\u2032 (11)\nAttention models predict subsequent tokens based on the combination of the last step outputted LSTM vectors ht\u22121 and attention vectors mt:\n~ht\u22121 = tanh(Wc \u00b7 [ht\u22121,mt]) p(yt|y<, x) = softmax(Ws \u00b7 ~ht\u22121) (12)\nwhere Wc \u2208 RK\u00d72K , Ws \u2208 RV\u00d7K with V denoting vocabulary size. Luong et al. (2015a) reported a significant performance boost by integrating ~ht\u22121 into the next step LSTM hidden state computation (referred to as the input-feeding model), making LSTM compositions in decoding as follows:\nit = \u03c3(Wi \u00b7 [ht\u22121, et,~ht\u22121])\nft = \u03c3(Wf \u00b7 [ht\u22121, et,~ht\u22121])\not = \u03c3(Wo \u00b7 [ht\u22121, et,~ht\u22121])\nlt = tanh(Wl \u00b7 [ht\u22121, et,~ht\u22121])\n(13)\nwhere Wi, Wf , Wo, Wl \u2208 RK\u00d73K . For the attention models implemented in this work, we adopt the input-feeding strategy."}, {"heading": "3.3 Unknown Word Replacements", "text": "One of the major issues in neural MT models is the computational complexity of the softmax function for target word prediction, which requires summing over all tokens in the vocabulary. Neural models tend to keep a shortlist of 50,00-80,000 most frequent words and use an unknown (UNK) token to represent all infrequent tokens, which significantly impairs BLEU scores. Recent work has proposed to deal with this issue: (Luong et al., 2015b) adopt a post-processing strategy based on aligner from IBM models, while (Jean et al., 2014) approximates softmax functions by selecting a small subset of target vocabulary.\nIn this paper, we use a strategy similar to that of Jean et al. (2014), thus avoiding the reliance on external IBM model word aligner. From the attention models, we obtain word alignments from the training dataset, from which a bilingual dictionary is extracted. At test time, we first generate target sequences. Once a translation is generated, we link the generated UNK tokens back to positions in the source inputs, and replace each UNK token with the translation word of its correspondent source token using the pre-constructed dictionary.\nAs the unknown word replacement mechanism relies on automatic word alignment extraction which is not explicitly modeled in vanilla SEQ2SEQ models, it can not be immediately applied to vanilla SEQ2SEQ models. However, since unknown word replacement can be viewed as a post-processing technique, we can apply a pretrained attention-model to any given translation. For SEQ2SEQ models, we first generate translations and replace UNK tokens within the translations using the pre-trained attention models to postprocess the translations."}, {"heading": "4 Mutual Information via Reranking", "text": "As discussed in Li et al. (2015), direct decoding from (2) is infeasible since the second part, p(x|y), requires completely generating the target before it can be computed. We therefore use an approximation approach:\n1. Train p(y|x) and p(x|y) separately using vanilla SEQ2SEQ models or Attention models.\n2. Generate N-best lists from p(y|x).\n3. Rerank the N-best list by linearly adding p(x|y)."}, {"heading": "4.1 Standard Beam Search for N-best lists", "text": "N-best lists are generated using a beam search decoder with beam size set to K = 200 from p(y|x) models. As illustrated in Figure 1, at time step t\u22121 in decoding, we keep record ofN hypotheses based on score S(Yt\u22121|x) = log p(y1, y2, ..., yt\u22121|x). As we move on to time step t, we expand each of the K hypotheses (denoted as Y kt\u22121 = {yk1 , yk2 , ..., ykt\u22121}, k \u2208 [1,K]), by selecting top K of the translations, denoted as yk,k \u2032\nt , k \u2032 \u2208 [1,K], leading to the con-\nstruction of K \u00d7K new hypotheses:\n[Y kt\u22121, y k,k\u2032 t ], k \u2208 [1,K], k\u2032 \u2208 [1,K]\nThe score for each of the K \u00d7 K hypotheses is computed as follows:\nS(Y kt\u22121, y k,k\u2032 t |x) = S(Y kt\u22121|x)+log p(y k,k\u2032\nt |x, Y kt\u22121) (14)\nIn a standard beam search model, the top K hypotheses are selected (from the K \u00d7 K hypotheses computed in the last step) based on the score S(Y kt\u22121, y k,k\u2032\nt |x). The remaining hypotheses are ignored as we proceed to the next time step.\nWe set the minimum length and maximum length to 0.75 and 1.5 times the length of sources. Beam size N is set to 200. To be specific, at each time step of decoding, we are presented with K \u00d7 K word candidates. We first add all hypotheses with an EOS token being generated at current time step to the N-best list. Next we preserve the top K unfinished hypotheses and move to next time step. We therefore maintain batch size of 200 constant when some hypotheses are completed and taken down by adding in more unfinished hypotheses. This will lead the size of final N-best list for each input much larger than the beam size2."}, {"heading": "4.2 Generating a Diverse N-best List", "text": "Unfortunately, the N-best lists outputted from standard beam search are a poor surrogate for the entire search space (Finkel et al., 2006; Huang, 2008). The beam search algorithm can only keep a small proportion of candidates in the search space and most of the generated translations in N-best list\n2For example, for the development set of the EnglishGerman WMT14 task, each input has an average of 2,500 candidates in the N-best list.\nare similar, differing only by punctuation or minor morphological variations, with most of the words overlapping. Because this lack of diversity in the N-best list will significantly decrease the impact of our reranking process, it is important to find a way to generate a more diverse N-best list.\nWe propose to change the way S(Y kt\u22121, y k,k\u2032\nt |x) is computed in an attempt to promote diversity, as shown in Figure 1. For each of the hypotheses Y kt\u22121 (he and it), we generate the top K translations, yk,k \u2032\nt , k \u2032 \u2208 [1,K] as in the standard beam\nsearch model. Next we rank the K translated tokens generated from the same parental hypothesis based on p(yk,k \u2032\nt |x, Y kt\u22121) in descending order: he is ranks the first among he is and he has, and he has ranks second; similarly for it is and it has.\nNext we rewrite the score for [Y kt\u22121, y k,k\u2032\nt ] by adding an additional part \u03b3k\u2032, where k\u2032 denotes the ranking of the current hypothesis among its siblings, which is first for he is and it is, second for he has and it has.\nS\u0302(Y kt\u22121, y k,k\u2032 t |x) = S(Y kt\u22121, y k,k\u2032 t |x)\u2212 \u03b3k\u2032 (15)\nThe top K hypothesis are selected based on S\u0302(Y kt\u22121, y k,k\u2032\nt |x) as we move on to the next time step. By adding the additional term \u03b3k\u2032, the model punishes bottom ranked hypotheses among siblings (hypotheses descended from the same parent). When we compare newly generated hypotheses descended from different ancestors, the model gives more credit to top hypotheses from each of different ancestors. For instance, even though the original score for it is is lower than he has, the model favors the former as the latter is more severely punished\nby the intra-sibling ranking part \u03b3k\u2032. The model thus generally favors choosing hypotheses from diverse parents, leading to a more diverse N-best list.\nThe proposed model is straightforwardly implemented with minor adjustment to the standard beam search model3.\nWe employ the diversity evaluation metrics in (Li et al., 2015) to evaluate the degree of diversity of the N-best lists: calculating the average number of distinct unigrams distinct-1 and bigrams distinct2 in the N-best list given each source sentence, scaled by the total number of tokens. By employing the diversity promoting model with \u03b3 tuned from the development set based on BLEU score, the value of distinct-1 increases from 0.54% to 0.95%, and distinct-2 increases from 1.55% to 2.84% for English-German translation. Similar phenomenon are observed from English-French translation tasks and details are omitted for brevity."}, {"heading": "4.3 Reranking", "text": "The generated N-best list is then reranked by linearly combining log p(y|x) with log p(x|y). The score of the source given each generated translation can be immediately computed from the previously trained p(x|y).\nOther than log p(y|x), we also consider log p(y), which denotes the average language model probability trained from monolingual data. It is worth\n3Decoding for neural based MT model using large batchsize can be expensive resulted from softmax word prediction function. The proposed model supports batched decoding using GPU, significantly speed up decoding process than other diversity fostering models tailored to phrase based MT systems.\nnothing that integrating log p(y|x) and log p(y) into reranking is not a new one and has long been employed by in noisy channel models in standard MT. In neural MT literature, recent progress has demonstrated the effectiveness of modeling reranking with language model (Gulcehre et al., 2015).\nWe also consider an additional term that takes into account the length of targets (denotes as LT ) in decoding. We thus linearly combine the three parts, making the final ranking score for a given target candidate y as follows:\nScore(y) = log p(y|x) + \u03bb log p(x|y) + \u03b3 log p(y) + \u03b7LT\n(16)\nWe optimize \u03b7, \u03bb and \u03b3 using MERT (Och, 2003) BLEU score (Papineni et al., 2002) on the development set."}, {"heading": "5 Experiments", "text": "Our models are trained on the WMT\u201914 training dataset containing 4.5 million pairs for EnglishGerman and German-English translation, and 12 million pairs for English-French translation. For English-German translation, we limit our vocabularies to the top 50K most frequent words for both languages. For English-French translation, we keep the top 200K most frequent words for the source language and 80K for the target language. Words that are not in the vocabulary list are noted as the universal unknown token.\nFor the English-German and English-German translation, we use newstest2013 (3000 sentence pairs) as the development set and translation performances are reported in BLEU (Papineni et al., 2002) on newstest2014 (2737) sentences. For English-French translation, we concatenate newstest-2012 and news-test-2013 to make a development set (6,003 pairs in total) and evaluate the models on news-test-2014 with 3,003 pairs4.\n5.1 Training Details for p(x|y) and p(y|x) We trained neural models on Standard SEQ2SEQ Models and Attention Models. We trained p(y|x) following the standard training protocols described in (Sutskever et al., 2014). p(x|y) is trained identically but with sources and targets swapped.\nWe adopt a deep structure with four LSTM layers for encoding and four LSTM layers for decod-\n4As in (Luong et al., 2015a). All texts are tokenized with tokenizer.perl and BLEU scores are computed with multibleu.perl\ning, each of which consists of a different set of parameters. We followed the detailed protocols from Luong et al. (2015a): each LSTM layer consists of 1,000 hidden neurons, and the dimensionality of word embeddings is set to 1,000. Other training details include: LSTM parameters and word embeddings are initialized from a uniform distribution between [-0.1,0.1]; For English-German translation, we run 12 epochs in total. After 8 epochs, we start halving the learning rate after each epoch; for English-French translation, the total number of epochs is set to 8, and we start halving the learning rate after 5 iterations. Batch size is set to 128; gradient clipping is adopted by scaling gradients when the norm exceeded a threshold of 5. Inputs are reversed.\nOur implementation on a single GPU5 processes approximately 800-1200 tokens per second. Training for the English-German dataset (4.5 million pairs) takes roughly 12-15 days. For the FrenchEnglish dataset, comprised of 12 million pairs, training takes roughly 4-6 weeks."}, {"heading": "5.2 Training p(y) from Monolingual Data", "text": "We respectively train single-layer LSTM recurrent models with 500 units for German and French using monolingual data. We News Crawl corpora from WMT136 as additional training data to train monolingual language models. We used a subset of the original dataset which roughly contains 50- 60 millions sentences. Following (Gulcehre et al., 2015; Sennrich et al., 2015a), we remove sentences with more than 10% Unknown words based on the vocabulary constructed using parallel datasets. We adopted similar protocols as we trained SEQ2SEQ models, such as gradient clipping and mini batch."}, {"heading": "5.3 English-German Results", "text": "We reported progressive performances as we add in more features for reranking. Results for different models on WMT2014 English-German translation task are shown in Figure 1. Among all the features, reverse probability from mutual information (i.e., p(x|y)) yields the most significant performance boost, +1.4 and +1.1 for standard SEQ2SEQ models without and with unknown word replacement, +0.9 for attention models7. In line with (Gul-\n5Tesla K40m, 1 Kepler GK110B, 2880 Cuda cores. 6http://www.statmt.org/wmt13/\ntranslation-task.html 7Target length has long proved to be one of the most important features in phrase based MT due to the BLEU score\u2019s significant sensitiveness to target lengths. However, here we\ncehre et al., 2015; Sennrich et al., 2015a), we observe consistent performance boost introduced by language model.\nWe see the benefit from our diverse N-best list by comparing mutual+diversity models with diversity models. On top of the improvements from standard beam search due to reranking, the diversity models introduce additional gains of +0.4, +0.3 and +0.3, leading the total gains roughly up to +2.6, +2.6, +2.1 for different models. The unknown token replacement technique yields significant gains, in line with observations from Jean et al. (2014; Luong et al. (2015a).\nWe compare our English-German system with various others: (1) The end-to-end neural MT system from Jean et al. (2014) using a large vocabulary size. (2) Models from Luong et al. (2015a) that combines different attention models. For the models described in (Jean et al., 2014) and (Lu-\ndo not observe as large performance boost here as in phrase based MT. This is due to the fact that during decoding, target length has already been strictly constrained. As described in 4.1, we only consider candidates of lengths between 0.75 and 1.5 times that of the source.\nong et al., 2015a), we reprint their results from both the single model setting and the ensemble setting, which a set of (usually 8) neural models that differ in random initializations and the order of minibatches are trained, the combination of which jointly contributes in the decoding process. The ensemble procedure is known to result in improved performance (Luong et al., 2015a; Jean et al., 2014; Sutskever et al., 2014).\nNote that the reported results from the standard SEQ2SEQ models and attention models in Table 1 (those without considering mutual information) are from models identical in structure to the corresponding models described in (Luong et al., 2015a), and achieve similar performances (13.2 vs 14.0 for standard SEQ2SEQ models and 20.5 vs 20.7 for attention models). Due to time and computational constraints, we did not implement an ensemble mechanism, making our results incomparable to the ensemble mechanisms in these papers."}, {"heading": "5.4 French-English Results", "text": "Results from the WMT\u201914 French-English datasets are shown in Table 2, along with results reprinted from Sutskever et al. (2014; Luong et al. (2015b). We again observe that applying mutual information yields better performance than the corresponding standard neural MT models.\nRelative to the English-German dataset, the English-French translation task shows a larger gap between our new model and vanilla models where reranking information is not considered; our models respectively yield up to +3.2, +2.6, +2.7 boost in BLEU compared to standard neural models without and with unknown word replacement, and Attention models."}, {"heading": "6 Discussion", "text": "In this paper, we introduce a new objective for neural MT based on the mutual dependency between the source and target sentences, inspired by recent work in neural conversation generation (Li et al., 2015). We build an approximate implementation of our model using reranking, and then to make reranking more powerful we introduce a\nnew decoding method that promotes diversity in the first-pass N-best list. On English\u2192French and English\u2192German translation tasks, we show that the neural machine translation models trained using the proposed method perform better than corresponding standard models, and that both the mutual information objective and the diversity-increasing decoding methods contribute to the performance boost..\nThe new models come with the advantages of easy implementation with sources and targets interchanged, and of offering a general solution that can be integrated into any neural generation models with minor adjustments. Indeed, our diversityenhancing decoder can be applied to generate more diverse N-best lists for any NLP reranking task. Finding a way to introduce mutual information based decoding directly into a first-pass decoder without reranking naturally constitutes our future work."}], "references": [{"title": "Neural machine translation by jointly", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Maximum mutual information estimation of hidden Markov model parameters for speech recognition", "author": ["LR Bahl", "Peter F Brown", "Peter V De Souza", "Robert L Mercer."], "venue": "proc. icassp, volume 86, pages 49\u201352.", "citeRegEx": "Bahl et al\\.,? 1986", "shortCiteRegEx": "Bahl et al\\.", "year": 1986}, {"title": "Diverse m-best solutions in Markov random fields", "author": ["Dhruv Batra", "Payman Yadollahpour", "Abner GuzmanRivera", "Gregory Shakhnarovich."], "venue": "Computer Vision\u2013ECCV 2012, pages 1\u201316. Springer.", "citeRegEx": "Batra et al\\.,? 2012", "shortCiteRegEx": "Batra et al\\.", "year": 2012}, {"title": "Positive diversity tuning for machine translation system combination", "author": ["Daniel Cer", "Christopher D Manning", "Daniel Jurafsky."], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 320\u2013328.", "citeRegEx": "Cer et al\\.,? 2013", "shortCiteRegEx": "Cer et al\\.", "year": 2013}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Traitbased hypothesis selection for machine translation", "author": ["Jacob Devlin", "Spyros Matsoukas."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Devlin and Matsoukas.,? 2012", "shortCiteRegEx": "Devlin and Matsoukas.", "year": 2012}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul."], "venue": "ACL (1), pages 1370\u20131380. Citeseer.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines", "author": ["Jenny Rose Finkel", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Lan-", "citeRegEx": "Finkel et al\\.,? 2006", "shortCiteRegEx": "Finkel et al\\.", "year": 2006}, {"title": "A systematic exploration of diversity in machine translation", "author": ["Kevin Gimpel", "Dhruv Batra", "Chris Dyer", "Gregory Shakhnarovich", "Virginia Tech."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, October.", "citeRegEx": "Gimpel et al\\.,? 2013", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1503.03535.", "citeRegEx": "Gulcehre et al\\.,? 2015", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Forest reranking: Discriminative parsing with non-local features", "author": ["Liang Huang."], "venue": "ACL, pages 586\u2013 594.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.2007.", "citeRegEx": "Jean et al\\.,? 2014", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP, pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Minimum Bayes-risk decoding for statistical machine translation", "author": ["Shankar Kumar", "William Byrne."], "venue": "Technical report, DTIC Document.", "citeRegEx": "Kumar and Byrne.,? 2004", "shortCiteRegEx": "Kumar and Byrne.", "year": 2004}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1510.03055.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "EMNLP.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of ACL.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Lattice-based minimum error rate training for statistical machine translation", "author": ["Wolfgang Macherey", "Franz Josef Och", "Ignacio Thayer", "Jakob Uszkoreit."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Macherey et al\\.,? 2008", "shortCiteRegEx": "Macherey et al\\.", "year": 2008}, {"title": "Encoding source language with convolutional neural network for machine translation", "author": ["Fandong Meng", "Zhengdong Lu", "Mingxuan Wang", "Hang Li", "Wenbin Jiang", "Qun Liu."], "venue": "arXiv preprint arXiv:1503.01838.", "citeRegEx": "Meng et al\\.,? 2015", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of ACL 2002, pages 295\u2013302.", "citeRegEx": "Och and Ney.,? 2002", "shortCiteRegEx": "Och and Ney.", "year": 2002}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160\u2013167. Association for Computational Linguistics.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1511.06709.", "citeRegEx": "Sennrich et al\\.,? 2015a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1507.04808.", "citeRegEx": "Serban et al\\.,? 2015a", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "A survey of available corpora for building data-driven dialogue systems", "author": ["Iulian Vlad Serban", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1512.05742.", "citeRegEx": "Serban et al\\.,? 2015b", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "String-to-dependency statistical machine translation", "author": ["Libin Shen", "Jinxi Xu", "Ralph Weischedel."], "venue": "Computational Linguistics, 36(4):649\u2013671.", "citeRegEx": "Shen et al\\.,? 2010", "shortCiteRegEx": "Shen et al\\.", "year": 2010}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lattice minimum bayes-risk decoding for statistical machine translation", "author": ["Roy W Tromble", "Shankar Kumar", "Franz Och", "Wolfgang Macherey."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 620\u2013629. As-", "citeRegEx": "Tromble et al\\.,? 2008", "shortCiteRegEx": "Tromble et al\\.", "year": 2008}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "arXiv preprint arXiv:1412.7449.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Large scale discriminative training of hidden Markov models for speech recognition", "author": ["P.C. Woodland", "D. Povey."], "venue": "Computer Speech and Language, 16:25\u201347.", "citeRegEx": "Woodland and Povey.,? 2002", "shortCiteRegEx": "Woodland and Povey.", "year": 2002}, {"title": "Bagging and boosting statistical machine translation systems", "author": ["Tong Xiao", "Jingbo Zhu", "Tongran Liu."], "venue": "Artificial Intelligence, 195:496\u2013527.", "citeRegEx": "Xiao et al\\.,? 2013", "shortCiteRegEx": "Xiao et al\\.", "year": 2013}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 29, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 0, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 4, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 13, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 23, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 24, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 9, "context": "Sequence-to-sequence models for machine translation (SEQ2SEQ) (Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sennrich et al., 2015a; Sennrich et al., 2015b; Gulcehre et al., 2015) are of growing interest for their capacity to learn semantic and syntactic relations between sequence pairs, capturing contextual dependencies in a more continuous way than phrase-based SMT approaches.", "startOffset": 62, "endOffset": 230}, {"referenceID": 17, "context": "SEQ2SEQ models require minimal domain knowledge, can be trained end-to-end, have a much smaller memory footprint than the large phrase tables needed for phrase-based SMT, and achieve state-of-the-art performance in large-scale tasks like English to French (Luong et al., 2015b) and English to German (Luong et al.", "startOffset": 256, "endOffset": 277}, {"referenceID": 16, "context": ", 2015b) and English to German (Luong et al., 2015a; Jean et al., 2014) translation.", "startOffset": 31, "endOffset": 71}, {"referenceID": 12, "context": ", 2015b) and English to German (Luong et al., 2015a; Jean et al., 2014) translation.", "startOffset": 31, "endOffset": 71}, {"referenceID": 20, "context": ", p(y|x), they ignore p(x|y), the dependency from the target to the source, which has long been an important feature in phrase-based translation (Och and Ney, 2002; Shen et al., 2010).", "startOffset": 145, "endOffset": 183}, {"referenceID": 27, "context": ", p(y|x), they ignore p(x|y), the dependency from the target to the source, which has long been an important feature in phrase-based translation (Och and Ney, 2002; Shen et al., 2010).", "startOffset": 145, "endOffset": 183}, {"referenceID": 15, "context": "As Li et al. (2015) recently showed in the context of conversational response generation, the MMI based objective function is equivalent to linearly combining p(x|y) and p(y|x).", "startOffset": 3, "endOffset": 20}, {"referenceID": 15, "context": "(2) But as also discussed in Li et al. (2015), direct decoding from (2) is infeasible because computing p(x|y) cannot be done until the target has been ar X iv :1 60 1.", "startOffset": 29, "endOffset": 46}, {"referenceID": 31, "context": "They yield good performance in a variety of NLP generation tasks including conversational response generation (Vinyals and Le, 2015; Serban et al., 2015a; Li et al., 2015), and parsing (Vinyals et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 25, "context": "They yield good performance in a variety of NLP generation tasks including conversational response generation (Vinyals and Le, 2015; Serban et al., 2015a; Li et al., 2015), and parsing (Vinyals et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 15, "context": "They yield good performance in a variety of NLP generation tasks including conversational response generation (Vinyals and Le, 2015; Serban et al., 2015a; Li et al., 2015), and parsing (Vinyals et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 15, "context": "As demonstrated in (Li et al., 2015)", "startOffset": 19, "endOffset": 36}, {"referenceID": 19, "context": "Similar convolutional networks are used in (Meng et al., 2015) for encoding.", "startOffset": 43, "endOffset": 62}, {"referenceID": 11, "context": "Kalchbrenner and Blunsom (2013) used an encoding model akin to convolutional networks for encoding and standard hidden unit recurrent nets for decoding.", "startOffset": 0, "endOffset": 32}, {"referenceID": 11, "context": "Kalchbrenner and Blunsom (2013) used an encoding model akin to convolutional networks for encoding and standard hidden unit recurrent nets for decoding. Similar convolutional networks are used in (Meng et al., 2015) for encoding. Sutskever et al. (2014; Luong et al. (2015a) employed a stacking LSTM model for both encoding and decoding.", "startOffset": 0, "endOffset": 275}, {"referenceID": 0, "context": "Bahdanau et al. (2014), Jean et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bahdanau et al. (2014), Jean et al. (2014) adopted bi-directional recurrent nets for the encoder.", "startOffset": 0, "endOffset": 43}, {"referenceID": 1, "context": "Maximum Mutual Information Maximum Mutual Information (MMI) was introduced in speech recognition (Bahl et al., 1986) as a way of measuring the mutual dependence between inputs (acoustic feature vectors) and outputs (words) and improving discriminative training (Woodland and Povey, 2002).", "startOffset": 97, "endOffset": 116}, {"referenceID": 33, "context": ", 1986) as a way of measuring the mutual dependence between inputs (acoustic feature vectors) and outputs (words) and improving discriminative training (Woodland and Povey, 2002).", "startOffset": 152, "endOffset": 178}, {"referenceID": 28, "context": ", I don\u2019t know) regardless of the inputs (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2015b).", "startOffset": 41, "endOffset": 107}, {"referenceID": 31, "context": ", I don\u2019t know) regardless of the inputs (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2015b).", "startOffset": 41, "endOffset": 107}, {"referenceID": 26, "context": ", I don\u2019t know) regardless of the inputs (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2015b).", "startOffset": 41, "endOffset": 107}, {"referenceID": 1, "context": "Maximum Mutual Information Maximum Mutual Information (MMI) was introduced in speech recognition (Bahl et al., 1986) as a way of measuring the mutual dependence between inputs (acoustic feature vectors) and outputs (words) and improving discriminative training (Woodland and Povey, 2002). Li et al. (2015) show that MMI could solve an important problem in SEQ2SEQ conversational response generation.", "startOffset": 98, "endOffset": 306}, {"referenceID": 1, "context": "Maximum Mutual Information Maximum Mutual Information (MMI) was introduced in speech recognition (Bahl et al., 1986) as a way of measuring the mutual dependence between inputs (acoustic feature vectors) and outputs (words) and improving discriminative training (Woodland and Povey, 2002). Li et al. (2015) show that MMI could solve an important problem in SEQ2SEQ conversational response generation. Prior SEQ2SEQ models tended to generate highly generic, dull responses (e.g., I don\u2019t know) regardless of the inputs (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2015b). Li et al. (2015) show that modeling the mutual dependency between messages and response promotes the diversity of response outputs.", "startOffset": 98, "endOffset": 602}, {"referenceID": 20, "context": "In that sense, our work is designed to incorporate into SEQ2SEQ models features that have proved useful in phrasebased MT, like the reverse translation probability or sentence length (Och and Ney, 2002; Shen et al., 2010; Devlin et al., 2014).", "startOffset": 183, "endOffset": 242}, {"referenceID": 27, "context": "In that sense, our work is designed to incorporate into SEQ2SEQ models features that have proved useful in phrasebased MT, like the reverse translation probability or sentence length (Och and Ney, 2002; Shen et al., 2010; Devlin et al., 2014).", "startOffset": 183, "endOffset": 242}, {"referenceID": 6, "context": "In that sense, our work is designed to incorporate into SEQ2SEQ models features that have proved useful in phrasebased MT, like the reverse translation probability or sentence length (Och and Ney, 2002; Shen et al., 2010; Devlin et al., 2014).", "startOffset": 183, "endOffset": 242}, {"referenceID": 18, "context": "Generating Diverse Translations Various algorithms have been proposed for generated diverse translations in phrase-based MT, including compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 189, "endOffset": 257}, {"referenceID": 30, "context": "Generating Diverse Translations Various algorithms have been proposed for generated diverse translations in phrase-based MT, including compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 189, "endOffset": 257}, {"referenceID": 14, "context": "Generating Diverse Translations Various algorithms have been proposed for generated diverse translations in phrase-based MT, including compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 189, "endOffset": 257}, {"referenceID": 5, "context": ", 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 65, "endOffset": 93}, {"referenceID": 34, "context": ", 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al., 2013), or multiple systems (Cer et al.", "startOffset": 112, "endOffset": 131}, {"referenceID": 3, "context": ", 2013), or multiple systems (Cer et al., 2013).", "startOffset": 29, "endOffset": 47}, {"referenceID": 2, "context": "(2013; Batra et al. (2012), produce diverse N-best lists by adding a dissimilarity function based on N-gram overlaps, distancing the current translation from already-generated ones by choosing translations that have higher scores but distinct from previous ones.", "startOffset": 7, "endOffset": 27}, {"referenceID": 10, "context": "A long-short term memory model (Hochreiter and Schmidhuber, 1997) associates each time step with an input gate, a memory gate and an output gate, denoted respectively as it, ft and ot.", "startOffset": 31, "endOffset": 65}, {"referenceID": 35, "context": "Attention models adopt a look-back strategy that links the current decoding stage with input time steps to represent which portions of the input are most responsible for the current decoding state (Xu et al., 2015; Luong et al., 2015b; Bahdanau et al., 2014).", "startOffset": 197, "endOffset": 258}, {"referenceID": 17, "context": "Attention models adopt a look-back strategy that links the current decoding stage with input time steps to represent which portions of the input are most responsible for the current decoding state (Xu et al., 2015; Luong et al., 2015b; Bahdanau et al., 2014).", "startOffset": 197, "endOffset": 258}, {"referenceID": 0, "context": "Attention models adopt a look-back strategy that links the current decoding stage with input time steps to represent which portions of the input are most responsible for the current decoding state (Xu et al., 2015; Luong et al., 2015b; Bahdanau et al., 2014).", "startOffset": 197, "endOffset": 258}, {"referenceID": 0, "context": ", 2015b; Bahdanau et al., 2014). Let H = {\u01251, \u01252, ..., \u0125Nx} be the collection of hidden vectors outputted from LSTMs during encoding. Each element in H contains information about the input sequences, focusing on the parts surrounding each specific token. Let ht\u22121 be the LSTM outputs for decoding at time t \u2212 1. Attention models link the current-step decoding information, i.e., ht with each of the representations at decoding step \u0125t\u2032 using a weight variable at. at can be constructed from different scoring functions such as the dot product between the two vectors, i.e., ht\u22121 \u00b7 \u0125t, a general model akin to tensor operation i.e., ht\u22121 \u00b7 W \u00b7 \u0125t, and the concatenation model by concatenating the two vectors i.e., UT \u00b7tanh(W \u00b7 [ht\u22121, \u0125t]). The behavior of different attention scoring functions have been extensively studied in Luong et al. (2015a). For all experiments in this paper, we adopt the general strategy where the relevance score between the current step of the decoding representation and the encoding representation is given by:", "startOffset": 9, "endOffset": 848}, {"referenceID": 16, "context": "Luong et al. (2015a) reported a significant performance boost by integrating ~ht\u22121 into the next step LSTM hidden state computation (referred to as the input-feeding model), making LSTM compositions in decoding as follows:", "startOffset": 0, "endOffset": 21}, {"referenceID": 17, "context": "Recent work has proposed to deal with this issue: (Luong et al., 2015b) adopt a post-processing strategy based on aligner from IBM models, while (Jean et al.", "startOffset": 50, "endOffset": 71}, {"referenceID": 12, "context": ", 2015b) adopt a post-processing strategy based on aligner from IBM models, while (Jean et al., 2014) approximates softmax functions by selecting a small subset of target vocabulary.", "startOffset": 82, "endOffset": 101}, {"referenceID": 12, "context": "In this paper, we use a strategy similar to that of Jean et al. (2014), thus avoiding the reliance on external IBM model word aligner.", "startOffset": 52, "endOffset": 71}, {"referenceID": 15, "context": "As discussed in Li et al. (2015), direct decoding from (2) is infeasible since the second part, p(x|y), requires completely generating the target before it can be computed.", "startOffset": 16, "endOffset": 33}, {"referenceID": 7, "context": "Unfortunately, the N-best lists outputted from standard beam search are a poor surrogate for the entire search space (Finkel et al., 2006; Huang, 2008).", "startOffset": 117, "endOffset": 151}, {"referenceID": 11, "context": "Unfortunately, the N-best lists outputted from standard beam search are a poor surrogate for the entire search space (Finkel et al., 2006; Huang, 2008).", "startOffset": 117, "endOffset": 151}, {"referenceID": 15, "context": "We employ the diversity evaluation metrics in (Li et al., 2015) to evaluate the degree of diversity of the N-best lists: calculating the average number of distinct unigrams distinct-1 and bigrams distinct2 in the N-best list given each source sentence, scaled by the total number of tokens.", "startOffset": 46, "endOffset": 63}, {"referenceID": 9, "context": "In neural MT literature, recent progress has demonstrated the effectiveness of modeling reranking with language model (Gulcehre et al., 2015).", "startOffset": 118, "endOffset": 141}, {"referenceID": 21, "context": "We optimize \u03b7, \u03bb and \u03b3 using MERT (Och, 2003) BLEU score (Papineni et al.", "startOffset": 34, "endOffset": 45}, {"referenceID": 22, "context": "We optimize \u03b7, \u03bb and \u03b3 using MERT (Och, 2003) BLEU score (Papineni et al., 2002) on the development set.", "startOffset": 57, "endOffset": 80}, {"referenceID": 22, "context": "For the English-German and English-German translation, we use newstest2013 (3000 sentence pairs) as the development set and translation performances are reported in BLEU (Papineni et al., 2002) on newstest2014 (2737) sentences.", "startOffset": 170, "endOffset": 193}, {"referenceID": 22, "context": "For the English-German and English-German translation, we use newstest2013 (3000 sentence pairs) as the development set and translation performances are reported in BLEU (Papineni et al., 2002) on newstest2014 (2737) sentences.", "startOffset": 171, "endOffset": 217}, {"referenceID": 29, "context": "We trained p(y|x) following the standard training protocols described in (Sutskever et al., 2014).", "startOffset": 73, "endOffset": 97}, {"referenceID": 16, "context": "As in (Luong et al., 2015a).", "startOffset": 6, "endOffset": 27}, {"referenceID": 16, "context": "As in (Luong et al., 2015a). All texts are tokenized with tokenizer.perl and BLEU scores are computed with multibleu.perl ing, each of which consists of a different set of parameters. We followed the detailed protocols from Luong et al. (2015a): each LSTM layer consists of 1,000 hidden neurons, and the dimensionality of word embeddings is set to 1,000.", "startOffset": 7, "endOffset": 245}, {"referenceID": 9, "context": "Following (Gulcehre et al., 2015; Sennrich et al., 2015a), we remove sentences with more than 10% Unknown words based on the vocabulary constructed using parallel datasets.", "startOffset": 10, "endOffset": 57}, {"referenceID": 23, "context": "Following (Gulcehre et al., 2015; Sennrich et al., 2015a), we remove sentences with more than 10% Unknown words based on the vocabulary constructed using parallel datasets.", "startOffset": 10, "endOffset": 57}, {"referenceID": 12, "context": "Baselines performances are reprinted from Jean et al. (2014), Luong et al.", "startOffset": 42, "endOffset": 61}, {"referenceID": 12, "context": "The unknown token replacement technique yields significant gains, in line with observations from Jean et al. (2014; Luong et al. (2015a).", "startOffset": 97, "endOffset": 137}, {"referenceID": 12, "context": "For the models described in (Jean et al., 2014) and (Lu-", "startOffset": 28, "endOffset": 47}, {"referenceID": 12, "context": "We compare our English-German system with various others: (1) The end-to-end neural MT system from Jean et al. (2014) using a large vocabulary size.", "startOffset": 99, "endOffset": 118}, {"referenceID": 12, "context": "We compare our English-German system with various others: (1) The end-to-end neural MT system from Jean et al. (2014) using a large vocabulary size. (2) Models from Luong et al. (2015a) that combines different attention models.", "startOffset": 99, "endOffset": 186}, {"referenceID": 16, "context": "The ensemble procedure is known to result in improved performance (Luong et al., 2015a; Jean et al., 2014; Sutskever et al., 2014).", "startOffset": 66, "endOffset": 130}, {"referenceID": 12, "context": "The ensemble procedure is known to result in improved performance (Luong et al., 2015a; Jean et al., 2014; Sutskever et al., 2014).", "startOffset": 66, "endOffset": 130}, {"referenceID": 29, "context": "The ensemble procedure is known to result in improved performance (Luong et al., 2015a; Jean et al., 2014; Sutskever et al., 2014).", "startOffset": 66, "endOffset": 130}, {"referenceID": 16, "context": "Note that the reported results from the standard SEQ2SEQ models and attention models in Table 1 (those without considering mutual information) are from models identical in structure to the corresponding models described in (Luong et al., 2015a), and achieve similar performances (13.", "startOffset": 223, "endOffset": 244}, {"referenceID": 27, "context": "Google is the LSTM-based model proposed in Sutskever et al. (2014). Luong et al.", "startOffset": 43, "endOffset": 67}, {"referenceID": 16, "context": "Luong et al. (2015) is the extension of Google models with unknown token replacements.", "startOffset": 0, "endOffset": 20}, {"referenceID": 16, "context": "(2014; Luong et al. (2015b). We again observe that applying mutual information yields better performance than the corresponding standard neural MT models.", "startOffset": 7, "endOffset": 28}, {"referenceID": 15, "context": "In this paper, we introduce a new objective for neural MT based on the mutual dependency between the source and target sentences, inspired by recent work in neural conversation generation (Li et al., 2015).", "startOffset": 188, "endOffset": 205}], "year": 2016, "abstractText": "Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., p(y|x), an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, the proposed models offers a consistent performance boost on both standard LSTM and attention-based neural MT architectures.", "creator": "TeX"}}}