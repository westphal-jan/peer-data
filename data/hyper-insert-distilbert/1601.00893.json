{"id": "1601.00893", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jan-2016", "title": "The Role of Context Types and Dimensionality in Learning Word Embeddings", "abstract": "we systematically provide forward the first extensive evaluation of how using markedly different functional types of context variables to hopefully learn skip - gram word understanding embeddings affects performance on a particularly wide acceptable range of intrinsic and extrinsic nlp measured tasks. our results suggest observations that today while dynamic intrinsic measurement tasks tend to exhibit a notably clear empirical preference to particular relevant types familiar of model contexts and preserve higher dimensionality, more careful behavioral tuning is required for manually finding yet the optimal settings for exploring most of today the extrinsic tasks encountered that we considered. furthermore,. for knowing these extrinsic tasks, we find that once learns the people benefit from systematically increasing the embedding improved dimensionality sensitivity is mostly exhausted, simple concatenation approaches of linear word embeddings, learned with different context types, similarly can yield further performance theoretical gains. as on an additional contribution, we first propose a new cognitive variant critique of the skip - gram cognitive model synthesis that learns structured word language embeddings specifically from context weighted parameter contexts or of socially substitute words.", "histories": [["v1", "Tue, 5 Jan 2016 16:28:42 GMT  (1093kb,D)", "https://arxiv.org/abs/1601.00893v1", null], ["v2", "Wed, 19 Jul 2017 15:32:54 GMT  (1121kb,D)", "http://arxiv.org/abs/1601.00893v2", "Accepted to NAACL 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["oren melamud", "david mcclosky", "siddharth patwardhan", "mohit bansal"], "accepted": true, "id": "1601.00893"}, "pdf": {"name": "1601.00893.pdf", "metadata": {"source": "CRF", "title": "The Role of Context Types and Dimensionality in Learning Word Embeddings", "authors": ["Oren Melamud", "David McClosky", "Siddharth Patwardhan", "Mohit Bansal"], "emails": ["melamuo@cs.biu.ac.il", "dmcc@google.com", "siddharth@us.ibm.com", "mbansal@ttic.edu"], "sections": [{"heading": null, "text": "We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words."}, {"heading": "1 Introduction", "text": "Word embeddings have become increasingly popular lately, proving to be valuable as a source of features in a broad range of NLP tasks with limited supervision (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013; Bansal et al., 2014). word2vec1 skip-gram (Mikolov et al., 2013a)\n\u2217Majority of work performed while at IBM Watson. 1http://code.google.com/p/word2vec/\nand GloVe2 (Pennington et al., 2014) are among the most widely used word embedding models today. Their success is largely due to an efficient and user-friendly implementation that learns highquality word embeddings from very large corpora.\nBoth word2vec and GloVe learn lowdimensional continuous vector representations for words by considering window-based contexts, i.e., context words within some fixed distance of each side of the target word. However, the underlying models are equally applicable to different choices of context types. For example, Bansal et al. (2014) and Levy and Goldberg (2014) showed that using syntactic contexts rather than window contexts in word2vec captures functional similarity (as in lion:cat) rather than topical similarity or relatedness (as in lion:zoo). Further, Bansal et al. (2014) and Melamud et al. (2015b) showed the benefits of such modified-context embeddings in dependency parsing and lexical substitution tasks. However, to the best of our knowledge, there has not been an extensive evaluation of the effect of multiple, diverse context types on a wide range of NLP tasks.\nWord embeddings are typically evaluated on intrinsic and extrinsic tasks. Intrinsic tasks mostly include predicting human judgments of semantic relations between words, e.g., as in WordSim-353 (Finkelstein et al., 2001), while extrinsic tasks include various \u2018real\u2019 downstream NLP tasks, such as coreference resolution and sentiment analysis. Re-\n2http://nlp.stanford.edu/projects/glove/\nar X\niv :1\n60 1.\n00 89\n3v 2\n[ cs\n.C L\n] 1\n9 Ju\ncent works have shown that while intrinsic evaluations are easier to perform, their correlation with results on extrinsic evaluations is not very reliable (Schnabel et al., 2015; Tsvetkov et al., 2015), stressing the importance of the latter.\nIn this work, we provide the first extensive evaluation of word embeddings learned with different types of context, on a wide range of intrinsic similarity and relatedness tasks, and extrinsic NLP tasks, namely dependency parsing, named entity recognition, coreference resolution, and sentiment analysis. We employ contexts based of different word window sizes, syntactic dependencies, and a lesserknown substitute words approach (Yatbaz et al., 2012). Finally, we experiment with combinations of the above word embeddings, comparing two approaches: (1) simple vector concatenation that offers a wider variety of features for a classifier to choose and learn weighted combinations from, and (2) dimensionality reduction via either Singular Value Decomposition or Canonical Correlation Analysis, which tries to find a smaller subset of features.\nOur results suggest that it is worthwhile to carefully choose the right type of word embeddings for an extrinsic NLP task, rather than rely on intrinsic benchmark results. Specifically, picking the optimal context type and dimensionality is critical. Furthermore, once the benefit from increasing the embedding dimensionality is mostly exhausted, concatenation of word embeddings learned with different context types can yield further performance gains."}, {"heading": "2 Word Embedding Context Types", "text": ""}, {"heading": "2.1 Learning Corpus", "text": "We use a fixed learning corpus for a fair comparison of all embedding types: a concatenation of three large English corpora: (1) English Wikipedia 2015, (2) UMBC web corpus (Han et al., 2013), and (3) English Gigaword (LDC2011T07) newswire corpus (Parker et al., 2011). Our concatenated corpus is diverse and substantial in size with approximately 10B words. This allows us to learn high quality embeddings that cover a large vocabulary. After extracting clean text from these corpora, we used Stanford CoreNLP (Manning et al., 2014) for sentence splitting, tokenization, part-of-speech tagging and\ndependency parsing.3 Then, all tokens were lowercased, and sentences were shuffled to prevent structured bias. When learning word embeddings, we ignored words with corpus frequency lower than 100, yielding a vocabulary of about 500K words.4"}, {"heading": "2.2 Window-based Word Embeddings", "text": "We used word2vec\u2019s skip-gram model with negative sampling (Mikolov et al., 2013b) to learn window-based word embeddings. 5 This popular method embeds both target words and contexts in the same low-dimensional space, where the embeddings of a target and context are pushed closer together the more frequently they co-occur in a learning corpus. Indirectly, this also results in similar embeddings for target words that co-occur with similar contexts. More formally, this method optimizes the following objective function:\n(1)L = \u2211\n(t,c)\u2208PAIRS\nLt,c\n(2)Lt,c = log \u03c3(v\u2032c \u00b7 vt) + \u2211\nneg\u2208NEGS (t,c)\nlog \u03c3(\u2212v\u2032neg \u00b7 vt)\nwhere vt and v\u2032c are the vector representations of target word t and context word c. PAIRS is the set of window-based co-occurring target-context pairs considered by the model that depends on the window size, and NEGS (t,c) is a set of randomly sampled context words used with the pair (t, c).6\nWe experimented with window sizes of 1, 5, and 10, and various dimensionalities. We denote a window-based word embedding with window size of n and dimensionality of m with Wnm. For example, W5300 is a word embedding learned using a window size of 5 and dimensionality of 300."}, {"heading": "2.3 Dependency-based Word Embeddings", "text": "We used word2vecf7 (Levy and Goldberg, 2014), to learn dependency-based word embeddings from\n3Parses follow the Universal Dependencies formalism and were produced by Stanford CoreNLP, version 3.5.2\n4Our word embeddings are available at: www.cs.biu.ac. il/nlp/resources/downloads/embeddings-contexts/\n5We used negative sampling = 5 and iterations = 3 in all of the experiments described in this paper.\n6For more details refer to Mikolov et al. (2013b). 7http://bitbucket.org/yoavgo/word2vecf\nthe parsed version of our corpus, similar to the approach of Bansal et al. (2014). word2vecf accepts as its input arbitrary target-context pairs. In the case of dependency-based word embeddings, the context elements are the syntactic contexts of the target word, rather than the words in a window around it. Specifically, following Levy and Goldberg (2014), we first \u2018collapsed\u2019 prepositions (as implemented in word2vecf). Then, for a target word t with modifiers m1,...,mk and head h, we paired the target word with the context elements (m1, r1),...,(mk, rk),(h, r\u22121h ), where r is the type of the dependency relation between the head and the modifier (e.g., dobj, prep of ) and r\u22121 denotes an inverse relation. We denote a dependency-based word embedding with dimensionality of m by DEPm. We note that under this setting word2vecf optimizes the same objective function described in Equation (1), with PAIRS now comprising dependencybased pairs instead of window-based ones."}, {"heading": "2.4 Substitute-based Word Embeddings", "text": "Substitute vectors are a recent approach to representing contexts of target words, proposed in Yatbaz et al. (2012). Instead of the neighboring words themselves, a substitute vector includes the potential filler words for the target word slot, weighted according to how \u2018fit\u2019 they are to fill the target slot given the neighboring words. For example, the substitute vector representing the context of the word love in \u201cI love my job\u201d, could look like: [quit 0.5, love 0.3, hate 0.1, lost 0.1]. Substitute-based contexts are generated using a language model and were successfully used in distributional semantics models for part-of-speech induction (Yatbaz et al., 2012), word sense induction (Baskaya et al., 2013), functional semantic similarity (Melamud et al., 2014) and lexical substitution tasks (Melamud et al., 2015a).\nSimilar to Yatbaz et al. (2012), we consider the words in a substitute vector, as a weighted set of contexts \u2018co-occurring\u2019 with the observed target word. For example, the above substitute vector is considered as the following set of weighted target-context pairs: {(love, quit, 0.5), (love, love, 0.3), (love, hate, 0.1), (love, lost, 0.1)}. To learn word embeddings from such weighted target-context pairs, we extended word2vecf by modifying the objective\nfunction in Equation (1) as follows: (3)L = \u2211\n(t,c)\u2208PAIRS\n\u03b1t,c \u00b7 Lt,c\nwhere \u03b1t,c is the weight of the target-context pair (t, c). With this simple modification, the effect of target-context pairs on the learned word representations becomes proportional to their weights.\nTo generate the substitute vectors we followed the methodology in (Yatbaz et al., 2012; Melamud et al., 2015a). We learned a 4-gram KneserNey language model from our learning corpus using KenLM (Heafield et al., 2013). Then, we used FASTSUBS (Yuret, 2012) with this language model to efficiently generate substitute vectors, where the weight of each substitute s is the conditional probability p(s|C) for this substitute to fill the target slot given the sentential context C. For efficiency, we pruned the substitute vectors to their top-10 substitutes, s1..s10, and normalized their probabilities such that \u2211 i=1..10 p(si|C) = 1. We also generated only up to 20,000 substitute vectors for each target word type. Finally, we converted each substitute vector into weighted target-substitute pairs and used our extended version of word2vecf to learn the substitute-based word embeddings, denoted SUBm."}, {"heading": "2.5 Qualitative Effect of Context Type", "text": "To motivate the rest of our work, we first qualitatively inspect the top most-similar words to some target words, using cosine similarity of their respective embeddings. As illustrated in Table 1, in embeddings learned with large window contexts, we see both functionally similar words and topically similar words, sometimes with a different part-of-speech. With small windows and dependency contexts, we generally see much fewer topically similar words, which is consistent with previous findings (Bansal et\nal., 2014; Levy and Goldberg, 2014). Finally, with substitute-based contexts, there appears to be even a stronger preference for functional similarity, with a tendency to also strictly preserve verb tense."}, {"heading": "3 Word Embedding Combinations", "text": "As different choices of context type yield word embeddings with different properties, we hypothesize that combinations of such embeddings could be more informative for some extrinsic tasks. We experimented with two alternative approaches to combine different sets of word embeddings: (1) Simple vector concatenation, which is a lossless combination that comes at the cost of increased dimensionality, and (2) SVD and CCA, which are lossy combinations that attempt to capture the most useful information from the different embeddings sets with lower dimensionality. The methods used are described in more detail next."}, {"heading": "3.1 Concatenation", "text": "Perhaps the simplest way to combine two different sets of word embeddings (sharing the same vocabulary) is to concatenate their word vectors for every word type. We denote such a combination of word embedding set A with word embedding set B using the symbol (+). For example W10+DEP600 is the concatenation of W10300 with DEP300. Naturally, the dimensionality of the concatenated embeddings is the sum of the dimensionalities of the component embeddings. In our experiments, we only ever combine word embeddings of equal dimensionality.\nThe motivation behind concatenation relates primarily to supervised models in extrinsic tasks. In such settings, we hypothesize that using concatenated word embeddings as input features to a classifier could let it choose and combine (i.e., via learned weights) the most suitable features for the task. Consider a situation where the concatenated embedding W10+DEP600 is used to represent the word inputs to a named entity recognition classifier. In this case, the classifier could choose, for instance, to represent entity words mostly with dependency-based embedding features (reflecting functional semantics), and surrounding words with large window-based embedding features (reflecting topical semantics)."}, {"heading": "3.2 Singular Value Decomposition", "text": "Singular Value Decomposition (SVD) has been shown to be effective in compressing sparse word representations (Levy et al., 2015). In this work, we use this technique in the same way to reduce the dimensionality of concatenated word embeddings."}, {"heading": "3.3 Canonical Correlation Analysis", "text": "Recent work used Canonical Correlation Analysis (CCA) to derive an improved set of word embeddings. The main idea is that two distinct sets of word embeddings, learned with different types of input data, are considered as multi-views of the same vocabulary. Then, CCA is used to project each onto a lower dimensional space, where correlation between the two is maximized. The correlated information is presumably more reliable. Dhillon et al. (2011) considered their two CCA views as embeddings learned from the left and from the right context of the target words, showing improvements on chunking and named entity recognition. Faruqui and Dyer (2014) and Lu et al. (2015) considered multilingual views, showing improvements in several intrinsic tasks, such as word and phrase similarity.\nInspired by this prior work, we consider pairs of word embedding sets, learned with different types of context, as different views and correlate them using linear CCA.8 We use either the SimLex999 or WordSim-353-R intrinsic benchmark (section 4.1) to tune the CCA hyperparameters9 with the Spearmint Bayesian optimization tool10 (Snoek et al., 2012). This results in different projections for each of these tuning objectives, where SimLex999/WordSim-353-R is expected to give some bias towards functional/topical similarity, respectively."}, {"heading": "4 Evaluation", "text": ""}, {"heading": "4.1 Intrinsic Benchmarks", "text": "We employ several commonly used intrinsic benchmarks for assessing how well word embeddings mimic human judgements of semantic similarity of words. The popular WordSim-353 dataset (Finkelstein et al., 2001) includes 353 word pairs manually\n8See Faruqui and Dyer (2014), Lu et al. (2015) for details. 9These are projection dimensionality and regularization.\n10github.com/JasperSnoek/spearmint\nannotated with a degree of similarity. For example, computer:keyboard is annotated with 7.62, indicating a relatively high degree of similarity. While WordSim-353 does not make a distinction between different \u2018flavors\u2019 of similarity, Agirre et al. (2009) proposed two subsets of this dataset, WordSim-353S and WordSim-353-R, which focus on functional and topical similarities, respectively. SimLex-999 (Hill et al., 2014) is a larger word pair similarity dataset with 999 annotated pairs, purposely built to focus on functional similarity. We evaluate our embeddings on these datasets by computing a score for each pair as the cosine similarity of two word vectors. The Spearman\u2019s correlation11 between the ranking of word pairs induced from the human annotations and that from the embeddings is reported.\nThe TOEFL task contains 80 synonym selection items, where a synonym of a target word is to be selected out of four possible choices. We report the overall accuracy of a system that uses cosine distance between the embeddings of the target word and each of the choices to select the one most similar to the target word as the answer."}, {"heading": "4.2 Extrinsic Benchmarks", "text": "The following four diverse downstream NLP tasks serve as our extrinsic benchmarks.12\n1) Dependency Parsing (PARSE) The Stanford Neural Network Dependency (NNDEP) parser (Chen and Manning, 2014) uses dense continuous representations of words, parts-of-speech and dependency labels. While it can learn these representations entirely during the training on labeled data, Chen and Manning (2014) show that initialization with word embeddings, which were pre-trained on unlabeled data, yields improved performance. Hence, we used our different types of embeddings to initialize the NNDEP parser and compared their performance on a standard Penn Treebank benchmark. We used WSJ sections 2\u201321 for training and 22 for development. We used predicted tags produced via 20-fold jackknifing on sections 2\u201321 with the Stanford CoreNLP tagger.\n11We used spearmanr, SciPy version 0.15.1. 12Since our goal is to explore performance trends, we mostly\nexperimented with the tasks\u2019 development sets.\n2) Named Entity Recognition (NER) We used the NER system of Turian et al. (2010), which allows adding word embedding features (on top of various other features) to a regularized averaged perceptron classifier, and achieves near state-of-the-art results using several off-the-shelf word representations. We varied the type of word embeddings used as features when training the NER model, to evaluate their effect on NER benchmarks results. Following Turian et al. (2010), we used the CoNLL-2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003) with 204K/51K train/dev words, as our main benchmark. We also performed an out-of-domain evaluation, using CoNLL-2003 as the train set and the MUC7 formal run (59K words) as the test set.13\n3) Coreference Resolution (COREF) We used the Berkeley Coreference System (Durrett and Klein, 2013), which achieves near state-of-the-art results with a log-linear supervised model. Most of the features in this model are associated with pairs of current and antecedent reference mentions, for which a coreference decision needs to be made. To evaluate the contribution of different word embedding types to this model, we extended it to support the following additional features: {ai}i=1..m, {ci}i=1..m and {ai \u00b7 ci}i=1..m, where ai or ci is the value of the ith dimension in a word embedding vector representing the antecedent or current mention, respectively. We considered two different word embedding representations for a mention: (1) the embedding of the head word of the mention and (2) the average embedding of all words in the mention. The features of both types of representations were presented to the learning model as inputs at the same time. They were added on top of Berkeley\u2019s full feature list (\u2018FINAL\u2019) as described in Durrett and Klein (2013). We evaluated our features on the CoNLL-2012 coreference shared task (Pradhan et al., 2012).\n4) Sentiment Analysis (SENTI) Following Faruqui et al. (2014), we used a sentence-level binary decision version of the sentiment analysis task from Socher et al. (2013). In this setting, neutral sentences were discarded and all remaining sentences were labeled coarsely as positive or negative. Maintaining the original split into train/dev\n13See Turian et al. (2010) for more details on this setting.\nresults, we get a dataset containing 6920/872 sentences. To evaluate different types of word embeddings, we represented each sentence as an average of its word embeddings and then used an L2-regularized logistic regression classifier trained on these features to predict the sentiment labels."}, {"heading": "5 Results", "text": ""}, {"heading": "5.1 Intrinsic Results for Context Types", "text": "The results on the intrinsic tasks are illustrated in Figure 1. First, we see that the performance on all tasks generally increases with the number of dimensions, reaching near-optimal performance at around 300 dimensions, for all types of contexts. This is in line with similar observations on skip-gram word embeddings (Mikolov et al., 2013a).\nLooking further, we observe that there are significant differences in the results when using different types of contexts. The effect of context\nchoice is perhaps most evident in the WordSim-353R task, which captures topical similarity. As might be expected, in this benchmark, the largest-window word embeddings perform best. The performance decreases with the decrease in window size and then reaches significantly lower levels for dependency (DEP) and substitute-based (SUB) embeddings. Conversely, in WordSim-353-S and SimLex999, both of which capture a more functional similarity, the DEP embeddings are the ones that perform best, strengthening similar observations in Levy and Goldberg (2014). Finally, in the TOEFL benchmark, all contexts except for SUB, perform comparably."}, {"heading": "5.2 Extrinsic Results for Context Types", "text": "The extrinsic tasks results are illustrated in Figure 2. A first observation is that optimal extrinsic results may be reached with as few as 50 dimensions. Furthermore, performance may even degrade when us-\ning too many dimensions, as is most evident in the NER task. This behavior presumably depends on various factors, such as the size of the labeled training data or the type of classifier used, and highlights the importance of tuning the dimensionality of word embeddings in extrinsic tasks. This is in contrast to intrinsic tasks, where higher dimensionality typically yields better results.\nNext, comparing the results of different types of contexts, we see, as might be expected, that dependency embeddings work best in the PARSE task. More generally, embeddings that do well in functional similarity intrinsic benchmarks and badly in topical ones (DEP, SUB and W1) work best for PARSE, while large window contexts perform worst, similar to observations in Bansal et al. (2014).\nIn the rest of the tasks it\u2019s difficult to say which context works best for what. One possible expla-\nnation to this in the case of NER and COREF is that the embedding features are used as add-ons to an already competitive learning system. Therefore, the total improvement on top of a \u2018no embedding\u2019 baseline is relatively small, leaving little room for significant differences between different contexts.\nWe did find a more notable contribution of word\nembedding features to the overall system performance in the out-of-domain NER MUC evaluation, described in Table 2. In this out-of-domain setting, all types of contexts achieve at least five points improvement over the baseline. Presumably, this is because continuous word embedding features are more robust to differences between train and test data, such as the typical vocabulary used. However, a detailed investigation of out-of-domain settings is out of scope for this paper and left for future work."}, {"heading": "5.3 Extrinsic Results for Combinations", "text": "A comparison of the results obtained on the extrinsic tasks using the word embedding concatenations (concats), described in section 3.1, versus the original single context word embeddings (singles), appears in Table 3. To control for dimensionality, concats are always compared against sin-\ngles with identical dimensionality. For example, the 200-dimensional concat W10+DEP200, which is a concatenation of W10100 and DEP100, is compared against 200-dimensional singles, such as W10200.\nLooking at the results, it seems like the benefit from concatenation depends on the dimensionality and task at hand, as also illustrated in Figure 3. Given task X and dimensionality d, if d2 is in the range where increasing the dimensionality yields significant improvement on task X , then it\u2019s better to simply increase dimensionality of singles from d2 to d rather than concatenate. The most evident example for this are the results on the SENTI task with d = 50. In this case, the benefit from concatenating two 25-dimensional singles is notably lower than that of using a single 50-dimensional word embedding. On the other hand, if d2 is in the range where near-optimal performance is reached on taskX , then concatenation seems to pay off. This can be seen in SENTI with d = 600, PARSE with d = 200, and NER with d = 50. More concretely, looking at the best performing concatenations, it seems like combinations of the topical W10 embedding with one of the more functional ones, SUB, DEP or W1, typically perform best, suggesting that there is added value in combining embeddings of different nature.\nFinally, our experiments with the methods using SVD (section 3.2) and CCA (section 3.3) yielded degraded performance compared to single word embeddings for all extrinsic tasks and therefore are not reported for brevity. These results seem to further strengthen the hypothesis that the information captured with varied types of context is different and complementary, and therefore it is beneficial to preserve these differences as in our concatenation approach."}, {"heading": "6 Related Work", "text": "There are a number of recent works whose goal is a broad evaluation of the performance of different word embeddings on a range of tasks. However, to the best of our knowledge, none of them focus on embeddings learned with diverse context types as we do. Levy et al. (2015), Lapesa and Evert (2014), and Lai et al. (2015) evaluate several design choices when learning word representations. However, Levy et al. (2015) and Lapesa and Evert (2014)\nperform only intrinsic evaluations and restrict context representation to word windows, while Lai et al. (2015) do perform extrinsic evaluations, but restrict their context representation to a word window with the default size of 5. Schnabel et al. (2015) and Tsvetkov et al. (2015) report low correlation between intrinsic and extrinsic results with different word embeddings (they did not evaluate different context types), which is consistent with differences we found between intrinsic and extrinsic performance patterns in all tasks, except parsing. Bansal et al. (2014) show that functional (dependency-based and small-window) embeddings yield higher parsing improvements than topical (large-window) embeddings, which is consistent with our findings.\nSeveral works focus on particular types of contexts for learning word embeddings. Cirik and Yuret (2014) investigates S-CODE word embeddings based on substitute word contexts. Ling et al. (2015b) and Ling et al. (2015a) propose extensions to the standard window-based context modeling. Alternatively, another recent popular line of work (Faruqui et al., 2014; Kiela et al., 2015) attempts to improve word embeddings by using manuallyconstructed resources, such as WordNet. These techniques could be complementary to our work. Finally, Yin and Schu\u0308tze (2015) and Goikoetxea et al. (2016) propose word embeddings combinations, using methods such as concatenation and CCA, but\nevaluate mostly on intrinsic tasks and do not consider different types of contexts."}, {"heading": "7 Conclusions", "text": "In this paper we evaluated skip-gram word embeddings on multiple intrinsic and extrinsic NLP tasks, varying dimensionality and type of context. We show that while the best practices for setting skipgram hyperparameters typically yield good results on intrinsic tasks, success on extrinsic tasks requires more careful thought. Specifically, we suggest that picking the optimal dimensionality and context type are critical for obtaining the best accuracy on extrinsic tasks and are typically task-specific. Further improvements can often be achieved by combining complementary word embeddings of different context types with the right dimensionality."}, {"heading": "Acknowledgments", "text": "We thank Do Kook Choe for providing us the jackknifed version of WSJ. We also wish to thank the IBM Watson team for helpful discussions and our anonymous reviewers for their comments. This work was partially supported by the Israel Science Foundation grant 880/12 and the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1)."}], "references": [{"title": "A study on similarity and relatedness using distributional and WordNet-based approaches", "author": ["Agirre et al.2009] Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Ai-ku: Using substitute vectors and co-occurrence modeling for word sense induction and disambiguation", "author": ["Enis Sert", "Volkan Cirik", "Deniz Yuret"], "venue": "In Proceedings of the SemEval", "citeRegEx": "Baskaya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baskaya et al\\.", "year": 2013}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Substitute based scode word embeddings in supervised nlp tasks. arXiv preprint arXiv:1407.6853", "author": ["Cirik", "Yuret2014] Volkan Cirik", "Deniz Yuret"], "venue": null, "citeRegEx": "Cirik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cirik et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Multi-view learning of word embeddings via cca", "author": ["Dean P Foster", "Lyle H Ungar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Easy victories and uphill battles in coreference resolution", "author": ["Durrett", "Klein2013] Greg Durrett", "Dan Klein"], "venue": "In Proc. of EMNLP", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "Improving vector space word representations using multilingual correlation", "author": ["Faruqui", "Dyer2014] Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of EACL", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith"], "venue": "In Proceedings of Deep Learning and Representation Learning Workshop,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Single or multiple? combining word representations independently learned from text and wordnet", "author": ["Eneko Agirre", "Aitor Soroa"], "venue": "In Proceedings of AAAI", "citeRegEx": "Goikoetxea et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goikoetxea et al\\.", "year": 2016}, {"title": "UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems", "author": ["Han et al.2013] Lushan Han", "Abhay L. Kashyap", "Tim Finin", "James Mayfield", "Johnathan Weese"], "venue": "In Proceedings of the Second Joint Conference on Lexical and Computational Semantics", "citeRegEx": "Han et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn"], "venue": "In Proceedings of ACL", "citeRegEx": "Heafield et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "arXiv preprint arXiv:1408.3456", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Specializing word embeddings for similarity or relatedness", "author": ["Kiela et al.2015] Douwe Kiela", "Felix Hill", "Stephen Clark"], "venue": null, "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "How to generate a good word embedding? arXiv preprint arXiv:1507.05523", "author": ["Lai et al.2015] Siwei Lai", "Kang Liu", "Liheng Xu", "Jun Zhao"], "venue": null, "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "A large scale evaluation of distributional semantic models: Parameters, interactions and model selection. Transactions of the Association for Computational Linguistics, 2:531\u2013545", "author": ["Lapesa", "Evert2014] Gabriella Lapesa", "Stefan Evert"], "venue": null, "citeRegEx": "Lapesa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lapesa et al\\.", "year": 2014}, {"title": "Dependencybased word embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of ACL", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211\u2013225", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "2015a. Not all contexts are created equal: Better word representations with variable attention", "author": ["Ling et al.2015a] Wang Ling", "Lin Chu-Cheng", "Yulia Tsvetkov", "Silvio Amir", "Ram\u00f3n Fernandez Astudillo", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "2015b. Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling et al.2015b] Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso"], "venue": "In Proceedings of NAACL-HLT", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["Lu et al.2015] Ang Lu", "Weiran Wang", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In Proceedings of NAACL", "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55\u201360.", "citeRegEx": "Bethard and McClosky.,? 2014", "shortCiteRegEx": "Bethard and McClosky.", "year": 2014}, {"title": "Probabilistic modeling of joint-context in distributional similarity", "author": ["Melamud et al.2014] Oren Melamud", "Ido Dagan", "Jacob Goldberger", "Idan Szpektor", "Deniz Yuret"], "venue": "In Proceedings of CoNLL", "citeRegEx": "Melamud et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2014}, {"title": "Modeling word meaning in context with substitute vectors", "author": ["Ido Dagan", "Jacob Goldberger"], "venue": "In Proceedings of NAACL", "citeRegEx": "Melamud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2015}, {"title": "2015b. A simple word embedding model for lexical substitution", "author": ["Omer Levy", "Ido Dagan"], "venue": "In Proceedings of the Vector Space Modeling for NLP Workshop,", "citeRegEx": "Melamud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of ICLR", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Proceedings of NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes", "author": ["Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang"], "venue": "In Joint Conference on EMNLP and CoNLL-Shared Task,", "citeRegEx": "Pradhan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["Igor Labutov", "David Mimno", "Thorsten Joachims"], "venue": "In Proc. of EMNLP", "citeRegEx": "Schnabel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951\u20132959", "author": ["Snoek et al.2012] Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": null, "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of the seventh conference on Natural language learning", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": "In Proc. of EMNLP", "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Word representations: A simple and general method for semisupervised learning", "author": ["Turian et al.2010] J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proc. of ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Learning syntactic categories using paradigmatic representations of word context", "author": ["Enis Sert", "Deniz Yuret"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Yatbaz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yatbaz et al\\.", "year": 2012}, {"title": "Learning word meta-embeddings by using ensembles of embedding sets. arXiv preprint arXiv:1508.04257", "author": ["Yin", "Sch\u00fctze2015] Wenpeng Yin", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "FASTSUBS: An efficient and exact procedure for finding the most likely lexical substitutes based on an n-gram language model", "author": ["Deniz Yuret"], "venue": "Signal Processing Letters, IEEE,", "citeRegEx": "Yuret.,? \\Q2012\\E", "shortCiteRegEx": "Yuret.", "year": 2012}], "referenceMentions": [{"referenceID": 36, "context": "features in a broad range of NLP tasks with limited supervision (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013; Bansal et al., 2014).", "startOffset": 64, "endOffset": 151}, {"referenceID": 5, "context": "features in a broad range of NLP tasks with limited supervision (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013; Bansal et al., 2014).", "startOffset": 64, "endOffset": 151}, {"referenceID": 33, "context": "features in a broad range of NLP tasks with limited supervision (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013; Bansal et al., 2014).", "startOffset": 64, "endOffset": 151}, {"referenceID": 1, "context": "features in a broad range of NLP tasks with limited supervision (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2013; Bansal et al., 2014).", "startOffset": 64, "endOffset": 151}, {"referenceID": 29, "context": "com/p/word2vec/ and GloVe2 (Pennington et al., 2014) are among the most widely used word embedding models today.", "startOffset": 27, "endOffset": 52}, {"referenceID": 1, "context": "For example, Bansal et al. (2014) and Levy and Goldberg (2014) showed that using syntactic contexts rather than window contexts in word2vec captures functional similarity (as in lion:cat) rather than topical similarity or relatedness", "startOffset": 13, "endOffset": 34}, {"referenceID": 1, "context": "For example, Bansal et al. (2014) and Levy and Goldberg (2014) showed that using syntactic contexts rather than window contexts in word2vec captures functional similarity (as in lion:cat) rather than topical similarity or relatedness", "startOffset": 13, "endOffset": 63}, {"referenceID": 1, "context": "Further, Bansal et al. (2014) and Melamud et al.", "startOffset": 9, "endOffset": 30}, {"referenceID": 1, "context": "Further, Bansal et al. (2014) and Melamud et al. (2015b) showed the benefits of such modified-context embeddings in dependency parsing and lexical substitution tasks.", "startOffset": 9, "endOffset": 57}, {"referenceID": 10, "context": ", as in WordSim-353 (Finkelstein et al., 2001), while extrinsic tasks include various \u2018real\u2019 downstream NLP tasks, such as coreference resolution and sentiment analysis.", "startOffset": 20, "endOffset": 46}, {"referenceID": 31, "context": "cent works have shown that while intrinsic evaluations are easier to perform, their correlation with results on extrinsic evaluations is not very reliable (Schnabel et al., 2015; Tsvetkov et al., 2015), stressing the importance of the latter.", "startOffset": 155, "endOffset": 201}, {"referenceID": 35, "context": "cent works have shown that while intrinsic evaluations are easier to perform, their correlation with results on extrinsic evaluations is not very reliable (Schnabel et al., 2015; Tsvetkov et al., 2015), stressing the importance of the latter.", "startOffset": 155, "endOffset": 201}, {"referenceID": 12, "context": "We use a fixed learning corpus for a fair comparison of all embedding types: a concatenation of three large English corpora: (1) English Wikipedia 2015, (2) UMBC web corpus (Han et al., 2013), and (3) English Gigaword (LDC2011T07) newswire corpus (Parker et al.", "startOffset": 173, "endOffset": 191}, {"referenceID": 27, "context": "For more details refer to Mikolov et al. (2013b). http://bitbucket.", "startOffset": 26, "endOffset": 49}, {"referenceID": 1, "context": "the parsed version of our corpus, similar to the approach of Bansal et al. (2014). word2vecf accepts as its input arbitrary target-context pairs.", "startOffset": 61, "endOffset": 82}, {"referenceID": 1, "context": "the parsed version of our corpus, similar to the approach of Bansal et al. (2014). word2vecf accepts as its input arbitrary target-context pairs. In the case of dependency-based word embeddings, the context elements are the syntactic contexts of the target word, rather than the words in a window around it. Specifically, following Levy and Goldberg (2014), we first \u2018collapsed\u2019 prepositions (as implemented in word2vecf).", "startOffset": 61, "endOffset": 357}, {"referenceID": 37, "context": "Substitute vectors are a recent approach to representing contexts of target words, proposed in Yatbaz et al. (2012). Instead of the neighboring words them-", "startOffset": 95, "endOffset": 116}, {"referenceID": 37, "context": "Substitute-based contexts are generated using a language model and were successfully used in distributional semantics models for part-of-speech induction (Yatbaz et al., 2012), word sense induction (Baskaya et al.", "startOffset": 154, "endOffset": 175}, {"referenceID": 2, "context": ", 2012), word sense induction (Baskaya et al., 2013), functional semantic similarity (Melamud et al.", "startOffset": 30, "endOffset": 52}, {"referenceID": 24, "context": ", 2013), functional semantic similarity (Melamud et al., 2014) and lexical substitution tasks (Melamud et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 37, "context": "Similar to Yatbaz et al. (2012), we consider the words in a substitute vector, as a weighted set of contexts \u2018co-occurring\u2019 with the observed target word.", "startOffset": 11, "endOffset": 32}, {"referenceID": 13, "context": "We learned a 4-gram KneserNey language model from our learning corpus using KenLM (Heafield et al., 2013).", "startOffset": 82, "endOffset": 105}, {"referenceID": 39, "context": "Then, we used FASTSUBS (Yuret, 2012) with this language model to efficiently generate substitute vectors, where the", "startOffset": 23, "endOffset": 36}, {"referenceID": 19, "context": "shown to be effective in compressing sparse word representations (Levy et al., 2015).", "startOffset": 65, "endOffset": 84}, {"referenceID": 6, "context": "Dhillon et al. (2011) considered their two CCA views as embed-", "startOffset": 0, "endOffset": 22}, {"referenceID": 22, "context": "Faruqui and Dyer (2014) and Lu et al. (2015) considered multilingual views, showing improvements in several in-", "startOffset": 28, "endOffset": 45}, {"referenceID": 32, "context": "1) to tune the CCA hyperparameters9 with the Spearmint Bayesian optimization tool10 (Snoek et al., 2012).", "startOffset": 84, "endOffset": 104}, {"referenceID": 10, "context": "The popular WordSim-353 dataset (Finkelstein et al., 2001) includes 353 word pairs manually", "startOffset": 32, "endOffset": 58}, {"referenceID": 22, "context": "See Faruqui and Dyer (2014), Lu et al. (2015) for details.", "startOffset": 29, "endOffset": 46}, {"referenceID": 14, "context": "SimLex-999 (Hill et al., 2014) is a larger word pair similarity dataset with 999 annotated pairs, purposely built to focus on functional similarity.", "startOffset": 11, "endOffset": 30}, {"referenceID": 0, "context": "While WordSim-353 does not make a distinction between different \u2018flavors\u2019 of similarity, Agirre et al. (2009) proposed two subsets of this dataset, WordSim-353S and WordSim-353-R, which focus on functional and topical similarities, respectively.", "startOffset": 89, "endOffset": 110}, {"referenceID": 36, "context": "2) Named Entity Recognition (NER) We used the NER system of Turian et al. (2010), which allows adding word embedding features (on top of various other features) to a regularized averaged perceptron classifier, and achieves near state-of-the-art results using several off-the-shelf word representations.", "startOffset": 60, "endOffset": 81}, {"referenceID": 36, "context": "2) Named Entity Recognition (NER) We used the NER system of Turian et al. (2010), which allows adding word embedding features (on top of various other features) to a regularized averaged perceptron classifier, and achieves near state-of-the-art results using several off-the-shelf word representations. We varied the type of word embeddings used as features when training the NER model, to evaluate their effect on NER benchmarks results. Following Turian et al. (2010), we used the CoNLL-2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003) with 204K/51K train/dev words, as our main benchmark.", "startOffset": 60, "endOffset": 470}, {"referenceID": 30, "context": "We evaluated our features on the CoNLL-2012 coreference shared task (Pradhan et al., 2012).", "startOffset": 68, "endOffset": 90}, {"referenceID": 8, "context": "4) Sentiment Analysis (SENTI) Following Faruqui et al. (2014), we used a sentence-level binary decision version of the sentiment analysis task from Socher et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 8, "context": "4) Sentiment Analysis (SENTI) Following Faruqui et al. (2014), we used a sentence-level binary decision version of the sentiment analysis task from Socher et al. (2013). In this setting, neutral sentences were discarded and all remaining sentences were labeled coarsely as positive or negative.", "startOffset": 40, "endOffset": 169}, {"referenceID": 36, "context": "See Turian et al. (2010) for more details on this setting.", "startOffset": 4, "endOffset": 25}, {"referenceID": 1, "context": "More generally, embeddings that do well in functional similarity intrinsic benchmarks and badly in topical ones (DEP, SUB and W1) work best for PARSE, while large window contexts perform worst, similar to observations in Bansal et al. (2014).", "startOffset": 221, "endOffset": 242}, {"referenceID": 17, "context": "Levy et al. (2015), Lapesa and Evert (2014), and Lai et al.", "startOffset": 0, "endOffset": 19}, {"referenceID": 17, "context": "Levy et al. (2015), Lapesa and Evert (2014), and Lai et al.", "startOffset": 0, "endOffset": 44}, {"referenceID": 16, "context": "(2015), Lapesa and Evert (2014), and Lai et al. (2015) evaluate several design choices when learning word representations.", "startOffset": 37, "endOffset": 55}, {"referenceID": 16, "context": "(2015), Lapesa and Evert (2014), and Lai et al. (2015) evaluate several design choices when learning word representations. However, Levy et al. (2015) and Lapesa and Evert (2014)", "startOffset": 37, "endOffset": 151}, {"referenceID": 16, "context": "(2015), Lapesa and Evert (2014), and Lai et al. (2015) evaluate several design choices when learning word representations. However, Levy et al. (2015) and Lapesa and Evert (2014)", "startOffset": 37, "endOffset": 179}, {"referenceID": 16, "context": "perform only intrinsic evaluations and restrict context representation to word windows, while Lai et al. (2015) do perform extrinsic evaluations, but re-", "startOffset": 94, "endOffset": 112}, {"referenceID": 31, "context": "Schnabel et al. (2015) and Tsvetkov et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 31, "context": "Schnabel et al. (2015) and Tsvetkov et al. (2015) report low correlation between intrinsic and extrinsic results with different word embeddings (they did not evaluate different context types), which is consistent with differences", "startOffset": 0, "endOffset": 50}, {"referenceID": 1, "context": "Bansal et al. (2014) show that functional (dependency-based and small-window) embeddings yield higher parsing improvements than topical (large-window) embeddings, which is consistent with our findings.", "startOffset": 0, "endOffset": 21}, {"referenceID": 37, "context": "Cirik and Yuret (2014) investigates S-CODE word embeddings based on substitute word contexts.", "startOffset": 10, "endOffset": 23}, {"referenceID": 20, "context": "Ling et al. (2015b) and Ling et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 20, "context": "Ling et al. (2015b) and Ling et al. (2015a) propose extensions to the standard window-based context modeling.", "startOffset": 0, "endOffset": 44}, {"referenceID": 8, "context": "Alternatively, another recent popular line of work (Faruqui et al., 2014; Kiela et al., 2015) attempts to improve word embeddings by using manuallyconstructed resources, such as WordNet.", "startOffset": 51, "endOffset": 93}, {"referenceID": 15, "context": "Alternatively, another recent popular line of work (Faruqui et al., 2014; Kiela et al., 2015) attempts to improve word embeddings by using manuallyconstructed resources, such as WordNet.", "startOffset": 51, "endOffset": 93}, {"referenceID": 8, "context": "Alternatively, another recent popular line of work (Faruqui et al., 2014; Kiela et al., 2015) attempts to improve word embeddings by using manuallyconstructed resources, such as WordNet. These techniques could be complementary to our work. Finally, Yin and Sch\u00fctze (2015) and Goikoetxea et al.", "startOffset": 52, "endOffset": 272}, {"referenceID": 8, "context": "Alternatively, another recent popular line of work (Faruqui et al., 2014; Kiela et al., 2015) attempts to improve word embeddings by using manuallyconstructed resources, such as WordNet. These techniques could be complementary to our work. Finally, Yin and Sch\u00fctze (2015) and Goikoetxea et al. (2016) propose word embeddings combinations, using methods such as concatenation and CCA, but evaluate mostly on intrinsic tasks and do not consider different types of contexts.", "startOffset": 52, "endOffset": 301}], "year": 2017, "abstractText": "We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words.", "creator": "LaTeX with hyperref package"}}}