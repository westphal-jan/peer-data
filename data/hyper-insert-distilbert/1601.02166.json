{"id": "1601.02166", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2016", "title": "Empirical Gaussian priors for cross-lingual transfer learning", "abstract": "sequence model formal learning algorithms typically maximize log - likelihood variability minus if the highest norm error of the model ( or minimize hamming marginal loss + a norm ). in cross - lingual part - coding of - speech ( termed pos ) grammar tagging, our target expressive language initial training image data consists of linguistic sequences generated of sentences with word - or by - word labels separately projected externally from translations made in $ 89 k $ languages for which stimuli we typically have exclusively labeled instruction data, compiled via word graph alignments. our training data collection is quite therefore very noisy, resulting and occasionally if underlying rademacher complexity complexity is high, learning algorithms are prone to overfit. norm - function based regularization assumes prefer a constant width length and equals zero mean prior. normally we instead propose to use the $ 8 k $ source adaptive language recognition models model to adequately estimate the parameters of only a refined gaussian expectation prior technique for learning new pos taggers. setting this leads roughly to obtaining significantly better performance efficiency in initial multi - source linguistic transfer set - ups. while we also explicitly present myself a drop - out version hypothesis that injects ( empirical ) gaussian noise appropriately during quantitative online learning. finally, still we note that modelling using empirical inference gaussian priors too leads to much lower rademacher problem complexity, especially and is superior sometimes to explicit optimally weighted model interpolation.", "histories": [["v1", "Sat, 9 Jan 2016 23:34:05 GMT  (11kb)", "http://arxiv.org/abs/1601.02166v1", "Presented at NIPS 2015 Workshop on Transfer and Multi-Task Learning"]], "COMMENTS": "Presented at NIPS 2015 Workshop on Transfer and Multi-Task Learning", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anders s{\\o}gaard"], "accepted": false, "id": "1601.02166"}, "pdf": {"name": "1601.02166.pdf", "metadata": {"source": "CRF", "title": "Empirical Gaussian priors for cross-lingual transfer learning", "authors": ["Anders S\u00f8gaard"], "emails": ["soegaard@hum.ku.dk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 1.\n02 16\n6v 1\n[ cs\n.C L\n] 9\nSequence model learning algorithms typically maximize log-likelihood minus the norm of the model (or minimize Hamming loss + norm). In cross-lingual part-ofspeech (POS) tagging, our target language training data consists of sequences of sentences with word-by-word labels projected from translations in k languages for which we have labeled data, via word alignments. Our training data is therefore very noisy, and if Rademacher complexity is high, learning algorithms are prone to overfit. Norm-based regularization assumes a constant width and zero mean prior. We instead propose to use the k source language models to estimate the parameters of a Gaussian prior for learning new POS taggers. This leads to significantly better performance in multi-source transfer set-ups. We also present a drop-out version that injects (empirical) Gaussian noise during online learning. Finally, we note that using empirical Gaussian priors leads to much lower Rademacher complexity, and is superior to optimally weighted model interpolation."}, {"heading": "1 Cross-lingual transfer learning of sequence models", "text": "The people of the world speak about 6,900 different languages. Open-source off-the-shelf natural language processing (NLP) toolboxes like OpenNLP1 and CoreNLP2 cover only 6\u20137 languages, and we have sufficient labeled training data for inducing models for about 20\u201330 languages. In other words, supervised sequence learning algorithms are not sufficient to induce POS models for but a small minority of the world\u2019s languages.\nWhat can we do for all the languages for which no training data is available? Unsupervised POS induction algorithms have methodological problems (in-sample evaluation, community-wide hyperparameter tuning, etc.), and performance is prohibitive of downstream applications. Some work on unsupervised POS tagging has assumed other resources such as tag dictionaries [Li et al., 2012], but such resources are also only available for a limited number of languages. In our experiments, we assume that no training data or tag dictionaries are available. Our only assumption is a bit of text translated into multiple languages, specifically, fragments of the Bible. We will use Bible data for annotation projection, as well as for learning cross-lingual word embeddings (\u00a73).\nUnsupervised learning with typologically informed priors [Naseem et al., 2010] is an interesting approach to unsupervised POS induction that is more applicable to low-resource languages. Our work is related to this work, but we learn informed priors rather than stipulate them and combine these priors with annotation projection (learning from noisy labels) rather than unsupervised learning.\n1https://opennlp.apache.org/ 2http://nlp.stanford.edu/software/corenlp.shtml\nAnnotation projection refers to transferring annotation from one or more source languages to the target language (for which no labeled data is otherwise available), typically through word alignments. In our experiments below, we use an unsupervised word alignment algorithm to align 15 \u00d7 12 language pairs. For 15 languages, we have predicted POS tags for each word in our multi-parallel corpus. For each word in one of our 12 target language training datasets, we thus have up to 15 votes for each word token, possibly weighted by the confidence of the word alignment algorithm. In this paper, we simply use the majority votes. This is the set-up assumed throughout in this paper (see \u00a73 for more details):\nLow-resource cross-lingual POS tagging We have at our disposal k (=15) source language models and a multi-parallel corpus (the Bible) that we can use to project annotation from the k source languages to new target languages for which no labeled data is available. If we use k > 1 source languages, we refer to this as multi-source cross-lingual transfer; if we only use a single source language, we refer to this as single-source cross-lingual transfer. In this paper, we only consider multi-source cross-language transfer learning.\nSince the training data sets for our target languages (the annotation projections) are very noisy, the risk of over-fitting is extremely high. We are therefore interested in learning algorithms that efficiently limit the Rademacher complexity of the learning problem, i.e., the chance of fitting to random noise. In other words, we want a model with higher integrated bias and lower integrated variance [Geman et al., 1992]. Our approach \u2013 using empirical Gaussian priors \u2013 is introduced in \u00a72, including a drop-out version of the regularizer. \u00a73 describes our experiments. In \u00a74, we provide some observations, namely that using empirical Gaussian priors reduces (i) Rademacher complexity and (ii) integrated variance, and (iii) that using empirical Gaussian priors is superior to optimally weighted model interpolation."}, {"heading": "2 Empirical Gaussian priors", "text": "We will apply empirical Gaussian priors to linear-chain conditional random fields (CRFs; Lafferty et al. [2001]) and averaged structured perceptrons [Collins, 2002]. Linear-chain CRFs are trained by maximising the conditional log-likelihood of labeled sequences LL(w,D) = \u2211\n\u3008x,y\u3009\u2208D logP (y|x) with w \u2208 R m and D a dataset consisting of sequences of discrete input symbols x = x1, . . . , xn associated with sequences of discrete labels y = y1, . . . , yn. Lk-regularized CRFs maximize LL(w,D) \u2212 |w|k with typically k \u2208 {0, 1, 2,\u221e}, which all introduce costantwidth, zero-mean regularizers. We refer to Lk-regularized CRFs as L2-CRF. Lk regularizers are parametric priors where the only parameter is the width of the bounding shape. The L2-regularizer is a Gaussian prior with zero mean, for example. The regularised log-likelihood with a Gaussian\nprior is LL(w,D)\u2212 1 2\n\u2211m\nj\n(\n\u03bbj\u2212\u00b5j \u03c32 j\n)2\n. For practical reasons, hyper-parameters \u00b5j and \u03c3j are typi-\ncally assumed to be constant for all values of j. This also holds for recent work on parametric noise injection, e.g., S\u00f8gaard [2013]. If these parameters are assumed to be constant, the above objective becomes equivalent to L2-regularization. However, you can also try to learn these parameters. In empirical Bayes [Casella, 1985], the parameters are learned from D itself. Smith and Osborne [2005] suggest learning the parameters from a validation set. In our set-up, we do not assume that we can learn the priors from training data (which is noisy) or validation data (which is generally not available in cross-lingual learning scenarios). Instead we estimate these parameters directly from source language models.\nWhen we estimate Gaussian priors from source language models, we will learn which features are invariant across languages, and which are not. We thereby introduce an ellipsoid regularizer whose centre is the average source model. In our experiments, we consider both the case where variance is assumed to be constant \u2013 which we call L2-regularization with priors (L2-PRIOR)\u2014 and the case where both variances and means are learned \u2013 which we call empirical Gaussian priors (EMPGAUSS). L2-PRIOR is the L2-CRF objective with \u03c32j = C with C a regularization parameter, and \u00b5j = \u00b5\u0302j the average value of the corresponding parameter in the observed source models.\nEMPGAUSS replaces the above objective with LL(\u03bb) + \u2211 j log 1 \u03c3 \u221a 2\u03c0\ne\u2212 (\u03bbj\u2212\u00b5j)\n2\n2\u03c32 , which, assuming model parameters are mutually independent, is the same as jointly optimising model probability and likelihood of the data. Note that minimizing the squared weights is equivalent to maximizing\nthe log probability of the weights under a zero-mean Gaussian prior, and in the same way, this is equivalent to minimising the above objective with empirically estimated parameters \u00b5\u0302j and \u03c3\u00b5j . In other words, empirical Gaussian priors are bounding ellipsoids on the hypothesis space with learned widths and centres. Also, note that in single-source cross-lingual transfer learning, observed variance is zero, and we therefore replace this with a regularization parameter C shared with the baseline. In the single-source set-up, L2-PRIOR is thus equivalent to EMPGAUSS. We use LBFGS to maximize our baseline L2-regularized objectives, as well as our empirical Gaussian prior objectives.\nPractical observations (i) Using empirical Gaussian priors does not assume identical feature representations in the source and target models. Model parameters for which features were unseen in the source languages, can naturally be assigned Gaussians with parameters \u3008\u00b5 = 0, \u03c3 = \u03c3av \u3009 where \u03c3av is the average variance in the estimated Gaussians. In our experiments, we rely on simple feature representations that are identical for all languages. (ii) Also, consider the obvious extension of using empirical Gaussian priors in the multi-source set-up, where we regularize the target to stay in one of several bounding ellipsoids rather than the one given by the full set of source models. These ellipsoids could come from typologically different groups of source languages or from individual source languages (and then have constant width). While this is technically a non-convex regularizer, we can simply run one model per source group and choose the one with the best fit to data. We do not explore this direction further in this paper."}, {"heading": "2.1 Empirical Gaussian noise injection", "text": "We also introduce a drop-out variant of empirical Gaussian priors. Our point of departure is average structured perceptron. We implement empirical Gaussian noise injection with Gaussians \u3008(\u00b51, \u03c31), . . . , (\u00b5m, \u03c3m)\u3009 for m features as follows. We initialise our model parameters with the means \u00b5j . For every instance we pass over, we draw a corruption vector g of random values vi from the corresponding Gaussians (1, \u03c3i). We inject the noise in g by taking pairwise multiplications of g and our feature representations of the input sequence with the relevant label sequences. Note that this drop-out algorithm is parameter-free, but of course we could easily throw in a hyper-parameter controlling the degree of regularization. We give the algorithm in Algorithm 1.\nAlgorithm 1 Averaged structured perceptron with empirical Gaussian noise 1: T = {\u3008x1,y1\u3009, . . . , \u3008xn,yn\u3009} w. xi = \u3008v1, . . .\u3009 and vk = \u3008f1, . . . , fm\u3009,w0 = \u3008w1 : \u00b5\u03021, . . . , wm :\n\u00b5\u0302m\u3009 2: for i \u2264 I \u00d7 |T | do 3: for j \u2264 n do 4: g \u2190 sample(N (1, \u03c31), . . . ,N (1, \u03c3m)) 5: y\u0302 \u2190 argmaxy wi \u00b7 g 6: wi+1 \u2190 wi + \u03a6(xj ,yj) \u00b7 g \u2212 \u03a6(xj , y\u0302) \u00b7 g 7: end for 8: end for"}, {"heading": "3 Cross-lingual POS Experiments", "text": "Data In our multi-source cross-language transfer learning set-up, we rely on 15 multiple source language models to estimate our priors. We use a subset of the data in [Agic et al., 2015].\nAnnotation projection We learn IBM-2 word alignment models from the Bible using EM to project annotation from the 15 source languages to our 10 target languages. We assign each word in each target language the majority vote tag after projecting from all source languages.\nFeatures We use a simple feature template considering only orthographic features and crosslingual word embeddings. The orthographic features include whether the current word contains capital letters, hyphens, or numbers. The embeddings are 40-dimensional distributional vectors capturing information about the distribution of words in a multi-parallel corpus. We learned these embeddings using an improvement over the technique suggested in S\u00f8gaard et al. [2015]. S\u00f8gaard et al.\n[2015] suggest a remarkably simple approach to learning distributional representations of words that transfer across languages. In parallel document collections or parallel corpora, we can represent the meaning of words by a vector encoding in what documents or sentences each word occurs. This is known as inverted indexing in database theory. We encode the meaning of words by binary vectors encoding their presence in biblical verses and then apply SVD to reduce these vectors to 40 dimensions. Dimensionality was chosen for comparability with other publicly available bilingual embeddings. While this approach assumes fewer resources available, published results suggest that such representations are superior to previous work [S\u00f8gaard et al., 2015]. We improve on this approach by shifting and row normalisation. We tuned the parameters on Danish development data.\nBaselines and systems Our first baseline is L2-regularized CRF learned using L-BFGS. Our batch CRF systems are L2-PRIOR and EMPGAUSS.Our second baseline is an online averaged structured perceptron with L2 weight decay, learned using additive updates. We augment averaged structured perceptron with empirical Gaussian noise injection (Algorithm 2), leading to EMPGAUSSNOISE.\nParameters In L2-PRIOR, we set the variance to be the same for all parameters, namely equivalent to the regularization parameter in our L2-regularized baseline. The parameter was optimized on Danish development data.\nResults We report the results of several systems: Our CRF models \u2013 L2-CRF, L2-PRIOR, MULTI-L2-PRIOR and EMPGAUSS \u2013 as well as our online models \u2013 L2-PERC and EMPGAUSSNOISE. We present the macro-average performances across our 10 target languages below. We compute significance using Wilcoxon over datasets following Demsar [2006] and mark p < 0.01 by **.\nL2-CRF L2-PRIOR EMPGAUSS L2-PERC EMPGAUSSNOISE\n76.1 80.31\u2217\u2217 81.02\u2217\u2217 75.04 80.54\u2217\u2217"}, {"heading": "4 Observations", "text": "We make the following additional observations: (i) Following the procedure in Zhu et al. [2009], we can compute the Rademacher complexity of our models, i.e., their ability to learn noise in the labels (overfit). Sampling POS tags randomly from a uniform distribution, chance complexity is 0.083. With small sample sizes, L2-CRFs actually begin to learn patterns with Rademacher complexity rising to 0.086, whereas both L2-PRIOR and EMPGAUSS never learn a better fit than chance. (ii) Geman et al. [1992] present a simple approach to explicitly studying bias-variance trade-offs during learning. They draw subsamples of l < m training data points D1, . . . ,Dk and use a validation dataset of m\u2032 data points to define the integrated variance of our methods. Again, we see that using empirical Gaussian priors lead to less integrated variance. (iii) An empirical Gaussian prior effectively limits us to hypotheses in H in a ellipsoid around the average source model. When inference is exact, and our loss function is convex, we learn the model with the smallest loss on the training data within this ellipsoid. Model interpolation of (some weighting of) the average source model and the unregularized target model can potentially result in the same model, but since model interpolation is limited to the hyperplane connecting the two models, the probability of this to happen is infinitely small ( 1\u221e ). Since for any effective regularization parameter value (such that the regularized model is different from the unregularized model), the empirical Gaussian prior can be expected to have the same Rademacher complexity as model interpolation, we conclude that using empirical Gaussian priors is superior to model interpolation (and data concatenation)."}], "references": [{"title": "If all you have is a bit of the Bible: Learning POS taggers for truly low-resource languages", "author": ["Zeljko Agic", "Dirk Hovy", "Anders S\u00f8gaard"], "venue": "In ACL,", "citeRegEx": "Agic et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agic et al\\.", "year": 2015}, {"title": "An introduction to empirical Bayes data analysis", "author": ["George Casella"], "venue": "American Statistician,", "citeRegEx": "Casella.,? \\Q1985\\E", "shortCiteRegEx": "Casella.", "year": 1985}, {"title": "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms", "author": ["Michael Collins"], "venue": "In EMNLP,", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["Janez Demsar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Demsar.,? \\Q2006\\E", "shortCiteRegEx": "Demsar.", "year": 2006}, {"title": "Neural networks and the bias/variance dilemma", "author": ["Stuart Geman", "Elie Bienenstock", "Rene Doursat"], "venue": "Neural Computation,", "citeRegEx": "Geman et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1992}, {"title": "Conditional random fields: probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": "In ICML,", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Wiki-ly supervised part-of-speech tagging", "author": ["Shen Li", "Jo\u00e3o Gra\u00e7a", "Ben Taskar"], "venue": "In EMNLP,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Using universal linguistic knowledge to guide grammar induction", "author": ["Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Naseem et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Naseem et al\\.", "year": 2010}, {"title": "Regularisation techniques for conditional random fields: Parameterised versus parameter-free", "author": ["Andrew Smith", "Miles Osborne"], "venue": "In IJCNLP,", "citeRegEx": "Smith and Osborne.,? \\Q2005\\E", "shortCiteRegEx": "Smith and Osborne.", "year": 2005}, {"title": "Zipfian corruptions for robust pos tagging", "author": ["Anders S\u00f8gaard"], "venue": "In Proceedings of NAACL,", "citeRegEx": "S\u00f8gaard.,? \\Q2013\\E", "shortCiteRegEx": "S\u00f8gaard.", "year": 2013}, {"title": "Inverted indexing for cross-lingual nlp", "author": ["Anders S\u00f8gaard", "\u017deljko Agi\u0107", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "Barbara Plank", "Bernd Bohnet", "Anders Johannsen"], "venue": "In ACL,", "citeRegEx": "S\u00f8gaard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2015}, {"title": "Human Rademacher complexity", "author": ["Jerry Zhu", "Timothy Rogers", "Bryan Gibson"], "venue": "In NIPS,", "citeRegEx": "Zhu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "Some work on unsupervised POS tagging has assumed other resources such as tag dictionaries [Li et al., 2012], but such resources are also only available for a limited number of languages.", "startOffset": 91, "endOffset": 108}, {"referenceID": 7, "context": "Unsupervised learning with typologically informed priors [Naseem et al., 2010] is an interesting approach to unsupervised POS induction that is more applicable to low-resource languages.", "startOffset": 57, "endOffset": 78}, {"referenceID": 4, "context": "In other words, we want a model with higher integrated bias and lower integrated variance [Geman et al., 1992].", "startOffset": 90, "endOffset": 110}, {"referenceID": 2, "context": "[2001]) and averaged structured perceptrons [Collins, 2002].", "startOffset": 44, "endOffset": 59}, {"referenceID": 4, "context": "We will apply empirical Gaussian priors to linear-chain conditional random fields (CRFs; Lafferty et al. [2001]) and averaged structured perceptrons [Collins, 2002].", "startOffset": 89, "endOffset": 112}, {"referenceID": 1, "context": "In empirical Bayes [Casella, 1985], the parameters are learned from D itself.", "startOffset": 19, "endOffset": 34}, {"referenceID": 7, "context": ", S\u00f8gaard [2013]. If these parameters are assumed to be constant, the above objective becomes equivalent to L2-regularization.", "startOffset": 2, "endOffset": 17}, {"referenceID": 1, "context": "In empirical Bayes [Casella, 1985], the parameters are learned from D itself. Smith and Osborne [2005] suggest learning the parameters from a validation set.", "startOffset": 20, "endOffset": 103}, {"referenceID": 0, "context": "We use a subset of the data in [Agic et al., 2015].", "startOffset": 31, "endOffset": 50}, {"referenceID": 9, "context": "We learned these embeddings using an improvement over the technique suggested in S\u00f8gaard et al. [2015]. S\u00f8gaard et al.", "startOffset": 81, "endOffset": 103}, {"referenceID": 10, "context": "While this approach assumes fewer resources available, published results suggest that such representations are superior to previous work [S\u00f8gaard et al., 2015].", "startOffset": 137, "endOffset": 159}, {"referenceID": 3, "context": "We compute significance using Wilcoxon over datasets following Demsar [2006] and mark p < 0.", "startOffset": 63, "endOffset": 77}, {"referenceID": 10, "context": "We make the following additional observations: (i) Following the procedure in Zhu et al. [2009], we can compute the Rademacher complexity of our models, i.", "startOffset": 78, "endOffset": 96}, {"referenceID": 4, "context": "(ii) Geman et al. [1992] present a simple approach to explicitly studying bias-variance trade-offs during learning.", "startOffset": 5, "endOffset": 25}], "year": 2016, "abstractText": "Sequence model learning algorithms typically maximize log-likelihood minus the norm of the model (or minimize Hamming loss + norm). In cross-lingual part-ofspeech (POS) tagging, our target language training data consists of sequences of sentences with word-by-word labels projected from translations in k languages for which we have labeled data, via word alignments. Our training data is therefore very noisy, and if Rademacher complexity is high, learning algorithms are prone to overfit. Norm-based regularization assumes a constant width and zero mean prior. We instead propose to use the k source language models to estimate the parameters of a Gaussian prior for learning new POS taggers. This leads to significantly better performance in multi-source transfer set-ups. We also present a drop-out version that injects (empirical) Gaussian noise during online learning. Finally, we note that using empirical Gaussian priors leads to much lower Rademacher complexity, and is superior to optimally weighted model interpolation. 1 Cross-lingual transfer learning of sequence models The people of the world speak about 6,900 different languages. Open-source off-the-shelf natural language processing (NLP) toolboxes like OpenNLP1 and CoreNLP2 cover only 6\u20137 languages, and we have sufficient labeled training data for inducing models for about 20\u201330 languages. In other words, supervised sequence learning algorithms are not sufficient to induce POS models for but a small minority of the world\u2019s languages. What can we do for all the languages for which no training data is available? Unsupervised POS induction algorithms have methodological problems (in-sample evaluation, community-wide hyperparameter tuning, etc.), and performance is prohibitive of downstream applications. Some work on unsupervised POS tagging has assumed other resources such as tag dictionaries [Li et al., 2012], but such resources are also only available for a limited number of languages. In our experiments, we assume that no training data or tag dictionaries are available. Our only assumption is a bit of text translated into multiple languages, specifically, fragments of the Bible. We will use Bible data for annotation projection, as well as for learning cross-lingual word embeddings (\u00a73). Unsupervised learning with typologically informed priors [Naseem et al., 2010] is an interesting approach to unsupervised POS induction that is more applicable to low-resource languages. Our work is related to this work, but we learn informed priors rather than stipulate them and combine these priors with annotation projection (learning from noisy labels) rather than unsupervised learning. https://opennlp.apache.org/ http://nlp.stanford.edu/software/corenlp.shtml", "creator": "LaTeX with hyperref package"}}}