{"id": "1601.04574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2016", "title": "SimpleDS: A Simple Deep Reinforcement Learning Dialogue System", "abstract": "this flagship paper similarly presents'simpleds ', proposing a widely simple and publicly available dialogue system trained investigators with deep underlying reinforcement vocabulary learning. in contrast, to previous reinforcement learning dialogue systems, this negotiation system avoids manual feature template engineering by performing action selection decisions directly from internal raw phonetic text of \" the last system and ( noisy ) user responses. whereas our initial breakthrough results, in searching the clinical restaurant ethics domain, broadly show that it is indeed possible to induce inherently reasonable dialogue behaviour with an apt approach that aims importantly for high levels of competitive automation in conventional dialogue control for integrating intelligent / interactive agents.", "histories": [["v1", "Mon, 18 Jan 2016 15:37:22 GMT  (140kb,D)", "http://arxiv.org/abs/1601.04574v1", "International Workshop on Spoken Dialogue Systems (IWSDS), 2016"]], "COMMENTS": "International Workshop on Spoken Dialogue Systems (IWSDS), 2016", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["heriberto cuay\\'ahuitl"], "accepted": false, "id": "1601.04574"}, "pdf": {"name": "1601.04574.pdf", "metadata": {"source": "CRF", "title": "SimpleDS: A Simple Deep Reinforcement Learning Dialogue System", "authors": ["Heriberto Cuay\u00e1huitl"], "emails": ["hc213@hw.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Almost two decades ago, the (spoken) dialogue systems community adopted the Reinforcement Learning (RL) paradigm since it offered the possibility to treat dialogue design as an optimisation problem, and because RL-based systems can improve their performance over time with experience. Although a large number of methods have been proposed for training (spoken) dialogue systems using RL, the question of \u201cHow to train dialogue policies in an efficient, scalable and effective way across domains?\u201d still remains as an open problem. One limitation of current approaches is the fact that RL-based dialogue systems still require high-levels of human intervention (from system developers), as opposed to automating the dialogue design. Training a system of this kind requires a system developer to provide a set of features to describe the dialogue state, a set of actions to control the interaction, and a performance function to reward or penalise the action-selection process. All of these elements have to be carefully engineered in order to learn a good dialogue policy (or policies). This suggests that one way of advancing the state-of-the-art in this field is by reducing the amount of human intervention in the dialogue design process through higher degrees of automation, i.e. by moving towards truly autonomous learning.\n1Computer Science at Heriot-Watt University, Edinburgh, Scotland, e-mail: hc213@hw.ac.uk\n1\nar X\niv :1\n60 1.\n04 57\n4v 1\n[ cs\n.A I]\n1 8\nJa n\n2 Heriberto Cuaya\u0301huitl1\nRecent advances in machine learning have proposed machine learning methods as a way to reduce human intervention in the creation of intelligent agents. In particular, the field of Deep Reinforcement Learning (DRL) targets feature learning and policy learning simultaneously\u2014which reduces the effort in feature engineering [1]. This is relevant because the vast majority of previous RL-based dialogue systems make use of carefully engineered features to represent the dialogue state [2].\nMotivated by the advantages of DRL methods over traditional RL methods, in this paper we present an extended dialogue system, recently applied to strategic dialogue management[3], that makes use of raw noisy text\u2014without any engineered features to represent the dialogue state. By using this representation, the dialogue system does not require a Spoken Language Understanding (SLU) component. We bypass SLU by learning dialogue policies directly from (simulated) speech recognition outputs. The rest of the paper describes a proof of concept system which is trained based on this idea."}, {"heading": "2 Deep Reinforcement Learning for Dialogue Control", "text": "A Reinforcement Learning (RL) agent learns its behaviour from interaction with an environment and the physical or virtual agents within it, where situations are mapped to actions by maximising a long-term reward signal [4]. An RL agent is typically characterised by: (i) a finite or infinite set of states S = {si}; (ii) a finite or infinite set of actions A = {a j}; (iii) a state transition function T (s,a,s\u2032) that specifies the next state s\u2032 given the current state s and action a; (iv) a reward function R(s,a,s\u2032) that specifies the reward given to the agent for choosing action a in state s and transitioning to state s\u2032; and (v) a policy \u03c0 : S\u2192 A that defines a mapping from states to actions. The goal of an RL agent is to select actions by maximising its cumulative discounted reward defined as Q\u2217(s,a) = max\u03c0 E[rt + \u03b3rt+1 + \u03b32rt+1 + ...|st = s,at = a,\u03c0], where function Q\u2217 represents the maximum sum of rewards rt discounted by factor \u03b3 at each time step. While the RL agent takes actions with probability Pr(a|s) during training, it takes the best actions maxa Pr(a|s) at test time.\nTo induce the Q function above we use Deep Reinforcement Learning as in [1], which approximates Q\u2217 using a multilayer convolutional neural network. The Q function of a DRL agent is parameterised as Q(s,a;\u03b8i), where \u03b8i are the parameters (weights) of the neural net at iteration i. More specifically, training a DRL agent requires a dataset of experiences Dt = {e1, ...et} (also referred to as \u2018experience replay memory\u2019), where every experience is described as a tuple et = (st ,at ,rt ,st+1). The Q function can be induced by applying Q-learning updates over minibatches of experience MB = {(s,a,r,s\u2032)\u223cU(D)} drawn uniformly at random from dataset D. A Q-learning update at iteration i is thus defined as the loss function Li(\u03b8i) = EMB [ (r+ \u03b3 maxa\u2032Q(s\u2032,a\u2032;\u03b8 i)\u2212Q(s,a;\u03b8i))2 ] , where \u03b8i are the parameters of the neural net at iteration i, and \u03b8 i are the target parameters of the neural net at iteration i. The latter are only updated every C steps. This process is implemented in the learning algorithm Deep Q-Learning with Experience Replay described in [1].\nSimpleDS: A Simple Deep Reinforcement Learning Dialogue System 3\n3 The SimpleDS Dialogue System\nFigure 1 shows a high-level diagram of the SimpleDS dialogue system. At the bottom, the learning environment receives an action (dialogue act) and outputs the next environment state and numerical reward. To do that, the environment first generates the word sequence of the last system action, the user simulator generates a word sequence as a response to that action, and the user response is distorted given some noise level and word-level confidence scores. Based on the system\u2019s verbalisation and noisy user response, the next dialogue state and reward are calculated and given as a result of having executed the given action. At the top of the diagram, a Deep Reinforcement Learning (DRL) agent receives the state and reward, updates its policy during learning, and outputs an action according to its learnt policy.\nThis system runs under a client-server architecture, where the environment acts as the server and the learning agent acts as the client. They communicate by exchanging messages, where the client tells the server the action to execute, and the server tells the client the dialogue state and reward observed. The SimpleDS learning agent is based on the ConvNetJS tool [5], which implements the algorithm \u2018Deep QLearning with experience replay\u2019 proposed by [1]. We extended this tool to support multi-threaded and client-server processing with constrained search spaces.1\nThe state space includes up to 100 word-based features depending on the vocabulary of the SimpleDS agent in the restaurant domain. The initial release of SimpleDS provides support for English, German and Spanish. While words derived from system responses are treated as binary variables (i.e. word present or absent), the words derived from noisy user responses can be seen as continuous variables by taking\n1 The code of SimpleDS is available at https://github.com/cuayahuitl/SimpleDS\n4 Heriberto Cuaya\u0301huitl1\nconfidence scores into account. Since we use a single variable per word, user features override system ones in case of overlaps.\nThe action space includes 35 dialogue acts in the Restaurant domain2. They include 2 salutations, 9 requests, 7 apologies, 7 explicit confirmations, 7 implicit confirmations, 1 retrieve information, and 2 provide information. Rather than learning with whole action sets, SimpleDS supports learning from constrained actions by applying Q-learning learning updates only on the set of valid actions. The constrained actions come from the most likely actions (e.g. Pr(a|s) > 0.01) with probabilities derived from a Naive Bayes classifier trained from example dialogues. The latter are motivated by the fact that a new system does not have training data apart from a small number of demonstration dialogues. In addition to the most probable data-like actions, the constrained action set is extended with the legitimate requests, apologies and confirmations in state s. The fact that constrained actions are data-driven and driven by application independent heuristics facilitates its usage across domains.\nThe state transition function is based on a numerical vector representing the last system and user responses. The former are straightforward, 0 if absent and 1 if present. The latter correspond to the confidence level [0..1] of noisy user responses. Given that SimpleDS targets a simple and extensible dialogue system, it uses templates for language generation, a rule-based user simulator, and confidence scores generated uniformly at random (words with scores under a threshold were distorted).\nThe reward function is motivated by the fact that human-machine dialogues should confirm the information required and that interactions should be human-like. It is defined as R(s,a,s\u2032)= (CR\u00d7w)+(DR\u00d7(1\u2212w))\u2212DL, where CR is the number of positively confirmed slots divided by the slots to confirm; w is a weight over the confirmation reward (CR), we used w=0.5; DR is a data-like probability of having observed action a in state s, and DL is used to encourage efficient interactions, we used DL=0.1. The DR scores are derived from the same statistical classifier above, which allows us to do statistical inference over actions given states (Pr(a|s)).\nThe model architecture consists of a fully-connected multilayer neural net with up to 100 nodes in the input layer (depending on the vocabulary), 40 nodes in the first hidden layer, 40 nodes in the second hidden layer, and 35 nodes (action set) in the output layer. The hidden layers use Rectified Linear Units to normalise their weights [6]. Finally, the learning parameters are as follows: experience replay size=10000, discount factor=0.7, minimum epsilon=0.01, batch size=32, learning steps=20000. A comprehensive analysis comparing multiple state representations, action sets, reward functions and learning parameters is left as future work.\nFigure 2 shows the learning curve of a SimpleDS agent using 3000 simulated dialogues. This agent uses a smaller set of actions per state (between 4 and 5 actions)\n2 Actions: Salutation(greeting), Request(hmihy), Request(food), Request(price), Request(area), Request(food, price), Request(food, area), Request(price, area), Request(food, price, area), AskFor(more), Apology(food), Apology(price), Apology(area), Apology(food, price), Apology(food, area), Apology(price, area), Apology(food, price, area), ExpConfirm(food), ExpConfirm(price), ExpConfirm(area), ExpConfirm(food, price), ExpConfirm(food, area), ExpConfirm(price, area), ExpConfirm(food, price, area), ImpConfirm(food), ImpConfirm(price), ImpConfirm(area), ImpConfirm(food,price), ImpConfirm(food, area), ImpConfirm(price, area), ImpConfirm(food, price, area), Retrieve(info), Provide(unknown), Provide(known), Salutation(closing).\nSimpleDS: A Simple Deep Reinforcement Learning Dialogue System 5\nrather than the whole action set per state\u2014according to the application-independent heuristics mentioned in the previous Section. This reduction has the advantage that policies can be learnt quicker, that more sensible dialogues can potentially be learnt, and that it is inherent that some domains make use of legitimate actions during the interaction. In the case of more complex systems, with higher amounts of features and actions, learning with valid actions (rather than all actions) can make a huge difference in terms of computational resources and learning time. The quality of the learnt policies will depend on the learning environment and given constraints.\nTable 1 shows an example dialogue of the learnt policy with user inputs derived from simulated speech recognition results. Our initial tests suggest that reasonable interactions can be generated using the proposed learning approach. SimpleDS has been demonstrated on a mobile App, and a human evaluation is left as future work."}, {"heading": "4 Summary", "text": "We describe a publicly available dialogue system motivated by the idea that future dialogue systems should be trained with almost no intervention from system developers. In contrast to previous reinforcement learning dialogue systems, SimpleDS selects dialogue actions directly from raw (noisy) text of the last system and user responses. It remains to be demonstrated how far one can go with such an approach. Future work includes to (a) compare different model architectures, training parameters and reward functions; (b) extend or improve the abilities of the proposed dialogue system; (c) train deep learning agents in other (larger scale) domains [7, 8, 9]; (d) evaluate end-to-end systems with real users; (e) compare or combine different types of neural nets [10]; and (e) perform fast learning based on parallel computing.\n6 Heriberto Cuaya\u0301huitl1"}, {"heading": "Acknowledgments", "text": "Funding from the European Research Council (ERC) project \u201cSTAC: Strategic Conversation\u201d no. 269427 is gratefully acknowledged."}], "references": [{"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Automating spoken dialogue management design using machine learning: An industry perspective", "author": ["T. Paek", "R. Pieraccini"], "venue": "Speech Communication 50(8-9)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Strategic dialogue management via deep reinforcement learning", "author": ["H. Cuay\u00e1huitl", "S. Keizer", "O. Lemon"], "venue": "NIPS Deep Reinforcement Learning Workshop.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": "Morgan and Claypool Pub.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "ConvNetJS: A javascript library for training deep learning models", "author": ["A. Karpathy"], "venue": "https://github.com/karpathy/convnetjs", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation of a hierarchical reinforcement learning spoken dialogue system", "author": ["H. Cuay\u00e1huitl", "S. Renals", "O. Lemon", "H. Shimodaira"], "venue": "Computer Speech & Language 24(2)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Spatially-aware dialogue control using hierarchical reinforcement learning", "author": ["H. Cuay\u00e1huitl", "N. Dethlefs"], "venue": "TSLP 7(3)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonstrict hierarchical reinforcement learning for interactive systems and robots", "author": ["H. Cuay\u00e1huitl", "I. Kruijff-Korbayov\u00e1", "N. Dethlefs"], "venue": "TiiS 4(3)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A.W. Senior", "H. Sak"], "venue": "ICASSP.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In particular, the field of Deep Reinforcement Learning (DRL) targets feature learning and policy learning simultaneously\u2014which reduces the effort in feature engineering [1].", "startOffset": 170, "endOffset": 173}, {"referenceID": 1, "context": "This is relevant because the vast majority of previous RL-based dialogue systems make use of carefully engineered features to represent the dialogue state [2].", "startOffset": 155, "endOffset": 158}, {"referenceID": 2, "context": "Motivated by the advantages of DRL methods over traditional RL methods, in this paper we present an extended dialogue system, recently applied to strategic dialogue management[3], that makes use of raw noisy text\u2014without any engineered features to represent the dialogue state.", "startOffset": 175, "endOffset": 178}, {"referenceID": 3, "context": "A Reinforcement Learning (RL) agent learns its behaviour from interaction with an environment and the physical or virtual agents within it, where situations are mapped to actions by maximising a long-term reward signal [4].", "startOffset": 219, "endOffset": 222}, {"referenceID": 0, "context": "To induce the Q function above we use Deep Reinforcement Learning as in [1], which approximates Q\u2217 using a multilayer convolutional neural network.", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "the learning algorithm Deep Q-Learning with Experience Replay described in [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 4, "context": "The SimpleDS learning agent is based on the ConvNetJS tool [5], which implements the algorithm \u2018Deep Q-", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "Learning with experience replay\u2019 proposed by [1].", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "The hidden layers use Rectified Linear Units to normalise their weights [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "Future work includes to (a) compare different model architectures, training parameters and reward functions; (b) extend or improve the abilities of the proposed dialogue system; (c) train deep learning agents in other (larger scale) domains [7, 8, 9]; (d) evaluate end-to-end systems with real users; (e) compare or combine different types of neural nets [10]; and (e) perform fast learning based on parallel computing.", "startOffset": 241, "endOffset": 250}, {"referenceID": 7, "context": "Future work includes to (a) compare different model architectures, training parameters and reward functions; (b) extend or improve the abilities of the proposed dialogue system; (c) train deep learning agents in other (larger scale) domains [7, 8, 9]; (d) evaluate end-to-end systems with real users; (e) compare or combine different types of neural nets [10]; and (e) perform fast learning based on parallel computing.", "startOffset": 241, "endOffset": 250}, {"referenceID": 8, "context": "Future work includes to (a) compare different model architectures, training parameters and reward functions; (b) extend or improve the abilities of the proposed dialogue system; (c) train deep learning agents in other (larger scale) domains [7, 8, 9]; (d) evaluate end-to-end systems with real users; (e) compare or combine different types of neural nets [10]; and (e) perform fast learning based on parallel computing.", "startOffset": 241, "endOffset": 250}, {"referenceID": 9, "context": "Future work includes to (a) compare different model architectures, training parameters and reward functions; (b) extend or improve the abilities of the proposed dialogue system; (c) train deep learning agents in other (larger scale) domains [7, 8, 9]; (d) evaluate end-to-end systems with real users; (e) compare or combine different types of neural nets [10]; and (e) perform fast learning based on parallel computing.", "startOffset": 355, "endOffset": 359}], "year": 2016, "abstractText": "This paper presents SimpleDS, a simple and publicly available dialogue system trained with deep reinforcement learning. In contrast to previous reinforcement learning dialogue systems, this system avoids manual feature engineering by performing action selection directly from raw text of the last system and (noisy) user responses. Our initial results, in the restaurant domain, report that it is indeed possible to induce reasonable behaviours with such an approach that aims for higher levels of automation in dialogue control for intelligent interactive agents.", "creator": "LaTeX with hyperref package"}}}