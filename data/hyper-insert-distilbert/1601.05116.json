{"id": "1601.05116", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2016", "title": "A Theory of Local Matching: SIFT and Beyond", "abstract": "why, has temporal sift been actually so successful? unclear why its rapid extension, notably dsp - sift, can further sufficiently improve evolutionary sift? is but there a theory that can truly explain both? then how adequately can such simpler theory benefit real problem applications?? can could it suggest why new algorithms with reduced computational complexity efficiently or new functional descriptors compared with better accuracy for matching? we could construct essentially a less general theory of local descriptors for visual matching. our theory ultimately relies simply on alternative concepts in energy minimization principle and heat differential diffusion. we show that dynamic sift coordinates and dsp - differential sift solutions approximate the correct solution the theory nicely suggests. in particular, dsp - sift gives almost a possibly better gradient approximation response to modify the theoretical solution solution ; more justifying much why dsp - sift outperforms sift. using the considerably developed continuum theory, we derive from new constrained descriptors that do have exactly fewer parameters and are arguably potentially better in handling affine deformations.", "histories": [["v1", "Tue, 19 Jan 2016 22:15:48 GMT  (985kb,D)", "http://arxiv.org/abs/1601.05116v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hossein mobahi", "stefano soatto"], "accepted": false, "id": "1601.05116"}, "pdf": {"name": "1601.05116.pdf", "metadata": {"source": "CRF", "title": "A Theory of Local Matching SIFT and Beyond", "authors": ["Hossein Mobahi", "Stefano Soatto"], "emails": ["hmobahi@csail.mit.edu", "soatto@cs.ucla.edu"], "sections": [{"heading": "1 Introduction", "text": "Questions: Why has SIFT been so successful? Why DSP-SIFT [Dong and Soatto, 2015] can further improve SIFT? Is there a theory that can explain both? How can such theory benefit real applications? Can it suggest new algorithms with reduced computational complexity or new descriptors with better accuracy for matching?\nContributions: We construct a general theory of local descriptors for visual matching. Our theory relies on concepts in energy minimization and heat diffusion. We show that SIFT and DSP-SIFT approximate the solution the theory suggests. In particular, DSP-SIFT gives a better approximation to the theoretical solution; justifying why DSP-SIFT outperforms SIFT. We derive new algorithms based on this theory. Specifically, we present a computationally efficient approximation to DSP-SIFT algorithm [Dong and Soatto, 2015] by replacing the sampling procedure in DSP-SIFT with a closed-form approximation that does not need any sampling. This leads to a significantly faster\nar X\niv :1\n60 1.\n05 11\n6v 1\n[ cs\n.C V\n] 1\n9 Ja\nalgorithm compared to DSP-SIFT. In addition, we derive new descriptors directly from this theory. The new descriptors have fewer parameters as well as the potential of better handling affine deformations, compared to SIFT and DSP-SIFT."}, {"heading": "2 Contributions", "text": "Throughout this text, isotropic multivariate Gaussian kernel and periodic univariate Gaussian are denoted by k and k\u0303,\nk\u03c3(x) , (2\u03c0\u03c3 2)\u2212 dim(x) e\u2212\n\u2016x\u20162 2\u03c32 , k\u0303\u03c3(\u03c6) , \u221e\u2211\nk=\u2212\u221e\nk\u03c3(\u03c6+ 2\u03c0k) . (1)\nConsider an image f . Given an origin-centered detected key point x with assigned scale \u03c3 and orientation \u03b2. The continuous form of a SIFT descriptor can be expressed as [Dong et al., 2015, Vedaldi and Fulkerson, 2010],\nhSIFT (\u03b2,x) , \u222b X k\u0303\u03c3r (\u03b2 \u2212 \u2220\u2207f(y)) k\u03c3d(y \u2212 x) \u2016\u2207f(y)\u2016 dy , (2)\nwhere \u03c3r resembles the size of each orientation bin, e.g. 2\u03c0 8 for 8 bins. \u03c3d determines the spatial support of the descriptor as a function of \u03c3, e.g. \u03c3d , 3\u03c3. By observing that the above descriptor is pooling (weighted averaging) across displacement, [Dong et al., 2015] adds domain size pooling to this construction and suggests, hDSP (\u03b2,x) , \u222b S \u222b X k\u0303\u03c3r (\u03b2\u2212\u2220\u2207f(y)) k\u03c3d(y\u2212x) \u2016\u2207f(y)\u2016 dyk\u03c3s(\u03c3d\u2212\u03c3d0) d\u03c3d ,\n(3) where S , R and \u03c3d0 is a function of key point\u2019s scale \u03c3, e.g. \u03c3d0 , 3\u03c3. We develop a theory for descriptor construction by returning to the origin of the problem. Specifically we formulate matching as an energy optimization problem. It is known that the resulted cost function is nonconvex for a any realistic matching setup [Dong and Soatto, 2015]. Ideally, one would need to brute-force search across all possible transformations to find the right match. This is obviously not practical.\nRecently a theory of nonconvex optimization by heat diffusion has been proposed [Mobahi and Fisher III, 2015a, Mobahi and Fisher III, 2015b]. The theory offers the best (in a certain sense) tractable solution for nonconvex problems. We show that SIFT and DSP-SIFT approximate the energy minimization solution that this theory suggests. By leveraging this connection, we present the following contributions.\nThe domain-size integration (3) is approximated by numerical sampling in [Dong et al., 2015], which is slow. Instead, we propose the following two closed-form approximations to this integral.\nhDSP (\u03b2,x) \u2248 \u222b X k\u0303\u03c3r (\u03b2\u2212\u2220\u2207f(y)) k\u03c3d(y\u2212x) \u2016\u2207f(y)\u2016 \u03c3d\u221a \u03c32d + \u2016x\u20162\u03c32s e \u03c32s(\u2016x\u2016 2\u2212xT y\u2212\u03c32d) 2 2\u03c32 d (\u03c32 d +\u2016x\u20162\u03c32s) .\n(4)\nhDSP (\u03b2,x) \u2248 \u222b X k\u0303\u03c3r (\u03b2 \u2212 \u2220\u2207f(y)) k\u03c3d(y \u2212 x) \u2016\u2207f(y)\u2016 \u03c32d \u2212 \u03c3sxTy (\u03c32d + \u03c3 2 s\u2016x\u20162) 3 2 e \u2212\u03c3 2 d\u2016x+y\u2016 2+\u03c32s(x T y\u22a5)2 2\u03c32 d (\u03c32 d +\u03c32s\u2016x\u2016 2) . (5)\nIn addition, through this theory, we propose a new descriptor. This descriptor is exact in terms of what this theory suggests. In addition, this descriptor is derived from an affine matching formulation, hence may better tolerate affine transforms than SIFT and DSP-SIFT1. Interestingly, despite handling a broader transformation space, it has fewer parameters than DSP-SIFT. Finally, it is analytical and does not need any sampling.\nhheat(\u03b2,x) , \u222b X e \u2212 (y T \u2207\u0303f(y))2 2\u03c32 d w ( \u2212 12t \u2207\u0303 T f(y) (\u03c3\u22122d yx T + \u03c3\u22122a I ) v\u0303(\u03b2,y) ) \u2016\u2207f(y)\u20162 t3\n\u00d7k\u221a \u03c32d+\u03c3 2 a \u2016x\u20162\n( (\u2207f(y))T (x\u2212 y)\u22a5\n\u2016\u2207f(y)\u2016 ) dy , (6)\nwhere \u2207\u0303f(y) , \u2207f(y)\u2016\u2207f(y)\u2016 , v\u0303(\u03b2,y) , (cos(\u03b2),sin(\u03b2)) \u2016\u2207f(y)\u2016 , t ,\n\u221a (xT v\u0303(\u03b2,y))2\n2\u03c32d + 12\u03c32a \u2016\u2207f(y)\u20162 ,\nw(x) , \u221a \u03c0ex 2\n(1 + 2x2) erfc(x)\u2212 2x, and (a, b)\u22a5 , (b,\u2212a). Note that compared to DSP-SIFT, this descriptor reduces number of parameters from two (\u03c3r and \u03c3s) to one (\u03c3a). An illustration of how hheat differs from hSIFT is as follows. Consider a pair of images, namely image 1 and image 2, each consisting of two patches returned by some key point detector. The goal is to establish correspondence between patches using the `2 distance between normalized descriptors, d(h1 , h2) , \u222b 2\u03c0\n0 \u222b X ( h1(\u03b2,x)( \u222b 2\u03c0 0 \u222b X h 2 1(\u03b2 \u2020,x\u2020) dx\u2020 d\u03b2\u2020 ) 1 2 \u2212 h2(\u03b2,x)( \u222b 2\u03c0 0 \u222b X h 2 2(\u03b2 \u2020,x\u2020) dx\u2020 d\u03b2\u2020 ) 1 2 )2 dx d\u03b2 ,\n(7) where X , X1 \u2229X2. There are two possible matches: P 1A \u2194 P 2A \u2227 P 1B \u2194 P 2B or P 1A \u2194 P 2B \u2227 P 1B \u2194 P 2A; obviously only the former is correct. Distance of matches using SIFT are listed in table 2. Note that SIFT descriptor attains lower distance for the wrong match and thus fails, while the heat descriptor finds the correct match. A visualization of SIFT descriptor and heat descriptor are presented in Figures 2 and 3 respectively.\n1SIFT descriptor gains robustness against displacement by pooling across it. DSP-SIFT gains further robustness against scaling by scale pooling. However, none is robust to affine transform, which would require pooling across more parameters.\nAlthough this work focuses on SIFT, our diffusion theory can possible relate and extend other descriptors as well. For example, the recently developed distribution fields [Mears et al., 2013] are similar to (2) and (3), except that instead of histogram of gradient orientation, the histogram of intensity values are used,\nhDF (l,x) , \u222b X k\u03c3l(l \u2212 f(y)) k\u03c3d(y \u2212 x) dy , (8)\nwhere \u03c3l determines the smoothing strength of pixel intensity values. Similar to SIFT arguments, the convolution k\u03c3d may correspond to diffusion w.r.t. translation, and thus diffusion w.r.t. larger class of transformation, e.g., affine, may lead to geometrically more robust descriptors. Such extensions of distribution fields are not studied in the report, but are subject of future research."}, {"heading": "3 Matching as Energy Minimization", "text": "For clarity of presentation, we focus on a restricted matching setup with simplifying assumptions. Nevertheless, this setup has enough complexity to make the point on nonconvexity and diffusion."}, {"heading": "3.1 Problem Setup", "text": "Notation: An image is a map of form f : X \u2192 [0, 1], where X \u2282 R2. Similarly, a patch is p : P \u2192 [0, 1], where P \u2286 X , i.e. the map is defined over a subset of the domain X .\nAssumptions: Given a set of patches pk : Xk \u2192 [0, 1] for k = 1, . . . , n. We assume that one of these patches, indexed by k\u2217, appears somewhere in f up to\na geometric transformation \u03c4\u2217 : Xk\u2217 \u2192 X and some reasonable intensity noise2, \u2203(k\u2217, \u03c4\u2217) \u2200x \u2208 Xk\u2217 ; f ( \u03c4\u2217(x) ) \u2248 pk\u2217(x) . (9)\nObjective: The goal is to estimate (k\u2217, \u03c4\u2217). For tractability, the space of \u03c4 is parameterized by a vector \u03b8. For mathematical convenience, we assume the noise effect is best minimized via `2 discrepancy,\n(k\u2217,\u03b8\u2217) , argmin (k,\u03b8) \u222b Xk ( f ( \u03c4(x ; \u03b8) ) \u2212 pk(x) )2 dx . (10)\nThe tools we later use apply to continuous variables, while (10) involves the integer variable k. However, we can equivalently rewrite the problem in the following continuous form,\n(c\u2217,\u03b8\u2217) , argmin (c,\u03b8) \u2211 k c2k \u222b Xk ( f ( \u03c4(x ; \u03b8) ) \u2212 pk(x) )2 dx\ns.t \u2211 k ck = 1 , \u2200k ; ck(1\u2212 ck) = 0 . (11)"}, {"heading": "3.2 Intractability", "text": "Despite simplicity of the setup, estimation of (k\u2217,\u03b8\u2217) is generally intractable because the optimization problem (11) is nonconvex. Hence, local optimization methods may converge to a local minimum. In the following, we illustrate this by a toy example. The example involves a univariate signal f(x), a pair of univariate templates p1(x) and p2(x) and a translation transform \u03c4 so that f(\u03c4(x, \u03b8)) , f(x \u2212 \u03b8). Thus, (11) can be expressed as below, after eliminating c2 by the equality constraint c1 + c2 = 1,\n(c\u22171, \u03b8 \u2217) , argmin\n(c1,\u03b8)\nc21 \u222b X1 ( f(x\u2212 \u03b8)\u2212 p1(x) )2 dx+ (1\u2212 c1)2 \u222b X2 ( f(x\u2212 \u03b8)\u2212 p2(x) )2 dx\ns.t c1(1\u2212 c1) = 0 . (12)\nThe solution c\u22171 determines to which template f belongs to; p1 if c \u2217 1 = 1 and\np2 if c \u2217 1 = 0.\nWe proceed by choosing f , p1, and p2 as the blue, green, and red curves in Figure 4-a. Here X = [\u22122, 2] and X1 = X2 = [\u22121.2, 1.2]. The goal is to slide the blue curve to the left or right, such that it coincides with either the green or red curve. Recall from (11) that matching error is examined only over the support of the templates (gray shade). As shown in Figure 4-b, by sliding f to the left by \u03b8 = 0.25 units, a perfect match with the green curve is achieved. However, there is no way to attain similar match with the red curve. Thus, by inspection we know that c\u2217 = (1, 0) and \u03b8 = 0.25.\n2In this setting each patch pk may be called a template.\nFor visualization purpose, we replace the equality in (12) by a quadratic penalty3. This encompasses both the objective and constraint into a single objective to be visualized. The resulted optimization landscape is shown in Figure 5. A local minimum is apparent around c1 = 0, \u03b8 = 0 while the global minimum is around c1 = 1, \u03b8 = 0.25.\n4 Diffusion\nOne way to approximate the solution of a nonconvex optimization problem is by diffusion and the continuation method. The idea is to follow the minimizer of the diffused cost function while progressively transforming that function to the original nonconvex cost. It has recently been shown that, this procedure with the choice of the heat kernel as the diffusion operator, provides the optimal transformation in a certain sense4 [Mobahi and Fisher III, 2015a]. In\n3Similar local minima could be obtained for the exact constrained optimization (12) using Lagrange multiplier technique.\n4It is shown that Gaussian convolution is resulted by the best affine approximation to a nonlinear PDE that generates the convex envelope. Note that computing the convex envelope of a function is generally intractable as well. Thus, it is not surprising that the associated nonlinear PDE lacks a closed form solution. However, by replacing the nonlinear PDE by its best affine approximation, we strike the optimal balance between tractability (closed form solution for the linear PDE) and accuracy of the approximation. The motivation for approximation the convex envelope is that the latter is an optimal object in several senses for the original nonconvex cost function. In particular, global minima of a nonconvex cost are contained in the global minima of its convex envelope.\nAlgorithm 1 Optimization by Diffusion and Continuation\nfact, some performance guarantees have been recently developed for this scheme [Mobahi and Fisher III, 2015b]. The procedure is defined more formally below. Given an unconstrained and nonconvex cost function h : Rn \u2192 R to be minimized. Instead of applying a local optimization algorithm directly to h, we embed h into a family of functions parameterized by \u03c3,\ng(x ; \u03c3) , [h ? k\u03c3](x) , (13)\nwhere ? is the convolution operator and k\u03c3(x) is the Gaussian function with zero mean and covariance \u03c32I. The Gaussian convolution appears here due to the known analytical solution form of the heat diffusion. Observe that lim\u03c3\u2192\u221e g( . ;\u03c3) = h(x). Thus by starting from a large \u03c3 and shrinking it toward zero, a sequence of cost function converging to h is obtained. The optimization process then follows the path of the minimizer of g( . ;\u03c3) through this sequence as listed in Algorithm 1.\nNow let us revisit the problem (12). Like before, we use a quadratic penalty to obtain an unconstrained approximate to (12). A sequence of diffused landscapes of this problem is shown in Figure 6. Note that the problem becomes convex, with a unique strict minimizer at the large \u03c3. The solution path originated from that point eventually lands at the global minimum in this example."}, {"heading": "5 Deriving SIFT via the Diffusion Theory", "text": "Instead of pixel intensity as (11) to guide the matching, we switch to orientation of gradient. This change adds limited robustness to illumination changes [Dong and Soatto, 2015]. Nevertheless, the cost function remains nonconvex\nand difficult to minimize. Such nonconvex optimization may be treated via diffusion and continuation by the theory of [Mobahi and Fisher III, 2015a]. We show that SIFT descriptor emerges as an approximation to this process when \u03c4 is a similarity transformation, i.e. \u03c4(x ; \u03b8) , esR\u03b1x+ b, where \u03b8 , (\u03b1, s, b).\nThe approximation comes from two sources. First, the theory of [Mobahi and Fisher III, 2015a] suggests a continuation method by gradually reducing \u03c3 while following the path of the minimizer. SIFT provides an approximation to this process by solving the optimization at only one value of \u03c3, i.e. it terminates after the first iteration of the algorithm suggested by [Mobahi and Fisher III, 2015a]. Second, the cost function is diffused only w.r.t. a subset of optimization variables (\u03b1 and b). This deviates from the theory in [Mobahi and Fisher III, 2015a] that requires diffusion of the cost function in all variables, i.e. to use [cost ? k\u03c3](c,\u03b8)."}, {"heading": "5.1 Energy Function", "text": "Define the density of gradient orientations of image f as below,\nh(\u03b2,x ; f) ,X(\u2220\u2207f(x)\u2212 \u03b2) \u2016\u2207f(x)\u2016 , (14) where X denotes the Dirac comb of period 2\u03c0, i.e. X(x) , \u2211\u221e n=\u2212\u221e \u03b4(x+ 2\u03c0n). The Dirac comb accounts for the periodicity of the angle (gradient orientation). Let the dissimilarity between a pair of density functions over a region X be expressed as the negated dot product,\nd(f1, f2,X ) , \u2212 \u222b 2\u03c0\n0 \u222b X h(\u03b2,x ; f1) h(\u03b2,x ; f2) d\u03b2 dx . (15)\nIn this setting, the problem of template matching is to find \u03b8 such that the gradient orientations of f(\u03c4(x ; \u03b8)) match that of a template,\n(c\u2217,\u03b8\u2217) , argmin (c,\u03b8) \u2211 k ck d(f \u25e6 \u03c4\u03b8, pk,Xk)\ns.t \u2211 k ck = 1 , \u2200k ; ck(1\u2212 ck) = 0 . (16)\nReplacing the equality constraint by some penalty function q leads to the following unconstrained optimization,\ncost(c,\u03b8) , q(c) + \u2211 k ck d(f \u25e6 \u03c4\u03b8, pk,Xk) . (17)"}, {"heading": "5.2 Solution", "text": "The goal is to tackle the nonconvex problem (17) using the diffusion and continuation theory of [Mobahi and Fisher III, 2015a]. This would give the algorithm listed in Table 5.2-left.\nSIFT based matching can be derived by simplifying this algorithm as described below,\n\u2022 Partial Diffusion: Instead of diffusion w.r.t. all variables (\u03b1, s, b, c), diffuse the energy function (17) partially, i.e. only with respect to (\u03b1, b).\n\u2022 Fixed \u03c3: Instead of gradual refinement of the energy function by shrinking \u03c3 toward zero, stick to a single choice \u03c3 = \u03c30.\n\u2022 Limited Optimization: Rather than searching the entire parameter space for (\u03b1, s, b) for the optimal solution, restrict to a small candidate set \u0398 , \u222aJj=1{(\u03b1j , sj , bj)}. This set is generated outside of the optimization loop by a keypoint detector5. Consequently, \u0398 does not necessarily contain the optimal parameter as keypoint estimation is done for each image in isolation and thus separately from the full matching problem.\nApplying these simplifications yields the algorithm in Table 5.2-middle. The central optimization in this algorithm is the following,\nmin (\u03b1,s,b)\u2208\u0398 min c\n[ [cost ( c, ( . , s, . ) ) ? k\u03c3d ](b) ? k\u0303\u03c3r ] (\u03b1) , (18)\nwhere we have replaced the joint optimization minc,(\u03b1,s,b)\u2208\u0398 by the equivalent nested form min(\u03b1,s,b)\u2208\u0398 minc. The outer minimization is trivial; it just loops over the candidates and evaluates the resulted cost to pick the best one. Below we only focus on the inner optimization.\nAssuming the penalty function q(c) accurately enforces the constraint ck \u2208 {0, 1}, the inner optimization becomes a winner take all problem; the winning patch pk to match f is the one which minimizes the following cost,\n5The location and scale of candidate sets are determined by an interest point detector, and the orientation angle is set to the dominant gradient direction.\nk\u2217 = argmin k\n[ [d(f \u25e6 \u03c4( . ,s, . ), pk,Xk) ? k\u03c3d ](b) ? k\u0303\u03c3r ] (\u03b1) (19)\n= argmax k \u222b 2\u03c0 0 \u222b Xk ([ [h(\u03b2,x ; f \u25e6 \u03c4( . ,s, . )) ? k\u03c3d ](b) ? k\u0303\u03c3r ] (\u03b1) ) h(\u03b2,x ; pk) d\u03b2 dx .\nWe doubt that the convolutions in (19) are computationally tractable6. Thus, in order to derive a computationally tractable algorithm, we resort to a closed form approximation to the above convolutions. The approximation is stated in the following lemma.\nLemma 1 The following approximation holds,\n[ [h(\u03b2,x ; f \u25e6 \u03c4( . ,s, . )) ? k\u03c3d ](b) ? k\u0303\u03c3r ] (\u03b1)\n\u2248 \u2212 es \u222b R2 k\u0303\u03c3r (\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2)\u2016\u2207f(y)\u2016 k\u03c3d(y \u2212 esR\u03b1x\u2212 b) dy .\nProof See Appendix B for the proof.\nUsing this lemma, the computationally intractable optimization (18) is replaced by the following tractable approximation,\nmax (\u03b1,s,b)\u2208\u0398 max k\nes \u222b 2\u03c0\n0 \u222b Xk \u222b R2 k\u0303\u03c3r (\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2)\u2016\u2207f(y)\u2016 k\u03c3d(y \u2212 esR\u03b1x\u2212 b) dy\n\u00d7h(\u03b2,x ; pk) dx d\u03b2 , (20)\nIn the inner optimization, since (\u03b1, s, b) is fixed (to some (\u03b1j , sj , bj)), the image f can be warped prior to optimization by \u03c4( . ;\u03b1j , sj , bj). This allows optimization w.r.t. k to be performed for \u03c4 being the identity transform (because the effect of (\u03b1j , sj , bj) is already taken care of by the warp) 7, i.e. (\u03b1 = 0, s = 0, b = 0). Denoting the warped f due to (\u03b1j , sj , bj) by fj , f \u25e6 \u03c4\u03b8j , the inner optimization simplifies,\nmax k \u222b 2\u03c0 0 \u222b Xk \u222b R2 k\u0303\u03c3r (\u2220\u2207fj(y)\u2212 \u03b2)\u2016\u2207fj(y)\u2016 k\u03c3d(y \u2212 x) dy\ufe38 \ufe37\ufe37 \ufe38\nhSIFT (\u03b2,x ; fj)\nh(\u03b2,x ; pk) dx d\u03b2 ,(21)\n6We will later show in Section 7.2 that by a different parameterization of the geometric transform, we can handle a larger class, namely the affine transform, and yet are able to derive a closed form expression for the convolution integrals.\n7In this section we do not consider the full optimization loop (shrinking \u03c3). However, if we wanted to do so, the idea of 1.Gradual reduction of the blur \u03c3 and 2.Warping by the current estimate of the geometric transform in each iteration, would lead to a LucasKanade[Lucas and Kanade, 1981] type algorithm. However, the resulted algorithm performs gradient density matching instead of Lucas-Kanade that relies on pixel intensity matching.\nPart of the computation involving fj is independent of pk and can be precomputed. This precomputed result in fact provides a new representation for fj that matches the definition of hSIFT in (2). This gives the algorithm presented in Table 5.2-Right."}, {"heading": "6 Deriving DSP-SIFT via the Diffusion Theory", "text": "Here we show that the DSP-SIFT descriptor also relates to partial diffusion of the cost function. Specifically, this descriptor can be derived by considering the diffusion w.r.t. the transformation parameters (\u03b1, s, b). Note that this involves more of the optimization variables in diffusion compared to SIFT (which diffuses w.r.t. (\u03b1, b)), and thus provides a better approximation to the theory of [Mobahi and Fisher III, 2015a] which suggests the diffusion must be applied to all optimization variables, i.e. to (c,\u03b8). This improvement in approximation fidelity could be an explanation of why DSP-SIFT works better than SIFT in practice. However, it still misses diffusion of c.\nThe derivation is quite similar to that of SIFT in Section 5. The is to minimize the same energy function in (17). However, on top of diffusion w.r.t. variables (\u03b1, b) we add a Gaussian convolution in s. By linearity of convolution, we can just take the diffused energy we obtained in (20) and put it under the Gaussian convolution in s. This leads to the following expression,\nmax (\u03b1,s,b)\u2208\u0398 max k \u222b 2\u03c0 0 \u222b Xk \u222b R2 [ ( e . k\u0303\u03c3r (\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2)\u2016\u2207f(y)\u2016 k\u03c3d(y \u2212 e .R\u03b1x\u2212 b))\n? k\u03c3s ](s) dy h(\u03b2,x ; pk) dx d\u03b2 . (22)"}, {"heading": "7 Implication for Future Algorithms", "text": ""}, {"heading": "7.1 Closed Form Approximations for Domain Size Pooling", "text": "In DSP-SIFT, pooling over the scale is done numerically via sampling [Dong and Soatto, 2015]. Our theory suggests that the scale pooling should also be performed by Gaussian convolution, i.e. (22). Using this form, we present a closed-form approximation, which consequently does not require any sampling. Whether or not this approximation provides a satisfactory fidelity must be investigated by experiments.\nRecall energy minimization formulation of DSP-SIFT (22) has the following form,\nmax (\u03b1,s,b)\u2208\u0398 max k \u222b 2\u03c0 0 \u222b Xk \u222b R2 ( [ ( e . k\u03c3d(y \u2212 e .R\u03b1x\u2212 b) ) ? k\u03c3s ](s) ) k\u0303\u03c3r (\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2)\u2016\u2207f(y)\u2016 dy h(\u03b2,x ; pk) dx d\u03b2 .(23)\nThis convolution does not have a closed form. However, we consider approximating es by its linearized form around s = 0 (identity scaling transform), i.e. es \u2248 1 + s. Then the convolution will have a closed form. Below we present two approximation based on this idea.\nLinearizing only the inner es :\nProposition 2 [ ( e . k\u03c3d(y \u2212 (1 + . )R\u03b1x\u2212 b) ) ? k\u03c3s ](s)\n= k\u03c3d(y \u2212R\u03b1x\u2212 b)\n\u00d7k\u22121\u03c3d \u2016x\u2016\n(1 + (R\u03b1x) T (b\u2212 y)\u2212 \u03c32d \u2016x\u20162 )\u00d7 k\u221a \u03c32s+ \u03c32 d\n\u2016x\u20162\n(s+ 1 + (R\u03b1x) T (b\u2212 y)\u2212 \u03c32d \u2016x\u20162 ) .\nProof See Appendix A for the proof.\nIn particular, when the region is already warped, we can set (\u03b1, s, b) = (0, 0,0). This allows the template matching solution (23) as below,\nmax j,k \u222b 2\u03c0 0 \u222b Xk \u222b R2 k\u03c3d(y \u2212 x)\u00d7 k \u22121 \u03c3d \u2016x\u2016 (1\u2212 x Ty + \u03c32d \u2016x\u20162 )\u00d7 k\u221a \u03c32s+ \u03c32 d\n\u2016x\u20162\n(1\u2212 x Ty + \u03c32d \u2016x\u20162 )\n\u00d7 k\u0303\u03c3r (\u2220\u2207fj(y)\u2212 \u03b2)\u2016\u2207fj(y)\u2016 dy h(\u03b2,x ; pk) dx d\u03b2 . (24)\nLinearizing both the inner and outer es : We use the following identity,\n[(1 + . )k\u03c3(y + (1 + . )x) ? kscale](s) (25)\n= \u03c32(1 + s)\u2212 \u03c3scalexTy 2\u03c0\u03c3(\u03c32 + \u03c32scale\u2016x\u20162) 3 2 e \u2212\u03c3\n2\u2016(1+s)x+y\u20162+\u03c32scale(x T y\u22a5)2\n2\u03c32(\u03c32+\u03c32 scale \u2016x\u20162) (26)\nThus,\n[(1 + . )k\u03c3d(y \u2212 b\u2212 (1 + . )R\u03b1 x) ? k\u03c3s ](s) (27)\n= \u03c32d(1 + s) + \u03c3sx TRT\u03b1(y \u2212 b) 2\u03c0\u03c3d(\u03c32d + \u03c3 2 s\u2016x\u20162) 3 2 e \u2212\u03c3\n2 d\u2016\u2212(1+s)R\u03b1x+y\u2212b\u2016 2+\u03c32s((R\u03b1x) T (y\u2212b)\u22a5)2\n2\u03c32 d (\u03c32 d +\u03c32s\u2016x\u2016 2) .(28)\nIn particular, when the region is already warped, we can set (\u03b1, s, b) = (0, 0,0). This allows the template matching solution (23) as below,\nmax j,k \u222b 2\u03c0 0 \u222b Xk \u222b R2 \u03c32d + \u03c3sx Ty (\u03c32d + \u03c3 2 s\u2016x\u20162) 3 2 e \u2212\u03c3 2 d\u2016y\u2212x\u2016 2+\u03c32s(x T y\u22a5)2 2\u03c32 d (\u03c32 d +\u03c32s\u2016x\u2016 2) (29)\n\u00d7 k\u0303\u03c3r (\u2220\u2207fj(y)\u2212 \u03b2)\u2016\u2207fj(y)\u2016 dy h(\u03b2,x ; pk) dx d\u03b2 .(30)"}, {"heading": "7.2 Exact Diffusion for Affine Transform", "text": "Using the diffusion theory, by using a different parameterization for the geometric transformation, we can potentially improved SIFT and DSP-SIFT in two ways.\n1. We can extend the descriptor from handling similarity transform to affine transform.\n2. Recall that the computation of the diffusion in SIFT and DSP-SIFT relies on some approximation. In addition, regardless of the diffusion theory, DSP-SIFT involves sampling to approximate one of the required integrals. The finite sampling process is inaccurate and expensive to compute. Here despite working with a larger transformation space, the new parameterization allows deriving exact and closed form expression for the diffusion in all transformation parameters.\nThe formulation of the energy function is similar to that of SIFT and DSPSIFT, except the parameterization. Instead of the similarity transform \u03c4 as \u03c4(x ; \u03b1, s, b) , esR\u03b1x+b we switch to the affine transform \u03c4(x ; A, b) , Ax+b, as listed below,\ncost(c,A, b) , q(c) + \u2211 k ck d(f \u25e6 \u03c4A,b, pk,Xk) , (31)\nwhere the dissimilarity functional d is defined as earlier in (15). Recall that the goal is to tackle the nonconvex problem (31) using the diffusion theory of [Mobahi and Fisher III, 2015a] and similar simplifications as in Section 5, we obtain the following solution for the template matching problem.\nk\u2217 = argmin k\n[ [d(f \u25e6 \u03c4( . , . ), pk,Xk) ? k\u03c3b ](b) ? k\u0303\u03c3a ] (A) (32)\n= argmax k \u222b 2\u03c0 0 \u222b Xk ([ [h(\u03b2,x ; f \u25e6 \u03c4( . , . )) ? k\u03c3b ](b) ? k\u0303\u03c3a ] (A) ) h(\u03b2,x ; pk) d\u03b2 dx .\nInteresting, we can replace the above convolutions by a closed form and exact expression. This is stated in the following lemma.\nLemma 3[( [h(\u03b2,x ; f \u25e6 \u03c4( . , . )) ? k\u03c3b ](b) ) ? k\u03c3a ] (A)\n= e \u2212 ((b\u2212y)\nT \u2207\u0303f(y))2 2\u03c32 b \u2212 \u2016A T \u2207\u0303f(y)\u20162 2\u03c3a2 w(\u2212\u03c3 \u22122 b \u2207\u0303\nT f(y)(y\u2212b)xT v\u0303(\u03b2,y)+\u03c3a\u22122\u2207\u0303T f(y)Av\u0303(\u03b2,y) 2t )\n8 \u221a 2\u03c0 3 2\u03c3b\u03c3a2 \u2016\u2207f(y)\u20162 t3\n,\nwhere \u2207\u0303f(y) , \u2207f(y)\u2016\u2207f(y)\u2016 , v\u0303(\u03b2,y) , v(\u03b2) \u2016\u2207f(y)\u2016 , and t ,\n\u221a (xT v\u0303(\u03b2,y))2\n2\u03c32b + 12\u03c3a2 \u2016\u2207f(y)\u20162\nand w(x) , \u221a \u03c0ex 2 (1 + 2x2) erfc(x)\u2212 2x.\nProof See Appendix C for the proof.\nSimilar to the arguments about SIFT solution in Section 5, the inner optimization in (32) can work with the warped f so that the transformation \u03c4(x ; A, b) simplifies to the identity transform (A = I, b = 0). Letting the warped f be fj , f \u25e6 \u03c4Aj ,bj , the inner optimization simplifies,\nwhere hheat is defined as the result in lemma 2 (diffused h) with (A = I, b = 0), \u03c3a and \u03c3b fixed, and all constants dropped,\n(j\u2217, k\u2217) , argmax j,k \u222b 2\u03c0 0 \u222b Xk hheat(\u03b2,x ; fj)h(\u03b2,x ; pk) dx d\u03b2\nhheat(\u03b2,x ; f) , e \u2212 (y\nT \u2207\u0303f(y))2\n2\u03c32 b w ( \u2212 12t \u2207\u0303 T f(y) (\u03c3\u22122b yx T + \u03c3a \u22122 I ) v\u0303(\u03b2,y) ) \u2016\u2207f(y)\u20162 t3\n\u2207\u0303f(y) , \u2207f(y) \u2016\u2207f(y)\u2016\nv\u0303(\u03b2,y) , v(\u03b2)\n\u2016\u2207f(y)\u2016\nt ,\n\u221a (xT v\u0303(\u03b2,y))2\n2\u03c32b +\n1\n2\u03c3a2 \u2016\u2207f(y)\u20162\nw(x) , \u221a \u03c0ex 2 (1 + 2x2) erfc(x)\u2212 2x ."}, {"heading": "A Proof of Proposition 1", "text": "We proceed with the following identity8,\nes \u00d7 k\u03c3(y \u2212 (1 + s)R\u03b1x\u2212 b) (35)\n= k\u03c3(y \u2212R\u03b1x\u2212 b)\u00d7 k\u22121\u03c3 \u2016R\u03b1x\u2016\n(1 + (R\u03b1x)\nT (b\u2212 y)\u2212 \u03c32 \u2016R\u03b1x\u20162 )\u00d7 k \u03c3 \u2016R\u03b1x\u2016 (s+ 1 + (R\u03b1x) T (b\u2212 y)\u2212 \u03c32 \u2016R\u03b1x\u20162 )(36\n= k\u03c3(y \u2212R\u03b1x\u2212 b)\u00d7 k\u22121\u03c3 \u2016x\u2016\n(1 + (R\u03b1x)\nT (b\u2212 y)\u2212 \u03c32\n\u2016x\u20162 )\u00d7 k \u03c3 \u2016x\u2016 (s+ 1 +\n(R\u03b1x) T (b\u2212 y)\u2212 \u03c32\n\u2016x\u20162 ) (37)\n(38)\nIn this form, it is now very easy to compute convolution with k\u03c3scale(s),\n[e . \u00d7 k\u03c3(y \u2212 (1 + . )R\u03b1x\u2212 b) ? k\u03c3scale ](s) (39)\n= k\u03c3(y \u2212R\u03b1x\u2212 b)\u00d7 k\u22121\u03c3 \u2016x\u2016\n(1 + (R\u03b1x)\nT (b\u2212 y)\u2212 \u03c32 \u2016x\u20162 )\u00d7 k\u221a\n\u03c32scale+ \u03c32\n\u2016x\u20162\n(s+ 1 + (R\u03b1x)\nT (b\u2212 y)\u2212 \u03c32\n\u2016x\u20162 )(40\n(41)\nTherefore,\n[ ( e . \u222b R2 k\u0303\u03c3\u0303(\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2)\u2016\u2207f(y)\u2016 k\u03c3(y \u2212 (1 + . )R\u03b1x\u2212 b) dy ) ? k\u03c3scale ](s) (42)\n= \u222b R2 k\u0303\u03c3\u0303(\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2)\u2016\u2207f(y)\u2016 k\u03c3(y \u2212R\u03b1x\u2212 b) (43)\n\u00d7k\u22121\u03c3 \u2016x\u2016\n(1 + (R\u03b1x)\nT (b\u2212 y)\u2212 \u03c32 \u2016x\u20162 )\u00d7 k\u221a\n\u03c32scale+ \u03c32\n\u2016x\u20162\n(s+ 1 + (R\u03b1x)\nT (b\u2212 y)\u2212 \u03c32\n\u2016x\u20162 ) dy .\n8 We essentially have es\u00d7k\u03c3(y+(1+s)x) which by completing the square of the exponent w.r.t. s can be expressed as below,\nes \u00d7 k\u03c3(y + (1 + s)x) (33)\n= k\u03c3(x + y)\u00d7 k\u22121\u03c3 \u2016x\u2016\n(1 + xTy \u2212 \u03c32\n\u2016x\u20162 )\u00d7 k \u03c3 \u2016x\u2016 (s+ 1 +\nxTy \u2212 \u03c32\n\u2016x\u20162 ) , (34)\nwhere the first k is 2D, and the next two k\u2019s are 1D.\nB Proof of Lemma 1\ncost(c,\u03b8) (44) , q(c) + \u2211 k ck d(f \u25e6 \u03c4(\u03b1 ,s, b ), pk,Xk) (45)\n= q(c) + \u2211 k ck \u222b 2\u03c0 0 ( h(\u03b2 ; f \u25e6 \u03c4(\u03b1,s, b ),Xk)\u2212 h(\u03b2 ; pk,Xk) )2 d\u03b2 . (46)\nNote that,\nh(\u03b2 ; f \u25e6 \u03c4(\u03b1,s, b ),Xk) (47)\n= \u222b Xk X(\u2220\u2207(f(esR\u03b1x+ b))\u2212 \u03b2)\u2016\u2207f(esR\u03b1x+ b)\u2016 dx (48)\n= \u222b Xk X(\u2220esRT\u03b1 [\u2207f ](esR\u03b1x+ b)\u2212 \u03b2)\u2016esR T \u03b1\u2207f(esR\u03b1x+ b)\u2016 dx (49)\n= \u222b Xk X(\u2220\u2207f(esR\u03b1x+ b)\u2212 \u03b1\u2212 \u03b2)es\u2016\u2207f(esR\u03b1x+ b)\u2016 dx , (50)\nwhere (49) uses the chain rule of derivative \u2207 ( f(Ax+b) ) = AT ( [\u2207f ](Ax+\nb) ) . Also, for any a > 0, (50) uses the identities \u2016aRx\u2016 = a\u2016x\u2016 and \u2220aR\u03b1x = \u03b1+ \u2220x. Thus, it follows that,\ncost(c,\u03b8) (51) = q(c) + \u2211 k ck \u222b 2\u03c0 0 (\u222b Xk X(\u2220\u2207f(esR\u03b1x+ b)\u2212 \u03b1\u2212 \u03b2)es\u2016\u2207f(esR\u03b1x+ b)\u2016 dx\u2212 h(\u03b2 ; pk,Xk) )2 d\u03b2 .(52)\nBy the linearity of the convolution operator and the unity of Gaussian\u2019s total mass, smoothed cost amounts only to replacing ( \u222b Xk X(\u2220\u2207f(e sR\u03b1x+b)\u2212\u03b1\u2212\n\u03b2)es\u2016\u2207f(esR\u03b1x+ b)\u2016 dx\u2212h(\u03b2 ; pk,Xk) )2 by its smoothed version. Expansion\nof the quadratic form yields,\n(\u222b Xk X(\u2220\u2207f(esR\u03b1x+ b)\u2212 \u03b1\u2212 \u03b2)es\u2016\u2207f(esR\u03b1x+ b)\u2016 dx\u2212 h(\u03b2 ; pk,Xk) )2 (53)\n= (\u222b Xk X(\u2220\u2207f(esR\u03b1x+ b)\u2212 \u03b1\u2212 \u03b2)es\u2016\u2207f(esR\u03b1x+ b)\u2016 dx )2 + h2(\u03b2 ; pk,Xk)(54)\n\u22122 (\u222b Xk X(\u2220\u2207f(esR\u03b1x+ b)\u2212 \u03b1\u2212 \u03b2)es\u2016\u2207f(esR\u03b1x+ b)\u2016 dx ) \u00d7 h(\u03b2 ; pk,Xk) .(55)\nThe first term can be rewritten as below,\n(\u222b Xk X(\u2220\u2207f(esR\u03b1x+ b)\u2212 \u03b1\u2212 \u03b2)es\u2016\u2207f(esR\u03b1x+ b)\u2016 dx )2 (56)\n= e2s \u222b Xk |{x2 |\u2220\u2207f(esR\u03b1x2 + b) = \u2220\u2207f(esR\u03b1x+ b)}| (57)\n\u00d7X(\u2220\u2207f(esR\u03b1x+ b)\u2212 \u03b1\u2212 \u03b2) \u2016\u2207f(esR\u03b1x+ b)\u20162 dx . (58)\nNote that |{x2 |\u2220\u2207f(esR\u03b1x2 + b) = \u2220\u2207f(esR\u03b1x + b)}| \u2265 1 because at least there is one such x2 for which \u2220\u2207f(esR\u03b1x2 + b) = \u2220\u2207f(esR\u03b1x + b) holds, that is x2 = x. However, we assume that the cardinality of the set is exactly one, i.e. besides x2 = x, there is no other choice for x2 so that the condition \u2220\u2207f(esR\u03b1x2 +b) = \u2220\u2207f(esR\u03b1x+b) can hold. The rationale is that the variables are continuous and thus their representation has infinite precision. The odds that the gradient orientation at two different points in the image are exactly the same is almost impossible, although they might be very close. With this assumption, the quadratic form simplifies as below,\n(\u222b Xk X(\u2220\u2207f(esR\u03b1x+ b)\u2212 \u03b1\u2212 \u03b2)es\u2016\u2207f(esR\u03b1x+ b)\u2016 dx\u2212 h(\u03b2 ; pk,Xk) )2\n(59) = e2s \u222b Xk X(\u2220\u2207f(esR\u03b1x+ b)\u2212 \u03b1\u2212 \u03b2) \u2016\u2207f(esR\u03b1x+ b)\u20162 dx+ h2(\u03b2 ; pk,Xk) (60)\n\u22122es (\u222b Xk X(\u2220\u2207f(esR\u03b1x+ b)\u2212 \u03b1\u2212 \u03b2)\u2016\u2207f(esR\u03b1x+ b)\u2016 dx ) \u00d7 h(\u03b2 ; pk,Xk) (61)\n= e2s \u222b Xk \u222b R2 X(\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2) \u2016\u2207f(y)\u20162 \u03b4(y \u2212 esR\u03b1x\u2212 b) dy dx+ h2(\u03b2 ; pk,Xk)(62)\n\u22122es (\u222b Xk \u222b R2 X(\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2)\u2016\u2207f(y)\u2016 \u03b4(y \u2212 esR\u03b1x\u2212 b) dy dx ) \u00d7 h(\u03b2 ; pk,Xk) ,(63)\nwhere (62) and (63) use the sifting property of the delta function. The goal is to convolve cost with a multivariate Gaussian kernel of covariance \u03c32I in variables jointly in (\u03b1, b). Due to the diagonal form of the covariance, the convolution can be decoupled to that of \u03b1 and b.\nWe first proceed with smoothing w.r.t. b. By linearity of the convolution operator and that the Gaussian kernel integrates to one, we obtained the following,\n[cost(c, \u03b1, s, . ) ? k\u03c3](b) (64) = q(c) + \u2211 k ck \u222b 2\u03c0 0 [ (\u222b Xk X(\u2220[\u2207f ](esR\u03b1x+ . )\u2212 \u03b1\u2212 \u03b2)es\u2016[\u2207f ](esR\u03b1x+ . )\u2016 dx\u2212 h(\u03b2 ; pk,Xk) )2 ? k\u03c3](b) d\u03b2(65)\n= q(c) + \u2211 k ck \u222b 2\u03c0 0 ( (66)\ne2s \u222b Xk \u222b R2 X(\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2) \u2016\u2207f(y)\u20162 k\u03c3(y \u2212 esR\u03b1x\u2212 b) dy dx+ h2(\u03b2 ; pk,Xk) (67)\n\u22122es (\u222b Xk \u222b R2 X(\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2)\u2016\u2207f(y)\u2016 k\u03c3(y \u2212 esR\u03b1x\u2212 b) dy dx ) \u00d7 h(\u03b2 ; pk,Xk) ) d\u03b2 . (68)\nWe now continue by trying to smooth w.r.t. \u03b1.\n[ [ cost(c, . , s, . ) ? k\u03c3 ] (b) ? k\u03c3\u0303 ] (\u03b1) (69) = q(c) + \u2211 k ck \u222b 2\u03c0 0 ( (70)\ne2s \u222b Xk \u222b R2 \u2016\u2207f(y)\u20162 ( [X(\u2220\u2207f(y)\u2212 . \u2212 \u03b2)k\u03c3(y \u2212 esR .x\u2212 b) ? k\u03c3\u0303](\u03b1) ) dy dx+ h2(\u03b2 ; pk,Xk) (71)\n\u22122es (\u222b Xk \u222b R2 \u2016\u2207f(y)\u2016 ( [X(\u2220\u2207f(y)\u2212 . \u2212 \u03b2)k\u03c3(y \u2212 esR .x\u2212 b) ? k\u03c3\u0303](\u03b1) ) dy dx ) \u00d7 h(\u03b2 ; pk,Xk) ) d\u03b2 .(72)\nComputation of the convolution X(\u2220\u2207f(y)\u2212 . +\u03b2)k\u03c3(y\u2212esR .x\u2212b)?k\u03c3\u0303 is intractable. However, it can be approximated by applying the convolution only to the X function. The rationale behind this approximation is that Gaussian convolution affects delta function much more than the Gaussian factor9.\n[X(\u2220\u2207f(y)\u2212 . \u2212 \u03b2)k\u03c3(y \u2212 esR .x\u2212 b) ? k\u03c3\u0303](\u03b1) (73) \u2248 ( [X(\u2220\u2207f(y)\u2212 . \u2212 \u03b2) ? k\u03c3\u0303](\u03b1) ) k\u03c3(y \u2212 esR\u03b1x\u2212 b) (74)\n= k\u0303\u03c3\u0303(\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2)k\u03c3(y \u2212 esR\u03b1x\u2212 b) . (75)\nUsing this approximation, it follows that,\n9Gaussian smoothing affects high frequency functions more than low frequency ones; essentially it kills high frequency components, while leaving low frequency components intact.\n[ [ cost(c, . , s, . ) ? k\u03c3 ] (b) ? k\u03c3\u0303 ] (\u03b1) (76) \u2248 q(c) + \u2211 k ck \u222b 2\u03c0 0 ( (77)\ne2s \u222b Xk \u222b R2 k\u0303\u03c3\u0303(\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2) \u2016\u2207f(y)\u20162 k\u03c3(y \u2212 esR\u03b1x\u2212 b) dy dx+ h2(\u03b2 ; pk,Xk) (78)\n\u22122es (\u222b Xk \u222b R2 k\u0303\u03c3\u0303(\u2220\u2207f(y)\u2212 \u03b1\u2212 \u03b2)\u2016\u2207f(y)\u2016 k\u03c3(y \u2212 esR\u03b1x\u2212 b) dy dx ) \u00d7 h(\u03b2 ; pk,Xk) ) d\u03b2 .(79)"}, {"heading": "C Proof of Lemma 2", "text": "1. revert to previous proof with Z, just for the propsotiion. Do poper variable replacement in the proof.\nUse proposition. Justify why Df having no zero component makes sense. Mention integration w.r.t y is over X \u2229Xk. We assume that this integral is\nzero outside of the domain R2 \u2212X. 10\nProposition 4\n\u03b4(r v(\u03b2)\u2212AT\u2207f(y)) k\u03c3(y \u2212Ax\u2212 b) (80)\n= k\u03c3( xTz \u2212 (y \u2212 b)T\u2207f(y)\n\u2016\u2207f(y)\u2016 ) k\u221a \u03c32+\u03c3\u20202 \u2016x\u20162( (\u2207f(y))T (Ax+ b\u2212 y)\u22a5 \u2016\u2207f(y)\u2016 ) k\u03c3\u2020 \u2016\u2207f(y)\u2016(z \u2212AT\u2207f(y)) .(81)\n\u2207f(y) \u2194 g (82) \u2212z \u2194 c (83)\nb\u2212 y \u2194 d (84) (85)\nProposition 5 [ ( \u03b4( . Tg + c) k\u03c3( . x+ d) ) ? k\u03c3\u2020 ](A) (86)\n= k\u03c3( gTd\u2212 xT c \u2016g\u2016 ) k\u221a \u03c32+\u03c3\u20202 \u2016x\u20162( gT (Ax+ d)\u22a5 \u2016g\u2016 ) k\u03c3\u2020 \u2016g\u2016(A Tg + c) .(87)\nProof We first provide an outline of the proof. The \u03b4 function can be replaced by the limit of a Gaussian whose variance tends to zero lim \u21920 k . Now k and k\u03c3\u2020 form the product of Gaussians. The idea is to write this product as a new single Gaussian in A (because then we know how to convolve two Gaussians). We do this by replacing the pair with a single exponential whose exponent is trivially the sum of the original exponents. Using completing the square method for the joint exponent, the center and covariance of the single Gaussian emerges. There is a problem though; the resulted quadratic form will have a singular covariance whose inverse does not exist11. We tackle this problem by the change of coordinate system.\n10These conditions can be assumed as granted. The gradient is no where perfectly zero in the image. It is perfectly zero outside of the image f , but that can be taken care of by limiting the integration domain of y from R2 to X . Having x1 = 0 or x2 = 0 has zero measure, and can be removed from the integration w.r.t. x without affecting the integration result.\n11The inverse of the covariance is required as it directly appears in the definition of the Gaussian.\nWe begin with the coordinate system transform. Since the covariance of the Gaussian kernel is isotropic, the resulted Gaussian is radially symmetric, i.e. k\u03c3(x) = k\u03c3(Rx) for any rotation matrix R. Consequently, instead of directly smoothing the above expression, we can rotate the coordinate system, smooth in the latter system, and then invert the rotation to obtain the smoothed function in the original coordinate. In particular, we use the following rotation matrix,\nR , 1\n\u2016x\u2016 \u2016g\u2016\n g2x2 sign ( g1x1 ) \u2212g2 |x1| sign ( g1 ) \u2212x2 |g1| sign(x1) |g1x1| \u2212g1x2 sign ( g2x1 ) g1 |x1| sign ( g2 ) \u2212x2 |g2| sign(x1) |g2x1| \u2212g2x1 sign ( g1x2 ) \u2212g2 |x2| sign ( g1 )\nx1 |g1| sign(x2) |g1x2| g1x1 sign ( g2x2 ) g1 |x2| sign ( g2 ) x1 |g2| sign(x2) |g2x2|  . (88)\nDue to the assumptions g1 6= 0, g2 6= 0, x1 6= 0, and x2 6= 0,R is well-defined. Let a , vec(A), i.e. a = (a11, a12, a21, a22), and let U , RA and u , vec(U). Changing the coordinate system from A to U leads to the following identity,\nk (A Tg + c) k\u03c3(Ax+ d) (89)\n= k ( (R TU) T g + c) k\u03c3(R TU x+ d) (90) = 1\u221a 2\u03c0 e \u2212 2\u2016x\u20162(dT g\u22a5)2\u2212\u03c32\u2016g\u20162(\u2016g\u20162\u2016d\u20162+(2(\u2212d)T g+xT c)(xT c)) 2\u03c32\u2016g\u20162(\u03c32\u2016g\u20162+ 2\u2016x\u20162) (91)\n\u00d7e (g1(\u2212d2)\u2212g2(\u2212d1))\n2\n2\u03c32\u2016g\u20162 (92)\n\u00d7 1 \u2016g\u2016 k \u2016g\u2016 (u2 \u2212 \u2212x1c2 + x2c1 \u2016g\u2016 \u2016x\u2016 sign(g2x1) ) (93) \u00d7 1 \u2016x\u2016 k \u03c3 \u2016x\u2016 (u3 \u2212 g1(\u2212d2)\u2212 g2(\u2212d1) \u2016g\u2016 \u2016x\u2016 sign(g1x2) ) (94) \u00d7 1\u221a \u03c32\u2016g\u20162 + 2\u2016x\u20162 k \u03c3 \u221a \u03c32\u2016g\u20162 + 2\u2016x\u20162 (u4 \u2212 2\u2016x\u20162(\u2212d)Tg \u2212 \u03c32\u2016g\u20162 xT c \u2016g\u2016 \u2016x\u2016 (\u03c32\u2016g\u20162 + 2\u2016x\u20162) sign(g2x2) ) .(95)\nThe value of the coordinate transformation is that we can now write this expression as the product of independent Gaussian kernels. Convolution of this expression with the isotropic kernel k\u03c3\u2020(u) is straightforward,\n\u03b4(z \u2212AT\u2207f(y)) k\u03c3(y \u2212Ax\u2212 b) (96)\n\u2194 1\u221a 2\u03c0 e 2\u2016x\u20162(((y\u2212b)T\u2207f(y))2\u2212\u2016\u2207f(y)\u20162\u2016y\u2212b\u20162)\u2212\u03c32\u2016\u2207f(y)\u20162(\u2016\u2207f(y)\u20162\u2016y\u2212b\u20162\u2212(2(y\u2212b)T\u2207f(y)\u2212xT z)(xT z)) 2\u03c32\u2016\u2207f(y)\u20162(\u03c32\u2016\u2207f(y)\u20162+ 2\u2016x\u20162) (97)\n\u00d7e (f1(y)(y2\u2212b2)\u2212f2(y)(y1\u2212b1))\n2\n2\u03c32\u2016\u2207f(y)\u20162 (98)\n\u00d7 1 \u2016\u2207f(y)\u2016 k\u221a \u03c3\u20202+\n2\n\u2016\u2207f(y)\u20162\n(u2 \u2212 x1z2 \u2212 x2z1\n\u2016\u2207f(y)\u2016 \u2016x\u2016 sign(f2(y)x1) ) (99)\n\u00d7 1 \u2016x\u2016 k\u221a \u03c3\u20202+ \u03c3 2\n\u2016x\u20162\n(u3 \u2212 f1(y)(y2 \u2212 b2)\u2212 f2(y)(y1 \u2212 b1) \u2016\u2207f(y)\u2016 \u2016x\u2016 sign(f1(y)x2) ) (100)\n\u00d7 1\u221a \u03c32\u2016\u2207f(y)\u20162 + 2\u2016x\u20162 k\u221a \u03c3\u20202+ \u03c3 2 2\n\u03c32\u2016\u2207f(y)\u20162 + 2\u2016x\u20162\n(u4 \u2212 2\u2016x\u20162(y \u2212 b)T\u2207f(y) + \u03c32\u2016\u2207f(y)\u20162 xTz\n\u2016\u2207f(y)\u2016 \u2016x\u2016 (\u03c32\u2016\u2207f(y)\u20162 + 2\u2016x\u20162) sign(f2(y)x2) ) .(101)\nSetting \u2192 0, and given that \u03c3 > 0 and \u2016\u2207f(y)\u2016 6= 0, it follows thats,\n\u03b4(z \u2212AT\u2207f(y)) k\u03c3(y \u2212Ax\u2212 b) (102)\n\u2194 1\u221a 2\u03c0 e \u2212 ((b\u2212y)\nT\u2207f(y)+xT z)2\n2\u03c32\u2016\u2207f(y)\u20162 (103)\n\u00d7 1 \u2016\u2207f(y)\u2016 k\u03c3\u2020(u2 \u2212 x1z2 \u2212 x2z1 \u2016\u2207f(y)\u2016 \u2016x\u2016 sign(f2(y)x1) ) (104) \u00d7 1 \u2016x\u2016 k\u221a \u03c3\u20202+ \u03c3 2\n\u2016x\u20162\n(u3 \u2212 f1(y)(y2 \u2212 b2)\u2212 f2(y)(y1 \u2212 b1) \u2016\u2207f(y)\u2016 \u2016x\u2016 sign(f1(y)x2) ) (105)\n\u00d7 1 \u03c3\u2016\u2207f(y)\u2016 k\u03c3\u2020(u4 \u2212 xTz \u2016\u2207f(y)\u2016 \u2016x\u2016 sign(f2(y)x2) ) . (106)\nBy inverting the coordinate system from u to (a11, a12, a21, a22) we obtain,\nk\u03c3( xTz \u2212 (y \u2212 b)T\u2207f(y)\n\u2016\u2207f(y)\u2016 ) k\u221a \u03c32+\u03c3\u20202 \u2016x\u20162( (\u2207f(y))T (Ax+ b\u2212 y)\u22a5 \u2016\u2207f(y)\u2016 ) k\u03c3\u2020 \u2016\u2207f(y)\u2016(z\u2212AT\u2207f(y)) .\n(107) As a sanity check, we can see that the above expression becomes the same\nas the original non-smoothed function when \u03c3\u2020 \u2192 0,\nlim \u03c3\u2020\u21920\nk\u03c3( xTz \u2212 (y \u2212 b)T\u2207f(y)\n\u2016\u2207f(y)\u2016 ) k\u221a \u03c32+\u03c3\u20202 \u2016x\u20162( (\u2207f(y))T (Ax+ b\u2212 y)\u22a5 \u2016\u2207f(y)\u2016 ) k\u03c3\u2020 \u2016\u2207f(y)\u2016(z \u2212AT\u2207f(y))(108\n= k\u03c3( xTz \u2212 (y \u2212 b)T\u2207f(y)\n\u2016\u2207f(y)\u2016 ) k\u03c3(\n(\u2207f(y))T (Ax+ b\u2212 y)\u22a5\n\u2016\u2207f(y)\u2016 ) \u03b4(z \u2212AT\u2207f(y)) (109)\n= k\u03c3( xT (AT\u2207f(y))\u2212 (y \u2212 b)T\u2207f(y)\n\u2016\u2207f(y)\u2016 ) k\u03c3(\n(\u2207f(y))T (Ax+ b\u2212 y)\u22a5\n\u2016\u2207f(y)\u2016 ) \u03b4(z \u2212AT\u2207f(y)) (110)\n= k\u03c3,1( (\u2207f(y))T (Ax+ b\u2212 y)\n\u2016\u2207f(y)\u2016 ) k\u03c3,1(\n(\u2207f(y))T (Ax+ b\u2212 y)\u22a5\n\u2016\u2207f(y)\u2016 ) \u03b4(z \u2212AT\u2207f(y)) (111)\n= k\u03c3,2 ( 1 \u2016\u2207f(y)\u2016 ( (\u2207f(y))T (Ax+ b\u2212 y) , (\u2207f(y))T (Ax+ b\u2212 y)\u22a5 )) \u03b4(z \u2212AT\u2207f(y)) (112)\n= k\u03c3,2 ( 1 \u2016\u2207f(y)\u2016 \u2016\u2207f(y)\u2016 (Ax+ b\u2212 y) ) \u03b4(z \u2212AT\u2207f(y)) (113)\n= k\u03c3(Ax+ b\u2212 y) \u03b4(z \u2212AT\u2207f(y)) (114) (115)\nThe goal is to convolve d(f \u25e6 \u03c4(A,b), pk,Xk) with the Gaussian kernel. By linearity of the convolution operator we obtain,\n[( [d(f \u25e6 \u03c4( . , . ), pk,Xk) ? k\u03c3b ](b) ) ? k\u03c3a ] (A) (116)\n, [( [\u2212 \u222b Xk \u222b 2\u03c0 0 h(\u03b2,x ; f \u25e6 \u03c4( . , . ))\u00d7 h(\u03b2,x ; pk) d\u03b2 dx ? k\u03c3b ](b) ) ? k\u03c3a ] (A) dx(117)\n= \u2212 \u222b Xk \u222b 2\u03c0 0 ([( [h(\u03b2,x ; f \u25e6 \u03c4( . , . )) ? k\u03c3b ](b) ) ? k\u03c3a ] (A) ) \u00d7 h(\u03b2,x ; pk) d\u03b2 dx .(118)\nThus in the following we focus on h(\u03b2,x ; f \u25e6 \u03c4( . , . )) ? k\u03c3b ? k\u03c3a . We first manipulate h(\u03b2,x ; f \u25e6 \u03c4(A,b)) by applying the chain rule of derivate \u2207 ( f(Ax+\nb) ) = AT ( [\u2207f ](Ax+ b) ) followed by the sifting property of the delta function,\nh(\u03b2,x ; f \u25e6 \u03c4(A,b)) (119) , X(\u03b2 \u2212 \u2220\u2207(f(Ax+ b)))\u2016\u2207f(Ax+ b)\u2016 (120) = X(\u03b2 \u2212 \u2220AT [\u2207f ](Ax+ b))\u2016AT\u2207f(Ax+ b)\u2016 (121)\n= \u222b R2 X(\u03b2 \u2212 \u2220AT\u2207f(y))\u2016AT\u2207f(y)\u2016 \u03b4(y \u2212Ax\u2212 b) dy . (122)\nComputing the inner convolution, i.e. w.r.t. b, is straightforward,\n[h(\u03b2,x ; f \u25e6 \u03c4(A, . )) ? k\u03c3b ](b) (123) = [ (\u222b\nR2 X(\u03b2 \u2212 \u2220AT\u2207f(y))\u2016AT\u2207f(y)\u2016 \u03b4(y \u2212Ax\u2212 b) dy\n) ? k\u03c3b ](b)(124)\n= \u222b R2 X(\u03b2 \u2212 \u2220AT\u2207f(y))\u2016AT\u2207f(y)\u2016 k\u03c3b(y \u2212Ax\u2212 b) dy . (125)\nThe latter can be expressed by the sifting property of the delta function as below,\n[h(\u03b2,x ; f \u25e6 \u03c4(A, . )) ? k\u03c3b ](b) (126)\n= \u222b R2 \u222b R2 \u03b4 ( z \u2212AT\u2207f(y) ) X(\u03b2 \u2212 \u2220z)\u2016z\u2016 k\u03c3b(y \u2212Ax\u2212 b) dz dy .(127)\nWe now apply a change of variable to move from the Cartesian coordinate (z1, z2) to the polar coordinate (r, \u03c6) such that (z1, z2) = (r cos(\u03c6), r sin(\u03c6)).\nThis results in replacing \u222b R2 f(z1, z2) dz1 dz2 by \u222b\u221e 0 \u222b 2\u03c0 0 r f(r v(\u03c6)) d\u03c6 dr, where v(\u03c6) , (cos(\u03c6), sin(\u03c6)).\n[h(\u03b2,x ; f \u25e6 \u03c4(A, . )) ? k\u03c3b ](b) (128)\n= \u222b R2 \u222b \u221e 0 \u222b 2\u03c0 0 r \u03b4 ( rv(\u03c6)\u2212AT\u2207f(y) ) X(\u03b2 \u2212 \u2220rv(\u03c6))\u2016rv(\u03c6)\u2016 k\u03c3b(y \u2212Ax\u2212 b) d\u03c6 dr dy(129)\n= \u222b R2 \u222b \u221e 0 \u222b 2\u03c0 0 r \u03b4 ( rv(\u03c6)\u2212AT\u2207f(y) ) X(\u03b2 \u2212 \u2220v(\u03c6)) r k\u03c3b(y \u2212Ax\u2212 b) d\u03c6 dr dy (130)\n= \u222b R2 \u222b \u221e 0 r2 \u03b4 ( rv(\u03b2)\u2212AT\u2207f(y) ) k\u03c3b(y \u2212Ax\u2212 b) dr dy . (131)\nWe are now ready to smooth this form w.r.t. A. That is, we want to compute convolution of this expression with a multivariate Gaussian in (a11, a12, a21, a22) of covariance \u03c32I.\nUsing this result, we can continue as below,\n[[cost(c, . , . ) ? k\u03c3](b) ? k\u03c3\u2020 ](A) (132) = q(c) + \u2211 k ck \u222b 2\u03c0 0 ( (133)\u222b\nXk \u222b R2 \u222b R2 \u03b4(vT (\u03b2)z) \u2016z\u20162 [ ( \u03b4(z \u2212 . T\u2207f(y)) k\u03c3(y \u2212 .x\u2212 b) ) ? k\u03c3\u2020 ](A) dz dy dx+ h 2(\u03b2 ; pk,Xk) (134)\n\u22122 (\u222b Xk \u222b R2 \u222b R2 \u03b4(vT (\u03b2)z) \u2016z\u2016 [ ( \u03b4(z \u2212 . T\u2207f(y)) k\u03c3(y \u2212 .x\u2212 b) ) ? k\u03c3\u2020 ](A) dz dy dx ) \u00d7 h(\u03b2 ; pk,Xk) ) d\u03b2(135)\n= q(c) + \u2211 k ck \u222b 2\u03c0 0 ( (136)\u222b\nXk \u222b R2 \u222b R2 \u03b4(vT (\u03b2)z) \u2016z\u20162 k\u03c3( xTz \u2212 (y \u2212 b)T\u2207f(y) \u2016\u2207f(y)\u2016 ) k\u03c3\u2020 \u2016\u2207f(y)\u2016(z \u2212AT\u2207f(y)) dz (137)\n\u00d7k\u221a \u03c32+\u03c3\u20202 \u2016x\u20162(\n(\u2207f(y))T (Ax+ b\u2212 y)\u22a5\n\u2016\u2207f(y)\u2016 ) dy dx+ h2(\u03b2 ; pk,Xk) (138)\n\u22122 (\u222b Xk \u222b R2 \u222b R2 \u03b4(vT (\u03b2)z) \u2016z\u2016 k\u03c3( xTz \u2212 (y \u2212 b)T\u2207f(y) \u2016\u2207f(y)\u2016 ) k\u03c3\u2020 \u2016\u2207f(y)\u2016(z \u2212AT\u2207f(y)) dz (139)\n\u00d7k\u221a \u03c32+\u03c3\u20202 \u2016x\u20162(\n(\u2207f(y))T (Ax+ b\u2212 y)\u22a5\n\u2016\u2207f(y)\u2016 ) dy dx\n) \u00d7 h(\u03b2 ; pk,Xk) ) d\u03b2 . (140)\nWe now apply a change of variable to move from the Cartesian coordinate (z1, z2) to the polar coordinate (r, \u03c6) such that (z1, z2) = (r cos(\u03c6), r sin(\u03c6)).\nThis transforms the form \u222b R2 f(z1, z2) dz1 dz2 to \u222b\u221e 0 \u222b 2\u03c0 0 r f(r cos(\u03c6), r sin(\u03c6)) d\u03c6 dr.\n\u222b R2 X(\u03b2 \u2212 \u2220z) \u2016z\u2016 k\u03c3( xTz \u2212 (y \u2212 b)T\u2207f(y) \u2016\u2207f(y)\u2016 ) k\u03c3\u2020 \u2016\u2207f(y)\u2016(z \u2212AT\u2207f(y)) dz (141)\n= \u222b \u221e 0 \u222b 2\u03c0 0 rX(\u03b2 \u2212 \u03c6) r k\u03c3( rxTv(\u03c6)\u2212 (y \u2212 b)T\u2207f(y) \u2016\u2207f(y)\u2016 ) k\u03c3\u2020 \u2016\u2207f(y)\u2016(rv(\u03c6)\u2212AT\u2207f(y)) d\u03c6 dr(142)\n= \u222b \u221e 0 r2 k\u03c3(r xTv(\u03b2) \u2016\u2207f(y)\u2016 + (b\u2212 y)T\u2207f(y) \u2016\u2207f(y)\u2016 ) k\u03c3\u2020 \u2016\u2207f(y)\u2016(rv(\u03b2)\u2212AT\u2207f(y)) dr (143) = e \u2212 ((b\u2212y) T \u2207\u0303f(y))2 2\u03c32 \u2212 \u2016A T \u2207\u0303f(y)\u20162 2\u03c3\u20202 w(\u2212\u03c3 \u22122\u2207\u0303T f(y)(y\u2212b)xT v\u0303(\u03b2,y)+\u03c3\u2020\u22122\u2207\u0303T f(y)Av\u0303(\u03b2,y) 2t )\n8 \u221a 2\u03c0 3 2\u03c3\u03c3\u2020 2 \u2016\u2207f(y)\u20162 t3 , (144)\nwhere \u2207\u0303f(y) , \u2207f(y)\u2016\u2207f(y)\u2016 , v\u0303(\u03b2,y) , v(\u03b2) \u2016\u2207f(y)\u2016 , and t ,\n\u221a (xT v\u0303(\u03b2,y))2\n2\u03c32 + 1 2\u03c3\u20202 \u2016\u2207f(y)\u20162\nand w(x) , \u221a \u03c0ex 2\n(1 + 2x2) erfc(x) \u2212 2x. In (144) we use an elementary identity12.\n12 We use the identity,\n\u222b \u221e 0 r2 k\u03c31,1(rc1 + c2)k\u03c32,2(rc3 + c4) dr = e \u2212 c2 2 2\u03c321 \u2212 \u2016c4\u2016 2 2\u03c322 ( \u221a \u03c0(1 + 2t22)e t22 erfc(t2)\u2212 2t2) 8 \u221a\n2\u03c0 3 2 \u03c31\u03c322t 3 1\n,\n(145)\nfor t1 ,\n\u221a c12\n2\u03c321 + \u2016c3\u20162 2\u03c322 and t2 ,\nc1c2 s12\n+ cT3 c4\n\u03c322 2t1 . This identity is derived in two steps:\n1. Completing the square of the exponent in the integrand.\n\u2212 (rc1 + c2)2 2\u03c321 \u2212 \u2016rc3 + c4\u20162 2\u03c322 = \u2212 1 2 (r+ cT3 c4\u03c3 2 1 + c1c2\u03c3 2 2 \u2016c3\u20162\u03c321 + c21\u03c322 )2( c21 \u03c321 + \u2016c3\u20162 \u03c322 )+ 1 2 ( (c1c2\u03c322 + \u03c3 2 1c T 3 c4) 2 \u2016c3\u20162\u03c341\u03c322 + c21\u03c321\u03c342 \u2212 c22 \u03c321 \u2212 \u2016c4\u20162 \u03c322 ) .\n(146)\n2. Using the identity about Gaussian moments,\n\u222b \u221e 0 r2e \u2212 (r\u2212a1) 2 2a22 dr = a1a 2 2e \u2212 a21 2a22 + \u221a \u03c0 2 a2(a 2 1 + a 2 2)(1 + erf( a1\u221a 2a2 )) . (147)"}], "references": [{"title": "Multi-view feature engineering and learning", "author": ["Dong et al", "J. 2015] Dong", "N. Karianakis", "D. Davis", "J. Hernandez", "J. Balzer", "S. Soatto"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Domain-size pooling in local descriptors: Dsp-sift", "author": ["Dong", "Soatto", "J. 2015] Dong", "S. Soatto"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "An iterative image registration technique with an application to stereo vision", "author": ["Lucas", "Kanade", "B.D. 1981] Lucas", "T. Kanade"], "venue": "In Proceedings of the 7th International Joint Conference on Artificial Intelligence Volume 2,", "citeRegEx": "Lucas et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Lucas et al\\.", "year": 1981}, {"title": "Distribution fields with adaptive kernels for large displacement image alignment", "author": ["Mears et al", "B. 2013] Mears", "L. Sevilla-Lara", "E.G. Learned-Miller"], "venue": "In British Machine Vision Conference,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "On the Link between Gaussian Homotopy Continuation and Convex Envelopes", "author": ["Mobahi", "Fisher III", "H. 2015a] Mobahi", "J.W. Fisher III"], "venue": "Energy Minimization Methods in Computer Vision and Pattern Recognition,", "citeRegEx": "Mobahi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mobahi et al\\.", "year": 2015}, {"title": "A theoretical analysis of optimization by gaussian continuation", "author": ["Mobahi", "Fisher III", "H. 2015b] Mobahi", "J.W. Fisher III"], "venue": "In TwentyNinth AAAI Conference on Artificial Intelligence", "citeRegEx": "Mobahi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mobahi et al\\.", "year": 2015}, {"title": "Vlfeat: An open and portable library of computer vision algorithms", "author": ["Vedaldi", "Fulkerson", "A. 2010] Vedaldi", "B. Fulkerson"], "venue": "In Proceedings of the International Conference on Multimedia, MM", "citeRegEx": "Vedaldi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vedaldi et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "By observing that the above descriptor is pooling (weighted averaging) across displacement, [Dong et al., 2015] adds domain size pooling to this construction and suggests,", "startOffset": 92, "endOffset": 111}, {"referenceID": 1, "context": "The domain-size integration (3) is approximated by numerical sampling in [Dong et al., 2015], which is slow.", "startOffset": 73, "endOffset": 92}], "year": 2016, "abstractText": "Why has SIFT been so successful? Why its extension, DSP-SIFT, can further improve SIFT? Is there a theory that can explain both? How can such theory benefit real applications? Can it suggest new algorithms with reduced computational complexity or new descriptors with better accuracy for matching? We construct a general theory of local descriptors for visual matching. Our theory relies on concepts in energy minimization and heat diffusion. We show that SIFT and DSP-SIFT approximate the solution the theory suggests. In particular, DSP-SIFT gives a better approximation to the theoretical solution; justifying why DSP-SIFT outperforms SIFT. Using the developed theory, we derive new descriptors that have fewer parameters and are potentially better in handling affine deformations.", "creator": "LaTeX with hyperref package"}}}