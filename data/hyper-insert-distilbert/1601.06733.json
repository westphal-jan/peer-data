{"id": "1601.06733", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jan-2016", "title": "Long Short-Term Memory-Networks for Machine Reading", "abstract": "teaching machines to process all text books with psycholinguistics insights simultaneously is a rather challenging manual task. alternatively we propose an attentive research machine and reader that reads language text internally from left, to at right, whilst linking the word at the current fixation point to previous verbal words stored in underneath the memory bag as a kind variable of implicit information string parsing to facilitate understanding. the reader is equipped typically with a characteristic long short - term memory architecture, which, different drastically from previous work, notably has released a memory capacity tape ( instead of a corresponding memory cell ) to manually store explicitly the past textual information and adaptively use them internally without severe information compression. we greatly demonstrate demonstrating the excellent performance of aiding the knowledge machine proof reader in robust language modeling as relatively well broadly as the interactive downstream sentiment analysis interface and natural statistical language inference.", "histories": [["v1", "Mon, 25 Jan 2016 19:25:48 GMT  (737kb,D)", "http://arxiv.org/abs/1601.06733v1", null], ["v2", "Tue, 26 Jan 2016 20:48:02 GMT  (675kb,D)", "http://arxiv.org/abs/1601.06733v2", null], ["v3", "Mon, 1 Feb 2016 14:29:04 GMT  (675kb,D)", "http://arxiv.org/abs/1601.06733v3", null], ["v4", "Thu, 17 Mar 2016 13:28:16 GMT  (1354kb,D)", "http://arxiv.org/abs/1601.06733v4", null], ["v5", "Thu, 7 Apr 2016 09:53:49 GMT  (1354kb,D)", "http://arxiv.org/abs/1601.06733v5", null], ["v6", "Wed, 1 Jun 2016 12:27:42 GMT  (1623kb,D)", "http://arxiv.org/abs/1601.06733v6", "Fixed the incomparable parameters in experiments to previous work; fixed a few typos"], ["v7", "Tue, 20 Sep 2016 21:20:09 GMT  (1618kb,D)", "http://arxiv.org/abs/1601.06733v7", "Published as a conference paper at EMNLP 2016"]], "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["jianpeng cheng 0001", "li dong", "mirella lapata"], "accepted": true, "id": "1601.06733"}, "pdf": {"name": "1601.06733.pdf", "metadata": {"source": "CRF", "title": "Long Short-Term Memory-Networks for Machine Reading", "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "emails": ["jianpeng.cheng@ed.ac.uk", "li.dong@ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "We are generally interested in the task of developing intelligent machines with inspirations drawn from psycholinguistics, in order to understand text and encode it properly. It has been studied in psycholinguistics that humans process text by shifting their fixation point in a sequential order. When reading from left to right, a human reader forms a complete dynamic representation of the text portions up to the current fixation point. Modern language modeling tools, such as recurrent neural networks (RNN), agree with this cognitive behavior. RNNs treat each sentence as a sequence of words and recursively compose each word with its previous memory, until a meaning representation of the whole sentence has been derived.\nHowever, natural language is more complicated than pure sequences. What RNNs fail to address\nis the process of information gisting. While reading a sentence, human readers implicitly analyze the words they have read so far and infer dependency relations among them\u2014which is known as parsing or language comprehension in psycholinguistics. This meaning acquisition step has also been largely studied in computational linguistics but from a crucially different perspective. While psycholinguistic models incrementally build up the syntactic and semantic structures on the fly of reading, a natural language parser constructs a parse tree with a whole sentence as the input (Abney, 1989).\nWe propose a novel machine reader which offers a middle ground between the recurrent neural networks and psycholinguistic models. The reader processes texts from left to right while implicitly capturing the dependencies among tokens. It is designed by enforcing psycholinguistic insights in the architecture of a RNN\u2014as a way not only to better simulate the human behavior of language comprehension, but also to address the intrinsic limitations of the RNN architecture for long sequence modeling in an interactive environment.\nAlthough proven Turing complete (Siegelmann and Sontag, 1995), RNNs have at least two limiting factors in processing long input sequences. One concerns training and has to do with the vanishing and exploding gradient problem (Hochreiter, 1991; Bengio et al., 1994), which can be ameliorated with gated activation functions, such as the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), and the gradient clipping trick (Pascanu et al., 2012). The other limitation of RNNs relates to the memory compression effect. As an input sequence gets compressed into a single dense vector, it requires a sufficiently large memory capacity to store all the past ar X\niv :1\n60 1.\n06 73\n3v 1\n[ cs\n.C L\n] 2\n5 Ja\nn 20\n16\ninformation\u2014which indicates poor generalization ability to long sequences as well as a significant memory wastage for shorter ones. This mixture effect also prevents further structural analysis or adaptive modulation within the memories.\nThere have been a few recent papers on storing and querying sequence information with external memories (Weston et al., 2014; Sukhbaatar et al., 2015; Grefenstette et al., 2015). In this work multiple memory slots are used to memorize a sequence in streams, and the read and write operations to each slot are modeled as an attention mechanism depending on the current input token and the state of a neural controller. Inspired by this work, we equip our machine reader with a memory tape whose size grows with the input sequence1. As a major difference, we embed the memory tape within an LSTM unit, enabling the model to recurrently read texts without any external state controller.\nThe resulting model is a Long Short-Term Memory-Network (LSTMN) machine reader, which can be used for any sequence processing task. While reading a text from left to right, the LSTMN performs implicit dependency analysis of its tokens with soft attention, learning how to modulate the memory and extracting meanings (shown in Figure 1). We further explain how the internal attention complements with the external widely used between an encoder and a decoder. We validate the performance of our model in three tasks, including language modeling, sentiment analysis and natural language inference.\n1It is possible to restrict the size of the tape to simulate a limited memory span."}, {"heading": "2 Related Work", "text": "Machine Reading. Developing intelligent machines that read text is a long-standing goal in artificial intelligence. Most of the previous research in this area analyzes text with traditional NLP pipelines such as tagging and parsing, which are substantially different from the human reading behavior. These systems require a considerable amount of manual engineering and labeled training examples, restricting their application domain to the specific task and data distribution (Poon and Domingos, 2010). In fact, machine reading is inherently an unsupervised task, with the relations encountered during reading implicitly parsed on the fly (Etzioni et al., 2006). While end-to-end machine reading systems are still rare, a few recent works leverage recurrent neural networks to understand text from scratch (Mikolov et al., 2010; Bahdanau et al., 2014).\nRecurrent Sequence Modeling. Recurrent Neural Networks (RNNs) are powerful sequence modeling tools that can principally capture long input histories. There has been a surge of works on using RNNs to solve both single sequence modeling and sequence-to-sequence transduction. The later has assumed several guises in tasks like machine translation (Bahdanau et al., 2014), sentence compression (Rush et al., 2015), program execution (Zaremba and Sutskever, 2014), to name just a few. In practice, training RNNs to capture long term dependencies is challenging due to the wellknown exploding or vanishing gradient problem (Bengio et al., 1994). This motivates the development of models with gated activation functions such as the Long Short-Term Memory (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (Cho et al., 2014), and more advanced architectures that enhance the information flow within the network (Koutn\u0131\u0301k et al., 2014; Chung et al., 2015). Another problem relating RNNs is the memory compression effect. Since the model recursively combines all inputs into a single memory which is typically too small, RNNs are known to have difficulty in memorizing sequences well (Zaremba and Sutskever, 2014), which becomes a bottle neck for downstream tasks (Bahdanau et al., 2014). In the encoder-decoder architecture, this problem can be sidestepped with an attention mechanism to learn a soft alignment between the encoding and decoding states (Bahdanau et al., 2014). To the best of our knowledge, no attempts have been made to model\nattention within a sequence encoder. Differentiable Memories. Recently, there has been a resurgence of methods with external storage of differentiable memories, which obviate both long-term dependency and memory compression. This idea dates back to an early work of Das et al. (1992), who connect a recurrent neural network state controller with an external stack memory for learning context free grammars. Very recently, Weston et al. (2014) propose Memory Networks to explicitly segregate the memory storage from the computation of a neural network. This model can be trained end-to-end with a memory addressing mechanism closely related to soft attention (Sukhbaatar et al., 2015). Meng et al. (2015) apply a variant of the memory network in neural machine translation. Their model explicitly stores the source sequence in the memory, applies multiple steps of read-write transformations and finally decode the target sequence based on the transformed memory. Grefenstette et al. (2015) define a set of differentiable data structures (stacks, queues and dequeues) as memories controlled by a recurrent neural network. Their model has shown promising results in simple sequence transduction tasks, such as copying. Concurrent to our work is the pre-print Recurrent Memory Networks (Tran et al., 2016), which equips an LSTM with an external memory block interacting with the hidden state of the LSTM. To the best of our knowledge, these models introduce external memories that interact with a neural controller. In comparison, our work directly enhances the internal memory of an LSTM."}, {"heading": "3 The Machine Reader", "text": "In this section we propose a novel machine reader inspired by psycholinguistics. The core of the reader is a Long Short-Term Memory recurrent neural network with an extended memory tape that explicitly simulates the human memory span. The reader performs implicit dependency analysis with an attention-based memory addressing mechanism at every input time step. In the following we first review the standard Long Short-Term Memory unit."}, {"heading": "3.1 Long Short-Term Memory", "text": "A Long Short-Term Memory (LSTM) recurrent neural network processes a variable-length sequence x = (x1, x2, \u00b7 \u00b7 \u00b7 , xn) by incrementally\nadding up new content into a single slot of maintained memory, with gates controlling the extent to which new content should be memorized, old content should be erased and current content should be exposed. At time step t, the memory ct and the hidden state ht are updated with the following equations\nit ft ot c\u0302t\n =  \u03c3 \u03c3 \u03c3\ntanh W \u00b7 [ht\u22121, st] (1) ct = ft ct\u22121 + it c\u0302t (2)\nht = ot tanh(ct) (3)\nwhere i, f and o are gate activations. Compared to the standard RNN, LSTM separates the memory c from the hidden state h that actually interacts with the environment when computing the output. This network principally memorizes long input histories, conditioned on which future predictions can be made. In the case of language modeling, the model estimates the probability distribution of the next word as follows:\np(xt+1 = w|x1, \u00b7 \u00b7 \u00b7 , xt) = exp(u(w, ht))/Z (4) where w is a word in the vocabulary V , and u is a scoring function that relates w to the current hidden state ht. Z = \u2211 w\u2032\u2208V exp(u(w\n\u2032, ht)) is the normalization constant.\nA limitation of the standard LSTM/RNN lies in the memory compression effect. Since all the past inputs get recursively compressed into a single fixed-size vector, the model is potentially vulnerable to suffer from information loss or inefficient usage of the past information. Cho et al. (2014) show that the performance of a sequenceto-sequence model drops significantly as the input sequence becomes longer."}, {"heading": "3.2 Long Short-Term Memory-Network", "text": "We aim at developing a machine reader that explicitly stores memory segments so that it learns how to analyze and modulate past information in order to facilitate the understanding of the present input. To this end, we modify the standard LSTM structure by replacing the memory cell with a memory network, whose size grows with the input sequence. The resulting Long Short-Term Memory-Network (LSTMN) stores the input at\neach time step with a unique memory slot, obviating the problem of information compression whilst enabling adaptive modulation within the memories. While it is feasible to apply both read and write operations to the memory network based on the current input token, we focus on the read operation only\u2014as a way to attentively link the current input token to previous contents in the memory and accordingly select useful memory contents when processing the current token. Although it is not the focus of this work, the significance of the write operation can be similarly justified\u2014as a way to incrementally update the previous memories to correct wrong understandings when processing for example garden path sentences.\nThe architecture of the LSTMN is shown in Figure 2. At each time step, the model computes the memory activation based on the present input token and the previous attention vector. Then it uses the adaptively weighted hidden contents to compute various gate activations like the LSTM. Finally it mixes the adaptively weighted memory contents with the current input token to obtain the new input memory. In this model, each input token corresponds to one memory slot, which stores the transformed input representation under the given context. Formally given the current input xt, previous memory tape Ct\u22121 = (c1, \u00b7 \u00b7 \u00b7 , ct\u22121) and previous hidden states Ht\u22121 = (h1, \u00b7 \u00b7 \u00b7 , ht\u22121), the model computes at the time step t the values of ct and ht as follows:\nati = v T tanh(Whhi +Wxxt +Wh\u0303h\u0303t\u22121) (5)\nsti = softmax(a t i) (6)[\nh\u0303t c\u0303t\n] = t\u22121\u2211 i=1 sti \u00b7 [ hi ci ] (7)\n it ft ot c\u0302t  =  \u03c3 \u03c3 \u03c3 tanh W \u00b7 [h\u0303t, xt] (8) ct = ft c\u0303t + it c\u0302t (9)\nht = ot tanh(ct) (10)\nwhere v, Wh and Wx are weights of the network. Compared to the standard LSTM in Equation 1 to 3, we additionally introduce an attention layer to compute the adaptive memory representation c\u0303t and hidden representation (i.e., the attention vector) h\u0303t, which are then used in computing the gated activations it, ft, ot and the transformed memory c\u0302t. The feedback of the previous attention vector to Equation 5 is optional, but doing so can help the model to recurrently remember the previous memory activation choices (Rockta\u0308schel et al., 2015; Wang and Jiang, 2015)."}, {"heading": "3.3 Deep LSTMNs", "text": "It is possible to construct deep LSTMNs by stacking multiple hidden layers on top of each other, resembling a stacked LSTM (Graves, 2013), or from another perspective, a multi-hop memory network (Sukhbaatar et al., 2015). This is achieved by feeding the attention vector h\u0303kt of the k layer as input to the (k + 1)th layer. We find skip-connections (Graves, 2013) across layers important for easing the training and information flow."}, {"heading": "4 LSTMNs for Dual Sequence Modeling", "text": "Practical natural language tasks, such as machine translation, are concerned more with dual sequences rather than a single sequence. Central to these tasks is a dual sequence processor (e.g., an encoder-decoder), where the second sequence (i.e., target) is processed conditioned on the first one (i.e., source). In this section we draw connections between the inherent attention mechanism of the LSTMN and that widely used in between two sequences. We then explain how an LSTMN can be used in the dual sequence processing task.\nIn general, the intra attention within a sequence and the inter attention between two sequences complement each other. While inter attention derives the alignment between the source and target tokens, intra attention provides implicit dependency analysis within a sequence, resulting in enhanced memories that could benefit subsequent inter alignment. In the following we propose two\nways of using the LSTMN in a dual architecture, shown in Figure 3a and 3b respectively.\nShallow Attention Fusion. Shallow fusion treats LSTMN as a separate module that is readily plugged into a dual architecture (e.g, an encoderdecoder), in place of a standard RNN or LSTM. In the complete picture, both of the two sequence processors are modeled as LSTMNs with intraattention. Meanwhile, we apply inter attention between them at every time step when the target sequence is analyzed, same as the RNNSearch (Bahdanau et al., 2014).\nDeep Attention Fusion. Deep fusion aims to fuse inter and intra attention initiated by the target sequence processor, in a way that both previous memories of the target and the entire memories of the source are used to compute the current inputting memory. Denote source memories and hidden states with A = [\u03b11, \u00b7 \u00b7 \u00b7 , \u03b1m] and Y = [\u03b31, \u00b7 \u00b7 \u00b7 , \u03b3m] respectively ,where m is the length of the source sequence conditioned on. We can compute the inter attention at time step t when processing the target sequence as follows\nbtj = u T tanh(W\u03b3\u03b3j +Wxxt +W\u03b3\u0303 \u03b3\u0303t\u22121) (11)\nptj = softmax(b t j) (12)[\n\u03b3\u0303t \u03b1\u0303t\n] = m\u2211 j=1 ptj \u00b7 [ \u03b3j \u03b1j ] (13)\nand then we can cast the source memory to the target with another gating operation:\nrt = \u03c3(Wr[\u03b3\u0303t, xt]) (14)\nct = ft c\u0303t\u22121 + it c\u0302t + rt \u03b1\u0303t (15)\nht = ot tanh(ct) (16)\nThe major change of deep fusion lies in the recurrent storage of the inter alignment vector in the target memory network. This tighter integration is potentially helpful for sequence to sequence transduction tasks which involve a significant amount of reordering, in spirit similar to the Grid LSTM Reencoder (Kalchbrenner et al., 2015)."}, {"heading": "5 Experiments", "text": "In this section we detail a series of experiments conducted to evaluate the performance of the LSTMN machine reader. We start by specifying general aspects of the implementation."}, {"heading": "5.1 Implementation Details", "text": "All experiments are performed on an Nvidia GTX 980 Graphic Card. The size of the word vectors, hidden vectors, and memory vectors are all set to 300. We initialized model weights with a uniform distribution between [-0.05, 0.05]."}, {"heading": "5.2 Language Modeling", "text": "We first train our model on the language modeling task of the English Penn Treebank dataset. Following the preprocessing of Mikolov et al. (2010), we use splits 0-20 for training, 21-22 for validation, and 23-24 for test. The entire dataset contains approximately 1 million tokens and a vocabulary of size 10k. As in the standard language modeling task, we use as evaluation metric the perplexity PPL = exp(NLL/T ), where NLL denotes the negative log likelihood of the entire test set and T is the corresponding number of tokens. We use stochastic gradient descent for optimization with an initial learning rate 0.65, which decays by a factor 0.85 per epoch if no significant improvement has been observed on the validation set. We renormalize the gradient if its norm is greater than 5.\nWe compared the single-layer LSTMN and the stacked LSTMN with a variety of baselines. The first one is the Kneser-Ney 5-gram language model (KN5). It generally serves as a non-neural baseline for the language modeling task. Besides that, we compare against the standard RNN and\nLSTM. We also implemented more sophisticated LSTM variations, including the stacked (multilayer) LSTM (sLSTM), the gated-feedback LSTM (gLSTM) (Chung et al., 2015) and the depth-gated LSTM (dLSTM) (Yao et al., 2015). The gatedfeedback LSTM is a generalization of the clockwork RNN (Koutn\u0131\u0301k et al., 2014), with feedback gates connecting the hidden states across multiple time steps as an adaptive control of the information flow. The depth-gated LSTM is a one dimensional special case of the Grid LSTM (Kalchbrenner et al., 2015), with extended memory cells on the depth dimension to capture the linear dependency on that dimension. For all the deep architectures, we set the number of layers to 3.\nThe results of the language modeling task is shown in Table 1. Both numbers for the KN-5 and RNN are copied from Mikolov et al. (2014). As we can see, the single-layer LSTMN outperformed the standard baselines with a significant margin. Among all the deep architectures the three-layer LSTMN also performed the best. It should be noted that the gLSTM and dLSTM extend the LSTM in the depth dimension, complementing to the LSTMN with extension on the width dimension. It is possible to develop more powerful models by combining the two.\nWe can study the memory activation mechanism of the machine reader by visualizing the attention scores. Figure 3 shows two sentences (randomly sampled with length 10-15) from the Penn Treebank validation set. The darkness of the color indicates the weight of the attention.\nAlthough we explicitly encourage the reader to attend to any memory slot, it turns out that much attention focuses on the recent memories. This makes sense because recent memories summarize more information (but with a focus on recent words). It also agrees with the linguistic statement that long-term dependencies are relatively rare. We observe that for some cases the attention ac-\ntivation agrees with the true dependency between words, and their concurrence frequency. Besides, it is interesting to find that the reader learns to skip unimportant words which do not draw much attention in general."}, {"heading": "5.3 Sentiment Analysis", "text": "The second experiment concerns the prediction of sentiment labels of sentences. We used the Stanford Sentiment Treebank (Socher et al., 2013), which contains 11,855 sentences and their 5-way fine-grained sentiment labels (very negative, negative, neutral, positive, very positive). Following previous experiments on this dataset, we use 8,544 sentences for training, 1,101 for validation and 2,210 for test. In addition, it is possible to perform another binary classification task by removing the neutral label. This results in 6,920 sentence for training, 872 for validation and 1,821 for test. We report results for both the fine-grained and the binary classification tasks.\nAs shown in Table 2, we compare our models with 7 existing systems and 2 LSTM base-\nlines. For the LSTMNs, we predict the sentential sentiment label based on the averaged hidden vector passed into a 2-layer neural network classifier with ReLU as the activation function. We use pre-trained word2vec embeddings to initialize the word embeddings. The vocabulary coverage of the word2vec on this dataset is 14,549 out of 15,220. For words within the coverage, we scale their gradient weight by a factor of 0.1, while other words are updated normally during training. In addition, we find that applying a standard language modeling objective for the first few rounds of training helps warming up the network and is always helpful for the classification task. We use Adam (Kingma and Ba, 2014) for optimization with the two momentum parameters set to 0.9 and 0.999 respectively. The initial learning rate is set to 2E-3.\nAs the results reveal, both 1- and 2-layer LSTMNs outperform the LSTM baselines while achieving numbers comparable to state-of-the-art. We can similarly visualize the alignment between words from the attention scores. Figure 5 shows one sentence from the training set. It is interesting to find out that the network learns to align important sentimental words. One interpretation is that the network needs such alignment in order to make decisions by jointly considering these words."}, {"heading": "5.4 Natural Language Inference", "text": "Natural language inference deals with the relational reasoning between a pair of sentences. In this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation (entailment, contradiction or neutral). After removing those with unknown labels, we end up having 549,367 pairs for training, 9,842 for devel-\nopment and 9,824 for test. The vocabulary size is 37,249. Recent approaches use two sequential LSTMs to encode the premise and the hypothesis respectively, and apply neural attention to reason about their logical relationship (Rockta\u0308schel et al., 2015; Wang and Jiang, 2015). It has been found in Rockta\u0308schel et al. (2015) that processing the hypothesis conditioned on the premise results in a significant performance boost, making the whole system a non-standard encoder-decoder architecture.\nWe tackle this task with three different methods with the LSTMNs. As the standard approach, we use two dependent LSTMNs to read the premise and hypothesis, after which we match the premise and hypothesis by comparing their hidden state tapes. We perform average pooling within each network Y = [\u03b31, \u00b7 \u00b7 \u00b7 , \u03b3m] and H = [h1, \u00b7 \u00b7 \u00b7 , hn], and concatenate the two hidden vectors [\u03b3avg, havg] to form the input to a neural network classifier.\nIn the second approach we apply word-by-word neural attention (Rockta\u0308schel et al., 2015) in a way that the alignments between words in the two sentences are implicitly addressed. At each time step of the decoding, we compute the attention over Y = [\u03b31, \u00b7 \u00b7 \u00b7 , \u03b3m] given ht followed by a weighted average to get an alignment vector \u03b3\u0304t. Different from Rockta\u0308schel et al. (2015), we did not feed the previous alignment vector to the attention scorer, as a simplified version of their model. We then compute the average for each of the two tapes Y\u0304 = [\u03b3\u03041, \u00b7 \u00b7 \u00b7 , \u03b3\u0304n] and H = [h1, \u00b7 \u00b7 \u00b7 , hn], and feed the concatenation of two averages [\u03b3\u0304avg, havg] into the classifier. We tested both shallow\nand deep attention on this model. With a slight difference, the third approach (denoted by mLSTMN) employs an upper layer LSTM to recursively compose each pair [\u03b3\u0304t, ht], instead of the average pooling. This method proposed in Wang and Jiang (2015) inspects hidden state pairs to determine the overall matching quality. However, we did not use the recurrent feedback of the upper layer LSTM to reinforce the attention scorer, again as a simplified version.\nIn all approaches, we initialize word vectors with the pre-trained word2vec embeddings, which covers 28,567 words in the dataset. For these words, we scale their gradient weights by a factor of 0.1 during training. For other words beyond the coverage, we initialize them randomly with Gaussian (\u00b5=0, \u03c3=1) samples and update them normally. ReLU is used as the activation function in the classification layer. We use Adam (Kingma and Ba, 2014) for optimization with the two momentum parameters set to 0.9 and 0.999 respectively, and initial learning rate 2E-3.\nWe compared the results of our models with a simple bag-of-words baseline and some of the previously published results. The baseline averages the pre-trained word2vec embeddings for words in each sentence and concatenate them to form input features to a logistic regression classifier. We report the training and test accuracies for each model.\nThe results of various models are presented in Table 3. In general, LSTMNs achieve better performance than the LSTMs both with and without attention. As to the two ways of fusing intra and inter attention, they perform on comparable with each other on this task. In addition, we observe that applying an upper recurrent layer to process paired hidden states leads to a marginal improvement for our model. This improvement is less sig-\nnificant than Wang and Jiang (2015), possibly due to the simplified attention scoring function.\nWe provide additional analysis of the model by visualizing the inter attention between the two LSTMNs used for the premise and hypothesis. We selected the same sentence as in Wang and Jiang (2015). As shown in Figure 6, we observe high alignment scores between related words in the sentence pair. We notice structured alignments between groups of words, such as for the phrases in the cold weather and in the snow."}, {"heading": "6 Discussion and Future Work", "text": "We propose a novel machine reader that reads a sequence in a left to right manner while implicitly addresses its internal structures on the fly of reading. The reader employs a Long Short-Term Memory architecture with an extended memory tape, explicitly storing all the past input informations without recursive memory compressions. This architecture enables implicit dependency analysis and adaptive memory modulation driven by an internal attention mechanism.\nWhile the idea regarding internal attention is general, many components in the model are flexible in design. For example, the internal attention mechanism can be analogously applied in a Gated Recurrent Unit (Cho et al., 2014). The way how the current input interacts with the memories may vary as well. Besides, it is reasonable to introduce the memory updating mechanism, which can be modeled as the read operation with another set of attention parameters. This is potentially helpful for the machine reader to correct wrong understandings in the past.\nOn the other hand, it would be interesting to develop a more general framework that integrates explicit natural language information within the implicit attentional network of the reader."}], "references": [{"title": "A computational model of human parsing", "author": ["Steven P Abney"], "venue": "Journal of psycholinguistic Research,", "citeRegEx": "Abney.,? \\Q1989\\E", "shortCiteRegEx": "Abney.", "year": 1989}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "A convolutional neural network for modelling sentences", "author": ["Blunsom et al.2014] Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Blunsom et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2014}, {"title": "A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder", "author": ["Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks. arXiv preprint arXiv:1502.02367", "author": ["Chung et al.2015] Junyoung Chung", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["Das et al.1992] Sreerupa Das", "C Lee Giles", "GuoZheng Sun"], "venue": null, "citeRegEx": "Das et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Das et al\\.", "year": 1992}, {"title": "Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Learning to transduce", "author": ["Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": null, "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Untersuchungen zu dynamischen neuronalen netzen. Diploma, Technische Universit\u00e4t M\u00fcnchen", "author": ["Sepp Hochreiter"], "venue": null, "citeRegEx": "Hochreiter.,? \\Q1991\\E", "shortCiteRegEx": "Hochreiter.", "year": 1991}, {"title": "Deep recursive neural networks for compositionality in language", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "A clockwork rnn", "author": ["Klaus Greff", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1402.3511", "citeRegEx": "Koutn\u0131\u0301k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutn\u0131\u0301k et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "A deep memory-based architecture for sequence-to-sequence learning", "author": ["Meng et al.2015] Fandong Meng", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Qun Liu"], "venue": "arXiv preprint arXiv:1506.06442", "citeRegEx": "Meng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Meng et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH 2010,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753", "author": ["Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks. arXiv preprint arXiv:1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Machine reading: A\u201d killer app", "author": ["Poon", "Domingos2010] Hoifung Poon", "Pedro Domingos"], "venue": null, "citeRegEx": "Poon et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2010}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1509.06664", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "On the computational power of neural nets", "author": ["Siegelmann", "Sontag1995] Hava T Siegelmann", "Eduardo D Sontag"], "venue": "Journal of computer and system sciences,", "citeRegEx": "Siegelmann et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Siegelmann et al\\.", "year": 1995}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank. Citeseer", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Recurrent memory network for language modeling", "author": ["Tran et al.2016] Ke Tran", "Arianna Bisazza", "Christof Monz"], "venue": "arXiv preprint arXiv:1601.01272", "citeRegEx": "Tran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Learning natural language inference with lstm", "author": ["Wang", "Jiang2015] Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1512.08849", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Depth-gated recurrent neural networks. arXiv preprint arXiv:1508.03790", "author": ["Yao et al.2015] Kaisheng Yao", "Trevor Cohn", "Katerina Vylomova", "Kevin Duh", "Chris Dyer"], "venue": null, "citeRegEx": "Yao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Learning to execute. arXiv preprint arXiv:1410.4615", "author": ["Zaremba", "Sutskever2014] Wojciech Zaremba", "Ilya Sutskever"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "One concerns training and has to do with the vanishing and exploding gradient problem (Hochreiter, 1991; Bengio et al., 1994), which can be ameliorated with gated activation functions, such as the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), and the gradient clipping trick (Pascanu et al.", "startOffset": 86, "endOffset": 125}, {"referenceID": 2, "context": "One concerns training and has to do with the vanishing and exploding gradient problem (Hochreiter, 1991; Bengio et al., 1994), which can be ameliorated with gated activation functions, such as the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), and the gradient clipping trick (Pascanu et al.", "startOffset": 86, "endOffset": 125}, {"referenceID": 20, "context": ", 1994), which can be ameliorated with gated activation functions, such as the Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997), and the gradient clipping trick (Pascanu et al., 2012).", "startOffset": 177, "endOffset": 199}, {"referenceID": 27, "context": "There have been a few recent papers on storing and querying sequence information with external memories (Weston et al., 2014; Sukhbaatar et al., 2015; Grefenstette et al., 2015).", "startOffset": 104, "endOffset": 177}, {"referenceID": 9, "context": "There have been a few recent papers on storing and querying sequence information with external memories (Weston et al., 2014; Sukhbaatar et al., 2015; Grefenstette et al., 2015).", "startOffset": 104, "endOffset": 177}, {"referenceID": 18, "context": "cent works leverage recurrent neural networks to understand text from scratch (Mikolov et al., 2010; Bahdanau et al., 2014).", "startOffset": 78, "endOffset": 123}, {"referenceID": 1, "context": "cent works leverage recurrent neural networks to understand text from scratch (Mikolov et al., 2010; Bahdanau et al., 2014).", "startOffset": 78, "endOffset": 123}, {"referenceID": 1, "context": "The later has assumed several guises in tasks like machine translation (Bahdanau et al., 2014), sentence", "startOffset": 71, "endOffset": 94}, {"referenceID": 23, "context": "compression (Rush et al., 2015), program execution (Zaremba and Sutskever, 2014), to name just a few.", "startOffset": 12, "endOffset": 31}, {"referenceID": 2, "context": "In practice, training RNNs to capture long term dependencies is challenging due to the wellknown exploding or vanishing gradient problem (Bengio et al., 1994).", "startOffset": 137, "endOffset": 158}, {"referenceID": 15, "context": ", 2014), and more advanced architectures that enhance the information flow within the network (Koutn\u0131\u0301k et al., 2014; Chung et al., 2015).", "startOffset": 94, "endOffset": 137}, {"referenceID": 6, "context": ", 2014), and more advanced architectures that enhance the information flow within the network (Koutn\u0131\u0301k et al., 2014; Chung et al., 2015).", "startOffset": 94, "endOffset": 137}, {"referenceID": 1, "context": "Since the model recursively combines all inputs into a single memory which is typically too small, RNNs are known to have difficulty in memorizing sequences well (Zaremba and Sutskever, 2014), which becomes a bottle neck for downstream tasks (Bahdanau et al., 2014).", "startOffset": 242, "endOffset": 265}, {"referenceID": 1, "context": "In the encoder-decoder architecture, this problem can be sidestepped with an attention mechanism to learn a soft alignment between the encoding and decoding states (Bahdanau et al., 2014).", "startOffset": 164, "endOffset": 187}, {"referenceID": 27, "context": "This model can be trained end-to-end with a memory addressing mechanism closely related to soft attention (Sukhbaatar et al., 2015).", "startOffset": 106, "endOffset": 131}, {"referenceID": 7, "context": "This idea dates back to an early work of Das et al. (1992), who connect a recurrent neural network state controller with an external stack memory for learning context free grammars.", "startOffset": 41, "endOffset": 59}, {"referenceID": 7, "context": "This idea dates back to an early work of Das et al. (1992), who connect a recurrent neural network state controller with an external stack memory for learning context free grammars. Very recently, Weston et al. (2014) propose Memory Networks to explicitly segregate the memory storage from the computation of a neural network.", "startOffset": 41, "endOffset": 218}, {"referenceID": 9, "context": "Grefenstette et al. (2015) define a set of differentiable data structures (stacks, queues and dequeues) as memories controlled by a recurrent neural network.", "startOffset": 0, "endOffset": 27}, {"referenceID": 29, "context": "Concurrent to our work is the pre-print Recurrent Memory Networks (Tran et al., 2016), which equips an LSTM with an external memory block interacting with the hidden state of the LSTM.", "startOffset": 66, "endOffset": 85}, {"referenceID": 8, "context": "It is possible to construct deep LSTMNs by stacking multiple hidden layers on top of each other, resembling a stacked LSTM (Graves, 2013), or from", "startOffset": 123, "endOffset": 137}, {"referenceID": 27, "context": "another perspective, a multi-hop memory network (Sukhbaatar et al., 2015).", "startOffset": 48, "endOffset": 73}, {"referenceID": 8, "context": "We find skip-connections (Graves, 2013) across layers important for easing", "startOffset": 25, "endOffset": 39}, {"referenceID": 1, "context": "Meanwhile, we apply inter attention between them at every time step when the target sequence is analyzed, same as the RNNSearch (Bahdanau et al., 2014).", "startOffset": 128, "endOffset": 151}, {"referenceID": 18, "context": "lowing the preprocessing of Mikolov et al. (2010), we use splits 0-20 for training, 21-22 for validation, and 23-24 for test.", "startOffset": 28, "endOffset": 50}, {"referenceID": 6, "context": "We also implemented more sophisticated LSTM variations, including the stacked (multilayer) LSTM (sLSTM), the gated-feedback LSTM (gLSTM) (Chung et al., 2015) and the depth-gated LSTM (dLSTM) (Yao et al.", "startOffset": 137, "endOffset": 157}, {"referenceID": 31, "context": ", 2015) and the depth-gated LSTM (dLSTM) (Yao et al., 2015).", "startOffset": 41, "endOffset": 59}, {"referenceID": 15, "context": "The gatedfeedback LSTM is a generalization of the clockwork RNN (Koutn\u0131\u0301k et al., 2014), with feedback", "startOffset": 64, "endOffset": 87}, {"referenceID": 18, "context": "Both numbers for the KN-5 and RNN are copied from Mikolov et al. (2014). As we can see, the single-layer LSTMN outperformed the standard baselines with a significant margin.", "startOffset": 50, "endOffset": 72}, {"referenceID": 26, "context": "We used the Stanford Sentiment Treebank (Socher et al., 2013), which contains 11,855 sentences and their 5-way fine-grained sentiment labels (very negative, negative, neutral, positive, very positive).", "startOffset": 40, "endOffset": 61}, {"referenceID": 25, "context": "Models Fine-grained Binary RAE (Socher et al., 2011) 43.", "startOffset": 31, "endOffset": 52}, {"referenceID": 26, "context": "4 RNTN (Socher et al., 2013) 45.", "startOffset": 7, "endOffset": 28}, {"referenceID": 3, "context": "4 DCNN (Blunsom et al., 2014) 48.", "startOffset": 7, "endOffset": 29}, {"referenceID": 13, "context": "8 CNN (Kim, 2014) 48.", "startOffset": 6, "endOffset": 17}, {"referenceID": 28, "context": "6 CT-LSTM (Tai et al., 2015) 51.", "startOffset": 10, "endOffset": 28}, {"referenceID": 28, "context": "0 LSTM (Tai et al., 2015) 46.", "startOffset": 7, "endOffset": 25}, {"referenceID": 28, "context": "3 2-layer LSTM (Tai et al., 2015) 46.", "startOffset": 15, "endOffset": 33}, {"referenceID": 4, "context": "In this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation (entailment, contradiction or neutral).", "startOffset": 76, "endOffset": 97}, {"referenceID": 22, "context": "respectively, and apply neural attention to reason about their logical relationship (Rockt\u00e4schel et al., 2015; Wang and Jiang, 2015).", "startOffset": 84, "endOffset": 132}, {"referenceID": 22, "context": "respectively, and apply neural attention to reason about their logical relationship (Rockt\u00e4schel et al., 2015; Wang and Jiang, 2015). It has been found in Rockt\u00e4schel et al. (2015) that processing the hypothesis conditioned on the premise results in a", "startOffset": 85, "endOffset": 181}, {"referenceID": 22, "context": "In the second approach we apply word-by-word neural attention (Rockt\u00e4schel et al., 2015) in a way that the alignments between words in the two sentences are implicitly addressed.", "startOffset": 62, "endOffset": 88}, {"referenceID": 22, "context": "In the second approach we apply word-by-word neural attention (Rockt\u00e4schel et al., 2015) in a way that the alignments between words in the two sentences are implicitly addressed. At each time step of the decoding, we compute the attention over Y = [\u03b31, \u00b7 \u00b7 \u00b7 , \u03b3m] given ht followed by a weighted average to get an alignment vector \u03b3\u0304t. Different from Rockt\u00e4schel et al. (2015), we did not feed the previous alignment vector to the attention scorer, as a simplified version of their model.", "startOffset": 63, "endOffset": 378}, {"referenceID": 4, "context": "8 LSTM (Bowman et al., 2015) 84.", "startOffset": 7, "endOffset": 28}, {"referenceID": 4, "context": "6 Hardcrafted features (Bowman et al., 2015) 99.", "startOffset": 23, "endOffset": 44}, {"referenceID": 22, "context": "2 LSTM shared (Rockt\u00e4schel et al., 2015) 84.", "startOffset": 14, "endOffset": 40}, {"referenceID": 22, "context": "4 LSTM attention (Rockt\u00e4schel et al., 2015) 85.", "startOffset": 17, "endOffset": 43}], "year": 2017, "abstractText": "Teaching machines to process text with psycholinguistics insights is a challenging task. We propose an attentive machine reader that reads text from left to right, whilst linking the word at the current fixation point to previous words stored in the memory as a kind of implicit information parsing to facilitate understanding. The reader is equipped with a Long ShortTerm Memory architecture, which, different from previous work, has a memory tape (instead of a memory cell) to store the past information and adaptively use them without severe information compression. We demonstrate the excellent performance of the machine reader in language modeling as well as the downstream sentiment analysis and natural language inference.", "creator": "LaTeX with hyperref package"}}}