{"id": "1602.00367", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Feb-2016", "title": "Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers", "abstract": "interactive document classification tasks were areas primarily historically tackled in at word level. recent research that works visually with isolated character - name level inputs shows well several benefits over older word - response level research approaches presented such as natural sound incorporation possibilities of morphological morphemes and better handling of rare words. we carefully propose a neural network architecture device that utilizes both generic convolution nodes and recurrent layers to better efficiently easily encode irrelevant character / inputs. afterward we validate the proposed model on across eight large scale document processing classification tasks and compare respectively with select character - level convolution - size only models. it achieves comparable retrieval performances with sounding much less parameters.", "histories": [["v1", "Mon, 1 Feb 2016 02:53:41 GMT  (86kb,D)", "http://arxiv.org/abs/1602.00367v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yijun xiao", "kyunghyun cho"], "accepted": false, "id": "1602.00367"}, "pdf": {"name": "1602.00367.pdf", "metadata": {"source": "CRF", "title": "Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers", "authors": ["Yijun Xiao", "Kyunghyun Cho"], "emails": ["ryjxiao@nyu.edu", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Document classification is a task in natural language processing where one needs to assign a single or multiple predefined categories to a sequence of text. A conventional approach to document classification generally consists of a feature extraction stage followed by a classification stage. For instance, it is usual to use a TF-IDF vector of a given document as an input feature to a subsequent classifier.\nMore recently, it has become more common to use a deep neural network, which jointly performs feature extraction and classification, for document classification (Kim, 2014; Mesnil et al., 2014; Socher et al., 2013; Carrier and Cho, 2014). In most cases, an input document is represented as a sequence of words, of which each is presented as a one-hot vector.1 Each word in the sequence is projected into a\n1 A one-hot vector of the i-th word is a binary vector whose\ncontinuous vector space by being multiplied with a weight matrix, forming a sequence of dense, realvalued vectors. This sequence is then fed into a deep neural network which processes the sequence in multiple layers, resulting in a prediction probability. This whole pipeline, or a network, is tuned jointly to maximize the classification accuracy on a training set.\nOne important aspect of these recent approaches based on deep learning is that they often work at the level of words. Despite its recent success, the wordlevel approach has a number of major shortcomings. First, it is statistically inefficient, as each word token is considered separately and estimated by the same number of parameters, despite the fact that many words share common root, prefix or suffix. This can be overcome by using an external mechanism to segment each word and infer its components (root, prefix, suffix), but this is not desirable as the mechanism is highly language-dependent and is tuned independently from the target objective of document classification.\nSecond, the word-level approach cannot handle out-of-vocabulary words. Any word that is not present or rare in a training corpus, is mapped to an unknown word token. This is problematic, because the model cannot handle typos easily, which happens frequently in informal documents such as postings from social network sites. Also, this makes it difficult to use a trained model to a new domain, as there may be large mismatch between the domain of the training corpus and the target domain.\nelements are all zeros, except for the i-th element which is set to one.\nar X\niv :1\n60 2.\n00 36\n7v 1\n[ cs\n.C L\n] 1\nF eb\n2 01\nRecently this year, a number of researchers have noticed that it is not at all necessary for a deep neural network to work at the word level. As long as the document is represented as a sequence of one-hot vectors, the model works without any change, regardless of whether each one-hot vector corresponds to a word, a sub-word unit or a character. Based on this intuition, Kim et al. (Kim et al., 2015) and Ling et al. (Ling et al., 2015) proposed to use a character sequence as an alternative to the word-level one-hot vector. A similar idea was applied to dependency parsing in (Ballesteros et al., 2015). The work in this direction, most relevant to this paper, is the character-level convolutional network for document classification by Zhang et al. (Zhang et al., 2015).\nThe character-level convolutional net in (Zhang et al., 2015) is composed of many layers of convolution and max-pooling, similarly to the convolutional network in computer vision (see, e.g., (Krizhevsky et al., 2012).) Each layer first extracts features from small, overlapping windows of the input sequence and pools over small, non-overlapping windows by taking the maximum activations in the window. This is applied recursively (with untied weights) for many times. The final convolutional layer\u2019s activation is flattened to form a vector which is then fed into a small number of fully-connected layers followed by the classification layer.\nWe notice that the use of a vanilla convolutional network for character-level document classification has one shortcoming. As the receptive field of each convolutional layer is often small (7 or 3 in (Zhang et al., 2015),) the network must have many layers in order to capture long-term dependencies in an input sentence. This is likely the reason why Zhang et al. (Zhang et al., 2015) used a very deep convolutional network with six convolutional layers followed by two fully-connected layers.\nIn order to overcome this inefficiency in modeling a character-level sequence, in this paper we propose to make a hybrid of convolutional and recurrent networks. This was motivated by recent successes of applying recurrent networks to natural languages (see, e.g., (Cho et al., 2014; Sundermeyer et al., 2015)) and from the fact that the recurrent network can efficiently capture long-term dependencies even with a single layer. The hybrid model processes\nan input sequence of characters with a number of convolutional layers followed by a single recurrent layer. Because the recurrent layer, consisting of either gated recurrent units (GRU, (Cho et al., 2014) or long short-term memory units (LSTM, (Hochreiter and Schmidhuber, 1997; Gers et al., 2000), can efficiently capture long-term dependencies, the proposed network only needs a very small number of convolutional layers.\nWe empirically validate the proposed model, to which we refer as a convolution-recurrent network, on the eight large-scale document classification tasks from (Zhang et al., 2015). We mainly compare the proposed model against the convolutional network in (Zhang et al., 2015) and show that it is indeed possible to use a much smaller model to achieve the same level of classification performance when a recurrent layer is put on top of the convolutional layers."}, {"heading": "2 Basic Building Blocks: Neural Network Layers", "text": "In this section, we describe four basic layers in a neural network that will be used later to constitute a single network for classifying a document."}, {"heading": "2.1 Embedding Layer", "text": "As mentioned earlier, each document is represented as a sequence of one-hot vectors. A one-hot vector of the i-th symbol in a vocabulary is a binary vector whose elements are all zeros except for the i-th element which is set to one. Therefore, each document is a sequence of T one-hot vectors (x1,x2, . . . ,xT ).\nAn embedding layer projects each of the onehot vectors into a d-dimensional continuous vector space Rd. This is done by simply multiplying the one-hot vector from left with a weight matrix W \u2208 Rd\u00d7|V |, where |V | is the number of unique symbols in a vocabulary:\net = Wxt.\nAfter the embedding layer, the input sequence of one-hot vectors becomes a sequence of dense, realvalued vectors (e1, e2, . . . , eT )."}, {"heading": "2.2 Convolutional Layer", "text": "A convolutional layer consists of two stages. In the first stage, a set of d\u2032 filters of receptive field size r,\nF \u2208 Rd\u2032\u00d7r, is applied to the input sequence: ft = \u03c6(F [ et\u2212(r/2)+1; . . . ; et; . . . , et+(r/2) ] ),\nwhere \u03c6 is a nonlinear activation function such as tanh or a rectifier. This is done for every time step of the input sequence, resulting in a sequence F = (f1, f2, . . . , fT ).\nThe resulting sequence F is max-pooled with size r\u2032:\nf \u2032t = max ( f(t\u22121)\u00d7r\u2032+1, . . . , ft\u00d7r\u2032 ) ,\nwhere max applies for each element of the vectors, resulting in a sequence\nF \u2032 = (f \u20321, f \u2032 2, . . . , f \u2032 T/r\u2032)."}, {"heading": "2.3 Recurrent Layer", "text": "A recurrent layer consists of a recursive function f which takes as input one input vector and the previous hidden state, and returns the new hidden state:\nht = f(xt,ht\u22121),\nwhere xt \u2208 Rd is one time step from the input sequence (x1,x2, . . . ,xT ). h0 \u2208 Rd \u2032 is often initialized as an all-zero vector.\nRecursive Function The most naive recursive function is implemented as\nht = tanh (Wxxt +Uhht\u22121) ,\nwhere Wx \u2208 Rd \u2032\u00d7d and Uh \u2208 Rd \u2032\u00d7d\u2032 are the weight matrices. This naive recursive function however is known to suffer from the problem of vanishing gradient (Bengio et al., 1994; Hochreiter et al., 2001).\nMore recently it is common to use a more complicated function that learns to control the flow of information so as to prevent the vanishing gradient and allows the recurrent layer to more easily capture long-term dependencies. Long short-term memory (LSTM) unit from (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) is a representative example.\nThe LSTM unit consists of four sub-units\u2013input, output, forget gates and candidate memory cell, which are computed by\nit = \u03c3 (Wixt +Uiht\u22121) ,\not = \u03c3 (Woxt +Uoht\u22121) ,\nft = \u03c3 (Wfxt +Ufht\u22121) ,\nc\u0303t = tanh (Wcxt +Ucht\u22121) .\nBased on these, the LSTM unit first computes the memory cell:\nct = it c\u0303t + ft ct\u22121,\nand computes the output, or activation:\nht = ot tanh(ct).\nThe resulting sequence from the recurrent layer is then\n(h1,h2, . . . ,hT ),\nwhere T is the length of the input sequence to the layer.\nBidirectional Recurrent Layer One property of the recurrent layer is that there is imbalance in the amount of information seen by the hidden states at different time steps. The earlier hidden states only observe a few vectors from the lower layer, while the later ones are computed based on the most of the lower-layer vectors. This can be easily alleviated by having a bidirectional recurrent layer which is composed of two recurrent layers working in opposite directions. This layer will return two sequences of hidden states from the forward and reverse recurrent layers, respectively."}, {"heading": "2.4 Classification Layer", "text": "A classification layer is in essence a logistic regression classifier. Given a fixed-dimensional input from the lower layer, the classification layer affinetransforms it followed by a softmax activation function (Bridle, 1990) to compute the predictive probabilities for all the categories. This is done by\np(y = k|X) = exp(w>k x+ bk)\u2211K\nk\u2032=1 exp(w > k\u2032x+ bk\u2032)\n,\nwhere wk\u2019s and bk\u2019s are the weight and bias vectors. We assume there are K categories.\nIt is worth noting that this classification layer takes as input a fixed-dimensional vector, while the recurrent layer or convolutional layer returns a variable-length sequence of vectors (the length determined by the input sequence). This can be addressed by either simply max-pooling the vectors (Kim, 2014) over the time dimension (for both convolutional and recurrent layers), taking the last hidden state (for recurrent layers) or taking the last hidden states of the forward and reverse recurrent networks (for bidirectional recurrent layers.)"}, {"heading": "3 Character-Level Convolutional-Recurrent Network", "text": "In this section, we propose a hybrid of convolutional and recurrent networks for character-level document classification."}, {"heading": "3.1 Motivation", "text": "One basic motivation for using the convolutional layer is that it learns to extract higher-level features that are invariant to local translation. By stacking multiple convolutional layers, the network can extract higher-level, abstract, (locally) translationinvariant features from the input sequence, in this case the document, efficiently.\nDespite this advantage, we noticed that it requires many layers of convolution to capture long-term dependencies, due to the locality of the convolution and pooling (see Sec. 2.2.) This becomes more severe as the length of the input sequence grows, and in the case of character-level modeling, it is usual for a document to be a sequence of hundreds or thousands of characters. Ultimately, this leads to the need for a very deep network having many convolutional layers.\nContrary to the convolutional layer, the recurrent layer from Sec. 2.3 is able to capture long-term dependencies even when there is only a single layer. This is especially true in the case of a bidirectional recurrent layer, because each hidden state is computed based on the whole input sequence. However, the recurrent layer is computationally more expensive. The computational complexity grows linearly with respect to the length of the input sequence, and most of the computations need to be done sequentially. This is in contrast to the convolutional layer for which computations can be efficiently done in parallel.\nBased on these observations, we propose to combine the convolutional and recurrent layers into a single model so that this network can capture longterm dependencies in the document more efficiently for the task of classification."}, {"heading": "3.2 Model Description", "text": "The proposed model, to which we refer as a convolution-recurrent network (ConvRec), starts\nwith a one-hot sequence input\nX = (x1,x2, . . . ,xT ).\nThis input sequence is turned into a sequence of dense, real-valued vectors\nE = (e1, e2, . . . , eT )\nusing the embedding layer from Sec. 2.1. We apply multiple convolutional layers (Sec. 2.2) to E to get a shorter sequence of feature vectors:\nF = (f1, f2, . . . , fT \u2032).\nThis feature vector is then fed into a bidirectional recurrent layer (Sec. 2.3), resulting in two sequences\nHforward = ( \u2212\u2192 h 1, \u2212\u2192 h 2, . . . , \u2212\u2192 h T \u2032), Hreverse = ( \u2190\u2212 h 1, \u2190\u2212 h 2, . . . , \u2190\u2212 h T \u2032).\nWe take the last hidden states of both directions and concatenate them to form a fixed-dimensional vector:\nh = [\u2212\u2192 h T \u2032 ; \u2190\u2212 h 1 ] .\nFinally, the fixed-dimensional vector h is fed into the classification layer to compute the predictive probabilities p(y = k|X) of all the categories k = 1, . . . ,K given the input sequence X .\nSee Fig. 1 (b) for the graphical illustration of the proposed model."}, {"heading": "3.3 Related Work", "text": "Convolutional network for document classification The convolutional networks for document classification, proposed earlier in (Kim, 2014; Zhang et al., 2015) and illustrated in Fig. 1 (a), is almost identical to the proposed model. One major difference is the lack of the recurrent layer in their models. Their model consists of the embedding layer, a number of convolutional layers followed by the classification layer only.\nRecurrent network for document classification Carrier and Cho in (Carrier and Cho, 2014) give a tutorial on using a recurrent neural network for sentiment analysis which is one type of document classification. Unlike the convolution-recurrent network proposed in this paper, they do not use any convolutional layer in their model. Their model starts with the embedding layer followed by the recurrent layer. The hidden states from the recurrent layer are then averaged and fed into the classification layer.\nHybrid model: Conv-GRNN Perhaps the most related work is the convolution-gated recurrent neural net (Conv-GRNN) from (Tang et al., 2015). They proposed a hierarchical processing of a document. In their model, either a convolutional network or a recurrent network is used to extract a feature vector from each sentence, and another (bidirectional) recurrent network is used to extract a feature vector of the document by reading the sequence of sentence vectors. This document vector is used by the classification layer.\nThe major difference between their approach and the proposed ConvRec is in the purpose of combining the convolutional network and recurrent net-\nwork. In their model, the convolutional network is strictly constrained to model each sentence, and the recurrent network to model inter-sentence structures. On the other hand, the proposed ConvRec network uses a recurrent layer in order to assist the convolutional layers to capture long-term dependencies (across the whole document) more efficiently. These are orthogonal to each other, and it is possible to plug in the proposed ConvRec as a sentence feature extraction module in the Conv-GRNN from (Tang et al., 2015). Similarly, it is possible to use the proposed ConvRec as a composition function for the sequence of sentence vectors to make computation more efficient, especially when the input document consists of many sentences.\nRecursive Neural Networks A recursive neural network has been applied to sentence classification earlier (see, e.g., (Socher et al., 2013).) In this approach, a composition function is defined and recursively applied at each node of the parse tree of an input sentence to eventually extract a feature vector of the sentence. This model family is heavily dependent on an external parser, unlike all the other models such as the ConvRec proposed here as well as other related models described above. It is also not trivial to apply the recursive neural network to documents which consist of multiple sentences. We do not consider this family of recursive neural networks directly related to the proposed model."}, {"heading": "4 Experiment Settings", "text": ""}, {"heading": "4.1 Task Description", "text": "We validate the proposed model on eight large-scale document classification tasks from (Zhang et al.,\n2015). The sizes of the data sets range from 200,000 to 4,000,000 documents. These tasks include sentiment analysis (Yelp reviews, Amazon reviews), ontology classification (DBPedia), question type classification (Yahoo! Answers), and news categorization (AG\u2019s news, Sogou news).\nData Sets A summary of the statistics for each data set is listed in Table 1. There are equal number of examples in each class for both training and test sets. DBPedia data set, for example, has 40,000 training and 5,000 test examples per class. For more detailed information on the data set construction process, see (Zhang et al., 2015)."}, {"heading": "4.2 Model Settings", "text": "Referring to Sec. 2.1, the vocabulary V for our experiments consists of 96 characters including all upper-case and lower-case letters, digits, common punctuation marks, and spaces. Character embedding size d is set to 8.\nAs described in Sec. 3.1, we believe by adding recurrent layers, one can effectively reduce the number of convolutional layers needed in order to capture long-term dependencies. Thus for each data set, we consider models with two to five convolutional layers. Following notations in Sec. 2.2, each layer has d\u2032 = 128 filters. For AG\u2019s news and Yahoo! Answers, we also experiment larger models with 1,024 filters in the convolutional layers. Receptive field size r is either five or three depending on the depth. Max pooling size r\u2032 is set to 2. Rectified linear units (ReLUs, (Glorot et al., 2011)) are used as activation functions in the convolutional layers. The recurrent layer (Sec. 2.3) is fixed to a single layer of bidirectional LSTM for all models. Hidden states dimension d\u2032 is set to 128. More detailed setups are described in Table 2.\nDropout (Srivastava et al., 2014) is an effective way to regularize deep neural networks. We apply dropout after the last convolutional layer as well as after the recurrent layer. Without dropout, the inputs to the recurrent layer xt\u2019s are\nxt = f \u2032 t\nwhere f \u2032t is the t-th output from the last convolutional layer defined in Sec. 2.2. After adding dropout, we have\nrit \u223c Bernoulli(p)\nxt = rt f \u2032t p is the dropout probability which we set to 0.5; rit is the i-th component of the binary vector rt \u2208 Rd \u2032 ."}, {"heading": "4.3 Training and Validation", "text": "For each of the data sets, we randomly split the full training examples into training and validation. The validation size is the same as the corresponding test size and is balanced in each class.\nThe models are trained by minimizing the following regularized negative log-likelihood or cross entropy loss. X\u2019s and y\u2019s are document character sequences and their corresponding observed class assignments in the training set D. w is the collection of model weights. Weight decay is applied with \u03bb = 5\u00d7 10\u22124.\nl = \u2212 \u2211\nX,y\u2208D log(p(y|X)) + \u03bb 2 \u2016w\u20162\nWe train our models using AdaDelta (Zeiler, 2012) with \u03c1 = 0.95, = 10\u22125 and a batch size of 128. Examples are padded to the longest sequence in each batch and masks are generated to help identify the padded region. The corresponding masks of\nthe outputs from convolutional layers can be computed analytically and are used by the recurrent layer to properly ignore padded inputs. The gradient of the cost function is computed with backpropagation through time (BPTT, (Werbos, 1990)). If the gradient has an L2 norm larger than 5, we rescale the gradient by a factor of 5\u2016g\u20162 . i.e.\ngc = g \u00b7min ( 1, 5\n\u2016g\u20162 ) where g = dldw and gc is the clipped gradient.\nEarly stopping strategy is employed to prevent overfitting. Before training, we set an initial patience value. At each epoch, we calculate and record the validation loss. If it is lower than the current lowest validation loss by 0.5%, we extend patience by two. Training stops when the number of epochs is larger than patience. We report the test error rate evaluated using the model with the lowest validation error."}, {"heading": "5 Results and Analysis", "text": "Experimental results are listed in Table 3. We compare to the best character-level convolutional model without data augmentation from (Zhang et al., 2015) on each data set. Our model achieves comparable performances for all the eight data sets with significantly less parameters. Specifically, it performs better on AG\u2019s news, Sogou news, DBPedia, Yelp review full, and Yahoo! Answers data sets.\nNumber of classes Fig. 2 (a) shows how relative performance of our model changes with respect to the number of classes. It is worth noting that as the number of classes increases, our model achieves better results compared to convolution-only models. For example, our model has a much lower test error on DBPedia which has 14 classes, but it scores worse on Yelp review polarity and Amazon review polarity both of which have only two classes. Our conjecture is that more detailed and complete information needs to be preserved from the input text for the model to assign one of many classes to it. The convolution-only model likely loses detailed local features because it has more pooling layers. On the other hand, the proposed model with less pooling layers can better maintain the detailed information and hence performs better when such needs exist.\nNumber of training examples Although it is less significant, Fig. 2 (b) shows that the proposed model generally works better compared to the convolutiononly model when the data size is small. Considering the difference in the number of parameters, we suspect that because the proposed model is more compact, it is less prone to overfitting. Therefore it generalizes better when the training size is limited.\nNumber of convolutional layers An interesting observation from our experiments is that the model accuracy does not always increase with the number of convolutional layers. Performances peak at two or three convolutional layers and decrease if we add\nmore to the model. As more convolutional layers produce longer character n-grams, this indicates that there is an optimal level of local features to be fed into the recurrent layer. Also, as discussed above, more pooling layers likely lead to the lost of detailed information which in turn affects the ability of the recurrent layer to capture long-term dependencies.\nNumber of filters We experiment large models with 1,024 filters on AG\u2019s news and Yahoo! Answers data sets. Although adding more filters in the convolutional layers does help with the model performances on these two data sets, the gains are limited compared to the increased number of parameters. Validation error improves from 8.75% to 8.39% for AG\u2019s news and from 29.48% to 28.62% for Yahoo! Answers at the cost of a 70 times increase in the number of model parameters.\nNote that in our model we set the number of filters in the convolutional layers to be the same as the dimension of the hidden states in the recurrent layer. It is possible to use more filters in the convolutional layers while keeping the recurrent layer dimension the same to potentially get better performances with less sacrifice of the number of parameters."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed a hybrid model that processes an input sequence of characters with a number of convolutional layers followed by a single recurrent layer. The proposed model is able to encode documents from character level capturing sub-word\ninformation.\nWe validated the proposed model on eight large scale document classification tasks. The model achieved comparable results with much less convolutional layers compared to the convolution-only architecture. We further discussed several aspects that affect the model performance. The proposed model generally performs better when number of classes is large, training size is small, and when the number of convolutional layers is set to two or three.\nThe proposed model is a general encoding architecture that is not limited to document classification tasks or natural language inputs. For example, (Chen et al., 2015; Visin et al., 2015) combined convolution and recurrent layers to tackle image segmentation tasks; (Sainath et al., 2015) applied a similar model to do speech recognition. It will be interesting to see future research on applying the architecture to other applications such as machine translation and music information retrieval. Using recurrent layers as substitutes for pooling layers to potentially reduce the lost of detailed local information is also a direction that worth exploring."}, {"heading": "Acknowledgments", "text": "This work is done as a part of the course DS-GA 1010-001 Independent Study in Data Science at the Center for Data Science, New York University."}], "references": [{"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A Smith"], "venue": "arXiv preprint arXiv:1508.00657", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["John S Bridle"], "venue": "In Neurocomputing,", "citeRegEx": "Bridle.,? \\Q1990\\E", "shortCiteRegEx": "Bridle.", "year": 1990}, {"title": "LSTM networks for sentiment analysis", "author": ["Carrier", "Cho2014] Pierre Luc Carrier", "Kyunghyun Cho"], "venue": "Deep Learning Tutorials", "citeRegEx": "Carrier et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Carrier et al\\.", "year": 2014}, {"title": "Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform", "author": ["Jonathan T. Barron", "George Papandreou", "Kevin Murphy", "Alan L. Yuille"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers et al.2000] Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Deep sparse rectifier neural networks", "author": ["Glorot et al.2011] Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statis-", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Yoshua Bengio", "Paolo Frasconi", "Jfirgen Schmidhuber"], "venue": "long-term dependencies,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Characteraware neural language models", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush"], "venue": "arXiv preprint arXiv:1508.06615", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882", "author": ["Yoon Kim"], "venue": null, "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Ensemble of generative and discriminative techniques for sentiment analysis of movie reviews. arXiv preprint arXiv:1412.5335", "author": ["Marc\u2019Aurelio Ranzato", "Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Mesnil et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2014}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["Sainath et al.2015] T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts Potts"], "venue": "In EMNLP", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "From feedforward to recurrent lstm neural networks for language modeling", "author": ["Hermann Ney", "Ralf Schluter"], "venue": null, "citeRegEx": "Sundermeyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2015}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Tang et al.2015] Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "Reseg: A recurrent neural network for object segmentation. CoRR, abs/1511.07053", "author": ["Kyle Kastner", "Aaron C. Courville", "Yoshua Bengio", "Matteo Matteucci", "KyungHyun Cho"], "venue": null, "citeRegEx": "Visin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Visin et al\\.", "year": 2015}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "In Proceedings of IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advanced in Neural Information Processing Systems (NIPS 2015),", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "More recently, it has become more common to use a deep neural network, which jointly performs feature extraction and classification, for document classification (Kim, 2014; Mesnil et al., 2014; Socher et al., 2013; Carrier and Cho, 2014).", "startOffset": 161, "endOffset": 237}, {"referenceID": 14, "context": "More recently, it has become more common to use a deep neural network, which jointly performs feature extraction and classification, for document classification (Kim, 2014; Mesnil et al., 2014; Socher et al., 2013; Carrier and Cho, 2014).", "startOffset": 161, "endOffset": 237}, {"referenceID": 16, "context": "More recently, it has become more common to use a deep neural network, which jointly performs feature extraction and classification, for document classification (Kim, 2014; Mesnil et al., 2014; Socher et al., 2013; Carrier and Cho, 2014).", "startOffset": 161, "endOffset": 237}, {"referenceID": 10, "context": "(Kim et al., 2015) and Ling et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 13, "context": "(Ling et al., 2015) proposed to use a character sequence as an alternative to the word-level one-hot vector.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "A similar idea was applied to dependency parsing in (Ballesteros et al., 2015).", "startOffset": 52, "endOffset": 78}, {"referenceID": 23, "context": "(Zhang et al., 2015).", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "The character-level convolutional net in (Zhang et al., 2015) is composed of many layers of convolution and max-pooling, similarly to the convolutional network in computer vision (see, e.", "startOffset": 41, "endOffset": 61}, {"referenceID": 12, "context": ", (Krizhevsky et al., 2012).", "startOffset": 2, "endOffset": 27}, {"referenceID": 23, "context": "As the receptive field of each convolutional layer is often small (7 or 3 in (Zhang et al., 2015),) the network must have many layers in order to capture long-term dependencies in an input sentence.", "startOffset": 77, "endOffset": 97}, {"referenceID": 23, "context": "(Zhang et al., 2015) used a very deep convolutional network with six convolutional layers followed by two fully-connected layers.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": ", (Cho et al., 2014; Sundermeyer et al., 2015)) and from the fact that the recurrent network can efficiently capture long-term dependencies even with a single layer.", "startOffset": 2, "endOffset": 46}, {"referenceID": 18, "context": ", (Cho et al., 2014; Sundermeyer et al., 2015)) and from the fact that the recurrent network can efficiently capture long-term dependencies even with a single layer.", "startOffset": 2, "endOffset": 46}, {"referenceID": 5, "context": "Because the recurrent layer, consisting of either gated recurrent units (GRU, (Cho et al., 2014) or long short-term memory units (LSTM, (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 78, "endOffset": 96}, {"referenceID": 6, "context": ", 2014) or long short-term memory units (LSTM, (Hochreiter and Schmidhuber, 1997; Gers et al., 2000), can efficiently capture long-term dependencies, the proposed network only needs a very small number of convolutional layers.", "startOffset": 47, "endOffset": 100}, {"referenceID": 23, "context": "We empirically validate the proposed model, to which we refer as a convolution-recurrent network, on the eight large-scale document classification tasks from (Zhang et al., 2015).", "startOffset": 158, "endOffset": 178}, {"referenceID": 23, "context": "We mainly compare the proposed model against the convolutional network in (Zhang et al., 2015) and show that it is indeed possible to use a much smaller model to achieve the same level of classification performance when a recurrent layer is put on top of the convolutional layers.", "startOffset": 74, "endOffset": 94}, {"referenceID": 1, "context": "This naive recursive function however is known to suffer from the problem of vanishing gradient (Bengio et al., 1994; Hochreiter et al., 2001).", "startOffset": 96, "endOffset": 142}, {"referenceID": 9, "context": "This naive recursive function however is known to suffer from the problem of vanishing gradient (Bengio et al., 1994; Hochreiter et al., 2001).", "startOffset": 96, "endOffset": 142}, {"referenceID": 6, "context": "Long short-term memory (LSTM) unit from (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) is a representative example.", "startOffset": 40, "endOffset": 93}, {"referenceID": 2, "context": "Given a fixed-dimensional input from the lower layer, the classification layer affinetransforms it followed by a softmax activation function (Bridle, 1990) to compute the predictive probabilities for all the categories.", "startOffset": 141, "endOffset": 155}, {"referenceID": 11, "context": "This can be addressed by either simply max-pooling the vectors (Kim, 2014) over the time dimension (for both convolutional and recurrent layers), taking the last hidden state (for recurrent layers) or taking the last hidden states of the forward and reverse recurrent networks (for bidirectional recurrent layers.", "startOffset": 63, "endOffset": 74}, {"referenceID": 11, "context": "Convolutional network for document classification The convolutional networks for document classification, proposed earlier in (Kim, 2014; Zhang et al., 2015) and illustrated in Fig.", "startOffset": 126, "endOffset": 157}, {"referenceID": 23, "context": "Convolutional network for document classification The convolutional networks for document classification, proposed earlier in (Kim, 2014; Zhang et al., 2015) and illustrated in Fig.", "startOffset": 126, "endOffset": 157}, {"referenceID": 19, "context": "Hybrid model: Conv-GRNN Perhaps the most related work is the convolution-gated recurrent neural net (Conv-GRNN) from (Tang et al., 2015).", "startOffset": 117, "endOffset": 136}, {"referenceID": 19, "context": "These are orthogonal to each other, and it is possible to plug in the proposed ConvRec as a sentence feature extraction module in the Conv-GRNN from (Tang et al., 2015).", "startOffset": 149, "endOffset": 168}, {"referenceID": 16, "context": ", (Socher et al., 2013).", "startOffset": 2, "endOffset": 23}, {"referenceID": 23, "context": "For more detailed information on the data set construction process, see (Zhang et al., 2015).", "startOffset": 72, "endOffset": 92}, {"referenceID": 7, "context": "Rectified linear units (ReLUs, (Glorot et al., 2011)) are used as activation functions in the convolutional layers.", "startOffset": 31, "endOffset": 52}, {"referenceID": 17, "context": "Dropout (Srivastava et al., 2014) is an effective way to regularize deep neural networks.", "startOffset": 8, "endOffset": 33}, {"referenceID": 22, "context": "We train our models using AdaDelta (Zeiler, 2012) with \u03c1 = 0.", "startOffset": 35, "endOffset": 49}, {"referenceID": 23, "context": "Our Model (Zhang et al., 2015) Data set # Ex.", "startOffset": 10, "endOffset": 30}, {"referenceID": 21, "context": "The gradient of the cost function is computed with backpropagation through time (BPTT, (Werbos, 1990)).", "startOffset": 87, "endOffset": 101}, {"referenceID": 23, "context": "We compare to the best character-level convolutional model without data augmentation from (Zhang et al., 2015) on each data set.", "startOffset": 90, "endOffset": 110}, {"referenceID": 4, "context": "For example, (Chen et al., 2015; Visin et al., 2015) combined convolution and recurrent layers to tackle image segmentation tasks; (Sainath et al.", "startOffset": 13, "endOffset": 52}, {"referenceID": 20, "context": "For example, (Chen et al., 2015; Visin et al., 2015) combined convolution and recurrent layers to tackle image segmentation tasks; (Sainath et al.", "startOffset": 13, "endOffset": 52}, {"referenceID": 15, "context": ", 2015) combined convolution and recurrent layers to tackle image segmentation tasks; (Sainath et al., 2015) applied a similar model to do speech recognition.", "startOffset": 86, "endOffset": 108}], "year": 2016, "abstractText": "Document classification tasks were primarily tackled at word level. Recent research that works with character-level inputs shows several benefits over word-level approaches such as natural incorporation of morphemes and better handling of rare words. We propose a neural network architecture that utilizes both convolution and recurrent layers to efficiently encode character inputs. We validate the proposed model on eight large scale document classification tasks and compare with character-level convolution-only models. It achieves comparable performances with much less parameters.", "creator": "LaTeX with hyperref package"}}}