{"id": "1602.00991", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Feb-2016", "title": "Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks", "abstract": "highlighting this flagship paper presents challenging to the instant best of building our knowledge the first end - to - end unmanned object surface tracking approach which simply directly maps performance from raw sensor input to object tracks continuously in sensor space without possibly requiring any feature describing engineering or system identification in the form identification of plant or sensor models. specifically, our system analysis accepts a stream of filtered raw sensor sensor coded data at source one / end interface and, still in real - time, this produces an estimate distribution of the entire object environment overall state at average the vector output area including even discarded occluded plant objects. so we achieve this by framing the problem essentially as having a 2d deep learning detection task and exploit sequence learning models displayed in the limited form of recurrent synthetic neural reasoning networks created to learn easily a mapping from sensor measurements produced to object tracks. in some particular, we separately propose a learning method based successfully on a form of input dropout which allows implicit learning in an unsupervised manner, resulting only based on raw, occluded sensor pattern data generated without access to ground - truth annotations. we jointly demonstrate our approach using a synthetic dataset designed to mimic model the digital task of highly tracking objects manifested in 2d laser dot data - - primarily as technique commonly presently encountered in surface robotics applications - - and recently show that : it learns properly to also track simultaneously many automatically dynamic objects despite occlusions and the presence of sensor specific noise.", "histories": [["v1", "Tue, 2 Feb 2016 16:10:16 GMT  (379kb,D)", "http://arxiv.org/abs/1602.00991v1", "Published in The Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16) Video:this https URLCode:this http URL"], ["v2", "Tue, 8 Mar 2016 22:09:05 GMT  (379kb,D)", "http://arxiv.org/abs/1602.00991v2", "Published in The Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16), Video:this https URL, Code:this http URL"]], "COMMENTS": "Published in The Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16) Video:this https URLCode:this http URL", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE cs.RO", "authors": ["peter ondruska", "ingmar posner"], "accepted": true, "id": "1602.00991"}, "pdf": {"name": "1602.00991.pdf", "metadata": {"source": "CRF", "title": "Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks", "authors": ["Peter Ondr\u00fa\u0161ka", "Ingmar Posner"], "emails": ["ingmar}@robots.ox.ac.uk"], "sections": [{"heading": "Introduction", "text": "As we demand more from our robots the need arises for them to operate in increasingly complex, dynamic environments where scenes \u2013 and objects of interest \u2013 are often only partially observable. However, successful decision making typically requires complete situational awareness. Commonly this problem is tackled by a processing pipeline which uses separate stages of object detection and tracking, both of which require considerable hand-engineering. Classical approaches to object tracking in highly dynamic environments require the specification of plant and observation models as well as robust data association.\nIn recent years neural networks and deep learning approaches have revolutionised how we think about classification and detection in a number of domains (Krizhevsky, Sutskever, and Hinton 2012; Szegedy, Toshev, and Erhan 2013). The often unreasonable effectiveness of such approaches is commonly attributed to both an ability to learn\nCopyright c\u00a9 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nWhat the robot seesTrue world state y xt t\nRecurrent neural network\nRobot sensor\nFigure 1: Often a robot\u2019s sensors provide only partial observations of the surrounding environment. In this work we leverage a recurrent neural network to effectively reveal occluded parts of a scene by learning to track objects from raw sensor data \u2013 thereby effectively reversing the sensing process.\nrelevant representations directly from raw data as well as a vastly increased capacity for function approximation afforded by the depth of the networks (Bengio 2009).\nIn this work we propose a framework for deep tracking, which effectively provides an off-the-shelf solution for learning the dynamics of complex environments directly from raw sensor data and mapping it to an intuitive representation of a complete and unoccluded scene around the robot as illustrated in Figure 1.\nTo achieve this we leverage Recurrent Neural Networks (RNNs), which were recently demonstrated to be able to effectively handle sequence data (Graves 2013; Sutskever, Hinton, and Taylor 2009). In particular we consider a complete yet uninterpretable hidden state of the world and then train the network end-to-end to update its belief of this state using sequence of partial observations and map it back into an interpretable unoccluded scene. This gives the network a freedom to optimally define the content of the hidden state\nPresented at AAAI-16 conference, February 12-17, 2016, Phoenix, Arizona USA and the considered operations without the need of any handengineering.\nWe demonstrate that such a system can be trained in an entirely unsupervised manner only based on raw sensor data and without the need for supervisor annotation. To the best of our knowledge this is the first time when such a system is demonstrated and we believe our work will provide a new paradigm for an end-to-end tracking. In particular, our contributions can be summarized as follows: \u2022 A novel, end-to-end way of filtering partial observations\nof dynamic scenes using recurrent neural networks to provide a full, unoccluded scene estimation.\n\u2022 A new method of unsupervised training based on a novel form of dropout encouraging the network to correctly predict objects in unobserved spaces.\n\u2022 A compelling demonstration and attached video 1 of the method in a synthesised scenario of robot tracking the state of the surrounding environment populated by many moving objects. The network directly accepts unprocessed raw sensor inputs and learns to predict positions of all objects in real time even through complete sensor occlusions and noise 2."}, {"heading": "Deep Tracking", "text": "Formally, our goal is to predict the current, un-occluded scene around the robot given the sequence of sensor observations, i.e. to model P (yt|x1:t) where yt \u2208 RN is a discretetime stochastic process with unknown dynamics modelling the scene around the robot containing other both static and dynamic objects and xt \u2286 yt are the sensor measurements of directly visible scene parts. We use encoding xt = {vt, rt} where {vit, zit} = {1, yit} if i-th element of yt is observed and {0, 0} otherwise. Moreover, as the scene changes the robot can observe different parts of the scene at different times.\nIn general, this problem can not be solved effectively if the process yt is purely random and unpredictable as in such case there is no way to determine the unobserved elements of yt. In practice, however, the sequences yt and xt exhibit structural and temporal regularities which can be exploited for effective prediction. For example, objects tend to follow certain motion patterns and have certain appearance. This allows the behaviour of the objects to be estimated when they are visible such that later this knowledge can be used for prediction when they become occluded.\nDeep tracking leverages recurrent neural networks to effectively model P (yt|x1:t). In the remainder of this section we first describe the probabilistic model assumed to underlie the sensing process before detailing how RNNs can be used for tracking."}, {"heading": "The Model", "text": "Like many approaches to object tracking, our model is inspired by Bayesian filtering (Chen 2003). Note, however, the\n1Video is available at: https://youtu.be/cdeWCpfUGWc 2The source code of our experiments is available at:\nhttp://mrg.robots.ox.ac.uk/mrg people/peter-ondruska/\nh0 h1 h2\ny1 y2\nx1 x2\nh3 ht\ny3 yt\nx3 xt\nFigure 2: The assumed graphical model of the generative process. World dynamics are modelled by the hidden Markov process ht with appearance yt which is partially observed by sensor measurements xt.\nprocess yt does not satisfy Markov property: Even knowledge of yt at any particular time t provides only partial information about the state of the world such as object positions, but does not contain all necessary information such as their speed or acceleration required for prediction. The latter can be obtained only by relating subsequent measurements. The methods such as the Hidden Markov Model (Rabiner 1989) are therefore not directly applicable.\nTo handle this problem we assume the generative model in Figure 2 such that alongside yt there exists another underlying Markov process, ht, which completely captures the state of the world with the joint probability density\nP (y1:N , x1:N , h1:N ) = N\u220f t=1 P (xt|yt)P (yt|ht)P (ht|ht\u22121),\n(1) where\n\u2022 P (ht|ht\u22121) denotes the hidden state transition probability capturing the dynamics of the world;\n\u2022 P (yt|ht) is modelling the instantaneous unoccluded sensor space;\n\u2022 and P (xt|yt) describes the actual sensing process. The task of estimating P (yt|x1:t) can now be framed in the context of recursive Bayesian estimation (Bergman 1999) of the belief Bt which, at any point in time t, corresponds to a distribution Bel(ht) = P (ht|x1:t). This belief can be computed recursively for the considered model as\nBel\u2212(ht) = \u222b ht\u22121 P (ht|ht\u22121)Bel(ht\u22121) (2)\nBel(ht) \u221d \u222b yt P (xt|yt)P (yt|ht)Bel\u2212(ht) (3)\nwhere Bel\u2212(ht) denotes belief prediction one time step into the future and Bel(ht) denotes the corrected belief after the latest measurement has become available. P (yt|x1:t) in turn can then be expressed simply using this belief,\nP (yt|x1:t) = \u222b ht P (yt|ht)Bel(ht). (4)\nPresented at AAAI-16 conference, February 12-17, 2016, Phoenix, Arizona USA Ra w o cc lu de d se ns or in pu t Fi lte re d ou tp ut Re cu rr en t ne ur al n et w or k\nt = 1 t = 2 t = 3\ny1 B1 B2 B3 y2 y3\nx1 x2 x3\nB0\nFigure 3: An illustration of the filtering process using a recurrent neural network."}, {"heading": "Filtering Using Recurrent Neural Network", "text": "Computation of P (yt|x1:t) is carried easily by iteratively updating the current belief Bt\nBt = F (Bt\u22121, xt) (5) defined by Equations 2,3 and simultaneously providing prediction P (yt|x1:t) = P (yt|Bt) (6) defined by equation 4. Moreover, instead of yt the same method can be used to predict any state in the future P (yt+n|x1:t) by providing empty observations of the form x(t+1):(t+n) = \u2205.\nFor realistic applications this approach is, however, limited by the need for the suitable belief state representation as well as the explicit knowledge of the distributions in Equation 1 modelling the generative process.\nThe key idea of our solution is to avoid the need to specify this knowledge and instead use a highly expressive neural networks, governed by weights WF and WP , to model both F (Bt\u22121, xt) and P (yt|Bt) allowing to learn their function directly form the data. Specifically, we assume Bt can be approximated and represented as a vector, Bt \u2208 RM . As illustrated in Figure 3, the function F is then simply a recurrent mapping from RM \u00d7 R2N \u2192 RM corresponding to an implementation by a Feed-Forward Recurrent Neural Network (Medsker and Jain 2001) where the Bt acts as the network\u2019s memory passed from one time step to the next.\nImportantly, we do not impose any restrictions on the content or function of this hidden representation. Provided both networks for F (Bt, xt) and P (yt|Bt) are differentiable they can be trained together end-to-end as a single recurrent network with a cost function directly corresponding to the data likelihood provided in Equation 4. This allows the neural networks to adapt to each other and to learn an optimal hidden state representation for Bt together with procedures for belief updates using partial observations xt and to use this information to decode the final scene state yt. Training of the network is detailed in the next section while the concrete architecture used for our experiment is discussed as a part of Experimental Results."}, {"heading": "Training", "text": "The network can be trained both in a supervised mode, i.e. with both known y1:N , x1:N as well as in an unsupervised mode where only x1:N is known. A common method of supervised training is to minimize the negative log-likelihood of the true scene state given the sensor input\nL = \u2212 N\u2211 t=1 logP (yt|x1:t) (7)\nThis objective can be optimised by the gradient descent with partial derivatives given by backpropagation through time (Rumelhart, Hinton, and Williams 1985)\n\u2202L \u2202Bt = \u2202F (Bt, xt+1) \u2202Bt \u00b7 \u2202L \u2202Bt+1 \u2212 \u2202 logP (yt|Bt) \u2202Bt (8)\n\u2202L \u2202WF = N\u2211 t=1 \u2202F (Bt\u22121, xt) \u2202WF \u00b7 \u2202L \u2202Bt\n(9)\n\u2202L \u2202WP = \u2212 N\u2211 t=1 \u2202 logP (yt|Bt) \u2202WP . (10)\nHowever, a major disadvantage of the supervised training is the requirement of the ground-truth scene state yt containing parts not visible by the robot sensor. One way of obtaining this information is to place additional sensors in the environment which jointly provide information about every part of the scene. Such an approach, however, can be impractical, costly or even impossible. In the following part we describe a training method which only requires raw, occluded sensor observations alone. This dramatically increases the practicality of the method as the only necessary learning information is a long enough record of x1:N as obtained by the sensor in the field."}, {"heading": "Unsupervised Training", "text": "The aim of unsupervised training is to learn F (Bt\u22121, xt) and P (yt|Bt) using only x1:t. This might seem impossible without knowledge of yt as there would be no way to correct the network predictions. However, some values of yt as xt \u2286 yt are known in the directly observed sensor measurements. A naive approach would be to train the network using the objective from Equation 7 of predicting the known values of yt and not back-propagating from the unobserved ones. Such an approach would fail, however, as the network would only learn to copy the observed elements of xt to the output and never predict objects in occlusion.\nInstead, and crucially, we propose to train the network not to predict the current state P (yt|x1:t) but a state in the future P (yt+n|x1:t). In order for the network to correctly predict the visible part of the object after several time steps the network must learn how to represent and simulate object dynamics during the time when the object is not seen using F and then convert this information to predict P (yt+n|Bt+n).\nThis form of learning can be implemented easily by considering the cost function from Equation 7 of predicting visible elements of yt:t+n and dropping-out all xt:t+n by setting them to 0 marking the input unobserved. This is similar to\nPresented at AAAI-16 conference, February 12-17, 2016, Phoenix, Arizona USA standard node dropout (Srivastava et al. 2014) to prevent network overfitting. Here, we are dropping out the observations both spatially across the entire scene and temporarily across multiple time steps in order to prevent the network to overfit to the task of simply copying over the elements. This forces the network to learn the correct patterns.\nAn interesting observation demonstrated in the next section is that even though the network is in theory trained to predict only the future observations xt+n, in order to achieve this the network learns to predict yt+n. We hypothesise this is due to the tendency of the regularized network to find the simplest explanation for observations where it is easier to model P (yt|Bt) than P (xt|Bt) = \u222b yt P (xt|yt)P (yt|Bt)."}, {"heading": "Experimental Results", "text": "In this section we demonstrate the effectiveness of our approach in estimating the full scene state in a simulated scenario representing a robot equipped with a 2D laser scanner surrounded by many dynamic objects. Our inspiration here is a situation of a robot in a middle of a busy street surrounded by a crowd of moving people. Objects are modelled as circles independently moving with constant velocity in a random direction but never colliding with each other or with the robot. The number of objects present in the scene is varied over time between two and 12 as new objects randomly appear and later disappear at a distance from the robot.\nSensor Input At each time step we simulated the sensor input as commonly provided by a planar laser scanner. We divided the scene yt around the robot using a 2D grid of 50\u00d750 pixels with the robot placed towards the bottom centre. To model sensor observations xt we ray-traced the visibility of each pixel and assigned values xit = {vit, rit} indicating pixel visibility (by the robot) and presence of an obstacle if the pixel is visible. An example of the input yt and corresponding observation xt is depicted in Figure 3. Note that, as in the case of a real sensor, only the surface of the object as seen from particular viewpoint is ever observed. This input was then fed to the network as a stream of 50\u00d750 2-channel binary images for filtering."}, {"heading": "Neural Network", "text": "To jointly model F (Bt\u22121, xt) and P (yt|Bt) we used a small feed-forward recurrent network as illustrated in Figure 4. This architecture has four layers and uses convolutional operations followed by a sigmoid nonlinearity as the basic information processing step at every layer. The network has in total 11k parameters and its hyperparameters such as number of channels in each layer and size of the kernels were set by cross-validation. The belief state Bt is represented by the 3rd layer of size 50\u00d750\u00d716 which is kept between time steps.\nThe mapping F (Bt\u22121, xt) is composed of the stage of input-preprocessing (the Encoder) followed by a stage of hidden state propagation (the Belief tracker). The aim of the Encoder is to analyse the sensor measurements, to perform operations such as detecting objects directly visible to the\n2 7x7 Kernel\n16\n8\n1\nInput\nEncoder\nBelief tracker\nDecoder\n5x5 Kernel\n7x7 Kernel\ntt-1\n5x5 Kernel\nP(y |B )t t\nF(B , x )t-1 t\nx t\n50\n50\nFigure 4: The 4-layer recurrent network architecture used for the experiment. Directly visible objects are detected in Encoder and fed into Belief tracker to update belief Bt used for scene deocclusion in Decoder.\nsensor and to convert this information into a 50\u00d750\u00d78 embedding as input to the Belief tracker. This information is concatenated with the previous belief state Bt\u22121 and in Belief tracker combined into a new belief state Bt.\nFinally, P (yt|Bt) is modelled by a Bernoulli distribution by interpreting the network final layer output as a probabilistic occupancy grid pt of the corresponding pixels being part of an object giving rise to the total joint probability\nP (yt|pt) = \u220f i (pit) yit(1\u2212 pit) (1\u2212yit) (11)\nwith logarithm corresponding to the binary cross-entropy. One pass through the network takes 10ms on a standard laptop drawing the method suitable for real-time data filtering."}, {"heading": "Training", "text": "We generated 10,000 sequences of length 200 time steps and trained the network for a total of 50,000 iterations using stochastic gradient descent with learning rate 0.9. The initial belief Bt was modelled as a free parameter being jointly optimised with the network weights WF ,WP .\nBoth supervised (i.e. when the groundtruth of yt is known) and un-supervised training were attempted with almost identical results. Figure 5 illustrates the unsupervised training progress. Initially, the network produces random output, then it gradually learns to predict the correct shape of the visible objects and finally it learns to track their position even through complete occlusion. As illustrated in the attached video, the fully trained network is able to confidently and correctly start tracking an object immediately after it has seen even only a part of it and is then able to confidently keep predicting its position even through long periods of occlusion.\nFinally, Figure 6 shows example activations of layers from different parts of the network. It can be seen that the Encoder module learned to produce in one of its layers a spike at the center of visible objects. Particularly interesting is the learned structure of the belief state. Here, the network learned to represent hypotheses of objects having different\nmotion patterns with different patterns of unit activations and to track this pattern from frame to frame. None of this behaviour is hard-coded and it is a pure result of the network adaptation to the task.\nAdditionally we performed experiments changing the shape of objects from circles to squares and also simulating sensor noise on 1% of all pixel observations3. In all cases the network correctly learned and predicted the world state, suggesting the approach and the learning procedure is able to perform well in a variety of scenarios. We however expect more complex cases will require networks having more intermediate layers or using different units such as LSTM (Hochreiter and Schmidhuber 1997)."}, {"heading": "Related Works", "text": "Our work concerns modelling partially-observable stochastic processes with a particular focus on the application\n3See the attached video.\nof tracking objects. This problem is commonly solved by Bayesian filtering which gave rise to tracking methods for a variety of domains (Yilmaz, Javed, and Shah 2006). Commonly, in these approaches the state representation is designed by hand and the prediction and correction operations are made tractable under a number of assumption on model distributions or via sampling based methods. The Kalman filter (Kalman 1960), for example, assumes a multivariate normal distribution to represent the belief over the latent state leading to the well-known prediction/update equations but limiting its expressive power. In contrast, the Particle filter (Thrun, Burgard, and Fox 2005) foregoes any assumptions on belief distributions and employs a sample-based approach. In addition, where multiple objects are to be tracked, correct data association is crucial for the tracking process to succeed.\nIn contrast to using such hand-designed pipelines we provide a novel end-to-end trainable solution allowing to automatically learn an appropriate belief state representation as\nwell as the corresponding predict and update operations for an environment containing multiple objects with different appearance and potentially very complex behaviour. While we are not the first to leverage the expressive power of neural networks in the context of tracking, prior art in this domain primarily concerned only the detection part of the tracking pipeline (Fan et al. 2010), (Ja\u0308nen et al. 2010).\nA full neural-network pipeline requires the ability to learn and simulate the dynamics of the underlying system state in the absence of measurements such as when the tracked object becomes occluded. Modelling complex dynamic scenes however requires modelling distributions of exponentiallylarge state representations such as real-valued vectors having thousands of elements. A pivotal work to model highdimensional sequences was based on using Temporal Restricted Boltzman Machines (Sutskever and Hinton 2007; Sutskever, Hinton, and Taylor 2009; Mittelman et al. 2014). This generative model allows to model the joint distribution of P (y, x). The downside of using an RBM, however, is the need of sampling, making the inference and learning computationally expensive. In our work, we assume an underlying generative model but, in the spirit of Conditional Random Fields (Lafferty, McCallum, and Pereira 2001) we directly model only P (y|x). Instead of RBM our architecture features Feed-forward Recurrent Neural Networks (Medsker and Jain 2001; Graves 2013) making the inference and weight gradients computation exact using a standard back-propagation learning procedure (Rumelhart, Hinton,\nand Williams 1988). Moreover, unlike undirected models, a feed-forward network allows the freedom to straightforwardly apply a wide variety of network architectures such as fully-connected layers and LSTM (Hochreiter and Schmidhuber 1997) to design the network. Our used architecture is similar to recent Encoder-Recurrent-Decoder (Fragkiadaki et al. 2015) and similarly to (Shi et al. 2015) features convolutions for spatial processing."}, {"heading": "Conclusions", "text": "In this work we presented Deep Tracking, an end-to-end approach using recurrent neural networks to map directly from raw sensor data to an interpretable yet hidden sensor space, and employ it to predict the unoccluded state of the entire scene in a simulated 2D sensing application. The method avoids any hand-crafting of plant or sensor models and instead learns the corresponding models directly from raw, occluded sensor data. The approach was demonstrated on a synthetic dataset where it achieved highly faithful reconstructions of the underlying world model.\nAs future work our aim is to evaluate Deep Tracking on real data gathered by robots in a variety of situations such as pedestrianised areas or in the context of autonomous driving in the presence of other traffic participant. In both situations knowledge of the likely unoccluded scene is a pivotal requirement for robust robot decision making. We further intend to extend our approach to different modalities such as 3D point-cloud data and depth cameras.\nPresented at AAAI-16 conference, February 12-17, 2016, Phoenix, Arizona USA"}], "references": [{"title": "Bayesian filtering: From kalman filters to particle filters, and beyond", "author": ["Z. Chen"], "venue": null, "citeRegEx": "Chen,? \\Q2003\\E", "shortCiteRegEx": "Chen", "year": 2003}, {"title": "Human tracking using convolutional neural networks", "author": ["Fan"], "venue": "Neural Networks, IEEE Transactions on 21(10):1610\u20131623", "citeRegEx": "Fan,? \\Q2010\\E", "shortCiteRegEx": "Fan", "year": 2010}, {"title": "Recurrent network models for human dynamics", "author": ["Fragkiadaki"], "venue": null, "citeRegEx": "Fragkiadaki,? \\Q2015\\E", "shortCiteRegEx": "Fragkiadaki", "year": 2015}, {"title": "Long short-term memory. Neural computation 9(8):1735\u20131780", "author": ["Hochreiter", "S. Schmidhuber 1997] Hochreiter", "J. Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Multi-object tracking using feed-forward neural networks", "author": ["J\u00e4nen"], "venue": "In Soft Computing and Pattern Recognition (SoCPaR),", "citeRegEx": "J\u00e4nen,? \\Q2010\\E", "shortCiteRegEx": "J\u00e4nen", "year": 2010}, {"title": "A new approach to linear filtering and prediction problems", "author": ["R.E. Kalman"], "venue": "Journal of Fluids Engineering", "citeRegEx": "Kalman,? \\Q1960\\E", "shortCiteRegEx": "Kalman", "year": 1960}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, 1097\u20131105", "author": ["Sutskever Krizhevsky", "A. Hinton 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["McCallum Lafferty", "J. Pereira 2001] Lafferty", "A. McCallum", "F.C. Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Recurrent neural networks. Design and Applications", "author": ["Medsker", "L. Jain 2001] Medsker", "L. Jain"], "venue": null, "citeRegEx": "Medsker et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Medsker et al\\.", "year": 2001}, {"title": "Structured recurrent temporal restricted boltzmann machines", "author": ["Mittelman"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Mittelman,? \\Q2014\\E", "shortCiteRegEx": "Mittelman", "year": 2014}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner 1989] Rabiner"], "venue": "Proceedings of the IEEE 77(2):257\u2013286", "citeRegEx": "Rabiner,? \\Q1989\\E", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Learning internal representations by error propagation", "author": ["Hinton Rumelhart", "D.E. Williams 1985] Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Learning representations by back-propagating errors. Cognitive modeling 5:3", "author": ["Hinton Rumelhart", "D.E. Williams 1988] Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["Shi"], "venue": "arXiv preprint arXiv:1506.04214", "citeRegEx": "Shi,? \\Q2015\\E", "shortCiteRegEx": "Shi", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Srivastava"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "Srivastava,? \\Q2014\\E", "shortCiteRegEx": "Srivastava", "year": 2014}, {"title": "Learning multilevel distributed representations for high-dimensional sequences", "author": ["Sutskever", "I. Hinton 2007] Sutskever", "G.E. Hinton"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Sutskever et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2007}, {"title": "The recurrent temporal restricted boltzmann machine", "author": ["Hinton Sutskever", "I. Taylor 2009] Sutskever", "G.E. Hinton", "G.W. Taylor"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Deep neural networks for object detection", "author": ["Toshev Szegedy", "C. Erhan 2013] Szegedy", "A. Toshev", "D. Erhan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Object tracking: A survey. Acm computing surveys (CSUR) 38(4):13", "author": ["Javed Yilmaz", "A. Shah 2006] Yilmaz", "O. Javed", "M. Shah"], "venue": null, "citeRegEx": "Yilmaz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yilmaz et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Like many approaches to object tracking, our model is inspired by Bayesian filtering (Chen 2003).", "startOffset": 85, "endOffset": 96}, {"referenceID": 10, "context": "The methods such as the Hidden Markov Model (Rabiner 1989) are therefore not directly applicable.", "startOffset": 44, "endOffset": 58}, {"referenceID": 5, "context": "The Kalman filter (Kalman 1960), for example, assumes a multivariate normal distribution to represent the belief over the latent state leading to the well-known prediction/update equations but limiting its expressive power.", "startOffset": 18, "endOffset": 31}], "year": 2016, "abstractText": "This paper presents to the best of our knowledge the first end-to-end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or system identification in the form of plant or sensor models. Specifically, our system accepts a stream of raw sensor data at one end and, in real-time, produces an estimate of the entire environment state at the output including even occluded objects. We achieve this by framing the problem as a deep learning task and exploit sequence models in the form of recurrent neural networks to learn a mapping from sensor measurements to object tracks. In particular, we propose a learning method based on a form of input dropout which allows learning in an unsupervised manner, only based on raw, occluded sensor data without access to ground-truth annotations. We demonstrate our approach using a synthetic dataset designed to mimic the task of tracking objects in 2D laser data \u2013 as commonly encountered in robotics applications \u2013 and show that it learns to track many dynamic objects despite occlusions and the presence of sensor noise.", "creator": "LaTeX with hyperref package"}}}