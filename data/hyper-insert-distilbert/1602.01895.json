{"id": "1602.01895", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "Generate Image Descriptions based on Deep RNN and Memory Cells for Images Features", "abstract": "generating natural language stimulus descriptions for images is itself a challenging task. at the traditional hacker way he is attempted to use the convolutional neural semantic network ( abbreviated cnn ) to digitally extract unconscious image ancestral features, randomly followed vertically by recurrent neural functional network ( rnn ) to precisely generate sentences. in this paper, sometimes we present a new model that added memory starved cells to gate the maximal feeding of image features engineered to evade the deep neural network. the intuition is enabling our mental model user to memorize explicitly how that much unwanted information from image images should be fed at each possible stage of developing the rnn. experiments on various flickr8k squirrel and flickr30k monkey datasets directly showed that our reconstruction model far outperforms among other state - of - are the - picture art primate models already with higher conditional bleu scores.", "histories": [["v1", "Fri, 5 Feb 2016 00:17:18 GMT  (140kb,D)", "http://arxiv.org/abs/1602.01895v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["shijian tang", "song han"], "accepted": false, "id": "1602.01895"}, "pdf": {"name": "1602.01895.pdf", "metadata": {"source": "CRF", "title": "Generate Image Descriptions based on Deep RNN and Memory Cells for Images Features", "authors": ["Shijian Tang", "Song Han"], "emails": ["sjtang@stanford.edu", "songhan@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Generating natural language descriptions for images has became an attractive research topic in recent years. The task is to generate sentences or phrases to summarize and describe the contents shown in images. With this technique, the machines are enabled to imitate the behaviour of human beings who are able to capture the semantic meaning encoded in images. Some previous work from Gupta and Mannem (2012), Kulkarni et al. (2011) and Desmond Elliott and Frank Keller (2013) designed templates for the sentence descriptions. The task is to fill in the templates based on the images. However, these approaches strongly limited the capability of models to generate sentence descriptions to only fixed patterns. Other approaches transfer this task into a multimodal embedding problem. These work from Farhadi et al. (2011), Jia et al. (2011), Socher et al. (2011), Ordonez et al. (2011) overlap with the scope of information retrieval. The goal is to map the images with sentences appearing in the train-\ning dataset together in a multimodal space. However, these models are only capable of returning sentence descriptions that existed in the training dataset.\nMost of the state-of-the-art approaches are based on neural networks. These work combined convolutional neural network (CNN) with recurrent neural network (RNN) to generate image descriptions. Karpathy (2015) develop a multimodal RNN for this task. In this neural network, the image features extracted from the VGGNet (a pretrained CNN proposed in Simonyan and Zisserman (2014)) are fed into a RNN. Conditioned on the image features and previous words, the RNN will generate a sequence of words recurrently to describe the images. Similar to Kaparthy\u2019s work, Vinyals Vinyals et al. (2014) used the GoogLeNet CNN to extract image features and train a LSTM (in Hochreiter and Schmidhuber (1997)) as sequence generator. Mao et al. (2014) report a deep complex multimodal RNN for sentence generation.\nIn our approach, VGGNet is employed to extract image features and a deep multilayer RNN is chosen as a sequence generator, on top of which we informatively added memory gate that controls image feeding. In each time step of RNN 1, we feed word in current time step as well as the image features into the hidden layer of RNN. Inspired by the ideas in Rolls and Deco (2002) that the visual perception depends on short-term memory and has the recurrent natural, a memory gate is designed to control the input of image features to the hidden layer. The output of memory gate depends on the output of hidden layer at the previous time step. Before feeding into the hidden layer, the image features are multiplied by the output of gate element-wisely. Therefore, the memory gates act as memory cells for image features. Our model\n1In the RNN language model, the time step is defined as the position of word in sentence.\nar X\niv :1\n60 2.\n01 89\n5v 1\n[ cs\n.C V\n] 5\nF eb\n2 01\n6\nis trained on the Flickr8K and Flickr30K datasets from Hodosh et al. (2013). We evaluate the BLEU score (proposed in Papineni et al. (2002)) of our model on the test datasets of both Flickr8K and Flickr30K. The preliminary results show that the performance of our model outperforms the stateof-the-art work."}, {"heading": "2 The Architecture of Model", "text": ""}, {"heading": "2.1 Image Features Representation", "text": "CNN has been proved as a powerful tool to extract image features, and has been widely used in image classification (Krizhevsky et al. (2012)), object detection (Girshick et al. (2014)) and other tasks. In this paper, We select the deep and powerful VGGNet to extract image features. Specifically, each raw image is fed into the VGGNet as input. After the forward propagation, the last fully-connected layer will output a 4096 dimensions vector as the image features for each image."}, {"heading": "2.2 Sentence Representation", "text": "The sentence can be represented as a sequence of single word. The time step t is defined as the index of t th word in the sentence to represent the position of each word. Suppose the sentence contains T words, the time step of first word is t = 1, the second word is t = 2, and for the last word is t = T . For each sentence, we add a special START token at the first time step to indicate the start of the sentence, as well as the END token at the last time step as the end of each sentence.\nThe single word is represented as a vector. Some pretrained word vector models have been developed such as word2vec by Mikolov et al. (2013) and Glove by Pennington et al. (2014). However, in this model, we trained the word vectors from scratch instead of directly adopted the pretrained model, since generally the retrained word vector will achieve higher performance for specific task.\nThe standard RNN model can be expressed as,\nh(t) = f(W sx(t) +W hh(t\u2212 1) + bh) (1) y(t) = softmax(W dh(t) + bd)\nwhere h(t) is the output of the hidden layer at time step t, x(t) is the word vector for the word at t, h(t\u22121) is the output of hidden layer for the previous time step t \u2212 1, f() is the activation function.\nW s has the dimension of H by D where H is the dimension of hidden layer and D is the dimension of word vector. W h has the dimension ofH byH . W d has the dimension of V by H with V as the vocabulary size. bd and bh are the bias terms. Vector y(t) represents the probability of each word in the vocabulary to be the next word conditioned on the input words from time step 1 to t.\nIn our model, to improve the model capacity, we increase the depth of RNN by adding multiple hidden layers which is the same as deep transition RNN (DT-RNN) model reported by Pascanu et al. (2014). Pascanu et al. (2014) shows that the DT-RNN is able to increase the size of family of functions it can represent in language modeling. Unlike the standard RNN in equation 1 with only a single hidden layer at each time step, N hidden layers are stacked together at each time step in DTRNN. The forward propagation of this deep RNN model is,\nh1(t) = f(W sx(t) +W hhN (t\u2212 1) + bh)\nh2(t) = f(W hh1(t) + b h)\n... (2)\nhN (t) = f(W hhN\u22121(t) + b h)\ny(t) = softmax(W dhN (t) + b d)\nwhere h1(t), h2(t), ..., hN (t) represent the output of N hidden layers at t. The word vector x(t) as well as the output of last hidden layer at previous time step hN (t \u2212 1) are fed into the first hidden layer h1(t) at t. Then, output of current hidden layer feeds into the next hidden layer consecutively. The output y(t) depends on the output of last hidden layer at current time step hN (t). In our model, the deep RNN in equation 2 is chosen as the sequence learner of sentences."}, {"heading": "2.3 Memory Cells for Image Features", "text": "We consider how to control feeding the image features into the deep RNN. Instead of feeding the image features directly, we add a gate to control the magnitude of image feature feeds. The value of the gate depends on the state of hidden layers at previous time step.\nh1(t) = f(W sx(t) +W hhN (t\u2212 1)+\ng(t) \u25e6 (W iCNN(I) + bi) + bh) (3) g(t) = \u03c3(W ghN (t\u2212 1) + bg)\nwhere I represents the raw image, and CNN(I) is the image features extracted by CNN. W i has the dimension of H by 4096 which maps the image features to the same space of hidden layers of RNN. g(t) is the output of gate, and \u25e6 is the element-wise multiplication. W g transfers the value of the last hidden layer in the previous time step (hN (t \u2212 1) in the equation) to the gate g(t). bi and bg are the bias terms. Here we use the \u03c3 activation function and the value of g(t) ranges from 0 to 1.\nBased on equation 3, the image features are fed into the first hidden layer at each time step, multiplied by the output of gate. Since the value of gate depends on the last hidden layer of previous time step, the gate controls how much information from image is still needed for the current time step. In the case of g(t) = 0, the image features are not fed into RNN, while for g(t) = 1, we feed full image features at each time step.\nCombining equations 2 and 3 together, this model can be represented as:\ng(t) = \u03c3(W ghN (t\u2212 1) + bg) h1(t) = f(W\nsx(t) +W hhN (t\u2212 1)+ g(t) \u25e6 (W iCNN(I) + bi) + bh)\nh2(t) = f(W hh1(t) + b h)\n... (4)\nhN (t) = f(W hhN\u22121(t) + b h)\ny(t) = softmax(W dhN (t) + b d)\nFigure 1 shows the architecture of this model.\nRecall the work of Karpathy (2015), image features are only fed at the first time step of RNN. Due to the vanishing gradient problem, image features will not be learned well with long sentence\nand deep network. However, our model feed image features into RNN at each time step. Therefore, our model is still able to learn information from the image even for larger time steps. The magnitude of image features is conditioned on the hidden state of previous time step. In another word, the image features are encoded based on the status of how well our model has learned.\nCompared with other work of Vinyals et al. (2014) based on LSTM and work of Mao et al. (2014) based on multimodal embeddings, our model has the advantage of lower model complexity and easier to train."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Dataset", "text": "We experimented on the Flickr8K and Flickr30K datasets introduced in Hodosh et al. (2013). Each image in these datasets is described by 5 independent sentences. Therefore, for each image, we can create 5 samples with each one as an imagesentence pair. We have 8000 and 31000 images for Flickr8K and Flickr30K respectively. Each dataset has been splited into development data with 1000 images, test data with 1000 images and the rest images as training data. The data preprocessing procedure is the same as the work of Karpathy (2015)."}, {"heading": "3.2 Training", "text": "During training, cross entropy loss was chosen as the loss function. Stochastic gradient descent (SGD) with minibatch size of 100 image-sentence pairs was used during training. To make the model converge faster, RMSprop annealing policy Hinton et al. (2012) was adopted, where the step size of each parameter is scaled by the windowaveraged norm of its gradient.\nTo overcome the vanishing gradient problem, ReLU is chosen as the activation function. Also, we adopted the element-wise clip gradient tricks, where we clipped the gradient to 5. To regularize the model, we add L2 norm of weights to the loss function, and as Zaremba et al. (2014) suggested, we used dropout ratio of 0.5 to all the layers except for the hidden layers.\nAs equation 4 indicates, a model with large N has deeper hidden layers, which leads to a large capacity. Considering the size of the dataset is not large and in order to prevent overfitting, we adopt a small N = 2 with 2 hidden layers in the experiments in equation 4.\nWe find 50 epochs are enough to train this model for both datasets, and the hidden size was tuned to 512 to achieve the best performance."}, {"heading": "3.3 Generate Image Description", "text": "The sentence description for each image in test dataset is generated by feeding the image features into the trained model with a START token. At each time step, we can directly choose the word corresponds to the one with highest probability in vocabulary as the output word, which is also the input word of next time step. Following this method, we can generate a sentence recurrently until we reach the END token.\nTo evaluate the performance, we use the BLEU score as evaluation metrics which has been widely adopted in the papers focus on this topic (Karpathy (2015), Vinyals et al. (2014), Mao et al. (2014)). The BLEU score will evaluate the similarity of the generated sentences with the ground truth sentences. Table 1 and Table 2 show the BLEU score for several models.\nAs shown on Table 1 and Table 2, our model outperforms the results from Karpathy (2015) and Mao et al. (2014). While the performance of our model is lower than the original work from Vinyals et al. (2014). However, this is because in the original work of Vinyals et al. (2014), the authors used the GoogleNet (in Szegedy et al. (2014)) to extract the image features, while we\nused VGGNet. Therefore, it is unfair to directly compare the BLEU score of our model with results reported by Vinyals et al. (2014).\nTo make a fair comparison with the network in Vinyals et al. (2014), we have downloaded the reproduced version of Vinyals\u2019 model from http://cs.stanford.edu/ people/karpathy/neuraltalk/. In this reproduced model trained on Flickr8K, the image features feed into Vinyals\u2019 model are extracted by VGGNet, which is the same as the case in our model. From the last row of Table 1, we can find that the performance of our model is better than the model in Vinyals et al. (2014) if both models use the VGGNet image features. Note that even though the reproduced model of Vinyals et al. (2014) based on Flickr30K dataset is unavailable now, our model still outperforms other stateof-the-art works.\nWe also tried to feed image features only at first time step (i.e., set g(t) = 0 except for the first time step) as well as feed full image features at each time step (i,e., set g(t) = 1 for all time steps). But the results show that the performance all of these two schemes are lower than feeding image features at each time step with memory cells."}, {"heading": "4 Conclusion", "text": "In this paper, we developed a new model for generating image descriptions. The image features extracted from VGGNet are fed into each time step of a multilayer deep RNN, where the image features vector is element-wisely multiplied by a memory vector determined by the state of the hidden layer at previous time step. Experiments on Flickr8K and Flickr30K datasets show that this model achieves higher performance on BLEU score. Our model also benefit from its low complexity and ease of training.\nAs the extension of this work, we will train our model on a larger dataset such as MSCOCO, and will increase the number of hidden layers at each time step to further improve the performance of our model. We will also try to adopt other CNNs such as GoogleNet to extract image features. Also, in this work, we do not fine-tune the CNNs on the new datasets, in future, we will try to train the model and tune the CNNs together."}], "references": [{"title": "From image annotation to image description", "author": ["Gupta", "Mannem2012] Ankush Gupta", "Prashanth Mannem"], "venue": null, "citeRegEx": "Gupta et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2012}, {"title": "Alexander Berg", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi"], "venue": "and Tamara Berg.", "citeRegEx": "Kulkarni et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Image description using visual dependency representations", "author": ["Desmond Elliott", "Frank Keller"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2013}, {"title": "Julia Hockenmaier", "author": ["Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian"], "venue": "and David Forsyth.", "citeRegEx": "Farhadi et al. 2011", "shortCiteRegEx": null, "year": 2010}, {"title": "Mathieu Salzmann", "author": ["Yangqing Jia"], "venue": "and Trevor Darrell.", "citeRegEx": "Jia et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Andrew Y", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning"], "venue": "Ng.", "citeRegEx": "Socher et al. 2011", "shortCiteRegEx": null, "year": 2014}, {"title": "and Tamara L", "author": ["Vicente Ordonez", "Girish Kulkarni"], "venue": "Berg.", "citeRegEx": "Ordonez et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": null, "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Samy Bengio", "author": ["Oriol Vinyals", "Alexander Toshev"], "venue": "and Dumitru Erhan.", "citeRegEx": "Vinyals et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "and Alan L", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang"], "venue": "Yuille.", "citeRegEx": "Mao et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Peter Young", "author": ["Micah Hodosh"], "venue": "and Julia Hockenmaier.", "citeRegEx": "Hodosh et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Todd Ward", "author": ["Kishore Papineni", "Salim Roukos"], "venue": "and Wei-Jing Zhu", "citeRegEx": "Papineni et al. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Ilya Sutskever", "author": ["Alex Krizhevsky"], "venue": "and Geoffrey Hinton.", "citeRegEx": "Krizhevsky et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Trevor Darrell", "author": ["Ross Girshick", "Jeff Donahue"], "venue": "and Jitendra Malik", "citeRegEx": "Girshick et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Greg Corrado", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen"], "venue": "and Jeffrey Dean", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Christopher D", "author": ["Jeffrey Pennington", "Richard Socher"], "venue": "Manning", "citeRegEx": "Pennington et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Kyunghyun Cho", "author": ["Razvan Pascanu", "Caglar Gulcehre"], "venue": "and Yoshua Bengio", "citeRegEx": "Pascanu et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Nitish Srivastava", "author": ["Geoffrey Hinton"], "venue": "and Kevin Swersky.", "citeRegEx": "Hinton et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Ilya Sutskever", "author": ["Wojciech Zaremba"], "venue": "Oriol Vinyals", "citeRegEx": "Zaremba et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Vincent Vanhoucke", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan"], "venue": "Andrew Rabinovich", "citeRegEx": "Szegedy et al. 2014", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "Generating natural language descriptions for images is a challenging task. The traditional way is to use the convolutional neural network (CNN) to extract image features, followed by recurrent neural network (RNN) to generate sentences. In this paper, we present a new model that added memory cells to gate the feeding of image features to the deep neural network. The intuition is enabling our model to memorize how much information from images should be fed at each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed that our model outperforms other state-of-the-art models with higher BLEU scores.", "creator": "LaTeX with hyperref package"}}}