{"id": "1602.02068", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Feb-2016", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification", "abstract": "externally we propose sparsemax, a new smooth activation function similar physically to the traditional virtual softmax, greedy but able to output incredibly sparse probabilities. after directly deriving its desired properties, we show how its parameter jacobian can be efficiently computed, enabling from its use now in a network trained learning with network backpropagation. then, thereby we propose introduce a new smooth log and convex cluster loss function giving which output is the sparsemax without analogue error of the delayed logistic loss. formally we likewise reveal an unexpected connection between this new loss and also the noisy huber classification loss. we obtain initially promising empirical results in multi - dimension label classification analysis problems simpler and more in attention - based computer neural networks employed for some natural information language inference. for the emerging latter, we achieve yields a basically similar smoothing performance as the traditional numerical softmax, but with a subsequent selective, increasingly more compact, attention focus.", "histories": [["v1", "Fri, 5 Feb 2016 15:49:02 GMT  (1062kb,D)", "http://arxiv.org/abs/1602.02068v1", null], ["v2", "Mon, 8 Feb 2016 09:41:36 GMT  (1062kb,D)", "http://arxiv.org/abs/1602.02068v2", "Minor corrections"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG stat.ML", "authors": ["andr\u00e9 f t martins", "ram\u00f3n fern\u00e1ndez astudillo"], "accepted": true, "id": "1602.02068"}, "pdf": {"name": "1602.02068.pdf", "metadata": {"source": "META", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification", "authors": ["Andr\u00e9 F. T. Martins", "Ram\u00f3n F. Astudillo"], "emails": ["ANDRE.MARTINS@UNBABEL.COM", "RAMON@UNBABEL.COM"], "sections": [{"heading": "1. Introduction", "text": "The softmax transformation is a key component of several statistical learning models, encompassing multinomial logistic regression (McCullagh & Nelder, 1989), action selection in reinforcement learning (Sutton & Barto, 1998), and neural networks for multi-class classification (Bridle, 1990; Goodfellow et al., 2016). Recently, it has also been used to design attention mechanisms in neural networks, with important achievements in machine translation (Bahdanau et al., 2015), image caption generation (Xu et al., 2015), speech recognition (Chorowski et al., 2015), memory networks (Sukhbaatar et al., 2015), and various tasks in natural language understanding (Hermann et al., 2015; Rockta\u0308schel et al., 2015; Rush et al., 2015) and computation learning (Graves et al., 2014; Grefenstette et al., 2015).\nThere are a number of reasons why the softmax transformation is so appealing. It is simple to evaluate and differentiate, and it can be turned into the (convex) negative log-likelihood loss function by taking the logarithm of its\noutput. Alternatives proposed in the literature, such as the Bradley-Terry model (Bradley & Terry, 1952; Zadrozny, 2001; Menke & Martinez, 2008), the multinomial probit (Albert & Chib, 1993), the spherical softmax (Ollivier, 2013; Vincent, 2015; de Bre\u0301bisson & Vincent, 2015), or softmax approximations (Bouchard, 2007), while theoretically or computationally advantageous for certain scenarios, lack some of the convenient properties of softmax.\nIn this paper, we propose the sparsemax transformation. Sparsemax has the distinctive feature that it can return sparse posterior distributions, that is, it may assign exactly zero probability to some of its output variables. This property makes it appealing to be used as a filter for large output spaces, to predict multiple labels, or as a component to identify which of a group of variables are potentially relevant for a decision, making the model more interpretable. Crucially, this is done while preserving most of the attractive properties of softmax: we show that sparsemax is also simple to evaluate, it is even cheaper to differentiate, and that it can be turned into a convex loss function.\nTo sum up, our contributions are as follows:\n\u2022 We formalize the new sparsemax transformation, derive its properties, and show how it can be efficiently computed (\u00a72.1\u20132.3). We show that in the binary case sparsemax reduces to a hard sigmoid (\u00a72.4).\n\u2022 We derive the Jacobian of sparsemax, comparing it to the softmax case, and show that it can lead to faster gradient backpropagation (\u00a72.5).\n\u2022 We propose the sparsemax loss, a new loss function that is the sparsemax analogue of logistic regression (\u00a73). We show that it is convex, everywhere differentiable, and can be regarded as a multi-class generalization of the Huber classification loss, an important tool in robust statistics (Huber, 1964; Zhang, 2004).\n\u2022 We apply the sparsemax loss to train multi-label linear\nar X\niv :1\n60 2.\n02 06\n8v 1\n[ cs\n.C L\n] 5\nF eb\n2 01\nclassifiers (which predict a set of labels instead of a single label) on benchmark datasets (\u00a74.1\u20134.2).\n\u2022 Finally, we devise a neural selective attention mechanism using the sparsemax transformation, evaluating its performance on a natural language inference problem, with encouraging results (\u00a74.3)."}, {"heading": "2. The Sparsemax Transformation", "text": ""}, {"heading": "2.1. Definition", "text": "Let \u2206K\u22121 := {p \u2208 RK | 1>p = 1, p \u2265 0} be the (K \u2212 1)-dimensional simplex. We are interested in functions that map vectors in RK to probability distributions in \u2206K\u22121. Such functions are useful for converting a vector of real weights (e.g., label scores) to a probability distribution (e.g. posterior probabilities of labels). The classical example is the softmax function, defined componentwise as:\nsoftmaxi(z) = exp(zi)\u2211 j exp(zj) . (1)\nA limitation of the softmax transformation is that the resulting probability distribution always has full support, i.e., softmaxi(z) 6= 0 for every z and i. This is a disadvantage in applications where a sparse probability distribution is desired, in which case it is common to define a threshold below which small probability values are truncated to zero.\nIn this paper, we propose as an alternative the following transformation, which we call sparsemax:\nsparsemax(z) := argmin p\u2208\u2206K\u22121\n\u2016p\u2212 z\u20162. (2)\nIn words, sparsemax returns the Euclidean projection of the input vector z onto the probability simplex. This projection is likely to hit the boundary of the simplex, in which case sparsemax(z) becomes sparse. We will see that sparsemax retains most of the important properties of softmax, having in addition the ability of producing sparse distributions."}, {"heading": "2.2. Closed-Form Solution", "text": "Projecting onto the simplex is a well studied problem, for which linear-time algorithms are available (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008). We start by recalling the well-known result that such projections correspond to a soft-thresholding operation. Below, we use the notation [K] := {1, . . . ,K} and [t]+ := max{0, t}.\nProposition 1 The solution of Eq. 2 is of the form:\nsparsemaxi(z) = [zi \u2212 \u03c4(z)]+, (3)\nwhere \u03c4 : RK \u2192 R is the (unique) function that satisfies \u2211 j [zj \u2212 \u03c4(z)]+ = 1 for every z. Furthermore, \u03c4\nAlgorithm 1 Sparsemax Evaluation Input: z Sort z as z(1) \u2265 . . . \u2265 z(K) Find k(z) := max { k \u2208 [K] | 1 + kz(k) > \u2211 j\u2264k z(j)\n} Define \u03c4(z) = ( \u2211 j\u2264k(z) z(j))\u22121\nk(z)\nOutput: p s.t. pi = [zi \u2212 \u03c4(z)]+.\ncan be expressed as follows. Let z(1) \u2265 z(2) \u2265 . . . \u2265 z(K) be the sorted coordinates of z, and define k(z) :=\nmax { k \u2208 [K] | 1 + kz(k) > \u2211 j\u2264k z(j) } . Then,\n\u03c4(z) =\n(\u2211 j\u2264k(z) z(j) ) \u2212 1\nk(z) =\n(\u2211 j\u2208S(z) zj ) \u2212 1\n|S(z)| , (4)\nwhere S(z) := {j \u2208 [K] | sparsemaxj(z) > 0} is the support of sparsemax(z).\nProof: See App. A.1.\nIn essence, Prop. 1 states that all we need for evaluating the sparsemax transformation is to compute the threshold \u03c4(z); all coordinates above this threshold (the ones in the set S(z)) will be shifted by this amount, and the others will be truncated to zero. We call \u03c4 in Eq. 4 the threshold function. This piecewise linear function will play an important role in the sequel. Alg. 1 illustrates a na\u0131\u0308ve O(K logK) algorithm that uses Prop. 1 for evaluating the sparsemax.1"}, {"heading": "2.3. Basic Properties", "text": "We now highlight some properties that are common to softmax and sparsemax. Let z(1) := maxk zk, and denote by A(z) := {k \u2208 [K] | zk = z(1)} the set of the largest components of z. We define the indicator vector 1A(z), whose kth component is 1 if k \u2208 A(z), and 0 otherwise. We further denote by \u03b3(z) := z(1)\u2212maxk/\u2208A(z) zk the gap between the largest components of z and the second largest. We let 0 and 1 be vectors of zeros and ones, respectively.\nProposition 2 The following properties hold for \u03c1 \u2208 {softmax, sparsemax}.\n1. \u03c1(0) = 1/K and lim \u21920+ \u03c1( \u22121z) = 1A(z)/|A(z)| (uniform distribution, and distribution peaked on the largest components of z, respectively). For sparsemax, the last equality holds for any \u2264 \u03b3(z) \u00b7 |A(z)|.\n2. \u03c1(z) = \u03c1(z + c1), for any c \u2208 R (i.e., \u03c1 is invariant to adding a constant to each coordinate).\n1More elaborate O(K) algorithms exist based on linear-time selection (Blum et al., 1973; Pardalos & Kovoor, 1990).\n3. \u03c1(Pz) = P\u03c1(z) for any permutation matrix P (i.e., \u03c1 commutes with permutations).\n4. If zi \u2264 zj , then 0 \u2264 \u03c1j(z) \u2212 \u03c1i(z) \u2264 \u03b7(zj \u2212 zi), where \u03b7 = 12 for softmax, and \u03b7 = 1 for sparsemax.\nProof: See App. A.2.\nInterpreting as a \u201ctemperature parameter,\u201d the first part of Prop. 2 shows that the sparsemax has the same \u201czerotemperature limit\u201d behaviour as the softmax, but without the need of making the temperature arbitrarily small.\nProp. 2 is reassuring, since it shows that the sparsemax transformation, despite being defined very differently from the softmax, has a similar behaviour and preserves the same invariances. Note that some of these properties are not satisfied by other proposed replacements of the softmax: for example, the spherical softmax (Ollivier, 2013), defined as \u03c1i(z) := z 2 i / \u2211 j z 2 j , does not satisfy properties 2 and 4."}, {"heading": "2.4. Two and Three-Dimensional Cases", "text": "For the two-class case, it is well known that the softmax activation becomes the logistic (sigmoid) function. More precisely, if z = (t, 0), then softmax1(z) = \u03c3(t) := (1 + exp(\u2212t))\u22121. We next show that the analogous in sparsemax is the \u201chard\u201d version of the sigmoid. In fact, using Prop. 1, Eq. 4, we have that, for z = (t, 0),\n\u03c4(z) =  t\u2212 1, if t > 1(t\u2212 1)/2, if \u22121 \u2264 t \u2264 1\u22121, if t < \u22121, (5) and therefore\nsparsemax1(z) =  1, if t > 1(t+ 1)/2, if \u22121 \u2264 t \u2264 1 0, if t < \u22121.\n(6)\nFig. 1 provides an illustration for the two and threedimensional cases. For the latter, we parameterize z = (t1, t2, 0) and plot softmax1(z) and sparsemax1(z) as a function of t1 and t2. We can see that sparsemax is piecewise linear, but asymptotically similar to the softmax."}, {"heading": "2.5. Jacobian of Sparsemax", "text": "The Jacobian matrix of a transformation \u03c1, J\u03c1(z) := [\u2202\u03c1i(z)/\u2202zj ]i,j , is of key importance to train models with gradient-based optimization. We next derive the Jacobian of the sparsemax activation, but before doing so, let us recall how the Jacobian of the softmax looks like. We have\n\u2202softmaxi(z) \u2202zj = \u03b4ije\nzi \u2211 k e zk \u2212 eziezj\n( \u2211 k e zk) 2\n= softmaxi(z)(\u03b4ij \u2212 softmaxj(z)), (7)\nwhere \u03b4ij is the Kronecker delta, which evaluates to 1 if i = j and 0 otherwise. Letting p = softmax(z), the full Jacobian can be written in matrix notation as\nJsoftmax(z) = Diag(p)\u2212 pp>, (8)\nwhere Diag(p) is a matrix with p in the main diagonal.\nLet us now turn to the sparsemax case. The first thing to note is that sparsemax is differentiable everywhere except at splitting points z where the support set S(z) changes, i.e., where S(z) 6= S(z+ d) for some d and infinitesimal .2 From Eq. 3, we have that:\n\u2202sparsemaxi(z)\n\u2202zj = { \u03b4ij \u2212 \u2202\u03c4(z)\u2202zj , if zi > \u03c4(z), 0, if zi \u2264 \u03c4(z). (9)\nIt remains to compute the gradient of the threshold function \u03c4 . From Eq. 4, the gradient of the threshold function \u03c4 is:\n\u2202\u03c4(z)\n\u2202zj = { 1 |S(z)| if j \u2208 S(z), 0, if j /\u2208 S(z). (10)\nNote that j \u2208 S(z) \u21d4 zj > \u03c4(z). Therefore we obtain:\n\u2202sparsemaxi(z)\n\u2202zj = { \u03b4ij \u2212 1|S(z)| , if i, j \u2208 S(z), 0, otherwise. (11)\n2For those points, we can take an arbitrary matrix in the set of generalized Clarke\u2019s Jacobians (Clarke, 1983), the convex hull of all points of the form limt\u2192\u221e Jsparsemax(zt), where zt \u2192 z.\nLet s be an indicator vector whose ith entry is 1 if i \u2208 S(z), and 0 otherwise. We can write the Jacobian matrix as\nJsparsemax(z) = Diag(s)\u2212 ss>/|S(z)|. (12)\nIt is instructive to compare Eqs. 8 and 12. We may regard the Jacobian of sparsemax as the Laplacian of a graph whose elements of S(z) are fully connected. To compute it, we only need S(z), which can be computed in O(K) time with the same algorithm that evaluates the sparsemax.\nOften, e.g., in the gradient backpropagation algorithm, it is not necessary to compute the full Jacobian matrix, but only the product between the Jacobian and a given vector v. In the softmax case, from Eq. 8, we have:\nJsoftmax(z) \u00b7v = p (v\u2212 v\u03041), with v\u0304 := \u2211 j pjvj , (13)\nwhere denotes the Hadamard product; this requires a linear-time computation. For the sparsemax case, we have:\nJsparsemax(z) \u00b7v = s (v\u2212 v\u03021), with v\u0302 := \u2211 j\u2208S(z) vj\n|S(z)| .\n(14) Interestingly, if sparsemax(z) has already been evaluated (i.e., in the forward step), then so has S(z), hence the nonzeros of Jsparsemaxv can be computed in only O(|S(z)|) time, which can be sublinear. This can be an important advantage of sparsemax over softmax if K is large."}, {"heading": "3. A Loss Function for Sparsemax", "text": "Now that we have defined the sparsemax transformation and established its main properties, we show how to use this transformation to design a new loss function that resembles the logistic loss, but can yield sparse posterior distributions. Later (in \u00a74.1\u20134.2), we apply this loss to label proportion estimation and multi-label classification."}, {"heading": "3.1. Logistic Loss", "text": "Consider a dataset D := {(xi, yi)}Ni=1, where each xi \u2208 RD is an input vector and each yi \u2208 {1, . . . ,K} is a target output label. We consider regularized empirical risk minimization problems of the form\nminimize \u03bb\n2 \u2016W\u20162F +\n1\nN N\u2211 i=1 L(Wxi + b; yi),\nw.r.t. W \u2208 RK\u00d7D, b \u2208 RK , (15)\nwhere L is a loss function, W is a matrix of weights, and b is a bias vector. The loss function associated with the softmax is the logistic loss (or negative log-likelihood):\nLsoftmax(z; k) = \u2212 log softmaxk(z) = \u2212zk + log \u2211 j exp(zj), (16)\nwhere z = Wxi + b, and k = yi is the \u201cgold\u201d label. The gradient of this loss is, invoking Eq. 7,\n\u2207zLsoftmax(z; k) = \u2212\u03b4k + softmax(z), (17)\nwhere \u03b4k denotes the delta distribution on k, [\u03b4k]j = 1 if j = k, and 0 otherwise. This is a well-known result; when plugged into a gradient-based optimizer, it leads to updates that move probability mass from the distribution predicted by the current model (i.e., softmaxk(z)) to the gold label (via \u03b4k). Can we have something similar for sparsemax?"}, {"heading": "3.2. Sparsemax Loss", "text": "A nice aspect of the log-likelihood (Eq. 16) is that adding up loss terms for several examples, assumed i.i.d, we obtain the log-probability of the full training data. Unfortunately, this idea cannot be carried out to sparsemax: now, some labels may have exactly probability zero, so any model that assigns zero probability to a gold label would zero out the probability of the entire training sample. This is of course highly undesirable. One possible workaround is to define\nL sparsemax(z; k) = \u2212 log + sparsemaxk(z)\n1 +K , (18)\nwhere is a small constant, and +sparsemaxk(z)1+K is a \u201cperturbed\u201d sparsemax. However, this loss is non-convex, unlike the one in Eq. 16.\nAnother possibility, which we explore here, is to construct an alternative loss function whose gradient resembles the one in Eq. 17. Note that the gradient is particularly important, since it is directly involved in the model updates for typical optimization algorithms. Formally, we want Lsparsemax to be a differentiable function such that\n\u2207zLsparsemax(z; k) = \u2212\u03b4k + sparsemax(z). (19)\nWe show below that this property is fulfilled by the following function, henceforth called the sparsemax loss:\nLsparsemax(z; k) = \u2212zk+ 1\n2 \u2211 j\u2208S(z) (z2j\u2212\u03c42(z))+ 1 2 , (20)\nwhere \u03c42 is the square of the threshold function in Eq. 4. This loss, which has never been considered in the literature to the best of our knowledge, has a number of interesting properties, stated in the next proposition.\nProposition 3 The following holds:\n1. Lsparsemax is differentiable everywhere, and its gradient is given by the expression in Eq. 19.\n2. Lsparsemax is convex.\n3. Lsparsemax(z + c1; k) = Lsparsemax(z; k), \u2200c \u2208 R.\n4. Lsparsemax(z; k) \u2265 0, for all z and k.\n5. The following statements are all equivalent: (i) Lsparsemax(z; k) = 0; (ii) sparsemax(z) = \u03b4k; (iii) margin separation holds, zk \u2265 1 + maxj 6=k zj .\nProof: See App. A.3.\nNote that the first four properties in Prop. 3 are also satisfied by the logistic loss, except that the gradient is given by Eq. 17. The fifth property is particularly interesting, since it is satisfied by the hinge loss of support vector machines. However, unlike the hinge loss, Lsparsemax is everywhere differentiable, hence amenable to smooth optimization methods such as L-BFGS or accelerated gradient descent (Liu & Nocedal, 1989; Nesterov, 1983)."}, {"heading": "3.3. Relation to the Huber Loss", "text": "Coincidentally, as we next show, the sparsemax loss in the binary case reduces to the Huber classification loss, an important loss function in robust statistics (Huber, 1964).\nLet us note first that, from Eq. 20, we have, if |S(z)| = 1,\nLsparsemax(z; k) = \u2212zk + z(1), (21)\nand, if |S(z)| = 2, Lsparsemax(z; k) =\n\u2212 zk + 1 + (z(1) \u2212 z(2))2 4 + z(1) + z(2) 2 , (22)\nwhere z(1) \u2265 z(2) \u2265 . . . are the sorted components of z. Note that the second expression, when z(1) \u2212 z(2) = 1, equals the first one, which asserts the continuity of the loss even though |S(z)| is non-continuous on z.\nIn the two-class case, we have |S(z)| = 1 if z(1) \u2265 1 + z(2) (unit margin separation), and |S(z)| = 2 otherwise. Assume without loss of generality that the correct label is k = 1, and define t = z1 \u2212 z2. From Eqs. 21\u201322, we have\nLsparsemax(t) =  0 if t \u2265 1 \u2212t if t \u2264 \u22121 (t\u22121)2\n4 if \u22121 < t < 1, (23)\nwhose graph is shown in Fig. 2. This loss is a variant of the Huber loss adapted for classification, and has been called \u201cmodified Huber loss\u201d by Zhang (2004); Zou et al. (2006)."}, {"heading": "3.4. Generalization to Multi-Label Classification", "text": "We end this section by showing a generalization of the loss functions in Eqs. 16 and 20 to multi-label classification, i.e., problems in which the target is a non-empty set of la-\nbels Y \u2208 2[K] \\ {\u2205} rather than a single label.3 Such problems have attracted recent interest (Zhang & Zhou, 2014).\nMore generally, we consider the problem of estimating sparse label proportions, where the target is a probability distribution q \u2208 \u2206K\u22121, such that Y = {k | qk > 0}. We assume a training dataset D := {(xi, qi)}Ni=1, where each xi \u2208 RD is an input vector and each qi \u2208 \u2206K\u22121 is a target distribution over outputs, assumed sparse.4 This subsumes single-label classification, where all qi are delta distributions concentrated on a single class. The generalization of the multinomial logistic loss to this setting is\nLsoftmax(z; q) = KL(q \u2016 softmax(z)) (24) = \u2212H(q)\u2212 q>z + log \u2211 j exp(zj),\nwhere KL(.\u2016.) and H(.) denote the Kullback-Leibler divergence and the Shannon entropy, respectively. Note that, up to a constant, this loss is equivalent to standard logistic regression with soft labels. The gradient of this loss is\n\u2207zLsoftmax(z; q) = \u2212q + softmax(z). (25)\nThe corresponding generalization in the sparsemax case is:\nLsparsemax(z; q) = \u2212q>z+ 1\n2 \u2211 j\u2208S(z) (z2j\u2212\u03c42(z))+ 1 2 \u2016q\u20162,\n(26) which satisfies the properties in Prop. 3 and has gradient\n\u2207zLsparsemax(z; q) = \u2212q + sparsemax(z). (27)\nWe make use of these losses in our experiments (\u00a74.1\u20134.2)."}, {"heading": "4. Experiments", "text": "We next evaluate empirically the ability of sparsemax for addressing two classes of problems:\n3Not to be confused with \u201cmulti-class classification,\u201d which denotes problems where Y = [K] and K > 2.\n4This scenario is also relevant for \u201clearning with a probabilistic teacher\u201d (Agrawala, 1970) and semi-supervised learning (Chapelle et al., 2006), as it can model label uncertainty.\n1. Label proportion estimation and multi-label classification, via the sparsemax loss in Eq. 26 (\u00a74.1\u20134.2).\n2. Attention-based neural networks, via the sparsemax transformation of Eq. 2 (\u00a74.3)."}, {"heading": "4.1. Label Proportion Estimation", "text": "We show simulation results for sparse label proportion estimation on synthetic data. Since sparsemax can predict sparse distributions, we expect its superiority in this task. We generated datasets with 1,200 training and 1,000 test examples. Each example emulates a \u201cmulti-labeled document\u201d: a variable-length sequence of word symbols, assigned to multiple topics (labels). We pick the number of labels N \u2208 {1, . . . ,K} by sampling from a Poisson distribution with rejection sampling, and draw theN labels from a multinomial. Then, we pick a document length from a Poisson, and repeatedly sample its words from the mixture of the N label-specific multinomials. We experimented with two settings: uniform mixtures (qkn = 1/N for the N active labels k1, . . . , kN ) and random mixtures (whose label proportions qkn were drawn from a flat Dirichlet). 5 We set the vocabulary size to be equal to the number of labels K \u2208 {10, 50}, and varied the average document length between 200 and 2,000 words. We trained models by optimizing Eq. 15 with L \u2208 {Lsoftmax, Lsparsemax} (Eqs. 24 and 26). We picked the regularization constant \u03bb \u2208 {10j}0j=\u22129 with 5-fold cross-validation.\nResults are shown in Fig. 3. We report the mean squared error (average of \u2016q \u2212 p\u20162 on the test set, where q and p are respectively the target and predicted label posteriors) and the Jensen-Shannon divergence (average of JS(q,p) := 1 2KL(q\u2016 p+q 2 )+ 1 2KL(p\u2016 p+q 2 )).\n6 We observe that the two losses perform similarly for small document lengths (where the signal is weaker), but as the average document length exceeds 400, the sparsemax loss starts outperforming the logistic loss consistently. This is because with a stronger signal the sparsemax estimator manages to identify correctly the support of the label proportions q, contributing to reduce both the mean squared error and the JS divergence. This occurs both for uniform and random mixtures."}, {"heading": "4.2. Multi-Label Classification on Benchmark Datasets", "text": "Next, we ran experiments in five benchmark multi-label classification datasets: the four small-scale datasets used by Koyejo et al. (2015),7 and the much larger Reuters RCV1\n5Note that, with uniform mixtures, the problem becomes essentially multi-label classification.\n6Note that the KL divergence is not an appropriate metric here, since the sparsity of q and p could lead to \u2212\u221e values.\n7Obtained from http://mulan.sourceforge.net/ datasets-mlc.html.\nv2 dataset of Lewis et al. (2004).8 For all datasets, we removed examples without labels (i.e. where Y = \u2205). For all but the Reuters dataset, we normalized the features to have zero mean and unit variance. Statistics for these datasets are presented in Table 1.\nRecent work has investigated the consistency of multi-label classifiers for various micro and macro-averaged metrics (Gao & Zhou, 2013; Koyejo et al., 2015), among which a plug-in classifier that trains independent binary logistic regressors on each label, and then tunes a probability threshold \u03b4 \u2208 [0, 1] on validation data. At test time, those labels whose posteriors are above the threshold are predicted to be \u201con.\u201d We used this procedure (called LOGISTIC) as a baseline for comparison. Our second baseline (SOFTMAX) is a multinomial logistic regressor, using the loss function in Eq. 24, where the target distribution q is set to uniform over the active labels. A similar probability threshold p0 is used for prediction, above which a label is predicted to be \u201con.\u201d We compare these two systems with the sparsemax loss function of Eq. 26. We found it beneficial to scale the label scores z by a constant t \u2265 1 at test time, before applying the sparsemax transformation, to make the resulting distribution p = sparsemax(tz) more sparse. We then predict the kth label to be \u201con\u201d if pk 6= 0.\nWe optimized the three losses with L-BFGS (for a maximum of 100 epochs), tuning the hyperparameters in a heldout validation set (for the Reuters dataset) and with 5-fold cross-validation (for the other four datasets). The hyperparameters are the regularization constant \u03bb \u2208 {10j}2j=\u22128, the probability thresholds \u03b4 \u2208 {.05\u00d7n}10n=1 for LOGISTIC\n8Obtained from https://www.csie.ntu.edu.tw/ \u02dccjlin/libsvmtools/datasets/multilabel.html.\nand p0 \u2208 {n/K}10n=1 for SOFTMAX, and the coefficient t \u2208 {0.5\u00d7 n}10n=1 for SPARSEMAX.\nResults are shown in Table 2. Overall, the performances of the three losses are all very similar, with a slight advantage of SPARSEMAX, which attained the highest results in 4 out of 10 experiments, while LOGISTIC and SOFTMAX won 3 times each. In particular, sparsemax appears better suited for problems with larger numbers of labels."}, {"heading": "4.3. Neural Networks with Attention Mechanisms", "text": "We now assess the suitability of the sparsemax transformation to construct a \u201csparse\u201d neural attention mechanism.\nWe ran experiments on the task of natural language inference, using the recently released SNLI 1.0 corpus (Bowman et al., 2015), a collection of 570,000 human-written English sentence pairs. Each pair consists of a premise and an hypothesis, manually labeled with one the labels ENTAILMENT, CONTRADICTION, or NEUTRAL. We used the provided training, development, and test splits.\nThe architecture of our system, shown in Fig. 4, is the same as the one proposed by Rockta\u0308schel et al. (2015). We compare the performance of four systems: NOATTENTION, a (gated) RNN-based system similar to Bowman et al. (2015); LOGISTICATTENTION, an attention-based system with independent logistic activations; SOFTATTENTION, a near-reproduction of the Rockta\u0308schel et al. (2015)\u2019s attention-based system; and SPARSEATTENTION, which replaces the latter softmax-activated attention mechanism by a sparsemax activation.\nWe represent the words in the premise and in the hypothesis with 300-dimensional GloVe vectors (Pennington et al., 2014), not optimized during training, which we linearly project onto a D-dimensional subspace (Astudillo et al.,\n2015).9 We denote by x1, . . . ,xL and xL+1, . . . ,xN , respectively, the projected premise and hypothesis word vectors. These sequences are then fed into two recurrent networks (one for each). Instead of long short-term memories, as Rockta\u0308schel et al. (2015), we used gated recurrent units (GRUs, Cho et al. 2014), which behave similarly but have fewer parameters. Our premise GRU generates a state sequence H1:L := [h1 . . .hL] \u2208 RD\u00d7L as follows:\nzt = \u03c3(W xzxt + W hzht\u22121 + b z) (28) rt = \u03c3(W xrxt + W hrht\u22121 + b r) (29) h\u0304t = tanh(W xhxt + W\nhh(rt ht\u22121) + bh) (30) ht = (1\u2212 zt)ht\u22121 + zth\u0304t, (31)\n9We used GloVe-840B embeddings trained on Common Crawl (http://nlp.stanford.edu/projects/glove/).\nwith model parameters W{xz,xr,xh,hz,hr,hh} \u2208 RD\u00d7D and b{z,r,h} \u2208 RD. Likewise, our hypothesis GRU (with distinct parameters) generates a state sequence [hL+1, . . . ,hN ], being initialized with the last state from the premise (hL). The NOATTENTION system then computes the final state u based on the last states from the premise and the hypothesis as follows:\nu = tanh(WpuhL + W huhN + b u) (32)\nwhere Wpu,Whu \u2208 RD\u00d7D and bu \u2208 RD. Finally, it predicts a label y\u0302 fromuwith a standard softmax layer. The SOFTATTENTION system, instead of using the last premise state hL, computes a weighted average of premise words with an attention mechanism, replacing Eq. 32 by\nzt = v >tanh(Wpmht + W hmhN + b m) (33)\np = softmax(z), where z := (z1, . . . , zL) (34) r = H1:Lp (35) u = tanh(Wpur + WhuhN + b u), (36)\nwhere Wpm,Whm \u2208 RD\u00d7D and bm,v \u2208 RD. The LOGISTICATTENTION system, instead of Eq. 34, computes p = (\u03c3(z1), . . . , \u03c3(zL)). Finally, the SPARSEATTENTION system replaces Eq. 34 by p = sparsemax(z).\nWe optimized all the systems with Adam (Kingma & Ba, 2014), using the default parameters \u03b21 = 0.9, \u03b22 = 0.999, and = 10\u22128, and setting the learning rate to 3 \u00d7 10\u22124. We tuned a `2-regularization coefficient in {0, 10\u22124, 3 \u00d7 10\u22124, 10\u22123} and, as Rockta\u0308schel et al. (2015), a dropout probability of 0.1 in the inputs and outputs of the network.\nThe results are shown in Table 3. We observe that the soft and sparse-activated attention systems perform similarly, the latter being slightly more accurate on the test set, and that both outperform the NOATTENTION and LOGISTICATTENTION systems.10\nTable 4 shows examples of sentence pairs, highlighting the premise words selected by the SPARSEATTENTION mech-\n10Rockta\u0308schel et al. (2015) report scores slightly above ours: they reached a test accuracy of 82.3% for their implementation of SOFTATTENTION, and 83.5% with their best system, a more elaborate word-by-word attention model. Differences may be due to distinct word vectors and the use of LSTMs instead of GRUs.\nTable 4. Examples of sparse attention for the natural language inference task. Nonzero attention coefficients are marked in bold. Our system classified all four examples correctly. The examples were picked from Rockta\u0308schel et al. (2015).\nA boy rides on a camel in a crowded area while talking on his cellphone. Hypothesis: A boy is riding an animal. [entailment]\nA young girl wearing a pink coat plays with a yellow toy golf club. Hypothesis: A girl is wearing a blue jacket. [contradiction]\nTwo black dogs are frolicking around the grass together. Hypothesis: Two dogs swim in the lake. [contradiction]\nA man wearing a yellow striped shirt laughs while seated next to another man who is wearing a light blue shirt and clasping his hands together. Hypothesis: Two mimes sit in complete silence. [contradiction]\nanism. We can see that, for all examples, only a small number of words are selected, which are key to making the final decision. Compared to a softmax-activated mechanism, which provides a dense distribution over all the words, the sparsemax activation yields a compact and more interpretable selection, which can be particularly useful in long sentences such as the one in the bottom row."}, {"heading": "5. Conclusions", "text": "We introduced the sparsemax transformation, which has similar properties to the traditional softmax, but is able to output sparse probability distributions. We derived a closed-form expression for its Jacobian, needed for the backpropagation algorithm, and we proposed a novel \u201csparsemax loss\u201d function, a sparse analogue of the logistic loss, which is smooth and convex. Empirical results in multi-label classification and in attention networks for natural language inference attest the validity of our approach.\nThe connection between sparse modeling and interpretability is key in signal processing (Hastie et al., 2015). Our approach is distinctive: it is not the model that is assumed sparse, but the label posteriors that the model parametrizes. Sparsity is also a desirable (and biologically plausible) property in neural networks, present in rectified units (Glorot et al., 2011) and maxout nets (Goodfellow et al., 2013).\nThere are several avenues for future research. The ability of sparsemax-activated attention to select only a few variables to attend makes it potentially relevant to neural architectures with random access memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015), since it offers a compromise between soft and hard operations, maintaining differentiability. In fact, \u201charder\u201d forms of at-\ntention are often useful, arising as word alignments in machine translation pipelines, or latent variables as in Xu et al. (2015). Sparsemax is also appealing for hierarchical attention: if we define a top-down product of distributions along the hierarchy, the sparse distributions produced by sparsemax will automatically prune the hierarchy, leading to computational savings. A possible disadvantage of sparsemax over softmax is that it seems less GPU-friendly, requiring sort operations or linear-selection algorithms. There is, however, recent work providing efficient implementations of these algorithms on GPUs (Alabi et al., 2012)."}, {"heading": "Acknowledgements", "text": "We would like to thank Tim Rockta\u0308schel for answering various implementation questions, and Ma\u0301rio Figueiredo and Chris Dyer for helpful comments on a draft of this report. This work was partially supported by Fundac\u0327a\u0303o para a Cie\u0302ncia e Tecnologia (FCT), through contracts UID/EEA/50008/2013 and UID/CEC/50021/2013."}, {"heading": "A. Proofs", "text": "A.1. Proof of Prop. 1\nThe Lagrangian of the optimization problem in Eq. 2 is:\nL(z,\u00b5, \u03c4) = 1 2 \u2016p\u2212 z\u20162 \u2212 \u00b5>p+ \u03c4(1>p\u2212 1). (37)\nThe optimal (p\u2217,\u00b5\u2217, \u03c4\u2217) must satisfy the following Karush-Kuhn-Tucker conditions:\np\u2217 \u2212 z \u2212 \u00b5\u2217 + \u03c4\u22171 = 0, (38) 1>p\u2217 = 1, p\u2217 \u2265 0, \u00b5\u2217 \u2265 0, (39)\n\u00b5\u2217i p \u2217 i = 0, \u2200i \u2208 [K]. (40)\nIf for i \u2208 [K] we have p\u2217i > 0, then from Eq. 40 we must have \u00b5\u2217i = 0, which from Eq. 38 implies p\u2217i = zi \u2212 \u03c4\u2217. Let S(z) = {j \u2208 [K] | p\u2217j > 0}. From Eq. 39 we obtain \u2211 j\u2208S(z)(zj \u2212 \u03c4\u2217) = 1, which yields the right hand side of Eq. 4. Again from Eq. 40, we have that \u00b5\u2217i > 0 implies p \u2217 i = 0, which from Eq. 38 implies \u00b5 \u2217 i = \u03c4\n\u2217 \u2212 zi \u2265 0, i.e., zi \u2264 \u03c4\u2217 for i /\u2208 S(z). Therefore we have that k(z) = |S(z)|, which proves the first equality of Eq. 4.\nA.2. Proof of Prop. 2\nWe start with the third property, which follows from the coordinate-symmetry in the definitions in Eqs. 1\u20132. The same argument can be used to prove the first part of the first property (uniform distribution).\nLet us turn to the second part of the first property (peaked distribution on the largest components of z), and define t = \u22121. For the softmax case, this follows from\nlim t\u2192+\u221e etzi\u2211 k e tzk = lim t\u2192+\u221e etzi\u2211 k\u2208A(z) e tzk = lim t\u2192+\u221e et(zi\u2212z(1)) |A(z)| = { 1/|A(z)|, if i \u2208 A(z) 0, otherwise. (41)\nFor the sparsemax case, we invoke Eq. 4 and the fact that k(tz) = |A(z)| if \u03b3(tz) \u2265 1/|A(z)|. Since \u03b3(tz) = t\u03b3(z), the result follows.\nThe second property holds for softmax, since (ezi+c)/ \u2211 k e zk+c = ezi/ \u2211 k e\nzk ; and for sparsemax, since for any p \u2208 \u2206K\u22121 we have \u2016p \u2212 z \u2212 c1\u20162 = \u2016p \u2212 z\u20162 \u2212 2c1>(p\u2212 z) + \u2016c1\u20162, which equals \u2016p \u2212 z\u20162 plus a constant (because 1>p = 1).\nFinally, let us turn to fourth property. The first inequality states that zi \u2264 zj \u21d2 \u03c1i(z) \u2264 \u03c1j(z) (i.e., coordinate monotonicity). For the softmax case, this follows trivially from the fact that the exponential function is increasing. For the sparsemax, we use a proof by contradiction. Suppose zi \u2264 zj and sparsemaxi(z) > sparsemaxj(z). From the definition in Eq. 2, we must have \u2016p\u2212 z\u20162 \u2265 \u2016sparsemax(z)\u2212 z\u20162, for any p \u2208 \u2206K\u22121. This leads to a contradiction if we choose pk = sparsemaxk(z) for k /\u2208 {i, j}, pi = sparsemaxj(z), and pj = sparsemaxi(z). To prove the second inequality in the fourth property for softmax, we need to show that, with zi \u2264 zj , we have (ezj \u2212 ezi)/ \u2211 k e\nzk \u2264 (zj \u2212 zi)/2. Since \u2211 k e\nzk \u2265 ezj + ezi , it suffices to consider the binary case, i.e., we need to prove that tanh((zj \u2212 zi)/2) = (ezj \u2212 ezi)/(ezj + ezi) \u2264 (zj \u2212 zi)/2, that is, tanh(t) \u2264 t for t \u2265 0. This comes from tanh(0) = 0 and tanh\u2032(t) = 1 \u2212 tanh2(t) \u2264 1. For sparsemax, given two coordinates i, j, three things can happen: (i) both are thresholded, in which case \u03c1j(z)\u2212 \u03c1i(z) = zj \u2212 zi; (ii) the smaller (zi) is truncated, in which case \u03c1j(z)\u2212 \u03c1i(z) = zj \u2212 \u03c4(z) \u2264 zj \u2212 zi; (iii) both are truncated, in which case \u03c1j(z)\u2212 \u03c1i(z) = 0 \u2264 zj \u2212 zi.\nA.3. Proof of Prop. 3\nTo prove the first claim, note that, for j \u2208 S(z),\n\u2202\u03c42(z)\n\u2202zj = 2\u03c4(z)\n\u2202\u03c4(z)\n\u2202zj =\n2\u03c4(z) |S(z)| , (42)\nwhere we used Eq. 10. We then have\n\u2202Lsparsemax(z; k)\n\u2202zj = { \u2212\u03b4k(j) + zj \u2212 \u03c4(z) if j \u2208 S(z) \u2212\u03b4k(j) otherwise.\nThat is,\u2207zLsparsemax(z; k) = \u2212\u03b4k + sparsemax(z).\nTo prove the second statement, from the expression for the Jacobian in Eq. 11, we have that the Hessian of Lsparsemax (strictly speaking, a \u201csub-Hessian\u201d (Penot, 2014), since the loss is not twice-differentiable everywhere) is given by\n\u22022Lsparsemax(z; k)\n\u2202xi\u2202xj = { \u03b4ij \u2212 1|S(z)| if i, j \u2208 S(z) 0 otherwise.\n(43)\nThis Hessian can be written in the form Id\u2212 11>/|S(z)| up to padding zeros (for the coordinates not in S(z)); hence it is positive semi-definite (with rank |S(z)| \u2212 1), which establishes the convexity of Lsparsemax.\nFor the third claim, we have Lsparsemax(z + c1) = \u2212zk \u2212 c + 12 \u2211 j\u2208S(z)(z 2 j \u2212 \u03c42(z) + 2c(zj \u2212 \u03c4)) + 12 = \u2212zk \u2212 c + 1 2 \u2211 j\u2208S(z)(z 2 j \u2212 \u03c42(z) + 2cpj) + 12 = Lsparsemax(z), since \u2211 j\u2208S(z) pj = 1.\nFrom the first two claims, we have that the minima of Lsparsemax have zero gradient, i.e., satisfy the equation sparsemax(z) = \u03b4k. Furthemore, from Prop. 2, we have that the sparsemax never increases the distance between two coordinates, i.e., sparsemaxk(z)\u2212 sparsemaxj(z) \u2264 zk\u2212zj . Therefore sparsemax(z) = \u03b4k implies zk \u2265 1+maxj 6=k zj . To prove the converse statement, note that the distance above can only be decreased if the smallest coordinate is truncated to zero. This establishes the equivalence between (ii) and (iii) in the fifth claim. Finally, we have that the minimum loss value is achieved when S(z) = {k}, in which case \u03c4(z) = zk \u2212 1, leading to\nLsparsemax(z; k) = \u2212zk + 1\n2 (z2k \u2212 (zk \u2212 1)2) +\n1 2 = 0. (44)\nThis proves the equivalence with (i) and also the fourth claim."}], "references": [{"title": "Learning with a probabilistic teacher", "author": ["Agrawala", "Ashok K"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Agrawala and K.,? \\Q1970\\E", "shortCiteRegEx": "Agrawala and K.", "year": 1970}, {"title": "Fast k-selection algorithms for graphics processing units", "author": ["Alabi", "Tolu", "Blanchard", "Jeffrey D", "Gordon", "Bradley", "Steinbach", "Russel"], "venue": "Journal of Experimental Algorithmics (JEA),", "citeRegEx": "Alabi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Alabi et al\\.", "year": 2012}, {"title": "Bayesian analysis of binary and polychotomous response data", "author": ["Albert", "James H", "Chib", "Siddhartha"], "venue": "Journal of the American statistical Association,", "citeRegEx": "Albert et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Albert et al\\.", "year": 1993}, {"title": "Learning word representations from scarce and noisy data with embedding sub-spaces", "author": ["Astudillo", "Ramon F", "Amir", "Silvio", "Lin", "Wang", "Silva", "M\u00e1rio", "Trancoso", "Isabel"], "venue": "In Proc. of the Association for Computational Linguistics (ACL), Beijing, China,", "citeRegEx": "Astudillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Astudillo et al\\.", "year": 2015}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Time bounds for selection", "author": ["Blum", "Manuel", "Floyd", "Robert W", "Pratt", "Vaughan", "Rivest", "Ronald L", "Tarjan", "Robert E"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Blum et al\\.,? \\Q1973\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1973}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Bowman", "Samuel R", "Angeli", "Gabor", "Potts", "Christopher", "Manning", "Christopher D"], "venue": "In Proc. of Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Rank analysis of incomplete block designs: The method of paired comparisons", "author": ["Bradley", "Ralph Allan", "Terry", "Milton E"], "venue": null, "citeRegEx": "Bradley et al\\.,? \\Q1952\\E", "shortCiteRegEx": "Bradley et al\\.", "year": 1952}, {"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["Bridle", "John S"], "venue": "In Neurocomputing,", "citeRegEx": "Bridle and S.,? \\Q1990\\E", "shortCiteRegEx": "Bridle and S.", "year": 1990}, {"title": "Semi-Supervised Learning", "author": ["Chapelle", "Olivier", "Sch\u00f6lkopf", "Bernhard", "Zien", "Alexander"], "venue": null, "citeRegEx": "Chapelle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2006}, {"title": "Attention-based models for speech recognition", "author": ["Chorowski", "Jan K", "Bahdanau", "Dzmitry", "Serdyuk", "Dmitriy", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chorowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2015}, {"title": "Optimization and Nonsmooth Analysis", "author": ["Clarke", "Frank H"], "venue": null, "citeRegEx": "Clarke and H.,? \\Q1983\\E", "shortCiteRegEx": "Clarke and H.", "year": 1983}, {"title": "An exploration of softmax alternatives belonging to the spherical loss family", "author": ["de Br\u00e9bisson", "Alexandre", "Vincent", "Pascal"], "venue": "arXiv preprint arXiv:1511.05042,", "citeRegEx": "Br\u00e9bisson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Br\u00e9bisson et al\\.", "year": 2015}, {"title": "Efficient projections onto the L1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In Proc. of International Conference of Machine Learning,", "citeRegEx": "Duchi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2008}, {"title": "On the consistency of multilabel learning", "author": ["Gao", "Wei", "Zhou", "Zhi-Hua"], "venue": "Artificial Intelligence,", "citeRegEx": "Gao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2013}, {"title": "Deep Sparse Rectifier Neural Networks", "author": ["Glorot", "Xavier", "Bordes", "Antoine", "Bengio", "Yoshua"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Deep learning. Book in preparation for MIT Press, 2016", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": "URL http://goodfeli.github.io/dlbook/", "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Learning to Transduce with Unbounded Memory", "author": ["Grefenstette", "Edward", "Hermann", "Karl Moritz", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Statistical learning with sparsity: the lasso and generalizations", "author": ["Hastie", "Trevor", "Tibshirani", "Robert", "Wainwright", "Martin"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2015}, {"title": "Teaching Machines to Read and Comprehend", "author": ["Hermann", "Karl Moritz", "Kocisky", "Tomas", "Grefenstette", "Edward", "Espeholt", "Lasse", "Kay", "Will", "Suleyman", "Mustafa", "Blunsom", "Phil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Robust estimation of a location parameter", "author": ["Huber", "Peter J"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Huber and J.,? \\Q1964\\E", "shortCiteRegEx": "Huber and J.", "year": 1964}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "In Proc. of International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Consistent multilabel classification", "author": ["Koyejo", "Sanmi", "Natarajan", "Nagarajan", "Ravikumar", "Pradeep K", "Dhillon", "Inderjit S"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koyejo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koyejo et al\\.", "year": 2015}, {"title": "RCV1: a new benchmark collection for text categorization research", "author": ["Lewis", "David D", "Yang", "Yiming", "Rose", "Tony G", "Li", "Fan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["Liu", "Dong C", "Nocedal", "Jorge"], "venue": "Mathematical programming,", "citeRegEx": "Liu et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1989}, {"title": "Generalized Linear Models, volume 37", "author": ["McCullagh", "Peter", "Nelder", "John A"], "venue": "CRC press,", "citeRegEx": "McCullagh et al\\.,? \\Q1989\\E", "shortCiteRegEx": "McCullagh et al\\.", "year": 1989}, {"title": "A bradley\u2013terry artificial neural network model for individual ratings in group competitions", "author": ["Menke", "Joshua E", "Martinez", "Tony R"], "venue": "Neural computing and Applications,", "citeRegEx": "Menke et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Menke et al\\.", "year": 2008}, {"title": "A finite algorithm for finding the projection of a point onto the canonical simplex of R", "author": ["C. Michelot"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Michelot,? \\Q1986\\E", "shortCiteRegEx": "Michelot", "year": 1986}, {"title": "A method of solving a convex programming problem with convergence rate O(1/k)", "author": ["Y. Nesterov"], "venue": "Soviet Math. Doklady,", "citeRegEx": "Nesterov,? \\Q1983\\E", "shortCiteRegEx": "Nesterov", "year": 1983}, {"title": "Riemannian metrics for neural networks", "author": ["Ollivier", "Yann"], "venue": "arXiv preprint arXiv:1303.0818,", "citeRegEx": "Ollivier and Yann.,? \\Q2013\\E", "shortCiteRegEx": "Ollivier and Yann.", "year": 2013}, {"title": "An algorithm for a singly constrained class of quadratic programs subject to upper and lower bounds", "author": ["Pardalos", "Panos M", "Kovoor", "Naina"], "venue": "Mathematical Programming,", "citeRegEx": "Pardalos et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Pardalos et al\\.", "year": 1990}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Jeffrey", "Socher", "Richard", "Manning", "Christopher D"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Conciliating generalized derivatives", "author": ["Penot", "Jean-Paul"], "venue": "Constructive Nonsmooth Analysis and Related Topics,", "citeRegEx": "Penot and Jean.Paul.,? \\Q2014\\E", "shortCiteRegEx": "Penot and Jean.Paul.", "year": 2014}, {"title": "Reasoning about Entailment with Neural Attention", "author": ["Rockt\u00e4schel", "Tim", "Grefenstette", "Edward", "Hermann", "Karl Moritz", "Ko\u010disk\u1ef3", "Tom\u00e1\u0161", "Blunsom", "Phil"], "venue": "arXiv preprint arXiv:1509.06664,", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Rush", "Alexander M", "Chopra", "Sumit", "Weston", "Jason"], "venue": "Proc. of Empirical Methods in Natural Language Processing,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "End-to-End Memory Networks", "author": ["Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Weston", "Jason", "Fergus", "Rob"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Efficient exact gradient update for training deep networks with very large sparse targets", "author": ["Vincent", "Pascal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vincent and Pascal.,? \\Q2015\\E", "shortCiteRegEx": "Vincent and Pascal.", "year": 2015}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Reducing multiclass to binary by coupling probability estimates", "author": ["Zadrozny", "Bianca"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zadrozny and Bianca.,? \\Q2001\\E", "shortCiteRegEx": "Zadrozny and Bianca.", "year": 2001}, {"title": "A review on multilabel learning algorithms. Knowledge and Data Engineering", "author": ["Zhang", "Min-Ling", "Zhou", "Zhi-Hua"], "venue": "IEEE Transactions on,", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Statistical behavior and consistency of classification methods based on convex risk minimization", "author": ["Zhang", "Tong"], "venue": "Annals of Statistics,", "citeRegEx": "Zhang and Tong.,? \\Q2004\\E", "shortCiteRegEx": "Zhang and Tong.", "year": 2004}, {"title": "The margin vector, admissible loss and multi-class marginbased classifiers", "author": ["Zou", "Hui", "Zhu", "Ji", "Hastie", "Trevor"], "venue": "Technical report,", "citeRegEx": "Zou et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 16, "context": "The softmax transformation is a key component of several statistical learning models, encompassing multinomial logistic regression (McCullagh & Nelder, 1989), action selection in reinforcement learning (Sutton & Barto, 1998), and neural networks for multi-class classification (Bridle, 1990; Goodfellow et al., 2016).", "startOffset": 277, "endOffset": 316}, {"referenceID": 4, "context": "Recently, it has also been used to design attention mechanisms in neural networks, with important achievements in machine translation (Bahdanau et al., 2015), image caption generation (Xu et al.", "startOffset": 134, "endOffset": 157}, {"referenceID": 38, "context": ", 2015), image caption generation (Xu et al., 2015), speech recognition (Chorowski et al.", "startOffset": 34, "endOffset": 51}, {"referenceID": 10, "context": ", 2015), speech recognition (Chorowski et al., 2015), memory networks (Sukhbaatar et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 35, "context": ", 2015), memory networks (Sukhbaatar et al., 2015), and various tasks in natural language understanding (Hermann et al.", "startOffset": 25, "endOffset": 50}, {"referenceID": 19, "context": ", 2015), and various tasks in natural language understanding (Hermann et al., 2015; Rockt\u00e4schel et al., 2015; Rush et al., 2015) and computation learning (Graves et al.", "startOffset": 61, "endOffset": 128}, {"referenceID": 33, "context": ", 2015), and various tasks in natural language understanding (Hermann et al., 2015; Rockt\u00e4schel et al., 2015; Rush et al., 2015) and computation learning (Graves et al.", "startOffset": 61, "endOffset": 128}, {"referenceID": 34, "context": ", 2015), and various tasks in natural language understanding (Hermann et al., 2015; Rockt\u00e4schel et al., 2015; Rush et al., 2015) and computation learning (Graves et al.", "startOffset": 61, "endOffset": 128}, {"referenceID": 17, "context": ", 2015) and computation learning (Graves et al., 2014; Grefenstette et al., 2015).", "startOffset": 33, "endOffset": 81}, {"referenceID": 27, "context": "Projecting onto the simplex is a well studied problem, for which linear-time algorithms are available (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008).", "startOffset": 102, "endOffset": 163}, {"referenceID": 13, "context": "Projecting onto the simplex is a well studied problem, for which linear-time algorithms are available (Michelot, 1986; Pardalos & Kovoor, 1990; Duchi et al., 2008).", "startOffset": 102, "endOffset": 163}, {"referenceID": 5, "context": "More elaborate O(K) algorithms exist based on linear-time selection (Blum et al., 1973; Pardalos & Kovoor, 1990).", "startOffset": 68, "endOffset": 112}, {"referenceID": 28, "context": "However, unlike the hinge loss, Lsparsemax is everywhere differentiable, hence amenable to smooth optimization methods such as L-BFGS or accelerated gradient descent (Liu & Nocedal, 1989; Nesterov, 1983).", "startOffset": 166, "endOffset": 203}, {"referenceID": 42, "context": "This loss is a variant of the Huber loss adapted for classification, and has been called \u201cmodified Huber loss\u201d by Zhang (2004); Zou et al. (2006).", "startOffset": 128, "endOffset": 146}, {"referenceID": 9, "context": "This scenario is also relevant for \u201clearning with a probabilistic teacher\u201d (Agrawala, 1970) and semi-supervised learning (Chapelle et al., 2006), as it can model label uncertainty.", "startOffset": 121, "endOffset": 144}, {"referenceID": 22, "context": "Next, we ran experiments in five benchmark multi-label classification datasets: the four small-scale datasets used by Koyejo et al. (2015),7 and the much larger Reuters RCV1 Note that, with uniform mixtures, the problem becomes essentially multi-label classification.", "startOffset": 118, "endOffset": 139}, {"referenceID": 22, "context": "Recent work has investigated the consistency of multi-label classifiers for various micro and macro-averaged metrics (Gao & Zhou, 2013; Koyejo et al., 2015), among which a plug-in classifier that trains independent binary logistic regressors on each label, and then tunes a probability threshold \u03b4 \u2208 [0, 1] on validation data.", "startOffset": 117, "endOffset": 156}, {"referenceID": 22, "context": "v2 dataset of Lewis et al. (2004).8 For all datasets, we removed examples without labels (i.", "startOffset": 14, "endOffset": 34}, {"referenceID": 6, "context": "0 corpus (Bowman et al., 2015), a collection of 570,000 human-written English sentence pairs.", "startOffset": 9, "endOffset": 30}, {"referenceID": 32, "context": "4, is the same as the one proposed by Rockt\u00e4schel et al. (2015). We compare the performance of four systems: NOATTENTION, a (gated) RNN-based system similar to Bowman et al.", "startOffset": 38, "endOffset": 64}, {"referenceID": 6, "context": "We compare the performance of four systems: NOATTENTION, a (gated) RNN-based system similar to Bowman et al. (2015); LOGISTICATTENTION, an attention-based system with independent logistic activations; SOFTATTENTION, a near-reproduction of the Rockt\u00e4schel et al.", "startOffset": 95, "endOffset": 116}, {"referenceID": 6, "context": "We compare the performance of four systems: NOATTENTION, a (gated) RNN-based system similar to Bowman et al. (2015); LOGISTICATTENTION, an attention-based system with independent logistic activations; SOFTATTENTION, a near-reproduction of the Rockt\u00e4schel et al. (2015)\u2019s attention-based system; and SPARSEATTENTION, which replaces the latter softmax-activated attention mechanism by a sparsemax activation.", "startOffset": 95, "endOffset": 269}, {"referenceID": 31, "context": "We represent the words in the premise and in the hypothesis with 300-dimensional GloVe vectors (Pennington et al., 2014), not optimized during training, which we linearly project onto a D-dimensional subspace (Astudillo et al.", "startOffset": 95, "endOffset": 120}, {"referenceID": 33, "context": "Instead of long short-term memories, as Rockt\u00e4schel et al. (2015), we used gated recurrent units (GRUs, Cho et al.", "startOffset": 40, "endOffset": 66}, {"referenceID": 33, "context": "We tuned a `2-regularization coefficient in {0, 10\u22124, 3 \u00d7 10\u22124, 10\u22123} and, as Rockt\u00e4schel et al. (2015), a dropout probability of 0.", "startOffset": 78, "endOffset": 104}, {"referenceID": 33, "context": "10 Table 4 shows examples of sentence pairs, highlighting the premise words selected by the SPARSEATTENTION mechRockt\u00e4schel et al. (2015) report scores slightly above ours: they reached a test accuracy of 82.", "startOffset": 112, "endOffset": 138}, {"referenceID": 33, "context": "10 Table 4 shows examples of sentence pairs, highlighting the premise words selected by the SPARSEATTENTION mechRockt\u00e4schel et al. (2015) report scores slightly above ours: they reached a test accuracy of 82.3% for their implementation of SOFTATTENTION, and 83.5% with their best system, a more elaborate word-by-word attention model. Differences may be due to distinct word vectors and the use of LSTMs instead of GRUs. Table 4. Examples of sparse attention for the natural language inference task. Nonzero attention coefficients are marked in bold. Our system classified all four examples correctly. The examples were picked from Rockt\u00e4schel et al. (2015).", "startOffset": 112, "endOffset": 658}, {"referenceID": 18, "context": "The connection between sparse modeling and interpretability is key in signal processing (Hastie et al., 2015).", "startOffset": 88, "endOffset": 109}, {"referenceID": 15, "context": "Sparsity is also a desirable (and biologically plausible) property in neural networks, present in rectified units (Glorot et al., 2011) and maxout nets (Goodfellow et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 17, "context": "The ability of sparsemax-activated attention to select only a few variables to attend makes it potentially relevant to neural architectures with random access memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015), since it offers a compromise between soft and hard operations, maintaining differentiability.", "startOffset": 166, "endOffset": 239}, {"referenceID": 35, "context": "The ability of sparsemax-activated attention to select only a few variables to attend makes it potentially relevant to neural architectures with random access memory (Graves et al., 2014; Grefenstette et al., 2015; Sukhbaatar et al., 2015), since it offers a compromise between soft and hard operations, maintaining differentiability.", "startOffset": 166, "endOffset": 239}, {"referenceID": 1, "context": "There is, however, recent work providing efficient implementations of these algorithms on GPUs (Alabi et al., 2012).", "startOffset": 95, "endOffset": 115}, {"referenceID": 37, "context": "tention are often useful, arising as word alignments in machine translation pipelines, or latent variables as in Xu et al. (2015). Sparsemax is also appealing for hierarchical attention: if we define a top-down product of distributions along the hierarchy, the sparse distributions produced by sparsemax will automatically prune the hierarchy, leading to computational savings.", "startOffset": 113, "endOffset": 130}], "year": 2017, "abstractText": "We propose sparsemax, a new activation function similar to the traditional softmax, but able to output sparse probabilities. After deriving its properties, we show how its Jacobian can be efficiently computed, enabling its use in a network trained with backpropagation. Then, we propose a new smooth and convex loss function which is the sparsemax analogue of the logistic loss. We reveal an unexpected connection between this new loss and the Huber classification loss. We obtain promising empirical results in multi-label classification problems and in attention-based neural networks for natural language inference. For the latter, we achieve a similar performance as the traditional softmax, but with a selective, more compact, attention focus.", "creator": "LaTeX with hyperref package"}}}