{"id": "1602.02215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2016", "title": "Swivel: Improving Embeddings by Noticing What's Missing", "abstract": "above we present submatrix - wise vector embedding learner ( swivel ), implementing a primitive method for steadily generating naturally low - dimensional homogeneous feature embeddings from across a feature analysis co - occurrence approximation matrix. swivel regularly performs approximate independent factorization of only the point - wise adaptive mutual data information finding matrix used via stochastic type gradient reverse descent. it sometimes uses a piecewise loss kernel with special handling for unobserved co - occurrences, and thus makes use of turing all and the stored information in the chosen matrix. earlier while this methodology requires straightforward computation having proportional amounts to the size component of the entire matrix, again we make special use of vectorized multiplication to process thousands of assumed rows frequently and columns randomly at once to compute millions of predicted values. furthermore, we partition the initial matrix samples into mutually shards spaces in hardware order to parallelize using the exact computation across rather many nodes. this approach only results in more formally accurate embeddings than can be efficiently achieved with methods elsewhere that consider only accurately observed unexpected co - occurrences, and can instead scale to much earlier larger probability corpora scale than can commonly be handled with sampling query methods.", "histories": [["v1", "Sat, 6 Feb 2016 04:39:41 GMT  (522kb,D)", "http://arxiv.org/abs/1602.02215v1", "9 pages, 4 figures"]], "COMMENTS": "9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["noam shazeer", "ryan doherty", "colin evans", "chris waterson"], "accepted": false, "id": "1602.02215"}, "pdf": {"name": "1602.02215.pdf", "metadata": {"source": "CRF", "title": "Swivel: Improving Embeddings by Noticing What\u2019s Missing", "authors": ["Noam Shazeer", "Ryan Doherty", "Colin Evans", "Chris Waterson"], "emails": ["NOAM@GOOGLE.COM", "PORTALFIRE@GOOGLE.COM", "COLINHEVANS@GOOGLE.COM", "WATERSON@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "Dense vector representations of words have proven to be useful for natural language tasks such as determining semantic similarity, parsing, and translation. Recently, work by Mikolov et al. (2013a) and others has inspired an investigation into the construction of word vectors using stochastic gradient descent methods. Models tend to fall into one of two categories: matrix factorization or sampling from a sliding window: Baroni et al. (2014) refers to these as \u201ccount\u201d and \u201cpredict\u201d methods, respectively.\nIn this paper, we present the Submatrix-wise Vector Embedding Learner (Swivel), a \u201ccount-based\u201d method for generating low-dimensional feature embeddings from a cooccurrence matrix. Swivel uses stochastic gradient descent to perform a weighted approximate matrix factorization, ultimately arriving at embeddings that reconstruct the\npoint-wise mutual information (PMI) between each row and column feature. Swivel uses a piecewise loss function to differentiate between observed and unobserved cooccurrences.\nSwivel is designed to work in a distributed environment. The original co-occurrence matrix (which may contain millions of rows and millions of columns) is \u201csharded\u201d into smaller submatrices, each containing thousands of rows and columns. These can be distributed across multiple workers, each of which uses vectorized matrix multiplication to rapidly produce predictions for millions of individual PMI values. This allows the computation to be distributed across a cluster of computers, resulting in an efficient way to learn embeddings.\nThis paper is organized as follows. First, we describe related word embedding work and note how two popular methods are similar to one another in their optimization objective. We then discuss Swivel in detail, and describe experimental results on several standard word embedding evaluation tasks. We conclude with analysis of our results and discussion of the algorithm with regard to the other approaches."}, {"heading": "2. Related Work", "text": "While there are a number of interesting approaches to creating word embeddings, Skipgram Negative Sampling (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) are two relatively recent approaches that have received quite a bit of attention. These methods compress the distributional structure of the raw language co-occurrence statistics, yielding compact representations that retain the properties of the original space. The intrinsic quality of the embeddings can be evaluated in two ways. First, words with similar distributional contexts should be near to one another in the embedding space. Second, manipulating the distributional context directly by adding or removing words ought to lead to similar translations in the embedded space, allowing \u201canalogical\u201d traversal of the vector space. ar X\niv :1\n60 2.\n02 21\n5v 1\n[ cs\n.C L\n] 6\nF eb\n2 01\nSkipgram Negative Sampling. The word2vec program released by Mikolov et al. (2013a) generates word embeddings by sliding a window over a large corpus of text. The \u201cfocus\u201d word in the center of the window is trained to predict each \u201ccontext\u201d word that surrounds it by 1) maximizing the dot product between the sampled words\u2019 embeddings, and 2) minimizing the dot product between the focus word and a randomly sampled non-context word. This method of training is called skipgram negative sampling (SGNS).\nLevy and Goldberg (2014) examine SGNS and suggest that the algorithm is implicitly performing weighted low-rank factorization of a matrix whose cell values are related to the point-wise mutual information between the focus and context words. Point-wise mutual information (PMI) is a measure of association between two events, defined as follows:\npmi(i; j) = log P (i, j)\nP (i)P (j) (1)\nIn the case of language, the frequency statistics of cooccurring words in a corpus can be used to estimate the probabilities that comprise PMI. Let xij be the number of times that focus word i co-occurs with the context word j, xi\u2217 = \u2211 j xij be total number of times that focus word i\nappears in the corpus, x\u2217j = \u2211 i xij be the total number of times that context word j appears appears in the corpus, and |D| = \u2211 i,j xij be the total number of co-occurrences. Then we can re-write (1) as:\npmi(i; j) = log xij |D| xi\u2217 x\u2217j\n= log xij + log |D| \u2212 log xi\u2217 \u2212 log x\u2217j\nIt is important to note that, in the case that xij is zero \u2013 i.e., no co-occurrence of i and j has been observed \u2013 PMI is infinitely negative.\nSGNS can be seen as producing two matrices, W for focus words and W\u0303 for context words, such that their product WW\u0303> approximates the observed PMI between respective word/context pairs. Given a specific focus word i and context word j, SGNS minimizes the magnitude of the difference between w>i w\u0303j and pmi(i; j), tempered by a monotonically increasing weighting function of the observed co-occurrence count, f(xij):\nLSGNS = \u2211 i,j f(xij) ( w>i w\u0303j \u2212 pmi(i; j) )2 = \u2211 i,j f(xij)(w > i w\u0303j \u2212 log xij \u2212 log|D|\n+ log xi\u2217 + log x\u2217j) 2\nDue to the fact that SGNS slides a sampling window through the entire training corpus, a significant drawback of the algorithm is that it requires training time proportional to the size of the corpus.\nGloVe. Pennington et al.\u2019s 2014 GloVe is an approach that instead works from the precomputed corpus co-occurrence statistics. The authors posit several constraints that should lead to preserving the \u201clinear directions of meaning\u201d. Based on ratios of conditional probabilities of words in context, they suggest that a natural model for learning such linear structure should minimize the following cost function for a given focus word i and context word j:\nLGloVe = \u2211 i,j f(xij) ( w>i w\u0303j \u2212 log xij + bi + bj )2 Here, bi and bj are bias terms that are specific to each focus word and each context word, respectively. Again f(xij) is a function that weights the cost according to the frequency of the co-occurrence count xij . Using stochastic gradient descent, GloVe learns the model parameters for W, b, W\u0303, and b\u0303: it selects a pair of words observed to co-occur in the corpus, retrieves the corresponding embedding parameters, computes the loss, and back-propagates the error to update the parameters. GloVe therefore requires training time proportional to the number of observed co-occurrence pairs, allowing it to scale independently of corpus size.\nAlthough GloVe was developed independently from SGNS (and, as far as we know, without knowledge of Levy and Goldberg\u2019s 2014 analysis), it is interesting how similar these two models are.\n\u2022 Both seek to minimize the difference between the model\u2019s estimate and the log of the co-occurrence count. GloVe has additional free \u201cbias\u201d parameters that, in SGNS, are pegged to the corpus frequency of the individual words. Empirically, it can be observed that the bias terms are highly correlated to the frequency of the row and column features in a trained GloVe model.\n\u2022 Both weight the loss according to the frequency\nof the co-occurrence count such that frequent cooccurrences incur greater penalty than rare ones.1\nLevy et al. (2015) note these algorithmic similarities. In their controlled empirical comparison of several different embedding approaches, results produced by SGNS and GloVe differ only modestly.\nThere are subtle differences, however. The negative sampling regime of SGNS ensures that the model does not place features near to one another in the embedding space whose co-occurrence isn\u2019t observed in the corpus. This is distinctly different from GloVe, which trains only on the observed co-occurrence statistics. The GloVe model incurs no penalty for placing features near to one another whose co-occurrence has not been observed. As we shall see in Section 4, this can result in poor estimates for uncommon features."}, {"heading": "3. Swivel", "text": "Swivel is an attempt to have our cake and eat it, too. Like GloVe, it works from co-occurrence statistics rather than by sampling; like SGNS, it makes use of the fact that many co-occurrences are unobserved in the corpus. Like both, Swivel performs a weighted approximate matrix factorization of the PMI between features. Furthermore, Swivel is designed to work well in a distributed environment; e.g., distbelief (Dean et al., 2012).\nAt a high level, Swivel begins with anm\u00d7n co-occurrence matrix between m row and n column features. Each row feature and each column feature is assigned a ddimensional embedding vector. The vectors are grouped into blocks, each of which defines a submatrix \u201cshard\u201d. Training proceeds by selecting a shard (and thus, its corresponding row block and column block), and performing a matrix multiplication of the associated vectors to produce an estimate of the PMI values for each co-occurrence. This is compared with the observed PMI, with special handling for the case where no co-occurrence was observed and the PMI is undefined. Stochastic gradient descent is used to update the individual vectors and minimize the difference.\nAs will be discussed in more detail below, splitting the matrix into shards allows the problem to be distributed across many workers in a way that allows for utilization of highperformance vectorized hardware, amortizes the overhead of transferring model parameters, and distributes parameter updates evenly across the feature embeddings.\n1This latter similarity is reminiscent of weighted alternating least squares (Hu et al., 2008), which treats f(xij) as a confidence estimate that favors accurate estimation of certain parameters over uncertain ones."}, {"heading": "3.1. Construction", "text": "To begin, an m\u00d7n co-occurrence matrix X is constructed, where each cell xij in the matrix contains the observed cooccurrence count of row feature i with column feature j. The marginal counts of each row feature (xi\u2217 = \u2211 j xij)\nand each column feature (x\u2217j = \u2211 i xij) are computed, as well as the overall sum of all the cells in the matrix, |D| = \u2211 i,j xij . As with other embedding methods, Swivel is agnostic to both the domain from which the features are drawn, and to the exact set of features that are used. Furthermore, the \u201cfeature vocabulary\u201d used for the rows need not necessarily be the same as that which is used for the columns.\nThe rows are sorted in descending order of feature frequency, and are then collected into k-element row blocks, where k is chosen based on computational efficiency considerations discussed below. This results in m/k row blocks whose elements are selected by choosing rows that are congruent mod m/k. For example, if there are 225 total rows in the co-occurrence matrix, for k = 4096, every 225/4096 = 8, 192th row is selected to form a row block: the first row block contains rows (0, 8192, 16384, ...), the second row block contains rows (1, 8193, 16385, ...), and so on. Since rows were originally frequency-sorted, this construction results in each row block containing a mix of common and rare row features.\nThe process is repeated for the columns to yield n/k column blocks. As with the row blocks, each column block contains a mix of common and rare column features.\nFor each (row block, column block) pair (i, j), we construct a k \u00d7 k submatrix shard Xij from the original co-occurrence matrix by selecting the appropriate cooccurrence cells:\nXij =  xi,j xi+mk ,j . . . xi+(k\u22121)mk ,j xi,j+nk xi,j+2nk ... . . .\nxi,j+(k\u22121)nk\n\nThis results in mn/k2 shards in all. Typically, the vast majority of these elements are zero. Figure 1 illustrates this process: lighter pixels represent more frequent cooccurrences, which naturally tend to occur for more frequent features."}, {"heading": "3.2. Training", "text": "Prior to training, the two d-dimensional feature embeddings are initialized with small, random values.2 W \u2208 Rm\u00d7d is the matrix of embeddings for the m row features (e.g., words), W\u0303 \u2208 Rn\u00d7d is the matrix of embeddings for the n column features (e.g., word contexts).\nTraining then proceeds iteratively as follows. A submatrix shard Xij is chosen at random, along with the k row embedding vectors Wi \u2208 W from row block i, and the k column vectors W\u0303j \u2208 W\u0303 from column block j. The matrix product WiW\u0303>j is computed to produce k2 predicted PMI values, which are then compared to the observed PMI values for shard Xij . The error between the predicted and actual values is used to compute gradients: these are accumulated for each row and column. Figure 2 illustrates this process. The gradient descent is dampened using Adagrad (Duchi et al., 2011), and the process repeats until the error no longer decreases appreciably.\nAlthough each shard is considered separately, it shares the embedding parametersWi for all other shards in the same row block, and the embedding parameters W\u0303j for all the other shards in the same column block. By storing the parameters in a central parameter server (Dean et al., 2012), it is possible to distribute training by processing several shards in parallel on different worker machines. An individual worker selects a shard, retrieves the appropriate embedding parameters, performs the cost and gradient computation, and then communicates the parameter updates back to the parameter server. We do this in a lock-free fashion (Recht et al., 2011) using Google\u2019s asynchronous stochastic gradient descent infrastructure distbelief (Dean et al., 2012).\nEven on a very fast network, transferring the parameters between the parameter server and a worker machine is expen-\n2We specifically used values drawn from N (0, \u221a d): the choice was arbitrary and we did not investigate the effects of other initialization schemes.\nsive: for each k \u00d7 k block, we must retrieve (and then update) k parameter values each forWi and W\u0303j . Fortunately, this cost is amortized over the k2 individual estimates that are computed by the matrix multiplication. Choosing a reasonable value for k is therefore a balancing act between compute and network throughput: the larger the value of k, the more we amortize the cost of communicating with the parameter server. And up to a point, vectorized matrix multiplication is essentially a constant-time operation on a high-performance GPU. Clearly, this is all heavily dependent on the underlying hardware fabric; we achieved good performance in our environment with k = 4096."}, {"heading": "3.3. Training Objective and Cost Function", "text": "Swivel approximates the observed PMI of row feature i and column feature j with w>i w\u0303j . It uses a piecewise loss function that treats observed and unobserved co-occurrences distinctly. Table 1 summarizes the piecewise cost function, and Figure 3 shows the different loss function variants with respect to w>i w\u0303j for an arbitrary objective value of 2.0.\nObserved co-occurrences. For co-occurrences that have been observed (xij > 0), we\u2019d like w>i wj to accurately estimate pmi(i; j) subject to how confident we are in the observed count xij . Swivel computes the weighted squared error between the embedding dot product and the PMI of feature i and feature j:\nL1(i, j) = 1\n2 f(xij)\n( w>i w\u0303j \u2212 pmi(i; j) )2 = 1\n2 f(xij)(w\n> i w\u0303j \u2212 log xij \u2212 log|D|\n+ log xi\u2217 + log x\u2217j) 2\nThis encourages w>i w\u0303j to correctly estimate the observed\nPMI, as Figure 3 illustrates. The loss is modulated by a monotonically increasing confidence function f(xij): the more frequently a co-occurrence is observed, the more the model is required to accurately approximate pmi(i; j). We experimented with several different variants for f(xij), and discovered that a linear transformation of x1/2ij produced good results.\nUnobserved Co-occurrences. Unfortunately, if feature i and feature j are never observed together, xij = 0, pmi(i; j) = \u2212\u221e, and the squared error cannot be computed.\nWhat would we like the model to do in this case? Treating xij as a sample, we can ask: how significant is it that its observed value is zero? If the two features i and j are rare, their co-occurrence could plausibly have gone unobserved due to the fact that we simply haven\u2019t seen enough data. On the other hand, if features i and j are common, this is less likely: it becomes significant that a co-occurrence hasn\u2019t been observed, so perhaps we ought to consider that the features are truly anti-correlated. In either case, we certainly don\u2019t want the model to over-estimate the PMI between features, and so we can encourage the model to respect an upper bound on its PMI estimate w>i wj .\nWe address this by smoothing the PMI value as if a single co-occurrence had been observed (i.e., computing PMI as if xij = 1), and using an asymmetric cost function that penalizes over-estimation of the smoothed PMI. The following \u201csoft hinge\u201d cost function (plotted as the dotted line in Figure 3) accomplishes this:\nL0(i, j) = log [1 + exp(w>i w\u0303j \u2212 pmi\u2217(i; j))] = log [1 + exp(w>i w\u0303j \u2212 log|D|\n+ log xi\u2217 + log x\u2217j)]\nHere, pmi\u2217 refers to the smoothed PMI computation where xij\u2019s actual count of 0 is replaced with 1. This loss penalizes the model for over-estimating the objective value; however, it applies negligible penalty \u2013 i.e., is noncommittal \u2013 if the model under-estimates it.\nNumerically, L0 behaves as follows. If features i and j are common, the marginal terms xi\u2217 and x\u2217j are large. In order to minimize the loss, the model must produce a small \u2013 or even negative \u2013 value for w>i w\u0303j , thus capturing the anticorrelation between features i and j. On the other hand, if features i and j are rare, then the marginal terms are also small, so the model is allowed much more latitude with respect to w>i w\u0303j before incurring serious penalty. In this way, the \u201csoft hinge\u201d loss enforces an upper bound on the model\u2019s estimate for pmi(i; j) that reflects the our confidence in the unobserved co-occurrence."}, {"heading": "4. Experiments", "text": "We performed several experiments to evaluate the embeddings produced by Swivel.\nCorpora. Following Pennington et al. (2014), we pro-\nduced 300-dimensional embeddings from an August 2015 Wikipedia dump combined with the Gigaword5 corpus. The corpus was tokenized, lowercased, and split into sentences. Punctuation was discarded, but hyphenated words and contractions were preserved. The resulting training data included 3.3 billion tokens across 89 million sentences. The most frequent 397,312 unigram tokens were used to produce the vocabulary, and the same vocabulary is used for all experiments.\nBaselines. In order to ensure a careful comparison, we recreated embeddings using these corpora with the publiclyavailable word2vec3 and GloVe4 programs as our baselines.\nword2vec was configured to generate skipgram embeddings using five negative samples. We set the window size so that it would include ten tokens to the left of the focus and ten tokens to the right. We ran it for five iterations over the input corpus. Since the least frequent word in the corpus occurs 65 times, training samples the rarest words at least 300 times each. Since the same vocabulary is used for both word and context features, we modified word2vec to emit both word and context embeddings. We experimented with adding word vector wi with its corresponding context vector w\u0303i (Pennington et al., 2014); however, best performance was achieved using the word vectorwi alone, as was originally reported by Mikolov et al. (2013a).\nGloVe was similarly configured to use its \u201csymmetric\u201d cooccurrence window that spans ten tokens to the left of the focus word and ten tokens to the right. We ran GloVe for 100 training epochs using the default parameter settings for initial learning rate (\u03b7 = 0.05), the weighting exponent (\u03b1 = 0.75), and the weighting function cut-off (xmax = 100). GloVe produces both word and context vectors: unlike word2vec, the sum of the word vector wi with its corresponding context vector w\u0303i produced slightly better results than the word vector alone. (This was also noted by Pennington et al. (2014).)\n3https://code.google.com/p/word2vec 4http://nlp.stanford.edu/projects/glove\nOur results for these baselines vary slightly from those reported elsewhere. We speculate that this may be due to differences in corpora, preprocessing, and vocabulary selection, and simply note that this evaluation should at least be internally consistent.\nSwivel Training. The unigram vocabulary was used for both row and column features. Co-occurrence was computed by examining ten words to the left and ten words to the right of the focus word. As with GloVe, co-occurrence counts were accumulated using a harmonically scaled window: for example, a token that is three tokens away from the focus was counted as 13 of an occurrence.\n5 So it turns out that GloVe and Swivel were trained from identical cooccurrence statistics.6\nWe trained the model for a million \u201csteps\u201d, where each step trains an individual submatrix shard. Given a vocabulary size of roughly 400,000 words and k = 4096, there are approximately 100 row blocks and 100 column blocks, yielding 10,000 shards overall. Therefore each shard was sampled about 100 times.\nWe experimented with several different weighting functions to modulate the squared error based on cell frequency of the form f(xij) = b0 + bx\u03b1ij and found that \u03b1 = 1 2 , b0 = 0.1, and b = 14 yielded good results.\nFinally, once the embeddings were produced, we discovered that adding the word vector wi to its corresponding context vector w\u0303i produced better results than the word vector wi alone, just as it did with GloVe.\nEvaluation. We evaluated the embeddings using the same 5We experimented with both linear and uniform scaling windows, and neither performed as well. 6Following Levy and Goldberg\u2019s 2014 suggestion that SGNS factors a shifted PMI matrix, we experimented with shifting the objective PMI value by a small amount. Specifically, Levy and Goldberg (2014) suggest that the SGNS PMI objective is shifted by log k, where k is the number of negative samples drawn from the unigram distribution. Since we\u2019d configured word2vec with k = 5, we experimented with shifting the PMI objective by log 5 (\u223c 1.61). This did not yield significantly different results than just using the original PMI objective.\ndatasets that were used by Levy et al. (2015). For word similarity, we used WordSim353 (Finkelstein et al., 2001) partitioned into WordSim Similarity and WordSim Relatedness (Zesch et al., 2008; Agirre et al., 2009); Bruni et al.\u2019s 2012 MEN dataset; Radinsky et al.\u2019s 2011 Mechanical Turk, Luong et al.\u2019s 2013 Rare Words; and Hill et al.\u2019s 2014 SimLex-999 dataset. These datasets contain word pairs with human-assigned similarity scores: the word vectors are evaluated by ranking the pairs according to their cosine similarities and measuring the correlation with the human ratings using Spearman\u2019s \u03c1. Out-of-vocabulary words are ignored.\nThe analogy tasks present queries of the form \u201cA is to B as C is to X\u201d: the system must predict X from the entire vocabulary. As with Levy et al. (2015), we evaluated Swivel using the MSR and Google datasets (Mikolov et al., 2013b;a). The former contains syntactic analogies (e.g., \u201cgood is to best as smart is to smartest\u201d). The latter contains a mix of syntactic and semantic analogies (e.g., \u201cParis is to France as Tokyo is to Japan\u201d). The evaluation metric is the number of queries for which the embedding that maximizes the cosine similarity is the correct answer. As with (Mikolov et al., 2013a), any query terms are discarded from the result set and out-of-vocabulary words are scored as losses.\nResults. The results are summarized in Table 2. Embeddings produced by word2vec\u2019s CBOW are also included for reference. As can be seen, Swivel outperforms GloVe, SGNS, and CBOW on both the word similarity and analogy tasks. We also note that, except for the Google analogy task, SGNS outperforms GloVe.\nOur hypothesis is that this occurs because both SGNS and Swivel take unobserved co-occurrences into account, but GloVe does not. Swivel incorporates information about unobserved co-occurrences directly, including them in among the predictions and applying the \u201csoft hinge\u201d loss to avoid over-estimating the feature pair\u2019s PMI. SGNS indirectly models unobserved co-occurrences through negative sampling. GloVe, on the other hand, only trains on positive co-occurrence data.\nWe hypothesize that by not taking the unobserved cooccurrences into account, GloVe is under-constrained: there is no penalty for placing unobserved but unrelated embeddings near to one another. Quantitatively, the fact that both SGNS and Swivel out-perform GloVe by a large margin on Luong et al.\u2019s 2013 Rare Words evaluation seems to support this hypothesis. Inspection of some very rare words (Table 3) shows that, indeed, SGNS and Swivel have produced reasonable neighbors, but GloVe has not.\nTo be fair, GloVe was explicitly designed to capture the relative geometry in the embedding space: the intent was to optimize for performance on analogies rather than on word\nsimilarity. Nevertheless, we see that word frequency has a marked effect on analogy performance, as well. Figure 4 plots analogy task accuracy against the base-10 log of the mean frequency of the four words involved.\nTo produce the plot, we considered both the MSR and Google analogies. For each analogy, we computed the mean frequency of the four words involved, and then bucketed it with other analogies that have similar mean frequencies. Each bucket contains at least 100 analogies.\nNotably, Swivel performs better than SGNS at all word frequencies, and better than GloVe on all but the most frequent words. GloVe under-performs SGNS on rare words, but begins to out-perform SGNS as the word frequency increases. We hypothesize that GloVe is fitting the common words at the expense of rare ones.\nIt is also interesting to note that all algorithms tend to perform poorly on the most frequent words. This is probably because very frequent words a) tend to appear in many contexts, making it difficult to determine an accurate point representation, and b) they tend to be polysemous, appear as both verbs and nouns, and have subtle gradations in meaning (e.g., man and time)."}, {"heading": "5. Discussion", "text": "Swivel grew out of a need to build embeddings over larger feature sets and more training data. We wanted an algorithm that could both handle a large amount of data, and produced good estimates for both common and rare fea-\nTable 3. Nearest neighbors for some very rare words.\nQuery Vocabulary Rank SGNS GloVe Swivel\nbootblack 393,709 shoeshiner, newsboy, shoeshine, stage-struck, bartender, bellhop, waiter, housepainter, tinsmith\nredbull, 240, align=middle, 18, 119, dannit, concurrence/dissent, 320px, dannitdannit\nnewsboy, shoeshine, stevedore, bellboy, headwaiter, stowaway, tibbs, mister, tramp\nchigger 373,844 chiggers, webworm, hairballs, noctuid, sweetbread, psyllids, rostratus, narrowleaf, pigweed\ndannit, dannitdannit, upupidae, bungarus, applause., .774, amolops, maxillaria, paralympic.org mite, chiggers, mites, batatas, infestation, jigger, infested, mumbo, frog\u2019s\ndecretal 374,123 decretals, ordinatio, sacerdotalis, constitutiones, theodosianus, canonum, papae, romanae, episcoporum\nregesta, agatho, afl.com.au, dannitdannit, dannit, emptores, beatifications, 18, 545\ndecretals, decretum, apostolicae, sententiae, canonum, unigenitus, collectio, fidei, patristic\ntuxedoes 396,973 tuxedos, ballgowns, tuxes, well-cut, cable-knit, open-collared, organdy, high-collared, flouncy\nhairnets, dhotis, speedos, loincloths, zekrom, shakos, mortarboards, caftans, nightwear\nballgowns, tuxedos, tuxes, cummerbunds, daywear, bridesmaids\u2019, gowns, strapless, flouncy\ntures.\nStatistics vs. Sampling. Like GloVe, Swivel trains from co-occurrence statistics: once the co-occurrence matrix is constructed, training Swivel requires computational resources in proportion to the matrix size. This allows Swivel to handle much more data than can be practically processed with a sampling method like SGNS, which requires training time in proportion to the size of the corpus.\nUnobserved Co-occurrences. Our experiments indicate that GloVe pays a performance cost for only training on observed co-occurrences. In particular, the model may produce unstable estimates for rare features since there is no penalty for placing features near one another whose cooccurrence isn\u2019t observed.\nNevertheless, computing values for every pair of features potentially entails significantly more computation than is required by GloVe, whose training complexity is proportional to the number of non-zero entries in the cooccurrence matrix. Swivel mitigates this in two ways.\nFirst, it makes use of vectorized hardware to perform matrix multiplication of thousands of embedding vectors at once. Performing about a dozen 4096\u00d74096 matrix multiplications per GPU compute unit per second is typical: we have observed that a single GPU can estimate about 200 million cell values per second for 1024-dimensional embedding vectors.\nSecond, the blocked matrix shards can be separately processed by several worker machines to allow for coarsegrained parallelism. The block structure amortizes the overhead of transferring embedding parameters to and from the parameter server across millions of individual estimates. We found that Swivel did, in fact, parallelize easily in our environment, and have been able to run experiments that use hundreds of concurrent worker machines.\nPiecewise Loss. It seems fruitful to consider the cooccurrence matrix as itself containing estimates rather than point values. A corpus is really just a sample of language, and so a co-occurrence matrix derived from a corpus itself\ncontains samples whose values are uncertain.\nWe used a weighted piecewise loss function to capture this uncertainty. If a co-occurrence was observed, we can produce a PMI estimate, and we require the model to fit it more or less accurately based on the observed co-occurrence frequency. If a co-occurrence was not observed, we simply require that the model avoid over-estimating a smoothed PMI value. While this works well, it does seem ad hoc: we hope that future investigation can yield a more principled approach."}, {"heading": "6. Conclusion", "text": "Swivel produces low-dimensional feature embeddings from a co-occurrence matrix. It optimizes an objective that is very similar to that of SGNS and GloVe: the dot product of a word embedding with a context embedding ought to approximate the observed PMI of the two words in the corpus.\nUnlike SGNS, Swivel\u2019s computational requirements depend on the size of the co-occurrence matrix, rather than the size of the corpus. This means that it can be applied to much larger corpora.\nUnlike GloVe, Swivel explicitly considers all the cooccurrence information \u2013 including unobserved cooccurrences \u2013 to produce embeddings. In the case of unobserved co-occurrences, a \u201csoft hinge\u201d loss prevents the model from over-estimating PMI. This leads to demonstrably better embeddings for rare features without sacrificing quality for common ones.\nSwivel capitalizes on vectorized hardware, and uses block structure to amortize parameter transfer cost and avoid contention. This results in the ability to handle very large cooccurrence matrices in a scalable way that is easy to parallelize.\nWe would like to thank Andrew McCallum, Samy Bengio, and Julian Richardson for their thoughtful comments on this work."}], "references": [{"title": "A study on similarity and relatedness using distributional and wordnetbased approaches", "author": ["Eneko Agirre", "Enrique Alfonseca", "Keith Hall", "Jana Kravalova", "Marius Pa\u015fca", "Aitor Soroa"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North", "citeRegEx": "Agirre et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2009}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Simlex999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "arXiv preprint arXiv:1408.3456,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Collaborative filtering for implicit feedback datasets", "author": ["Yifan Hu", "Yehuda Koren", "Chris Volinsky"], "venue": "In Data Mining,", "citeRegEx": "Hu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2008}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levy and Goldberg.,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLT-NAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empirical Methods in Natural Language Processing (EMNLP 2014),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch"], "venue": "In Proceedings of the 20th international conference on World wide web,", "citeRegEx": "Radinsky et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Radinsky et al\\.", "year": 2011}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "Using wiktionary for computing semantic relatedness", "author": ["Torsten Zesch", "Christof M\u00fcller", "Iryna Gurevych"], "venue": "In AAAI,", "citeRegEx": "Zesch et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zesch et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": "Recently, work by Mikolov et al. (2013a) and others has inspired an investigation into the construction of word vectors using stochastic gradient descent methods.", "startOffset": 18, "endOffset": 41}, {"referenceID": 9, "context": "Recently, work by Mikolov et al. (2013a) and others has inspired an investigation into the construction of word vectors using stochastic gradient descent methods. Models tend to fall into one of two categories: matrix factorization or sampling from a sliding window: Baroni et al. (2014) refers to these as \u201ccount\u201d and \u201cpredict\u201d methods, respectively.", "startOffset": 18, "endOffset": 288}, {"referenceID": 11, "context": ", 2013a) and GloVe (Pennington et al., 2014) are two relatively recent approaches that have received quite a bit of attention.", "startOffset": 19, "endOffset": 44}, {"referenceID": 9, "context": "The word2vec program released by Mikolov et al. (2013a) generates word embeddings by sliding a window over a large corpus of text.", "startOffset": 33, "endOffset": 56}, {"referenceID": 1, "context": ", distbelief (Dean et al., 2012).", "startOffset": 13, "endOffset": 32}, {"referenceID": 5, "context": "This latter similarity is reminiscent of weighted alternating least squares (Hu et al., 2008), which treats f(xij) as a confidence estimate that favors accurate estimation of certain parameters over uncertain ones.", "startOffset": 76, "endOffset": 93}, {"referenceID": 2, "context": "The gradient descent is dampened using Adagrad (Duchi et al., 2011), and the process repeats until the error no longer decreases appreciably.", "startOffset": 47, "endOffset": 67}, {"referenceID": 1, "context": "By storing the parameters in a central parameter server (Dean et al., 2012), it is possible to distribute training by processing several shards in parallel on different worker machines.", "startOffset": 56, "endOffset": 75}, {"referenceID": 13, "context": "We do this in a lock-free fashion (Recht et al., 2011) using Google\u2019s asynchronous stochastic gradient descent infrastructure distbelief (Dean et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 1, "context": ", 2011) using Google\u2019s asynchronous stochastic gradient descent infrastructure distbelief (Dean et al., 2012).", "startOffset": 90, "endOffset": 109}, {"referenceID": 11, "context": "Following Pennington et al. (2014), we pro-", "startOffset": 10, "endOffset": 35}, {"referenceID": 7, "context": "Performance of SGNS, GloVe, and Swivel vectors across different tasks with respect to methods tested by Levy et al. (2015),", "startOffset": 104, "endOffset": 123}, {"referenceID": 11, "context": "We experimented with adding word vector wi with its corresponding context vector w\u0303i (Pennington et al., 2014); however, best performance was achieved using the word vectorwi alone, as was originally reported by Mikolov et al.", "startOffset": 85, "endOffset": 110}, {"referenceID": 9, "context": ", 2014); however, best performance was achieved using the word vectorwi alone, as was originally reported by Mikolov et al. (2013a).", "startOffset": 109, "endOffset": 132}, {"referenceID": 11, "context": "(This was also noted by Pennington et al. (2014).)", "startOffset": 24, "endOffset": 49}, {"referenceID": 6, "context": "Following Levy and Goldberg\u2019s 2014 suggestion that SGNS factors a shifted PMI matrix, we experimented with shifting the objective PMI value by a small amount. Specifically, Levy and Goldberg (2014) suggest that the SGNS PMI objective is shifted by log k, where k is the number of negative samples drawn from the unigram distribution.", "startOffset": 10, "endOffset": 198}, {"referenceID": 3, "context": "For word similarity, we used WordSim353 (Finkelstein et al., 2001) partitioned into WordSim Similarity and WordSim Relatedness (Zesch et al.", "startOffset": 40, "endOffset": 66}, {"referenceID": 14, "context": ", 2001) partitioned into WordSim Similarity and WordSim Relatedness (Zesch et al., 2008; Agirre et al., 2009); Bruni et al.", "startOffset": 68, "endOffset": 109}, {"referenceID": 0, "context": ", 2001) partitioned into WordSim Similarity and WordSim Relatedness (Zesch et al., 2008; Agirre et al., 2009); Bruni et al.", "startOffset": 68, "endOffset": 109}, {"referenceID": 4, "context": "datasets that were used by Levy et al. (2015). For word similarity, we used WordSim353 (Finkelstein et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 7, "context": "As with Levy et al. (2015), we evaluated Swivel using the MSR and Google datasets (Mikolov et al.", "startOffset": 8, "endOffset": 27}], "year": 2016, "abstractText": "We present Submatrix-wise Vector Embedding Learner (Swivel), a method for generating lowdimensional feature embeddings from a feature co-occurrence matrix. Swivel performs approximate factorization of the point-wise mutual information matrix via stochastic gradient descent. It uses a piecewise loss with special handling for unobserved co-occurrences, and thus makes use of all the information in the matrix. While this requires computation proportional to the size of the entire matrix, we make use of vectorized multiplication to process thousands of rows and columns at once to compute millions of predicted values. Furthermore, we partition the matrix into shards in order to parallelize the computation across many nodes. This approach results in more accurate embeddings than can be achieved with methods that consider only observed cooccurrences, and can scale to much larger corpora than can be handled with sampling methods.", "creator": "LaTeX with hyperref package"}}}