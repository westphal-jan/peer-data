{"id": "1602.02743", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2016", "title": "The IMP game: Learnability, approximability and adversarial learning beyond $\\Sigma^0_1$", "abstract": "firstly we introduce mainly a problem where set - up we hence call the iterated finite matching class pennies ( big imp ) game and show importantly that it is a seemingly powerful practical framework for studying the study of three problems : adversarial maximum learnability, extremely conventional ( sometimes i. g e., potentially non - adversarial ) learnability coefficient and approximability. indeed using explicitly it, presently we are better able extensively to derive the following regression theorems. ( 1 ) secondly it always is possible directly to learn behavior by handing example all classes of $ \\ sigma ^ 0 _ 1 \\ cup \\ data pi ^ 4 0 _ 1 $ as relatively well as some supersets ; ( perhaps 2 ) similarly in some adversarial linear learning ( on which we incorrectly describe as clearly a risky pursuit - evasion game ), presumably the aspiring pursuer has designed a winning objective strategy ( in his other bare words, $ \\ sigma ^ a 0 _ 1 $? can normally be approximate learned accurately adversarially, therefore but $ \\ mathematician pi ^ 2015 0 _ 1 $ not ) ; ( 3 ) some verbal languages in $ \\ math pi ^ 0 _ 15 1 $ cannot be reasonably approximated readily by any artificial language in $ \\ sigma ^ 0 _ l 1 $.", "histories": [["v1", "Sun, 7 Feb 2016 04:17:17 GMT  (22kb)", "http://arxiv.org/abs/1602.02743v1", "23 pages"]], "COMMENTS": "23 pages", "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.CC cs.FL", "authors": ["michael brand", "david l dowe"], "accepted": false, "id": "1602.02743"}, "pdf": {"name": "1602.02743.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Michael Brand", "David L. Dowe"], "emails": ["michael.brand@monash.edu", "david.dowe@monash.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n02 74\n3v 1\n[ cs\n.L O\n] 7\nF eb\nWe introduce a problem set-up we call the Iterated Matching Pennies (IMP) game and show that it is a powerful framework for the study of three problems: adversarial learnability, conventional (i.e., non-adversarial) learnability and approximability. Using it, we are able to derive the following theorems. (1) It is possible to learn by example all of \u03a301 \u222a \u03a001 as well as some supersets; (2) in adversarial learning (which we describe as a pursuit-evasion game), the pursuer has a winning strategy (in other words, \u03a301 can be learned adversarially, but \u03a001 not); (3) some languages in \u03a0 0 1 cannot be approximated by any language in \u03a3 0 1.\nWe show corresponding results also for \u03a30i and \u03a0 0 i for arbitrary i.\nKeywords: Turing machine, recursively enumerable, decidable, approximation, matching pennies, halting, halting problem, elusive model paradox, red herring sequence, learnability, Nash equilibrium, approximability, adversarial learning"}, {"heading": "1. Introduction", "text": "This paper deals with three widely-discussed topics: approximability, conventional learnability and adversarial learnability, and introduces a unified framework in which all three can be studied.\nFirst, consider approximability. Turing\u2019s seminal 1936 result [21] demonstrated that some languages that can be accepted by Turing machines (TMs) are not decidable. Otherwise stated, some R.E. languages are not recursive. Equivalently: some co-R.E. languages are not R.E.; any R.E. language must differ from them by at least one word. However, the diagonalisation process by which this result was originally derived makes no stronger claim regarding the number of words differentiating a co-R.E. language and an R.E. one. It merely shows one example of a word where a difference must exist.\nWe extend this original result by showing that some co-R.E. languages are, in some sense, as different from any R.E. language as it is possible to be.\nTo formalise this statement, consider an arbitrary (computable) enumeration, w1, w2, . . ., over the complete language (the language that includes all words over the chosen alpha-\nEmail addresses: michael.brand@monash.edu (Michael Brand), david.dowe@monash.edu (David L. Dowe)\nPreprint submitted to Elsevier February 10, 2016\nbet). Over this enumeration, {wi}, we define a distance metric, dissimilarity, between two languages, L1 and L2, as follows.\nDisSim(L1, L2) \u2261 lim sup n\u2192\u221e |(L1\u25b3L2) \u2229 {w1, . . . , wn}| n ,\nwhere L1\u25b3L2 is the symmetric difference. We note that the value of DisSim(L1, L2) depends on the enumeration chosen, and therefore, technically, DisSim(\u00b7) = DisSim{wi}(\u00b7). However, all results in this paper are true for all possible choices of the enumeration, for which reason we omit the choice of enumeration, opting for this more simplified notation.\nDisSim(L1, L2) ranges between 0 (the languages are essentially identical) and 1 (the languages are completely dissimilar).\nWe prove:\nTheorem 1. There is a co-R.E. language L\u0304 such that every R.E. language has a dissimilarity distance of 1 from L\u0304.\nConsider now learnability. Learnability is an important concept in statistics, econometrics, machine learning, inductive inference, data mining and other fields. This has been discussed by E. M. Gold and by L. G. Valiant in terms of language identification in the limit [7, 22], and also in statistics via the notion of statistical consistency, also known as \u201ccompleteness\u201d (converging arbitrarily closely in the limit to an underlying true model).\nFollowing upon his convergence results in [17], Solomonoff writes [20, sec. 2 (Completeness and Incomputability)]:\n\u201cIt is notable that completeness and incomputability are complementary properties: It is easy to prove that any complete prediction method must be incomputable. Moreover, any computable prediction method can not be complete \u2013 there will always be a large space of regularities for which its predictions are catastrophically poor.\u201d\nIn other words, in Solomonoff\u2019s problem set-up it is impossible for a Turing machine to learn every R.E. language: every computable learner is limited.\nNevertheless, in the somewhat different context within which we study learnability, we are able to show that this tension does not exist: a Turing machine can learn any computable language. Moreover, we will consider a set of languages that includes, as a proper subset of it, the languages \u03a301\u222a\u03a001 and will prove that while no deterministic learning algorithm can learn every language in the set, a probabilistic one can (with probability 1), and a mixed strategy involving several deterministic learning algorithms can approximate this arbitrarily well.1\nLastly, consider adversarial learning [12, 11, 9]. This is different from the conventional learning scenario described above in that while in conventional learning we attempt to\n1Here and elsewhere we use the standard notations for language families in the arithmetical hierarchy [15]: \u03a30\n1 is the set of recursively enumerable languages, \u03a00 1 is the set of co-R.E. languages.\nconverge to an underlying \u201ctrue model\u201d based on given observations, adversarial learning is a multi-player process in which each participant can observe (to some extent) other players\u2019 predictions and adjust their own actions accordingly. This game-theoretic set-up becomes of practical importance in many scenarios. For example, in online bidding bidders use information available to them (e.g., whether they won a particular auction) to learn the strategy used by competing bidders, so as to be able to optimise their own strategy accordingly.\nWe consider, specifically, an adversarial learning scenario in which one player (the pursuer) attempts to copy a second player, while the second player (the evader) is attempting to avoid being copied. Specifically, each player generates a bit (0 or 1) and the pursuer wins if the two bits are equal while the evader wins if they are not. Though on the face of it this scenario may seem symmetric, we show that the pursuer has a winning strategy.\nTo attain all these results (as well as their higher-Turing-degree equivalents), we introduce a unified framework in which these questions and related ones can all be studied. The set-up used is an adaptation of one initially introduced by Scriven [16] of a predictor and a contrapredictive (or avoider) effectively playing what we might nowadays describe as a game of iterated matching pennies. In Section 2, we give a formal description of this problem set-up and briefly describe its historical evolution. In Section 3, we explain the relevance of the set-up to the learnability and approximability problems and analyse, as an example case, adversarial learning in the class of decidable languages. In Section 4, we extend the analysis to adversarial learning in all other classes in the arithmetical hierarchy, and in particular to Turing machines.\nIn Sections 5 and 6 we then return to conventional learnability and to approximability, respectively, and prove the remaining results by use of the set-up developed, showing how it can be adapted to these problems."}, {"heading": "2. Matching Pennies", "text": "The matching pennies game is a zero-sum two-player game where each player is required to output a bit. If the two bits are equal, this is a win for Player \u201c=\u201d; if they differ, this is a win for Player \u201c 6=\u201d. The game is a classic example used in teaching mixed strategies [see, e.g. 6, pp. 283\u2013284]: its only Nash equilibrium [14, 13] is a mixed strategy wherein each player chooses each of the two options with probability 1/2.\nConsider, now, an iterative version of this game, where at each round the players choose a new bit with perfect information of all previous rounds. Here, too, the best strategy is to choose at each round a new bit with probability 1/2 for each option, and with the added caveat that each bit must be independent of all previous bits. In the iterative variation, we define the payoff (of the entire game) to be\nS = S 6= =\n(\nlim inf N\u2192\u221e\nN \u2211\nn=1\n\u03b4n 2N\n)\n+\n(\nlim sup N\u2192\u221e\nN \u2211\nn=1\n\u03b4n 2N\n)\n(1)\nfor Player \u201c 6=\u201d, where \u03b4n is 0 if the bits output in the n\u2019th round are equal and 1 if they\nare different. The payoff for Player \u201c=\u201d is\nS= = 1\u2212 S 6= = (\nlim inf N\u2192\u221e\nN \u2211\nn=1\n1\u2212 \u03b4n 2N\n)\n+\n(\nlim sup N\u2192\u221e\nN \u2211\nn=1\n1\u2212 \u03b4n 2N\n)\n(2)\nThese payoff functions were designed to satisfy the following criteria:\n\u2022 They are always defined.\n\u2022 The game is zero-sum and strategically symmetric, except for the essential distinction between a player aiming to copy (Player \u201c=\u201d, the pursuer) and a player aiming for dissimilarity (Player \u201c 6=\u201d, the evader).\n\u2022 The payoff is a function solely of the {\u03b4i} sequence. (This is important because in the actual IMP game being constructed players will only have visibility into past \u03b4i, not full information regarding the game\u2019s evolution.)\n\u2022 Where a limit exists (in the lim sense) to the percentage of rounds to be won by a player, the payoff is this percentage.\nIn particular, note that when the payoff functions take the value 0 or 1, there exists a limit (in the lim sense) to the percentage of rounds to be won by a player, and in this case the payoff is this limit.\nIn the case of the strategy pair described above, for example, where bits are determined by independent, uniform-distribution coin tosses, the limit exists and the payoff is 1/2 for both players, indicating that the game is not biased towards either. This is a Nash equilibrium of the game: neither player can ensure a higher payoff for herself as long as the other persists in the equilibrium strategy. The game has other Nash equilibria, but all share the (1/2, 1/2) payoffs.\nAbove, we describe the players in the game as agents capable of randomisation: they choose a random bit at each new round. However, the game can be played, with the same strategies, also by deterministic agents. For this, consider every possible infinite bit-string as a possible strategy for each of the players. In this case, the game\u2019s Nash equilibrium would be a strategy pair where each player allots a bit-string from a uniform distribution among all options.\nWe formalise this deterministic outlook on the matching pennies game as follows.\nDefinition 1 (Iterative Matching Pennies game). An Iterative Matching Pennies game (or IMP), denoted IMP(\u03a3=,\u03a36=), is a two player game where each player chooses a language: Player \u201c=\u201d chooses L= \u2208 \u03a3= and Player \u201c 6=\u201d chooses L6= \u2208 \u03a36=, where \u03a3= and \u03a36= are two collections of languages over the binary alphabet.\nWhere \u03a3= = \u03a36= (= \u03a3), we denote the game IMP(\u03a3). Define \u22060 to be the empty string and define, for every natural i,\n\u03b4i def =\n{\n1 if \u2206i\u22121 \u2208 L=\u25b3L6= 0 if \u2206i\u22121 6\u2208 L=\u25b3L6= ,\n\u2206i def = \u2206i\u22121\u03b4i,\nThen the payoffs S= = S=(L=, L6=) and S 6= = S 6=(L=, L6=) are as defined in (2) and (1), respectively. The notation \u201c\u2206i\u22121\u03b4i\u201d indicates string concatenation.\nPlayer (mixed) strategies in this game are described as distributions, D= and D 6=, over \u03a3= and \u03a36=, respectively. In this case, we define\nS=(D=, D 6=) = E(S=(L=, L6=)) L= \u223c D=, L6= \u223c D 6=.\nS 6=(D=, D 6=) = E(S 6=(L=, L6=)) L= \u223c D=, L6= \u223c D 6=.\nNote again that the game is zero sum: any pair of strategies, pure or mixed, satisfies\nS=(D=, D 6=) + S 6=(D=, D 6=) = 1. (3)\nTo better illustrate the dynamics embodied by Definition 1, let us add two more definitions: let\nO=(i) def =\n{\n1 if \u2206i\u22121 \u2208 L= 0 if \u2206i\u22121 6\u2208 L=\n(4)\nand let\nO 6=(i) def =\n{\n1 if \u2206i\u22121 \u2208 L6= 0 if \u2206i\u22121 6\u2208 L6= , (5)\nnoting that by Definition 1, \u03b4i = O=(i)\u2295O 6=(i), where \u201c\u2295\u201d denotes the exclusive or (\u201cxor\u201d) function.\nThe scenario encapsulated by the IMP game is that of a competition between two players, Player \u201c=\u201d and Player \u201c 6=\u201d, where the strategy of the players is encoded in the form of the languages L= and L6=, respectively (or distributions over these in the case of mixed strategies).\nAfter i rounds, each player has visibility to the set of results so far. This is encoded by means of \u2206i, a word composed of the characters \u03b41, . . . , \u03b4i, where each \u03b4k is 0 if the bits that were output by the two players in round k are equal and 1 if they are not. It is based on this history that the players now generate a new bit: Player \u201c=\u201d generates O=(i + 1) and Player \u201c 6=\u201d generates O 6=(i + 1). The players\u2019 strategies are therefore functions from a word (\u2206i) to a bit (O=(i + 1) for Player \u201c=\u201d, O 6=(i + 1) for Player \u201c 6=\u201d). To encode these strategies in the most general form, we use languages: L= and L6= are simply sets containing all the words to which the response is \u201c1\u201d. Our choice of how weak or how strong a player can be is then ultimately in the question of what language family, \u03a3, its strategy is chosen from.\nOnce O=(i + 1) and O 6=(i + 1) are determined, \u03b4i+1 is simply their xor (1 if the bits differ, 0 if they are the same), and in this way the definition generates the infinite list of \u03b4i that is ultimately used to compute the game\u2019s overall payoff for each player.\nWere we to actually try and run a real-world IMP competition by directly implementing the definitions above, and were we to try to implement the Nash equilibrium player strategies, we would immediately run into two elements in the set-up that are incomputable:\nfirst, the choice of a uniform infinitely-long bit-string, our chosen distribution among the potential strategies, is incomputable (it is a choice among uncountably many elements); second, for a deterministic player (an agent) to output all the bits of an arbitrary (i.e., general) bit-string, that player cannot be a Turing machine. There are only countably many Turing machines, so only countably many bit-strings that can thus be output.\nIn this paper, we examine the IMP game with several choices for \u03a3= and \u03a36=. The main case studied is where \u03a3= = \u03a36= = \u03a3 0 1. In this case, we still allow player mixed strategies to be incomputable distributions, but any L= and L6= are computable by TMs. The set-up described here, where Iterated Matching Pennies is essentially described as a pursuit-evasion game, was initially introduced informally by Scriven [16] in order to prove that unpredictability is innate to humans. Lewis and Richardson [10], without explicitly mentioning Turing machines or any (equivalent) models of computation, reinvestigated the model and used it to refute Scriven\u2019s claim, with a proof that hinges on the halting problem, but references it only implicitly.\nThe set-up was redeveloped independently by Dowe, first in the context of the avoider trying to choose the next number in an integer sequence to be larger (by one) than the (otherwise) best inference that one might expect [1, sec. 0.2.7, p. 545, col. 2 and footnote 211], and then, as in [16], in the context of predicting bits in a sequence [2, p. 455][4, pp. 16\u201317]. Dowe was the first to introduce the terminology of TMs into the set-up. His aim was to illicit a paradox, which he dubbed \u201cthe elusive model paradox\u201d, whose resolution relies on the undecidability of the halting problem. Thus, it would provide an alternative to the method of [21] to prove this undecidability. Variants of the elusive model paradox and of the \u201cred herring sequence\u201d (the optimal sequence to be used by an avoider) are discussed in [3, sec. 7.5], with the paradox also mentioned in [5, sec. 2.2][8, footnote 9].\nYet a third independent incarnation of the model was by Solomonoff, who discussed variants of the elusive model paradox and the red herring sequence in [19, Appendix B] and [18, sec. 3].\nWe note that the more formal investigations of Dowe and of Solomonoff were in contexts in which the \u201cgame\u201d character of the set-up was not explored. Rather, the set-up was effectively a one-player game, where regardless of the player\u2019s choice of next bit, the red herring sequence\u2019s next bit was its reverse. We, on the other hand, return to the original spirit of Scriven\u2019s formulation, investigating the dynamics of the two player game, but do so in a formal setting.\nSpecifically, we investigate the question of which of the two players (if either) has an advantage in this game, and, in particular, we will be interested in the game\u2019s Nash equilibria, which are the pairs of strategies (D\u2217=, D \u2217 6=) for which\nS=(D \u2217 =, D \u2217 6=) = sup\nD=\nS=(D=, D \u2217 6=)\nand S 6=(D \u2217 =, D \u2217 6=) = sup\nD 6=\nS 6=(D \u2217 =, D 6=).\nWe define minmax(\u03a3=,\u03a36=) = inf\nD= sup D 6= S(D=, D 6=)\nand maxmin(\u03a3=,\u03a36=) = sup\nD 6=\ninf D= S(D=, D 6=),\nwhere D= is a (potentially incomputable) distribution over \u03a3= and D 6= is a (potentially incomputable) distribution over \u03a36=. Where \u03a3= = \u03a36= (= \u03a3), we will abbreviate this to minmax(\u03a3) and maxmin(\u03a3).\nA Nash equilibrium (D\u2217=, D \u2217 6=) must satisfy\nS(D\u2217=, D \u2217 6=) = maxmin(\u03a3=,\u03a36=) = minmax(\u03a3=,\u03a36=), (6)\nwhere, as before, S = S 6=. We note that while it may seem, at first glance, that the introduction of game dynamics into the problems of learnability and approximability inserts an unnecessary complication into their analysis, in fact, we will show that the ability to learn and/or approximate languages, when worded formally, involves a large number of interlocking \u201clim\u201d, \u201csup\u201d, \u201cinf\u201d, \u201clim sup\u201d and \u201clim inf\u201d clauses that are most naturally expressed in terms of minmax and maxmin solutions, Nash equilibria and mixed strategies."}, {"heading": "3. Halting Turing machines", "text": "The IMP game serves as a natural platform for investigating adversarial learning: each of the players has the opportunity to learn from all previous rounds, extrapolate from this to the question of what algorithm their adversary is employing and then choose their own course of action to best counteract the adversary\u2019s methods.\nFurthermore, where \u03a3= = \u03a36= (= \u03a3), IMP serves as a natural arena to differentiate between the learning of a language (e.g., one selected from R.E.) and its complement (e.g., a language selected from co-R.E.), because Player \u201c=\u201d, the copying player, is essentially trying to learn a language from \u03a3, namely that chosen by Player \u201c 6=\u201d, whereas Player \u201c 6=\u201d is attempting to learn a language from co-\u03a3, namely the complement to that chosen by Player \u201c=\u201d. Any advantage to Player \u201c=\u201d can be attributed solely to the difficulty to learn co-\u03a3 by an algorithm from \u03a3, as opposed to the ability to learn \u03a3.\nTo exemplify IMP analysis, consider first the game where \u03a3 = \u220601, the set of decidable languages. Because decidable languages are a set known to be closed under complement, we expect Player \u201c 6=\u201d to be equally as successful as Player \u201c=\u201d in this variation. Consider, therefore, what would be the Nash equilibria in this case.\nTheorem 2. Let \u03a3 be the set of decidable languages over {0, 1}\u2217. The game IMP(\u03a3) does not have any Nash equilibria.\nWe remark here that most familiar and typically-studied games belong to a family of games where the space of mixed strategies is compact and convex, such as those having\na finite number of pure strategies, and such games necessarily have at least one Nash equilibrium. However, the same is not true for arbitrary games. (For example, the game of \u201cguess the highest number\u201d does not have a Nash equilibrium.) IMP, specifically, does not belong to a game family that guarantees the existence of Nash equilibria.\nProof. We begin by showing that for any (mixed) strategy D 6=,\nsup D= S=(D=, D 6=) = 1. (7)\nLet T0, T1, . . . be any (necessarily incomputable) enumeration over those Turing machines that halt on every input, and let L0, L1, . . . be the sequence of languages that is accepted by them. The sequence {Li} enumerates (with repetitions) over all languages in \u03a3 = \u220601. Under this enumeration we have\nlim X\u2192\u221e\nProb(\u2203x \u2264 X , such that L6= = Lx) = 1; L6= \u223c D 6=.\nFor this reason, for any \u01eb there exists an X such that\nProb(\u2203x \u2264 X , such that L6= = Lx) \u2265 1\u2212 \u01eb; L6= \u223c D 6=.\nWe devise a strategy, D=, to be used by Player =. This strategy will be pure: the player will always choose language L=, which we will now describe. The language L= is the one accepted by Algorithm 1.\nAlgorithm 1 Algorithm for learning a mixed strategy\n1: function calculate bit(\u2206) 2: d \u2190 \u2016\u2206\u20161. \u22b2 Number of prediction errors so far. 3: if d > X then 4: Accept. 5: else if \u2206 \u2208 Ld then 6: Accept. 7: else 8: Reject. 9: end if 10: end function\nNote that while the enumeration T0, T1, . . . is not computable, Algorithm 1 only requires T0, . . . , TX to be accessible to it, and this can be done because any such finite set of TMs can be hard coded into Algorithm 1.\nConsider the game, on the assumption that Player \u201c 6=\u201d\u2019s strategy is Lx for x \u2264 X . After at most x prediction errors, Algorithm 1 will begin mimicking a strategy equivalent to Lx and will win every round from that point on.\nWe see, therefore, that for any x \u2208 {0, . . . , X} we have S=(L=, Lx) = 1, from which we conclude that S=(D=, D 6=) \u2265 1\u2212 \u01eb (or, equivalently, S 6=(D=, D 6=) \u2264 \u01eb), in turn proving that for any Nash equilibrium (D\u2217=, D \u2217 6=) we necessarily must have\nmaxmin(\u03a3) = 0. (8)\nFor exactly the symmetric reasons, when \u03a3 = \u220601 we also have\nminmax(\u03a3) = 1 : (9)\nPlayer \u201c 6=\u201d can follow a strategy identical to that described in Algorithm 1, except reversing the condition in Step 5.\nBecause we now have that minmax(\u03a3) 6= maxmin(\u03a3), we know that Equation (6) cannot be satisfied for any strategy pair. In particular, there are no Nash equilibria.\nThis result is not restricted to \u03a3 = \u220601, the decidable languages, but also to any set of languages that is powerful enough to encode Algorithm 1 and its complement. It is true, for example, for \u220600 as well as for \u2206 0 1 with any set of Oracles, i.e., specifically, for any \u2206 0 i .\nDefinition 2. We say that a collection of languages \u03a36= is adversarially learnable by a collection of strategies \u03a3= if minmax(\u03a3=,\u03a36=) = 0.\nIf a collection is adversarially learnable by \u03a301, we simply say that it is adversarially learnable.\nCorollary 2.1. \u2200i,\u22060i is not adversarially learnable by \u22060i .\nProof. As was shown in the proof of Theorem 2, minmax(\u22060i ,\u2206 0 i ) = 1.\nWe proceed, therefore, to the question of how well each player fares when \u03a3 includes non-decidable R.E. languages, and is therefore no longer closed under complement."}, {"heading": "4. Adversarial learning", "text": "We claim that R.E. languages are adversarially learnable, and that it is therefore not possible to learn the complement of R.E. languages in general, in the adversarial learning scenario.\nTheorem 3. The game IMP(\u03a301) has a strategy, L=, for Player \u201c=\u201d that guarantees S 6=(L=, L6=) = 0 for all L6= (and, consequently, also for all distributions among potential L6= candidates).\nIn particular, \u03a301 is adversarially learnable.\nProof. We describe L= explicitly by means of an algorithm accepting it. This is given in Algorithm 2.\nNote that Algorithm 2 does not have any \u201cAccept\u201d or \u201cReject\u201d statements. It returns a bit only if Td returns a bit and does not terminate if Td fails to terminate. To actually\nAlgorithm 2 Algorithm for learning an R.E. language\n1: function calculate bit(\u2206) 2: Let T0, T1, . . . be an enumeration over all Turing machines. 3: d \u2190 \u2016\u2206\u20161. \u22b2 Number of prediction errors so far. 4: Simulate Td 5: end function\nsimulate Td and to encode the enumeration T0, . . ., Algorithm 2 can simply use a universal Turing machine, U , and define the enumeration in a way such that U accepts the input \u201cd#\u2206\u201d if and only if Td accepts the input \u2206.\nTo show that Algorithm 2 cannot be countered, consider any R.E. language to be chosen by Player \u201c 6=\u201d. This language, L6=, necessarily corresponds to the output of Tx for some (finite) x. In total, Player \u201c=\u201d can lose at most x rounds. In every subsequent round, its output will be identical to that of Tx, and therefore identical to the bit chosen by Player \u201c 6=\u201d.\nWe see, therefore, that the complement of Algorithm 2\u2019s language cannot be learned by any R.E. language. Player \u201c 6=\u201d cannot hope to win more than a finite number of rounds.\nNote that these results do not necessitate that \u03a3 = \u03a301, the R.E. languages. As long as \u03a3 is rich enough to allow implementing Algorithm 2, the results hold. This is true, for example, for \u03a3 sets that allow Oracle calls. In particular:\nCorollary 3.1. For all i > 0, \u03a30i is adversarially learnable by \u03a3 0 i but not by \u03a0 0 i ; \u03a0 0 i is adversarially learnable by \u03a00i but not by \u03a3 0 i .\nProof. To show the learnability results, we use Algorithm 2. To show the non-learnability results, we appeal to the symmetric nature of the game: if Player \u201c=\u201d has a winning learning strategy, Player \u201c 6=\u201d does not."}, {"heading": "5. Conventional learnability", "text": "To adapt the IMP game for the study of conventional (i.e., non-adversarial) learning and approximation, we introduce the notion of nonadaptive strategies.\nDefinition 3. A nonadaptive strategy is a language, L, over {0, 1}\u2217 such that\n\u2200u, v, |u| = |v| \u21d2 (u \u2208 L \u21d4 v \u2208 L),\nwhere |u| is the bit length of u. Respective to an arbitrarily chosen (computable) enumeration w1, w2, . . . over the complete language, we define the function NA() such that, for any language L, NA(L) is the language such that\nx \u2208 NA(L) \u21d4 w|x| \u2208 L. Furthermore, for any collection of languages, \u03a3, we define NA(\u03a3) = {NA(L)|L \u2208 \u03a3}. NA(\u03a3) is the nonadaptive application of \u03a3.\nTo elucidate this definition, consider once again a (computable) enumeration, w1, w2, . . . over the complete language.\nIn previous sections, we have analysed the case where the two competing strategies are adaptive (i.e., general). This was the case of adversarial learning. Modelling the conventional learning problem is simply done by restricting \u03a36= to nonadaptive strategies. The question of whether a strategy L= (or D=) can learn L is the question of whether it can learn adversarially NA(L). The reason this is so is because the bit output at any round i by a nonadaptive strategy is independent of any response made by either player at any previous round: at each round i, O 6=(i+ 1), the response of Player \u201c 6=\u201d, as defined in (5), is a function of \u2206i, a word composed of exactly i bits. Definition 3 now adds to this the restriction that the response must be invariant to the value of these i bits and must depend only on the bit length, i, which is to say on the round number. Regardless of what the strategy of Player \u201c=\u201d is, the sequence O 6=(1), O 6=(2), . . . output by Player \u201c 6=\u201d will always remain the same. Thus, a nonadaptive strategy for Player \u201c 6=\u201d is one where the player\u2019s output is a predetermined, fixed string of bits, and it is this string that the opposing strategy of Player \u201c=\u201d must learn to mimic.\nNote, furthermore, that if \u03a3NA is the set of all nonadaptive languages, then for every i > 0 we have\nNA(\u03a30i ) = \u03a3 0 i \u2229 \u03a3NA. (10)\nThe equality stems from the fact that calculating w|x| from x and vice versa (finding any x that matches w|x|) is, by definition, recursive, so there is a reduction from any L to NA(L) and back. If a language can be computed over the input w|x| by means of a certain nonempty set of quantifiers, no additional unbounded quantifiers are needed to compute it from x.\nThis leads us to Definition 4.\nDefinition 4. We say that a collection of languages \u03a36= is (conventionally) learnable by a collection of strategies \u03a3= if minmax (\u03a3=,NA(\u03a36=)) = 0.\nIf a collection is learnable by \u03a301, we simply say that it is learnable.\nCorollary 3.2. For all i > 0, \u03a30i is learnable by \u03a3 0 i . In particular, \u03a3 0 1 is learnable.\nProof. We have already shown (Corollary 3.1) that \u03a30i is adversarially learnable by \u03a3 0 i , and NA(\u03a30i ) is a subset of \u03a3 0 i , as demonstrated by (10).\nConstraining Player \u201c 6=\u201d to only be able to choose nonadaptive strategies can only lower the minmax value. Because it is already at 0, it makes no change: we are weakening the player that is already weaker. It is more rewarding to constrain Player \u201c=\u201d and to consider the game IMP (NA(\u03a30i ),\u03a3 0 i ). Note, however, that this is equivalent to the game IMP (\u03a30i ,NA(\u03a0 0 i )) under role reversal.\nTheorem 4. \u03a001 is learnable.\nProof. To begin, let us consider a simpler scenario than was discussed so far. Specifically, we will consider a scenario in which the feedback available to the learning algorithm at each point is not only \u2206n, the information of which rounds it had \u201cwon\u201d and which it had \u201clost\u201d, but also O=(n) and O 6=(n), what the bit output by each machine was, at every step.2\nIn this scenario, Player \u201c=\u201d can calculate a co-R.E. function by calculating its complement in round n and then reading the result as the complement to O=(n), which is given to it in all later rounds.\nFor example, at round n Player \u201c=\u201d may simulate a particular Turing machine, T , in order to test whether it halts. If it does halt, the player halts and accepts the input, but it may also continue indefinitely. The end effect is that if T halts then O=(n) = 1 and otherwise it is 0. At round n + 1, Player \u201c=\u201d gets new inputs. (Recall that if one views the player as a Turing machine, it is effectively restarted at each round.) The new input in the real IMP game is \u2206n, but for the moment we are assuming a simpler version where the input is the pair of strings (O=(1) . . .O=(n), O 6=(1) . . .O 6=(n)). This being the case, though whether T halts or not is in general not computable by a \u03a301 player, once a simulation of the type described here is run at round n, starting with round n+1 the answer is available to the player in the form of O=(n), which forms part of its input.\nMore concretely, one algorithm employable by Player \u201c=\u201d against a known nonadaptive language NA(L6=) is one that calculates \u201cw2n+1 /\u2208 L6=?\u201d (which is an R.E. function) in every 2n\u2019th round, and then uses this information in the next round in order to make the correct prediction. This guarantees S (L=,NA(L6=)) \u2264 1/2. However, it is possible to do better.\nTo demonstrate how, consider that Player \u201c=\u201d can determine the answer to the question \u201c|{wi, . . . , wj} \\ L6=| \u2265 k?\u201d for any chosen i, j and k. The way to do this is to simulate simultaneously all j + 1 \u2212 i Turing machine runs that calculate \u201cwl /\u2208 L6=?\u201d for each i \u2264 l \u2264 j and to halt if k of them halt. As with the previous example, by performing this algorithm at any stage n, the algorithm will then be able to read out the result as O=(n) in all later rounds.\nConsider, now, that this ability can be used to determine |{wi, . . . , wj} \\ L6=| exactly (rather than simply bounding it) by means of a binary search, starting with the question \u201c|{wi, . . . , wj} \\ L6=| \u2265 2m\u22121?\u201d in the first round, and proceeding to increasingly finer determination of the actual set size on each later round. Player \u201c=\u201d can therefore determine the number of \u201c1\u201d bits in a set of j + 1 \u2212 i = 2m \u2212 1 outputs of a co-R.E. function in this way in only m queries, after which the number will be written in binary form, from most significant bit to least significant bit, in its O= input. Once this cardinality has been determined, Player \u201c=\u201d can compute via a terminating computation the value of each of \u201cwl \u2208 L6=?\u201d: the player will simulate, in parallel, all j + 1 \u2212 i machines, and will terminate the computation either when the desired bit value is found via a halting of the corresponding machine, or until the full cardinality of halting machines has been reached,\n2Because O=(n)\u2295O6=(n)\u2295 \u03b4n = 0, using any two of these as input to the TM is equivalent to using all three, because the third can always be calculated from the others.\nat which point, if the desired bit is not among the machines that halted, then the player can safely conclude that its computation will never halt.\nLet {mt} be an arbitrary (computable) sequence with limt\u2192\u221emt = \u221e. If Player \u201c=\u201d repeatedly uses mt bits (each time picking the next value in the sequence) of its own output in order to determine Player \u201c 6=\u201d\u2019s next 2mt \u2212 1 bits, the proportion of bits determined correctly by this will approach 1.\nHowever, the actual problem at hand is one where Player \u201c=\u201d does not have access to its own output bits, (O=(1), . . . , O=(n)). Rather, it can only see (\u03b41, . . . , \u03b4n), the exclusive or (xor) values of its bits and those of Player \u201c 6=\u201d. To deal with this situation, we use a variation over the strategy described above.\nFirst, for convenience, assume that Player \u201c=\u201d knows the first m0 bits to be output by Player \u201c 6=\u201d. Knowing Player \u201c 6=\u201d\u2019s bits and having visibility as to whether they are the same or different to Player \u201c=\u201d\u2019s bits give, together, Player \u201c=\u201d access to its own past bits.\nNow, it can use these first m0 bits in order to encode, as before, the cardinality of the next 2m0 \u2212 1 bits, and by this also their individual values (as was demonstrated previously with the calculation of \u201cwl \u2208 L6=?\u201d). This now gives Player \u201c=\u201d the ability to win every one of the next 2m0\u22121 rounds. However, instead of utilising this ability to the limit, Player \u201c=\u201d will only choose to win the next 2m0 \u2212 1 \u2212 m1, leaving the remaining m1 bits free to be used for encoding the cardinality of the next 2m1 \u2212 1. This strategy can be continued to all mt. The full list of criteria required of the sequence {mt} for this construction to work and to ultimately lead to S (L=,NA(L6=)) = 0 is:\n1. limt\u2192\u221e mt = \u221e. 2. \u2200t,mt+1 \u2264 2mt \u2212 1. 3. limt\u2192\u221e mt+1 2mt = 0.\nA sequence satisfying all these criteria can easily be found, e.g. mt = t+ 2. Two problems remain to be solved: (1) How to determine the value of the first m0 bits, and (2) how to deal with the fact that L6= is not known. We begin by tackling the second of these problems. Because L6= is not known, we utilise a strategy of enumerating over the possible languages, similar to what is done in Algorithm 2. That is to say, we begin by assuming that co-L6= = L0 and respond accordingly. Then, if we detect that the responses from Player \u201c 6=\u201d do not match those of L0 we progress to assume that co-L6= = L1, etc.. We are not always in a position to tell if our current hypothesis of L6= is correct, but we can verify that it matches at least the first 2mt \u2212mt+1 \u2212 1 bits of each 2mt \u2212 1 set. If Player \u201c=\u201d makes any incorrect predictions during any of these 2mt \u2212mt+1\u22121 rounds, it can progress to the next hypothesis. We note that it is true that Player \u201c=\u201d can remain mistaken about the identity of L6= forever, as long as L6= is such that the first 2\nmt \u2212mt+1 \u2212 1 predictions of every 2mt \u2212 1 are correct, but because these correct predictions alone are enough to ensure S (L=,NA(L6=)) = 0, the question of whether the correct L6= is ultimately found or not is moot.\nTo tackle the remaining problem, that of determiningm0 bits of L= in order to bootstrap the process, we make use of mixed strategies.\nConsider a mixed strategy involving probability 1/2m0 for each of 2m0 strategies, differing only by the m0 bits they assign as the first bits for each language in order to bootstrap the learning process. If co-L6= = L0, of the 2\nm0 strategies one will make the correct guess regarding the first m0 input bits, after which that strategy can ensure S (L=,NA(L6=)) = 0. However, note that, if implemented as described so far, this is not the case for any other Li. Suppose, for example, that co-L6= = L1. All 2\nm0 strategies begin by assuming, falsely, that co-L6= = L0, and all may discover later on that this assumption is incorrect, but they may do so at different rounds. Because of this, a counter-strategy can be designed to fool all 2m0 learner strategies.\nTo avoid this pitfall, all strategies must use the same bit positions in order to bootstrap learning for each Li, so these bit positions must be pre-allocated. We will use bits a2, . . . , a2 + m0 \u2212 1 in order to bootstrap the i\u2019th hypothesis, for some known a = a(i), regardless of whether the hypothesis L= = Li is known to require checking before these rounds, after, or not at all. The full set of rounds pre-allocated in this way still has only density zero among the integers, so even without a win for Player \u201c=\u201d in any of these rounds its final payoff remains 1.\nSuppose, now, that Li is still not the assumption currently being verified (or falsified) at rounds a2, . . . , a2+m0\u22121. The Hamming weight (number of \u201c1\u201ds) of which 2m0 \u22121 bits should be encoded by Player \u201c=\u201d in these rounds\u2019 bits? To solve this, we will pre-allocate to each hypothesis an infinite number of bit positions, which, altogether for all hypotheses, still amount to a set of density 0 among the integers. The hypothesis will continuously predict the values of this pre-allocated infinite sequence of bits until it becomes the \u201cactive\u201d assumption. If and when it does, it will expand its predictions to all remaining bit positions.\nThis combination of 2m0 strategies, of which one guarantees a payoff of 1, therefore guarantees in total an expected payoff of at least 1/2m0. We want to show, however, that minmax (\u03a301,NA(\u03a0 0 1)) = 0. To raise from 1/2\nm0 to 1, we describe a sequence of mixed strategies for which the expected payoff for Player \u201c=\u201d converges to 1.\nThe k\u2019th element in the sequence of mixed strategies will be composed of 2m0k equal probability pure strategies. The strategies will follow the algorithm so far, but instead of moving from the hypothesis co-L6= = Li to co-L6= = Li+1 after a single failed attempt (which may be due to incorrect bootstrap bits), the algorithm will try each Li language k times. In total, it will guess at most m0k bits for each language, which are the m0k bits defining the strategy.\nThis strategy ensures a payoff of at least 1\u2212 (1\u22121/2m0)k, so converges to 1, as desired, for an asymptotically large k.\nThe full algorithm is described in Algorithm 3. It uses the function triangle, defined as follows: let\nbase(x) =\n\u230a\u230a \u221a 8x+ 1\u230b \u2212 1\n2\n\u230b\nand triangle(x) = x\u2212 base(x)(base(x) + 1)/2. (11)\nThe value of triangle(x) for x = 0, 1, 2, . . . equals\n0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, . . . ,\ndescribing a triangular walk through the nonnegative integers. The algorithm is divided into two stages. In Step 1, the algorithm simulates its actions in all previous rounds, but without simulating any (potentially non-halting) Turing machine associated with any hypothesis. The purpose of this step is to determine which hypothesis (choice of Turing machine and bootstrapping) is to be used for predicting the next bit. Once the hypothesis is determined, Step 2 once again simulates all previous rounds, only this time simulating the chosen hypothesis wherever it is the active hypothesis. In this way, the next bit predicted by the hypothesis can be determined.\nThe specific {mt} sequence used in Algorithm 3 is mt = t + 2 (which was previously mentioned as an example of a sequence satisfying all necessary criteria).\nSome corollaries follow immediately.\nCorollary 4.1. There exists a probabilistic Turing machine that is able to learn any language in \u03a001 with probability 1.\nProof. Instead of using a mixed strategy, it is possible to use probabilistic Turing machines in order to generate the m0 guessed bits that bootstrap each hypothesis. In this case, there is neither a need for a mixed strategy nor a need to consider asymptotic limits: a single probabilistic Turing machine can perform a triangular walk over the hypotheses for L6=, investigating each option an unbounded number of times. The probability that for the correct L6= at least one bootstrap guess will be correct in this way equals 1.\nThe method for doing this is essentially the same as was described before. The only caveat is that because the probabilistic TM is re-initialised at each round and because it needs, as part of the algorithm, to simulate its actions in all previous rounds, the TM must have a way to store its random choices, so as to make them accessible in all later rounds.\nThe way to do this is to extend the hypothesis \u201cbootstrap\u201d phase from m0 bits to 2m0 bits. In each of the first m0 bits, the TM outputs a uniform random bit. The \u03b4n bit available to it in all future rounds is then this random bit xor the output of Player \u201c 6=\u201d. \u03b4n is therefore also a uniform random bit. In this way, in all future rounds the TM has access to these m0 consistent random bits. It can then use these in the second set of m0 bootstrap bits as was done with the j value in the deterministic set-up.\nWe note, as before, that the construction described continues to hold, and therefore the results remain true, even if Oracles are allowed, that are accessible to both players, and, in particular, the results hold for any \u03a00i with i > 0:\nCorollary 4.2. For all i > 0, \u03a00i is learnable by \u03a3 0 i .\nFurthermore:\nAlgorithm 3 Algorithm for learning any co-R.E. language\n1: \u22b2 The strategy is a uniform mixture of 4k algorithms. 2: \u22b2 We describe the j\u2019th algorithm. 3: function calculate bit(\u2206) 4: n \u2190 length of \u2206 \u22b2 The round number. Let \u2206 = \u03b41, . . . , \u03b4n. 5: \u22b2 Step 1: Identify h, the current hypothesis. 6: NonActiveHypotheses \u2190 {} 7: PredPos \u2190 {} \u22b2 A set managing which positions are predicted by which\nhypothesis. 8: for i \u2208 0, . . . , n do 9: if \u2203(h, S, S \u2032) \u2208 PredPos such that i \u2208 S then 10: Let h, S, S \u2032 be as above. 11: \u22b2 h = hypothesis number. 12: \u22b2 S = predicted positions. 13: \u22b2 S \u2032 = next positions to be predicted. 14: Let m be such that 2m\u22121 \u2212 1 = |S \u2032|. \u22b2 We only construct S \u2032 that have such\nan m. 15: else if \u2203a, h such that a2 = i, h = triangle(a) and h /\u2208 NonActiveHypotheses\nthen 16: \u22b2 First bootstrap bit for hypothesis h. 17: Let h be as above. 18: S \u2190 {} 19: S \u2032 \u2190 {i, i+ 1} 20: bootstrap(h) \u2190 i 21: m \u2190 2 22: else if i = n then \u22b2 Unusable bits. 23: Accept input. \u22b2 Arbitrary choice. 24: else 25: Next i. 26: end if 27: e \u2190 |{x \u2208 S|x > i}| 28: if e \u2265 m then 29: \u22b2 These bits are predicted accurately for the correct hypothesis. 30: if i < n and \u03b4i+1 = 1 then 31: \u22b2 Incorrect prediction, so hypothesis is false. 32: NonActiveHypotheses \u2190 NonActiveHypotheses \u222a {h} 33: PredPos \u2190 {(h\u0303, S\u0303, S\u0303 \u2032) \u2208 PredPos|h\u0303 6= h} 34: end if 35: else if e = m\u2212 1 then \u22b2 Bits with e < m are used to encode next bit counts. 36: S\u0303 \u2190 {} \u22b2 New positions to predict on. 37: p \u2190 max(S \u2032)\n38: while |S\u0303| < 2m \u2212 1 do 39: p \u2190 p+ 1 40: if (\u2203a, b such that b \u2208 {0, 1}, a2 + b = p and h = triangle(a)) or (h =\nmex(NonActiveHypotheses) and \u2204a, b, h\u0303 such that b \u2208 {0, 1}, a2+b = p, h\u0303 = triangle(a), h\u0303 /\u2208 NonActiveHypotheses) then 41: \u22b2 \u201cmex(T )\u201d is the minimum nonnegative integer not appearing in T . 42: S\u0303 \u2190 S\u0303 \u222a {p} 43: end if 44: end while 45: PredPos \u2190 PredPos \u222a (h, S \u2032, S\u0303) 46: end if 47: end for 48: \u22b2 Step 2: Predict, assuming h. 49: i \u2190 bootstrap(h) 50: S \u2190 {i, i+ 1} 51: M \u2190 h div k \u22b2 TM is the machine to be simulated. x div y def= \u230ax/y\u230b. 52: try \u2190 h mod k \u22b2 The try number of this machine. 53: Prediction(i) \u2190 (j div 4try) mod 2 54: Prediction(i+ 1) \u2190 (j div (2 \u00b7 4try)) mod 2 55: for i \u2208 0, . . . , n do 56: if \u2203S, S \u2032, (h, S, S \u2032) \u2208 PredPos and i \u2208 S then 57: Let m be such that 2m \u2212 1 = |S \u2032|. 58: e \u2190 |{x \u2208 S|x > i}| 59: if e = m\u2212 1 then 60: counter \u2190 0 \u22b2 Number of 1\u2019s in S \u2032. 61: end if 62: if e \u2265 m then 63: if i = n then 64: if Prediction(i) = 1 then 65: Accept input. 66: else 67: Reject input. 68: end if 69: end if 70: else if i = n then 71: Simulate TM simultaneously on all inputs in S\n\u2032 until counter + 2e are accepted. 72: \u22b2 If this simulation does not terminate, this is a rejection of the input. 73: Accept input. 74: else\n75: if Prediction(i) 6= \u03b4i then \u22b2 Previous simulation terminated. 76: counter \u2190 counter+ 2e \u22b2 Binary search. 77: end if 78: if e = 0 then \u22b2 counter holds the number of terminations in S \u2032. 79: Simulate TM simultaneously on all inputs in S\n\u2032 until counter are accepted. \u22b2 Guaranteed to halt, if hypothesis is correct.\n80: Let Prediction(x) be 0 on all x \u2208 S \u2032 that terminated, 1 otherwise. 81: end if 82: end if 83: end if 84: end for 85: end function\nCorollary 4.3. For all i > 0, the collection of languages learnable by \u03a30i is a strict superset of \u03a30i \u222a\u03a00i .\nProof. We have already shown that \u03a30i and \u03a0 0 i are both learnable by \u03a3 0 i . Adding the \u03a3 0 i languages as additional hypotheses to Algorithm 3 we can see that the set \u03a30i \u222a\u03a00i is also learnable.\nTo give one example of a family of languages beyond this set which is also learnable by \u03a30i , consider the following. Let \u03a3 (c) i , for a fixed c > 1, be the set of languages recognisable by a \u220600 Turing machine which can make at most c calls to a \u03a3 0 i Oracle.\nThis set contains \u03a30i and \u03a0 0 i , but it also contains, for example, the xor of any two languages in \u03a30i , which is outside of \u03a3 0 i \u222a\u03a00i , and therefore strictly beyond the i\u2019th level of the arithmetic hierarchy. We will adapt Algorithm 3 to learn \u03a3\n(c) i . The core of Algorithm 3 is its ability to use m\nbits of \u2206n in order to predict 2 m \u2212 1 bits. We will, instead, use cm bits in order to predict the same amount. Specifically, we will use the first m bits in order to predict the result of the first Oracle call in each of the predicted 2m \u2212 1 positions, the next m bits in order to predict the second Oracle call in each of the predicted 2m \u2212 1 positions, and so on.\nIn total, for this to work, all we need is to replace criterion 2 in our list of criteria for the {mt} sequence with the new criterion\n\u2200t, cmt+1 \u2264 2mt \u2212 1.\nAn example of such a sequence is mt = t+max(c, 5).\nIn fact, Algorithm 3 can be extended even beyond what was described in the proof to Corollary 4.3. For example, instead of using a constant c, it is possible to adapt the algorithm to languages that use c(n) Oracle calls at the n\u2019th round, for a sufficiently low-complexity c(n) by similar methods.\nAltogether, it seems that R.E. learning is significantly more powerful than being able to learn merely the first level of the arithmetic hierarchy, but we do not know whether it\ncan learn every language in \u220602. Indeed, we have no theoretical result that implies R.E. learning cannot be even more powerful than the second level of the arithmetic hierarchy.\nA follow-up question which may be asked at this point is whether it was necessary to use a mixed strategy, as was used in the proof of Theorem 4, or whether a pure strategy could have been designed to do the same.\nIn fact, no pure strategy would have sufficed:\nLemma 4.1. For all i, inf\nL=\u2208\u03a30i sup L 6=\u2208NA(\u03a0 0 i ) S(L=, L6=) = 1.\nThis result is most interesting in the context of Corollary 4.1, because it describes a concrete task that is accomplishable by a probabilistic Turing machine but not by a deterministic Turing machine.\nProof. We devise for each L= a specific L6= antidote. The main difficulty in doing this is that we cannot choose, as before, L6= = co-L=, because L6= is now restricted to be nonadaptive, whereas L= is general.\nHowever, consider L6= such that its bit for round k is the complement of L=\u2019s response on \u2206k\u22121 = 1 k\u22121. This is a nonadaptive strategy, but it ensures that \u2206k will be 1 k for every k. Effectively, L6= describes L=\u2019s \u201cred herring sequence\u201d."}, {"heading": "6. Approximability", "text": "When both players\u2019 strategies are restricted to be nonadaptive, they have no means of learning each other\u2019s behaviours: determining whether their next output bit will be 0 or 1 is done solely based on the present round number, not on any previous outputs. The output of the game is therefore solely determined by the dissimilarity of the two independently-chosen output strings.\nDefinition 5. We say that a collection of languages \u03a36= is approximable by a collection of strategies \u03a3= if minmax (NA(\u03a3=),NA(\u03a36=)) = 0.\nIf a collection is approximable by \u03a301, we simply say that it is approximable.\nIn this context it is clear that for any \u03a3\nsup L 6=\u2208NA(\u03a3) inf L=\u2208NA(\u03a3) S(L=, L6=) = 0,\nbecause L= can always be chosen to equal L6=, but unlike in the case of adversarial learning, here mixed strategies do make a difference.\nThough we do not know exactly what the value of minmax (NA(\u03a301)) is, we do know the following.\nLemma 4.2. If D= and D 6= are mixed strategies from NA(\u03a3 0 1), then\nsup D 6= inf D= E\n(\nlim sup N\u2192\u221e\nN \u2211\nn=1\n\u03b4n N\n)\n\u2265 1 2\n(12)\nand\ninf D= sup D 6= E\n(\nlim sup N\u2192\u221e\nN \u2211\nn=1\n\u03b4n N\n)\n\u2265 1 2 , (13)\nwhere \u03b4n is as in the definition of the IMP game.\nIn other words, Player \u201c 6=\u201d can always at the very least break even, from a lim sup perspective.\nProof. Let D 6= be a mixture of the following two strategies: all zeros (L0), with probability 1/2; all ones (L1), with probability 1/2. By the triangle inequality, we have that for any language L=,\nE\n(\nlim sup N\u2192\u221e\nN \u2211\nn=1\n\u03b4n N\n)\n= DisSim(L=, L0) + DisSim(L=, L1) 2 \u2265 DisSim(L0, L1) 2 = 1 2 ,\nand because this is true for each L= in D=, it is also true in expectation over all D=. The fact that D 6= is independent of D= in the construction means that this bound is applicable for both (12) and (13).\nJust as interesting (and with tighter results) is the investigation of lim inf. We show\nLemma 4.3.\ninf L=\u2208NA(\u03a301) sup L 6=\u2208NA(\u03a3 0 1 ) lim inf N\u2192\u221e\nN \u2211\nn=1\n\u03b4n N\n= sup L 6=\u2208NA(\u03a3 0 1 ) inf L=\u2208NA(\u03a301) lim inf N\u2192\u221e\nN \u2211\nn=1\n\u03b4n N = 0, (14)\nwhere \u03b4n is as in the definition of the IMP game.\nProof. Let triangle(x) be as in (11), and let caf(x) be the maximum integer, y, such that y! \u2264 x.\nThe language L= will be defined by\nwi \u2208 L= \u21d4 wi \u2208 Ltriangle(caf(i)),\nwhere L0, L1, L2, . . . is an enumeration over all R.E. languages. To prove that for any j, if L6= = Lj the claim holds, let us first join the rounds into \u201csuper-rounds\u201d, this being the partition of the rounds set according to the value of y = caf(i). At each super-round, L= equals a specific Lx, and by the end of the superround, a total of (y \u2212 1)/y of the total rounds will have been rounds in which L= equals this Lx. Hence, the Hamming distance between the two (the number of differences) at this time is at most 1/y of the string lengths. Because each choice of x repeats an infinite number of times, the lim inf of this proportion is 0.\nWith this lemma, we can now prove Theorem 1.\nProof. The theorem is a direct corollary of the proof of Lemma 4.3, because the complement of the language L= that was constructed in the proof to attain the infimum can be used as L\u0304.\nCombining Lemma 4.2 and 4.3 with the definition of the payoff function in (1), we get, in total:\nCorollary 4.4. 1/4 \u2264 maxmin (\nNA(\u03a301) ) \u2264 1/2 and\n1/4 \u2264 minmax ( NA(\u03a301) ) \u2264 1/2.\nThough we have the exact value of neither maxmin nor minmax in this case, we do see that the case is somewhat unusual in that neither player has a decisive advantage."}, {"heading": "7. Conclusions and further research", "text": "We have introduced the IMP game as an arena within which to test the ability of algorithms to learn and be learnt, and specifically investigated three scenarios:\nAdversarial learning, where both algorithms are simultaneously trying to learn each other by observations.\nNon-adversarial (conventional) learning, where an algorithm is trying to learn a language by examples.\nApproximation, where languages (or language distributions) try to mimic each other without having any visibility to their opponent\u2019s actions.\nIn the case of adversarial learning, we have shown that \u03a30i can learn \u03a3 0 i but not \u03a0 0 i . In conventional learning, however, we have shown that \u03a3i can learn \u03a3 0 i , \u03a0 0 i and beyond into the (i+ 1)th level of the arithmetic hierarchy, but this learnability is yet to be upperbounded. Our conjecture is that the class of learnable languages is strictly a subset of \u220602. If so, then this defines a new class of languages between the first and second levels of the arithmetic hierarchy, and, indeed, between any consecutive levels of it.\nRegarding approximability, we have shown that (unlike in the previous results) no side has the absolute upper hand in the game, with the game value for Player \u201c 6=\u201d, if it exists, lying somewhere between 1/4 and 1/2. We do not know, however, whether the game is completely unbiased or not.\nAn investigation of adversarial learning in the context of recursive languages was given as a demonstration of the fact that in IMP it may be the case that no Nash equilibrium exists at all, and pure-strategy learning was given as a concrete example of a task where probabilistic Turing machines have a provable advantage over deterministic ones."}], "references": [{"title": "Foreword re C", "author": ["D.L. Dowe"], "venue": "S. Wallace. Computer Journal, 51(5):523\u2013560, September", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Minimum Message Length and statistically consistent invariant (objective?) Bayesian probabilistic inference \u2013 from (medical) \u201cevidence", "author": ["D.L. Dowe"], "venue": "Social Epistemology, 22(4):433\u2013460, Oct\u2013Dec", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "MML, hybrid Bayesian network graphical models, statistical consistency, invariance and uniqueness", "author": ["D.L. Dowe"], "venue": "Bandyopadhyay, P.S. and Forster, M.R., editor, Handbook of the Philosophy of Science \u2013 Volume 7: Philosophy of Statistics, pages 901\u2013982. Elsevier,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Introduction to Ray Solomonoff 85th Memorial Conference", "author": ["D.L. Dowe"], "venue": "Proceedings of Solomonoff 85th memorial conference \u2013 Lecture Notes in Artificial Intelligence (LNAI), volume 7070, pages 1\u201336. Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Compression and intelligence: Social environments and communication", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo", "P.K. Das"], "venue": "AGI: 4th Conference on Artificial General Intelligence \u2013 Lecture Notes in Artificial Intelligence (LNAI), pages 204\u2013211,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "The Computational Beauty of Nature: Computer Explorations of Fractals, Chaos, Complex Systems, and Adaptation", "author": ["G.W. Flake"], "venue": "A Bradford book. Cambridge, Massachusetts,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Language identification in the limit", "author": ["E.M. Gold"], "venue": "Information and Control, 10(5):447\u2013 474,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1967}, {"title": "On more realistic environment distributions for defining, evaluating and developing intelligence", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Insa-Cabrera"], "venue": "AGI: 4th Conference on Artificial General Intelligence \u2013 Lecture Notes in Artificial Intelligence (LNAI), volume 6830, pages 82\u201391. Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Adversarial machine learning", "author": ["Ling Huang", "Anthony D. Joseph", "Blaine Nelson", "Benjamin I.P. Rubinstein", "J.D. Tygar"], "venue": "In Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Scriven on human unpredictability", "author": ["D.K. Lewis", "J.S. Richardson"], "venue": "Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition, 17(5):69\u2013 74, October", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1966}, {"title": "A Game Theoretical Model for Adversarial Learning", "author": ["Wei Liu", "Sanjay Chawla"], "venue": "PS and Wu, XD, editor,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Adversarial learning", "author": ["Daniel Lowd", "Christopher Meek"], "venue": "Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Non-cooperative Games", "author": ["J. Nash"], "venue": "The Annals of Mathematics, 54(2):286\u2013295,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1951}, {"title": "Theory of Games and Economic Behavior", "author": ["J.v. Neumann", "O. Morgenstern"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1944}, {"title": "Theory of Recursive Functions and Effective Computability", "author": ["Hartley Rogers", "Jr."], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1987}, {"title": "An essential unpredictability in human behavior", "author": ["M. Scriven"], "venue": "B.B. Wolman and E. Nagel, editors, Scientific Psychology: Principles and Approaches, pages 411\u2013425. Basic Books (Perseus Books),", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1965}, {"title": "Complexity-based induction systems: Comparisons and convergence theorems", "author": ["R.J. Solomonoff"], "venue": "IEEE Transaction on Information Theory, IT-24(4):422\u2013432,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1978}, {"title": "Algorithmic probability: Theory and applications", "author": ["R.J. Solomonoff"], "venue": "F. Emmert- Streib and M. Dehmer, editors, Information Theory and Statistical Learning, Springer Science and Business Media, pages 1\u201323. Springer, N.Y., U.S.A.,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithmic probability, heuristic programming and AGI", "author": ["R.J. Solomonoff"], "venue": "Proceedings of the Third Conference on Artificial General Intelligence, AGI 2010, pages 251\u2013257, Lugano, Switzerland, March", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Algorithmic probability \u2013 its discovery \u2013 its properties and application to strong AI", "author": ["R.J. Solomonoff"], "venue": "H. Zenil, editor, Randomness Through Computation: Some Answers, More Questions, pages 1\u201323. World Scientific Publishing Co., Inc., River Edge, NJ, USA,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "On computable numbers, with an application to the Entscheidungsproblem", "author": ["A.M. Turing"], "venue": "Proc. London Math. Soc., 42:230\u2013265,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1936}, {"title": "A theory of the learnable", "author": ["Leslie G Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1984}], "referenceMentions": [{"referenceID": 20, "context": "Turing\u2019s seminal 1936 result [21] demonstrated that some languages that can be accepted by Turing machines (TMs) are not decidable.", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "Valiant in terms of language identification in the limit [7, 22], and also in statistics via the notion of statistical consistency, also known as \u201ccompleteness\u201d (converging arbitrarily closely in the limit to an underlying true model).", "startOffset": 57, "endOffset": 64}, {"referenceID": 21, "context": "Valiant in terms of language identification in the limit [7, 22], and also in statistics via the notion of statistical consistency, also known as \u201ccompleteness\u201d (converging arbitrarily closely in the limit to an underlying true model).", "startOffset": 57, "endOffset": 64}, {"referenceID": 16, "context": "Following upon his convergence results in [17], Solomonoff writes [20, sec.", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": "Lastly, consider adversarial learning [12, 11, 9].", "startOffset": 38, "endOffset": 49}, {"referenceID": 10, "context": "Lastly, consider adversarial learning [12, 11, 9].", "startOffset": 38, "endOffset": 49}, {"referenceID": 8, "context": "Lastly, consider adversarial learning [12, 11, 9].", "startOffset": 38, "endOffset": 49}, {"referenceID": 14, "context": "Here and elsewhere we use the standard notations for language families in the arithmetical hierarchy [15]: \u03a3 1 is the set of recursively enumerable languages, \u03a0 1 is the set of co-R.", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "The set-up used is an adaptation of one initially introduced by Scriven [16] of a predictor and a contrapredictive (or avoider) effectively playing what we might nowadays describe as a game of iterated matching pennies.", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "283\u2013284]: its only Nash equilibrium [14, 13] is a mixed strategy wherein each player chooses each of the two options with probability 1/2.", "startOffset": 36, "endOffset": 44}, {"referenceID": 12, "context": "283\u2013284]: its only Nash equilibrium [14, 13] is a mixed strategy wherein each player chooses each of the two options with probability 1/2.", "startOffset": 36, "endOffset": 44}, {"referenceID": 15, "context": "The set-up described here, where Iterated Matching Pennies is essentially described as a pursuit-evasion game, was initially introduced informally by Scriven [16] in order to prove that unpredictability is innate to humans.", "startOffset": 158, "endOffset": 162}, {"referenceID": 9, "context": "Lewis and Richardson [10], without explicitly mentioning Turing machines or any (equivalent) models of computation, reinvestigated the model and used it to refute Scriven\u2019s claim, with a proof that hinges on the halting problem, but references it only implicitly.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "2 and footnote 211], and then, as in [16], in the context of predicting bits in a sequence [2, p.", "startOffset": 37, "endOffset": 41}, {"referenceID": 20, "context": "Thus, it would provide an alternative to the method of [21] to prove this undecidability.", "startOffset": 55, "endOffset": 59}], "year": 2016, "abstractText": "We introduce a problem set-up we call the Iterated Matching Pennies (IMP) game and show that it is a powerful framework for the study of three problems: adversarial learnability, conventional (i.e., non-adversarial) learnability and approximability. Using it, we are able to derive the following theorems. (1) It is possible to learn by example all of \u03a31 \u222a \u03a01 as well as some supersets; (2) in adversarial learning (which we describe as a pursuit-evasion game), the pursuer has a winning strategy (in other words, \u03a31 can be learned adversarially, but \u03a01 not); (3) some languages in \u03a0 0 1 cannot be approximated by any language in \u03a3 0 1. We show corresponding results also for \u03a3i and \u03a0 0 i for arbitrary i.", "creator": "LaTeX with hyperref package"}}}