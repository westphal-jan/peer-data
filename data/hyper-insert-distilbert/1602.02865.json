{"id": "1602.02865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "The Role of Typicality in Object Classification: Improving The Generalization Capacity of Convolutional Neural Networks", "abstract": "deep artificial neural networks have already made apparent remarkable progress in different numerical tasks in the major field of molecular computer tissue vision. however, the primary empirical analysis utilized of these these models and investigation of likely their failure event cases has continually received sustained attention recently. published in this work, we proudly show unanimously that \" deep theoretical learning models routinely cannot independently generalize responses to completely atypical ct images that can are substantially intentionally different from training images. this knowledge is primarily in weak contrast to eliminating the superior generalization ability typical of the 3d visual retrieval system in the human brain. we focus instead on incorporating convolutional neural networks ( abbreviated cnn ) as the state - domain of - the - art cnn models in object intelligence recognition and classification ; helped investigate this problem why in slightly more basic detail, design and hypothesize that training cnn models generally suffer injury from unstructured neural loss minimization. afterward we successfully propose computational fuzzy models how to fundamentally improve the generalization robust capacity of cnns by considering how typical a customized training image pattern looks like. by conducting an extensive set of experiments we show that involving designing a typicality measure can improve the classification versus results required on testing a new set of md images by a increasingly large margin. more importantly, this significant improvement it is achieved without fine - tuning the new cnn imaging model on the target image set.", "histories": [["v1", "Tue, 9 Feb 2016 05:30:33 GMT  (2500kb,D)", "http://arxiv.org/abs/1602.02865v1", "In Submission"]], "COMMENTS": "In Submission", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["babak saleh", "ahmed elgammal", "jacob feldman"], "accepted": false, "id": "1602.02865"}, "pdf": {"name": "1602.02865.pdf", "metadata": {"source": "CRF", "title": "The Role of Typicality in Object Classification: Improving The Generalization Capacity of Convolutional Neural Networks", "authors": ["Babak Saleh", "Ahmed Elgammal", "Jacob Feldman"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Convolutional Neural Networks (CNN) have made remarkable progress in a variety of computer vision tasks. To just name few of the recent advances, CNN-based models greatly improved the performance in object classification [Simonyan and Zisserman, 2015], object detection [Ren et al., 2015], image retrieval [Sharif Razavian et al., 2015], fine-grained recognition [Lin et al., 2015], scene classification [Zhou et al., 2014], action classification [Gkioxari et al., 2015] and generating image descriptions [Vinyals et al., 2014].\nDespite surpassing the human categorization performance on large-scale visual object datasets [He et al., 2015], convolution neural networks cannot emulate the generalization power of the human visual system in real-world object categorization [Ghodrati et al., 2014; Pinto et al., 2008], especially when it comes to objects that differ substantially from the training examples. Figure 1 shows examples of these atypical images, which human subjects categorize correctly,\nbut which a CNN model misclassified with a high confidence. We evaluate the performance of state-of-the-art CNNs for the purpose of object classification on atypical images. Humans are capable of perceiving atypical objects and reasoning about them, even though they had not seen them before [Saleh et al., 2013]. But our experiments have shown that state-of-the-art CNNs failed drastically to recognize atypical objects. Table 1 shows the results of this experiment, where we took off-theshelf CNNs and applied them on atypical images. The significant performance drop, when tested on atypical images, is rooted in the limited generalization power of CNN models versus the human visual system.\nOne might argue that this issue of cross-dataset generalization is implicitly rooted in dataset biases, and not limited to CNN models [Torralba et al., 2011]. However, we argue that the huge number of labeled images in the training set of these models (here ImageNet) should alleviate this drawback. By providing a wide range of variation in terms of visual appearances of objects in training images, the effect of biases fades away. We support our argument by testing same networks on a new set of images that are disjoint from the training set of ImageNet [Deng et al., 2009], but look typical. Results of this experiment as it is reported in columns \u201cTest-T\u201d in Table 1 show a much smaller drop in accuracy, compared to the case of testing on atypical images (Test-A). We conclude that dataset bias can affect the performance of CNNs for object categorization, but it is not the main reason behind its poor generalization to new datasets. ar X\niv :1\n60 2.\n02 86\n5v 1\n[ cs\n.C V\n] 9\nF eb\n2 01\n6\nInstead, inspired by the way humans learn object categories, we can empower CNN models with the ability to categorize extremely difficult cases of atypical images. Humans begin to form categories and abstractions at an early age[Murphy, 2002]. The mechanisms underlying human category formation are the subject of many competing accounts, including those based on prototypes[Minda and Smith, 2001], exemplars[Nosofsky, 1984], density estimation[Ashby and Alfonso-Reese, 1995], and Bayesian inference[Goodman et al., 2008]. But all modern models agree that human category representations involve subjective variations in the typicality or probability of objects within categories. In other words, typicality is a graded concept and there is no simple decision boundary between typical vs. atypical examples. A category like bird, would include both highly typical examples such as robins, as well as extremely atypical examples like penguins and ostriches, which while belonging to the category seem like subjectively \u201catypical\u201d examples. Visual images can also seem atypical, in that they exhibit features that depart in some way from what is typical for the categories to which they belong. Humans learn object categories and form their visual biases by looking at typical samples [Sloman, 1993; Rips, 1975]. But they are able to recognize atypical/abnormal objects which show significant visual variations from training set, without even observing them at the learning stage.\nFrom computer vision and machine learning perspectives, state-of-the-art object classification and detection is based on discriminative models (e.g. SVM, CNN, Boosting) rather than generative ones. Discriminative training focuses more on learning boundaries between object classes, instead of finding common characteristics in each class. Training CNN models is based on minimization of a loss function, defined as the misclassification of training samples. In that sense CNN implicitly emphasizes on the boundary between classes. However, training samples are not weighted based on how typical they look like, or equivalently how representative they are for a given category.\nIn this work, we hypothesize that not all images are equally important for the purpose of training visual classifiers, and in particular deep convolutional neural networks. Instead, we show that if training images are weighted based on how typical they look, we can learn visual classifiers with a better generalization capacity. Our final CNN model is fine-tuned\nonly with typical images, but outperforms the baseline model (training samples are not weighted) on dataset of atypical images. We also empirically compare a large set of functions that can be used for weighting samples, and conclude that an even-degree polynomial function of typicality ratings is the best strategy to weight training images. We also investigate the effect of loss functions and depth of network by conducting experiments on two datasets of ImageNet and PASCAL.\nThe main contributions of this paper are as following: \u2022 Evaluating CNN models on datasets of images that are\ndifferent from training data, and characterizing failure cases as the poor generalization capacity of CNN models. Especially contrasting these failures to the superior performance of humans in categorizing atypical objects. \u2022 Inspired by theories in psychology and machine learn-\ning, we propose three hypotheses to improve the generalization capacity of CNN models. These hypotheses are based on weighting train images depending on how typical they look. \u2022 We conduct an extensive set of experiments, to empiri-\ncally compare different functions of typicality rating for weighting training images."}, {"heading": "2 Related Work", "text": "Space does not allow an encyclopedic review of the prior literature on deep learning, but we refer interested readers to literature reviews of [LeCun et al., 2015]. For our research, we focus on convolutional neural networks [Fukushima, 2013; Krizhevsky et al., 2012; LeCun et al., 1998] as the state-ofthe-art deep learning model for the task of object recognition. CNN [LeCun et al., 1989] has its roots in Neocognitron [Fukushima, 1980], which is a hierarchical model based on the classic notion of simple and complex cells in visual neuroscience [Hubel and Wiesel, 1962]. However, CNN has additional hidden layers to model more complex nonlinearities in visual data and its overall architecture is reminiscent of the LGN 7\u2192 V1 7\u2192 V2 7\u2192 V4 7\u2192 IT hierarchy in the visual cortex ventral pathway. Additionally it uses an end-toend supervised learning algorithm, called \u201cBackpropagation\u201d to learn weights of layers. Different variations of CNN models have made breakthrough performance improvements in a variety of tasks in the field of computer vision.\nDespite an extensive amount of prior works on application of CNN and proposed variations of it, theoretical understanding of them remains limited. More importantly, even when CNN models achieve human-level performance on visual recognition tasks [He et al., 2015], what will be the difference between computer and human vision? On the one hand, Szegedy et al.[2013] demonstrated that CNN classification can be severely altered by very small changes to images, leading to radically different CNN classification of images that are indistinguishable to the human visual system. On the other hand, Nguyen et al.[2015] generated images that are completely unrecognizable by humans, but which a CNN model would classify them with 99.99% confidence. This strategy to fool CNN models, raises questions about the true generalization capabilities of such models, which we investigate it in this paper.\nIn addition, recent studies in the field of neuroscience and cognition have shown the connection between deep neural networks (mainly CNN) and the visual system in human brain. Yamins et al.[2014] have shown correlation (similarity) between activation of middle layers of CNN and brain responses in both V4 and inferior temporal (IT), the top two layers of the ventral visual hierarchy. Cadieu et al.[2014] proposed a kernel analysis approach to show that deep neural networks rival the representational performance of IT cortex on visual recognition tasks. Khaligh-Razavi and Kriegeskorte[2014] studied 37 computational model representations and found that the CNN model of [Krizhevsky et al., 2012] came the closest to explaining the brain representation. Interestingly, the amount of correlation between human IT and layers of CNN increases by moving to higher layers (fully-connected layers). They concluded that weighted combination of features of the last fully connected layer can explain IT to a full extent. Also it has been shown that CNN models predict human brain activity accurately in early and intermediate stages of visual pathway [Agrawal et al., 2014].\nThere are some prior works on finding the right features [Blum and Langley, 1997], choosing the appropriate train set and how to order training examples for learning better classifiers [Bengio et al., 2009]. Also, It has been shown that CNN models benefit from training with larger datasets of images. This is because the greatest gains in detection perfor-\nmance will continue to derive from improved representations and learning algorithms that can make efficient use of large datasets [Zhu et al., 2015]. However, this leaves open the question of how the visual classifier should weight all training images, equally or not?"}, {"heading": "3 Computational Framework", "text": "In this section we first go through theoretical background and compelling theories about learning visual concepts in both fields of psychology and computer vision. We explain how one can measure typicality of objects in an image. Then we propose three hypotheses to use these typicality scores to improve generalization capacity of visual classifiers."}, {"heading": "3.1 Framework Motivation", "text": "Humans learn a visual object class by looking at examples that are more representative for that object category, or what is called typical samples [Sloman, 1993; Rips, 1975]. It has been shown that children who learn a category by looking at more typical samples, later can recognize its members better [Rosch, 1978]. If training examples look more typical, they fall close to each other in an underlying space of visual features. This learning strategy not only helps humans to form a concept, but also allows them to more easily apply the concept to novel images. This great ability of human visual system allows them to recognize completely different variations of an object, even to the extent of atypical objects. This suggests that emphasizing typical examples might be helpful in improving the generalization ability of classifiers.\nHowever, state-of-the-art object classifiers in computer vision are discriminative models, where they distinguish different objects by learning category boundaries. CNN models as discriminative deep neural networks have multiple layers to learn a hierarchy of visual features and categorize objects based on loss minimization as a function of misclassification errors. In other words, if an image is classified correctly (usually the case for typical images), it has little or no impact on the loss function, hence can be ignored in training. This implies that examples close to the decision boundary, which are likely to be more atypical images, play a substantial role in learning CNN models. This suggests training CNN emphasize on more atypical images to learn visual classifiers with a better performance. Figure 2 illustrates the concept of typicality where examples of two classes (C) are shown with diamonds and crosses, and the red dotted line is one possible decision boundary. There are two main points to be taken from this illustration:\nFirst, as we discussed in Section1 typicality is a graded concept, which directly relates to the likelihood of an observation given its class distributionP(X|C). Very typical examples are expected to be located close to the mean of each class distribution (center of clouds), with a high probability [Feldman, 2000]. Moreover, as we move away form the center, we still observe examples of the same category. But every member of the category shows a different rate of typicality P(X|C). This is visualized as a smooth transition when moving away from the center of a class. More importantly there is no clear boundary between typical and atypical members.\nSecond, atypicality happens for a variety of reasons. This is visualized as there is not a unique axis for transition from darker to brighter shades of gray. Although examples close to the decision boundary might be atypical for their category; but the atypical examples are more diverse and not limited to the boundary examples. In conclusion, the two sets of atypical and boundary examples are not equal."}, {"heading": "3.2 Sample-Based Weighted Loss", "text": "CNN architecture consists of multiple blocks, where each block has convolution layer followed by pooling and normalization layers. On the top of these blocks, there are fully connected layers that are designed to learn more complex structures of object categories. The last layer of CNN computes the \u201closs\u201d as a function of mismatch between model prediction and ground truth label. The training of CNN is formulated as minimization of this loss function [LeCun et al., 1989]. However, our work is the first study to analyze the effect of weighting samples and using different loss functions incorporating in typicality scores, for improving generalization of CNN. We associate each sample X with a weight \u03c4 as a function of its typicality, which we explain later. We build our models by weighting samples based on two loss functions: Softmax log and Multi-class structured hinge. While the first one is the fastest and widely used in prior works, the later is more suitable for our purpose.\nSoftmax log loss: For classification problems using deep learning techniques, it is common to use the softmax of one of the C encodings at the top layer of the network, where C is the number of classes. Assuming the output to the i-th node in the last layer, for the image X is: zi(X ). Then our goal is to minimize the weighted multinomial logistic loss (L) of its softmax over N training images :\nL = \u2211 n \u2212\u03c4(Xn) \u2217 log(\u03c3i(Xn)) (n = 1, ..., N)\n\u03c3i(Xn) = exp(zi(Xn))/ \u2211 j exp(zj(Xn)), (i, j = 1, ..., C).\nMulti-class structured hinge loss: It is also known as the Crammer-Singh loss, and widely used for the problem of structured prediction. This loss function is similar to hinge-loss, but it is computed based on the margin between score of the desired category and all other prediction scores ( \u03c6(i)) [Crammer and Singer, 2002]. We aggregate this loss function (L) by a weighted summation over training samples:\nL = \u2211 n \u03c4(Xn) \u2217max(0, 1\u2212 \u03c6i(Xn))\n\u03c6i(Xn) = zi(Xn)\u2212maxi 6=j(zj(Xn)). Multi-class hinge loss is particularly of our interest as it considers the margin between all class predictions. This is an importance piece of information when we want to generalize our visual classifiers to the case of atypical objects. These examples are harder to be classified, and as a result the class prediction is not a distribution with its peak around the desired class. In fact, the object might get high class confidence\nfor multiple categories. This results in a smaller \u03c6 and bigger L. Consequently, this loss would implicitly favor atypical examples as they generate larger losses."}, {"heading": "3.3 Measuring Typicality of Objects", "text": "We have two approaches for measuring the typicality of objects. On the one hand, we compute the probability score P(T |X ) as how typical (T ) is the object only based on its visual features X . For the case of class-specific typicality we can infer: P(T |X ) \u221d P(X|C) where C indicates the category, and independent of the class: P(T |X ) \u221d P(X ). Then its complement (1\u2212P(T |X )) is the probability of atypicality.\nTo implement this probability, we use one-class SVM where only positive samples of one category (here typical images) are used and there is no negative (atypical) training example. This model can be understood as a density estimation model where there is no prior knowledge about the family of the underlying distribution. We learn this one-class SVM in two scenarios: 1) General class-independent typicality: all images are used; 2) Class-specific typicality: for each category one SVM is trained only based on typical images of the category of interest. We refer to these models as \u201cexternal score of typicality\u201d. This is because these scores are computed using a model distinct from CNN, and based on visual features different from what we use for object categorization. These scores are computed offline for all training images and not changing over different epochs of CNN training.\nOn the other hand, we can judge typicality of training images directly from the output of CNN visual classifiers. Lake et al.[2015] showed that the output of the last layer of CNN models can be used as a signal for how typical an input image looks like. In other words, typicality ratings are proportional to the strength of the classification response to the category of interest. Assuming loss is defined over C object categories and there are N nodes in the last layer, we compute \u201cinternal probability of typicality\u201d as:\nZi = exp(yi)/ C\u2211 j=1 exp(yj); where : yj = N\u2211 i=1 xiWij (1)\nAlternatively, we use the entropy of a category prediction as a measure of uncertainty in responses, which punishes more uncertain classifications. We call this \u201cinternal entropy of typicality\u201d and compute it as : \u2212Zilog(Zi)."}, {"heading": "3.4 Hypotheses", "text": "We propose three hypotheses to improve generalization of visual classifiers, especially when the test image looks substantially different from training images (atypical).\nFirst, Inspired by the prototype theories from psychology, we hypothesize that learning with more emphasis towards representative (typical) samples would increase the generalization capacity of the visual classifier.\nSecond, Learning with emphasis on more atypical examples in the training set would enhance the generalization capacity. This is because it complements the way that loss function emphasizes boundary examples. This hypothesis, places additional emphasis on other possible directions of atypicality in training data that might not be on the boundary.\nThird, We hypothesize that emphasizing both typical and atypical examples might be the key for a better generalization performance, and should be used for learning visual classifiers. The main idea behind this hypothesis is the fact that any visual classifier should learn how the object category is formed (mainly typical examples), and how much a variation it would allow for its members (atypical samples).\nTo implement the first two hypotheses we multiply the loss of each sample by \u03c4(X ), which is a function of typicality (for the first hypothesis) or atypicality (second hypothesis). To investigate the effect of different functions of typicality score, we evaluate exponential (expP(T |X )) and gamma (\u03b3P(T |X )) functions to emphasize typicality versus a logarithmic function (\u2212 log(P(T |X ))) to emphasize atypicality. This helps us to evaluate the generalization capacity of a CNN model, when trained with non-linear weighting. We evaluate our last hypothesis by implementing the weighting function as an even-degree polynomial:\nF(T ) = \u03b1(T \u2212 \u00b5)d + \u03b2; d = 2k(k = 1, ..., n) (2)\nThese functions are symmetric around the average typicality score in the dataset (\u00b5), and place emphasis on data points in both extremes of the typicality axis."}, {"heading": "4 Experimental Results", "text": "Datasets: We used three image datasets: 1) ImageNet challenge (ILSVRC 2012 & 2015), 2) Abnormal Object Dataset [Saleh et al., 2013], 3) PASCAL VOC 2011 train and validation set. We conducted our experiments with six object categories: Aeroplane, Boat, Car, Chair, Motorbike and Sofa. We did this to be able to verify our generalization enhancement for atypical images in Abnormal Objects dataset, which contains these categories. We merged related synsets of ILSVRC 2012 to get images of these categories, resulting in 16153 images, which we refer to as \u201ctrain set I\u201d.\nAdditionally, we experimented with train and validation set of PASCAL 2011. This is needed because due to a higher level of supervision in PASCAL data collection process, images are more likely to look typical. However, ImageNet data shows significant variations in terms of visual appearance (pose, missing or occluded parts, etc.) that can make the image and object look less typical. We collected 4950 images from PASCAL dataset, which we refer to as \u201ctrain set II\u201d. We also used a subset of 8570 images from ILSVRC 2015 detection challenge, which we call \u201ctest typical\u201d, and are completely disjoint from the set used in training. Images of [Saleh et al., 2013] form our \u201ctest atypical\u201d set, which represents confirmed atypical/abnormal objects.\nTypicality estimation: We measured the typicality of images via one-class SVMs in two settings: General and Class-specific. The first case is independent of the objectcategory and only measures how typical the input image looks in general. But, for the latter we trained six (one for each category) one-class SVMs with typical images of the category of interest. We extracted kernel descriptors of [Bo et al., 2010] at three scales as the input features.\nVisual classifier: We evaluated our three hypotheses with the CNN model of AlexNet [Krizhevsky et al., 2012]. Nevertheless, our approach can be used for other state-of-theart CNN models of object classification as well. We acquired the implementation of Caffe [Jia et al., 2014] for AlexNet and fine-tuned the network for all the following experiments."}, {"heading": "4.1 Comparison of Loss Functions", "text": "In order to find the proper loss function in fine-tuning the network, we conducted an experiment with two losses: Softmax and Multi-structured hinge (MS-Hinge) loss. For this experiment we only fine-tuned the last fully-connected layer with \u201cTrain Set I\u201d. Table 2 shows the performance comparison based on using different loss functions and weighting methods. We conclude that independent of the weighting strategy, Multi-structured hinge loss (MS-Hinge) performs better than the Softmax loss. Consequently, the rest of experiments were conducted based on fine-tuning with MS-hinge loss."}, {"heading": "4.2 Comparison of Weighting Functions", "text": "We conducted a set of experiments to compare the performance of CNN models for the task of object classification, when fine-tuned using different weighting functions. Table 3 shows the result of these experiments on the two test sets of Typical and Atypical. We report the mean accuracy after the first and tenth epochs. While the result of the first epoch indicates how fast the network can learn a category, the tenth epoch elaborates the performance when the network has matured (trained for a longer time).\nExternal score of typicality: The first box in Table 3 shows baseline experiments, when the first row is the AlexNet\nfine-tuned without any weighting. Second row shows weighting training images with a random number between zero and one. The result shows that randomly weighting training data does not have any effect on improving the generalization performance of the trained network. Next box shows the results of using the typicality and atypicality probabilities. We conclude fine-tuning with raw atypicality/typicality weighting can significantly enhance the generalization of CNN, even after the first epoch. However, fine-tuning with raw typicality can degrade the performance, when tested on typical images. The third box has similar results, where typicality or atypicality are computed based on class-specific one-class SVMs.\nFourth box in Table 3 investigates the importance of nonlinear weighting functions. First and second row are the results of using logarithmic functions, where \u03c4() is either typicality score (first row) or class-specific atypicality scores (second row). We conclude that networks do not gain much from non-linear functions of either typicality or atypicality scores, when test on atypical images. But non-linearities help stabilizing the performance on typical images. The last row of the fourth box, indicates that fine-tuning AlexNet with the memorability score [Khosla et al., 2015] will increase its generalization performance (comparing to baselines). However, fine-tuning with memorability do not outperform typicality weightings. The fifth box in Table 3 evaluates our third hypothesis, where three polynomials are used for weighting the training samples. In general, this strategy outperforms other methods (comparing the tenth epoch performance) on atypical test set, and do not degrade the performance on typical set (compare to baseline).\nInternal score of typicality: The last box in Table 3 have the classification performance when networks are finetuned with an internal signal of typicality. These scores can be either normalized class predictions, or what we call \u201c internal probability of typicality\u201d as it is in the first row; Or \u201cinternal entropy of class distribution\u201d in the second row. The last experiment (row) follows a hybrid approach, which in the first epoch samples are weighted with atypicality scores (from one-class SVM), and starting the second epoch, samples are weighted with internal scores.\nExperiment with fine-tuning on PASCAL: In Table 4 we recompile previous experiments when networks were finetuned on \u201cTrain Set II\u201d (PASCAL images). These results verify our hypothesis that we can help the generalization of CNN\nwith weighting training examples based on functions of typicality scores. Interestingly, the performance gain after the first epoch is higher when fine-tuned on PASCAL, rather than ImageNet . We relate this to the bigger diversity in visual appearance on ImageNet collection."}, {"heading": "4.3 Investigation of The Effect of Depth", "text": "We investigated the importance of fine-tuning deeper layers of CNN, to train models with a better generalization capacity. Table 5 shows the results of fine-tuning top-two or topthree fully connected layers of AlexNet. In the first row of each box, we changed the FC7 to have 2048 nodes. Similarly in the second row of each box, we halved the number of nodes in both FC6 and FC7. In all three models (including one reported in previous sections), we used MS-hinge loss to learn the parameters of the network. These experiments show that going deeper can hurt the fine-tuned network, especially when tested on atypical images. We would partially relate this to the limited number of images that are available for fine-tuning, therefore the network overfits to the training date. Digging deeper into this experiment with more training examples is considered as the future work."}, {"heading": "5 Conclusion", "text": "There are several points that we can conclude from this study. Atypicality is not necessary equivalent to samples on the boundary, which typical loss functions try to emphasize in learning. The main result of this paper is that involving information about the typicality/atypicality of training samples as a weighting term in the loss function helps greatly in enhancing the performance on unseen atypical examples when training only using typical examples. We propose different ways to achieve this weighting of samples based on external (from the sample distribution) and internal signals to the network. We also found that symmetrically weighting highly typical and highly atypical examples in training gives better generalization performance. We believe that this is because the typicality/atypicality scoring of the data include information about the distribution of the samples, and therefore it incorporates in generative \u201chints\u201d to the discriminative classifier. The typicality weighting not only helps the generalization, but also helps faster learning where the network was shown to converge to significantly better results after a single epoch."}], "references": [{"title": "Pixels to voxels: Modeling visual representation in the human brain", "author": ["Pulkit Agrawal", "Dustin Stansbury", "Jitendra Malik", "Jack L Gallant"], "venue": "arXiv preprint arXiv:1407.5104,", "citeRegEx": "Agrawal et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Journal of mathematical psychology", "author": ["F Gregory Ashby", "Leola A Alfonso-Reese. Categorization as probability density estimation"], "venue": "39(2):216\u2013233,", "citeRegEx": "Ashby and Alfonso.Reese. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "In Proceedings of the 26th annual international conference on machine learning", "author": ["Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Ronan Collobert", "Jason Weston. Curriculum learning"], "venue": "pages 41\u201348. ACM,", "citeRegEx": "Bengio et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Artificial intelligence", "author": ["Avrim L Blum", "Pat Langley. Selection of relevant features", "examples in machine learning"], "venue": "97(1):245\u2013271,", "citeRegEx": "Blum and Langley. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Liefeng Bo", "Xiaofeng Ren", "Dieter Fox. Kernel descriptors for visual recognition"], "venue": "pages 244\u2013252,", "citeRegEx": "Bo et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "PLoS computational biology", "author": ["Charles F Cadieu", "Ha Hong", "Daniel LK Yamins", "Nicolas Pinto", "Diego Ardila", "Ethan A Solomon", "Najib J Majaj", "James J DiCarlo. Deep neural networks rival the representation of primate it cortex for core visual object recognition"], "venue": "10(12),", "citeRegEx": "Cadieu et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The Journal of Machine Learning Research", "author": ["Koby Crammer", "Yoram Singer. On the algorithmic implementation of multiclass kernel-based vector machines"], "venue": "2:265\u2013292,", "citeRegEx": "Crammer and Singer. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE,", "citeRegEx": "Deng et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Journal of Experimental Psychology: Human Perception and Performance", "author": ["Jacob Feldman. Bias toward regular form in mental shape spaces"], "venue": "26(1):152,", "citeRegEx": "Feldman. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "Neocognitron: A selforganizing neural network model for a mechanism of pattern recognition unaffected by shift in position", "author": ["Kunihiko Fukushima"], "venue": "Biological cybernetics, 36(4):193\u2013202,", "citeRegEx": "Fukushima. 1980", "shortCiteRegEx": null, "year": 1980}, {"title": "Artificial vision by multi-layered neural networks: Neocognitron and its advances", "author": ["Kunihiko Fukushima"], "venue": "Neural Networks, 37:103\u2013119,", "citeRegEx": "Fukushima. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Feedforward object-vision models only tolerate small image variations compared to human", "author": ["Masoud Ghodrati", "Amirhossein Farzmahdi", "Karim Rajaei", "Reza Ebrahimpour", "Seyed-Mahdi KhalighRazavi"], "venue": "Frontiers in computational neuroscience,", "citeRegEx": "Ghodrati et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Jitendra Malik", "author": ["Georgia Gkioxari", "Ross Girshick"], "venue": "Contextual action recognition with r*cnn.", "citeRegEx": "Gkioxari et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Cognitive Science", "author": ["Noah D Goodman", "Joshua B Tenenbaum", "Jacob Feldman", "Thomas L Griffiths. A rational analysis of rule-based concept learning"], "venue": "32(1):108\u2013154,", "citeRegEx": "Goodman et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1502.01852,", "citeRegEx": "He et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Receptive fields, binocular interaction and functional architecture", "author": ["Hubel", "Wiesel", "1962] David H Hubel", "Torsten N Wiesel"], "venue": null, "citeRegEx": "Hubel et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Hubel et al\\.", "year": 1962}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "but not unsupervised", "author": ["Seyed-Mahdi KhalighRazavi", "Nikolaus Kriegeskorte. Deep supervised"], "venue": "models may explain it cortical representation.", "citeRegEx": "Khaligh.Razavi and Kriegeskorte. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "and Aude Oliva", "author": ["Aditya Khosla", "Akhil Raju S.", "Antonio Torralba"], "venue": "Understanding and predicting image memorability at a large scale.", "citeRegEx": "Khosla et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Advances in neural information processing systems", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks"], "venue": "pages 1097\u20131105,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Todd M Gureckis", "author": ["Brenden M Lake", "Wojciech Zaremba", "Rob Fergus"], "venue": "Deep neural networks predict category typicality ratings for images.", "citeRegEx": "Lake et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural computation", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition"], "venue": "1(4):541\u2013551,", "citeRegEx": "LeCun et al.. 1989", "shortCiteRegEx": null, "year": 1989}, {"title": "Proceedings of the IEEE", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner. Gradient-based learning applied to document recognition"], "venue": "86(11):2278\u20132324,", "citeRegEx": "LeCun et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Nature", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton. Deep learning"], "venue": "521(7553):436\u2013444,", "citeRegEx": "LeCun et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Bilinear cnn models for fine-grained visual recognition", "author": ["Tsung-Yu Lin", "Aruni RoyChowdhury", "Subhransu Maji"], "venue": "International Conference on Computer Vision (ICCV),", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Prototypes in category learning: the effects of category size", "author": ["John Paul Minda", "J David Smith"], "venue": "category structure, and stimulus complexity. Journal of Experimental Psychology: Learning, Memory, and Cognition, 27(3):775,", "citeRegEx": "Minda and Smith. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "The Big Book of Concepts (Bradford Books)", "author": ["G.L. Murphy"], "venue": "The MIT Press, March", "citeRegEx": "Murphy. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "and Jeff Clune", "author": ["Anh Nguyen", "Jason Yosinski"], "venue": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.", "citeRegEx": "Nguyen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "similarity", "author": ["Robert M Nosofsky. Choice"], "venue": "and the context theory of classification. Journal of Experimental Psychology: Learning, memory, and cognition, 10(1):104,", "citeRegEx": "Nosofsky. 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "and James J DiCarlo", "author": ["Nicolas Pinto", "David D Cox"], "venue": "Why is real-world visual object recognition hard?", "citeRegEx": "Pinto et al.. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": "arXiv preprint arXiv:1506.01497,", "citeRegEx": "Ren et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Inductive judgments about natural categories", "author": ["L.J. Rips"], "venue": "Journal of verbal learning and verbal behavior, 14:665\u2013 681", "citeRegEx": "Rips. 1975", "shortCiteRegEx": null, "year": 1975}, {"title": "Principles of categorization", "author": ["E. Rosch"], "venue": "E. Rosch and B. Lloyd, editors, Cognition and categorization. Lawrence Erlbaum", "citeRegEx": "Rosch. 1978", "shortCiteRegEx": null, "year": 1978}, {"title": "In Conference on Computer Vision and Pattern Recognition (CVPR)", "author": ["Babak Saleh", "Ali Farhadi", "Ahmed Elgammal. Object-centric anomaly detection by attribute-based reasoning"], "venue": "IEEE,", "citeRegEx": "Saleh et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Overfeat: Integrated recognition", "author": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "venue": "localization and detection using convolutional networks. In International Conference on Learning Representations (ICLR 2014), page 16. CBLS,", "citeRegEx": "Sermanet et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "May 7-9", "author": ["Ali Sharif Razavian", "Josephine Sullivan", "Atsuto Maki", "Stefan Carlsson. A baseline for visual instance retrieval with deep convolutional networks. In International Conference on Learning Representations"], "venue": "2015, San Diego, CA. ICLR,", "citeRegEx": "Sharif Razavian et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman", "2015] Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Feature-based induction", "author": ["S.A. Sloman"], "venue": "Cognitive Psychology, 25:231\u2013280", "citeRegEx": "Sloman. 1993", "shortCiteRegEx": null, "year": 1993}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["Antonio Torralba", "Alexei Efros"], "venue": "Unbiased look at dataset bias. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1521\u2013 1528. IEEE,", "citeRegEx": "Torralba et al.. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "arXiv preprint arXiv:1411.4555,", "citeRegEx": "Vinyals et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Proceedings of the National Academy of Sciences", "author": ["Daniel LK Yamins", "Ha Hong", "Charles F Cadieu", "Ethan A Solomon", "Darren Seibert", "James J DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex"], "venue": "111(23):8619\u20138624,", "citeRegEx": "Yamins et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Object detectors emerge in deep scene cnns", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "venue": "arXiv preprint arXiv:1412.6856,", "citeRegEx": "Zhou et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Do we need more training data? International Journal of Computer Vision", "author": ["Xiangxin Zhu", "Carl Vondrick", "Charless C Fowlkes", "Deva Ramanan"], "venue": "pages 1\u201317,", "citeRegEx": "Zhu et al.. 2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "To just name few of the recent advances, CNN-based models greatly improved the performance in object classification [Simonyan and Zisserman, 2015], object detection [Ren et al., 2015], image retrieval [Sharif Razavian et al.", "startOffset": 165, "endOffset": 183}, {"referenceID": 35, "context": ", 2015], image retrieval [Sharif Razavian et al., 2015], fine-grained recognition [Lin et al.", "startOffset": 25, "endOffset": 55}, {"referenceID": 24, "context": ", 2015], fine-grained recognition [Lin et al., 2015], scene classification [Zhou et al.", "startOffset": 34, "endOffset": 52}, {"referenceID": 42, "context": ", 2015], scene classification [Zhou et al., 2014], action classification [Gkioxari et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 12, "context": ", 2014], action classification [Gkioxari et al., 2015] and generating image descriptions [Vinyals et al.", "startOffset": 31, "endOffset": 54}, {"referenceID": 40, "context": ", 2015] and generating image descriptions [Vinyals et al., 2014].", "startOffset": 42, "endOffset": 64}, {"referenceID": 14, "context": "Despite surpassing the human categorization performance on large-scale visual object datasets [He et al., 2015], convolution neural networks cannot emulate the generalization power of the human visual system in real-world object categorization [Ghodrati et al.", "startOffset": 94, "endOffset": 111}, {"referenceID": 11, "context": ", 2015], convolution neural networks cannot emulate the generalization power of the human visual system in real-world object categorization [Ghodrati et al., 2014; Pinto et al., 2008], especially when it comes to objects that differ substantially from the training examples.", "startOffset": 140, "endOffset": 183}, {"referenceID": 29, "context": ", 2015], convolution neural networks cannot emulate the generalization power of the human visual system in real-world object categorization [Ghodrati et al., 2014; Pinto et al., 2008], especially when it comes to objects that differ substantially from the training examples.", "startOffset": 140, "endOffset": 183}, {"referenceID": 19, "context": "AlexNet [Krizhevsky et al., 2012] 38.", "startOffset": 8, "endOffset": 33}, {"referenceID": 34, "context": "07 OverFeat [Sermanet et al., 2013] 35.", "startOffset": 12, "endOffset": 35}, {"referenceID": 16, "context": "73 Caffe [Jia et al., 2014] 39.", "startOffset": 9, "endOffset": 27}, {"referenceID": 33, "context": "Humans are capable of perceiving atypical objects and reasoning about them, even though they had not seen them before [Saleh et al., 2013].", "startOffset": 118, "endOffset": 138}, {"referenceID": 39, "context": "One might argue that this issue of cross-dataset generalization is implicitly rooted in dataset biases, and not limited to CNN models [Torralba et al., 2011].", "startOffset": 134, "endOffset": 157}, {"referenceID": 7, "context": "We support our argument by testing same networks on a new set of images that are disjoint from the training set of ImageNet [Deng et al., 2009], but look typical.", "startOffset": 124, "endOffset": 143}, {"referenceID": 26, "context": "Humans begin to form categories and abstractions at an early age[Murphy, 2002].", "startOffset": 64, "endOffset": 78}, {"referenceID": 25, "context": "The mechanisms underlying human category formation are the subject of many competing accounts, including those based on prototypes[Minda and Smith, 2001], exemplars[Nosofsky, 1984], density estimation[Ashby and Alfonso-Reese, 1995], and Bayesian inference[Goodman et al.", "startOffset": 130, "endOffset": 153}, {"referenceID": 28, "context": "The mechanisms underlying human category formation are the subject of many competing accounts, including those based on prototypes[Minda and Smith, 2001], exemplars[Nosofsky, 1984], density estimation[Ashby and Alfonso-Reese, 1995], and Bayesian inference[Goodman et al.", "startOffset": 164, "endOffset": 180}, {"referenceID": 1, "context": "The mechanisms underlying human category formation are the subject of many competing accounts, including those based on prototypes[Minda and Smith, 2001], exemplars[Nosofsky, 1984], density estimation[Ashby and Alfonso-Reese, 1995], and Bayesian inference[Goodman et al.", "startOffset": 200, "endOffset": 231}, {"referenceID": 13, "context": "The mechanisms underlying human category formation are the subject of many competing accounts, including those based on prototypes[Minda and Smith, 2001], exemplars[Nosofsky, 1984], density estimation[Ashby and Alfonso-Reese, 1995], and Bayesian inference[Goodman et al., 2008].", "startOffset": 255, "endOffset": 277}, {"referenceID": 37, "context": "Humans learn object categories and form their visual biases by looking at typical samples [Sloman, 1993; Rips, 1975].", "startOffset": 90, "endOffset": 116}, {"referenceID": 31, "context": "Humans learn object categories and form their visual biases by looking at typical samples [Sloman, 1993; Rips, 1975].", "startOffset": 90, "endOffset": 116}, {"referenceID": 23, "context": "Space does not allow an encyclopedic review of the prior literature on deep learning, but we refer interested readers to literature reviews of [LeCun et al., 2015].", "startOffset": 143, "endOffset": 163}, {"referenceID": 10, "context": "For our research, we focus on convolutional neural networks [Fukushima, 2013; Krizhevsky et al., 2012; LeCun et al., 1998] as the state-ofthe-art deep learning model for the task of object recognition.", "startOffset": 60, "endOffset": 122}, {"referenceID": 19, "context": "For our research, we focus on convolutional neural networks [Fukushima, 2013; Krizhevsky et al., 2012; LeCun et al., 1998] as the state-ofthe-art deep learning model for the task of object recognition.", "startOffset": 60, "endOffset": 122}, {"referenceID": 22, "context": "For our research, we focus on convolutional neural networks [Fukushima, 2013; Krizhevsky et al., 2012; LeCun et al., 1998] as the state-ofthe-art deep learning model for the task of object recognition.", "startOffset": 60, "endOffset": 122}, {"referenceID": 21, "context": "CNN [LeCun et al., 1989] has its roots in Neocognitron [Fukushima, 1980], which is a hierarchical model based on the classic notion of simple and complex cells in visual neuroscience [Hubel and Wiesel, 1962].", "startOffset": 4, "endOffset": 24}, {"referenceID": 9, "context": ", 1989] has its roots in Neocognitron [Fukushima, 1980], which is a hierarchical model based on the classic notion of simple and complex cells in visual neuroscience [Hubel and Wiesel, 1962].", "startOffset": 38, "endOffset": 55}, {"referenceID": 14, "context": "More importantly, even when CNN models achieve human-level performance on visual recognition tasks [He et al., 2015], what will be the difference between computer and human vision? On the one hand, Szegedy et al.", "startOffset": 99, "endOffset": 116}, {"referenceID": 19, "context": "Khaligh-Razavi and Kriegeskorte[2014] studied 37 computational model representations and found that the CNN model of [Krizhevsky et al., 2012] came the closest to explaining the brain representation.", "startOffset": 117, "endOffset": 142}, {"referenceID": 0, "context": "Also it has been shown that CNN models predict human brain activity accurately in early and intermediate stages of visual pathway [Agrawal et al., 2014].", "startOffset": 130, "endOffset": 152}, {"referenceID": 3, "context": "There are some prior works on finding the right features [Blum and Langley, 1997], choosing the appropriate train set and how to order training examples for learning better classifiers [Bengio et al.", "startOffset": 57, "endOffset": 81}, {"referenceID": 2, "context": "There are some prior works on finding the right features [Blum and Langley, 1997], choosing the appropriate train set and how to order training examples for learning better classifiers [Bengio et al., 2009].", "startOffset": 185, "endOffset": 206}, {"referenceID": 43, "context": "This is because the greatest gains in detection performance will continue to derive from improved representations and learning algorithms that can make efficient use of large datasets [Zhu et al., 2015].", "startOffset": 184, "endOffset": 202}, {"referenceID": 37, "context": "Humans learn a visual object class by looking at examples that are more representative for that object category, or what is called typical samples [Sloman, 1993; Rips, 1975].", "startOffset": 147, "endOffset": 173}, {"referenceID": 31, "context": "Humans learn a visual object class by looking at examples that are more representative for that object category, or what is called typical samples [Sloman, 1993; Rips, 1975].", "startOffset": 147, "endOffset": 173}, {"referenceID": 32, "context": "It has been shown that children who learn a category by looking at more typical samples, later can recognize its members better [Rosch, 1978].", "startOffset": 128, "endOffset": 141}, {"referenceID": 8, "context": "Very typical examples are expected to be located close to the mean of each class distribution (center of clouds), with a high probability [Feldman, 2000].", "startOffset": 138, "endOffset": 153}, {"referenceID": 21, "context": "The training of CNN is formulated as minimization of this loss function [LeCun et al., 1989].", "startOffset": 72, "endOffset": 92}, {"referenceID": 6, "context": "This loss function is similar to hinge-loss, but it is computed based on the margin between score of the desired category and all other prediction scores ( \u03c6(i)) [Crammer and Singer, 2002].", "startOffset": 162, "endOffset": 188}, {"referenceID": 33, "context": "Datasets: We used three image datasets: 1) ImageNet challenge (ILSVRC 2012 & 2015), 2) Abnormal Object Dataset [Saleh et al., 2013], 3) PASCAL VOC 2011 train and validation set.", "startOffset": 111, "endOffset": 131}, {"referenceID": 33, "context": "Images of [Saleh et al., 2013] form our \u201ctest atypical\u201d set, which represents confirmed atypical/abnormal objects.", "startOffset": 10, "endOffset": 30}, {"referenceID": 4, "context": "We extracted kernel descriptors of [Bo et al., 2010] at three scales as the input features.", "startOffset": 35, "endOffset": 52}, {"referenceID": 19, "context": "Visual classifier: We evaluated our three hypotheses with the CNN model of AlexNet [Krizhevsky et al., 2012].", "startOffset": 83, "endOffset": 108}, {"referenceID": 16, "context": "We acquired the implementation of Caffe [Jia et al., 2014] for AlexNet and fine-tuned the network for all the following experiments.", "startOffset": 40, "endOffset": 58}, {"referenceID": 18, "context": "The last row of the fourth box, indicates that fine-tuning AlexNet with the memorability score [Khosla et al., 2015] will increase its generalization performance (comparing to baselines).", "startOffset": 95, "endOffset": 116}], "year": 2016, "abstractText": "Deep artificial neural networks have made remarkable progress in different tasks in the field of computer vision. However, the empirical analysis of these models and investigation of their failure cases has received attention recently. In this work, we show that deep learning models cannot generalize to atypical images that are substantially different from training images. This is in contrast to the superior generalization ability of the visual system in the human brain. We focus on Convolutional Neural Networks (CNN) as the state-of-the-art models in object recognition and classification; investigate this problem in more detail, and hypothesize that training CNN models suffer from unstructured loss minimization. We propose computational models to improve the generalization capacity of CNNs by considering how typical a training image looks like. By conducting an extensive set of experiments we show that involving a typicality measure can improve the classification results on a new set of images by a large margin. More importantly, this significant improvement is achieved without finetuning the CNN model on the target image set.", "creator": "LaTeX with hyperref package"}}}