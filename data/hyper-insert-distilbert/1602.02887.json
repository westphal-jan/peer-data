{"id": "1602.02887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2016", "title": "Classification with Boosting of Extreme Learning Machine Over Arbitrarily Partitioned Data", "abstract": "balanced machine domain learning based enhanced computational intelligence methods are again widely lately used to analyze inherently large weighted scale data sets in this age of emerging big data. extracting useful predictive modeling attributes from overlapping these types of sensor data sets is a most challenging problem due wholly to their high relative complexity. analyzing large amount loads of soft streaming data that can be leveraged to routinely derive business memory value quantities is another complex problem to independently solve. problems with maintaining high levels measurement of data availability ( \\ input textit { i. j e.. big data } ) automatic fuzzy classification of them has now become an therefore important and complex balancing task. now hence, we explore the power consumption of accurately applying semantic mapreduce simulation based remote distributed adaboosting of extreme learning machine ( elm ) behavior to build a streamlined predictive bag of classification program models. accordingly, ( i ) data lessons set ensembles are increasingly created ; ( - ii ) custom elm algorithm scaling is used to build weak learners ( classifier enhancement functions ) ; iv and ( iii ) builds a customized strong motivation learner from behind a lesson set full of weak learners. we currently applied this powerful training model matrix to shape the iso benchmark knowledge discovery and data mining data analysis sets.", "histories": [["v1", "Tue, 9 Feb 2016 08:09:26 GMT  (165kb)", "http://arxiv.org/abs/1602.02887v1", "Springer Soft Computing"]], "COMMENTS": "Springer Soft Computing", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ferhat \\\"ozg\\\"ur \\c{c}atak"], "accepted": false, "id": "1602.02887"}, "pdf": {"name": "1602.02887.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Ferhat \u00d6zg\u00fcr \u00c7atak"], "emails": ["ozgur.catak@tubitak.gov.tr"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n02 88\n7v 1\n[ cs\n.L G\n] 9\nF eb\n2 01\nKeywords Extreme Learning Machine \u00b7 AdaBoost \u00b7 Ensemble Methods \u00b7 MapReduce"}, {"heading": "1 Introduction", "text": "It is clear that there has been an unexpected increase in the quantity and variety of data generated worldwide by computers, mobile phones, and sensors. Just as computer technology evolved, the quantity and variety of data has also increased, becoming more focused on storing every type of data, the socalled Big Data. As the volume of data to build a predictive model increases,\nFerhat O\u0308zgu\u0308r C\u0327atak TU\u0308BI\u0307TAK BI\u0307LGEM, Cyber Security Institute Kocaeli, Turkey Tel.: +90-262-6481000 Fax: +90-262-6481100 E-mail: ozgur.catak@tubitak.gov.tr\nthe complexity of that training increases too. As a result, building actionable predictive modeling of a large scale unstructured data set is a definitive Big Data problem. Predictive learning models try to discover patterns of training data and label new data instances to the correct output value. To efficiently handle unstructured large scale big data sets, it is critical to develop new machine learning methods that combine several boosting and classification algorithms.\nExtreme Learning Machine (ELM) was proposed by [1] based on generalized Single-hidden Layer Feedforward Networks (SLFNs). Main characteristics of ELM are small training time compared to traditional gradient-based learning methods, high generalization property of predicting unseen examples with multi-class labels and parameter free with randomly generated hidden nodes. ELM algorithm is used in many different areas including document classification [2], bioinformatics [3] multimedia recognition [4,5].\nIn recent years, much computational intelligence research has been devoted to building predictive modeling of distributed and parallel frameworks. In this research, the proposed learning model creates data chunks with varying size and bag of classifier functions using ELM algorithm trained with these arbitrary chosen sub data set with AdaBoosting method for large scale predictions. By creating data chunks from the training data set using the MapReduce paradigm, each subset of the training data set is used to find out the set of ELM ensembles as a single global classifier function.\nThe main objective of this work is to train large scale data sets using ELM and AdaBoost. Another objective is to achieve the model\u2019s classification performance with same or close to the conventional ELM method. Conventional ELM training cannot be applied to large scale data sets on a single computer because of their complexity. Then experiments section is split into two subsections: \u201dcommonly used data sets\u201d in Section 5.1.1 and \u201dlarge scale data sets\u201d in Section 5.1.2. Commonly used data sets are suitable for training on a single computer with the conventional ELM algorithm. We trained these data sets both conventional and proposed methods to show the classification performance changes of the proposed method. Classification performance results are shown in Section 5.3.\nThe contributions of this paper are as follows:\n\u2013 A generative MapReduce technique based AdaBoosted ELM classification model is proposed for learning, and thus, faster classification model training is achieved. \u2013 This research proposes a new learning method for AdaBoosted ELM that achieves parallelization both in large scale data sets and reduced computational time of learning algorithm. \u2013 Training computations of working nodes are independent from each other thus minimizing the data communication. The other approaches, including Support Vector Machine training need data communication for the support vector exchange. [6,7,8].\nThe rest of the paper is organized as follows: Section 2 briefly introduces some of the earlier works related to our problem. Section 3 describes algorithm ELM, AdaBoost and MapReduce technique. Section 4 and Section 5 evaluates the proposed learning model. Section 6 concludes this paper."}, {"heading": "2 Related work", "text": "In this section, we describe the general overview of literature review. Section 2.1 describes the general distributed ELM methods. Section 2.2 shows the MapReduce based ELM training methods.\n2.1 Literature Review Overview\nMapReduce based learning algorithms from distributed data chunks has been studied by many researchers. Many different MapReduce based learning solutions over arbitrary partitioned data have been proposed recently. Some popular MapReduce based solutions to train machine learning algorithms in the literature include the following. Panda et al. proposed a learning tree model which is based on series of distributed computations, and implements each one using the MapReduce model of distributed computation [9]. Zhang et al. develops some algorithms using MapReduce to perform parallel data joins on large scale data sets [10]. Sun et al. use batch updating based hierarchical clustering to reduce computational time and data communication [11]. Their approach uses co-occurence based feature selection to remove noisy features and decrease the dimension of the feature vectors. He et al. proposed parallel density based clustering algorithm (DBSCAN). They developed a partitioning strategy for large scale non-indexed data with a 4-stages MapReduce paradigm [12]. Zhao et al. proposed parallel k-means clustering based on MapReduce [13]. Their approaches focus on implementing k-means with the read-only convergence heuristic in the MapReduce pattern.\n2.2 MapReduce Based ELM Training Methods\nSection 2.2.1 - Section 2.2.5 describe five different MapReduce training methods of ELM algorithm."}, {"heading": "2.2.1 ELM\u22c6", "text": "Xin et al. proposed MapReduce based ELM training method called as ELM\u2217 [14]. Main idea behind this method is to calculate matrix multiplication of ELM to find weight vector. They show that Moore-Penrose generalized inverse operator is the most expensive computation part of the algorithm. As we know, matrix multiplication can be divide into smaller part. Using this property, they\nproposed an efficient implementation of training phase to manage massive data sets. The final output of this method is a single classifier function. In this paper, they proposed two different versions of ELM\u2217, naive and improved. In naive-ELM\u2217, the algorithm has two classes, Class Mapper and Class Reducer. Both classes contain only one method. In improved ELM\u2217, they decompose the calculation of matrix multiplication using MapReduce framework. Moreover, the proposed algorithm decreases the computation and communication cost. In the experimental platform, they used their synthetic data sets to evaluate the performance of the proposed algorithms with MapReduce framework."}, {"heading": "2.2.2 OS-ELM based Classification in Hierarchical P2P Network", "text": "Sun et al. proposed OS-ELM [15] based distributed ensemble classification in P2P networks [16]. They apply the incremental learning principle of OSELM to hierarchical P2P network. They proposed two different versions of the ensemble classifier in hierarchical P2P, one-by-one ensemble classification and parallel ensemble classification. In one-by-one learning method, each peer, one by one, calculates the classifier with all the data. Therefore, this approach has a large network delay. In the parallel ensemble learning, all the classifiers are learnt from all the data in parallel manner. Conversely to ELM\u2217, their experimental results are based on three different real data sets downloaded from the UCI repository."}, {"heading": "2.2.3 Parallel online sequential ELM: POS-ELM", "text": "Wang et al. have been proposed parallel online sequential extreme learning machine (POS-ELM) method [17]. Main idea behind in this approach is to analyze the dependency relationships and the matrix calculations of OS-ELM [15]. Their experimental results are based on nine different real data sets downloaded from the UCI repository."}, {"heading": "2.2.4 Distributed and Kernelized ELM: DK-ELM", "text": "Bi et al. have been proposed both distributed and kernelized ELM (DK-ELM) based on MapReduce [18]. The difference between ELM and Kernelized ELM is that K-ELM applies kernels opposite to create random feature mappings. They provide a distributed implementation RBF kernel matrix calculation in massive data learning applications. Their experimental results are based on four different real data sets downloaded from the UCI repository and four synthetic data sets."}, {"heading": "2.2.5 ELM-MapReduce", "text": "Chen et al. have been proposed MapReduce based ELM ensemble classifier called ELM-MapReduce, for large scale land cover classification of remote sensing data [19]. Their approach contains two sequential phases: parallel training\nof multiple ELM classifiers and voting mechanism. In parallel training phase of proposed method, each Map function computes an ELM classifier with a given training data set. In second phase called voting mechanism, a new MapReduce job is executed with a new partitioned test set into each Map function with notation dataj . In Reduce function of this phase, each dataj is predicted with each ELM classifier trained in parallel training phase. Final classification predictions are the output of final Reduce function. Therefore, this approach has a high communication cost. Their experimental results are based synthetic remote sensing image of training data.\n2.3 The Differences Between Proposed Model and Literature Review\nThe main differences are:\n\u2013 In ELM\u22c6, they use matrix multiplication decomposition. Each Map function is responsible to calculate the Moore-Penrose generalized inverse operation. And their method produces one single classifier. In the proposed model in our paper, each Reduce function produces ensemble classifier based on AdaBoost method. The final output ensemble classifier is a voting based combination of ensemble classifier trained in each Reduce phase. \u2013 In OS-ELM based classification in hierarchical P2P Network, POS-ELM and DK-ELM, they propose ensemble classifier that combines multiple classifier trained with data chunks. Each peer classifier is learned from the local data. Therefore, each peer produces a single ELM classifier. In our method, each node (or peer) produces ensemble classifier to increase the classification accuracy. \u2013 In ELM-MapReduce, they propose ensemble classifier with two different MapReduce jobs. In first MpaReduce job, their approach produces a single ELM classifier in eachMap function. In second MapReduce job, the test set is partitioned into each Map function and produces final predicted labels based on the voting mechanism of ELM classifiers that are trained in the first MapReduce job. In our method, prediction is not included, our aim is to create a final ensemble classifier in only one MapReduce job.\nTable 1 shows the main differences of all proposed methods. There are five different columns that are ensemble methods, single pass MapReduce, matrix multiplication, entire data set and network communication. Ensemble column shows that the method builds a set of classifier function (i.e. ensemble model) to improve the accuracy performance of the final classification model. If an ensemble method is applied, then the performance of final model will have better accuracy result [20]. Single Pass MapReduce column shows that an iterative approach is not applied to the model. Entire learning phase is performed in a single pass of data through the job. Matrix Multiplication column shows the hidden layer matrix is calculated in each Map function. The hidden layer matrix computation is a compute intensive operation. Entire Data Set column shows each Map operation needs entire data set to build a final classifier model. Network Communication column shows that each MapReduce job\nneeds to communicate with another job. Network communication will affect negatively on training time of the algorithm."}, {"heading": "3 Preliminaries", "text": "In this section, we introduce preliminary knowledge of ELM, AdaBoost and MapReduce briefly.\n3.1 Extreme learning machine\nELM was originally proposed for the single-hidden layer feedforward neural networks [21,22,1] . Then, ELM was extended to the generalized single-hidden layer feedforward networks where the hidden layer may not be neuron like [23, 24]. The main advantages of the ELM classification algorithm are that ELM can be trained hundred times faster than traditional neural network or support vector machine algorithm since its input weights and hidden node biases are randomly created and output layer weights can be analytically calculated by using a least-squares method [25,26]. The most noticeable feature of ELM is that its hidden layer parameters are selected randomly.\nGiven a set of training data D = {(xi, yi) | i = 1, ..., n},xi \u2208 R p, yi \u2208 {1, 2, ...,K}} sampled independently and identically distributed (i.i.d.) from some unknown distribution. The goal of a neural network is to learn a function f : X \u2192 Y where X is instance and Y is the set of all possible labels. The output label of an single hidden-layer feedforward neural networks (SLFNs) with N hidden nodes can be described as\nfN (x) =\nN \u2211\ni=1\n\u03b2iG(ai, bi,x), x \u2208 R n, ai \u2208 R n (1)\nwhere ai and bi are the learning parameters of hidden nodes and \u03b2i is the weight connecting the ith hidden node to the output node.\nThe output function of ELM for generalized SLFNs can be identified by\nfN (x) =\nN \u2211\ni=1\n\u03b2iG(ai, bi,x) = \u03b2 \u00d7 h(x) (2)\nFor the binary classification applications, the decision function of ELM becomes\nfN(x) = sign\n(\nN \u2211\ni=1\n\u03b2iG(ai, bi,x)\n)\n= sign (\u03b2 \u00d7 h(x)) (3)\nEquation 2 can be written in another form as\nH\u03b2 = T (4)\nH and T are respectively hidden layer matrix and output matrix. Hidden layer matrix can be described as\nH(a\u0303, b\u0303, x\u0303) =\n\n  G(a1, b1, x1) \u00b7 \u00b7 \u00b7 G(aL, bL, x1) ... . . . ...\nG(a1, b1, xN ) \u00b7 \u00b7 \u00b7 G(aL, bL, xN )\n\n \nN\u00d7L\n(5)\nwhere a\u0303 = a1, ..., aL, b\u0303 = b1, ..., bL, x\u0303 = x1, ..., xN . Output matrix can be described as\nT = [ t1 . . . tN ]T\n(6)\nThe hidden nodes of SLFNs can be randomly generated. They can be independent of the training data.\n3.2 AdaBoost\nThe AdaBoost [27] is a supervised learning algorithm designed to solve classification problems [28]. The algorithm takes as input a training set (x1, y1), ..., (xn, yn) where the input sample xi \u2208 R\np, and the output value, yi, in a finite space y \u2208 1, ...K. AdaBoost algorithm assumes, like ELM, a set of training data sampled independently and identically distributed (i.i.d.) from some unknown distribution X .\nGiven a space of feature vectors X and two possible class labels, y \u2208 {\u22121,+1}, AdaBoost goal is to learn a strong classifier H(x) as a weighted ensemble of weak classifiers ht(x) predicting the label of any instance x \u2208 X [29].\nH(x) = sign(f(x)) = sign\n(\nT \u2211\nt=1\n\u03b1tht(x)\n)\n(7)\nPseudocode for AdaBoost is given in Alg. 1.\nAlgorithm 1 The AdaBoost algorithm.\n1: Inputs:\nD = {{(xi, yi) | i = 1, ..., n},xi \u2208 R p, yi \u2208 {\u22121,+1}} m i=1\n2: Initialize D(i) = 1 m\nfor all i 3: while t < T do 4: Train WeakLearner using distribution Dt 5: get back a weak hypothesis ht : X \u2192 {1, 2, ...,K} 6: calculate the error of ht : \u01ebt = Pri\u223cDt [ht(xi) 6= yi] 7: Sets \u03b1t = 1 2 ln 1\u2212\u01ebt \u01ebt\n8: update distribution Dt+1 = Dt Zt \u00d7\n{\ne\u2212\u03b1t , if ht = yi e\u03b1t , if ht 6= yi\n9: equivalently Dt+1 = Dt\u00d7exp(\u2212\u03b1tyiht(xi))\nZt where Zt is a normalization\nconstant. 10: end while 11: Outputs:\nfinal hypothesis h\u2217 = sign ( \u2211T t=1 \u03b1tht(x) )\n.\n3.3 MapReduce\nMapReduce is a new programming model to run parallel applications for large scale data sets processing to support data-intensive applications. It is derived from the map and reduce function combination from functional programming. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. The MapReduce was originally developed by Google and built on principles in parallel manner [30]. The MapReduce framework first takes the input, divides it into smaller data chunks, and distributes them to worker nodes. MapReduce is divided into three major phases called map, reduce and a separated internal shuffle phase. The MapReduce framework automatically executes all those functions in a parallel manner over any number of processors/servers [31].\nPseudo code of MapReduce framework is shown in Eq. 8.\nmap(key1, value1) \u2192 list(key2, value2)\nreduce(key2, list(value2)) \u2192 list(key3, value3) (8)\nMapreduce programming technique is widely used on different scientific fields, i.e. cyber-security [32,33], high energy physics [34], biology [35]."}, {"heading": "4 Proposed Approach", "text": "In this section we provide the details of the MapReduce based distributed AdaBoosted ELM algorithm. The basic idea of AdaBoost-ELM based on MapRe-\nduce technique is introduced in Section 4.1. The MapReduce implementation of AdaBoosted ELM is described in Section 4.3.\n4.1 Basic Idea\nOur main task is to parallel and distributed execute the computation of AdaBoosted ELM classification method. AdaBoosted ELM\u2019s basic idea is to calculate ensemble of classifier functions over partitioned data (Xm, Ym) in parallel manner. In Table 2, a summary of commonly used variables and notations to assess the classifier model performance of the AdaBoosted ELM method is given for convenience.\n4.2 Analysis of the proposed algorithm\nBarlett showed that the size of the weights is more important than the size of the neural network [36]. Kragh et al. also showed that ensemble methods of neural networks get better accuracy performance over unseen examples [37]. The main motivation of the this work is the idea that small size ELM ensembles can obtain more accurate classifier model that are comparable to individual classifiers.\nIn the proposed model, at every data chunk, there is a set of classifier functions that acts as a single classification model. The single model at every data chunk m is defined as follows:\nf (m)(x) = argmax k\nT \u2211\nt=1\n\u03b1tht(x) (9)\nThe selected ensemble ELM classifier models from the reduce phase of MapReduce algorithm are combined into one single classification model.\nh\u0302(x) = argmax k\nm \u2211\ni=1\nf (m)(x) (10)\n4.3 Implementation of the Model\nThe pseudocodes of MapReduce-based AdaBoost ELM are shown in Algorithm 2 and Algorithm 3. The Map procedure of our training model is implemented based on random assignment of each row of the training data set with split size of data, M , in line 2 of Algorithm 2. The input, x , is a row of traing data set D. Map procedure partition the input matrix by row, producing < randomSplitId,x > key-value pairs. randomSplitId is the identifier of the data chunk and is transferred as the input key to Reduce phase. The\nAlgorithm 2 AdaBoostedELM::Map\n1: Inputs: Training record (x, y) \u2208 D, Data set split size M 2: k \u2190 rand(0,M) 3: Output(k, (x, y))\npseudo code of Reduce phase is shown in Algorithm 3. Reduce procedure is implemented based on the for-loop of lines 3 - 8 of Algorithm 3. The output ELM classifier of sub data set (Xk,yk) is calculated using AdaBoost constantly block by block, so every reduce task completes training phase and outputs an AdaBoosted set of classifier functions. The mapper\u2019s input k is the randomSplitId to create the data chunk and created in the Map phase of our training model.\nAlgorithm 3 AdaBoostedELM::Reduce\n1: Inputs: Key k, Value Set V ,AdaBoost Iteration Size T 2: Split V into input space Xn and out labels yn with (Xn,yn) \u2190 V 3: for t = 1..T do 4: Train sub data set with ELM: ht \u2190 ELM(X, y) 5: ypred, \u01ebt \u2190 ht(X) 6: \u03b1t \u2190 1 2 ln 1\u2212\u01ebt \u01ebt 7: Dt+1 = Dt\u00d7exp(\u2212\u03b1tyiht(xi)) Zt 8: end for 9: Outputs:\nFinal hypothesis for the reduce function m : hm \u2190 argmaxk \u2211T t=1 \u03b1tht(x) ."}, {"heading": "5 Experiments", "text": "In this section, we perform experiments on real-world data sets from the public available data set repositories. Public data sets are used to evaluate the proposed learning method. Then, classification models of each data set are compared for accuracy results with the single instance of learning algorithm performance.\nIn Section 5.1 we explain the data sets and parameters that are used in experiments. The conventional ELM is applied all data sets and we find the accuracy performance over number of hidden nodes in Section 5.3. In Section 5.2, we show the empirical results of proposed distributed adaboost ELM training algorithm.\n5.1 Experimental setup\nIn this section we apply our approach to five different data sets to verify its effectivity and efficiency. To demonstrate the effectiveness and performance of the proposed model, we apply it on various classification data sets from public data set repositories. To obtain an optimal value of Mapper size, m, we range it in the range from 20 to 100."}, {"heading": "5.1.1 Commonly Used Classification Data Sets", "text": "We experiment on five public data sets which are summarized in Table 3, including Pendigit, Letter, Statlog, Page-blocks and Waveform. They are all multiclass data sets. All experiments are repeated 5 times and the results are averaged. All data sets are publicly available in svmlight format on the LIBSVM web site [38].\nPendigit data set is a collection of pen-based recognition of handwritten digits [39]. The data set contains 250 samples from 44 people. The first 7494 instances written by 30 people are used for the training data set, and the digits written by other 14 people are used for the independent testing purpose.\nSkin data set is a collection of skin segmentation constructed over R, G, B color space [40]. The data set contains face images of different age groups (young, middle, old), genders and racial groups (White, Black, Asian). The data set contains 245057 instances; out of which 50859 is the skin labeled instances and 194198 is non-skin instances.\nStatlog / Shuttle data set is a collection of space shuttle created by NASA [41]. The data set contains 43500 training instances and 14500 testing instances. 80% of the data belongs to class 1.\nPage Blocks data set is a collection of page layout of a document that has been detected by a segmentation process [42]. The data set contains 4500 training instances and 973 testing instances.\nWaveform data set is a collection of Breiman\u2019s waveform domains of CART book\u2019s [43]. The data set contains 4400 training instances and 600 testing instances."}, {"heading": "5.1.2 Large Scale Classification Data Sets", "text": "We experiment on three public large scale data sets which are summarized in Table 4, including \u201dRecord Linkage Comparison Patterns (Donation) \u201d, \u201dSUSY \u201d and \u201dHIGGS\u201d. All experiments are repeated 5 times and the results are averaged.\nDonation represent individual data, including first and family name, sex, date of birth and postal code, which were collected through iterative insertions in the course of several years. The comparison patterns in this data set are based on a sample of 100.000 records dating from 2005 to 2008 [44]. The data set contains 5,749,132 training instances and 1,000,000 testing instances. The data set is available on UCI web site [45].\nSUSY is a classification data set that distinguish between a signal process which produces supersymmetric particles and a background process which does not [46]. The first 8 features are kinematic properties measured by the particle detectors in the accelerator. The last ten features are functions of the first 8 features. The data set contains 5,000,000 training instances and 50,000 testing instances. The data set is available on UCI web site [47].\nHIGSS is a classification problem to distinguish between a signal process which produces Higgs bosons and a background process which does not [46]. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features. The data set contains 11,000,000 training instances and 500,000 testing instances. The data set is available on UCI web site [48].\n5.2 Evaluation\nSince the data sets that are used in our experiments are highly imbalanced, traditional accuracy based performance evaluation is not enough to find out\nan optimal classifier. We used four different metrics, the overall prediction accuracy, average recall, average precision [49] and F -score, to evaluate the classification accuracy which are common measurement metrics in information retrieval [50,51].\nPrecision is defined as the fraction of retrieved samples that are relevant. Precision is shown in Eq. 11.\nPrecision = Correct\nCorrect + False (11)\nRecall is defined as the fraction of relevant samples that is retrieved. Recall is shown in Eq. 12.\nPrecision = Correct\nCorrect +Missed (12)\nThe proposed evaluation model calculates the precision and recall for each class from prediction scores then finds their mean. Average precision and recall is shown in Eq. 13 and Eq. 14.\nPrecisionavg = 1\nnclasses\nnclasses\u22121 \u2211\ni=0\nPreci (13)\nRecallavg = 1\nnclasses\nnclasses\u22121 \u2211\ni=0\nRecalli (14)\nF -measure is defined as the harmonic mean of precision and recall. The\nF1 = 2\u00d7 Precavg \u00d7Recallavg Precavg +Recallavg\n(15)\n5.3 Data set results with conventional ELM\nFigure 1 shows that the accuracy performance of ELM for experimental data sets becomes steady-state after a threshold value of N . The testing classification performance is measured through accuracy, precision, recall and F1 measure. N varies from 150 to 500.\nTable 5 shows the best performance of the conventional ELM method of each data set.\nThe conventional ELM training algorithm can be applied only in Section 5.1.1. The large scale data sets in Section 5.1.2 are not feasible to train on a single computer.\n5.4 Testing Accuracy Analysis\nBecause of two different data set type (\u201dcommonly used\u201d, \u201dlarge scale\u201d) are used, the results are divided into two different sections. In Section 5.4.1, the figures and the plots show the implementation results of commonly used classification data sets. Section 5.4.2 shows the large scale data sets results."}, {"heading": "5.4.1 Commonly Used Classification Data Sets", "text": "The results of accuracy and performance tests with real data are shown in Table 6 and Figure 2 - Figure 6. According to the these results, AdaBoost T size and Mapper size have more impact on the accuracy of ensemble ELM classifier than number of hidden nodes in ELM network.\nAccuracy of classification models are visualized by heatmap color coding according to\n\u2013 Map size (M) - AdaBoost size (T ) \u2013 Map size (M) - Number of hidden nodes (nh) \u2013 AdaBoost size (T ) - Number of hidden nodes (nh)\nFigure 2 - Figure 6 are used to plot the quantitative differences in accuracy score of each data set. Heatmaps are two dimensional graphical representations of data with a pre-defined colormap to display values of a matrix [52]. Heatmaps can be used to understand what parameters affect the accuracy of the classification model. The figures are used to comparatively illustrate accuracy levels across a number of different parameters including Map size, AdaBoost size and the number of hidden nodes in ELM algorithm obtained from the proposed learning method.\nAccording to Table 7, classification performance results of the proposed method have almost the same values with the conventional ELM method."}, {"heading": "5.4.2 Large Scale Classification Data Sets", "text": "Figure 7 shows the speed up on mapper size over proposed method on large scale data sets. To asses the effectiveness of the learning algorithm, the time is measured with varying mapper size. Because of high dimensionality, the\ndata sets cannot be trained on a single computer. Then, the standart speed up percentage is modifed such that:\nSp = targminm\u2208M\ntp (16)\nwhere targminm\u2208M is the total time on minimum mapper that can be achieved to build a classifier model.\nAs can be seen from the figure, the data sets achives performance improvement in learning time of the algorithm. By examining the trends observed as the number of mappers increases, one can see that non-linear speed up is achieved.\n5.5 Stability Analysis\nStandard deviation of testing accuracy of the method is shown in Figure 8a and Figure 8b. We analyzed the stability of ensemble ELM classifier with two aspects, Mapper size and AdaBoost T size. Mapper size is the most important variable for the model stability according to the Figure 8a. From Figure 8a and Figure 8b, we can find that standard deviation of testing accuracy decreases enormously with the increasing of Mapper function size. Through this analysis, one can argue that a model with high Mapper function size do has higher stability than low Mapper function size."}, {"heading": "6 Conclusion and Future Works", "text": "In this paper,a parallel AdaBoost extreme learning machine algorithm implementation has been proposed for massive data learning. By creating the overall data set into data chunks, MapReduce based learning algorithm reduces the training time of ELM classification. To overcome the accuracy performance decreasing, distributed ELM is enhanced with AdaBoost method. The experimental results show that AdaBoosted ELM not only reduce the training time of large-scale data sets, but also evaluation metrics of accuracy performance compared with the conventional ELM.\nThe proposed AdaBoost based ELM has three different trade-off parameters which are (i) data chunk split size, M , (ii) maximum number of iterations,\nT ,in AdaBoost Algorithm and lastly (iii) number of hidden layer nodes nh in ELM algorithm. The empirical results in heatmap figures show that parameters M and T are more dominant than parameter nh for the classification accuracy of the hypothesis.\nThe algorithm is designed to deal with large scale data set ELM training problems. Another objective is to achieve the model\u2019s classification performance with same or close to the conventional ELM method. Classification performance results are shown in Section 5.3. The empirical results show us that classification performance results of the proposed method have almost the same values with the conventional ELM method."}], "references": [{"title": "Extreme learning machine: Theory and applications,", "author": ["G.-B. Huang", "Q.-Y. Zhu", "C.-K. Siew"], "venue": "Neurocomputing, vol. 70,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Xml document classification based on elm,", "author": ["X.-g. Zhao", "G. Wang", "X. Bi", "P. Gong", "Y. Zhao"], "venue": "Neurocomputing, vol. 74,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "A protein secondary structure prediction framework based on the extreme learning machine,", "author": ["G. Wang", "Y. Zhao", "D. Wang"], "venue": "Neurocomputing, vol. 72,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Face recognition based on extreme learning machine,", "author": ["W. Zong", "G.-B. Huang"], "venue": "Neurocomputing, vol. 74,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "An extreme learning machine approach for speaker recognition,", "author": ["Y. Lan", "Z. Hu", "Y.C. Soh", "G.-B. Huang"], "venue": "Neural Computing and Applications,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Distributed parallel support vector machines in strongly connected networks,", "author": ["Y. Lu", "V. Roychowdhury", "L. Vandenberghe"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Study on parallel svm based on mapreduce,", "author": ["Z. Sun", "G. Fox"], "venue": "in International Conference on Parallel and Distributed Processing Techniques and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Cloudsvm: Training an svm classifier in cloud computing systems,\u201d in Pervasive Computing and the Networked World (Q", "author": ["F. Catak", "M. Balaban"], "venue": "vol. 7719 of Lecture Notes in Computer Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Planet: Massively parallel learning of tree ensembles with mapreduce,", "author": ["B. Panda", "J.S. Herbach", "S. Basu", "R.J. Bayardo"], "venue": "Proc. VLDB Endow.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Efficient parallel knn joins for large data in mapreduce,", "author": ["C. Zhang", "F. Li", "J. Jestes"], "venue": "Proceedings of the 15th International Conference on Extending Database Technology, EDBT \u201912,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "An efficient hierarchical clustering method for large datasets with map-reduce,", "author": ["T. Sun", "C. Shu", "F. Li", "H. Yu", "L. Ma", "Y. Fang"], "venue": "Parallel and Distributed Computing, Applications and Technologies,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Mr-dbscan: An efficient parallel density-based clustering algorithm using mapreduce,", "author": ["Y. He", "H. Tan", "W. Luo", "H. Mao", "D. Ma", "S. Feng", "J. Fan"], "venue": "Parallel and Distributed Systems (ICPADS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Parallel k-means clustering based on mapreduce,", "author": ["W. Zhao", "H. Ma", "Q. He"], "venue": "in Cloud Computing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Elm: distributed extreme learning machine with mapreduce,", "author": ["J. Xin", "Z. Wang", "C. Chen", "L. Ding", "G. Wang", "Y. Zhao"], "venue": "World Wide Web,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks,", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "An os-elm based distributed ensemble classification framework in {P2P} networks,", "author": ["Y. Sun", "Y. Yuan", "G. Wang"], "venue": "Neurocomputing, vol. 74,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Elm-mapreduce: Mapreduce accelerated extreme learning machine for big spatial data analysis,", "author": ["J. Chen", "G. Zheng", "H. Chen"], "venue": "in Control and Automation (ICCA),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Measures of diversity in classifier ensembles and their relationship with the ensemble accuracy,", "author": ["L.I. Kuncheva", "C.J. Whitaker"], "venue": "Machine learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}, {"title": "Extreme learning machine: A new learning scheme of feedforward neural networks,", "author": ["G. bin Huang", "Q. yu Zhu", "C. kheong Siew"], "venue": "IN PROC. INT. JOINT CONF. NEURAL NETW,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Siew, \u201cUniversal approximation using incremental constructive feedforward networks with random hidden nodes,", "author": ["G.-B. Huang", "L. Chen", "C.-K"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Convex incremental extreme learning machine,", "author": ["G.-B. Huang", "L. Chen"], "venue": "Neurocomputing, vol. 70,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Enhanced random search based incremental extreme learning machine,", "author": ["G.-B. Huang", "L. Chen"], "venue": "Neurocomputing, vol. 71,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Compressed-domain ship detection on spaceborne optical image using deep neural network and extreme learning machine,", "author": ["J. Tang", "C. Deng", "G.-B. Huang", "B. Zhao"], "venue": "Geoscience and Remote Sensing, IEEE Transactions on,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Siew, \u201cIncremental extreme learning machine with fully complex hidden nodes,", "author": ["G.-B. Huang", "M.-B. Li", "L. Chen", "C.-K"], "venue": "Neurocomputing, vol. 71,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting,", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Computational learning theory,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1995}, {"title": "A short introduction to boosting,", "author": ["Y. Freund", "R. Schapire", "N. Abe"], "venue": "JournalJapanese Society For Artificial Intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Double-base asymmetric adaboost,", "author": ["I. Landesa-Vzquez", "J.L. Alba-Castro"], "venue": "Neurocomputing, vol. 118,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Mapreduce: Simplified data processing on large clusters,", "author": ["J. Dean", "S. Ghemawat"], "venue": "Commun. ACM,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "CloudBurst: highly sensitive read mapping with MapReduce.,", "author": ["M.C. Schatz"], "venue": "Bioinformatics (Oxford, England),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "Hepdoop: High-energy physics analysis using hadoop,", "author": ["W. Bhimji", "T. Bristow", "A. Washbrook"], "venue": "Journal of Physics: Conference Series,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Privacy preserving large scale dna read-mapping in mapreduce framework using fpgas,", "author": ["L. Xu", "H. Kim", "X. Wang", "W. Shi", "T. Suh"], "venue": "in Field Programmable Logic and Applications (FPL),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network,", "author": ["P. Bartlett"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1998}, {"title": "Vedelsby, \u201cNeural network ensembles, cross validation, and active learning,", "author": ["J.A. Krogh"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}, {"title": "Methods of combining multiple classifiers based on different representations for pen-based handwritten digit recognition,", "author": ["F. Alimoglu", "E. Alpaydin"], "venue": "Proceedings of the Fifth Turkish Artificial Intelligence and Artificial Neural Networks Symposium (TAINN", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1996}, {"title": "Efficient skin region segmentation using low complexity fuzzy decision tree model,", "author": ["R. Bhatt", "G. Sharma", "A. Dhall", "S. Chaudhury"], "venue": "in India Conference (INDICON),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "A comparison of methods for multiclass support vector machines,", "author": ["C.-W. Hsu", "C.-J. Lin"], "venue": "Trans. Neur. Netw.,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2002}, {"title": "A further comparison of simplification methods for decision-tree induction,", "author": ["D. Malerba", "F. Esposito", "G. Semeraro"], "venue": "In D. Fisher and H. Lenz (Eds.),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1996}, {"title": "Searching for exotic particles in high-energy physics with deep learning,", "author": ["P. Baldi", "P. Sadowski", "D. Whiteson"], "venue": "Nature communications,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "User performance versus precision measures for simple search tasks,", "author": ["A. Turpin", "F. Scholer"], "venue": "Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2006}, {"title": "Performance measures for information extraction,", "author": ["J. Makhoul", "F. Kubala", "R. Schwartz", "R. Weischedel"], "venue": "Proceedings of DARPA Broadcast News Workshop,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1999}, {"title": "Heatmapgenerator: high performance rnaseq and microarray visualization software suite to examine differential gene expression levels using an r and c++ hybrid computational pipeline,", "author": ["B. Khomtchouk", "D. Van Booven", "C. Wahlestedt"], "venue": "Source Code for Biology and Medicine,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Extreme Learning Machine (ELM) was proposed by [1] based on generalized Single-hidden Layer Feedforward Networks (SLFNs).", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "ELM algorithm is used in many different areas including document classification [2], bioinformatics [3] multimedia recognition [4,5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 2, "context": "ELM algorithm is used in many different areas including document classification [2], bioinformatics [3] multimedia recognition [4,5].", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "ELM algorithm is used in many different areas including document classification [2], bioinformatics [3] multimedia recognition [4,5].", "startOffset": 127, "endOffset": 132}, {"referenceID": 4, "context": "ELM algorithm is used in many different areas including document classification [2], bioinformatics [3] multimedia recognition [4,5].", "startOffset": 127, "endOffset": 132}, {"referenceID": 5, "context": "[6,7,8].", "startOffset": 0, "endOffset": 7}, {"referenceID": 6, "context": "[6,7,8].", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[6,7,8].", "startOffset": 0, "endOffset": 7}, {"referenceID": 8, "context": "proposed a learning tree model which is based on series of distributed computations, and implements each one using the MapReduce model of distributed computation [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "develops some algorithms using MapReduce to perform parallel data joins on large scale data sets [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "use batch updating based hierarchical clustering to reduce computational time and data communication [11].", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "They developed a partitioning strategy for large scale non-indexed data with a 4-stages MapReduce paradigm [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "proposed parallel k-means clustering based on MapReduce [13].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "proposed MapReduce based ELM training method called as ELM [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "proposed OS-ELM [15] based distributed ensemble classification in P2P networks [16].", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "proposed OS-ELM [15] based distributed ensemble classification in P2P networks [16].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "Main idea behind in this approach is to analyze the dependency relationships and the matrix calculations of OS-ELM [15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "have been proposed MapReduce based ELM ensemble classifier called ELM-MapReduce, for large scale land cover classification of remote sensing data [19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "If an ensemble method is applied, then the performance of final model will have better accuracy result [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 18, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feedforward neural networks [21,22,1] .", "startOffset": 111, "endOffset": 120}, {"referenceID": 19, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feedforward neural networks [21,22,1] .", "startOffset": 111, "endOffset": 120}, {"referenceID": 0, "context": "1 Extreme learning machine ELM was originally proposed for the single-hidden layer feedforward neural networks [21,22,1] .", "startOffset": 111, "endOffset": 120}, {"referenceID": 20, "context": "Then, ELM was extended to the generalized single-hidden layer feedforward networks where the hidden layer may not be neuron like [23, 24].", "startOffset": 129, "endOffset": 137}, {"referenceID": 21, "context": "Then, ELM was extended to the generalized single-hidden layer feedforward networks where the hidden layer may not be neuron like [23, 24].", "startOffset": 129, "endOffset": 137}, {"referenceID": 22, "context": "The main advantages of the ELM classification algorithm are that ELM can be trained hundred times faster than traditional neural network or support vector machine algorithm since its input weights and hidden node biases are randomly created and output layer weights can be analytically calculated by using a least-squares method [25,26].", "startOffset": 329, "endOffset": 336}, {"referenceID": 23, "context": "The main advantages of the ELM classification algorithm are that ELM can be trained hundred times faster than traditional neural network or support vector machine algorithm since its input weights and hidden node biases are randomly created and output layer weights can be analytically calculated by using a least-squares method [25,26].", "startOffset": 329, "endOffset": 336}, {"referenceID": 24, "context": "2 AdaBoost The AdaBoost [27] is a supervised learning algorithm designed to solve classification problems [28].", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "2 AdaBoost The AdaBoost [27] is a supervised learning algorithm designed to solve classification problems [28].", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "Given a space of feature vectors X and two possible class labels, y \u2208 {\u22121,+1}, AdaBoost goal is to learn a strong classifier H(x) as a weighted ensemble of weak classifiers ht(x) predicting the label of any instance x \u2208 X [29].", "startOffset": 222, "endOffset": 226}, {"referenceID": 27, "context": "The MapReduce was originally developed by Google and built on principles in parallel manner [30].", "startOffset": 92, "endOffset": 96}, {"referenceID": 28, "context": "The MapReduce framework automatically executes all those functions in a parallel manner over any number of processors/servers [31].", "startOffset": 126, "endOffset": 130}, {"referenceID": 29, "context": "cyber-security [32,33], high energy physics [34], biology [35].", "startOffset": 44, "endOffset": 48}, {"referenceID": 30, "context": "cyber-security [32,33], high energy physics [34], biology [35].", "startOffset": 58, "endOffset": 62}, {"referenceID": 31, "context": "2 Analysis of the proposed algorithm Barlett showed that the size of the weights is more important than the size of the neural network [36].", "startOffset": 135, "endOffset": 139}, {"referenceID": 32, "context": "also showed that ensemble methods of neural networks get better accuracy performance over unseen examples [37].", "startOffset": 106, "endOffset": 110}, {"referenceID": 33, "context": "Pendigit data set is a collection of pen-based recognition of handwritten digits [39].", "startOffset": 81, "endOffset": 85}, {"referenceID": 34, "context": "Skin data set is a collection of skin segmentation constructed over R, G, B color space [40].", "startOffset": 88, "endOffset": 92}, {"referenceID": 35, "context": "Statlog / Shuttle data set is a collection of space shuttle created by NASA [41].", "startOffset": 76, "endOffset": 80}, {"referenceID": 36, "context": "Page Blocks data set is a collection of page layout of a document that has been detected by a segmentation process [42].", "startOffset": 115, "endOffset": 119}, {"referenceID": 37, "context": "SUSY is a classification data set that distinguish between a signal process which produces supersymmetric particles and a background process which does not [46].", "startOffset": 156, "endOffset": 160}, {"referenceID": 37, "context": "HIGSS is a classification problem to distinguish between a signal process which produces Higgs bosons and a background process which does not [46].", "startOffset": 142, "endOffset": 146}, {"referenceID": 38, "context": "We used four different metrics, the overall prediction accuracy, average recall, average precision [49] and F -score, to evaluate the classification accuracy which are common measurement metrics in information retrieval [50,51].", "startOffset": 99, "endOffset": 103}, {"referenceID": 39, "context": "We used four different metrics, the overall prediction accuracy, average recall, average precision [49] and F -score, to evaluate the classification accuracy which are common measurement metrics in information retrieval [50,51].", "startOffset": 220, "endOffset": 227}, {"referenceID": 40, "context": "Heatmaps are two dimensional graphical representations of data with a pre-defined colormap to display values of a matrix [52].", "startOffset": 121, "endOffset": 125}], "year": 2016, "abstractText": "Machine learning based computational intelligence methods are widely used to analyze large scale data sets in this age of big data. Extracting useful predictive modeling from these types of data sets is a challenging problem due to their high complexity. Analyzing large amount of streaming data that can be leveraged to derive business value is another complex problem to solve. With high levels of data availability (i.e. Big Data) automatic classification of them has become an important and complex task. Hence, we explore the power of applying MapReduce based Distributed AdaBoosting of Extreme Learning Machine (ELM) to build a predictive bag of classification models. Accordingly, (i) data set ensembles are created; (ii) ELM algorithm is used to build weak learners (classifier functions); and (iii) builds a strong learner from a set of weak learners. We applied this training model to the benchmark knowledge discovery and data mining data sets.", "creator": "LaTeX with hyperref package"}}}