{"id": "1602.04282", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2016", "title": "Conservative Bandits", "abstract": "initially we study toward a partly novel multi - armed bandit problem that models presenting the primary challenge primarily faced by a company wishing to explore compelling new strategies determined to maximize revenue whilst also simultaneously maintaining significantly their revenue above a fixed baseline, uniformly operating over guaranteed time. while previous work addressed terminating the problem, under the weaker practical requirement of maintaining only the operational revenue against constraint requirements only usable at above a widely given fixed time in the future, the algorithms previously proposed are unsuitable due substantially to decreasing their redundant design obligations under the more reliable stringent constraints. occasionally we actually consider both the strengths stochastic and the weaker adversarial settings, precisely where feasible we concurrently propose, natural, yet novel strategies and successfully analyze exactly the price for manually maintaining the production constraints. amongst other things, we prove both high probability distribution and expectation beyond bounds on the simulated regret, while we also consider both the problem of maintaining the available constraints with high probability of or expectation. for satisfying the adversarial setting the price of maintaining the constraint parameter appears to be higher, at rather least enough for the algorithm than considered. alternatively a possible lower bound is given this showing that repeating the algorithm for the strong stochastic operational setting is almost optimal. early empirical performance results obtained in mixed synthetic environments demonstrate complement in our true theoretical findings.", "histories": [["v1", "Sat, 13 Feb 2016 03:47:11 GMT  (38kb,D)", "http://arxiv.org/abs/1602.04282v1", "9 pages, plus 4-page appendix, with 3 figures. Submitted to ICML 2016"]], "COMMENTS": "9 pages, plus 4-page appendix, with 3 figures. Submitted to ICML 2016", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["yifan wu", "roshan shariff", "tor lattimore", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1602.04282"}, "pdf": {"name": "1602.04282.pdf", "metadata": {"source": "CRF", "title": "Conservative Bandits", "authors": ["Yifan Wu", "Roshan Shariff", "Tor Lattimore", "Csaba Szepesv\u00e1ri"], "emails": ["ywu12@ualberta.ca", "rshariff@ualberta.ca", "tlattimo@ualberta.ca", "szepesva@ualberta.ca"], "sections": [{"heading": "1. Introduction", "text": "The manager of Zonlex, a fictional company, has just learned about bandit algorithms and is very excited about the opportunity to use this advanced technology to maximize Zonlex\u2019s revenue by optimizing the content on the landing page of the company\u2019s website. Every click on the content of their website pays a small reward; thanks to the high traffic that Zonlex\u2019s website enjoys, this translates into a decent revenue stream. Currently, Zonlex chooses the website\u2019s contents using a strategy designed over the years by its best engineers, but the manager suspects that some\nalternative strategies could potentially extract significantly more revenue. The manager is willing to explore bandit algorithms to identify the winning strategy. The manager\u2019s problem is that Zonlex cannot afford to lose more than 10% of its current revenue during its day-to-day operations and at any given point in time, as Zonlex needs a lot of cash to support its operations. The manager is aware that standard bandit algorithms experiment \u201cwildly\u201d, at least initially, and as such may initially lose too much revenue and jeopardize the company\u2019s stable operations. As a result, the manager is afraid of deploying cutting-edge bandit methods, but notes that this just seems to be a chicken-and-egg problem: a learning algorithm cannot explore due to the potential high loss, whereas it must explore to be good in the long run.\nThe problem described in the previous paragraph is ubiquitous. It is present, for example, when attempting to learn better human-computer interaction strategies, say in dialogue systems or educational games. In these cases a designer may feel that experimenting with sub-par interaction strategies could cause more harm than good (e.g., Rieser and Lemon, 2008; Liu et al., 2014). Similarly, optimizing a production process in a factory via learning (and experimentation) has much potential (e.g., Gabel and Riedmiller, 2011), but deviating too much from established \u201cbest practices\u201d will often be considered too dangerous. For examples from other domains see the survey paper of Garc\u0131\u0301a and Ferna\u0301ndez (2015).\nStaying with Zonlex, the manager also knows that the standard practice in today\u2019s internet companies is to employ A/B testing on an appropriately small percentage of the traffic for some period of time (e.g., 10% in the case of Zonlex). The manager even thinks that perhaps a best-arm identification strategy from the bandit literature, such as the recent lil\u2019UCB method of Jamieson et al. (2014), could be more suitable. While this is appealing, identifying the best possible option may need too much time even with a good learning algorithm (e.g., this happens when the difference in payoff between the best and second best strate-\nar X\niv :1\n60 2.\n04 28\n2v 1\n[ st\nat .M\nL ]\n1 3\nFe b\n20 16\ngies is small). One can of course stop earlier, but then the potential for improvement is wasted: when to stop then becomes a delicate question on its own. As Zonlex only plans for the next five years anyway, they could adopt the more principled yet quite simple approach of first using their default favorite strategy until enough payoff is collected, so that in the time remaining of the five years the return-constraint is guaranteed to hold regardless of the future payoffs. While this is a solution, the manager suspects that other approaches may exist. One such potential approach is to discourage a given bandit algorithm from exploring the alternative options, while in some way encouraging its willingness to use the default option. In fact, this approach has been studied recently by Lattimore (2015a) (in a slightly more general setting than ours). However, the algorithm of Lattimore (2015a) cannot be guaranteed to maintain the return constraint uniformly in time. It is thus unsuitable for the conservative manager of Zonlex; a modification of the algorithm could possibly meet this stronger requirement, but it appears that this will substantially increase the worst-case regret.\nIn this paper we ask whether better approaches than the above naive one exist in the context of multi-armed bandits, and whether the existing approaches can achieve the best possible regret given the uniform constraint on the total return. In particular, our contributions are as follows: (i) Starting from multi-armed bandits, we first formulate what we call the family of \u201cconservative bandit problems\u201d. As expected in these problems, the goal is to design learning algorithms that minimize regret under the additional constraint that at any given point in time, the total reward (return) must stay above a fixed percentage of the return of a fixed default arm, i.e., the return constraint must hold uniformly in time. The variants differ in terms of how stringent the constraint is (i.e., should the constraint hold in expectation, or with high probability?), whether the bandit problem is stochastic or adversarial, and whether the default arm\u2019s payoff is known before learning starts. (ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a \u201csmoother\u201d fashion. (iii) We prove that the new algorithm, which we call Conservative UCB, meets the uniform return constraint (in various senses), while it can achieve significantly less regret than BudgetFirst. In particular, while BudgetFirst is shown to pay a multiplicative penalty in the regret for maintaining the return constraint, Conservative UCB only pays an additive penalty. We provide both high probability and expectation bounds, consider both high probability and expectation constraints\non the return, and also consider the case when the payoff of the default arm is initially unknown. (iv) We also prove a lower bound on the best regret given the constraint and as a result show that the additive penalty is unavoidable; thus Conservative UCB achieves the optimal regret in a worst-case sense. While Unbalanced MOSS of Lattimore (2015a), when specialized to our setting, also achieves the optimal regret (as follows from the analysis of Lattimore, 2015a), as mentioned earlier it does not maintain the constraint uniformly in time (it will explore too much at the beginning of time); it also relies heavily on the knowledge of the mean payoff of the default strategy. (v) We also consider the adversarial setting where we design an algorithm similar to Conservative UCB: the algorithm uses an underlying \u201cbase\u201d adversarial bandit strategy when it finds that the return so far is sufficiently higher than the minimum required return. We prove that the resulting method indeed maintains the return constraint uniformly in time and we also prove a high-probability bound on its regret. We find, however, that the additive penalty in this case is higher than in the stochastic case. Here, the Exp3-\u03b3 algorithm of Lattimore (2015a) is an alternative, but again, this algorithm is not able to maintain the return constraint uniformly in time. (vi) The theoretical analysis is complemented by synthetic experiments on simple bandit problems whose purpose is to validate that the newly designed algorithm is reasonable and to show that the algorithms\u2019 behave as dictated by the theory developed. We also compare our method to Unbalanced MOSS to provide a perspective to see how much is lost due to maintaining the return constraint uniformly over time. We also identify future work. In particular, we expect our paper to inspire further works in related, more complex online learning problems, such as contextual bandits, or even reinforcement learning."}, {"heading": "1.1. Previous Work", "text": "Our constraint is equivalent to a constraint on the regret to a default strategy, or in the language of prediction-withexpert-advice, or bandit literature, regret to a default action. In the full information, mostly studied in the adversarial setting, much work has been devoted to understanding the price of such constraints (Hutter and Poland, 2005; EvenDar et al., 2008; Koolen, 2013; Sani et al., 2014, e.g.,). In particular, Koolen (2013) studies the Pareto frontier of regret vectors (which contains the non-dominated worst-case regret vectors of all algorithms). The main lesson of these works is that in the full information setting even a constant regret to a fixed default action can be maintained with essentially no increase in the regret to the best action. The situation quickly deteriorates in the bandit setting as shown by Lattimore (2015a). This is perhaps unsurprising given that, as opposed to the full information setting, in the bandit setting one needs to actively explore to get improved estimates\nof the actions\u2019 payoffs. As mentioned earlier, Lattimore describes two learning algorithms relevant to our setting: In the stochastic setting we consider, Unbalanced MOSS (and its relative, Unbalanced UCB) are able to achieve a constant regret penalty while maintaining the return constraint while Exp3-\u03b3 achieves a much better regret as compared to our strategy for the adversarial setting. However, neither of these algorithms maintain the return constraint uniformly in time. Neither will the constraint hold with high probability. While Unbalanced UCB achieves problem-dependent bounds, it has the same issues as Unbalanced MOSS with maintaining the return constraint. Also, all these strategies rely heavily on knowing the payoff of the default action.\nMore broadly, the issue of staying safe while exploring has long been recognized in reinforcement learning (RL). Garc\u0131\u0301a and Ferna\u0301ndez (2015) provides a comprehensive survey of the relevant literature. Lack of space prevents us from including much of this review. However, the short summary is that while the issue has been considered to be important, no previous approach addresses the problem from a theoretical angle. Also, while it has been recognized that adding constraints on the return is one way to ensure safety, as far as we know, maintaining the constraints during learning (as opposed to imposing them as a way of restricting the set of feasible policies) has not been considered in this literature. Our work, while it considers a much simpler setting, suggest a novel approach to address the safe exploration problem in RL.\nAnother line of work considers safe exploration in the related context of optimization (Sui et al., 2015). However, the techniques and the problem setting (e.g., objective) in this work is substantially different from ours."}, {"heading": "2. Conservative Multi-Armed Bandits", "text": "The multi-armed bandit problem is a sequential decisionmaking task in which a learning agent repeatedly chooses an action (called an arm) and receives a reward corresponding to that action. We assume there are K + 1 arms and denote the arm chosen by the agent in round t \u2208 {1, 2, . . .} by It \u2208 {0, . . . ,K}. There is a reward Xt,i associated with each arm i at each round t and the agent receives the reward corresponding to its chosen arm, Xt,It . The agent does not observe the other rewards Xt, j ( j , It).\nThe learning performance of an agent over a time horizon n is usually measured by its regret, which is the difference between its reward and what it could have achieved by consistently choosing the single best arm in hindsight:\nRn = max i\u2208{0,...,K} n\u2211 t=1 Xt,i \u2212 Xt,It . (1)\nAn agent is failing to learn unless its regret grows sub-\nlinearly: Rn \u2208 o(n); good agents achieve Rn \u2208 O( \u221a\nn) or even Rn \u2208 O(log n). We also use the notation Ti(n) = \u2211n t=1 1{It = i} for the number of times the agent chooses arm i in the first n time steps."}, {"heading": "2.1. Conservative Exploration", "text": "Let arm 0 correspond to the conservative default action with the other arms 1, . . . ,K being the alternatives to be explored. We want to be able to choose some \u03b1 > 0 and constrain the learner to earn at least a 1 \u2212 \u03b1 fraction of the reward from simply playing arm 0:\nt\u2211 s=1 Xs,Is \u2265 (1 \u2212 \u03b1) t\u2211 s=1 Xs,0 for all t \u2208 {1, . . . , n}. (2)\nFor the introductory example above \u03b1 = 0.1, which corresponds to losing at most 10% of the revenue compared to the default website. It should be clear that small values of \u03b1 force the learner to be highly conservative, whereas larger \u03b1 correspond to a weaker constraint.\nWe introduce a quantity Zn, called the budget, which quantifies how close the constraint (2) is to being violated:\nZt = t\u2211\ns=1\nXs,Is \u2212 (1 \u2212 \u03b1)Xs,0; (3)\nthe constraint is satisfied if and only if Zt \u2265 0 for all t \u2208 {1, . . . , n}. Note that the constraints must hold uniformly in time.\nOur objective is to design algorithms that minimize the regret (1) while simultaneously satisfying the constraint (2). In the following sections, we will consider two variants of multi-armed bandits: the stochastic setting in Section 3 and the adversarial setting in Section 4. In each case we will design algorithms that satisfy different versions of the constraint and give regret guarantees.\nOne may wonder: what if we only care about Zn \u2265 0 instead of Zt \u2265 0 for all t. Although our algorithms are designed for satisfying the anytime constraint on Zt our lower bound, which is based on Zn \u2265 0 only, shows that in the stochastic setting we cannot improve the regret guarantee even if we only want to satisfy the overall constraint Zn \u2265 0."}, {"heading": "3. The Stochastic Setting", "text": "In the stochastic multi-armed bandit setting each arm i and round t has a stochastic reward Xt,i = \u00b5i + \u03b7t,i, where \u00b5i \u2208 [0, 1] is the expected reward of arm i and the \u03b7t,i are independent random noise variables that we assume have 1-subgaussian distributions. We denote the expected reward of the optimal arm by \u00b5\u2217 = maxi \u00b5i and the gap between it and the expected reward of the ith arm by \u2206i = \u00b5\u2217 \u2212 \u00b5i.\nThe regret Rn is now a random variable. We can bound it in expectation, of course, but we are often more interested in high-probability bounds on the weaker notion of pseudoregret:\nR\u0303n = n\u00b5\u2217 \u2212 n\u2211\nt=1\n\u00b5It = K\u2211 i=0 Ti(n)\u2206i, (4)\nin which the noise in the arms\u2019 rewards is ignored and the randomness arises from the agent\u2019s choice of arm. The regret Rn and the pseudo-regret R\u0303n are equal in expectation. High-probability bounds for the latter, however, can capture the risk of exploration without being dominated by the variance in the arms\u2019 rewards.\nWe use the notation \u00b5\u0302i(n) = 1Ti(n) \u2211n\nt=1 1{It = i} Xt,i for the empirical mean of the rewards from arm i observed by the agent in the first n rounds. If Ti(n) = 0 then we define \u00b5\u0302i(n) = 0. The algorithms for the stochastic setting will estimate the \u00b5i by \u00b5\u0302i and will construct and act based on high-probability confidence intervals for the estimates."}, {"heading": "3.1. The Budget Constraint", "text": "Just as we substituted regret with pseudo-regret, in the stochastic setting we will use the following form of the constraint (2):\nt\u2211 s=1 \u00b5Is \u2265 (1 \u2212 \u03b1)\u00b50t for all t \u2208 {1, . . . , n}; (5)\nthe budget then becomes\nZ\u0303t = t\u2211\ns=1\n\u00b5Is \u2212 (1 \u2212 \u03b1)t\u00b50 . (6)\nThe default arm is always safe to play because it increases the budget by \u00b50\u2212(1\u2212\u03b1)\u00b50 = \u03b1\u00b50. The budget will decrease for arms i with \u00b5i < (1\u2212 \u03b1)\u00b50; the constraint Z\u0303n \u2265 0 is then in danger of being violated (Fig. 1).\nIn the following sections we will construct algorithms that satisfy pseudo-regret bounds and the budget constraint (5) with high probability 1 \u2212 \u03b4 (where \u03b4 > 0 is a tunable parameter). In Section 3.4 we will see how these algorithms can be adapted to satisfy the constraint in expectation and with bounds on their expected regret.\nFor simplicity, we will initially assume that the algorithms know \u00b50, the expected reward of the default arm. This is reasonable in situations where the default action has been used for a long time and is well-characterized. Even so, in Section 3.5 we will see that having to learn an unknown \u00b50 is not a great hindrance."}, {"heading": "3.2. BudgetFirst \u2014 A Naive Algorithm", "text": "Before presenting the new algorithm it is worth remarking on the most obvious naive attempt, which we call the BudgetFirst algorithm. A straightforward modification of UCB leads to an algorithm that accepts a confidence parameter \u03b4 \u2208 (0, 1) and suffers regret at most\nR\u0303n = O  \u221a Kn log ( log(n) \u03b4 ) = Rworst . (7) Of course this algorithm alone will not satisfy the constraint (5), but that can be enforced by naively modifying the algorithm to deterministically choose It = 0 for the first t0 rounds where\n(\u2200 t0 \u2264 t \u2264 n) t\u00b50 \u2212 Rworst \u2265 (1 \u2212 \u03b1)t\u00b50 . Subsequently the algorithm plays the high probability version of UCB and the regret guarantee (7) ensures the constraint (5) is satisfied with high probability. Solving the equation above leads to t0 = O\u0303(Rworst/\u03b1\u00b50), and since the regret while choosing the default arm may be O(1) the worst-case regret guarantee of this approach is\nR\u0303n = \u2126  1\u00b50\u03b1 \u221a Kn log ( log(n) \u03b4 ) . This is significantly worse than the more sophisticated algorithm that is our main contribution and for which the price of satisfying (5) is only an additive term rather than a large multiplicative factor."}, {"heading": "3.3. Conservative UCB", "text": "A better strategy is to play the default arm only until the budget (6) is large enough to start exploring other arms with a low risk of violating the constraint. It is safe to keep exploring as long as the budget remains large, whereas if it\ndecreases too much then it must be replenished by playing the default arm. In other words, we intersperse the exploration of a standard bandit algorithm with occasional budget-building phases when required. We show that accumulating a budget does not severely curtail exploration and thus gives small regret.\nConservative UCB (Algorithm 1) is based on UCB with the novel twist of maintaining a positive budget. In each round, UCB calculates upper confidence bounds for each arm; let Jt be the arm that maximizes this calculated confidence bound. Before playing this arm (as UCB would) our algorithm decides whether doing so risks the budget becoming negative. Of course, it does not know the actual budget Z\u0303t because the \u00b5i (i , 0) are unknown; instead, it calculates a lower confidence bound \u03bet based on confidence intervals for the \u00b5i. More precisely, it calculates a lower confidence bound for what the budget would be if it played arm Jt. If this lower bound is positive then the constraint will not be violated as long as the confidence bounds hold. If so, the the algorithm chooses It = Jt just as UCB would; otherwise it acts conservatively by choosing It = 0.\n1: Input: K, \u00b50, \u03b4, \u03c8\u03b4(\u00b7) 2: for t \u2208 1, 2, . . . do . Compute confidence intervals. . .\n3: \u03b80(t), \u03bb0(t)\u2190 \u00b50 . . . . for known \u00b50, 4: for i \u2208 1, . . . ,K do . . . . for other arms, 5: \u2206i(t)\u2190 \u221a \u03c8\u03b4(Ti(t \u2212 1))/Ti(t \u2212 1) 6: \u03b8i(t)\u2190 \u00b5\u0302i(t \u2212 1) + \u2206i(t) 7: \u03bbi(t)\u2190 max {0, \u00b5\u0302i(t \u2212 1) \u2212 \u2206i(t)} 8: end for 9: Jt \u2190 arg maxi \u03b8i(t) . . . . and find UCB arm. . Compute budget and. . .\n10: \u03bet \u2190 \u2211t\u22121\ns=1 \u03bbIs (t) + \u03bbJt (t) \u2212 (1 \u2212 \u03b1)t\u00b50 11: if \u03bet \u2265 0 then 12: It \u2190 Jt . . . . choose UCB arm if safe, 13: else 14: It \u2190 0 . . . . default arm otherwise. 15: end if\n16: end for Algorithm 1: Conservative UCB\nRemark 1 (Choosing \u03c8\u03b4). The confidence intervals in Algorithm 1 are constructed using the function \u03c8\u03b4. Let F be the event that for all rounds t \u2208 {1, 2, . . .} and every action i \u2208 [K], the confidence intervals are valid:\n|\u00b5\u0302i(t) \u2212 \u00b5i| \u2264\n\u221a \u03c8\u03b4(Ti(t))\nTi(t) .\nOur goal is to choose \u03c8\u03b4(\u00b7) such that P {F} \u2265 1 \u2212 \u03b4 . (8)\nA simple choice is\n\u03c8\u03b4(s) = 2 log(Ks3/\u03b4),\nfor which (8) holds by Hoeffding\u2019s inequality and union bounds. The following choice achieve better performance in practice:\n\u03c8\u03b4(s) = log max{3, log \u03b6} + log(2e2\u03b6)\n+ \u03b6(1 + log(\u03b6)) (\u03b6 \u2212 1) log(\u03b6) log log(1 + s), (9)\nwhere \u03b6 = K/\u03b4; it can be seen to achieve (8) by more careful analysis motivated by Garivier (2013),\nSome remarks on Algorithm 1\n\u2022 \u00b50 is known, so the upper and lower confidence bounds can both be set to \u00b50 (line 3). See Section 3.5 for a modification that learns an unknown \u00b50.\n\u2022 The max in the definition of the lower confidence bound \u03bbi(t) (line 7) is because we have assumed \u00b5i \u2265 0 and so the lower confidence bound should never be less than 0.\n\u2022 \u03bet (line 10) is a lower confidence bound on the budget (6) if action Jt is chosen. More precisely, it is a lower confidence bound on\nZ\u0303t = t\u22121\u2211 s=1 \u00b5Is + \u00b5Jt \u2212 (1 \u2212 \u03b1)t\u00b50.\n\u2022 If the default arm is also the UCB arm (Jt = 0) and the confidence intervals all contain the true values, then \u00b5\u2217 = \u00b50 and the algorithm will choose action 0 for all subsequent rounds, incurring no regret.\nThe following theorem guarantees that Conservative UCB satisfies the constraint while giving a high-probability upper bound on its regret. Theorem 2. In any stochastic environment where the arms have expected rewards \u00b5i \u2208 [0, 1] with 1-subgaussian noise, Algorithm 1 satisfies the following with probability at least 1 \u2212 \u03b4 and for every time horizon n:\nt\u2211 s=1 \u00b5Is \u2265 (1 \u2212 \u03b1)\u00b50t for all t \u2208 {1, . . . , n}, (5)\nR\u0303n \u2264 \u2211\ni>0:\u2206i>0\n( 4L \u2206i + \u2206i ) + 2(K + 1)\u22060 \u03b1\u00b50\n+ 6L \u03b1\u00b50 K\u2211 i=1\n\u22060\nmax{\u2206i,\u22060 \u2212 \u2206i} , (10)\nR\u0303n \u2208 O (\u221a\nnKL + KL \u03b1\u00b50\n) , (11)\nwhen \u03c8\u03b4 is chosen in accordance with Remark 1 and where L = \u03c8\u03b4(n).\nStandard unconstrained UCB algorithms achieve a regret of order O( \u221a nKL); Theorem 2 tells us that the penalty our algorithm pays to satisfy the constraint is an extra additive regret of order O(KL/\u03b1\u00b50).\nRemark 3. We take a moment to understand how the regret of the algorithm behaves if \u03b1 is polynomial in 1/n. Clearly if \u03b1 \u2208 O(1/n) then we have a constant exploration budget and the problem is trivially hard. In the slightly less extreme case when \u03b1 is as small as n\u2212a for some 0 < a < 1, the extra regret penalty is still not negligible: satisfying the constraint costs us O(na) more regret in the worst case.\nWe would argue that the problem-dependent regret penalty (10) is more informative than the worst case of O(na); our regret increases by\n6L \u03b1\u00b50 K\u2211 i=1\n\u22060\nmax{\u2206i,\u22060 \u2212 \u2206i} .\nIntuitively, even if \u03b1 is very small, we can still explore as long as the default arm is close-to-optimal (i.e. \u22060 is small) and most other arms are clearly sub-optimal (i.e. the \u2206i are large). Then the sub-optimal arms are quickly discarded and even the budget-building phases accrue little regret: the regret penalty remains quite small. More precisely, if \u22060 \u2248 n\u2212b0 and mini>0:\u2206i>0 \u2206i \u2248 n\u2212b, then the regret penalty is\nO ( na+min{0,b\u2212b0} ) ;\nsmall \u22060 and large \u2206i means b \u2212 b0 < 0, giving a smaller penalty than the worst case of O(na).\nRemark 4. Curious readers may be wondering if It = 0 is the only conservative choice when the arm proposed by UCB risks violating the constraint. A natural alternative would be to use the lower confidence bound \u03bbi(t) by choosing\nIt = Jt , if \u03bet \u2265 0 ;arg maxi \u03bbi(t) , otherwise . (12) It is easy to see that if F does not occur, then choosing arg maxi \u03bbi(t) increases the budget at least as much as choosing action 0 while incurring less regret and so this algorithm is preferable to Algorithm 1 in practice. Theoretically speaking, however, it is possible to show that the improvement is by at most a constant factor so our analysis of the simpler algorithm suffices. The proof of this claim is somewhat tedious so instead we provide two intuitions:\n1. The upper bound approximately matches the lower bound in the minimax regime, so any improvement must be relatively small in the minimax sense.\n2. Imagine we run the unmodified Algorithm 1 and let t be the first round when It , Jt and where there exists an i > 0 with \u03bbi(t) \u2265 \u00b50. If F does not hold, then the\nactions chosen by UCB satisfy Ti(t) \u2208 \u2126 min  L\u22062i ,maxj T j(t)  , which means that arms are being played in approximately the same frequency until they are proving suboptimal (for a similar proof, see Lattimore, 2015b). From this it follows that once \u03bbIt (t) \u2265 \u00b50 for some i it will not be long before either \u03bb j(t + s) \u2265 \u00b50 or T j(t + s) \u2265 4L/\u22062i and in both cases the algorithm will cease playing conservatively. Thus it takes at most a constant proportion more time before the naive algorithm is exclusively choosing the arm chosen by UCB.\nNext we discuss how small modifications to Algorithm 1 allow it to handle some variants of the problem while guaranteeing the same order of regret."}, {"heading": "3.4. Considering the Expected Regret and Budget", "text": "One may care about the performance of the algorithm in expectation rather than with high probability, i.e. we want an upper bound on E [ R\u0303n ] and the constraint (5) becomes\nE [ t\u2211\ns=1\n\u00b5Is ] \u2265 (1 \u2212 \u03b1)\u00b50t, for all t \u2208 {1, . . . , n}. (13)\nWe argued in Remark 3 that if \u03b1 \u2208 O(1/n) then the problem is trivially hard; let us assume therefore that \u03b1 \u2265 c/n for some c > 1. By running Algorithm 1 with \u03b4 = 1/n and \u03b1\u2032 = (\u03b1\u2212 \u03b4)/(1\u2212 \u03b4) we can achieve (13) and a regret bound with the same order as in Theorem 2.\nTo show (13) we have\nE [ t\u2211\ns=1\n\u00b5Is ] \u2265 P {F}E [ t\u2211 s=1 \u00b5Is \u2223\u2223\u2223\u2223 F] \u2265 (1 \u2212 \u03b4)(1 \u2212 \u03b1\u2032)\u00b50t = (1 \u2212 \u03b1)\u00b50t .\nIn the upper bound of E [Rn], we have E [Rn] \u2264 E [Rn|F] + \u03b4n = E [Rn|F] + 1 .\nE [Rn|F] can be upper bounded by Theorem 2 with two changes: (i) L becomes O(log nK) after replacing \u03b4 with 1/n, and (ii) \u03b1 becomes \u03b1\u2032. Since \u03b1\u2032/\u03b1 \u2265 1 \u2212 1/c we get essentially the same order of regret bound as in Theorem 2."}, {"heading": "3.5. Learning an Unknown \u00b50", "text": "Two modifications to Algorithm 1 allow it to handle the case when \u00b50 is unknown. First, just as we do for the nondefault arms, we need to set \u03b80(t) and \u03bb0(t) based on confidence intervals. Second, the lower bound on the budget\nneeds to be set as\n\u03be\u2032t = K\u2211\ni=1\nTi(t \u2212 1)\u03bbi(t) + \u03bbJt (t)\n+ (T0(t \u2212 1) \u2212 (1 \u2212 \u03b1)t)\u03b80(t) . (14)\nTheorem 5. Algorithm 1, modified as above to work without knowing \u00b50 but otherwise the same conditions as Theorem 2, satisfies with probability 1 \u2212 \u03b4 and for all time horizons n the constraint (5) and the regret bound\nR\u0303n \u2264 \u2211\ni:\u2206i>0\n( 4L \u2206i + \u2206i ) + 2(K + 1)\u22060 \u03b1\u00b50\n+ 7L \u03b1\u00b50 K\u2211 i=1\n\u22060\nmax{\u2206i,\u22060 \u2212 \u2206i} . (15)\nTheorem 5 shows that we get the same order of regret for unknown \u00b50. The proof is very similar to the one for Theorem 2 and is also left for the appendix."}, {"heading": "4. The Adversarial Setting", "text": "Unlike the stochastic case, in the adversarial multi-armed bandit setting we do not make any assumptions about how the rewards are generated. Instead, we analyze a learner\u2019s worst-case performance over all possible sequences of rewards (Xt,i). In effect, we are treating the environment as an adversary that has intimate knowledge of the learner\u2019s strategy and will devise a sequence of rewards that maximizes regret. To preserve some hope of succeeding, however, the learner is allowed to behave randomly: in each round it can randomize its choice of arm It using a distribution it constructs; the adversary cannot influence nor predict the result of this random choice.\nOur goal is, as before, to satisfy the constraint (2) while bounding the regret (1) with high probability (the randomness comes from the learner\u2019s actions). We assume that the default arm has a fixed reward: Xt,0 = \u00b50 \u2208 [0, 1] for all t; the other arms\u2019 rewards are generated adversarially in [0, 1]. The constraint to be satisfied then becomes\u2211t\ns=1 Xs,Is \u2265 (1 \u2212 \u03b1)\u00b50t for all t.\nSafe-playing strategy: We take any standard any-time high probability algorithm for adversarial bandits and adapt it to play as usual when it is safe to do so, i.e. when Zt \u2265 \u2211t\u22121 s=1 Xs,Is \u2212 (1 \u2212 \u03b1)\u00b50t \u2265 0. Otherwise it should play It = 0. To demonstrate a regret bound, we only require that the bandit algorithm satisfy the following requirement. Definition 6. An algorithm A is R\u0302\u03b4t -admissible (R\u0302\u03b4t sublinear) if for any \u03b4, in the adversarial setting it satisfies\nP { \u2200t \u2208 {1, 2, . . .},Rt \u2264 R\u0302\u03b4t } \u2265 1 \u2212 \u03b4.\nNote that this performance requirement is stronger than the\ntypical high probability bound but is nevertheless achievable. For example, Neu (2015) states the following for the any-time version of their algorithm: given any time horizon n and confidence level \u03b4, P { Rn \u2264 R\u0302\u2032n(\u03b4) } \u2265 1 \u2212 \u03b4 for some sub-linear R\u0302\u2032t(\u03b4). If we let R\u0302 \u03b4 t = R\u0302 \u2032 t(\u03b4/2t\n2) then P { Rt \u2264 R\u0302\u03b4t } \u2265 1 \u2212 \u03b42t2 holds for any fixed t. Since the algorithm does not require n and \u03b4 as input, a union bound shows it to be R\u0302\u03b4t -admissible.\nHaving satisfied ourselves that there are indeed algorithms that meet our requirements, we can prove a regret guarantee for our safe-playing strategy.\nTheorem 7. Any R\u0302\u03b4t -admissible algorithm A, when adapted with our safe-playing strategy, satisfies the constraint (2) and has a regret bound of Rn \u2264 t0 + R\u0302\u03b4n with probability at least 1\u2212\u03b4 where t0 = max{t |\u03b1\u00b50t \u2264 R\u0302\u03b4t +\u00b50}.\nCorollary 8. The any-time high probability algorithm of Neu (2015) adapted with our safe-playing strategy gives R\u0302\u03b4t = 7 \u221a Kt log K log(4t2/\u03b4) and\nRn \u2264 7 \u221a\nKn log K log(4n2/\u03b4) + 49K log K \u03b12\u00b520 log2 4n2 \u03b4\nwith probability at least 1 \u2212 \u03b4.\nCorollary 8 shows that a strategy similar to that of Algorithm 1 also works for the adversarial setting. However, we pay a higher regret penalty to satisfy the constraint:\nO ( KL2\n(\u03b1\u00b50)2\n) rather than the O ( KL \u03b1\u00b50 ) we had in the stochastic\nsetting. Whether this is because (i) our algorithm is sub-optimal, (ii) the analysis is not tight, or (iii) there is some intrinsic hardness in the non-stochastic setting is still not clear and remains an interesting open problem."}, {"heading": "5. Lower Bound on the Regret", "text": "We now present a worst-case lower bound where \u03b1, \u00b50 and n are fixed, but the mean rewards are free to change. For any vector \u00b5 \u2208 [0, 1]K , we will write E\u00b5 to denote expectations under the environment where all arms have normallydistributed unit-variance rewards and means \u00b5i (i.e., the fixed value \u00b50 is the mean reward of arm 0 and the components of \u00b5 are the mean rewards of the other arms). We assume normally distributed noise for simplicity: Other subgaussian distributions work identically as long as the subgaussian parameter can be kept fixed independently of the mean rewards.\nTheorem 9. Suppose for any \u00b5i \u2208 [0, 1] (i > 0) and \u00b50 satisfying\nmin{\u00b50, 1 \u2212 \u00b50} \u2265 max { 1/2 \u221a \u03b1, \u221a e + 1/2 } \u221a K/n,\nan algorithm satisfies E\u00b5 \u2211n\nt=1 Xt,It \u2265 (1\u2212\u03b1)\u00b50n. Then there is some \u00b5 \u2208 [0, 1]K such that its expected regret satisfies\nE\u00b5Rn \u2265 B where B = max  K(16e + 8)\u03b1\u00b50 , \u221a Kn \u221a 16e + 8 . (16) Theorem 9 shows that our algorithm for the stochastic setting is near-optimal (up to a logarithmic factor L) in the worst case. A problem-dependent lower bound for the stochastic setting would be interesting but is left for future work. Also note that in the lower bound we only use E\u00b5 \u2211n t=1 Xt \u2265 (1 \u2212 \u03b1)n\u00b50 for the last round n, which means that the regret guarantee cannot be improved if we only care about the last-round budget instead of the anytime budget. In practice, however, enforcing the constraint in all rounds will generally lead to significantly worse results because the algorithm cannot explore early on. This is demonstrated empirically in Section 6, where we find that the Unbalanced MOSS algorithm performs very well in terms of the expected regret, but does not satisfy the constraint in early rounds.\nRemark 10. The theorem above almost follows from the lower bound given by Lattimore (2015a), but in that paper \u00b50 is unknown, while here it may be known. This makes our result strictly stronger, as the lower bound is the same up to constant factors."}, {"heading": "6. Experiments", "text": "We evaluate the performance of Conservative UCB compared to UCB and Unbalanced MOSS Lattimore (2015a) using simulated data in two regimes. In the first we fix the horizon and sweep over \u03b1 \u2208 [0, 1] to show the degradation of the average regret of Conservative UCB relative to UCB as the constraint becomes harsher (\u03b1 close to zero). In the second regime we fix \u03b1 = 0.1 and plot the long-term average regret, showing that Conservative UCB is eventually nearly as good as UCB, despite the constraint. Each data point is an average of N \u2248 4000 i.i.d. samples, which makes error bars too small to see. All code and data will be made available in any final version. Results are shown for both versions of Conservative UCB: The first knows the mean \u00b50 of the default arm while the second does not and must act more conservatively while learning this value. As predicted by the theory, the difference in performance between these two versions of the algorithm is relatively small, but note that even when \u03b1 = 1 the algorithm that knows \u00b50 is performing better because this knowledge is useful in the unconstrained setting. This is also true of the BudgetFirst algorithm, which is unconstrained when \u03b1 = 1 and exploits its knowledge of \u00b50 to eliminate the default arm. This algorithm is so conservative that even when \u03b1 is nearly zero it must first build a significant budget. We tuned the Unbalanced MOSS algorithm with the following\nparameters.\nB0 = nK\n\u221a nK + K\n\u03b1\u00b50\nBi = BK = \u221a nK + K \u03b1\u00b50 .\nThe quantity Bi determines the regret of the algorithm with respect to arm i up to constant factors, and must be chosen to lie inside the Pareto frontier given by Lattimore (2015a). It should be emphasised that Unbalanced MOSS does not constraint the return except for the last round, and has no high-probability guarantees. This freedom allows it to explore early, which gives it a significant advantage over the highly constrained Conservative UCB. Furthermore, it also requires B0, . . . , BK as inputs, which means that \u00b50 must be known in advance. The mean rewards in both experiments are \u00b50 = 0.5, \u00b51 = 0.6, \u00b52 = \u00b53 = \u00b54 = 0.4, which means that the default arm is slightly sub-optimal."}, {"heading": "7. Conclusion", "text": "We introduced a new family of multi-armed bandit frameworks motivated by the requirement of exploring conservatively to maintain revenue. We also demonstrated various strategies that act effectively while maintaining such constraints. We expect that similar strategies generalize to other settings, like contextual bandits and reinforcement learning. We want to emphasize that this is just the beginning of a line of research that has many potential applications. We hope that others will join us in improving the current results, closing open problems, and generalizing the model so it is more widely applicable."}, {"heading": "A. Proof of Theorem 2", "text": "Theorem 2. In any stochastic environment where the arms have expected rewards \u00b5i \u2208 [0, 1] with 1-subgaussian noise, Algorithm 1 satisfies the following with probability at least 1 \u2212 \u03b4 and for every time horizon n:\nt\u2211 s=1 \u00b5Is \u2265 (1 \u2212 \u03b1)\u00b50t for all t \u2208 {1, . . . , n}, (5)\nR\u0303n \u2264 \u2211\ni>0:\u2206i>0\n( 4L \u2206i + \u2206i ) + 2(K + 1)\u22060 \u03b1\u00b50\n+ 6L \u03b1\u00b50 K\u2211 i=1\n\u22060\nmax{\u2206i,\u22060 \u2212 \u2206i} , (10)\nR\u0303n \u2208 O (\u221a\nnKL + KL \u03b1\u00b50\n) , (11)\nwhen \u03c8\u03b4 is chosen in accordance with Remark 1 and where L = \u03c8\u03b4(n).\nProof. By Remark 1, with probability P {F} \u2265 1 \u2212 \u03b4 the confidence intervals are valid for all t and all arms i \u2208 {1, . . . ,K}:\n|\u00b5\u0302i(t \u2212 1) \u2212 \u00b5i| \u2264 \u221a \u03c8\u03b4(Ti(t \u2212 1))/Ti(t \u2212 1)\n\u2264 \u221a\nL/Ti(t \u2212 1); we will henceforth assume that this is the case (i.e. that F holds). By the definition of the confidence intervals and by the construction of Algorithm 1 we immediately satisfy the constraint\nn\u2211 t=1 \u00b5It \u2265 (1 \u2212 \u03b1)n\u00b50 for all n.\nWe now bound the regret. Let i > 0 be the index of a sub-optimal arm and suppose It = i. Since the confidence intervals are valid,\n\u00b5\u2217 \u2264 \u03b8i(t) \u2264 \u00b5\u0302i(t \u2212 1) + \u221a\nL/Ti(t \u2212 1) \u2264 \u00b5i + 2 \u221a L/Ti(t \u2212 1) ,\nwhich implies that arm i has not been chosen too often; in particular we obtain\nTi(n) \u2264 Ti(n \u2212 1) + 1 \u2264 4L \u22062i + 1. (17)\nand the regret satisfies\nR\u0303n = K\u2211\ni=0\nTi(n)\u2206i \u2264 \u2211\ni>0:\u2206i>0\n( 4L \u2206i + \u2206i ) + T0(n)\u22060.\nIf \u22060 = 0 then the theorem holds trivially; we therefore assume that \u22060 > 0 and find an upper bound for T0(n).\nLet \u03c4 = max{t \u2264 n | It = 0} be the last round in which the default arm is played. Since F holds and \u03b80(t) = \u00b50 < \u00b5\u2217 < maxi \u03b8i(t), it follows that Jt = 0 is never the UCB choice; the default arm was only played because \u03be\u03c4 < 0:\nK\u2211 i=0 Ti(\u03c4 \u2212 1)\u03bbi(\u03c4) + \u03bbJ\u03c4 (\u03c4) \u2212 (1 \u2212 \u03b1)\u00b50\u03c4 < 0 (18)\nBy dropping \u03bbJ\u03c4 (\u03c4), replacing \u03c4 with \u2211K\ni=0 Ti(\u03c4\u22121) + 1, and rearranging the terms in (18), we get\n\u03b1T0(\u03c4 \u2212 1)\u00b50\n< (1 \u2212 \u03b1)\u00b50 + K\u2211\ni=1\nTi(\u03c4 \u2212 1) ((1 \u2212 \u03b1)\u00b50 \u2212 \u03bbi(\u03c4))\n\u2264 (1 \u2212 \u03b1)\u00b50\n+ K\u2211 i=1 Ti(\u03c4 \u2212 1) (1 \u2212 \u03b1)\u00b50 \u2212 \u00b5i + \u221a LTi(\u03c4 \u2212 1)  \u2264 1 +\nK\u2211 i=1 S i . (19)\nwhere ai = (1 \u2212 \u03b1)\u00b50 \u2212 \u00b5i and S i = Ti(\u03c4 \u2212 1) \u00b7 ( (1 \u2212 \u03b1)\u00b50 \u2212 \u00b5i + \u221a L/Ti(\u03c4 \u2212 1) ) = aiTi(\u03c4 \u2212 1) + \u221a LTi(\u03c4 \u2212 1)\nis a bound on the decrease in \u03bet in the first \u03c4\u2212 1 rounds due to choosing arm i. We will now bound S i for each i > 0.\nThe first case is ai \u2265 0, i.e. \u2206i \u2265 \u22060 + \u03b1\u00b50. Then (17) gives Ti(\u03c4 \u2212 1) \u2264 4L/\u22062i + 1 and we get\nS i \u2264 4Lai \u22062i + 2L \u2206i + 2 \u2264 6L \u2206i + 2 . (20)\nThe other case is ai < 0, i.e. \u2206i < \u22060 + \u03b1\u00b50. Then S i \u2264 \u221a\nLTi(\u03c4 \u2212 1) \u2264 2L \u2206i + 1, (21)\nand by using ax2 + bx \u2264 \u2212b2/4a for a < 0 we have\nS i \u2264 \u2212 L\n4ai = L 4(\u22060 + \u03b1\u00b50 \u2212 \u2206i) . (22)\nSummarizing (20) to (22) gives\nS i \u2264 6L\nmax{\u2206i,\u22060 \u2212 \u2206i} + 2 .\nContinuing from (19), we get\nT0(n) = T0(\u03c4 \u2212 1) + 1\n\u2264 2K + 2 \u03b1\u00b50 + 1 \u03b1\u00b50 K\u2211 i=1 6L max{\u2206i,\u22060 \u2212 \u2206i} .\nWe can now upper bound the regret by\nR\u0303n \u2264 \u2211\ni>0:\u2206i>0\n( 4L \u2206i + \u2206i ) + 2(K + 1)\u22060 \u03b1\u00b50\n+ 6L \u03b1\u00b50 K\u2211 i=1\n\u22060\nmax{\u2206i,\u22060 \u2212 \u2206i} . (10)\nWe will now show (11). To bound the regret due to the non-default arms, Jensen\u2019s inequality gives\u2211\ni>0\nTi(n)\u2206i 2 \u2264 m2 \u2211 i>0 Ti(n) m \u22062i ,\nwhere m \u2264 n is the number of times non-default arms were chosen. Combining this with \u22062i \u2264 4L/Ti(n) for suboptimal arms from (17) gives\u2211\ni>0\nTi(n)\u2206i \u2264 2 \u221a mKL \u2208 O( \u221a nKL).\nTo bound the regret due to the default arm, observe that max{\u2206i,\u22060 \u2212 \u2206i} \u2265 \u22060/2 and thus T0(n)\u22060 \u2208 O(KL/\u03b1\u00b50). Combining these two bounds gives (11)."}, {"heading": "B. Proof of Theorem 5", "text": "Theorem 5. Algorithm 1, modified as above to work without knowing \u00b50 but otherwise the same conditions as Theorem 2, satisfies with probability 1 \u2212 \u03b4 and for all time horizons n the constraint (5) and the regret bound\nR\u0303n \u2264 \u2211\ni:\u2206i>0\n( 4L \u2206i + \u2206i ) + 2(K + 1)\u22060 \u03b1\u00b50\n+ 7L \u03b1\u00b50 K\u2211 i=1\n\u22060\nmax{\u2206i,\u22060 \u2212 \u2206i} . (15)\nProof. We proceed very similarly to the proof of Theorem 2 in Appendix A. As we did there, we assume that F holds: the confidence intervals are valid for all rounds and all arms (including the default), which happens with probability P {F} \u2265 1 \u2212 \u03b4. To show that the modified algorithm satisfies the constraint (5), we write the budget (6) as\nZ\u0303t = K\u2211\ni=1\nTi(t \u2212 1)\u00b5i + \u00b5Jt + (T0(t \u2212 1) \u2212 (1 \u2212 \u03b1)t)\u00b50\nwhen the UCB arm Jt is chosen and show that it is indeed lower-bounded by\n\u03be\u2032t = K\u2211\ni=1\nTi(t \u2212 1)\u03bbi(t) + \u03bbJt (t)\n+ (T0(t \u2212 1) \u2212 (1 \u2212 \u03b1)t)\u03b80(t) . (14) This is apparent if T0(t \u2212 1) < (1 \u2212 \u03b1)t, since the last term\nin (14) is then negative and \u03b80(t) \u2265 \u00b50. On the other hand, if T0(t \u2212 1) \u2265 (1 \u2212 \u03b1)t then the constraint is still satisfied:\nt\u2211 s=1 \u00b5Is \u2265 T0(t \u2212 1)\u00b50 \u2265 (1 \u2212 \u03b1)\u00b50t.\nWe now upper-bound the regret. As in the earlier proof, we can show that for any arm i > 0 with \u2206i > 0 we have Ti(n) \u2264 4L/\u22062i + 1. If this also holds for i = 0 or if \u22060 = 0 then R\u0303n \u2264 \u2211 i:\u2206>0(4L/\u2206i + \u2206i) and the theorem holds trivially. From now on we only consider the case when \u22060 > 0 and T0(n) > 4L/\u220620 + 1. As before, we will proceed to upperbound T0(n).\nLet \u03c4 be the last round in which I\u03c4 = 0. We can ignore the possibility that J\u03c4 = 0, since then the above bound on Ti(n) would apply even to the default arm, contradicting our assumption above. Thus we can assume that the default arm was played because \u03be\u2032\u03c4 < 0:\nK\u2211 i=1 Ti(\u03c4 \u2212 1)\u03bbi(\u03c4) + \u03bbJ\u03c4 (\u03c4)\n+ ( T0(\u03c4 \u2212 1) \u2212 (1 \u2212 \u03b1)\u03c4 ) \u03b80(\u03c4) < 0 ,\nin which we drop \u03bbJ\u03c4 (\u03c4), replace \u03c4 with \u2211K\ni=0 Ti(\u03c4 \u2212 1) + 1, and rearrange the terms to get\n\u03b1T0(\u03c4 \u2212 1)\u03b80(\u03c4) < (1 \u2212 \u03b1)\u03b80(\u03c4)\n+ K\u2211 i=1 Ti(\u03c4 \u2212 1) ( (1 \u2212 \u03b1)\u03b80(\u03c4) \u2212 \u03bbi(\u03c4) ) . (23)\nWe lower-bound the left-hand side of (23) using \u03b80(\u03c4) \u2265 \u00b50, whereas we upper-bound the right-hand side using\n\u03b80(\u03c4) \u2264 \u00b50 + \u221a\nL T0(\u03c4 \u2212 1) \u2264 \u00b50 + \u22060 2 ,\nwhich comes from T0(\u03c4 \u2212 1) \u2265 4L/\u220620. Combining these in (23) with the lower confidence bound \u03bbi(\u03c4) \u2265 \u00b5i \u2212\u221a\nL/Ti(\u03c4 \u2212 1) gives \u03b1\u00b50T0(\u03c4 \u2212 1) < (1 \u2212 \u03b1) ( \u00b50 + \u22060\n2 ) +\nK\u2211 i=1 Ti(\u03c4 \u2212 1) ( (1 \u2212 \u03b1) ( \u00b50 + \u22060 2 )\n\u2212 \u00b5i + \u221a\nL Ti(\u03c4 \u2212 1) ) = (1 \u2212 \u03b1) ( \u00b50 + \u22060\n2\n) + K\u2211 i=1 S i\n\u2264 1 + K\u2211\ni=1\nS i , (24)\nwhere ai = (1 \u2212 \u03b1)(\u00b50 + \u22060/2) \u2212 \u00b5i and S i = aiTi(\u03c4 \u2212 1) + \u221a LTi(\u03c4 \u2212 1)\nis a bound on the decrease in \u03be\u2032t in the first \u03c4\u2212 1 rounds due to choosing arm i. We will now bound S i for each i > 0.\nAnalogously to the previous proof, we get the bounds\nS i \u2264 6L \u2206i + 2, when ai \u2265 0 ; (25) S i \u2264 2L \u2206i + 1 , otherwise; (26)\nand in the latter case, using ax2 + bx \u2264 \u2212b2/4a gives\nS i \u2264 \u2212 L\n4ai = L 4 ( (1 + \u03b1)\u22060/2 + \u03b1\u00b50 \u2212 \u2206i ) . (27) Summarizing (25) to (27) gives\nS i \u2264 6L max { \u2206i, 24 ( (1 + \u03b1)\u22060/2 + \u03b1\u00b50 \u2212 \u2206i )} + 2 \u2264 7L\nmax{\u2206i,\u22060 \u2212 \u2206i} + 2 .\nContinuing with (24), if T0(n) > 4L\u220620 + 1, we get\nT0(n) = T0(\u03c4 \u2212 1) + 1\n\u2264 2K + 2 \u03b1\u00b50 + 1 \u03b1\u00b50 K\u2211 i=1 7L max{\u2206i,\u22060 \u2212 \u2206i} .\nWe can now upper bound the regret by\nR\u0303n \u2264 \u2211\ni:\u2206i>0\n( 4L \u2206i + \u2206i ) + 2(K + 1)\u22060 \u03b1\u00b50\n+ 7L \u03b1\u00b50 K\u2211 i=1\n\u22060\nmax{\u2206i,\u22060 \u2212 \u2206i} . (15)"}, {"heading": "C. Proof of Theorem 7", "text": "Theorem 7. Any R\u0302\u03b4t -admissible algorithm A, when adapted with our safe-playing strategy, satisfies the constraint (2) and has a regret bound of Rn \u2264 t0 + R\u0302\u03b4n with probability at least 1\u2212\u03b4 where t0 = max{t |\u03b1\u00b50t \u2264 R\u0302\u03b4t +\u00b50}.\nProof of Theorem 7. It is clear from the description of the safe-playing strategy that it is indeed safe: the constraint (2) is always satisfied.\nThe algorithm plays safe when the following quantity, which is a lower bound on the budget Zt, is negative:\nZ\u2032t = Zt \u2212 Xt,It = t\u22121\u2211 s=1 Xs,Is \u2212 (1 \u2212 \u03b1)\u00b50t\nTo upper bound the regret, consider only the rounds in which our safe-playing strategy does not interfere with playingA\u2019s choice of arm. Then with probability 1 \u2212 \u03b4,\nmax i\u2208{0,...,K} t\u2211 s=1 1 { Z\u2032s \u2265 0 } (Xs,i \u2212 Xs,Is ) \u2264 R\u0302\u03b4B(t)\nwhere B(t) = \u2211t s=1 1 { Z\u2032s \u2265 0 } . Let \u03c4 be the last round in which the algorithm plays safe.\n\u00b50B(\u03c4 \u2212 1)\n\u2264 max i \u03c4\u22121\u2211 s=1 1 { Z\u2032s \u2265 0 } Xs,i\n\u2264 R\u0302\u03b4B(\u03c4\u22121) + \u03c4\u22121\u2211 s=1 1 { Z\u2032s \u2265 0 } Xs,Is\n= R\u0302\u03b4B(\u03c4\u22121) + \u03c4\u22121\u2211 s=1 Xs,Is \u2212 \u00b50(\u03c4 \u2212 1 \u2212 B(\u03c4 \u2212 1)) \u2264 R\u0302\u03b4B(\u03c4\u22121) + (1 \u2212 \u03b1)\u00b50\u03c4 \u2212 \u00b50(\u03c4 \u2212 1 \u2212 B(\u03c4 \u2212 1)) ,\nwhich indicates \u03b1\u00b50\u03c4 \u2264 R\u0302\u03b4\u03c4 + \u00b50 and thus \u03c4 \u2264 t0. It follows that Rn \u2264 t0 + R\u0302\u03b4n."}, {"heading": "D. Proof of Theorem 9", "text": "Theorem 9. Suppose for any \u00b5i \u2208 [0, 1] (i > 0) and \u00b50 satisfying\nmin{\u00b50, 1 \u2212 \u00b50} \u2265 max { 1/2 \u221a \u03b1, \u221a e + 1/2 } \u221a K/n,\nan algorithm satisfies E\u00b5 \u2211n\nt=1 Xt,It \u2265 (1\u2212\u03b1)\u00b50n. Then there is some \u00b5 \u2208 [0, 1]K such that its expected regret satisfies E\u00b5Rn \u2265 B where\nB = max  K(16e + 8)\u03b1\u00b50 , \u221a Kn \u221a 16e + 8 . (16)\nProof of Theorem 9. Pick any algorithm. We want to show that the algorithm\u2019s regret on some environment is at least as large as B. If E\u00b5Rn > B for some \u00b5 \u2208 [0, 1]K , there is nothing to be proven. Hence, without loss of generality, we can assume that the algorithm is consistent in the sense that E\u00b5Rn \u2264 B for all \u00b5 \u2208 [0, 1]K . For some \u2206 > 0, define environment \u00b5 \u2208 RK such that \u00b5i = \u00b50 \u2212 \u2206 for all i \u2208 [K]. For now, assume that \u00b50 and \u2206 are such that \u00b5i \u2265 0; we will get back to this condition later. Also define environment \u00b5(i) for each i = 1, . . . ,K by\n\u00b5(i)j = \u00b50 + \u2206, for j = i ;\u00b50 \u2212 \u2206, otherwise. In this proof, we use Ti = Ti(n) to denote the number of times arm i was chosen in the first n rounds. We distinguish two cases, based on how large the exploration budget is. Case 1: \u03b1 \u2265 \u221a\nK \u00b50 \u221a (16e + 8)n .\nIn this case, B = \u221a\nKn\u221a 16e+8 and we use \u2206 = (4e + 2)B/n. For each i \u2208 [K] define event Ai = {Ti \u2264 2B/\u2206}. First we prove\nthat P\u00b5(Ai) \u2265 1/2: P\u00b5{Ti \u2264 2B/\u2206} = 1 \u2212 P\u00b5{Ti > 2B/\u2206}\n\u2265 1 \u2212 \u2206E\u00b5[Ti]\n2B\n\u2265 1 \u2212 E\u00b5[Rn] 2B \u2265 1 2 .\nNext we prove that P\u00b5(i) (Ai) \u2264 1/4e: P\u00b5(i) {Ti \u2264 2B/\u2206} = P\u00b5(i) {n \u2212 Ti \u2265 n \u2212 2B/\u2206}\n\u2264 E\u00b5(i) [n \u2212 Ti] n \u2212 2B/\u2206 \u2264 B \u2206n \u2212 2B = 1 4e .\nNote that \u00b5 and \u00b5(i) differ only in the ith component: \u00b5i = \u00b50 \u2212 \u2206 whereas \u00b5(i)i = \u00b50 + \u2206. Then the KL divergence between the reward distributions of the ith arms is KL(\u00b5i, \u00b5 (i) i ) = (2\u2206)\n2/2 = 2\u22062. Define the binary relative entropy to be\nd(x, y) = x log x y + (1 \u2212 x) log 1 \u2212 x 1 \u2212 y ;\nit satisfies d(x, y) \u2265 (1/2) log(1/4y) for x \u2208 [1/2, 1] and y \u2208 (0, 1). By a standard change of measure argument (see, e.g., Kaufmann et al., 2015, Lemma 1) we get that\nE\u00b5[Ti] \u00b7 KL(\u00b5i; \u00b5(i)i ) \u2265 d(P\u00b5(Ai),P\u00b5(i) (Ai))\n\u2265 1 2 log 1 4(1/4e) = 1 2\nand so E\u00b5[Ti] \u2265 1/4\u22062 for each i \u2208 [K]. Hence\nE\u00b5[Rn] = \u2206 \u2211 i\u2208[K] E\u00b5[Ti] \u2265 K 4\u2206 = \u221a Kn \u221a 16e + 8 = B .\nCase 2: \u03b1 < \u221a\nK \u00b50 \u221a (16e + 8)n .\nIn this case, B = K(16e+8)\u03b1\u00b50 and we use \u2206 = K/4\u03b1\u00b50n. For each i define the event Ai = {Ti \u2264 2\u03b1\u00b50n/\u2206}. First we prove that P\u00b5(Ai) \u2265 1/2:\nP\u00b5{Ti \u2264 2\u03b1\u00b50n/\u2206} = 1 \u2212 P\u00b5{Ti > 2\u03b1\u00b50n/\u2206}\n\u2265 1 \u2212 \u2206E\u00b5[Ti] 2\u03b1\u00b50n \u2265 1 \u2212 E\u00b5[Rn] 2\u03b1\u00b50n \u2265 1 2 ,\nwhere we use the fact that E\u00b5[Rn] = n\u00b50 \u2212 E\u00b5 [ n\u2211\nt=1\nXt,It ]\n\u2264 n\u00b50 \u2212 (1 \u2212 \u03b1)\u00b50n = \u03b1\u00b50n.\nNext, we show that P\u00b5(i) (Ai) < 1/4e: P\u00b5(i) {Ti \u2264 2\u03b1\u00b50n/\u2206}\n= P\u00b5(i) {n \u2212 Ti \u2265 n \u2212 2\u03b1\u00b50n/\u2206} \u2264 E\u00b5(i) [n \u2212 Ti] n \u2212 2\u03b1\u00b50n/\u2206 \u2264 B \u2206n \u2212 2\u03b1\u00b50n = K\n(4e + 2)K \u2212 (32e + 16)\u03b12\u00b520n <\n1 4e .\nAs in the other case, we have E\u00b5[Ti] > 1/4\u22062 for each i \u2208 [K]. Therefore\nE\u00b5[Rn] = \u2206 \u2211 i\u2208[K] E\u00b5[Ti] > K 4\u2206 = \u03b1\u00b50n,\nwhich contradicts the fact that E\u00b5[Rn] \u2264 \u03b1\u00b50n. So there does not exist an algorithm whose worst-case regret is smaller than B.\nTo summarize, we proved that\nE\u00b5Rn \u2265  \u221a Kn \u221a 16e + 8 , when \u03b1 \u2265\n\u221a K\n\u00b50 \u221a\n(16e + 8)n K\n(16e + 8)\u03b1\u00b50 , otherwise,\nfinishing the proof."}], "references": [{"title": "Sample mean based index policies with o(log n) regret for the multi-armed bandit problem", "author": ["R. Agrawal"], "venue": "Advances in Applied Probability,", "citeRegEx": "Agrawal.,? \\Q1995\\E", "shortCiteRegEx": "Agrawal.", "year": 1995}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Regret to the best vs. regret to the average", "author": ["E. Even-Dar", "M. Kearns", "Y. Mansour", "J. Wortman"], "venue": "Machine Learning,", "citeRegEx": "Even.Dar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2008}, {"title": "Distributed policy search reinforcement learning for job-shop scheduling", "author": ["T. Gabel", "M. Riedmiller"], "venue": "tasks. International Journal of Production Research,", "citeRegEx": "Gabel and Riedmiller.,? \\Q2011\\E", "shortCiteRegEx": "Gabel and Riedmiller.", "year": 2011}, {"title": "A comprehensive survey on safe reinforcement learning", "author": ["J. Garc\u0131\u0301a", "F. Fern\u00e1ndez"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Garc\u0131\u0301a and Fern\u00e1ndez.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a and Fern\u00e1ndez.", "year": 2015}, {"title": "Informational confidence bounds for selfnormalized averages and applications", "author": ["A. Garivier"], "venue": "arXiv preprint arXiv:1309.3376,", "citeRegEx": "Garivier.,? \\Q2013\\E", "shortCiteRegEx": "Garivier.", "year": 2013}, {"title": "Adaptive online prediction by following the perturbed leader", "author": ["M. Hutter", "J. Poland"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hutter and Poland.,? \\Q2005\\E", "shortCiteRegEx": "Hutter and Poland.", "year": 2005}, {"title": "lil\u2019UCB: An optimal exploration algorithm for multiarmed bandits", "author": ["K. Jamieson", "M. Malloy", "R. Nowak", "S. Bubeck"], "venue": null, "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Sequential choice from several populations", "author": ["M.N. Katehakis", "H. Robbins"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Katehakis and Robbins.,? \\Q1995\\E", "shortCiteRegEx": "Katehakis and Robbins.", "year": 1995}, {"title": "On the complexity of best arm identification in multi-armed bandit models", "author": ["E. Kaufmann", "A. Garivier", "O. Capp\u00e9"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kaufmann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2015}, {"title": "The Pareto regret frontier", "author": ["W.M. Koolen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Koolen.,? \\Q2013\\E", "shortCiteRegEx": "Koolen.", "year": 2013}, {"title": "The Pareto regret frontier for bandits", "author": ["T. Lattimore"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Lattimore.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore.", "year": 2015}, {"title": "Optimally confident UCB : Improved regret for finite-armed bandits", "author": ["T. Lattimore"], "venue": "Technical report,", "citeRegEx": "Lattimore.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore.", "year": 2015}, {"title": "Towards automatic experimentation of educational knowledge", "author": ["Y.-E. Liu", "T. Mandel", "E. Brunskill", "Z. Popovi\u0107"], "venue": "In SIGCHI Conference on Human Factors in Computing Systems (CHI", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Explore no more: Improved high-probability regret bounds for non-stochastic bandits", "author": ["G. Neu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Neu.,? \\Q2015\\E", "shortCiteRegEx": "Neu.", "year": 2015}, {"title": "Learning effective multimodal dialogue strategies from Wizard-of-Oz data: Bootstrapping and evaluation", "author": ["V. Rieser", "O. Lemon"], "venue": "In ACL-08: HLT,", "citeRegEx": "Rieser and Lemon.,? \\Q2008\\E", "shortCiteRegEx": "Rieser and Lemon.", "year": 2008}, {"title": "Exploiting easy data in online optimization", "author": ["A. Sani", "G. Neu", "A. Lazaric"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sani et al\\.", "year": 2014}, {"title": "Safe exploration for optimization with Gaussian processes", "author": ["Y. Sui", "A. Gotovos", "J. Burdick", "A. Krause"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Sui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sui et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "In these cases a designer may feel that experimenting with sub-par interaction strategies could cause more harm than good (e.g., Rieser and Lemon, 2008; Liu et al., 2014).", "startOffset": 122, "endOffset": 170}, {"referenceID": 3, "context": ", Gabel and Riedmiller, 2011), but deviating too much from established \u201cbest practices\u201d will often be considered too dangerous. For examples from other domains see the survey paper of Garc\u0131\u0301a and Fern\u00e1ndez (2015).", "startOffset": 2, "endOffset": 213}, {"referenceID": 7, "context": "The manager even thinks that perhaps a best-arm identification strategy from the bandit literature, such as the recent lil\u2019UCB method of Jamieson et al. (2014), could be more suitable.", "startOffset": 137, "endOffset": 160}, {"referenceID": 8, "context": "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a \u201csmoother\u201d fashion.", "startOffset": 229, "endOffset": 366}, {"referenceID": 1, "context": "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a \u201csmoother\u201d fashion.", "startOffset": 229, "endOffset": 366}, {"referenceID": 8, "context": "In fact, this approach has been studied recently by Lattimore (2015a) (in a slightly more general setting than ours).", "startOffset": 52, "endOffset": 70}, {"referenceID": 8, "context": "In fact, this approach has been studied recently by Lattimore (2015a) (in a slightly more general setting than ours). However, the algorithm of Lattimore (2015a) cannot be guaranteed to maintain the return constraint uniformly in time.", "startOffset": 52, "endOffset": 162}, {"referenceID": 0, "context": "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a \u201csmoother\u201d fashion. (iii) We prove that the new algorithm, which we call Conservative UCB, meets the uniform return constraint (in various senses), while it can achieve significantly less regret than BudgetFirst. In particular, while BudgetFirst is shown to pay a multiplicative penalty in the regret for maintaining the return constraint, Conservative UCB only pays an additive penalty. We provide both high probability and expectation bounds, consider both high probability and expectation constraints on the return, and also consider the case when the payoff of the default arm is initially unknown. (iv) We also prove a lower bound on the best regret given the constraint and as a result show that the additive penalty is unavoidable; thus Conservative UCB achieves the optimal regret in a worst-case sense. While Unbalanced MOSS of Lattimore (2015a), when specialized to our setting, also achieves the optimal regret (as follows from the analysis of Lattimore, 2015a), as mentioned earlier it does not maintain the constraint uniformly in time (it will explore too much at the beginning of time); it also relies heavily on the knowledge of the mean payoff of the default strategy.", "startOffset": 304, "endOffset": 1227}, {"referenceID": 0, "context": "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a \u201csmoother\u201d fashion. (iii) We prove that the new algorithm, which we call Conservative UCB, meets the uniform return constraint (in various senses), while it can achieve significantly less regret than BudgetFirst. In particular, while BudgetFirst is shown to pay a multiplicative penalty in the regret for maintaining the return constraint, Conservative UCB only pays an additive penalty. We provide both high probability and expectation bounds, consider both high probability and expectation constraints on the return, and also consider the case when the payoff of the default arm is initially unknown. (iv) We also prove a lower bound on the best regret given the constraint and as a result show that the additive penalty is unavoidable; thus Conservative UCB achieves the optimal regret in a worst-case sense. While Unbalanced MOSS of Lattimore (2015a), when specialized to our setting, also achieves the optimal regret (as follows from the analysis of Lattimore, 2015a), as mentioned earlier it does not maintain the constraint uniformly in time (it will explore too much at the beginning of time); it also relies heavily on the knowledge of the mean payoff of the default strategy. (v) We also consider the adversarial setting where we design an algorithm similar to Conservative UCB: the algorithm uses an underlying \u201cbase\u201d adversarial bandit strategy when it finds that the return so far is sufficiently higher than the minimum required return. We prove that the resulting method indeed maintains the return constraint uniformly in time and we also prove a high-probability bound on its regret. We find, however, that the additive penalty in this case is higher than in the stochastic case. Here, the Exp3-\u03b3 algorithm of Lattimore (2015a) is an alternative, but again, this algorithm is not able to maintain the return constraint uniformly in time.", "startOffset": 304, "endOffset": 2117}, {"referenceID": 6, "context": "In the full information, mostly studied in the adversarial setting, much work has been devoted to understanding the price of such constraints (Hutter and Poland, 2005; EvenDar et al., 2008; Koolen, 2013; Sani et al., 2014, e.g.,). In particular, Koolen (2013) studies the Pareto frontier of regret vectors (which contains the non-dominated worst-case regret vectors of all algorithms).", "startOffset": 143, "endOffset": 260}, {"referenceID": 6, "context": "In the full information, mostly studied in the adversarial setting, much work has been devoted to understanding the price of such constraints (Hutter and Poland, 2005; EvenDar et al., 2008; Koolen, 2013; Sani et al., 2014, e.g.,). In particular, Koolen (2013) studies the Pareto frontier of regret vectors (which contains the non-dominated worst-case regret vectors of all algorithms). The main lesson of these works is that in the full information setting even a constant regret to a fixed default action can be maintained with essentially no increase in the regret to the best action. The situation quickly deteriorates in the bandit setting as shown by Lattimore (2015a). This is perhaps unsurprising given that, as opposed to the full information setting, in the bandit setting one needs to actively explore to get improved estimates", "startOffset": 143, "endOffset": 674}, {"referenceID": 17, "context": "Another line of work considers safe exploration in the related context of optimization (Sui et al., 2015).", "startOffset": 87, "endOffset": 105}, {"referenceID": 4, "context": "Garc\u0131\u0301a and Fern\u00e1ndez (2015) provides a comprehensive survey of the relevant literature.", "startOffset": 0, "endOffset": 29}, {"referenceID": 5, "context": "where \u03b6 = K/\u03b4; it can be seen to achieve (8) by more careful analysis motivated by Garivier (2013),", "startOffset": 83, "endOffset": 99}, {"referenceID": 14, "context": "For example, Neu (2015) states the following for the any-time version of their algorithm: given any time horizon n and confidence level \u03b4, P { Rn \u2264 R\u0302n(\u03b4) } \u2265 1 \u2212 \u03b4 for some sub-linear R\u0302t(\u03b4).", "startOffset": 13, "endOffset": 24}, {"referenceID": 14, "context": "The any-time high probability algorithm of Neu (2015) adapted with our safe-playing strategy gives R\u0302t = 7 \u221a Kt log K log(4t2/\u03b4) and", "startOffset": 43, "endOffset": 54}, {"referenceID": 11, "context": "The theorem above almost follows from the lower bound given by Lattimore (2015a), but in that paper \u03bc0 is unknown, while here it may be known.", "startOffset": 63, "endOffset": 81}, {"referenceID": 11, "context": "We evaluate the performance of Conservative UCB compared to UCB and Unbalanced MOSS Lattimore (2015a) using simulated data in two regimes.", "startOffset": 84, "endOffset": 102}, {"referenceID": 11, "context": "The quantity Bi determines the regret of the algorithm with respect to arm i up to constant factors, and must be chosen to lie inside the Pareto frontier given by Lattimore (2015a). It should be emphasised that Unbalanced MOSS does not constraint the return except for the last round, and has no high-probability guarantees.", "startOffset": 163, "endOffset": 181}], "year": 2016, "abstractText": "We study a novel multi-armed bandit problem that models the challenge faced by a company wishing to explore new strategies to maximize revenue whilst simultaneously maintaining their revenue above a fixed baseline, uniformly over time. While previous work addressed the problem under the weaker requirement of maintaining the revenue constraint only at a given fixed time in the future, the algorithms previously proposed are unsuitable due to their design under the more stringent constraints. We consider both the stochastic and the adversarial settings, where we propose, natural, yet novel strategies and analyze the price for maintaining the constraints. Amongst other things, we prove both high probability and expectation bounds on the regret, while we also consider both the problem of maintaining the constraints with high probability or expectation. For the adversarial setting the price of maintaining the constraint appears to be higher, at least for the algorithm considered. A lower bound is given showing that the algorithm for the stochastic setting is almost optimal. Empirical results obtained in synthetic environments complement our theoretical findings.", "creator": "LaTeX with hyperref package"}}}