{"id": "1602.04375", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Feb-2016", "title": "Science Question Answering using Instructional Materials", "abstract": "can we provide a reliable solution software for elementary science citation test using instructional materials. we posit that learning there typically is a hidden structure that precisely explains the apparent correctness of an answer given the question and instructional materials. and learning present forms a unified max - mat margin framework that learns systematically to find combining these various hidden structures ( given roughly a broad corpus of question - answer comparison pairs and revised instructional textbook materials ), meets and uses approximately what it learns to successfully answer novel extended elementary science questions. our evaluation test shows that our framework far outperforms easily several strong baselines.", "histories": [["v1", "Sat, 13 Feb 2016 20:13:48 GMT  (2363kb,D)", "https://arxiv.org/abs/1602.04375v1", null], ["v2", "Tue, 5 Apr 2016 01:17:56 GMT  (547kb,D)", "http://arxiv.org/abs/1602.04375v2", "Corrected that the science QA dataset is NOT freely available"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR cs.LG", "authors": ["mrinmaya sachan", "kumar avinava dubey", "eric p xing"], "accepted": true, "id": "1602.04375"}, "pdf": {"name": "1602.04375.pdf", "metadata": {"source": "CRF", "title": "Science Question Answering using Instructional Materials", "authors": ["Mrinmaya Sachan", "Avinava Dubey", "Eric P. Xing"], "emails": ["epxing}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "We propose an approach for answering multiplechoice elementary science tests (Clark, 2015) using the science curriculum of the student and other domain specific knowledge resources. Our approach learns latent answer-entailing structures that align question-answers with appropriate snippets in the curriculum. The student curriculum usually comprises of a set of textbooks. Each textbook, in-turn comprises of a set of chapters, each chapter is further divided into sections \u2013 each discussing a particular science concept. Hence, the answer-entailing structure consists of selecting a particular textbook from the curriculum, picking a chapter in the textbook, picking a section in the chapter, picking a few sentences in the section and then aligning words/multi-word expressions (mwe\u2019s) in the hypothesis (formed by combining the question and an answer candidate) to a words/mwe\u2019s in the picked sentences. The answerentailing structures are further refined using external domain-specific knowledge resources such as science dictionaries, study guides and semistructured tables (see Figure 1). These domain-\nspecific knowledge resources can be very useful forms of knowledge representation as shown in previous works (Clark et al., 2016).\nAlignment is a common technique in many NLP applications such as MT (Blunsom and Cohn, 2006), RTE (Sammons et al., 2009; MacCartney et al., 2008; Yao et al., 2013; Sultan et al., 2014), QA (Berant et al., 2013; Yih et al., 2013; Yao and Van Durme, 2014; Sachan et al., 2015), etc. Yet, there are three key differences between our approach and alignment based approaches for QA in the literature: (i) We incorporate the curriculum hierarchy (i.e. the book, chapter, section bifurcation) into the latent structure. This helps us jointly learn the retrieval and answer selection modules of a QA system. Retrieval and answer selection are usually designed as isolated or loosely connected components in QA systems (Ferrucci, 2012) leading to loss in performance \u2013 our approach mitigates this shortcoming. (ii) Modern textbooks typically provide a set of review questions after each section to help students understand the material better. We make use of these review problems to further improve our model. These review problems have additional value as part of the latent structure is known for these questions. (ii) We utilize domain-specific knowledge sources such as study guides, science dictionaries or semistructured knowledge tables within our model.\nThe joint model is trained in max-margin fashion using a latent structural SVM (LSSVM) where the answer-entailing structures are latent. We train and evaluate our models on a set of 8th grade science problems, science textbooks and multiple domain-specific knowledge resources. We achieve superior performance vs. a number of baselines."}, {"heading": "2 Method", "text": "Science QA as Textual Entailment: First, we\nar X\niv :1\n60 2.\n04 37\n5v 2\n[ cs\n.C L\n] 5\nA pr\n2 01\n6\nconsider the case when review questions are not used. For each question qi \u2208 Q, let Ai = {ai1, . . . , aim} be the set of candidate answers to the question 1. We cast the science QA problem as a textual entailment problem by converting each question-answer candidate pair (qi, ai,j) into a hypothesis statement hij (see Figure 1)2. For each question qi, the science QA task thereby reduces to picking the hypothesis h\u0302i that has the highest likelihood of being entailed by the curriculum among the set of hypotheses hi = {hi1, . . . , him} generated for that question. Let h\u2217i \u2208 hi be the correct hypothesis corresponding to the correct answer. Latent Answer-Entailing Structures help the model in providing evidence for the correct hypothesis. As described before, the structure depends on: (a) snippet from the curriculum hierarchy chosen to be aligned to the hypothesis, (b) external knowledge relevant for this entailment, and (c) the word/mwe alignment. The snippet from the curriculum to be aligned to the hypothesis is determined by walking down the curriculum hierarchy and then picking a set of sentences from the section chosen. Then, a subset of relevant external knowledge in the form of triples and equivalences (called knowledge bits) is selected from our\n1Candidate answers may be pre-defined, as in multiplechoice QA, or may be undefined but easy to extract with a degree of confidence (e.g., by using a pre-existing system)\n2We use a set of question matching/rewriting rules to achieve this transformation. The rules match each question into one of a large set of pre-defined templates and applies a unique transformation to the question & answer candidate to achieve the hypothesis. Code provided in the supplementary.\nreservoir of external knowledge (science dictionaries, cheat sheets, semi-structured tables, etc). Finally, words/mwe\u2019s in the hypothesis are aligned to words/mwe\u2019s in the snippet or knowledge bits. Learning these alignment edges helps the model determine which semantic constituents should be compared to each other. These alignments are also used to generate more effective features. The choice of snippets, choice of the relevant external knowledge and the alignments in conjunction form the latent answer-entailing structure. Let zij represent the latent structure for the question-answer candidate pair (qi, ai,j). Max-Margin Approach: We treat science QA as a structured prediction problem of ranking the hypothesis set hi such that the correct hypothesis is at the top of this ranking. We learn a scoring function Sw(h, z) with parameter w such that the score of the correct hypothesis h\u2217i and the corresponding best latent structure z\u2217i is higher than the score of the other hypotheses and their corresponding best latent structures. In fact, in a max-margin fashion, we want that Sw(h\u2217i , z \u2217 i ) > S(hij , zij) + 1 \u2212 \u03bei for all hj \u2208 h \\ h\u2217 for some slack \u03bei. Writing the relaxed max margin formulation:\nmin ||w||\n1 2 ||w||22 + C \u2211 i max zij ,hij\u2208hi\\h\u2217i Sw(hij , zij) + \u2206(h \u2217 i , hij)\n\u2212C \u2211 i Sw(h \u2217 i , z \u2217 i ) (1)\nWe use 0-1 cost, i.e. \u2206(h\u2217i , hij) = 1(h \u2217 i 6= hij) If\nthe scoring function is convex then this objective is in concave-convex form and hence can be\nsolved by the concave-convex programming procedure (CCCP) (Yuille and Rangarajan, 2003). We assume the scoring function to be linear:Sw(h, z) = wT\u03c8(h, z). Here, \u03c8(h, z) is a feature map discussed later. The CCCP algorithm essentially alternates between solving for z\u2217i , zij \u2200j s.t. hij \u2208 hi \\ h\u2217i and w to achieve a local minima. In the absence of information regarding the latent structure z we pick the structure that gives the best score for a given hypothesis i.e. arg maxz Sw(h, z). The complete procedure is given in the supplementary. Inference and knowledge selection: We use beam search with a fixed beam size (5) for inference. We infer the textbook, chapter, section, snippet and alignments one by one in this order. In each step, we only expand the five most promising (given by the current score) substructure candidates so far. During inference, we select top 5 knowledge bits (triples, equivalences, etc.) from the knowledge resources that could be relevant for this question-answer. This is done heuristically by picking knowledge bits that explain parts of the hypothesis not explained by the chosen snippets. Incorporating partially known structures: Now, we describe how review questions can be incorporated. As described earlier, modern textbooks often provide review problems at the end of each section. These review problems have value as part of the answer-entailing structure (textbook, chapter and section) is known for these problems. In this case, we use the formulation (equation 1) except that the max over z for the review questions is only taken over the unknown part of the latent structure. Multi-task Learning: Question analysis is a key component of QA systems. Incoming questions are often of different types (counting, negation, entity queries, descriptive questions, etc.). Different types of questions usually require different processing strategies. Hence, we also extend of our LSSVM model to a multi-task setting where each question qi now also has a pre-defined associated type ti and each question-type is treated as a separate task. Yet, parameters are shared across tasks,which allows the model to exploit the commonality among tasks when required. We use the MTLSSVM formulation from Evgeniou and Pontil (2004) which was also used in a reading comprehension setting by Sachan et al. (2015). In a nutshell, the approach redefines the LSSVM\nfeature map and shows that the MTLSSVM objective takes the same form as equation 1 with a kernel corresponding to the feature map. Hence, one can simply redefine the feature map and reuse LSSVM algorithm to solve the MTLSSVM. Features: Our feature vector \u03c8(h, z) decomposes into five parts, where each part corresponds to a part of the answer-entailing structure. For the first part, we index all the textbooks and score the top retrieved textbook by querying the hypothesis statement. We use tf-idf and BM25 scorers resulting in two features. Then, we find the jaccard similarity of bigrams and trigrams in the hypothesis and the textbook to get two more features for the first part. Similarly, for the second part we index all the textbook chapters and compute the tf-idf, BM25 and bigram, trigram features. For the third part we index all the sections instead. The fourth part has features based on the text snippet part of the answer-entailing structure. Here we do a deeper linguistic analysis and include features for matching local neighborhoods in the snippet and the hypothesis: features for matching bigrams, trigrams, dependencies, semantic roles, predicate-argument structure as well as the global syntactic structure: a tree kernel for matching dependency parse trees of entire sentences (Srivastava and Hovy, 2013). If a text snippet contains the answer to the question, it should intuitively be similar to the question as well as to the answer. Hence, we add features that are the element-wise product of features for the text-question match and text-answer match. Finally, we also have features corresponding to the RST (Mann and Thompson, 1988) and coreference links to enable inference across sentences. RST tells us that sentences with discourse relations are related to each other and can help us answer certain kinds of questions (Jansen et al., 2014). For example, the \u201ccause\u201d relation between sentences in the text can often give cues that can help us answer \u201cwhy\u201d or \u201chow\u201d questions. Hence, we add additional features - conjunction of the rhetorical structure label from a RST parser and the question word - to our feature vector. Similarly, the entity and event co-reference relations allow us to reason about repeating entities or events. Hence, we replace an entity/event mention with their first mentions if that results into a greater score. For the alignment part, we induce features based on word/mwe level similarity of aligned words:\n(a) Surface-form match (Edit-distance), and (b) Semantic word match (cosine similarity using SENNA word vectors (Collobert et al., 2011) and \u201cAntonymy\u201d \u2018Class-Inclusion\u2019 or \u2018Is-A\u2019 relations using Wordnet). Distributional vectors for mwe\u2019s are obtained by adding the vector representations of comprising words (Mitchell and Lapata, 2008). To account for the hypothesized knowledge bits, whenever we have the case that a word/mwe in the hypothesis can be aligned to a word/mwe in a hypothesized knowledge bit to produce a greater score, then we keep the features for the alignment with the knowledge bit instead. Negation Negation is a concern for our approach as facts usually align well with their negated versions. To overcome this, we use a simple heuristic. During training, if we detect negation using a set of simple rules that test for the presence of negation words (\u201cnot\u201d, \u201cn\u2019t\u201d, etc.), we flip the partial order adding constraints that require that the correct hypothesis to be ranked below all the incorrect ones. During test phase if we detect negation, we predict the answer corresponding to the hypothesis with the lowest score."}, {"heading": "3 Experiments", "text": "Dataset: We used a set of 8th grade science questions released as the training set in the Allen AI Science Challenge3 for training and evaluating our model. The dataset comprises of 2500 questions. Each question has 4 answer candidates, of which exactly one is correct. We used questions 1- 1500 for training, questions 1500-2000 for development and questions 2000-2500 for testing. We also used publicly available 8th grade science textbooks available through ck12.org. The science curriculum consists of seven textbooks on Physics, Chemistry, Biology, Earth Science and Life Science. Each textbook on an average has 18 chapters, and each chapter in turn is divided into 12 sections on an average. Also, as described before, each section, on an average, is followed by 3-4 multiple choice review questions (total 1369 review questions). We collected a number of domain specific science dictionaries, study guides, flash cards and semi-structured tables (Simple English Wiktionary and Aristo Tablestore) available online and create triples and equivalences used as external knowledge.\n3https://www.kaggle.com/c/the-allen-ai-sciencechallenge/\nBaselines: We compare our framework with ten baselines. The first two baselines (Lucene and PMI) are taken from Clark et al. (2016). The Lucene baseline scores each answer candidate ai by searching for the combination of the question q and answer candidate ai in a lucene-based search engine and returns the highest scoring answer candidate. The PMI baseline similarly scores each answer candidate ai by computing the pointwise mutual information to measure the strength of the association between parts of the questionanswer candidate combine and parts of the CK12 curriculum. The next three baselines, inspired from Richardson et al. (2013), retrieve the top two CK12 sections querying q+ai in Lucene and score the answer candidates using these documents. The SW and SW+D baselines match bag of words constructed from the question and the answer answer candidate to the retrieved document. The RTE baseline uses textual entailment (Stern and Dagan, 2012) to score answer candidates as the likelihood of being entailed by the retrieved document. Then we also tried other approaches such as the RNN approach described in Clark et al. (2016), Jacana aligner (Yao et al., 2013) and two neural network approaches, LSTM (Hochreiter and Schmidhuber, 1997) and QANTA (Iyyer et al., 2014) They form our next four baselines. To test if our approach indeed benefits from jointly learning the retrieval and the answer selection modules, our final baseline Lucene+LSSVM Alignment retrieves the top section by querying q + ai in Lucene and then learns the remaining answer-entailment structure (alignment part of the answer-entailing structure in Figure 1) using a LSSVM. Task Classification for Multitask Learning: We explore two simple question classification schemes. The first classification scheme classifies questions based on the question word (what, why, etc.). We call this Qword classification. The second scheme is based on the type of the question asked and classifies questions into three coarser categories: (a) questions without context,\n(b) questions with context and (c) negation questions. This classification is based on the observation that many questions lay down some context and then ask a science concept based on this context. However, other questions are framed without any context and directly ask for the science concept itself. Then there is a smaller, yet, important subset of questions that involve negation that also needs to be handled separately. Table 1 gives examples of this classification. We call this classification Qtype classification4. Results: We compare variants of our method5 where we consider our modification for negation or not and multi-task LSSVMs. We consider both kinds of task classification strategies and joint training (JT). Finally, we compare our methods against the baselines described above. We report accuracy (proportion of questions correctly answered) in our results. Figure 2 shows the results. First, we can immediately observe that all the LSSVM models have a better performance than all the baselines. We also found an improvement when we handle negation using the heuristic described above6. MTLSSVMs showed a boost over single task LSSVM. Qtype classification scheme was found to work better than Qword classification which simply classifies questions based on the question word. The multi-task learner could benefit even more if we can learn a better separation between the various strategies needed to answer science questions. We found that joint training with review questions helped improve accuracy as well.\nFeature Ablation: As described before, our feature set comprises of five parts, where each part corresponds to a part of the answer-entailing structure \u2013 textbook (z1), chapter (z2), section (z3), snippets (z4), and alignment (z5). It is interesting to know the relative importance of these parts in our model. Hence, we perform feature ablation on our best performing model - MTLSSVM(QWord, JT) where we remove the five feature parts one by one and measure the loss in accuracy. Figure\n4We wrote a set of question matching rules (similar to the rules used to convert question answer pairs to hypotheses) to achieve this classification\n5We tune the SVM regularization parameter C on the development set. We use Stanford CoreNLP, the HILDA parser (Feng and Hirst, 2014), and jMWE (Kulkarni and Finlayson, 2011) for linguistic preprocessing\n6We found that the accuracy over test questions tagged by our heuristic as negation questions went up from 33.64 percent to 42.52 percent and the accuracy over test questions not tagged as negation did not decrease significantly\n3 shows that the choice of section and alignment are important components of our model. Yet, all components are important and removing any of them will result in a loss of accuracy. Finally, in order to understand the value of external knowledge resources (K), we removed the component that induces and aligns the hypothesis with knowledge bits. This results in significant loss in performance, estabishing the efficacy of adding in external knowledge via our approach."}, {"heading": "4 Conclusion", "text": "We addressed the problem of answering 8th grade science questions using textbooks, domain specific dictionaries and semi-structured tables. We posed the task as an extension to textual entailment and proposed a solution that learns latent structures that align question answer pairs with appropriate snippets in the textbooks. Using domain specific dictionaries and semi-structured tables, we further refined the structures. The task required handling a variety of question types so we extended our technique to multi-task setting. Our technique showed improvements over a number of baselines. Finally, we also used a set of associated review questions, which were used to gain further improvements."}], "references": [{"title": "Semantic parsing on freebase from question-answer pairs", "author": ["Andrew Chou", "Roy Frostig", "Percy Liang"], "venue": "In Proceedings of Empirical Methods in Natural Language Processing,", "citeRegEx": "Berant et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Discriminative word alignment with conditional random fields", "author": ["Blunsom", "Cohn2006] Phil Blunsom", "Trevor Cohn"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Asso-", "citeRegEx": "Blunsom et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2006}, {"title": "Combining retrieval, statistics, and inference to answer elementary science questions", "author": ["Clark et al.2016] Peter Clark", "Oren Etzioni", "Daniel Khashabi", "Tushar Khot", "Ashish Sabharwal", "Oyvind Tafjord", "Peter Turney"], "venue": "Proceedings of AAAI", "citeRegEx": "Clark et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2016}, {"title": "Elementary School Science and Math Tests as a Driver for AI:Take the Aristo Challenge", "author": ["Peter Clark"], "venue": "In Proceedings of IAAI", "citeRegEx": "Clark.,? \\Q2015\\E", "shortCiteRegEx": "Clark.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Regularized multi\u2013task learning", "author": ["Evgeniou", "Pontil2004] Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Evgeniou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2004}, {"title": "A linear-time bottom-up discourse parser with constraints and post-editing", "author": ["Feng", "Hirst2014] Vanessa Wei Feng", "Graeme Hirst"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Feng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2014}, {"title": "Introduction to \u201cthis is watson", "author": ["David A Ferrucci"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "Ferrucci.,? \\Q2012\\E", "shortCiteRegEx": "Ferrucci.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer et al.2014] Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Proceedings of Empirical Methods in Natural Language Process-", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Discourse complements lexical semantics for non-factoid answer reranking", "author": ["Jansen et al.2014] Peter Jansen", "Mihai Surdeanu", "Peter Clark"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Jansen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jansen et al\\.", "year": 2014}, {"title": "jmwe: A java toolkit for detecting multi-word expressions", "author": ["Kulkarni", "Finlayson2011] Nidhi Kulkarni", "Mark Alan Finlayson"], "venue": "In Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World,", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "A phrasebased alignment model for natural language inference", "author": ["Michel Galley", "Christopher D Manning"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "MacCartney et al\\.,? \\Q2008\\E", "shortCiteRegEx": "MacCartney et al\\.", "year": 2008}, {"title": "Rhetorical Structure Theory: Toward a functional theory of text organisation", "author": ["Mann", "Thompson1988] William C Mann", "Sandra A Thompson"], "venue": null, "citeRegEx": "Mann et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Mann et al\\.", "year": 1988}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata"], "venue": "ACL", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Christopher JC Burges", "Erin Renshaw"], "venue": "In Proceedings of Empirical Methods in Natural Language Processing", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Learning answer-entailing structures for machine comprehension", "author": ["Avinava Dubey", "Eric P Xing", "Matthew Richardson"], "venue": "In Proceedings of the Annual Meeting of the Association", "citeRegEx": "Sachan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sachan et al\\.", "year": 2015}, {"title": "Relation alignment for textual entailment recognition", "author": ["Sammons et al.2009] M. Sammons", "V. Vydiswaran", "T. Vieira", "N. Johri", "M. Chang", "D. Goldwasser", "V. Srikumar", "G. Kundu", "Y. Tu", "K. Small", "J. Rule", "Q. Do", "D. Roth"], "venue": "In TAC", "citeRegEx": "Sammons et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sammons et al\\.", "year": 2009}, {"title": "A walk-based semantically enriched tree kernel over distributed word representations", "author": ["Srivastava", "Hovy2013] Shashank Srivastava", "Dirk Hovy"], "venue": "In Proceedings of Empirical Methods in Natural Language Processing,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "Biutee: A modular open-source system for recognizing textual entailment", "author": ["Stern", "Dagan2012] Asher Stern", "Ido Dagan"], "venue": "In Proceedings of the ACL 2012 System Demonstrations,", "citeRegEx": "Stern et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stern et al\\.", "year": 2012}, {"title": "Back to basics for monolingual alignment: Exploiting word similarity and contextual evidence", "author": ["Steven Bethard", "Tamara Sumner"], "venue": "Transactions of the Association of Computational Linguistics", "citeRegEx": "Sultan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sultan et al\\.", "year": 2014}, {"title": "Information extraction over structured data: Question answering with freebase", "author": ["Yao", "Van Durme2014] Xuchen Yao", "Benjamin Van Durme"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Yao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "A lightweight and high performance monolingual word aligner", "author": ["Yao et al.2013] Xuchen Yao", "Benjamin Van Durme", "Chris Callison-Burch", "Peter Clark"], "venue": "ACL", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Question answering using enhanced lexical semantic models", "author": ["Yih et al.2013] Wentau Yih", "Ming-Wei Chang", "Christopher Meek", "Andrzej Pastusiak"], "venue": "In Proceedings of the 51st Annual Meeting of the Association", "citeRegEx": "Yih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yih et al\\.", "year": 2013}, {"title": "The concave-convex procedure", "author": ["Yuille", "Rangarajan2003] A.L. Yuille", "Anand Rangarajan"], "venue": "Neural Comput", "citeRegEx": "Yuille et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yuille et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "choice elementary science tests (Clark, 2015) using the science curriculum of the student and other domain specific knowledge resources.", "startOffset": 32, "endOffset": 45}, {"referenceID": 2, "context": "These domainspecific knowledge resources can be very useful forms of knowledge representation as shown in previous works (Clark et al., 2016).", "startOffset": 121, "endOffset": 141}, {"referenceID": 17, "context": "applications such as MT (Blunsom and Cohn, 2006), RTE (Sammons et al., 2009; MacCartney et al., 2008; Yao et al., 2013; Sultan et al., 2014), QA (Berant et al.", "startOffset": 54, "endOffset": 140}, {"referenceID": 12, "context": "applications such as MT (Blunsom and Cohn, 2006), RTE (Sammons et al., 2009; MacCartney et al., 2008; Yao et al., 2013; Sultan et al., 2014), QA (Berant et al.", "startOffset": 54, "endOffset": 140}, {"referenceID": 22, "context": "applications such as MT (Blunsom and Cohn, 2006), RTE (Sammons et al., 2009; MacCartney et al., 2008; Yao et al., 2013; Sultan et al., 2014), QA (Berant et al.", "startOffset": 54, "endOffset": 140}, {"referenceID": 20, "context": "applications such as MT (Blunsom and Cohn, 2006), RTE (Sammons et al., 2009; MacCartney et al., 2008; Yao et al., 2013; Sultan et al., 2014), QA (Berant et al.", "startOffset": 54, "endOffset": 140}, {"referenceID": 0, "context": ", 2014), QA (Berant et al., 2013; Yih et al., 2013; Yao and Van Durme, 2014; Sachan et al., 2015), etc.", "startOffset": 12, "endOffset": 97}, {"referenceID": 23, "context": ", 2014), QA (Berant et al., 2013; Yih et al., 2013; Yao and Van Durme, 2014; Sachan et al., 2015), etc.", "startOffset": 12, "endOffset": 97}, {"referenceID": 16, "context": ", 2014), QA (Berant et al., 2013; Yih et al., 2013; Yao and Van Durme, 2014; Sachan et al., 2015), etc.", "startOffset": 12, "endOffset": 97}, {"referenceID": 7, "context": "Retrieval and answer selection are usually designed as isolated or loosely connected components in QA systems (Ferrucci, 2012) leading to loss in performance \u2013 our approach mit-", "startOffset": 110, "endOffset": 126}, {"referenceID": 16, "context": "Pontil (2004) which was also used in a reading comprehension setting by Sachan et al. (2015). In a nutshell, the approach redefines the LSSVM feature map and shows that the MTLSSVM objective takes the same form as equation 1 with", "startOffset": 72, "endOffset": 93}, {"referenceID": 10, "context": "RST tells us that sentences with discourse relations are related to each other and can help us answer certain kinds of questions (Jansen et al., 2014).", "startOffset": 129, "endOffset": 150}, {"referenceID": 4, "context": "Semantic word match (cosine similarity using SENNA word vectors (Collobert et al., 2011) and \u201cAntonymy\u201d \u2018Class-Inclusion\u2019 or \u2018Is-A\u2019 relations using Wordnet).", "startOffset": 64, "endOffset": 88}, {"referenceID": 2, "context": "The first two baselines (Lucene and PMI) are taken from Clark et al. (2016). The Lucene baseline scores each answer candidate ai by searching for the combination of the ques-", "startOffset": 56, "endOffset": 76}, {"referenceID": 15, "context": "The next three baselines, inspired from Richardson et al. (2013), retrieve the top two CK12 sections querying q+ai in Lucene and score the answer candidates using these documents.", "startOffset": 40, "endOffset": 65}, {"referenceID": 22, "context": "(2016), Jacana aligner (Yao et al., 2013) and two neural network approaches, LSTM (Hochreiter and Schmidhuber,", "startOffset": 23, "endOffset": 41}, {"referenceID": 2, "context": "Then we also tried other approaches such as the RNN approach described in Clark et al. (2016), Jacana aligner (Yao et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 9, "context": "1997) and QANTA (Iyyer et al., 2014) They form our next four baselines.", "startOffset": 16, "endOffset": 36}], "year": 2016, "abstractText": "We provide a solution for elementary science tests using instructional materials. We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures (given a corpus of questionanswer pairs and instructional materials), and uses what it learns to answer novel elementary science questions. Our evaluation shows that our framework outperforms several strong baselines.", "creator": "TeX"}}}