{"id": "1602.04741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2016", "title": "Delay and Cooperation in Nonstochastic Bandits", "abstract": "later we slowly study networks of centralized communicating trace learning technique agents that occasionally cooperate to solve a common nonstochastic temporal bandit feedback problem. agents use potentially an underlying communication delivery network to frequently get messages messages about spying actions freely selected by only other spy agents, and instead drop messages of that not took neither more than $ d $ hops to automatically arrive, exactly where $ e d $ is strictly a reduced delay penalty parameter. lastly we introduce \\ textsc { exp3 - coop }, make a cooperative version equivalent of the { \\ math sc & exp3 } algorithm and have prove proof that with $ 35 k $ actions and $ n $ agents rated the average residual per - agent versus regret _ after $ \u2019 t $ 9 rounds is at most sort of order $ \\ co sqrt { \\ sam bigl ( ] d + 1 + \\ tfrac { k } { ba n } \\ * alpha _ { \\ le le \u03b4 d } \\ bigr ) (? t \\ \u03b4 ln 1 k ) } $, where $ \\ alpha _ { \\ le d } $ [UNK] is the independence control number compared of having the $ 11 d $ - th power ratio of understanding the connected communication graph $ g $. we proceed then always show that for completing any shortest connected graph, for $ 2 d = \\ no sqrt { d k } $ the regret bound is $ k ^ { 0 1 / 4 } \\ \u03b4 sqrt { + t } $, behavior strictly ranked better than the maximum minimax maximum regret $ \\ sqrt { ] kt } $ for noncooperating trust agents. more efficient informed strategy choices required of $ d $ lead to tighter bounds which programs are arbitrarily close to converge the full sensor information minimax conditional regret $ \\ ( sqrt { t \\ \" ln k } $ when $ 22 g $ is dense. when $ g $ has limited sparse operational components, eventually we generally show criteria that contains a variant correction of \\ textsc { def exp3 - cum coop }, eventually allowing agents to simply choose primarily their policy parameters according to ensure their constant centrality bounded in $ 16 g $, strictly geared improves towards the policy regret. finally, as a by - product subgroup of our analysis, we provide clearly the first optimal characterization of the minimax regret for central bandit learning graphs with variable delay.", "histories": [["v1", "Mon, 15 Feb 2016 17:20:28 GMT  (195kb,D)", "https://arxiv.org/abs/1602.04741v1", "27 pages"], ["v2", "Wed, 1 Jun 2016 16:20:30 GMT  (199kb,D)", "http://arxiv.org/abs/1602.04741v2", "30 pages"]], "COMMENTS": "27 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nicolo' cesa-bianchi", "claudio gentile", "yishay mansour", "alberto minora"], "accepted": false, "id": "1602.04741"}, "pdf": {"name": "1602.04741.pdf", "metadata": {"source": "CRF", "title": "Delay and Cooperation in Nonstochastic Bandits", "authors": ["Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile", "Yishay Mansour", "MANSOUR MINORA"], "emails": ["NICOLO.CESA-BIANCHI@UNIMI.IT", "CLAUDIO.GENTILE@UNINSUBRIA.IT", "MANSOUR@TAU.AC.IL", "AMINORA@UNINSUBRIA.IT"], "sections": [{"heading": null, "text": "at most of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK), where \u03b1\u2264d is the independence number of the d-th power of the communication graphG. We then show that for any connected graph, for d = \u221a K the regret bound is K1/4 \u221a T , strictly better than the minimax regret \u221a KT for noncooperating agents. More informed choices of d lead to bounds which are arbitrarily close to the full information minimax regret \u221a T lnK when G is dense. When G has sparse components, we show that a variant of EXP3-COOP, allowing agents to choose their parameters according to their centrality in G, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay."}, {"heading": "1. Introduction", "text": "Delayed feedback naturally arises in many sequential decision problems. For instance, a recommender system typically learns the utility of a recommendation by detecting the occurrence of certain events (e.g., a user conversion), which may happen with a variable delay after the recommendation was issued. Other examples are the communication delays experienced by interacting learning agents. Concretely, consider a network of geographically distributed ad servers using real-time bidding to sell their inventory. Each server sequentially learns how to set the auction parameters (e.g., reserve price) in order to maximize the network\u2019s overall revenue, and shares feedback information with other servers in order to speed up learning. However, the rate at which information is exchanged through the communication network is slower than the typical rate at which ads are\nc\u00a9 2016. Cesa-Bianchi, C. Gentile, Y. Mansour & A. Minora.\nar X\niv :1\n60 2.\n04 74\n1v 2\n[ cs\n.L G\n] 1\nserved. This causes each learner to acquire feedback information from other servers with a delay that depends on the network\u2019s structure.\nMotivated by the ad network example, we consider networks of learning agents that cooperate to solve the same nonstochastic bandit problem, and study the impact of delay on the global performance of these agents. We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent\u2019s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent\u2019s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t\u2212 s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret).\nIn the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like \u221a KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with com-\nmunication graph G can achieve an average welfare regret of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK). Here \u03b1\u2264d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = \u221a K this bound is at mostK1/4 \u221a T lnK+ \u221a K(lnT ) for any connected graph \u2014see Remark 7 in Section 4.1\u2014 which is asymptotically better than \u221a KT .\nNetworks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order \u221a( 1 + KN ) T ignoring polylog factors.1 We recover the same bound as a special case of our bound when G is a clique and\nd = 1. In the clique case our bound is also similar to the bound \u221a\nK N (T lnK) achieved by Seldin\net al. (2014) in a single-agent bandit setting where, at each time step, the agent can choose a subset ofN \u2264 K actions and observe their loss. In the case whenN = 1 (single agent), our analysis can be applied to the nonstochastic bandit problem where the player observes the loss of each played action with a delay of d steps. In this case we improve on the previous result of \u221a (d+ 1)KT by Neu et al. (2010, 2014), and give the first characterization (up to logarithmic factors) of the minimax regret, which is of order \u221a (d+K)T .\nIn principle, the problem of delays in online learning could be tackled by simple reductions. Yet, these reductions give rise to suboptimal results. In the single agent setting, where the delay is constant and equal to d, one can use the technique of Weinberger and Ordentlich (2002) and run d+1 in-\n1. The rate proven in (Awerbuch and Kleinberg, 2008, Theorem 2.1) has a worse dependence on T , but we believe this is due to the fact that their setting allows for dishonest agents and agent-specific loss vectors.\nstances of an online algorithm for the nondelayed case, where each instance is used every d+1 steps. This delivers a suboptimal regret bound of \u221a (d+ 1)KT . In the case of multiple delays, like in our multi-agent setting, one can repeat the same action for d+ 1 steps while accumulating information from the other agents, and then perform an update on scaled-up losses. The resulting (suboptimal)\nbound on the average welfare regret would be of the form \u221a (d+ 1) ( 1 + KN \u03b1\u2264d ) (T lnK).\nRather than using reductions, the analysis of EXP3-COOP rests on quantifying the performance of suitable importance weighted estimates. In fact, in the single-agent setting with delay parameter d, using EXP3-COOP reduces to running the standard EXP3 algorithm performing an update as soon a new loss becomes available. This implies that at any round t > d, EXP3 selects an action without knowing the losses incurred during the last d rounds. The resulting regret is bounded by relating the standard analysis of EXP3 to a detailed quantification of the extent to which the distribution maintained by EXP3 can drift in d steps.\nIn the multi-agent case, the importance weighted estimate of EXP3-COOP is designed in such a way that at each time t > d the instance of the algorithm run by an agent v updates all actions that were played at time t \u2212 d by agent v or by other agents not further away than d from v. Compared to the single agent case, here each agent can exploit the information circulated by the other agents. However, in order to compute the importance weighted estimates used locally by each agent, the probabilities maintained by the agents must be propagated together with the observed losses. Here, further concerns may show up, like the amount of communication, and the location of each agent within the network. In particular, when G has sparse components, we show that a variant of EXP3COOP, allowing agents to choose their parameters according to their centrality within G, strictly improves on the regret of EXP3-COOP."}, {"heading": "2. Additional Related Work", "text": "Many important ideas in delayed online learning, including the observation that the effect of delays can be limited by controlling the amount of change in the agent strategy, were introduced by Mesterharm (2005) \u2014see also (Mesterharm, 2007, Chapter 8). A more recent investigation on delayed online learning is due to Neu et al. (2010, 2014), who analyzed exponential weights with delayed feedbacks. Furher progress is made by Joulani et al. (2013), who also study delays in the general partial monitoring setting. Additional works (Joulani et al., 2016; Quanrud and Khashabi, 2015) prove regret bounds for the full-information case of the form \u221a (D + T ) lnK, where D is the total delay experienced over the T rounds. In the stochastic case, bandit learning with delayed feedback was considered by Dud\u0131\u0301k et al. (2011); Joulani et al. (2013).\nTo the best of our knowledge, the first paper about nonstochastic cooperative bandit networks is (Awerbuch and Kleinberg, 2008). More papers analyze the stochastic setting, and the closest one to our work is perhaps (Szorenyi et al., 2013). In that paper, delayed loss estimates in a network of cooperating stochastic bandits are analyzed using a dynamic P2P random networks as communication model. A more recent paper is (Landgren et al., 2015), where the communication network is a fixed graph and a cooperative version of the UCB algorithm is introduced which uses a distributed consensus algorithm to estimate the mean rewards of the arms. The main result is an individual (per-agent) regret bound that depends on the network structure without taking delays into account.\nAnother interesting paper about cooperating bandits in a stochastic setting is (Kar et al., 2011). Similar to our model, agents sit on the nodes of a communication network. However, only one designated agent observes the rewards of actions he selects, whereas the others remain in the dark. This designated agent broadcasts his sampled actions through the networks to the other agents, who must learn their policies relying only on this indirect feedback. The paper shows that in any connected network this information is sufficient to achieve asymptotically optimal regret. Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf. In a related paper, Tekin et al. (2014) look at a stochastic bandit model with combinatorial actions in a distributed recommender system setting, and study incentives among agents who can now recommend items taken from other agents\u2019 inventories. Another line of relevant work involves problems of decentralized bandit coordination. For example, Stranders et al. (2012) consider a bandit coordination problem where the the reward function is global and can be represented as a factor graph in which each agent controls a subset of the variables. A parallel thread of research concerns networks of bandits that compete for shared resources. A paradigmatic application domain is that of cognitive radio networks, in which a number of channels are shared among many users and any two or more users interfere whenever they simultaneously try to use the same channel. The resulting bandit problem is one of coordination in a competitive environment, because every time two or more agents select the same action at the same time step they both get a zero reward due to the interference \u2014see (Rosenski et al., 2015) for recent work on stochastic competitive bandits and (Kleinberg et al., 2009) for a study of more general congestion games in a game-theoretic setting. Finally, there exists an extensive literature on the adaptation of gradient descent and related algorithms to distributed computing settings, where asynchronous processors naturally introduce delays \u2014see, e.g., (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015). However, none of these works considers bandit settings, which are an essential ingredient for our analysis."}, {"heading": "3. Preliminaries", "text": "We now establish our notation, along with basic assumptions and preliminary facts related to our algorithms. Notation and setting here both refer to the single agent case. The cooperative setting with multiple agents (and notation thereof) will be introduced in Section 4. Proofs of all the results stated here can be found in (Cesa-Bianchi et al., 2016).\nLet A = {1, . . . ,K} be the action set. A learning agent runs an exponentially-weighted algorithm with weights wt(i), and learning rate \u03b7 > 0. Initially, w1(i) = 1 for all i \u2208 A. At each time step t = 1, 2, . . . , the agent draws action It with probability P(It = i) = pt(i) = wt(i)/Wt, where Wt = \u2211 j\u2208Awt(j). After observing the loss `t(It) \u2208 [0, 1] associated with the chosen action It, and possibly some additional information, the agent computes, for each i \u2208 A, nonnegative loss estimates \u0302\u0300t(i), and performs the exponential update\nwt+1(i) = pt(i) exp ( \u2212\u03b7 \u0302\u0300t(i)) (1)\nto these weights. The following two lemmas are general results that control the evolution of the probability distributions in the exponentially-weighted algorithm. As we said in the introduction, bounding the extent to which the distribution used by our algorithms can drift in d steps is key to controlling regret in a delayed setting. The first result bounds the additive change in the probability of any action, and it holds no matter how \u0302\u0300t(i) is defined. Lemma 1 Under the update rule (1), for all t \u2265 1 and for all i \u2208 A,\n\u2212\u03b7 pt(i)\u0302\u0300t(i) \u2264 pt+1(i)\u2212 pt(i) \u2264 \u03b7 pt+1(i)\u2211 j\u2208A pt(j)\u0302\u0300t(j) holds deterministically with respect to the agent\u2019s randomization.\nThe second result delivers a multiplicative bound on the change in the probability of any action when the loss estimates \u0302\u0300t(i) are of the following form:\n\u0302\u0300 t(i) =  `t\u2212d(i) qt\u2212d(i) Bt\u2212d(i) if t > d,\n0 otherwise , (2)\nwhere d \u2265 0 is a delay parameter, Bt\u2212d(i) \u2208 {0, 1}, for i \u2208 A, are indicator functions, and qt\u2212d(i) \u2265 pt\u2212d(i) for all i and t > d. In all later sections, Bt\u2212d(i) will be instantiated to the indicator function of the event that action i has been played at time t\u2212d by some agent, and qt\u2212d(i) will be the (conditional) probability of this event. Lemma 2 Let \u0302\u0300t(i) be of the form (2) for each t \u2265 1 and i \u2208 A. If \u03b7 \u2264 1Ke(d+1) in the update rule (1), then\npt+1(i) \u2264 ( 1 + 1\nd\n) pt(i)\nholds for all t \u2265 1 and i \u2208 A, deterministically with respect to the agent\u2019s randomization.\nAs we said in Section 1, the idea of controlling the drift of the probabilities in order to bound the effects of delayed feedback is not new. In particular, variants of Lemma 1 were already derived in the work of Neu et al. (2010, 2014). However, Lemma 2 appears to be new, and this is the key result to achieving our improvements."}, {"heading": "4. The Cooperative Setting on a Communication Network", "text": "In our multi-agent bandit setting, there are N agents sitting on the vertices of a connected and undirected communication graph G = (V,E), with V = {1, . . . , N}. The agents cooperate to solve the same instance of a nonstochastic bandit problem while limiting the communication among them. Let Ns(v) be the set of nodes v\u2032 \u2208 V whose shortest-path distance distG(v, v\u2032) from v in G is exactly s. At each time step t = 1, 2, . . . , each agent v \u2208 V draws an action It(v) from the common action set A. Note that each action i \u2208 A delivers the same loss `t(i) \u2208 [0, 1] to all agents v such that It(v) = i. At the end of round t, each agent v observes his own loss `t ( It(v) ) , and sends to his neighbors in G the message\nmt(v) = \u2329 t, v, It(v), `t ( It(v) ) ,pt(v) \u232a\nwhere pt(v) = ( pt(1, v), . . . , pt(K, v) ) is the distribution of It(v). Moreover, v also receives from his neighbors a variable number of messages mt\u2212s(v\u2032). Each message mt\u2212s(v\u2032) that v receives from a neighbor is used to update pt(v) and then forwarded to the other neighbors only if s < d, otherwise it is dropped.2 Here d is the maximum delay, a parameter of the communication protocol. Therefore, at the end of round t, each agent v receives one message mt\u2212s(v\u2032) for each agent v\u2032 such that distG(v, v\u2032) = s, where s \u2208 {1, . . . , d}. Graph G can thus be seen as a synchronous multi-hop communication network where messages are broadcast, each hop causing a delay of one time step. Our learning protocol is summarized in Figure 1, while Figure 2 contains a pictorial example.\nOur model is similar to the LOCAL communication model in distributed computing (Linial, 1992; Suomela, 2013), where the output of a node depends only on the inputs of other nodes in a constantsize neighborhood of it, and the goal is to derive algorithms whose running time is independent of the network size. (The main difference is that the task here has no completion time, however, also in our model influence on a node is only through a constant-size neighborhood of it.)\nOne aspect deserving attention is that, apart from the common delay parameter d, the agents need not share further information. In particular, the agents need not know neither the topology of the graph G nor the total number of agents N . In Section 5, we show that our distributed algorithm can also be analyzed when each agent v uses a personalized delay d(v), thus doing away with the need of a common delay parameter, and guaranteeing a generally better performance.\nFurther graph notation is needed at this point. Given G as above, let us denote by G\u2264d the graph (V,E\u2264d) where (u, v) \u2208 E\u2264d if and only if the shortest-path distance between agents u and v in G is at most d (hence G\u22641 = G). Graph G\u2264d is sometimes called the d-th power of G. We also use G0 to denote the graph (V, \u2205). Recall that an independent set of G is any subset T \u2286 V such that no two i, j \u2208 T are connected by an edge in E. The largest size of an independent set is the independence number of G, denoted by \u03b1(G). Let dG be the diameter of G (maximal length over all possible shortest paths between all pairs of nodes); then G\u2264dG is a clique, and one can easily see that N = \u03b1(G0) > \u03b1(G) \u2265 \u03b1(G\u22642) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b1(G\u2264dG) = 1. We show in Section 4.1 that the\n2. Dropping messages older than d rounds is clearly immaterial with respect to proving bandit regret bounds. We added this feature just to prove a point about the message complexity of the protocol. See Remark 8 in Section 5 for further discussion.\ncollective performance of our algorithms depends on \u03b1(G\u2264d). If the graph G under consideration is directed (see Section 5), then \u03b1(G) is the independence number of the undirected graph obtained from G by disregarding edge orientation.\nThe adversary generating losses is oblivious: loss vectors `t = ( `t(1), . . . , `t(K) ) \u2208 [0, 1]K do not depend on the agents\u2019 internal randomization. The agents\u2019 goal is to control the average welfare regret RcoopT , defined as\nRcoopT =\n( 1\nN \u2211 v\u2208V E [ T\u2211 t=1 `t ( It(v) )] \u2212min i\u2208A T\u2211 t=1 `t(i) ) ,\nthe expectation being with respect to the internal randomization of each agent\u2019s algorithm. In the sequel, we write Et[\u00b7] to denote the expectation w.r.t. the product distribution \u220f v\u2208V pt(v), conditioned on I1(v), . . . , It\u22121(v), v \u2208 V ."}, {"heading": "4.1. The Exp3-Coop algorithm", "text": "Our first algorithm, called EXP3-COOP (Cooperative Exp3) is described in Figure 3. The algorithm works in the learning protocol of Figure 1. Each agent v \u2208 V runs the exponentially-weighted algorithm (1), combined with a \u201cdelayed\u201d importance-weighted loss estimate \u0302\u0300t(i, v) that incorporates the delayed information sent by the other agents. Specifically, denote by N\u2264d(v) = \u22c3 s\u2264dNs(v) the set of nodes in G whose shortest-path distance from v is at most d, and note that, for all v, {v} = N\u22640(v) \u2286 N\u22641(v) \u2286 N\u22642(v) \u2286 \u00b7 \u00b7 \u00b7 . If any of the agents in N\u2264d(v) has played at time t \u2212 d action i (that is, Bd,t\u2212d(i, v) = 1 in Eq. in (3)), then the corresponding loss `t\u2212d(i) is incorporated by v into \u0302\u0300t(i, v). The denominator qd,t\u2212d(i, v) is simply, conditioned on the history, the probability of Bd,t\u2212d(i, v) = 1, i.e., qd,t\u2212d(i, v) = Et[Bd,t\u2212d(i, v)]. Observe that {v} \u2286 N\u2264d(v) for all d \u2265 0 implies qd,t\u2212d(i, v) \u2265 pt\u2212d(i, v), as required by (2). It is also worth mentioning that, despite this is not strictly needed by our learning protocol, each agent v actually exploits the loss information gathered from playing action It(v) only d time steps later. A relevant special case of this learning mode is when we only have a single bandit agent receiving delayed feedback (Section 6).\nBy their very definition, the loss estimates \u0302\u0300t(\u00b7, \u00b7) at time t are determined by the realizations of Is(\u00b7), for s = 1, . . . , t \u2212 d. This implies that the numbers pt(\u00b7, \u00b7) defining qd,t\u2212d(\u00b7, \u00b7), are determined by the realizations of Is(\u00b7) for s = 1, . . . , t \u2212 d \u2212 1 (because the probabilities pt(v) at time t are\ndetermined by the loss estimates up to time t \u2212 1, see (1)). We have, for all t > d, i \u2208 A, and v \u2208 V , Et\u2212d [\u0302\u0300 t(i, v) ] = `t\u2212d(i) . (4)\nFurther, because of what we just said about pt(\u00b7, \u00b7) and qd,t\u2212d(\u00b7, \u00b7) being determined by I1(\u00b7), . . . , It\u2212d\u22121(\u00b7), we also have\nEt\u2212d [ pt(i, v)\u0302\u0300t(i, v)] = pt(i, v)`t\u2212d(i) , Et\u2212d[pt(i, v)\u0302\u0300t(i, v)2] = pt(i, v) `t\u2212d(i)2\nqd,t\u2212d(i, v) . (5)\nThe following theorem quantifies the behavior of EXP3-COOP in terms of a free parameter \u03b3 in the learning rate, the tuning of which will be addressed in the subsequent Theorem 4.\nTheorem 3 The regret of EXP3-COOP run over a network G = (V,E) of N agents, each using delay d and learning rate \u03b7 = \u03b3 /( Ke(d+ 1) ) , for \u03b3 \u2208 (0, 1], satisfies\nRcoopT \u2264 2d+ Ke(d+ 1) lnK\n\u03b3 + \u03b3\n( \u03b1(G\u2264d)\n2(1\u2212 e\u22121)(d+ 1)N +\n3\nKe\n) T .\nWith this bound handy, we might be tempted to optimize for \u03b3. However, this is not a legal learning rate setting in a distributed scenario, for the optimized value of \u03b3 would depend on the global\nquantities N and \u03b1(G\u2264d). Thus, instead of this global tuning, we let each agent set its own learning rate \u03b3 through a \u201cdoubling trick\u201d played locally. The doubling trick3 works as follows. For each v \u2208 V , we let \u03b3r(v) = Ke(d+1) \u221a (lnK)/2r for each r = r0, r0+1, . . . , where r0 = \u2308 log2 lnK+\n2 log2(Ke(d + 1)) \u2309\nis chosen in such a way that \u03b3r(v) \u2264 1 for all r \u2265 r0. Let Tr be the random set of consecutive time steps where the same \u03b3r(v) was used. Whenever the local algorithm at v is running with \u03b3r(v) and detects \u2211 s\u2208Tr Qs(v) > 2\nr, then we restart this algorithm with \u03b3(v) = \u03b3r+1(v).\nWe have the following result.\nTheorem 4 The regret of EXP3-COOP run over a networkG = (V,E) ofN agents, each using delay d, and an individual learning rate \u03b7(v) = \u03b3(v)/ ( Ke(d+ 1) ) , where \u03b3(v) \u2208 (0, 1] is adaptively selected by each agent through the above doubling trick, satisfies, when T grows large,4\nRcoopT = O\n(\u221a (lnK) ( d+ 1 + K\nN \u03b1(G\u2264d)\n) T + d log T ) .\nRemark 5 Theorem 4 shows a natural trade-off between delay and information. To make it clear, suppose N \u2248 K. In this case, the regret bound becomes of order \u221a( d+ \u03b1(G\u2264d) ) T lnK + d lnT .\nNow, if d is as big as the diameter dG of G, then \u03b1(G\u2264d) = 1. This means that at every time step all N \u2248 K agents observe (with some delay) the losses of each other\u2019s actions. This is very much reminiscent of a full information scenario, and in fact our bound becomes of order\u221a\n(dG + 1)T lnK + dG lnT , which is close to the full information minimax rate \u221a\n(d+ 1)T lnK when feedback has a constant delay d (Weinberger and Ordentlich, 2002). When G is sparse (i.e., dG is likely to be large, say dG \u2248 N ), then agents have no advantage in taking d = dG since dG \u2248 N \u2248 K. In this case, agents may even give up cooperation (choosing d = 0 in Figure 3), and fall back on the standard bandit bound \u221a TK lnK, which corresponds to running EXP3-COOP on the edgeless graph G0. (No doubling trick is needed in this case, hence no extra log T term appears.)\nRemark 6 When d = dG, each neighborhood N\u2264d(v) used in the loss estimate (3) is equal to V , hence all agents receive the same feedback. Because they all start off from the same initial weights, the agents end up computing the same updates. This in turn implies that: (1) the individual regret incurred by each agent is the same as the average welfare regretRcoopT ; (2) the messages exchanged by the agents (see Figure 1) may be shortened by dropping the distribution part pt\u2212s(v \u2032).\nRemark 7 An interesting question is whether the agents can come up with a reasonable choice for the value of d even when they lack any information whatsoever about the global structure of G. A partial answer to this question follows. It is easy to show that the choice d = \u221a K in Theorem 4 yields a bound on the average welfare regret of the form K1/4 \u221a T lnK + \u221a K(lnT ) for all G (and irrespective to the value of N = |V |), provided G is connected. This holds because, for any\n3. There has been some recent work on adaptive learning rate tuning applied to nonstochastic bandit algorithms (Koca\u0301k et al., 2014; Neu, 2015). One might wonder whether the same techniques may apply here as well. Unfortunately, the specific form of our update (1) makes this adaptation nontrivial, and this is why we resorted to a more traditional \u201cdoubling trick\u201d. 4. The big-oh notation here hides additive terms that are independent of T and do depend polynomially on the other parameters.\nconnected graph G, the independence number \u03b1(G\u2264d) is always bounded by5 \u2308 2N / (d + 2) \u2309 . To see why this latter statement is true, observe that the neighborhood N\u2264d/2(v) of any node v in G\u2264d/2 contains at least d/2 + 1 nodes (including v), and any pair of nodes v\u2032, v\u2032\u2032 \u2208 N\u2264d/2(v) are adjacent in G\u2264d. Therefore, no independent set of G\u2264d can have size bigger than d2N / (d+ 2) \u2309 . A more detailed bound is contained, e.g., in (Firby and Haviland, 1997)."}, {"heading": "5. Extensions: Cooperation with Individual Parameters", "text": "In this section, we analyze a modification of EXP3-COOP that allows each agent v in the network to use a delay parameter d(v) different from that of the other agents. We then show how such individual delays may improve the average welfare regret of the agents. In the previous setting, where all agents use the same delay parameter d, messages have an implicit time-to-live equal to d. In this setting, however, agents may not have a detailed knowledge of the delay parameters used by the other agents. For this reason we allow an agent v to generate messages with a time-to-live ttl(v) possibly different from the delay parameter d(v). Note that the role of the two parameters d(v) and ttl(v) is inherently different. Whereas d(v) rules the extent to which v uses the messages received from the other agents, ttl(v) limits the number of times a message from v is forwarded to the other agents, thereby limiting the message complexity of the algorithm. In order to accomodate this additional parameter, we are required to modify the cooperative bandit protocol of Figure 1. As in Section 4, we have an undirected communication network G = (V,E) over the agents. However, in this new protocol the message that at the end of round t each agent v sends to his neighbors in G has the format\nmt(v) = \u2329 t, v, ttl(v), It(v), `t ( It(v) ) ,pt(v) \u232a where ttl(v) is the time-to-live parameter of agent v. Each message mt\u2212s(v\u2032), which v receives from a neighbor, first has its time-to-leave decremented by one. If the resulting value is positive, the message is forwarded to the other neighbors, otherwise it is dropped. Moreover, v uses this message to update pt(v) only if s \u2264 d(v). Hence, at time t an agent v uses the message sent at time t \u2212 s by v\u2032 if and only if distG(v\u2032, v) = s with s \u2264 min{d(v), ttl(v\u2032)}, where distG(v, v\u2032) is the shortest-path distance from v\u2032 to v in G.\nBased on the collection P = {d(v), ttl(v)}v\u2208V of individual parameters, we define the directed graph GP = (V,EP) as follows: arc (v\u2032, v) \u2208 EP if and only if distG(v, v\u2032) \u2264 min{d(v), ttl(v\u2032)}. The in-neighborhood N\u2212P (v) of v thus contains the set of all v\n\u2032 \u2208 V whose distance from v is not larger than min{d(v), ttl(v\u2032)}. Notice that, with this definition, v \u2208 N\u2212P (v), so that (V,EP) includes all self-loops (v, v). Figure 4(a) illustrates these concepts through a simple pictorial example.\nRemark 8 It is important to remark that the communication structure encoded by P is an exogenous parameter of the regret minimization problem, and so our algorithms cannot trade it off against regret. In addition to that, the parameterization P = {d(v), ttl(v)}v\u2208V defines a simple and static communication graph which makes it relatively easy to express regret as a function of the amount of available communication. This would not be possible if we had each individual node\n5. Because it holds for a worst-case (connected) G, this upper bound on \u03b1(G\u2264d) can be made tighter when specific graph topologies are considered.\nv decide whether to forward a message based, say, on its own local delay parameter d(v). To see why, consider the situation where nodes v and v\u2032 are along the route of a message that is reaching v before v\u2032. The decision of v to drop the message may clash with the willingness of v\u2032 to receive it, and this may clearly happen when d(v) < d(v\u2032). The structure of the communication graph resulting from this individual behavior of the nodes would be rather complicated. On the contrary, the time-to-live-based parametrization, which is commonly used in communication networks to control communication complexity, does not have this issue.\nFigure 5 contains our algorithm (called EXP3-COOP2) for this setting. EXP3-COOP2 is a strict generalization of EXP3-COOP, and so is its analysis. The main difference between the two algorithms is that EXP3-COOP2 deals with directed graphs. This fact prevents us from using the same techniques of Section 4.1 in order to control the regret. Intuitively, adding orientations to the edges reduces the information available to the agents and thus increases the variance of their loss estimates. Thus, in order to control this variance, we need a lower bound6 on the probabilities pt(i, v). From Figure 5, one can easily see that\n1 = \u2211 i\u2208A wt(i, v) Wt(v) \u2264 P\u0303t(v) \u2264 \u2211 i\u2208A ( wt(i, v) Wt(v) + \u03b4 K ) = 1 + \u03b4 (6)\nimplying the lower bound pt(i, v) \u2265 \u03b4K(1+\u03b4) , holding for all i, t, and v.\nThe following theorem is the main result of this section.\n6. We find it convenient to derive this lower bound without mixing with the uniform distribution over A \u2014see, e.g., (Auer et al., 2002)\u2014 but in a slightly different manner. This facilitates our delayed feedback analysis.\nTheorem 9 The regret of EXP3-COOP2 run over a network G = (V,E) of N agents, each agent v using individual delay d(v), individual time-to-leave ttl(v), exploration parameter \u03b4 = 1/T , and learning rate \u03b7 such that \u03b7 \u2192 0 as T \u2192\u221e satisfies, when T grows large,\nRcoopT = O ( lnK \u03b7 + \u03b7 ( d\u0304V + K N \u03b1 (GP) ln(TNK) ) T ) , where d\u0304V = 1 N \u2211 v\u2208V d(v) .\nUsing a doubling trick in much the same way we used it to prove Theorem 4, we can prove the following result.\nCorollary 10 The regret of EXP3-COOP2 run over a networkG = (V,E) ofN agents, each agent v using individual delay d(v), individual time-to-leave ttl(v), exploration parameter \u03b4 = 1/T , and individual learning rate \u03b7(v) adaptively selected by each agent through a doubling trick, satisfies,\nwhen T grows large\nRcoopT = O\n(\u221a (lnK) ( d\u0304V + 1 + K\nN \u03b1(GP) ln(TNK)\n) T + d\u0304V ( lnT + ln ln(TNK) )) .\nTo illustrate the advantage of having individual delays as opposed to sharing the same delay value, it suffices to consider a communication network including regions of different density. Concretely, consider the graph in Figure 4(b) with a large densely connected region (red agents) and a small sparsely connected region black agents). In this example, the black agents prefer a large value of their individual delay so as to receive more information from nearby agents, but this comes at the price of a larger bias for their estimators \u0302\u0300t(i, v). On the contrary, information from nearby agents is readily available to the red agents, so that they do not gain any regret improvement from a large delay parameter. A similar argument applies here to the individual time-to-live values: red agents v will set a small ttl(v) to reduce communication. Black agents v\u2032 may decide to set ttl(v\u2032) depending on their intention to reach the red nodes. But because the red agents have set a small d(v), any effort made by v\u2032 trying to reach them would be a communication waste. Hence, it is reasonable for a black agent v\u2032 to set a moderately large value for ttl(v\u2032), but perhaps not so large as to reach the red agents. One can read this off the bounds in both Theorem 9 and Corollary 10, as explained next. Suppose for simplicity that K \u2248 N so that, disregarding log factors, these bounds depend on parameters P only through the quantity H = d\u0304V + \u03b1 (GP). Now, in the case of a common delay parameter d (Section 4.1), it is not hard to see that the best setting for d in order to minimize H is of the form d = N1/4, resulting in H = \u0398(N1/4). On the other hand, the best setting for the individual delays is d(v) = 1 when v is red, and d(v) = \u221a N when v is black, resulting in H = \u0398(1).\nThe time-to-live parameters ttl(v) affect the regret bound only through \u03b1 (GP), but they clearly play the additional role of bounding the message complexity of the algorithm. In our example of Figure 4(b), we essentially have d(v) \u2248 ttl(v) for all v. A typical scenario where agents may have d(v) 6= ttl(v) is illustrated in Figure 4(c). In this case, we have star-like graph where a central agent is connected through long rays to all others agents. The center v prefers to set a small d(v), since it has a large degree, but also a large ttl(v) in order to reach the green peripheral nodes. The green nodes v\u2032 are reasonably doing the opposite: a large d(v\u2032) in order to gather information from other nodes, but also a smaller time-to-live than the center, for the information transmitted by v\u2032 is comparatively less valuable to the whole network than the one transmitted by the center.\nAgents can set their individual parameters in a topology-dependent manner using any algorithm for assessing the centrality of nodes in a distributed fashion \u2014e.g., (Wehmuth and Ziviani, 2013), and references therein. This can be done at the beginning in a number of rounds which only depends on the network topology (but not on T ). Hence, this initial phase would affect the regret bound only by an additive constant."}, {"heading": "6. Delayed Losses (for a Single Agent)", "text": "EXP3-COOP can be specialized to the setting where a single agent is facing a bandit problem in which the loss of the chosen action is observed with a fixed delay d. In this setting, at the end of\neach round t the agent incurs loss `t(It) and observes `t\u2212d(It\u2212d), if t > d, and nothing otherwise. The regret is defined in the usual way,\nRT = E [ T\u2211 t=1 `t(It) ] \u2212 min i=1,...,K T\u2211 t=1 `t(i) .\nThis problem was studied by Weinberger and Ordentlich (2002) in the full information case, for which they proved that \u221a (d+ 1)T lnK is the optimal order for the minimax regret. The result was extended to the bandit case by Neu et al. (2010, 2014) \u2014see also Joulani et al. (2013)\u2014 whose techniques can be used to obtain a regret bound of order \u221a (d+ 1)KT . Yet, no matching lower bound was available for the bandit case.\nAs a matter of fact, the upper bound \u221a\n(d+ 1)KT for the bandit case is easily obtained: just run in parallel d + 1 instances of the minimax optimal bandit algorithm for the standard (no delay) setting, achieving RT \u2264 \u221a KT (ignoring constant factors). At each time step t = (d + 1)r + s (for r = 0, 1, . . . and s = 0, . . . , d), use instance s + 1 for the current play. Hence, the no-delay bound applies to every instance and, assuming d + 1 divides T , we immediately obtain RT \u2264\u2211d+1\ns=1 \u221a K Td+1 \u2264 \u221a (d+ 1)KT , again, ignoring constant factors.\nNext, we show that the machinery we developed in Section 4.1 delivers an improved upper bound on the regret for the bandit problem with delayed losses, and then we complement this result by providing a lower bound matching the upper bound up to log factors, thereby characterizing (up to log factors) the minimax regret for this problem.\nCorollary 11 In the nonstochastic bandit setting with K \u2265 2 actions and delay d \u2265 0, where at the end of each round t the predictor has access to the losses `1(I1), . . . , `s(Is) \u2208 [0, 1]K for s = max{1, t\u2212 d}, the minimax regret is of order \u221a (K + d)T , ignoring logarithmic factors."}, {"heading": "7. Conclusions and Ongoing Research", "text": "We have investigated a cooperative and nonstochastic bandit scenario where cooperation comes at the price of delayed information. We have proven average welfare regret bounds that exhibit a natural tradeoff between amount cooperation and delay, the tradeoff being ruled by the underlying communication network topology. As a by-product of our analysis, we have also provided the first characterization to date of the regret of learning with (constant) delayed feedback in an adversarial bandit setting. There are a number of possible extensions which we are currently considering:\n1. So far our analysis only delivers average welfare regret bounds. It would be interesting to show simultaneous regret bounds that hold for each agent individually. We conjecture that the individual regret bound of an agent v is of the form \u221a (lnK) ( d+ K|N\u2264d(v)| ) T , where\n|N\u2264d(v)| is the degree of v in G\u2264d (plus one). Such bound would in fact imply, e.g., the one in Theorem 4. A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).\n2. It would be nice to characherize the average welfare regret by complementing our upper bounds with suitable lower bounds: Is the upper bound of Theorem 4 optimal in the communication model considered here?\n3. The two algorithms we designed do not use the loss information in the most effective way, for they both postpone the update step by d (Figure 3) or d(v) ((Figure 5) time steps. In fact, we do have generalized versions of both algorithms where all losses `t\u2212s(i) coming from agents at distance s from any given agent v are indeed used at time t by agent v i.e., as soon as these losses become available to v. The resulting regret bounds mix delays and independence numbers of graphs at different levels of delay. (Details will be given in the full version of this paper.) More ambitiously, it is natural to think of ways to adaptively tune our algorithms so as to automatically determine the best delay parameter d. For instance, disregarding message complexity, is there a way for each agent to adaptively tune d locally so to minimize the bound in Theorem 4?\n4. Our messages mt(v) contain both action/loss information and distribution information. Is it possible to drop the distribution information and still achieve average welfare regret bounds similar to those in Theorems 3 and 4? 5. Even for the single-agent setting, we do not know whether regret bounds of the form\u221a (D + T ) lnK, where D is the total delay experienced over the T rounds, could be proven\n\u2014see (Joulani et al., 2016; Quanrud and Khashabi, 2015) for similar results in the fullinformation setting. In general, the study of learning on a communication network with time-varying delays, and its impact on the regret rates, is a topic which is certainly worth of attention."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their careful reading, and for their thoughtful suggestions that greatly improved the presentation of this paper. Yishay Mansour is supported in part by the Israeli Centers of Research Excellence (I-CORE) program, (Center No. 4/11), by a grant from the Israel Science Foundation (ISF), by a grant from United States-Israel Binational Science Foundation (BSF) and by a grant from the Len Blavatnik and the Blavatnik Family Foundation."}, {"heading": "Appendix A. Proofs from Section 3", "text": "Proof of Lemma 1.\nProof Directly from the definition of the update (1), wt+1(i) \u2264 pt(i) for all i \u2208 A, so that Wt+1 \u2264 1, which in turn implies wt+1(i) \u2264 wt+1(i)/Wt+1 = pt+1(i). Therefore\npt+1(i)\u2212 pt(i) \u2265 wt+1(i)\u2212 pt(i) = pt(i) ( e\u2212\u03b7 \u0302\u0300 t(i) \u2212 1 ) \u2265 \u2212\u03b7 pt(i)\u0302\u0300t(i) ,\nthe last inequality using 1\u2212 e\u2212x \u2264 x for x \u2265 0. Similarly,\npt+1(i)\u2212 pt(i) \u2264 pt+1(i)\u2212 wt+1(i) = pt+1(i)\u2212 pt+1(i)Wt+1 = pt+1(i)\n\u2211 j\u2208A ( pt(j)\u2212 wt+1(j) ) = pt+1(i)\n\u2211 j\u2208A pt(j) ( 1\u2212 e\u2212\u03b7 \u0302\u0300t(j)) \u2264 \u03b7 pt+1(i)\n\u2211 j\u2208A pt(j)\u0302\u0300t(j) concluding the proof.\nProof of Lemma 2. Proof We proceed by induction over t. For all t \u2264 d, \u0302\u0300t(\u00b7) = 0. Hence pt(\u00b7) = 1/K, and the lemma trivially holds. For t > d we can write\u2211\ni\u2208A pt(i)\u0302\u0300t(i) = \u2211 i\u2208A pt(i) `t\u2212d(i) qt\u2212d(i) Bt\u2212d(i)\n\u2264 \u2211 i\u2208A pt(i) qt\u2212d(i) (because Bt\u2212d(i)`t\u2212d(i) \u2264 1)\n\u2264 \u2211 i\u2208A ( 1 + 1 d )d pt\u2212d(i) qt\u2212d(i)\n(by the inductive hypothesis)\n\u2264 ( 1 + 1\nd\n)d K (because qt\u2212d(i) \u2265 pt\u2212d(i))\n\u2264 Ke .\nHence, using Lemma 1,\npt+1(i) ( 1\u2212 \u03b7 Ke ) \u2264 pt+1(i) 1\u2212 \u03b7 \u2211 j\u2208A pt(j)\u0302\u0300t(j)  \u2264 pt(i)\nwhich implies pt+1(i) \u2264 ( 1 + 1d ) pt(i) whenever \u03b7 \u2264 1Ke(d+1) ."}, {"heading": "Appendix B. Proofs from Section 4.1", "text": "The next lemma relates the variance of the estimates (3) to the structure of the communication graph G. The lemma is stated for a generic undirected communication graph G, but our application of it actually involves graph G\u2264d.\nLemma 12 Let G = (V,E) be an undirected graph with independence number \u03b1(G). For each v \u2208 V , let N\u22641(v) be the neighborhood of node v (including v itself), and p(v) =( p(1, v), . . . , p(K, v) ) be a probability distribution over A = {1, . . . ,K}. Then, for all i \u2208 A,\n\u2211 v\u2208V p(i, v) q(i, v) \u2264 1 1\u2212 e\u22121\n( \u03b1(G) +\n\u2211 v\u2208V p(i, v)\n) where q(i, v) = 1\u2212\n\u220f v\u2032\u2208N\u22641(v) ( 1\u2212 p(i, v\u2032) ) .\nProof Fix i \u2208 A and set for brevity P (i, v) = \u2211\nv\u2032\u2208N\u22641(v) p(i, v \u2032). We can write\n\u2211 v\u2208V p(i, v) q(i, v) = \u2211 v\u2208V :P (i,v)\u22651 p(i, v)\nq(i, v)\ufe38 \ufe37\ufe37 \ufe38 (I)\n+ \u2211\nv\u2208V :P (i,v)<1\np(i, v)\nq(i, v)\ufe38 \ufe37\ufe37 \ufe38 (II)\n,\nand proceed by upper bounding the two terms (I) and (II) separately. Let r(v) be the cardinality of N\u22641(v). We have, for any given v \u2208 V ,\nmin q(i, v) : \u2211 v\u2032\u2208N\u22641(v) p(i, v\u2032) \u2265 1  = 1\u2212 ( 1\u2212 1 r(v) )r(v) \u2265 1\u2212 e\u22121 .\nThe equality is due to the fact that the minimum is achieved when p(i, v\u2032) = 1r(v) for all v \u2032 \u2208 N\u22641(v), and the inequality comes from r(v) \u2265 1 (for, v \u2208 N\u22641(v)). Hence\n(I) \u2264 \u2211\nv\u2208V :P (i,v)\u22651\np(i, v) 1\u2212 e\u22121 \u2264 \u2211 v\u2208V p(i, v) 1\u2212 e\u22121 .\nAs for (II), using the inequality 1\u2212 x \u2264 e\u2212x, x \u2208 [0, 1], with x = p(i, v\u2032), we can write\nq(i, v) \u2265 1\u2212 exp \u2212 \u2211 v\u2032\u2208N\u22641(v) p(i, v\u2032)  = 1\u2212 exp (\u2212P (i, v)) . In turn, because P (i, v) < 1 in terms (II), we can use the inequality 1\u2212 e\u2212x \u2265 (1\u2212 e\u22121)x, holding when x \u2208 [0, 1], with x = P (i, v), thereby concluding that\nq(i, v) \u2265 (1\u2212 e\u22121)P (i, v)\nThus\n(II) \u2264 \u2211\nv\u2208V :P (i,v)<1\np(i, v) (1\u2212 e\u22121)P (i, v) \u2264 1 1\u2212 e\u22121 \u2211 v\u2208V p(i, v) P (i, v) \u2264 \u03b1(G) 1\u2212 e\u22121 ,\nwhere in the last step we used (Alon et al., 2014, Lemma 10). Notice that despite the statement of this lemma refers to a directed graph and its maximum acyclic subgraph, in the special case of undirected graphs, the size of the maximum acyclic subgraph coincides with the independence number. Moreover, observe that p(i, 1), . . . , p(i,N) \u2265 0 need not sum to one in order for this\nlemma to hold.\nProof of Theorem 3.\nProof The standard analysis of the exponentially-weighted algorithm with importance-sampling estimates (see, e.g., the proof of (Alon et al., 2014, Lemma 1)) gives for each agent v and each action k the deterministic bound\nT\u2211 t=1 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v) \u2264 T\u2211 t=1 \u0302\u0300 t(k, v) + lnK \u03b7 + \u03b7 2 T\u2211 t=1 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v)2 . (7) We take expectations of the three (double) sums in (7) separately. As for the first sum, notice that an iterative application of Lemma 1 gives, for t > d,\npt(i, v) \u2265 pt\u2212d(i, v)\u2212 \u03b7 d\u2211 s=1 pt\u2212s(i, v)\u0302\u0300t\u2212s(i, v) , so that, setting for brevity At(i, v) = \u2211d s=1 pt\u2212s(i, v) \u0302\u0300 t\u2212s(i, v), we have\nT\u2211 t=1 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v) \u2265 T\u2211 t=2d+1 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v) \u2265\nT\u2211 t=2d+1 K\u2211 i=1 pt\u2212d(i, v)\u0302\u0300t(i, v)\u2212 \u03b7 T\u2211 t=2d+1 K\u2211 i=1 At(i, v) \u0302\u0300t(i, v) . Hence\nE [ T\u2211 t=1 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v)] \u2265 E[ T\u2211 t=2d+1 K\u2211 i=1 pt\u2212d(i, v)\u0302\u0300t(i, v) ] \u2212 \u03b7 E [ T\u2211 t=2d+1 K\u2211 i=1 At(i, v) \u0302\u0300t(i, v)]\n= E\n[ T\u2211\nt=2d+1 K\u2211 i=1 pt\u2212d(i, v)Et\u2212d [\u0302\u0300 t(i, v)\n]]\n\u2212 \u03b7 E\n[ T\u2211\nt=2d+1 K\u2211 i=1 At(i, v)Et\u2212d [\u0302\u0300 t(i, v) ]] (since pt(i, v) is determined by I1(\u00b7), . . . , It\u2212d\u22121(\u00b7))\n= E\n[ T\u2211\nt=2d+1 K\u2211 i=1 pt\u2212d(i, v) `t\u2212d(i)\n] \u2212 \u03b7 E [ T\u2211\nt=2d+1 K\u2211 i=1 At(i, v) `t\u2212d(i) ] (using (4))\n\u2265 E [ T\u2211 t=1 K\u2211 i=1 pt(i, v) `t(i) ] \u2212 2d\u2212 \u03b7 T d .\nThe last step uses\nE [ K\u2211 i=1 At(i, v) `t\u2212d(i) ] \u2264 E [ K\u2211 i=1 At(i, v) ]\n= E [ K\u2211 i=1 d\u2211 s=1 pt\u2212s(i, v)\u0302\u0300t\u2212s(i, v)]\n= E [ K\u2211 i=1 d\u2211 s=1 pt\u2212s(i, v)`t\u2212s\u2212d(i) ]\n\u2264 E [ K\u2211 i=1 d\u2211 s=1 pt\u2212s(i, v) ] = d\nholding for t \u2265 2d+ 1. Similarly, for the second sum in (7), we have\nE [ T\u2211 t=1 \u0302\u0300 t(k, v) ] = T\u2211 t=d+1 `t\u2212d(k) \u2264 T\u2211 t=1 `t(k) .\nFinally, for the third sum in (7), an iterative application of Lemma 2 yields, for t > d, pt(i, v) \u2264 ( 1 + 1\nd\n)d pt\u2212d(i, v) \u2264 e pt\u2212d(i, v) ,\nso that we can write\nE [ T\u2211 t=1 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v)2] = E[ T\u2211 t=d+1 K\u2211 i=1 Et\u2212d [ pt(i, v)\u0302\u0300t(i, v)2]]\n\u2264 E\n[ T\u2211\nt=d+1 K\u2211 i=1 pt(i, v) qd,t\u2212d(i, v)\n] (using (5) and `t(\u00b7) \u2264 1)\n\u2264 eE\n[ T\u2211\nt=d+1 K\u2211 i=1 pt\u2212d(i, v) qd,t\u2212d(i, v)\n] ,\nthe last inequality being due to an iterative application of Lemma 2, and the observation that( 1 + 1d )d \u2264 e. Hence, summing over all agents v, dividing by N , and using Lemma 12 on G\u2264d gives\n1\nN E [ T\u2211 t=1 K\u2211 i=1 \u2211 v\u2208V pt(i, v)\u0302\u0300t(i, v)2] \u2264 e N E [ T\u2211 t=d+1 K\u2211 i=1 \u2211 v\u2208V pt\u2212d(i, v) qd,t\u2212d(i, v) ]\n\u2264 e (1\u2212 e\u22121)N E\n[ T\u2211\nt=d+1 K\u2211 i=1\n( \u03b1(G\u2264d) +\n\u2211 v\u2208V pt\u2212d(i, v)\n)]\n\u2264 e 1\u2212 e\u22121 T\n( K\nN \u03b1(G\u2264d) + 1\n) .\nFinally, putting together as in (7), setting \u03b7 = \u03b3 /( Ke(d + 1) ) , and overapproximating, we obtain the desired bound.\nProof of Theorem 4.\nProof We start off from first part of the proof of Theorem 3 which, after rearranging terms, gives the following bound for each agent v:\nE [ T\u2211 t=1 K\u2211 i=1 pt(i, v)`t(i) ] \u2212 T\u2211 t=1 `t(k)\n\u2264 2d+ E\n[ lnK\n\u03b7(v) + \u03b7(v) d2 + \u03b7(v) T\u2211 t=d+1\n( d+ e\n2 K\u2211 i=1 pt\u2212d(i, v) qd,t\u2212d(i, v)\n)]\n\u2264 3d+ E Ke(d+ 1) lnK\u03b3(v) + \u03b3(v)Ke(d+ 1) T\u2211 t=1 ( I{t > d} d+ e 2 ( K\u2211 i=1 pt\u2212d(i, v) qd,t\u2212d(i, v) ) I{t > d} ) \ufe38 \ufe37\ufe37 \ufe38\nQt(v)\n . (8)\nNote that the optimal tuning of \u03b3(v) depends on the random quantity\nQT (v) = T\u2211 t=1 Qt(v) .\nWe now apply the doubling trick to each instance of EXP3-COOP. Recall that, for each v \u2208 V , we let \u03b3r(v) = Ke(d + 1) \u221a (lnK)/2r for each r = r0, r0 + 1, . . . , where r0 = \u2308 log2 lnK +\n2 log2(Ke(d + 1)) \u2309\nis chosen in a way that \u03b3r(v) \u2264 1 for all r \u2265 r0. Let Tr be the random set of consecutive time steps where the same \u03b3r(v) was used. Whenever the algorithm is running with \u03b3r(v) and detects \u2211 s\u2208Tr Qs(v) > 2\nr, then we restart the algorithm with \u03b3(v) = \u03b3r+1(v). The largest r = r(v) we need is \u2308 log2QT (v) \u2309 and\u2308\nlog2QT (v) \u2309\u2211\nr=r0\n2r/2 < 5 \u221a QT (v) .\nBecause of (8), the regret agent v suffers when using \u03b3r(v) within Tr is at most 3d+ 2 \u221a\n(lnK)2r. Now, since we pay at most regret d at each restart, we have\nE [ T\u2211 t=1 \u2211 i pt(i, v)`t(i) ] \u2212 T\u2211 t=1 `t(k) \u2264 3d+ 4Ke(d+ 1) lnK\n+ E [ 10 \u221a (lnK)QT (v) + 3d \u2308 log2QT (v) \u2309] .\nThe term 3d + 4Ke(d + 1) lnK bounds the regret when the algorithm is never restarted implying that only \u03b3r0(v) is used.\nTaking averages with respect to v, using Jensen\u2019s inequality multiple times, and applying the (deterministic) bound\n1\nN \u2211 v\u2208V QT (v) \u2264 ( d+\ne 2(1\u2212 e\u22121) K (\u03b1(G\u2264d) + 1) N\n) T\nderived with the aid of Lemma 12 at the end of the proof of Theorem 3, gives\nRcoopT \u2264 3d+ 4Ke(d+ 1) lnK\n+ 10 \u221a\u221a\u221a\u221a(lnK)E[ 1 N \u2211 v\u2208V QT (v) ] + 3d log2 ( E [ 1 N \u2211 v\u2208V QT (v) ])\n\u2264 10 \u221a (lnK) ( d+\ne 2(1\u2212 e\u22121) K (\u03b1(G\u2264d) + 1) N\n) T + 3d log2 T + C ,\nwhereC is independent of T and depends polynomially on the other parameters. Hence, as T grows large,\nRcoopT = O\n(\u221a (lnK) ( d+ 1 + K\nN \u03b1(G\u2264d)\n) T + d log T ) ,\nas claimed."}, {"heading": "Appendix C. Proofs from Section 5", "text": "We first need to adapt the preliminary Lemmas 1 and 2 to the new update rule of EXP3-COOP2 contained in Figure 5.\nLemma 13 Under the update rule contained in Figure 5, for all t \u2265 1, for all i \u2208 A, and for all v \u2208 V\n\u2212pt(i, v) ( \u03b7\u0302\u0300t(i, v) + \u03b4) \u2264 pt+1(i, v)\u2212 pt(i, v)\n\u2264 pt+1(i, v) K\u2211 j=1 pt(j, v) ( 1\u2212 I{p\u0303t+1(i, v) > \u03b4/K} ( 1\u2212 \u03b7 \u0302\u0300t(i, v)))\nholds deterministically with respect to the agents\u2019 randomization.\nProof For the lower bound, we have\npt+1(i, v)\u2212 pt(i, v) = p\u0303t+1(i, v)\nP\u0303t+1(v) \u2212 pt(i, v) \u2265\nwt+1(i, v)\nWt+1(v) P\u0303t+1(v) \u2212 pt(i, v) .\nSince Wt+1(v) = \u2211 i\u2208A pt(i, v)e \u2212\u03b7\u0302\u0300t(i,v) \u2264 \u2211i\u2208A pt(i, v) = 1, and P\u0303t+1(v) \u2264 1 + \u03b4 by (6), we can write\npt+1(i, v)\u2212 pt(i, v) \u2265 wt+1(i, v)\n1 + \u03b4 \u2212 pt(i, v)\n= pt(i, v)\n( e\u2212\u03b7 \u0302\u0300 t(i,v)\n1 + \u03b4 \u2212 1\n)\n\u2265 pt(i, v)\n( 1\u2212 \u03b7\u0302\u0300t(i, v)\n1 + \u03b4 \u2212 1\n) (using e\u2212x \u2265 1\u2212 x)\n\u2265 pt(i, v) ( \u2212\u03b4 \u2212 \u03b7\u0302\u0300t(i, v))\nas claimed. As for the upper bound, we first claim that\nwt+1(i, v)\nWt+1(v) \u2265 pt+1(i, v)I{p\u0303t+1(i, v) > \u03b4/K} . (9)\nTo prove (9), we recall that p\u0303t+1(i, v) = max { wt+1(i,v) Wt+1(v) , \u03b4K } . Then we distinguish two cases:\n1. If wt+1(i,v)Wt+1(v) \u2264 \u03b4 K , then p\u0303t+1(i, v) = \u03b4/K, and wt+1(i, v)/Wt+1(v) > 0 by definition, hence\n(9) holds;\n2. If wt+1(i,v)Wt+1(v) > \u03b4 K then p\u0303t+1(i, v) = wt+1(i,v) Wt+1(v) , so that pt+1(i, v) \u2264 pt+1(i, v) P\u0303t+1(v) = p\u0303t+1(i, v) and (9) again holds.\nThen, setting for brevity C = I{p\u0303t+1(i, v) > \u03b4/K}, we can write\npt+1(i, v)\u2212 pt(i, v) \u2264 pt+1(i, v)\u2212 wt+1(i, v) (from the update (1)) \u2264 pt+1(i, v)\u2212Wt+1(v)pt+1(i, v)C (using (9)) = pt+1(i, v) ( 1\u2212Wt+1(v)C\n) = pt+1(i, v)\n\u2211 j\u2208A ( pt(j, v)\u2212 C wt+1(j, v) ) = pt+1(i, v)\n\u2211 j\u2208A pt(j, v) ( 1\u2212 C e\u2212\u03b7 \u0302\u0300t(j,v)) \u2264 pt+1(i, v)\n\u2211 j\u2208A pt(j, v) ( 1\u2212 C(1\u2212 \u03b7 \u0302\u0300t(j, v))) where in the last step we again used e\u2212x \u2265 1\u2212 x. This concludes the proof.\nLemma 14 Under the update rule contained in Figure 5, if \u03b4 \u2264 1/d(v) and \u03b7 \u2264 1Ke(d(v)+1) , then\npt+1(i, v) \u2264 ( 1 + 1\nd(v)\n) pt(i, v) (10)\nholds for all t \u2265 1 and i \u2208 A, deterministically with respect to the agents\u2019 randomization.\nProof If p\u0303t+1(i, v) = \u03b4/K then, from (6), we have \u03b4/K = pt+1(i, v)P\u0303t+1(v) \u2265 pt+1(i, v), and pt(i, v) \u2265 \u03b4K(1+\u03b4) . Hence, pt+1(i,v) pt(i,v) \u2264 \u03b4/K\u03b4/(K(1+\u03b4)) = 1 + \u03b4, so the claim follows from \u03b4 \u2264 1\nd(v) . On the other hand, if p\u0303t+1(i, v) > \u03b4/K, then the proof is exactly the same as the proof of Lemma 2, for the second inequality in the statement of Lemma 13 turns out to be exactly the same as the corresponding inequality in the statement in Lemma 1.\nNext, we generalize Lemma 12 to the case of directed graphs. This is where we need a lower bound on the probabilities pt(i, v). If G = (V,E) is a directed graph, then for each v \u2208 V let N\u2212\u22641(v) be the in-neighborhood of node v (i.e., the set of v\u2032 \u2208 V such that arc (v\u2032, v) \u2208 E), including v itself.\nLemma 15 Let G = (V,E) be a directed graph with independence number \u03b1(G). Let p(v) = ( p(1, v), . . . , p(K, v) ) be a probability distribution over A = {1, . . . ,K} such that p(i, v) \u2265 \u03b4K(1+\u03b4) . Then, for all i \u2208 A,\n\u2211 v\u2208V p(i, v) q(i, v) \u2264 1 1\u2212 e\u22121\n( 6\u03b1(G) ln ( 1 + N2K(1 + \u03b4)\n\u03b4\n) + \u2211 v\u2208V p(i, v) ) ,\nwhere q(i, v) = 1\u2212 \u220f v\u2032\u2208N\u2212\u22641(v) ( 1\u2212 p(i, v\u2032) ) .\nProof We follow the notation and the proof of Lemma 12, where it is shown that\u2211 v\u2208V p(i, v) q(i, v) \u2264 1 1\u2212 e\u22121 \u2211 v\u2208V ( p(i, v) P (i, v) + p(i, v) ) . In order to bound from above the sum \u2211\nv\u2208V p(i,v) P (i,v) , we combine (Alon et al., 2014, Lemma 14\nand 16) and derive the upper bound\n\u2211 v\u2208V p(i, v) P (i, v) \u2264 6\u03b1(G) ln ( 1 + N2K(1 + \u03b4) \u03b4 )\nholding when p(i, v) \u2265 \u03b4K(1+\u03b4) . Again, the probabilities p(i, 1), . . . , p(i,N) \u2265 0 need not sum to one in order for this lemma to apply.\nWith the above three lemmas handy, we are ready to prove Theorem 9.\nProof [Theorem 9] This proof is similar to the proof of Theorem 3, hence we only emphasize the differences between the two.\nFrom the update rule in Figure 5, we have, for each v \u2208 V ,\nWT+1(v) = K\u2211 i=1 p\u0303T (i) P\u0303T (v) e\u2212\u03b7 \u0302\u0300 T (i,v)\n\u2265 K\u2211 i=1 wT (i, v) WT (v)P\u0303T (v) e\u2212\u03b7 \u0302\u0300 T (i,v) (since p\u0303T (i) \u2265 wT (i, v)/WT (v))\n= K\u2211 i=1 p\u0303T\u22121(i, v)e \u2212\u03b7\u0302\u0300T\u22121(i,v)e\u2212\u03b7\u0302\u0300T (i,v) WT (v)P\u0303T\u22121(v)P\u0303T (v)\n...\n\u2265 K\u2211 i=1\nw1(i, v) e \u2212\u03b7\n\u2211T t=1 \u0302\u0300 t(i,v)\nW1(v) \u00b7 \u00b7 \u00b7WT (v)P\u03031(v) \u00b7 \u00b7 \u00b7 P\u0303T (v) .\nNow, because w1(i, v) = 1, W1(v) = K, and P\u0303t(v) \u2264 1 + \u03b4 for all t, see (6), the above chain of inequalities implies that, for any fixed action k \u2208 A,\n(1 + \u03b4)T K ( T\u220f t=1 Wt+1(v) ) \u2265 e\u2212\u03b7 \u2211T t=1 \u0302\u0300 t(k,v) . (11)\nAs usual, the quantity Wt+1(v) can be upper bounded as\nWt+1(v) = K\u2211 i=1 pt(i, v)e \u2212\u03b7\u0302\u0300t(i,v)\n\u2264 K\u2211 i=1 pt(i, v) ( 1\u2212 \u03b7\u0302\u0300t(i, v) + \u03b72 2 \u0302\u0300 t(i, v) 2 ) (from e\u2212x \u2264 1\u2212 x+ x2/2 for all x \u2265 0) = 1\u2212 \u03b7 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v) + \u03b72 2 K\u2211 i=1\npt(i, v)\u0302\u0300t(i, v)2 . Plugging back into (11) and taking logs of both sides gives\nT ln(1+\u03b4)+lnK+ T\u2211 t=1 ln\n( 1\u2212 \u03b7\nK\u2211 i=1 pt(i, v)\u0302\u0300t(i, v) + \u03b72 2 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v)2) \u2265 \u2212\u03b7 K\u2211 i=1 \u0302\u0300 t(k, v) .\nFinally, using ln(1 + x) \u2264 x, dividing by \u03b7, using \u03b4 = 1/T , and rearranging yields\nT\u2211 t=1 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v) \u2264 1 + lnK \u03b7 + T\u2211 t=1 \u0302\u0300 t(k, v) + \u03b7 2 T\u2211 t=1 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v)2 (12) hence arriving at the counterpart to (7).\nFrom this point on, we proceed as in the proof of Theorem 3 by taking expectation on the three sums in (12). Notice that we do still have, for all v \u2208 V , t > d(v), and i \u2208 A,\nEt\u2212d(v) [\u0302\u0300 t(i, v) ] = `t\u2212d(v)(i)\nEt\u2212d(v) [ pt(i, v)\u0302\u0300t(i, v)] = pt(i, v)`t\u2212d(v)(i)\nEt\u2212d(v) [ pt(i, v)\u0302\u0300t(i, v)2] = pt(i, v) `t\u2212d(v)(i)2\nqP,t\u2212d(v)(i, v) .\nWe can write\nE [ T\u2211 t=1 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v)] \u2265 E[ T\u2211 t=1 K\u2211 i=1 pt(i, v) `t(i) ] \u2212 2d(v)\u2212 (\u03b7 + \u03b4)T d(v)\nE [ T\u2211 t=1 \u0302\u0300 t(k, v) ] \u2264 T\u2211 t=1 `t(k)\nand, as in the proof of Theorem 3,\nE [ T\u2211 t=1 K\u2211 i=1 pt(i, v)\u0302\u0300t(i, v)2] \u2264 eE  T\u2211 t=d(v)+1 K\u2211 i=1 pt\u2212d(v)(i, v) qP,t\u2212d(v)(i, v)  . Summing over all agents v, dividing by N , and applying Lemma 15 to the directed graph GP , the latter inequality gives\n1\nN E [ T\u2211 t=1 K\u2211 i=1 \u2211 v\u2208V pt(i, v)\u0302\u0300t(i, v)2] \u2264 e 1\u2212 e\u22121 T ( 6K N \u03b1 (GP) ln ( 1 + 2TN2K ) + 1 ) .\nCombining as in (12), recalling that \u03b4 = 1/T , and setting for brevity d\u0304V = 1N \u2211\nv\u2208V d(v), we have thus obtained that the average welfare regret of EXP3-COOP2 satisfies\nRcoopT \u2264 3d\u0304V + \u03b7 T d\u0304V + 1 + lnK\n\u03b7 +\ne\u03b7\n2(1\u2212 e\u22121) T\n( 6K\nN \u03b1 (GP) ln\n( 1 + 2TN2K ) + 1 ) = O ( \u03b7 T d\u0304V + lnK\n\u03b7 + \u03b7 TK N \u03b1 (GP) ln (TNK) ) as T grows large. This concludes the proof."}, {"heading": "Appendix D. Proofs regarding Section 6", "text": "Proof of Corollary 11.\nProof In order to prove the upper bound, we use the exponentially-weighted algorithm with Estimate (3) specialized to the case of one agent only, namely Bt\u2212d(i) = I{It\u2212d = i} and\nqd,t\u2212d(i) = pt\u2212d(i). Notice that this amounts to running the standard Exp3 algorithm performing an update as soon a new loss becomes available. In this case, because N = \u03b1(G\u2264d) = 1, the bound of Theorem 3, with a suitable choice of \u03b3 (which depends on T , K, and d) reduces to\nRT = O ( d+ \u221a (K + d)T lnK ) .\nWe now prove a lower bound matching our upper bound up to logarithmic factors. The proof hinges on combining the known lower bound \u2126 (\u221a KT ) for bandits without delay of Auer et al. (2002) with the following argument by Weinberger and Ordentlich (2002) that provides a lower bound for the full information case with delay. The proof of the latter bound is by contradiction: we show that a low-regret full information algorithm for delay d > 0 can be used to design a low-regret full information algorithm for the d = 0 (no delay) setting. We then apply the known lower bound for the minimax regret in the no-delay setting to derive a lower bound for the setting with delay.\nFix d > 0 and let A be a predictor for the full-information online prediction problem with delay d. Let pt be the probability distribution used by A at time t. We now apply algorithm A to design a new algorithm A\u2032 for a full information online prediction problem with arbitrary loss vectors `\u20321, . . . , ` \u2032 B \u2208 [0, 1]K and no delay. More specifically, we create a sequence `1, . . . , `T \u2208 [0, 1]K of\nloss vectors such that T = (d+1)B and `t = `\u2032b where b = \u2308 t/(d+1) \u2309 . At each time b = 1, . . . , B algorithm A\u2032 uses the distribution\np\u2032b = 1\nd+ 1 d+1\u2211 s=1 p(d+1)(b\u22121)+s\nwhere pt = ( 1 K , . . . , 1 K ) for all t \u2264 1. Note that p\u2032b is defined using p(d+1)(b\u22121)+1, . . . ,p(d+1)b. These are in turn defined using the same loss vectors `\u20321, . . . , ` \u2032 b\u22121 since, by definition, each pt+1\nuses `1, . . . , `t\u2212d, and \u2308 (t\u2212 d)/(d+ 1) \u2309 = b\u2212 1 for all t = (d+ 1)(b\u2212 1), . . . , (d+ 1)b\u2212 1. So A\u2032 is a legitimate full-information online algorithm for the problem `\u20321, . . . , `\u2032B with no delay. As a consequence,\nT\u2211 t=1 K\u2211 i=1 `t(i)pt(i) = B\u2211 b=1 d+1\u2211 s=1 K\u2211 i=1 `\u2032b(i)p(d+1)(b\u22121)+s(i)\n= (d+ 1) B\u2211 b=1 K\u2211 i=1 1 d+ 1 d+1\u2211 s=1 `\u2032b(i)p(d+1)(b\u22121)+s(i)\n= (d+ 1) B\u2211 b=1 K\u2211 i=1 `\u2032b(i)p \u2032 b(i) .\nMoreover,\nmin k\u2208A T\u2211 t=1 `t(k) = (d+ 1) min k\u2208A B\u2211 b=1 `\u2032b(k) .\nSince we know that for any predictor A\u2032 there exists a loss sequence `\u20321, `\u20322, . . . such that the regret of A\u2032 is at least ( 1 \u2212 o(1) )\u221a (T/2) lnK, where o(1) \u2192 0 for K,B \u2192 \u221e, we have that the regret of A is at least\n(d+ 1)RT/(d+1)(A\u2032) = ( 1\u2212 o(1) ) (d+ 1)\n\u221a T\n2(d+ 1) lnK =\n( 1\u2212 o(1) )\u221a (d+ 1) T\n2 lnK ,\nwhere RT/(d+1)(A\u2032) is the regret of A\u2032 over T/(d + 1) time steps. The proof is completed by observing that that the regret of any predictor in the bandit setting with delay d cannot be smaller than the regret of the predictor in the bandit setting with no delay or smaller than the regret of the predictor in the full information setting with delay d. Hence, the minimax regret in the bandit setting with delay d must be at least of order\nmax {\u221a KT, \u221a (d+ 1)T lnK } = \u2126 (\u221a (K + d)T ) ."}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["Alekh Agarwal", "John C Duchi"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Agarwal and Duchi.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal and Duchi.", "year": 2011}, {"title": "Nonstochastic multi-armed bandits with graph-structured feedback", "author": ["Noga Alon", "Nicol\u00f2 Cesa-Bianchi", "Claudio Gentile", "Shie Mannor", "Yishay Mansour", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1409.8428,", "citeRegEx": "Alon et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alon et al\\.", "year": 2014}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Competitive collaborative learning", "author": ["Baruch Awerbuch", "Robert Kleinberg"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Awerbuch and Kleinberg.,? \\Q2008\\E", "shortCiteRegEx": "Awerbuch and Kleinberg.", "year": 2008}, {"title": "Ad hoc teamwork modeled with multi-armed bandits: An extension to discounted infinite rewards", "author": ["Samuel Barrett", "Peter Stone"], "venue": "In Proceedings of 2011 AAMAS Workshop on Adaptive and Learning Agents,", "citeRegEx": "Barrett and Stone.,? \\Q2011\\E", "shortCiteRegEx": "Barrett and Stone.", "year": 2011}, {"title": "Delay and cooperation in nonstochastic bandits", "author": ["Nicolo\u2019 Cesa-Bianchi", "Claudio Gentile", "Yishay Mansour", "Alberto Minora"], "venue": "arXiv preprint,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2016}, {"title": "Estimation, optimization, and parallelism when data is sparse", "author": ["John Duchi", "Michael I Jordan", "Brendan McMahan"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Duchi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2013}, {"title": "Asynchronous stochastic convex optimization", "author": ["John C Duchi", "Sorathan Chaturapruek", "Christopher R\u00e9"], "venue": "arXiv preprint arXiv:1508.00882,", "citeRegEx": "Duchi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2015}, {"title": "Efficient optimal learning for contextual bandits", "author": ["Miroslav Dud\u0131\u0301k", "Daniel J. Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang"], "venue": "UAI", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Independence and average distance in graphs", "author": ["P. Firby", "J. Haviland"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Firby and Haviland.,? \\Q1997\\E", "shortCiteRegEx": "Firby and Haviland.", "year": 1997}, {"title": "Online learning under delayed feedback", "author": ["Pooria Joulani", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Joulani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Joulani et al\\.", "year": 2013}, {"title": "Delay-tolerant online convex optimization: Unified analysis and adaptive-gradient algorithms", "author": ["Pooria Joulani", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17,", "citeRegEx": "Joulani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Joulani et al\\.", "year": 2016}, {"title": "Bandit problems in networks: Asymptotically efficient distributed allocation rules", "author": ["Soummya Kar", "H Vincent Poor", "Shuguang Cui"], "venue": "In 50th IEEE Conference on Decision and Control and European Control Conference (CDC-ECC),", "citeRegEx": "Kar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kar et al\\.", "year": 2011}, {"title": "Multiplicative updates outperform generic no-regret learning in congestion games", "author": ["Robert Kleinberg", "Georgios Piliouras", "\u00c9va Tardos"], "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,", "citeRegEx": "Kleinberg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kleinberg et al\\.", "year": 2009}, {"title": "Efficient learning by implicit exploration in bandit problems with side observations", "author": ["Tom\u00e1\u0161 Koc\u00e1k", "Gergely Neu", "Michal Valko", "Remi Munos"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2014}, {"title": "On distributed cooperative decision-making in multiarmed bandits", "author": ["Peter Landgren", "Vaibhav Srivastava", "Naomi Ehrich Leonard"], "venue": "arXiv preprint arXiv:1512.06888,", "citeRegEx": "Landgren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Landgren et al\\.", "year": 2015}, {"title": "Distributed delayed proximal gradient methods", "author": ["Mu Li", "David G. Andersen", "Alexander Smola"], "venue": "In NIPS Workshop on Optimization for Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Locality in distributed graph algorithms", "author": ["Nathan Linial"], "venue": "SIAM J. Comput.,", "citeRegEx": "Linial.,? \\Q1992\\E", "shortCiteRegEx": "Linial.", "year": 1992}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["Ji Liu", "Stephen J Wright", "Christopher R\u00e9", "Victor Bittorf", "Srikrishna Sridhar"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Perturbed iterate analysis for asynchronous stochastic optimization", "author": ["Horia Mania", "Xinghao Pan", "Dimitris Papailiopoulos", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan"], "venue": "arXiv preprint arXiv:1507.06970,", "citeRegEx": "Mania et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mania et al\\.", "year": 2015}, {"title": "Delay-tolerant algorithms for asynchronous distributed online learning", "author": ["Brendan McMahan", "Matthew Streeter"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "McMahan and Streeter.,? \\Q2014\\E", "shortCiteRegEx": "McMahan and Streeter.", "year": 2014}, {"title": "On-line learning with delayed label feedback. In Algorithmic Learning Theory, pages 399\u2013413", "author": ["Chris Mesterharm"], "venue": null, "citeRegEx": "Mesterharm.,? \\Q2005\\E", "shortCiteRegEx": "Mesterharm.", "year": 2005}, {"title": "Improving Online Learning", "author": ["Chris Mesterharm"], "venue": "PhD thesis,", "citeRegEx": "Mesterharm.,? \\Q2007\\E", "shortCiteRegEx": "Mesterharm.", "year": 2007}, {"title": "Explore no more: Improved high-probability regret bounds for non-stochastic bandits", "author": ["Gergely Neu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Neu.,? \\Q2015\\E", "shortCiteRegEx": "Neu.", "year": 2015}, {"title": "Online Markov decision processes under bandit feedback", "author": ["Gergely Neu", "Andras Antos", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Neu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "Online markov decision processes under bandit feedback", "author": ["Gergely Neu", "Andras Gyorgy", "Csaba Szepesvari", "Andras Antos"], "venue": "Automatic Control, IEEE Transactions on,", "citeRegEx": "Neu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2014}, {"title": "Parallel correlation clustering on big graphs", "author": ["Xinghao Pan", "Dimitris Papailiopoulos", "Samet Oymak", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Pan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2015}, {"title": "Online learning with adversarial delays", "author": ["Kent Quanrud", "Daniel Khashabi"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Quanrud and Khashabi.,? \\Q2015\\E", "shortCiteRegEx": "Quanrud and Khashabi.", "year": 2015}, {"title": "Szlak. Multi-player bandits \u2013 a musical chairs approach", "author": ["Jonathan Rosenski", "Ohad Shamir", "Liran"], "venue": "CoRR, abs/1512.02866,", "citeRegEx": "Rosenski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rosenski et al\\.", "year": 2015}, {"title": "Prediction with limited advice and multiarmed bandits with paid observations", "author": ["Yevgeny Seldin", "Peter Bartlett", "Koby Crammer", "Yasin Abbasi-Yadkori"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Seldin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seldin et al\\.", "year": 2014}, {"title": "Dcops and bandits: Exploration and exploitation in decentralised coordination", "author": ["Ruben Stranders", "Long Tran-Thanh", "Francesco M Delle Fave", "Alex Rogers", "Nicholas R Jennings"], "venue": "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume", "citeRegEx": "Stranders et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stranders et al\\.", "year": 2012}, {"title": "Survey of local algorithms", "author": ["Jukka Suomela"], "venue": "ACM Computing Surveys,", "citeRegEx": "Suomela.,? \\Q2013\\E", "shortCiteRegEx": "Suomela.", "year": 2013}, {"title": "Gossip-based distributed stochastic bandit algorithms", "author": ["Balazs Szorenyi", "R\u00f3bert Busa-Fekete", "Istv\u00e1n Heged\u00fcs", "R\u00f3bert Orm\u00e1ndi", "M\u00e1rk Jelasity", "Bal\u00e1zs K\u00e9gl"], "venue": "In 30th International Conference on Machine Learning (ICML 2013),", "citeRegEx": "Szorenyi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szorenyi et al\\.", "year": 2013}, {"title": "Distributed online learning via cooperative contextual bandits", "author": ["Cem Tekin", "Mihaela van der Schaar"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Tekin and Schaar.,? \\Q2015\\E", "shortCiteRegEx": "Tekin and Schaar.", "year": 2015}, {"title": "Distributed online learning in social recommender systems", "author": ["Cem Tekin", "Simpson Z. Zhang", "Mihaela van der Schaar"], "venue": "J. Sel. Topics Signal Processing,", "citeRegEx": "Tekin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tekin et al\\.", "year": 2014}, {"title": "Daccer: Distributed assessment of the closeness centrality ranking in complex networks", "author": ["Klaus Wehmuth", "Artur Ziviani"], "venue": "Computer Networks,", "citeRegEx": "Wehmuth and Ziviani.,? \\Q2013\\E", "shortCiteRegEx": "Wehmuth and Ziviani.", "year": 2013}, {"title": "On delayed prediction of individual sequences", "author": ["Marcelo J Weinberger", "Erik Ordentlich"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Weinberger and Ordentlich.,? \\Q2002\\E", "shortCiteRegEx": "Weinberger and Ordentlich.", "year": 2002}, {"title": "Slow learners are fast", "author": ["Martin Zinkevich", "John Langford", "Alex J. Smola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Zinkevich et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3).", "startOffset": 101, "endOffset": 120}, {"referenceID": 2, "context": "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent\u2019s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent\u2019s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t\u2212 s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like \u221a KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK). Here \u03b1\u2264d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = \u221a K this bound is at mostK1/4 \u221a T lnK+ \u221a K(lnT ) for any connected graph \u2014see Remark 7 in Section 4.1\u2014 which is asymptotically better than \u221a KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay.", "startOffset": 101, "endOffset": 1988}, {"referenceID": 2, "context": "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent\u2019s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent\u2019s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t\u2212 s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like \u221a KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK). Here \u03b1\u2264d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = \u221a K this bound is at mostK1/4 \u221a T lnK+ \u221a K(lnT ) for any connected graph \u2014see Remark 7 in Section 4.1\u2014 which is asymptotically better than \u221a KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order \u221a( 1 + KN ) T ignoring polylog factors.", "startOffset": 101, "endOffset": 2109}, {"referenceID": 2, "context": "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent\u2019s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent\u2019s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t\u2212 s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like \u221a KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK). Here \u03b1\u2264d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = \u221a K this bound is at mostK1/4 \u221a T lnK+ \u221a K(lnT ) for any connected graph \u2014see Remark 7 in Section 4.1\u2014 which is asymptotically better than \u221a KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order \u221a( 1 + KN ) T ignoring polylog factors.1 We recover the same bound as a special case of our bound when G is a clique and d = 1. In the clique case our bound is also similar to the bound \u221a K N (T lnK) achieved by Seldin et al. (2014) in a single-agent bandit setting where, at each time step, the agent can choose a subset ofN \u2264 K actions and observe their loss.", "startOffset": 101, "endOffset": 2395}, {"referenceID": 2, "context": "We introduce the EXP3-COOP algorithm, a distributed and cooperative version of the EXP3 algorithm of Auer et al. (2002). EXP3-COOP works within a distributed and synced model where each agent runs an instance of the same bandit algorithm (EXP3). All bandit instances are initialized in the same way irrespective to the agent\u2019s location in the network (that is, agents have no preliminary knowledge of the network), and we assume the information about an agent\u2019s actions is propagated through the network with a unit delay for each crossed edge. In each round t, each agent selects an action and incurs the corresponding loss (which is the same for all agents that pick that action in round t). Besides observing the loss of the selected action, each agent obtains the information previously broadcast by other agents with a delay equal to the shortest-path distance between the agents. Namely, at time t an agent learns what the agents at shortest-path distance s did at time t\u2212 s for each s = 1, . . . , d, where d is a delay parameter. In this scenario, we aim at controlling the growth of the regret averaged over all agents (the so-called average welfare regret). In the noncooperative case, when agents ignore the information received from other agents, the average welfare regret grows like \u221a KT (the minimax rate for standard bandit setting), whereK is the number of actions and T is the time horizon. We show that, using cooperation, N agents with communication graph G can achieve an average welfare regret of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK). Here \u03b1\u2264d denotes the independence number of the d-th power of G (i.e., the graph G augmented with all edges between any two pair of nodes at shortest-path distance less than or equal to d). When d = \u221a K this bound is at mostK1/4 \u221a T lnK+ \u221a K(lnT ) for any connected graph \u2014see Remark 7 in Section 4.1\u2014 which is asymptotically better than \u221a KT . Networks of nonstochastic bandits were also investigated by Awerbuch and Kleinberg (2008) in a setting where the distribution over actions is shared among the agents without delay. Awerbuch and Kleinberg (2008) prove a bound on the average welfare regret of order \u221a( 1 + KN ) T ignoring polylog factors.1 We recover the same bound as a special case of our bound when G is a clique and d = 1. In the clique case our bound is also similar to the bound \u221a K N (T lnK) achieved by Seldin et al. (2014) in a single-agent bandit setting where, at each time step, the agent can choose a subset ofN \u2264 K actions and observe their loss. In the case whenN = 1 (single agent), our analysis can be applied to the nonstochastic bandit problem where the player observes the loss of each played action with a delay of d steps. In this case we improve on the previous result of \u221a (d+ 1)KT by Neu et al. (2010, 2014), and give the first characterization (up to logarithmic factors) of the minimax regret, which is of order \u221a (d+K)T . In principle, the problem of delays in online learning could be tackled by simple reductions. Yet, these reductions give rise to suboptimal results. In the single agent setting, where the delay is constant and equal to d, one can use the technique of Weinberger and Ordentlich (2002) and run d+1 in1.", "startOffset": 101, "endOffset": 3197}, {"referenceID": 11, "context": "Additional works (Joulani et al., 2016; Quanrud and Khashabi, 2015) prove regret bounds for the full-information case of the form \u221a (D + T ) lnK, where D is the total delay experienced over the T rounds.", "startOffset": 17, "endOffset": 67}, {"referenceID": 27, "context": "Additional works (Joulani et al., 2016; Quanrud and Khashabi, 2015) prove regret bounds for the full-information case of the form \u221a (D + T ) lnK, where D is the total delay experienced over the T rounds.", "startOffset": 17, "endOffset": 67}, {"referenceID": 3, "context": "To the best of our knowledge, the first paper about nonstochastic cooperative bandit networks is (Awerbuch and Kleinberg, 2008).", "startOffset": 97, "endOffset": 127}, {"referenceID": 32, "context": "More papers analyze the stochastic setting, and the closest one to our work is perhaps (Szorenyi et al., 2013).", "startOffset": 87, "endOffset": 110}, {"referenceID": 15, "context": "A more recent paper is (Landgren et al., 2015), where the communication network is a fixed graph and a cooperative version of the UCB algorithm is introduced which uses a distributed consensus algorithm to estimate the mean rewards of the arms.", "startOffset": 23, "endOffset": 46}, {"referenceID": 16, "context": "Additional Related Work Many important ideas in delayed online learning, including the observation that the effect of delays can be limited by controlling the amount of change in the agent strategy, were introduced by Mesterharm (2005) \u2014see also (Mesterharm, 2007, Chapter 8).", "startOffset": 218, "endOffset": 236}, {"referenceID": 8, "context": "Furher progress is made by Joulani et al. (2013), who also study delays in the general partial monitoring setting.", "startOffset": 27, "endOffset": 49}, {"referenceID": 7, "context": "In the stochastic case, bandit learning with delayed feedback was considered by Dud\u0131\u0301k et al. (2011); Joulani et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 7, "context": "In the stochastic case, bandit learning with delayed feedback was considered by Dud\u0131\u0301k et al. (2011); Joulani et al. (2013). To the best of our knowledge, the first paper about nonstochastic cooperative bandit networks is (Awerbuch and Kleinberg, 2008).", "startOffset": 80, "endOffset": 124}, {"referenceID": 12, "context": "Another interesting paper about cooperating bandits in a stochastic setting is (Kar et al., 2011).", "startOffset": 79, "endOffset": 97}, {"referenceID": 28, "context": "The resulting bandit problem is one of coordination in a competitive environment, because every time two or more agents select the same action at the same time step they both get a zero reward due to the interference \u2014see (Rosenski et al., 2015) for recent work on stochastic competitive bandits and (Kleinberg et al.", "startOffset": 222, "endOffset": 245}, {"referenceID": 13, "context": ", 2015) for recent work on stochastic competitive bandits and (Kleinberg et al., 2009) for a study of more general congestion games in a game-theoretic setting.", "startOffset": 62, "endOffset": 86}, {"referenceID": 37, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 0, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 16, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 20, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 27, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 18, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 7, "context": ", (Zinkevich et al., 2009; Agarwal and Duchi, 2011; Li et al., 2013; McMahan and Streeter, 2014; Quanrud and Khashabi, 2015; Liu et al., 2015; Duchi et al., 2015).", "startOffset": 2, "endOffset": 162}, {"referenceID": 3, "context": "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control.", "startOffset": 65, "endOffset": 90}, {"referenceID": 3, "context": "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf.", "startOffset": 65, "endOffset": 251}, {"referenceID": 3, "context": "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf. In a related paper, Tekin et al. (2014) look at a stochastic bandit model with combinatorial actions in a distributed recommender system setting, and study incentives among agents who can now recommend items taken from other agents\u2019 inventories.", "startOffset": 65, "endOffset": 449}, {"referenceID": 3, "context": "Cooperative bandits with asymmetric feedback are also studied by Barrett and Stone (2011). In their model, an agent must teach the reward distribution to another agent while keeping the discounted regret under control. Tekin and van der Schaar (2015) investigate a stochastic contextual bandit model where each agent can either privately select an action or have another agent select an action on his behalf. In a related paper, Tekin et al. (2014) look at a stochastic bandit model with combinatorial actions in a distributed recommender system setting, and study incentives among agents who can now recommend items taken from other agents\u2019 inventories. Another line of relevant work involves problems of decentralized bandit coordination. For example, Stranders et al. (2012) consider a bandit coordination problem where the the reward function is global and can be represented as a factor graph in which each agent controls a subset of the variables.", "startOffset": 65, "endOffset": 778}, {"referenceID": 5, "context": "Proofs of all the results stated here can be found in (Cesa-Bianchi et al., 2016).", "startOffset": 54, "endOffset": 81}, {"referenceID": 17, "context": "Our model is similar to the LOCAL communication model in distributed computing (Linial, 1992; Suomela, 2013), where the output of a node depends only on the inputs of other nodes in a constantsize neighborhood of it, and the goal is to derive algorithms whose running time is independent of the network size.", "startOffset": 79, "endOffset": 108}, {"referenceID": 31, "context": "Our model is similar to the LOCAL communication model in distributed computing (Linial, 1992; Suomela, 2013), where the output of a node depends only on the inputs of other nodes in a constantsize neighborhood of it, and the goal is to derive algorithms whose running time is independent of the network size.", "startOffset": 79, "endOffset": 108}, {"referenceID": 36, "context": "This is very much reminiscent of a full information scenario, and in fact our bound becomes of order \u221a (dG + 1)T lnK + dG lnT , which is close to the full information minimax rate \u221a (d+ 1)T lnK when feedback has a constant delay d (Weinberger and Ordentlich, 2002).", "startOffset": 231, "endOffset": 264}, {"referenceID": 14, "context": "There has been some recent work on adaptive learning rate tuning applied to nonstochastic bandit algorithms (Koc\u00e1k et al., 2014; Neu, 2015).", "startOffset": 108, "endOffset": 139}, {"referenceID": 23, "context": "There has been some recent work on adaptive learning rate tuning applied to nonstochastic bandit algorithms (Koc\u00e1k et al., 2014; Neu, 2015).", "startOffset": 108, "endOffset": 139}, {"referenceID": 9, "context": ", in (Firby and Haviland, 1997).", "startOffset": 5, "endOffset": 31}, {"referenceID": 2, "context": ", (Auer et al., 2002)\u2014 but in a slightly different manner.", "startOffset": 2, "endOffset": 21}, {"referenceID": 35, "context": ", (Wehmuth and Ziviani, 2013), and references therein.", "startOffset": 2, "endOffset": 29}, {"referenceID": 31, "context": "This problem was studied by Weinberger and Ordentlich (2002) in the full information case, for which they proved that \u221a (d+ 1)T lnK is the optimal order for the minimax regret.", "startOffset": 28, "endOffset": 61}, {"referenceID": 10, "context": "(2010, 2014) \u2014see also Joulani et al. (2013)\u2014 whose techniques can be used to obtain a regret bound of order \u221a (d+ 1)KT .", "startOffset": 23, "endOffset": 45}, {"referenceID": 26, "context": "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).", "startOffset": 102, "endOffset": 188}, {"referenceID": 6, "context": "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).", "startOffset": 102, "endOffset": 188}, {"referenceID": 19, "context": "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).", "startOffset": 102, "endOffset": 188}, {"referenceID": 20, "context": "A possible line of attack to solve this problem could be the use of graph sparsity along the lines of (Pan et al., 2015; Duchi et al., 2013; Mania et al., 2015; McMahan and Streeter, 2014).", "startOffset": 102, "endOffset": 188}, {"referenceID": 11, "context": "Even for the single-agent setting, we do not know whether regret bounds of the form \u221a (D + T ) lnK, where D is the total delay experienced over the T rounds, could be proven \u2014see (Joulani et al., 2016; Quanrud and Khashabi, 2015) for similar results in the fullinformation setting.", "startOffset": 179, "endOffset": 229}, {"referenceID": 27, "context": "Even for the single-agent setting, we do not know whether regret bounds of the form \u221a (D + T ) lnK, where D is the total delay experienced over the T rounds, could be proven \u2014see (Joulani et al., 2016; Quanrud and Khashabi, 2015) for similar results in the fullinformation setting.", "startOffset": 179, "endOffset": 229}, {"referenceID": 2, "context": "The proof hinges on combining the known lower bound \u03a9 (\u221a KT ) for bandits without delay of Auer et al. (2002) with the following argument by Weinberger and Ordentlich (2002) that provides a lower bound for the full information case with delay.", "startOffset": 91, "endOffset": 110}, {"referenceID": 2, "context": "The proof hinges on combining the known lower bound \u03a9 (\u221a KT ) for bandits without delay of Auer et al. (2002) with the following argument by Weinberger and Ordentlich (2002) that provides a lower bound for the full information case with delay.", "startOffset": 91, "endOffset": 174}], "year": 2016, "abstractText": "We study networks of communicating learning agents that cooperate to solve a common nonstochastic bandit problem. Agents use an underlying communication network to get messages about actions selected by other agents, and drop messages that took more than d hops to arrive, where d is a delay parameter. We introduce EXP3-COOP, a cooperative version of the EXP3 algorithm and prove that with K actions and N agents the average per-agent regret after T rounds is at most of order \u221a( d+ 1 + KN \u03b1\u2264d ) (T lnK), where \u03b1\u2264d is the independence number of the d-th power of the communication graphG. We then show that for any connected graph, for d = \u221a K the regret bound is K \u221a T , strictly better than the minimax regret \u221a KT for noncooperating agents. More informed choices of d lead to bounds which are arbitrarily close to the full information minimax regret \u221a T lnK when G is dense. When G has sparse components, we show that a variant of EXP3-COOP, allowing agents to choose their parameters according to their centrality in G, strictly improves the regret. Finally, as a by-product of our analysis, we provide the first characterization of the minimax regret for bandit learning with delay.", "creator": "LaTeX with hyperref package"}}}