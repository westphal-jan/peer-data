{"id": "1602.04847", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2016", "title": "Black-box Optimization with a Politician", "abstract": "recently we formally propose a new framework introduced for numerical black - box convex optimization which is indeed well - suited application for situations situation where adaptive gradient cost computations are expensive. we derive a remarkable new method for solving this framework which leverages several several concepts from explicit convex structural optimization, from standard descriptive first - order analytic methods ( e. g. algebraic gradient descent or quasi - newton method methods ) to additional analytical convergence centers ( i. e. minimizers consist of self - crossing concordant barriers ). formally we demonstrate empirically such that utilizing our new integration technique compares favorably with maximum state optimization of the art algorithms ( cite such they as bfgs ).", "histories": [["v1", "Mon, 15 Feb 2016 21:35:58 GMT  (247kb,D)", "http://arxiv.org/abs/1602.04847v1", "19 pages"]], "COMMENTS": "19 pages", "reviews": [], "SUBJECTS": "math.OC cs.DS cs.LG cs.NA", "authors": ["s\u00e9bastien bubeck", "yin tat lee"], "accepted": true, "id": "1602.04847"}, "pdf": {"name": "1602.04847.pdf", "metadata": {"source": "CRF", "title": "Black-box optimization with a politician", "authors": ["S\u00e9bastien Bubeck", "Yin Tat Lee"], "emails": ["sebubeck@microsoft.com", "yintat@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In standard black-box convex optimization Nemirovski and Yudin [1983], Nesterov [2004], Bubeck [2015] first-order methods interact with an oracle: given a query point x, the oracle reports the value and gradient of the underlying objective function f at x. In this paper we propose to replace the oracle by a politician. Instead of answering the original query x the politician changes the question and answers a new query y which is guaranteed to be better than the original query x in the sense that f(y) \u2264 f(x). The newly selected query y also depends on the history of queries that were made to the politician. Formally we introduce the following definition (for sake of simplicty we write\u2207f(x) for either a gradient or a subgradient of f at x).\nDefinition 1 Let X \u2282 Rn and f : X \u2192 R. A politician \u03a6 for f is a mapping from X \u00d7\u222a\u221ek=0(X \u00d7 R \u00d7 Rn)k to X such that for any k \u2265 0, x \u2208 X , h \u2208 (X \u00d7 R \u00d7 Rn)k one has f(\u03a6(x, h)) \u2264 f(x). Furthermore when queried at x with history h a politician for f also output f(\u03a6(x, h)) and \u2207f(\u03a6(x, h)) (in order to not overload notation we do not include these outputs in the range of \u03a6).\nLet us clarify the interaction of a first-order method with a politician. Note that we refer to the couple (first-order method, politician) as the algorithm. Let M : \u222a\u221ek=0(X \u00d7 R \u00d7 Rn)k \u2192 X be a first-order method and \u03a6 a politician for some function f : X \u2192 R. The course of the algorithm (M,\u03a6) then goes as follows: at iteration k+1 one first calculates the method\u2019s query point xk+1 =\n\u2217Most of this work were done while the author was at Microsoft Research, Redmond. The author was supported by NSF awards 0843915 and 1111109.\nar X\niv :1\n60 2.\n04 84\n7v 1\n[ m\nat h.\nO C\n] 1\n5 Fe\nM(hk) (with h0 = \u2205), then one calculates the politician\u2019s new query point yk+1 = \u03a6(xk+1, hk) and the first order information at this point (f(yk+1),\u2207f(yk+1)), and finally one updates the history with this new information hk+1 = (hk, (yk+1, f(yk+1),\u2207f(yk+1))). Note that a standard oracle simply corresponds to a politician O for f such that O(x, h) = (x, f(x),\u2207f(x)) (in particular the algorithm (M,O) is the usual algorithm corresponding to the first-order method M ).\nThe philosophy of the above definition is that it gives in some sense an automatic way to combine different optimization algorithms. Say for example that we wish to combine the ellipsoid method with gradient descent. One way to do so is to design an \u201cellipsoidal politician\u201d: the politician keeps track of a feasible ellipsoidal region based on the previously computed gradients, and when asked with the query x the politician chooses as a new query y the result of a line-search on the line between x and the center of current ellipsoid. Gradient descent with this ellipsoidal politician would then replace the step x\u2190 x\u2212 \u03b7\u2207f(x) by x\u2190 y \u2212 \u03b7\u2207f(y). The hope is that in practice such a combination would integrate the fast incremental progress of gradient descent with the geometrical progress of the ellipsoid method.\nIn this paper we focus on unconstrained convex optimization. We are particularly interested in situations where calculating a (sub)gradient has superlinear complexity (i.e., n) such as in logistic regression and semidefinite programming. In such cases it is natural to try to make the most out of the computed gradients by incorporating geometric reasoning (such as in the ellipsoid method). We do so by introducing the geometric politician (Section 3), which is based on a combination of the recent ideas of Bubeck et al. [2015] with standard cutting plane/interior point methods machinery (through the notion of a \u201ccenter\u201d of a set, see Section 4). For a given first order method M , we denote by M+ the algorithm obtained by running M with the geometric politician. We demonstrate empirically (Section 5) the effectiveness of the geometric politician on various standard first-order methods for convex optimization (gradient descent, Nesterov\u2019s accelerated gradient descent, non-linear conjugate gradient, BFGS). In particular we show that BFGS+ is a surprisingly robust and parameter-free algorithm with state of the art performance across a wide range of problems (both smooth and non-smooth)."}, {"heading": "2 Affine invariant politician", "text": "As mentioned above we assume that the complexity of computing the map x 7\u2192 \u2207f(x) is superlinear. This implies that we can afford to have a politician such that the complexity of computing the map (x, h) 7\u2192 \u03a6(x, h) is O(n \u00d7 poly(k)) (we think of the number of iterations k as typically much smaller than the dimension n). We show in this section that this condition is (essentially) automatically satisfied as long as the politician is affine invariant in the following sense (we use a slight abuse of language and refer to a map f 7\u2192 \u03a6f , where \u03a6f is a politician for f , as a politician):\nDefinition 2 A politician f 7\u2192 \u03a6f is called affine invariant if for any function f and any affine map T : Rm \u2192 Rn such that T (x) = z + Lx for some matrix L, k \u2265 0, x \u2208 Rn, (yi, vi, gi) \u2208 Rm \u00d7 R\u00d7 Rn, one has\nT (\u03a6f\u25e6T (x, (yi, vi, L >gi)i\u2208[k])) = \u03a6f (T (x), (T (yi), vi, gi)i\u2208[k]).\nWe say that an affine invariant politician has cost \u03c8 : N \u2192 N if for any f : Rk \u2192 R the map (x, h) \u2208 Rk \u00d7 (Rk \u00d7 R\u00d7 Rk)k 7\u2192 \u03a6f (x, h) can be computed in time \u03c8(k).\nProposition 1 Let \u03a6 be an affine invariant politician with cost \u03c8. Then for any f : Rn \u2192 R, (yi, vi, gi) \u2208 Rn \u00d7 R \u00d7 Rn, i \u2208 [k] and x, yi \u2208 y1 + Span(g1, . . . , gk) one can compute \u03a6f (x, (yi, vi, gi)i\u2208[k]) \u2208 Rn in time \u03c8(k) +O(nk2).\nProof Let G be the n\u00d7 k matrix with ith column given by gi. We consider the QR decomposition of G which can be computed in time O(nk2), that is Q is an n \u00d7 k matrix and R a k \u00d7 k matrix such that G = QR and Q>Q = Ik. Let T be the affine map defined by T = y1 + Q. Note that since x \u2208 y1 + Span(g1, . . . , gk) one has x = T (Q>(x\u2212 y1)) (and similarly for yi). Thus by affine invariance one has\n\u03a6f (x, (yi, vi, gi)) = \u03a6f (T (Q >(x\u2212 y1)), (T (Q>(yi \u2212 y1)), vi, gi))\n= y1 +Q\u03a6f\u25e6T (Q >(x\u2212 y1), (Q>(yi \u2212 y1), vi, Ri)),\nwhere Ri is the ith column of R. Furthermore by definition of the cost \u03c8 and since f \u25e6T is defined on Rk we see that this last quantity can be computed in time \u03c8(k) + O(nk2), thus concluding the proof.\nThe above proposition shows that with an affine invariant politician and a first order method M verifying for any (yi, vi, gi)i\u2208[k] \u2208 (Rn \u00d7 R\u00d7 Rn)k,\nM((yi, vi, gi)i\u2208[k]) \u2208 y1 + Span(y1, . . . , yk, g1, . . . , gk),\none can run k steps of the corresponding algorithm in time O(nk2 + k\u03c8(k)) plus the time to compute the k function values and gradients of the underlying function f to be optimized. Note that one gets a time of O(nk2) instead of O(nk3) as one can store the QR decomposition from one step to the next, and updating the decomposition only cost O(nk)."}, {"heading": "3 Geometric politician", "text": "We describe in this section the geometric politician which is based on ideas developed in Bubeck et al. [2015]. A key observation in the latter paper is that if f is a \u03b1-strongly convex function minimized at x\u2217 then one has for any x,\u2225\u2225\u2225\u2225x\u2217 \u2212 x\u2212 1\u03b1\u2207f(x)\n\u2225\u2225\u2225\u22252 \u2264 \u2016\u2207f(x)\u20162\u03b12 \u2212 2\u03b1 (f(x)\u2212 f(x\u2217)) . This motivates the following definition:\nB(x, \u03b1, fval) := { z \u2208 Rn : \u2225\u2225\u2225\u2225z \u2212 x\u2212 1\u03b1\u2207f(x) \u2225\u2225\u2225\u22252 \u2264 \u2016\u2207f(x)\u20162\u03b12 \u2212 2\u03b1 (f(x)\u2212 fval) } .\nIn particular given the first order information at y1, . . . , yk one knows that the optimum x\u2217 lies in the region Rk \u2282 Rn defined by\nRk = \u22c2 i\u2208[k] B(yi, \u03b1, fval) where fval = min i\u2208[k] f(yi). (1)\nNow suppose that given this first order information at y1, . . . , yk the first order method asks to query x. How should we modify this query in order to take into account the geometric information that x\u2217 \u2208 Rk? First observe that for any z, B(z, \u03b1, fval) is contained in a halfspace that has z on its boundary (in the limiting case \u03b1 \u2192 0 the set B(z, \u03b1, f(z)) is exactly a halfspace). In particular if the next query point yk+1 is the center of gravity of Rk then we have that the volume of Rk+1 is at most 1 \u2212 1/e times the volume of Rk (see Gru\u0308nbaum [1960]), thus leading to an exponential convergence rate. However the region Rk can be very large initially, and the center of gravity might have a large function value and gradient, which means that Rk would be intersected with a large sphere (possibly so large that it is close to a halfspace). On the other hand the first order method recommends to query x, which we can think of as a local improvement of yk, which should lead to a much smaller sphere. The issue is that the position of this sphere might be such that the intersection with Rk is almost as large as the sphere itself. In order to balance between the geometric and function value/gradient considerations we propose for the new query to do a line search between the center of Rk and the recommended query x. The geometric politician follows this recipe with two important modifications: (i) there are many choices of centers that would guarantee an exponential convergence rate while being much easier to compute than the center of gravity, and we choose here to consider the volumetric center, see Section 4 for the definition and more details about this notion; (ii) we use a simple heuristic to adapt online the strong convexity parameter \u03b1, namely we start with some large value for \u03b1 and if it happens that the feasible region Rk is empty then we know that \u03b1 was too large, in which case we reduce it. We can now describe formally the geometric politician, see Algorithm 1. Importantly one can verify that the geometric politician is affine invariant and thus can be implemented efficiently (see the proof of Proposition 1).\nAlgorithm 1: Geometric Politician Parameter: An upper bound on the strong convexity parameter \u03b1. (Can be +\u221e.) Input: Query x, past queries and the corresponding first order information (yi, f(yi),\u2207f(yi))i\u2208[k]. Let fval = mini\u2208[k] f(yi) and the feasible region Rk(\u03b1) = \u22c2 i\u2208[k] B(yi, \u03b1, fval).\nif Rk(\u03b1) = \u2205 then Let \u03b1 be the largest number such that Rk(\u03b1) 6= \u2205. \u03b1\u2190 \u03b1/4. end Let yk+1 = argminy\u2208{tx+(1\u2212t)c(Rk(\u03b1)),t\u2208R} f(y) where c(Rk(\u03b1)) is the volumetric center of Rk(\u03b1) (see Section 4). Output: yk+1, f(yk+1) and \u2207f(yk+1)."}, {"heading": "4 Volumetric center", "text": "The volumetric barrier for a polytope was introduced in Vaidya [1996] to construct an algorithm with both the oracle complexity of the center of gravity method and the computational complexity of the ellipsoid method (see [Section 2.3, Bubeck [2015]] for more details and Lee et al. [2015] for\nrecent advances on this construction). Recalling that the standard logarithmic barrier FP for the polytope P = {x \u2208 Rn : a>i x < bi, i \u2208 [m]} is defined by\nFP (x) = \u2212 m\u2211 i=1 log(bi \u2212 a>i x),\none defines the volumetric barrier vP for P by\nvP (x) = logdet(\u22072FP (x)). The volumetric center c(P ) is then defined as the minimizer of vP . In the context of the geometric politician (see Algorithm 1) we are dealing with an intersection of balls rather than an intersection of halfspaces. More precisely the region of interest is of the form:\nR = k\u22c2 i=1 {x \u2208 Rn : \u2016x\u2212 ci\u2016 \u2264 ri} .\nFor such a domain the natural self-concordant barrier to consider is:\nFR(x) = \u2212 1\n2 k\u2211 i=1 log ( r2i \u2212 \u2016x\u2212 ci\u20162 ) .\nThe volumetric barrier is defined as before by\nvR(x) = logdet(\u22072FR(x)), and the volumetric center of R is the minimizer of vR. It is shown in Anstreicher [2004] that vR is a self-concordant barrier which means that the center can be updated (when a new ball is added to R) via few iterations of Newton\u2019s method. Often in practice, it takes less than 5 iterations to update the minimizer of a self-concordant barrier Goffin and Vial [1999], Bahn et al. [1995] when we add a new constraint. Hence, the complexity merely depends on how fast we can compute the gradient and Hessian of FR and vR.\nProposition 2 For the analytic barrier FR, we have that\n\u2207FR(x) =A>1k\u00d71, \u22072FR(x) =2A>A+ \u03bb(1)I\nwhere d is a vector defined by (r2i \u2212 \u2016x\u2212 ci\u20162) \u22121, A is a k \u00d7 n matrix with ith row given by di(x)(x\u2212 ci), \u03bb(p) = \u2211 i\u2208[k] d p i (x) and 1k\u00d71 is a k \u00d7 1 matrix with all entries being 1.\nFor the volumetric center, we have that \u2207vR(x) = (( 2trH\u22121 ) I + 4H\u22121 ) A>d+ 8A>\u03c3,\n\u22072vR(x) = 48A>\u03a3A\u2212 64A> ( AH\u22121A> )(2) A\n+ ( 8tr(D\u03a3) + 2\u03bb(2)tr(H\u22121) ) I + 4\u03bb(2)H\u22121\n+ 8tr(H\u22121)A>DA+ 16sym ( A>DAH\u22121 ) \u2212 4tr(H\u22122)A>DJDA\u2212 8H\u22121A>DJDAH\u22121\n\u2212 8sym(A>DJDAH\u22122)\u2212 8 ( d>AH\u22121A>d ) H\u22121\n\u2212 16sym ( A>diag ( AH\u22122A> ) JDA ) \u2212 32sym ( A>diag(AH\u22121A>d)AH\u22121 )\nwhere H = \u22072FR(x), \u03c3i = e>i AH\u22121A>ei, ei is the indicator vector with ith coordinate, J is a k \u00d7 k matrix with all entries being 1, sym(B) = B + B>, diag(v) is a diagonal matrix with diag(v)ii = vi, \u03a3 = diag(\u03c3), and B(2) is the Schur square of B defined by B (2) ij = B 2 ij .\nThe above proposition shows that one step of Newton method for analytic center requires 1 dense matrix multiplication and solving 1 linear system; and for volumetric center, it requires 5 dense matrix multiplications, 1 matrix inversion and solving 1 linear system if implemented correctly. Although the analytic center is a more popular choice for \u201cgeometrical\u201d algorithms, we choose volumetric center here because it gives a better convergence rate Vaidya [1996], Atkinson and Vaidya [1995] and the extra cost \u03c8(k) is negligible to the cost of updating QR decomposition nk."}, {"heading": "5 Experiments", "text": "In this section, we compare the geometric politician against two libraries for first order methods, minFunc Schmidt [2012] and TFOCS Becker et al. [2011]. Both are popular MATLAB libraries for minimizing general smooth convex functions. Since the focus of this paper is all about how to find a good step direction using a politician, we use the exact line search (up to machine accuracy) whenever possible. This eliminates the effect of different line searches and reduces the number of algorithms we need to test. TFOCS is the only algorithm we use which does not use line search because they do not provide such option. To compensate on the unfairness to TFOCS, we note that the algorithm TFOCS uses is accelerated gradient descent and hence we implement the GonzagaKaras\u2019s accelerated gradient descent Gonzaga and Karas [2013], which is specifically designed to be used with exact line search. Another reason we pick this variant of accelerated gradient descent is because we found it to be the fastest variant of accelerated gradient descent (excluding the geometric descent of Bubeck et al. [2015]) for our tested data (Gonzaga and Karas also observed that on their own dataset).\nThe algorithms to be tested are the following:\n\u2022 [SD] Steepest descent algorithm in minFunc.\n\u2022 [Nes] Accelerated gradient descent, General Scheme 2.2.6 in Nesterov [2004].\n\u2022 [TFOCS] Accelerated gradient descent in TFOCS.\n\u2022 [GK] Gonzaga-Karas\u2019s of Accelerated Gradient Descent (Sec 5.1).\n\u2022 [Geo] Geometric Descent Bubeck et al. [2015].\n\u2022 [CG] Non-Linear Conjugate Gradient in minFunc.\n\u2022 [BFGS] Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm in minFunc.\n\u2022 [PCG] Preconditioned Non-Linear Conjugate Gradient in minFunc.\n\u2022 [\u2205+] Geometric Politician itself (Sec 5.1).\n\u2022 [GK+] Using GK with Geometric Oracle (Sec 5.1).\n\u2022 [BFGS+] Using BFGS with Geometric Oracle (Sec 5.1).\nWe only tested the geometric oracle on GK and BFGS because they are respectively the best algorithms in theory and practice on our tested data. The \u2205+ algorithm is used as the control group to test if the geometric politician by itself is sufficient to achieve good convergence rate. We note that all algorithms except Nes are parameter free; each step of SD, Nes, TFOCS, GK, Geo, CG takes O(n) time and each step of BFGS, PCG, \u2205+, GK+ and BFGS+ takes roughly O(nk) time for kth iteration."}, {"heading": "5.1 Details of Implementations", "text": "The first algorithm we implement is the \u2205+ algorithm which simply repeatedly call the politician. As we will see, this algorithm is great for non smooth problems but not competitive for smooth problems.\nAlgorithm 2: \u2205+ Input: x0. for k \u2190 1, 2, \u00b7 \u00b7 \u00b7 do\nSet xk+1 \u2190 \u03a6f (xk, (xi, f(xi),\u2207f(xi))i\u2208[k]). end\nThe second algorithm we implement is the accelerated gradient descent proposed by Gonzaga and Karas Gonzaga and Karas [2013]. This algorithm uses line search to learn the the smoothness parameter and strong convexity parameter, see Algorithm 3. We disable the line (*) in the algorithm if \u03a6f is a politician instead of an oracle because \u03b3 \u2265 \u03b1 does not hold for the strong convexity parameter \u03b1 if \u03a6f is not an oracle.\nThe third algorithm we implemented is the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) algorithm. This algorithm uses the gradients to reconstruct the Hessian and use it to approximate Newton\u2019s method, see Algorithm 4. We note that another natural way to employ the politician with BFGS is to set xk+1 = line search(\u03a6f (xk), p) and this runs faster in practice; however, this algorithm computes two gradients per iteration (namely \u2207f(xk) and \u2207f(\u03a6f (xk))) while we restrict ourselves to algorithms which compute one gradient per iteration."}, {"heading": "5.2 Quadratic function", "text": "We consider the function\nf(x) = (x\u2212 c)>D(x\u2212 c), (2)\nwhere D is a diagonal matrix with entries uniformly sampled from [0, 1] and c is a random vector with entries uniformly sampled from the normal distribution N(0, 1). Since this is a quadratic function, CG, BFGS and BFGS+ are equivalent and optimal, namely, they output the minimum point in the span of all previous gradients.\nAlgorithm 3: Gonzaga-Karas\u2019s variant of Accelerated Gradient Descent Input: x1. \u03b3 = 2\u03b1, v0 = x0 and y0 = x0. for k \u2190 1, 2, \u00b7 \u00b7 \u00b7 do\nyk \u2190 \u03a6f (yk\u22121). xk+1 = line search(yk,\u2212\u2207f(yk)). if \u03b1 \u2265 \u03b3/1.02 and we are using first order oracle then \u03b1 = \u03b3/2. (*) if \u03b1 \u2265 \u2016\u2207f(yk)\u2016 2\n2(f(yk)\u2212f(xk+1)) then \u03b1 = \u2016\u2207f(yk)\u2016 2 20(f(yk)\u2212f(xk+1)) . G = \u03b3 ( \u03b1 2 \u2016vk \u2212 yk\u20162 + \u3008\u2207f(yk), vk \u2212 yk\u3009 ) . A = G+ 1 2 \u2016\u2207f(yk)\u20162 + (\u03b1\u2212 \u03b3)(f(xk)\u2212 f(yk)). B = (\u03b1\u2212 \u03b3)(f(xk+1)\u2212 f(xk))\u2212 \u03b3(f(yk)\u2212 f(xk))\u2212G. C = \u03b3(f(xk+1)\u2212 f(xk)). \u03b2 = \u2212B+ \u221a B2\u22124AC 2A\n, \u03b3 = (1\u2212 \u03b2)\u03b3 + \u03b2\u03b1. vk+1 = 1 \u03b3 ((1\u2212 \u03b2)\u03b3vk + \u03b2(\u03b1yk \u2212\u2207f(yk)).\nend"}, {"heading": "5.3 Variant of Nesterov\u2019s Worst Function", "text": "Nesterov [2004] introduced the function\nf(x) = (1\u2212 x[1])2 + n\u22121\u2211 k=1 (x[k]\u2212 x[k + 1])2\nand used it to give a lower bound for all first-order methods. To distinguish the performance between CG, BFGS and BFGS+, we consider the following non-quadratic variant\nf(x) = g(1\u2212 x[1]) + n\u22121\u2211 k=1 g(x[k]\u2212 x[k + 1]) (3)\nfor some function g to be defined. If we pick g(x) = |x| then all first order methods takes at least n iterations to minimize f exactly. On the other hand with g(x) = max(|x| \u2212 0.1, 0) one of the minimizer of f is (1, 9\n10 , 8 10 , \u00b7 \u00b7 \u00b7 , 1 10 , 0, 0, \u00b7 \u00b7 \u00b7 , 0), and thus it takes at least 11 iterations for\nfirst order methods to minimize f in this case. We \u201cregularize\u201d the situation a bit and consider the function\ng(x) =  \u221a (x\u2212 0.1)2 + 0.0012 \u2212 0.001 if x \u2265 0.1\u221a (x+ 0.1)2 + 0.0012 \u2212 0.001 if x \u2264 \u22120.1\n0 otherwise\n.\nSince this function is far from quadratic, our algorithms (\u2205+, GK+, BFGS+) converge much faster. This is thus a nice example where the geometric politician helps a lot because the underlying dimension of the problem is small.\nAlgorithm 4: BFGS Input: x1. for k \u2190 1, 2, \u00b7 \u00b7 \u00b7 do\np = \u2212\u2207f(xk). for i\u2190 k \u2212 1, \u00b7 \u00b7 \u00b7 , 1 do\n\u03b1i \u2190 \u3008si, p\u3009 / \u3008si, yi\u3009. p = p\u2212 \u03b1iyi.\nend p = \u3008sk\u22121, yk\u22121\u3009 / \u3008yk\u22121, yk\u22121\u3009 p. for i\u2190 1, \u00b7 \u00b7 \u00b7 , k \u2212 1 do\n\u03b2i \u2190 \u3008yi, p\u3009 / \u3008si, yi\u3009. p = p+ (\u03b1i \u2212 \u03b2i)yi.\nend xk+1 = \u03a6f (line search(xk, p)). sk = xk+1 \u2212 xk, yk = \u2207f(xk+1)\u2212\u2207f(xk).\nend"}, {"heading": "5.4 Binary regression with smoothed hinge loss", "text": "We consider the binary classification problem on the datasets from Chang and Lin [2011]. The problem is to minimize the regularized empirical risk:\nft(x) = 1\nn n\u2211 i=1 \u03d5t(bia T i x) + \u03bb 2 |x|2 (4)\nwhere ai \u2208 Rd, bi \u2208 R are given by the datasets, \u03bb is the regularization coefficient, \u03d5t is the smoothed hinge loss defined by\n\u03d5t(z) =  0 if z \u2264 \u22121 z + 1\u2212 t 2 if z \u2265 \u22121 + t\n1 2t (z + 1)2 otherwise\nand t is the smoothness parameter. The usual choice for t is 1, here we test both t = 1 and t = 10\u22124. The latter case is to test how well the algorithms perform when the function is non-smooth.\nWe note that for this problem it would be natural to compare ourselves with SGD (stochastic gradient descent) or more refined stochastic algorithms such as SAG Le Roux et al. [2012] or SVRG Johnson and Zhang [2013]. However since the focus of this paper is on general black-box optimization we stick to comparing only to general methods. It is an interesting open problem to extend our algorithms to the stochastic setting, see Section 6.\nIn figures 3 and 4, we show the performance profile for problems in the LIBSVM datasets (and with different values for the regularization parameter \u03bb). More precisely for a given algorithm we plot x \u2208 [1, 10] versus the fraction of datasets that the algorithm can solve (up to a certain prespecified accuracy) in a number of iterations which is at most x times the number of iterations of the best algorithm for this dataset. Figure 3 shows the case t = 1 with the targeted accuracy 10\u22126; Figure 4 shows the case t = 10\u22124 with the targeted accuracy 10\u22123. We see that TFOCS is\nslower than SD for many problems, this is simply because SD uses the line search while TFOCS does not, and this makes a huge difference for simple problems. Among algorithms taking O(n) time per iteration, CG and Geo perform the best, while for theO(nk) algorithms we see that BFGS, BFGS+ and GK+ perform the best. The gap in performance is particularly striking in the nonsmooth case where BFGS+ is the fastest algorithm on almost all problems and all other methods (except GK+) are lagging far behind (for 20% of the problems all other methods take 10 times more iterations than BFGS+ and GK+).\nFinally in figures 5 and 6 we test five algorithms on three specific datasets (respectively in the smooth and non-smooth case). In both figures we see that BFGS+ performs the best for all three datasets. BFGS performs second for smooth problems while GK+ performs second for nonsmooth problems."}, {"heading": "5.5 Summary", "text": "The experiments show that BFGS+ and BFGS perform the best among all methods for smooth test problems while BFGS+ and GK+ perform the best for nonsmooth test problems. The first phenomenon is due to the optimality of these algorithm for quadratic problems. We leave the explanation for the second phenomenon as an open problem. At least, the experiments show that this is not due to the geometric oracle itself since \u2205+ is much slower, and this is not due to the original algorithm since GK performs much worse than GK+ for those problems. Overall these experiments are very promising for the geometric oracle as a replacement of quasi Newton method for non-smooth problems and as a general purpose solver due to its robustness."}, {"heading": "6 Discussion", "text": "First order methods generally involve only very basic operations at each step (addition, scalar multiplication). In this paper we formalize each step\u2019s operations (besides the gradient calculation) as the work of the politician. We showed that the cost per step of an affine invariant politician \u03c8(k) is negligible compared to the gradient calculation (which is \u2126(n)). This opens up a lot of possibilities: instead of basic addition or scalar multiplication one can imagine computing a center of gravity, solving a linear program, or even searching over an exponential space (indeed, say k < 30 and n > 1010, then 2k < n). Our experiments demonstrate the effectiveness of this strategy. On the other hand from a theoretical point of view a lot remains to be done. For example one can prove results of the following flavor:\nTheorem 1 Let f such that \u03b1I \u22072f(x) \u03b2I,\u2200x \u2208 Rn and let \u03ba = \u03b2/\u03b1. Suppose that in the Geometric Politician we replace the volumetric center by the center of gravity or the center of the John ellipsoid. Let yk be the output of the kth step of SD+ with some initial point x0. Then, we have that\nf(yk)\u2212 f(x\u2217) \u2264 \u03ba (\n1\u2212 1 \u0398(min(n log(\u03ba), \u03ba))\n)k (f(x0)\u2212 f(x\u2217)) .\nand\nf(yk)\u2212 f(x\u2217) \u2264 2\u03b2R2\nk + 4\nwhere R = maxf(x)\u2264f(x0) \u2016x\u2212 x\u2217\u2016.\nThis claim says that, up to a logarithmic factor, SD+ enjoys simultaneously the incremental progress of gradient descent and the geometrical progress of cutting plane methods. There are three caveats in this claim:\n\u2022 We use the center of gravity or the center of the John ellipsoid instead of the volumetric center. Note however that it is well-known that the volumetric center is usually more difficult to analyze, Vaidya [1996], Atkinson and Vaidya [1995].\n\u2022 The extraneous log(\u03ba) comes from the number of potential restart when we decrease \u03b1. Is there a better way to learn \u03b1 that would not incur this additional logarithmic term?\n\u2022 Bubeck et al. [2015] shows essentially that one can combine the ellipsoid method with gradient descent to achieve the optimal 1\u2212 \u221a 1/\u03ba rate. Can we prove such a result for SD+?\nThe geometric politician could be refined in many ways. Here are two simple questions that we leave for future work:\n\u2022 One can think that gradient descent stores 1 gradient information, accelerated gradient descent stores 2 gradient information, and our method stores all past gradient information. We believe that neither 1, 2 nor all is the correct answer. Instead, the algorithm should dynamically decide the number of gradients to store based on the size of its memory, the cost of computing gradients, and the information each gradient reveals.\n\u2022 Is there a stochastic version of our algorithm? How well would such a method compare with state of the art stochastic algorithms such as SAG Le Roux et al. [2012] and SVRG Johnson and Zhang [2013]?\nA Convergence of SD+ Let f such that \u03b1I \u22072f(x) \u03b2I for all x \u2208 Rn and let \u03ba = \u03b2/\u03b1. Let yk be the output of the kth step of SD+ (where the volumetric center is replaced by the center of gravity or the center of the John ellipsoid) with some initial point x0. We prove two rates of convergence for SD+, one with the condition number \u03ba, and one with the ambient dimension n. We start by the former.\nTheorem 2 One has\nf(yk)\u2212 f(x\u2217) \u2264 (\n1\u2212 1 \u03ba\n)k (f(x0)\u2212 f(x\u2217))\nand\nf(yk)\u2212 f(x\u2217) \u2264 2\u03b2r2\nk + 4\nwhere r = maxf(x)\u2264f(x0) \u2016x\u2212 x\u2217\u2016.\nProof Let \u03b4k = f(yk)\u2212 f(x\u2217). Since f is \u03b1-strongly convex we have that\n\u03b4k \u2264 1\n2\u03b1 \u2016\u2207f(yk)\u20162.\nDue to the decrease guarantee of politicians and the line search in steepest descent, we have that f(yk+1) \u2264 f(xk+1) \u2264 f(yk)\u2212 12\u03b2\u2016\u2207f(yk)\u2016 2 and hence\n\u03b4k \u2212 \u03b4k+1 \u2265 1\n2\u03b2 \u2016\u2207f(yk)\u20162 \u2265 \u03b4k \u03ba . (5)\nHence, we have \u03b4k+1 \u2264 ( 1\u2212 1\n\u03ba\n) \u03b4k and this gives the first inequality.\nTo obtain a rate independent of \u03b1 we instead use the following estimate\n\u03b4k \u2264 \u3008\u2207f(yk), yk \u2212 x\u2217\u3009 \u2264 \u2016\u2207f(yk)\u2016 \u00b7 \u2016yk \u2212 x\u2217\u2016.\nUsing the decrease guarantee of politicians and line search we have that f(yk) \u2264 f(xk) \u2264 f(yk\u22121) \u2264 \u00b7 \u00b7 \u00b7 \u2264 f(x0), and thus by definition of R:\n\u2016yk \u2212 x\u2217\u2016 \u2264 r.\nDue to the line search in steepest descent again, we have that\n\u03b4k \u2212 \u03b4k+1 \u2265 1\n2\u03b2 \u2016\u2207f(yk)\u20162 \u2265\n1\n2\u03b2 ( \u03b4k r )2 .\nSince \u03b4k \u2265 \u03b4k+1, we have\n1 \u03b4k+1 \u2212 1 \u03b4k = \u03b4k \u2212 \u03b4k+1 \u03b4k\u03b4k+1 \u2265 \u03b4k \u2212 \u03b4k+1 \u03b42k \u2265 1 2\u03b2r2 .\nSo, by induction, we have that 1 \u03b4k \u2265 1 \u03b40 + k 2\u03b2r2 . Now, we note that\n\u03b40 \u2264 \u3008\u2207f(x\u2217), x0 \u2212 x\u2217\u3009+ \u03b2\n2 \u2016x0 \u2212 x\u2217\u20162 \u2264\n\u03b2r2\n2 .\nThus, we have that\n\u03b4k \u2264 2\u03b2r2\nk + 4 .\nWe now turn to the dimension dependent analysis of SD+. We first show a simple geometric result, namely that if an intersection of spheres has a \u201csmall\u201d volume then the intersection must lie close close to the boundary of one of the spheres.\nLemma 1 Let R = \u2229ki=1{x \u2208 Rn : \u2016x\u2212 ci\u2016 \u2264 ri}, D = maxi\u2208[k] ri, and \u03c9n the volume of the unit ball in Rn. Then, there exists i \u2208 [k] such that for all x \u2208 R,\n\u2016x\u2212 ci\u20162 \u2265 r2i \u2212 24k2 ( volR\nDn\u03c9n\n)1/n D2.\nProof Since \u2212 log(1 \u2212 \u2016x\u20162) is a 1-self concordant barrier function, 2FR is a k-self concordant function. Let y be the minimizer of FR. Let E = {x \u2208 Rn : x>\u22072(2FR)(y)x \u2264 1}. Theorem 4.2.6 in Nesterov [2004] shows that\ny + E \u2282 R \u2282 y + (k + 2 \u221a k)E. (6)\nIn particular, this shows that volE \u2264 volR. We have that( det\u22072FR(y) )1/2 =\n\u03c9n 2n/2 1 volE \u2265 \u03c9n 2n/2 1 volR .\nBy the AM-GM inequality, we have that\ntr\u22072FR(y) n \u2265 1 2 ( \u03c9n volR )2/n . (7)\nBy Proposition 1, we have that\u22072FR(y) = 2A>A+ \u03bb(1)I and hence,\ntr\u22072FR(y) = 2trA>A+ n\u03bb(1)\n= 2 k\u2211 i=1\n\u2016y \u2212 ci\u20162\n(r2i \u2212 \u2016y \u2212 ci\u20162)2 + n k\u2211 i=1\n1\nr2i \u2212 \u2016y \u2212 ci\u20162 .\nApplying (7), we have that\n2\nn k\u2211 i=1\n\u2016y \u2212 ci\u20162\n(r2i \u2212 \u2016y \u2212 ci\u20162)2 + k\u2211 i=1\n1 r2i \u2212 \u2016y \u2212 ci\u20162 \u2265 1 2 ( \u03c9n volR )2/n So, there exists i such that\n\u2016y \u2212 ci\u20162\n(r2i \u2212 \u2016y \u2212 ci\u20162)2 \u2265 n 8k ( \u03c9n volR )2/n or\n1 r2i \u2212 \u2016y \u2212 ci\u20162 \u2265 1 4k ( \u03c9n volR )2/n .\nUsing \u2016y \u2212 ci\u2016 \u2264 ri \u2264 D and volR \u2264 Dn\u03c9n, we have that\nr2i \u2212 \u2016y \u2212 ci\u20162 \u2264 max\n(\u221a 8k\nn\n( volR\n\u03c9n\n)1/n \u2016y \u2212 ci\u2016, 4k ( volR\n\u03c9n\n)2/n)\n\u2264 4k ( volR\nDn\u03c9n\n)1/n D2.\nTherefore, the width of the ellipsoid E in the direction y \u2212 ci is at most\nri \u2212 \u2016y \u2212 ci\u2016 \u2264 ri \u2212 \u221a r2i \u2212 4k ( volR\nDn\u03c9n\n)1/n D2.\nThe right hand side of (6) shows that, for all x \u2208 R, we have\n\u2016x\u2212 ci\u2016 \u2265 ri \u2212 (1 + k + 2 \u221a k) ri \u2212 \u221a r2i \u2212 4k ( volR\nDn\u03c9n\n)1/n D2  \u2265 ri \u2212 (1 + k + 2 \u221a k) ( ri \u2212 ri ( 1\u2212 4k ( volR\nDn\u03c9n\n)1/n D2\nr2i\n))\n\u2265 [ 1\u2212 12k2 ( volR\nDn\u03c9n\n)1/n D2\nr2i\n] ri.\nHence, we have\n\u2016x\u2212 ci\u20162 \u2265 [ 1\u2212 24k2 ( volR\nDn\u03c9n\n)1/n D2\nr2i\n] r2i .\nFinally, equipped with the above geometrical result, we can bound the convergence of SD+ using the dimension n. We start with a lemma taking care of the adaptivity to the strong convexity in the geometric politician.\nLemma 2 In the first k = \u0398(n log(\u03ban \u03b5 )) iterations, either SD+ restarts the estimate of the strong convexity or\nf(yk)\u2212 f(x\u2217) \u2264 \u03b5 (f(x0)\u2212 f(x\u2217)) .\nProof The decrease guarantee and the smoothness imply that\n\u2016\u2207f(yk)\u20162\n2\u03b2 \u2264 f(yk)\u2212 f(x\u2217) \u2264 f(x0)\u2212 f(x\u2217).\nTherefore, all the spheres found by the geometric politician have radius squared at most D2 where, denoting \u03b1 for the convexity upper bound the algorithm is currently using,\nD2 = max k\u22651\n\u2016\u2207f(yk)\u20162 \u03b12 \u2264 2\u03b2(f(x0)\u2212 f(x \u2217)) \u03b12 .\nLemma 1 shows that for any step k, there is i \u2208 [k] such that for all x \u2208 Rk,\n\u2016x\u2212 ci\u20162 \u2265 r2i \u2212 48\u03b2k2\n\u03b12 ( volRk Dn\u03c9n )1/n (f(x0)\u2212 f(x\u2217)).\nLet k = \u0398(n log(\u03ban \u03b5 )) and recall the discussion in Section 3 about the volume decrease of the geometric politician with the center of gravity (the same discussion applies to the John ellipsoid). We see that if the algorithm does not restart \u03b1 within the first k iterations then we have\nvolRk Dn\u03c9n = ( O ( \u03b5 \u03ba2k2 ))n ,\nand hence (for an appropriate numerical constant in k)\n\u2016x\u2212 ci\u20162 \u2265 r2i \u2212 \u03b5(f(x0)\u2212 f(x\u2217))\n\u03b1\u03ba . (8)\nRecall from (5) that\nf(yk+1) \u2264 f(yk)\u2212 f(yk)\u2212 f(x\u2217)\n\u03ba ,\nand therefore we have (by the improvement of the previous balls): Rk+1 \u2282 { \u2016x\u2212 ci\u20162 \u2264 r2i \u2212\n2(f(yk)\u2212 f(x\u2217)) \u03b1\u03ba\n} \u2229Rk.\nHowever, from (8), we know that either the above intersection is empty or f(yk) \u2212 f(x\u2217) < \u03b5(f(x0)\u2212 f(x\u2217)). This proves the statement.\nTheorem 3 We have that f(yk)\u2212 f(x\u2217) \u2264 \u03ba (\n1\u2212 1 \u0398(n log(\u03ba))\n)k (f(x0)\u2212 f(x\u2217)) .\nProof If \u03ba < n, the statements follows from Theorem 2. Hence, we can assume \u03ba \u2265 n. Set T = \u0398(n log(n\u03ba\n\u03b5 ) log(\u03ba)), Lemma 2 shows that for every \u0398(n log(n\u03ba \u03b5 )) iteration, the algo-\nrithm either finds y such that\nf(y)\u2212 f(x\u2217) \u2264 \u03b5 (f(x0)\u2212 f(x\u2217))\nor decreases \u03b1k by a constant where \u03b1k is the convexity upper bound the algorithm is using at kth iteration. Note that \u03b11 \u2264 \u03b2 because of the line search, and thus the algorithm can restart \u03b1k at most log(\u03ba) many times. Hence, after T iterations, we must have\nf(yT )\u2212 f(x\u2217) \u2264 \u03b5 (f(x0)\u2212 f(x\u2217)) ,\nthus concluding the proof."}], "references": [{"title": "The volumetric barrier for convex quadratic constraints", "author": ["M.K. Anstreicher"], "venue": "Mathematical Programming,", "citeRegEx": "Anstreicher.,? \\Q2004\\E", "shortCiteRegEx": "Anstreicher.", "year": 2004}, {"title": "A cutting plane algorithm for convex programming that uses analytic centers", "author": ["David S Atkinson", "Pravin M Vaidya"], "venue": "Mathematical Programming,", "citeRegEx": "Atkinson and Vaidya.,? \\Q1995\\E", "shortCiteRegEx": "Atkinson and Vaidya.", "year": 1995}, {"title": "A cutting plane method from analytic centers for stochastic programming", "author": ["Olivier Bahn", "O Du Merle", "J-L Goffin", "J-P Vial"], "venue": "Mathematical Programming,", "citeRegEx": "Bahn et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Bahn et al\\.", "year": 1995}, {"title": "Templates for convex cone problems with applications to sparse signal recovery", "author": ["Stephen R Becker", "Emmanuel J Cand\u00e8s", "Michael C Grant"], "venue": "Mathematical programming computation,", "citeRegEx": "Becker et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2011}, {"title": "Convex optimization: Algorithms and complexity", "author": ["S. Bubeck"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bubeck.,? \\Q2015\\E", "shortCiteRegEx": "Bubeck.", "year": 2015}, {"title": "A geometric alternative to nesterov\u2019s accelerated gradient descent", "author": ["S. Bubeck", "Y.-T. Lee", "M. Singh"], "venue": "Arxiv preprint arXiv:1506.08187,", "citeRegEx": "Bubeck et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2015}, {"title": "Libsvm: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST).,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "Shallow, deep and very deep cuts in the analytic center cutting plane method", "author": ["Jean-Louis Goffin", "Jean-Philippe Vial"], "venue": "Mathematical Programming,", "citeRegEx": "Goffin and Vial.,? \\Q1999\\E", "shortCiteRegEx": "Goffin and Vial.", "year": 1999}, {"title": "Fine tuning nesterovs steepest descent algorithm for differentiable convex programming", "author": ["Cl\u00f3vis C Gonzaga", "Elizabeth W Karas"], "venue": "Mathematical Programming,", "citeRegEx": "Gonzaga and Karas.,? \\Q2013\\E", "shortCiteRegEx": "Gonzaga and Karas.", "year": 2013}, {"title": "Partitions of mass-distributions and of convex bodies by hyperplanes", "author": ["B. Gr\u00fcnbaum"], "venue": "Pacific J. Math,", "citeRegEx": "Gr\u00fcnbaum.,? \\Q1960\\E", "shortCiteRegEx": "Gr\u00fcnbaum.", "year": 1960}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["R. Johnson", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Johnson and Zhang.,? \\Q2013\\E", "shortCiteRegEx": "Johnson and Zhang.", "year": 2013}, {"title": "A stochastic gradient method with an exponential convergence rate for strongly-convex optimization with finite training sets", "author": ["N. Le Roux", "M. Schmidt", "F. Bach"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Roux et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2012}, {"title": "A faster cutting plane method and its implications for combinatorial and convex optimization", "author": ["Y.-T. Lee", "A. Sidford", "S.C.-W Wong"], "venue": "Arxiv preprint arXiv:1508.04874,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A. Nemirovski", "D. Yudin"], "venue": "Wiley Interscience,", "citeRegEx": "Nemirovski and Yudin.,? \\Q1983\\E", "shortCiteRegEx": "Nemirovski and Yudin.", "year": 1983}, {"title": "Introductory lectures on convex optimization: A basic course", "author": ["Y. Nesterov"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Nesterov.,? \\Q2004\\E", "shortCiteRegEx": "Nesterov.", "year": 2004}, {"title": "minfunc: unconstrained differentiable multivariate optimization in matlab", "author": ["M Schmidt"], "venue": "URL http://www. di. ens. fr/mschmidt/Software/minFunc. html,", "citeRegEx": "Schmidt.,? \\Q2012\\E", "shortCiteRegEx": "Schmidt.", "year": 2012}, {"title": "A new algorithm for minimizing convex functions over convex sets", "author": ["P.M. Vaidya"], "venue": "Mathematical programming,", "citeRegEx": "Vaidya.,? \\Q1996\\E", "shortCiteRegEx": "Vaidya.", "year": 1996}], "referenceMentions": [{"referenceID": 12, "context": "In standard black-box convex optimization Nemirovski and Yudin [1983], Nesterov [2004], Bubeck [2015] first-order methods interact with an oracle: given a query point x, the oracle reports the value and gradient of the underlying objective function f at x.", "startOffset": 42, "endOffset": 70}, {"referenceID": 12, "context": "In standard black-box convex optimization Nemirovski and Yudin [1983], Nesterov [2004], Bubeck [2015] first-order methods interact with an oracle: given a query point x, the oracle reports the value and gradient of the underlying objective function f at x.", "startOffset": 42, "endOffset": 87}, {"referenceID": 4, "context": "In standard black-box convex optimization Nemirovski and Yudin [1983], Nesterov [2004], Bubeck [2015] first-order methods interact with an oracle: given a query point x, the oracle reports the value and gradient of the underlying objective function f at x.", "startOffset": 88, "endOffset": 102}, {"referenceID": 4, "context": "We do so by introducing the geometric politician (Section 3), which is based on a combination of the recent ideas of Bubeck et al. [2015] with standard cutting plane/interior point methods machinery (through the notion of a \u201ccenter\u201d of a set, see Section 4).", "startOffset": 117, "endOffset": 138}, {"referenceID": 4, "context": "We describe in this section the geometric politician which is based on ideas developed in Bubeck et al. [2015]. A key observation in the latter paper is that if f is a \u03b1-strongly convex function minimized at x\u2217 then one has for any x, \u2225\u2225\u2225\u2225x\u2217 \u2212 x\u2212 1 \u03b1 \u2225\u2225\u2225\u22252 \u2264 \u2016\u2207f(x)\u2016 \u03b12 \u2212 2 \u03b1 (f(x)\u2212 f(x\u2217)) .", "startOffset": 90, "endOffset": 111}, {"referenceID": 9, "context": "In particular if the next query point yk+1 is the center of gravity of Rk then we have that the volume of Rk+1 is at most 1 \u2212 1/e times the volume of Rk (see Gr\u00fcnbaum [1960]), thus leading to an exponential convergence rate.", "startOffset": 158, "endOffset": 174}, {"referenceID": 14, "context": "The volumetric barrier for a polytope was introduced in Vaidya [1996] to construct an algorithm with both the oracle complexity of the center of gravity method and the computational complexity of the ellipsoid method (see [Section 2.", "startOffset": 56, "endOffset": 70}, {"referenceID": 4, "context": "3, Bubeck [2015]] for more details and Lee et al.", "startOffset": 3, "endOffset": 17}, {"referenceID": 4, "context": "3, Bubeck [2015]] for more details and Lee et al. [2015] for", "startOffset": 3, "endOffset": 57}, {"referenceID": 0, "context": "It is shown in Anstreicher [2004] that vR is a self-concordant barrier which means that the center can be updated (when a new ball is added to R) via few iterations of Newton\u2019s method.", "startOffset": 15, "endOffset": 34}, {"referenceID": 0, "context": "It is shown in Anstreicher [2004] that vR is a self-concordant barrier which means that the center can be updated (when a new ball is added to R) via few iterations of Newton\u2019s method. Often in practice, it takes less than 5 iterations to update the minimizer of a self-concordant barrier Goffin and Vial [1999], Bahn et al.", "startOffset": 15, "endOffset": 312}, {"referenceID": 0, "context": "It is shown in Anstreicher [2004] that vR is a self-concordant barrier which means that the center can be updated (when a new ball is added to R) via few iterations of Newton\u2019s method. Often in practice, it takes less than 5 iterations to update the minimizer of a self-concordant barrier Goffin and Vial [1999], Bahn et al. [1995] when we add a new constraint.", "startOffset": 15, "endOffset": 332}, {"referenceID": 15, "context": "Although the analytic center is a more popular choice for \u201cgeometrical\u201d algorithms, we choose volumetric center here because it gives a better convergence rate Vaidya [1996], Atkinson and Vaidya [1995] and the extra cost \u03c8(k) is negligible to the cost of updating QR decomposition nk.", "startOffset": 160, "endOffset": 174}, {"referenceID": 1, "context": "Although the analytic center is a more popular choice for \u201cgeometrical\u201d algorithms, we choose volumetric center here because it gives a better convergence rate Vaidya [1996], Atkinson and Vaidya [1995] and the extra cost \u03c8(k) is negligible to the cost of updating QR decomposition nk.", "startOffset": 175, "endOffset": 202}, {"referenceID": 10, "context": "In this section, we compare the geometric politician against two libraries for first order methods, minFunc Schmidt [2012] and TFOCS Becker et al.", "startOffset": 108, "endOffset": 123}, {"referenceID": 3, "context": "In this section, we compare the geometric politician against two libraries for first order methods, minFunc Schmidt [2012] and TFOCS Becker et al. [2011]. Both are popular MATLAB libraries for minimizing general smooth convex functions.", "startOffset": 133, "endOffset": 154}, {"referenceID": 3, "context": "In this section, we compare the geometric politician against two libraries for first order methods, minFunc Schmidt [2012] and TFOCS Becker et al. [2011]. Both are popular MATLAB libraries for minimizing general smooth convex functions. Since the focus of this paper is all about how to find a good step direction using a politician, we use the exact line search (up to machine accuracy) whenever possible. This eliminates the effect of different line searches and reduces the number of algorithms we need to test. TFOCS is the only algorithm we use which does not use line search because they do not provide such option. To compensate on the unfairness to TFOCS, we note that the algorithm TFOCS uses is accelerated gradient descent and hence we implement the GonzagaKaras\u2019s accelerated gradient descent Gonzaga and Karas [2013], which is specifically designed to be used with exact line search.", "startOffset": 133, "endOffset": 830}, {"referenceID": 3, "context": "In this section, we compare the geometric politician against two libraries for first order methods, minFunc Schmidt [2012] and TFOCS Becker et al. [2011]. Both are popular MATLAB libraries for minimizing general smooth convex functions. Since the focus of this paper is all about how to find a good step direction using a politician, we use the exact line search (up to machine accuracy) whenever possible. This eliminates the effect of different line searches and reduces the number of algorithms we need to test. TFOCS is the only algorithm we use which does not use line search because they do not provide such option. To compensate on the unfairness to TFOCS, we note that the algorithm TFOCS uses is accelerated gradient descent and hence we implement the GonzagaKaras\u2019s accelerated gradient descent Gonzaga and Karas [2013], which is specifically designed to be used with exact line search. Another reason we pick this variant of accelerated gradient descent is because we found it to be the fastest variant of accelerated gradient descent (excluding the geometric descent of Bubeck et al. [2015]) for our tested data (Gonzaga and Karas also observed that on their own dataset).", "startOffset": 133, "endOffset": 1103}, {"referenceID": 3, "context": "In this section, we compare the geometric politician against two libraries for first order methods, minFunc Schmidt [2012] and TFOCS Becker et al. [2011]. Both are popular MATLAB libraries for minimizing general smooth convex functions. Since the focus of this paper is all about how to find a good step direction using a politician, we use the exact line search (up to machine accuracy) whenever possible. This eliminates the effect of different line searches and reduces the number of algorithms we need to test. TFOCS is the only algorithm we use which does not use line search because they do not provide such option. To compensate on the unfairness to TFOCS, we note that the algorithm TFOCS uses is accelerated gradient descent and hence we implement the GonzagaKaras\u2019s accelerated gradient descent Gonzaga and Karas [2013], which is specifically designed to be used with exact line search. Another reason we pick this variant of accelerated gradient descent is because we found it to be the fastest variant of accelerated gradient descent (excluding the geometric descent of Bubeck et al. [2015]) for our tested data (Gonzaga and Karas also observed that on their own dataset). The algorithms to be tested are the following: \u2022 [SD] Steepest descent algorithm in minFunc. \u2022 [Nes] Accelerated gradient descent, General Scheme 2.2.6 in Nesterov [2004]. \u2022 [TFOCS] Accelerated gradient descent in TFOCS.", "startOffset": 133, "endOffset": 1356}, {"referenceID": 3, "context": "In this section, we compare the geometric politician against two libraries for first order methods, minFunc Schmidt [2012] and TFOCS Becker et al. [2011]. Both are popular MATLAB libraries for minimizing general smooth convex functions. Since the focus of this paper is all about how to find a good step direction using a politician, we use the exact line search (up to machine accuracy) whenever possible. This eliminates the effect of different line searches and reduces the number of algorithms we need to test. TFOCS is the only algorithm we use which does not use line search because they do not provide such option. To compensate on the unfairness to TFOCS, we note that the algorithm TFOCS uses is accelerated gradient descent and hence we implement the GonzagaKaras\u2019s accelerated gradient descent Gonzaga and Karas [2013], which is specifically designed to be used with exact line search. Another reason we pick this variant of accelerated gradient descent is because we found it to be the fastest variant of accelerated gradient descent (excluding the geometric descent of Bubeck et al. [2015]) for our tested data (Gonzaga and Karas also observed that on their own dataset). The algorithms to be tested are the following: \u2022 [SD] Steepest descent algorithm in minFunc. \u2022 [Nes] Accelerated gradient descent, General Scheme 2.2.6 in Nesterov [2004]. \u2022 [TFOCS] Accelerated gradient descent in TFOCS. \u2022 [GK] Gonzaga-Karas\u2019s of Accelerated Gradient Descent (Sec 5.1). \u2022 [Geo] Geometric Descent Bubeck et al. [2015]. \u2022 [CG] Non-Linear Conjugate Gradient in minFunc.", "startOffset": 133, "endOffset": 1519}, {"referenceID": 8, "context": "The second algorithm we implement is the accelerated gradient descent proposed by Gonzaga and Karas Gonzaga and Karas [2013]. This algorithm uses line search to learn the the smoothness parameter and strong convexity parameter, see Algorithm 3.", "startOffset": 82, "endOffset": 125}, {"referenceID": 14, "context": "3 Variant of Nesterov\u2019s Worst Function Nesterov [2004] introduced the function", "startOffset": 13, "endOffset": 55}, {"referenceID": 6, "context": "4 Binary regression with smoothed hinge loss We consider the binary classification problem on the datasets from Chang and Lin [2011]. The problem is to minimize the regularized empirical risk:", "startOffset": 112, "endOffset": 133}, {"referenceID": 10, "context": "We note that for this problem it would be natural to compare ourselves with SGD (stochastic gradient descent) or more refined stochastic algorithms such as SAG Le Roux et al. [2012] or SVRG Johnson and Zhang [2013].", "startOffset": 163, "endOffset": 182}, {"referenceID": 10, "context": "[2012] or SVRG Johnson and Zhang [2013]. However since the focus of this paper is on general black-box optimization we stick to comparing only to general methods.", "startOffset": 15, "endOffset": 40}, {"referenceID": 11, "context": "Note however that it is well-known that the volumetric center is usually more difficult to analyze, Vaidya [1996], Atkinson and Vaidya [1995].", "startOffset": 100, "endOffset": 114}, {"referenceID": 1, "context": "Note however that it is well-known that the volumetric center is usually more difficult to analyze, Vaidya [1996], Atkinson and Vaidya [1995]. \u2022 The extraneous log(\u03ba) comes from the number of potential restart when we decrease \u03b1.", "startOffset": 115, "endOffset": 142}, {"referenceID": 1, "context": "Note however that it is well-known that the volumetric center is usually more difficult to analyze, Vaidya [1996], Atkinson and Vaidya [1995]. \u2022 The extraneous log(\u03ba) comes from the number of potential restart when we decrease \u03b1. Is there a better way to learn \u03b1 that would not incur this additional logarithmic term? \u2022 Bubeck et al. [2015] shows essentially that one can combine the ellipsoid method with gradient descent to achieve the optimal 1\u2212 \u221a 1/\u03ba rate.", "startOffset": 115, "endOffset": 341}, {"referenceID": 1, "context": "Note however that it is well-known that the volumetric center is usually more difficult to analyze, Vaidya [1996], Atkinson and Vaidya [1995]. \u2022 The extraneous log(\u03ba) comes from the number of potential restart when we decrease \u03b1. Is there a better way to learn \u03b1 that would not incur this additional logarithmic term? \u2022 Bubeck et al. [2015] shows essentially that one can combine the ellipsoid method with gradient descent to achieve the optimal 1\u2212 \u221a 1/\u03ba rate. Can we prove such a result for SD+? The geometric politician could be refined in many ways. Here are two simple questions that we leave for future work: \u2022 One can think that gradient descent stores 1 gradient information, accelerated gradient descent stores 2 gradient information, and our method stores all past gradient information. We believe that neither 1, 2 nor all is the correct answer. Instead, the algorithm should dynamically decide the number of gradients to store based on the size of its memory, the cost of computing gradients, and the information each gradient reveals. \u2022 Is there a stochastic version of our algorithm? How well would such a method compare with state of the art stochastic algorithms such as SAG Le Roux et al. [2012] and SVRG Johnson and Zhang [2013]?", "startOffset": 115, "endOffset": 1212}, {"referenceID": 1, "context": "Note however that it is well-known that the volumetric center is usually more difficult to analyze, Vaidya [1996], Atkinson and Vaidya [1995]. \u2022 The extraneous log(\u03ba) comes from the number of potential restart when we decrease \u03b1. Is there a better way to learn \u03b1 that would not incur this additional logarithmic term? \u2022 Bubeck et al. [2015] shows essentially that one can combine the ellipsoid method with gradient descent to achieve the optimal 1\u2212 \u221a 1/\u03ba rate. Can we prove such a result for SD+? The geometric politician could be refined in many ways. Here are two simple questions that we leave for future work: \u2022 One can think that gradient descent stores 1 gradient information, accelerated gradient descent stores 2 gradient information, and our method stores all past gradient information. We believe that neither 1, 2 nor all is the correct answer. Instead, the algorithm should dynamically decide the number of gradients to store based on the size of its memory, the cost of computing gradients, and the information each gradient reveals. \u2022 Is there a stochastic version of our algorithm? How well would such a method compare with state of the art stochastic algorithms such as SAG Le Roux et al. [2012] and SVRG Johnson and Zhang [2013]?", "startOffset": 115, "endOffset": 1246}, {"referenceID": 14, "context": "6 in Nesterov [2004] shows that", "startOffset": 5, "endOffset": 21}], "year": 2016, "abstractText": "We propose a new framework for black-box convex optimization which is well-suited for situations where gradient computations are expensive. We derive a new method for this framework which leverages several concepts from convex optimization, from standard first-order methods (e.g. gradient descent or quasi-Newton methods) to analytical centers (i.e. minimizers of self-concordant barriers). We demonstrate empirically that our new technique compares favorably with state of the art algorithms (such as BFGS).", "creator": "LaTeX with hyperref package"}}}