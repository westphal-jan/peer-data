{"id": "1602.05350", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2016", "title": "Relative Error Embeddings for the Gaussian Kernel Distance", "abstract": "a naturally reproducing convex kernel can then define merely an equivalent embedding volume of a noisy data point x into exactly an overall infinite dimensional reproducing kernel hilbert weight space ( pseudo rkhs ). the absolute norm measured in this mapping space mostly describes skipping a distance, which usually we call here the shortest kernel distance. the random variable fourier features ( such of rahimi and recht ) collectively describe an appropriately oblivious fixed approximate mapping into finite dimensional euclidean space places that obviously behaves similar to the rkhs. naturally we show indeed in this paper that typically for the maximal gaussian kernel partition the euclidean norm between these just mapped places to features has $ ( * 1 + \\ epsilon ) $ - relative error with respect exactly to the kernel distance. normally when taken there are $ all n $ data points, traditionally we would show that $ o ( ( formula 1 / \\ epsilon ^ 2 ) \\ log ( n ) ) $ dimensions differences of the empty approximate feature bit space itself are also sufficient additive and necessary.", "histories": [["v1", "Wed, 17 Feb 2016 09:35:08 GMT  (643kb)", "https://arxiv.org/abs/1602.05350v1", null], ["v2", "Tue, 20 Sep 2016 17:13:17 GMT  (714kb)", "http://arxiv.org/abs/1602.05350v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["di chen", "jeff m phillips"], "accepted": false, "id": "1602.05350"}, "pdf": {"name": "1602.05350.pdf", "metadata": {"source": "CRF", "title": "Relative Error Embeddings of the Gaussian Kernel Distance", "authors": ["Di Chen", "Jeff M. Phillips"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 2.\n05 35\n0v 2\n[ cs\n.L G\n] 2\n0 Se\np 20\nand have diameter bounded by M, then we show that O((d/\u03b52) logM) dimensions are sufficient, and that this many are required, up to log(1/\u03b5) factors. We empirically confirm that relative error is indeed preserved for kernel PCA using these approximate feature maps."}, {"heading": "1 Introduction", "text": "The kernel trick in machine learning allows for non-linear analysis of data using many techniques such as PCA and SVM which were originally designed for linear analysis. The \u201ctrick\u201d is that these procedures only access data through inner products between data points, here we consider this non-linear inner product defined by a kernel K(\u00b7, \u00b7). Now given n data points, one can compute the n\u00d7 n gram matrix G of all pairwise inner products; that is so Gi,j = K(xi, xj) for all xi, xj in input data set X. Then the analysis can proceed using just the gram matrix G.\nHowever, for large data sets, constructing this n \u00d7 n matrix is a computational bottleneck, so methods have been devised for lifting n data points P \u2282 Rd to a high-dimensional space Rm (but where m \u226a n) so that the Euclidean dot-product in this space approximates the non-linear inner product.\nFor reproducing kernels K, there exists a lifting \u03c6 : Rd \u2192 HK , where HK is the reproducing kernel Hilbert space. It is in general infinite dimensional, but for every finite subset of points \u03a6(X) = {\u03c6(x) | x \u2208 X} spans an n-dimensional Euclidean space. That is K(x, y) = \u3008\u03c6(x), \u03c6(y)\u3009. Moreover, we can define the norm of a point in HK as \u2016\u03c6(x)\u2016HK = \u221a\n\u3008\u03c6(x), \u03c6(x)\u3009 using the inner product, and then due to linearity, a distance (the kernel distance) between two points is defined:\nDK(x, y) = \u221a\n\u2016\u03c6(x)\u20162 HK + \u2016\u03c6(y)\u20162 HK \u2212 2\u3008\u03c6(x), \u03c6(y)\u3009\n= \u221a K(x, x) +K(y, y)\u2212 2K(x, y). \u2217Thanks to support by NSF CCF-1350888, IIS-1251019, ACI-1443046, and CNS-1514520.\nFor reproducing kernels (actually a slightly smaller set called characteristic kernels) this is a metric [17, 13].\nThus we may desire an approximate lifting \u03c6\u0302 : Rd \u2192 Rm such that with probability at least 1\u2212 \u03b4 and all x, y \u2208 X\n(1\u2212 \u03b5) \u2264 DK(x, y) \u2016\u03c6\u0302(x)\u2212 \u03c6\u0302(y)\u2016 \u2264 (1 + \u03b5).\nIt turns out, one can always construct such a lifting with m = O((1/\u03b52) log(n/\u03b4)) by the famous Johnson-Lindenstrauss (JL) Lemma [7]. However, unlike the JL Lemma, there is not always known an implicit construction. In general, we must first construct the n \u00d7 n gram matrix, revealing an n-dimensional subspace (through an O(n3) time eigendecomposition) and then apply m = O((1/\u03b52) log(n/\u03b4)) random projections.\nSo in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].\nIn this document we reanalyze one of the most widely used and first variants, the Random Fourier Features, introduced by [16]. It applies to symmetric shift-invariant kernels which include Laplace, Cauchy, and most notably Gaussian. We will primarily focus on Gaussian kernels, defined K(x, y) = e\u2212 \u2016x\u2212y\u20162\n2\u03c32 , and use this definition for K henceforth unless otherwise specified. It is characteristic, hence DK is a metric."}, {"heading": "1.1 Existing Properties of Gaussian Kernel Embeddings", "text": "[16] defined two approximate embedding functions: \u03c6\u0303 : Rd \u2192 Rm and \u03c6\u0302 : Rd \u2192 Rm. Only the former appears in the final version of paper, but the latter is also commonly used throughout the literature [19]. Both features use random variables \u03c9i \u2208 Rd drawn uniformly at random from the Fourier transform of the kernel function; in the case of the Gaussian kernel, the Fourier transform is again a Gaussian specifically \u03c9i \u223c Nd(0, \u03c3\u22122).\nIn the former case, they define m functions of the form f\u0303i(x) = cos(\u03c9 T i x + \u03b3i), where \u03b3i \u223c Unif(0, 2\u03c0], uniformly at random from the interval (0, 2\u03c0], is a random shift. Applying each f\u0303i on a datapoint x gives the ith coordinate of \u03c6\u0303(x) in Rm as \u03c6\u0303(x)i = f\u0303i(x)/ \u221a m.\nIn the latter case, they define t = m/2 functions of the form\nf\u0302i(x) =\n[\ncos(\u3008\u03c9i, x\u3009) sin(\u3008\u03c9i, x\u3009)\n]\nas a single 2 \u00d7 1 dimensional vector, and one feature pair. Then applying f\u0302i on a data point x yields the (2i)th and (2i+ 1)th coordinate of \u03c6\u0302(x) in Rm as [\u03c6\u0302(x)2i; \u03c6\u0302(x)2i+1] = f\u0302i(x)/ \u221a t.\nThey showed that E[\u03c6\u0303(x)T \u03c6\u0303(y)] = K(x, y) for any x, y \u2208 Rd, and that this implied\nPr[|\u3008\u03c6\u0303(x), \u03c6\u0303(y)\u3009 \u2212K(x, y)| \u2265 \u03b5] \u2264 \u03b4\n\u2022 with m = O((1/\u03b52) log(1/\u03b4)) for each x, y \u2208 Rd, \u2022 with m = O((1/\u03b52) log(n/\u03b4)), for all x, y \u2208 X, for X \u2282 Rd of size n, or \u2022 with m = O((d/\u03b52) log(M/\u03b4)), for all x, y \u2208 X, for X \u2282 Rd so M = max\nx,y\u2208X \u2016x\u2212 y\u2016/\u03c3.\nRecently [18] tightened the above asymptotic bounds to show actual constants. It is folklore (apparently removed from final version of [16]; reproved in Section 2) that also E[\u03c6\u0302(x)T \u03c6\u0302(y)] = K(x, y), and thus all of the above PAC bounds hold for \u03c6\u0302 as well.\nAlso recently, [19] compared \u03c6\u0303 and \u03c6\u0302 (they used symbol \u03c6\u0306 in place of our symbol \u03c6\u0302), and demonstrated that \u03c6\u0302 performs better (for the same m) and has provably lower variance in approximating K(x, y) with \u03c6\u0302(x)T \u03c6\u0302(y) as opposed to with \u03c6\u0303(x)T \u03c6\u0303(y). However, these results do not obtain a bound on \u2016\u03c6\u0302(x) \u2212 \u03c6\u0302(y)\u2016/DK(x, y) since for very small distances, the additive error bounds on K(x, y) are not sufficient to say much about DK(x, y)."}, {"heading": "1.2 Our Results", "text": "In this paper we show that \u03c6\u0302 probabilistically induces a kernel K\u0302(x, y) = \u3008\u03c6\u0302(x), \u03c6\u0302(y)\u3009 and a distance\nDK\u0302(x, y) =\n\u221a\n\u2016\u03c6\u0302(x)\u20162 + \u2016\u03c6\u0302(y)\u20162 \u2212 2K\u0302(x, y) = \u2016\u03c6\u0302(x)\u2212 \u03c6\u0302(y)\u2016,\nwhich has strong relative error bounds with respect to DK(x, y), namely for a parameter \u03b5 \u2208 (0, 1)\n(1\u2212 \u03b5) \u2264 DK(x, y) DK\u0302(x, y) \u2264 (1 + \u03b5). (1)\nIn Section 2 we show (1) holds for each x, y such that \u2016x \u2212 y\u2016/\u03c3 \u2265 1, with probability at least 1\u2212 \u03b4, with m = O((1/\u03b52) log(1/\u03b4)). We also review known or folklore properties about \u03c6\u0302 and DK .\nWe first prove bounds that depend on the size n of a data set X \u2282 Rd. We show that m = O((1/\u03b52) log n) features are necessary (Section 3) and sufficient (Section 4) to achieve (1) with high probability (e.g., at least 1\u2212 1/n), when d and X are otherwise unrestricted.\nIn Section 5 we prove bounds for X \u2282 Rd where d is small, but the size n = |X| is unrestricted. Let M = maxx,y\u2208X \u2016x\u2212 y\u2016/\u03c3. We show that m = O((d/\u03b52) log(d\u03b5 M\u03b4 )) is sufficient to show (1) with probability 1 \u2212 \u03b4. Then in Section 6 we show that m = \u2126( d\n\u03b52 log(1/\u03b5) log( Mlog(1/\u03b5))) is necessary for\nany feature map.\nWe also empirically confirm the relative error through some simulations in Section 7. This includes showing that kernel PCA using these approximate features obtains relative error.\nFurther Implications in Machine Learning. There has been extensive recent effort to find oblivious subspace embeddings (OSE) of data sets into Euclidian spaces that preserve relative error. Such strong guarantees are, for instance, required to prove results about regression on the resulting set since we may not know the units on different coordinates; additive error bounds do not make sense in directions which are linear combinations of several coordinates.\nThe obliviousness of the features (they can be defined without seeing the data, and in some cases are independent of the data size) are essential for many large-scale settings such as streaming or distributed computing where we are not able to observe all of the data at once.\nOur results to do not describe unrestricted OSEs, as are possible with polynomial kernels [3]. Rather our lower bounds show that any OSE must have dimension depend on n or M.\nHowever, we show that random feature mappings allow for a finer notion of approximating the geometry of RKHS than previously known. In particular, the approximation error of inner products is proportional to the approximation error of distances. This is because both \u03c6 and \u03c6\u0302 map every input point to a unit vector; thus DK(x, y)\n2 = 2 \u2212 2K(x, y) and DK\u0302(x, y)2 = 2 \u2212 2K\u0302(x, y), for any distinct x, y \u2208 Rd. Therefore |K(x, y) \u2212 K\u0302(x, y)| is the same as 12 |DK(x, y)2 \u2212 DK\u0302(x, y)2|. Hence approximation error of the Gram matrix is bounded in terms of the sum of errors in pairwise distances\n\u2016G\u2212 G\u0302\u20161 \u2264 1\n2\n\u2211\nx,y\u2208X |DK(x, y)2 \u2212DK\u0302(x, y)2|.\nMoreover, with our low-dimensional bounds in Section 5, we can see that if an object U \u2282 Rd (such as a non-linear decision boundary) and training data S \u2282 Rd both lie within a ball with finite radiusM, then for any point x \u2208 S, the minimum kernel distance between U and x is approximately preserved in the random feature space as miny\u2208U \u2016\u03c6(x)\u2212 \u03c6(y)\u2016. This suggests better performance guarantees for kernelized learning problems regarding the minimization of \u21132 distances, such as in kernel SVM (hinge-loss) and in kernel PCA (recovery error); see Section 7."}, {"heading": "2 Basic Bounds", "text": "We first review some properties of the Gaussian kernel and the theory of Random Fourier Features. An earlier version of Rahimi-Recht [16] seemed to prove the following lemma. We repeat it for completeness.\nLemma 2.1. Given a random d-dimensional Gaussian \u03c9i \u223c Nd(0, \u03c3\u22122), and any two x, y \u2208 Rd then E\u03c9i [\u3008f\u0302i(x), f\u0302i(y)\u3009] = K(x, y). Proof. First we expand\n\u3008f\u0302i(x), f\u0302i(y)\u3009 =cos(\u3008\u03c9i, x\u3009) cos(\u3008\u03c9i, y\u3009) + sin(\u3008\u03c9i, x\u3009) sin(\u3008\u03c9i, y\u3009) = cos(\u3008\u03c9i, x\u2212 y\u3009).\nBy the stability and scaling of the Gaussian distribution, we can write \u3008\u03c9i, x \u2212 y\u3009 as \u03c9i,x,y \u2016x\u2212y\u2016\u03c3 where \u03c9i,x,y \u223c N (0, 1). Now note\nE\u03c9i [\u3008f\u0302i(x), f\u0302i(y)\u3009] = \u222b\nz\u2208R\n1\u221a 2\u03c0 exp(\u2212z2/2)) cos(z \u2016x \u2212 y\u2016 \u03c3 ) dz.\nWe next show that the right hand side evaluates to K(x, y). Using \u2206 = x\u2212y\u03c3 , let g(\u2016\u2206\u2016) := \u222b\nz\u2208R exp(\u2212z2/(2)) cos(z\u2016\u2206\u2016)dz. Now with integration by parts,\nd d\u2016\u2206\u2016g(\u2016\u2206\u2016) =\u2212 \u222b \u2208R e\u2212 z2 2 \u00b7 z sin(z\u2016\u2206\u2016) dz\n=\n\u222b\nz\u2208R\n\u2202\n\u2202z\n(\ne\u2212 z2 2 sin(z\u2016\u2206\u2016) ) \u2212 \u2016\u2206\u2016e\u2212 z 2 2 cos(z\u2016\u2206\u2016) dz\n=\u2212 \u2016\u2206\u2016 \u222b\nz\u2208R e\u2212\nz2\n2 cos(z\u2016\u2206\u2016) dz = \u2212\u2016\u2206\u2016g(\u2016\u2206\u2016).\nSo g(\u2016\u2206\u2016) = \u03bae\u2212 \u2016\u2206\u2016 2 2 for some constant \u03ba. But \u03ba = g(0) = \u222b z\u2208R e \u2212 z2\n2 dz = \u221a 2\u03c0, therefore:\nE\u03c9i [\u3008\u03c8i(x), \u03c8i(y)\u3009] = \u221a 2\u03c0 \u00b7 1\u221a\n2\u03c0 e\u2212\n\u2016\u2206\u20162 2 = e\u2212 \u2016x\u2212y\u20162 2\u03c32 = K(x, y).\nWe can then state the next simple corollary.\nCorollary 2.1. The higher dimensional random feature maps \u03c6\u0302 also satisfy for any x, y \u2208 Rd\nE\u03c91,...,\u03c9d [\u3008\u03c6\u0302(x), \u03c6\u0302(y)\u3009] = K(x, y).\nFor any x, y \u2208 Rd and any \u03c9i \u223c N (0, \u03c2) we have that \u3008\u03c8i(x), \u03c8i(y)\u3009 \u2208 [\u22122, 2]. Thus we can apply a standard Chernoff-Hoeffding bound to show the next corollary.\nCorollary 2.2. Given a random feature map \u03c6\u0302 defined by t = O((1/\u03b52) log(1/\u03b4)) iid random ddimensional Gaussian \u03c9i \u223c Nd(0, \u03c3\u22122), for \u03b5 \u2208 (0, 1/2) and \u03b4 \u2208 (0, 1), then for any x, y \u2208 Rd\nPr\n[ \u2223\n\u2223 \u2223 K(x, y)\u2212 \u3008\u03c6\u0302(x), \u03c6\u0302(y)\u3009\n\u2223 \u2223 \u2223 \u2264 \u03b5 ] \u2265 1\u2212 \u03b4.\nSince K\u0302(x, x, ) = 1, then DK\u0302(x, y) 2 = 2\u22122K\u0302(x, y), and additive error bounds between DK(x, y)2 and DK\u0302(x, y) 2 follow directly. But we can also state some relative error bounds when \u2016x \u2212 y\u2016 is large enough.\nLemma 2.2. For each x, y \u2208 Rd such that \u2016x \u2212 y\u2016 \u2265 \u03c3 and m = O((1/\u03b52) log(1/\u03b4)) with \u03b5 \u2208 (0, 1/10) and \u03b4 \u2208 (0, 1). Then with probability at least 1\u2212 \u03b4, we have DK(x,y)D K\u0302 (x,y) \u2208 [1\u2212 \u03b5, 1 + \u03b5].\nProof. By choosing m = O((1/\u03b52) log(1/\u03b4)) so that |K(x, y) \u2212 K\u0302(x, y)| \u2264 \u03b5/4 (via Lemma 2.1), we have that |D2K(x, y) \u2212D2K\u0302(x, y)| \u2264 \u03b5/2. We also note that when \u2016x \u2212 y\u2016 \u2265 \u03c3 then K(x, y) \u2264 exp(\u2212\u03c32/(2\u03c32)) = 1\u221a\ne \u2264 0.61. Hence D2K(x, y) \u2265 2(1 \u2212 0.61) = 0.78 \u2265 0.5, and we have that\n|D2K(x, y)\u2212D2K\u0302(x, y)| \u2264 \u03b5/2 \u2264 \u03b5D 2 K(x, y).\nThen |1 \u2212 D 2 K\u0302 (x,y)\nD2 K (x,y)\n| \u2264 \u03b5, implying 1 \u2212 \u03b5 \u2264 D 2 K\u0302 (x,y)\nD2 K (x,y) \u2264 1 + \u03b5. Taking the square root of all parts completes the proof via \u221a 1 + \u03b5 < (1 + \u03b5) and \u221a 1\u2212 \u03b5 > (1\u2212 \u03b5).\nSo to understand the relative error in DK(x, y), what remains is the case when \u2016x\u2212 y\u2016 is small. As we will see, when \u2016x \u2212 y\u2016 is small, then DK(x, y) behaves like \u2016x \u2212 y\u2016 and we can borrow insights from \u21132 embeddings. Then combining the two cases (when \u2016x \u2212 y\u2016 is large and when \u2016x\u2212y\u2016 is small) we can achieve \u201cfor all bounds\u201d either via simple union bounds, or through careful \u201ccontinuous net\u201d arguments when X is in a bounded range. Similarly, we will show near-matching lower bounds via appealing to near-\u21132 properties or via net arguments."}, {"heading": "3 Approximations and Relation to \u21132 on Small Distances", "text": "For the remainder of the paper, it will be convenient to let \u2206 = (x \u2212 y)/\u03c3 be the scaled vector between some pair of points x, y \u2208 X. Define DK(\u2206) = DK(x, y) = \u221a\n2\u2212 2e 12\u2016\u2206\u20162 . Lemma 2.2 already established that when \u2016\u2206\u2016 \u2265 1, then additive error bounds imply relative error bounds. In this section we consider the alternate case of when \u2016\u2206\u2016 \u2264 1, and show that in this setting that DK\u0302 is indeed close to DK , and to varying degrees also approximates \u21132.\nAs a warm up, and as was similarly observed in [15], a simple Taylor expansion when \u2016\u2206\u2016 \u2264 1, implies that\n\u2016\u2206\u20162 \u2212 1 4 \u2016\u2206\u20164 \u2264 DK(\u2206)2 = 2\u2212 2 exp(\u2016\u2206\u20162/2) \u2264 \u2016\u2206\u20162,\nand by 14\u2016\u2206\u20164 \u2264 14\u2016\u2206\u20162 and a square root\n0.86\u2016\u2206\u2016 \u2264 DK(\u2206) \u2264 \u2016\u2206\u2016. (2)\nMoreover, when \u2016\u2206\u2016 \u2264 2\u221a\u03b5 then\n(1\u2212 \u03b5)\u2016\u2206\u20162 \u2264 DK(\u2206)2 \u2264 \u2016\u2206\u20162. (3)\nIn what follows we derive more nuanced and powerful bounds relating DK(x, y) and DK\u0302(x, y); which transitively relates DK\u0302(x, y) to \u2016x\u2212 y\u2016.\nUseful expansions. We first observe that by cos(a) cos(b) + sin(a) sin(b) = cos(a\u2212 b) that\n\u3008f\u0302i(x), f\u0302i(y)\u3009 = cos(\u3008\u03c9i, x\u3009) cos(\u3008\u03c9i, y\u3009) + sin(\u3008\u03c9i, x\u3009) sin(\u3008\u03c9i, y\u3009) = cos(\u3008\u03c9i, (x\u2212 y)\u3009).\nHence by \u3008f\u0302i(x), f\u0302i(x)\u3009 = cos(\u3008\u03c9i, 0) = 1 we have DK\u0302(x, y)2 = 2\u2212 21t \u2211t i cos(\u3008\u03c9i, (x\u2212 y)\u3009). By the rotational stability of the Gaussian distribution we can replace \u3008\u03c9i, (x\u2212y)\u3009 with \u03c9i,x,y \u2016x\u2212y\u2016\u03c3 where \u03c9i,x,y \u223c N (0, 1). It will be more convenient to write \u03c9i,x,y as \u03c9i,\u2206, so \u3008\u03c9i, (x \u2212 y)\u3009 = \u03c9i,\u2206\u2016\u2206\u2016. Thus \u3008f\u0302i(x), f\u0302i(y)\u3009 = cos(\u03c9i,\u2206\u2016\u2206\u2016). Moreover, we can define DK\u0302(\u2206) = DK\u0302(x, y) = \u221a\n2\u2212 21t \u2211t i=1 cos(\u03c9i,\u2206\u2016\u2206\u2016). Now considering\nDK\u0302(\u2206) 2 DK(\u2206)2 = 1\u2212 1t \u2211t i=1 cos(\u03c9i,\u2206\u2016\u2206\u2016) 1\u2212 e 12\u2016\u2206\u20162 ,\nthe following Taylor expansion, for \u03c9i,\u2206\u2016\u2206\u2016 \u2264 1, will be extremely useful:\n1 t \u2211t i=1 1 2\u03c9 2 i,\u2206\u2016\u2206\u20162\n1 2\u2016\u2206\u20162 \u2212 \u00b714\u2016\u2206\u20164\n\u2265 DK\u0302(\u2206) 2\nDK(\u2206)2 \u2265\n1 t \u2211t i=1\n(\n1 2\u03c9 2 i,\u2206\u2016\u2206\u20162 \u2212 124(\u03c94i,\u2206\u2016\u2206\u20164)\n)\n1 2\u2016\u2206\u20162\n.\nSimplifying gives\n1\n1\u2212 12\u2016\u2206\u20162\n(\n1\nt\nt \u2211\ni=1\n\u03c92i,\u2206\n)\n\u2265 DK\u0302(\u2206) 2 DK(\u2206)2 \u2265 ( 1 t t \u2211\ni=1\n\u03c92i,\u2206\n)\n\u2212 \u2016\u2206\u2016 2 12 \u00b7 1 t\nt \u2211\ni=1\n\u03c94i,\u2206. (4)\nVery small distances. Next we can understand what happens in the limit as we shrink the region containing X. We do so by using a scaling parameter \u03bb, and observe what happens to the ratio DK\u0302(\u03bb\u2206) 2/DK(\u03bb\u2206) in the limit as \u03bb \u2192 0.\nLemma 3.1. For scalar scaling parameter \u03bb,\nlim \u03bb\u21920\nDK\u0302(\u03bb\u2206) 2 DK(\u03bb\u2206)2 = 1 t\nt \u2211\ni=1\n\u03c92i,\u2206.\nProof. Observe that \u03c9i,\u2206 = \u03c9i,\u03bb\u2206, for any \u03bb > 0. Thus in equation (4), lim\u03bb\u21920 1/(1 \u2212 12\u2016\u03bb\u2206\u20162) goes to 1 so the left hand-side approaches 1t \u2211t i=1 \u03c9 2 i,\u2206. Similarly, lim\u03bb\u21920 \u2016\u03bb\u2206\u20162/12 goes to 0, and the right-hand side also approaches 1t \u2211t i=1 \u03c9 2 i,\u2206.\nIf we fix \u2206 then \u03c9i,\u2206, 1 \u2264 i \u2264 t are i.i.d Gaussian variables with mean 0 and standard deviation 1. Thus\n\u2211t i=1 \u03c9 2 i,\u2206 is a \u03c7 2-variable with t degrees of freedom. This observation paired with Lemma 3.1 will be useful for a lower bound.\nBut to prove an upper bound, we do not need to go all the way to the limit. Common concentration results for \u03c72 variables give us the following.\nLemma 3.2. For \u03b5 \u2208 (0, 1) and \u03b4 \u2208 (0, 1/2), if t \u2265 8 1 \u03b52 ln(2/\u03b4) then\nPr\n[\n1\nt\nt \u2211\ni=1\n\u03c92i,\u2206 /\u2208 [1\u2212 \u03b5, 1 + \u03b5] ] \u2264 \u03b4.\nProof. Here we use Lemma 1 from [4]; if X is a \u03c72 random variable with t degrees of freedom\nPr[t\u2212 2 \u221a tx \u2264 X \u2264 t+ 2 \u221a tx+ 2x] \u2265 1\u2212 2e\u2212x.\nHere we can set x := 18 t\u03b5 2 then t\u22122\n\u221a tx = t\u2212\u03b5t/ \u221a 2, and t+2 \u221a tx+2x = t+\u03b5t/ \u221a 2+ 14\u03b5 2t < t+\u03b5t.\nAlso, 2e\u2212x = 2e\u2212 1 8 t\u03b52 = 2e\u2212 ln(2/\u03b4) = \u03b4/2 \u2264 \u03b4 for \u03b4 \u2264 1/2. Therefore, 1t \u2211t i=1 \u03c9 2 i,\u2206 /\u2208 [1\u2212 \u03b5, 1 + \u03b5] with probability at most \u03b4.\nThis result bounds to [1 \u2212 \u03b5, 1 + \u03b5] for some t = O((1/\u03b52) log(1/\u03b4), the main terms of equation (4). However, the other parts (\u2016\u2206\u20162/2 and the term containing \u03c94i,\u2206) require a further restriction on \u2016\u2206\u2016 to be handled directly, as we show in the following. Lemma 3.3. For \u03b5 \u2208 (0, 1) and \u03b4 \u2208 (0, 1/2), if \u2016\u2206\u2016 \u2264 \u221a \u03b5\nlog(1/\u03b4) , and t = \u2126( 1 \u03b52 log(1/\u03b4)), then with\nprobability at least 1\u2212O(\u03b4), for all \u03bb \u2208 [0, 1] we have DK\u0302(\u03bb\u00b7\u2206) 2\nDK(\u03bb\u00b7\u2206)2 \u2208 [1\u2212 \u03b5, 1 + \u03b5].\nProof. If \u03c9 is a standard Gaussian variable, then |\u03c9| \u2264 C \u00b7 q \u221a log(1/\u03b4) with probability at least 1\u2212 \u03b4q for any q > 0, for some constant C. This means, if \u2016\u2206\u2016 \u2264 \u221a \u03b5\nlog(1/\u03b4) then \u03c9i,\u2206\u2016\u2206\u2016 \u2264 \u221a \u03b5 log(1/\u03b4)\nwith probability at least 1\u2212O(\u03b4). Also then \u03c9i,\u2206\u2016\u2206\u2016 \u2264 1, which satisfies the conditions for (4). This also bounds the term \u03c94i,\u2206 \u2264 \u03b5 2 log2(1/\u03b4) used in equation (4). In particular, along with \u2016\u2206\u2016 \u2264 \u221a \u03b5\nlog(1/\u03b4) , this implies\n\u2016\u2206\u20162 12 \u00b7 1 t\nt \u2211\ni=1\n\u03c94i,\u2206 \u2264 1 12 \u2016\u2206\u2016 \u00b7 \u03b5\n2\nlog2(1/\u03b4) =\n\u03b5\n12\n\u03b5\nlog2(1/\u03b4)\n1 \u2016\u2206\u20162 \u2264 \u03b5 12 .\nThen along with Lemma 3.2 and RHS of (4), we have D K\u0302 (\u2206)2\nDK(\u2206)2 \u2265 1\u2212O(\u03b5).\nSimilarly, Lemma 3.2 and 12\u2016\u2206\u20162 \u2264 \u03b52 log2(1/\u03b4) imply the LHS of (4) is bounded above by 1+O(\u03b5). Thus, after adjusting constants in t, we have D K\u0302 (\u2206)2\nDK(\u2206)2 \u2208 [1\u2212 \u03b5, 1 + \u03b5].\nFor D K\u0302 (\u03bb\u2206)2\nDK(\u03bb\u2206)2 \u2208 [1\u2212 \u03b5, 1+ \u03b5], note that the above analysis still holds if we scale \u2016\u2206\u2016 to be smaller,\ni.e. as long as \u03bb \u2208 [0, 1]. In particular, \u03c9i,\u2206 is unchanged by the scaling \u03bb."}, {"heading": "3.1 Lower Bounds, Based on Very Small \u2016x\u2212 y\u2016", "text": "Lemma 3.1 implies that when \u2016x\u2212 y\u2016 is small, DK\u0302(x, y) behaves like a Johnson-Lindenstrauss (JL) random projection of \u2016x\u2212 y\u2016, and we can invoke known JL lower bounds.\nIn particular, Lemma 3.1 implies if the input data set X \u2282 Rd is in a sufficiently small neighborhood of zero, the relative error is preserved only when\n\u2211t i=1 \u03c9 2 i,x,y\u2016\u03bb(x \u2212 y)\u20162 \u2208 [(1\u2212 \u01eb)\u2016\u03bb(x\u2212\ny)\u20162, (1 + \u01eb)\u2016\u03bb(x \u2212 y)\u20162] for all x, y \u2208 X, and for all arbitrary \u03bb \u2208 R. Which implies for arbitrary x, y \u2208 X, and \u03bb \u2208 R that\n\u221a \u221a \u221a \u221a t \u2211\ni=1\n|\u03c9i \u00b7 \u03bb(x\u2212 y)|2 =\n\u221a \u221a \u221a \u221a t \u2211\ni=1\n\u03c92i,x,y\u03bb\u2016x\u2212 y\u20162 \u2208 [(1\u2212 \u03b5)\u03bb \u2016x\u2212 y\u2016 , (1 + \u03b5)\u03bb \u2016x\u2212 y\u2016] .\nThe far left hand side is in fact the norm \u2016g(x)\u2212 g(y)\u2016 where g(x) is the vector with coordinates (\u03c91 \u00b7 \u03bbx, ..., \u03c9t \u00b7 \u03bbx). Thus these are the exact conditions for relative error bounds on embedding \u21132 via the Johnson-Lindenstrauss transforms, which gives the following.\nLemma 3.4. If for any n, d > 0,X \u2282 Rd s.t. |X| = n, using t(n, \u03b5) random Fourier features, D\nK\u0302 (x,y)\nDK(x,y) \u2208 [1 \u2212 \u03b5, 1 + \u03b5] with probability 1 \u2212 \u03b4, then there exists a random linear embedding with t(n, \u03b5) projected dimensions preserving the \u21132-norm for all pairs x, y \u2208 S up to relative error with probability at least 1\u2212 \u03b4.\nTheorem 3.1. There exists a set of n points X \u2282 Rd so that t = \u2126( 1\u03b52 log n) dimensions are necessary so for any x, y \u2208 X that DK\u0302(x,y)DK(x,y) \u2208 [1\u2212 \u03b5, 1 + \u03b5].\nProof. A lower bound of \u2126( 1 \u03b52\nlog n) projected dimensions for linear embeddings in \u21132 is given by e.g. [9].\nSection 6 shows another lower bound for point sets with unbounded n, but that are contained in a ball of bounded radius M and bounded dimension d.\nRemark: A new result of Larsen and Nelson [14] provides a t = \u2126( 1\u03b52 log n) lower bound for even non-linear embeddings of a size n point set in Rd into Rt that preserve distances within (1 \u00b1 \u03b5). It holds for any \u03b5 \u2208 (1/min{n, d}0.4999, 1). Since, there exists an isometric embedding of any set of n points in any RKHS into Rn, then this t = \u2126( 1\n\u03b52 log n) lower bound suggests that it applies\nto \u03c6\u0302 and \u03c6\u0303 or any other technique, for almost any \u03b5. However, it is not clear that any point set (including the ones used in the strong lower bound proof [14]), can result from an isomorphic (or approximate) embedding of RKHS into Rn. Hence, this new result does not immediately imply our lower bound.\nMoreover, the above proof of Theorem 3.1 retains two points of potential interest. First it holds for a (very slightly) larger range of \u03b5 \u2208 (0, 1). Second, Lemma 3.4 highlights that at very small ranges, \u03c6\u0302 is indistinguishable from the standard JL embedding."}, {"heading": "4 Relative Error Bounds For Small Distances and Small Data Sets", "text": "The Taylor expansion in equation (4) and additive errors via Lemma 2.1 are only sufficient to provide us bounds for \u2016\u2206\u2016 \u2264 \u221a\u03b5/ log n or for \u2016\u2206\u2016 \u2265 1. In this section we need to use a more powerful technique or moment generating functions to fill in this gap.\nIn particular, 1 \u2212 cos (\u03c9i,\u2206\u2016\u2206\u2016) is a sub-Gaussian random variable so it is expected to have a good concentration, but this fact is not enough for relative error bounds. We will use a more precise bound of the moment generating function of 1\u2212 cos (\u03c9i,\u2206\u2016\u2206\u2016). Recall that the moment generating function M(s) of a random variable X is given by E[esX ].\nLemma 4.1. For \u03c9 \u223c N (0, 1) and 0 \u2264 \u2016\u2206\u2016 \u2264 1, let M(s) be the moment generating function of 1\u2212 cos (\u03c9\u2016\u2206\u2016)\u2212 ( 1\u2212 e\u2212 12\u2016\u2206\u20162 ) = e\u2212 1 2 \u2016\u2206\u20162 \u2212 cos (\u03c9\u2016\u2206\u2016). Then for all s \u2208 [0, 1\n2\u2016\u2206\u20162 ),\nlnM(s) \u2264 1 4 s2\u2016\u2206\u20164.\nProof. First we note two Taylor approximations which hold for all x \u2208 R:\ncosx \u2265 1\u2212 1 2 x2 and e\u2212x \u2264 1\u2212 x+ 1 2 x2.\nNow\nM(s) = E [ exp ( s(e\u2212 1 2 \u2016\u2206\u20162 \u2212 cos(\u03c9\u2016\u2206\u2016)) )]\n\u2264 E [ exp ( s(1\u2212 1 2 \u2016\u2206\u20162 + 1 8 \u2016\u2206\u20164)\u2212 s(1\u2212 1 2 \u03c92\u2016\u2206\u20162) )] = E [ exp (\n\u2212s 2 \u2016\u2206\u20162 + s 8 \u2016\u2206\u20164 + s 2 \u03c92\u2016\u2206\u20162 )]\n= exp( s 8 \u00b7 \u2016\u2206\u20164 \u2212 s 2 \u2016\u2206\u20162) \u00b7 E [ e\u2212s 1 2 \u03c92\u2016\u2206\u20162 ] .\nBut\nE\n[\ne\u2212s 1 2 \u03c92\u2016\u2206\u20162\n]\n=\n\u222b \u221e\n\u2212\u221e\n1\u221a 2\u03c0 e\u2212 u2 2 \u00b7 e\u2212s 12u2\u2016\u2206\u20162du\n=\n\u222b \u221e\n\u2212\u221e\n1\u221a 2\u03c0 e\u2212 1 2 u2(1+s\u2016\u2206\u20162)du\n= 1 \u221a 1 + s\u2016\u2206\u20162 \u222b \u221e \u2212\u221e \u221a 1 + s\u2016\u2206\u20162 1\u221a 2\u03c0 e\u2212 1 2 u2(1+s\u2016\u2206\u20162)du = 1 \u221a\n1 + s\u2016\u2206\u20162 .\nNoting that ln(1 + x) \u2265 x\u2212 x22 for x \u2208 [0, 12), then whenever s \u2208 [0, 12\u2016\u2206\u20162 ):\nlnM(s) \u2264 ln ( exp( s8\u2016\u2206\u20164 \u2212 s2\u2016\u2206\u20162) \u221a\n1 + s\u2016\u2206\u20162\n)\n= s 8 \u2016\u2206\u20164 \u2212 s 2 \u2016\u2206\u20162 \u2212 1 2 ln(1 + s\u2016\u2206\u20162) \u2264 s 8 \u2016\u2206\u20164 \u2212 s 2 \u2016\u2206\u20162 \u2212 1 2 (s\u2016\u2206\u20162 \u2212 1 2 s2\u2016\u2206\u20164) = s2\n4 \u2016\u2206\u20164 \u2212 s 8 \u2016\u2206\u20164 \u2212 s\u2016\u2206\u20162\n\u2264 s 2\n4 \u2016\u2206\u20164.\nWe next combine this result with an existing bound on sub-exponential random variables [5](Lemma 4.1 in Chapter 1). Let X be a random variable, and let M(s) be the moment generating function of X \u2212 E[X]. Let X\u0304t := 1t \u2211t i=1 Xi where X1, ...,Xt are i.i.d. samples of X. If lnM(s) \u2264 s 2p 2 for all s \u2208 [0, 1q ), then\nP (|X\u0304t \u2212 E[X]| \u2265 \u03b5E[X]) \u2264 2 exp ( \u2212min ( t \u03b52E[X]2 2p , t \u03b5E[X] 2q )) . (5)\nLemma 4.2. If \u2016x \u2212 y\u2016 \u2264 \u03c3, m = \u2126(\u03b5\u22122 log 1\u03b4 ), then D K\u0302 (x,y) DK(x,y) \u2208 [1 \u2212 \u03b5, 1 + \u03b5] with probability at least 1\u2212 \u03b4.\nProof. Recall that \u3008f\u0302i(x), f\u0302i(y)\u3009 = cos(\u3008\u03c9i, (x\u2212 y)\u3009) and (1/2)DK\u0302 (x, y)2 = 1t \u2211t i=1(1\u2212 cos(\u3008\u03c9i(x\u2212 y)\u3009)). Then define random variable Xi = (1 \u2212 cos(\u3008\u03c9i(x \u2212 y)\u3009)), and X = 1t \u2211t i=1Xi. Since E[X] = E[DK\u0302(x, y) 2] = DK(x, y) 2, then E[Xi] = 1\u2212 exp(\u221212\u2016\u2206\u20162).\nFor M(s) the moment generating function of Xi\u2212E[Xi], by Lemma 4.1 we have ln(M(s)) \u2264 12s2p for p = 12\u2016\u2206\u20164 for s \u2208 [0, 1q ] with q = 2\u2016\u2206\u20162. Also recall by equation (2) we have for any x, y \u2208 Rd with \u2016x\u2212 y\u2016 \u2264 \u03c3, that 0.86 \u2264 DK(x,y)\u2016\u2206\u2016 \u2264 1. Plugging these values into equation (5) with t = 6\n\u03b52 \u2016\u2206\u20164 DK(x,y)4 ln(2/\u03b4) = O( 1 \u03b52 log 1\u03b4 ), we obtain that\nPr[|DK\u0302(x, y)\u2212DK(x, y)| \u2265 \u03b5DK(x, y)] = Pr[|X \u2212 E[X]| \u2265 \u03b5E[X]]\n\u22642 exp ( \u2212min ( t \u03b52E[X]2 2p , t \u03b5E[X] 2q ))\n=2exp\n( \u2212min ( t \u03b52E[X]2 \u2016\u2206\u20164 , t \u03b5E[X] 4\u2016\u2206\u20162 )) .\n=2exp\n( \u2212min ( 6, 3\n2\u03b5 \u2016\u2206\u20162 DK(x, y)2 ) ln 2 \u03b4 )\n\u22642 exp ( \u2212min ( 6, 1\n\u03b5\n)\nln 2\n\u03b4\n)\n\u2264 \u03b4.\nTogether with Lemma 2.2 (for \u2016x\u2212 y\u2016 \u2265 \u03c3), we apply a union bound over all (n 2 )\npairs vectors from a set of n vectors.\nTheorem 4.1. For any set X \u2282 Rd of size n, then m = \u2126( 1 \u03b52\nlog n) dimensions are sufficient so D\nK\u0302 (x,y)\nDK(x,y) \u2208 [1\u2212 \u03b5, 1 + \u03b5] with high probability (e.g., at least 1\u2212 1/n)."}, {"heading": "5 Relative Error Bounds for Low Dimensions and Diameter", "text": "A common approach in subspace embeddings replaces n with the size of a sufficiently fine net. Given a smoothness condition, once the error is bound on the net points, the guarantee is extended to the \u2018gaps\u2019 in between. To bound the size of the gaps, we derive the Lipschitz constant of DK\u0302(\u00b7)2, with respect to the vector \u2206 (not individual points in Rd).\nAs opposed to previous work regarding the norms of the Euclidean space, the Gaussian kernel distance is non-linear; nonetheless we observe that it is near linear close to 0, and make use of a special construct that allows us to take advantage of this insight.\nLemma 5.1. For any \u2206 \u2208 Rd, |\u2207DK\u0302(\u2206)2| \u2264 \u2211t i=1 \u221a d\u2016\u03c92i,\u2206\u2016\u2016\u2206\u2016.\nProof. We denote by \u03c9 (j) i,\u2206 the j-th coordinate of \u03c9i,\u2206.\n\u2223 \u2223\u2207DK\u0302(\u2206)2 \u2223 \u2223 =\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 t \u2211\ni=1\nd \u2211\nj=1\n\u03c9 (j) i,\u2206 sin(\u03c9i,\u2206 \u00b7\u2206)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u2264 t \u2211\ni=1\nd \u2211\nj=1\n|\u03c9(j)i,\u2206| |sin(\u03c9i,\u2206 \u00b7\u2206)|\n\u2264 t \u2211\ni=1\n\u2016\u03c9i,\u2206\u20161|\u03c9i,\u2206 \u00b7\u2206| \u2264 t \u2211\ni=1\n\u2016\u03c9i,\u2206\u20161\u2016\u03c9i,\u2206\u2016\u2016\u2206\u2016 \u2264 t \u2211\ni=1\n\u221a d\u2016\u03c9i,\u2206\u20162\u2016\u2206\u2016\nCorollary 5.1. For any c \u2265 0, over the region \u2016\u2206\u2016 \u2264 c, the Lipschitz constant of DK\u0302(\u2206)2 is bounded above by O(c \u00b7 t \u221a d log n) with probability at least 1\u2212O( 1n).\nProof. We can bound \u2016\u03c9i,\u2206\u20162 \u2264 O(log n) with probability at least 1\u2212 1nq for any large fixed q > 0. So the gradient is bounded by \u221a d\u2016\u2206\u2016\u2211ti=1 \u2016\u03c9i,\u2206\u20162 \u2264 \u221a d\u2016\u2206\u2016t \u00b7O(log n) \u2264 O(c \u00b7 t \u221a d log n), which also bounds the Lipschitz constant.\nIn case c = \u221a \u03b5 logn , the Lipschitz constant is O(t \u221a \u03b5d). We then have enough ingredients for the following main result. Intuitively, what separates typical net arguments from ours is the scaling \u03bb in Lemma 3.3; our \u2018net\u2019 contains, which we call a \u03bb-urchin, a set of line segments extending from the origin, in addition to discrete points.\nLemma 5.2. If t = \u2126( 1 \u03b52 d log\n(\nd \u03b5 1 \u03b4\n)\n), then with probability at least 1 \u2212 \u03b4, for all \u2206 such that \u2016\u2206\u2016 \u2264 \u221a \u03b5\nlogn , then D K\u0302 (\u2206)2 DK(\u2206)2 \u2208 [1\u2212 \u03b5, 1 + \u03b5].\nProof. The proof will first consider distances \u2206 such that {\u2206 : \u2016\u2206\u2016 = \u221a \u03b5\nlogn}, and then generalize to smaller distances using Lemma 3.3 and a construction we call a \u03bb-urchin. Fixed distance case: Consider two points \u22061,\u22062 from the surface {\u2206 : \u2016\u2206\u2016 = \u221a \u03b5\nlogn}. If \u2016\u22061 \u2212\u22062\u2016 \u2264 \u03b5 1.5\nt \u221a d log2 n then Corollary 5.1 implies\n\u2223 \u2223DK\u0302(\u22061) 2 \u2212DK\u0302(\u22062)2 \u2223 \u2223 \u2264 O(t \u221a \u03b5d) \u00b7 \u2016\u22061 \u2212\u22062\u2016 \u2264 O(t \u221a \u03b5d) \u00b7 \u03b5 1.5\nt \u221a d log2 n\n= O\n( \u03b5 \u00b7 ( \u221a \u03b5\nlog n\n)2 )\n= O(\u03b5) \u00b7DK(\u22061)2.\nNow let \u0393\u03b3 be a \u03b3-net over {\u2206 : \u2016\u2206\u2016 = \u221a \u03b5 logn} where \u03b3 \u2264 \u03b5 1.5 t \u221a d log2 n\n. For any \u22061 \u2208 {\u2206 : \u2016\u2206\u2016 = \u221a \u03b5\nlogn}, there exists \u22062 \u2208 \u0393\u03b3 such that \u2016\u22061 \u2212\u22062\u2016 \u2264 \u03b3. Then the above implies\n(1\u2212O(\u03b5))DK\u0302(\u22062)2 \u2264 DK\u0302(\u22061)2 \u2264 (1 +O(\u03b5))DK\u0302(\u22062)2. (6)\nBy the triangle inequality, equation (2), and t \u221a d log n > 1, we have\n|DK(\u22061)\u2212DK(\u22062)| \u2264 DK(\u22061,\u22062) \u2264 \u2016\u22061\u2212\u22062\u2016 \u2264 \u03b3 \u2264 \u03b5 \u00b7 \u221a \u03b5\nlog n \u00b7 1 t \u221a d log n \u2264 \u03b5 \u00b7O(DK(\u22061)). (7)\nWe will choose t = \u2126( 1 \u03b52 log |\u0393\u03b3 |) such that the following holds over \u0393\u03b3 with high probability\n(1\u2212O(\u03b5))DK(\u22062)2 \u2264 DK\u0302(\u22062)2 \u2264 (1 +O(\u03b5))DK(\u22062)2. (8)\nThese equations (6), (7), and (8) show, respectively that the ratios D K\u0302 (\u22061)\nD K\u0302 (\u22062)\n, D K\u0302 (\u22062) DK(\u22062) , and DK(\u22062)DK(\u22061)\nare all in [1 +O(\u03b5), 1 \u2212O(\u03b5)]; hence we can conclude\n|DK(\u22061)\u2212DK\u0302(\u22061)| \u2264 O(\u03b5) \u00b7DK(\u22061). (9)\nWhich are in turn 1\u00b1O(\u03b5) relative error bounds for the kernel distance, over {\u2206 : \u2016\u2206\u2016 = \u221a \u03b5\nlogn}. All distances case: For the region {\u2206 : \u2016\u2206\u2016 < \u221a \u03b5\nlogn}, consider again \u0393\u03b3 . For each net point p \u2208 \u0393\u03b3 we draw a line segment from p to the origin, producing the set of line segments \u0393\u0304\u03b3 , that we call the \u03b3-urchin. By Lemma 3.3, and t = \u2126( 1\n\u03b52 log |\u0393\u03b3 |), we have relative error bounds for the\nGaussian kernel distance over the \u03b3-urchin.\nNow for any \u03bb \u2208 (0, 1), consider the intersection {\u2206 : \u2016\u2206\u2016 = \u03bb \u221a \u03b5\nlogn} \u2229 \u0393\u0304\u03b3 . We see that the \u03b3-urchin induces a net over {\u2206 : \u2016\u2206\u2016 = \u03bb \u221a \u03b5\nlogn}. Due to scaling we can see that, in fact, it is a (\u03bb\u03b3)-net. So the distance between any point in {\u2206 : \u2016\u2206\u2016 = \u03bb \u221a \u03b5\nlogn} and the closest net point is bounded above by \u03bb\u00b7\u03b5 1.5\nt \u221a d log2 n\n. From Corollary 5.1, the Lipschitz constant is now O(t\u03bb \u221a \u03b5d).\nBy arguments similar to those leading to (9) we obtain, for any \u22061 \u2208 {\u2206 : \u2016\u2206\u2016 = \u03bb \u221a \u03b5 logn}\n|DK(\u22061)\u2212DK\u0302(\u22061)| \u2264 O(\u03b5) \u00b7 \u03bb \u00b7 \u221a \u03b5\nlog n \u2264 O(\u03b5) \u00b7DK(\u22061). (10)\nSince this holds for all \u03bb \u2208 [0, 1], we obtain relative error bounds over {\u2206 : \u2016\u2206\u2016\u03c3 \u2264 \u221a \u03b5\nlogn}. The size of \u0393\u03b3 is bounded above by O( ( t \u221a d logn \u03b5 )d ). It is sufficient to set log n = O(d log(d/\u03b5)) and\nthus t = O( 1 \u03b52 d log(d\u03b5 1 \u03b4 )) so that relative error holds over the \u03b3-net and the \u03b3-urchin simultaneously, which imply (10) and (9), with probability at least 1\u2212 \u03b4.\nCorollary 5.2. If t = \u2126( 1\u03b52d log( d \u03b5 1 \u03b4 )), then for all \u2206 such that \u2016\u2206\u2016 \u2264 1,\nD K\u0302 (\u2206)\nDK(\u2206) \u2208 [1 \u2212 \u03b5, 1 + \u03b5]\nwith probability at least 1\u2212 \u03b4. Proof. Consider the region 1 \u2265 \u2016\u2206\u2016 > \u221a \u03b5 logn . The Lipschitz constant is bounded above by O(t \u221a d log n) by Corollary 5.1, so we only need a \u03b3-net where \u03b3 \u2264 \u03b52 t \u221a d logn to give relative error by standard net arguments. The size of this net is at most ( t \u221a d logn \u03b52 )d , so again it suffices to set log n = O(d log(d\u03b5 )) and t = O( 1\u03b52d log( d \u03b5 1 \u03b4 )) for our embeddings as above.\nCombined with Lemma 2.2 for \u2016\u2206\u2016 > 1 we obtain:\nTheorem 5.1. If t = \u2126 ( d \u03b52 log ( d \u03b5 M \u03b4 )) , then for any M \u2265 0, DK\u0302(x,y) 2 DK(x,y)2 \u2208 [1\u2212 \u03b5, 1 + \u03b5] holds for all x, y \u2208 Rd such that \u2016x\u2212 y\u2016/\u03c3 \u2264 M with probability at least 1\u2212 \u03b4.\nProof. Set t = \u2126( 1 \u03b52 d log(d\u03b5 1 \u03b4 )) + \u2126( 1 \u03b52 d log(d\u03b5 M \u03b4 )) = \u2126 ( d \u03b52 log ( d \u03b5 M \u03b4 )) to account for both cases \u2016\u2206\u2016 = \u2016x\u2212y\u2016\u03c3 \u2264 1 and 1 \u2264 \u2016x\u2212y\u2016 \u03c3 \u2264 M, respectively."}, {"heading": "6 Lower Bounds for Low Dimensions", "text": "When is n is unbounded, a recent paper [18] implies that, even for small d, DK\u0302 cannot (1 + \u03b5)approximate DK unless M is bounded. Here we provide an explicit and general lower bound depending on M and d that matches the our upper bound up to a O(log 1\u03b5 ) factor.\nFirst we need the following general result ([2] Theorem 9.3) related to embedding to \u21132. Let B be an n\u00d7 n real matrix with bi,i = 1 for all i and |bi,j | \u2264 \u03b5 for all i 6= j. If the rank of B is r, and 1\u221a n < \u03b5 < 1/2, then r \u2265 \u2126( 1 \u03b52 log(1/\u03b5) log n). Geometrically, r is the minimum number of dimensions that can contain a set of n near-orthogonal vectors. Indeed, any set S of n near-orthogonal vectors can be rotated to form the rows of a matrix of the form of B, and the rank is then the lowest number of dimensions that contain S.\nLemma 6.1. Given M \u2265 0, let BM(0) be the ball in Rd centered at the origin with radius M. Let h : Rd \u2192 Rt be a mapping such that for any x 6= y \u2208 BM(0) we have |K(x, y)\u2212h(x) \u00b7h(y)| \u2264 \u03b5 \u2264 14 . Then with sufficiently large M, t = \u2126( d\u03b52 log(1/\u03b5) log( Mlog(1/\u03b5))).\nProof. Consider a subset S \u2282 Rd in BM(0) so for all x, y \u2208 S, with x 6= y, we have \u2016x \u2212 y\u2016 \u2265 \u03c3 \u221a\n2 log 1\u03b5 . Then for any x, y \u2208 S, K(x, y) = exp(\u2212 \u2016x\u2212y\u20162 2\u03c32 ) \u2264 \u03b5. In particular, define S as the inter-\nsection of BM(0) with an orthogonal grid of side length \u03c3 \u221a 2 log(1/\u03b5); it has size \u2126\n(\n(\nM log(1/\u03b5)\n)d )\n.\nFor any x, y \u2208 S, |h(x) \u00b7h(y)| \u2264 2\u03b5, and also |{h(s) | s \u2208 S}| = |S|. Then [2] Theorem 9.3 implies the dimension of h must be t = \u2126( 1\u03b52 log(1/\u03b5) log |S|) = \u2126( d\u03b52 log(1/\u03b5) log( Mlog(1/\u03b5))).\nTheorem 6.1. Given M \u2265 0, let BM(0) be the ball in Rd centered at the origin with radius M. Let h : Rd \u2192 Rt be a mapping such that for any x, y \u2208 BM(0) we have 1 \u2212 \u03b5 \u2264 DK(x,y)\u2016h(x)\u2212h(y)\u2016 \u2264 1 + \u03b5 with \u03b5 \u2264 14 . Restrict that for any x \u2208 Rd that \u2016h(x)\u2016 = 1. If M is sufficiently large, t = \u2126( d\u03b52 log(1/\u03b5) log( M log(1/\u03b5))).\nProof. Consider a set (as in proof of Lemma 6.1) S \u2282 BM(0). If for all x, y \u2208 S we have 1 \u2212 \u03b5 \u2264 DK(x,y)\n\u2016h(x)\u2212h(y)\u2016 \u2264 1 + \u03b5, then it implies\n|DK(x, y)2 \u2212 \u2016h(x) \u2212 h(y)\u20162| \u2264 \u0398(\u03b5)DK(x, y)2 \u2264 \u0398(\u03b5),\nsince DK(x, y) < 2. Expanding DK(x, y) 2 = 2 \u2212 2K(x, y) and \u2016h(x) \u2212 h(y)\u20162 = 2 \u2212 2\u3008h(x), h(y)\u3009 implies that |K(x, y)\u2212\u3008h(x), h(y)\u3009| \u2264 \u0398(\u03b5) as well. However, Lemma 6.1 implies that for sufficiently small \u03b5 (adjusting the constant in \u0398(\u03b5)) that we require the t = \u2126( d\u03b52 log(1/\u03b5) log( M log(1/\u03b5))).\nThis implies the impossibility of fully embedding into \u21132 the Gaussian kernel distance over the entire Rd, i.e. for an infinite number of points, answering a question raised by [18]. This argument can also extend to show a dependency on d logM is inevitable when we do not have a bound on n."}, {"heading": "7 Empirical Demonstration of Relative Error", "text": "We demonstrate that relative error actually results from the \u03c6\u0302 kernel embeddings in two ways. First we demonstrate relative error bounds for kernel PCA. Second we show this explicitly for pairwise distances in the embedding."}, {"heading": "7.1 Relative Error for Kernel PCA", "text": "We consider two ways of running kernel PCA on the USPS data. By default we use the first n = 2000 data points in Rd for d = 256, the first n/10 data points of each digit. In the first way, we create the n\u00d7n (centered) gram matrix G of all inner products, and then use the top k eigenvectors to describe the best subspace of RKHS to represent the data; this is treated as a baseline. Second we embed each point into Rm using \u03c6\u0302, generating an n\u00d7m matrix Q (after centering). The top k right singular values Vk of Q describe the kernel PCA subspace.\nError in PCA is typically measured as the sum of squared residuals, that is for each point q \u2208 Q \u2282 Rm, its projection onto Vk is V Tk Vkq, and its residual is rq = \u2016q \u2212 V Tk Vkq\u20162. Thus rq is precisely the squared kernel distance between q and its projection. And then the full error is R\u0302k = \u2016Q\u2212 V Tk VkQ\u20162F = \u2211\nq\u2208Q \u2016q \u2212 V Tk Vkq\u20162. For the non-approximate case, it can be calculated as the sum of eigenvalues in the tail Rk = \u2211n i=k+1 \u03bbi.\nGiven Rk and R\u0302k we can measure the relative error as R\u0302k/Rk. Our analysis indicates this should be in [1 \u2212 \u03b5, 1 + \u03b5] using roughly t = C/\u03b52 features, where C depends on n or d logM. To isolate \u03b5 we calculate | R\u0302kRk \u2212 1| averaged over 10 trials in the randomness in \u03c6\u0302. This is shown in Figure 1 using k = 40, with \u03c3 \u2208 {4, 8, 16} and varying t \u2208 {50, 100, 200, 400, 800}. We observe that our measured error decreases quadratically in t as expected. Moreover, this rate is stable as a function of \u03c3 as would be expected where the correct way to quantify error is the relative error we measure.\nPairwise Demonstrations of Relative Error Here we provide simulations that confirm our theoretical findings. We randomly generate pairs of points (x1, y1) . . . (xn, yn) with varying \u21132 distance \u2016xi \u2212 yi\u2016; in particular, xi is a random point in a ball or radius 500 and yi is generated to be a random point in the sphere \u2016x \u2212 y\u2016 = ri where r1, ..., rn follow a geometric distribution, ranging from approximately 10\u22124 to 104.\nIn Figure 2(left), for different values of t (the number of features) we generate a fresh sequence\nof 2000 random pairs, and record the maximum relative error \u03b5max = maxi DK(xi,yi)\n\u2016\u03c6(xi)\u2212\u03c6(yi)\u2016 . The graph\nshows that t is roughly proportional to \u03b5\u22122max. In Figure 2(right), we examine the relative errors for all the random pairs at a wide range of \u21132 norms, for t = 100 and t = 1000. A slight change in the error profile occurs within \u2016xi \u2212 yi\u2016/\u03c3 \u2208 [100, 101], coinciding with the separation of cases \u2016x \u2212 y\u2016 \u2264 \u03c3 and \u2016x \u2212 y\u2016 > \u03c3 i.e. whether \u2016x\u2212y\u2016\n\u03c3 = \u0398(1) in the analyses. In either case, the relative error is bounded by a small constant value, even when \u2016xi \u2212 yi\u2016 is several magnitudes smaller than 1, demonstrating that the extremely high concentration of the RFF for very small \u2016xi \u2212 yi\u2016 results in relative error approximation for the Gaussian kernel distance.\nConclusion: We demonstrate theoretically and empirically tight relative error for kernel distance using random Fourier features, indicating tighter approximations for important learning applications.\n0 200 400 600 800 1000 Number of Features\n0\n50\n100\n150 200 1/ (M a x im u m R e la t iv e E r r o r )2\n10\u22124 10\u22123 10\u22122 10\u22121 100 101 102 103 104\nOriginal \u21132 norm \u2016x\u2212 y\u2016\n\u22120.2\n\u22120.1\n0.0\n0.1\n0.2\nR e la\nti v e\nE rr\no r:\nD K\u0302 (x ,y )/ D\nK (x ,y ) \u2212 1\nRelative Error for Small to Large Distances\n#features=1000 #features=100"}], "references": [{"title": "Sketching, embedding, and dimensionality reduction for information spaces", "author": ["Amirali Abdullah", "Ravi Kumar", "Andrew McGregor", "Sergei Vassilvitskii", "Suresh Venkatasubramanian"], "venue": "AIStats,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Problems and results in extremal combinatorics-i", "author": ["Noga Alon"], "venue": "Discrete Math.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Subspace embeddings for the polynomial kernel", "author": ["Haim Avron", "Huy L. Nguyen", "David P. Woodruff"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Adaptive estimation of a quadratic functional by model selection", "author": ["P. Massart B. Laurent"], "venue": "The Annals of Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Metric characterization of random variables and random processes. Translations of mathematical monographs", "author": ["Valerij Vladimirovi\u010d Buldygin", "IU.V. Kozachenko", "V. Zaiats"], "venue": "Providence, R.I. American Mathematical Society,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Compact random feature maps", "author": ["Raffay Hamid", "Ying Xiao", "Alex Gittens", "Dennis DeCoste"], "venue": "arXiv preprint arXiv:1312.4626,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Extensions of Lipschitz maps into a Hilbert space", "author": ["William B. Johnson", "Joram Lindenstrauss"], "venue": "Contemporary Mathematics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1984}, {"title": "Random feature maps for dot product kernels", "author": ["Purushottam Kar", "Harish Karnick"], "venue": "arXiv preprint arXiv:1201.6530,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "The johnson-lindenstrauss lemma is optimal for linear dimensionality reduction", "author": ["Kasper Green Larsen", "Jelani Nelson"], "venue": "CoRR, abs/1411.2404,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Random fourier approximations for skewed multiplicative histogram kernels", "author": ["Fuxin Li", "Catalin Ionescu", "Cristian Sminchisescu"], "venue": "In Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Randomized nonlinear component analysis", "author": ["David Lopez-Paz", "Suvrit Sra", "Alex Smola", "Zoubin Ghahramani", "Bernhard Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1402.0119,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Max-margin additive classifiers for detection", "author": ["Subhransu Maji", "Alexander C Berg"], "venue": "In Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Integral probability metrics and their generating classes of functions", "author": ["Alfred M\u00fcller"], "venue": "Advances in Applied Probability,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Optimality of the johnson-lindenstrauss lemma", "author": ["Jelani Nelson", "Kasper Green Larsen"], "venue": "Technical Report 1609.02094,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Geomtric inference on kernel density estimates", "author": ["Jeff M. Phillips", "Bei Wang", "Yan Zheng"], "venue": "In SOCG,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Hilbert space embeddings and metrics on probability measures", "author": ["Bharath K. Sriperumbudur", "Arthur Gretton", "Kenji Fukumizu", "Bernhard Sch\u00f6lkopf", "Gert R.G. Lanckriet"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Optimal rates for random fourier features", "author": ["Bharath K. Sriperumbudur", "Zoltan Szabo"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "On the error of random fourier features", "author": ["Dougal J. Sutherland", "Jeff Schneider"], "venue": "In UAI,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "For reproducing kernels (actually a slightly smaller set called characteristic kernels) this is a metric [17, 13].", "startOffset": 105, "endOffset": 113}, {"referenceID": 12, "context": "For reproducing kernels (actually a slightly smaller set called characteristic kernels) this is a metric [17, 13].", "startOffset": 105, "endOffset": 113}, {"referenceID": 6, "context": "It turns out, one can always construct such a lifting with m = O((1/\u03b52) log(n/\u03b4)) by the famous Johnson-Lindenstrauss (JL) Lemma [7].", "startOffset": 129, "endOffset": 132}, {"referenceID": 15, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 165, "endOffset": 181}, {"referenceID": 10, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 165, "endOffset": 181}, {"referenceID": 17, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 165, "endOffset": 181}, {"referenceID": 18, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 165, "endOffset": 181}, {"referenceID": 9, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 206, "endOffset": 210}, {"referenceID": 11, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 237, "endOffset": 241}, {"referenceID": 7, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 263, "endOffset": 266}, {"referenceID": 0, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 287, "endOffset": 290}, {"referenceID": 5, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 315, "endOffset": 321}, {"referenceID": 2, "context": "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].", "startOffset": 315, "endOffset": 321}, {"referenceID": 15, "context": "In this document we reanalyze one of the most widely used and first variants, the Random Fourier Features, introduced by [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 15, "context": "1 Existing Properties of Gaussian Kernel Embeddings [16] defined two approximate embedding functions: \u03c6\u0303 : Rd \u2192 Rm and \u03c6\u0302 : Rd \u2192 Rm.", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "Only the former appears in the final version of paper, but the latter is also commonly used throughout the literature [19].", "startOffset": 118, "endOffset": 122}, {"referenceID": 17, "context": "Recently [18] tightened the above asymptotic bounds to show actual constants.", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "It is folklore (apparently removed from final version of [16]; reproved in Section 2) that also E[\u03c6\u0302(x)T \u03c6\u0302(y)] = K(x, y), and thus all of the above PAC bounds hold for \u03c6\u0302 as well.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "Also recently, [19] compared \u03c6\u0303 and \u03c6\u0302 (they used symbol \u03c6\u0306 in place of our symbol \u03c6\u0302), and demonstrated that \u03c6\u0302 performs better (for the same m) and has provably lower variance in approximating K(x, y) with \u03c6\u0302(x)T \u03c6\u0302(y) as opposed to with \u03c6\u0303(x)T \u03c6\u0303(y).", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "Our results to do not describe unrestricted OSEs, as are possible with polynomial kernels [3].", "startOffset": 90, "endOffset": 93}, {"referenceID": 15, "context": "An earlier version of Rahimi-Recht [16] seemed to prove the following lemma.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "As a warm up, and as was similarly observed in [15], a simple Taylor expansion when \u2016\u2206\u2016 \u2264 1, implies that \u2016\u2206\u2016 \u2212 1 4 \u2016\u2206\u2016 \u2264 DK(\u2206) = 2\u2212 2 exp(\u2016\u2206\u2016/2) \u2264 \u2016\u2206\u2016, and by 1 4\u2016\u2206\u2016 \u2264 4\u2016\u2206\u2016 and a square root 0.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "Here we use Lemma 1 from [4]; if X is a \u03c72 random variable with t degrees of freedom Pr[t\u2212 2 \u221a tx \u2264 X \u2264 t+ 2 \u221a tx+ 2x] \u2265 1\u2212 2e\u2212x.", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "For \u03b5 \u2208 (0, 1) and \u03b4 \u2208 (0, 1/2), if \u2016\u2206\u2016 \u2264 \u221a \u03b5 log(1/\u03b4) , and t = \u03a9( 1 \u03b5 log(1/\u03b4)), then with probability at least 1\u2212O(\u03b4), for all \u03bb \u2208 [0, 1] we have DK\u0302(\u03bb\u00b7\u2206) 2 DK(\u03bb\u00b7\u2206)2 \u2208 [1\u2212 \u03b5, 1 + \u03b5].", "startOffset": 134, "endOffset": 140}, {"referenceID": 0, "context": "as long as \u03bb \u2208 [0, 1].", "startOffset": 15, "endOffset": 21}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Remark: A new result of Larsen and Nelson [14] provides a t = \u03a9( 1 \u03b5 log n) lower bound for even non-linear embeddings of a size n point set in Rd into Rt that preserve distances within (1 \u00b1 \u03b5).", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "However, it is not clear that any point set (including the ones used in the strong lower bound proof [14]), can result from an isomorphic (or approximate) embedding of RKHS into Rn.", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "We next combine this result with an existing bound on sub-exponential random variables [5](Lemma 4.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "(10) Since this holds for all \u03bb \u2208 [0, 1], we obtain relative error bounds over {\u2206 : \u2016\u2206\u2016 \u03c3 \u2264 \u221a \u03b5 logn}.", "startOffset": 34, "endOffset": 40}, {"referenceID": 17, "context": "6 Lower Bounds for Low Dimensions When is n is unbounded, a recent paper [18] implies that, even for small d, DK\u0302 cannot (1 + \u03b5)approximate DK unless M is bounded.", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "First we need the following general result ([2] Theorem 9.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "Then [2] Theorem 9.", "startOffset": 5, "endOffset": 8}, {"referenceID": 17, "context": "for an infinite number of points, answering a question raised by [18].", "startOffset": 65, "endOffset": 69}], "year": 2016, "abstractText": "A reproducing kernel defines an embedding of a data point into an infinite dimensional reproducing kernel Hilbert space (RKHS). The norm in this space describes a distance, which we call the kernel distance. The random Fourier features (of Rahimi and Recht) describe an oblivious approximate mapping into finite dimensional Euclidean space that behaves similar to the RKHS. We show in this paper that for the Gaussian kernel the Euclidean norm between these mapped to features has (1 + \u03b5)-relative error with respect to the kernel distance. When there are n data points, we show that O((1/\u03b5) logn) dimensions of the approximate feature space are sufficient and necessary. Without a bound on n, but when the original points lie in R and have diameter bounded by M, then we show that O((d/\u03b5) logM) dimensions are sufficient, and that this many are required, up to log(1/\u03b5) factors. We empirically confirm that relative error is indeed preserved for kernel PCA using these approximate feature maps.", "creator": "LaTeX with hyperref package"}}}