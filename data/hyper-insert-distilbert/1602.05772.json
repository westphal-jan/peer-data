{"id": "1602.05772", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2016", "title": "Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure", "abstract": "when looking at the item structure problem of natural language, \" fluent phrases \" values and \" words \" are central notions., we still consider the critical problem field of separately identifying a such \" meaningful subparts \" of language of any length and recognizing underlying expressive composition function principles performed in a coherent completely corpus - based and distinct language - meaning independent way without using any kind inference of prior linguistic modeling knowledge. unsupervised methods for quickly identifying \" phrases \", mining substantive subphrase sentence structure and for finding words in not a fully synthetic automated retrieval way are described. this can be seriously considered as a new step towards automatically computing a \" general simple dictionary sequence and grammar of surveying the human corpus \". we hope that attempts in accelerating the long run systematic variants of our literary approach turn yielding out something to inevitably be useful formats for making other kind organisms of data sequence finding data as well, variables such as, e. g., oral speech, genom coding sequences, or music content annotation. even just if again we are not truly primarily interested in immediate analytical applications, results obtained for a variety of elementary languages may show that so our methods are interesting for many seemingly practical tasks in text mining, terminology for extraction and lexicography, functional search engine technology, robotics and related information fields.", "histories": [["v1", "Thu, 18 Feb 2016 12:08:05 GMT  (679kb,D)", "http://arxiv.org/abs/1602.05772v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stefan gerdjikov", "klaus u schulz"], "accepted": false, "id": "1602.05772"}, "pdf": {"name": "1602.05772.pdf", "metadata": {"source": "CRF", "title": "Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure", "authors": ["Stefan Gerdjikov", "Klaus U. Schulz"], "emails": ["gerdjikov@abv.bg", "schulz@cis.uni-muenchen.de"], "sections": [{"heading": "1 Introduction", "text": "While there is no complete agreement on what the structural subparts of natural language exactly are, there is a general understanding that natural language is built \u2217st gerdjikov@abv.bg \u2020schulz@cis.uni-muenchen.de\nar X\niv :1\n60 2.\n05 77\n2v 1\n[ cs\nin a compositional way using characteristic classes of substrings (or utterances) such as phrases, words, and morphems. In natural language processing (NLP) and related fields, all these units play an important practical role. Phrases are used, e.g., in Statistical Machine Translation (SMT) [ON04, Koe10], Text Classification [Seb01, DIK11], and Information Retrieval [MRS08, Lui07, ZLC+13]. Words (\u201ctokens\u201d and \u201ctypes\u201d) represent the basis for text indexing approaches in Information Retrieval, and since centuries linguists collect words in general and specific lexica, including typical contexts and collocations. Morphems carry information on part-of-speech, syntax and semantic roles [Lui07].\nIn this paper we present results from an ongoing research project which is centered around the problem of how to detect structural subparts of natural (or other) language and is characterized by the following methodological principles.\n1. Try to detect meaningful units of any length, as opposed to, e.g., fixed length n-grams of words or letters.\n2. Only use corpora (or sequence data) as an empirical basis to find these units.\n3. When analyzing corpora, do not use any kind of prior linguistic knowledge and no knowledge on the nature/functionality of a particular symbol (such as blank, hyphen, brackets, uppercase vs. lowercase etc.) but rather simple mathematical assumptions.\n4. Proceed in a completely unsupervised way.\nWhile we acknowledge the success of NLP approaches based on n-gram statistics, it is clear that using units of fixed length does not naturally lead to meaningful subparts. In anatomic terms, our aim here is to find the \u201cmuscles, organs, and nerves\u201d of a given language corpus. The other above principles are related in the sense that they stress purely empirical evidence. Corpora, from our point, are just raw sequences of symbols. For analyzing corpora we do not use prior linguistic knowledge since we want to see what \u201cthe corpus structure itself tells us about language\u201d. This might help to compare existing linguistic knowledge and hypotheses with \u201cpurely empirical\u201d corpus-driven evidence. Furthermore, principles and methods obtained from this puristic approach can be applied to any (not necessarily natural) language and might contribute to the analysis of other kind of sequence data such as biological sequences, sequence of gestures, animal language, music annotation, and others. We avoid any kind of supervision since we are not primarily interested in the path from prior human linguistic knowledge to the analysis of data, but rather on the question how structure can be detected and then \u201cdestilled\u201d to knowledge. The focus of this paper is the mining of phrases, subphrase\nstructure, and words. From a practical side, one vision is to automatically and \u201conline\u201d create a kind of \u201cgeneral dictionary and grammar of a given corpus\u201d that collects, connects and relates all interesting language units of the corpus, with a direct reference to all occurrences. In the Conclusion we add a remark on possible interaction of our approach with unsupervised morphology induction approaches, such as [Gol01, Zem08, CL05, SO15].\nAs a matter of fact, this programmatic approach is by no means new. As an early influential advocate, Zellig Harris [Har54, Har91] has stressed that corpus analysis based on mathematical principles should be used to derive empirical evidence for linguistic hypotheses. Later, Maurice Gross [Gro93, Gro99], Franz Guenthner [Gue05], Johannes Goller [Gol10] and others followed this line. Corpus analysis is often used to find lexico-semantic relations [Sto10]. Further related work is described in Section 6. One algorithmic contribution of our work is the use of a special index structure for corpora as a basis: symmetric directed acyclic word graphs (SDAWGs) [IHS+01] represent a generalization of suffix trees [Ukk95]. Using the SDAWG index of a corpus, for any substring of the corpus we have immediate access to all left and right contexts of arbitrary length. In this way we see \u201chow\u201d a substring occurs in the corpus (i.e., in which contexts and larger sequences, how often), which provides an excellent basis for recognizing meaningful units and connected tasks.\nWhen asking for the composition of natural language in terms of meaningful subparts, a fundamental question is how small parts are combined to form larger units. Most grammar approaches (e.g., context-free grammars) are \u201cconcatenation based\u201d in the sense that always disjoint, non-overlapping parts are glued together to build larger units. In contrast, our base assumption is that in general \u201cphrases\u201d can overlap, and combination principles go beyond plain concatenation.\nOur base view of language composition is briefly explained in Section 2. Using this view, Section 3 introduces mathematical principles and an algorithm for recognizing meaningful units and decomposing sentences into \u201cphrases\u201d. We illustrate the sentence decompositions and \u201cphrases\u201d obtained for a variety of distinct languages (English, German, French, Spanish, Dutch, Chinese) and corpora (including Wikipedia, Europarl, Medline). Section 4 introduces principles for further decomposing \u201cphrases\u201d. The base units found in this way (in most cases) are \u201cwords\u201d. Recall that we start from a point where we do not have the concept of a \u201cword\u201d. Again we presents results obtained for distinct corpora and languages. In Section 5 we discuss some possible applications of the phrase detection mechanisms: first we show how subphrase structure can be used to find a ranked spectrum of \u201cmost characteristic words\u201d of the given corpus, and to extract paradigmatic word networks based on co-occurrence of terms in phrases. We also look at stylis-\ntic corpus analysis, terminology extraction, and automated query expansion for search engines. In Section 6 we briefly comment on related work. Section 7 gives a conclusion and comments on future work."}, {"heading": "2 Phrase model - intuitions and motivation", "text": "As a starting point we look at the notion of a \u201cphrase\u201d. When we want to recognize \u201cphrases\u201d in a corpus-driven way we need as a starting point some primitive base assumptions how \u201cmeaningful phrases\u201d can be recognized and distinguished from arbitrary, \u201cmeaningless subsegments\u201d of language. In our view, recognition of phrases should proceed in parallel with the recognition of those language units that connect and interlink phrases. Our main motivation comes from psycholinguistics [HW03, SWC06, Tra07]: Young children recognise function words but do not use them in the beginning. This suggests that there are some natural principles that govern the structure of natural language that are strongly related to the use of function words. Children are able to detect these regularities, and to distinguish between language parts that serve to combine and connect meaningful parts - function words - on the one-hand side, and meaningful parts - content words and phrases - on the other side.\nFunction words, or closed-class words, alone bear little sense, [Fri52]. Their main function is to bind other words or expressions in a grammatically correct way to form more complex meaningful language units. To suit these needs of the language, only a few function words, e.g. prepositions, articles, suffice. Thus, although some specific word may act as a function word in a special domain, generally the set of function words is stable in every particular language.\nIn our situation we do not assume that we know the set of function words, and the notion of a \u201cword\u201d is partially misleading: we do not assume that \u201cphrase connectives\u201d are always words, we even do not have a notion of \u201cword\u201d at the beginning. Yet, for convenience, we say \u201cfunction word\u201d when we mean \u201cphrase connective\u201d. We assume that the corpus consists of sentences1 which make sense. The remaining assumptions we make about phrases and function words in the corpus are:\n(A1) Phrases occur in different contexts.\n(A2) Sentences are built of (overlapping) phrases.\n(A3) Phrases overlap on function words. 1Other small meaningful text units like paragraphs also conform with our model.\nThe first assumption means that the corpus provides structural evidence that helps to recognize phrases. The circumstance that the corpus consists of sentences that make sense is important for the second and third assumptions2. Assumption (A2) follows the intuition that the sense of a sentence is determined by the sense of the individual phrases and the way they are composed, whereas the final assumption reflects the fact that function words bind smaller meaningful language units to build larger ones. In our case, the smaller meaningful language units are the phrases we are looking for, and the larger ones are the sentences in the corpus.\nSince function words alone, to a large extent, are lacking sense, we can fairly approximate the sense of the sentence with the sense accumulated by the individual phrases. Thus, we do not need to bother of modelling what sense is, rather we will be concerned with the issue how regular w.r.t. the given corpus the bindings between the phrases are. We try to summarise this intuition in the following two principles:\n(P1) The function words occur regularly as the boundaries of phrases in the corpus.\n(P2) A particular sentence should exhibit these regularities as good as its structure w.r.t. the corpus allows."}, {"heading": "3 Computing phrases and function words", "text": "Following the considerations of the previous section, we first look at the problem of how to compute the phrases and function words of a corpus. In the first subsection we develop an algorithm to solve this task. Afterwards we present the results obtained for distinct languages and corpora."}, {"heading": "3.1 Algorithmic principles", "text": "We consider a corpus, C, consisting of a finite number of sentences. The individual sentences are merely nonempty strings Sn \u2208 \u03a3+ over a finite alphabet \u03a3. In practice, the alphabet \u03a3 is the set of all the symbols, i.e. letters, punctuation, white space, etc., that were used to create the corpus. According to Assumption (A1), the phrases are among those strings S \u2208 \u03a3\u2217 that occur in different contexts within the corpus C. Formally, for a phrase S the following two properties must hold:\n2However, this is not a prerequisite for the mathematical approach described in the next section.\n1. there are distinct a, b \u2208 \u03a3 such that aS and bS occur in C, or S is a prefix of some of the sentences Sn, and\n2. there are distinct c, d \u2208 \u03a3 such that Sc and Sd occur in C, or S is an suffix of some of the sentences Sn.\nStrings with the above two properties are called general phrase candidates of the corpus, the set of general phrase candidates is denoted G(C). For example, if the strings unfortunate and fortunately occur as part of some sentences in the corpus3, then they witness that fortunate can be followed by at least two distinct letters, l and , and similarly it can be preceded by at least two distinct letters, n and . Therefore fortunate is a general phrase candidate in this case. However, if it turned out that the string ortuna is always preceded by the letter f in the corpus, then the string ortuna will be not a member of G(C). The same would be true if ortuna was always followed by a t in the corpus.\nAccording to (A2), a sentence Sn in the corpus can be decomposed into a sequence of nonempty strings (P1, P2, . . . , Pkn) with kn > 1 such that Pi \u2208 G(C). For instance, if a sentence starts like\nWe are not making the connection ...\nit may be the case that P1 = We are not , P2 = are not making the , P3 = the connection. In this case P1 and P2 overlap on the string F1 = are not , whereas P2 and P3 overlap on the string F2 = the . In general, Pi and Pi+1 overlap on a common possibly empty string Fi \u2208 \u03a3\u2217, i.e. Pi = P \u2032i \u25e6 Fi and Pi+1 = Fi \u25e6 P \u2032\u2032i+1, such that:\nSn = P1 \u25e6 F\u221211 \u25e6 P2 \u25e6 F \u22121 2 \u00b7 \u00b7 \u00b7 \u25e6 F \u22121 kn\u22121 \u25e6 Pkn .\nWe call the Fi\u2019s function strings. Upon termination of our approach will declare some of these strings to be function words. Recall that if Pi, Pi+1 are in the set of general phrase candidates G(C), then each suffix of Pi satisfies Property 2 and each prefix of Pi+1 satisfies Property 1. In particular, since Fi is a suffix of Pi and Fi is a prefix of Pi+1, it follows that each function string Fi is always a general phrase candidate.\nIn general, there can be many decompositions, (P1, P2, . . . , Pkn), for a given sentence Sn. According to Principle (P2) we should select the one(s) that exhibit(s) the most regular sequence of function strings, (F1, F2, . . . , Fkn\u22121). Further, according to Principle (P1), the function strings should occur regularly as the boundaries of phrases. We will incorporate these two principles in terms of appropriate probability and likelihood measures.\n3We use the symbol \u201c \u201d to make a white space explicit.\nTo do so, let us assume for a moment that we have already determined a multiset of phrases P \u2286 G(C). Based on (P1) we are going to assess the property of a function string F to be a function word in the corpus. Next, based on (P2), we are going to determine the best decomposition of each sentence in the corpus as a sequence of general phrase candidates. This second step will allow us to refine (redefine) the multiset of phrases we have started with. We can then iterate this process until the multiset of phrases stabilizes. Based on the obtained probabilities and the eventual sentence decompositions we define the ultimate set of function words and phrases.\nFormally, to assess the property of F to be a function string w.r.t. the current multiset of phrases P , we simply consider how often F occurs as a proper prefix, suffix or proper infix in P:\npref(F |P) = |{F \u25e6 S \u2208 P |S \u2208 \u03a3+}| suff(F |P) = |{S \u25e6 F \u2208 P |S \u2208 \u03a3+}|\ninf(F |P) = |{S1FS2 \u2208 P \\ {F} | (S1, S2) \u2208 (\u03a3\u2217)2}|.\nThis statistics accounts for multiplicities. For example, if F = the and the phrase P = the connection occurs 12 times in P , then P will contribute with 12 units to pref( the |P). Now, we define the probability of a string F \u2208 G(C) to be a function word w.r.t. to a given multiset P \u2286 G(C) as the empirical probability that F is both a proper prefix and a proper suffix of a string in P:\npfw(F |P) = ppref (F |P) \u00b7 psuf (F |P) (1)\nwhere\nppref (F |P) = pref(F |P) inf(F |P) , psuf (F |P) = suff(F |P) inf(F |P) .\nTo model (P2) we optimize the likelihood w.r.t. pfw(.|P). This means, that given a sentence Sn \u2208 C we search for those sequences of strings (P1, P2, . . . , Pkn) (kn > 1) whose sequence of overlaps F = (F1, F2, . . . , Fkn\u22121) maximises the function:\n`(F |Sn,P, C) = kn\u22121\u220f i=1 pfw(Fi|P). (2)\nDetermining the maxima of Equation 2 supplies us with (an) optimal decomposition(s) (P1, P2, . . . , Pkn) for each sentence Sn in the corpus C. We call a substring Sn[i..j] of Sn an optimal phrase candidate for Sn if it belongs to some optimal decomposition for Sn. We call the overlap of two consecutive strings in an optimal decomposition for Sn an optimal overlap. Now, we are in a position to reconsider\nthe multiset of phrases P we have started with. The intuition is, that if we have made a reasonable even if not perfect guess for the set of phrases in the first step, then we should have obtained good scores for the typical function words. Thus, in the second step, optimising the likelihood across the sentences the function words will guide us to optimal strings which better describe the proper phrases in C. Following these considerations we define the new multisets\nF(P) := {Sn[j1..j2] |Sn[j1..j2] optimal overlap in a sentence Sn} P \u2032 := {Sn[j1..j2] |Sn[j1..j2] optimal phrase candidate for a sentence Sn}.\nAlthough the approach described above reflects principles (A1) - (A3) as well as (P1) and (P2), there is one subtle point that it overlooks. The optimisation of the likelihood implicitly struggles for shorter sequences of phrases. For instance, if\nP1 = are not making the\nP2 = the connections"}, {"heading": "P = are not making the connections", "text": "are three general phrase candidates, the optimisation of Equation 2 in situations like We are not making the connections... will always prefer the longer phrase, P , rather than the two shorter overlapping phrases P1 and P2. Thus, P = are not making the connections contributes to the count inf( the |P), but it will not contribute to the counts pref( the |P) and suff( the |P). This is a problem, because if are not making the and the connections are indeed phrases, then the presence of the phrase\nare not making the connections\nwitnesses that the acts as binding element in this case. However, this example will contribute to decreasing instead of increasing the probability pfw( the |P).\nTo remedy this inconsistency in the model, we take additional care for the cases described above and reflect them in the probability measure for a function word. Specifically, we define the set of stable prefix (suffix) phrases of a phrase: let P \u2208 P . A prefix (suffix) P \u2032 of P is a stable prefix (suffix) phrase of P w.r.t. the the set of phrases P iff two conditions are satisfied:\n1. P \u2032 is a phrase in P ,\n2. every prefix (suffix) P \u2032\u2032 of P of length |P \u2032\u2032| > |P \u2032| which is a general phrase candidate is again a phrase in P .\nBy SPref(P |P) and SSuff(P |P) we respectively denote the set of all stable prefix (suffix) phrases of phrase P . Now, we account for the inconsistency of the model described above in the following way. We consider phrases P \u2208 P which can be represented as two overlapping phrases P1, P2 \u2208 P but at least one of them should be stable. This means that either P1 is a stable prefix phrase of P , or P2 is a stable suffix phrase of P . The overlap value ov(F |P) of a function string F w.r.t. the set of phrases P is defined as the number of phrases, P , in the from P1 \u25e6F\u22121 \u25e6P2 \u2208 P where (P1, P2) \u2208 P2 such that P1 \u2208 SPref(P ) or P2 \u2208 SSuff(P ). As before we account for possible multiplicities P in the set P . The overlap value ov(F |P) is used to boost the empirical probabilities ppref (F |P) and psuf (F |P):\np\u0303pref (F |P) = ppref (F |P) + \u03b7pref (F |P) ov(F |P) inf(F |P)\np\u0303suf (F |P) = psuf (F |P) + \u03b7suf (F |P) ov(F |P) inf(F |P)\nwhere \u03b7i(F |P) = pi(F |P)/(ppref (F |P) + psuf (F |P)) for i \u2208 {pref,suf}. Essentially, the above formulae assume that the counts pref(F |P) and suff(F |P) are representative and thus the ratio pref(F |P)/suff(F |P) reliably represents the true ratio of prefix-suffix property. Yet, due to cases accounted for in ov(F |P), they are underestimated with respect to the total number of occurrences of F as an infix. This is why we distribute the amount ov(F |P) as to maximise the entropy w.r.t. the empirical pref(F |P)/suff(F |P). In the final model instead of ppref and psuf in Equation 1 we use p\u0303pref and p\u0303suf , respectively.\nAlgorithm for detecting phrases and function words. Our complete algorithm for detecting phrases and function words proceeds in the following way. 1. Initialisation. We start with the multiset of phrases P0 := G(C). The multiplicity of each string is simply the total number of occurrences of the string in the corpus. 2. Iteration. Afterwards we follow the spirit of the Maximum A Posteriori Principle, [BTS87, GJ08], and compute p\u0303(i)pref = p\u0303pref (.|Pi) and p\u0303 (i) suf = p\u0303suf (.|Pi) and define the next set of phrases as Pi+1 = P \u2032i (see above). 3. Termination. As in other similar approaches the final goal is to obtain convergence. In our case we want to arrive at a state where the parameters Pi, p\u0303pref and p\u0303suf stabilize. Since the probability functions p\u0303pref and p\u0303suf are entirely determined by the current set of phrases Pi it is enough to define a convergence criterium for the multisets Pi.\nIn order to assess the similarity between the multisets Pi and Pi+1, we use a pure set theoretical measure. Let mi(P ) denote the multiplicity of P in the set Pi,\ni.e. the number of times the general phrase candidate P is assigned to Pi. We can express the size of the symmetric difference Pi\u2206Pi+1 of the two multisets Pi and Pi+1 as\n\u2016Pi\u2206Pi+1\u2016 = \u2211 P |mi(P )\u2212mi+1(P )|.\nSimilarly we compute the size of the union Pi \u222a Pi+1 of the two multisets as \u2016Pi \u222a Pi+1\u2016 = \u2211 P max(mi(P ),mi+1(P )).\nAs a similarity measure between the multisets Pi and Pi+1 we use the relative size\n\u03c1i = \u03c1(Pi,Pi+1) = \u2016Pi\u2206Pi+1\u2016 \u2016Pi \u222a Pi+1\u2016 .\nWe iterate Step 2 until:\n[Halting Criterion] \u03c1i \u2264 \u03c1i+1 + \u03b8, where \u03b8 = 10\u22126. (3)\nWe will comment on this halting criterion at the end of this section. 4. Final model. Assume that the algorithm terminates after n+1 iterations of Step 2 when for the first time \u03c1n+1 \u2265 \u03c1n \u2212 \u03b8. We define:\nP = Pn p\u0303pref = p\u0303 (n) pref\np\u0303suf = p\u0303 (n) suf\npfw(F | P) = p\u0303pref (F |P) \u00b7 p\u0303suf (F |P).\nWe define the set of function words of the corpus, F(C), as the set of strings F \u2208 Fn that satisfy the following properties:\n\u2022 F has multiplicity at least |C|/103 in F(P) = F(Pn),\n\u2022 p\u0303pref (F |P) + p\u0303suf (F |P) > 0.4 and\n\u2022 p\u0303pref (F |P)/p\u0303suf (F |P) \u2208 (1/4; 4).\nThe first condition expresses that F not only occurs often in the corpus, but also serves as an overlap in many different cases. The second and third conditions4 capture the intuition that F is both a typical prefix and suffix boundary of a phrase, and not only, say, a typical prefix.\n4Note that the inequality p\u0303pref (F |P) + p\u0303suf (F |P) \u2264 1 is always fulfilled.\nOur experiments have shown that the particular choice of these parameters is not significant. Particularly, among the strings satisfying the above conditions almost all achieve p\u0303pref (F |P)+p\u0303suf (F |P) > 0.5 whereas the ratio between the prefix and suffix property varies in spans (1/3; 3). Additionally, typically the strings meeting these constraints have total probability at rates:\np\u0303pref (F |P)p\u0303suf (F |P) > 9/100\nwhere the possible maximum is 25/100 (attained for p\u0303pref (F |P) = p\u0303suf (F |P) = 0.5). At the same the strings which are pruned have probabilities of one magnitude less.\nWe define the set of phrases of the corpus, P(C) as those P \u2208 P that (i) start with some element of F(C) or are the beginning of a sentence and at the same time (ii) end with some element of F(C) or are the end of a sentence.\nRemarks on the Halting Criterion. Naively, one could expect that the similarity \u03c1i (normalized size of symmetric difference) between two consecutive sets of phrases converges to 0. However, in practice this does not happen The reason for this phenomenon are rare strings which in particular sentences can serve both as overlaps in an ambiguous sequence of phrases. Thus, when they achieve good scores as overlaps the number of phrases they participate increases and this makes their scores drop in the next phase. However, when they are not active the part of the phrases where they are suffixes and prefixes become optimal and they boost the probabilities of these rare strings implying a vicious circle.\nThis is confirmed in our experiments, see Figure 1, where we depict in logarithmic scale the behaviour of \u03c1i for distinct datasets. Multiset similarities \u03c1i start with values around 1 and drop very quickly to values below 0.01. Afterwards values \u03c1i slowly decrease to stabilise at rates 0.2\u00d7 10\u22123 and 0.5\u00d7 10\u22122, but they do not continue to converge to 0. Nevertheless, the multisets Pi exhibit a rather stable behaviour. In fact, more than 90% of the value \u03c1i is due to cases where the multiplicity of a phrase P is increased or decreased by just 1, |mi(P )\u2212mi+1(P )| = 1. Further the weighted signed symmetric difference between Pi and Pi+1:\n\u03b4i = \u2211 P mi(P )\u2212mi+1(P ) \u2016Pi \u222a Pi+1\u2016 .\nranges at rates |\u03b4i| \u2208 [10\u22126; 10\u22124] and below, which shows that typically a \u201csmall gain\u201d for one phrase is compensated by a \u201csmall loss\u201d of another phrase. This means, that on average, the changes between p(i+1)pref and p (i) pref (resp. p (i+1) suf and p (i) suf ) are small for function words with high probabilities. This shows that function words with high probabilities p(i)fw do not change significantly their probabilities to\np (i+1) fw . Whenever such a function word is admissible in a particular decomposition, it will be selected for a split. \u201cPersistent turbulences\u201d are caused by words occurring only once, or words occurring in a very specific syntactical contexts.\nTechnical realization. Technically, the set of all general phrase candidates G(C) can be represented after a linear preprocessing in linear space as a compressed acyclic word graph, [BBH+87, IHS+01], which generalises suffix trees, [Ukk95]. In this structure the required statistics, pref , suff , inf and ov can be easily collected by a straightforward traversal. Meanwhile, the optimisation required in Equation 2 reduces to a standard dynamic programming scheme. The graph necessary for this computation can be constructed by the means of an Aho-Corasicklike algorithm, [AC75]. Further speed-up can be achieved by an A-star algorithm, [HNR68, HNR72], built on top of the shortest path problem algorithm in proper interval graphs, [ACL93]."}, {"heading": "3.2 Experimental basis - languages and corpora", "text": "We considered corpora of different language, domain and size, see Table 1. The corpora cover six languages: English, German, French, Spanish, Dutch, and Chinese. Domains covered vary from philosophy through scientific medical abstracts, politics and journalistic articles, to general mixed topics. The size of the corpora scaled from several thousand to several Million sentences.\nWittgenstein Corpus. The Wittgenstein Corpus consists of 3871 original remarks of the German philosopher Ludwig Wittgenstein. Although the general language is German, the corpus also contains a few remarks in English.\nMedline Corpus. The Medline Corpus is a collection of about 15 Million medical abstracts in English. Each abstract consists of about 10 sentences. We considered two subsets of this corpus. They were obtained by randomly selecting 0.5% and 10%, respectively, of the abstracts and gathering all the sentences in the resulting abstracts. The 0.5%-Medline Corpus consists of 800 K sentences, whereas the 10%-Medline Corpus consists of about 15 M sentences.\nEU-Parliament Corpus. The EU-Parliament Corpus is a collection of the political statements in EU-Parliament from 1997 through 2012. The statements are maintained in all the languages of the EU-members at the date of the statement. Thus the entire corpus amounts to ca. 2 Million sentences for the languages of the long-lasting EU-members, English, German, etc. and to several hundreds of thousands for the more recent EU-members.\nFrom this corpus we selected excerpts from the 2000-2001 sessions of the EUParliament in English, German, French, Spanish and Dutch. The size of the se-\nlected corpora amounts to 20 K sentences. We also processed the entire French and Spanish corpora, amounting at about 2 Million sentences, each.\nAccountant Corpus. Accountant Corpus is a collection of the issues of the Dutch newspaper \u201cAccountant\u201d. The texts are from the period of World War I. The digital version of the corpus was obtained through OCR. We considered as sentences sequences of characters separated by at least two new lines. Single new lines were replaced by white spaces. In this way we tried to roughly reflect the natural reading order of paper articles. The obtained corpus, Accountant, contains about 500 K different sentences (of total 780 K) most of which have the structure of a paragraph or article.\nWikipedia. We considered also dumps of the English and German Wikipedias from May 14, 2014. In the English version we selected the pages AA \u2013 AG in their raw form. In the German Corpus we selected the pages AA \u2013 AH. We used WIKIPEDIAEXTRACTOR [AF13] to remove the hyperlinks. In both cases the sentences were defined as sequences of characters terminated by a new line. In this way we roughly reflect the paragraph structure of these electronic resources. The size of the English corpus amounted to 1,1 Million paragraphs of which more than 890 K different and the German Corpus resulted in about 650 K paragraphs, more than 510 K of which were distinct.\nChinese Corpus. \u201c\u4fdd\u9556\u5929\u4e0b\u201d (\u201dBodyguard Legend\u201d) is a Chinese electronic fiction which is freely available online for research, [htt]. It contains about 24K of small paragraphs or single sentences. The total number of characters is about 2M."}, {"heading": "3.3 Results on function words", "text": "In this section we comment on the function words, F(C), that the algorithm described in the previous sections detected in the various corpora. Table 2 gives an overview for the European languages and distinct corpora. For each corpus we present the number of different strings in |F(C)|, and we present a top segment of the function words. At this point, the order of the function words is determined by the number of times a particular string has occurred as an overlap in an optimal decomposition. The corresponding results for the Chinese corpus are illustrated at the end of this subsection.\nSpeaking in general terms, the number of the detected function words varies between 100\u2013300. The results in Table 2 suggest that the actual number of function words does not depend on the size of the corpus, rather on its domain. For instance, the two Medline excerpts have a similar number of function words, although the size of one corpus is 20 times the size of the other. It is expected that in corpora with paragraph-like structure the number of function words increases. This is due\nto binding words occurring in the beginning of sentences which in a sentence-based corpus could not be used as overlaps. The ratio of the corpus size to the number of corpus units also exerts certain role.\nFurther, in a domain-specific corpus, such as EU-Parliament or Medline, we may expect that certain words usually considered as content words will often occur at the boundaries of phrases. For example5, this is the case with patients with in the larger Medline corpus and with patients and cell in the smaller one. A manual inspection of the results reveals that there are 10-20 such strings in the domain-specific corpora which have the above property, yet only one or two in the Wikipedia corpora, i.e. million and John .\nBesides the particular cases mentioned above, the obtained lists comprise of white space, punctuation symbols, articles, prepositions, pronouns, auxiliary verbs. A significant number of function \u201cwords\u201d are multi-tokens such as \u201cto the\u201d. All the function words in the Medline corpora and in most EU-Parliament corpora (EN, DE, NL and the smaller ES and FR) are delimited by white-spaces or punctuation symbols. There is a minimal number of exceptions in the other corpora (two in\n5Here and in what follows we use the symbol to highlight an occurrence of the white-space symbol.\nthe English Wikipedia, one in the larger Spanish, and three exceptions in the larger French corpus, five in the Dutch Accountant Corpus and German Wikipedia and two in the Wittgenstein Corpus). The reason for these exceptions, e.g. \u201ce. \u201d, \u201ces. \u201d, \u201cs. \u201d in French or \u201cen. \u201d in German (Wittgenstein Corpus) and Dutch (Accountant), \u201cs\" \u201d in the English Wikipedia, can be explained in the following way. Some classes of words are underrepresented in the context of specific punctuation symbols. For instance, in German relative clauses often end with a verb with suffix en. Thus, we often observe \u201cen. \u201d at the end of the sentence. Yet, considering a particular verb form it is by no means evident that it will occur twice at the end of a sentence.\nFunction words obtained for the \u201c\u4fdd\u9556\u5929\u4e0b\u201d corpus. Since in Chinese the words of a sentence are not delimited it was interesting to see how the approach would work in this situation. Remarkably, results do not strongly differ from the other cases. For our Chinese corpus, \u201c\u4fdd\u9556\u5929\u4e0b\u201d, which represents fiction, we obtained a total of 291 function words. We present the top 50 function words most often used in the decompositions with accompanying English equivalent. \u201c\u201d (empty string); \u201c\uff0c\u201d (comma white space); \u201c\u7684\u201d (of); \u201c\u4e86\u201d (over); \u201c\u662f\u201d (is/are); \u201c\u4f60\u201d (you); \u201c\u6211\u201d (I/me); \u201c\u8fd9\u201d (this); \u201c\u4eba\u201d (people/human); \u201c\u7740\u201d (no specific meaning in Chinese); \u201c\u5728\u201d (at/in); \u201c\u5c31\u201d (no specific meaning in Chinese); \u201c\u201d\u201d (apostrophs); \u201c\u9ece\u7bab\u201d (Li Xiao \u2013 a person\u2019s name in this fiction); \u201c\u4ed6\u201d (he); \u201c\u4e5f\u201d (also/too); \u201c\u5230\u201d (arrive); \u201c\uff1f\u201d (?); \u201c\u90fd\u201d (both/all); \u201c\u53bb\u201d (go); \u201c\uff01\u201d (!); \u201c\u6765\u201d (come); \u201c\u8fd8\u201d (still/again); \u201c\u770b\u7740\u201d (look/stare); \u201c\uff01\u201d ( !); \u201c\uff01\u201d\u201d ( !); \u201c\u90a3\u201d (that); \u201c\u4e00\u4e2a\u201d (one); \u201c\u51fa\u201d (go/get out); \u201c\u4e0a\u201d (up/above); \u201c\u8981\u201d (need); \u201c\u548c\u201d (and); \u201c\uff1a\u201c\u201d (:apostrophs) \u201c\u81ea\u5df1\u201d (myself); \u201c\u5c06\u201d (will/shall); \u201c\u6ca1\u6709\u201d (no/didn\u2019t);\n\u201c\u80fd\u201d (can/be able to); \u201c\u540e\u201d (behind/later); \u201c\u88ab\u201d (be\u2013used in passive tense, before a verb); \u201c\u4f1a\u201d (will/can); \u201c\u5411\u201d (towards); \u201c\u5df2\u7ecf\u201d (already); \u201c\u8ba9\u201d (let); \u201c\u5bf9\u201d (write/correct); \u201c\u5374\u201d (but/and yet/however); \u201c\u7ed9\u201d (give/provide); \u201c\u7adf\u7136\u201d (unexpectedly/to one\u2019s surprise); \u201c\u6211\u4eec\u201d (we/us); \u201c\u4e00\u8138\u201d (the whole face); \u201c\u73b0 \u5728\u201d (now/currently);"}, {"heading": "3.4 Quantitative results on phrases", "text": "In this section we provide some quantitative statistics on the multiset of phrases P extracted in the final step of our algorithm. From our point of view it is interesting to assess how complex the resulting phrases are and how often they have been selected by the algorithm to obtain an optimal decomposition of a sentence.\nTo assess the first characteristic of P(C) we counted in a postprocessing step the number of words, word(P ), contained in each of the strings P \u2208 P(C). This\nwas done by counting the number of white spaces, ws(P ), in each of the strings P \u2208 P(C) and subtracting one, unless the string itself turned out to be a beginning or an end of a sentences in the corpus. Thus, ws(P ) takes on values 0, 1, . . . and word(P ) takes on values \u22121, 0, . . . . The values word(P ) = \u22121 and word(P ) = 0 mean that the string contains no word. We found out that strings of this form are unavoidable, details are left out.\nFigure 2 shows the distribution of the number of phrases in P(C) w.r.t. their word(P )-lengths for word(P ) \u2264 30. The variety of longer phrases word(P ) = 2, 3, 4 is larger than the variety of single-word phrases. It is worth to note that number of cases word(P ) = \u22121, 0 is very small. Manual inspection of such cases shows that these strings result from some particular features of the corpus, e.g. special symbols such as \u201c\u02c6\u201d in the OCR-ed corpus \u201cAccountant\u201d, special nomenclature and indices of the remarks in the Wittgenstein Corpus, chemical formulas in the Medline Corpus etc.\nIn order to estimate phrases of what length were actually preferred by the algorithm we proceeded in the following way. For w = \u22121, 0, . . . we computed the total number of multiplicities, M(w), of a string P \u2208 P(C) with word(P ) = w, i.e.:\nM(w) = \u2211\nP\u2208C:word(P )=w\nmn(P ).\nWe also computed the total number of occurrences, O(w), in the corpus of the strings P \u2208 P with word(P ) = w, i.e.:\nO(w) = \u2211\nP\u2208C:word(P )=w,mn(P )\u22651\nocc(P ).\nThus, the ratio M(w)/O(w) provides an average on which percentage of the occurrences of a phrase P \u2208 P with word(P ) = w has been used in the final decompositions determined by P . In Figure 3 we depict the relationship between w and M(w)/O(w) for w \u2264 30 for the corpora w.r.t. their size. Results confirm that the algorithm selects longer phrases with higher preference than shorter ones."}, {"heading": "3.5 Qualitative results on phrases", "text": "In our approach, the set of phrases represents a subset of the set of general phrase candidates, G(C). The above conditions imposed on general phrase candidates imply that a string with only one occurrence in C that does not represent a full sentence cannot belong to G(C). This means that only those proper phrases can be found that have at least two occurrences (with distinct immediate contexts) in the given corpus C. Since long substrings often have just one occurrence, many long substrings\nthat we would treat - from a linguistic point of view - as phrases are not found in P(C). As a matter of fact, we cannot obtain another result when we demand that phrases must occur in distinct contexts in the given corpus. From a qualitative point of view, the essential characteristics of the set of extracted phrases P(C) can be summarized in the following way. Examples for all cases are presented below.\n1. Ignoring a small percentage of exceptions, elements P \u2208 P(C) are sequences of \u201cwords\u201d delimited by blanks or special sentence symbols (,.;:). At this point it should be kept in mind that our algorithm does not know the notion of a word, nor the role of distinct letters in the alphabet. Exceptions where elements P \u2208 P(C) are not delimited by blanks (or sentence symbols) are caused by strings with only one occurrence in the corpus.\n2. In most cases, elements P \u2208 P(C) extracted are meaningful units in the sense that they represent phrases, or phrases extended on the left/right border by characteristic overlapping units, function words. In addition we often find units that would be interesting for language learners, showing how kernel sequences of words are typically connected with neighboured phrases. For the sake of reference, such units will be called phrases extended by typical connectors.\n3. When ignoring delimiting functions words, linguistic phrases in the corpus in most cases are representable as elements in P(C) or sequences of such elements.\nExample 3.1 We present the decomposition of the Europarl-sentence You have requested a debate on this subject in the course of the next few days , during this part-session . (en) and the parallel sentences for the languages French (fr), Spanish (es), Dutch (du), and German (de). In the following sentence decompositions, phrases P \u2208 P(C) extracted are the substrings marked by upper brackets ( . . .) or lower brackets ( . . .), and overlaps of decompositions have the form ( . . .) or ( . . .), function \u201cwords\u201d in overlaps highlighted in bold.\n(en) (You have( )requested a( )debate( on this )subject( in the )course of the next few days( ), during\nthis( )part-session . )\n(fr) (Vous avez( )souhaite\u0301( )un de\u0301bat a\u0300 ce sujet( )dans les prochains jours( , )au cours( de cette )pe\u0301riode de session( ). )\n(es) (Sus Sen\u0303or\u0131\u0301as han( )solicitado un( )debate sobre el\n( )tema para( )los pro\u0301ximos d\u0131\u0301as ,( )en el curso( de ) este per\u0131\u0301odo( )de sesiones .)\n(du) (U heeft aangegeven dat u( )deze vergaderperiode( ) een debat( )wilt( )over deze( )rampen .)\n(de) (Im Parlament( )besteht der( )Wunsch nach( einer )\nAussprache ( )im Verlauf( )dieser Sitzungsperiode( ) in den na\u0308chsten Tagen( ).)\nSeveral long phrases with not just one occurrences in the corpus are recognized. Examples are\nin the course of the next few days\nun de\u0301bat a\u0300 ce sujet\nin den na\u0308chsten Tagen\nLooking at the sentence beginnings we find \u201cphrases extended by typical connectors\u201d in the above sense:\nYou have\nVous avez\nU heeft aangegeven dat u\nSus Sen\u0303or\u0131\u0301as han\nExamples where long phrases can be represented as sequence of units in P(C) (possibly deleting overlaps on the two sides) are\nYou have requested a debate on this subject\nVous avez souhaite\u0301 un de\u0301bat a\u0300 ce sujet\nau cours de cette pe\u0301riode de session\nde este per\u0131\u0301odo de sesiones .\nim Verlauf dieser Sitzungsperiode\nExample 3.2 We present a typical decomposition of a sentence of the Medline Corpus.\n(A( viral )etiology,( )eg, the( )congenital rubella syndrome, ( )was considered most likely,(\n)but detailed( ) investigations( proved to be )negative.)\nIn this sentence, ignoring extensions caused by the comma \u201c,\u201d, terminological expressions such as\nviral etiology\ncongenital rubella syndrome\nare found as phrases. In fact, since terminological expressions often appear in distinct contexts, one strength of the method is the ability to find terminological phrases (see Section 5.2). The noun phrase detailed investigations is split, at this point corpus characteristics cause the effect that but detailed and investigations proved to be are prefered. Phrases found also include\nwas considered most likely,\nproved to be negative.\nPhrases that can be obtained by combining recognized units are, e.g.,\nbut detailed investigations proved to be negative.\nAlso note that viral and proved to be occur as an overlap, a characteristic feature of the Medline corpus.\nExample 3.3 We present a typical decomposition of a sentence of the English Wikipedia.\n(She further( )demonstrated that( )much of the other( )\nresearch in the book( )arguing for( )a link( between ) Sir( John )Williams and( )the Ripper crimes( )was flawed.)\nHere between and John occur as an overlap. Phrases found are\nresearch in the book\nthe Ripper crimes\nInteresting units that language learners would consider as important expressions, but not representing linguistic phrases (\u201cphrases extended by typical connectors\u201d\nin the above sense), are\narguing for\na link between\nExample 3.4 We present a typical decomposition of a sentence of the German Wikipedia.\n(vom( )Orchestre de la Suisse Romande) (, den( )Berliner\nPhilharmoniker,( )dem Tonhalle-Orchester Zu\u0308rich( )mit Werken ( )klass(ischer Komponisten), (wie )auch( )Komponisten (\n) Polens( von )Fre\u0301de\u0301ric Chopin( )bis( )Witold Lutoslawski (\n)u\u0308ber( )Krzysztof Penderecki.)\nDue to particularities of the corpus, ischer Komponisten occurs as an overlap. Phrases directly found are\nOrchestre de la Suisse Romande\nBerliner Philharmoniker,\ndem Tonhalle-Orchester Zu\u0308rich\nPhrases corresponding to combinations of recognized units are\nmit Werken klassischer Komponisten\nvon Fre\u0301de\u0301ric Chopin bis Witold Lutoslawski\nAlso note that person names are well recognized. This is again due to the effect that these person names appear in distinct contexts in the corpus.\nExample 3.5 We present a typical decomposition of a sentence of the Chinese corpus. The sentence is\n\u201c\u8001\u8005\u987a\u624b\u9012\u8fc7\u6765\u4e00\u4efd\u6587\u4ef6\uff0c\u9ece\u7bab\u6253\u5f00\u4e00\u770b\uff0c\u773c\u775b\u7acb\u523b\u5c31\u76f4 \u4e86\uff0c\u6ca1\u770b\u522b\u7684\uff0c\u5c31\u770b\u90a3\u5f20\u4e0d\u5927\u7684\u7167\u7247\uff0c\u7f8e\u5973\u554a\uff0c\u7edd\u5bf9\u7684\u7f8e\u5973 \u554a\uff0c\u6211\u4eec\u9ece\u7bab\u53e3\u6c34\u201c\u98de\u6d41\u76f4\u4e0b\u4e09\u5343\u5c3a\u201d\u201d\nthe English translation is\n\u201cThe old man handed over a document, LiXiao stared at it with motionless eyes as soon as he opened it, looking at nothing else, just the small picture, beautiful girl, a really beautiful girl, the saliva of our Lixiao has flowed three thousand feet.\u201d\nThe sentence decomposition determined by our algorithm is the following.\n\u201c(\u8001\u8005)(\u987a\u624b ( )\u9012\u8fc7(\u6765 )\u4e00\u4efd ( )\u6587\u4ef6(\uff0c )\u9ece\u7bab\u6253\u5f00 ( )\u4e00\u770b\uff0c\u773c\u775b\u7acb\n\u523b ) (\u5c31\u76f4\u4e86 (\uff0c)\u6ca1\u770b ) (\u522b\u7684 (\uff0c)\u5c31\u770b ) (\u90a3\u5f20 ( )\u4e0d\u5927(\u7684 )\u7167\u7247(\uff0c)\u7f8e\u5973 ) (\u554a\uff0c \u7edd\u5bf9\u7684 ( )\u7f8e\u5973\u554a(\uff0c )\u6211\u4eec\u9ece\u7bab ( )\u53e3\u6c34 ) (\u201c\u98de ( )\u6d41 ) (\u76f4\u4e0b ( )\u4e09\u5343 ) (\u5c3a\u201d)\u201d\nIt has the following (partially overlapping) phrases:\n\u201c(\u8001\u8005old man)(\u987a\u624bsmoothly ( )\u9012\u8fc7\u6765hand (something to somebody) )\n(\u6765\u4e00\u4efdone piece ( )\u6587\u4ef6document, or paper\uff0c ) (, Lixiao opens\u9ece\u7bab\u6253 \u5f00 ( )\u4e00\u770b\uff0c\u773c\u775b\u7acb\u523blooks, eyes immediately ) (\u5c31\u76f4\u4e86motionless/steady\uff0c) (, \u6ca1\u770bdid not look)(\u522b\u7684others\uff0c ( ), \u5c31\u770bonly look ) (\u90a3\u5f20that piece of()\u4e0d\u5927\u7684not big, small ) (\u7684\u7167\u7247picture\uff0c ( ),\u7f8e\u5973beautiful girl ) (\u554aa\uff0c \u7edd\u5bf9\u7684absolute()\u7f8e\u5973\u554abeautiful girl\uff0c ) (,\u6211\u4eec\u9ece\u7babLixiao or our Lixiao ( )\u53e3 \u6c34saliva) (\u201c\u98defly ( )\u6d41flow ) (\u76f4\u4e0bstraightly down ( )\u4e09\u5343three thousand ) (\u5c3a\u201dfeet)\u201d"}, {"heading": "3.6 Computing extended sets of phrases", "text": "An evident drawback of the approach proposed above is its incapability to detect phrases occurring only once: since these phrases do not appear in distinct contexts they are not contained in the initial set of general phrase candidates G(C). Obviously, many substrings occurring only once represent interesting language units. The problem is challenging because we do not have immediate statistical evidence that helps to detect such substrings. We looked at two ways to address this problem, in both cases using a dynamic approach.\n1. Extended island phrases. We consider the sentences, Sn, in the natural order of their appearance in the corpus C. At each time step, t, we let Ct be the set of sentences observed up to moment t. Using a dynamic interpretation of assumptions (A1)-(A3), the set of phrase candidates G(Ct) w.r.t. the restricted corpus Ct only contains those strings that occur in different left and right contexts within Ct. Now, when processing sentence St+1 we compute \u201cisland\u201d substrings V characterized by the following property:\n1. V is a maximal infix of St+1 which does not contain as a substring any phrase candidate in G(Ct) bounded with function words, F(P).\n\u201cIsland substrings\u201d V with these properties are extended on both sides until we reach the sentence border or a delimiting function word. Extended island strings\nV\u0302 are treated as candidates for phrases. For example, if sentence S = St+1 has the form\nThe Commission proposal is a start but it is not enough.\nit might be the case \u201cThe \u201d, \u201c is a \u201d, \u201c but \u201d, \u201c it is \u201d, and \u201c not \u201d are already elements of G(C)t, but none of the remaining words in the sentence has this property. The strings \u201cCommission proposal\u201d, \u201cstart\u201d and \u201cenough.\u201d are islands not covered by these phrase candidates:\nThe Commission proposal is a start but it is not enough.\nAssume that when decomposing the sentence these strings are decomposed in a brute force way into substrings like \u201cC\u201d, \u201commission\u201d, \u201cpro\u201d, \u201cpo\u201d, \u201cosal\u201d and so on. None of these smaller strings is bounded by function words on both sides. In this situation we guess that\nCommission proposal , start and enough. are phrases. Applying this simple procedure we extend the set of phrases, P(C) with these extended islands. Results obtained are illustrated in Table 3.\n2. Instances of island schemes. In a second step we extended this idea. For each sentence St in the original corpus a copy Sat was introduced where the longest infixes which cannot be covered by phrases in Pt are substituted by a new character \u201cUNK\u201d. For example, the above sentence would be transformed to\nThe UNK is a UNK but it is not UNK\nIn this way we obtain a new corpus Ca = {San}Nn=1 consisting of abstracted sentence transformations of the original corpus. Abstract phrases detected in this corpus can be pulled back to C, in this way recognising patterns that we were unable to detect originally. For example, if The UNK is a UNK is a phrase in Ca we may conclude that The Commission proposal is a start could be treated also as a phrase in the original corpus, C. For phrase detection in Ca we apply the algorithm described in Section 3.1. Afterwards we can simply take the resulting phrases containing UNK in Pa - called island schemes - and replace the special symbol by the original island. Table 4 represents some island schemes and instances obtained by applying this procedure to the excerpts of English and German EU Parliament Corpus."}, {"heading": "4 Computing subphrase structure and content words", "text": "In this section we further analyse the structure of phrases. As a side result we obtain a method for finding content words (non-function words) of the corpus."}, {"heading": "4.1 Algorithmic principles", "text": "Now that we have determined a set of function words F(C) and a multiset of phrases P(C) we proceed to structure phrases in a hierarchical way. We address this issue in two steps. In the first step we explain how to generate the constituents of the phrases in P(C), which we call subphrases. As a by-product of this step we will be able to derive a notion corresponding to the common \u201cwords\u201d. Recall that our algorithm just analyzes plain sequences of symbols without having any notion of word \u201cbuilt in\u201d. In a second step, we continue to structure the resulting subphrases in a hierarchical way, obtaining a form of decomposition tree.\n1. Splitting phrases and subphrases. Following the discussion in Section 2 and the model that we developed in Section 3, given a phrase P \u2208 P(C) we look for a decomposition into substrings of G(C) and determine those that exhibit the most regular function words. Formally, we search for a decomposition of P into a sequence P1, P2, . . . , Pk of general phrase candidates Pi such that the sequence of induced function words F = (F1, F2, . . . , Fk\u22121) optimises the likelihood:\n`(F |P) = k\u22121\u220f i=1 p\u0303pref (Fi|P) \u00b7 p\u0303suf (Fi|P).\nThe only difference to the situation above is that we now have fixed the set of function words, requiring that Fi \u2208 F(C) for all 1 \u2264 i < k. Strings Pi only need to be general phrase candidates in G(C) but may fail to be elements of P(C). In this way, a larger spectrum of meaningful units in the corpus is obtained, beyond the multiset of (full) phrases in P(C). We call each Pi a subphrase in C. Using the same principles again, each subphrase P can be further decomposed into finer subphrases until we arrive at atomic subphrases that cannot be further partitioned. By SP(C) we denote the union of P(C) with all subphrases obtained from iterated decomposition of phrases and subphrases. SP(C) is called the set of subphrases of the corpus C. The set of atomic subphrases of C is denoted ASP(C).\nIt is easy to see that each subphrase P that is not a sentence prefix (suffix) has a prefix (suffix) which is a function word in F(C). The kernel of P is obtained from P by deleting the prefix (suffix) of P in F(C) which has the maximal probability pfw(.|P) as a function word. If P does not have such a prefix (suffix), we \u201cdelete\u201d from P the empty prefix (suffix). In this way, from the set of phrases P(C) we obtain the set of phrase kernels PK(C), from the set of subphrases we obtain the set of subphrase kernels SPK(C), and from the set of atomic subphrases we obtain the set of atomic subphrase kernelsASPK(C). It turns out that the atomic subphrase kernels of a phrase P typically are the \u201ccontent words\u201d contained in P , i.e., words that are not in the set of F(C).\n2. Decomposition trees and functional schemes. For a phrase P we may select an optimal decomposition. When we continue to decompose the subphrases obtained, always selecting an optimal decomposition, we obtain a decomposition tree t for P . Each node of t represent a subphrase of P . The children of a node P \u2032 in t are obtained using the set of function words F(C) and the above optimization principle, selecting one optimal decomposition. A parallel kernel decomposition tree tK is obtained replacing each node in t by its kernel. Using the two trees and looking at the leaves, each phrase P has a representation as a sequence of function words and atomic kernels. For phrases P that do not correspond to a sentence prefix or suffix, P is described as\nP = F0A1F1 . . . Fn\u22121AnFn\nwhere Fi \u2208 F(C) and the strings Ai represent the leaves of tK . In this situation, the sequence F0|F1| . . . |Fn is called a functional scheme for P . For sentence prefixes (suffixes) function word F0 (resp. Fn) has to be omitted. A functional scheme can be considered as a rudimentary form of grammar rule. In general a (sub)phrase may have several optimal decompositions. Hence, for a given phrase P we obtain a set of decomposition trees T (P ), a parallel set of kernel decomposition trees TK(P ), and a set of functional schemesFS(P ). However, we found that the functional scheme for a phrase and the leaf representation P = F0A1F1 . . . Fn\u22121AnFn is unique in most cases. From the multiset of phrases of the corpus C we obtain the multiset FS(C) of all functional schemes in C, which may be ordered by the number of occurrences."}, {"heading": "4.2 Results for distinct languages and corpora", "text": "As an illustration, in Figure 4 we present a decomposition tree and the parallel kernel decomposition tree for the German phrase\nu\u0308ber den Vorschlag fu\u0308r eine Richtlinie des Europa\u0308ischen Parlaments und des Rates\nEmpty nodes in the kernel decomposition tree are caused by phrases that become empty when removing the function word borders. The functional scheme for the phrase has the form\n| den | fu\u0308r eine | des || und des |.\nAtomic subphrase kernels obtained are u\u0308ber, Vorschlag, Richtlinie, Europa\u0308ischen, Parlaments, and Rates.\nThe results for phrase decomposition in European languages are similar. As mentioned above, in almost each case, the functional scheme for a phrase turned out to be unique, even if there are several decomposition trees. This shows that the difference between distinct decompositions is mainly an issue of distinct orders in which to decompose (sub)phrases, not an issue of distinct final decomposition parts.\nFor all European languages and corpora considered, the set of atomic subphrase kernels obtained is essentially a lexicon of the content words of the corpus. Hence, despite of the fact that we do make any distinction between the distinct symbols in the textual alphabet, words are recognized as the \u201catomic\u201d units from phrase decomposition. For Chinese, iterated subphrase splitting often leads to single symbols. Though multi-symbol words in general occur at one level of the decomposition of phrases, we yet do not have good principles to exactly find this point.\nAn obvious question is if the functional schemes obtained can be considered as a kind of induced grammar. However, there is no direct correspondence between traditional linguistic categories and the spectrum of functional schemes. Many of the functional schemes capture small sequence of words of a similar form with many instances, but the distinct instances do not always have a common linguistic functionality. In Table 5 we show the most frequent functional schemes for the English and German Wikipedia Corpora, respectively.\nOn the other hand we also found a considerable number of interesting patterns that point to a special form of grammatical construction. In Table 6 we add a selection of interesting complex schemes with several instances."}, {"heading": "5 Applications", "text": "In this section we consider a selection of possible applications of the above methods. In each case the main point is to show that the automated computation of phrases and subphrases of a corpus and the bidirectional structure of the index structure used offer interesting possibilities to approach known problems on new paths."}, {"heading": "5.1 Corpus lexicology", "text": "The predominant perspective in lexicology and corpus linguistics is to use a repository of corpora for collecting and verifying knowledge about a language or language variety. Under this perspective, language is the central object of study, and\ncorpora are just a means to obtain insights about language, e.g., by collecting the vocabulary found in the corpora. There is second perspective, which in general finds less attention: here a given corpus is considered as an independent object of study and the goal is to obtain insights about the \u201clanguage of the corpus\u201d. In this subsection we follow the second perspective. The methods described below, as those discussed above, can be applied in an unsupervised and fully automated way. They can be considered as a preliminary step to dynamically generate in an \u201conline\u201d manner views that characterize the vocabulary and language of a given corpus in distinct ways. The long-term objective is to enable \u201ctravels\u201d in the language of the corpus, to compare the languages of distinct corpora etc. We start with an study where we try to find the most characteristic vocabulary of a given corpus. Afterwards we look at connections between words, semantic fields and phrase nets and at language style.\nCharacteristic words of a corpus. The problem of how to find \u201cimportant\u201d terms of a corpus or document collection has found considerable attention (see Section 6). Since both very rare and very frequent terms in general are inappropriate, there is no simple solution. Here we suggest a method that takes the composition of sentences and phrases into account. In order to find characteristic words of the corpus C we first try to find the most \u201ccharacteristic\u201d or \u201cspecific\u201d child Pi of a phrase P \u2208 P(C) in the decomposition trees t \u2208 T (P ). As we see below, at this step \u201crare\u201d language units are prefered. Afterwards, looking at those kernels in ASPK(C) that appear as the kernels of many \u201cmost characteristic subphrases\u201d, we derive a ranked set of \u201ccharacteristic words\u201d or \u201ckey words\u201d of the corpus C. At this step, terms that are too rare obtain a low score. In order to formalise the intuitive notion of \u201ccharacteristic\u201d, we argue probabilistically. The intuition is that we want to decrease the uncertainty to obtain a longer phrase P starting from a shorter subphrase Pi. Probabilistically, this means that we need to maximise the conditional probability p(P |Pi) where Pi ranges over all subphrases of P . Now, clearly:\np(P |Pi) = p(P, Pi)\np(Pi)\nand if we look at the probability for these events as the event that the particular strings occur, then it is obvious that p(P, Pi) = p(P ) because Pi is a substring of P . Thus, if we naively consider the probability p as empirical probability, then the probability p(P |Pi) is simply the ratio of the occurrences of Pi that imply an occurrence of P , i.e.:\np(P |Pi) = occ(P )\nocc(Pi) .\nPractically, the maximal value is simply obtained by selecting the child Pi of P with the smallest number of occurrences occ(Pi) in the corpus. In our experiments, if two children have the same number of occurrences we simply selected the leftmost child.\nUsing the above measure for specificity we assign to each subphrase P \u2208 SP(C) its most specific child P \u2032. We write P \u2032 = msc(P ). Since a given subphrase P \u2032 in general acts as the most specific child of several superphrases P we obtain a forest\nMSSC = (SP(C),msc)\ncalled the forest of most specific subphrases. The roots ofMSSC are the \u201ccharacteristic\u201d elements in the set of atomic subphrases,ASP(C). The nodes ofMSSC are the elements of SP(C), leaves representing maximal phrases. As before, as an alternative view we may look at the modified forest MSSKC where we replace each subphase by its kernel. The roots of this forest, which represent \u201ccharacteristic\u201d content words, are called characteristic atomic kernels.\nAs a measure for the importance of an atomic phrase P in ASP(C) we may use the number \u03c1C(P ) of distinct phrases in P(C) that are descendants of P in the forestMSSC . In the experiments described below we found that selecting atomic phrases (or their kernels) with high \u03c1C-score in fact yields a characteristic profile for a given corpus.\nIn our experiments we computed a ranked list of characteristic atomic kernels for our fragment of the Medline corpus, following the above method. The 60 topranked kernels are shown in Table 7. For the sake of comparison, similar ranking list were computed for the English Europarl corpus and for 2% of the English Wikipedia. In Table 7, pairs (n/m) present Europarl ranks (n) and Wikipedia ranks (m). Entries \u201c-\u201d mean that the entry did not occur in the ranked list for the corpus. The ranked lists for the corpora, despite of the large variety of word types in the lists, are radically distinct. When taking the top 60 kernels from Medline, only three (\u201conly\u201d, \u201cone\u201d, \u201ctime\u201d) occur among the top 60 words in the ranking list for the Europarl corpus, and only seven (\u201cused\u201d, \u201conly\u201d, \u201cfound\u201d, \u201cone\u201d, \u201cdeveloped\u201d, \u201cuse\u201d, \u201ctime\u201d) occur among the top 60 words in the ranking list for the Wikipedia corpus. This shows that top segments of the ranking lists yield an interesting \u201cprofile\u201d for a given corpus, which might also be useful for classification tasks and for stylistic studies (see below).\nConnections between words, semantic fields and phrase nets. Automated approaches for exploring the paradigmatics of lexical units typically look at the cooccurrence of terms in documents, paragraphs, sentences or fixed size neighbourhoods [SP93, Sto10]. Another, more \u201csyntactic\u201d view is obtained when looking\nat the co-occurrence of terms in phrases. To this end, the forest of most specific subphrases MSSC can be used. We compute the network G = (ASP(C), E) where E is the set of all edges of the form e = (A1, P,A2) where P \u2208 P(C) is a descendant of the root nodes Ai inMSSC (i = 1, 2). In the graph G, two atomic phrases A1 and A2 are strongly interlinked if they co-occur in many full phrases. Note that phrases P can have variable length and we do not ask if A1 and A2 are direct neighbours in P . Figures 5 and 6 show some of these graphs. Networks of this kind, when visualized in an appropriate way, offer interesting possibilities for mining and exploring corpora and related corpus vocabulary. Figure 5 starts from four characteristic words from Table 7, \u201cblood\u201d, \u201cchildren\u201d, \u201crisk\u201d, and \u201ctumor\u201d. Presented are atomic phrases that co-occur with these words in phrases, and the phrases where the co-occurrence appeared. Figure 6 is obtained in a similar way from the Wittgenstein corpus, starting from \u201cErkla\u0308rung\u201d, \u201cBedeutung\u201d and \u201cGebrauch\u201d.\nMining language style. It is a well-known phenomenon that the style even of official language differs when looking at distinct communities, fields or domains. When entering a new community, even native speakers often have difficulties to \u201cadapt\u201d language style and to meet the typical phrases and constructions of the\ndomain. For foreign language learners this task is even more complex. Many phrases automatically extracted from corpora using the above methods reflect the typical style of the corpus domain. For mining \u201cstylistic\u201d phrases we may start from subphrases such as \u201c we \u201d, \u201c my \u201d, and \u201cit is \u201d and then look at the full phrases containing these strings. These phrases exhibit another specific \u201cface\u201d for each corpus. Below we present phrases including the strings \u201c my \u201d, and \u201cit is \u201d derived (a) from the Europarl corpus, and (b) from Medline. It is important to note that the phrases extracted may contain the given string at any position, such as in \u201cas a conclusion it is suggested that\u201d, or in \u201cexpress my regret at the\u201d.\n\u201cmy\u201d in Europarl: my wholehearted support - my compliments to - extend my special thanks to - my congratulations to - my colleague , Commissioner Wallstro\u0308m - In my opinion , this is the only way - In my view , this - close to my heart - to thank my colleagues on the committee for - I expressed my views on - It is my belief that - On behalf of my - my honour to present to you - my group welcomes this - express my regret at the ...\n\u201cmy\u201d in Medline: to my knowledge this is the first - my own experience - my personal experience - studies in my laboratory - my professional life - of my research career - my assessment - my findings suggest that - answer to my question - my colleagues and i have developed ... .\n\u201cit is\u201d in Europarl: it is extremely important - it is therefore necessary to - it is a question of - it is , however , - it is in this spirit that - it is especially important - it is high time that - it is indeed the case that - it is my belief that - it is worth pointing out - it is unacceptable that - it is not enough to ... \u201cit is\u201d in Medline: from these results it is concluded that - it is suggested that - it is shown that - it is argued that - it is postulated that - however it is not known whether - therefore it is suggested that - it is tempting to speculate that - indicating that it is - it is important to understand - furthermore it is shown that - as a conclusion it is suggested that ..."}, {"heading": "5.2 Terminology extraction", "text": "Automated terminology extraction helps to build up terminological dictionaries and thesauri, it has often been studied in the literature (Section 6). Most methods suggested use characteristic sequences of POS, word association measures such as log likelihood, mutual information etc., and corpus comparison to find terminological expressions in a given corpus. Here we suggest a three step approach only based on the techniques described above and using distinct corpora. In Step 1 we compute the characteristic atomic kernels of phrases for the given corpus, only relying on the input corpus. In Step 2, given the set of characteristic atomic kernels we try to single out \u201cterminological\u201d kernels (e.g., \u201ccancer\u201d) using the ranked lists of characteristic atomic kernels obtained from another corpora for comparison. In Step 3, given a terminological kernel (\u201ccancer\u201d) we try to find terminological (multi-word)\nexpressions that contain this kernel as substring (\u201cfemale lung cancer\u201d, \u201cprostate cancer\u201d).\nStep 1 - characteristic atomic kernels. Our starting point for searching kernels is the decomposition of phrases described in Section 4. We compute the ranked list of characteristic atomic kernels. In general there are much more truly \u201ccharacteristic\u201d kernels than \u201cterminological\u201d kernels. Even if there is hardly a general agreement of what \u201cterminological\u201d exactly means, typically, mainly noun phrases are considered as terminological expressions. The list in Table 7 show that words of many distinct types can be \u201ccharacteristic\u201d for a given corpus.\nStep 2 - terminological kernels. When trying to filter out \u201cterminological\u201d kernels from the list of \u201ccharacteristic\u201d kernels we could of course use POS or other type of linguistic information. However, for the sake of consistency of the approach we do not use external or human linguistic knowledge here. The numbers found in Table 7 suggest that \u201cterminological\u201d kernels typically do not appear in the top segment of the list obtained for the Wikipedia corpus. If we ignore kernels with a rank below 1,000 in one of the two corpora used for comparison, from the Medline top 60 segment we obtain the following list of kernels\ntreatment (2223/1106), obtained (3743/1141), patients (4104/1897), patient (-/3414), cancer (-/2935), expression (1333/1666),\ndemonstrated (1167/1900), disease (3185/1674), cells (-/1123), decreased (5109/3881), factors (1920/1089), tumor (-/10737),\nexamined (4705/5826), normal (3079/1211), blood (-/1376), subjects (1068/2110), decrease (3735/2992), protein (7516/1941),\ncompared (2692/1199), investigated (3658/5008), valuated (8548/7615)\nWe still find entries that are usually not considered as \u201cterminological\u201d, but clearly a much better focus is obtained. With larger bounds, the percentage of non-terminological kernels can be further reduced, a suitable compromise between precision and recall needs to be defined for each application.\nStep 3 - kernel expansion. For each of the remaining top-ranked phrase kernels we consider the list of all phrases that contain this kernel as a substring. In this way, kernels are expanded to the left, to the right or in both directions. Since our phrases typically come with function word delimiters we clean each set by eliminating left and right \u201cborders\u201d that appear in our list of function words (see above). Cleaned substrings are ranked by frequency of occurrence. As a result we obtain a set of multi-word expressions. The characteristic features of the terminological expression found in this way are the following\n1. the number of words in these expressions is not fixed,\n2. the kernels used may appear at any position in such a multi-word expression.\nAs an illustration, in Appendix A we present upper parts of the ranked lists obtained for the kernels \u201cdisease\u201d, \u201csyndrome\u201d, and \u201cinflammatory\u201d. In most cases, \u201cdisease\u201d occupies the last position of the phrase. Expressions can be long - some examples from the full list are \u201crisk factor for cerebrovascular disease\u201d, or \u201cintestinal failure-associated liver disease\u201d. Examples where \u201cdisease\u201d is not at the last position are \u201cdisease-related modification of exposure\u201d and \u201cdisease is known to be caused\u201d. For \u201csyndrom\u201d, an example is \u201cmetabolic syndrome components\u201d. In contrast, \u201cinflammatory\u201d typically is found at the beginning of an expression, as expected. Examples not following this rule are \u201cproinflammatory cytokine\u201d and \u201cnonsteroidal anti-inflammatory drugs\u201d. We did do not use any kind of additional cosmetics (which would easily be possible in practice). From our point, this third step is working extremely well."}, {"heading": "5.3 Automated query expansion for search engines and free context size answer browsing", "text": "Querying a large corpus can be a difficult task since it is not simple to estimate in advance the number of answers to a search query. If the query is too specific, the number of answers might be small or empty, in other cases the answer set becomes extremely large and a manual inspection is not possible. In order to support users, search engines in the Internet often present a selection of possible right expansions to a unspecific string. For example, when typing \u201cJohn\u201d in the search window of the English Wikipedia, some expansions such as \u201cJohn McCain\u201d, \u201cJohn Lennon\u201d, and others (s.b.) are suggested.\nThe index-based technology presented above can be used to automatically produce interesting expansions. A given short query is just treated as a kernel in the sense of Section 5.2. Just using Step 3 \u201ckernel expansion\u201d of the above procedure, interesting right and left expansions for a query are found. In order to judge how well this approach works we indexed the complete English Wikipedia7 lists of possible expansions for a number of short queries. Possible expansions were ranked by the number of occurrences. Topmost Expansion suggestions for \u201cJohn\u201d, \u201cPeter\u201d, \u201cBlue\u201d and \u201cBrown\u201d are presented in Appendix B. We then compared this with Wikipedia\u2019s expansion suggestions. Main insights obtained are:\n1. Wikipedia only suggests a small number of expansions. The list of possible expansions produced by our method is much larger.\n7The corpus, Wikipedia dumps May 2014, used for these experiments was split into sentences by PUNKT, [KS06], and afterwards processed by our approach from Section 3.1.\n2. Wikipedia only produces right expansions for a query. Our lists also include left or two-sided expansion suggestions.\n3. As to recall, most of Wikipedia\u2019s suggestions are also found in our lists. In most exceptional cases, variants of Wikipedia\u2019s suggestions are found.\nThe third point is illustrated in Table 8 where we show the full list of Wikipedia\u2019s expansion suggestions (June 18, 2015) for \u201cJohn\u201d, \u201cPeter\u201d, \u201cBlue\u201d, \u201cBrown\u201d, \u201cItalian\u201d, and \u201cFrench\u201d. We show which suggestions are also found using our indexbased \u201ckernel expansion\u201d method, and where we found variants of Wikipedia\u2019s suggestions. The recall is excellent. In many cases, distinct variant phrases of Wikipedia\u2019s suggestions were found in addition. Some of these are mentioned in Table 8.\nFree context size answer browsing. Once the user has selected one of the expansion suggestions (\u201cJohn McCain\u201d), Wikipedia leads her to the Wikipedia page for the expansion (https://en.wikipedia.org/wiki/John McCain). The symmetric index technology can be used for another type of interaction, which we call \u201cfree context size answer browsing\u201d. We present to the user a two-sided list of concordances that includes all occurrences of the expansion in the indexed corpus. Since in the index left and right contexts of arbitrary length are directly accessible, the size of contexts shown can be selected and varied by the user. In Figure 7 we see a user interface with this functionality embedded. In this search interface, after typing any string (here: \u201cagen\u201d), the user may select the number of hits and the size of the concordances, which includes both left and right contexts for a hit. Using the lower button, the size of contexts presented can be changed in real-time. This kind of search interface is used at a project at CIS, LMU, where we collaborate with philosophers to find new and innovative ways of how to access corpora in Digital Humanities. See http://www.cis.uni-muenchen.de/dighum/research-groupco/index.html."}, {"heading": "6 Related work", "text": "The discussion of related work is separated into two parts. We first consider contributions that are related when looking at the general characteristics of the approach presented above. Afterwards we take a more application oriented view.\nGeneral characteristics of approach\nFinding phrases. When looking for phrases, we do not use linguistic knowledge and only consider structural properties of the corpus. Our approach is completely language independent. In many application fields, as a common standard approach phrases for simplicity are replaced by n-grams of words, [ON04, Koe10]. Additional grammatical features, e.g. part of speech [Lui07], syntactic [ZLC+13], or semantic markers [EWVN11] may be applied to find n-grams that exhibit phrase characteristics. For larger values of n, statistical uncertainty is growing. Deep learning approaches [CWB+11, LST13] try to resolve this issue by deriving regularities in smaller dimension spaces. However, the embedding of data in a smaller dimension space is achieved by extracting features from a fixed length context or from the entire sentence, [CWB+11], with no attempt to reflect the general structure of the corpus. Recent approaches in this area try to account for this drawback by more advanced network topology, [TSM15]. Due to the fixed size of n-grams considered at one time, and since n-grams start at any point in the sentence, the subphrase structure of sentences is not analyzed.\nGrammar development and induction. When trying to analyze sentence structure, grammar-based parsing algorithms can be used [Tom13]. The sentence and phrase decomposition strategies described in this paper can be considered as a preliminary step towards corpus based, automated, and language independent grammar induction. While our approach leads from corpus structure to grammatical rules, in parsing, fixed grammar rules impose a \u201cpredefined\u201d structure on the corpus. For most languages, a full and correct syntactic analysis of arbitrary sentences in unrestricted texts is not possible. For searching phrases of a particular type, partial/local grammars [Gro97] can achieve high precision and recall. Until today, the development of (full, partial, or local) grammmars with high coverage and precision is an ambitious and difficult task that needs a high amount of human work. Furthermore, most methods we are aware of are language dependent. Language independent and automated construction steps are found in probabilistic context-free grammars [Sak10] and learning regular expressions [DLT04]. The latter problem is mainly of theoretical interest.\nSequence analysis without linguistic knowledge. The problem of detecting meaningful units in a text corpus can be considered also as a kind of segmentation problem. In [LK09] and in [GGJ09] the authors consider the problem of segmenting sequences of letters without blanks and punctuation into words. [GGJ09] is focused on the segmentation of children\u2019s utterances transcriptions into words. The authors present a Bayesian approach combined with the Maximum A Posteriori Principle in order to detect the most probable boundaries of words within\na sequence. In particular, they use Dirichlet distribution to model the number of different words contained in the corpus and a probability for an end of a word.\nThe approach presented in [LK09] is a generic one and the authors apply it also to Chinese word segmentation, Machine Translation alignment and others. The model defines only a probability for a sequence of letters to belong together. In this measure a key role is played by a fine-tuned parameter that controls the length of sequences. Other parameters of the model are trained via Expectation Maximisation Principle.\nAn extensive work has been also done on supervised word segmentation in Chinese. The methods range from statistical approaches based on CRFS [PFM04] to Recurrent Neural Networks [CQZH15] and others.\nThere are two main differences between the works cited above and the current paper. Firstly, the problem of segmentation requires that each letter is assigned to exactly one word. In contrast, we allow overlaps between these units which represents the possibility of sharing small pieces of information which however have high frequency and can provide us with reliable statistics. Secondly, it does not seem that previous approaches reflect the structure of the corpus, rather it is the statistics which aggregates the statistics obtained from the different entries of the corpus. In the presented approach we use this structure in a two-fold manner: (i) to constrain the number of available strings and thus define their possible boundaries in terms of other strings; and (ii) to find the best fit of these shorter strings within a particular corpus\u2019 entry. This considerations allow us to avoid the explicit modelling of phrase/word types and phrase/word length.\nIndex-based analysis of corpora. In [Gol10] (see also [Gue05]), suffix arrays are used as a text index structure for mining local grammars. POS-info stored in a synchronized index structure enables a rich set of queries including textual and grammatical conditions. In many fields, suffix trees/arrays are used for various tasks, applications ranging from sequence analysis in bioinformatics [Gus97] to text-reuse in digital humanities. The index structure used for our approach, symmetric directed acyclic word graphs (SDAWGs) [IHS+01], has yet not found much attention. The power of this index relies on the fact that it gives access to left and right contexts of any length. This feature is used in many of the above algorithms.\nApplications\nWe mentioned several possible applications of the methods developed. In all those fields there exists a large literature, only a rudimentary impression can be given. Finding characteristic terms of a document collection is studied, for example, in Information Retrieval. \u201cGlobal\u201d term weights are, e.g., Poisson overestimation, dis-\ncrimination value, and inverse document frequency [MRS08]. Specialized methods have been developed for the related problem of automated keyword extraction [RECC10]. Terminology extraction is studied, for example, in [PL05]. Methods for automated query expansion are discussed, e.g., in [LSM12]. In corpus linguistics and lexicology, the problem of finding syntagmatic and paradigmatic relations is a central topic. [SP93, Sto10] present contributions to finding lexical-semantic relations, collocation, word maps, and paradigmatics of a lexical unit. In all cases, the methods suggested above are unique by using information on the subphrase structure of sentences inferred from general structural properties of the corpus. For achieving optimal results it would be interesting to combine distinct approaches."}, {"heading": "7 Conclusion", "text": "In this paper we described results of an ongoing research project where we analyze corpora in a completely language independent and unsupervised way without any prior linguistic knowledge. We introduced an algorithm for detecting function words and phrases. In many cases the phrases (combinations of phrases) obtained are linguistic phrases or extensions with \u201cadjuncts\u201d (words connecting linguistic phrases). We then showed how phrases can be split into subphrases and how to determine content words. For Chinese, detection of phrases yields satisfactory results, as to word segmentation it is difficult to detect the best level of phrase decomposition.\nA weakness of the main algorithm for partitioning sentences into phrases presented in Sections 3.1-3.5 is that only strings can qualify as phrases that occur within distinct contexts in the corpus. All sequences with just one occurrence in the corpus cannot become phrases. To avoid this problem, and to come closer to phrases in a proper linguistic sense we need to abstract from plain symbol sequences in some way. The extensions of the main algorithm presented in Section 3.6 represent one promising path for abstraction that deserves further investigation. The detection of phrase patterns such as those in Section 3.6 and Section 5 combined with the pure character-based structure underlying the approach may be also useful for recognising the morphological structure of the language and its dependence on the local phrase context. Some preliminary experiments in this direction showed that our index-structure provides structural indicators that can be used to extract reliable sets of morphemes and to segment words. We assume that unsupervised statistical methods for morphology induction, e.g. [Gol01, Zem08, CL05], can be combined with methods for mining the phrase structure of the corpus, providing another form of abstraction. This might provide a solid alternative to the deep neural network approach taken by [SO15] where embeddings account for the\nlocal context of the words and their morphological properties. The applications discussed show that the unsupervised analysis of (sub)phrase structure and the two-directional symmetric index used for corpus representation offer many interesting options for finding new solutions to prominent problems in distinct fields.\nIn this paper, when analyzing corpora we were radical in the sense that we do not use any kind of explicit linguistic knowledge/resource and no knowledge on the nature/functionality of a symbol (blank, hyphen,...). As a matter of fact, from a practical perspective it is interesting to give up this rigid principle. We intend to adapt the methods obtained to \u201cnon-raw\u201d texts. For example, text annotated with POS info just represents another kind of interesting source data [Gol10] for \u201clanguage mining\u201d. The active use of prior knowledge is one point of future work.\nAcknowledgements. We express our gratitude to: Max Hadersbeck for the sentence segmentation of the Accountant Corpus; Thomas Mu\u0308ller for providing us with sentence segmentation of the German and English dumps of Wikipedia from May 2014. Wenpeng Yin for the translations from Chinese to English. Stoyan Mihov, Petar Mitankin and Tinko Tinchev for comments and discussions on the ongoing research. Hinrich Schu\u0308tze for a discussion on function words. The second co-author would like to explicitly acknowledge the technical lead of the first coauthor: in the wake of many joint discussions, it was the first co-author who found and implemented the algorithms suggested in this paper. This research was carried out within the ISALTeC Project, Grant Agreement 625160 of Marie Curie Intra Fellowship within the 7th European Community Framework Programme."}, {"heading": "Appendix A", "text": "We present top segments of the ranked lists of terminological expressions obtained for some kernels using the method described in Section 5.2. No filtering or cleansing steps were applied to improve the lists.\nExample 7.1 Terminological expressions obtained for kernel \u201cdisease\u201d: disease/8418 disease./832 diseases/526 disease,/390 diseases./348 diseases,/158 this disease./153 coronary artery disease/85 infectious dis-\neases/85 diseased/82 Alzheimer\u2019s disease./82 this disease/77 coronary artery disease./73 liver disease./68 cardiovascular disease./63 liver disease/53\nAlzheimer\u2019s disease (AD)/52 Alzheimer\u2019s disease,/52 heart disease/52 Parkinson\u2019s disease./51 disease-specific/51 heart disease./50 disease)/47 Crohn\u2019s\ndisease/46 disease-free survival/46 coronary heart disease./45 autoimmune diseases./45 metastatic disease./45 cardiovascular disease/43 Alzheimer\u2019s\ndisease/42 diseases such/42 cardiovascular disease,/42 The disease/41 autoimmune diseases/41 vascular disease/40 cardiovascular diseases/40 Alzheimer\u2019s\ndisease (AD)./39 coronary heart disease (CHD/39 coronary artery disease,/38 Parkinson\u2019s disease/37 cardiovascular diseases./37 disease progres-\nsion/36 coronary heart disease/35 renal disease./34 metastatic disease/33 disease-related/33 liver disease,/31 infectious diseases./31 Parkinson\u2019s dis-\nease (PD)/31 Crohn\u2019s disease./30 disease-free/30 Parkinson\u2019s disease,/30 this disease,/29 course of the disease./29 inflammatory bowel disease/29\nautoimmune disease/28 coronary heart disease,/28 vascular disease./28 diseases, including/28 inflammatory bowel disease./28 coronary artery dis-\nease (CAD)/28 Alzheimer\u2019s disease (AD/28 renal disease/27 disease-/27 Hodgkin\u2019s disease/26 cardiovascular disease (CVD/26 neurodegenerative\ndiseases/26 Hodgkin\u2019s disease./25 periodontal disease./25 Crohn\u2019s disease,/25 neurodegenerative diseases./25 kidney disease./25 inflammatory dis-\neases/25 disease)./24 obstructive pulmonary disease (COPD)./24 diseases, such/23 lung disease./23 Parkinson\u2019s disease (PD)./23 and disease./23 dis-\nease severity/23 lung disease/22 periodontal disease/22 liver diseases/22 inflammatory diseases./22 ischemic heart disease/22 disease, including/21\nobstructive pulmonary disease (COPD/21 heart disease,/21 coeliac disease./20 ischaemic heart disease./20 disease;/20 autoimmune disease./20 coro-\nnary artery disease (CAD)./20 Graves\u2019 disease./20 cardiovascular diseases,/20 Parkinson\u2019s disease (PD/20 diseases in/19 congenital heart disease/19\nvarious diseases/19 liver diseases./19 sickle cell disease./19 renal disease,/19 sexually transmitted diseases/19 advanced disease./19 end-stage renal dis-\nease (ESRD/19 respiratory diseases/19 diseases associated/19 and diseased/18 vascular diseases/18 inflammatory bowel disease,/18 graft-versus-host\ndisease (GVHD/18 genetic diseases/18 kidney disease/18 kidney disease (CKD/18 chronic obstructive pulmonary disease (COPD/18 lung diseases/17\nunderlying disease./17 Paget\u2019s disease/17 allergic diseases/17 stable disease./17 cerebrovascular disease./17 infectious disease/17 disease-associated/17\nend-stage renal disease./17 cerebrovascular disease,/17 pulmonary disease./17 infectious disease./17 malignant diseases/16 ischemic heart disease./16\ndisease, especially/16 malignant disease./16 disease, particularly/16 coronary disease./16 progressive disease/16 other diseases/16 kidney disease,/16\ndisease activity/16 course of the disease/15 vascular disease,/15 a disease/15 Hodgkin\u2019s disease,/15 cardiac disease/15 Graves\u2019 disease/15 congenital\nheart disease./15 periodontal diseases/15 chronic diseases/15 infectious diseases,/15 recurrent disease./15 the disease/15 severity of the disease./15 in-\nflammatory bowel disease (IBD)/15 coronary heart disease (CHD)./15 various diseases./15 obstructive pulmonary disease./15 disease, and/15 recurrent\ndisease/15\nExample 7.2 Terminological expressions obtained for kernel \u201csyndrome\u201d: syndrome/549 syndrome./287 syndrome,/173 syndromes/157 metabolic syndrome/100 syndromes./82 this syndrome/62 syndromes,/60 metabolic\nsyndrome./47 nephrotic syndrome/44 this syndrome./33 acquired immunodeficiency syndrome (AIDS/33 metabolic syndrome,/31 Down syndrome/30\nsyndrome)/27 Down\u2019s syndrome/26 Sjo\u0308gren\u2019s syndrome/26 The syndrome/22 irritable bowel syndrome/20 syndrome)./20 syndrome and/20 carpal\ntunnel syndrome/19 Cushing\u2019s syndrome/19 syndrome characterized/19 respiratory distress syndrome/17 a syndrome/16 acquired immune deficiency\nsyndrome (AIDS)/16 Cushing\u2019s syndrome./16 compartment syndrome/16 This syndrome/15 irritable bowel syndrome (IBS)/14 long QT syndrome/13\nDown syndrome./13 toxic shock syndrome/13 nephrotic syndrome./12 Cushing\u2019s syndrome,/12 Rett syndrome/12 myelodysplastic syndrome (MDS/12\nmeconium aspiration syndrome/11 clinical syndrome/11 syndrome associated/11 -syndrome/11 Budd-Chiari syndrome/11 Turner\u2019s syndrome/11 Down\nsyndrome,/11 coronary syndromes/11 polycystic ovary syndrome (PCOS)/11 syndrome&quot/11 Tourette syndrome/11 this syndrome,/10 Down\u2019s\nsyndrome,/10 syndrome is/10 The metabolic syndrome/10 antiphospholipid syndrome/10 metabolic syndrome (MetS)/10 \u2019s syndrome/9 syndrome\nis presented./9 Wolff-Parkinson-White syndrome/9 Down syndrome (DS)/9 respiratory distress syndrome,/9 acute coronary syndrome/9 hepatopul-\nmonary syndrome/9 acquired immunodeficiency syndrome/9 pain syndromes/8 Marfan\u2019s syndrome/8 adult respiratory distress syndrome/8 respiratory\ndistress syndrome (RDS/8 nephrotic syndrome,/8 Reiter\u2019s syndrome/8 syndrome in/8 syndrome, including/8 Turner syndrome/8 Tourette\u2019s syndrome,/8\nMarfan syndrome/8 myelodysplastic syndrome/8 coronary syndrome/8 Metabolic syndrome/8 pain syndrome/8 pain syndrome./8 Marfan syndrome./8\nLennox-Gastaut syndrome/8 sicca syndrome/8 Cockayne syndrome/8 Sheehan\u2019s syndrome/7 Goodpasture\u2019s syndrome/7 Sjo\u0308gren\u2019s syndrome./7 res-\npiratory distress syndrome./7 pain syndromes./7 acquired immunodeficiency syndrome (AIDS)./7 syndrome is characterized/7 Felty\u2019s syndrome/7\npolycystic ovary syndrome/7 withdrawal syndrome/7 sleep apnoea syndrome/7 Terson syndrome/7 coronary syndromes./7 Williams syndrome/7 ac-\nquired immunodeficiency syndrome./7 coronary syndrome./7 fatigue syndrome (CFS/7 Rett syndrome./7 Behc\u0327et\u2019s syndrome/7 SVC syndrome/6 Bart-\nter\u2019s syndrome/6 ) syndrome/6 Zollinger-Ellison syndrome/6 Down\u2019s syndrome./6 syndrome occurred/6 adult respiratory distress syndrome (ARDS)/6\nTourette\u2019s syndrome/6 Wiskott-Aldrich syndrome/6 fragile X syndrome,/6 obstructive sleep apnea syndrome/6 sudden infant death syndrome (SIDS)./6\nGuillain-Barre\u0301 syndrome/6 compartment syndrome,/6 syndrome&quot;./6 obstructive sleep apnea syndrome (OSAS)/6 syndromes such/6 respiratory\ndistress syndrome (ARDS)./6 systemic inflammatory response syndrome/6 wasting syndrome/6 Lowe syndrome/6 metabolic syndrome (MS)/6 Bru-\ngada syndrome/6 Apert syndrome/6 Beckwith-Wiedemann syndrome/6 Ogilvie\u2019s syndrome/6 syndrome patients./6 fragile X-associated tremor/ataxia\nsyndrome/6 Gilbert\u2019s syndrome/5 syndrome due/5 \u2019s syndrome,/5 deficiency syndrome/5 fat embolism syndrome/5 -like syndrome/5 syndrome (P/5\npain syndromes,/5 Reye\u2019s syndrome/5 adult respiratory distress syndrome./5 syndrome\u2019/5 sick sinus syndrome/5 abstinence syndrome/5 Raynaud\u2019s\nsyndrome/5 acquired immune deficiency syndrome (AIDS)./5 myelodysplastic syndromes/5 heart-asthenia syndrome/5 Sjo\u0308gren\u2019s syndrome,/5 Mar-\nfan\u2019s syndrome./5 syndrome caused/5 syndromes, including/5 HELLP syndrome/5 The clinical syndrome/5 carpal tunnel syndrome./5 sudden infant\ndeath syndrome (SIDS)/5 short bowel syndrome./5 syndrome (/5 post-polio syndrome/5 Turner syndrome./5 antiphospholipid syndrome./5 Zollinger-\nEllison syndrome (ZES)/5 fragile X syndrome./5 sudden infant death syndrome/5 Guillain-Barre\u0301 syndrome,/5 syndrome (HUS)/5 syndrome are de-\nscribed./5 Angelman syndrome/5 Kallmann syndrome/5 systemic inflammatory response syndrome (SIRS)/5 acquired immune deficiency syndrome/5\nsyndrome occurs/5 Stevens-Johnson syndrome/5 Klinefelter\u2019s syndrome/5 Bartter syndrome/5 the metabolic syndrome/5 Panayiotopoulos syndrome/5\nNoonan syndrome/5 polycystic ovary syndrome (PCOS)./5 polycystic ovary syndrome./5 Sweet\u2019s syndrome/5 metabolic syndrome and/5 acute coro-\nnary syndromes (ACS/5 syndrome was diagnosed/5 syndrome was diagnosed./5 syndrome-associated/5 WAGR syndrome/5 Churg-Strauss syndrome\n(CSS)/5 Sjogren\u2019s syndrome/5 post-Q-fever fatigue syndrome/5 coronary syndrome,/5 coronary syndromes (ACS/5 Hunter syndrome./5 Eisenmenger\nsyndrome./5 syndrome, especially/5 scimitar syndrome/5 5q- syndrome/5\nExample 7.3 Terminological expressions obtained for kernel \u201cinflammatory\u201d: inflammatory/381 anti-inflammatory/117 inflammatory response/111 proinflammatory/63 proinflammatory cytokines/52 inflammatory response./46\nantiinflammatory/41 pro-inflammatory/40 inflammatory reaction/37 inflammatory process/35 inflammatory cells/34 inflammatory mediators/33 inflam-\nmatory cytokines/33 inflammatory bowel/32 anti-inflammatory effects/31 inflammatory responses/31 pro-inflammatory cytokines/30 inflammatory\nbowel disease/29 inflammatory bowel disease./28 inflammatory cells./28 inflammatory markers/27 an inflammatory/26 inflammatory diseases/25 in-\nflammatory lesions/24 proinflammatory cytokine/24 pro-inflammatory cytokine/23 inflammatory processes/23 chronic inflammatory/23 anti-inflammatory\neffect/22 inflammatory diseases./22 inflammatory processes./21 inflammatory responses./21 inflammatory changes/21 inflammatory response,/19 in-\nflammatory process./19 inflammatory bowel disease,/18 inflammatory,/17 inflammatory reaction./17 inflammatory reactions/17 anti-inflammatory\ndrugs/16 inflammatory disorders/15 inflammatory conditions./15 inflammatory bowel disease (IBD)/15 the inflammatory/15 inflammatory infiltrate/14\ninflammatory cell infiltration/14 non-steroidal anti-inflammatory drugs (NSAIDs)/14 systemic inflammatory response/14 nonsteroidal anti-inflammatory\ndrugs/13 inflammatory disease./13 nonsteroidal anti-inflammatory drugs (NSAIDs/13 inflammatory conditions/13 anti-inflammatory activity/12 in-\nflammatory cytokine/12 anti-inflammatory properties/12 proinflammatory mediators/12 anti-inflammatory activity./12 inflammatory markers,/12 The\ninflammatory response/11 inflammatory mediators,/11 systemic inflammatory/11 inflammatory bowel disease (IBD)./11 inflammatory markers./11\ninflammatory bowel disease (IBD/11 The inflammatory/10 inflammatory disease/10 proinflammatory cytokines./10 inflammatory process,/10 anti-\ninflammatory agents/9 inflammatory diseases,/9 inflammatory disorders./9 inflammatory bowel diseases/9 The anti-inflammatory activity/9 inflamma-\ntory cytokines./9 inflammatory cytokines,/9 pro- and anti-inflammatory/9 pelvic inflammatory/8 anti-inflammatory drugs./8 inflammatory demyeli-\nnating/8 noninflammatory/8 anti-inflammatory properties./8 anti-inflammatory effects./8 proinflammatory cytokines,/8 neuroinflammatory/8 inflam-\nmatory cell/7 inflammatory changes./7 inflammatory infiltrate./7 inflammatory mediator/7 The anti-inflammatory/7 inflammatory parameters/7 in-\nflammatory diseases such/7 infiltration of inflammatory/7 nonsteroidal anti-inflammatory drug/7 inflammatory reactions./7 inflammatory cascade/7\nanti-inflammatory cytokine IL-10/7 inflammatory disease,/7 inflammatory state/7 inflammatory activity/6 nonsteroidal anti-inflammatory drugs./6 in-\nflammatory infiltrates/6 non-inflammatory/6 Anti-inflammatory/6 various inflammatory/6 anti-inflammatory cytokine/6 inflammatory pseudotumor/6\ninflammatory pain/6 inflammatory events/6 inflammatory conditions,/6 inflammatory infiltration/6 inflammatory mediators such/6 systemic inflamma-\ntory response syndrome/6 inflammatory mediators./6 pelvic inflammatory disease,/6 non-steroidal anti-inflammatory drug (NSAID)/6 macrophage in-\nflammatory protein-1alpha/6 of inflammatory/6 inflammatory states./6 pro-inflammatory cytokines,/6 pro-inflammatory cytokines./6 Pro-inflammatory\ncytokines/6 inflammatory responses,/6 non-steroidal anti-inflammatory drugs/6 macrophage inflammatory/6 inflammatory changes,/6 Proinflamma-\ntory cytokines/6 anti-inflammatory agents./5 fibro-inflammatory/5 pelvic inflammatory disease (PID)/5 inflammatory pain./5 nonsteroidal antiinflam-\nmatory/5 inflammatory reactions,/5 necroinflammatory/5 inflammatory and neoplastic/5 inflammatory skin/5 inflammatory lung/5 An inflammatory/5\nproinflammatory mediators,/5 non-specific inflammatory/5 systemic inflammatory response syndrome (SIRS)/5 inflammatory reaction,/5 proinflam-\nmatory cytokine,/5 active inflammatory bowel/5 nonsteroidal anti-inflammatory drugs,/5 inflammatory disorder/5 inflammatory stimuli/5 postinflam-\nmatory/5 non-steroidal anti-inflammatory drugs,/5 production of proinflammatory/5 inflammatory component./5 systemic inflammatory response./5\nseveral inflammatory/5 inflammatory status/5 inflammatory cell infiltration./5 inflammatory signaling/5 anti-inflammatory activities./5 inflammatory\nstimuli./5 non-steroidal anti-inflammatory drug/5 nonsteroidal antiinflammatory drugs (NSAIDs)/5 release of inflammatory mediators/5"}], "references": [{"title": "Efficient string mathing: An aid to bibliographic search", "author": ["Alfred Aho", "Margaret Corasick"], "venue": "Communication of the Association for Computing Machinery,", "citeRegEx": "Aho and Corasick.,? \\Q1975\\E", "shortCiteRegEx": "Aho and Corasick.", "year": 1975}, {"title": "An optimal algorithm for shortest paths on weighted interval and circular-arc graphs with applications", "author": ["Mikhail Atallah", "Danny Chen", "D. Lee"], "venue": "Technical report, Purdue University,", "citeRegEx": "Atallah et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Atallah et al\\.", "year": 1993}, {"title": "Compete inverted files for effixient text retrieval", "author": ["A. Blumer", "J. Blumer", "D. Haussler", "R. McConnell", "A. Ehrenfeucht"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "Blumer et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Blumer et al\\.", "year": 1987}, {"title": "Posterior Convergence under Incomplete Information", "author": ["D.E. Ben-Tal", "A. abd Brown", "R.L. Smith"], "venue": "Technical report, Department of Industrial and Operations Engineering,", "citeRegEx": "Ben.Tal et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Ben.Tal et al\\.", "year": 1987}, {"title": "Gated recursive neural network for chinese word segmentation", "author": ["Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Micahel Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Empirical studies on machine learning based text classification algorithms", "author": ["Shweta C. Dharmadhikari", "Maya Ingle", "Parag Kulakrni"], "venue": "Advanced Computing: An International Journal,", "citeRegEx": "Dharmadhikari et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dharmadhikari et al\\.", "year": 2011}, {"title": "Learning regular languages using rfsas", "author": ["Fran\u00e7ois Denis", "Aur\u00e9lien Lemay", "Alain Terlutte"], "venue": "Theoretical computer science,", "citeRegEx": "Denis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Denis et al\\.", "year": 2004}, {"title": "Combining semantic and syntactic generalization in example-based machine translation", "author": ["Sarah Ebling", "Andy Way", "Martin Volk", "Sudip Kumar Naskar"], "venue": "In Proceedings of the 15th Conference of the European Association for Machine Translation,", "citeRegEx": "Ebling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ebling et al\\.", "year": 2011}, {"title": "The Structure of English. An Introduction to the Construction of English Sentences", "author": ["Charles Fries"], "venue": "New York: Harcourt, Brace and Company,", "citeRegEx": "Fries.,? \\Q1952\\E", "shortCiteRegEx": "Fries.", "year": 1952}, {"title": "A bayesian framework for word segmentation: Exploring effects of context", "author": ["Sharon Goldwater", "Thomas L. Griffiths", "Mark Johnson"], "venue": null, "citeRegEx": "Goldwater et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2009}, {"title": "Consistency of Empirical Likelihood and Maximum A-Posteriori Probability under Misspecification", "author": ["Marian Grendar", "George G. Judge"], "venue": "Technical report, Department of Agricultural & Resource Economics,", "citeRegEx": "Grendar and Judge.,? \\Q2008\\E", "shortCiteRegEx": "Grendar and Judge.", "year": 2008}, {"title": "Unsupervised learning of the morphology of a natural language", "author": ["John A. Goldsmith"], "venue": "Computational linguistics,", "citeRegEx": "Goldsmith.,? \\Q2001\\E", "shortCiteRegEx": "Goldsmith.", "year": 2001}, {"title": "Exploring the Grammar of Natural Language Using Index Structures. PhD thesis, Center for language and information processing (CIS)", "author": ["Johannes Goller"], "venue": "University of Munich,", "citeRegEx": "Goller.,? \\Q2010\\E", "shortCiteRegEx": "Goller.", "year": 2010}, {"title": "The construction of local grammars", "author": ["Maurice Gross"], "venue": "Finite-State Language Processing,", "citeRegEx": "Gross.,? \\Q1997\\E", "shortCiteRegEx": "Gross.", "year": 1997}, {"title": "A bootstrap method for constructing local grammars", "author": ["Maurice Gross"], "venue": "In Contemporary Mathematics: Proceedings of the Symposium, University of Belgrad,", "citeRegEx": "Gross.,? \\Q1999\\E", "shortCiteRegEx": "Gross.", "year": 1999}, {"title": "Algorithms on strings, trees and sequences: computer science and computational biology", "author": ["Dan Gusfield"], "venue": "Cambridge university press,", "citeRegEx": "Gusfield.,? \\Q1997\\E", "shortCiteRegEx": "Gusfield.", "year": 1997}, {"title": "A theory of language and information. A mathematical approach", "author": ["S. Harris"], "venue": null, "citeRegEx": "Harris.,? \\Q1991\\E", "shortCiteRegEx": "Harris.", "year": 1991}, {"title": "A formal basis for the heuristic determination of minimal cost paths. Systems Science and Cybernetics", "author": ["Peter Hart", "Nils Nillson", "Bertram Raphael"], "venue": "IEEE Transactions on,", "citeRegEx": "Hart et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Hart et al\\.", "year": 1968}, {"title": "German-learning infants\u2018 ability to detect unstressed closed-class elements in continuous speech", "author": ["Barbara H\u00f6hle", "J\u00fcrgen Weissenborn"], "venue": "Developmental Science,", "citeRegEx": "H\u00f6hle and Weissenborn.,? \\Q2003\\E", "shortCiteRegEx": "H\u00f6hle and Weissenborn.", "year": 2003}, {"title": "On-line construction of symmetric compact directed acyclic word graphs", "author": ["Shunsuke Inenaga", "Hiromasa Hoshino", "Ayumi Shinohara", "Masayuki Takeda", "Setsuo Arikawa"], "venue": "In Proc. of 8th International Symposium on String Processing and Information Retrieval", "citeRegEx": "Inenaga et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Inenaga et al\\.", "year": 2001}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": null, "citeRegEx": "Koehn.,? \\Q2010\\E", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "Unsupervised multilingual sentence boundary detection", "author": ["Tibor Kiss", "Jan Strunk"], "venue": "Computational Linguistics,", "citeRegEx": "Kiss and Strunk.,? \\Q2006\\E", "shortCiteRegEx": "Kiss and Strunk.", "year": 2006}, {"title": "Online em for unsupervised models", "author": ["Percy Liang", "Dan Klein"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Liang and Klein.,? \\Q2009\\E", "shortCiteRegEx": "Liang and Klein.", "year": 2009}, {"title": "Improving retrieval results with discipline-specific query expansion", "author": ["Thomas L\u00fcke", "Philipp Schaer", "Philipp Mayr"], "venue": "In Theory and Practice of Digital Libraries,", "citeRegEx": "L\u00fcke et al\\.,? \\Q2012\\E", "shortCiteRegEx": "L\u00fcke et al\\.", "year": 2012}, {"title": "Extraction of significant phrases from text", "author": ["Yuan Lui"], "venue": "International Journal of Computer, Control, Quantum and Information Engineering,", "citeRegEx": "Lui.,? \\Q2007\\E", "shortCiteRegEx": "Lui.", "year": 2007}, {"title": "Introduction to Information Retrieval", "author": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "The alignment template approach to statistical machine translation", "author": ["Franz Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och and Ney.,? \\Q2004\\E", "shortCiteRegEx": "Och and Ney.", "year": 2004}, {"title": "Chinese segmentation and new word detection using conditional random fields", "author": ["Fuchun Peng", "Fangfang Feng", "Andre McCallum"], "venue": "Technical report, University of Massachusetts - Amherst,", "citeRegEx": "Peng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2004}, {"title": "Corpus-based terminology extraction", "author": ["Alexandre Patry", "Philippe Langlais"], "venue": "In Proceedings of the 7th International Conference on Terminology and Knowledge Engineering,", "citeRegEx": "Patry and Langlais.,? \\Q2005\\E", "shortCiteRegEx": "Patry and Langlais.", "year": 2005}, {"title": "Automatic keyword extraction from individual documents", "author": ["Stuart Rose", "Dave Engel", "Nick Cramer", "Wendy Cowley"], "venue": "Text Mining,", "citeRegEx": "Rose et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rose et al\\.", "year": 2010}, {"title": "Probabilistic context-free grammars", "author": ["Yasubumi Sakakibara"], "venue": "Encyclopedia of Machine Learning,", "citeRegEx": "Sakakibara.,? \\Q2010\\E", "shortCiteRegEx": "Sakakibara.", "year": 2010}, {"title": "Machine learning in automated text categorization", "author": ["Fabrizio Sebastiani"], "venue": "ArXiv e-prints,", "citeRegEx": "Sebastiani.,? \\Q2001\\E", "shortCiteRegEx": "Sebastiani.", "year": 2001}, {"title": "Unsupervised morphology induction using word embeddings", "author": ["Radu Soricut", "Franz Och"], "venue": "In Proc. NAACL,", "citeRegEx": "Soricut and Och.,? \\Q2015\\E", "shortCiteRegEx": "Soricut and Och.", "year": 2015}, {"title": "A vector model for syntagmatic and paradigmatic relatedness", "author": ["Hinrich Sch\u00fctze", "Jan Pedersen"], "venue": "In Proc. of Ninth Annual Conference ofthe UW Centre for the New OED and Text Research,", "citeRegEx": "Sch\u00fctze and Pedersen.,? \\Q1993\\E", "shortCiteRegEx": "Sch\u00fctze and Pedersen.", "year": 1993}, {"title": "Lexical-semantic Relations: Theoretical and Practical Perspectives. Linguisticae investigationes: Supplementa : LIS : studies in French and general linguistics", "author": ["Petra Storjohann"], "venue": null, "citeRegEx": "Storjohann.,? \\Q2010\\E", "shortCiteRegEx": "Storjohann.", "year": 2010}, {"title": "Recognition and representation of function words in english learning infants", "author": ["Rushen Shi", "Janet Werker", "Anne Cutler"], "venue": "Infancy, 10(2):187\u2013198,", "citeRegEx": "Shi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2006}, {"title": "Efficient parsing for natural language: A fast algorithm for practical systems, volume 8", "author": ["Masaru Tomita"], "venue": "Springer Science & Business Media,", "citeRegEx": "Tomita.,? \\Q2013\\E", "shortCiteRegEx": "Tomita.", "year": 2013}, {"title": "Wie Kinder Sprachen Lernen: Und wie wir sie dabei unterst\u00fctzen k\u00f6nnen", "author": ["Rosemarie Tracy"], "venue": "Nar Francke Attempto Verlag,", "citeRegEx": "Tracy.,? \\Q2007\\E", "shortCiteRegEx": "Tracy.", "year": 2007}, {"title": "Improved semantic representations form tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "ArXiv e-prints,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "On-line construction of suffix-trees", "author": ["Esko Ukkonen"], "venue": null, "citeRegEx": "Ukkonen.,? \\Q1995\\E", "shortCiteRegEx": "Ukkonen.", "year": 1995}, {"title": "Unsupervised acquiring of morphological paradigms from tokenized text. In Advances in Multilingual and Multimodal Information, pages 892\u2013899", "author": ["Daniel Zeman"], "venue": null, "citeRegEx": "Zeman.,? \\Q2008\\E", "shortCiteRegEx": "Zeman.", "year": 2008}, {"title": "Combination of unsupervised keyphrase extraction algorithms", "author": ["Zede Zhu", "Miao LI", "Lei Chen", "Zhenxin Yang", "Sheng Chen"], "venue": "In 2013 International Conference on Asian Language Processing,", "citeRegEx": "Zhu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "When looking at the structure of natural language, \u201cphrases\u201d and \u201dwords\u201d are central notions. We consider the problem of identifying such \u201cmeaningful subparts\u201d of language of any length and underlying composition principles in a completely corpus-based and language-independent way without using any kind of prior linguistic knowledge. Unsupervised methods for identifying \u201cphrases\u201d, mining subphrase structure and finding words in a fully automated way are described. This can be considered as a step towards automatically computing a \u201cgeneral dictionary and grammar of the corpus\u201d. We hope that in the long run variants of our approach turn out to be useful for other kind of sequence data as well, such as, e.g., speech, genom sequences, or music annotation. Even if we are not primarily interested in immediate applications, results obtained for a variety of languages show that our methods are interesting for many practical tasks in text mining, terminology extraction and lexicography, search engine technology, and related fields.", "creator": "LaTeX with hyperref package"}}}