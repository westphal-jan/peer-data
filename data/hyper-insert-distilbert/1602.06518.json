{"id": "1602.06518", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2016", "title": "Multi-task Learning with Labeled and Unlabeled Tasks", "abstract": "in this previous paper we again consider proposing the several problem dimensions of multi - task group learning, in which premise a continuously learner system is given a collection of prediction based tasks exactly that need to be concurrently solved. in contrast to essentially previous work, there we give thinking up on the assumption that fixed labeled training data collection is available for all tasks. instead, we propose for an active task output selection specification framework, where based however only mostly on checking the known unlabeled individual data, the learner can choose a, typically small, subset of tasks for which he gets some labeled examples. for recognizing the remaining mental tasks, which presumably have no available mental annotation, solutions thereof are found by gradually transferring information from in the selected specific tasks. we additionally analyze into two transfer strategies and develop generic generalization bounds for each of them. broadly based on understanding this theoretical system analysis construct we mainly propose two algorithms named for making meaningful the default choice of discrete labeled tasks in representing a narrowly principled way and show their effectiveness on synthetic and using real interactive data.", "histories": [["v1", "Sun, 21 Feb 2016 11:18:10 GMT  (73kb,D)", "http://arxiv.org/abs/1602.06518v1", null], ["v2", "Wed, 30 Mar 2016 09:30:21 GMT  (73kb,D)", "http://arxiv.org/abs/1602.06518v2", null], ["v3", "Wed, 1 Mar 2017 12:22:56 GMT  (1705kb,D)", "http://arxiv.org/abs/1602.06518v3", null], ["v4", "Thu, 8 Jun 2017 09:14:03 GMT  (1706kb,D)", "http://arxiv.org/abs/1602.06518v4", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["anastasia pentina", "christoph h lampert"], "accepted": true, "id": "1602.06518"}, "pdf": {"name": "1602.06518.pdf", "metadata": {"source": "CRF", "title": "Active Task Selection for Multi-Task Learning", "authors": ["Anastasia Pentina", "Christoph H. Lampert"], "emails": ["apentina@ist.ac.at", "chl@ist.ac.at"], "sections": [{"heading": "1 Introduction", "text": "In multi-task learning a learner is given a collection of prediction tasks that need to be solved. By learning all tasks jointly instead of independently he can identify similarities between the task and transfer information between them, thereby potentially improving the prediction quality. All existing multi-task learning approaches have in common, however, that they need at least some labeled training data from each task in order to solve it.\nIn this paper, we study a new and more challenging setting, in which only for a subset of the tasks (typically a small minority) labeled data is needed. In practice, it is highly desirable to be able to handle this situation for prediction problem, for which the fixed cost of obtaining any labels for a task can be high, even when the variable cost per label are reasonable. Examples of such learning tasks are personalized classifiers for which labels are provided by end users. For example, to build a personalized speech recognition system, it would be preferable to have only a few speakers annotate a reasonable amount of data each, instead of collecting a few labeled examples from every potential user of the system. Similarly, the fixed cost of obtaining labels is high when the annotators are non-experts and first have to be trained for any task. This is, e.g., a major issue when using Amazon Mechanical Turk for data annotation: recruiting and training annotators first imposes a large overhead, and only afterwards many labels can be obtained within a short time and at a low cost.\nEspecially if the number of tasks is large and many of them are related, the labels only for a representative subset of tasks may already contain enough information to solve all of them, namely by transferring information from the labeled to the unlabeled tasks. Whether this strategy will succeed or not depends on the answer to two core questions: how representative is the subset of labeled tasks, and how effective can information be transferred between the tasks? One of our contributions in this work is a theoretical approach to quantify these effects in the form of two generalization bounds.\nThe fact that different labeled task subsets lead to predictors of different quality suggests that it will be beneficial if the labeled subset is not arbitrary but can be chosen in a data-dependent way. We call this learning scenario active task selection, and formalize it as follows: initially, for each task that should be learned only unlabeled samples are available. The learner then chooses for which of the tasks he wants to request some labels. After obtaining those, the learner constructs solutions for all tasks. We believe this is a scenario of great practical potential, but to our knowledge we are the first to formally formulate and study it.\nActive task selection resembles active learning, where one also chooses objects to be labeled (though individual examples instead of tasks), and hopes for better predictions by choosing the objects of interest in an intelligent manner instead of, e.g., randomly. The multi-task setting, however, adds a second level of complication: where active learning only needs to identify one prediction function for all data, in multi-task learning each task requires its own predictor, including the unlabeled ones. Therefore, one also has to decide on a method for transferring information from labeled to unlabeled tasks, a problem typically studied in domain adaptation. In fact, the transfer\nar X\niv :1\n60 2.\n06 51\n8v 1\n[ st\nat .M\nL ]\n2 1\nFe b\n20 16\nmethod should be taken into account already when choosing the tasks to be labeled, as different transfer methods will likely require different tasks to be labeled for optimal effectiveness.\nIn this paper we concentrate on two transfer methods that use the discrepancy distance [14] to quantify the similarity between unlabeled tasks. One method deals with the case where each unlabeled task is solved by transferring the predictor from the nearest labeled task (single-source transfer). The second method allows transferring from multiple labeled task (multi-source transfer). For each of them we prove a new generalization bound that upper bounds the total multi-task error by quantities depending on the labeled tasks and the way information is transferred between tasks. Using the computable quantities in the bounds as objective functions and minimizing them, we obtain principled algorithms for selecting which tasks to have labeled and for choosing predictors for all tasks, labeled as well as unlabeled."}, {"heading": "2 Related Work", "text": "Most existing multi-task learning methods work in the fully supervised setting and rely on the idea of improving the overall prediction quality by sharing information between the tasks. For this, they either assume that the predictors for all tasks are similar to each other in some norm and exploit this fact through specific regularizers [9], or they assume that the predictors for all tasks share a common low-dimensional representation that can be learned from the data [8, 1]. Follow-up works extended and generalized these concepts, e.g. learning the relatedness of tasks [25, 12] or sharing only between subgroups of tasks [30, 13, 3]. However, all of the above methods require labeled data for each task, because they relate tasks to each other by means of their predictors.\nActive learning has so far not found widespread use in the multi-task setting. Two works in this direction are [21, 24], which, however, use active learning on the level of training examples, not tasks. The idea of choosing tasks was used in active curriculum selection [23, 20], where the learner can influence the order in which tasks are processed. However these methods nevertheless require annotated examples for all tasks of interest.\nTo transfer information between tasks, our work builds on existing results for single-source and multi-source domain adaptation [5, 14]. We choose these because they come with theoretical guarantees and therefore allow us to prove generalization bounds and derive principled algorithms. We suspect, however, that also other domain adaptation techniques could be exploited for the active task selection scenario, in particular those based on source reweighting [26], representation learning [18, 10], or semi-supervised transfer [29]."}, {"heading": "3 Preliminaries", "text": "Before we explain the details of our contribution, we introduce some notation and restate some central definitions and results from the multi-task and domain adaptation literature."}, {"heading": "3.1 Formal setting", "text": "In the multi-task setting the learner observes a collection of prediction tasks and its goal is to learn all of them. Formally, we assume that there is a set of T tasks {\u3008D1, f1\u3009, . . . , \u3008DT , fT \u3009}, where each task t is defined by a marginal distribution Dt over the input space X and a deterministic labeling function ft : X \u2192 Y . The goal of the learner is to find T predictors h1, . . . , hT in a hypothesis set H \u2282 {h : X \u2192 Y} that would minimize the average expected risk:\ner(h1, . . . , hT ) = 1\nT T\u2211 t=1 ert(ht), (1)\nwhere ert(ht) = E x\u223cDt `(ht(x), ft(x)).\nIn this work we concentrate on the case of binary tasks, Y = {\u22121, 1}, and 0/1-loss, `(y1, y2) = 0 if y1 = y2, and `(y1, y2) = 1 otherwise. However, we expect that the analysis can be extended to hold for a general bounded loss function that satisfies triangle inequality.\nIn the fully-supervised setting the learner is given a training set of annotated examples for every task of interest. In contrast, in the active task selection scenario initially every task t is represented only by a set St = {xt1, . . . , xtn} of n unlabeled examples sampled i.i.d. according to the marginal distribution Dt. Based on this data the learner is allowed to choose k tasks {i1, . . . , ik} and for each of them he obtains labels for a random subset Sij \u2282 Sij of m points. Since for all other tasks the learner has access only to unlabeled data, in order to find predictors for them he has to transfer information from the chosen labeled tasks."}, {"heading": "3.2 Background on domain adaptation", "text": "Unsupervised domain adaptation considers the problem of learning a predictor for a target domain, for which there is only unlabeled data available, using the labeled data from a different, source, domain. The success of any such method depends on how similar the source is to the target. For both methods that we consider in this work a sensible measure of similarity is provided by the notion of discrepancy:\nDefinition 1 (Definition 4 in [14]). The discrepancy between distributions D1 and D2 over X with respect to a hypothesis set H is defined as:\ndisc(D1, D2) = max h,h\u2032\u2208H\n|erD1(h, h\u2032)\u2212 erD2(h, h\u2032)| , (2)\nwhere erDi(h, h \u2032) = Ex\u223cDi `(h(x), h \u2032(x)).\nThis measure has two advantages: 1. The discrepancy allows relating the performance of a hypothesis on one task to its performance on a\ndifferent task:\nProposition 1 (Theorem 2 in [4]). For any two tasks \u3008D1, f1\u3009 and \u3008D2, f2\u3009 and any hypothesis h \u2208 H the following holds: er2(h) \u2264 er1(h) + disc(D1, D2) + \u03bb12, where \u03bb12 = minh\u2208H(er1(h) + er2(h)).\n2. The discrepancy can be estimated from unlabeled samples:\nProposition 2 (Lemma 1 in [4]). Let d be the VC dimension of the hypothesis set H and S1, S2 be two i.i.d. samples of size n from D1 and D2 respectively. Then for any \u03b4 > 0 with probability at least 1\u2212 \u03b4:\ndisc(D1, D2)\u2264disc(S1, S2)+2 \u221a 2d log(2n) + log(2/\u03b4)\nn ,\nwhere\ndisc(S1, S2) = max h,h\u2032\u2208H\n|e\u0302rS1(h, h\u2032)\u2212 e\u0302rS2(h, h\u2032)|\nis the empirical discrepancy between the samples and\ne\u0302rSi(h, h \u2032) =\n1\nn \u2211 x\u2208Si `(h(x), h\u2032(x))."}, {"heading": "4 Transfer from a single task", "text": "Probably the most straightforward method that can be used in unsupervised domain adaptation is to train a classifier on the labeled examples from the source task and directly use it on the target. In this section we examine what the optimal choice of labeled tasks would be if this transfer method is used to learn the remaining tasks.\nFormally, we assume that based on the unlabeled data the learner chooses k tasks and assigns each of the remaining unlabeled tasks to one of them. Each unlabeled task is solved by using the hypothesis trained on the corresponding labeled task (Figure 1). We encode such an assignment by a vector C = (c1, . . . , cT ) that has at most k different components. These values correspond to the chosen labeled tasks and ct specifies which of them is used as a source of information for the t-th task. The following theorem provides an upper-bound on the performance of this approach:\nTheorem 1. Let d be the VC dimension of the hypothesis set H , k be the maximum number of tasks for which the learner may ask for labels, S1, . . . , ST be T random sets of size n each, where St\ni.i.d.\u223c Dt , and S1, . . . , ST be their random subsets of size m each, for which labels can be provided upon learner\u2019s request. Then, for any \u03b4 > 0 with probability at least 1\u2212\u03b4 uniformly for all possible choices of the assignments C and the corresponding hypotheses, the following inequality holds:\n1\nT T\u2211 t=1 ert(hct) \u2264 1 T T\u2211 t=1 e\u0302rct(hct) + 1 T T\u2211 t=1 disc(St, Sct) + 1 T T\u2211 t=1 \u03bbtct\n+2\n\u221a 2d log(2n) + 2 log(T ) + log(4/\u03b4)\nn +\n\u221a 2d log(em/d)\nm +\n\u221a log(T ) + log(2/\u03b4)\n2m , (3)\nwhere e\u0302rt(h) = 1m \u2211\n(x,y)\u2208St `(h(x), y).\nProof Sketch (the full proof can be found in Appendix B). First, we use Proposition 1 to relate the average expected risk on all tasks (1) to the expected risk on only those tasks that were chosen by the learner to be labeled:\n1\nT T\u2211 t=1 ert(hct) \u2264 1 T T\u2211 t=1 erct(hct) + 1 T T\u2211 t=1 disc(Dt, Dct) + 1 T T\u2211 t=1 \u03bbtct . (4)\nThe statement of the theorem then follows from (4) by applying Proposition 2 to every pair of tasks, applying the standard VC bound to each individual task and combining them by a union bound argument.\nInterpretation. The left hand side of (3) is the average expected error on all T tasks, the quantity that the learner would like to minimize, but cannot directly compute. It is upper bounded by the sum of three complexity terms and three task-dependent terms: training errors on the labeled tasks, average distances to the prototype in terms of the empirical discrepancies and an average of \u03bb-s.\nFirst, note that as the number of unlabeled examples per task n tends to infinity, the first complexity term converges to 0 as 1/ \u221a n, showing that in this case the discrepancies between the tasks can be estimated precisely. If in addition the number of labeled examples for each of the chosen tasks m tends to infinity, the remaining complexity terms converge to 0 as 1/ \u221a m and inequality (3) reduces to (4), which is the best we can expect for the considered type of information transfer. As the bound in Proposition 1, the result of Theorem 1 is relative to \u03bb. While discrepancy captures the similarity between the marginal distributions, \u03bb in addition embodies the similarity between the labeling functions. For every pair of unlabeled task t and the corresponding labeled task ct, \u03bbtct is small if there exists a hypothesis that performs well on both these tasks. As in unsupervised domain adaptation this quantity cannot be estimated based on the data. However, if it is large, there is no classifier that works well on both domains and one cannot expect to find a good hypothesis for the target t by training only on the source ct. Note that not all \u03bbij have to be small for (3) to be useful, but only the ones between an unlabeled task and the labeled one it is assigned to.\nThe remaining two terms in the right hand side of (3) - the average empirical error and the sum of discrepancies - depend on the chosen labeled tasks and can be estimated from the data. Therefore they can be used as a quality measure of the assignmentC and the corresponding hypotheses to guide the learner. However, the choice of which tasks to label, of course, has to be done based only on the unlabeled data. The only data-dependent part of (3) that can be evaluated at this stage and therefore used to direct this choice is the average discrepancy between the tasks with respect to the assignment C:\n1\nT T\u2211 t=1 disc(St, Sct). (5)\nThis quantity can be interpreted as the k-medoids clustering risk where tasks correspond to points in the space with (semi-)metric defined by empirical discrepancy and labeled tasks correspond to the centers of the clusters. Therefore we propose the following strategy for the active task selection with single-source transfer (ATS-SS):\nAlgorithm 1 (ATS-SS). 1. estimate pairwise discrepancies between the tasks based on the unlabeled data 2. cluster the tasks using the k-medoids method based on the obtained empirical discrepancies 3. train classifiers for the cluster centers and transfer them to the other tasks in the corresponding clusters.\nSince the inequality (3) holds uniformly with respect to assignment C and the corresponding hypotheses, it also holds for the output of ATS-SS. Therefore, by finding an assignment with a low value of the right hand side of (3) the learner is guaranteed to have low average expected error. The only non-controllable term 1T \u2211T t=1 \u03bbtct will be small in this case if tasks with similar marginal distribution, i.e. close with respect to empirical discrepancy, are likely to have similar labeling functions. This can be seen as an analog of a \u201dsmoothness\u201d assumption in semi-supervised learning that states that close points are likely to have similar labels."}, {"heading": "5 Transfer from multiple tasks", "text": "In the previous section we assumed that for solving every task the learner uses only one of the chosen labeled tasks as a source of information. However, this is not the only solution since after the labels for the chosen tasks are obtained the learner is in the multi-source domain adaptation setting: potentially all k labeled tasks could be used to obtain predictors for the remaining unlabeled tasks and it might be suboptimal to use only one of them. Moreover, even for learning the labeled tasks it might be beneficial to transfer information from the other labeled tasks as well.\nIn order to exploit this possibility we consider an extension of the method described in the previous section for the case of multiple sources. Instead of training a classifier for the target domain based on the labeled data from only a single source, this method minimizes a convex combination of training errors on several source domains.\nFor a set of tasks I = {i1, . . . , ik} \u2282 {1, . . . , T} define:\n\u039bI = { \u03b1 \u2208 [0, 1]T :\nT\u2211 i=1 \u03b1i = 1; supp\u03b1 \u2286 I\n} (6)\nfor supp\u03b1 = {i \u2208 {1, . . . , T} : \u03b1i 6= 0}. Given a weight vector \u03b1 \u2208 \u039bI , an \u03b1-weighted empirical error of a hypothesis h \u2208 H is defined as follows:\ne\u0302r\u03b1(h) = \u2211 i\u2208I \u03b1ie\u0302ri(h). (7)\nWe consider the setting where the learner in order to obtain a solution for every task t minimizes e\u0302r\u03b1t(h) for some \u03b1t \u2208 \u039bI , where I is the set of labeled tasks (Figure 2). Note that this approach reduces to the single-source transfer described in the previous section if every weight vector \u03b1t has only one non-zero component. Real-values weights, however, can potentially improve the performance and choosing the set of tasks to label based on the k-medoids approach described in the previous section might not be optimal in this case. Therefore we develop an analog of Theorem 1 that can be used to make a principled choice.\nTheorem 2. Let d be the VC dimension of the hypothesis set H , k be the maximum number of tasks for which the learner may ask for labels, S1, . . . , ST be T sets of size n each, where Si\ni.i.d.\u223c Di, and S1, . . . , ST be their random subsets of size m each for which labels would be provided upon learner\u2019s request. Then for any \u03b4 > 0, provided that the choice of labeled tasks I = {i1, . . . , ik} and the weights \u03b11, . . . , \u03b1T \u2208 \u039bI are fully determined by the unlabeled data only, the following inequality holds with probability at least 1 \u2212 \u03b4 over S1, . . . , ST and\nS1, . . . , ST for all possible choices of h1, . . . , hT \u2208 H:\n1\nT T\u2211 t=1 ert(ht)\u2264 1 T T\u2211 t=1 e\u0302r\u03b1t(ht)+ 1 T T\u2211 t=1 \u2211 i\u2208I \u03b1ti disc(St, Si) + A T \u2016\u03b1\u20162,1 + B T \u2016\u03b1\u20161,2 +C+D+ 1 T T\u2211 t=1 \u2211 i\u2208I \u03b1ti\u03bbti,\n(8)\nwhere:\n\u2016\u03b1\u20162,1 = T\u2211 t=1 \u221a\u2211 i\u2208I (\u03b1ti) 2, \u2016\u03b1\u20161,2 = \u221a\u221a\u221a\u221a\u2211 i\u2208I ( T\u2211 t=1 \u03b1ti )2 , A = \u221a 2d log(ekm/d) m , B = \u221a log(4/\u03b4) 2m ,\nC =\n\u221a 8(log T + d log(enT/d))\nn +\n\u221a 2\nn log\n4 \u03b4 , D = 2\n\u221a 2d log(2n) + 2 log(T ) + log(4/\u03b4)\nn .\nProof Sketch (the full proof can be found in the supplemental material). As for Theorem 1, we begin with bounding the average expected error over all tasks by the error on the labeled tasks:\n1\nT T\u2211 t=1 ert(ht)\u2264 1 T T\u2211 t=1 er\u03b1t(ht) + 1 T T\u2211 t=1 \u2211 i\u2208I \u03b1ti disc(Dt, Di) + 1 T T\u2211 t=1 \u2211 i\u2208I \u03b1ti\u03bbti,\nwhere: er\u03b1t(ht) = \u2211 i\u2208I \u03b1ti E x\u223cDi `(ht(x), fi(x)). (9)\nIn order to prove the statement of the theorem we need to relate the \u03b1-weighted expected errors and discrepancies between the marginal distributions in (31) to their empirical estimates.\nThe proof consists of three steps. First, we show that, conditioned on the unlabeled data, 1T \u2211T t=1 e\u0303r\u03b1t can be\nupper bounded in terms of 1T \u2211T t=1 e\u0302r\u03b1t , where:\ne\u0303r\u03b1(h) = \u2211 i\u2208I \u03b1ie\u0303ri(h) = \u2211 i\u2208I \u03b1i n n\u2211 j=1 `(h(xij), fi(x i j)).\nThis quantity can be interpreted as a training error if the learner would receive the labels for all the samples for the chosen tasks I . Note that in case of m = n this step is not needed and we can avoid the corresponding complexity terms. In the second step we relate the average \u03b1-weighted expected errors to 1T \u2211T t=1 e\u0303r\u03b1t . In the third step we conclude the proof by bounding the pairwise discrepancies in terms of their empirical estimates. Step 1. Fix the unlabeled sets S1, . . . , ST . They fully determine the choice of labeled tasks I and the weights \u03b11, . . . , \u03b1T . Therefore, conditioned on the unlabeled data, these quantities can be considered constant and we need a bound that holds uniformly only with respect to h1, . . . , hT .\nIn order to simplify the notation we assume that I = {1, . . . , k} and define:\n\u03a6(S, . . . , Sk) = sup h1,...,hT\n1\nT T\u2211 t=1 e\u0303r\u03b1t(ht)\u2212 e\u0302r\u03b1t(ht). (10)\nNote that one could analyze this quantity using standard techniques from Rademacher analysis, if the labeled examples were sampled from the unlabeled sets i.i.d., i.e. with replacement. However, since we assume that for every i Si is a subset of Si, i.e. the labeled examples are sampled randomly without replacement, there are dependencies between the labeled examples. Therefore we utilize techniques from the literature on transductive learning [7] instead. We first apply Doob\u2019s construction to \u03a6 in order to obtain a martingale sequence and then use McDiarmid\u2019s inequality for martingales [16]. As a result we obtain that with probability at least 1\u2212 \u03b4/4 over sampling labeled examples:\n\u03a6 \u2264 E S1,...,Sk\n\u03a6 + 1\nT \u221a\u221a\u221a\u221a k\u2211 i=1 ( T\u2211 t=1 \u03b1ti )2\u221a log(4/\u03b4) 2m . (11)\nNow we need to upper bound E\u03a6. Using results from [28] and [11] we observe that:\nE S1,...,Sk \u03a6(S1, . . . , Sk) \u2264 E S\u03031,...,S\u0303k \u03a6(S\u03031, . . . , S\u0303k), (12)\nwhere S\u0303i is a set ofm points sampled from Si i.i.d. with replacement (in contrast to sampling without replacement corresponding to Si). This means that we can upper bound the expectation of \u03a6 over samples with dependencies by the expectation over independent samples. By doing so, applying the symmetrization trick, and introducing Rademacher random variables, we obtain that:\nE S1,...,Sk \u03a6 \u2264 1 T T\u2211 t=1 \u221a\u221a\u221a\u221a k\u2211 i=1 (\u03b1ti) 2 \u00b7 \u221a 2d log(ekm/d) m . (13)\nA combination of (11) and (13) shows that (conditioned on the unlabeled data) with probability at least 1 \u2212 \u03b4/4 over sampling labeled examples uniformly for all choices of h1, . . . , hT the following holds:\n1\nT T\u2211 t=1 e\u0303r\u03b1t(ht) \u2264 1 T T\u2211 t=1 e\u0302r\u03b1t(ht) + A T \u2016\u03b1\u20162,1 + B T \u2016\u03b1\u20161,2. (14)\nStep 2. Now we relate 1T \u2211T t=1 e\u0303r\u03b1t to 1 T \u2211T t=1 er\u03b1t .\nThe choice of the tasks to label, I , the corresponding weights, \u03b1, and the predictors, h, all depend on the unlabeled data. Therefore, we aim for a bound that is uniform in all three parameters. We define:\n\u03a8(S1, . . . , ST ) = sup I sup \u03b11,...,\u03b1T\u2208\u039bI sup h1,...,hT\n1\nT T\u2211 t=1 T\u2211 i=1 \u03b1ti(eri(ht)\u2212 e\u0303ri(ht)).\nThe main instrument that we used here is a refined version of McDiarmid\u2019s inequality, which is due to [15]. It allows us to use the standard Rademacher analysis, while taking into account the internal structure of the weights \u03b11, . . . , \u03b1T . As a result we obtain that with probability at least 1\u2212 \u03b4/4 simultaneously for all choices of tasks to be labeled, I , weights \u03b11, . . . , \u03b1T \u2208 \u039bI and hypotheses h1, . . . , hT :\n1\nT T\u2211 t=1 er\u03b1t(ht) \u2264 1 T T\u2211 t=1 e\u0303r\u03b1t(ht) + C. (15)\nStep 3. To conclude the proof we bound the pairwise discrepancies in terms of their finite sample estimates by using the same argument as for Theorem 1: we apply Proposition 2 to every pair of tasks and combine the results using the uniform bound argument. This yields the remaining two terms on the right hand side: the weighted average of the sample-based discrepancies and the constant D. By combining the result with (14) with (15) we obtain the statement of the theorem. Interpretation. The complexity terms C andD behave asO( \u221a d log(nT )/n), while AT \u2016\u03b1\u20162,1 + B T \u2016\u03b1\u20161,2 in the\nworst case of \u2016\u03b1\u20162,1 = \u2016\u03b1\u20161,2 = T behaves as O( \u221a d log(km)/m). In order for these terms to be balanced, i.e. for the uncertainty coming from the estimation of discrepancy to not dominate the uncertainty from the estimation of the \u03b1-weighted risks, the number of unlabeled examples per task n should be significantly (for k T ) larger than m. However, this is not a strong limitation under the common assumption that obtaining enough unlabeled examples is significantly cheaper than annotated ones.\nThe remaining terms in the bound of Theorem 2 can be estimated from the data (apart from the \u03bb-term) and depend on the choice of labeled tasks I and weights \u03b11, . . . , \u03b1T . Therefore they can be used as a quality measure to choose data-appropriate labeled tasks and the corresponding weights and hypotheses. Since, analogously to the case of transferring from a single task, the empirical errors cannot be evaluated before observing the labels, the objective function for choosing the labeled tasks I and the weights \u03b11, . . . , \u03b1T reduces to:\n1\nT T\u2211 t=1 \u2211 i\u2208I \u03b1ti disc(St, Si) + A T \u2016\u03b1\u20162,1 + B T \u2016\u03b1\u20161,2. (16)\nBy minimizing (16) with respect to I and \u03b1 one can obtain the labeled tasks and the weights that are beneficial for solving the tasks based on the given data. Thus we propose the following algorithm for active task selection in case of multi-source transfer (ATM-MS):\nAlgorithm 2 (ATS-MS). 1. estimate pairwise discrepancies between the tasks based on the unlabeled data 2. choose the labeled tasks I and the weights \u03b11, . . . , \u03b1T by minimizing (16) 3. for every task t train a classifier by minimizing (7) using the obtained weight vector \u03b1t.\nNote that the above algorithm fully determines the labeled tasks and the weights based only on the unlabeled data. Therefore, the conditions of Theorem 2 are satisfied, and its guarantees also hold for the resulting solution.\nAn important aspect of the bound (8) and, consequently, of Algorithm 2 is the effect of \u2016\u03b1\u20161,2 and \u2016\u03b1\u20162,1. As already mentioned above, in the worst case of every \u03b1t having only one non-zero component \u2016\u03b1\u20162,1 is equal to T and does not improve the convergence rate. However, in the opposite extreme, if every \u03b1t weights all the labeled tasks equally, i.e. \u03b1ti = 1/k for all t \u2208 {1, . . . , T} and i \u2208 I , \u2016\u03b1\u20162,1 = T\u221ak . Therefore the convergence of the corresponding term improves from O( \u221a d log(km)/m) to O( \u221a d log(km)/km), which is the best we can expect from having km labeled examples. Thus, this term in (16) encourages the learner to use data from multiple labeled tasks for adaptation and captures the intuition that multi-source approach may improve the performance.\nThe role of the second norm, \u2016\u03b1\u20161,2, is less transparent. While \u2016\u03b1\u20162,1 influences every \u03b1t independently, \u2016\u03b1\u20161,2 connects the weights for all tasks t. It is equal to T\u221ak when every labeled task i \u2208 I is used equally much, i.e. \u2211T t=1 \u03b1 t i = T/k for every i \u2208 I . Therefore, this term suggests to select the tasks that all would be equally useful, thus preventing labeling tasks that would be useful for only few of the remaining ones."}, {"heading": "6 Experiments", "text": "We benchmark the performance of the proposed algorithms by experiments on synthetic and real data. In both cases we choose H to be the set of all linear predictors without a bias term on X = Rd.\nSynthetic data. We generate T = 1000 binary classification tasks in R2. For each task t its marginal distributionDt is a unit-variance Gaussian with mean \u00b5t chosen uniformly at random from the set [\u22125, 5]\u00d7[\u22125, 5]. The label +1 is assigned to all points that have angle between 0 and \u03c0 with \u00b5t (computed counter-clockwise), the other points are labeled \u22121. We use n = 1000 unlabeled and m = 100 labeled examples per task and augment all samples with a constant feature, resulting in d = 3.\nReal Data. We use the train part of the ImageNet ILSVRC2010 dataset [22], which consists of approximately 1.2 million natural images from 1000 classes. We extract features using a deep convolutional neural network [27] that was pretrained on a different dataset (ILSVRC2012), reduce their dimension to 5 using PCA and augment them with a constant feature, resulting in d = 6. We construct 999 balanced binary tasks of classifying the largest class, Yorkshire terrier, versus one of the remaining classes. We use n = 400 unlabeled samples per task and label a subset of m = 100 examples for each of the selected tasks.\nMethods. We report results for the proposed active task selection method in the single-source transfer setting (ATS-SS) as well as the multi-source transfer setting (ATS-MS). Since no earlier methods for multi-task learning with unlabeled task exist we compare in both cases to the natural baseline of choosing the labeled tasks randomly and then applying the same adaptation method, i.e. each unlabeled task is solved using the predictor from the closest labeled task (RTS-SS), or by training on a task-specific weighted combination of labeled tasks (RTSMS) with only the weights obtained by minimizing (16). We found that randomizing also the step of assigning unlabeled to labeled tasks leads to essentially random classification results, and we do not evaluate it formally. To provide context for the results we also report the results of learning independent ridge regressions with access to labels for all tasks (denoted by Fully Labeled). However, this baseline has access to many more annotated examples in total than the active and random task selection method. In order to quantify this effect we also evaluate a modification of the Fully Labeled baseline that has fewer labels per task available (denoted by Partially Labeled), namely when the number of labeled tasks is k, the number of labels per task is mk/T , i.e. the total amount of labeled examples is mk, the same as for the active and random task selection methods. To avoid the need for heuristic choices, we report results for this baseline only for integer values of mk/T .\nImplementation. We estimate the empirical discrepancies between pairs of tasks (step 1 in Algorithms 1 and 2) by finding a hypothesis in H that minimizes the squared loss for the binary classification problem of separating the two sets of instances, as in [4]. To minimize the k-medoid risk (step 2 in Algorithm 1) we perform a local search as in [19]. For the corresponding minimization of (16) in Algorithm 2 we use the GraSP algorithm [2]. It requires as a subroutine a method for optimizing the objective with respect to a given sparsity pattern, for which we use gradient descent. To obtain classifiers for the individual tasks in all scenarios we use least-squares ridge regression with regularization constant from the set {10\u22123, 10\u22122, 10\u22121, 1, 10, 102, 103} found by 5\u00d7 5-fold cross validation.\nResults. The results are shown in Figure 3. One can see that choosing labeled tasks by minimizing (5) followed by single-source transfer is advantageous over a random choice, especially when the budget only allows for a small fraction of tasks to be labeled. The same observation holds for multi-source transfer: choosing labeled tasks and weights using (16) is beneficial to choosing a random subset of labeled tasks. In both cases, the proposed methods require substantially fewer tasks labeled to achieve the same accuracy as the baseline of randomly choosing tasks.\nThe difference between the proposed method and the Partially Labeled baseline is even bigger than to the random baseline, indicating that in this case, having more labels for fewer tasks rather than only few labels for all tasks is beneficial not only in terms of annotation costs, but also in terms of prediction accuracy.\nAs the number of labeled tasks gets larger, e.g. half of all tasks, the performance of the active task selection learner becomes almost identical to the performance of the fully supervised method, even improving over it in the case of multi-source transfer on synthetic data. This confirms the intuition that in the case of many related tasks even a fraction of the tasks can contain enough information for solving all tasks."}, {"heading": "7 Conclusion", "text": "In this work we introduced and studied the active task selection framework: a modification of the multi-task learning setting inspired by the active learning paradigm. While initially all tasks are represented only by unlabeled training samples, the learner, given a budget, decides for which tasks of interest to query labels and solves the remaining tasks based only on their unlabeled data and information transferred from the selected tasks. We analyzed this framework for two domain adaptation methods and established generalization bounds that can be used to make the choice of labeled tasks in a principled way. We also provided an empirical evaluation that demonstrates the advantages of the proposed methods.\nFor future work we plan to further exploit the idea of active learning in application to the multi-task setting. In particular, we are interested to see whether by allowing the learner to make his decision on which tasks to label in an iterative way, rather than forcing him to choose all the tasks at the same time, one could obtain better learning guarantees as well as more effective learning methods."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Marius Kloft for helpful discussions. This work was funded in parts by the European Research Council under the European Unions Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no 308036. A Tesla K40 card used for this research was donated by the NVIDIA Corporation."}, {"heading": "A Technical Lemmas", "text": "By using a union bound argument we generalize Proposition 2 to hold for all T (T \u2212 1)/2 pairwise discrepancies simultaneously:\nCorollary 1. Let d be the VC dimension of the hypothesis class H , D1, . . . , DT be T probability distributions over X and S1, . . . , ST be the corresponding sample sets of i.i.d. examples of size n each. Then for any \u03b4 > 0 with probability at least 1\u2212 \u03b4 simultaneously for all i, j = 1, . . . , T :\ndisc(Di, Dj) \u2264 disc(Si, Sj) + 2 \u221a 2d log(2n) + 2 log(T ) + log(2/\u03b4)\nn . (17)\nLemma 1 (Corollary 3.4 in [17]). Let d be the VC dimension of the hypothesis set H and S be a random sample of m examples from some unknown distribution over X \u00d7 Y . Then for any \u03b4 > 0 with probability at least 1 \u2212 \u03b4 for any h \u2208 H:\ner(h) \u2264 e\u0302r(h) + \u221a 2d log(em/d)\nm +\n\u221a log(1/\u03b4)\n2m . (18)\nLemma 2 (Corollary 6.10 in [16]). Let Wn0 be a martingale with respect to a sequence of random variables (B1, . . . , Bn). Let bn1 = (b1, . . . , bn) be a vector of possible values of the random variables B1, . . . , Bn. Let\nri(b i\u22121 1 ) = sup\nbi\n{Wi : Bi\u221211 = b i\u22121 1 , Bi = bi} \u2212 inf bi {Wi : Bi\u221211 = b i\u22121 1 , Bi = bi}. (19)\nLet r2(bn1 ) = \u2211n i=1(ri(b i\u22121 1 )) 2 and R\u03022 = supbn1 r 2(bn1 ). Then\nPr Bn1 {Wn \u2212W0 > } < exp\n( \u22122 2\nR\u03022\n) . (20)\nLemma 3 (Part of Lemma 19 in [28]). Let x = (x1, . . . , xl) \u2208 Rl. Then the following function is convex:\nF (x) = sup i=1...l xi. (21)\nLemma 4 (Originally [11]; in this form Theorem 18 in [28]). Let {U1, . . . , Um} and {W1, . . . ,Wm} be sampled uniformly from a finite set of d-dimensional vectors {v1, . . . , vN} \u2282 Rd with and without replacement respectively. Then for any continuous and convex function F : Rd \u2192 R the following holds:\nE [ F ( m\u2211 i=1 Wi )] \u2264 E [ F ( m\u2211 i=1 Ui )] (22)\nLemma 5 (Theorem 1 in [15]). Let X1, . . . , Xn be independent random variables taking values in the set X and f be a function f : Xn \u2192 R. For any x = (x1, . . . , xn) \u2208 Xn and y \u2208 X define:\nxy,k = (x1, . . . , xk\u22121, y, xk+1, . . . , xn)\n(inf k f)(x) = inf y\u2208X f(xy,k)\n\u2206+,f = n\u2211 i=1 (f \u2212 inf k f)2.\nThen for t > 0:\nPr{f \u2212E f \u2265 t} \u2264 exp ( \u2212t2\n2\u2016\u2206+\u2016\u221e\n) . (23)"}, {"heading": "B Proof of Theorem 1", "text": "First, we note that for any I = {i1, . . . , ik}, any c = (c1, . . . , cT ) and any (hi)i\u2208I the following consequence of Proposition 1 holds:\n1\nT T\u2211 t=1 ert(hct) \u2264 1 T T\u2211 t=1 erct(hct) + 1 T T\u2211 t=1 disc(Dt, Dct) + 1 T T\u2211 t=1 \u03bbtct . (24)\nNext, we bound the expected error terms on the right hand by their empirical counterparts. For this, we apply a union bound argument to Lemma 1 and obtain that with probability at least 1 \u2212 \u03b4/2 simultaneously for all i = 1, . . . , T and all hi \u2208 H the following inequality holds:\neri(hi) \u2264 e\u0302ri(hi) + \u221a 2d log(em/d)\nm +\n\u221a log(T ) + log(2/\u03b4)\n2m , (25)\nTo bound the discrepancy, disc(Dt, Dct), by its empirical version, disc(St, Sct), we use Corollary 1 and obtain that with probability 1\u2212 \u03b4/2 for all i, j = 1, . . . , T :\ndisc(Di, Dj) \u2264 disc(Si, Sj) + 2 \u221a 2d log(2n) + 2 log(T ) + log(4/\u03b4)\nn . (26)\nThe statement of the theorem follows from inserting (25) and (26) into (24), noting that the two high probability statements do not depend on the choice of the assignments C.\nNote that we intentionally use only elementary tools in the proof to showcase the main steps of relating the source to target error using discrepancies and bounding the uncertainty due to random sampling of the empirical quantities. By a more careful analysis, some of the log T terms could be avoided and the constants refined."}, {"heading": "C Proof of Theorem 2", "text": "As in the proof of Theorem 1, we bound the multi-task error by the errors on the source tasks, and transition to empirical quantities while keeping the effect of random sampling controlled. However, the steps will be more involved, since we now require the bounds to be uniform also in the (continuous) weights \u03b1, so we cannot rely on simple union bounds.\nFor the first step, we establish the following result (inspired by Theorem 4 in [4]):\nLemma 6. Let \u3008D1, t1\u3009, . . . , \u3008DT , fT \u3009 be T tasks and I = {i1, . . . , ik}. Then, for every t = 1, . . . , T the following inequality holds for all h \u2208 H and all \u03b1 \u2208 \u039bI :\nert(h) \u2264 er\u03b1(h) + \u2211 i\u2208I \u03b1i(\u03bbit + disc(Di, Dt)). (27)\nProof. For fixed t, let h\u2217i \u2208 arg minh\u2208H(ert(h) + eri(h)).1 Writing `(h, h\u2032) as shorthand for `(h(x), h\u2032(x)), we have\n| er\u03b1(h)\u2212 ert(h)| = \u2223\u2223\u2223\u2211 i\u2208I \u03b1i eri(h)\u2212 ert(h) \u2223\u2223\u2223 \u2264\u2211 i\u2208I \u03b1i \u2223\u2223 eri(h)\u2212 ert(h)\u2223\u2223 (28)\n\u2264 \u2211 i\u2208I \u03b1i (\u2223\u2223 eri(h)\u2212 E x\u223cDi `(h, h\u2217i ) \u2223\u2223+ \u2223\u2223 E x\u223cDi `(h, h\u2217i )\u2212 E x\u223cDt `(h, h\u2217i ) \u2223\u2223+ \u2223\u2223 ert(h)\u2212 E x\u223cDt `(h, h\u2217i ) \u2223\u2223)\n(29)\nWe can bound each summand: | eri(h)\u2212 E x\u223cDi `(h, h\u2217i )| \u2264 eri(h\u2217), because eri(h) = E x\u223cDi `(fi, h), and ` fulfills the triangular inequality. |Ex\u223cDi`(h, h\u2217i ) \u2212 Ex\u223cDt`(h, h\u2217i ) \u2223\u2223 \u2264 disc(Di, Dt) by the definition of discrepancy, and | ert(h)\u2212 E x\u223cDt `(h, h\u2217i ) \u2223\u2223 \u2264 ert(h\u2217i ) by the same reasoning as for the first summand. Therefore,\n\u2264 \u2211 i\u2208I \u03b1i(eri(h \u2217 i ) + disc(Di, Dt) + ert(h \u2217 i )) = \u2211 i\u2208I \u03b1i(\u03bbit + disc(Di, Dt)). (30)\nAssuming that each task t has its own weights \u03b1t we obtain as a direct corollary of the previous result:\nCorollary 2. Let \u3008D1, t1\u3009, . . . , \u3008DT , fT \u3009 be T tasks and I = {i1, . . . , ik}. Then the following inequality holds for all h1, . . . , hT \u2208 H and all \u03b11, . . . , \u03b1T \u2208 \u039bI :\n1\nT T\u2211 t=1 ert(h) \u2264 1 T T\u2211 t=1 er\u03b1t(ht) + 1 T T\u2211 t=1 \u2211 i\u2208I \u03b1ti disc(Dt, Di) + 1 T T\u2211 t=1 \u2211 i\u2208I \u03b1ti\u03bbti. (31)\nThe following proposition bounds the effect estimating the first term on the right hand side in (31) from finite data.\nProposition 3. For any \u03b4 > 0 the following inequality holds uniformly in h1, . . . , hT \u2208 H with probability at least 1 \u2212 \u03b4 over the sampling of the unlabeled training sets, S1, . . . , ST , and labeled training sets, (S\u0304i)i\u2208I , provided that the subset of labeled tasks, I \u2282 {1, . . . , T}, and the task weights, \u03b11, . . . , \u03b1T \u2208 \u039bI , depend deterministically on the unlabeled training only.\n1\nT T\u2211 t=1 er\u03b1t(ht) \u2264 1 T T\u2211 t=1 e\u0302r\u03b1t(ht)+ 1 T \u2016\u03b1\u20162,1\n\u221a 2d log(ekm/d)\nm +\n1 T \u2016\u03b1\u20161,2\n\u221a log(2/\u03b4)\n2m (32)\n+\n\u221a 8(log T + d log(enT/d))\nn +\n\u221a 2\nn log\n2 \u03b4 , (33)\n1If the minimum is not attained, the same inequality follows by an argument of arbitrary close approximation.\nThe proof of the theorem consists of three steps. In the first two steps we condition on the unlabeled data sets and bound 1T \u2211T t=1 e\u0302r\u03b1t in terms of 1 T \u2211T t=1 e\u0303r\u03b1t , where for any index set I and weights \u03b1 we set\ne\u0303r\u03b1(h) = \u2211 i\u2208I \u03b1ie\u0303ri(h) for e\u0303ri(h) = 1 n n\u2211 j=1 `(h(xij), fi(x i j)). (34)\nThis quantity can be seen as a training error if the learner would receive all the labels for the chosen tasks. Clearly, if m = n this part is not necessary and we can avoid the resulting complexity terms. In the third step we relate 1 T \u2211T t=1 e\u0303r\u03b1t to 1 T \u2211T t=1 er\u03b1t .\nStep 1. Fix the unlabeled samples S1, . . . , ST . This uniquely determines the chosen tasks I and the weights \u03b11, . . . , \u03b1T \u2208 \u039bI , so the only remaining source of randomness the uncertainty which subsets of the selected tasks are labeled. Analyzing this would be rather straightforward if the labeled points, S\u0304i, were sampled i.i.d. from Si (i.e. randomly with replacement). This is not the case, however, since we assume that exactlym points are labeled, i.e. S\u0304i is sampled from Si randomly without replacement, and this introduces dependencies between the elements.\nFor notational simplicity we pretend that exactly the first k tasks were selected, i.e. I = {1, . . . , k}. The general case can be obtained by changing the indices in the proof from 1, . . . , k to i1, . . . , ik.\nTo deal with the dependencies between the labeled data points we first note that any random labeled subset S\u0304i = (s\u0304 i 1, . . . , s\u0304 i m) can be described as the first m elements of a random permutation Zi = (z i 1, . . . , z i n) over n elements that correspond to the unlabeled sample Si, i.e. s\u0304ij = (x\u0304 i j , y\u0304 i j) = (x\ni zij , yi zij ). With this notation and\nwriting Z = (Z1, . . . , Zk) and `(h, zij) = `(h(x\u0304 i j), y\u0304 i j) we define the following function\n\u03a6(Z) = sup h1,...,hT\n1\nT T\u2211 t=1 e\u0303r\u03b1t(ht)\u2212 e\u0302r\u03b1t(ht) = sup h1,...,hT k\u2211 i=1 1 T T\u2211 t=1 \u03b1ti ( 1 n n\u2211 j=1 `(ht, z i j)\u2212 1 m m\u2211 j=1 `(ht, z i j) ) .\n(35)\nFirst, we establish a large deviation bound for \u03a6.\nLemma 7. The following inequality holds with probability at least 1\u2212\u03b4/2 over the labeled sample sets, S\u03041, . . . , S\u0304k.\n\u03a6(Z)\u2212E Z \u03a6(Z) \u2264 1 T \u221a\u221a\u221a\u221a k\u2211 i=1 ( T\u2211 t=1 \u03b1ti )2\u221a log(2/\u03b4) 2m . (36)\nProof. The main ingredient to the proof will be an application of McDiarmid\u2019s inequality (Lemma 2, page 11) for martingales. For this, we interpret Z = (z11 , z 1 2 , . . . , z k n) as a sequence of as kn dependent variables, z11, . . . , zkn. For the sake of notational consistency we will keep using double indices, with the convention that the sample index, j = 1, . . . , n, runs faster than the task index, i = 1, . . . , k. Segments of a sequence will be denoted by upper and lower double indices, z \u0131\u0304\u0304ij = (zij , zi(j+1), . . . , z\u0131\u0304\u0304) for ij \u2264 \u0131\u0304\u0304 and z \u0131\u0304\u0304 ij = \u2205 otherwise. We now create a martingale sequence using Doob\u2019s construction [6]:\nWij = E Z {\u03a6(Z)| zij11 }. (37)\nwhere here and in the following when taking expectations over Z it is silently assumed that the expectation is taken only with respect to variables that are not conditioned on. Note that because of this convention, the expectations in (37) is only with respect to zi(j+1), . . . , zkn, so each Wij is a random variable of z11, . . . , zij . In particular, W00 = EZ \u03a6(Z) and Wkn = \u03a6(Z), and the in between sequence is a martingale with respect to z11, . . . , zkn:\nE Z {Wij |zi(j\u22121)11 } = E Z { E Z {\u03a6(Z)| zij11} \u2223\u2223 zi(j\u22121)11 } = E Z {\u03a6(Z)|zi(j\u22121)11 } = Wi(j\u22121). (38)\nFollowing the path of the proof of Lemma 2 in [7] we would like to apply Lemma 2. For this we compute an upper bound on the coefficient R\u03022 defined there.\nLet i \u2208 {1, . . . , k} and j \u2208 {1, . . . , n} be fixed and let \u03c0 = (\u03c01, . . . , \u03c0k) be specific permutations of n elements for which we use the same index conventions as for Z. By \u03c3 and \u03c4 will denote elements in \u03c0ini(j+1), i.e. \u03c3 and \u03c4 do not occur in any of the first j positions of the permutation \u03c0i. Then\nrij(\u03c0 i(j\u22121) 11 ) = sup\n\u03c3\u2208\u03c0in i(j+1)\n{Wij : zi(j\u22121)11 = \u03c0 i(j\u22121) 11 , zij = \u03c3} \u2212 inf\n\u03c3\u2208\u03c0in i(j+1)\n{Wij : zi(j\u22121)11 = \u03c0 i(j\u22121) 11 , zij = \u03c3}\n(39)\n= sup \u03c3\u2208\u03c0in\ni(j+1)\nsup \u03c4\u2208\u03c0in\ni(j+1)\n[ E\nzkn i(j+1) {\u03a6(\u03c0i(j\u22121)11 , \u03c3, zkni(j+1))} \u2212 E zkn i(j+1)\n{\u03a6(\u03c0i(j\u22121)11 , \u03c4, zkni(j+1))} ] . (40)\nTo analyze (40) further, recall that:\nE zkn i(j+1)\n{\u03a6(\u03c0i(j\u22121)11 , \u03c3, zkni(j+1))} (41)\n= \u2211\n\u03c0kn i(j+1)\n\u03a6(\u03c0 i(j\u22121) 11 , \u03c3, \u03c0 kn i(j+1))\u00d7 Pr( z kn i(j+1) = \u03c0 kn i(j+1) |z i(j\u22121) 11 = \u03c0 i(j\u22121) 11 \u2227 zij = \u03c3 ) (42)\nwhere here and in the following we use the convention that sums over parts of \u03c0 run only over values that lead to valid permutations. Because the permutations of different task are independent, this is equal to\n= \u2211\n\u03c0kn i(j+1)\n\u03a6(\u03c0 i(j\u22121) 11 , \u03c3, \u03c0 kn i(j+1))\u00d7 Pr( z in i(j+1) = \u03c0 in i(j+1) |z i(j\u22121) i1 = \u03c0 i(j\u22121) i1 \u2227 zij = \u03c3 ) Pr(z kn (i+1)1 = \u03c0 kn (i+1)1)\n(43)\nWe make the following observation: for any fixed \u03c0iji1 and any \u03c4 6\u2208 \u03c0 ij i1, we can rephrase a summation over \u03c0ini(j+1) into a sum over all positions where \u03c4 can occur, and a sum over all configuration for the entries that are not \u03c4 : \u2211\n\u03c0in i(j+1)\nF (\u03c0ini(j+1)) = n\u2211 l=j+1 \u2211 \u03c0 i(l\u22121) i(j+1) \u2211 \u03c0in i(l+1) F (\u03c0 i(l\u22121) i(j+1), \u03c4, \u03c0 in i(l+1)) (44)\nfor any function F . Applying this to the summation in (43), we obtain\u2211 \u03c0kn i(j+1) \u03a6(\u03c0 i(j\u22121) 11 , \u03c3, \u03c0 kn i(j+1)) Pr( z in i(j+1) = \u03c0 in i(j+1) |z i(j\u22121) i1 = \u03c0 i(j\u22121) i1 \u2227 zij = \u03c3 ) Pr(z kn (i+1)1 = \u03c0 kn (i+1)1)\n(45)\n= n\u2211 l=j+1 \u2211 \u03c0 i(l\u22121) i(j+1) \u2211 \u03c0kn i(l+1) \u03a6(\u03c0 i(j\u22121) 11 , \u03c3, \u03c0 i(l\u22121) i(j+1), \u03c4, \u03c0 kn i(l+1)) (46)\n\u00d7 Pr( zi(l\u22121)i(j+1) = \u03c0 i(l\u22121) i(j+1) \u2227 z kn i(l+1) = \u03c0 kn i(l+1)|z i(j\u22121) 11 = \u03c0 i(j\u22121) 11 \u2227 zij = \u03c3 \u2227 zil = \u03c4) Pr(zkn(i+1)1 = \u03c0 kn (i+1)1)\n= E l\u223cUnj+1 E Z\n\u03a6(Z|zi(j\u22121)11 = \u03c0 i(j\u22121) 11 \u2227 zij = \u03c3 \u2227 zil = \u03c4), (47)\nwhere Unj+1 denotes the uniform distribution over the set {j + 1, . . . , n}. The analogue derivation can be applied to the quantity in line (40) with \u03c3 and \u03c4 exchanged.\nFor any Z denote by Zij\u2194il the permutation obtained by switching zij and zil. Then, due to the linearity of the expectation:\nrij(\u03c0 i(j\u22121) 11 ) = sup\n\u03c3,\u03c4 { E l\u223cUnj+1 E Z {\u03a6(Z)\u2212 \u03a6(Zij\u2194il)|zi[1:j\u22121] = \u03c0 i [1:j\u22121], zij = \u03c3, zil = \u03c4). (48)\nFrom the definition of \u03a6 we see that \u03a6(Z)\u2212\u03a6(Zij\u2194il) = 0 when j, l \u2208 {1, . . . ,m} or j, l \u2208 {m+ 1 . . . , n}. Since l > j in (48) this implies rij(\u03c0 i(j\u22121) 11 ) = 0 for j \u2208 {m + 1, . . . , n}. The only remaining cases are j \u2208 {1, . . . ,m} and l \u2208 {m+ 1, . . . , n}, for which we obtain\n\u03a6(Z)\u2212 \u03a6(Zij\u2194il) \u2264 sup h1,...,hT\n1\nT T\u2211 t=1 \u03b1ti 1 m (\u2212`(ht, zij) + `(ht, zil )) \u2264 1 Tm T\u2211 t=1 \u03b1ti. (49)\nwhere for the first inequality we used that supF\u2212supG \u2264 sup(F\u2212G) for any F,G, and for the second inequality we used that ` is bounded by [0, 1]. Consequently, rij(\u03c0 i(j\u22121) 11 ) \u2264 n\u2212mn\u2212j 1 Tm \u2211T t=1 \u03b1 t i in this case. Therefore 2\nR\u03022 = k\u2211 i=1 n\u2211 j=1 ( rij(\u03c0 i(j\u22121) 11 ) )2 \u2264 1 T 2m2 m\u2211 j=1 (n\u2212m n\u2212 j )2 k\u2211 i=1 ( T\u2211 t=1 \u03b1ti )2 \u2264 1 T 2m k\u2211 i=1 ( T\u2211 t=1 \u03b1ti )2 .\n2We generously bound n\u2212m n\u2212j \u2264 1 in this step. By keeping the corresponding factor in the analysis one obtains that the constant B in the\ntheorem can be improved at least by a factor of (n\u2212m) 2\n(n\u22120.5)(n\u2212m\u22120.5) .\nNow from Lemma 2 we obtain that with probability at least 1\u2212 \u03b4/2:\n\u03a6(Z)\u2212E Z\n\u03a6(Z) = Wkn \u2212W0 \u2264 1\nT \u221a\u221a\u221a\u221a k\u2211 i=1 ( T\u2211 t=1 \u03b1ti )2\u221a log(2/\u03b4) 2m . (50)\nStep 2. As a second step, we establish an upper bound on EZ \u03a6(Z) itself.\nLemma 8.\nE Z \u03a6(Z) \u2264 1 T T\u2211 t=1 \u221a\u221a\u221a\u221a k\u2211 i=1 (\u03b1ti) 2 \u00b7 \u221a 2d log(ekm/d) m . (51)\nProof. The main ingredient will be Lemma 4 (page 4). First we rewrite \u03a6(Z) in the following way:\n\u03a6(Z) = 1\nT T\u2211 t=1 sup h k\u2211 i=1 \u03b1ti(e\u0303ri(h)\u2212 e\u0302ri(h)) = 1 Tm T\u2211 t=1 \u03a6t(Z)\n\u03a6t(Z) = sup h k\u2211 i=1 m\u03b1ti(e\u0303ri(h)\u2212 e\u0302ri(h))\nNote that even though H can be infinitely large, we can identify a finite subset that represents all possible predictions of hypothesis in H on S1 \u222a \u00b7 \u00b7 \u00b7 \u222a Sk. We denote their number by L \u2264 2kn and the corresponding hypotheses by h1, . . . , hL.\nLet t \u2208 {1, . . . , T} be fixed. For every i \u2208 {1, . . . , k} define a set of n L-dimensional vectors, V ti = {vti1, . . . , vtin}, where for every j \u2208 {1, . . . , n}:\nvtij = [ \u03b1ti ( e\u0303ri(h 1)\u2212 `(h1(xij), yij) ) , . . . , \u03b1ti ( e\u0303ri(h L)\u2212 `(hL(xij), yij) )] . (52)\nWith this notation, for every i \u2208 {1, . . . , k} choosing a random subset S\u0304i \u2282 Si corresponds to samplingm vectors from V ti uniformly without replacement.\nFor every i \u2208 {1, . . . , k}, let Ui = {ui1, . . . , uim} be sampled from V ti in that way. Then\n\u03a6t(Z) = F  k\u2211 i=1 m\u2211 j=1 uij  , (53) where the function F takes as input an L-dimensional vector and returns the value of its maximum component. We now bound EZ \u03a6t(Z) by applying Lemma 4 k times:\nE Z \u03a6t(Z) = E U1,...,Uk F ( k\u2211 i=1 m\u2211 j=1 uij ) = E U1,...,Uk\u22121 E Uk [ F ( k\u22121\u2211 i=1 m\u2211 j=1 uij + m\u2211 j=1 ukj )\u2223\u2223\u2223U1, . . . , Uk\u22121]  (54)\nBy Lemma 3 F (x) is a convex function. Thus F (const + x) is also convex and we can apply Lemma 4 with respect to Uk.\n\u2264 E U1,...,Uk\u22121 E U\u0302k [ F k\u22121\u2211 i=1 m\u2211 j=1 uij + m\u2211 j=1 u\u0302kj \u2223\u2223\u2223U1, . . . , Uk\u22121]  (55)\nwhere U\u0302k = {uki, . . . , ukm} is a set of m vectors sampled from V tk with replacement.\n= E U1,...,Uk\u22121,U\u0302k F k\u22121\u2211 i=1 m\u2211 j=1 uij + m\u2211 j=1 u\u0302kj  . (56)\nRepeating the process k times, we obtain\n\u2264 \u00b7 \u00b7 \u00b7 \u2264 E U\u03021,...,U\u0302k F  k\u2211 i=1 m\u2211 j=1 u\u0302ij  . (57) Note that writing the conditioning in the above expressions is just for clarity of presentation, since the U1, . . . , Uk are actually independent of each other.\nSwitching from the U sets by the U\u0302 sets in \u03a6 corresponds to switching from random subsets S\u0304i to random sets S\u0303i consisting of m points sampled from Si uniformly with replacement. Therefore we obtain\nE Z \u03a6t(Z) = E S\u03041,...,S\u0304k \u03a6t(S\u03041, . . . , S\u0304k) \u2264 E S\u03031,...,S\u0303k \u03a6t(S\u03031, . . . , S\u0303k), (58)\nwhich allows us to continue analyzing EZ \u03a6t(Z) in the standard way using Rademacher complexities and independent samples. Applying the common symmetrization trick and introducing Rademacher random variables \u03c3ij we obtain\n\u03a6t(S\u03031, . . . , S\u0303k) \u2264 2E \u03c3 sup h k\u2211 i=1 m\u2211 j=1 \u03c3ij\u03b1 t i`(h(x i j), y i j).\nWe can rewrite this using the fact that `(y, y\u2032) = Jy 6= y\u2032K = 1\u2212yy\u20322\nE \u03c3 sup h k\u2211 i=1 m\u2211 j=1 \u03c3ij\u03b1 t i`(h(x i j), y i j) = E \u03c3 sup h k\u2211 i=1 m\u2211 j=1 \u03c3ij\u03b1 t i 1\u2212 h(xij)yij 2 = 1 2 E \u03c3 sup h k\u2211 i=1 m\u2211 j=1 \u2212\u03c3ijyij\u03b1tih(xij)\nSince \u2212\u03c3ijyij has the same distribution as \u03c3ij :\n= 1\n2 E \u03c3 sup a(h)\u2208A k\u2211 i=1 m\u2211 j=1 \u03c3ijaij(h),\nwhere aij(h) = \u03b1tih(x i j) and A = {a(h) : h \u2208 H}. According to Sauer\u2019s lemma (Corollary 3.3 in [17]):\n|A| \u2264 ( ekm\nd\n)d . (59)\nAt the same time:\n\u2016a\u20162 = \u221a\u221a\u221a\u221a k\u2211 i=1 m\u2211 j=1 (\u03b1tih(x i j)) 2 = \u221a m \u221a\u221a\u221a\u221a k\u2211 i=1 (\u03b1ti) 2. (60)\nTherefore, by Massart\u2019s lemma (Theorem 3.3 in [17]):\nE \u03c3 sup h k\u2211 i=1 m\u2211 j=1 \u03c3ij\u03b1 t i`(h(x i j), y i j) \u2264 1 2 \u221a\u221a\u221a\u221a k\u2211 i=1 (\u03b1ti) 2 \u00b7 \u221a 2dm log(ekm/d). (61)\nBy applying this result for all t we obtain:\nE Z\n\u03a6(Z) = 1\nTm T\u2211 t=1 E Z \u03a6t(Z) \u2264 1 Tm T\u2211 t=1 E S\u0303 \u03a6t(S\u0303) \u2264 1 T T\u2211 t=1 \u221a\u221a\u221a\u221a k\u2211 i=1 (\u03b1ti) 2 \u00b7 \u221a 2d log(ekm/d) m . (62)\nCombining steps 1 and 2, we obtain that for fixed unlabeled samples S1, . . . , ST with probability at least 1\u2212 \u03b4/2 for all choices of h1, . . . , hT :\n1\nT T\u2211 t=1 e\u0303r\u03b1t(ht) \u2264 1 T T\u2211 t=1 e\u0302r\u03b1t(ht) + 1 T \u2016\u03b1\u20162,1\n\u221a 2d log(ekm/d)\nm +\n1 T \u2016\u03b1\u20161,2\n\u221a log(2/\u03b4)\n2m . (63)\nStep 3. In the last step we relate the empirical multi-task error, e\u0303r\u03b1 to the expected multi-task error, er\u03b1. Because the choice of the tasks to label, I , their weights, \u03b1, and the predictors, h, all depend on the unlabeled data, we aim for a bound that is holds simultaneous for all choices of these quantities, under the condition that I and \u03b1 depend only on the unlabeled samples, while h can be chosen based also on the labeled subsets.\nLemma 9. For any \u03b4 > 0, the following inequality holds with probability at least 1\u2212 \u03b4/2 simultaneously for all choices of tasks to be labeled, I , weights \u03b1 and hypotheses h:\n1\nT T\u2211 t=1 er\u03b1t(ht) \u2264 1 T T\u2211 t=1 e\u0303r\u03b1t(ht) +\n\u221a 8(log T + d log(enT/d))\nn +\n\u221a 2\nn log\n2 \u03b4 . (64)\nProof. The main ingredient is a refined version of McDiarmid\u2019s inequality, due to Maurer [15] (Lemma 5, page 12), which allows us to make use of the internal structure of the weights, \u03b1, while deriving a large deviation bound.\nFor any S = (S1, . . . , Sn) with Si = {(xi1, yi1), . . . , (xin, yin)} define:\n\u03a8(S) = sup I={i1,...,ik} sup \u03b11,...,\u03b1n\u2208\u039bI sup h1,...,hT\n1\nT T\u2211 t=1 T\u2211 i=1 \u03b1ti(eri(ht)\u2212 e\u0303ri(ht)) = sup I sup \u03b1 sup h g(\u03b1,h, S) (65)\nfor\ng(\u03b1,h, S) = T\u2211 i=1 n\u2211 j=1\n( 1\nTn T\u2211 t=1 \u03b1ti(eri(ht)\u2212 `(ht(xij), yij))\n) . (66)\nTo apply Lemma 5 we establish a bound on \u2206+,\u03a8(S) = \u2211 i \u2211 j(\u03a8(S)\u2212\u03a8ij(S))2, with\n\u03a8ij(S) = inf (x,y) sup \u03b1 sup h\ng(\u03b1,h, S \\ {(xij , yij)} \u222a {(x, y)} ). (67)\nLet \u03b1\u2217,h\u2217 be the point where the sup in the (65) is attained3, i.e. \u03a8(S) = g(\u03b1\u2217,h\u2217, S). Then\n\u03a8ij(S) \u2265 inf (x,y) g(\u03b1\u2217, bh\u2217, S \\ {(xij , yij)} \u222a {(x, y)} ) (68)\nand therefore\n\u03a8(S)\u2212\u03a8ij(S) \u2264 g(\u03b1\u2217,h\u2217, S)\u2212 inf (x,y) g(\u03b1\u2217,h\u2217, S \\ {(xij , yij)} \u222a {(x, y)}) (69)\n\u2264 sup (x,y)\n1\nTn T\u2211 t=1 \u03b1\u2217ti (\u2212`(h\u2217t (xij), yij) + `(h\u2217t (x), y)) \u2264 1 Tn T\u2211 t=1 \u03b1\u2217ti , (70)\nwhere for the last inequality we use that ` is bounded in [0, 1]. Because also \u03a8(S)\u2212\u03a8ij(S) \u2265 0, we obtain\n\u2206+,\u03a8(S) = T\u2211 i=1 n\u2211 j=1 (\u03a8(S)\u2212\u03a8ij(S))2 \u2264 T\u2211 i=1 n\u2211 j=1 1 T 2n2 ( T\u2211 t=1 \u03b1\u2217ti )2 \u2264 1 T 2n ( T\u2211 i=1 T\u2211 t=1 \u03b1\u2217ti )2 = 1 n , (71)\n(remember that \u2211 i \u03b1i = 1 for any \u03b1 \u2208 \u039b). Therefore, according to Lemma 5 with probability at least 1\u2212 \u03b4/2:\n\u03a8(S) \u2264 E \u03a8(S) + \u221a 2\nn log\n2 \u03b4 . (72)\nTo bound ES \u03a8(S) we again use symmetrization and Rademacher variables, \u03c3ij :\nE S \u03a8(S) = E S sup I sup \u03b11,...,\u03b1T\u2208\u039bI sup h1,...,hT T\u2211 i=1 n\u2211 j=1\n( 1\nTn T\u2211 t=1 \u03b1ti(eri(ht)\u2212 `(ht(xij), yij))\n) (73)\n\u2264 2E S E \u03c3 sup I sup \u03b11,...,\u03b1T\u2208\u039bI sup h1,...,hT T\u2211 i=1 n\u2211 j=1 ( \u03c3ij Tn T\u2211 t=1 \u03b1ti`(ht(x i j), y i j) ) (74)\n\u2264 2E S E \u03c3\n1\nT T\u2211 t=1 sup \u03b1t\u2208\u039b,ht T\u2211 i=1 n\u2211 j=1 \u03c3ij\u03b1 t i n T\u2211 t=1 `(ht(x i j), y i j) (75)\n\u2264 2E S E \u03c3 sup \u03b1,h T\u2211 i=1 n\u2211 j=1 \u03c3ij\u03b1i n `(h(xij), y i), (76)\nwhere line (75) is obtained from line (74) by dropping the assumption of a common sparsity pattern between the \u03b1-s. Note that the function inside the last sup is linear in \u03b1 \u2208 \u039b, therefore sup\u03b1 can be reduced to the sup over the corners of the simplex, {(1, 0, . . . , 0), . . . , (0, . . . , 0, 1)}. At the same time, by Sauer\u2019s lemma, the number of different choices of h on S is bounded by ( eTn d )d . Therefore, the total number of different choices in (76)\nis bounded by T ( enT d )d . Furthermore, for any choice of \u03b1 and h, the norm of the Tn-vector formed by the\nsummands of (76) is bounded by 1/ \u221a n, because\nT\u2211 i=1 n\u2211 j=1 (\u03c3ij\u03b1i n `(h(xij), y i) )2 = 1 n2 T\u2211 i=1 n\u2211 j=1 ( \u03b1i`(h(x i j), y i) )2 \u2264 1 n2 n\u2211 j=1 ( T\u2211 i=1 \u03b1i )2 = 1 n . (77)\nTherefore, by Massart\u2019s lemma:\nE \u03c3 sup \u03b1,h T\u2211 i=1 n\u2211 j=1 \u03c3il\u03b1i n `(h(xil), y i l) \u2264 \u221a 2(log T + d log(enT/d))\u221a n . (78)\nCombining (72) and (78) we obtain the statement of Lemma 9: with probability at least 1\u2212\u03b4/2 simultaneously for all choices of tasks to be labeled, I , weights \u03b1 and hypotheses h:\n1\nT T\u2211 t=1 er\u03b1t(ht) \u2264 1 T T\u2211 t=1 e\u0303r\u03b1t(ht) +\n\u221a 8(log T + d log(enT/d))\nn +\n\u221a 2\nn log\n2 \u03b4 . (79)\nBy combining this inequality with (63) by a union bound we obtain the statement of Theorem 3.\nThe statement of Theorem 2 follows by combining Corollary 2, Proposition 3 and Corollary 1."}], "references": [{"title": "Convex multi-task feature learning", "author": ["Andreas Argyriou", "Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "Machine Learning (ML),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Greedy sparsity-constrained optimization", "author": ["Sohail Bahmani", "Bhiksha Raj", "Petros T. Boufounos"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Convex multi-task learning by clustering", "author": ["Aviad Barzilai", "Koby Crammer"], "venue": "In Conference on Uncertainty in Artificial Intelligence (AISTATS),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "A theory of learning from different domains", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan"], "venue": "Machine Learning (ML),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Analysis of representations for domain adaptation", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Fernando Pereira"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Regularity properties of certain families of chance variables", "author": ["Joseph L Doob"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1940}, {"title": "Transductive Rademacher complexity and its applications", "author": ["Ran El-Yaniv", "Dmitry Pechyony"], "venue": "In Workshop on Computational Learning Theory (COLT),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Multi-task feature learning", "author": ["A Evgeniou", "Massimiliano Pontil"], "venue": "In Conference on Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Regularized multi-task learning", "author": ["Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "In International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In International Conference on Machine Learing (ICML),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Wassily Hoeffding"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1963}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Zhuoliang Kang", "Kristen Grauman", "Fei Sha"], "venue": "In International Conference on Machine Learing (ICML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["Abhishek Kumar", "Hal Daum\u00e9 III"], "venue": "In International Conference on Machine Learing (ICML),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh"], "venue": "In Workshop on Computational Learning Theory (COLT),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Concentration inequalities for functions of independent variables", "author": ["Andreas Maurer"], "venue": "Random Structures and Algorithms,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": "In Surveys in Combinatorics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1989}, {"title": "Foundations of Machine Learning", "author": ["Mehryar Mohri", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Domain adaptation via transfer component analysis", "author": ["Sinno Jialin Pan", "Ivor W Tsang", "James T Kwok", "Qiang Yang"], "venue": "IEEE Transactions on Neural Networks (T-NN),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "A simple and fast algorithm for k-medoids clustering", "author": ["Hae-Sang Park", "Chi-Hyuck Jun"], "venue": "Expert Systems with Applications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Curriculum learning of multiple tasks", "author": ["Anastasia Pentina", "Viktoriia Sharmanska", "Christoph H Lampert"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Multi-task active learning for linguistic annotations", "author": ["Roi Reichart", "Katrin Tomanek", "Udo Hahn", "Ari Rappoport"], "venue": "In Conference of the Association for Computational Linguistics (ACL),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Active task selection for lifelong machine learning", "author": ["Paul Ruvolo", "Eric Eaton"], "venue": "In Conference on Artificial Intelligence (AAAI),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Active online multitask learning", "author": ["Avishek Saha", "Piyush Rai", "Hal Daum\u00e9 III", "Suresh Venkatasubramanian"], "venue": "In ICML Workshop on Budget Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Online learning of multiple tasks and their relationships", "author": ["Avishek Saha", "Piyush Rai", "Hal Daum\u00e9 III", "Suresh Venkatasubramanian"], "venue": "In Conference on Uncertainty in Artificial Intelligence (AISTATS),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["Hidetoshi Shimodaira"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Localized complexities for transductive learning", "author": ["I. Tolstikhin", "G. Blanchard", "M. Kloft"], "venue": "In Workshop on Computational Learning Theory (COLT),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Bridged refinement for transfer learning", "author": ["Dikan Xing", "Wenyuan Dai", "Gui-Rong Xue", "Yong Yu"], "venue": "In Knowledge Discovery in Databases (PKDD),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "In this paper we concentrate on two transfer methods that use the discrepancy distance [14] to quantify the similarity between unlabeled tasks.", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "For this, they either assume that the predictors for all tasks are similar to each other in some norm and exploit this fact through specific regularizers [9], or they assume that the predictors for all tasks share a common low-dimensional representation that can be learned from the data [8, 1].", "startOffset": 154, "endOffset": 157}, {"referenceID": 7, "context": "For this, they either assume that the predictors for all tasks are similar to each other in some norm and exploit this fact through specific regularizers [9], or they assume that the predictors for all tasks share a common low-dimensional representation that can be learned from the data [8, 1].", "startOffset": 288, "endOffset": 294}, {"referenceID": 0, "context": "For this, they either assume that the predictors for all tasks are similar to each other in some norm and exploit this fact through specific regularizers [9], or they assume that the predictors for all tasks share a common low-dimensional representation that can be learned from the data [8, 1].", "startOffset": 288, "endOffset": 294}, {"referenceID": 24, "context": "learning the relatedness of tasks [25, 12] or sharing only between subgroups of tasks [30, 13, 3].", "startOffset": 34, "endOffset": 42}, {"referenceID": 11, "context": "learning the relatedness of tasks [25, 12] or sharing only between subgroups of tasks [30, 13, 3].", "startOffset": 34, "endOffset": 42}, {"referenceID": 12, "context": "learning the relatedness of tasks [25, 12] or sharing only between subgroups of tasks [30, 13, 3].", "startOffset": 86, "endOffset": 97}, {"referenceID": 2, "context": "learning the relatedness of tasks [25, 12] or sharing only between subgroups of tasks [30, 13, 3].", "startOffset": 86, "endOffset": 97}, {"referenceID": 20, "context": "Two works in this direction are [21, 24], which, however, use active learning on the level of training examples, not tasks.", "startOffset": 32, "endOffset": 40}, {"referenceID": 23, "context": "Two works in this direction are [21, 24], which, however, use active learning on the level of training examples, not tasks.", "startOffset": 32, "endOffset": 40}, {"referenceID": 22, "context": "The idea of choosing tasks was used in active curriculum selection [23, 20], where the learner can influence the order in which tasks are processed.", "startOffset": 67, "endOffset": 75}, {"referenceID": 19, "context": "The idea of choosing tasks was used in active curriculum selection [23, 20], where the learner can influence the order in which tasks are processed.", "startOffset": 67, "endOffset": 75}, {"referenceID": 4, "context": "To transfer information between tasks, our work builds on existing results for single-source and multi-source domain adaptation [5, 14].", "startOffset": 128, "endOffset": 135}, {"referenceID": 13, "context": "To transfer information between tasks, our work builds on existing results for single-source and multi-source domain adaptation [5, 14].", "startOffset": 128, "endOffset": 135}, {"referenceID": 25, "context": "We suspect, however, that also other domain adaptation techniques could be exploited for the active task selection scenario, in particular those based on source reweighting [26], representation learning [18, 10], or semi-supervised transfer [29].", "startOffset": 173, "endOffset": 177}, {"referenceID": 17, "context": "We suspect, however, that also other domain adaptation techniques could be exploited for the active task selection scenario, in particular those based on source reweighting [26], representation learning [18, 10], or semi-supervised transfer [29].", "startOffset": 203, "endOffset": 211}, {"referenceID": 9, "context": "We suspect, however, that also other domain adaptation techniques could be exploited for the active task selection scenario, in particular those based on source reweighting [26], representation learning [18, 10], or semi-supervised transfer [29].", "startOffset": 203, "endOffset": 211}, {"referenceID": 28, "context": "We suspect, however, that also other domain adaptation techniques could be exploited for the active task selection scenario, in particular those based on source reweighting [26], representation learning [18, 10], or semi-supervised transfer [29].", "startOffset": 241, "endOffset": 245}, {"referenceID": 13, "context": "For both methods that we consider in this work a sensible measure of similarity is provided by the notion of discrepancy: Definition 1 (Definition 4 in [14]).", "startOffset": 152, "endOffset": 156}, {"referenceID": 3, "context": "The discrepancy allows relating the performance of a hypothesis on one task to its performance on a different task: Proposition 1 (Theorem 2 in [4]).", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "The discrepancy can be estimated from unlabeled samples: Proposition 2 (Lemma 1 in [4]).", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": ", T} define: \u039b = { \u03b1 \u2208 [0, 1] : T \u2211", "startOffset": 23, "endOffset": 29}, {"referenceID": 6, "context": "Therefore we utilize techniques from the literature on transductive learning [7] instead.", "startOffset": 77, "endOffset": 80}, {"referenceID": 15, "context": "We first apply Doob\u2019s construction to \u03a6 in order to obtain a martingale sequence and then use McDiarmid\u2019s inequality for martingales [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 27, "context": "Using results from [28] and [11] we observe that: E S1,.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "Using results from [28] and [11] we observe that: E S1,.", "startOffset": 28, "endOffset": 32}, {"referenceID": 14, "context": "The main instrument that we used here is a refined version of McDiarmid\u2019s inequality, which is due to [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": "We use the train part of the ImageNet ILSVRC2010 dataset [22], which consists of approximately 1.", "startOffset": 57, "endOffset": 61}, {"referenceID": 26, "context": "We extract features using a deep convolutional neural network [27] that was pretrained on a different dataset (ILSVRC2012), reduce their dimension to 5 using PCA and augment them with a constant feature, resulting in d = 6.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "We estimate the empirical discrepancies between pairs of tasks (step 1 in Algorithms 1 and 2) by finding a hypothesis in H that minimizes the squared loss for the binary classification problem of separating the two sets of instances, as in [4].", "startOffset": 240, "endOffset": 243}, {"referenceID": 18, "context": "To minimize the k-medoid risk (step 2 in Algorithm 1) we perform a local search as in [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 1, "context": "For the corresponding minimization of (16) in Algorithm 2 we use the GraSP algorithm [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "References [1] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Sohail Bahmani, Bhiksha Raj, and Petros T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Aviad Barzilai and Koby Crammer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Joseph L Doob.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] Ran El-Yaniv and Dmitry Pechyony.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] A Evgeniou and Massimiliano Pontil.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Theodoros Evgeniou and Massimiliano Pontil.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Xavier Glorot, Antoine Bordes, and Yoshua Bengio.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Wassily Hoeffding.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Zhuoliang Kang, Kristen Grauman, and Fei Sha.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Abhishek Kumar and Hal Daum\u00e9 III.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Andreas Maurer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Hae-Sang Park and Chi-Hyuck Jun.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Anastasia Pentina, Viktoriia Sharmanska, and Christoph H Lampert.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Roi Reichart, Katrin Tomanek, Udo Hahn, and Ari Rappoport.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Paul Ruvolo and Eric Eaton.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Avishek Saha, Piyush Rai, Hal Daum\u00e9 III, and Suresh Venkatasubramanian.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] Avishek Saha, Piyush Rai, Hal Daum\u00e9 III, and Suresh Venkatasubramanian.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Hidetoshi Shimodaira.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] Karen Simonyan and Andrew Zisserman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] Dikan Xing, Wenyuan Dai, Gui-Rong Xue, and Yong Yu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "4 in [17]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "10 in [16]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 27, "context": "Lemma 3 (Part of Lemma 19 in [28]).", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": "Lemma 4 (Originally [11]; in this form Theorem 18 in [28]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 27, "context": "Lemma 4 (Originally [11]; in this form Theorem 18 in [28]).", "startOffset": 53, "endOffset": 57}, {"referenceID": 14, "context": "Lemma 5 (Theorem 1 in [15]).", "startOffset": 22, "endOffset": 26}, {"referenceID": 3, "context": "For the first step, we establish the following result (inspired by Theorem 4 in [4]): Lemma 6.", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "We now create a martingale sequence using Doob\u2019s construction [6]: Wij = E Z {\u03a6(Z)| z 11 }.", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "(38) Following the path of the proof of Lemma 2 in [7] we would like to apply Lemma 2.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "(49) where for the first inequality we used that supF\u2212supG \u2264 sup(F\u2212G) for any F,G, and for the second inequality we used that ` is bounded by [0, 1].", "startOffset": 142, "endOffset": 148}, {"referenceID": 16, "context": "3 in [17]): |A| \u2264 ( ekm d )d .", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "3 in [17]):", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "The main ingredient is a refined version of McDiarmid\u2019s inequality, due to Maurer [15] (Lemma 5, page 12), which allows us to make use of the internal structure of the weights, \u03b1, while deriving a large deviation bound.", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "where for the last inequality we use that ` is bounded in [0, 1].", "startOffset": 58, "endOffset": 64}], "year": 2017, "abstractText": "In this paper we consider the problem of multi-task learning, in which a learner is given a collection of prediction tasks that need to be solved. In contrast to previous work, we give up on the assumption that labeled training data is available for all tasks. Instead, we propose an active task selection framework, where based only on the unlabeled data, the learner can choose a, typically small, subset of tasks for which he gets some labeled examples. For the remaining tasks, which have no available annotation, solutions are found by transferring information from the selected tasks. We analyze two transfer strategies and develop generalization bounds for each of them. Based on this theoretical analysis we propose two algorithms for making the choice of labeled tasks in a principled way and show their effectiveness on synthetic and real data.", "creator": "LaTeX with hyperref package"}}}