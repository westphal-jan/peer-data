{"id": "1603.00988", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2016", "title": "Learning Functions: When Is Deep Better Than Shallow", "abstract": "we describe constrained computational tasks - especially approximate in vision - that correspond quickly to efficient compositional / directed hierarchical matrix functions. while computing the universal approximation property holds both consistency for generalized hierarchical and shallow networks, we prove there that deep ( sometimes hierarchical ) networks can approximate the class of compositional data functions with the same accuracy as shallow networks but with exponentially lower intrinsic vc - dimension approaches as surely well as by the number \u03bb of training parameters. checking this complexity leads finally to the question ahead of slow approximation performed by sparse lie polynomials ( in the specific number form of least independent parameters ) and, judging as probably a consequence, efficiency by comparing deep networks. we also discuss connections between exactly our results and exponential learnability of sparse - boolean functions, particularly settling onto an old conjecture by bengio.", "histories": [["v1", "Thu, 3 Mar 2016 06:26:31 GMT  (445kb,D)", "http://arxiv.org/abs/1603.00988v1", null], ["v2", "Sat, 5 Mar 2016 22:41:36 GMT  (445kb,D)", "http://arxiv.org/abs/1603.00988v2", "Corrected typos"], ["v3", "Thu, 24 Mar 2016 04:22:29 GMT  (408kb,D)", "http://arxiv.org/abs/1603.00988v3", null], ["v4", "Sun, 29 May 2016 16:43:23 GMT  (686kb,D)", "http://arxiv.org/abs/1603.00988v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hrushikesh mhaskar", "qianli liao", "tomaso poggio"], "accepted": false, "id": "1603.00988"}, "pdf": {"name": "1603.00988.pdf", "metadata": {"source": "CRF", "title": "Learning Real and Boolean Functions: When Is Deep Better Than Shallow", "authors": ["Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio"], "emails": [], "sections": [{"heading": null, "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. HNM was supported in part by ARO Grant W911NF-15-1-0385.\nar X\niv :1"}, {"heading": "1 INTRODUCTION", "text": "The main goal of this paper is to begin to answer the question: why should deep networks be better than shallow networks? Our claim here is that hierarchical networks are a more efficient approximation of the computations that need to be performed on images \u2013 and possibly other sensory signals. The argument of the paper compares shallow (one-hidden layer) networks with deep networks (see for example Figure 1). Both types of networks use the same small set of operations \u2013 dot products, linear combinations, a fixed nonlinear function of one variable, possibly convolution and pooling. The logic of the paper is as follows.\n\u2022 Both shallow (a) and deep (b) networks are universal, that is they can approximate arbitrarily well any continuous function of d variables on a compact domain.\n\u2022 We show that the approximation of functions with a compositional structure \u2013 such as f(x1, \u00b7 \u00b7 \u00b7 , xd) = h1(h2 \u00b7 \u00b7 \u00b7 (hj(hi1(x1, x2), hi2(x3, x4)), \u00b7 \u00b7 \u00b7 )) \u2013 can be achieved with the same degree of accuracy by deep and shallow networks but that the VC-dimension and the fat-shattering dimension are much smaller for the deep networks than for the shallow network with equivalent approximation accuracy. It is intuitive that a hierarchical network matching the structure of a compositional function should be \u201cbetter\u201d at approximating it than a generic shallow network but universality of shallow networks makes the statement less than obvious. Our result makes clear that the intuition is indeed correct and provides quantitative bounds.\n\u2022 Why are compositional functions important? We show that some basic visual recognition tasks do in fact require compositional functions. More in general, and less formally, it can be argued that symmetry properties of image statistics require hierarchies such as the bottom of Figure 1. In particular, we argue that hierarchical functions are effectively dictated by the statistics of natural images consisting of several overlapping objects, that is objects in clutter, rather than a single object against an homeogeneous background.\n\u2022 Finally we discuss the relation between compositionality and sparsity. We sketch new results that lead to interesting connections between the learning of Boolean functions and the learning of functions of real variables."}, {"heading": "2 PREVIOUS WORK", "text": "The success of Deep Learning in the present landscape of machine learning poses again an old theory question: why are multi-layer networks better than one-hidden-layer networks? Under which conditions? The question is relevant\nin several related fields from machine learning to function approximation and has appeared many times before.\nA personal (TP) version of this question starts with an old paper on nonlinear associative memories which described under which conditions higher and higher degree operators should be learned from data to improve the performance of linear regression. The idea that compositionality is important in networks for learning and requires several layers in a network was the subject of a chapter in an early paper on (mostly RBF) networks for regularization(Poggio and Girosi, 1989). Most Deep Learning references these days start with Hinton\u2019s backpropagation and with Lecun\u2019s convolutional networks (see for a nice review (LeCun et al., 2015)). Of course, multilayer convolutional networks have been around at least as far back as the optical processing era of the 70s. Fukushima\u2019s Neocognitron(Fukushima, 1980) was a convolutional neural network that was trained to recognize characters. The HMAX model of visual cortex(Riesenhuber and Poggio, 1999a) was described as a series of AND and OR layers to represent hierarchies of disjunctions of conjunctions.\nA version of the questions about why hierarchies was asked in (Poggio and Smale, 2003) as follow: A comparison with real brains offers another, and probably related, challenge to learning theory. The \u201clearning algorithms\u201d we have described in this paper correspond to one-layer architectures. Are hierarchical architectures with more layers justifiable in terms of learning theory? It seems that the learning theory of the type we have outlined does not offer any general argument in favor of hierarchical learning machines for regression or classification. This is somewhat of a puzzle since the organization of cortex \u2013 for instance visual cortex \u2013 is strongly hierarchical. At the same time, hierarchical learning systems show superior performance in several engineering applications. ...The theoretical issues surrounding hierarchical systems of this type are wide open, and likely to be of paramount importance for the next major development of efficient classifiers in several application domains...\nMore specific, intriguing work (Montufar et al., 2014) provided an estimation of the number of linear regions that a network with ReLU nonlinearities can in principle synthesize but leaves open the question of whether they can be used for learning. Sum-Product networks, which are equivalent to polynomial networks (see (B. Moore and Poggio, 1998; Livni et al., 2013), are a simple case of a hierarchy that can be analyzed (Delalleau and Bengio, 2011). Work on hierarchical quadratic networks (Livni et al., 2013), together with function approximation results (Pinkus, 1999; Mhaskar, 1993b), is most relevant to the approach here. This paper is a short, updated version of material that appeared in (Poggio et al., 2015b) and especially in (Poggio et al., 2015a)."}, {"heading": "3 MAIN RESULTS", "text": ""}, {"heading": "3.1 COMPOSITIONAL FUNCTIONS", "text": "It is natural to conjecture that hierarchical compositions of functions such as\nf(x1, \u00b7 \u00b7 \u00b7 , x8) = h3(h21(h11(x1, x2), h12(x3, x4)), h22(h13(x5, x6), h14(x7, x8))) (1)\nare approximated more efficiently by deep than by shallow networks.\nWe assume that the shallow networks do not have any structural information on the function to be learned (here its compositional structure), because they cannot represent it directly. Deep networks with standard architectures on the other hand do represent compositionality and can be adapted to the details of such prior information.\nIn addition, both shallow and deep representations may or may not reflect invariance to group transformations of the inputs of the function (Soatto, 2011; Anselmi et al., 2015). Invariance is expected to decrease the complexity of the network, for instance its VC-dimension. Since we are interested in the comparison of shallow vs deep architectures, here we consider the generic case of networks (and functions) for which invariance is not assumed.\nWe approximate functions of d variables of the form of Equation 1 functions with networks in which the activation nonlinearity is a so called ReLU, originally called ramp by Breiman and given by \u03c3(x) = |x|+ where |x|+ = max(0, x) . The architecture of the deep networks reflects Equation 1 with each node hi being a ridge function (in particular it may be an additive piecewise linear spline in the generalized sense of (Girosi et al., 1995), see Figure 1). As such, each node contains a certain number of units.\nIn the following we will estimate the number of units and layers required by shallow and hierarchical networks to approximate a function with a desired accuracy; we will then estimate the VC-dimension associated with the resulting network (assuming its output is used for binary classification). A direct connection between regression and binary classification is provided by the following observation due to (Livni et al., 2013). Combining Theorems 11.13 and 14.1 from (Anthony and Bartlett, 2002), it is possible to show that the fat-shattering dimension is upper-bounded by the VC-dimension of a slightly larger class of networks, which have an additional real input and an additional output node computing a linear threshold function in R2. Such a class of networks has a similar VC-dimension to the original class, hence the fat-shattering dimension can also be bounded."}, {"heading": "3.2 VC BOUNDS", "text": "To compute the relevant VC bounds we use a well-known result (Anthony and Bartlett, 2002):\nTheorem 1. Any binary function class in a Euclidean space, which is parameterized by at most n parameters and such that each activation function can be specified using at most p floating point operations, has VC-dimension at most O(np).\nThis result provides immediately VC bounds for shallow and deep networks with d input variables. Examples for d = 8 are shown in Figure 1. We recall that for ramp functions p = 1. We assume that the node in the shallow network consists of N units computing \u2211N i=1 ci\u03c3(\u3008wi,x\u3009 + bi), with wi,x \u2208 Rd and ci, bi \u2208 R. Similarly each of the d \u2212 1 nodes of the deep network consists of n units computing \u2211n i=1 ai| \u3008vi,x\u3009 + ti|, with vi,x \u2208 R\n2, ai, ti \u2208 R. We assume that d = 2` where ` is the number of layers. With this notation we obtain\nProposition 1. The VC-dimension of the shallow network with N units is bounded by (d + 2)N2; the VC-dimension of the binary tree network with n(d \u2212 1) units is bounded by 4n2(d\u2212 1)2.\nIt is important to emphasize here that state-of-art Deep Learning Neural Networks (DLNNs), with their small kernel size and many layers, are quite similar to the binary tree architecture (notice that the number of units decreases at higher and higher levels and that each unit receives only neighboring inputs from the layer below), independently of any additional (see remark in Figure 1) convolutional architecture. Visual cortex has a similar compositional architecture with receptive fields becoming larger and larger in higher and higher areas corresponding to layers here. An example is the old HMAX model (Riesenhuber and Poggio, 1999b) which took into account information about physiology of the ventral stream: it has an architecture of the binary tree type. Taking it all together, it is not suprising that DLNNs trained on Imagenet share properties with neurons at various stages of visual cortex: the results of this paper suggest that the reason is the very similar binary-tree-like architecture."}, {"heading": "3.3 DEGREE OF APPROXIMATION", "text": "We now describe the complexity of the shallow and deep networks, measured by the number of trainable parameters required in order to obtain uniform approximation to a target function up to an accuracy of > 0. Let Id = [\u22121, 1]d be the unit cube in the Euclidean space Rd, and \u2016 \u00b7 \u2016 denote\nthe uniform norm:\n\u2016f\u2016 = max x\u2208Id |f(x)|, f \u2208 C(Id).\nIf Vn \u2282 C(Id), where the parameter n indicates the complexity of Vn, the degree of approximation of f \u2208 C(Id) from V is defined by\ndist(f, Vn) = inf P\u2208Vn\n\u2016f \u2212 P\u2016. (2)\nA central problem in approximation theory is to study the interdependence of the rate at which dist(f, Vn) \u2192 0 as n \u2192 \u221e and the \u201csmoothness\u201d of f . It is customary in approximation theory to codify this smoothness information in terms of membership in a compact set K of functions. The worst case error in approximating a target function given the only a priori information that f \u2208 K is therefore\nworst(Vn,K) = sup{dist(f, Vn) : f \u2208 K}. (3)\nIf Vn \u2286 Vn+1 for all n, and V is the union of Vn\u2019s, then the complexity of approximation from V of functions in K is given for > 0 by\ncomplexity(V,K, ) = min{n \u2265 0 : worst(Vn,K) \u2264 }. (4) Typically, the index n in the notation Vn is proportional to the number of trainable parameters in describing the elements of Vn. The quantity complexity(V,K, ) thus represents the minimum number of parameters required to approximate an arbitrary function f \u2208 K by elements of V in order to guarantee that the error in approximation is \u2264 .\nLet \u03c3 : R\u2192 R be the activation function computed by each unit in the network. The number of trainable parameters in the set Sn of all shallow networks of the form\nn\u2211 i=1 ai\u03c3(\u3008wi,x\u3009+ bi) (5)\nis (d + 2)n, and dist(f,Sn) is the best error than can be expected in approximating f by such networks. We define W to be the set of all f : Id \u2192 R which have a continuous gradient, such that \u2016f\u2016W = \u2016f\u2016 + \u2211d k=1 \u2016Dkf\u2016 \u2264 1. We are interested in estimating complexity(S,W, ) (see (6) below).\nThe key idea here is that the closure of the space spanned by r linear combination of dilations and shifts of a univariate C\u221e function \u03c3 which is not a polynomial contains the space of univariate polynomials \u03a0r\u22121 of degree r \u2212 1 (see (Mhaskar, 1996, Proposition 2.2), (Pinkus, 1999, Corollary 3.6)). In fact, the following proposition is proved in (Mhaskar, 1996), (see also (Pinkus, 1999)):\nProposition 2. Let N \u2265 1, d \u2265 2 be integers, and \u03c3 be a univariate C\u221e function which is not a polynomial. Then the closure of SNd contains the space of all polynomials in d variables with coordinatewise degree < N .\nUsing this fact, it is observed in (Mhaskar, 1996, Theorem 2.1) that when \u03c3 is as in Proposition 2, the following estimate on the complexity of approximation by shallow networks S = \u22c3\u221e n=1 Sn holds:\ncomplexity(V,W, ) = O( \u2212d). (6)\nIs this the best? To investigate this question, let Mn : K \u2192 Rn be a continuous mapping (parameter selection), and An : Rn \u2192 C(Id) be any mapping (recovery algorithm). Then an approximation to f is given byAn(Mn(f)), where the continuity of Mn means that the selection of parameters is robust with respect to perturbations in f . Analogous to the way the complexity was defined in (4) using (3), one can define the nonlinear n\u2013width of a compact set K by (cf. (DeVore et al., 1989))\ndn(K) = inf Mn,An sup f\u2208K \u2016f \u2212An(Mn(f))\u2016, (7)\nand the curse for K by\ncurse(K, ) = min{n \u2265 1 : dn(K) \u2264 }. (8)\nWhile the complexity in (4) depends upon the choice of approximants V , the curse depends only on K, and represents the best that can be achieved by any continuous parameter selection and recovery processes. Neither of them addresses the question of how to accomplish the approximation. It is shown in (DeVore et al., 1989) that curse(W, ) \u2265 c \u2212d. So, the estimate (6) is the best possible among all reasonable methods of approximating arbitrary functions in W , although by itself, the estimate (6) is blind to the process by which the approximation is accomplished; in particular, this process is not required to be robust.\nWe note that both the curse and the complexity depends upon the norm in which the approximation is desired and the class K to which the target function in known to belong. In general, shallow networks do a better approximation, for example, with both the curse and complexity of the order \u22122, independent of the input dimension d, if the class K consists of functions which are inherently shallow networks in some abstract sense, i.e., where the target function has the form (5), except that the sum expression is replaced by an integral with respect to some measure (e.g., (Barron, 1993; Kurkova\u0301 and Sanguineti, 2001; Kurkova\u0301 and Sanguineti, 2002; Mhaskar, 2004)).\nFor the hierarchical binary tree network, the analogue of these results is obtained by considering the compact set WH to be the class of all functions f which have the same\nstructure (e.g., (1)), where each of the constituent functions h are in W (applied with only 2 variables). We define the corresponding class of deep networks Dn to be set of all functions with the same structure, where each of the functions h is in Sn, and D = \u22c3\u221e n=1Dn. Since each of the constituent function is a function in W of two variables, (6) applied with d = 2 implies that each of these functions can be approximated from S \u22122 up to accuracy . Our assumption that f \u2208 WH implies that there is a \u201cgood propagation\u201d of the error in an approximation of f from D \u22122 . Thus, for functions in WH , we have\ncomplexity(D,K, ) = O( \u22122). (9)\nWe note that the number of parameters involved in an element of D \u22122 is (d\u2212 1)(d+ 2)n. Thus, (9) is a substantial improvement over (6) in terms of the number of trainable parameters required to achieve the accuracy , provided the target function has the hierarchical structure.\nRemarks\n\u2022 Given the estimates in this section, a bound on the VC-dimension of the binary classifier induced by the deep network in D \u22122 is O( \u22122), whereas the VCdimension of the shallow network in S \u22122 is O( \u2212d).\n\u2022 The above estimates can be easily extended from the special case of a binary tree to similar types of trees, and even directed acyclic graphs.\n\u2022 The assumption f \u2208 W (respectively, f \u2208 WH ) may be guaranteed in actual implementations by commonly used normalization operations on the weights of a deep network.\n\u2022 The ReLU activation function satisfies in practice the assumptions of Proposition 2 (see Proposition 3.7 (Pinkus, 1999)).\n\u2022 Since the bounds (9) and (6) apply under different a priori assumptions on the target functions, it is misleading to conclude that deep networks always yield better approximations in terms of the number of parameters involved for every target function. From the point of view of approximation theory as applied to arbitrary smooth functions, an interesting advantage of deep networks (with more than one hidden layer) is that they provide optimal local approximation analogous to spline approximation, while shallow networks are inherently unable to do so (Mhaskar, 1993a; Mhaskar, 1993b; Chui et al., 1994; Chui et al., 1996).\n\u2022 The fact that hierarchical functions are approximated more efficiently by deep than by shallow networks has been shown to be true in an important special\ncase. For the hierarchical quadratic networks described in (Livni et al., 2013) (see section 4) there a VC-dimension bound is much lower than for the corresponding shallow network.\n\u2022 In a sense hierarchical deep networks can avoid the curse of dimensionality for compositional functions with respect to shallow networks because each module in the hierarchy has bounded dimensionality, which is equal to 2 in the binary tree case. As we mention elsewhere, the VC-dimension can be further reduced by invariances such as translation invariance (corresponding to weight sharing)."}, {"heading": "4 WHY DOES VISION REQUIRE COMPOSITIONAL FUNCTIONS?", "text": "We saw that, though both deep hierarchies and shallow hierarchies are universal, deep hierarchies are approximated inefficiently by shallow ones. The final step in the argument is to show that deep hierarchical functions represent critical computations for vision.\nThe general point is that hierarchies \u2013 and weight sharing \u2013 reflect symmetries in the physical world that manifest themselves through the image statistics. Assume for instance that a computational hierarchy such as\nhl(\u00b7 \u00b7 \u00b7h3(h21(h11(x1, x2), h12(x3, x4)), h22(h13(x5, x6), h14(x7, x8)) \u00b7 \u00b7 \u00b7 )))\n(10)\nis given. Then shift invariance of the image statistics could be reflected in the following property: the local node \u201cprocessors\u201d should satisfy h21 = h22 and h11 = h12 = h13 = h14 since there is no reason for them to be different across images. In a similar way h3 and h21 should be \u201cscaled\u201d versions of each other because of scale invariance. Similar invariances of image statistics \u2013 for instance to rotation \u2013 can be similarly use to constrain the local processes h.\nIt is natural to ask whether the hierarchy itself \u2013 for simplicity the idealized binary tree of the Figure 1\u2013 follows from a specific symmetry in the world and which one. A possible answer to this question follows from the fact that in natural images the target object is usually among several other objects at a range of scales and position, that is the target object is embedded in clutter. From the physical point of view, this is equivalent to the observation that there are several localized clusters of surfaces with similar properties (objects). These basic aspects of the physical world are reflected in properties of the statistics of images: locality, shift invariance and scale invariance. In particular, locality reflects clustering of similar surfaces in the world \u2013 the closer to each other pixels are in the image, the more likely they are to be correlated. Thus nearby patches are likely to be correlated (because of locality), and so are neighboring\n(because of shift invariance) image regions of increasing size (because of scale invariance). Ruderman\u2019s pioneering work (Ruderman, 1997) concludes that this set of properties is equivalent to the statement that natural images consist of many object patches that may partly occlude each other (object patches are image patches which have similar properties because they are induced by local groups of surfaces with similar properties). We argue that Ruderman\u2019s conclusion implies\n\u2022 the property of selfsimilarity of image statistics, which in part reflects the compositionality of objects and parts: parts are themselves objects, that is selfsimilar clusters of similar surfaces in the physical world.\n\u2022 the pervasive presence of clutter: in an image target objects are typically embedded among many objects at different scales and positions.\nThe first property \u2013 compositionality \u2013 was a main motivation for hierarchical architectures such as Fukushima\u2019s and later imitations of it such as HMAX which can be seen to be similar to a pyramid of AND and OR layers (Riesenhuber and Poggio, 1999b), that is a sequence of conjunctions and disjunctions. The presence of clutter, together with the need of position and scale invariance, implies that many visual computations, that is functions evaluated on images, should have a compositional structure. According to these arguments, compositional functions should be important for vision tasks. Notice that there is a subtle but important distinction between a) translation and scale invariance in object recognition and b) shift invariance and scale invariance of the statistics of natural images. Of course, the reason one wants shift and scale invariance in object recognition is because objects can appear at any position and scale in images, which is exactly why the statistics of images is position and scale invariant. In a related vein, it is possible to learn invariances, such as shift and scale invariance, from generic visual experience, because they are reflected in image statistics."}, {"heading": "4.1 SANITY CHECK: RECOGNITION IN CLUTTER", "text": "The arguments of the previous section are suggestive. Here we provide a more formal argument which shows a significant advantage of hierarchical functions relative to shallow ones. The advantage derives from the locality property of objects and concerns an important computation: recognition in clutter. Figure 2 summarizes the skeleton of the argument: the recognition of object A suffers from interference from clutter (B) in a shallow network.\nThe point can be formalized by starting from an (obvious) result (Anselmi et al., 2015) showing that pooling over a\nconvolution is a group average of dot products of the image with transformations of templates by group elements. This group average is invariant and can be unique. By adapting the proof there (Anselmi et al., 2015), it is easy to show (as we are doing in a longer version of this paper), that invariance and potential uniquenss of pooling do not hold when clutter \u2013 defined as image patches that may change from one presentation of the object to the next presentation \u2013 is present within the pooling regions.\nAs shown in Figure 2, in the case of the deep network each of the two objects can be processed in the lower layers without interference from the other object. Thus hierarchies, unlike shallow architectures, correspond to visual computations that are more robust to clutter."}, {"heading": "5 SPARSE FUNCTIONS", "text": "In this section we argue that the case of tree-like compositional functions is a special case of a more general formulation.\nFor the the intuition underlying our claim consider a node in the last layer of a deep network with two layers. The outputs of the layer are combined to provide the one-dimensional output of the network. The node in the last layer consists of m units, each evaluating an activation function \u03c3 as described in Section 3.3. The node receives inputs from the layer below which approximates an element of \u03a0k,d, the linear space of multivariate algebraic polynomials of coordinatewise degree at most k \u2212 1 in d variables. Suppose now that the node we are considering has the same inputs for each of its m units. The node then approximates a polynomial in\n\u03a0m,1, the linear space of (univariate) algebraic polynomials of degree at most m \u2212 1. The output of the node \u2013 which is the output of the network in our example \u2013\u2211m j=1 aj\u03c3 (\u2329 vj , \u2211kd i=1 ci\u03c3(\u3008wi,x\u3009+ bi) \u232a + tj ) thus approximates polynomials of coordinatewise degree (k \u2212 1)(m\u2212 1) in x which are sparse in the sense that they may have much fewer independent coefficients than a generic polynomial in x of the same degree.\nWe have already implied in Section 3.3 that a hierarchical network can approximate a high degree polynomial P in the input variables x1, \u00b7 \u00b7 \u00b7 , xd, that can be written as a hierarchical composition of lower degree polynomials. For example, let\nQ(x, y) = (Ax2y2 +Bx2y + Cxy2 +Dx2 + 2Exy +\nFy2 + 2Gx+ 2Hy + I)2 10 .\nSince Q is nominally a polynomial of coordinatewise degree 211, Proposition 2 shows that a shallow network with 211+1 units is able to approximateQ arbitrarily well on Id. However, because of the hierarchical structure ofQ, Proposition 2 shows also that a hierarchical network with 9 units can approximate the quadratic expression, and 10 further layers, each with 3 units can approximate the successive powers. Thus, a hierarchical network with 11 layers and 39 units can approximateQ arbitrarily well. We note that even ifQ is nominally of degree 211, each of the monomial coefficients in Q is a function of only 9 variables, A, \u00b7 \u00b7 \u00b7 , I . A similar, simpler example was tested using standard DLNN software and is shown in Figure 3.\nSimilar considerations apply to trigonometric polynomials and thus to Fourier approximations of functions. In this context, the lowest level layer can be regarded as approximating a trigonometric polynomial of a low degree, e.g., see (Mhaskar and Micchelli, 1995), and the layers after that can continue to be regarded as effectively approximating algebraic polynomials of high degree as above. In this way the network can be shown to approximate trigonometric polynomials containing high frequency terms. Notice that adding units to a shallow network with a single layer can also increase the highest frequency in the Fourier approximation but much more slowly than adding the same number of units to create an additional layer. The tradeoff is that now the Fourier terms that are contained in the function space approximated by the network are not anymore guaranteed to be a generic polynomial but just a sparse polynomial and have fewer trainable free parameters.\nDefinition 1. We formally say that a polynomial (algebraic or trigonometric) is sparse if each of the coefficients in the polynomial with respect to some basis is a function of a small number of parameters.\nIt is clear \u2013 and we will provide formal proofs in another paper \u2013 that this notion generalizes the hierarchical network\npolynomials as well as the notion of sparsity in compressive sensing, where sparsity is defined in terms of the number of non\u2013zero coefficients in some expansion.\nWith this definition we state results from previous sections in the following way.\nCompositional functions can be represented by sparse algebraic or trigonometric polynomials; such sparse polynomials can be approximated by hierarchical networks with a matching architecture better than by shallow networks.\nFollowing the discussion of this section and the example of Figure 3, we conjecture that general sparse functions, and not only compositional ones, may be approximated better by hierarchical networks. It will be interesting to explore the sense in which functions admitting sparse polynomial approximations may be generic (see section 4 for an analysis similar in this spirit). It is equally intriguing to speculate whether in practice (Poggio and Smale, 2003) only sparse functions may be learnable."}, {"heading": "6 BOOLEAN FUNCTIONS", "text": "Our results sketched in the previous section are interesting not only in themselves but also because they suggest several connections to similar properties of Boolean functions. In fact our results seem to generalize properties already known for Boolean functions which are of course a special case of functions of real variables. We first recall some definitions followed by a few observations.\nOne of the most important and versatile tools for theoretical computer scientists for the study of functions of n Boolean variables, their related circuit design and several associated learning problems, is the Fourier transform over the Abelian group Zn2 . This is known as Fourier analysis over the Boolean cube {\u22121, 1}n. The Fourier expansion of a Boolean function f : {\u22121, 1}n \u2192 {\u22121, 1} or even a real-valued Boolean function f : {\u22121, 1}n \u2192 [\u22121, 1] is its representation as a real polynomial, which is multilinear because of the Boolean nature of its variables. Thus for Boolean functions their Fourier representation is identical to their polynomial representation. In the following we will use the two terms interchangeably. Unlike functions of real variables, the full finite Fourier expansion is exact instead of an approximation and there is no need to distingush between trigonometric and real polynomials. Most of the properties of standard harmonic analysis are otherwise preserved, including Parseval theorem. The terms in the expansion correspond to the various monomials; the low order ones are parity functions over small subsets of the variables and correspond to low degrees and low frequencies in the case of polynomial and Fourier approximations, respectively, for functions of real variables.\nSection 5 suggests the following approach to characterize which functions are best learned by which type of network\n\u2013 for instance shallow or deep. The structure of the network is reflected in polynomials that are best approximated by it \u2013 for instance generic polynomials or sparse polynomials (in the coefficients) in d variables of order k. The tree structure of the nodes of a deep network reflects the structure of a specific sparse polynomial. Generic polynomial of degree k in d variables are difficult to learn because the number of terms, trainable parameters and associated VC-dimension are all exponential in d. On the other hand, functions approximated well by sparse polynomials can be learned efficiently by deep networks with a tree structure that matches the polynomial. We recall that in a similar way several properties of certain Boolean functions can be \u201cread out\u201d from the terms of their Fourier expansion corresponding to \u201clarge\u201d coefficients, that is from a polynomial that approximates well the function.\nClassical results (Hastad, 1987) about the depth-breadth tradeoff in circuits design show that deep circuits are more efficient in representing certain Boolean functions than shallow circuits. Hastad proved that highly-variable functions (in the sense of having high frequencies in their Fourier spectrum) in particular the parity function cannot even be decently approximated by small constant depth circuits (see also (Linial et al., 1993)). These results on Boolean functions have been often quoted in support of the claim that deep neural networks can represent functions that shallow networks cannot. For instance Bengio and LeCun (Bengio and LeCun, 2007) write \u201cWe claim that most functions that can be represented compactly by deep architectures cannot be represented by a compact shallow architecture\u201d.\u201d. Until now this conjecture lacked a formal proof. It seems now that the results summarized in this paper and especially the ones announced in section 5 settle the issue, justifying the original conjecture and providing a general approach connecting results on Boolean functions with current real valued neural networks. Of course, we do not imply that the capacity of deep networks is exponentially larger than the capacity of shallow networks. As pointed out by Shalev-Shwartz, this is clearly not true, since the VC dimension of a network depends on the number of nodes and parameters and not on the depth. We remark that a nice theorem was recently published (Telgarsky, 2015), showing that a certain family of classification problems with real-valued inputs cannot be approximated well by shallow networks with fewer than exponentially many nodes whereas a deep network achieves zero error. This is a special case of our results and corresponds to highfrequency, sparse trigonometric polynomials.\nFinally, we want to speculate about a series of observations on Boolean functions that may show an interesting use of our approach using the approximating polynomials and networks for studying the learning of general functions. It is known that within Boolean functions the AC0 class of polynomial size constant depth circuits is characterized by\nf(x) = 2(2cos2(x) 2-1-1)\nFourier transforms where most of the power spectrum is in the low order coefficients. Such functions can be approximated well by a polynomial of low degree and can be learned well by considering only such coefficients. In general, two algorithms (Mansour, 1994) seems to allow learning of certain Boolean function classes:\n1. the low order algorithm that approximates functions by considering their low order Fourier coefficients and\n2. the sparse algorithm which learns a function by approximating its significant coefficients.\nDecision lists and decision trees can be learned by algorithm 1. Functions with small L1 norm can be approximated well by algorithm 2. Boolean circuits expressing DNFs can be approximated by 1 but even better by 2. In\nfact, in many cases most of the coefficients of the low terms may still be negligeable and furthermore it may the case that a function can be approximated by a small set of coefficients but these coefficients do not correspond to loworder terms. All these cases are consistent with the description we have in section 5. For general functions they may suggest the following. Many functions can be learned efficiently in terms of their low order coefficients and thus by shallow networks. This corresponds to using Tikhonov regularization that effectively cuts out high frequencies. Other functions must be learned in terms of their sparse coefficients by a deep network with an appropriate architecture. This is more similar to L1 regularization. The sparsity approach which corresponds to deep networks includes the shallow Tikhonov approach and thus is more general and preferrable at least as long as computational and sample complexity issues are not taken into account."}, {"heading": "7 DISCUSSION", "text": "There are two basic questions about Deep Neural Networks. The first question is about the power of the architecture \u2013 which classes of functions can it approximate how well? In this paper we focus on the approximation properties of a polynomial in the input vector x which is in the span of the network parameters. The second question, which we do not address here, is about learning the unknown coefficients from the data: do multiple solutions exist? How \u201cmany\u201d? Why is SGD so unreasonably efficient, at least in appearance? Are good minima easier to find with deep than shallow networks? This last point is in practice very important. Notice that empirical tests like, Figure 3, typically mix answers to the two questions.\nIn this paper we address only the first question. We have introduced compositional functions and show that they are sparse in the sense that they can be approximated by sparse polynomials. Sparse polynomials do not need to be compositional in the specific sense of Figure 1 (consider the one-dimensional case). Deep networks can represent functions that have high frequencies in their sparse Fourier representation with fewer units than shallow networks (thus with lower VC-dimension). An unexpected outcome of our comparison of shallow vs. deep networks is that sparsity of Fourier coefficients is a a more general constraint for learning than Tikhonov-type smoothing priors. The latter is usually equivalent to cutting high order Fourier coefficients. The two priors can be regarded as two different implementations of sparsity, one more general than the other. Both reduce the number of terms, that is trainable parameters, in the approximating trigonometric polynomial \u2013 that is the number of Fourier coefficients.\nIn summary, we have derived new connections between a body of apparently disparate mathematical concepts that apply to learning of real-valued as well as of Boolean functions. They include compositionality, degree of approximating polynomials, sparsity, VC-dimension, Fourier analysis of Boolean functions and their role in determining when deep networks have a lower sample complexity than shallow networks. One of the conclusions is that standard Tikhonov regularization in RKHS should be replaced, when possible, by compositionality or sparsity via deep networks."}, {"heading": "Acknowledgment", "text": "This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. HNM was supported in part by ARO Grant W911NF-15-1-0385."}], "references": [{"title": "Unsupervised learning of invariant representations", "author": ["F. Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio"], "venue": "Theoretical Computer Science.", "citeRegEx": "Anselmi et al\\.,? 2015", "shortCiteRegEx": "Anselmi et al\\.", "year": 2015}, {"title": "Neural Network Learning - Theoretical Foundations", "author": ["M. Anthony", "P. Bartlett"], "venue": "Cambridge University Press.", "citeRegEx": "Anthony and Bartlett,? 2002", "shortCiteRegEx": "Anthony and Bartlett", "year": 2002}, {"title": "Representations properties of multilayer feedforward networks", "author": ["B.B. Moore", "T. Poggio"], "venue": "Abstracts of the First annual INNS meeting, 320:502.", "citeRegEx": "Moore and Poggio,? 1998", "shortCiteRegEx": "Moore and Poggio", "year": 1998}, {"title": "Universal approximation bounds for superpositions of a sigmoidal function", "author": ["A.R. Barron"], "venue": "Information Theory, IEEE Transactions on, 39(3):930\u2013945.", "citeRegEx": "Barron,? 1993", "shortCiteRegEx": "Barron", "year": 1993}, {"title": "Scaling learning algorithms towards ai", "author": ["Y. Bengio", "Y. LeCun"], "venue": "Bottou, L., Chapelle, O., and DeCoste, D.and Weston, J., editors, Large-Scale Kernel Machines. MIT Press.", "citeRegEx": "Bengio and LeCun,? 2007", "shortCiteRegEx": "Bengio and LeCun", "year": 2007}, {"title": "Neural networks for localized approximation", "author": ["C.K. Chui", "X. Li", "H.N. Mhaskar"], "venue": "Mathematics of Computation, 63(208):607\u2013623.", "citeRegEx": "Chui et al\\.,? 1994", "shortCiteRegEx": "Chui et al\\.", "year": 1994}, {"title": "Limitations of the approximation capabilities of neural networks with one hidden layer", "author": ["C.K. Chui", "X. Li", "H.N. Mhaskar"], "venue": "Advances in Computational Mathematics, 5(1):233\u2013243.", "citeRegEx": "Chui et al\\.,? 1996", "shortCiteRegEx": "Chui et al\\.", "year": 1996}, {"title": "Shallow vs", "author": ["O. Delalleau", "Y. Bengio"], "venue": "deep sum-product networks. In Advances in Neural Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems 2011. Proceedings of a meeting held 12-14 December 2011,", "citeRegEx": "Delalleau and Bengio,? 2011", "shortCiteRegEx": "Delalleau and Bengio", "year": 2011}, {"title": "Optimal nonlinear approximation", "author": ["R.A. DeVore", "R. Howard", "C.A. Micchelli"], "venue": "Manuscripta mathematica, 63(4):469\u2013478.", "citeRegEx": "DeVore et al\\.,? 1989", "shortCiteRegEx": "DeVore et al\\.", "year": 1989}, {"title": "Neocognitron: A self-organizing neural network for a mechanism of pattern recognition unaffected by shift in position", "author": ["K. Fukushima"], "venue": "Biological Cybernetics, 36(4):193\u2013202.", "citeRegEx": "Fukushima,? 1980", "shortCiteRegEx": "Fukushima", "year": 1980}, {"title": "Regularization theory and neural networks architectures", "author": ["F. Girosi", "M. Jones", "T. Poggio"], "venue": "Neural Computation, 7:219\u2013269.", "citeRegEx": "Girosi et al\\.,? 1995", "shortCiteRegEx": "Girosi et al\\.", "year": 1995}, {"title": "Computational Limitations for Small Depth Circuits", "author": ["J.T. Hastad"], "venue": "MIT Press.", "citeRegEx": "Hastad,? 1987", "shortCiteRegEx": "Hastad", "year": 1987}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy,? 2015", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Bounds on rates of variable basis and neural network approximation", "author": ["V. Kurkov\u00e1", "M. Sanguineti"], "venue": "IEEE Transactions on Information Theory, 47(6):2659\u20132665.", "citeRegEx": "Kurkov\u00e1 and Sanguineti,? 2001", "shortCiteRegEx": "Kurkov\u00e1 and Sanguineti", "year": 2001}, {"title": "Comparison of worst case errors in linear and neural network approximation", "author": ["V. Kurkov\u00e1", "M. Sanguineti"], "venue": "IEEE Transactions on Information Theory, 48(1):264\u2013275.", "citeRegEx": "Kurkov\u00e1 and Sanguineti,? 2002", "shortCiteRegEx": "Kurkov\u00e1 and Sanguineti", "year": 2002}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G.", "H."], "venue": "Nature, pages 436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Constant depth circuits, fourier transform, and learnability", "author": ["Linial N.", "Y.M.", "N.", "N."], "venue": "Journal of the ACM, 40(3):607620.", "citeRegEx": "N. et al\\.,? 1993", "shortCiteRegEx": "N. et al\\.", "year": 1993}, {"title": "A provably efficient algorithm for training deep networks", "author": ["R. Livni", "S. Shalev-Shwartz", "O. Shamir"], "venue": "CoRR, abs/1304.7045.", "citeRegEx": "Livni et al\\.,? 2013", "shortCiteRegEx": "Livni et al\\.", "year": 2013}, {"title": "Learning boolean functions via the fourier transform", "author": ["Y. Mansour"], "venue": "Roychowdhury, V., Siu, K., and Orlitsky, A., editors, Theoretical Advances in Neural Computation and Learning, pages 391\u2013424. Springer US.", "citeRegEx": "Mansour,? 1994", "shortCiteRegEx": "Mansour", "year": 1994}, {"title": "Approximation properties of a multilayered feedforward artificial neural network", "author": ["H.N. Mhaskar"], "venue": "Advances in Computational Mathematics, 1(1):61\u2013", "citeRegEx": "Mhaskar,? 1993a", "shortCiteRegEx": "Mhaskar", "year": 1993}, {"title": "Neural networks for localized approximation of real functions", "author": ["H.N. Mhaskar"], "venue": "Neural Networks for Processing [1993] III. Proceedings of the 1993 IEEESP Workshop, pages 190\u2013196. IEEE.", "citeRegEx": "Mhaskar,? 1993b", "shortCiteRegEx": "Mhaskar", "year": 1993}, {"title": "Neural networks for optimal approximation of smotth and analytic functions", "author": ["H.N. Mhaskar"], "venue": "Neural Computation, 8:164\u2013177.", "citeRegEx": "Mhaskar,? 1996", "shortCiteRegEx": "Mhaskar", "year": 1996}, {"title": "On the tractability of multivariate integration and approximation by neural networks", "author": ["H.N. Mhaskar"], "venue": "Journal of Complexity, 20(4):561\u2013590.", "citeRegEx": "Mhaskar,? 2004", "shortCiteRegEx": "Mhaskar", "year": 2004}, {"title": "Degree of approximation by neural and translation networks with a single hidden layer", "author": ["H.N. Mhaskar", "C.A. Micchelli"], "venue": "Advances in Applied Mathematics, 16(2):151\u2013183.", "citeRegEx": "Mhaskar and Micchelli,? 1995", "shortCiteRegEx": "Mhaskar and Micchelli", "year": 1995}, {"title": "On the number of linear regions of deep neural networks", "author": ["G.F. Montufar", "R. Pascanu", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 27:2924\u20132932.", "citeRegEx": "Montufar et al\\.,? 2014", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Approximation theory of the mlp model in neural networks", "author": ["A. Pinkus"], "venue": "Acta Numerica, pages 143\u2013195.", "citeRegEx": "Pinkus,? 1999", "shortCiteRegEx": "Pinkus", "year": 1999}, {"title": "I-theory on depth vs width: hierarchical function composition", "author": ["T. Poggio", "F. Anselmi", "L. Rosasco"], "venue": "CBMM memo 041.", "citeRegEx": "Poggio et al\\.,? 2015a", "shortCiteRegEx": "Poggio et al\\.", "year": 2015}, {"title": "A theory of networks for approximation and learning", "author": ["T. Poggio", "F. Girosi"], "venue": "Laboratory, Massachusetts Institute of Technology, A.I. memo n1140.", "citeRegEx": "Poggio and Girosi,? 1989", "shortCiteRegEx": "Poggio and Girosi", "year": 1989}, {"title": "Notes on hierarchical splines, dclns and i-theory", "author": ["T. Poggio", "L. Rosaco", "A. Shashua", "N. Cohen", "F. Anselmi"], "venue": "CBMM memo 037.", "citeRegEx": "Poggio et al\\.,? 2015b", "shortCiteRegEx": "Poggio et al\\.", "year": 2015}, {"title": "The mathematics of learning: Dealing with data", "author": ["T. Poggio", "S. Smale"], "venue": "Notices of the American Mathematical Society (AMS), 50(5):537\u2013544.", "citeRegEx": "Poggio and Smale,? 2003", "shortCiteRegEx": "Poggio and Smale", "year": 2003}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M. Riesenhuber", "T. Poggio"], "venue": "Nat. Neurosci., 2(11):1019\u20131025.", "citeRegEx": "Riesenhuber and Poggio,? 1999a", "shortCiteRegEx": "Riesenhuber and Poggio", "year": 1999}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M. Riesenhuber", "T. Poggio"], "venue": "Nature Neuroscience, 2(11):1019\u20131025.", "citeRegEx": "Riesenhuber and Poggio,? 1999b", "shortCiteRegEx": "Riesenhuber and Poggio", "year": 1999}, {"title": "Origins of scaling in natural images", "author": ["D. Ruderman"], "venue": "Vision Res., pages 3385 \u2013 3398.", "citeRegEx": "Ruderman,? 1997", "shortCiteRegEx": "Ruderman", "year": 1997}, {"title": "Steps Towards a Theory of Visual Information: Active Perception, Signal-to-Symbol Conversion and the Interplay Between Sensing and Control", "author": ["S. Soatto"], "venue": "arXiv:1110.2053, pages 0\u2013151.", "citeRegEx": "Soatto,? 2011", "shortCiteRegEx": "Soatto", "year": 2011}, {"title": "Representation benefits of deep feedforward networks", "author": ["M. Telgarsky"], "venue": "arXiv preprint arXiv:1509.08101v2 [cs.LG] 29 Sep 2015.", "citeRegEx": "Telgarsky,? 2015", "shortCiteRegEx": "Telgarsky", "year": 2015}, {"title": "Matconvnet: Convolutional neural networks for matlab", "author": ["A. Vedaldi", "K. Lenc"], "venue": "Proceedings of the 23rd Annual ACM Conference on Multimedia Conference, pages 689\u2013692. ACM.", "citeRegEx": "Vedaldi and Lenc,? 2015", "shortCiteRegEx": "Vedaldi and Lenc", "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "The idea that compositionality is important in networks for learning and requires several layers in a network was the subject of a chapter in an early paper on (mostly RBF) networks for regularization(Poggio and Girosi, 1989).", "startOffset": 200, "endOffset": 225}, {"referenceID": 15, "context": "Most Deep Learning references these days start with Hinton\u2019s backpropagation and with Lecun\u2019s convolutional networks (see for a nice review (LeCun et al., 2015)).", "startOffset": 140, "endOffset": 160}, {"referenceID": 9, "context": "Fukushima\u2019s Neocognitron(Fukushima, 1980) was a convolutional neural network that was trained to recognize characters.", "startOffset": 24, "endOffset": 41}, {"referenceID": 30, "context": "The HMAX model of visual cortex(Riesenhuber and Poggio, 1999a) was described as a series of AND and OR layers to represent hierarchies of disjunctions of conjunctions.", "startOffset": 31, "endOffset": 62}, {"referenceID": 29, "context": "A version of the questions about why hierarchies was asked in (Poggio and Smale, 2003) as follow: A comparison with", "startOffset": 62, "endOffset": 86}, {"referenceID": 24, "context": "More specific, intriguing work (Montufar et al., 2014) provided an estimation of the number of linear regions that a network with ReLU nonlinearities can in principle synthesize but leaves open the question of whether they can be used for learning.", "startOffset": 31, "endOffset": 54}, {"referenceID": 17, "context": "Sum-Product networks, which are equivalent to polynomial networks (see (B. Moore and Poggio, 1998; Livni et al., 2013), are a simple case of a hierarchy that can be analyzed (Delalleau and Bengio, 2011).", "startOffset": 71, "endOffset": 118}, {"referenceID": 7, "context": ", 2013), are a simple case of a hierarchy that can be analyzed (Delalleau and Bengio, 2011).", "startOffset": 63, "endOffset": 91}, {"referenceID": 17, "context": "Work on hierarchical quadratic networks (Livni et al., 2013), together with function approximation results (Pinkus, 1999; Mhaskar, 1993b), is most relevant to the approach here.", "startOffset": 40, "endOffset": 60}, {"referenceID": 25, "context": ", 2013), together with function approximation results (Pinkus, 1999; Mhaskar, 1993b), is most relevant to the approach here.", "startOffset": 54, "endOffset": 84}, {"referenceID": 20, "context": ", 2013), together with function approximation results (Pinkus, 1999; Mhaskar, 1993b), is most relevant to the approach here.", "startOffset": 54, "endOffset": 84}, {"referenceID": 28, "context": "This paper is a short, updated version of material that appeared in (Poggio et al., 2015b) and especially in (Poggio et al.", "startOffset": 68, "endOffset": 90}, {"referenceID": 26, "context": ", 2015b) and especially in (Poggio et al., 2015a).", "startOffset": 27, "endOffset": 49}, {"referenceID": 33, "context": "In addition, both shallow and deep representations may or may not reflect invariance to group transformations of the inputs of the function (Soatto, 2011; Anselmi et al., 2015).", "startOffset": 140, "endOffset": 176}, {"referenceID": 0, "context": "In addition, both shallow and deep representations may or may not reflect invariance to group transformations of the inputs of the function (Soatto, 2011; Anselmi et al., 2015).", "startOffset": 140, "endOffset": 176}, {"referenceID": 10, "context": "The architecture of the deep networks reflects Equation 1 with each node hi being a ridge function (in particular it may be an additive piecewise linear spline in the generalized sense of (Girosi et al., 1995), see Figure 1).", "startOffset": 188, "endOffset": 209}, {"referenceID": 17, "context": "A direct connection between regression and binary classification is provided by the following observation due to (Livni et al., 2013).", "startOffset": 113, "endOffset": 133}, {"referenceID": 1, "context": "1 from (Anthony and Bartlett, 2002), it is possible to show that the fat-shattering dimension is upper-bounded by the VC-dimension of a slightly larger class of networks, which have an additional real input and an additional output node computing a linear threshold function in R.", "startOffset": 7, "endOffset": 35}, {"referenceID": 1, "context": "To compute the relevant VC bounds we use a well-known result (Anthony and Bartlett, 2002):", "startOffset": 61, "endOffset": 89}, {"referenceID": 31, "context": "An example is the old HMAX model (Riesenhuber and Poggio, 1999b) which took into account information about physiology of the ventral stream: it has an architecture of the binary tree type.", "startOffset": 33, "endOffset": 64}, {"referenceID": 25, "context": "Each of the nodes in b) consists of n ReLU units and computes the ridge function (Pinkus, 1999) \u2211n i=1 ai| \u3008vi,x\u3009 + ti|+, with vi,x \u2208 R , ai, ti \u2208 R.", "startOffset": 81, "endOffset": 95}, {"referenceID": 21, "context": "In fact, the following proposition is proved in (Mhaskar, 1996), (see also (Pinkus, 1999)):", "startOffset": 48, "endOffset": 63}, {"referenceID": 25, "context": "In fact, the following proposition is proved in (Mhaskar, 1996), (see also (Pinkus, 1999)):", "startOffset": 75, "endOffset": 89}, {"referenceID": 8, "context": "(DeVore et al., 1989))", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "It is shown in (DeVore et al., 1989) that curse(W, ) \u2265 c \u2212d.", "startOffset": 15, "endOffset": 36}, {"referenceID": 3, "context": ", (Barron, 1993; Kurkov\u00e1 and Sanguineti, 2001; Kurkov\u00e1 and Sanguineti, 2002; Mhaskar, 2004)).", "startOffset": 2, "endOffset": 91}, {"referenceID": 13, "context": ", (Barron, 1993; Kurkov\u00e1 and Sanguineti, 2001; Kurkov\u00e1 and Sanguineti, 2002; Mhaskar, 2004)).", "startOffset": 2, "endOffset": 91}, {"referenceID": 14, "context": ", (Barron, 1993; Kurkov\u00e1 and Sanguineti, 2001; Kurkov\u00e1 and Sanguineti, 2002; Mhaskar, 2004)).", "startOffset": 2, "endOffset": 91}, {"referenceID": 22, "context": ", (Barron, 1993; Kurkov\u00e1 and Sanguineti, 2001; Kurkov\u00e1 and Sanguineti, 2002; Mhaskar, 2004)).", "startOffset": 2, "endOffset": 91}, {"referenceID": 25, "context": "7 (Pinkus, 1999)).", "startOffset": 2, "endOffset": 16}, {"referenceID": 19, "context": "From the point of view of approximation theory as applied to arbitrary smooth functions, an interesting advantage of deep networks (with more than one hidden layer) is that they provide optimal local approximation analogous to spline approximation, while shallow networks are inherently unable to do so (Mhaskar, 1993a; Mhaskar, 1993b; Chui et al., 1994; Chui et al., 1996).", "startOffset": 303, "endOffset": 373}, {"referenceID": 20, "context": "From the point of view of approximation theory as applied to arbitrary smooth functions, an interesting advantage of deep networks (with more than one hidden layer) is that they provide optimal local approximation analogous to spline approximation, while shallow networks are inherently unable to do so (Mhaskar, 1993a; Mhaskar, 1993b; Chui et al., 1994; Chui et al., 1996).", "startOffset": 303, "endOffset": 373}, {"referenceID": 5, "context": "From the point of view of approximation theory as applied to arbitrary smooth functions, an interesting advantage of deep networks (with more than one hidden layer) is that they provide optimal local approximation analogous to spline approximation, while shallow networks are inherently unable to do so (Mhaskar, 1993a; Mhaskar, 1993b; Chui et al., 1994; Chui et al., 1996).", "startOffset": 303, "endOffset": 373}, {"referenceID": 6, "context": "From the point of view of approximation theory as applied to arbitrary smooth functions, an interesting advantage of deep networks (with more than one hidden layer) is that they provide optimal local approximation analogous to spline approximation, while shallow networks are inherently unable to do so (Mhaskar, 1993a; Mhaskar, 1993b; Chui et al., 1994; Chui et al., 1996).", "startOffset": 303, "endOffset": 373}, {"referenceID": 17, "context": "For the hierarchical quadratic networks described in (Livni et al., 2013) (see section 4) there a VC-dimension bound is much lower than for the corresponding shallow network.", "startOffset": 53, "endOffset": 73}, {"referenceID": 32, "context": "Ruderman\u2019s pioneering work (Ruderman, 1997) concludes that this set of properties is equivalent to the statement that natural images con-", "startOffset": 27, "endOffset": 43}, {"referenceID": 31, "context": "The first property \u2013 compositionality \u2013 was a main motivation for hierarchical architectures such as Fukushima\u2019s and later imitations of it such as HMAX which can be seen to be similar to a pyramid of AND and OR layers (Riesenhuber and Poggio, 1999b), that is a sequence of conjunctions and disjunctions.", "startOffset": 219, "endOffset": 250}, {"referenceID": 0, "context": "The point can be formalized by starting from an (obvious) result (Anselmi et al., 2015) showing that pooling over a", "startOffset": 65, "endOffset": 87}, {"referenceID": 0, "context": "By adapting the proof there (Anselmi et al., 2015), it is easy to show (as we are doing in a longer version of this paper), that invariance and potential uniquenss of pooling do not hold when clutter \u2013 defined as image patches that may change from one presentation of the object to the next presentation \u2013 is present within the pooling regions.", "startOffset": 28, "endOffset": 50}, {"referenceID": 23, "context": ", see (Mhaskar and Micchelli, 1995), and the layers after that can continue to be regarded as effectively approximating algebraic polynomials of high degree as above.", "startOffset": 6, "endOffset": 35}, {"referenceID": 29, "context": "It is equally intriguing to speculate whether in practice (Poggio and Smale, 2003) only sparse functions may be learnable.", "startOffset": 58, "endOffset": 82}, {"referenceID": 11, "context": "Classical results (Hastad, 1987) about the depth-breadth tradeoff in circuits design show that deep circuits are more efficient in representing certain Boolean functions than shallow circuits.", "startOffset": 18, "endOffset": 32}, {"referenceID": 4, "context": "For instance Bengio and LeCun (Bengio and LeCun, 2007) write \u201cWe claim that most", "startOffset": 30, "endOffset": 54}, {"referenceID": 34, "context": "We remark that a nice theorem was recently published (Telgarsky, 2015), showing that a certain family of classification problems with real-valued inputs cannot be approximated well by shallow networks with fewer than exponentially many nodes whereas a deep network achieves zero error.", "startOffset": 53, "endOffset": 70}, {"referenceID": 12, "context": "For the experiments with 2 and 3 hidden layers, batch normalization (Ioffe and Szegedy, 2015) was used between every two hidden layers.", "startOffset": 68, "endOffset": 93}, {"referenceID": 35, "context": "Implementations were based on MatConvNet (Vedaldi and Lenc, 2015).", "startOffset": 41, "endOffset": 65}, {"referenceID": 18, "context": "In general, two algorithms (Mansour, 1994) seems to allow learning of certain Boolean function classes:", "startOffset": 27, "endOffset": 42}], "year": 2017, "abstractText": "We describe computational tasks \u2013 especially in vision \u2013 that correspond to compositional/hierarchical functions. While the universal approximation property holds both for hierarchical and shallow networks, we prove that deep (hierarchical) networks can approximate the class of compositional functions with the same accuracy as shallow networks but with exponentially lower VC-dimension as well as the number of training parameters. This leads to the question of approximation by sparse polynomials (in the number of independent parameters) and, as a consequence, by deep networks. We also discuss connections between our results and learnability of sparse Boolean functions, settling an old conjecture by Bengio. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. HNM was supported in part by ARO Grant W911NF-15-1-0385. ar X iv :1 60 3. 00 98 8v 1 [ cs .L G ] 3 M ar 2 01 6", "creator": "LaTeX with hyperref package"}}}