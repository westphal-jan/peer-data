{"id": "1603.01360", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2016", "title": "Neural Architectures for Named Entity Recognition", "abstract": "state - of - the - body art recently named domain entity recognition systems clearly rely heavily on rigorous hand - woven crafted features, and domain - specific specialized knowledge in sheer order to remotely learn effectively solely from the small, uniformly supervised training corpora groups that are available. basically in this paradigm paper, we introduce two new neural dictionary architectures - - - one based on bidirectional lstms and conditional evolutionary random fields, and the other being that radically constructs diagrams and labels segments using mainly a minimal transition - weight based approach inspired explicitly by shift - reduce template parsers. our models rely dramatically on two sources of vocabulary information encountered about words : character - relationship based word data representations far learned from completely the complete supervised corpus languages and slightly unsupervised computational word representations commonly learned from completely unannotated learning corpora. our genetic models nonetheless obtain approximately state - of - the the - art vocabulary performance in ner in four languages specified without resorting overly to any prior language - specific knowledge or electronic resources such as linguistic gazetteers.", "histories": [["v1", "Fri, 4 Mar 2016 06:36:29 GMT  (123kb,D)", "http://arxiv.org/abs/1603.01360v1", "Proceedings of NAACL 2016"], ["v2", "Wed, 6 Apr 2016 03:11:58 GMT  (124kb,D)", "http://arxiv.org/abs/1603.01360v2", "Proceedings of NAACL 2016"], ["v3", "Thu, 7 Apr 2016 15:09:36 GMT  (124kb,D)", "http://arxiv.org/abs/1603.01360v3", "Proceedings of NAACL 2016"]], "COMMENTS": "Proceedings of NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guillaume lample", "miguel ballesteros", "sandeep subramanian", "kazuya kawakami", "chris dyer"], "accepted": true, "id": "1603.01360"}, "pdf": {"name": "1603.01360.pdf", "metadata": {"source": "CRF", "title": "Neural Architectures for Named Entity Recognition", "authors": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer"], "emails": ["glample@cs.cmu.edu,", "sandeeps@cs.cmu.edu,", "kkawakam@cs.cmu.edu,", "cdyer@cs.cmu.edu,", "miguel.ballesteros@upf.edu"], "sections": [{"heading": null, "text": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures\u2014one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers. 1"}, {"heading": "1 Introduction", "text": "Named entity recognition (NER) is a challenging learning problem. One the one hand, in most languages and domains, there is only a very small amount of supervised training data available. On the other, there are few constraints on the kinds of words that can be names, so generalizing from this small sample of data is difficult. As a result, carefully constructed orthographic features and language-specific knowledge resources, such as gazetteers, are widely used for solving this task. Unfortunately, languagespecific resources and features are costly to develop in new languages and new domains, making NER a challenge to adapt. Unsupervised learning from unannotated corpora offers an alternative strategy for obtaining better generalization from small amounts of supervision. However, even systems\n1Code is available at https://github.com/glample/tagger\nthat have relied extensively on unsupervised features (Collobert et al., 2011; Turian et al., 2010; Lin and Wu, 2009; Ando and Zhang, 2005b, inter alia) have used these to augment, rather than replace, hand-engineered features (e.g., knowledge about capitalization patterns and character classes in a particular language) and specialized knowledge resources (e.g., gazetteers).\nIn this paper, we present neural architectures for NER that use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora. Our models are designed to capture two intuitions. First, since names often consist of multiple tokens, reasoning jointly over tagging decisions for each token is important. We compare two models here, (i) a bidirectional LSTM with a sequential conditional random layer above it (LSTM-CRF; \u00a72), and (ii) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition-based parsing with states represented by stack LSTMs (S-LSTM; \u00a73). Second, token-level evidence for \u201cbeing a name\u201d includes both orthographic evidence (what does the word being tagged as a name look like?) and distributional evidence (where does the word being tagged tend to occur in a corpus?). To capture orthographic sensitivity, we use character-based word representation model (Ling et al., 2015b) to capture distributional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b). Our word representations combine both of these, and dropout training is used to encourage the model to learn to trust both sources of evidence (\u00a74).\nExperiments in English, Dutch, German, and Spanish show that we are able to obtain stateof-the-art NER performance with the LSTM-CRF model in Dutch, German, and Spanish, and very near the state-of-the-art in English without any\nar X\niv :1\n60 3.\n01 36\n0v 1\n[ cs\n.C L\n] 4\nM ar\n2 01\n6\nhand-engineered features or gazetteers (\u00a75). The transition-based algorithm likewise surpasses the best previously published results in several languages, although it performs less well than the LSTM-CRF model."}, {"heading": "2 LSTM-CRF Model", "text": "We provide a brief description of LSTMs and CRFs, and present a hybrid tagging architecture."}, {"heading": "2.1 LSTM", "text": "Recurrent neural networks (RNNs) are a family of neural networks that operate on sequential data. They take as input a sequence of vectors (x1,x2, . . . ,xn) and return another sequence (h1,h2, . . . ,hn) that represents some information about the sequence at every step in the input. Although RNNs can, in theory, learn long dependencies, in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence (Bengio et al., 1994). Long Short-term Memory Networks (LSTMs) have been designed to combat this issue by incorporating a memory-cell and have been shown to capture long-range dependencies. They do so using several gates that control the proportion of the input to give to the memory cell, and the proportion from the previous state to forget (Hochreiter and Schmidhuber, 1997). We use the following implementation:\nit = \u03c3(Wxixt +Whiht\u22121 +Wcict\u22121 + bi)\nct = (1\u2212 it) ct\u22121+ it tanh(Wxcxt +Whcht\u22121 + bc)\not = \u03c3(Wxoxt +Whoht\u22121 +Wcoct + bo)\nht = ot tanh(ct),\nwhere \u03c3 is the element-wise sigmoid function, and is the element-wise product.\nFor a given sentence (x1,x2, . . . ,xn) containing nwords, each represented as a d-dimensional vector, an LSTM computes a representation \u2212\u2192 ht of the left context of the sentence at every word t. Naturally, generating a representation of the right context \u2190\u2212 ht as well should add useful information. This can be achieved using a second LSTM that reads the same sequence in reverse. We will refer to the former as\nthe forward LSTM and the latter as the backward LSTM. These are two distinct networks with different parameters. This forward and backward LSTM pair is referred to as a bidirectional LSTM (Graves and Schmidhuber, 2005).\nThe representation of a word using this model is obtained by concatenating its left and right context representations, ht = [ \u2212\u2192 ht; \u2190\u2212 ht]. These representations effectively include a representation of a word in context, which is useful for numerous tagging applications."}, {"heading": "2.2 CRF Tagging Models", "text": "A very simple\u2014but surprisingly effective\u2014tagging model is to use the ht\u2019s as features to make independent tagging decisions for each output yt (Ling et al., 2015b). Despite this model\u2019s success in simple problems like POS tagging, its independent classification decisions are limiting when there are strong dependencies across output labels. NER is one such task, since the \u201cgrammar\u201d that characterizes interpretable sequences of tags imposes several hard constraints (e.g., I-PER cannot follow B-LOC; see \u00a72.4 for details) that would be impossible to model with independence assumptions.\nTherefore, instead of modeling tagging decisions independently, we model them jointly using a conditional random field (Lafferty et al., 2001). For an input sentence\nX = (x1,x2, . . . ,xn),\nwe consider P to be the matrix of scores output by the bidirectional LSTM network. P is of size n \u00d7 k, where k is the number of distinct tags, and Pi,j corresponds to the score of the jth tag of the ith word in a sentence. For a sequence of predictions\ny = (y1, y2, . . . , yn),\nwe define its score to be\ns(X,y) = n\u2211\ni=0\nAyi,yi+1 + n\u2211\ni=1\nPi,yi\nwhere A is a matrix of transition scores such that Ai,j represents the score of a transition from the tag i to tag j. y0 and yn are the start and end tags of a sentence, that we add to the set of possible tags. A is therefore a square matrix of size k+2.\nA softmax over all possible tag sequences yields a probability for the sequence y:\np(y|X) = e s(X,y)\u2211\ny\u0303\u2208YX e s(X,y\u0303)\n.\nDuring training, we maximize the log-probability of the correct tag sequence:\nlog(p(y|X)) = s(X,y)\u2212 log  \u2211 y\u0303\u2208YX es(X,y\u0303)  = s(X,y)\u2212 logadd\ny\u0303\u2208YX s(X, y\u0303), (1)\nwhere YX represents all possible tag sequences (even those that do not verify the IOB format) for a sentence X. From the formulation above, it is evident that we encourage our network to produce a valid sequence of output labels. While decoding, we predict the output sequence that obtains the maximum score given by:\ny\u2217 = argmax y\u0303\u2208YX s(X, y\u0303). (2)\nSince we are only modeling bigram interactions between outputs, both the summation in Eq. 1 and the maximum a posteriori sequence y\u2217 in Eq. 2 can be computed using dynamic programming."}, {"heading": "2.3 Parameterization and Training", "text": "The scores associated with each tagging decision for each token (i.e., the Pi,y\u2019s) are defined to be the dot product between the embedding of a wordin-context computed with a bidirectional LSTM\u2014 exactly the same as the POS tagging model of Ling et al. (2015b) and these are combined with bigram compatibility scores (i.e., the Ay,y\u2032\u2019s). This architecture is shown in figure 1. Circles represent observed variables, diamonds are deterministic functions of their parents, and double circles are random variables.\nThe parameters of this model are thus the matrix of bigram compatibility scores A, and the parameters that give rise to the matrix P, namely the parameters of the bidirectional LSTM, the linear feature weights, and the word embeddings. As in part 2.2,\nlet xi denote the sequence of word embeddings for every word in a sentence, and yi be their associated tags. We return to a discussion of how the embeddings xi are modeled in Section 4. The sequence of word embeddings is given as input to a bidirectional LSTM, which returns a representation of the left and right context for each word as explained in 2.1.\nThese representations are concatenated (ci) and linearly projected onto a layer whose size is equal to the number of distinct tags. Instead of using the softmax output from this layer, we use a CRF as previously described to take into account neighboring tags, yielding the final predictions for every word yi. Additionally, we observed that adding a hidden layer between ci and the CRF layer marginally improved our results. All results reported with this model incorporate this extra-layer. The parameters are trained to maximize Eq. 1 of observed sequences of NER tags in an annotated corpus, given the observed words."}, {"heading": "2.4 IOBES Tagging Scheme", "text": "The task of named entity recognition is to assign a named entity label to every word in a sentence. A single named entity could span several tokens within a sentence. Sentences are usually represented in the IOB format (Inside, Outside, Beginning) where every token is labeled as B-label if the token is the\nbeginning of a named entity, I-label if it is inside a named entity but not the first token within the named entity, or O otherwise. However, we decided to use the IOBES tagging scheme, a variant of IOB, which encodes information about singleton entities (S) and explicitly marks the end of named entities (E). Using this scheme, tagging a word as I-label with high-confidence narrows down the choices for the subsequent word to I-label or E-label, however, the IOB scheme is only capable of determining that the subsequent word cannot be the interior of another label. Ratinov and Roth (2009) and Dai et al. (2015) showed that using a more expressive tagging scheme like IOBES improves model performance marginally. We observed a similar improvement to theirs using the IOBES notation."}, {"heading": "3 Transition-Based Chunking Model", "text": "As an alternative to the LSTM-CRF discussed in the previous section, we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition-based dependency parsing. This model directly constructs representations of the multi-token names (e.g., the name Mark Watney is composed into a single representation).\nThis model relies on a stack data structure to incrementally construct chunks of the input. To obtain representations of this stack used for predicting subsequent actions, we use the Stack-LSTM presented by Dyer et al. (2015), in which the LSTM is augmented with a \u201cstack pointer.\u201d While sequential LSTMs model sequences from left to right, stack LSTMs permit embedding of a stack of objects that are both added to (using a push operation) and removed from (using a pop operation). This allows the Stack-LSTM to work like a stack that maintains a \u201csummary embedding\u201d of its contents. We refer to this model as Stack-LSTM or S-LSTM model for simplicity."}, {"heading": "3.1 Chunking Algorithm", "text": "We designed a transition inventory which is given in Figure 2 that is inspired by transition-based parsers, in particular the arc-standard parser of Nivre (2004). In this algorithm, we make use of two stacks (designated output and stack representing, respectively, completed chunks and scratch space) and a buffer\nthat contains the words that have yet to be processed. The transition inventory contains the following transitions: The SHIFT transition moves a word from the buffer to the stack, the OUT transition moves a word from the buffer directly into the output stack while the REDUCE(y) transition pops all items from the top of the stack creating a \u201cchunk,\u201d labels this with label y, and pushes a representation of this chunk onto the output stack. The algorithm completes when the stack and buffer are both empty. The algorithm is depicted in Figure 2, which shows the sequence of operations required to process the sentence Mark Watney visited Mars.\nThe model is parameterized by defining a probability distribution over actions at each time step, given the current contents of the stack, buffer, and output, as well as the history of actions taken. Following Dyer et al. (2015), we use stack LSTMs to compute a fixed dimensional embedding of each of these, and take a concatenation of these to obtain the full algorithm state. This representation is used to define a distribution over the possible actions that can be taken at each time step. The model is trained to maximize the conditional probability of sequences of reference actions (extracted from a labeled training corpus) given the input sentences. To label a new input sequence at test time, the maximum probability action is chosen greedily until the algorithm reaches a termination state. Although this is not guaranteed to find a global optimum, it is effective in practice. Since each token is either moved directly to the output (1 action) or first to the stack and then the output (2 actions), the total number of actions for a sequence of length n is maximally 2n."}, {"heading": "3.2 Representing Labeled Chunks", "text": "When the REDUCE(y) operation is executed, the algorithm shifts a sequence of tokens (together with their vector embeddings) from the stack to the output buffer as a single completed chunk. To compute an embedding of this sequence, we run a bidirectional LSTM over the embeddings of its constituent tokens together with a token representing the type of the chunk being identified (i.e., y). This function is given as g(u, . . . ,v, ry), where ry is a learned embedding of a label type. Thus, the output buffer contains a single vector representation for each labeled chunk that is generated, regardless of its length."}, {"heading": "4 Input Word Embeddings", "text": "The input layers to both of our models are vector representations of individual words. Learning independent representations for word types from the limited NER training data is a difficult problem: there are simply too many parameters to reliably estimate. Since many languages have orthographic or morphological evidence that something is a name (or not a name), we want representations that are sensitive to the spelling of words. We therefore use a model that constructs representations of words from representations of the characters they are composed of (4.1). Our second intuition is that names, which may individually be quite varied, appear in regular contexts in large corpora. Therefore we use embeddings learned from a large corpus that are sensitive to word order (4.2). Finally, to prevent the models from depending on one representation or the other too strongly, we use dropout training and find this is crucial for good generalization performance (4.3)."}, {"heading": "4.1 Character-based models of words", "text": "An important distinction of our work from most previous approaches is that we learn character-level features while training instead of hand-engineering prefix and suffix information about words. Learning character-level embeddings has the advantage of learning representations specific to the task and domain at hand. They have been found useful for morphologically rich languages and to handle the outof-vocabulary problem for tasks like part-of-speech\ntagging and language modeling (Ling et al., 2015b) or dependency parsing (Ballesteros et al., 2015).\nFigure 4 describes our architecture to generate a word embedding for a word from its characters. A character lookup table initialized at random contains an embedding for every character. The character embeddings corresponding to every character in a word are given in direct and reverse order to a forward and a backward LSTM. The embedding for a\nword derived from its characters is the concatenation of its forward and backward representations from the bidirectional LSTM. This character-level representation is then concatenated with a word-level representation from a word lookup-table. During testing, words that do not have an embedding in the lookup table are mapped to a UNK embedding. To train the UNK embedding, we replace singletons with the UNK embedding with a probability 0.5. In all our experiments, the hidden dimension of the forward and backward character LSTMs are 25 each, which results in our character-based representation of words being of dimension 50.\nRecurrent models like RNNs and LSTMs are capable of encoding very long sequences, however, they have a representation biased towards their most recent inputs. As a result, we expect the final representation of the forward LSTM to be an accurate representation of the suffix of the word, and the final state of the backward LSTM to be a better representation of its prefix. Alternative approaches\u2014 most notably like convolutional networks\u2014have been proposed to learn representations of words from their characters (Zhang et al., 2015; Kim et al., 2015). However, convnets are designed to discover position-invariant features of their inputs. While this is appropriate for many problems, e.g., image recognition (a cat can appear anywhere in a picture), we argue that important information is position dependent (e.g., prefixes and suffixes encode different information than stems), making LSTMs an a priori better function class for modeling the relationship between words and their characters."}, {"heading": "4.2 Pretrained embeddings", "text": "As in Collobert et al. (2011), we use pretrained word embeddings to initialize our lookup table. We observe significant improvements using pretrained word embeddings over randomly initialized ones. Embeddings are pretrained using skip-n-gram (Ling et al., 2015a), a variation of word2vec (Mikolov et al., 2013a) that accounts for word order. These embeddings are fine-tuned during training.\nWord embeddings for Spanish, Dutch, German and English are trained using the Spanish Gigaword version 3, the Leipzig corpora collection, the German monolingual training data from the 2010 Machine Translation Workshop and the English Giga-\nword version 4 (with the LA Times and NY Times portions removed) respectively.2 We use an embedding dimension of 100 for English, 64 for other languages, a minimum word frequency cutoff of 4, and a window size of 8."}, {"heading": "4.3 Dropout training", "text": "Initial experiments showed that character-level embeddings did not improve our overall performance when used in conjunction with pretrained word representations. To encourage the model to depend on both representations, we use dropout training (Hinton et al., 2012), applying a dropout mask to the final embedding layer just before the input to the bidirectional LSTM in Figure 1. We observe a significant improvement in our model\u2019s performance after using dropout (see table 5)."}, {"heading": "5 Experiments", "text": "This section presents the methods we use to train our models, the results we obtained on various tasks and the impact of our networks\u2019 configuration on model performance."}, {"heading": "5.1 Training", "text": "For both models presented, we train our networks using the back-propagation algorithm updating our parameters on every training example, one at a time, using stochastic gradient descent (SGD) with a learning rate of 0.01 and a gradient clipping of 5.0. Several methods have been proposed to enhance the performance of SGD, such as Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014). Although we observe faster convergence using these methods, none of them perform as well as SGD with gradient clipping.\nOur LSTM-CRF model uses a single layer for the forward and backward LSTMs whose dimensions are set to 100. Tuning this dimension did not significantly impact model performance. We set the dropout rate to 0.5. Using higher rates negatively impacted our results, while smaller rates led to longer training time.\nThe stack-LSTM model uses two layers each of dimension 100 for each stack. The embeddings of\n2(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)\nthe actions used in the composition functions have 16 dimensions each, and the output embedding is of dimension 20. We experimented with different dropout rates and reported the scores using the best dropout rate for each language.3"}, {"heading": "5.2 Data Sets", "text": "We test our model on different datasets for named entity recognition. To demonstrate our model\u2019s ability to generalize to different languages, we present results on the CoNLL-2002 and CoNLL2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) that contain independent named entity labels for English, Spanish, German and Dutch. All datasets contain four different types of named entities: locations, persons, organizations, and miscellaneous entities that do not belong in any of the three previous categories. Although POS tags were made available for all datasets, we did not include them in our models. We did not perform any dataset preprocessing, apart from replacing every digit with a zero in the English NER dataset."}, {"heading": "5.3 Results", "text": "Table 1 presents our comparisons with other models for named entity recognition in English. To make the comparison between our model and others fair, we report the scores of other models with and without the use of external labeled data such as gazetteers and knowledge bases. Our models do not use gazetteers or any external labeled resources. The best score reported on this task is by Luo et al. (2015). They obtained a F1 of 91.2 by jointly modeling the NER and entity linking tasks (Hoffart et al., 2011). Their model uses a lot of hand-engineered features including spelling features, WordNet clusters, Brown clusters, POS tags, chunks tags, as well as stemming and external knowledge bases like Freebase and Wikipedia. Our LSTM-CRF model outperforms all other systems, including the ones using external labeled data like gazetteers. Our StackLSTM model also outperforms all previous models that do not incorporate external features, apart from the one presented by Chiu and Nichols (2015).\nTables 2, 3 and 4 present our results on NER for\n3English (D=0.2), German, Spanish and Dutch (D=0.3)\nGerman, Dutch and Spanish respectively in comparison to other models. On these three languages, the LSTM-CRF model significantly outperforms all previous methods, including the ones using external labeled data. The only exception is Dutch, where the model of Gillick et al. (2015) can perform better by leveraging the information from other NER datasets. The Stack-LSTM also consistently presents statethe-art (or close to) results compared to systems that do not use external data."}, {"heading": "5.4 Network architectures", "text": "Our models had several components that we could tweak to understand their impact on the overall per-\nformance. We explored the impact that the CRF, the character-level representations, pretraining of our word embeddings and dropout had on our LSTMCRF model. We observed that pretraining our word embeddings gave us the biggest improvement in overall performance of +7.31 in F1. The CRF layer gave us an increase of +1.79, while using dropout resulted in a difference of +1.17 and finally learning character-level word embeddings resulted in an increase of about +0.74. For the Stack-LSTM we performed a similar set of experiments. Results with different architectures are given in table 5."}, {"heading": "6 Related Work", "text": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus.\nSeveral other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers.\nLanguage independent NER models like ours have also been proposed in the past. Cucerzan and Yarowsky (1999; 2002) present semi-supervised bootstrapping algorithms for named entity recognition by co-training character-level (word-internal) and token-level (context) features. Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting. Ratinov and Roth (2009) quantitatively compare several approaches for NER and build their own supervised model using a regularized average perceptron and aggregating context information.\nFinally, there is currently a lot of interest in models for NER that use letter-based representations. Gillick et al. (2015) model the task of sequencelabeling as a sequence to sequence learning problem and incorporate character-based representations into their encoder model. Chiu and Nichols (2015) employ an architecture similar to ours, but instead use CNNs to learn character-level features, in a way similar to the work by Santos and Guimara\u0303es (2015)."}, {"heading": "7 Conclusion", "text": "This paper presents two neural architectures for sequence labeling that provide the best NER results ever reported in standard evaluation settings, even compared with models that use external resources, such as gazetteers.\nA key aspect of our models are that they model output label dependencies, either via a simple CRF architecture, or using a transition-based algorithm to explicitly construct and label chunks of the in-\nput. Word representations are also crucially important for success: we use both pre-trained word representations and \u201ccharacter-based\u201d representations that capture morphological and orthographic information. To prevent the learner from depending too heavily on one representation class, dropout is used."}, {"heading": "Acknowledgments", "text": "Miguel Ballesteros is supported by the European Commission under the contract numbers FP7-ICT610411 (project MULTISENSOR) and H2020-RIA645012 (project KRISTINA)."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005a] Rie Kubota Ando", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Learning predictive structures", "author": ["Ando", "Zhang2005b] Rie Kubota Ando", "Tong Zhang"], "venue": null, "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Improved transition-based dependency parsing by modeling characters instead of words with LSTMs", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "Proceedings of EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "The leipzig corpora collection-monolingual corpora of standard size", "author": ["Gerhard Heyer", "Uwe Quasthoff", "Matthias Richter"], "venue": "Proceedings of Corpus Linguistic", "citeRegEx": "Biemann et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Biemann et al\\.", "year": 2007}, {"title": "Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation", "author": ["Philipp Koehn", "Christof Monz", "Kay Peterson", "Mark Przybocki", "Omar F Zaidan"], "venue": null, "citeRegEx": "Callison.Burch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Callison.Burch et al\\.", "year": 2010}, {"title": "Named entity extraction using adaboost, proceedings of the 6th conference on natural language learning", "author": ["Llu\u0131\u0301s M\u00e0rquez", "Llu\u0131\u0301s Padr\u00f3"], "venue": null, "citeRegEx": "Carreras et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2002}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2015] Jason PC Chiu", "Eric Nichols"], "venue": "arXiv preprint arXiv:1511.08308", "citeRegEx": "Chiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Kuksa.,? \\Q2011\\E", "shortCiteRegEx": "Kuksa.", "year": 2011}, {"title": "Language independent named entity recognition combining morphological and contextual evidence", "author": ["Cucerzan", "Yarowsky1999] Silviu Cucerzan", "David Yarowsky"], "venue": "In Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC,", "citeRegEx": "Cucerzan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cucerzan et al\\.", "year": 1999}, {"title": "Language independent ner using a unified model of internal and contextual evidence", "author": ["Cucerzan", "Yarowsky2002] Silviu Cucerzan", "David Yarowsky"], "venue": "In proceedings of the 6th conference on Natural language learning-Volume", "citeRegEx": "Cucerzan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Cucerzan et al\\.", "year": 2002}, {"title": "Enhancing of chemical compound and drug name recognition using representative tag scheme and fine-grained tokenization", "author": ["Dai et al.2015] Hong-Jie Dai", "Po-Ting Lai", "Yung-Chun Chang", "Richard Tzong-Han Tsai"], "venue": "Journal of cheminformatics,", "citeRegEx": "Dai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dai et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Structured databases of named entities from bayesian nonparametrics", "author": ["Tae Yano", "William W Cohen", "Noah A Smith", "Eric P Xing"], "venue": "In Proceedings of the First Workshop on Unsupervised Learning in NLP,", "citeRegEx": "Eisenstein et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2011}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In Proceedings of the seventh conference on Natural language learning at HLT-NAACL", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": "arXiv preprint arXiv:1512.00103", "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Spanish gigaword third edition (ldc2011t12)", "author": ["David Graff"], "venue": "Linguistic Data Consortium,", "citeRegEx": "Graff.,? \\Q2011\\E", "shortCiteRegEx": "Graff.", "year": 2011}, {"title": "Framewise phoneme classification with bidirectional LSTM networks", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "In Proc. IJCNN", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Robust disambiguation of named entities in text", "author": ["Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum"], "venue": null, "citeRegEx": "Hoffart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Character-aware neural language models. CoRR, abs/1508.06615", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando CN Pereira"], "venue": "In Proc. ICML", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Phrase clustering for discriminative learning", "author": ["Lin", "Wu2009] Dekang Lin", "Xiaoyun Wu"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing", "citeRegEx": "Lin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "2015a. Not all contexts are created equal: Better word representations with variable attention", "author": ["Ling et al.2015a] Wang Ling", "Lin Chu-Cheng", "Yulia Tsvetkov", "Silvio Amir", "R\u00e1mon Fernandez Astudillo", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015b] Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Joint named entity recognition and disambiguation", "author": ["Luo et al.2015] Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie"], "venue": "In Proc. EMNLP", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Incrementality in deterministic dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together", "citeRegEx": "Nivre.,? \\Q2004\\E", "shortCiteRegEx": "Nivre.", "year": 2004}, {"title": "Learning multilingual named entity recognition from wikipedia", "author": ["Nothman et al.2013] Joel Nothman", "Nicky Ringland", "Will Radford", "Tara Murphy", "James R Curran"], "venue": "Artificial Intelligence,", "citeRegEx": "Nothman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Nothman et al\\.", "year": 2013}, {"title": "English gigaword fourth edition (ldc2009t13)", "author": ["Parker et al.2009] Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda"], "venue": "Linguistic Data Consortium,", "citeRegEx": "Parker et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Parker et al\\.", "year": 2009}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["Vineet Kumar", "Andrew McCallum"], "venue": "arXiv preprint arXiv:1404.5367", "citeRegEx": "Passos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Passos et al\\.", "year": 2014}, {"title": "Combining labeled and unlabeled data with word-class distribution learning", "author": ["Qi et al.2009] Yanjun Qi", "Ronan Collobert", "Pavel Kuksa", "Koray Kavukcuoglu", "Jason Weston"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge manage-", "citeRegEx": "Qi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2009}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Santos", "Victor Guimar\u00e3es"], "venue": "arXiv preprint arXiv:1505.05008", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proc. CoNLL", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Introduction to the conll-2002 shared task: Languageindependent named entity recognition", "author": [], "venue": "In Proc. CoNLL", "citeRegEx": "Sang.,? \\Q2002\\E", "shortCiteRegEx": "Sang.", "year": 2002}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proc. ACL", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Adadelta: An adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Character-level convolutional networks for text classification", "author": ["Zhang et al.2015] Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "dencies, in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence (Bengio et al., 1994).", "startOffset": 111, "endOffset": 132}, {"referenceID": 24, "context": "Therefore, instead of modeling tagging decisions independently, we model them jointly using a conditional random field (Lafferty et al., 2001).", "startOffset": 119, "endOffset": 142}, {"referenceID": 26, "context": ", the Pi,y\u2019s) are defined to be the dot product between the embedding of a wordin-context computed with a bidirectional LSTM\u2014 exactly the same as the POS tagging model of Ling et al. (2015b) and these are combined with bigram compatibility scores (i.", "startOffset": 171, "endOffset": 191}, {"referenceID": 11, "context": "Ratinov and Roth (2009) and Dai et al. (2015) showed that using a more expressive tagging scheme like IOBES improves model performance", "startOffset": 28, "endOffset": 46}, {"referenceID": 12, "context": "To obtain representations of this stack used for predicting subsequent actions, we use the Stack-LSTM presented by Dyer et al. (2015), in which the LSTM is augmented with a \u201cstack pointer.", "startOffset": 115, "endOffset": 134}, {"referenceID": 30, "context": "We designed a transition inventory which is given in Figure 2 that is inspired by transition-based parsers, in particular the arc-standard parser of Nivre (2004). In this algorithm, we make use of two stacks (designated output and stack representing, respectively, completed chunks and scratch space) and a buffer that contains the words that have yet to be processed.", "startOffset": 149, "endOffset": 162}, {"referenceID": 12, "context": "Following Dyer et al. (2015), we use stack LSTMs", "startOffset": 10, "endOffset": 29}, {"referenceID": 2, "context": ", 2015b) or dependency parsing (Ballesteros et al., 2015).", "startOffset": 31, "endOffset": 57}, {"referenceID": 18, "context": "To encourage the model to depend on both representations, we use dropout training (Hinton et al., 2012), applying a dropout mask to the final embedding layer just before the input to the bidirec-", "startOffset": 82, "endOffset": 103}, {"referenceID": 40, "context": "Several methods have been proposed to enhance the performance of SGD, such as Adadelta (Zeiler, 2012) or Adam (Kingma and Ba, 2014).", "startOffset": 87, "endOffset": 101}, {"referenceID": 16, "context": "(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)", "startOffset": 0, "endOffset": 85}, {"referenceID": 4, "context": "(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)", "startOffset": 0, "endOffset": 85}, {"referenceID": 5, "context": "(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)", "startOffset": 0, "endOffset": 85}, {"referenceID": 32, "context": "(Graff, 2011; Biemann et al., 2007; Callison-Burch et al., 2010; Parker et al., 2009)", "startOffset": 0, "endOffset": 85}, {"referenceID": 20, "context": "2 by jointly modeling the NER and entity linking tasks (Hoffart et al., 2011).", "startOffset": 55, "endOffset": 77}, {"referenceID": 27, "context": "The best score reported on this task is by Luo et al. (2015). They obtained a F1 of 91.", "startOffset": 43, "endOffset": 61}, {"referenceID": 20, "context": "2 by jointly modeling the NER and entity linking tasks (Hoffart et al., 2011). Their model uses a lot of hand-engineered features including spelling features, WordNet clusters, Brown clusters, POS tags, chunks tags, as well as stemming and external knowledge bases like Freebase and Wikipedia. Our LSTM-CRF model outperforms all other systems, including the ones using external labeled data like gazetteers. Our StackLSTM model also outperforms all previous models that do not incorporate external features, apart from the one presented by Chiu and Nichols (2015).", "startOffset": 56, "endOffset": 564}, {"referenceID": 15, "context": "The only exception is Dutch, where the model of Gillick et al. (2015) can perform better by leveraging the information from other NER datasets.", "startOffset": 48, "endOffset": 70}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.", "startOffset": 3, "endOffset": 23}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.", "startOffset": 3, "endOffset": 51}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.05 Passos et al. (2014)* 90.", "startOffset": 3, "endOffset": 78}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.05 Passos et al. (2014)* 90.90 Luo et al. (2015)* + gaz 89.", "startOffset": 3, "endOffset": 103}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.05 Passos et al. (2014)* 90.90 Luo et al. (2015)* + gaz 89.9 Luo et al. (2015)* + gaz + linking 91.", "startOffset": 3, "endOffset": 133}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.05 Passos et al. (2014)* 90.90 Luo et al. (2015)* + gaz 89.9 Luo et al. (2015)* + gaz + linking 91.2 Chiu and Nichols (2015) 90.", "startOffset": 3, "endOffset": 179}, {"referenceID": 21, "context": "90 Huang et al. (2015)* 90.10 Passos et al. (2014) 90.05 Passos et al. (2014)* 90.90 Luo et al. (2015)* + gaz 89.9 Luo et al. (2015)* + gaz + linking 91.2 Chiu and Nichols (2015) 90.69 Chiu and Nichols (2015)* 90.", "startOffset": 3, "endOffset": 209}, {"referenceID": 14, "context": "Model F1 Florian et al. (2003)* 72.", "startOffset": 9, "endOffset": 31}, {"referenceID": 14, "context": "Model F1 Florian et al. (2003)* 72.41 Ando and Zhang (2005a) 75.", "startOffset": 9, "endOffset": 61}, {"referenceID": 14, "context": "Model F1 Florian et al. (2003)* 72.41 Ando and Zhang (2005a) 75.27 Qi et al. (2009) 75.", "startOffset": 9, "endOffset": 84}, {"referenceID": 14, "context": "Model F1 Florian et al. (2003)* 72.41 Ando and Zhang (2005a) 75.27 Qi et al. (2009) 75.72 Gillick et al. (2015) 72.", "startOffset": 9, "endOffset": 112}, {"referenceID": 14, "context": "Model F1 Florian et al. (2003)* 72.41 Ando and Zhang (2005a) 75.27 Qi et al. (2009) 75.72 Gillick et al. (2015) 72.08 Gillick et al. (2015)* 76.", "startOffset": 9, "endOffset": 140}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002) 77.", "startOffset": 9, "endOffset": 32}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002) 77.05 Nothman et al. (2013) 78.", "startOffset": 9, "endOffset": 60}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002) 77.05 Nothman et al. (2013) 78.6 Gillick et al. (2015) 78.", "startOffset": 9, "endOffset": 87}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002) 77.05 Nothman et al. (2013) 78.6 Gillick et al. (2015) 78.08 Gillick et al. (2015)* 82.", "startOffset": 9, "endOffset": 115}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002)* 81.", "startOffset": 9, "endOffset": 32}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002)* 81.39 Santos and Guimar\u00e3es (2015) 82.", "startOffset": 9, "endOffset": 67}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002)* 81.39 Santos and Guimar\u00e3es (2015) 82.21 Gillick et al. (2015) 81.", "startOffset": 9, "endOffset": 95}, {"referenceID": 6, "context": "Model F1 Carreras et al. (2002)* 81.39 Santos and Guimar\u00e3es (2015) 82.21 Gillick et al. (2015) 81.83 Gillick et al. (2015)* 82.", "startOffset": 9, "endOffset": 123}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees.", "startOffset": 31, "endOffset": 54}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers.", "startOffset": 31, "endOffset": 230}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus.", "startOffset": 31, "endOffset": 334}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top.", "startOffset": 31, "endOffset": 553}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features.", "startOffset": 31, "endOffset": 793}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features.", "startOffset": 31, "endOffset": 896}, {"referenceID": 6, "context": "In the CoNLL-2002 shared task, Carreras et al. (2002) obtained among the best results on both Dutch and Spanish by combining several small fixed-depth decision trees. Next year, in the CoNLL2003 Shared Task, Florian et al. (2003) obtained the best score on German by combining the output of four diverse classifiers. Qi et al. (2009) later improved on this with a neural network by doing unsupervised learning on a massive unlabeled corpus. Several other neural architectures have previously been proposed for NER. For instance, Collobert et al. (2011) uses a CNN over a sequence of word embeddings with a CRF layer on top. This can be thought of as our first model without character-level embeddings and with the bidirectional LSTM being replaced by a CNN. More recently, Huang et al. (2015) presented a model similar to our LSTM-CRF, but using hand-crafted spelling features. Lin and Wu (2009) used a linear chain CRF with L2 regularization, they added phrase cluster features extracted from the web data and spelling features. Passos et al. (2014) also used a linear chain CRF with spelling features and gazetteers.", "startOffset": 31, "endOffset": 1051}, {"referenceID": 13, "context": "Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting.", "startOffset": 0, "endOffset": 25}, {"referenceID": 13, "context": "Eisenstein et al. (2011) use Bayesian nonparametrics to construct a database of named entities in an almost unsupervised setting. Ratinov and Roth (2009) quantitatively compare several approaches for NER and", "startOffset": 0, "endOffset": 154}], "year": 2016, "abstractText": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures\u2014one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers. 1", "creator": "LaTeX with hyperref package"}}}