{"id": "1603.02250", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2016", "title": "Online Sparse Linear Regression", "abstract": "we consider evaluating the general online sparse structured linear regression hardness problem, which often is the sorting problem composed of sequentially avoided making predictions previously observing only gives a limited number reduction of crucial features in prediction each rule round, to minimize regret with respect to the best sparse linear static regressor, determining where exact prediction range accuracy is measured further by square memory loss. we give problem an inefficient algorithm that smoothly obtains regret then bounded longer by $ \\ tilde { o } ( \\ sqrt { t } ) $ \u2020 after $ to t $ prediction block rounds. we complement up this result by favorably showing that indeed no convex algorithm still running in even polynomial length time per zero iteration time can sufficiently achieve regret reductions bounded by $ o ( t ^ {! 1 - \\ } delta } ) $ for avoiding any constant $ \\ smooth delta & gt ; 0 $ unless $ \\ full text { np } \\ subseteq \\ text { bpp } $. this computational programming hardness task result still resolves an open procedure problem conjecture presented primarily in colt press 2014 ( kale, holt 2014 ) and surprisingly also posed by lal zolghadr rahman et en al. ( 2013 ). this model hardness result holds even promise if the algorithm optimization is already allowed to access relatively more features efficient than predicting the best sparse linear progressive regressor up to a finite logarithmic recall factor in the real dimension.", "histories": [["v1", "Mon, 7 Mar 2016 20:49:52 GMT  (12kb)", "http://arxiv.org/abs/1603.02250v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dean foster", "satyen kale", "howard karloff"], "accepted": false, "id": "1603.02250"}, "pdf": {"name": "1603.02250.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Satyen Kale"], "emails": ["dean@foster.net", "satyen@yahoo-inc.com", "howard@cc.gatech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n02 25\n0v 1\n[ cs\n.L G\n] 7\nM ar\n\u221a T ) after T prediction rounds. We complement this result by\nshowing that no algorithm running in polynomial time per iteration can achieve regret bounded by O(T 1\u2212\u03b4) for any constant \u03b4 > 0 unless NP \u2286 BPP. This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension."}, {"heading": "1 Introduction", "text": "In various real-world scenarios, features for examples are constructed by running some computationally expensive algorithms. With resource constraints, it is essential to be able to make predictions with only a limited number of features computed per example. One example of this scenario, from (Cesa-Bianchi et al., 2011), is medical diagnosis of a disease, in which each feature corresponds to a medical test that the patient in question can undergo. Evidently, it is undesirable to subject a patient to a battery of medical tests, for medical as well as cost reasons. Another example from the same paper is a search engine, where a ranking of web pages must be generated for each incoming user query and the limited amount of time allowed to answer a query imposes restrictions on the number of attributes that can be evaluated in the process. In both of these problems, predictions need to be made sequentially as patients or search queries arrive online, learning a good model in the process.\nIn this paper, we model the problem of prediction with limited access to features in the most natural and basic manner as an online sparse linear regression problem. In this problem, an online learner makes real-valued predictions for the labels of examples arriving sequentially over a number of rounds. Each example has d features that can be potentially accessed by the learner. However, in each round, the learner is restricted to choosing an\narbitrary subset of features of size at most k, a budget parameter. The learner then acquires the values of the subset of features, and then makes its prediction, at which point the true label of the example is revealed to the learner. The learner suffers a loss for making an incorrect prediction (for simplicity, we use square loss in this paper). The goal of the learner is to make predictions with total loss comparable to the loss of the best sparse linear regressor with a bounded norm, where the term sparse refers to the fact that the linear regressor has nonzero weights on at most k features. To measure the performance of the online learner, we use the standard notion of regret, which is the difference between the total loss of the online learner and the total loss of the best sparse linear regressor.\nWhile regret is the primary performance metric, we are also interested in efficiency of the online learner. Ideally, we desire an online learning algorithm that minimizes regret while making predictions efficiently, i.e., in polynomial time (as a function of d and T ). In this paper, we prove that this goal is impossible unless there is a randomized polynomial-time algorithm for deciding satisfiability of 3CNF formulas, the canonical NP-hard problem. This computational hardness result resolves open problems from (Kale, 2014) and (Zolghadr et al., 2013). In fact, the computational hardness persists even if the online learner is given the additional flexibility of choosing k\u2032 = D log(d)k features for any constant D > 0. In light of this result, in this paper we also give an inefficient algorithm for the problem which queries k\u2032 \u2265 k + 2 features in each round, that runs in O( (\nd k\n) k\u2032) time per round, and that obtains\nregret bounded by O( d 2 (k\u2032\u2212k)2 \u221a k log(d)T )."}, {"heading": "2 Related Work and Known Results", "text": "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015). This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. While the aforementioned papers give efficient, near-optimal algorithms for this problem, these algorithms do not work in the online sparse regression setting in which we are interested because here we are required to make predictions only using a limited number of features.\nIn (Kale, 2014), a simple algorithm has been suggested, which is based on running a bandit algorithm in which the actions correspond to selecting one of (\nd k\n)\nsubsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm. This algorithm, with the right choice of interval lengths, has a regret bound of O(k2dk/3T 2/3 log(T/d)). The algorithm has exponential dependence on k both in running time and the regret. Also, Kale (2014) sketches a different algorithm with performance guarantees similar to the algorithm presented in this paper; our work builds upon that sketch and gives tighter regret bounds.\nZolghadr et al. (2013) consider a very closely related setting (called online probing) in which features and labels may be obtained by the learner at some cost (which may be dif-\nferent for different features), and this cost is factored into the loss of the learner. In the special case of their setting corresponding to the problem considered here, they given an algorithm, LQDExp3, which relies on discretizing all k-sparse weight vectors and running an exponential-weights experts algorithm on the resulting set with stochastic loss estimators, obtaining a O( \u221a dT ) regret bound. However the running time of their algorithm is prohibitive: O((dT )O(k)) time per iteration. In the same paper, they pose the open problem of finding a computationally efficient no-regret algorithm for the problem. The hardness result in this paper resolves this open problem.\nOn the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995). The hardness result in this paper is in fact inspired by the work of Foster et al. (2015), who proved that it is computationally hard to find even an approximately optimal sparse linear regressor for an ordinary least squares regression problem given a batch of labeled data. While these results imply that it is hard to properly1 solve the offline problem, in the online setting we allow improper learning, and hence these prior results don\u2019t yield hardness results for the online problem considered in this paper."}, {"heading": "3 Notation and Setup", "text": "We use the notation [d] = {1, 2, . . . , d} to refer to the coordinates. All vectors in this paper are in Rd, and all matrices in Rd\u00d7d. For a subset S of [d], and a vector x, we use the notation x(S) to denote the projection of x on the coordinates indexed by S. We also use the notation IS to denote the diagonal matrix which has ones in the coordinates indexed by S and zeros elsewhere: this is the identity matrix on the subspace of Rd induced by the coordinates in S, as well as the projection matrix for this subspace. We use the notation \u2016 \u00b7 \u2016 to denote the \u21132 norm in R\nd and \u2016 \u00b7 \u20160 to denote the zero \u201cnorm,\u201d i.e., the number of nonzero coordinates. We consider a prediction problem in which the examples are vectors in Rd with \u21132 norm bounded by 1, and labels are in the range [\u22121, 1]. We use square loss to measure the accuracy of a prediction: i.e., for a labeled example (x, y) \u2208 Rd \u00d7 [\u22121, 1], the loss of a prediction y\u0302 is (y\u0302\u2212y)2. The learner\u2019s task is to make predictions online as examples arrive one by one based on observing only k out of d features of the learner\u2019s choosing on any example (the learner is allowed to choose different subsets of features to observe in each round). The learner\u2019s goal is to minimize regret relative to the best k-sparse linear regressor whose \u21132 norm is bounded by 1.\nFormally, for t = 1, 2, . . . , T , the learner:\n1. selects a subset St \u2286 [d] of size at most k,\n2. observes xt(St), i.e., the values of the features of xt restricted to the subset St,\n3. makes a prediction y\u0302t \u2208 [\u22121, 1], 1Proper learning means finding the optimal sparse linear regressor, whereas improper learning means\nfinding an arbitrary predictor with performance comparable to that of the optimal sparse linear regressor.\n4. observes the true label yt, and suffers loss (y\u0302t \u2212 yt)2. Define regret of the learner as\nRegret :=\nT \u2211\nt=1\n(y\u0302t \u2212 yt)2 \u2212 min w: \u2016w\u20160\u2264k, \u2016w\u2016\u22641\nT \u2211\nt=1\n(w \u00b7 xt \u2212 yt)2.\nIn case y\u0302t is chosen using randomization, we consider expected regret instead. Given the NP-hardness of computing the optimal k-sparse linear regressor (Foster et al., 2015; Natarajan, 1995), we also consider a variant of the problem which gives the learner more flexibility than the comparator: the learner is allowed to choose k\u2032 \u2265 k coordinates to query in each round. The definition of the regret remains the same. We call this the (k, k\u2032, d)-online sparse regression problem.\nWe are interested in the following two goals2:\n1. (No Regret) Make predictions y\u0302t so that regret is bounded by poly(d)T 1\u2212\u03b4 for some\n\u03b4 > 0.\n2. (Efficiency) Make these predictions efficiently, i.e., in poly(d, T ) time per iteration.\nIn this paper, we show it is possible to get an inefficient no-regret algorithm for the online sparse regression problem. Complementing this result, we also show that an efficient noregret algorithm cannot exist, assuming the standard hardness assumption that NP 6\u2286 BPP."}, {"heading": "4 Upper bound", "text": "In this section we give an inefficient algorithm for the (k, k\u2032, d)-online sparse regression problem which obtains an expected regret of O( d 2\n(k\u2032\u2212k)2 \u221a k log(d)T ). The algorithm needs k\u2032\nto be at least k + 2. It is inefficient because it maintains statistics for every subset of [d] of size k, of which there are (\nd k\n)\n. At a high level, the algorithm runs an experts algorithm (specifically, Hedge) treating all such subsets as experts. Each expert internally runs stochastic gradient descent only on the coordinates specified by the corresponding subset, ensuring low regret to any bounded norm parameter vector that is nonzero only on those coordinates. The Hedge algorithm ensures low regret to the best subset of coordinates, and thus the overall algorithm achieves low regret with respect to any k-sparse parameter vector. The necessity of using k\u2032 \u2265 k + 2 features in the algorithm is that the algorithm uses the additional k\u2032\u2212k features to generate unbiased estimators for xtx \u22a4 t and ytxt in each round, which are needed to generate stochastic gradients for all the experts. These estimators have large variance unless k\u2032 \u2212 k is large. The pseudocode is given in Algorithm 1. In the algorithm, in round t, the algorithm generates a distribution Dt over the subsets of [d] of size k; for any such subset S, we use the notation Dt(S) to denote the probability of choosing the set S in this distribution. We also define the function \u03a0 on Rd to be the projection onto the unit ball, i.e., for w \u2208 Rd, \u03a0(w) = w if \u2016w\u2016 \u2264 1, and \u03a0(w) = 1\u2016w\u2016w otherwise.\n2In this paper, we use the poly(\u00b7) notation to denote a polynomially-bounded function of its arguments.\nAlgorithm 1 Algorithm for Online Sparse Regression\n1: Define the parameters p = k \u2032\u2212k d , q = (k \u2032\u2212k)(k\u2032\u2212k\u22121) d(d\u22121) , \u03b7Hedge = q \u221a ln(d) T , and \u03b7SGD = q \u221a 1 T . 2: Let D1 be the uniform distribution over all subsets of [d] of size k. 3: For every subset S of [d] of size k, let wS,1 = 0, the all-zeros vector in R\nd. 4: for t = 1, 2, . . . , T do 5: Sample a subset S\u0302t of [d] of size k from Dt, and a subset Rt of [d] of size k\n\u2032 \u2212 k drawn uniformly at random, independently of S\u0302t.\n6: Acquire xt(St) for St := S\u0302t \u222a Rt. 7: Make the prediction y\u0302t = wS\u0302t,t \u00b7 xt and obtain the true label yt. 8: Compute the matrix Xt \u2208 Rd\u00d7d and the vector zt \u2208 Rd defined as follows:\nXt(i, j) =\n\n \n \nxt(i)2\np if i = j and i \u2208 Rt\nxt(i)xt(j) q if i 6= j and i, j \u2208 Rt 0 otherwise,\nand zt(i) =\n{\nytxt(i) p if i \u2208 Rt 0 otherwise,\n9: Update the distribution over the subsets: for all subsets S of [d] of size k, let\nDt+1(S) = Dt(S) exp(\u2212\u03b7Hedge(w\u22a4S,tXtwS,t \u2212 2z\u22a4t wS,t + y2t ))/Zt,\nwhere Zt is the normalization factor to make Dt+1 a distribution. 10: For each subset S of [d] of size k, let\nwS,t+1 = \u03a0(wS,t \u2212 2\u03b7SGDIS(XtwS,t \u2212 zt)).\n11: end for\nTheorem 1. There is an algorithm for the online sparse regression problem with any given parameters (k, k\u2032, d) such that k\u2032 \u2265 k + 2 running in O( (\nd k\n) \u00b7 k\u2032) time per iteration with O( d 2\n(k\u2032\u2212k)2 \u221a k log(d)T ) expected regret.\nProof. The algorithm is given in Algorithm 1. Since the algorithm maintains a parameter vector in Rk for each subset of [d] of size k, the running time is dominated by the time to sample from Dt and update it, and the time to update the parameter vectors. The updates can be implemented in O(k\u2032) time, so overall each round can be implemented in O( (\nd k\n) \u00b7 k\u2032) time.\nWe now analyze the regret of the algorithm. Let Et[\u00b7] denote the expectation conditioned on all the randomness prior to round t. Then, it is easy to check, using the fact that k\u2032\u2212k \u2265 2, that the construction of Xt and zt in Step 8 of the algorithm has the following property:\nEt[Xt] = xtx \u22a4 t and Et[zt] = ytxt. (1)\nNext, notice that in Step 9, the algorithm runs the standard Hedge-algorithm update (see, for example, Section 2.1 in (Arora et al., 2012)) on (\nd k\n)\nexperts, one for each subset of\n[d] of size k, where, in round t, the cost of the expert corresponding to subset S is defined to be3 w\u22a4S,tXtwS,t \u2212 2z\u22a4t wS,t + y2t . (2) It is easy to check, using the facts that \u2016xt\u2016 \u2264 1, \u2016wS,t\u2016 \u2264 1 and p \u2265 q, that the cost (2) is bounded deterministically in absolute value by O(1\nq ) = O( d\n2\n(k\u2032\u2212k)2 ). Let EDt [\u00b7] denote the expectation over the random choice of S\u0302t from the distribution Dt conditioned on all other randomness up to and including round t. Since there are (\nd k\n)\n\u2264 dk experts in the Hedge algorithm here, the standard regret bound for Hedge (Arora et al., 2012, Theorem 2.3) with the specified value of \u03b7Hedge implies that for any subset S of [d] of size k, using ln ( d k )\n\u2264 k ln d, we have\nT \u2211\nt=1\nEDt [wS\u0302t,tXtwS\u0302t,t\u22122z \u22a4 t wS\u0302t,t+y 2 t ] \u2264\nT \u2211\nt=1\n(w\u22a4S,tXtwS,t\u22122z\u22a4t wS,t+y2t )+O( d 2 (k\u2032\u2212k)2 \u221a k ln(d)T ).\n(3) Next, we note, using (1) and the fact that conditioned on the randomness prior to round t, wS,t is completely determined, that (for any S)\nEt[w \u22a4 S,tXtwS,t \u2212 2z\u22a4t wS,t + y2t ] = w\u22a4S,txtx\u22a4t wS,t \u2212 2ytx\u22a4t wS,t + y2t = (wS,t \u00b7 xt \u2212 yt)2. (4)\nTaking expectations on both sides of (3) over all the randomness in the algorithm, and using (4), we get that for any subset S of [d] of size k, we have\nT \u2211\nt=1\nE[(wS\u0302t,t \u00b7 xt \u2212 yt) 2] \u2264\nT \u2211\nt=1\nE[(wS,t \u00b7 xt \u2212 yt)2] +O( d 2 (k\u2032\u2212k)2 \u221a k log(d)T ). (5)\nThe left-hand side of (5) equals \u2211T t=1 E[(y\u0302t \u2212 yt)2]. We now analyze the right-hand side. For any given subset S of [d] of size k, we claim that in Step 10 of the algorithm, the parameter vector wS,t is updated using stochastic gradient descent with the loss function \u2113t(w) := (x \u22a4 t ISw \u2212 yt)2 over the set over {w | ||w||2 \u2264 1}, only on the coordinates in S, while the coordinates not in S are fixed to 0. To prove this claim, first, we note that the premultiplication by IS in the update in Step 10 ensures that in the parameter vector wS,t+1 all coordinates that are not in S are set to 0, assuming that coordinates of wS,t not in S were 0.\nNext, at time t, consider the loss function \u2113t(w) = (x \u22a4 t ISw \u2212 yt)2. The gradient of this\nloss function at wS,t is\n\u2207\u2113t(wS,t) = 2(x\u22a4t ISwS,t \u2212 yt)ISxt = 2IS(xtx\u22a4t wS,t \u2212 ytxt),\nwhere we use the fact that ISwS,t = wS,t since wS,t has zeros in coordinates not in S. Now, by (1), we have\nEt[2IS(XtwS,t \u2212 zt)] = 2IS(xtx\u22a4t wS,t \u2212 ytxt), 3Recall that the costs in Hedge may be chosen adaptively.\nand thus, Step 10 of the algorithm is a stochastic gradient descent update as claimed. Furthermore, a calculation similar to the one for bounding the loss of the experts in the Hedge algorithm shows that the norm of the stochastic gradient is bounded deterministically by O(1\nq ), which is O( d\n2\n(k\u2032\u2212k)2 ).\nUsing a standard regret bound for stochastic gradient descent (see, for example, Lemma 3.1 in (Flaxman et al., 2005)) with the specified value of \u03b7SGD, we conclude that for any fixed vector w of \u21132 norm at most 1, we have,\nT \u2211\nt=1\nE[(x\u22a4t ISwS,t \u2212 yt)2] \u2264 T \u2211\nt=1\n(x\u22a4t ISw \u2212 yt)2 +O( d 2 (k\u2032\u2212k)2 \u221a T ).\nSince ISwS,t = wS,t, the left hand side of the above inequality equals \u2211T t=1 E[(wS,t \u00b7xt\u2212yt)2]. Finally, let w be an arbitrary k-sparse vector of \u21132 norm at most 1. Let S = {i | wi 6= 0}. Note that |S| \u2264 k, and IS(w) = w. Applying the above bound for this set S, we get T \u2211\nt=1\nE[(wS,t \u00b7 xt \u2212 yt)2] \u2264 T \u2211\nt=1\n(w \u00b7 xt \u2212 yt)2 +O( d 2 (k\u2032\u2212k)2 \u221a T ). (6)\nCombining the inequality (6) with inequality (5), we conclude that\nT \u2211\nt=1\nE[(y\u0302t \u2212 yt)2] \u2264 T \u2211\nt=1\n(w \u00b7 xt \u2212 yt)2 +O( d 2 (k\u2032\u2212k)2 \u221a k log(d)T ).\nThis gives us the required regret bound."}, {"heading": "5 Computational lower bound", "text": "In this section we show that there cannot exist an efficient no-regret algorithm for the online sparse regression problem unless NP \u2286 BPP. This hardness result follows from the hardness of approximating the Set Cover problem. We give a reduction showing that if there were an efficient no-regret algorithm AlgOSR for the online sparse regression problem, then we could distinguish, in randomized polynomial time, between two instances of the Set Cover problem: in one of which there is a small set cover, and in the other of which any set cover is large. This task is known to be NP-hard for specific parameter values. Specifically, our reduction has the following properties:\n1. If there is a small set cover, then in the induced online sparse regression problem there is a k-sparse parameter vector (of \u21132 norm at most 1) giving 0 loss, and thus the algorithm AlgOSR must have small total loss (equal to the regret) as well.\n2. If there is no small set cover, then the prediction made by AlgOSR in any round has at least a constant loss in expectation, which implies that its total (expected) loss must be large, in fact, linear in T .\nBy measuring the total loss of the algorithm, we can distinguish between the the two instances of the Set Cover problem mentioned above with probability at least 3/4, thus yielding a BPP algorithm for an NP-hard problem.\nThe starting point for our reduction is the work of Dinur and Steurer (2014) who give a polynomial-time reduction of deciding satisfiability of 3CNF formulas to distinguishing instances of Set Cover with certain useful combinatorial properties. We denote the satisfiability problem of 3CNF formulas by SAT.\nReduction 1. For any given constant D > 0, there is a constant cD \u2208 (0, 1) and a poly(nD)time algorithm that takes a 3CNF formula \u03c6 of size n as input and constructs a Set Cover instance over a ground set of size m = poly(nD) with d = poly(n) sets, with the following properties:\n1. if \u03c6 \u2208 SAT, then there is a collection of k = O(dcD) sets, which covers each element exactly once, and\n2. if \u03c6 /\u2208 SAT, then no collection of k\u2032 = \u230aD ln(d)k\u230b sets covers all elements; i.e., at least one element is left uncovered.\nThe Set Cover instance generated from \u03c6 can be encoded as a binary matrix M\u03c6 \u2208 {0, 1}m\u00d7d with the rows corresponding to the elements of the ground set, and the columns correspond to the sets, such that each column is the indicator vector of the corresponding set.\nUsing this reduction, we now show how an efficient, no-regret algorithm for online sparse regression can be used to give a BPP algorithm for SAT.\nTheorem 2. Let D > 0 be any given constant. Suppose there is an algorithm, AlgOSR, for the (k, k\u2032, d)-online sparse regression problem with k = O(dcD), where cD is the constant from Reduction 1, and k\u2032 = \u230aD ln(d)k\u230b, that runs in poly(d, T ) time per iteration and has expected regret bounded by poly(d)T 1\u2212\u03b4 for some constant \u03b4 > 0. Then NP \u2286 BPP.\nProof. Since the expected regret of AlgOSR is bounded by p(d)T 1\u2212\u03b4 (where p(d) is a polynomial function of d), by Markov\u2019s inequality we conclude that with probability at least 3/4, the regret of AlgOSR is bounded by p(d)\u00b7T 1\u2212\u03b4. Figure 2 gives a randomized algorithm, AlgSAT, for deciding satisfiability of a given 3CNF formula \u03c6 using the algorithm AlgOSR. Note that the feature vectors (i.e., the xt vectors) generated by AlgSAT are bounded in \u21132 norm by 1, as required. It is clear that AlgSAT is a polynomial-time algorithm since T is a polynomial function of n (since m, k, d, p(d) are polynomial functions of n), and AlgOSR runs in poly(d, T ) time per iteration.\nWe now claim that this algorithm correctly decides satisfiability of \u03c6 with probability at least 3/4, and is hence a BPP algorithm for SAT.\nTo prove this, suppose \u03c6 \u2208 SAT. Then, there are k sets in the Set Cover which cover all elements with each element being covered exactly once. Consider the k-sparse parameter vector w which has 1\u221a\nk in the positions corresponding to these k sets and 0 elsewhere. Note\nthat \u2016w\u2016 \u2264 1, as required. Note that since this collection of k sets covers each element\nAlgorithm 2 Algorithm AlgSAT for deciding satisfiability of 3CNF formulas\nRequire: A constant D > 0, and an algorithm AlgOSR for the (k, k \u2032, d)-online sparse re-\ngression problem with k = O(dcD), where cD is the constant from Reduction 1, and k\u2032 = \u230aD ln(d)k\u230b, that runs in poly(d, T ) time per iteration with regret bounded by p(d) \u00b7 T 1\u2212\u03b4 with probability at least 3/4.\nRequire: A 3CNF formula \u03c6. 1: Compute the matrix M\u03c6 and the associated parameters k, k\n\u2032, d,m from Reduction 1. 2: Run AlgOSR with the parameters k, k\n\u2032, d for T := \u2308max{(2p(d)mdk)1/\u03b4, 256m2d2k2}\u2309 iterations.\n3: for t = 1, 2, . . . , T do 4: Sample a row of M\u03c6 uniformly at random; call it x\u0302t. 5: Sample \u03c3t \u2208 {\u22121, 1} uniformly at random independently of x\u0302t. 6: Set xt =\n\u03c3t\u221a d x\u0302t and yt = \u03c3t\u221a dk .\n7: Obtain a set of coordinates St of size at most k \u2032 by running AlgOSR, and provide it the coordinates xt(St). 8: Obtain the prediction y\u0302t from AlgOSR, and provide it the true label yt. 9: end for\n10: if \u2211T t=1(yt \u2212 y\u0302t)2 \u2264 T2mdk then 11: Output \u201csatisfiable\u201d. 12: else 13: Output \u201cunsatisfiable\u201d. 14: end if\nexactly once, we have M\u03c6w = 1\u221a k 1, where 1 is the all-1\u2019s vector. In particular, since x\u0302t is a row of M\u03c6, we have\nw \u00b7 xt = w \u00b7 ( \u03c3t\u221a d x\u0302t ) = \u03c3t\u221a dk = yt.\nThus, (w \u00b7xt\u2212yt)2 = 0 for all rounds t. Since algorithm AlgOSR has regret at most p(d) \u00b7T 1\u2212\u03b4 with probability at least 3/4, its total loss\n\u2211T t=1(y\u0302t \u2212 yt)2 is bounded by p(d) \u00b7 T 1\u2212\u03b4 \u2264 T2mdk\n(since T \u2265 (2p(d)mdk)1/\u03b4) with probability at least 3/4. Thus, in this case algorithm AlgSAT correctly outputs \u201csatisfiable\u201d with probability at least 3/4.\nNext, suppose \u03c6 /\u2208 SAT. Fix any round t and let St be the set of coordinates of size at most k\u2032 selected by algorithm AlgOSR to query. This set St corresponds to k\n\u2032 sets in the Set Cover instance. Since \u03c6 /\u2208 SAT, there is at least one element in the ground set that is not covered by any set among these k\u2032 sets. This implies that there is at least one row of M\u03c6 that is 0 in all the coordinates in St. Since x\u0302t is a uniformly random row of M\u03c6 chosen independently of St, we have\nPr[xt(St) = 0] = Pr[x\u0302t(St) = 0] \u2265 1\nm .\nHere, 0 denotes the all-zeros vector of size k\u2032.\nNow, we claim that E[yty\u0302t | xt(St) = 0] = 0. This is because the condition that xt(St) = 0 is equivalent to the condition that x\u0302t(St) = 0. Since yt is chosen from {\u2212 1\u221adk , 1\u221a dk } uniformly at random independently of x\u0302t and y\u0302t, the claim follows. The expected loss of the online algorithm in round t can now be bounded as follows:\nE[ (y\u0302t \u2212 yt)2 \u2223 \u2223 xt(St) = 0] = E\n[\ny\u03022t + 1\ndk \u2212 2yty\u0302t\n\u2223 \u2223 \u2223 \u2223 xt(St) = 0 ]\n= E\n[\ny\u03022t + 1\ndk\n\u2223 \u2223 \u2223 \u2223 xt(St) = 0 ] \u2265 1 dk ,\nand hence\nE[(yt \u2212 y\u0302t)2] \u2265 E[(yt \u2212 y\u0302t)2 | xt(St) = 0] \u00b7 Pr[xt(St) = 0] \u2265 1 dk \u00b7 1 m = 1 mdk .\nLet Et[\u00b7] denote expectation of a random variable conditioned on all randomness prior to round t. Since the choices of xt and yt are independent of previous choices in each round, the same argument also implies that Et[(yt \u2212 y\u0302t)2] \u2265 1mdk . Applying Azuma\u2019s inequality (see Theorem 7.2.1 in (Alon and Spencer, 1992)) to the martingale difference sequence Et[(yt \u2212 y\u0302t)\n2] \u2212 (yt \u2212 y\u0302t)2 for t = 1, 2, . . . , T , since each term is bounded in absolute value by 4, we get\nPr\n[\nT \u2211\nt=1\nEt[(yt \u2212 y\u0302t)2]\u2212 (yt \u2212 y\u0302t)2 \u2265 8 \u221a T\n]\n\u2264 exp ( \u2212 64T 2\u00b716T ) \u2264 1 4 .\nThus, with probability at least 3/4, the total loss \u2211T t=1(y\u0302t\u2212yt)2 is greater than \u2211T t=1 Et[(yt\u2212 y\u0302t) 2]\u2212 8 \u221a T \u2265 1 mdk T \u2212 8 \u221a T \u2265 T 2mdk (since T \u2265 256m2d2k2). Thus in this case the algorithm correctly outputs \u201cunsatisfiable\u201d with probability at least 3/4.\nParameter settings for hard instances. Theorem 2 implies that for any given constant D > 0, there is a constant cD such that the parameter settings k = O(d\ncD), and k\u2032 = \u230aD ln(d)k\u230b yield hard instances for the online sparse regression problem. The reduction of Dinur and Steurer (2014) can be \u201ctweaked\u201d4 so that the cD is arbitrarily close to 1 for any constant D.\nWe can now extend the hardness results to the parameter settings k = O(d\u01eb) for any \u01eb \u2208 (0, 1) and k\u2032 = \u230aD ln(d)k\u230b either by tweaking the reduction of Dinur and Steurer (2014) so it yields cD = \u01eb if \u01eb is close enough to 1, or if \u01eb is small, by adding O(d\n1/\u01eb) all-zeros columns to the matrix M\u03c6. The two combinatorial properties of M\u03c6 in Reduction 1 are clearly still satisfied, and the proof of Theorem 2 goes through.\n4This is accomplished by simply replacing the Label Cover instance they construct with polynomially many disjoint copies of the same instance."}, {"heading": "6 Conclusions", "text": "In this paper, we prove that minimizing regret in the online sparse regression problem is computationally hard even if the learner is allowed access to many more features than the comparator, a sparse linear regressor. We complement this result by giving an inefficient no-regret algorithm.\nThe main open question remaining from this work is what extra assumptions can one make on the examples arriving online to make the problem tractable. Note that the sequence of examples constructed in the lower bound proof is i.i.d., so clearly stronger assumptions than that are necessary to obtain any efficient algorithms."}], "references": [{"title": "The Probabilistic Method", "author": ["Noga Alon", "Joel Spencer"], "venue": "John Wiley,", "citeRegEx": "Alon and Spencer.,? \\Q1992\\E", "shortCiteRegEx": "Alon and Spencer.", "year": 1992}, {"title": "The multiplicative weights update method: a meta-algorithm and applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Efficient learning with partially observed attributes", "author": ["Nicol\u00f2 Cesa-Bianchi", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2011}, {"title": "Analytical approach to parallel repetition", "author": ["Irit Dinur", "David Steurer"], "venue": "In STOC, pages 624\u2013633,", "citeRegEx": "Dinur and Steurer.,? \\Q2014\\E", "shortCiteRegEx": "Dinur and Steurer.", "year": 2014}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["Abraham Flaxman", "Adam Tauman Kalai", "H. Brendan McMahan"], "venue": "In SODA,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Variable selection is hard", "author": ["Dean Foster", "Howard Karloff", "Justin Thaler"], "venue": "In COLT,", "citeRegEx": "Foster et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2015}, {"title": "Linear regression with limited observation", "author": ["Elad Hazan", "Tomer Koren"], "venue": "In ICML,", "citeRegEx": "Hazan and Koren.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Koren.", "year": 2012}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Elad Hazan", "Amit Agarwal", "Satyen Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Open problem: Efficient online sparse regression", "author": ["Satyen Kale"], "venue": "In COLT, pages 1299\u20131301,", "citeRegEx": "Kale.,? \\Q2014\\E", "shortCiteRegEx": "Kale.", "year": 2014}, {"title": "Attribute efficient linear regression with distributiondependent sampling", "author": ["Doron Kukliansky", "Ohad Shamir"], "venue": "In ICML,", "citeRegEx": "Kukliansky and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Kukliansky and Shamir.", "year": 2015}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM J. Computing,", "citeRegEx": "Natarajan.,? \\Q1995\\E", "shortCiteRegEx": "Natarajan.", "year": 1995}, {"title": "Online learning with costly features and labels", "author": ["Navid Zolghadr", "G\u00e1bor Bart\u00f3k", "Russell Greiner", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Zolghadr et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zolghadr et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 8, "context": "This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al.", "startOffset": 83, "endOffset": 95}, {"referenceID": 8, "context": "This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension.", "startOffset": 84, "endOffset": 137}, {"referenceID": 2, "context": "One example of this scenario, from (Cesa-Bianchi et al., 2011), is medical diagnosis of a disease, in which each feature corresponds to a medical test that the patient in question can undergo.", "startOffset": 35, "endOffset": 62}, {"referenceID": 8, "context": "This computational hardness result resolves open problems from (Kale, 2014) and (Zolghadr et al.", "startOffset": 63, "endOffset": 75}, {"referenceID": 11, "context": "This computational hardness result resolves open problems from (Kale, 2014) and (Zolghadr et al., 2013).", "startOffset": 80, "endOffset": 103}, {"referenceID": 2, "context": "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015).", "startOffset": 50, "endOffset": 129}, {"referenceID": 6, "context": "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015).", "startOffset": 50, "endOffset": 129}, {"referenceID": 9, "context": "A related setting is attribute-efficient learning (Cesa-Bianchi et al., 2011; Hazan and Koren, 2012; Kukliansky and Shamir, 2015).", "startOffset": 50, "endOffset": 129}, {"referenceID": 8, "context": "In (Kale, 2014), a simple algorithm has been suggested, which is based on running a bandit algorithm in which the actions correspond to selecting one of ( d k )", "startOffset": 3, "endOffset": 15}, {"referenceID": 7, "context": "subsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm.", "startOffset": 165, "endOffset": 185}, {"referenceID": 7, "context": "subsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm. This algorithm, with the right choice of interval lengths, has a regret bound of O(kdT 2/3 log(T/d)). The algorithm has exponential dependence on k both in running time and the regret. Also, Kale (2014) sketches a different algorithm with performance guarantees similar to the algorithm presented in this paper; our work builds upon that sketch and gives tighter regret bounds.", "startOffset": 165, "endOffset": 444}, {"referenceID": 7, "context": "subsets of coordinates of size k at regular intervals, and within each interval, running an online regression algorithm (such as the Online Newton-Step algorithm of Hazan et al. (2007)) over the k coordinates chosen by the bandit algorithm. This algorithm, with the right choice of interval lengths, has a regret bound of O(kdT 2/3 log(T/d)). The algorithm has exponential dependence on k both in running time and the regret. Also, Kale (2014) sketches a different algorithm with performance guarantees similar to the algorithm presented in this paper; our work builds upon that sketch and gives tighter regret bounds. Zolghadr et al. (2013) consider a very closely related setting (called online probing) in which features and labels may be obtained by the learner at some cost (which may be dif-", "startOffset": 165, "endOffset": 642}, {"referenceID": 5, "context": "On the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995).", "startOffset": 114, "endOffset": 152}, {"referenceID": 10, "context": "On the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995).", "startOffset": 114, "endOffset": 152}, {"referenceID": 5, "context": "On the computational hardness side, it is known that it is NP-hard to compute the optimal sparse linear regressor (Foster et al., 2015; Natarajan, 1995). The hardness result in this paper is in fact inspired by the work of Foster et al. (2015), who proved that it is computationally hard to find even an approximately optimal sparse linear regressor for an ordinary least squares regression problem given a batch of labeled data.", "startOffset": 115, "endOffset": 244}, {"referenceID": 5, "context": "Given the NP-hardness of computing the optimal k-sparse linear regressor (Foster et al., 2015; Natarajan, 1995), we also consider a variant of the problem which gives the learner more flexibility than the comparator: the learner is allowed to choose k\u2032 \u2265 k coordinates to query in each round.", "startOffset": 73, "endOffset": 111}, {"referenceID": 10, "context": "Given the NP-hardness of computing the optimal k-sparse linear regressor (Foster et al., 2015; Natarajan, 1995), we also consider a variant of the problem which gives the learner more flexibility than the comparator: the learner is allowed to choose k\u2032 \u2265 k coordinates to query in each round.", "startOffset": 73, "endOffset": 111}, {"referenceID": 1, "context": "1 in (Arora et al., 2012)) on ( d k )", "startOffset": 5, "endOffset": 25}, {"referenceID": 4, "context": "1 in (Flaxman et al., 2005)) with the specified value of \u03b7SGD, we conclude that for any fixed vector w of l2 norm at most 1, we have,", "startOffset": 5, "endOffset": 27}, {"referenceID": 3, "context": "The starting point for our reduction is the work of Dinur and Steurer (2014) who give a polynomial-time reduction of deciding satisfiability of 3CNF formulas to distinguishing instances of Set Cover with certain useful combinatorial properties.", "startOffset": 52, "endOffset": 77}, {"referenceID": 0, "context": "1 in (Alon and Spencer, 1992)) to the martingale difference sequence Et[(yt \u2212 \u0177t) ] \u2212 (yt \u2212 \u0177t) for t = 1, 2, .", "startOffset": 5, "endOffset": 29}, {"referenceID": 3, "context": "The reduction of Dinur and Steurer (2014) can be \u201ctweaked\u201d so that the cD is arbitrarily close to 1 for any constant D.", "startOffset": 17, "endOffset": 42}, {"referenceID": 3, "context": "The reduction of Dinur and Steurer (2014) can be \u201ctweaked\u201d so that the cD is arbitrarily close to 1 for any constant D. We can now extend the hardness results to the parameter settings k = O(d) for any \u01eb \u2208 (0, 1) and k\u2032 = \u230aD ln(d)k\u230b either by tweaking the reduction of Dinur and Steurer (2014) so it yields cD = \u01eb if \u01eb is close enough to 1, or if \u01eb is small, by adding O(d ) all-zeros columns to the matrix M\u03c6.", "startOffset": 17, "endOffset": 294}], "year": 2016, "abstractText": "We consider the online sparse linear regression problem, which is the problem of sequentially making predictions observing only a limited number of features in each round, to minimize regret with respect to the best sparse linear regressor, where prediction accuracy is measured by square loss. We give an inefficient algorithm that obtains regret bounded by \u00d5( \u221a T ) after T prediction rounds. We complement this result by showing that no algorithm running in polynomial time per iteration can achieve regret bounded by O(T 1\u2212\u03b4) for any constant \u03b4 > 0 unless NP \u2286 BPP. This computational hardness result resolves an open problem presented in COLT 2014 (Kale, 2014) and also posed by Zolghadr et al. (2013). This hardness result holds even if the algorithm is allowed to access more features than the best sparse linear regressor up to a logarithmic factor in the dimension.", "creator": "LaTeX with hyperref package"}}}