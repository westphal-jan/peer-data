{"id": "1603.04350", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2016", "title": "An optimal algorithm for bandit convex optimization", "abstract": "we could consider developing the problem of requiring online circular convex graph optimization against predicting an arbitrary adversary with bandit induced feedback, roughly known primarily as bandit loop convex optimization. we therefore give the first $ \\ tilde { o } ( \\ { sqrt { t } ) $ - regret graph algorithm for this complete setting based on only a naive novel application of mapping the infinite ellipsoid method to online learning. eventually this demand bound is known to be tight up enough to logarithmic likelihood factors. our feedback analysis then is today introduces somewhat new tools in naive discrete convex geometry.", "histories": [["v1", "Mon, 14 Mar 2016 17:15:15 GMT  (756kb,D)", "http://arxiv.org/abs/1603.04350v1", "28 pages, 7 figures"], ["v2", "Tue, 15 Mar 2016 17:46:58 GMT  (756kb,D)", "http://arxiv.org/abs/1603.04350v2", "29 pages, 8 figures"]], "COMMENTS": "28 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["elad hazan", "yuanzhi li"], "accepted": false, "id": "1603.04350"}, "pdf": {"name": "1603.04350.pdf", "metadata": {"source": "CRF", "title": "An optimal algorithm for bandit convex optimization", "authors": ["Elad Hazan", "Yuanzhi Li"], "emails": [], "sections": [{"heading": null, "text": "\u221a T )-regret algorithm for this setting based\non a novel application of the ellipsoid method to online learning. This bound is known to be tight up to logarithmic factors. Our analysis introduces new tools in discrete convex geometry."}, {"heading": "1 Introduction", "text": "In the setting of Bandit Convex Optimization (BCO), a learner repeatedly chooses a point in a convex decision set. The learner then observes a loss which is equal to the value of an adversarially chosen convex loss function. The only feedback available to the learner is the loss \u2014 a single real number. Her goal is to minimize the regret, defined to be the difference between the sum of losses incurred and the loss of the best fixed decision (point in the decision set) in hindsight.\nThis fundamental decision making setting is extremely general, and has been used to efficiently model online prediction problems with limited feedback such as online routing, online ranking and ad placement, and many others (see [8] and [17] chapter 6 for applications and a detailed survey of BCO). This generality and importance is accompanied by significant difficulties: BCO allows for an adversarially chosen cost functions, and extremely limited information is available to the leaner in the form of a single scalar per iteration. The extreme exploration-exploitation tradeoff common in bandit problems is accompanied by the additional challenge of polynomial time convex optimization to make this problem one of the most difficult encountered in learning theory.\nAs such, the setting of BCO has been extremely well studied in recent years and the state-of-the-art significantly advanced. For example, in case the adversarial cost functions are linear, efficient algorithms are known that guarantee near-optimal regret bounds [2, 9, 18]. A host of techniques have been developed to tackle the difficulties of partial information, exploration-exploitation and efficient convex optimization. Indeed, most known optimization and algorithmic techniques have been applied, including interior point methods [2], random walk optimization [23], continuous multiplicative updates [13], random perturbation [6], iterative optimization methods [15] and many more.\nDespite this impressive and the long lasting effort and progress, the main question of BCO remains unresolved: construct an efficient and optimal regret algorithm for the full setting of BCO. Even the optimal regret attainable is yet unresolved in the full adversarial setting.\nA significant breakthrough was recently made by [10], who show that in the oblivious setting and in the special case of 1-dimensional BCO, O( \u221a T ) regret is attainable. Their result is existential in nature, showing that the minimax regret for the oblivious BCO setting (in which the adversary decides upon a distribution over cost functions independently of the learners\u2019 actions) behaves as \u0398\u0303( \u221a T ). This result was very recently extended to any dimension by [11], still with an existential bound rather than an explicit algorithm and in the oblivious setting. \u2217Princeton University, Email: ehazan@cs.princeton.edu \u2020Princeton University, Email: yuanzhil@cs.princeton.edu\nar X\niv :1\n60 3.\n04 35\n0v 1\n[ cs\n.L G\n] 1\n4 M\nar 2\n01 6\nIn this paper we advance the state of the art in bandit convex optimization and show the following results:\n1. We show that minimax regret for the full adversarial BCO setting is \u0398\u0303( \u221a T ).\n2. We give an explicit algorithm attaining this regret bound. Such an explicit algorithm was unknown previously even for the oblivious setting. 3. The algorithm guarantees \u0398\u0303( \u221a T ) regret with high probability and exponentially decaying tails. Specifi-\ncally, the algorithm guarantees regret of \u0398\u0303( \u221a T log 1\u03b4 ) with probability at least 1\u2212 \u03b4.\nIt is known that any algorithm for BCO must suffer regret \u2126( \u221a T ) in the worst case, even for oblivious adversaries and linear cost functions. Thus, up to logarithmic factors, our results close the gap of the attainable regret in terms of the number of iterations.\nTo obtain these results we introduce some new techniques into online learning, namely a novel online variant of the ellipsoid algorithm, and define some new notions in discrete convex geometry.\nWhat remains open? Our algorithms depend exponentially on the dimensionality of the decision set, both in terms of regret bounds as well as in computational complexity. As of the time of writing, we do not know whether this dependencies are tight or can be improved to be polynomial in terms of the dimension, and we leave it as an open problem to resolve this question1"}, {"heading": "1.1 Prior work", "text": "The best known upper bound in the regret attainable for adversarial BCO with general convex loss functions is O\u0303(T 5/6) due to [15] and [21] 2. A lower bound of \u2126( \u221a T ) is folklore, even the easier full-information setting of online convex optimization, see e.g. [17]. The special case of bandit linear optimization (BCO in case where the adversary is limited to using linear losses) is significantly simper. Informally, this is since the average of the function value on a sphere around a center point equals the value of the function in the center, regardless of how large is the sphere. This allows for very efficient exploration, and was first used by [13] to devise the Geometric Hedge algorithm that achieves an optimal regret rate of O\u0303( \u221a T ). An efficient algorithm inspired by interior point methods was later given by [2] with the same optimal regret bound. Further improvements in terms of the dimension and other constants were subsequently given in [9, 18].\nThe first gradient-descent-based method for BCO was given by [15]. Their regret bound was subsequently improved for various special cases of loss functions using ideas from [2]. For convex and smooth losses, [24] attained an upper bound on the regret of of O\u0303(T 2/3). This was recently improved to by [14] to O\u0303(T 5/8). [3] obtained a regret bound of O\u0303(T 2/3) for strongly-convex losses. For the special case of strongly-convex and smooth losses, [3] obtained a regret of O\u0303( \u221a T ) in the unconstrained case, and [19] obtain the same rate even in the constrained cased. [25] gives a lower bound of \u2126( \u221a T ) for the setting of strongly-convex and smooth BCO.\nA comprehensive survey by Bubeck and Cesa-Bianchi [8], provides a review of the bandit optimization literature in both stochastic and online setting.\nAnother very relevant line of work is that on zero-order convex optimization. This is the setting of convex optimization in which the only information available to the optimizer is a valuation oracle that given x \u2208 K for some convex set K \u2286 Rd, returns f(x) for some convex function f : K 7\u2192 R (or a noisy estimate of this number). This is considered one of the hardest areas in convex optimization (although strictly a special case of BCO), and a significant body of work has culminated in a polynomial time algorithm, see [12]. Recently, [4] give a polynomial time algorithm for regret minimization in the stochastic setting of zero-order optimization, greatly improving upon the known running times.\n1In the oblivious setting [11] show that the regret behaves polynomially in the dimension. It is not clear if this result can be extended to the adversarial setting.\n2although not specified precisely to the adversarial setting, this result is implicit in these works."}, {"heading": "1.2 Paper structure", "text": "In the next section we give some basic definitions and constructs that will be of use. In section 3 we survey a natural approach, motivated by zero-order optimization, and explain why completely new tools are necessary to apply it. We proceed to give the new mathematical constructions for discrete convex geometry in section 4. This is followed by our main technical lemma, the discretization lemma, in section 5. We proceed to give the new algorithm and the main result statement in section 6."}, {"heading": "2 Preliminaries", "text": "The setting of bandit convex optimization (BCO) is a repeated game between an online learner and an adversary (see e.g. [17] chapter 6). Iteratively, the learner makes a decision which is a point in a convex decision set, which is a subset of Euclidean space xt \u2208 K \u2286 Rd. Meanwhile, the adversary responds with an arbitrary Lipschitz convex loss function ft : K 7\u2192 R. The only feedback available to the learner is the loss, ft(xt) \u2208 R, and her goal is to minimize regret, defined as\nRT = \u2211 t ft(xt)\u2212 min x\u2217\u2208K \u2211 t ft(x \u2217)\nLet K \u2286 Rd be a convex compact and closed subset in Euclidean space. We denote by EK the minimal volume enclosing ellipsoid (MVEE) in K, also known as the John ellipsoid [20, 7]. For simplicity, assume that EK is centered at zero.\nGiven an ellipsoid E = { \u2211 i \u03b1ivi : \u2211 i \u03b1 2 i \u2264 1}, we shall use the notation \u2016x\u2016E \u2261 \u221a x>(V V >)\u22121x to denote the (Minkowski) semi-norm defined by the ellipsoid, where V is the matrix with the vectors vi\u2019s as columns.\nJohn\u2019s theorem says that if we shrink MVEE of K by a factor of 1/d, then it will be inside K. For connivence, we denote by \u2016 \u00b7 \u2016K the norm according to 1dEK, which is the matrix norm corresponding to the (shrinked by factor 1/d) MVEE ellipsoid of K . To be specific, Let E be the MVEE of K,\n\u2016x\u2016K = d\u2016x\u2016E = \u2016x\u2016 1 dE\nWe use d\u2016x\u2016E inside of \u2016x\u2016E merely to insure \u2200x /\u2208 K, \u2016x\u2016K \u2265 1, which simplifies our expression.\nEnclosing box. Denote by CK the bounding box of the ellipsoid EK, which is obtained by the box with axis parallel to the eigenpoles of EK. The containing box CK can be computed by first computing EK, then the diagonal transformation of this ellipsoid into a ball, computing the minimal enclosing cube of this ball, and performing the inverse diagonal transformation into a box.\nDefinition 2.1 (Minkowski Distance of a convex set). Given a convex set K \u2282 Rd and x \u2208 Rd, the Minkowski distance \u03b3(x,K) is defined as \u03b3(x,K) = ||x\u2212 x0||K\u2212x0 Where x0 is the center of the MVEE of K. K\u2212x0 denotes shifting K by\u2212x0 (so its MVEE is centered at zero)\nDefinition 2.2 (Scaled set). For \u03b2 > 0, define \u03b2K as the scaled set 3\n\u03b2K = {y | \u03b3(y,K) \u2264 \u03b2}\nHenceforth we will require a discrete representation of convex sets, which we call grids, as constructed in Algorithm 1.\nClaim 2.1. For every K \u2208 Rd, grid = grid(K, \u03b1) contains at most (2d\u03b1)d many points 3According to our definition of \u03b3, 1K \u2286 K \u2286 dK\nAlgorithm 1 construct grid 1: Input: convex set K \u2208 Rd, resolution \u03b1. 2: Compute the MVEE E \u2032 of K. Let E = 1dE \u2032\n3: Let A be the (unique) linear transformation such that A(E) = B\u03b1(0) (unit ball of radius \u03b1 centered at 0). 4: Let Zd = {(x1, ..., xd), xi \u2208 Z} be d-dimensional integer lattice. 5: Output: grid = A\u22121(Z) \u2229 K.\nLemma 2.1 (Property of the grid). LetK\u2032 \u2286 K \u2286 Rd 4 be two convex sets. For every \u03b2, \u03b3 such that \u03b2 > \u03b3 > 1, \u03b2 > d, for every \u03b1 \u2265 2(\u03b3 + 1)\u03b22 \u221a d such that the following holds. Let grid = grid(\u03b2K\u2032 \u2229 K, \u03b1), then we have:\n1. For every x \u2208 K\u2032: \u2203xg \u2208 grid such that xg + \u03b3(xg \u2212 x) \u2208 12\u03b2K \u2032\n2. For every x /\u2208 K\u2032, x \u2208 K: \u2203xg \u2208 grid such that xg + \u03b3\u03b3(x,K\u2032) (xg \u2212 x) \u2208 1 2\u03b2K \u2032\nProof of Lemma 2.1. Since \u03b2 > d, by John\u2019s theorem, K\u2032 \u2282 \u03b2K\u2032. Moreover, since we only interested in the distance ratio, we can assume that the MVEE E \u2032 of \u03b2K\u2032 \u2229 K is the ball centered at 0 of radius d\u03b1, and grid are all the integer points intersected with \u03b2K\u2032 \u2229 K. Let E = 1dE\n\u2032 = B\u03b1(0), by John\u2019s Theorem, we know that E \u2286 \u03b2K\u2032 \u2229 K \u2286 dE .\n(a). For every x \u2208 K\u2032, consider point z = \u03b3\u03b3+1x. Since E = B\u03b1(0) \u2286 \u03b2K \u2032, we know that B\u03b1 \u03b2 (0) \u2286 K\u2032.\nTherefore, B \u03b1 \u03b3\u03b2\n(z) \u2286 K\u2032, which implies when \u03b1 \u2265 \u03b3\u03b2 \u221a d, we can find xg \u2208 grid such that \u2016xg \u2212 z\u20162 \u2264 \u221a d.\nTherefore,\n\u2016xg + \u03b3(xg \u2212 x)\u20162 = \u2016 [z + \u03b3(z \u2212 x)] + [xg \u2212 z + \u03b3(xg \u2212 z)] \u20162 = ||xg \u2212 z + \u03b3(xg \u2212 z)||2 since z + \u03b3(z \u2212 x) = 0 = (\u03b3 + 1)\u2016xg \u2212 z\u20162 \u2264 (\u03b3 + 1) \u221a d by \u2016xg \u2212 z\u2016 \u2264 \u221a d\nMoreover, 12\u03b2K \u2287 1 2\u03b22 E = 1 2\u03b22B\u03b1(0) contains all points with norm \u03b1 2\u03b22 , and in particular it contains xg + \u03b3(xg \u2212 x) when \u03b1 \u2265 2(\u03b3 + 1)\u03b22 \u221a d.\n(b). For every x /\u2208 K\u2032 but x \u2208 K, take z = \u03b3\u03b3(x,K\u2032)+\u03b3x. When \u03b2 > 2\u03b3, we know that z \u2208 1 2\u03b2K \u2032. With same idea as (a), we can also conclude that\nB \u03b3(x,K\u2032) \u03b3(x,K\u2032)+\u03b3 \u03b1 \u03b22\n(z) \u2286 \u03b2K\u2032 \u2229 K\n4We will apply the lemma to K\u2032 being our working Ellipsoid and K being the original input convex set\nSince \u03b3(x,K\u2032) \u2265 1 for x /\u2208 K\u2032, we can find xg \u2208 grid be such that \u2016xg\u2212z\u20162 \u2264 \u221a dwhen \u03b1 \u2265 (\u03b3+1)\u03b22 \u221a d.\nTherefore,\n\u2225\u2225\u2225\u2225xg + \u03b3\u03b3(x,K) (xg \u2212 x) \u2225\u2225\u2225\u2225\n2\n= \u2225\u2225\u2225\u2225[z + \u03b3\u03b3(x,K) (z \u2212 x) ] + [ xg \u2212 z +\n\u03b3\n\u03b3(x,K) (xg \u2212 z) ]\u2225\u2225\u2225\u2225 2\n= \u2225\u2225\u2225\u2225xg \u2212 z + \u03b3\u03b3(x,K) (xg \u2212 z) \u2225\u2225\u2225\u2225\n2\nsince z + \u03b3\u03b3(x,K) (z \u2212 x) = 0\n= ( 1 +\n\u03b3\n\u03b3(x,K)\n) \u2016xg \u2212 z\u20162 \u2264 ( \u03b3\n\u03b3(x,K) + 1\n)\u221a d\n\u2264 (1 + \u03b3) \u221a d since \u03b3(x,K) \u2265 1\nAs before, this implies that when \u03b1 \u2265 2(\u03b3 + 1)\u03b22 \u221a d, it holds that xg + \u03b3\u03b3(x,K) (xg \u2212 x) \u2208 1 2\u03b2K."}, {"heading": "2.1 Non-stochastic bandit algorithms", "text": "Define the following\n(pt, vt, \u03c3t)\u2190 A(S, {pt\u22121, f1:t\u22121})\npt: A probability distribution over the discrete set S vt: Estimation of the values of F t = \u2211t i=1 fi on S. \u03c3t: Variance, such that for every x \u2208 S, vt(x)\u2212 \u03c3t(x) \u2264 Ft(x) \u2264 vt(x) + \u03c3t(x). For xt picking according to distribution pt, define the regret of A as:\nRT = \u2211 t ft(xt)\u2212min x {\u2211 t ft(x) }\nThe following theorem was essentially established in [5] (although the original version was stated for gains instead of losses, and had known horizon parameter), for the algorithm called EXP3.P, which is given in Appendix 8 for completeness:\nTheorem 2.1 ([5]). Algorithm EXP3.P over N arms guarantees that with probability at least 1\u2212 \u03b4,\nRT = \u2211 t ft(xt)\u2212min x {\u2211 t ft(x) } \u2264 8 \u221a TN log TN \u03b4"}, {"heading": "3 The insufficiency of convex regression", "text": "Before proceeding to give the main technical contributions of this paper, we give some description of the technical difficulties that are encountered and intuition as to how they are resolved.\nA natural approach for BCO, and generally for online learning, is to borrow ideas from the less general setting of stochastic zero-order optimization. Till recently, the only polynomial time algorithm for zero-order optimization was based on the ellipsoid method [16]. Roughly speaking, the idea is to maintain a subset, usually an ellipsoid, in space in which the minimum resides, and iteratively reduce the volume of this region till it is ultimately found.\nIn order to reduce the volume of the ellipsoid one has to find a hyperplane separating the minimum and a large constant fraction of the current ellipsoid in terms of volume. In the stochastic case, such a hyperplane can be found by sampling and estimating a sufficiently indicative region of space. A simple way to estimate the\nunderlying convex function in the stochastic setting is called convex regression (although much more time and query-efficient methods are known, e.g. [4]).\nFormally, given noisy observations from a convex function f : K 7\u2192 Rd, denoted {v(x1), ..., v(xn)}, such that v(xi) is a random variable whose expectation is f(xi), the problem of convex regression is to create an estimator of the value of f over the entire space which is consistent, i.e. approaches its expectation as the number of observations increases n 7\u2192 \u221e. The methodology of convex regression proceeds by solving a convex program to minimize the mean square error and ensuring convexity by adding gradient constraints, formally,\nmin n\u2211 i=1 (v(xi)\u2212 yi)2 yj \u2265 yi +\u2207>i (xj \u2212 xi)\nIn this convex program {\u2207i, yi} are variables, points xi are chosen by the algorithm designer to observe, and v(xi) the observed values from sampling. Intuitively, there are nd + n degrees of freedom (n scalars and n vectors in d dimensions) and O(n2) constraints, which ensures that this convex program has a unique solution and generates a consistent estimator for the values of f w.h.p. (see [22] for more details).\nThe natural approach of iteratively applying convex regression to find a separating hyperplane within an ellipsoid algorithm fails for BCO because of the following difficulties:\n1. The ellipsoid method was thus far not applied successfully in online learning, since the optimum is not fixed and can change in response to the algorithms\u2019 behavior. Even within a particular ellipsoid, the optimal strategy is not stationary.\n2. Estimation using convex regression over a fixed grid is insufficient, since arbitrarily deep \u201cvalleys\u201d can hide between the grid points.\nOur algorithm and analysis below indeed follows the general ellipsoidal scheme, and overcomes these difficulties by:\n1. The ellipsoid method is applied with an optional \u201crestart button\u201d. If the algorithm finds that the optimum is not within the current ellipsoidal set, it restarts from scratch. We show that by the time this happens, the algorithm has accumulated so much negative regret that it only helps the player. Further, inside each ellipsoid we use the standard multiarmed bandit algorithm EXP3.P due to [5], to exploit and explore it.\n2. A new estimation procedure is required to ensure that no valleys are missed. For this reason we develop some new machinery in convex geometry and convex regression that we call the lower convex envelope of a function. This is a convex lower bound on the original function that ensures there are no valleys missed, and in addition needs only constant-precision grids for being consistent with the original function.\nThis contribution is the most technical part of the paper, as culminates in the \u201ddiscretization lemma\u201d, and can be skimmed at first read."}, {"heading": "4 Geometry of discrete convex function", "text": ""}, {"heading": "4.1 Lower convex envelopes of continuous and discrete convex functions", "text": "Bandit algorithms generate a discrete set of evaluations, which we have to turn into convex functions. The technical definitions that allow this are called lower convex envelopes (LCE), which we define below. First, for continuous but non-convex function f , we can define the LCE denoted FLCE(f) as the maximal convex function that bounds f from below, or formally,\nDefinition 4.1 (Simple Lower Convex Envelope). Given a function f : K \u2192 R (not necessarily convex) where K \u2282 Rd, the simple lower convex envelope FSLCE = SLCE(f) : K \u2192 R is a convex function defined as:\nFSLCE(x) = min { s\u2211 i=1 \u03bbif(yi) \u2223\u2223\u2223\u2223\u2223 \u2203s \u2208 N\u2217, y1, ..., ys \u2208 K : \u2203(\u03bb1, ..., \u03bbs) \u2208 \u2206s, x = \u2211 i \u03bbiyi }\nIt can be seen that FSLCE is always convex, by showing for every x, y \u2208 K that f( 12x + 1 2y) \u2264 1 2f(x) + 1 2f(y), which follows from the definition. Further, for a convex function, FSLCE(f) = f , since for a convex function any convex combination of points satisfy f( \u2211 i \u03bbiyi) \u2264 \u2211 i \u03bbif(yi), and the minimum in the definition is realized at the point x itself. For a discrete function, the SLCE is defined to be the SLCE of the piecewise linear continuation. We will henceforth need a significant generalization of this notion, both for the setting above, and for the setting in which the discrete function is given as a random variable - on each point in the grid we have a value estimation and variance estimate. We first define the minimal extension, and then the SLCE of this minimal extension.\nDefinition 4.2 (Random Discrete Function). A Random Discrete Function (RDF), denoted (X, v, \u03c3), is a mapping f : X \u2192 R2 on a discrete domain X = {x1, ..., xk} \u2286 K \u2286 Rd, and range of values and variances denoted {v(x), \u03c3(x), x \u2208 X} such that f(xi) = (v(xi), \u03c3(xi)).\nDefinition 4.3 (Minimal Extension of a Random Discrete Function). Given a RDF (X, v, \u03c3), we define f\u0303 imin(X, v, \u03c3) : K \u2192 R as\nf\u0303 imin(x) = min h\u2208Rd:\u2200xj ,\u3008h,xj\u2212xi\u3009\u2264v(xj)+\u03c3(xj)\u2212[v(xi)\u2212\u03c3(xi)] {\u3008h, x\u2212 xi\u3009+ [v(xi)\u2212 \u03c3(xi)]}\nThe minimal extension f\u0303min(X, v, \u03c3) is now defined as\nf\u0303min(x) = max i\u2208[k]\nf\u0303 imin(x)\nWe can now define the LCE of a discrete random function\nDefinition 4.4 (Lower convex envelope of a random discrete function). Given a RDF (X, v, \u03c3) over domain X = grid \u2286 K \u2286 Rd, for the grid for K as constructed in Algorithm 1, its lower convex envelope is defined to be\nFLCE(X, v, \u03c3) = FSLCE(f\u0303min(X, v, \u03c3))\nWe now address the question of computation of an LCE of a discrete function, or how to provide oracle access to the LCE efficiently. The following theorem and algorithm establish the computational part of this section, whose proof is deferred to the appendix.\nAlgorithm 2 Fit-LCE 1: Input: RDF (X, v, \u03c3), and a convex set K where X \u2286 K. 2: (minimal extension): Compute the minimal extension f\u0303min(X, v, \u03c3) : CK \u2192 R (see Section 2 for definition\nof the bounding box C) 3: (LCE) Compute and return FLCE = SLCE(f\u0303min).\nTheorem 4.1 (LCE computation). Given a discrete random function over k points {x1, ..., xk} in a polytope K \u2286 Rd defined by N = poly(d) halfspaces, with confidence intervals [v(xi)\u2212 \u03c3(xi), v(xi) + \u03c3(xi)] for each point xi, then for every x \u2208 K, the value FLCE(x) can be computed in time O ( kd 2 )\nTo prove the running time of LCE computation, we need the following Lemma:\nLemma 4.1 (LCE properties). The lower convex envelope (LEC) has the follow properties:\n1. f\u0303min is a piece-wise linear function with kO(d 2) different regions, each region is a polytope with d + 1\nvertices. We denote all the vertices of all regions as v1, ..., vn where n = kO(d 2), where each vi and its value f\u0303min(vi) are computable in time kO(d 2).\n2.\nFLCE(x) = min \u2211 i\u2208[n] \u03bbif\u0303min(vi) \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208[n] \u03bbivi = x, (\u03bb1, ..., \u03bbn) \u2208 \u2206n \nProof. Recall the definition of f\u0303 imin : K \u2192 R as\nf\u0303 imin(x) = min h\u2208Rd:\u2200xj ,\u3008h,xj\u2212xi\u3009\u2264v(xj)+\u03c3(xj)\u2212[v(xi)\u2212\u03c3(xi)] {\u3008h, x\u2212 xi\u3009+ [v(xi)\u2212 \u03c3(xi)]}\nThe vector h in the above expression is the result of a linear program. Therefore, it belongs to the vertex set of the polyhedral set given by the inequalities \u3008h, xj \u2212 xi\u3009 \u2264 v(xj) + \u03c3(xj)\u2212 [v(xi)\u2212 \u03c3(xi)], or the objective is unbounded, a case which we can ignore since f\u0303min is finite. The number of vertices of a polyhedral set in Rd defined by k hyperplanes is bounded by ( k d ) \u2264 kd.\nThus, f\u0303 imin is the minimal of a finite set of linear functions at any point in space. This implies that it is a piecewise linear function with at most kd regions. More generally, the minimum of s linear functions is a piece-wise linear function of at most s regions, as we now prove:\nLemma 4.1. The minimum (or maximum) of s linear functions is a piecewise linear function with at most s regions.\nProof. Let f(x) = mini\u2208[s] fi(x) for linear functions {fi}, the proof for maxi\u2208[s] fi(x) is analoguous. Consider the sets Si = {x | f(x) = fi(x)}, inside which f = fi is linear. It suffices to show that each Si is a convex set, and thus each Si is a polyhedral region with at most s faces. Now suppose x1, x2 \u2208 Si, we want to argue that x3 = x1+x22 \u2208 Si: Observe that for every j, fj(x3) = fj(x1)+fj(x2) 2 (this is because fj is linear). If there is a j such that fj(x3) < fi(x3), then either fj(x1) < fi(x1) or fj(x2) < fi(x2), contradict to the fact that x1, x2 \u2208 Si.\nNext we consider f\u0303min(x) = max\ni\u2208[k] f\u0303 imin(x)\nRecall that each f\u0303 imin is piecewise linear with s = k d regions who are determined by at most s hyperplanes. Consider regions in which all these functions are jointly linear, we would like to bound the number of such regions. These regions are created by the hyperplanes that create the regions of the functions f\u0303 imin, a total of at most ks hyperplanes, plus N hyperplanes of the bounding polytope K. The number of regions these hyperplanes create is at most (N + ks)2 [1]. In each such region, the functions f\u0303 imin are linear, and according to the previous lemma there are at most k sub-regions, giving a total of k\u00d7 (N+ks)2 \u2264 kN2 +k3d polyhedral regions within which the function f\u0303min is linear.\nThe vertices of these regions can be computed by taking all d intersections of the (N + ks)2 hyperplanes and solving a system of d equations, in overall time (N + ks)2d = kO(d\n2). 2. By definition of FLCE, there exists points p1, ..., pm \u2208 K and (\u03bb1, ..., \u03bbm) \u2208 \u2206m such that\nFLCE(x) = \u2211 i\u2208[m] \u03bbif\u0303min(pi), \u2211 i\u2208[m] \u03bbipi = x (1)\nBy part 1, f\u0303min is a piece-wise linear function, we know that for every i \u2208 [m], there exists d + 1 vertices vi1 , ..., vid+1 such that there exists (\u03bbi1 , ..., \u03bbid+1) \u2208 \u2206d+1 with \u2211 j\u2208[d+1] \u03bbij f\u0303min(vij ) = f\u0303min(pi), \u2211 j\u2208[d+1] \u03bbijvij = pi. Put it into Equation 1 we get the result.\nHaving Lemma 4.1, we can calculate FLCE by first finding vertices v1, ..., vn and then solve an LP on \u03bbi. The algorithm runs in time kO(d 2)"}, {"heading": "5 The discretization lemma", "text": "The tools for discrete convex geometry developed in the previous section, and in particular the lower convex envelope, are culminated in the discretization lemma that shows consistency of the LCE for discrete random functions which we prove in this section.\nInformally, the discretization lemma asserts that for any value of a given RDF, the LCE has a point with value at least as large not too far away. Convexity is crucial for this lemma to be true at all, as demonstrated in Figure 3.\nWe now turn to a precise statement of this lemma and its proof:\nLemma 5.1 (Discretization). . Let (X, v, \u03c3) be a RDF on X = Zd \u2229 K such that v, \u03c3 are non-negative, moreover, for all x \u2208 X, v(x) \u2212 (8d2 + 1)\u03c3(x) \u2265 0. Assume further that there exists a convex function F : Rd 7\u2192 R such that for all x \u2208 X , F (x) \u2208 [v(x) \u2212 \u03c3(x), v(x) + \u03c3(x)]. Let K\u2032 = CK be the enclosing bounding box for K such that B24d2 (0) \u2286 4 d2K\n\u2032 \u2286 K \u2286 K\u2032 5. Define FLCE = LCE(X, v, \u03c3) : K\u2032 \u2192 R as in Definition 4.4.\nThen there exists a value r = 23d 2 such that for every y \u2208 14K with Br(y) \u2286 K, there exists a point y\u2032 \u2208 Br(y) with FLCE(y\u2032) \u2265 12F (y)."}, {"heading": "5.1 Proof intuition in one dimension", "text": "The discretization lemma is the main technical challenge of our result, and as such we first give intuition for the proof for one dimension, for readability purposes only, and for the special case that the input DRF is actually a deterministic function (i.e. all variances are zero, and v(xi) = F (xi) for a convex function F . The full proof is deferred to the appendix.\nProof. Please refer to Figure 4 for an illustration.\nAssume w.l.o.g. that y \u2208 Z , otherwise take the nearest point. Assume w.l.o.g that f \u2032(y) > 0, and thus all points x > y have value larger than F (y). Consider the discrete points {y = x\u22121, x0, x1, ...., }, and the value of f\u0303min on these integer points, which by definition has to be equal to F , and thus larger than F (y).\nSince F is increasing in the positive direction, we have that f\u0303min(x0) \u2264 f\u0303min(x1), and by the definition of f\u0303min, the gradient from x0 to x1, implies that\n\u2200z \u2265 x1, f\u0303min(z) \u2265 f\u0303min(x0)\nIn the open interval [x2,\u221e), the value of the LCE is by definition a convex combination of values f\u0303min(x) only for points in the range x \u2208 [x1,\u221e). Thus, the function FLCE obtains a value larger than f\u0303min(x1) \u2265 F (y) on all points within this range, which is within a distance of two from y.\nThe proof of the Discretization Lemma requires the following lemmas:\nLemma 5.2 (Convex cover). For every k \u2208 N\u2217, r \u2208 R\u2217, if k convex sets S1, ...,Sk covers a ball in Rd of radius r, then there exists a set Si that contains a ball of radius rkdd .\n5John\u2019s theorem implies 1 d3/2 K\u2032 \u2286 K \u2286 K\u2032 for any convex body K\nProof of Lemma 5.2. Consider the maximum volume contained Ellipsoid Ei of Si \u2229 Br(0), we know that the volume of Ei is at least 1/dd the volume of Si \u2229 Br(0). Now, since S1, ...,Sk covers Br(0), there exists a set Si \u2229 Br(0) of volume at least 1/k fraction of the volume of Br(0). Which implies that Ei has volume at least 1/(kdd) of Br(0), note that Ei \u2286 Br(0), therefore, it contains a ball of radius rkdd .\nLemma 5.3 (Approximation of polytope with integer points). Suppose a polytope Po = conv{v1, ..., vd+1} \u2286 Rd contains B4d8(0), then there exists d+ 1 integer points g1, ..., gd+1 \u2208 2d2Po such that:\n1. Let (\u03bb1, ..., \u03bbd+1) \u2208 \u2206d+1 be the coefficient such that \u2211 i \u03bbivi = 0, then there exists (\u03bb \u2032 1, ..., \u03bb \u2032 d+1) \u2208\n\u2206d+1 such that \u2211 i \u03bb \u2032 igi = 0. Moreover, 1 2\u03bb \u2032 i \u2264 \u03bbi \u2264 2\u03bb\u2032i\n2. For every i \u2208 [d+ 1], there exists {\u03bbij \u2208 \u2206d+1} such that \u03bbii \u2265 12d2 and\ngi = \u03bb i ivi + \u2211 j 6=i \u03bbijgj\nProof of Lemma 5.3. Property 1:\nLet ui = 1d2 vi. For every i \u2208 [n], since B4d8(0) \u2286 conv{v1, ..., vd+1}, it holds that\nBd (ui) \u2286 conv{v1, ..., vd+1}\nTherefore, we can find integer points around ui in conv{v1, ..., vd+1}. Now, let gi be the closest integer point to ui, which has distance at most d to ui, i.e. \u2016gi \u2212 ui\u20162 \u2264 d. Observe that\nBd6 (0) \u2286 conv{2u1, ..., 2ud+1}\nWhich implies that for every i \u2208 [d+1], Bd6/2 (ui) \u2286 conv{2u1, ..., 2ud+1}. Therefore, gi \u2208 conv{2u1, ..., 2ud+1} = 2 d2Po\nNow we want to show that 0 \u2208 conv{g1, ..., gd+1}. Consider a function f : conv{u1, u2, ..., ud+1} \u2192 Rd defined as: for x = \u2211 i \u03bb \u2032 iui where (\u03bb \u2032 1, \u03bb \u2032 2, ..., \u03bb \u2032 d+1) \u2208\n\u2206d+1: f(x) = \u2211 i \u03bb\u2032igi\nObserve that\n\u2016f(x)\u2212 x\u20162 = \u2225\u2225\u2225\u2225\u2225 d+1\u2211 i=1 \u03bb\u2032i (ui \u2212 gi) \u2225\u2225\u2225\u2225\u2225 2 \u2264 d\nNotice that for x1, x2 \u2208 conv{u1, u2, ..., ud+1},\nf\n( x1 + x2\n2\n) = f(x1) + f(x2)\n2\nWhich implies that f is a linear transformation. Moreover, Bd2(0) \u2286 conv{u1, u2, ..., ud+1}. Therefore, f(Bd2(0)) = \u222ax\u2208Bd2 (0){f(x)} is a convex set, since a linear transformation preserves convexity.\nNow, we want to show that 0 \u2208 f(Bd2(0)). Suppose on the contrary 0 /\u2208 f(Bd2(0)), then we know there is a separating hyperplane going through 0 that separates 0 and f(Bd2(0)). Which implies that there is a point g\u2032 \u2208 \u2202Bd2(0) such that\ndist(g\u2032, f(Bd2(0))) = min x\u2208f(Bd2 (0)) \u2016x\u2212 g\u2032\u2016 \u2265 d2\nIn particular, since f(g\u2032) \u2208 f(Bd2(0)), the above equality implies dist(g\u2032, f(g\u2032)) \u2265 d2, in contradiction to \u2016f(x)\u2212 x\u20162 \u2264 d. Therefore, 0 \u2208 f(Bd2(0)) \u2286 conv{g1, ..., gd}.\nWe proceed to argue about the coefficients. Denote gi = ui+bi, and by the above \u2016bi\u20162 \u2264 d. By symmetry it suffices to show that 12\u03bb \u2032 1 \u2264 \u03bb1 \u2264 2\u03bb\u20321.\nLet {\u03bb\u2032i} \u2208 \u2206d+1 be such that \u2211 i \u03bb \u2032 igi = \u2211 i \u03bb \u2032 i(ui + bi) = 0. Then\u2211\ni \u03bb\u2032iui = \u2212 \u2211 i \u03bb\u2032ibi\nSince \u2016bi\u20162 \u2264 d, by the triangle inequality it holds that \u2016 \u2211 i \u03bb \u2032 ibi\u20162 \u2264 d, which implies\u2225\u2225\u2225\u2225\u2225\u2211\ni\n\u03bb\u2032iui \u2225\u2225\u2225\u2225\u2225 \u2264 d Let H be the hyperplane going through u2, ..., ud+1. Without lost of generality, we can apply a proper rotation (unitary transformation) to put H = {x1 = \u2212a} for some value a > 0, where x1 denotes the first axis. Now, (after rotation) Define b = (b1, ..., bd) = \u2211 i \u03bb \u2032 iui and denote u1 = (a1, ..., ad). The point b is a\nconvex combination of u1 and c := 11\u2212\u03bb\u20321 \u2211 j\u22652 \u03bb \u2032 juj . In addition we know that c1 = \u2212a. Thus, we can write \u03bb\u20321 as:\n\u03bb\u20321 = b1 \u2212 c1 (u1)1 \u2212 c1 = b1 + a a1 + a\nOn the other hand, by \u2211 i \u03bbiui = 0, we know that\n\u03bb1 = a\na1 + a\nNote that \u2016b\u20162 \u2264 d, which implies |b1| < d. However, by assumption there is a ball centered at 0 of radius 4d6 in conv{u1, ..., ud+1}, which implies a \u2265 4d6 \u2265 4|b1|.\nTherefore 12\u03bb \u2032 1 \u2264 \u03bb1 \u2264 2\u03bb\u20321.\nProperty 2: By symmetry, it suffices to show for v1. there exists \u03bb11 \u2265 12d2 and \u03bb\n1 j \u2265 0(j = 2, 3, ..., d + 1), \u03bb11 +\u2211d+1\nj=2 \u03bb 1 j = 1 such that\ng1 = \u03bb 1 1v1 + d+1\u2211 j=2 \u03bb1jgj\nConsider a function f : conv{v1, u2, ..., ud+1} \u2192 Rd defined as: for x = \u03bb\u2032v1 + \u2211d+1 j=2 \u03bb \u2032 juj where\n(\u03bb\u2032, \u03bb\u20322, ..., \u03bb \u2032 d+1) \u2208 \u2206d+1:\nf(x) = \u03bb\u2032vi + d+1\u2211 j=2 \u03bb\u2032jgj\nObserve that\n\u2016f(x)\u2212 x\u20162 = \u2225\u2225\u2225\u2225\u2225\u2225 d+1\u2211 j=2 \u03bb\u2032j (uj \u2212 gj) \u2225\u2225\u2225\u2225\u2225\u2225 2 \u2264 d\nNotice that for x1, x2 \u2208 conv{v1, u2, ..., ud+1},\nf\n( x1 + x2\n2\n) = f(x1) + f(x2)\n2\nWhich implies that f is a linear transformation. Moreover, Bd2(g1) \u2286 B2d2 (u1) \u2286 conv{v1, u2, ..., ud+1}. Therefore, f(Bd2(g1)) = \u222ax\u2208Bd2 (g1){f(x)} is a convex set, since a linear transformation preserves convexity.\nNow, we want to show that g1 \u2208 f(Bd2(g1)). Suppose on the contrary g1 /\u2208 f(Bd2(g1)), then we know there is a separating hyperplane going through g1 that separates g1 and f(Bd2(g1)). Which implies that there is a point g\u2032 \u2208 Bd2(g1) such that\ndist(g\u2032, f(Bd2(g1))) = min x\u2208f(Bd2 (g1)) \u2016x\u2212 g\u2032\u2016 = d2\nIn particular, since f(g\u2032) \u2208 f(Bd2(g1)), the above equality implies dist(g\u2032, f(g\u2032)) \u2265 d2, in contradiction to \u2016f(x)\u2212 x\u20162 \u2264 d for all x \u2208 conv{v1, u2, ..., ud+1}.\nTherefore, there is a point g \u2208 Bd2(g1) such that f(g) = g1, i.e. g1 can be written as\ng1 = \u03bb \u2032v1 + d+1\u2211 j=2 \u03bb\u2032jgj (\u03bb \u2032, \u03bb\u20322, ..., \u03bb \u2032 d+1) \u2208 \u2206d+1\nWe proceed to give a bound on the coefficients. Since g1 = f(g), we know that\ng = \u03bb\u2032v1 + d+1\u2211 j=2 \u03bb\u2032juj\nOn the other hand, observe that (since \u2211 j \u03bbjuj = 0 as defined in Property 1)\nu1 = 1\nd2 v1 +\n( 1\u2212 1\nd2 ) d+1\u2211 j=1 \u03bbjuj\nBy \u2016g \u2212 u1\u20162 \u2264 2d2, using the same method as Property 1 we can obtain: \u03bb\u2032 \u2265 12d2 Which completes the proof.\nNow we can prove the discretization Lemma. The proof goes by the following steps:\n1. First, suppose the Lemma does not hold, then we can find a large hypercube that is contained inside K\u2032 and has entirely small LCE compared to the value of the point y.\n2. We proceed to identify the points whose f\u0303min value is associated with the LCE of the large hypercube, these f\u0303min have small values (compare to F (y)) and span a large region.\n3. We find a simplex of d + 1 points that span a large region in which the same holds, i.e. f\u0303min value compared to v(y).\n4. Using the approximation Lemma, we find an inner simplex of d + 1 discrete points inside the previous simplex. These discrete points all have f\u0303min value larger than f(y) by the fact that they are inside the first large region.\n5. We use the definition of f\u0303min to show that one of the vertices of the outer simplex has value of f\u0303min larger than f(y), in contradiction to the previous observations.\nProof of Lemma 5.1. Step 1:\nConsider a point y \u2208 P with Br(y) \u2208 K. By convexity of F , there is a hyperplane H going y such that on one side of the hyperplane, all the points have larger or equal F value than F (y). Therefore, there exists a point y\u2032, a cube Qr\u2032(y\u2032) \u2282 Br(y) centered at y\u2032 with radius r\u2032 = r2\u221ad such that for all integer points z \u2208 Qr\u2032(y\n\u2032), F (z) \u2265 F (y). Let v1, ..., v2d be the vertex of this cube.\nIf there exists i \u2208 [2d] such that FLCE(vi) \u2265 12F (y), then we already conclude the proof. Therefore, we can assume that for all i \u2208 [2d], FLCE(vi) < 12F (y). Step 2:\nBy the definition of FLCE, we know that for every i \u2208 [2d], there exists pi,1, ...., pi,m \u2208 K\u2032 such that vi = \u2211 j \u03bbi,jpi,j , (\u03bbi,1, ..., \u03bbi,m) \u2208 \u2206m with\nFLCE(vi) = \u2211 \u03bbi,jF\u0303min(pi,j) < 1\n2 F (y)\nMoreover, by Carathodory\u2019s theorem 6, we can make m = d+ 1. Now we get a set of (d + 1)2d many points Po = {pi,j}i\u2208[2d],j\u2208[d+1]. Consider a size d + 1 subset\nJ = {pi1,j1 , ..., pid+1,jd+1} of Po, define convex set\nSJ = { x \u2208 Qr\u2032(y\u2032) | \u2203(\u03bb1, ..., \u03bbd+1) \u2208 \u2206d+1 : x =\n\u2211 s \u03bbspis,js , \u2211 s \u03bbsF\u0303min(pis,js) < 1 2 F (y)\n}\nWe claim that \u22c3 J\u2286P0,|J|=d+1 SJ = Qr\u2032(y\u2032)\nThis is because for every x \u2208 Qr\u2032(y\u2032), there exists vi1 , ..., vid+1 and \u03bb1, ..., \u03bbd+1 \u2208 \u2206d+1 such that\u2211 s\u2208[d+1] \u03bbsvis = x\nMoreover, for each vi, vi = \u2211 j \u03bbi,jpi,j . Therefore:\nx = \u2211\ns\u2208[d+1]\n\u03bbs \u2211 j \u03bbis,jpis,j  On the other hand, \u2211 s\u2208[d+1] \u03bbs (\u2211 j \u03bbis,jF\u0303min(pis,j) ) < 12F (y). By Carathodory\u2019s theorem, we can\nmake the sum only contains d+ 1 such pis,j , which proves the claim.\nStep 3: By lemma 5.2, we know that there exists J\u2217 such that SJ\u2217 contains a ball Br\u2032\u2032(y\u2032\u2032) inside Qr\u2032(y\u2032) of radius\nr\u2032\u2032 = r \u2032\n2 \u221a dkdd\nwhere k = ( (d+1)2d\nd+1\n) \u2264 22d2 and y\u2032\u2032 is an integer point.\nFor simplicity, we denote J\u2217 = {p1, ..., pd+1}. By the definition of SJ\u2217 , there exists (\u03bb\u2032\u20321 , ..., \u03bb\u2032\u2032d+1) \u2208 \u2206d+1 such that\n1. \u2211 i \u03bb\u2032\u2032i pi = y \u2032\u2032\n6The original Carathedory\u2019s theorem only states for convex combination of points, but the same proof can be extended to convex functions by looking at the graph of the function\n2. \u2211 i \u03bb\u2032\u2032i F\u0303min(pi) < 1 2 F (y)\nStep 4:LetP = conv{p1, ..., pd+1}with center y\u2032\u2032. The above argument implies that Br\u2032\u2032(y\u2032\u2032) \u2286 conv{p1, ..., pd+1}, when r\u2032\u2032 = r \u2032\n2 \u221a dkdd \u2265 r (2d)2d2 \u2265 4d8. By lemma 5.3, there exists integer points g1, ..., gd+1 \u2208 2d2P (where 2 d2P denotes shrink P of factor 2 d2 according to center y \u2032\u2032) with\n1. y\u2032\u2032 \u2208 conv{g1, ..., gd+1}\n2. For every i \u2208 [d+ 1], there exists {\u03bbij \u2208 \u2206d+1} such that \u03bbii \u2265 12d2 and\ngi = \u03bb i ipi + \u2211 j 6=i \u03bbijgj\nThe conditions of the lemma assert that 2d2K \u2032 \u2286 12K, by y \u2032\u2032 \u2208 12K, P \u2286 K \u2032, we know that 2d2P \u2286 K. This implies that gi \u2208 Zd \u2229 K, over which the RDF is defined, and we have values v(gi) and \u03c3(gi) to construct F\u0303min. Step 5: By the fact that gi = \u03bbipii + \u2211 j 6=i \u03bb i jgj and the definition of F\u0303min, we know that\nF\u0303min(pi) \u2265 1\n\u03bbii v(gi)\u2212 \u03c3(gi)\u2212\u2211 j 6=i \u03bbij [v(gj) + \u03c3(gj)]  Let us write y\u2032\u2032 = \u2211 i \u03bb \u2032 igi : (\u03bb \u2032 1, ..., \u03bb\n\u2032 d+1) \u2208 \u2206d+1. By the fact that pi = 1\u03bbii (gi \u2212\n\u2211 j 6=i \u03bb i jgj) We can\ncalculate:\ny\u2032\u2032 = \u2211 i \u03bb\u2032\u2032i pi, where \u03bb\u2032\u2032i \u03bbii \u2212 \u2211 j 6=i \u03bb\u2032\u2032j \u03bb j i \u03bbjj = \u03bb\u2032i\nFrom Lemma 5.3 we also obtain that \u03bb\u2032\u2032i \u2264 2\u03bb\u2032i. Moreover, for the interpolation:\u2211\ni\n\u03bb\u2032\u2032i F\u0303min(pi)\n\u2265 \u2211 i \u03bb\u2032\u2032i \u03bbii v(gi)\u2212 \u03c3(gi)\u2212\u2211 j 6=i \u03bbij [v(gj) + \u03c3(gj)]  =\n\u2211 i \u03bb\u2032\u2032i \u03bbii v(gi) + \u03c3(gi)\u2212\u2211 j 6=i \u03bbij [v(gj) + \u03c3(gj)] \u2212 2\u2211 i \u03bb\u2032\u2032i \u03bbii \u03c3(gi)\n= \u2211 i \u03bb\u2032\u2032i \u03bbii \u2212 \u2211 j 6=i \u03bb\u2032\u2032j \u03bb j i \u03bbjj  [v(gi) + \u03c3(gi)] \u2212 2\u2211 i \u03bb\u2032\u2032i \u03bbii \u03c3(gi)\n= \u2211 i \u03bb\u2032i[v(gi) + \u03c3(gi)]\u2212 2 \u2211 i \u03bb\u2032\u2032i \u03bbii \u03c3(gi)\n\u2265 \u2211 i \u03bb\u2032i[v(gi) + \u03c3(gi)]\u2212 4d2 \u2211 i \u03bb\u2032i\u03c3(gi) since \u03bb i i \u2265 1d2 ,\u03bb \u2032\u2032 i \u2264 2\u03bb\u2032i\n= \u2211 i \u03bb\u2032i[v(gi)\u2212 (4d2 \u2212 1)\u03c3(gi)]\nBy assumption, since gi is a integer point, we get v(gi)\u2212 (8d2 + 1)\u03c3(gi) \u2265 0\nv(gi)\u2212 (4d2 \u2212 1)\u03c3(gi) \u2265 v(gi) + \u03c3(gi)\u2212 4d2\u03c3(gi) \u2265 F (gi)\u2212 4d2\u03c3(gi) since by definition v(gi) + \u03c3(gi) \u2265 F (gi)\n\u2265 F (gi)\u2212 v(gi)\u2212 \u03c3(gi)\n2 since (8d2 + 1)\u03c3(gi) \u2264 v(gi)\n\u2265 1 2 F (gi) since by definition v(gi)\u2212 \u03c3(gi) \u2264 F (gi)\nNote that by the convexity of F , \u2211 i \u03bb \u2032 iF (gi) \u2265 F (y\u2032\u2032) \u2265 F (y) (last inequality is due to our choice of y\u2032\u2032).\nThus, \u2211 i \u03bb\u2032\u2032i F\u0303min(pi) \u2265 \u2211 i 1 2 \u03bb\u2032iF (gi) \u2265 1 2 F (y)\nBy contradiction we complete the proof."}, {"heading": "6 Algorithm and statement of results", "text": ""}, {"heading": "6.1 Algorithm statement and parameter setting", "text": "1. \u03b4 > 0 - an upper bound on the failure probability of the algorithm\n2. The desired regret bound ` = ( 2d 4\n(log T )2d log 1\u03b4\n)\u221a T\n3. resolution of the grid: \u03b1 = 23d 2 log3 T \u2265 \u03b3\u03b22 \u221a d.\n4. Scaling factor \u03b2 = 4096d4 log T .\n5. Extension ratio: \u03b3 = 2048d4 log T .\n6. Blow up factor \u03b7 = 8d2 + 1.\n7. Upper bound on the number of epoch \u03c4 \u2264 8d2 log T .\nThis algorithm calls upon two subroutines, FitLCE which was defined in section 4, and ShrinkSet which we now define.\nAlgorithm 3 Bandit Ellipsoid 1: Input: A convex set K \u2286 Rd, A: a high-probability low regret bandit algorithm on discrete set of points 2: Initialize: Epoch \u03c4 = 0, epoch set \u0393\u03c4 = \u2205, K\u03c4 = K, Grid grid\u03c4 = grid(\u03b2K\u03c4 \u2229 K) 3: for t = 1 to T do 4: Apply the low-regret algorithm on current grid:\n(pt, vt, \u03c3t)\u2190 A(grid\u03c4 , {pi, xi, fi(xi) | i \u2208 \u0393\u03c4})\nwhere pt, vt, \u03c3t are defined as in section 2.1. 5: Play a point xt \u2208 grid\u03c4 from distribution pt(xt), observe value ft(xt). Set \u0393\u03c4 = \u0393\u03c4 \u222a {t}. 6: (Shift): Shift vt by a constant so that minx\u2208grid\u03c4 {vt(x)\u2212 \u03b7\u03c3t(x)} = 0, for simplicity we just keep the\nsame notation for the new vt. Moreover, we can shift F \u03c4 = \u2211 j\u2208\u0393\u03c4 fj by the same constant and assume that adversary presents us the (shifted) fj . For simplicity we also keep the same notation for the new F \u03c4 .\n7: Compute F \u03c4LCE = FitLCE(\u03b2K\u03c4 \u2229 K, [grid\u03c4 , v\u03c4 , \u03c3\u03c4 ]). 8: if \u2200x \u2208 K\u03c4 ,\u2203j \u2264 \u03c4, F jLCE(x) > ` 4 then\n9: RESTART (goto Initialize) 10: end if 11: if (DecideMove) \u2203x\u0303\u03c4 \u2208 K\u03c4\u03b2 such that F \u03c4 LCE(x\u0303\u03c4 ) \u2265 ` then 12: K\u03c4+1 = ShrinkSet(K\u03c4 , x\u0303\u03c4 , F \u03c4LCE, grid\u03c4 , {vt, \u03c3t}) 13: Set \u0393\u03c4+1 = \u2205, grid\u03c4+1 = grid(\u03b2K\u03c4+1 \u2229 K, \u03b1), \u03c4 = \u03c4 + 1. 14: end if 15: end for\nAlgorithm 4 ShrinkSet 1: Input: Convex set K\u03c4 , convex function F \u03c4LCE, point x\u0303\u03c4 \u2208 K\u03c4 , Grid grid\u03c4 , value estimation vt and variance\nestimation \u03c3t. 2: Compute a separation hyperplane H \u2032\u03c4 through x\u0303\u03c4 between x\u0303\u03c4 and {y | F \u03c4LCE(y) < `}. Assume H \u2032\u03c4 = {x | \u3008h\u03c4 , x\u3009 = w\u03c4} and {y | F \u03c4LCE(y) < `} \u2286 {y | \u3008h\u03c4 , x\u3009 \u2264 w\u03c4} 3: Let x\u03c4 be the center of the MVEE E\u03c4 of K\u03c4 . 4: (Amplify Distance). Let H\u03c4 = {x | \u3008h\u03c4 , x\u3009 = z\u03c4} for some z\u03c4 \u2265 0 such that the following holds:\n1. {y | F \u03c4LCE(y) < `} \u2286 {y | \u3008h\u03c4 , y\u3009 \u2264 z\u03c4}\n2. dist(x\u03c4 , H\u03c4 ) = 2dist(x\u03c4 , H \u2032\u03c4 ).\n3. \u3008h\u03c4 , x\u03c4 \u3009 \u2264 z\u03c4 .\n5: Return K\u03c4+1 = (K\u03c4 \u2229 {y | \u3008h\u03c4 , y\u3009 \u2264 z\u03c4})"}, {"heading": "6.2 Statement of main theorem", "text": "Theorem 6.1 (Main, full algorithm). Suppose for all time t in all epoch \u03c4 , A outputs vt and \u03c3t such that for all x \u2208 grid\u03c4 , (\u2211 j\u2208\u0393\u03c4 ,j\u2264t fj(x) ) \u2208 [vt(x)\u2212 \u03c3t(x), vt(x) + \u03c3t(x)]. Moreover, A achieves a value\nv\u03c4 (A) = \u2211\nj\u2208\u0393\u03c4 ,j\u2264t\nfj(xj) \u2264 min x\u2208grid\u03c4\n{vt(x)\u2212 \u03b7\u03c3t(x)}+ `\n1024d3 log T\nthen Algorithm 3 satisfies \u2211 t ft(xt)\u2212min x\u2217 \u2211 t ft(x \u2217) \u2264 `\nCorollary 6.1 (Exp3.P.). Algorithm 3 with A being Exp3.P satisfies the condition in Theorem 6.1 with probability 1\u2212 \u03b4 for\n` = ( 2d 4 (log T )2d log 1\n\u03b4\n)\u221a T"}, {"heading": "6.3 Running time", "text": "Our algorithm runs in time O ( (log T )poly(d) ) , which follows from Theorem 4.1 and the running time of\nExp3.P on K \u2264 (2d\u03b1)d Experts"}, {"heading": "6.4 Analysis sketch", "text": "Before going to the details, we briefly discuss the steps of the proof.\nStep 1: In the algorithm, we shift the input function so that the player can achieve a value\u2264 \u221a T . Therefore, to get the regret bound, we can just focus on the minimal value of \u2211 t ft.\nStep 2: We follow the standard Ellipsoid argument, maintaining a shrinking set, which at epoch \u03c4 is denoted K\u03c4 . We show the volume of this set decreases by a factor of at least 1 \u2212 1d , and hence the number of epochs between iterative RESTART operations can be bounded by O(d2 log T ) (when the diameter of K\u03c4 along one direction decreases below 1\u221a\nT , we do not need to further discretizate along that direction). Step 3: We will show that inside each epoch \u03c4 , for every x \u2208 K\u03c4 , \u2211 t:t in epoch \u03c4 ft(x) is lower bounded by\n\u2212 2`\u03b3 for ` \u2248 \u221a T , \u03b3 \u2265 1. For point x outside the K\u03c4 , \u2211 t:t in epoch \u03c4 ft(x) is lower bounded by \u2212 2` \u03b3 \u03b3(x,K\u03c4 ).\nStep 4: We will show that when one epoch \u03c4 ends, for every point x cut off by the separation hyperplane,\u2211 t:t in epoch \u03c4 ft(x) is lower bounded by ` 2\u03b3(x,K\u03c4 )\nStep 5: Putting the result of 3, 4 together, we know that for a point outside the current setK\u03c4 , it must be cut off by a separation hyperplane at some epoch j \u2264 \u03c4 . Moreover, we can find such j with \u03b3(x,Kj) \u2265 \u03b3(x,K\u03c4 )d . Which implies that\u2211\nt\nft(x) = \u2211\nt:t in epoch 1,2,...,j\u22121,j+1,...,\u03c4 ft(x) + \u2211 t:t in epoch j ft(x) \u2265 \u2212 2\u03c4` \u03b3 \u03b3(x,K\u03c4 ) + `\u03b3(x,K\u03c4 ) 2d \u2248 \u221a T\nBy our choice of \u03b3 \u2265 8d\u03c4 . Therefore, when the adversary wants to move the optimal outside the current set K\u03c4 , the player has zero regret. Moreover, by the result of 3, inside current set K\u03c4 , the regret is bounded by \u03c4 2`\u03b3 \u2248 \u221a T .\nThe crucial steps in our proof are Step 3 and Step 4. Here we briefly discuss about the intuition to prove the two steps.\nIntuition of Step 3: For x \u2208 K\u03c4 , we use the grid property (Property of grid, 2.1) to find a grid xg point such that xc = xg + \u03b3(xg \u2212 x) is close to the center of K\u03c4 . Since xg is a grid point, by shifting we know that\u2211\nt:t in epoch \u03c4\nft(xg) \u2265 0\nTherefore, if \u2211 t:t in epoch \u03c4 ft(x) < \u2212 2` \u03b3 , by convexity of ft, we know that \u2211 t:t in epoch \u03c4 ft(xc) \u2265 2`. Now,\napply discretization Lemma 5.1, we know that there is a point x\u2032c near xc such that F \u03c4 LCE(x \u2032 c) \u2265 `, by our DecideMove condition, the epoch \u03c4 should end. Same argument can be applied to x /\u2208 K\u03c4 . Intuition of Step 4: We use the fact that the algorithm does not RESTART, therefore, according to our condition, there is a point x0 \u2208 K\u03c4 with F \u03c4LCE(x0) \u2264 ` 4 . Observe that the separation hyperplane of our algorithm separates x0 with points whose F \u03c4LCE \u2265 `. Using the convexity of FLCE, we can show that F \u03c4LCE(x) \u2265 ` 2\u03b3(x,K\u03c4 ). Apply the fact that F \u03c4 LCE is a lower bound of \u2211 t:t in epoch \u03c4 ft we can conclude \u2211 t:t in epoch \u03c4 ft(x) \u2265 ` 2\u03b3(x,K\u03c4 ).\nNotice that here we use the convexity of FLCE, and also the fact that it is a lower bound on F (standard convex regression is not a lower bound on F , see section 3 for further discussion on this issue).\nNow we can present the proof for general dimension d To prove the main theorem we need the following lemma, starting from the following corollary of Lemma\n5.1:\nCorollary 6.2. (1). For every epoch \u03c4 , \u2200x \u2208 \u03b2K\u03c4 \u2229 K,\nF \u03c4LCE(x) \u2264 F \u03c4 (x) = \u2211 i\u2208\u0393\u03c4 fi(x)\n. (2). For every epoch \u03c4 , let x\u03c4 be the center of the MVEE of K\u03c4 , then F \u03c4 (x) = \u2211 i\u2208\u0393 fi(x) \u2264 2`.\nProof. (1) is just due to the definition of LCE. (2) is due to the Geometry Lemma on F \u03c4 : for every x \u2208 K2\u03b2 , there exists x\u2032 \u2208 ( x+ K\u03c42\u03b2 ) \u2286 K\u03c4\u03b2 such that F \u03c4 LCE(x \u2032) \u2265 12F \u03c4 (x)\nLemma 6.1 (During an epoch). During every epoch \u03c4 the following holds:\nF \u03c4 (x) \u2265  \u2212 2`\u03b3 x \u2208 K \u2229 K\u03c4\n\u2212 2\u03b3(x,K\u03c4 )`\u03b3 x \u2208 K \u2229 K c \u03c4\nLemma 6.2 (Number of epoch). There are at most 8d2 log T many epochs before RESTART.\nProof of 6.2. Let E\u03c4 be the minimal volume enclosing Ellipsoid of K\u03c4 , we will show that\nvol(E\u03c4+1) \u2264 (\n1\u2212 1 8d\n) vol(E\u03c4 )\nFirst note that K\u03c4+1 = K\u03c4 \u2229 H for some half space H corresponding to the separating hyperplane going through 1\u03b2E\u03c4 , therefore, K\u03c4+1 \u2282 E\u03c4 \u2229H.\nLet E \u2032\u03c4+1 be the minimal volume enclosing Ellipsoid of E\u03c4 \u2229H, we know that\nvol(E\u03c4 ) \u2264 vol(E \u2032\u03c4+1)\nWithout lose of generality, we can assume that E\u03c4 is centered at 0. Let A be a linear operator on Rd such that A(E\u03c4 ) is the unit ball B1(0), observe that\nvol(E \u2032\u03c4+1) vol(E\u03c4 ) = vol(AE \u2032\u03c4+1) vol(AE\u03c4 )\nSince AE \u2032\u03c4+1 is the MVEE of AE\u03c4 \u2229 AH, where AH is the halfspace corresponding to the separating hyperplane going through B 1\n\u03b2 (0). Without lose of generality, we can assume that H = {x \u2208 Rd | x1 \u2265 a} for\nsome a such that |a| \u2264 1\u03b2 \u2264 1 d2 .\nObserve that\nAE\u03c4 \u2229AH \u2286 { x \u2208 Rd | (x1 \u2212 14d ) 2(\n1\u2212 14d )2 + x221 + 112d2 + ...+ x 2 d 1 + 112d2 \u2264 1\n} = E\nTherefore,\nvol(AE \u2032\u03c4+1) \u2264 vol(E) \u2264 1\u2212 1\n8d .\nNow, observe that the algorithm will not cut through one eigenvector of the MVEE of K\u03c4 if its length is smaller than 1\u221a\nT , and the algorithm will stop when all its eigenvectors have length smaller than 1\u221a T . Therefore,\nthe algorithm will make at most\nd log1\u2212 18d ( 1\u221a T ) = 8d2 log T\nmany epochs.\nLemma 6.3 (Beginning of each epoch). For every \u03c4 \u2265 0:\n\u03c4\u22121\u2211 i=0 F i(x) \u2265  \u2212\u03c4 2`\u03b3 x \u2208 K \u2229 K\u03c4\n\u03b3(x,K\u03c4 )` 64d x \u2208 K \u2229 K c \u03c4\nLemma 6.4 (Restart). (After shifting) If A obtains a value vj(A) = \u2211 t\u2208\u0393j ft(xt) \u2264 ` 1024d3 log T for each epoch j, then when the algorithm RESTART, Regret = 0."}, {"heading": "6.5 Proof of main theorem", "text": "Now we can prove the regret bound assuming all the lemmas above, whose proof we defer to the next section.\nProof of Theorem 6.1. Using Lemma 6.4, we can only consider epochs between two RESTART. Now, for epoch \u03c4 , we know that for x \u2208 K \u2229 Kc\u03c4 , \u2211\ni\u2208\u03930\u222a...\u222a\u0393\u03c4\u22121\nfi(x) \u2265 \u03b3(x,K\u03c4 )`\n64d\n\u2211 i\u2208\u0393\u03c4 fi(x) \u2265 \u2212 2\u03b3(x,K\u03c4 )` \u03b3\nTherefore, for x \u2208 K \u2229 Kc\u03c4 \u2211 i\u2208\u03930\u222a...\u222a\u0393\u03c4 fi(x) \u2265 \u03b3(x,K\u03c4 )` ( 1 64d \u2212 2 \u03b3 ) \u2265 0\nBy our choice of \u03b3 = 2048d4 log T .\nIn the same manner, we know that for x \u2208 K \u2229 K\u03c4 ,\u2211 i\u2208\u03930\u222a...\u222a\u0393\u03c4 fi(x) \u2265 \u2212 2(\u03c4 + 1)` \u03b3 \u2265 \u2212 ` 2\nBy \u03c4 \u2264 8d2 log T . Which implies that for every x \u2208 K, \u2211 i\u2208\u03930\u222a...\u222a\u0393\u03c4 fi(x) \u2265 \u2212 ` 2 .\nDenote by vj(A) = \u2211 i\u2208\u0393j ,i\u2264t fj(xj) the overall loss incurred by the algorithm in epoch j before time t.\nThe low-regret algorithm A guarantees that in each epoch:\nvj(A) = \u2211\ni\u2208\u0393\u03c4 ,i\u2264t\nfi(xi)\n\u2264 min x\u2208grid\u03c4\n{vt(x)\u2212 \u03b7\u03c3t(x)}+ `\n1024d3 log T\n\u2264 ` 1024d3 log T\nby shifting min x\u2208grid\u03c4 {vt(x)\u2212 \u03b7\u03c3t(x)} = 0\nThus A obtains over all epochs a total value of at most\u2211 0\u2264j\u2264\u03c4 ` 1024d3 log T = (\u03c4 + 1)\u00d7 ` 1024d3 log T \u2264 ` 2 .\nTherefore, Regret = \u2211 0\u2264j\u2264\u03c4 vj(A)\u2212 \u2211 i\u2208\u03930\u222a...\u222a\u0393\u03c4 fi(x \u2217) \u2264 `"}, {"heading": "7 Analysis and proof of main lemmas", "text": ""}, {"heading": "7.1 Proof of Lemma 6.1", "text": "Proof of 6.1. Part 1: Consider any x \u2208 K\u03c4 . By Lemma 2.1 part 1, we know that there exists xg \u2208 grid\u03c4 such that xc = xg + \u03b3(x \u2212 xg) \u2208 K\u03c42\u03b2 . Any convex function f satisfies for any two points y, z that f(\u03b3x + (1 \u2212 \u03b3)y) \u2264 \u03b3f(x) + (1 \u2212 \u03b3)f(y). Applying this to the convex function F \u03c4 over the line on which the points x, xc, xg reside and observe \u03b3 = \u2016xc\u2212xg\u20162\u2016xg\u2212x\u20162 , we have\nF \u03c4 (xc)\u2212 F \u03c4 (xg) \u2265 ||xc \u2212 xg||2 ||xg \u2212 x||2 (F \u03c4 (xg)\u2212 F \u03c4 (x)) = \u03b3(F \u03c4 (xg)\u2212 F \u03c4 (x))\nSince xg \u2208 grid and we shifted all losses on the grid to be nonnegative, F \u03c4 (xg) \u2265 0. Thus, we can simplify the above to:\nF \u03c4 (xc) \u2265 \u2212\u03b3F \u03c4 (x)\nSince the epoch is ongoing, the conditions of DecideMove are not yet satisfied, and hence \u2200x\u2032 \u2208 1\u03b2K\u03c4 , F \u03c4 LCE(x \u2032) \u2264 `. By (2) of Lemma 6.2 for all points x\u2032\u2032 in 12\u03b2K\u03c4 it holds that F\n\u03c4 (x\u2032\u2032) \u2264 2`, in particular F \u03c4 (xc) \u2264 2`. The above simplifies to\nF \u03c4 (x) \u2265 \u2212 1 \u03b3 F \u03c4 (xc) \u2265 \u2212 2` \u03b3\nPart 2:\nFor x \u2208 Kc\u03c4 \u2229 K By Lemma 2.1 part 2, we know that there exists xg \u2208 grid\u03c4 such that xc = xg + \u03b3\n\u03b3(x,K\u03c4 ) (x\u2212 xg) \u2208 K\u03c4 \u03b22 . Now, by the convexity of F \u03c4 , we know that\nF \u03c4 (xc)\u2212 F \u03c4 (xg) \u2265 ||xc \u2212 xg||2 ||xg \u2212 x||2 (F \u03c4 (xg)\u2212 F \u03c4 (x)) = \u03b3 \u03b3(x,K\u03c4 ) (F \u03c4 (xg)\u2212 F \u03c4 (x))\nSince xg \u2208 grid and we shifted all losses on the grid to be nonnegative, F \u03c4 (xg) \u2265 0. Thus, we can simplify the above to:\nF \u03c4 (xc) \u2265 \u2212 \u03b3\n\u03b3(x,K\u03c4 ) F \u03c4 (x)\nSince the epoch is ongoing, the conditions of DecideMove are not yet satisfied, and hence \u2200x\u2032 \u2208 1\u03b2K\u03c4 , F \u03c4 LCE(x \u2032) \u2264 `. By (2) of Lemma 6.2 for all points x\u2032\u2032 in 12\u03b2K\u03c4 it holds that F\n\u03c4 (x\u2032\u2032) \u2264 2`, in particular F \u03c4 (xc) \u2264 2`. The above simplifies to\nF \u03c4 (x) \u2265 \u2212\u03b3(x,K\u03c4 ) \u03b3 F \u03c4 (xc) \u2265 \u2212 2\u03b3(x,K\u03c4 )` \u03b3"}, {"heading": "7.2 Proof of Lemma 6.3", "text": "Proof of Lemma 6.3. Part 1: For every x \u2208 K \u2229 K\u03c4 , since K\u03c4 \u2286 K\u03c4\u22121 \u2286 ... \u2286 K0 = K, we have x \u2208 Kj for every 0 \u2264 j \u2264 \u03c4 . Therefore, by Lemma 6.1 we get F j(x) \u2265 \u2212 2`\u03b3 . Summing over the epochs,\n\u03c4\u22121\u2211 i=0 F i(x) \u2265 \u2212\u03c4 2` \u03b3\nPart 2: Figure 8 illustrates the proof.\nFor every x \u2208 K \u2229 Kc\u03c4 , since the Algorithm does not RESTART, therefore, there must be a point x0 \u2208 K\u03c4 such that\n\u2200\u03c4 \u2032 \u2264 \u03c4, F \u03c4 \u2032 LCE(x0) \u2264 `\n4 (2)\nLet l be the line segment between x and x0. Since x /\u2208 K\u03c4 , the line l intersects K\u03c4 , and denote xm be the intersection point between l and K\u03c4 : {xm} = l \u2229 K\u03c4 . The corresponding boundary of K\u03c4 was constructed in an epoch j \u2264 \u03c4 , and a hyperplane which separates the `-level set of Kj , namely H = {xm | \u3008hj , xm\u3009 = zj}) (See ShrinkSet for definition of hj , zj) such that H \u2229 l = {xm}.\nNow, by the definition of Minkowski Distance, we know that (Since Minkowski Distance is the distance ratio to 1dE\u03c4 where E\u03c4 is the MVEE of K\u03c4 , 1 dE\u03c4 can be 1/d smaller than K\u03c4 , and xm is the intersection point to K\u03c4 ) ||x\u2212 xm||2 ||xm \u2212 x0||2 \u2265 \u03b3(x,K\u03c4 )\u2212 1 2d\nWe know that (by the convexity of F jLCE)\nF jLCE(x)\u2212 F j LCE(xm)\nF jLCE(xm)\u2212 F j LCE(x0)\n\u2265 ||x\u2212 xm||2 ||xm \u2212 x0||2 \u2265 \u03b3(x,K\u03c4 )\u2212 1 2d\nwhere the denominator is non-negative, by equation (2), F jLCE(x0) \u2264 ` 4 , and by the definition of H (separation hyperplane of the `-level-set of F jLCE), F j LCE(xm) \u2265 `. This implies\nF jLCE(x) \u2265 (\u03b3(x,K\u03c4 )\u2212 1) \u00b7 34` 2d + ` \u2265 \u03b3(x,K\u03c4 )` 4d\nWe consider the following two cases: (a). x \u2208 \u03b2Kj , (b). x /\u2208 \u03b2Kj . case (a): x \u2208 \u03b2Kj The LCE is a lower bound of the original function only for x in the LCE fitting domain, here LCE = F jLCE, original function F j : \u03b2Kj \u2229K \u2192 R, so it is only true for x \u2208 \u03b2Kj \u2229K. Now, by (1) in Lemma 6.2, we know that F j(x) \u2265 F jLCE(x) \u2265 \u03b3(x,K\u03c4 )` 4d .\nFor other epoch i < \u03c4 , we can apply Lemma 6.1 and get F i(x) \u2265 \u2212 2\u03b3(x,Ki)`\u03b3 . Since the set K\u03c4 \u2286 K\u03c4\u22121 \u2286 ... \u2286 K0, By John\u2019s theorem, we can conclude that \u03b3(x,Ki) \u2264 2d\u03b3(x,K\u03c4 )\nwhich implies\n\u03c4\u22121\u2211 i=0 F i(x) \u2265 \u2211 i 6=j F i(x) + F j(x)\n\u2265 \u03b3(x,K\u03c4 )` 4d \u2212 \u03c4 \u00d7 4d\u03b3(x,K\u03c4 )` \u03b3 \u2265 \u03b3(x,K\u03c4 )` 32d\nby our choice of parameters \u03c4 \u2264 8d2 log T and \u03b3 = 2048d4 log T . case (b): x /\u2208 \u03b2Kj , x \u2208 K 7 This part of the proof consists of three steps. First, We find a point xj in center ofKj that has low F j value. Then we find a point xp inside \u03b2Kj , on the line between xj and x, with large F jLCE value, which implies by lemma 6.2 it has large F j value. Finally, we use both x0, xp to deduce the large value of F j(x).\nStep1: Let xj be the center of MVEE Ej of Kj . By (2) in Lemma 6.2, we know that F j(xj) \u2264 2`. Step 2: Define H \u2032 = {y | \u3008y, hj\u3009 = wj} to be the hyperplane parallel to H such that dist(xj , H \u2032) =\n1 2dist(xj , H), and H \u2032\u2032 = {y | \u3008y, hj\u3009 = uj} to be the hyperplane parallel to H such that dist(x0, H \u2032\u2032) = 9dist(x0, H).\n7In the follow proof, if not mentioned specifically, every points are in K\nWe can assume \u3008x0, hj\u3009 < wj (x0, H are in different side of H \u2032), since we know that F jLCE(x0) \u2264 ` 4 by definition, and the hyperplaneH \u2032 separates such that all points with \u3008x0, hj\u3009 \u2265 wj (See ShrinkSet for definition of H , H \u2032) have value F jLCE(x) \u2265 `.\nNote \u3008x0, hj\u3009 < wj implies dist(x0, H) \u2265 12dist(xj , H) = dist(H,H \u2032) 8, which implies that\ndist(xj , H \u2032\u2032) \u2265 dist(H,H \u2032\u2032) = 8dist(x0, H) \u2265 4dist(xj , H).\nNow, let xs = l \u2229 H \u2032\u2032 be the intersection point between H \u2032\u2032 and l, we can get: xs = xm + 8(xm \u2212 x0). Since x0, xm \u2208 Kj , we can obtain xs \u2208 \u03b22Kj by our choice of \u03b2 \u2265 64d 2. Let x\u2032s = l \u2032 \u2229H \u2032\u2032 be the intersection point of H \u2032\u2032 and the line segment l\u2032 of x and xj . Let x1 be the intersection point of H \u2032 and l: {x1} = H \u2032 \u2229 l. Consider the plane defined by x0, xj , x. Define xp to be the intersecting point of the ray shooting from xs towards the interval [x, xj ], that is parallel to the line from x1 to xj . Note that \u2016xs \u2212 xp\u2016 \u2264 \u2016x1 \u2212 xj\u2016, we have:\nxp = xs + (xp \u2212 xs) = xs + (xj \u2212 x1) ||xs \u2212 xp|| ||x1 \u2212 xj ||\nWe know that x1, xj \u2208 Kj , xs \u2208 \u03b22Kj , therefore, xs + (xj \u2212x1) ||xs\u2212xp|| ||x1\u2212xj || \u2208 \u03b2Kj , which means xp \u2208 \u03b2Kj . Moreover, we know that ||x\u2032s \u2212 xp||2 \u2264 ||xp \u2212 x\u2032m||2 due to the fact that ||x\u2032s \u2212 xp||2 \u2264 ||x\u2032m \u2212 xj ||2 \u2264 1 2\u2016x \u2032 m \u2212 x\u2032s\u20162 (last inequality by dist(xj , H \u2032\u2032) \u2265 4dist(xj , H)). We also note that ||x\u2032s \u2212 xp||2 \u2264 ||xp \u2212 x\u2032m||2 implies\ndist(xp, H) \u2265 1\n2 dist(x\u2032s, H).\nNow, let l\u2032\u2032 be the line segment between xp and x0, let x\u2032\u2032m be the intersection point of H and l \u2032\u2032: H \u2229 l\u2032\u2032 = {x\u2032\u2032m}. Consider the value of F j(xp), by (1) in Lemma 6.2 and xp \u2208 \u03b2Kj , we know that F j(xp) \u2265 F jLCE(xp). By the convexity of F jLCE, we obtain:\nF jLCE(xp)\u2212 F j LCE(x \u2032\u2032 m) F jLCE(x \u2032\u2032 m)\u2212 F jLCE(x0) \u2265 ||xp \u2212 x \u2032\u2032 m||2 ||x\u2032\u2032m \u2212 x0||2\n= dist(xp, H) dist(x0, H) \u2265 1 2dist(x \u2032 s, H)\ndist(x0, H)\n= 1 2dist(H \u2032\u2032, H)\ndist(x0, H) = 4\nNote that F jLCE(x \u2032\u2032 m) \u2265 `, F j LCE(x0) \u2264 ` 4 , therefore, F j LCE(xp) \u2265 3`. Which implies F j(xp) \u2265 F jLCE(xp) \u2265 3`.\nStep 3: Due to x /\u2208 \u03b2Kj and xm \u2208 Kj , by our choice of xs and \u03b2, we know that ||x\u2212 xm||2 \u2265 8||xs \u2212 xm||2.\n8H,H\u2032, H\u2032\u2032 are parallel to each other, so we can define distance between them\nWe ready to bound the value of F j(x): By the convexity of F j , we have:\nF j(x)\u2212 F j(xp) F j(xp)\u2212 F j(xj) \u2265 ||x\u2212 xp||2 ||xp \u2212 xj ||2 = ||x\u2212 xs||2 ||xs \u2212 x1||2\ntriangle similarity\n= ||x\u2212 xm||2 \u2212 ||xs \u2212 xm||2 ||xm \u2212 x1||2 + ||xs \u2212 xm||2 \u2265 ||x\u2212 xm||2 2||xs \u2212 xm||2 by ||xs \u2212 xm||2 \u2265 8||xm \u2212 x1|| \u2265 ||x\u2212 xm||2 ||xm \u2212 x0||2 \u00d7 ||xm \u2212 x0||2 2||xs \u2212 xm||2 \u2265 \u03b3(x,K\u03c4 )\u2212 1 32d\nThe last inequality is due to ||xm\u2212x0||2||xs\u2212xm||2 = 1 8 and ||x\u2212xm||2 ||xm\u2212x0||2 \u2265 \u03b3(x,K\u03c4 )\u22121 2d Putting together, we obtain (by F j(xj) \u2264 2`):\nF j(x) \u2265 \u03b3(x,K\u03c4 )\u2212 1 32d\n( F j(xp)\u2212 F j(xj) ) \u2265 \u03b3(x,K\u03c4 )\u2212 1\n32d\nSame as case (a) , we can sum over rest epoch to obtain:\n\u03c4\u22121\u2211 i=0 F i(x) \u2265 (\u03b3(x,K\u03c4 )\u2212 1)` 32d \u2212 \u03c4 \u00d7 4d\u03b3(x,K\u03c4 )` \u03b3 \u2265 \u03b3(x,K\u03c4 )` 64d\nby our choice of parameters \u03c4 \u2264 8d2 log T and \u03b3 = 2048d4 log T ."}, {"heading": "7.3 Proof of Lemma 6.4", "text": "Proof of Lemma 6.4. Suppose algorithm RESTART at epoch \u03c4 , then \u2211 j\u2264\u03c4 vj(A) \u2264 ` 128d . Therefore, we just need to show that for every x \u2208 K, \u2211 i\u2208\u03930\u222a...\u222a\u0393\u03c4 fi(x) \u2265 ` 128d . (a). Since the algorithm RESTART, by the RESTART condition, for every x \u2208 K\u03c4 , we know that \u2203j \u2264 \u03c4\nsuch that F j(x) = \u2211 i\u2208\u0393j fi(x) \u2265 F j LCE(x) > ` 4 . Using Lemma 6.1, we know that for every j\n\u2032 \u2264 \u03c4, j\u2032 6= j: F j \u2032 (x) = \u2211 i\u2208\u0393j\u2032\nfi(x) \u2265 \u2212 2`\u03b3 . Which implies that \u2211\ni\u2208\u03930\u222a...\u222a\u0393\u03c4\nfi(x) \u2265 ` 4 \u2212 \u03c4 2` \u03b3 \u2265 ` 8\n(b). For every x /\u2208 K\u03c4 , by Lemma 6.3, we know that\u2211 i\u2208\u03930\u222a...\u222a\u0393\u03c4\u22121 fi(x) \u2265 \u03b3(x,K\u03c4 )` 64d\nMoreover, by Lemma 6.1, we know that\u2211 i\u2208\u0393\u03c4 fi(x) \u2265 \u2212 2\u03b3(x,K\u03c4 )` \u03b3\nPutting together we have: \u2211 i\u2208\u03930\u222a...\u222a\u0393\u03c4 fi(x) \u2265 \u03b3(x,K\u03c4 ) ( ` 64d \u2212 2` \u03b3 ) \u2265 ` 128d"}, {"heading": "8 The EXP3 algorithm", "text": "For completeness, we give in this section the definition of the EXP3.P algorithm of [5], in slight modification which allows for unknown time horizon and output of the variances.\nAlgorithm 5 Exp3.P 1: Initial: T = 1. 2: Input: K experts, unknown rounds. In round t the cost function is given by ft.\n3: Let \u03b3 = \u221a\nK lnK T , \u03b1 = \u221a ln ( KT \u03b4 ) ,\n4: for j = 1, ...,K do 5: set\nw1(j) = exp\n( \u03b7\u03b1\u03b3 \u221a T\nK ) 6: end for 7: for t = T, ..., 2T \u2212 1 do 8: for j = 1, ...,K do 9:\npt(j) = (1\u2212 \u03b3) wt(j)\u2211 j\u2032 wt(j \u2032) + \u03b3 K\n10: end for 11: pick jt at random according to pt(j), play expert jt and receive ft(jt) 12: for j = 1, ...,K do 13: Let\nf\u0302t(j) =\n{ ft(j) pt(j)\nif j = jt; 0 otherwise.\n14: And\ng\u0302t(j) =\n{ 1\u2212ft(j) pt(j)\nif j = jt; 0 otherwise.\n15: end for 16: Update\nwt+1(j) = wt(j) exp\n( \u03b3\nK\n( g\u0302t(j) +\n\u03b7\u03b1\npt(j) \u221a TK )) 17: return\nvt(j) = t\u2211 i=1 f\u0302i(j)\nand\n\u03c3t(j) = t\u2211 i=1\n\u03b1\npi(j) \u221a TK\n18: end for 19: Set T = 2T and Goto 3."}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["Jacob Abernethy", "Elad Hazan", "Alexander Rakhlin"], "venue": "In COLT,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Optimal algorithms for online convex optimization with multi-point bandit feedback", "author": ["Alekh Agarwal", "Ofer Dekel", "Lin Xiao"], "venue": "In COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Stochastic convex optimization with bandit feedback", "author": ["Alekh Agarwal", "Dean P. Foster", "Daniel Hsu", "Sham M. Kakade", "Alexander Rakhlin"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Online linear optimization and adaptive routing", "author": ["Baruch Awerbuch", "Robert Kleinberg"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "An elementary introduction to modern convex geometry. In Flavors of Geometry, pages 1\u201358", "author": ["Keith Ball"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi", "Sham M. Kakade"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Bandit convex optimization: \\(\\sqrt{T}\\) regret in one dimension", "author": ["S\u00e9bastien Bubeck", "Ofer Dekel", "Tomer Koren", "Yuval Peres"], "venue": "In Proceedings of The 28th Conference on Learning Theory, COLT", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Multi-scale exploration of convex functions and bandit convex optimization", "author": ["S\u00e9bastien Bubeck", "Ronen Eldan"], "venue": "CoRR, abs/1507.06580,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Introduction to Derivative-Free Optimization, volume 8", "author": ["Andrew R Conn", "Katya Scheinberg", "Luis N Vicente"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "The price of bandit information for online optimization", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham Kakade"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Bandit smooth convex optimization: Improving the biasvariance tradeoff", "author": ["Ofer Dekel", "Ronen Eldan", "Tomer Koren"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["Abraham Flaxman", "Adam Tauman Kalai", "H. Brendan McMahan"], "venue": "In SODA,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Geometric algorithms and combinatorial optimization", "author": ["M. Gr\u00f6tschel", "L. Lov\u00e1sz", "A. Schrijver"], "venue": "Algorithms and combinatorics. Springer-Verlag,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "DRAFT: Introduction to online convex optimimization", "author": ["Elad Hazan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Hard-margin active linear regression", "author": ["Elad Hazan", "Zohar Karnin"], "venue": "In 31st International Conference on Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Bandit convex optimization: Towards tight bounds", "author": ["Elad Hazan", "Kfir Y. Levy"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Extremum Problems with Inequalities as Subsidiary Conditions", "author": ["F. John"], "venue": "Studies and Essays: Courant Anniversary Volume,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1948}, {"title": "Nearly tight bounds for the continuum-armed bandit problem", "author": ["Robert D Kleinberg"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "Consistency of multidimensional convex regression", "author": ["Eunji Lim", "Peter W. Glynn"], "venue": "Oper. Res.,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Random walk approach to regret minimization", "author": ["Hariharan Narayanan", "Alexander Rakhlin"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Improved regret guarantees for online smooth convex optimization with bandit feedback", "author": ["Ankan Saha", "Ambuj Tewari"], "venue": "In AISTATS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "On the complexity of bandit and derivative-free stochastic convex optimization", "author": ["Ohad Shamir"], "venue": "In Conference on Learning Theory,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2013}], "referenceMentions": [{"referenceID": 6, "context": "This fundamental decision making setting is extremely general, and has been used to efficiently model online prediction problems with limited feedback such as online routing, online ranking and ad placement, and many others (see [8] and [17] chapter 6 for applications and a detailed survey of BCO).", "startOffset": 229, "endOffset": 232}, {"referenceID": 15, "context": "This fundamental decision making setting is extremely general, and has been used to efficiently model online prediction problems with limited feedback such as online routing, online ranking and ad placement, and many others (see [8] and [17] chapter 6 for applications and a detailed survey of BCO).", "startOffset": 237, "endOffset": 241}, {"referenceID": 0, "context": "For example, in case the adversarial cost functions are linear, efficient algorithms are known that guarantee near-optimal regret bounds [2, 9, 18].", "startOffset": 137, "endOffset": 147}, {"referenceID": 7, "context": "For example, in case the adversarial cost functions are linear, efficient algorithms are known that guarantee near-optimal regret bounds [2, 9, 18].", "startOffset": 137, "endOffset": 147}, {"referenceID": 16, "context": "For example, in case the adversarial cost functions are linear, efficient algorithms are known that guarantee near-optimal regret bounds [2, 9, 18].", "startOffset": 137, "endOffset": 147}, {"referenceID": 0, "context": "Indeed, most known optimization and algorithmic techniques have been applied, including interior point methods [2], random walk optimization [23], continuous multiplicative updates [13], random perturbation [6], iterative optimization methods [15] and many more.", "startOffset": 111, "endOffset": 114}, {"referenceID": 21, "context": "Indeed, most known optimization and algorithmic techniques have been applied, including interior point methods [2], random walk optimization [23], continuous multiplicative updates [13], random perturbation [6], iterative optimization methods [15] and many more.", "startOffset": 141, "endOffset": 145}, {"referenceID": 11, "context": "Indeed, most known optimization and algorithmic techniques have been applied, including interior point methods [2], random walk optimization [23], continuous multiplicative updates [13], random perturbation [6], iterative optimization methods [15] and many more.", "startOffset": 181, "endOffset": 185}, {"referenceID": 4, "context": "Indeed, most known optimization and algorithmic techniques have been applied, including interior point methods [2], random walk optimization [23], continuous multiplicative updates [13], random perturbation [6], iterative optimization methods [15] and many more.", "startOffset": 207, "endOffset": 210}, {"referenceID": 13, "context": "Indeed, most known optimization and algorithmic techniques have been applied, including interior point methods [2], random walk optimization [23], continuous multiplicative updates [13], random perturbation [6], iterative optimization methods [15] and many more.", "startOffset": 243, "endOffset": 247}, {"referenceID": 8, "context": "A significant breakthrough was recently made by [10], who show that in the oblivious setting and in the special case of 1-dimensional BCO, O( \u221a T ) regret is attainable.", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "This result was very recently extended to any dimension by [11], still with an existential bound rather than an explicit algorithm and in the oblivious setting.", "startOffset": 59, "endOffset": 63}, {"referenceID": 13, "context": "1 Prior work The best known upper bound in the regret attainable for adversarial BCO with general convex loss functions is \u00d5(T ) due to [15] and [21] 2.", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "1 Prior work The best known upper bound in the regret attainable for adversarial BCO with general convex loss functions is \u00d5(T ) due to [15] and [21] 2.", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "This allows for very efficient exploration, and was first used by [13] to devise the Geometric Hedge algorithm that achieves an optimal regret rate of \u00d5( \u221a T ).", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "An efficient algorithm inspired by interior point methods was later given by [2] with the same optimal regret bound.", "startOffset": 77, "endOffset": 80}, {"referenceID": 7, "context": "Further improvements in terms of the dimension and other constants were subsequently given in [9, 18].", "startOffset": 94, "endOffset": 101}, {"referenceID": 16, "context": "Further improvements in terms of the dimension and other constants were subsequently given in [9, 18].", "startOffset": 94, "endOffset": 101}, {"referenceID": 13, "context": "The first gradient-descent-based method for BCO was given by [15].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "Their regret bound was subsequently improved for various special cases of loss functions using ideas from [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 22, "context": "For convex and smooth losses, [24] attained an upper bound on the regret of of \u00d5(T ).", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "This was recently improved to by [14] to \u00d5(T ).", "startOffset": 33, "endOffset": 37}, {"referenceID": 1, "context": "[3] obtained a regret bound of \u00d5(T ) for strongly-convex losses.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "For the special case of strongly-convex and smooth losses, [3] obtained a regret of \u00d5( \u221a T ) in the unconstrained case, and [19] obtain the same rate even in the constrained cased.", "startOffset": 59, "endOffset": 62}, {"referenceID": 17, "context": "For the special case of strongly-convex and smooth losses, [3] obtained a regret of \u00d5( \u221a T ) in the unconstrained case, and [19] obtain the same rate even in the constrained cased.", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "[25] gives a lower bound of \u03a9( \u221a T ) for the setting of strongly-convex and smooth BCO.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "A comprehensive survey by Bubeck and Cesa-Bianchi [8], provides a review of the bandit optimization literature in both stochastic and online setting.", "startOffset": 50, "endOffset": 53}, {"referenceID": 10, "context": "This is considered one of the hardest areas in convex optimization (although strictly a special case of BCO), and a significant body of work has culminated in a polynomial time algorithm, see [12].", "startOffset": 192, "endOffset": 196}, {"referenceID": 2, "context": "Recently, [4] give a polynomial time algorithm for regret minimization in the stochastic setting of zero-order optimization, greatly improving upon the known running times.", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "1In the oblivious setting [11] show that the regret behaves polynomially in the dimension.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "[17] chapter 6).", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "We denote by EK the minimal volume enclosing ellipsoid (MVEE) in K, also known as the John ellipsoid [20, 7].", "startOffset": 101, "endOffset": 108}, {"referenceID": 5, "context": "We denote by EK the minimal volume enclosing ellipsoid (MVEE) in K, also known as the John ellipsoid [20, 7].", "startOffset": 101, "endOffset": 108}, {"referenceID": 3, "context": "The following theorem was essentially established in [5] (although the original version was stated for gains instead of losses, and had known horizon parameter), for the algorithm called EXP3.", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "1 ([5]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "Till recently, the only polynomial time algorithm for zero-order optimization was based on the ellipsoid method [16].", "startOffset": 112, "endOffset": 116}, {"referenceID": 2, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "(see [22] for more details).", "startOffset": 5, "endOffset": 9}, {"referenceID": 3, "context": "P due to [5], to exploit and explore it.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "If there exists i \u2208 [2] such that FLCE(vi) \u2265 12F (y), then we already conclude the proof.", "startOffset": 20, "endOffset": 23}, {"referenceID": 0, "context": "Therefore, we can assume that for all i \u2208 [2], FLCE(vi) < 12F (y).", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "Step 2: By the definition of FLCE, we know that for every i \u2208 [2], there exists pi,1, .", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "P algorithm of [5], in slight modification which allows for unknown time horizon and output of the variances.", "startOffset": 15, "endOffset": 18}], "year": 2017, "abstractText": "We consider the problem of online convex optimization against an arbitrary adversary with bandit feedback, known as bandit convex optimization. We give the first \u00d5( \u221a T )-regret algorithm for this setting based on a novel application of the ellipsoid method to online learning. This bound is known to be tight up to logarithmic factors. Our analysis introduces new tools in discrete convex geometry.", "creator": "LaTeX with hyperref package"}}}