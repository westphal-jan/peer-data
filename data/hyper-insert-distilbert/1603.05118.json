{"id": "1603.05118", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2016", "title": "Recurrent Dropout without Memory Loss", "abstract": "while this paper presents commercially a technically novel approach opposed to recurrent junction neural network ( cp rnn ) regularization. substantially differently from incorporating the simpler widely self adopted cascade dropout method, which additionally is applied to both forward connections of feed - initiated forward traffic architectures networking or rnns, likewise we propose to handle drop infected neurons directly in recurrent connections in seemingly a convenient way that presently does not cause loss of cooperative long - term total memory. clearly our approach also is as typically easy to gracefully implement and apply as demonstrating the simpler regular protocols feed - driven forward access dropout software and we demonstrate its effectiveness overall for the most popular cooperative recurrent networks : highly vanilla rnns, long short - end term memory ( lstm ) and and gated recurrent networking unit ( gru ) networks. our experiments on three nlp pattern benchmarks show consistent simulation improvements even worse when combined with conventional feed - forward dropout.", "histories": [["v1", "Wed, 16 Mar 2016 14:33:47 GMT  (47kb)", "http://arxiv.org/abs/1603.05118v1", null], ["v2", "Fri, 5 Aug 2016 09:59:25 GMT  (95kb)", "http://arxiv.org/abs/1603.05118v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stanislau semeniuta", "aliaksei severyn", "erhardt barth"], "accepted": false, "id": "1603.05118"}, "pdf": {"name": "1603.05118.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["stas@inb.uni-luebeck.de", "barth@inb.uni-luebeck.de", "severyn@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n05 11\n8v 1\n[ cs\n.C L\n] 1\n6 M\nar 2\n01 6"}, {"heading": "1 Introduction", "text": "Recurrent Neural Networks have recently gained increased attention in the NLP community for their superior ability to model and learn from sequential data, and for showing state-of-the-art results on various public benchmarks ranging from sentence classification (Wang et al., 2015; Irsoy and Cardie, 2014; Liu et al., 2015) and various tagging problems (Dyer et al., 2015) to language modelling (Kim et al., 2015; Zhang et al., 2015), text generation (Zhang and Lapata, 2014) and sequence-to-sequence prediction tasks (Sutskever et al., 2014).\nHaving shown excellent ability to capture and learn complex linguistic phenomena, RNN architectures, like any other type of deep learning models, are prone to overfitting, especially when the number of training examples at hand is limited.\nAmong the first and most widely used techniques to avoid overfitting and increase generalizability of neural networks is the dropout regularization (Hinton et al., 2012). Since its introduction, it has become, together with the L2 weight decay, the de-facto standard neural network regularization method.\nWhile showing significant improvements when used in feed-forward architectures, e.g., Convolutional Neural Networks (Krizhevsky et al., 2012), the application of dropout in RNNs has been somewhat limited with unclear benefits. Indeed, so far dropout in RNNs has been applied in the same fashion as in feed-forward architectures: it is typically injected between input-tohidden, hidden-to-hidden, and hidden-to-output layers, i.e., along the input axis, but not between the recurrent connections (time axis). Given that RNNs are mainly used to model sequential data with the goal of capturing short- and long-term interactions, it seems natural to also regularize the recurrent weights. This observation has led us to the idea of applying dropout to the recurrent connections in RNNs.\nHence, in this paper we propose an idea of a recurrent dropout and give answers to the following important questions: (i) how to apply the dropout in recurrent connections of different RNN architectures, e.g., vanilla RNNs, LSTMs and GRUs in a way that prevents possible corruption of the long-term memory; (ii) what is the relationship between our recurrent dropout and the widely adopted dropout in input-to-hidden and hidden-tooutput connections; (iii) how the dropout mask in RNNs should be sampled: once per step or once per sequence. The latter question of sampling the mask appears to be crucial in some cases to make the recurrent dropout work and, to the best of our knowledge, has received very little attention in the literature.\nRegarding empirical evaluation, we first highlight the problem of information loss in memory cells of LSTMs when applying recurrent dropout. We demonstrate that previous approaches of dropping hidden state vectors cause loss of memory while our proposed method to use dropout mask in hidden state updates does not suffer from this problem. We also experiment on three widely adopted NLP tasks: Language Modelling, Named Entity Recognition and Twitter Sentiment Analysis. The results demonstrate that recurrent dropout helps to achieve better regularization and yields improvements across all the tasks, even when combined with the conventional feedforward dropout.\nThe paper is structured as follows: Section 2 briefly discusses previous efforts to improve regularization of recurrent architectures, Section 3 introduces RNN architectures used in this paper and gives the detailed explanation of our recurrent dropout technique, Section 4 provides experimental results, while Section 5 concludes the paper."}, {"heading": "2 Related Work", "text": "Deep Neural Networks are machine-learning models with high capacity, i.e., they typically optimize a very large number of parameters. This often leads to overfitting, especially, when the amount of training data is small and has led to a lot of research directed towards improvement of neural networks generalization. Since dropout has proven to be a very effective and easy-to-use method, a number of works improving this method have been published. We will focus on the ones related to recurrent neural network regularization.\nPham et al. (2013) and Zaremba et al. (2014) have shown that LSTMs can be effectively regularized when using dropout in forward connections. While this already allows for effective regularization of recurrent networks, it is intuitive that introducing dropout also in the hidden state may force it to create more robust representations. Indeed, Moon et al. (2015) have extended the idea of dropping neurons in forward direction and proposed to drop cell states as well. The authors are able to achieve good results on a Speech Recognition task but their approach has a number of limitations. We will discuss these limitations later in Section 3.4. Bluche et al. (2015) carry out a study to find where dropout is most effective, e.g. inputto-hidden or hidden-to-output connections. The\nauthors conclude that it is more beneficial to use it once in the correct spot, rather than put it everywhere. Bengio et al. (2015) have proposed an algorithm called scheduled sampling to improve performance of recurrent networks on sequenceto-sequence labeling tasks. A disadvantage of this work is that the scheduled sampling is specifically tailored to this kind of tasks, what makes it impossible to use in, for example, sequence-to-label tasks.\nThe main contribution of this paper is the introduction of a new regularization technique specifically tailored for RNNs \u2013 recurrent dropout. It is based on a widely used dropout method which has been proven to be highly effective and easy to apply. The main difference between our approach and the existing approaches to RNN regularization is that we apply the dropout directly to the recurrent connections. We demonstrate that applying dropout to arbitrary vectors in LSTM and GRU cells may lead to loss of memory thus hindering the ability of the network to encode longterm information. In other words, our technique allows for adding a strong regularizer on the model weights responsible for learning short and longterm dependencies without affecting the ability to model long-term relationships, which are especially important to model when dealing with natural language. We demonstrate that our approach is effective when applied to LSTMs, GRUs and plain RNNs."}, {"heading": "3 Recurrent Dropout", "text": "This section describes how dropout could be applied to recurrent connections of three most widely used types of recurrent nets: vanilla RNNs, LSTMs and GRUs."}, {"heading": "3.1 Dropout", "text": "Dropout (Hinton et al., 2012) is one of the few effective tools today for regularizing neural networks. It is widely adopted in feed-forward architectures applied to various layers:\ny = f(Wd(x)), (1)\nwhere x is the layer\u2019s input, W is the matrix of weights, f is an activation function, y is the vector of activations, and d is dropout operation defined as:\nd(x) =\n{\nmask \u2217 x, if train phase (1\u2212 p)x otherwise, (2)\nwhere p is the dropout rate and mask is a vector, sampled from the Bernoulli distribution with success probability 1\u2212 p."}, {"heading": "3.2 Dropout in vanilla RNNs", "text": "Vanilla RNNs were one of the first ideas for modeling sequential data with neural networks. Formally, RNNs process the input sequences as follows:\nht = f(Wh[xt,ht\u22121] + bh), (3)\nwhere xt is the input at time step t; ht and ht\u22121 are hidden vectors that encode the current and previous states of the network; Wh is parameter matrix that models input-to-hidden and hidden-tohidden (recurrent) connections; b is a vector of bias terms, and f is the activation function.\nAs RNNs model sequential data by a fullyconnected layer, we apply dropout in the same way as it is applied in the feed-forward networks. Specifically, we modify Equation 3 in the following way:\nht = f(Wh[xt, d(ht\u22121)] + bh), (4)\nwhere d is the dropout function from Equation 2. While recurrent and feed-forward fully connected layers are similar, there is a significant difference: in feed-forward networks every fully-connected layer processes its input only once, while it is not the case for the recurrent layer: each training example is a sequence composed of a number of inputs. When applying dropout this results in hidden state being dropped on every step. This observation leads to the question of how to sample the dropout mask. There are two options: sample it once per whole training sequence (per-sequence) or to sample a new mask on every step (perstep). We discuss these two strategies for sampling dropout mask in more detail in Section 3.4."}, {"heading": "3.3 Dropout in LSTM and GRU networks", "text": "Despite simplicity and effectiveness of vanilla RNNs on a number of sequence modeling tasks, their training have been shown to pose serious problems, primarily caused by so called vanishing and exploding gradients (Bengio et al., 1994). While the problem of exploding gradients can be effectively addressed by performing gradient clipping (Pascanu et al., 2013), the vanishing gradients appear to be a more serious issue. There are two primary directions to address the latter issue:\n(i) use more sophisticated Hessian-free optimization algorithms instead of Stochastic Gradient Descend (SGD), e.g., Martens and Sutskever (2011), and (ii) make adjustments in the network architecture, e.g., by introducing gated inputs. Long Short-Term Memory networks (Hochreiter and Schmidhuber, 1997) have introduced the concept of gated inputs in RNNs, which effectively allow the network to preserve its memory over a larger number of time steps during both forward and backward passes, thus alleviating the problem of vanishing gradients. Formally, it is expressed with the following equations:\n\n   it ft ot gt\n\n   =\n\n  \n\u03c3(Wi [ xt,ht\u22121 ]\n+ bi) \u03c3(Wf [ xt,ht\u22121 ]\n+ bf ) \u03c3(Wo [ xt,ht\u22121 ]\n+ bo) f(Wg [ xt,ht\u22121 ] + bg)\n\n  \n(5)\nct = ft \u2217 ct\u22121 + it \u2217 gt (6)\nht = ot \u2217 f(ct), (7)\nwhere it, ft,ot are input, output and forget gates at step t; gt is the vector of cell updates and ct is the updated cell vector used to update the hidden state ht; \u03c3 is the sigmoid function and \u2217 is the elementwise multiplication.\nOur approach is to apply dropout to the cell update vector ct as follows:\nct = ft \u2217 ct\u22121 + it \u2217 d(gt) (8)\nIn contrast, Moon et al. (2015) propose to apply dropout directly to the cell values and use persequence sampling:\nct = d(ft \u2217 ct\u22121 + it \u2217 gt) (9)\nWe will discuss the limitations of the approach of Moon et al. (2015) in Section 3.4 and support our arguments with empirical evaluation in Sections 4.1 and 4.2. Gated Recurrent Unit (GRU) networks are a recently introduced variant of a recurrent network with hidden state protected by gates (Cho et al., 2014). Different from LSTMs, GRU networks use only two gates rt and zt to update the cell\u2019s hidden state ht:\n(\nzt rt\n)\n=\n( \u03c3(Wz [ xt,ht\u22121 ]\n+ bz) \u03c3(Wr [ xt,ht\u22121 ] + br)\n)\n(10)\ngt = f(Wg [ xt, rt \u2217 ht\u22121 ] + bg) (11)\nht = (1\u2212 zt) \u2217 ht\u22121 + zt \u2217 gt (12)\nSimilarly to the LSTMs, we propoose to apply dropout to the hidden state updates vector gt:\nht = (1\u2212 zt) \u2217 ht\u22121 + zt \u2217 d(gt) (13)\nTo the best of our knowledge, this work is the first to study the effect of recurrent dropout in GRU networks."}, {"heading": "3.4 Dropout and memory", "text": "Before going further with the explanation of our approach we need to clarify one notational difference between LSTM and GRU networks. Traditionally, LSTM networks use \u201ccells\u201d to store history and \u201chidden states\u201d to output the information to the rest of the network. GRU networks do not have this distinction. In this subsection we will refer to LSTM cells and GRU hidden states when we say network hidden states.\nFirstly, we found that an intuitive idea to drop previous hidden states directly, as proposed in Moon et al. (2015), produces mixed results. We have observed that it helps the network to generalize better when not coupled with the forward dropout, but is no longer beneficial when used together with regular forward dropout. While in some cases applying both forward and hidden state dropout yields an improvement in the network performance, we found this outcome to be rare and hard to achieve. Below we discuss two primary problems with the dropout method of Moon et al. (2015).\nThe first problem arises from the fact that gated networks do not overwrite their hidden state on every processing step. Instead, they gradually change it by using the element-wise multiplication with gate values and summation with a new hidden state candidate. Therefore, after a neuron has stored a value it will remain in neuron\u2019s\nmemory until the forget gate is activated. Dropping a neuron in LSTM or GRU networks is effectively equivalent to activating the forget gate because when it is zeroed out by the dropout it loses all of its history. As a consequence, training with per-step and per-sequence mask sampling results in very different dynamics in a network during training. In case of a per-step mask sampling, network\u2019s ability to learn long-term relationships is reduced because every unit\u2019s history gets deleted very often. In general a network can still be able to overcome this by constantly refreshing its hidden state, but this is not trivial to learn and forces a network to behave similarly to vanilla RNNs. As an example, when training a network with 0.2 recurrent dropout the probability that a neuron\u2019s state will be zeroed in 20 steps is almost 0.99. This can be addressed by sampling a dropout mask once per sequence. Moon et al. (2015) make the same observation and propose to use per-sequence mask sampling as well. However, in this case some neurons behave as if dropout was not used at all, while other neurons remain idle when processing a given input sequence.\nThe second problem is caused by the scaling of neuron activations. Consider the hidden state update rule in the test phase of an extremely simplified version of a gated network with all the gates always equal to 1:\nht = (ht\u22121 + gt)p, (14)\nwhere gt are update vectors, computed by Eq. 5 for LSTM and Eq. 11 for GRU networks and p is the probability to not drop a neuron. As ht\u22121 was, in turn, computed using the same rule, we can rewrite this equation as:\nht = ((ht\u22122 + gt\u22121)p+ gt)p (15)\nRecursively expanding h for every timestep results in the following equation:\nht = ((((h0 + g0)p+ g1)p+ ...)p + gt)p (16)\nPushing p inside parenthesis, Eq. 16 can be written as:\nht = p t+1h0 +\nt \u2211\ni=0\npt\u2212i+1gi (17)\nSince p is a value between zero and one, sum components that are far away in the past are multiplied by a very low value and are effectively removed from the summation. Thus, even though the network would be able to learn long-term dependencies, it will not be able to use them during test phase. The fact that Moon et al. (2015) have achieved an improvement can be explained by the experimentation domain. Le et al. (2015) have proposed a simple yet effective way to initialize vanilla RNNs and reported that they have achieved a good result in the Speech Recognition domain while having an effect similar to the one caused by Eq. 17. One can reduce the influence of this effect by selecting a low dropout rate. This solution however is partial, since it only increases the number of steps required to completely forget past history and does not remove the problem completely.\nOne important note is that the dropout function from Eq. 2 can be implemented as:\nd(x) =\n{\nmask \u2217 x/p, if train phase x otherwise (18)\nIn this case the above argument holds as well, but instead of observing vanishing hidden states during testing, we will observe exploding hidden states during training.\nOur approach addresses both problems discussed previously. Since we drop only candidates, we do not delete neuron\u2019s history during training, which allows us to use per-step mask sampling while still being able to learn long-term dependencies. Moreover, our approach allows for solving the scaling issue, as Eq. 17 becomes:\nht = ph0 + t \u2211\ni=0\np gi = ph0 + p t \u2211\ni=0\ngi (19)\nThus, our approach allows to freely apply dropout in the recurrent connections of a gated network without hindering its ability to process long-term relationships.\nFinally, we note that none of the discussed problems affect vanilla RNNs because they overwrite their hidden state at every timestep."}, {"heading": "4 Experiments", "text": "First, we empirically demonstrate the issues linked to memory loss when using various dropout techniques in recurrent nets (see Sec. 3.4). For this purpose we experiment with training LSTM networks on one of the synthetic tasks from (Hochreiter and Schmidhuber, 1997), specifically the Temporal Order task. We then validate the effectiveness of our recurrent dropout when applied to vanilla RNNs, LSTMs and GRUs on three diverse public benchmarks: Language Modelling, Named Entity Recognition, and Twitter Sentiment classification."}, {"heading": "4.1 Synthetic Task", "text": "Data. In this task a network processes sequences generated as follows: at fixed positions in a sequence, e.g., in the beginning and middle, nonnoisy symbols are chosen from {A, B}, while the remaining symbols are random. The task is to classify a sequence into one of four classes ({AA, AB, BA, BB}) based on the order of non-noisy symbols. We encode the symbols using two bits. The networks are trained on 200 minibatches with 32 sequences and tested on 10k sequences. Setup. We use LSTM with one layer that contains 256 hidden units and recurrent dropout with 0.5 strength. We use SGD with a learning rate of 0.1 and train for 5k epochs. We generate data so that every sequence is split into three parts with the same size and emit one meaningful symbol in first and second parts of a sequence. The prediction is taken after the full sequence has been processed. We use two modes in our experiments: Short with sequence length 15 and Medium with sequence length 30. Results. Table 1 reports the results on the Temporal Order task when recurrent dropout is applied\nto hidden states or hidden state updates. When dropout is applied to hidden states with per-sequence sampling networks are able to discover the long-term dependency, but fail to use it on the test set due to the scaling issue. Interestingly, in Medium case results on the test set are worse than random. Networks trained with per-step sampling exhibit different behavior: in Short case they are capable of capturing the temporal dependency and generalizing to the test set, but require 10-20 times more iterations to do so. In Medium case these networks do not fit into the allocated number of iterations. If dropout is applied in hidden state updates, networks are able to solve the problem in all cases. We have also ran the same experiments for longer sequences, but found that the results are equivalent to the Medium case."}, {"heading": "4.2 Language Modeling", "text": "Data. Following Mikolov et al. (2011) we use the Penn Treebank Corpus to train our Language Modeling (LM) models. The dataset contains approximately 1 million words and comes with pre-\ndefined training, validation and test splits, and a vocabulary of 10k words.\nSetup. In our LM experiments we use recurrent networks with a single layer with 256 hidden units. Network parameters were initialized uniformly in [-0.05, 0.05]. For training, we use plain SGD with batch size 32 with the maximum norm gradient clipping (Pascanu et al., 2013). Learning rate, clipping threshold and number of Backpropagation Through Time (BPTT) steps were set to 0.1, 30 and 15 respectively for vanilla RNNs, 1, 10 and 35 for LSTMs and 0.1, 20 and 35 for GRUs. For the learning rate decay we use the following strategy: if the validation error does not decrease after each epoch, we divide the learning rate by 1.5. The aforementioned choices were largely guided by the work of Mikolov et al. (2014)1. To ease reproducibility of our results on the LM and synthetic tasks, we have released the source code of\n1We found that to achieve reasonable results with vanilla RNNs on the LM task, it was important to construct training batches similarly to Mikolov et al. (2014), where the authors backpropagate 10 steps back every 5 steps forward.\nour experiments2. Results. Table 2 reports the results for vanilla RNNs, LSTM and GRU networks. For LSTM and GRU networks we also present results when the dropout is applied directly to hidden states as in (Moon et al., 2015).\nWe make the following observations: (i) dropping hidden state updates yields better results than dropping hidden states; (ii) per-step mask sampling is better when dropping hidden state directly; (iii) contrary to our expectations, when we apply dropout to hidden state updates per-step sampling seems to yield results similar to per-sequence sampling; (iv) applying dropout to hidden state updates rather than hidden states in some cases leads to a perplexity decrease by more than 30 points; and finally (v) our approach is effective even when combined with the forward dropout \u2013 for LSTMs we are able to bring down perplexity on the validation set from 130 to 91.6."}, {"heading": "4.3 Named Entity Recognition", "text": "Data. To assess our recurrent Named Entity Recognition (NER) taggers when using recurrent dropout we use a public benchmark from CONLL 2003 (Tjong Kim Sang and De Meulder, 2003). The dataset contains approximately 300k words split into train, validation and test partitions. Each word is labeled with either a named entity class it belongs to, such as Location or Person, or as being not named. The majority of words are labeled as not named entities. The vocabulary size is about 22k words. Setup. Previous state-of-the-art NER systems have shown the importance of using word context features around entities. Hence, we slightly modify the architecture of our recurrent networks to consume the context around the target word by simply concatenating their embeddings. The size of the context window is fixed to 5 words (the word to be labeled, two words before and two words after). The recurrent layer size is 1024 units. The network inputs include only word embeddings (initialized with pretrained word2vec embeddings (Mikolov et al., 2013) and kept static) and capitalization features. For training we use the RMSProp algorithm (Dauphin et al., 2015) with \u03c1 fixed at 0.9 and a learning rate of 0.01 and multiply the learning rate by 0.99 after every epoch. We also combine our recurrent dropout (with per-\n2https://github.com/stas-semeniuta/drop-rnn\nsequence mask sampling) with the conventional forward dropout with the rate 0.2 in input and 0.5 in output connections. Lastly, we found that using relu(x) = max(x, 0) nonlinearity resulted in higher performance than tanh(x).\nTo speed up the training we use a length expansion approach described in (Ng et al., 2015), where training is performed in two stages: (i) we first sample short 5-words input sequences with their contexts and train for 25 epochs; (ii) we fine tune the network on input 15-words sequences for 10 epochs. We found that further fine tuning on longer sequences yielded negligible improvements. Such strategy allows us to significantly speed up the training when compared to training from scratch on full-length input sentences. We use full sentences for testing. Results. F1 scores of our taggers are reported in Table 3 when trained on short 5-word and longer 15-word input sequences. We note that the gap between networks trained with and without our dropout scheme is larger for networks trained on shorter sequences. It suggests that dropout in recurrent connections might have an impact on how well a network generalizes to sequences that are longer than the ones used during training. The gain from using recurrent dropout is larger for the LSTM network. We have experimented with higher recurrent dropout rates, but found that it led to excessive regularization."}, {"heading": "4.4 Twitter Sentiment Analysis", "text": "Data. We use Twitter sentiment corpus from SemEval-2015 Task 10 (subtask B) (Rosenthal et al., 2015). It contains 15k labeled tweets split into training and validation partitions. The total number of words is approximately 330k and the vocabulary size is 22k. The task consists of classifying a tweet into three classes: positive,neutral, and negative.\nPerformance of a classifier is measured by the average of F1 scores of positive and negative classes. We evaluate our models on a number of datasets that were used for benchmarking during the last years. Setup. We use recurrent networks in the standard sequence labeling manner - we input words to a network one by one and take the label at the last step. Similarly to (Severyn and Moschitti, 2015), we use 1 million of weakly labeled tweets to pretrain our networks. We use networks composed of 500 neurons in all cases. Our models are trained with the RMSProp algorithm with a learning rate of 0.001. We use our recurrent dropout regularization with per-step mask sampling. All the other settings are equivalent to the ones used in the NER task. Results. The results of these experiments are presented in Table 4. Note that in this case our algorithm decreases the performance of the vanilla RNNs while this is not the case for LSTM and GRU networks. This is due to the nature of the problem: differently from LM and NER tasks, a network needs to aggregate information over a long sequence. Vanilla RNNs notoriously have difficulties with this and our dropout scheme impairs their ability to remember even further. The best result over most of the datasets is achieved by the GRU network with recurrent dropout. The only exception is the Twitter2015 dataset, where the LSTM network shows better results."}, {"heading": "5 Conclusions", "text": "This paper presents a novel recurrent dropout method specifically tailored to the gated recurrent neural networks. Our approach is easy to implement and is even more effective when com-\nbined with conventional forward dropout. We have shown that for LSTMs and GRUs applying dropout to arbitrary cell vectors results in suboptimal performance. We discuss in detail the cause of this effect and propose a simple solution to overcome it. The effectiveness of our approach is verified on three different public NLP benchmarks.\nOur findings along with our empirical results allow us to answer the questions posed in Section 1: (i) it is straight-forward to use dropout in vanilla RNNs due to their strong similarity with the feed-forward architectures, while its application to LSTM and GRU networks requires some care. We demonstrate that recurrent dropout is most effective when applied to hidden state update vectors rather than hidden states; (ii) we observe an improvement in the network\u2019s performance when our recurrent dropout is coupled with the standard forward dropout, though the extent of this improvement depends on the values of dropout rates; (iii) contrary to our expectations, networks trained with per-step and per-sequence mask sampling produce similar results when using our recurrent dropout method, both being better than the dropout scheme proposed by Moon et al. (2015).\nWhile our experimental results show that applying recurrent dropout method leads to significant improvements across various NLP benchmarks (especially when combined with conventional forward dropout), its benefits for other tasks, e.g., sequence-to-sequence prediction, or other domains, e.g., Speech Recognition, remain unexplored. We leave it as our future work."}, {"heading": "Acknowledgments", "text": "This project has received funding from the European Union\u2019s Framework Programme for Re-\nsearch and Innovation HORIZON 2020 (2014- 2020) under the Marie Skodowska-Curie Agreement No. 641805. Stanislau Semeniuta thanks the support from Pattern Recognition Company GmbH. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Bengio et al.1994] Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks. CoRR, abs/1506.03099", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Where to apply dropout in recurrent neural networks for handwriting recognition", "author": ["Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In 13th International Conference on Document Analysis", "citeRegEx": "Bluche et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bluche et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches. CoRR, abs/1409.1259", "author": ["Cho et al.2014] KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization. CoRR, abs/1502.04390", "author": ["Harm de Vries", "Junyoung Chung", "Yoshua Bengio"], "venue": null, "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "A. Noah Smith"], "venue": "In ACL,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Opinion mining with deep recurrent neural networks", "author": ["Irsoy", "Cardie2014] Ozan Irsoy", "Claire Cardie"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Irsoy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Irsoy et al\\.", "year": 2014}, {"title": "Character-aware neural language models. CoRR, abs/1508.06615", "author": ["Kim et al.2015] Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A simple way to initialize recurrent networks of rectified linear units. CoRR, abs/1504.00941", "author": ["Le et al.2015] Quoc V. Le", "Navdeep Jaitly", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Multi-timescale long short-term memory neural network for modelling sentences and documents", "author": ["Liu et al.2015] Pengfei Liu", "Xipeng Qiu", "Xinchi Chen", "Shiyu Wu", "Xuanjing Huang"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["Martens", "Sutskever2011] James Martens", "Ilya Sutskever"], "venue": "Proceedings of the 28th International Conference", "citeRegEx": "Martens et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Martens et al\\.", "year": 2011}, {"title": "Extensions of recurrent neural network language model", "author": ["Mikolov et al.2011] T. Mikolov", "S. Kombrink", "L. Burget", "J.H. Cernocky", "Sanjeev Khudanpur"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning longer memory in recurrent neural networks. CoRR, abs/1412.7753", "author": ["Armand Joulin", "Sumit Chopra", "Micha\u00ebl Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Rnndrop: A novel dropout for rnns in asr. Automatic Speech Recognition and Understanding (ASRU)", "author": ["Moon et al.2015] Taesup Moon", "Heeyoul Choi", "Hoshik Lee", "Inchul Song"], "venue": null, "citeRegEx": "Moon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moon et al\\.", "year": 2015}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["Ng et al.2015] Joe Yue-Hei Ng", "Matthew J. Hausknecht", "Sudheendra Vijayanarasimhan", "Oriol Vinyals", "Rajat Monga", "George Toderici"], "venue": null, "citeRegEx": "Ng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2015}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Dropout improves recurrent neural networks for handwriting recognition. CoRR, abs/1312.4569", "author": ["Pham et al.2013] Vu Pham", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": null, "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "Semeval-2015 task 10: Sentiment analysis in twitter", "author": ["Preslav Nakov", "Svetlana Kiritchenko", "Saif Mohammad", "Alan Ritter", "Veselin Stoyanov"], "venue": "In Proceedings of the 9th International", "citeRegEx": "Rosenthal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2015}, {"title": "Twitter sentiment analysis with deep convolutional neural networks", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference", "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of the Seventh Conference", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Predicting polarities of tweets by composing word embeddings with long short-term memory", "author": ["Wang et al.2015] Xin Wang", "Yuanchao Liu", "Chengjie SUN", "Baoxun Wang", "Xiaolong Wang"], "venue": "In ACL,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Lapata2014] Xingxing Zhang", "Mirella Lapata"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Tree recurrent neural networks with application to language modeling. CoRR, abs/1511.00060", "author": ["Zhang et al.2015] Xingxing Zhang", "Liang Lu", "Mirella Lapata"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "Recurrent Neural Networks have recently gained increased attention in the NLP community for their superior ability to model and learn from sequential data, and for showing state-of-the-art results on various public benchmarks ranging from sentence classification (Wang et al., 2015; Irsoy and Cardie, 2014; Liu et al., 2015) and various tagging problems (Dyer et al.", "startOffset": 263, "endOffset": 324}, {"referenceID": 12, "context": "Recurrent Neural Networks have recently gained increased attention in the NLP community for their superior ability to model and learn from sequential data, and for showing state-of-the-art results on various public benchmarks ranging from sentence classification (Wang et al., 2015; Irsoy and Cardie, 2014; Liu et al., 2015) and various tagging problems (Dyer et al.", "startOffset": 263, "endOffset": 324}, {"referenceID": 5, "context": ", 2015) and various tagging problems (Dyer et al., 2015) to language modelling (Kim et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 9, "context": ", 2015) to language modelling (Kim et al., 2015; Zhang et al., 2015), text generation (Zhang and Lapata, 2014) and sequence-to-sequence prediction tasks (Sutskever et al.", "startOffset": 30, "endOffset": 68}, {"referenceID": 28, "context": ", 2015) to language modelling (Kim et al., 2015; Zhang et al., 2015), text generation (Zhang and Lapata, 2014) and sequence-to-sequence prediction tasks (Sutskever et al.", "startOffset": 30, "endOffset": 68}, {"referenceID": 23, "context": ", 2015), text generation (Zhang and Lapata, 2014) and sequence-to-sequence prediction tasks (Sutskever et al., 2014).", "startOffset": 92, "endOffset": 116}, {"referenceID": 6, "context": "Among the first and most widely used techniques to avoid overfitting and increase generalizability of neural networks is the dropout regularization (Hinton et al., 2012).", "startOffset": 148, "endOffset": 169}, {"referenceID": 10, "context": ", Convolutional Neural Networks (Krizhevsky et al., 2012), the application of dropout in RNNs has been somewhat limited with unclear benefits.", "startOffset": 32, "endOffset": 57}, {"referenceID": 14, "context": "Indeed, Moon et al. (2015) have extended the idea of dropping neurons in forward direction and proposed to drop cell states as well.", "startOffset": 8, "endOffset": 27}, {"referenceID": 0, "context": "Bluche et al. (2015) carry out a study to find where dropout is most effective, e.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Bengio et al. (2015) have proposed an algorithm called scheduled sampling to improve performance of recurrent networks on sequenceto-sequence labeling tasks.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "Dropout (Hinton et al., 2012) is one of the few effective tools today for regularizing neural networks.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "Despite simplicity and effectiveness of vanilla RNNs on a number of sequence modeling tasks, their training have been shown to pose serious problems, primarily caused by so called vanishing and exploding gradients (Bengio et al., 1994).", "startOffset": 214, "endOffset": 235}, {"referenceID": 19, "context": "While the problem of exploding gradients can be effectively addressed by performing gradient clipping (Pascanu et al., 2013), the vanishing gradients appear to be a more serious issue.", "startOffset": 102, "endOffset": 124}, {"referenceID": 0, "context": "Despite simplicity and effectiveness of vanilla RNNs on a number of sequence modeling tasks, their training have been shown to pose serious problems, primarily caused by so called vanishing and exploding gradients (Bengio et al., 1994). While the problem of exploding gradients can be effectively addressed by performing gradient clipping (Pascanu et al., 2013), the vanishing gradients appear to be a more serious issue. There are two primary directions to address the latter issue: (i) use more sophisticated Hessian-free optimization algorithms instead of Stochastic Gradient Descend (SGD), e.g., Martens and Sutskever (2011), and (ii) make adjustments in the network architecture, e.", "startOffset": 215, "endOffset": 629}, {"referenceID": 17, "context": "In contrast, Moon et al. (2015) propose to apply dropout directly to the cell values and use persequence sampling:", "startOffset": 13, "endOffset": 32}, {"referenceID": 3, "context": "Gated Recurrent Unit (GRU) networks are a recently introduced variant of a recurrent network with hidden state protected by gates (Cho et al., 2014).", "startOffset": 130, "endOffset": 148}, {"referenceID": 16, "context": "We will discuss the limitations of the approach of Moon et al. (2015) in Section 3.", "startOffset": 51, "endOffset": 70}, {"referenceID": 17, "context": "Firstly, we found that an intuitive idea to drop previous hidden states directly, as proposed in Moon et al. (2015), produces mixed results.", "startOffset": 97, "endOffset": 116}, {"referenceID": 17, "context": "Firstly, we found that an intuitive idea to drop previous hidden states directly, as proposed in Moon et al. (2015), produces mixed results. We have observed that it helps the network to generalize better when not coupled with the forward dropout, but is no longer beneficial when used together with regular forward dropout. While in some cases applying both forward and hidden state dropout yields an improvement in the network performance, we found this outcome to be rare and hard to achieve. Below we discuss two primary problems with the dropout method of Moon et al. (2015).", "startOffset": 97, "endOffset": 580}, {"referenceID": 17, "context": "Moon et al. (2015) make the same observation and propose to use per-sequence mask sampling as well.", "startOffset": 0, "endOffset": 19}, {"referenceID": 16, "context": "The fact that Moon et al. (2015) have achieved an improvement can be explained by the experimentation domain.", "startOffset": 14, "endOffset": 33}, {"referenceID": 11, "context": "Le et al. (2015) have proposed a simple yet effective way to initialize vanilla RNNs and reported that they have achieved a good result in the Speech Recognition domain while having an effect similar to the one caused by Eq.", "startOffset": 0, "endOffset": 17}, {"referenceID": 14, "context": "Following Mikolov et al. (2011) we use the Penn Treebank Corpus to train our Language Modeling (LM) models.", "startOffset": 10, "endOffset": 32}, {"referenceID": 19, "context": "For training, we use plain SGD with batch size 32 with the maximum norm gradient clipping (Pascanu et al., 2013).", "startOffset": 90, "endOffset": 112}, {"referenceID": 14, "context": "The aforementioned choices were largely guided by the work of Mikolov et al. (2014)1.", "startOffset": 62, "endOffset": 84}, {"referenceID": 14, "context": "We found that to achieve reasonable results with vanilla RNNs on the LM task, it was important to construct training batches similarly to Mikolov et al. (2014), where the authors backpropagate 10 steps back every 5 steps forward.", "startOffset": 138, "endOffset": 160}, {"referenceID": 17, "context": "For LSTM and GRU networks we also present results when the dropout is applied directly to hidden states as in (Moon et al., 2015).", "startOffset": 110, "endOffset": 129}, {"referenceID": 15, "context": "The network inputs include only word embeddings (initialized with pretrained word2vec embeddings (Mikolov et al., 2013) and kept static) and capitalization features.", "startOffset": 97, "endOffset": 119}, {"referenceID": 4, "context": "For training we use the RMSProp algorithm (Dauphin et al., 2015) with \u03c1 fixed at 0.", "startOffset": 42, "endOffset": 64}, {"referenceID": 18, "context": "To speed up the training we use a length expansion approach described in (Ng et al., 2015), where training is performed in two stages: (i) we first sample short 5-words input sequences with their contexts and train for 25 epochs; (ii) we fine tune the network on input 15-words sequences for 10 epochs.", "startOffset": 73, "endOffset": 90}, {"referenceID": 21, "context": "We use Twitter sentiment corpus from SemEval-2015 Task 10 (subtask B) (Rosenthal et al., 2015).", "startOffset": 70, "endOffset": 94}, {"referenceID": 17, "context": "We demonstrate that recurrent dropout is most effective when applied to hidden state update vectors rather than hidden states; (ii) we observe an improvement in the network\u2019s performance when our recurrent dropout is coupled with the standard forward dropout, though the extent of this improvement depends on the values of dropout rates; (iii) contrary to our expectations, networks trained with per-step and per-sequence mask sampling produce similar results when using our recurrent dropout method, both being better than the dropout scheme proposed by Moon et al. (2015).", "startOffset": 555, "endOffset": 574}], "year": 2016, "abstractText": "This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feed-forward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most popular recurrent networks: vanilla RNNs, Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout.", "creator": "LaTeX with hyperref package"}}}