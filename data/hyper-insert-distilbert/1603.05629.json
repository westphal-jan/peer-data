{"id": "1603.05629", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2016", "title": "Discriminative Embeddings of Latent Variable Models for Structured Data", "abstract": "kernel classifiers and regressors structures designed for measuring structured parameter data, such as spatial sequences, trees and graphs, have significantly advanced in demonstrating a number features of broader interdisciplinary areas however such far as computational biology and drug device design. typically, kernel functions executed are designed beforehand for a balanced data processor type which adequately either exploit statistics while of interpreting the observed structures or make experimental use of intrinsic probabilistic generative models, such and then a complete discriminative array classifier macro is ultimately learned sufficiently based then on the kernels via convex optimization. furthermore however, such an elegant complex two - stage testing approach also limited kernel methods from scaling algorithms up to about millions of data points, and specifically exploiting discriminative information functions to typically learn some feature texture representations.", "histories": [["v1", "Thu, 17 Mar 2016 19:29:46 GMT  (598kb,D)", "http://arxiv.org/abs/1603.05629v1", "22 pages, 8 figures"], ["v2", "Sat, 19 Mar 2016 19:26:55 GMT  (751kb,D)", "http://arxiv.org/abs/1603.05629v2", "22 pages, 8 figures"], ["v3", "Sun, 29 May 2016 19:12:05 GMT  (722kb,D)", "http://arxiv.org/abs/1603.05629v3", "ICML 2016"], ["v4", "Mon, 26 Sep 2016 23:52:45 GMT  (753kb,D)", "http://arxiv.org/abs/1603.05629v4", "ICML 2016"]], "COMMENTS": "22 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hanjun dai", "bo dai", "le song"], "accepted": true, "id": "1603.05629"}, "pdf": {"name": "1603.05629.pdf", "metadata": {"source": "CRF", "title": "Discriminative Embeddings of Latent Variable Models for Structured Data", "authors": ["Hanjun Dai", "Bo Dai"], "emails": ["bodai}@gatech.edu,", "lsong@cc.gatech.edu"], "sections": [{"heading": null, "text": "We propose an effective and scalable approach for structured data representation which is based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Furthermore, our feature learning algorithm runs a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In real world applications involving sequences and graphs, we showed that the proposed approach is much more scalable than alternatives while at the same time produce comparable results to the state-of-the-art in terms of classification and regression."}, {"heading": "1 Introduction", "text": "Structured data, such as sequences, trees and graphs, are prevalent in a number of interdisciplinary areas such as protein design, genomic sequence analysis, and drug design (Scho\u0308lkopf et al., 2004). To learn from such complex data, we have to first transform such data explicitly or implicitly into some vectorial representations, and then apply machine learning algorithms in the resulting vector space. Kernel methods have emerged as one of the most effective tools for dealing with structured data, and have achieved the state-of-the-art classification and regression results in many sequence (Leslie et al., 2002a; Vishwanathan & Smola, 2003) and graph datasets (Ga\u0308rtner et al., 2003; Borgwardt, 2007).\nThe success of kernel methods on structured data relies crucially on the design of kernel functions \u2014 positive semidefinite similarity measures between pairs of data points (Scho\u0308lkopf & Smola, 2002). By designing a kernel function, we have implicitly chosen a corresponding feature representation for each data point which can potentially has infinite dimensions. Later learning algorithms for various tasks and with potentially very different nature can then work exclusively on these pairwise kernel values without the need to access the original data points. Such modular structure of kernel methods has been very powerful, making them the most elegant and convenient methods to deal with structured data. Thus designing kernel for different structured objects, such as strings, trees and graphs, has always been an important subject in the kernel community. However, in the big data era, this modular framework has also limited kernel methods in terms\nar X\niv :1\n60 3.\n05 62\n9v 1\n[ cs\n.L G\n] 1\n7 M\nof their ability to scale up to millions of data points, and exploit discriminative information to learn feature representations.\nFor instance, a class of kernels are designed based on the idea of \u201cbag of structures\u201d (BOS), where each structured data point is represented as a vector of counts for elementary structures. The spectrum kernel and variants for strings (Leslie et al., 2002a), subtree kernel (Ramon & Ga\u0308rtner, 2003), graphlet kernel (Shervashidze et al., 2009) and Weisfeiler-lehman graph kernel (Shervashidze et al., 2011) all follow this design principle. In other words, the feature representations of these kernels are fixed before learning, with each dimension corresponding to a substructure, independent of the supervised learning tasks at hand. Since there are many unique substructures which may or may not be useful for the learning tasks, the explicit feature space of such kernels typically has very high dimensions. Subsequently algorithms dealing with the pairwise kernel values have to work with a big kernel matrix squared in the number of data points. The square dependency on the number of data points largely limits these BOS kernels to datasets of size just thousands.\nA second class of kernels are based on the ingenious idea that the ability of probabilistic graphical models (GM) in describing noisy and structured data can be exploited to design kernels. For instance, one can use hidden Markov models for sequence data, and use pairwise Markov random fields for graph data. The Fisher kernel (Jaakkola & Haussler, 1999) and probability product kernel (Jebara et al., 2004) are two representative instances within the family. The former method first fits a common generative model to the entire dataset, and then uses the empirical Fisher information matrix and the Fisher score of each data point to define the kernel; The latter method instead fits a different generative model to each data point, and then uses inner products between distributions to define the kernel. Typically the parameterization of these GM kernels are chosen before hand. Although the process of fitting generative models allow the kernels to adapt to the geometry of the input data, the resulting feature representations are still independent of the discriminative task at hand. Furthermore, the extra step of fitting generative models to data can be a challenging computation and estimation task by itself, especially in the presence of latent variables. Very often in practice, one finds that BOS kernels perform better than GM kernels, although the latter is supposed to capture the additional geometry and uncertainty information of data.\nIn this paper, we wish to revisit the idea of using graphical models for kernel or feature space design, with the goal of scaling up kernel methods for structured data to millions of data points, and allowing the kernel to learn the feature representation from label information. Our idea is to model each structured data point as a latent variable model, then embed the graphical model into feature spaces (Smola et al., 2007; Song et al., 2009), and use inner product in the embedding space to define kernels. Instead of fixing a feature or embedding space beforehand, we will also learn the feature space by directly minimizing the empirical loss defined by the label information.\nThe resulting embedding algorithm runs in a scheme similar to graphical model inference procedures, such as mean field and belief propagation. But instead of performing probabilistic operations (such as sum, product and renormalization), the algorithm performs nonlinear function mappings in each step, inspired by kernel message passing algorithm in Song et al. (2010, 2011). Our method is also very different from the kernel message passing algorithm in several senses. First, we are dealing with a different scenario, i.e., learning similarity measure for structured data. Second, we will learn the nonlinear mappings using the discriminative information. And third, a variant of our algorithm runs in a mean field update fashion, which is not present in kernel message passing algorithm.\nBesides the above novel aspects, the new method is also very scalable in terms of both memory and computation requirements. First, it uses a small and explicit feature map for the nonlinear feature space, and avoid the need for keeping the kernel matrix. This makes the subsequent classifiers or regressors order of magnitude smaller compared to other kernels in big data setting. Second, the nonlinear function mapping in our method can be learned using stochastic gradient descent, allowing it to handle extremely large scale datasets.\nFinally in experiments, we show that our method compares favorably to other kernel methods in terms of classification accuracy in medium scale sequence and graph benchmark datasets including SCOP and NCI. Furthermore, our method can handle extremely large data set, such as the 2.3 million molecule dataset from\nHarvard Clean Energy Project, and achieved state-of-the-art accuracy. These empirical results suggest that the graphical models, theoretically well-grounded methods for capturing structure in data, combined with embedding techniques and discriminative training can significantly improve the performance in many large scale real-world structured data classification and regression problems."}, {"heading": "2 Backgrounds", "text": "We denote by X a random variable with domain X , and refer to instantiations of X by the lower case character, x. We denote a density on X by p(X), and denote the space of all such densities by P. We will also deal with multiple random variables, X1, X2, . . . , X`, with joint density p(X1, X2, . . . , X`). For simplicity of notation, we assume that the domains of all Xt, t \u2208 [`] are the same, but the methodology applies to the cases where they have different domains. In the case when X is a discrete domain, the density notation should be interpreted as probability, and integral should be interpreted as summation instead. Furthermore, we denote by H a hidden variable with domain H and distribution p(H). We use similar notation convention for variable H and X.\nKernel Methods. Kernel methods owe the name to the use of kernel functions, k(x, x\u2032) : X\u00d7X 7\u2192 R, which are symmetric positive definite (PD), meaning that for all n > 1, and x1, . . . , xn \u2208 X , and c1, . . . , cn \u2208 R, we have \u2211n i,j=1 cicjk(xi, xj) > 0. A signature of kernel methods is that learning algorithms for various tasks and with potentially very different nature can work exclusively on these pairwise kernel values without the need to access the original data points.\nKernels for Structured Data. Kernel functions play the key role in kernel methods. Each kernel function will correspond to some feature map \u03c6(x), where the kernel function can be expressed as the inner product between feature maps, i.e., k(x, x\u2032) = \u3008\u03c6(x), \u03c6(x\u2032)\u3009. For structured input domain, one can design kernels using counts on substructures. For instance, the spectrum kernel for two sequences x and x\u2032 is defined as (Leslie et al., 2002a)\nk(x, x\u2032) = \u2211\ns\u2208S #(s \u2208 x)#(s \u2208 x\u2032) (1)\nwhere S is the set of possible subsequences, #(s \u2208 x) counts the number occurrence of subsequence s in x. In this case, the feature map \u03c6(x) = (#(s1 \u2208 x),#(s2 \u2208 x), ...)> corresponds to a vector of dimension |S|. Similarly, the graphlet kernel (Shervashidze et al., 2009) for two graphs x and x\u2032 can be defined as in the same form as (1), where S is now the set of possible subgraphs, #(s \u2208 x) counts the number occurrence of subgraphs. We refer to this class of kernels as \u201cbag of structures\u201d (BOS) kernel.\nKernels can also be defined by leveraging the power of probabilistic graphical models. For instance, the Fisher kernel (Jaakkola & Haussler, 1999) is defined using a parametric model p(x|\u03b8\u2217) around its maximum likelihood estimate \u03b8\u2217, i.e., k(x, x\u2032) = U>x I\n\u22121Ux\u2032 , where Ux := \u2207\u03b8=\u03b8\u2217 log p(x|\u03b8) and I = EX [UXU>X ] is the Fisher information matrix. Another classical example along the same line is the probability product kernel (Jebara et al., 2004). Different from the Fisher kernel based on generative model fitted with the whole dataset, the probability product kernel is calculated based on the models p(x|\u03b8) fitted to individual data point, i.e., k(x, x\u2032) = \u222b X p(\u03c4 |\u03b8x)\n\u03c1p(\u03c4 |\u03b8x\u2032)\u03c1d\u03c4 where \u03b8x and \u03b8x\u2032 are the maximum likelihood parameters for data point x and x\u2032 respectively. We refer to this class of kernels as the \u201cgraphical model\u201d (GM) kernels.\nHilbert Space Embedding of Distributions. Hilbert space embeddings of distributions are mappings of distributions into potentially infinite dimensional feature spaces (Smola et al., 2007),\n\u00b5X := EX [\u03c6(X)] = \u222b X \u03c6(x)p(x)dx : P 7\u2192 F (2)\nwhere the distribution is mapped to its expected feature map, i.e., to a point in a feature space. Kernel embedding of distributions has rich representational power. Some feature map can make the mapping injective (Sriperumbudur et al., 2008), meaning that if two distributions, p(X) and q(X), are different, they are mapped to two distinct points in the feature space. For instance, when X = Rd, the feature spaces of many commonly used kernels, such as the Gaussian RBF kernel exp(\u2212\u2016x\u2212 x\u2032\u201622), can make the embedding\ninjective. Alternatively, one can treat an injective embedding \u00b5X of a density p(X) as a sufficient statistic of the density. Any information we need the density is preserved in \u00b5X : with \u00b5X one can uniquely recover p(X), and any operation on p(X) can be carried out via a corresponding operation on \u00b5X with the same result. For instance, this property will allow us to compute a functional f : P 7\u2192 R of the density using the embedding only, i.e.,\nf(p(x)) = f\u0303(\u00b5X) (3)\nwhere f\u0303 : F 7\u2192 R is a corresponding function applied on \u00b5X . Similarly the property can also be generalized to operators. For instance, applying an operator T : P 7\u2192 Rd to a density can also be equivalently carried out using its embedding, i.e.,\nT \u25e6 p(x) = T\u0303 \u25e6 \u00b5X , (4)\nwhere T\u0303 : F 7\u2192 Rd is the alternative operator working on the embedding. In our later sections, we will extensively exploit this property of injective embeddings, by assuming that there exists such a feature space such that the embeddings are injective."}, {"heading": "3 Model for a Structured Data Point", "text": "Without loss of generality, we assume each structured data point \u03c7 is a graph, with a set of nodes V = {1, . . . , V } and a set of edges E . We will use xi to denote the value of the node label for node i. We note the node labels are different from the label of the entire data point. For instance, each atom in a molecule will correspond to a node in the graph, and the node label will be the atomic number, while the label for the entire molecule can be whether the molecule is a good drug or not. Other structures, such as sequences and trees, can be viewed as special cases of general graphs.\nWe will model the structured data point \u03c7 as an instance drawn from a graphical model. More specifically, we will model the label of each node in the graph with a variable Xi, and furthermore, associate an additional hidden variable Hi with it. Then we will define a pairwise Markov random field on these collection of random variables\np({Hi} , {Xi}) \u221d \u220f i\u2208V \u03a6(Hi, Xi) \u220f (i,j)\u2208E \u03a8(Hi, Hj) (5)\nwhere \u03a8 and \u03a6 are nonnegative node and edge potentials respectively. In this model, the variables are connected according to the graph structure of the input data point. That is to say, we use the graph structure of the input data directly as the conditional independence structure of an undirected graphical model. Figure 1 illustrates two concrete examples in constructing the graphical models for strings and graphs.\nOne can design more complicated graphical models which go beyond pairwise Markov random fields, and consider longer range interactions with potentials involving more variables. We will focus on pairwise Markov random fields for simplicity of representation.\nWe note that such a graphical model is built for each individual data point, and the conditional independence structures of two graphical models can be different if the two data points \u03c7 and \u03c7\u2032 are different. Furthermore, we do not observe the value for the hidden variables {Hi}, which makes the learning of the graphical model potentials \u03a6 and \u03a8 even more difficult. Thus, we will not purse the standard route of maximum likelihood estimation, and rather we will consider the sequence of computations needed when we try to embed the posterior of {Hi} into a feature space."}, {"heading": "4 Embedding Latent Variable Models", "text": "We will embed the posterior marginal p(Hi| {xi}) of a hidden variable using a feature map \u03c6(Hi), i.e.,\n\u00b5i = \u222b H \u03c6(hi)p(hi| {xi})dhi. (6)\nThe exact form of \u03c6(Hi) is not fixed at the moment, and we will learn it later using supervision signals. For now, we will assume that \u03c6(Hi) \u2208 Rd is a finite dimensional feature space, and the exact value of d will determined by cross-validation in later experiments. However, compute the embedding is a very challenging task for general graphs: it involves performing an inference in graphical model where we need to integrate out all variables expect Hi, i.e.,\np(Hi| {xi}) = \u222b HV\u22121 p(Hi, {hj} | {xj}) \u220f j\u2208V\\i dhj . (7)\nOnly when the graph structure is a tree, exact computation can be carried out efficiently via a message passing algorithm (Pearl, 1988). Thus in the general case, approximate inference algorithms are developed which include mean field inference and loopy belief propagation (BP). In many applications, however, the resulting mean fields and loopy BP algorithm exhibits excellent empirical performance (Murphy et al., 1999). Several theoretical studies have also provided insight into the approximations made by loopy BP, partially justifying its application to graphs with cycles (Wainwright & Jordan, 2008; Yedidia et al., 2001a).\nIn the following subsection, we will explain the mean field, loopy BP and generalized BP algorithms. We show that the iterative update steps in these algorithms, which are essentially minimizing approximations to the exact free energy, can be simply viewed as function mappings of the embedded marginals using the alternative view in (3) and (4)."}, {"heading": "4.1 Embedding Mean-Field Inference", "text": "The vanilla mean-field inference tries to approximate p({Hi} | {xi}) with a product of independent density components p({Hi} | {xi}) \u2248 \u220f i\u2208V qi(hi) where each qi(hi) > 0 is a valid density, such that \u222b H qi(hi)dhi = 1. Furthermore, these density components are found by minimizing the following variational free energy (Wainwright & Jordan, 2008),\nmin q1,...,qd \u222b Hd \u220f i\u2208V qi(hi) log \u220f i\u2208V qi(hi) p({hi} | {xi}) \u220f i\u2208V dhi. (8)\nOne can show that the solution to the above optimization problem needs to satisfy the following fixed point equations for all i \u2208 V\nlog qi(hi) = ci + log(\u03a6(hi, xi)) + \u2211\nj\u2208N (i)\n\u222b H qj(hj) log(\u03a8(hi, hj)\u03a6(hj , xj))dhj\nAlgorithm 1 Embedded Mean Field\n1: Input: parameter W and V in the operator T\u0303 2: Initialize \u00b5\u0303 (0) i = 0, for all i \u2208 V 3: for t = 1 to T do 4: for i \u2208 V do 5: li = \u2211 j\u2208N (i) \u00b5\u0303 (t\u22121) i , ui = \u2211 j\u2208N (i) xj 6: \u00b5\u0303 (t) i = V \u03c3(W1xi +W2li +W3ui) 7: end for 8: end for{fixed point equation update} 9: return {\u00b5\u0303Ti }i\u2208V\nAlgorithm 2 Embedding Loopy BP\n1: Input: parameter W and V in T\u03031 and T\u03032 2: Initialize \u03bd\u0303\n(0) ij = 0, for all (i, j) \u2208 E\n3: for t = 1 to T do 4: for (i, j) \u2208 E do 5: \u03bd\u0303tij = V1\u03c3(W1xi+W2 \u2211 k\u2208N (i)\\j [\u03bd\u0303 (t\u22121) ki ]) 6: end for 7: end for 8: for i \u2208 V do 9: \u00b5\u0303i = V2\u03c3(W3xi +W4 \u2211 k\u2208N (i)\\j \u03bd\u0303 (T ) ki )\n10: end for 11: return {\u00b5\u0303i}i\u2208V\nwhere N (i) are the set of neighbors of variable Hi in the graphical model, and ci is a constant. The fixed point equations in (9) imply that qi(hi) is a functional of a set of neighboring marginals {qj}j\u2208N (i), i.e.,\nqi(hi) = f ( hi, xi, {qj}j\u2208N (i) , {xj}j\u2208N (i) ) . (9)\nIf for each marginal qi, we have an injective embedding\n\u00b5\u0303i = \u222b H \u03c6(hi)qi(hi)dhi,\nthen, using similar reasoning as in (3), we can equivalently express the fixed point equation from an embedding point of view, i.e., qi(hi) = f\u0303(hi, xi, {\u00b5\u0303j}j\u2208N (i), {xj}j\u2208N (i)), and consequently using view from (4), we have\n\u00b5\u0303i = T\u0303 \u25e6 ( xi, {\u00b5\u0303j}j\u2208N (i) , {xj}j\u2208N (i) ) . (10)\nFor this embedded mean field equation in (10), the function f\u0303 and operator T\u0303 have a complicated nonlinear dependency on the potential functions \u03a8 and \u03a6 which are unknown and need to learned from data. Instead of first learning the \u03a8 and \u03a6, and then working out T\u0303 , we will pursue a different route and directly parameterize T\u0303 and later learn it with supervision signals.\nIn terms of the parameterization, we will assume \u00b5\u0303i \u2208 Rd where d is a hyperparameter chosen using cross-validation. For T\u0303 , one can use any nonlinear function mappings. For instance, we can parameterize it as a neural network\n\u00b5\u0303i = V \u03c3 ( W1xi +W2 \u2211 j\u2208N (i) \u00b5\u0303j +W3 \u2211 j\u2208N (i) xj ) (11)\nwhere \u03c3(\u00b7) := max{0, \u00b7} is a rectified linear unit applied elementwisely to its argument, and W = {W1,W2,W3}, V are matrices of parameters of appropriate size. The number of the rows in W corresponds to the number of hidden units b, and the number of rows in V is equal to d, the embedding dimension. In such parametrization, the V is only used for rescaling the range of message embeddings. In fact, we argue that with or without V , the functions will be the same in terms of representing ability. Specifically, for any (W, V ), we can always find another \u2018equivalent\u2019 parameters (W\u2032, Ib\u00d7d) where W\n\u2032 = {W1,W2V,W3} such that the outputs are in the range of [0, 1]. With this parameterization, the mean field iterative update in the embedding space can be carried out as Algorithm 1."}, {"heading": "4.2 Embedding Loopy Belief Propagation", "text": "Loopy belief propagation is another variational inference method, which is essentially optimizing the Bethe free energy which generalizes the variational free energy by taking pairwise interactions into account (Yedidia et al., 2001b),\nmin {qij}(i,j)\u2208E \u2212 \u2211 i (|N (i)| \u2212 1) \u222b H qi(hi) log qi(hi) \u03a6(hi, xi) dhi + \u2211 i,j \u222b H2 qij(hi, hj) log\nqij(hi, hj)\n\u03a8(hi, hj)\u03a6(hi, xi)\u03a6(hj , xj) dhidhj (12)\nsubject to pairwise marginal consistency constraints: \u222b H qij(hi, hj)dhj = qi(hi), \u222b H qij(hi, hj)dhj = qi(hi),\nand \u222b H qi(hi)dhi = 1. One can obtain the fixed point condition for the above optimization for all (i, j) \u2208 E ,\nmij(hj) \u221d \u222b H \u220f k\u2208N (i)\\j mki(hi)\u03a6i(hi, xi)\u03a8ij(hi, hj)dhi,\nqi(hi) \u221d \u03a6(hi, xi) \u220f\nj\u2208N (i)\nmji(hi). (13)\nwhere mij(hj) is the intermediate result called the message from node i to j. Furthermore, mij(hj) is a nonnegative function which can be normalized to a density, and hence can also be embedded.\nSimilar to the reasoning in the mean field case, the (13) implies the messages mij(hj) and marginals qi(hi) are functionals of messages from neighbors, i.e.,\nmij(hj) = f ( hj , xi, {mki}k\u2208N (i)\\j ) ,\nqi(hi) = g ( hi, xi, {mki}k\u2208N (i) ) .\nWith the assumption that there is an injective embedding for each message \u03bd\u0303ij = \u222b \u03c6(hj)mij(hj)dhj and for\neach marginal \u00b5\u0303i = \u222b \u03c6(hi)qi(hi)dhi, we can apply the reasoning from (3) and (4), and express the messages and marginals from the embedding view,\n\u03bd\u0303ij = T\u03031 \u25e6 ( xi, {\u03bd\u0303ki}k\u2208N (i)\\j ) , (14)\n\u00b5\u0303i = T\u03032 \u25e6 ( xi, {\u03bd\u0303ki}k\u2208N (i) ) . (15)\nWith similar strategy as in mean field case, we will learn the parameters in T\u03031 and T\u03032 with supervision signals from the discriminative task.\nFurthermore, we will also use parametrization for loopy BP embedding similar to the mean field case, i.e., neural network with rectified linear unit \u03c3. Specifically, assume \u03bd\u0303ij \u2208 Rd, \u00b5\u0303i \u2208 Rd\n\u03bd\u0303ij = V1\u03c3 ( W1xi +W2 \u2211 k\u2208N (i)\\j \u03bd\u0303ki ) (16)\n\u00b5\u0303i = V2\u03c3 ( W3xi +W4 \u2211 k\u2208N (i) \u03bd\u0303ki ) (17)\nwhere W = {W1,W2,W3,W4}, V = {V1, V2} are matrices with appropriate sizes. Note that one can use other nonlinear function mappings to parameterize T\u03031 and T\u03032 as well. Overall, the loopy BP embedding updates is summarized in Algorithm 2. Remark: In fact, on pairwise MRF, the update of Expectation Propagation (EP) is exactly the same as loopy BP, therefore, resulting the same embedding forms for EP. Besides the loopy BP, several other double-loop message passing algorithms have been proposed to optimize the Bethe energy by exploiting different optimization techniques, e.g., convex-concave decomposition (Yuille, 2002) and duality (Minka, 2001). The dependences in these updates have different forms. With the injective embedding assumptions for corresponding messages, we can express the messages in embedding view\n\u03bd\u0303ij = T\u03031 \u25e6 ( xi, {\u03bd\u0303ki}k\u2208N (i)\\j , \u03bd\u0303ji, \u00b5\u0303i ) ,\nor \u03bd\u0303ij = T\u03031 \u25e6 ( xi, {\u03bd\u0303ki}k\u2208N (i) , \u00b5\u0303i ) ,\n\u00b5\u0303i = T\u03032 \u25e6 ( xi, {\u03bd\u0303ki}k\u2208N (i) , \u00b5\u0303i ) .\nBy optimizing a convexified Bethe free energy, Wainwright et al. (2003) propose the tree-reweighted version BP with weight for each message, {vij}i,j\u2208E , in spanning tree polytope. Similarly, the embedded messages,\nas well as the marginals on nodes and edges can be obtained as \u03bd\u0303ij = T\u03031 \u25e6 ( xi, {\u03bd\u0303ki}k\u2208N (i)\\j , \u03bd\u0303ji, {vki}k\u2208N i\\j , vij ) ,\n\u00b5\u0303i = T\u03032 \u25e6 ( xi, {\u03bd\u0303ki, vki}k\u2208N (i) ) ,\n\u03b7\u0303ij = T\u03033 \u25e6 ( xi, xj , {\u03bd\u0303ki, vki}k\u2208N (i) , {\u03bd\u0303kj , vkj}k\u2208N (j) ) .\nWe can parametrize these messages and marginals within the same function class, i.e., neural networks with ReLU. More generally, we can also treat the weights as parameters and learn them together. For the details of message updates in these algorithms, please refer to Appendix B."}, {"heading": "4.3 Embedding Generalized Belief Propagation", "text": "The Kikuchi free energy is the generalization of the Bethe free energy by involving high-order interactions. More specifically, given the MRFs, we denote R to be a set of regions, i.e., some basic clusters of nodes, their intersections, the intersections of the intersections, and so on. We denote the sub(r) or sup(r), i.e., subregions or superregions of r, as the set of regions completely contained in r or containing r, respectively. Let hr be the state of the nodes in region r, then, the Kikuchi free energy is\u2211\nr\u2208R cr\n(\u222b q(hr) log\nq(hr)\u220f i,j\u2208r \u03a8(hi, hj) \u220f i\u2208r \u03a6(hi, xi)\n) ,\nwhere cr is over-counting number of region r, defined by cr := 1\u2212 \u2211 s\u2208sup(r) cs with cr = 1 if r is the largest region in R. It is straightforward to verify that the Bethe free energy is a special case of the Kikuchi free energy by setting the basic cluster as pair of nodes. The generalized loopy BP (Yedidia et al., 2001b) is trying to seek the stationary points of the Kikuchi free energy under regional marginal consistency constraints and density validation constraints by following messages updates,\nmr,s(hs) \u221d \u222b \u03a8(hr, xr\\s)m\u0304r\\s(hr\\s)dhr\\s\nm\u0304r,s(hs) , qr(hr) \u221d \u220f i\u2208r \u03a6(hi, xi) \u220f\nmr\u2032,s\u2032\u2208M(r)\nmr\u2032,s\u2032(hs\u2032), (18)\nwhere\nm\u0304r\\s(hr\\s) = \u220f\n{r\u2032,s\u2032}\u2208M(r)\\M(s)\nmr\u2032,s\u2032(hs\u2032), m\u0304r,s(hs) = \u220f\n{r\u2032,s\u2032}\u2208M(r,s)\nmr\u2032,s\u2032(hs\u2032), \u03a8(hr, xr\\s) = \u220f i,j\u2208r \u03a8(hi, hj) \u220f i\u2208r\\s \u03a6(hi, xi).\nThe M(r) denotes the indices of messages mr\u2032,s\u2032 that s \u2032 \u2208 sub(r) \u222a {r}, and r\u2032 \\ s\u2032 is outside r. M(r, s) is the set of indices of messages mr\u2032,s\u2032 where r \u2032 \u2208 sub(r) \\ s and s\u2032 \u2208 sub(s) \u222a {s}.\nWith the injective embedding assumption for each message \u03bd\u0303r,s = \u222b \u03c6(hs)mr,s(hs)dhs and \u00b5\u0303r = \u222b \u03c6(hr)qr(hr)dhr,\nfollowing the reasoning (3) and (4), we can express the embeddings as \u03bd\u0303r,s = T\u03031 \u25e6 ( xr\\s, {\u03bd\u0303r\u2032,s\u2032}M(r)\\M(s),M(r,s) ) , (19)\n\u00b5\u0303r = T\u03032 \u25e6 ( xr, {\u03bd\u0303r\u2032,s\u2032}M(r) ) . (20)\nFollowing the same parameterization in loopy BP, we represent the embeddings by neural network with\nrectified linear units,\n\u03bd\u0303r,s =V1\u03c3 (\u2211 i\u2208r W i1x i r +W2 \u2211 M(r)\\M(s)\u0303 \u03bdr\u2032,s\u2032 \u2212W3 \u2211 M(r,s)\u0303 \u03bdr\u2032,s\u2032 )\n(21) \u00b5\u0303i = V2\u03c3 (\u2211 i\u2208r W i4xi +W5 \u2211 M(r) \u03bd\u0303r\u2032,s\u2032 ) (22)\nwhere W = {{W i1},W2,W3, {W i4},W5}, V = {V1, V2} are matrices with appropriate sizes. The generalized BP embedding updates will be almost the same as Algorithm 2 except the order of the iterations. We start from the messages into the smallest region first (Yedidia et al., 2001b).\nRemark: The choice of basis clusters and the form of messages determine the dependency in the embedding. Please refer to Yedidia et al. (2005) for details about the principles to partition the graph structure, and several other generalized BP variants with different messages. The algorithms proposed for minimizing the Bethe free energy (Minka, 2001; Heskes, 2002; Yuille, 2002) can also be extended for Kikuchi free energy, resulting in different embedding forms."}, {"heading": "5 Discriminative Training", "text": "Similar to kernel BP (Song et al., 2010, 2011) and kernel EP (Jitkrittum et al., 2015), our current work exploits embedding to reformulate the graphical models and avoids explicitly learning the potential functions of the graphical models. However, different from the kernel BP and kernel EP, in which the feature space for embeddings is chosen before hand due to the choice of the kernel function, and only the parameters in messages are learned from data by regressing through feature spaces, we will learn both the feature spaces, the transformation T\u0303 , as well as the ultimate regressor or classifier for the target discriminative task using supervision signals.\nSpecifically, we are provided with a training dataset D = {\u03c7n, yn}Nn=1, where \u03c7n is a structured data point and yn \u2208 R and yn \u2208 {1, . . . ,K} for regression or classification problem, respectively. With the model introduced in Section 3, we represent each \u03c7n by pn. Denote the embedded distribution as p\u0303n(\u03c7n;\u03c6) \u2208 P\u0303 with mapping \u03c6, the learning task becomes finding both \u03c6 and f : P\u0303 \u2192 Y.\nIn the case of regression problem, with the parametrization introduced in Section4, we will learn the embedding parameters by minimizing the empirical square loss,\nmin u,V,W N\u2211 n=1\n( yn \u2212 u>\u03c3 ( Vn\u2211 i=1 \u00b5\u0303ni ))2 . (23)\nwhere u \u2208 Rd is the final mapping from embeddings to real output. Note that each data point will have its own graphical model and embedded inference procedure, but the parameters V and W involved nonlinear mappings are shared across these graphical models.\nIn the case of K-class classification problem, we denote z is the 1-of-K representation of y, i.e., z \u2208 {0, 1}K , zk = 1 if y = k, and zi = 0, \u2200i 6= k. By adopt the softmax loss, we obtain the optimization for embedding parameters and discriminative classifier estimation as,\nmin u={uk}Kk=1,V,W N\u2211 n K\u2211 k=1 \u2212zkn log uk\u03c3 ( Vn\u2211 i=1 \u00b5\u0303ni ) , (24)\nwhere u = {uk}Kk=1, uk \u2208 Rd are the parameters for mapping embedding to output. The same idea can also be generalized to other discriminative tasks with different loss functions. As we can see from the optimization problems (23) and (24), the objective functions are directly related to the corresponding discriminative tasks, and so as to V, W and u. Conceptually, the procedure starts with representing each datum by a graphical model constructed corresponding to its individual structure, and then, we embed these graphical models with shared mappings. Finally the embedded marginals are aggregated with u for prediction.\nWe optimize the objective (23) or (24) with stochastic gradient descent for scalability consideration.\nAlgorithm 3 Discriminative Embedding\nInput: Dataset D = {\u03c7n, yn}Nn=1, loss function l(f(x), y). Initialize U0 = {V0,W0,u0} randomly. for t = 1 to T do\nSample {\u03c7t, yt} uniform randomly from D. Construct latent variable model p({Hti }|\u03c7n) as (5). Embed p({Hti }|\u03c7n) as {\u00b5\u0303ni }i\u2208Vn by Algorithm 1 or 2 with V t\u22121, Wt\u22121.\nUpdate Ut = Ut\u22121 + \u03bbt\u2207Ut\u22121 l(f(\u00b5\u0303n;Ut\u22121), yn). end for return UT = {VT ,WT ,uT }\nHowever, other optimization algorithms are also applicable, and our method does not depend on this particular choice. The gradients of the parameters W and V are calculated recursively similar to recurrent neural network for sequence models. In our case, the recursive structure will correspond the message passing structure. For details of the gradient calculation, please refer to Appendix C. The overall framework is illustrated in Algorithm 3."}, {"heading": "6 Related Work", "text": ""}, {"heading": "6.1 Comparison with Neural Networks on Graphs", "text": "Neural network is also a powerful tool on graph structured data. Scarselli et al. (2009) proposed a neural network which generates features by solving a heuristic nonlinear system iteratively, and is learned using Almeida-Pineda algorithm. To guarantee the existence of the solution to the nonlinear system, there are extra requirements for the features generating function. From this perspective, the model in (Li et al., 2015) can be considered as an extension of (Scarselli et al., 2009) where the gated recurrent unit is used for feature generation. Rather than these heuristic models, our model is based on the principled graphical model embedding framework, which results in flexible embedding functions for generating features. Meanwhile, the model can be learned efficiently by traditional stochastic gradient descent.\nSome works transfer locality concept of convolutional neural networks (CNN) from Euclidean domain to graph case, using hierarchical clustering, graph Laplacian (Bruna et al., 2013), or graph Fourier transform (Henaff et al., 2015). These models are still restricted to problems with the same graph structure, which is not suitable for learning with molecules. Mou et al. (2016) proposed a convolution operation on trees, while the locality are defined based on parent-child relations. Duvenaud et al. (2015) used CNN to learn the circulant fingerprints for graphs from end to end. The dictionary of fingerprints are maintained using softmax of subtree feature representations, in order to obtain a differentiable model. If we unroll the steps in Algorithm 3, it can also be viewed as an end to end learning system. However, the structures of the proposed model are deeply rooted in graphical model embedding, from mean field and loopy BP, respectively. Also, since the parameters will be shared across different unrolling steps, we would have more compact model. As will be shown in the experiment section, our model is easy to train, while yielding good generalization ability."}, {"heading": "6.2 Comparison with Learning Message Estimator", "text": "By recognizing inference as computational expressions, inference machines (Ross et al., 2011) incorporate learning into the messages passing inference for CRFs. More recently, Hershey et al. (2014); Zheng et al. (2015); Lin et al. (2015) designed specific recurrent neural networks and convolutional neural networks for imitating the messages in CRFs. Although these methods share the similarity, i.e., bypassing learning potential function, to the proposed framework, there are significant differences comparing to the proposed framework.\nThe most important difference lies in the learning setting. In these existing messages learning work (Hershey et al., 2014; Zheng et al., 2015; Lin et al., 2015), the learning task is still estimating the messages represented graphical models with designed function forms, e.g., RNN or CNN, by maximizing loglikelihood. While in our work, we represented each structured data as a distribution, and the learning task is regression or classification over these distributions. Therefore, we treat the embedded models as samples, and learn the nonlinear mapping for embedding, and regressor or classifier, f : P \u2192 Y, over these distributions jointly, with task-dependent user-specified loss functions.\nAnother difference is the way in constructing the messages forms, and thus, the neural networks architecture. In the existing work, the neural networks forms are constructed strictly follows the message updates forms (9) or (13). Due to such restriction, these works only focus on discrete variables with finite values, and is difficult to extend to continuous variables because of the integration. However, by exploiting the embedding point of view, we are able to build the messages with more flexible forms without losing the dependencies. Meanwhile, the difficulty in calculating integration for continuous variables is no longer a problem with the reasoning (3) and (4)."}, {"heading": "7 Experiments", "text": "Below we first compare our algorithm with kernel methods on string and graph benchmark datasets. Then we focus on Harvard Clean Energy Project dataset which contains 2.3 million samples. We demonstrate that while getting comparable performance on medium sized dataset, we are able to handle millions of data points, and getting much better when more training samples are given. The two proposed algorithms are denoted as DE-MF and DE-LBP, which stands for discriminative embedding using mean field or loopy belief propagation, respectively.\nSome kernel methods for structured data are included as baseline algorithm. Specifically, we compare with the spectrum string kernel (Leslie et al., 2002a) and mismatch string kernel (Leslie et al., 2002b) on string datasets. On graph benchmark datasets, we compare with subtree kernel (Ramon & Ga\u0308rtner, 2003) (R&G, for short), random walk kernel(Ga\u0308rtner et al., 2003; Vishwanathan et al., 2010), shortest path kernel (Borgwardt & Kriegel, 2005), graphlet kernel(Shervashidze et al., 2009) and the family of WeisfeilerLehman kernels (Shervashidze et al., 2011) (WL kernel, for short). We didn\u2019t compare with fisher kernel and probabilistic product kernel in the experiments since they are taking too much time to run, and some of our baselines (e.g., Leslie et al. (2002a)) has already shown better results than those methods.\nThe code of baseline algorithms are obtained from authors\u2019 website. Our algorithms are implemented with C++ and CUDA, and experiments are carried out on clusters equipped with NVIDIA Tesla K20. The code will be made available online once published."}, {"heading": "7.1 Benchmark structure datasets", "text": "Without explicitly mentioned, we perform cross validation for all methods, and report the average performance. For structured kernel methods, we tune the degree in {1, 2, . . . , 10} (for mismatch kernel, we also tune the maximum mismatch length in {1, 2, 3}) and train SVM classifier (Chang & Lin, 2001) on top, where the trade-off parameter C is also chosen in {0.01, 0.1, 1, 10} by cross validation.\nFor our methods, we simply use one-hot vector (the vector representation of discrete node label) as the embedding for observed nodes, and use a two-layer neural network for the embedding (prediction) of target value. The hidden layer size b \u2208 {16, 32, 64} of neural network, the embedding dimension d \u2208 {16, 32, 64} of hidden variables and the number of iterations t \u2208 {1, 2, 3, 4} are tuned via cross validation. We keep the number of parameters small, and use early stopping (Giles, 2001) to avoid overfitting in these small datasets."}, {"heading": "7.1.1 String Dataset", "text": "Here we do experiments on two string binary classification benchmark datasets. The first one (denoted as SCOP) contains 7329 sequences obtained from SCOP (Structural Classification of Proteins) 1.59 database (An-\ndreeva et al., 2004). Methods are evaluated on the ability to detect members of a target SCOP family (positive test set) belonging to the same SCOP superfamily as the positive training sequences, and no members of the target family are available during training. We use the same 54 target families and the same training/test splits as in remote homology detection (Kuang et al., 2005). The second one is FC and RES dataset (denoted as FC RES) provided by CRISPR/Cas9 system. It asks the algorithm to tell whether the guide RNA will direct Cas9 to target DNA, and totally 5310 guides are included. Details of this dataset can be found in Doench et al. (2014); Fusi et al. (2015).\nWe use two variants for spectrum string kernel: 1) kmer-single, where the constructed kernel matrix K (s) k only consider patterns of length k; 2) kmer-concat, where kernel matrix K(c) = \u2211k i=1K (s) k . We also find the normalized kernel matrix KNormk (x, y) = Kk(x,y)\u221a\nKk(x,x)Kk(y,y) helps.\nFC RES SCOP kmer-single 0.7606\u00b10.0187 0.7097\u00b10.0504 kmer-concat 0.7576\u00b10.0235 0.8467\u00b10.0489 mismatch 0.7690\u00b10.0197 0.8637\u00b10.1192 DE-MF 0.7713\u00b10.0208 0.9068\u00b10.0685 DE-LBP 0.7701\u00b10.0225 0.9167\u00b10.0639\nTable 1: Mean AUC on string classification datasets\nTable 1 reports the AUC of different algorithms. We found our proposed discriminative embedding is consistently better than the string kernels. Also, the improvement in SCOP is more significant than in FC RES. Since SCOP is a protein dataset, while FC RES is working on RNA sequences, the alphabet size |\u03a3| of former one is much larger than the latter one. Consider the dimension of k-mer kernel explicit features is O(|\u03a3|k), which makes the off-diagonal entries of kernel matrix very small (or even zero) with large alphabet size and k. That\u2019s also the reason why kmer-concat performs better than kmer-single. We argue that the learned embedding is more discriminative than pre-defined feature space in this case."}, {"heading": "7.1.2 Graph Dataset", "text": "We use the following five commonly used datasets in the literature of graph kernel: MUTAG, NCI1, NCI109, ENZYMES and D&D. MUTAG (Debnath et al., 1991), NCI1 and NCI109 (Wale et al., 2008) are chemical compounds dataset, while ENZYMES (Borgwardt & Kriegel, 2005) and D&D (Dobson & Doig, 2003) are\nof proteins. The task is to do multi-class or binary classification. We show the detailed statistics of these datasets in Table 2.\nThe results of baseline algorithms are taken from Shervashidze et al. (2011) since we use exactly the same setting here. From the accuracy comparison shown in Figure 3, we can see the proposed embedding methods are comparable to other graph kernels, on different graphs with different number of labels, nodes and edges. Also, in dataset D&D which consists of 82 different types of labels, our algorithm performs much better. As reported in Shervashidze et al. (2011), the time required for constructing dictionary for the graph kernel can take up to more than a year of CPU time in this dataset, while our algorithm can learn the discriminative embedding from structured data directly without the construction of dictionary."}, {"heading": "7.2 Harvard Clean Energy Project(CEP) dataset", "text": "The Harvard Clean Energy Project (Hachmann et al., 2011) is a theory-driven search for the next generation of organic solar cell materials. One of the most important properties of molecule for this task is the overall efficiency of the energy conversion process in a solar cell, which is determined by the power conversion efficiency (PCE). The Clean Energy Project (CEP) performed expensive simulations for the 2.3 million candidate molecules on IBMs World Community Grid, in order to get this property value. So using machine learning approach to accurately predict the PCE values is a promising direction for the high throughput screening and discovering new materials.\nIn this experiment, we randomly select 90% of the data for training, and the rest 10% for testing. This setting is similar to Pyzer-Knapp et al. (2015), except that we use entire 2.3m dataset here. Since the data is distributed unevenly (see Figure 4), we resampled the training data (but not the test data) to make the algorithm put more attention on molecules with higher PCE values, in order to make accurate prediction for promising candidate molecules.\nSince the traditional kernel methods are not scalable, we make the explicit feature maps for WL subtree kernel by collecting all the molecules and creating dictionary for the feature space. The other graph kernels, like edge kernel and shortest path kernel, are having too large feature dictionary to work with. We use RDKit (Landrum, 2012) to extract features for atoms (nodes) and bonds (edges).\nThe mean absolute error (MAE) and root mean square error (RMSE) are reported in Table 3. We found utilizing graph information can accurately predict PCE values. Also, our proposed two methods are\nworking equally good. As WL kernel with degree 6 is also working good, it requires ten thousand times more parameters than us. The preprocessing also makes it not easy to apply in large data. So by modeling this problem via graphical mode, and doing embedding with discriminative information, we are able to efficiently and accurately predict the PCE value.\nTo understand more about the details of our algorithms\u2019 performance, we further compare our methods for different number of fixed point iterations in Figure 5. We can see that, higher number of fixed point iterations will lead to faster convergence, though the number of parameters of the model in different settings are the same. The mean field embedding will get much worse result if only one iteration is allowed. Compare to the loopy BP case with same setting, the latter one will always have one more round message passing since we need to aggregate the messages from edge to node in the last step. And also, from the quality of prediction we find that, though making slightly higher prediction error for molecules with high PCE values due to insufficient data, we are not overfitting the \u2018easy\u2019 (i.e., the most popular) range of PCE value."}, {"heading": "8 Conclusion", "text": "Motivated by prevalent learning tasks over structure data, we propose a new approach based on latent variable model embeddings. We exploit the graphical models to capture the structure information in each data point effectively. Meanwhile, we represent them via learned embeddings to achieve both statistical,\ncomputational and memory efficiency. Specifically, the nonlinear mappings for graphical model embedding and the ultimate regressor or classifier are jointly learned with the supervision information, resulting more accurate prediction. Secondly, the stochastic approximation is adopted for optimization, make the algorithm scalable to millions of data. Finally, rather than keeping the kernel matrix, or remembering the extremely high dimension explicit features in existing methods, the learned nonlinear mapping for embedding is taskdependent, therefore, resulting a small yet efficient representation and achieving memory efficiency.\nEmpirically, we apply the discriminative embedding on nine structured datasets, containing thousands to millions of data in the form of graph or string, for regression or classification. Sufficient evidence shows the proposed algorithm performs significantly better than the existing state-of-the-art methods, in terms of both accuracy and scalability."}, {"heading": "A Derivation of the Fixed-Point Condition for Loopy BP", "text": "The derivation of the fixed-point condition for loopy BP can be found in Yedidia et al. (2001b). However, to keep the paper self-contained, we provide the details here. The objective of loopy BP is\nmin {qij}(i,j)\u2208E\n\u2212 \u2211 i (|N (i)| \u2212 1) \u222b H qi(hi) log qi(hi) \u03a6(hi, xi) dhi + \u2211 i,j \u222b H2 qij(hi, hj) log\nqij(hi, hj)\n\u03a8(hi, hj)\u03a6(hi, xi)\u03a6(hj , xj) dhidhj\ns.t.\n\u222b H qij(hi, hj)dhj = qi(hi), \u222b H qij(hi, hj)dhj = qi(hi), \u222b H qi(hi)dhi = 1.\nDenote \u03bbij(hj) is the multiplier to marginalization constraints \u222b H qij(hi, hj)dhi \u2212 qj(hj) = 0, the La-\ngrangian is formed as L({qij}, {qi}, {\u03bbij}, {\u03bbji}) = \u2212 \u2211 i (|N (i)| \u2212 1) \u222b H qi(hi) log qi(hi) \u03a6(hi, xi) dhi\n+ \u2211 i,j \u222b H2 qij(hi, hj) log\nqij(hi, hj)\n\u03a8(hi, hj)\u03a6(hi, xi)\u03a6(hj , xj) dhidhj\n\u2212 \u2211 i,j \u222b H \u03bbij(hj) (\u222b H qij(hi, hj)dhi \u2212 qj(hj) ) dhj\n\u2212 \u2211 i,j \u222b H \u03bbji(hi) (\u222b H qij(hi, hj)dhj \u2212 qi(hi) ) dhi\nwith normalization constraints \u222b H qi(hi)dhi = 1. Take functional gradients of L({qij}, {qi}, {\u03bbij}, {\u03bbji}) with respect to qij(hi, hj) and qi(hi), and set them to zero, we have\nqij(hi, hj) \u221d \u03a8(hi, hj)\u03a6(hi, xi)\u03a6(hj , xj) exp(\u03bbij(hj) + \u03bbji(hi)),\nqi(hi) \u221d \u03a6(hi, xi) exp\n(\u2211 k\u2208N (i) \u03bbki(hi)\n|N (i)| \u2212 1\n) .\nWe set mij(hj) = qj(hj)\n\u03a6(hi,xi) exp(\u03bbij(hj)) , therefore,\u220f\nk\u2208N (i)\nmki(hi) \u221d exp\n(\u2211 k\u2208N (i) \u03bbki(hi)\n|N (i)| \u2212 1\n) .\nPlug it into qij(hi, hj) and qi(hi), we recover the loopy BP update for marginal belief and\nexp(\u03bbji(hi)) = qi(hi)\n\u03a6(hi, xi)mji(hi) \u221d \u220f k\u2208N(i)\\j mki(hi).\nThe update rule for message mij(hj) can be recovered using the marginal consistency constraints,\nmij(hj) = qj(hj)\n\u03a6(hi, xi) exp(\u03bbij(hj)) =\n\u222b H qij(hi, hj)dhi\n\u03a6(hi, xi) exp(\u03bbij(hj))\n= \u03a6(hj , xj) exp(\u03bbij(hj))\n\u222b H\u03a8(hi, hj)\u03a6(hi, xi) exp(\u03bbji(hi))dhi\n\u03a6(hi, xi) exp(\u03bbij(hj)) \u221d \u222b H \u03a8(hi, hj)\u03a6(hi, xi) \u220f\nk\u2208N(i)\\j\nmki(hi)dhi.\nMoreover, we also obtain the other important relationship between mij(hj) and \u03bbji(hi) by marginal\nconsistency constraint and the definition of mij(hj), mij(hj) \u221d \u222b \u03a8(hi, hj)\u03a6(hj , xj) exp(\u03bbji(hi))dhi."}, {"heading": "B Message Updates for other Inference methods", "text": "In Section 4, we discuss the embedding for several alternatives to optimize the Bethe free energy or its convexified version. We provide a brief introduction to these algorithms.\nB.1 Double-Loop BP\nNotice the Bethe free energy can be decomposed into the summation of a convex function and a concave function, Yuille (2002) utilizes CCCP to minimize the Bethe free energy, resulting the double-loop algorithm. Take the gradient of Lagrangian of the objective function, and set to zero, the primal variable can be represented in dual form,\nqij(hi, hj) \u221d \u03a8(hi, hj)\u03a6(hi, xi)\u03a6(hj , xj) exp(\u03bbij(hj) + \u03bbji(hi)), qi(hi) \u221d \u03a6(hi, xi) exp ( |N (i)|\u03b3s(hi)\u2212 \u2211 k\u2208N (i) \u03bbki(hi) ) .\nThe algorithm updates \u03b3 and \u03bb alternatively, \u03b3newi (hi) = |N (i)|\u03b3i(hi)\u2212 \u2211\nk\u2208N (i)\n\u03bbki(hi),\n2\u03bbnewij (hj) = |N (j)|\u03b3i(hi)\u2212 \u2211\nk\u2208N (j)\\i\n\u03bbkj(hj)\u2212 log \u222b H \u03a8(hi, hj)\u03a6(hi, xi)\u03bbji(hi)dhi\nConsider the \u03bbij as messages, we obtain the embedding forms.\nB.2 Damped BP\nInstead of the primal form of Bethe free energy, Minka (2001) investigates the duality of the optimization,\nmin \u03b3 max \u03bb\n\u2211 i ( |N (i)| \u2212 1 ) log \u222b H \u03a6(hi, xi) exp(\u03b3i(hi))dhi\n\u2212 \u2211\n(i,j)\u2208E\nlog \u222b H2 \u03a8(hi, hj)\u03a6(hi, xi)\u03a6(hj , xj) exp(\u03bbij(hj) + \u03bbji(hi))dhjdhi,\nsubject to ( |N (i)| \u2212 1 ) \u03b3i(hi) = \u2211 k\u2208N (i) \u03bbki(hi). Define message as\nmij(hj) \u221d \u222b H \u03a8(hi, hj)\u03a6(hj , xj) exp(\u03bbji(hi))dhi,\nthe messages updates are mij(hi) \u221d \u222b H \u03a6i(hi, xi)\u03a8ij(hi, hj) exp ( |N (i)| \u2212 1 |N (i)| \u03c9i(hi) )\u220f k\u2208N (i)m 1 |N(i)| ki (hi) mji(hi) dhi,\n\u03b3newi (hi)) \u221d |N (i)| \u2212 1 |N (i)|\u03b3i(hi)\n+ \u2211\nk\u2208N (i)\n1\n|N (i)|ki logm(hi).\nB.3 Tree-reweighted BP\nDifferent from loopy BP and its variants which optimizing the Bethe free energy, the tree-reweighted BP (Wainwright et al., 2003) is optimizing a convexified Bethe energy,\nmin {qij}(i,j\u2208E) L = \u2211 i \u222b H q(hi) log q(hi)dhi + \u2211 i,j uij \u222b H2 qij(hi, hj) log qij(hi, hj) qi(hi)qj(hj) dhidhj\n\u2212 \u2211 i \u222b H q(hi) log \u03a6(hi, xi)dhi \u2212 \u2211 i,j \u222b H2 qij(hi, hj) log \u03a8(hi, hj)dhidhj\nsubject to pairwise marginal consistency constraints: \u222b H qij(hi, hj)dhj = qi(hi), \u222b H qij(hi, hj)dhj = qi(hi),\nand \u222b H qi(hi)dhi = 1. The {uij}(i,j)\u2208E represents the probabilities that each edge appears in a spanning tree randomly chose from all spanning tree from G = {V, E} under some measure. Follow the same strategy as loopy BP update derivations, i.e., take derivatives of the corresponding Lagrangian with respect to qi and qi,j and set to zero, meanwhile, incorporate with the marginal consistency, we can arrive the messages updates,\nmij(hj) \u221d \u222b H \u03a8 1 uji ij (hi, hj)\u03a6i(hi, xi) \u220f k\u2208H(i)\\jm uki ki (hi) m 1\u2212uij ji (hi) dhi,\nqij(hi, hj) \u221d \u03a8 1 uji ij (hi, hj)\u03a6i(hi, xi)\u03a6j(hj , xj)\n\u220f k\u2208N (i)\\jm uki ki (hi)\nm 1\u2212uij ji (hi)\n\u220f k\u2208N (j)\\im ukj kj (hj)\nm 1\u2212uji ij (hj)\n,\nqi(hi) \u221d \u03a6i(hi, xi) \u220f\nk\u2208N (i)\nmukiki (hi)."}, {"heading": "C Derivatives Computation in Algorithm 3", "text": "We can use the chain rule to obtain the derivatives with respect to UT = {VT ,WT ,uT }. According to Equation 23 and Equation 24, the message passed to supervised label yn for n\u2212th sample\ncan be represented as mny = \u2211Vn i=1 \u00b5\u0303i n, and the corresponding derivative can be denoted as\n\u2202l\n\u2202mny =\n\u2202l\n\u2202f\n\u2202f\n\u2202\u03c3(mny )\n\u2202\u03c3(mny )\n\u2202mny\nThe term \u2202l\u2202f is depending on the supervised information and the loss function we used, and \u2202f \u2202\u03c3(mny ) =\nuT \u2202l\u2202f . The last term \u2202\u03c3(mny ) \u2202mny depends on the nonlinear function \u03c3 we used here.\nThe derivatives with respect to u for the current encountered sample {\u03c7n, yn} SGD iteration are\n\u2207ul(f(\u00b5\u0303n;U), yn) = \u2202l\n\u2202\u03c3(mny ) \u03c3(mny )\nT = \u2202l\n\u2202f\n\u2202f\n\u2202\u03c3(mny ) \u03c3(mny ) T (25)\nIn order to update the embedding parameters V and W, we need to obtain the derivatives with respect to the embedding of each hidden node, i.e., \u2202l\u2202u\u0303in = \u2202l \u2202mny ,\u2200i \u2208 V.\nC.1 Embedded Mean Field\nIn mean field embedding, we unfold the fixed point equation by the iteration index t = 1, 2, . . . , T . At t\u2212th iteration, the partial derivative is denoted as \u2202l\n\u2202\u00b5\u0303in(t) . The partial derivative with respect to the embedding\nobtained by last round fixed point iteration is already defined above: \u2202l \u2202\u00b5\u0303in(T ) = \u2202l\u2202mny Then the derivatives can be obtained recursively: \u2202l \u2202\u00b5\u0303in(t) = \u2211 j,i\u2208N (j)W T 2 V T \u2202l \u2202\u00b5\u0303jn(t+1)\n\u2202\u03c3\n\u2202(W1xj+W2l (t) j +W3uj)\n,\nt = 1, 2, . . . , T \u2212 1. Similarly, the parameters W and V are also updated cumulatively as below. \u2202l\n\u2202(W1xi +W2l (t) i +W3ui)\n= \u2211\nj,i\u2208N (j)\nV T \u2202l\n\u2202\u00b5\u0303j n(t+1)\n\u2202\u03c3\n\u2202(W1xj +W2l (t) j +W3uj)\n(26)\n\u2207W1 l(f(\u00b5\u0303n;U), yn) = \u2211 i\u2208Vn T\u22121\u2211 t=1\n\u2202l\n\u2202(W1xi +W2l (t) i +W3ui)\nxTi (27)\n\u2207W2 l(f(\u00b5\u0303n;U), yn) = \u2211 i\u2208Vn T\u22121\u2211 t=1\n\u2202l\n\u2202(W1xi +W2l (t) i +W3ui)\nl (t)T i (28)\n\u2207W3 l(f(\u00b5\u0303n;U), yn) = \u2211 i\u2208Vn T\u22121\u2211 t=1\n\u2202l\n\u2202(W1xi +W2l (t) i +W3ui)\nuTi (29)\n\u2207V l(f(\u00b5\u0303n;U), yn) = \u2211 i\u2208Vn T\u22121\u2211 t=1\n\u2202l\n\u2202\u00b5\u0303i (t+1)\n\u03c3(W1xi +W2l (t) i +W3ui) T (30)\nC.2 Embedding Loopy BP\nSimilar as above case, we can first obtain the derivatives with respect to embeddings of hidden variables \u2202l \u2202\u00b5\u0303in = \u2202l\u2202mny . Since the last round of message passing only involves the edge-to-node operations, we can easily get the following derivatives.\n\u2207V2 l(f(\u00b5\u0303n;U), yn) = \u2211 i\u2208V \u2202l \u2202\u00b5\u0303i n\u03c3(W3xi +W4 \u2211 k\u2208N (i) \u03bd\u0303 n(T ) ki ) T (31)\n\u2207W3 l(f(\u00b5\u0303n;U), yn) = \u2211 i\u2208V V T2 \u2202l \u2202\u00b5\u0303i n\n\u2202\u03c3 \u2202(W3xi +W4 \u2211 k\u2208N (i) \u03bd\u0303 n(T ) ki ) xTi (32)\n\u2207W4 l(f(\u00b5\u0303n;U), yn) = \u2211 i\u2208V V T2 \u2202l \u2202\u00b5\u0303i n\n\u2202\u03c3 \u2202(W3xi +W4 \u2211 k\u2208N (i) \u03bd\u0303 n(T ) ki )\n( \u2211\nk\u2208N (i)\n\u03bd\u0303 n(T ) ki ) T (33)\n(34)\nNow we consider the partial derivatives for the pairwise message embeddings for different t. Again, the top level one is trivial, which is given by \u2202l\n\u2202\u03bd\u0303ij n(T ) = W\nT 4 V T 2 \u2202l \u2202\u00b5\u0303j\n\u2202\u03c3 \u2202(W3xj+W4 \u2211 k\u2208N(j) \u03bd\u0303 (T ) kj ) . Using similar\nrecursion trick, we can get the following chain rule for getting partial derivatives with respect to each pairwise message in each stage of fixed point iteration.\n\u2202l\n\u2202\u03bd\u0303 n(t) ij\n= \u2211\np\u2208N (j)\\i\nWT2 V T 1\n\u2202l\n\u2202\u03bd\u0303 n(t+1) jp\n\u2202\u03c3 \u2202(W1xj +W2 \u2211 k\u2208N (j)\\p[\u03bd\u0303 n(t) kj ])\n(35)\nThen, we can update the parameters V1,W1,W2 using following gradients.\n\u2207V1 l(f(\u00b5\u0303n;U), yn) = T\u22121\u2211 t=1 \u2211 (i,j)\u2208En\n\u2202l\n\u2202\u03bd\u0303 n(t+1) ij\n\u03c3(W1xi +W2 \u2211\nk\u2208N (i)\\j\n[\u03bd\u0303 n(t) ki ]) T (36)\n\u2207W1 l(f(\u00b5\u0303n;U), yn) = T\u22121\u2211 t=1 \u2211 (i,j)\u2208En V T1 \u2202l \u2202\u03bd\u0303 n(t+1) ij\n\u2202\u03c3 \u2202(W1xi +W2 \u2211 k\u2208N (i)\\j [\u03bd\u0303 n(t) ki ]) xTi (37)\n\u2207W2 l(f(\u00b5\u0303n;U), yn) = T\u22121\u2211 t=1 \u2211 (i,j)\u2208En V T1 \u2202l \u2202\u03bd\u0303 n(t+1) ij\n\u2202\u03c3 \u2202(W1xi +W2 \u2211 k\u2208N (i)\\j [\u03bd\u0303 n(t) ki ])\n( \u2211\nk\u2208N (i)\\j\n[\u03bd\u0303 n(t) ki ]) T (38)\n(39)"}], "references": [{"title": "Scop database in 2004: refinements integrate structure and sequence family data", "author": ["Andreeva", "Antonina", "Howorth", "Dave", "Brenner", "Steven E", "Hubbard", "Tim JP", "Chothia", "Cyrus", "Murzin", "Alexey G"], "venue": "Nucleic acids research,", "citeRegEx": "Andreeva et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Andreeva et al\\.", "year": 2004}, {"title": "Shortest-path kernels on graphs", "author": ["Borgwardt", "Karsten M", "Kriegel", "Hans-Peter"], "venue": "In Proc. Intl. Conf. Data Mining, pp", "citeRegEx": "Borgwardt et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Borgwardt et al\\.", "year": 2005}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["Bruna", "Joan", "Zaremba", "Wojciech", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1312.6203,", "citeRegEx": "Bruna et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bruna et al\\.", "year": 2013}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.C. Chang", "C.J. Lin"], "venue": null, "citeRegEx": "Chang and Lin,? \\Q2001\\E", "shortCiteRegEx": "Chang and Lin", "year": 2001}, {"title": "Structureactivity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity", "author": ["A.K. Debnath", "R.L. Lopez de Compadre", "G. Debnath", "A.J. Shusterman", "C. Hansch"], "venue": "J Med Chem,", "citeRegEx": "Debnath et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Debnath et al\\.", "year": 1991}, {"title": "Distinguishing enzyme structures from non-enzymes without alignments", "author": ["P.D. Dobson", "A.J. Doig"], "venue": "J Mol Biol,", "citeRegEx": "Dobson and Doig,? \\Q2003\\E", "shortCiteRegEx": "Dobson and Doig", "year": 2003}, {"title": "Rational design of highly active sgrnas for crispr-cas9-mediated gene inactivation", "author": ["Doench", "John G", "Hartenian", "Ella", "Graham", "Daniel B", "Tothova", "Zuzana", "Hegde", "Mudra", "Smith", "Ian", "Sullender", "Meagan", "Ebert", "Benjamin L", "Xavier", "Ramnik J", "Root", "David E"], "venue": "Nature biotechnology,", "citeRegEx": "Doench et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Doench et al\\.", "year": 2014}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["Duvenaud", "David K", "Maclaurin", "Dougal", "Iparraguirre", "Jorge", "Bombarell", "Rafael", "Hirzel", "Timothy", "AspuruGuzik", "Al\u00e1n", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Duvenaud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duvenaud et al\\.", "year": 2015}, {"title": "In silico predictive modeling of crispr/cas9 guide efficiency", "author": ["Fusi", "Nicolo", "Smith", "Ian", "Doench", "John", "Listgarten", "Jennifer"], "venue": "bioRxiv,", "citeRegEx": "Fusi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fusi et al\\.", "year": 2015}, {"title": "On graph kernels: Hardness results and efficient alternatives", "author": ["T. G\u00e4rtner", "P.A. Flach", "S. Wrobel"], "venue": "Proc. Annual Conf. Computational Learning Theory,", "citeRegEx": "G\u00e4rtner et al\\.,? \\Q2003\\E", "shortCiteRegEx": "G\u00e4rtner et al\\.", "year": 2003}, {"title": "Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping", "author": ["Giles", "Rich Caruana Steve Lawrence Lee"], "venue": "In Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference,", "citeRegEx": "Giles and Lee.,? \\Q2001\\E", "shortCiteRegEx": "Giles and Lee.", "year": 2001}, {"title": "The harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community", "author": ["Hachmann", "Johannes", "Olivares-Amaya", "Roberto", "Atahan-Evrenk", "Sule", "Amador-Bedolla", "Carlos", "S\u00e1nchezCarrera", "Roel S", "Gold-Parker", "Aryeh", "Vogt", "Leslie", "Brockway", "Anna M", "Aspuru-Guzik", "Al\u00e1n"], "venue": "grid. The Journal of Physical Chemistry Letters,", "citeRegEx": "Hachmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hachmann et al\\.", "year": 2011}, {"title": "Deep convolutional networks on graph-structured data", "author": ["Henaff", "Mikael", "Bruna", "Joan", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1506.05163,", "citeRegEx": "Henaff et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Henaff et al\\.", "year": 2015}, {"title": "Deep unfolding: Model-based inspiration of novel deep architectures", "author": ["Hershey", "John R", "Roux", "Jonathan Le", "Weninger", "Felix"], "venue": "arXiv preprint arXiv:1409.2574,", "citeRegEx": "Hershey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hershey et al\\.", "year": 2014}, {"title": "Stable fixed points of loopy belief propagation are local minima of the bethe free energy", "author": ["Heskes", "Tom"], "venue": null, "citeRegEx": "Heskes and Tom.,? \\Q2002\\E", "shortCiteRegEx": "Heskes and Tom.", "year": 2002}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["T.S. Jaakkola", "D. Haussler"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Jaakkola and Haussler,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola and Haussler", "year": 1999}, {"title": "Probability product kernels", "author": ["T. Jebara", "R. Kondor", "A. Howard"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Jebara et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Jebara et al\\.", "year": 2004}, {"title": "Kernel-based just-in-time learning for passing expectation propagation messages", "author": ["Jitkrittum", "Wittawat", "Gretton", "Arthur", "Heess", "Nicolas", "Eslami", "S.M. Ali", "Lakshminarayanan", "Balaji", "Sejdinovic", "Dino", "Szab\u00f3", "Zolt\u00e1n"], "venue": "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Jitkrittum et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jitkrittum et al\\.", "year": 2015}, {"title": "Profilebased string kernels for remote homology detection and motif extraction", "author": ["Kuang", "Rui", "Ie", "Eugene", "Wang", "Ke", "Kai", "Siddiqi", "Mahira", "Freund", "Yoav", "Leslie", "Christina"], "venue": "Journal of bioinformatics and computational biology,", "citeRegEx": "Kuang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kuang et al\\.", "year": 2005}, {"title": "Rdkit: Open-source cheminformatics", "author": ["G. Landrum"], "venue": null, "citeRegEx": "Landrum,? \\Q2013\\E", "shortCiteRegEx": "Landrum", "year": 2013}, {"title": "The spectrum kernel: A string kernel for SVM protein classification", "author": ["C. Leslie", "E. Eskin", "W.S. Noble"], "venue": "In Proceedings of the Pacific Symposium on Biocomputing,", "citeRegEx": "Leslie et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2002}, {"title": "Mismatch string kernels for SVM protein classification", "author": ["C. Leslie", "E. Eskin", "J. Weston", "W.S. Noble"], "venue": "Advances in Neural Information Processing Systems 15,", "citeRegEx": "Leslie et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2002}, {"title": "Gated graph sequence neural networks", "author": ["Li", "Yujia", "Tarlow", "Daniel", "Brockschmidt", "Marc", "Zemel", "Richard"], "venue": "arXiv preprint arXiv:1511.05493,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Deeply learning the messages in message passing inference", "author": ["G. Lin", "C. Shen", "I. Reid", "A. van den Hengel"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "The EP energy function and minimization schemes", "author": ["T. Minka"], "venue": "See www. stat. cmu. edu/minka/papers/learning. html,", "citeRegEx": "Minka,? \\Q2001\\E", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Convolutional neural networks over tree structures for programming language processing", "author": ["Mou", "Lili", "Li", "Ge", "Zhang", "Lu", "Wang", "Tao", "Jin", "Zhi"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Mou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["Murphy", "Kevin P", "Weiss", "Yair", "Jordan", "Michael I"], "venue": "In UAI, pp", "citeRegEx": "Murphy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Learning from the harvard clean energy project: The use of neural networks to accelerate materials discovery", "author": ["Pyzer-Knapp", "Edward O", "Li", "Kewei", "Aspuru-Guzik", "Alan"], "venue": "Advanced Functional Materials,", "citeRegEx": "Pyzer.Knapp et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pyzer.Knapp et al\\.", "year": 2015}, {"title": "Expressivity versus efficiency of graph kernels", "author": ["J. Ramon", "T. G\u00e4rtner"], "venue": "Technical report, First International Workshop on Mining Graphs, Trees and Sequences (held with ECML/PKDD\u201903),", "citeRegEx": "Ramon and G\u00e4rtner,? \\Q2003\\E", "shortCiteRegEx": "Ramon and G\u00e4rtner", "year": 2003}, {"title": "Learning message-passing inference machines for structured prediction", "author": ["Ross", "Stephane", "Munoz", "Daniel", "Hebert", "Martial", "Bagnell", "J Andrew"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "The graph neural network model", "author": ["Scarselli", "Franco", "Gori", "Marco", "Tsoi", "Ah Chung", "Hagenbuchner", "Markus", "Monfardini", "Gabriele"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Scarselli et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Scarselli et al\\.", "year": 2009}, {"title": "Kernel Methods in Computational Biology", "author": ["B. Sch\u00f6lkopf", "K. Tsuda", "Vert", "J.-P"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2004}, {"title": "Learning with Kernels", "author": ["Sch\u00f6lkopf", "Bernhard", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2002}, {"title": "Efficient graphlet kernels for large graph comparison", "author": ["Shervashidze", "Nino", "S.V.N. Vishwanathan", "Petri", "Tobias", "Mehlhorn", "Kurt", "Borgwardt", "Karsten"], "venue": "Proc. Intl. Conference on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics,", "citeRegEx": "Shervashidze et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shervashidze et al\\.", "year": 2009}, {"title": "Weisfeiler-lehman graph kernels", "author": ["Shervashidze", "Nino", "Schweitzer", "Pascal", "Van Leeuwen", "Erik Jan", "Mehlhorn", "Kurt", "Borgwardt", "Karsten M"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Shervashidze et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shervashidze et al\\.", "year": 2011}, {"title": "A Hilbert space embedding for distributions", "author": ["A.J. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "In Proceedings of the International Conference on Algorithmic Learning Theory,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Hilbert space embeddings of conditional distributions", "author": ["L. Song", "J. Huang", "A.J. Smola", "K. Fukumizu"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "Nonparametric tree graphical models", "author": ["L. Song", "A. Gretton", "C. Guestrin"], "venue": "In 13th Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Kernel belief propagation", "author": ["L. Song", "A. Gretton", "D. Bickson", "Y. Low", "C. Guestrin"], "venue": "In Proc. Intl. Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Song et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Song et al\\.", "year": 2011}, {"title": "Injective Hilbert space embeddings of probability measures", "author": ["B. Sriperumbudur", "A. Gretton", "K. Fukumizu", "G. Lanckriet", "B. Sch\u00f6lkopf"], "venue": "In Proc. Annual Conf. Computational Learning Theory,", "citeRegEx": "Sriperumbudur et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sriperumbudur et al\\.", "year": 2008}, {"title": "Halting in random walk kernels", "author": ["Sugiyama", "Mahito", "Borgwardt", "Karsten"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sugiyama et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2015}, {"title": "Fast kernels for string and tree matching", "author": ["S.V.N. Vishwanathan", "A.J. Smola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Vishwanathan and Smola,? \\Q2003\\E", "shortCiteRegEx": "Vishwanathan and Smola", "year": 2003}, {"title": "Tree-reweighted belief propagation and approximate ML estimation by pseudo-moment matching", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "In 9th Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Wainwright et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2003}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Comparison of descriptor spaces for chemical compound retrieval and classification", "author": ["Wale", "Nikil", "Watson", "Ian A", "Karypis", "George"], "venue": "Knowledge and Information Systems,", "citeRegEx": "Wale et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wale et al\\.", "year": 2008}, {"title": "Generalized belief propagation", "author": ["Yedidia", "Jonathan S", "Freeman", "William T", "Weiss", "Yair"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Yedidia et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2001}, {"title": "Bethe free energy, kikuchi approximations and belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Technical report, Mitsubishi Electric Research Laboratories,", "citeRegEx": "Yedidia et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2001}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}, {"title": "Cccp algorithms to minimize the bethe and kikuchi free energies: Convergent alternatives to belief propagation", "author": ["A.L. Yuille"], "venue": "Neural Computation,", "citeRegEx": "Yuille,? \\Q2002\\E", "shortCiteRegEx": "Yuille", "year": 2002}, {"title": "Conditional random fields as recurrent neural networks", "author": ["Zheng", "Shuai", "Jayasumana", "Sadeep", "Romera-Paredes", "Bernardino", "Vineet", "Vibhav", "Su", "Zhizhong", "Du", "Dalong", "Huang", "Chang", "Torr", "Philip"], "venue": "arXiv preprint arXiv:1502.03240,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "However, to keep the paper self-contained, we provide the details here", "author": ["Appendix A Derivation of the Fixed-Point Condition for Loopy BP The derivation of the fixed-point condition for loopy BP can be found in Yedidia"], "venue": "The objective of loopy BP is min", "citeRegEx": "Yedidia,? 2001b", "shortCiteRegEx": "Yedidia", "year": 2001}], "referenceMentions": [{"referenceID": 32, "context": "Structured data, such as sequences, trees and graphs, are prevalent in a number of interdisciplinary areas such as protein design, genomic sequence analysis, and drug design (Sch\u00f6lkopf et al., 2004).", "startOffset": 174, "endOffset": 198}, {"referenceID": 9, "context": ", 2002a; Vishwanathan & Smola, 2003) and graph datasets (G\u00e4rtner et al., 2003; Borgwardt, 2007).", "startOffset": 56, "endOffset": 95}, {"referenceID": 34, "context": ", 2002a), subtree kernel (Ramon & G\u00e4rtner, 2003), graphlet kernel (Shervashidze et al., 2009) and Weisfeiler-lehman graph kernel (Shervashidze et al.", "startOffset": 66, "endOffset": 93}, {"referenceID": 35, "context": ", 2009) and Weisfeiler-lehman graph kernel (Shervashidze et al., 2011) all follow this design principle.", "startOffset": 43, "endOffset": 70}, {"referenceID": 16, "context": "The Fisher kernel (Jaakkola & Haussler, 1999) and probability product kernel (Jebara et al., 2004) are two representative instances within the family.", "startOffset": 77, "endOffset": 98}, {"referenceID": 36, "context": "Our idea is to model each structured data point as a latent variable model, then embed the graphical model into feature spaces (Smola et al., 2007; Song et al., 2009), and use inner product in the embedding space to define kernels.", "startOffset": 127, "endOffset": 166}, {"referenceID": 37, "context": "Our idea is to model each structured data point as a latent variable model, then embed the graphical model into feature spaces (Smola et al., 2007; Song et al., 2009), and use inner product in the embedding space to define kernels.", "startOffset": 127, "endOffset": 166}, {"referenceID": 34, "context": "Similarly, the graphlet kernel (Shervashidze et al., 2009) for two graphs x and x\u2032 can be defined as in the same form as (1), where S is now the set of possible subgraphs, #(s \u2208 x) counts the number occurrence of subgraphs.", "startOffset": 31, "endOffset": 58}, {"referenceID": 16, "context": "Another classical example along the same line is the probability product kernel (Jebara et al., 2004).", "startOffset": 80, "endOffset": 101}, {"referenceID": 36, "context": "Hilbert space embeddings of distributions are mappings of distributions into potentially infinite dimensional feature spaces (Smola et al., 2007), \u03bcX := EX [\u03c6(X)] = \u222b", "startOffset": 125, "endOffset": 145}, {"referenceID": 40, "context": "Some feature map can make the mapping injective (Sriperumbudur et al., 2008), meaning that if two distributions, p(X) and q(X), are different, they are mapped to two distinct points in the feature space.", "startOffset": 48, "endOffset": 76}, {"referenceID": 27, "context": "Only when the graph structure is a tree, exact computation can be carried out efficiently via a message passing algorithm (Pearl, 1988).", "startOffset": 122, "endOffset": 135}, {"referenceID": 26, "context": "In many applications, however, the resulting mean fields and loopy BP algorithm exhibits excellent empirical performance (Murphy et al., 1999).", "startOffset": 121, "endOffset": 142}, {"referenceID": 49, "context": ", convex-concave decomposition (Yuille, 2002) and duality (Minka, 2001).", "startOffset": 31, "endOffset": 45}, {"referenceID": 24, "context": ", convex-concave decomposition (Yuille, 2002) and duality (Minka, 2001).", "startOffset": 58, "endOffset": 71}, {"referenceID": 43, "context": "By optimizing a convexified Bethe free energy, Wainwright et al. (2003) propose the tree-reweighted version BP with weight for each message, {vij}i,j\u2208E , in spanning tree polytope.", "startOffset": 47, "endOffset": 72}, {"referenceID": 24, "context": "The algorithms proposed for minimizing the Bethe free energy (Minka, 2001; Heskes, 2002; Yuille, 2002) can also be extended for Kikuchi free energy, resulting in different embedding forms.", "startOffset": 61, "endOffset": 102}, {"referenceID": 49, "context": "The algorithms proposed for minimizing the Bethe free energy (Minka, 2001; Heskes, 2002; Yuille, 2002) can also be extended for Kikuchi free energy, resulting in different embedding forms.", "startOffset": 61, "endOffset": 102}, {"referenceID": 45, "context": "We start from the messages into the smallest region first (Yedidia et al., 2001b). Remark: The choice of basis clusters and the form of messages determine the dependency in the embedding. Please refer to Yedidia et al. (2005) for details about the principles to partition the graph structure, and several other generalized BP variants with different messages.", "startOffset": 59, "endOffset": 226}, {"referenceID": 17, "context": ", 2010, 2011) and kernel EP (Jitkrittum et al., 2015), our current work exploits embedding to reformulate the graphical models and avoids explicitly learning the potential functions of the graphical models.", "startOffset": 28, "endOffset": 53}, {"referenceID": 22, "context": "From this perspective, the model in (Li et al., 2015) can be considered as an extension of (Scarselli et al.", "startOffset": 36, "endOffset": 53}, {"referenceID": 31, "context": ", 2015) can be considered as an extension of (Scarselli et al., 2009) where the gated recurrent unit is used for feature generation.", "startOffset": 45, "endOffset": 69}, {"referenceID": 2, "context": "Some works transfer locality concept of convolutional neural networks (CNN) from Euclidean domain to graph case, using hierarchical clustering, graph Laplacian (Bruna et al., 2013), or graph Fourier transform (Henaff et al.", "startOffset": 160, "endOffset": 180}, {"referenceID": 12, "context": ", 2013), or graph Fourier transform (Henaff et al., 2015).", "startOffset": 36, "endOffset": 57}, {"referenceID": 26, "context": "Scarselli et al. (2009) proposed a neural network which generates features by solving a heuristic nonlinear system iteratively, and is learned using Almeida-Pineda algorithm.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Some works transfer locality concept of convolutional neural networks (CNN) from Euclidean domain to graph case, using hierarchical clustering, graph Laplacian (Bruna et al., 2013), or graph Fourier transform (Henaff et al., 2015). These models are still restricted to problems with the same graph structure, which is not suitable for learning with molecules. Mou et al. (2016) proposed a convolution operation on trees, while the locality are defined based on parent-child relations.", "startOffset": 161, "endOffset": 378}, {"referenceID": 2, "context": "Some works transfer locality concept of convolutional neural networks (CNN) from Euclidean domain to graph case, using hierarchical clustering, graph Laplacian (Bruna et al., 2013), or graph Fourier transform (Henaff et al., 2015). These models are still restricted to problems with the same graph structure, which is not suitable for learning with molecules. Mou et al. (2016) proposed a convolution operation on trees, while the locality are defined based on parent-child relations. Duvenaud et al. (2015) used CNN to learn the circulant fingerprints for graphs from end to end.", "startOffset": 161, "endOffset": 508}, {"referenceID": 30, "context": "2 Comparison with Learning Message Estimator By recognizing inference as computational expressions, inference machines (Ross et al., 2011) incorporate learning into the messages passing inference for CRFs.", "startOffset": 119, "endOffset": 138}, {"referenceID": 13, "context": "More recently, Hershey et al. (2014); Zheng et al.", "startOffset": 15, "endOffset": 37}, {"referenceID": 13, "context": "More recently, Hershey et al. (2014); Zheng et al. (2015); Lin et al.", "startOffset": 15, "endOffset": 58}, {"referenceID": 13, "context": "More recently, Hershey et al. (2014); Zheng et al. (2015); Lin et al. (2015) designed specific recurrent neural networks and convolutional neural networks for imitating the messages in CRFs.", "startOffset": 15, "endOffset": 77}, {"referenceID": 13, "context": "In these existing messages learning work (Hershey et al., 2014; Zheng et al., 2015; Lin et al., 2015), the learning task is still estimating the messages represented graphical models with designed function forms, e.", "startOffset": 41, "endOffset": 101}, {"referenceID": 50, "context": "In these existing messages learning work (Hershey et al., 2014; Zheng et al., 2015; Lin et al., 2015), the learning task is still estimating the messages represented graphical models with designed function forms, e.", "startOffset": 41, "endOffset": 101}, {"referenceID": 23, "context": "In these existing messages learning work (Hershey et al., 2014; Zheng et al., 2015; Lin et al., 2015), the learning task is still estimating the messages represented graphical models with designed function forms, e.", "startOffset": 41, "endOffset": 101}, {"referenceID": 9, "context": "On graph benchmark datasets, we compare with subtree kernel (Ramon & G\u00e4rtner, 2003) (R&G, for short), random walk kernel(G\u00e4rtner et al., 2003; Vishwanathan et al., 2010), shortest path kernel (Borgwardt & Kriegel, 2005), graphlet kernel(Shervashidze et al.", "startOffset": 120, "endOffset": 169}, {"referenceID": 34, "context": ", 2010), shortest path kernel (Borgwardt & Kriegel, 2005), graphlet kernel(Shervashidze et al., 2009) and the family of WeisfeilerLehman kernels (Shervashidze et al.", "startOffset": 74, "endOffset": 101}, {"referenceID": 35, "context": ", 2009) and the family of WeisfeilerLehman kernels (Shervashidze et al., 2011) (WL kernel, for short).", "startOffset": 51, "endOffset": 78}, {"referenceID": 9, "context": "On graph benchmark datasets, we compare with subtree kernel (Ramon & G\u00e4rtner, 2003) (R&G, for short), random walk kernel(G\u00e4rtner et al., 2003; Vishwanathan et al., 2010), shortest path kernel (Borgwardt & Kriegel, 2005), graphlet kernel(Shervashidze et al., 2009) and the family of WeisfeilerLehman kernels (Shervashidze et al., 2011) (WL kernel, for short). We didn\u2019t compare with fisher kernel and probabilistic product kernel in the experiments since they are taking too much time to run, and some of our baselines (e.g., Leslie et al. (2002a)) has already shown better results than those methods.", "startOffset": 121, "endOffset": 547}, {"referenceID": 18, "context": "We use the same 54 target families and the same training/test splits as in remote homology detection (Kuang et al., 2005).", "startOffset": 101, "endOffset": 121}, {"referenceID": 6, "context": "Details of this dataset can be found in Doench et al. (2014); Fusi et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 6, "context": "Details of this dataset can be found in Doench et al. (2014); Fusi et al. (2015). We use two variants for spectrum string kernel: 1) kmer-single, where the constructed kernel matrix K (s) k only consider patterns of length k; 2) kmer-concat, where kernel matrix K = \u2211k i=1K (s) k .", "startOffset": 40, "endOffset": 81}, {"referenceID": 4, "context": "MUTAG (Debnath et al., 1991), NCI1 and NCI109 (Wale et al.", "startOffset": 6, "endOffset": 28}, {"referenceID": 45, "context": ", 1991), NCI1 and NCI109 (Wale et al., 2008) are chemical compounds dataset, while ENZYMES (Borgwardt & Kriegel, 2005) and D&D (Dobson & Doig, 2003) are", "startOffset": 25, "endOffset": 44}, {"referenceID": 34, "context": "The results of baseline algorithms are taken from Shervashidze et al. (2011) since we use exactly the same setting here.", "startOffset": 50, "endOffset": 77}, {"referenceID": 34, "context": "The results of baseline algorithms are taken from Shervashidze et al. (2011) since we use exactly the same setting here. From the accuracy comparison shown in Figure 3, we can see the proposed embedding methods are comparable to other graph kernels, on different graphs with different number of labels, nodes and edges. Also, in dataset D&D which consists of 82 different types of labels, our algorithm performs much better. As reported in Shervashidze et al. (2011), the time required for constructing dictionary for the graph kernel can take up to more than a year of CPU time in this dataset, while our algorithm can learn the discriminative embedding from structured data directly without the construction of dictionary.", "startOffset": 50, "endOffset": 467}, {"referenceID": 11, "context": "2 Harvard Clean Energy Project(CEP) dataset The Harvard Clean Energy Project (Hachmann et al., 2011) is a theory-driven search for the next generation of organic solar cell materials.", "startOffset": 77, "endOffset": 100}, {"referenceID": 11, "context": "2 Harvard Clean Energy Project(CEP) dataset The Harvard Clean Energy Project (Hachmann et al., 2011) is a theory-driven search for the next generation of organic solar cell materials. One of the most important properties of molecule for this task is the overall efficiency of the energy conversion process in a solar cell, which is determined by the power conversion efficiency (PCE). The Clean Energy Project (CEP) performed expensive simulations for the 2.3 million candidate molecules on IBMs World Community Grid, in order to get this property value. So using machine learning approach to accurately predict the PCE values is a promising direction for the high throughput screening and discovering new materials. In this experiment, we randomly select 90% of the data for training, and the rest 10% for testing. This setting is similar to Pyzer-Knapp et al. (2015), except that we use entire 2.", "startOffset": 78, "endOffset": 869}], "year": 2017, "abstractText": "Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced in a number of interdisciplinary areas such as computational biology and drug design. Typically, kernel functions are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. We propose an effective and scalable approach for structured data representation which is based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Furthermore, our feature learning algorithm runs a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In real world applications involving sequences and graphs, we showed that the proposed approach is much more scalable than alternatives while at the same time produce comparable results to the state-of-the-art in terms of classification and regression.", "creator": "LaTeX with hyperref package"}}}