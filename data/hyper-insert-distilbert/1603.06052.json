{"id": "1603.06052", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2016", "title": "Fast DPP Sampling for Nystr\\\"om with Application to Kernel Methods", "abstract": "the nystr \\ \" om } method has long been popular for scaling up general kernel methods. however, technically successful use of explicit nystr \\ \" sigma om \u2033 depends substantially crucially on the selected landmarks. currently we readily consider landmark selection by commonly using scaling a vector determinantal sampled point process ( vector dpp ) to can tractably select a diverse temporal subset array from moving the parallel columns of to an input map kernel expansion matrix. we prove that indeed the kernel landmarks locations selected by using dpp sampling enjoy strong guaranteed error cancellation bounds ; subsequently, we illustrate continuous impact of dpp - sampled regional landmarks on effective kernel ridge regression. moreover, now we show how impossible to efficiently sample from a dpp in linear time efficiently using a fast feature mixing ( under certain parameters constraints ) markov chain, alone which makes the overall imaging procedure potentially practical. empirical observation results support our advanced theoretical analysis : general dpp - based generalized landmark profile selection shows performance superior efficiency to rival existing approaches.", "histories": [["v1", "Sat, 19 Mar 2016 06:14:28 GMT  (914kb,D)", "https://arxiv.org/abs/1603.06052v1", null], ["v2", "Sat, 28 May 2016 04:04:55 GMT  (723kb,D)", "http://arxiv.org/abs/1603.06052v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chengtao li", "stefanie jegelka", "suvrit sra"], "accepted": true, "id": "1603.06052"}, "pdf": {"name": "1603.06052.pdf", "metadata": {"source": "CRF", "title": "Fast Dpp Sampling for Nystro\u0308m with Application to Kernel Methods", "authors": ["Chengtao Li", "Stefanie Jegelka", "Suvrit Sra"], "emails": ["ctli@mit.edu", "stefje@csail.mit.edu", "suvrit@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Low-rank matrix approximation is an important ingredient of modern machine learning methods. Numerous learning tasks rely on multiplication and inversion of matrices, operations that scale cubically in the number of data points N, and therefore quickly become a bottleneck for large data. In such cases, low-rank matrix approximations promise speedups with a tolerable loss in accuracy.\nA notable instance is the Nystro\u0308m method [32, 43], which takes a positive semidefinite matrix K \u2208 RN\u00d7N as input, selects from it a small subset C of columns K\u00b7,C, and constructs the approximation K\u0303 = K\u00b7,CK\u2020C,CKC,\u00b7. The matrix K\u0303 is then used in place of K, which can decrease runtimes from O(N3) to O(N|C|3), a huge savings (since typically |C| N).\nSince its introduction into machine learning, the Nystro\u0308m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1]. Recent work [2, 5, 12] shows risk bounds for Nystro\u0308m applied to various kernel methods.\nThe most important step of the Nystro\u0308m method is the selection of the subset C, the so-called landmarks. This choice governs the approximation error and subsequent performance of the approximated learning methods [12]. The most basic strategy is to sample landmarks uniformly at random [43]. More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].\nWe study landmark selection using Determinantal Point Processes (Dpp), discrete probability models that allow tractable sampling of diverse non-independent subsets [26, 31]. Our work generalizes the determinant based scheme of Belabbas and Wolfe [9].1 We refer to our scheme as Dpp-Nystro\u0308m, and analyze it from several perspectives.\n1The authors do not make any connection to Dpps.\nar X\niv :1\n60 3.\n06 05\n2v 2\n[ cs\n.L G\n] 2\n8 M\nay 2\n01 6\nA key quantity in our analysis is the error of the Nystro\u0308m approximation. Suppose k is the target rank; then for selecting c \u2265 k landmarks, Nystro\u0308m\u2019s error is typically measured using the Frobenius or spectral norm relative to the best achievable error via rank-k SVD Kk; i.e., we measure\n\u2016K\u2212 K\u00b7,CK\u2020C,CKC,\u00b7\u2016F \u2016K\u2212 Kk\u2016F or \u2016K\u2212 K\u00b7,CK\u2020C,CKC,\u00b7\u20162 \u2016K\u2212 Kk\u20162 .\nSeveral authors also use additive instead of relative bounds. However, such bounds are very sensitive to scaling, and become loose even if a single entry of the matrix is large. Thus, we focus on the above relative error bounds.\nFirst, we analyze this approximation error. Previous analyses [9] fix a cardinality c = k; we allow the general case of selecting c \u2265 k columns. Our relative error bounds rely on the properties of characteristic polynomials. Empirically, Dpp-Nystro\u0308m obtains approximations competitive to state-of-the-art methods.\nSecond, we consider its impact on kernel methods. Specifically, we address the impact of Nystro\u0308m-based kernel approximations on kernel ridge regression. This task has been noted as the main application in [2, 5]. We show risk bounds of Dpp-Nystro\u0308m that hold in expectation. Empirically, it achieves the best performance among competing methods.\nThird, we consider the efficiency of Dpp-Nystro\u0308m; specifically, its tradeoff between error and running time. Since its proposal, determinantal sampling has so far not been used widely in practice due to valid concerns about its scalability. We consider a Gibbs sampler for k-Dpp, and analyze its mixing time using a path coupling [11] argument. We prove that under certain conditions the chain is fast mixing, which implies a linear running time for Dpp sampling of landmarks. Empirical results indicate that the chain yields favorable results within a small number of iterations, and the best efficiency-accuracy traedoffs compared to state-of-art methods (Figure 6)."}, {"heading": "2 Background and Notation", "text": "Throughout, we are approximating a given positive semidifinite (PSD) matrix K \u2208 RN\u00d7N with eigendecomposition K = U\u039bU> and eigenvalues \u03bb1 \u2265 . . . \u2265 \u03bbN . We use Ki,\u00b7 for the i-th row and K\u00b7,j for the j-th column, and, likewise, KC,\u00b7 for the rows of K and K\u00b7,C for the columns of K indexed by C \u2286 [N]. Finally, KC,C is the submatrix of K with rows and columns indexed by C. In this notation, Kk = U\u00b7,[k]\u039b[k],[k]U>\u00b7,[k] is the best rank-k approximation to K in both Frobenius and spectral norm. We write r(\u00b7) for the rank and (\u00b7)\u2020 for the pseudoinverse, and denote a decomposition of K by B>B, where B \u2208 Rr(K)\u00d7N .\nThe Nystro\u0308m Method. The standard Nystro\u0308m method selects a subset C \u2286 [N] of c = |C| landmarks, and approximates K with K\u00b7,CK\u2020C,CKC,\u00b7. The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44]. Besides various landmark selection methods, there exist variations of the standard Nystro\u0308m method. The ensemble Nystro\u0308m method [27], for instance, uses a weighted combination of approximations. The modified Nystro\u0308m method constructs an approximation K\u00b7,CK\u2020\u00b7,CKK \u2020 C,\u00b7KC,\u00b7 [38]. In this paper, we focus on the standard Nystro\u0308m method. Determinantal Point Processes. A determinantal point process Dpp(K) is a distribution over all subsets of a ground set Y of cardinality N that is determined by a PSD kernel K \u2208 RN\u00d7N . The probability of observing a subset C \u2286 [N] is proportional to det(KC,C), that is,\nPr(C) = det(KC,C)/ det(K + I). (2.1)\nWhen conditioning on a fixed cardinality, one obtains a k-Dpp [25]. To avoid confusion with the target rank k, and since we use cardinality c = |C|, we will refer to this distribution as c-Dpp2, and\n2Note that we refer to Dpp-Nystro\u0308m as kDPP in experimental parts.\nnote that\nPr(C | |C| = c) = det(KC,C)ec(K)\u22121J |C| = cK,\nwhere ec(K) is the c-th coefficient of the characteristic polynomial det(\u03bbI\u2212K) = \u2211Nj=0(\u22121)jej(K)\u03bbN\u2212j. Sampling from a (c-)Dpp can be done in polynomial time, but requires a full eigendecomposition of K [23], which is prohibitive for large N. A number of approaches have been proposed for more efficient sampling [1, 29, 42]. We follow an alternative approach based on Gibbs sampling and show that it can offer fast polynomial-time Dpp sampling and Nystro\u0308m approximations."}, {"heading": "3 Dpp for the Nystro\u0308m Method", "text": "Next, we consider sampling c landmarks C \u2286 [N] from c-Dpp(K), and use the approximation K\u0303 = K\u00b7,CK\u2020C,CKC,\u00b7. We call this approach Dpp-Nystro\u0308m. It was essentially introduced in [9], but without making the explicit connection to Dpps. Our analysis builds on this connection and subsumes existing results that only apply to c being the rank k of the target approximation.\nWe begin with error bounds for matrix approximations:\nTheorem 1 (Relative Error). If C \u223c c-Dpp(K), then Dpp-Nystro\u0308m satisfies the relative error bounds\nEC\n[ \u2016K\u2212 K\u00b7C(KC,C)\u2020KC\u00b7\u2016F\n\u2016K\u2212 Kk\u2016F\n] \u2264 (\nc + 1 c + 1\u2212 k\n)\u221a N \u2212 k,\nEC\n[ \u2016K\u2212 K\u00b7C(KC,C)\u2020KC\u00b7\u20162\n\u2016K\u2212 Kk\u20162\n] \u2264 (\nc + 1 c + 1\u2212 k\n) (N \u2212 k).\nThese bounds hold in expectation. An additional argument based on [33] yields high probability bounds, too (Appendix A).\nTo show Theorem 1, we exploit a property of characteristic polynomials observed in [22]. But first recall that the coefficients of characteristic polynomials satisfy ec(K) = \u2211|S|=c det(B>\u00b7,SB\u00b7,S) = ec(\u039b).\nLemma 2 (Guruswami and Sinop [22]). For any c \u2265 k > 0, it holds that\nec+1(K) ec(K) \u2264 1 c + 1\u2212 k \u2211i>k \u03bbi.\nWith Lemma 2 in hand, we are ready to prove Theorem 1.\nProof (Thm. 1). We begin with the Frobenius norm error, and then show the spectral norm result. Using the decomposition K = B>B, it holds that\nEC [ \u2016K\u2212 K\u00b7CK\u2020C,CKC\u00b7\u2016F ] = EC [ \u2016B>B\u2212 B>B\u00b7,C(B>\u00b7,CB\u00b7,C)\u2020B>\u00b7,CB\u2016F ] = EC [ \u2016B>(I \u2212 B\u00b7,C(B>\u00b7,CB\u00b7,C)\u2020B>\u00b7,C)B\u2016F ] = EC [ \u2016B>(I \u2212UC(UC)>)B\u2016F ] ,\nwhere UC\u03a3C(VC)> is the SVD of B\u00b7,C. Next, we extend UC \u2208 Rr(K)\u00d7c to an orthogonal basis [UC (UC)\u22a5] \u2208 Rr(K)\u00d7r(K) of RN . Using that I \u2212 UC(UC)> = (UC)\u22a5((UC)\u22a5)> and applying\nCauchy-Schwartz yields\nEC [ \u2016B>(I \u2212UC(UC)>)B\u2016F ] = EC [ \u2016B>(UC)\u22a5((UC)\u22a5)>B\u2016F ] = EC [\u221a \u2211i,j(b>i (UC)\u22a5((UC)\u22a5)>bj)2 ] \u2264 EC [\u221a (\u2211i,j \u2016b>i (UC)\u22a5\u201622\u2016b>j (UC)\u22a5\u201622)\n] = EC [ \u2211i \u2016b>i (UC)\u22a5\u201622 ] = 1 ec(K)\n\u2211|C|=c \u2211i det(B>\u00b7,CB\u00b7,C)\u2016b>i (UC)\u22a5\u201622 (a) =\n1 ec(K) \u2211|C|=c \u2211i/\u2208C det(B\u00b7,C\u222a{i}B>\u00b7,C\u222a{i})\n(b) = (c + 1) ec+1(K) ec(K) .\nIn (a), we use that (UC)\u22a5 projects vectors onto the null (column) space of B, and (b) uses the definition of ec. With Lemma 2, it follows that\n(c + 1) ec+1(K)ec(K) \u2264 c+1\nc+1\u2212k \u2211i>k \u03bbi \u2264 c+1c+1\u2212k \u221a N \u2212 k \u221a \u2211i>k \u03bb2i = c+1c+1\u2212k \u221a N \u2212 k\u2016K\u2212 Kk\u2016F.\nThe bound on the Frobenius norm immediately implies the bound on the spectral norm:\nEC [ \u2016K\u2212 K\u00b7C(KC,C)\u2020KC\u00b7\u20162 ] \u2264 EC [ \u2016K\u2212 K\u00b7CK\u2020C,CKC\u00b7\u2016F ] \u2264 c + 1\nc + 1\u2212 k \u221a N \u2212 k\u2016K\u2212 Kk\u2016F \u2264 c + 1 c + 1\u2212 k (N \u2212 k)\u2016K\u2212 Kk\u20162\nRemarks. Compared to previous bounds (e.g., [19] on uniform and leverage score sampling), our bounds seem somewhat weaker asymptotically (since as c\u2192 N they do not converge to 1). This suggests that there is an opportunity for further tightening our bounds, which may be worthwhile, given than in Section Sec. 6.1 our extensive experiments on various datasets with Dpp-Nystro\u0308m show that it attains superior accuracies compared with various state-of-art methods."}, {"heading": "4 Low-rank Kernel Ridge Regression", "text": "Our theoretical (Section 3) and empirical (Section 6.1) results suggest that Dpp-Nystro\u0308m is wellsuited for scaling kernel methods. In this section, we analyze its implications on kernel ridge regression. The experiments in Section 6 confirm our results empirically.\nWe have N training samples {(xi, yi)}Ni=1, where yi = zi + ei are the observed labels under zero-mean noise with finite covariance. We minimize a regularized empirical loss\nmin f\u2208F 1 N\nN\n\u2211 i=1\n`(yi, f (xi)) + \u03b3\n2 \u2016 f \u20162\nover an RKHS F . Equivalently, we solve the problem\nmin \u03b1\u2208RN 1 N\nN\n\u2211 i=1\n`(yi, (K\u03b1)i) + \u03b3 2 \u03b1>K\u03b1,\nfor the corresponding kernel matrix K. With the squared loss `(y, f (x)) = 12 (y \u2212 f (x))2, the resulting estimator is\nf\u0302 (x) = N\n\u2211 i=1\n\u03b1\u0302ik(x, xi), \u03b1\u0302 = (K + n\u03b3I)\u22121y, (4.1)\nand the prediction for {xi}Ni=1 is given by z\u0302 = K(K + N\u03b3I)\u22121y \u2208 RN . Denoting the noise covariance by F, we obtain the risk\nR(z\u0302) = 1N E\u03b5\u2016z\u0302\u2212 z\u2016 2\n= N\u03b32z>(K + N\u03b3I)\u22122z + 1N tr(FK 2(K + N\u03b3I)\u22122) = bias(K) + var(K). (4.2)\nObserve that the bias term is matrix-decreasing (in K) while the variance term is matrix-increasing. Since the estimator (4.1) requires expensive matrix inversions, it is common to replace K in (4.1) by an approximation K\u0303. If K\u0303 is constructed via Nystro\u0308m we have K\u0303 K, and it directly follows that the variance shrinks with this substitution, while the bias increases. Denoting the predictions from K\u0303 by z\u0302K\u0303, Theorem 3 completes the picture of how using K\u0303 affects the risk.\nTheorem 3. If K\u0303 is constructed via Dpp-Nystro\u0308m, then\nEC [\u221a R(z\u0302K\u0303) R(z\u0302) ] \u2264 1 + (c + 1) N\u03b3 ec+1(K) ec(K) .\nAgain, using [33], we obtain bounds that hold with high probability (Appendix A).\nProof. We build on [2, 5]. Knowing that Var(K\u0303) \u2264 Var(K) as K\u0303 K, it remains to bound the bias. Using K = B>B and K\u0303 = B>B\u00b7,C(B>\u00b7,CB\u00b7,C) \u2020B>\u00b7,CB, we obtain\nK\u2212 K\u0303 = B>(I \u2212 B\u00b7,C(B>\u00b7,CB\u00b7,C)\u2020B>\u00b7,C)B\n= B>(UC)\u22a5((UC)\u22a5)>B \u2016B>(UC)\u22a5((UC)\u22a5)>B\u2016F I = \u221a\n\u2211i,j(b>i (UC)\u22a5((UC)\u22a5)>bj)2 I \u221a (\u2211i,j \u2016b>i (UC)\u22a5\u201622\u2016b>j (UC)\u22a5\u201622)I\n= \u2211i \u2016b>i (UC)\u22a5\u201622 I = \u03bdC I,\nwhere \u03bdC = \u2211i \u2016b>i (UC)\u22a5\u201622 \u2264 \u2211i \u2016b>i \u201622 = tr(K). Since (K\u2212 K\u0303) and \u03bdC I commute, we have\n\u2016(K\u0303+N\u03b3I)\u22121(K\u2212 K\u0303)\u201622 = \u2016(K\u0303 + N\u03b3I)\u22121(K\u2212 K\u0303)2(K\u0303 + N\u03b3I)\u22121\u20162\n\u2264 \u03bd2C\u2016(K\u0303 + N\u03b3I) \u22122\u20162 \u2264 ( \u03bdC N\u03b3 )2 .\nIt follows that\n\u2016(K\u0303+N\u03b3I)\u22121z\u2212 (K + N\u03b3I)\u22121z\u20162 = \u2016(K\u0303 + N\u03b3I)\u22121(K\u2212 K\u0303)(K + N\u03b3I)\u22121z\u20162 \u2264 \u2016(K\u0303 + N\u03b3I)\u22121(K\u2212 K\u0303)\u20162\u2016(K + N\u03b3I)\u22121z\u20162\n\u2264 \u03bdC N\u03b3 \u2016(K + N\u03b3I)\u22121z\u20162.\nHence, \u221a z>(K\u0303 + N\u03b3I)\u22122z = \u2016(K\u0303 + N\u03b3I)\u22121z\u20162 \u2264 \u2016(K + N\u03b3I)\u22121z\u20162 + \u2016(K\u0303 + N\u03b3I)\u22121z\u2212 (K + N\u03b3I)\u22121z\u20162\n\u2264 (1 + \u03bdC N\u03b3 )\u2016(K + N\u03b3I)\u22121z\u20162\n= (1 + \u03bdC N\u03b3\n) \u221a z>(K + N\u03b3I)\u22122z.\nFinally, this inequality implies that \u221a bias(K\u0303) bias(K) \u2264 (1 + \u03bdC N\u03b3 ).\nTaking the expectation over C \u223c c-Dpp(K) yields\nEC \u221abias(K\u0303) bias(K)  \u2264 1 + EC [ \u03bdCN\u03b3 ] = 1 + (c + 1) N\u03b3 ec+1(K) ec(K) .\nTogether with the fact that var(K\u0303) \u2264 var(K), we obtain\nEC [\u221a R(z\u0302K\u0303) R(z\u0302) ] = EC \u221abias(K\u0303) + var(K\u0303) bias(K) + var(K)  \u2264 1 + (c + 1)\nN\u03b3 ec+1(K) ec(K) (4.3)\nfor any k \u2264 c.\nRemarks. Theorem 3 quantifies how the learning results depend on the decay of the spectrum of K. In particular, the ratio ec+1(K)/ec(K) closely relates to the effective rank of K: if \u03bbc > a and \u03bbc+1 a, this ratio is almost zero, resulting in near-perfect approximations and no loss in learning.\nThere exist works that consider Nystro\u0308m methods in this scenario [2, 5]. Our theoretical bounds could also be tightened in this setting, possibly by a tighter bound on the elementary symmetric polynomial ratio. This theoretical exercise may be worthwhile given our extensive experiments comparing Dpp-Nystro\u0308m against other state-of-art methods in Sec. 6.2 that reveal the superior performance of Dpp-Nystro\u0308m."}, {"heading": "5 Fast Mixing Markov Chain Dpp", "text": "Despite its excellent empirical performance and strong theoretical results, determinantal sampling for Nystro\u0308m has rarely been used in applications due to the computational cost of O(N3) for directly sampling from a Dpp, which involves an eigendecomposition. Instead, we follow a different route: an MCMC sampler, which offers a promising alternative if the chain mixes fast enough. Recent empirical results provide initial evidence [24], but without a theoretical analysis3; other recent works [21, 34] do not apply to our cardinality-constrained setting. We offer a theoretical analysis that confirms fast mixing (i.e., polynomial or even linear-time sampling) under certain conditions, and connect it to our empirical results. The empirical results in Section 6 illustrate the favorable performance of Dpp-Nystro\u0308m in trading off time and error. Concurrently with this paper, Anari et al. [4] derived a different, general analysis of fast mixing that also confirms our observations.\nAlgorithm 1 shows a Gibbs sampler for k-Dpp. Starting with a uniformly random set Y0, at iteration t, we try to swap an element yin \u2208 Yt with an element yout /\u2208 Yt, according to Pr(Yt) and Pr(Yt \u222a {yout} \\ {yin}). The stationary distribution of this chain is exactly the desired k-Dpp(K).\nThe mixing time \u03c4(\u03b5) of the chain is the number of iterations until the distribution over the states (subsets) is close to the desired one, as measured by total variation: \u03c4(\u03b5) = min{t|maxY0 TV(Yt, \u03c0) \u2264 e}. We bound \u03c4(\u03b5) via coupling techniques. Given a Markov chain (Yt) on a state space \u2126 with\n3The analysis in [24] is not correct.\nAlgorithm 1 Gibbs sampler for c-Dpp Input: K the kernel matrix, Y = [N] the ground set Output: Y sampled from exact c-Dpp(K) Randomly Initialize Y \u2286 Y , |Y| = c while not mixed do\nSample b from uniform Bernoulli distribution if b = 1 then\nPick yin \u2208 Y and yout \u2208 Y\\Y uniformly randomly q(yin, yout, Y)\u2190 det(KY\u222a{yout}\\{yin})\ndet KY\u222a{yout}\\{yin}+det(KY)\nY \u2190 Y \u222a {yout}\\{yin} with prob. q(yin, yout, Y) end if\nend while\ntransition matrix P, a coupling is a new chain (Yt, Zt) on \u2126\u00d7\u2126 such that both (Yt) and (Zt), if considered marginally, are Markov chains with the same transition matrix P. The key point of coupling is to construct such a new chain to encourage Yt and Zt to coalesce quickly. If, in the new chain, Pr(Yt 6= Zt) \u2264 \u03b5 for some fixed t regardless of the starting state (Y0, Z0), then \u03c4(\u03b5) \u2264 t [3].\nSuch coalescing chains can be difficult to construct. Path coupling [11] relieves this burden by reducing the coupling to adjacent states in an appropriately constructed state graph. The coupling of arbitrary states follows by aggregation over a path between the states. Path coupling is formalized in the following lemma.\nLemma 4. [11, 16] Let \u03b4 be an integer-valued metric on \u2126\u00d7\u2126 where \u03b4(\u00b7, \u00b7) \u2264 D. Let E be a subset of \u2126\u00d7\u2126 such that for all (Yt, Zt) \u2208 \u2126\u00d7\u2126 there exists a path Yt = X0, . . . , Xr = Zt between Yt and Zt where (Xi, Xi+1) \u2208 E for i \u2208 [r\u2212 1] and \u2211i \u03b4(Xi, Xi+1) = \u03b4(Yt, Zt). Suppose a coupling (R, T)\u2192 (R\u2032, T\u2032) of the Markov chain is defined on all pairs in E such that there exists an \u03b1 < 1 such that E[\u03b4(R\u2032, T\u2032)] \u2264 \u03b1\u03b4(R, T) for all (R, T) \u2208 E, then we have\n\u03c4(\u03b5) \u2264 log(D\u03b5 \u22121)\n(1\u2212 \u03b1) .\nThe lemma says that if we have a contraction of the two chains in expectation (\u03b1 < 1), then the chain mixes fast. With the path coupling lemma, we obtain a bound on the mixing time that can be linear in the data set size N.\nThe actual mixing time depends on three quantities that relate to how sensitive the transition probabilities are to swapping a single element in a set of size c. Consider an arbitrary set S of columns, |S| = c\u2212 1, and complete it to two c-sets R = S \u222a {r} and T = S \u222a {t} that differ in exactly one element. Our quantities are, for u /\u2208 R \u222a T, and v \u2208 S:\np1(S, r, t, u) = min{q(r, u, R), q(t, u, T)} p2(S, r, t, u) = min{q(v, t, R), q(v, u, T)}\np3(S, r, t, v, u) = |q(v, u, R)\u2212 q(v, u, T)|. Theorem 5. Let the contraction coefficient \u03b1 be given by\n\u03b1 = max |S|=c\u22121,r,t\u2208[n]\\S,r 6=t \u2211 u3\u2208S,u4 /\u2208S\u222a{r,t} p3(S, r, t, u3, u4)\u2212 \u2211 u1 /\u2208S\u222a{r,t} p1(S, r, t, u1)\u2212 \u2211 u2\u2208S p2(S, r, t, u2).\nWhen \u03b1 < 1, the mixing time for the Gibbs sampler in Algorithm 1 is bounded as\n\u03c4(\u03b5) \u2264 2c(N \u2212 c) log(c\u03b5 \u22121)\n(1\u2212 \u03b1) .\nProof. We bound the mixing time via path coupling. Let \u03b4(R, T) = |R\u2295 T|/2 be half the Hamming distance on the state space, and define E to consist of all state pairs (R, T) in \u2126\u00d7\u2126 such that \u03b4(R, T) = 1. We intend to show that for all states (R, T) \u2208 E and next states (R\u2032, T\u2032) \u2208 E, we have E[\u03b4(R\u2032, T\u2032)] \u2264 \u03b1\u03b4(R, T) for an appropriate \u03b1.\nSince \u03b4(R, T) = 1, the sets R and T differ in only two entries. Let S = R \u2229 T, so |S| = c\u2212 1 and R = S\u222a {r} and T = S\u222a {t}. For a state transition, we sample an element rin \u2208 R and rout \u2208 [n]\\R as switching candidates for R, and elements tin \u2208 T and tout \u2208 [n]\\T as switching candidates for T. Let bR and bT be the Bernoulli random variables indicating whether we try to make a transition. In our coupling we always set bR = bT . Hence, if bR = 0 then both chains will not transition and the distance of states remains. For bR = bT = 1, we distinguish four cases:\nCase C1 If rin = r and rout = t, we let tin = t and tout = r. As a result, \u03b4(R\u2032, T\u2032) = 0.\nCase C2 If rin = r and rout = u1 /\u2208 S \u222a {r, t}, we let tin = t and tout = u1. In this case, if both chains transition, then the resulting distance is zero, otherwise it remains one. With probability p1(S, r, t, u1) = min{q(r, u1, R), q(t, u1, T)} both chains transition.\nCase C3 If rin = u2 \u2208 S and rout = t, we let tin = u2 and tout = r. Again, if both chains transition, then the resulting distance is \u03b4(R\u2032, T\u2032) = 0, otherwise it remains one. With probability p2(S, r, t, u2) = min{q(u2, t, R), q(u2, u1, T)} both chains transition.\nCase C4 If rin = u3 \u2208 S and rout = u4 /\u2208 S \u222a {r, t}, we let tin = u3 and tout = u4. If both chains make the same transition (both move or do not move), the resulting distance is one, otherwise it increases to 2. The distance increases with probability p3(S, r, t, u3, u4) = |q(u3, u4, R)\u2212 q(u3, u4, T)|.\nWith those four cases, we can now bound E[\u03b4(R\u2032, T\u2032)]. For all (R, T) \u2208 E, i.e., \u03b4(R, T) = 1:\nE[\u03b4(R\u2032, T\u2032)] E[\u03b4(R, T)] = 1 2 + Pr(C2)E[\u03b4(R\u2032, T\u2032)|C2] + Pr(C3)E[\u03b4(R\u2032, T\u2032)|C3] + Pr(C4)E[\u03b4(R\u2032, T\u2032)|C4]\n= 1 2 + 1 2c(n\u2212 c)\n( \u2211\nu1 /\u2208S\u222a{r,t} (1\u2212 p1(u1)) + \u2211 u2\u2208S (1\u2212 p2(u2)) + \u2211\nu3\u2208S, u4 /\u2208S\u222a{r,t}\n(1 + p3(u3, u4)) )\n= 1 2c(n\u2212 c) ( 2c(n\u2212 1) + \u2211\nu3\u2208S, u4 /\u2208S\u222a{r,t}\np3(u3, u4)\u2212 \u2211 u1 /\u2208S\u222a{r,t} p1(u1)\u2212 \u2211 u2\u2208S\np2(u2)\u2212 1 ) ,\nwhere we did not explicitly write the arguments S, r, t to p1, p2, p3. For\n\u03b1 = max |S|=c\u22121, r,t\u2208[n]\\S,\nr 6=t\n\u2211 u3\u2208S,\nu4 /\u2208S\u222a{r,t}\np3(u3, u4)\u2212 \u2211 u1 /\u2208S\u222a{r,t} p1(u1)\u2212 \u2211 u2\u2208S p2(u2)\nand \u03b1 < 1 the Path Coupling Lemma 4 implies that\n\u03c4(\u03b5) \u2264 2c(N \u2212 c) log(c\u03b5 \u22121)\n(1\u2212 \u03b1) .\nRemarks. If \u03b1 < 1 is fixed, then the mixing time (running time) depends only linearly on N. The coefficient \u03b1 itself depends on our three quantities. In particular, fast mixing requires p3 (the difference between transition probabilities) to be very small compared to p1, p2, at least on average. The difference p3 measures how exchangeable two points r and t are. This notion of symmetry is\nclosely related to a symmetry that determines the complexity of submodular maximization [41] (indeed, F(S) = log det KS is a submodular function). This symmetry only needs to hold for most pairs r, t, and most swapping points u, v. It holds for kernels with sufficiently fast-decaying similarities, similar to the conditions in [34] for unconstrained sampling.\nOne iteration of the sampler can be implemented efficiently in O(c2) time using block inversion [20]. Additional speedups via quadrature are also possible [30]. Together with the analysis of mixing time, this leads to fast sampling methods for k-Dpps."}, {"heading": "6 Experiments", "text": "In our experiments, we evaluate the performance of Dpp-Nystro\u0308m on both kernel approximation and kernel learning tasks, in terms of running time and accuracy.\nWe use 8 datasets: Abalone, Ailerons, Elevators, CompAct, CompAct(s), Bank32NH, Bank8FM and California Housing4. We subsample 4,000 points from each dataset (3,000 training and 1,000 test). Throughout our experiments, we use an RBF kernel and choose the bandwidth \u03c3 and regularization parameter \u03bb for each dataset by 10-fold cross-validation. We initialize the Gibbs sampler via Kmeans++ and run for 3,000 iterations. Results are averaged over 3 random subsets of data.\n6.1 Kernel Approximation\nWe first explore Dpp-Nystro\u0308m (kDPP in the figures) for approximating kernel matrices. We compare to uniform sampling (Unif) and leverage score sampling (Lev) [19] as baseline landmark selection methods. We also include AdapFull (AdapFull) [13] that performs quite well in practice but scales poorly, as O(N2), with the size of dataset. Although sampling with regularized leverage scores (RegLev) [2] is not originally designed for kernel approximations, we include its results to see how regularization affects leverage score sampling.\nFigure 1 shows example results on the Ailerons data; further results may be found in the appendix. Dpp-Nystro\u0308m performs well, achieving the lowest error as measured in both spectral\n4http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html\nand Frobenius norm. The only method that is on par in terms of accuracy is AdapFull, which has a much higher running time.\nFor a different perspective, Figure 2 shows the improvement in error over Unif. Relative improvements are averaged over all data sets. Again, the performance of Dpp-Nystro\u0308m almost always dominate those of other methods, and achieves an up to 80% reduction in error.\n6.2 Kernel Ridge Regression\nNext, we apply Dpp-Nystro\u0308m to kernel ridge regression, comparing against uniform sampling (Unif) [5] and regularized leverage score sampling (RegLev) [2] which have theoretical guarantees for this task. Figure 3 illustrates an example result: non-uniform sampling greatly improves accuracy, with kDPP improving over regularized leverage scores in particular for a small number of landmarks, where a single column has a larger effect.\nFigure 4 displays the average improvement over Unif, averaged over 8 data sets. Again, the\nperformance of kDPP dominates RegLev and Unif, and leads to gains in accuracy. On average kDPP consistently achieves more than 20% improvement over Unif.\n6.3 Mixing of the Gibbs Markov Chain\nIn the next experiment, we empirically study the mixing of the Gibbs chain with respect to matrix approximation errors, the ultimate measure that is of interest in our application of the sampler. We use c = 50 and choose N as 1,000 and 4,000. To exclude impacts of the initialization, we pick the initial state Y0 uniformly at random. We run the chain for 5,000 iterations, monitoring how the error changes with the number of iterations. Example results on the Ailerons data are shown in Figure 5. Empirically, the error drops very quickly and afterwards fluctuates only little, indicating a fast convergence of the approximation error. Other error measures and larger c, included in the appendix, confirm this trend.\nNotably, our empirical results suggest that the mixing time does not increase much as N increases greatly, suggesting that the Gibbs sampler remains fast even for large N.\nIn Theorem 5, the mixing time depends on the quantity \u03b1. By subsampling 1,000 random sets S and column indices r, t, we approximately computed \u03b1 on our data sets. We find that, as expected, \u03b1 < 1 in particular for kernels with a smaller bandwidth, and in general \u03b1 increases with k. In accordance with the theory, we found that the mixing time (in terms of error) too increases with k. In practice, we observe a fast drop in error even for cases where \u03b1 > 1, indicating that Theorem 5 is conservative and that the iterative MCMC approach is even more widely applicable.\n6.4 Time-Error Tradeoffs\nIterative methods like the Gibbs sampler offer tradeoffs between time and error. The longer the Markov Chain runs, the closer the sampling distribution is to the desired Dpp, and the higher the accuracy obtained by Nystro\u0308m. We hence explicitly show the time and accuracy trade-off of the sampler on Ailerons (of size 4,000) for up to 200 and California Housing (of size 12,000) for up to 100 iterations.\nA similar tradeoff occurs with leverage scores. For the experiments in the other sections, we computed the (regularized) leverage scores for Lev and RegLev exactly. This requires a full, computationally expensive eigendecomposition. For a fast, rougher approximation, we here compare to an approximation mentioned in [2]. Concretely, we sample p elements with probability proportional to the diagonal entries of kernel matrices Kii, and then use a Nystro\u0308m-like method to construct an approximate low-rank decomposition of K, and compute scores based on this approximation. We vary p from 20 to 340 on Ailerons and 20 to 140 on California Housing to show the tradeoff for approximate leverage score sampling (AppLev) and regularized leverage score sampling (AppRegLev). We also include AdapPartial (AdapPart) [28] that approximates AdapFull and is much more efficient, and Kmeans Nystro\u0308m (Kmeans) [44] that empirically perform very well in kernel approximation.\nFigure 6 summarizes and compares the tradeoffs offered by these different methods on the Ailerons and California Housing datasets. The x axis indicates time, the y axis error, so the lower left is the preferred corner. We see that AdapFull, Lev and RegLev are expensive and perform worse than kDPP. The approximate variants AdapPart, AppLev and AppRegLev have comparable efficiency but higher error. On the smaller data, Kmeans is accurate but needs more time than kDPP,\nwhile on the larger data it is dominated in both accuracy and time by kDPP. Overall, on the larger data, Dpp-Nystro\u0308m offers the best tradeoff of accuracy and efficiency."}, {"heading": "7 Conclusion", "text": "In this paper, we revisited the use of k-Determinantal Point Processes for sampling good landmarks for the Nystro\u0308m method. We theoretically and empirically observe its competitive performance, for both matrix approximation and ridge regression, compared to state-of-the-art methods.\nTo make this accurate method scalable to large matrices, we consider an iterative approach, and analyze it theoretically as well as empirically. Our results indicate that the iterative approach, a Gibbs sampler, achieves good landmark samples quickly; under certain conditions even in a number of iteratons linear in N, for an N by N matrix. Finally, our empirical results demonstrate that among state-of-the-art methods, the iterative sampler yields the best tradeoff between efficiency and accuracy."}, {"heading": "Acknowledgements", "text": "This research was partially supported by an NSF CAREER award 1553284, NSF grant IIS-1409802, and a Google Research Award. We also thank Xixian Chen for discussions."}, {"heading": "A Bounds that hold with High Probability", "text": "To show high probability bounds we employ concentration results on homogeneous strongly Rayleigh measures. Specifically, we use the following theorem.\nTheorem 6 (Pemantle and Peres [33]). Let P be a k-homogeneous strongly Rayleigh probability measure on {0, 1}N and f an `-Lipschitz function on {0, 1}N , then\nP( f \u2212E[ f ] \u2265 a`) \u2264 exp{\u2212a2/8k}.\nIt is known that a k-Dpp is a homogeneous strongly Rayleigh measure on {0, 1}N [4, 10], thus Theorem 6 applies to results obtained with k-Dpp. Concretely, for the bound in Theorem 1 that holds in expectation, we have the following bound that holds with high probability:\nCorollary 7. When sampling C \u223c k-Dpp(K), for any \u03b4 \u2208 (0, 1), with probability at least 1\u2212 \u03b4 we have\n\u2016K\u2212 K\u00b7C(KC,C)\u2020KC\u00b7\u2016F \u2016K\u2212 Kk\u2016F\n\u2264 (\nc + 1 c + 1\u2212 k\n)\u221a N \u2212 k + \u221a 8c log(1/\u03b4) \u221a\u221a\u221a\u221a \u2211Ni=1 \u03bb2i \u2211Ni=k+1 \u03bb 2 i ,\n\u2016K\u2212 K\u00b7C(KC,C)\u2020KC\u00b7\u20162 \u2016K\u2212 Kk\u20162\n\u2264 (\nc + 1 c + 1\u2212 k\n) (N \u2212 k) + \u221a 8c log(1/\u03b4) \u03bb1\u03bbk+1 ,\nwhere \u03bb1 \u2265 \u03bb2 \u2265 . . . \u2265 \u03bbN are the eigenvalues of K. Proof. The Lipschitz constants of the relative errors are upper bounded by \u221a \u2211Ni=1 \u03bb 2 i\n\u2211Ni=k+1 \u03bb 2 i\nand \u03bb1\u03bbk+1 ,\nrespectively. Applying Theorem 6 yields the results.\nFor the bound in Theorem 3 that holds in expectation, we have the following bound that holds with high probability:\nCorollary 8. If K\u0303 is constructed via Dpp-Nystro\u0308m, then with probability at least 1\u2212 \u03b4, \u221a\nbias(K\u0303) bias(K) is\nupper-bounded by\n1 + 1\nN\u03b3\n( (c + 1)ec+1(K) ec(K) + \u221a 8c log(1/\u03b4)tr(K) ) .\nProof. Consider the function fC(K) = \u03bdC = \u2211i \u2016b>i (UC)\u22a5\u201622 \u2264 \u2211i \u2016b>i \u201622 = tr(K). Since 0 \u2264 fC(K) \u2264 tr(K), it follows that the Lipschitz constant for fC is at most tr(K). Thus when C \u223c k-Dpp and \u03b4 \u2208 (0, 1), by applying Theorem 6 we see that the inequality \u03bdC \u2264 E [\u03bdC] + \u221a 8c log(1/\u03b4)tr(K) holds with probability at least 1\u2212 \u03b4. Hence\nEC \u221abias(K\u0303) bias(K)  \u2264 1 + E [ \u03bdC N\u03b3 ] + \u221a 8c log(1/\u03b4) tr(K) N\u03b3\n= 1 + 1\nN\u03b3\n( (c + 1)ec+1(K) ec(K) + \u221a 8c log(1/\u03b4)tr(K) )\nholds with probability at least 1\u2212 \u03b4."}, {"heading": "B Supplementary Experiments", "text": "B.1 Kernel Approximation\nFig. 7 shows the matrix norm relative error of various methods in kernel approximation on the remaining 7 datasets mentioned in the main text.\nB.2 Approximated Kernel Ridge Regression\nFig. 8 shows the training and test error of various methods for kernel ridge regression on the remaining 7 datasets.\nB.3 Mixing of Markov Chain k-Dpp\nWe first show the mixing of the Gibbs Dpp-Nystro\u0308m with 50 landmarks with different performance measures: relative spectral norm error, training error and test error of kernel ridge regression in Fig. 9.\nWe also show corresponding results with respect to 100 and 200 landmarks in Fig. 10 and Fig. 11, so as to illustrate that for varying number of landmarks the chain is indeed fast mixing and will give reasonably good result within a small number of iterations.\nB.4 Running Time Analysis\nWe next show time-error trade-offs for various sampling methods on small and larger datasets with respect to Fnorm and 2norm errors. We sample 20 landmarks from Ailerons dataset of size 4,000 and California Housing of size 12,000. The result is shown in Figure 12 and Figure 13 and similar trends as the example results in the main text could be spotted: on small scale dataset (size 4,000) kDPP get very good time-error trade-off. It is more efficient than Kmeans, though the error is a bit larger. While on larger dataset (size 12,000) the efficiency is further enhanced while the error is even lower than Kmeans. It also have lower variances in both cases compared to AppLev and AppRegLev. Overall, on larger dataset we obtain the best time-error trade-off with kDPP."}], "references": [{"title": "Nystr\u00f6m approximation for large-scale determinantal processes", "author": ["R.H. Affandi", "A. Kulesza", "E. Fox", "B. Taskar"], "venue": "AISTATS, pages 85\u201398,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast randomized kernel methods with statistical guarantees", "author": ["A.E. Alaoui", "M.W. Mahoney"], "venue": "NIPS,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Some inequalities for reversible Markov chains", "author": ["D.J. Aldous"], "venue": "Journal of the London Mathematical Society, pages 564\u2013576,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1982}, {"title": "Monte Carlo Markov chain algorithms for sampling strongly Rayleigh distributions and determinantal point processes", "author": ["N. Anari", "S.O. Gharan", "A. Rezaei"], "venue": "COLT,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Sharp analysis of low-rank kernel matrix approximations", "author": ["F.R. Bach"], "venue": "COLT,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "JMLR, pages 1\u201348,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Predictive low-rank decomposition for kernel methods", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "ICML, pages 33\u201340,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "On landmark selection and sampling in high-dimensional data analysis", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, pages 4295\u20134312,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["M.-A. Belabbas", "P.J. Wolfe"], "venue": "Proceedings of the National Academy of Sciences, pages 369\u2013374,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Negative dependence and the geometry of polynomials", "author": ["J. Borcea", "P. Br\u00e4nd\u00e9n", "T. Liggett"], "venue": "Journal of the American Mathematical Society, pages 521\u2013567,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Path coupling: A technique for proving rapid mixing in Markov chains", "author": ["R. Bubley", "M. Dyer"], "venue": "FOCS, pages 223\u2013231,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "On the impact of kernel approximation on learning accuracy", "author": ["C. Cortes", "M. Mohri", "A. Talwalkar"], "venue": "AISTATS, pages 113\u2013120,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["A. Deshpande", "L. Rademacher", "S. Vempala", "G. Wang"], "venue": "SODA, pages 1117\u20131126,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "JMLR, pages 2153\u20132175,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix", "author": ["P. Drineas", "R. Kannan", "M.W. Mahoney"], "venue": "SIAM Journal on Computing, pages 158\u2013183,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "A more rapidly mixing Markov chain for graph colorings", "author": ["M. Dyer", "C. Greenhill"], "venue": "Random Structures and Algorithms, pages 285\u2013317,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg"], "venue": "JMLR, pages 243\u2013264,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "TPAMI, pages 214\u2013225,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "author": ["A. Gittens", "M.W. Mahoney"], "venue": "ICML,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": "JHU Press,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Sampling from probabilistic submodular models", "author": ["A. Gotovos", "H. Hassani", "A. Krause"], "venue": "NIPS, pages 1936\u20131944,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["V. Guruswami", "A.K. Sinop"], "venue": "SODA, pages 1207\u20131214,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Determinantal processes and independence", "author": ["J.B. Hough", "M. Krishnapur", "Y. Peres", "B. Vir\u00e1g"], "venue": "Probability Surveys, pages 206\u2013229,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast determinantal point process sampling with application to clustering", "author": ["B. Kang"], "venue": "NIPS, pages 2319\u20132327,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "k-DPPs: Fixed-size determinantal point processes", "author": ["A. Kulesza", "B. Taskar"], "venue": "ICML, pages 1193\u20131200,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Determinantal point processes for machine learning", "author": ["A. Kulesza", "B. Taskar"], "venue": "arXiv preprint arXiv:1207.6083,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Ensemble Nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "NIPS, pages 1060\u20131068,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Sampling methods for the nystr\u00f6m method", "author": ["S. Kumar", "M. Mohri", "A. Talwalkar"], "venue": "The Journal of Machine Learning Research, pages 981\u20131006,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient sampling for k-determinantal point processes", "author": ["C. Li", "S. Jegelka", "S. Sra"], "venue": "AISTATS,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Gaussian quadrature for matrix inverse forms with applications", "author": ["C. Li", "S. Sra", "S. Jegelka"], "venue": "ICML,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "The coincidence approach to stochastic point processes", "author": ["O. Macchi"], "venue": "Advances in Applied Probability, pages 83\u2013122,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1975}, {"title": "\u00dcber die praktische Aufl\u00f6sung von Integralgleichungen mit Anwendungen auf Randwertaufgaben", "author": ["E.J. Nystr\u00f6m"], "venue": "Acta Mathematica, pages 185\u2013204,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1930}, {"title": "Concentration of Lipschitz functionals of determinantal and other strong Rayleigh measures", "author": ["R. Pemantle", "Y. Peres"], "venue": "Combinatorics, Probability and Computing, pages 140\u2013160,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast mixing for discrete point processes", "author": ["P. Rebeschini", "A. Karbasi"], "venue": "COLT,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Less is more: Nystr\u00f6m computational regularization", "author": ["A. Rudi", "R. Camoriano", "L. Rosasco"], "venue": "NIPS,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast kernel-based independent component analysis", "author": ["H. Shen", "S. Jegelka", "A. Gretton"], "venue": "IEEE Transactions on Signal Processing, pages 3498\u20133511,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse greedy matrix approximation for machine learning", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "ICML,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2000}, {"title": "A review of Nystr\u00f6m methods for large-scale machine learning", "author": ["S. Sun", "J. Zhao", "J. Zhu"], "venue": "Information Fusion, pages 36\u201348,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale manifold learning", "author": ["A. Talwalkar", "S. Kumar", "H. Rowley"], "venue": "CVPR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Large-scale SVD and manifold learning", "author": ["A. Talwalkar", "S. Kumar", "M. Mohri", "H. Rowley"], "venue": "JMLR, pages 3129\u20133152,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Symmetry and approximability of submodular maximization problems", "author": ["J. Vondr\u00e1k"], "venue": "SIAM Journal on Computing, 42(1):265\u2013304,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2013}, {"title": "Using the matrix ridge approximation to speedup determinantal point processes sampling algorithms", "author": ["S. Wang", "C. Zhang", "H. Qian", "Z. Zhang"], "venue": "AAAI,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "NIPS, pages 682\u2013688,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2001}, {"title": "Improved Nystr\u00f6m low-rank approximation and error analysis", "author": ["K. Zhang", "I.W. Tsang", "J.T. Kwok"], "venue": "ICML, pages 1232\u20131239,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 31, "context": "A notable instance is the Nystr\u00f6m method [32, 43], which takes a positive semidefinite matrix K \u2208 RN\u00d7N as input, selects from it a small subset C of columns K\u00b7,C, and constructs the approximation K\u0303 = K\u00b7,CK C,CKC,\u00b7.", "startOffset": 41, "endOffset": 49}, {"referenceID": 42, "context": "A notable instance is the Nystr\u00f6m method [32, 43], which takes a positive semidefinite matrix K \u2208 RN\u00d7N as input, selects from it a small subset C of columns K\u00b7,C, and constructs the approximation K\u0303 = K\u00b7,CK C,CKC,\u00b7.", "startOffset": 41, "endOffset": 49}, {"referenceID": 5, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 135, "endOffset": 142}, {"referenceID": 35, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 135, "endOffset": 142}, {"referenceID": 7, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 191, "endOffset": 198}, {"referenceID": 17, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 191, "endOffset": 198}, {"referenceID": 38, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 218, "endOffset": 226}, {"referenceID": 39, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 218, "endOffset": 226}, {"referenceID": 34, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 243, "endOffset": 247}, {"referenceID": 0, "context": "Since its introduction into machine learning, the Nystr\u00f6m method has been applied to a wide spectrum of problems, including kernel ICA [6, 36], kernel and spectral methods in computer vision [8, 18], manifold learning [39, 40], regularization [35], and efficient approximate sampling [1].", "startOffset": 284, "endOffset": 287}, {"referenceID": 1, "context": "Recent work [2, 5, 12] shows risk bounds for Nystr\u00f6m applied to various kernel methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 4, "context": "Recent work [2, 5, 12] shows risk bounds for Nystr\u00f6m applied to various kernel methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 11, "context": "Recent work [2, 5, 12] shows risk bounds for Nystr\u00f6m applied to various kernel methods.", "startOffset": 12, "endOffset": 22}, {"referenceID": 11, "context": "This choice governs the approximation error and subsequent performance of the approximated learning methods [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 42, "context": "The most basic strategy is to sample landmarks uniformly at random [43].", "startOffset": 67, "endOffset": 71}, {"referenceID": 36, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 129, "endOffset": 136}, {"referenceID": 16, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 129, "endOffset": 136}, {"referenceID": 13, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 198, "endOffset": 202}, {"referenceID": 14, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 222, "endOffset": 226}, {"referenceID": 18, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 262, "endOffset": 266}, {"referenceID": 43, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 280, "endOffset": 284}, {"referenceID": 8, "context": "More sophisticated non-uniform selection strategies include deterministic greedy schemes [37], incomplete Cholesky decomposition [7, 17], sampling with probabilities proportional to diagonal values [14] or to column norms [15], sampling based on leverage scores [19], via K-means [44], or using submatrix determinants [9].", "startOffset": 318, "endOffset": 321}, {"referenceID": 25, "context": "We study landmark selection using Determinantal Point Processes (Dpp), discrete probability models that allow tractable sampling of diverse non-independent subsets [26, 31].", "startOffset": 164, "endOffset": 172}, {"referenceID": 30, "context": "We study landmark selection using Determinantal Point Processes (Dpp), discrete probability models that allow tractable sampling of diverse non-independent subsets [26, 31].", "startOffset": 164, "endOffset": 172}, {"referenceID": 8, "context": "Our work generalizes the determinant based scheme of Belabbas and Wolfe [9].", "startOffset": 72, "endOffset": 75}, {"referenceID": 8, "context": "Previous analyses [9] fix a cardinality c = k; we allow the general case of selecting c \u2265 k columns.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "This task has been noted as the main application in [2, 5].", "startOffset": 52, "endOffset": 58}, {"referenceID": 4, "context": "This task has been noted as the main application in [2, 5].", "startOffset": 52, "endOffset": 58}, {"referenceID": 10, "context": "We consider a Gibbs sampler for k-Dpp, and analyze its mixing time using a path coupling [11] argument.", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 8, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 11, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 13, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 14, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 16, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 18, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 36, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 43, "context": "The actual set of landmarks affects the approximation quality, and is hence the subject of a substantial body of research [7, 9, 12, 14, 15, 17, 19, 37, 44].", "startOffset": 122, "endOffset": 156}, {"referenceID": 26, "context": "The ensemble Nystr\u00f6m method [27], for instance, uses a weighted combination of approximations.", "startOffset": 28, "endOffset": 32}, {"referenceID": 37, "context": "The modified Nystr\u00f6m method constructs an approximation K\u00b7,CK \u00b7,CKK \u2020 C,\u00b7KC,\u00b7 [38].", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "1) When conditioning on a fixed cardinality, one obtains a k-Dpp [25].", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "Sampling from a (c-)Dpp can be done in polynomial time, but requires a full eigendecomposition of K [23], which is prohibitive for large N.", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "A number of approaches have been proposed for more efficient sampling [1, 29, 42].", "startOffset": 70, "endOffset": 81}, {"referenceID": 28, "context": "A number of approaches have been proposed for more efficient sampling [1, 29, 42].", "startOffset": 70, "endOffset": 81}, {"referenceID": 41, "context": "A number of approaches have been proposed for more efficient sampling [1, 29, 42].", "startOffset": 70, "endOffset": 81}, {"referenceID": 8, "context": "It was essentially introduced in [9], but without making the explicit connection to Dpps.", "startOffset": 33, "endOffset": 36}, {"referenceID": 32, "context": "An additional argument based on [33] yields high probability bounds, too (Appendix A).", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "To show Theorem 1, we exploit a property of characteristic polynomials observed in [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Lemma 2 (Guruswami and Sinop [22]).", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": ", [19] on uniform and leverage score sampling), our bounds seem somewhat weaker asymptotically (since as c\u2192 N they do not converge to 1).", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": "Again, using [33], we obtain bounds that hold with high probability (Appendix A).", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "We build on [2, 5].", "startOffset": 12, "endOffset": 18}, {"referenceID": 4, "context": "We build on [2, 5].", "startOffset": 12, "endOffset": 18}, {"referenceID": 1, "context": "There exist works that consider Nystr\u00f6m methods in this scenario [2, 5].", "startOffset": 65, "endOffset": 71}, {"referenceID": 4, "context": "There exist works that consider Nystr\u00f6m methods in this scenario [2, 5].", "startOffset": 65, "endOffset": 71}, {"referenceID": 23, "context": "Recent empirical results provide initial evidence [24], but without a theoretical analysis3; other recent works [21, 34] do not apply to our cardinality-constrained setting.", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "Recent empirical results provide initial evidence [24], but without a theoretical analysis3; other recent works [21, 34] do not apply to our cardinality-constrained setting.", "startOffset": 112, "endOffset": 120}, {"referenceID": 33, "context": "Recent empirical results provide initial evidence [24], but without a theoretical analysis3; other recent works [21, 34] do not apply to our cardinality-constrained setting.", "startOffset": 112, "endOffset": 120}, {"referenceID": 3, "context": "[4] derived a different, general analysis of fast mixing that also confirms our observations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Given a Markov chain (Yt) on a state space \u03a9 with 3The analysis in [24] is not correct.", "startOffset": 67, "endOffset": 71}, {"referenceID": 2, "context": "If, in the new chain, Pr(Yt 6= Zt) \u2264 \u03b5 for some fixed t regardless of the starting state (Y0, Z0), then \u03c4(\u03b5) \u2264 t [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 10, "context": "Path coupling [11] relieves this burden by reducing the coupling to adjacent states in an appropriately constructed state graph.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "[11, 16] Let \u03b4 be an integer-valued metric on \u03a9\u00d7\u03a9 where \u03b4(\u00b7, \u00b7) \u2264 D.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[11, 16] Let \u03b4 be an integer-valued metric on \u03a9\u00d7\u03a9 where \u03b4(\u00b7, \u00b7) \u2264 D.", "startOffset": 0, "endOffset": 8}, {"referenceID": 40, "context": "closely related to a symmetry that determines the complexity of submodular maximization [41] (indeed, F(S) = log det KS is a submodular function).", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "It holds for kernels with sufficiently fast-decaying similarities, similar to the conditions in [34] for unconstrained sampling.", "startOffset": 96, "endOffset": 100}, {"referenceID": 19, "context": "One iteration of the sampler can be implemented efficiently in O(c2) time using block inversion [20].", "startOffset": 96, "endOffset": 100}, {"referenceID": 29, "context": "Additional speedups via quadrature are also possible [30].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "We compare to uniform sampling (Unif) and leverage score sampling (Lev) [19] as baseline landmark selection methods.", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "We also include AdapFull (AdapFull) [13] that performs quite well in practice but scales poorly, as O(N2), with the size of dataset.", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "Although sampling with regularized leverage scores (RegLev) [2] is not originally designed for kernel approximations, we include its results to see how regularization affects leverage score sampling.", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "2 Kernel Ridge Regression Next, we apply Dpp-Nystr\u00f6m to kernel ridge regression, comparing against uniform sampling (Unif) [5] and regularized leverage score sampling (RegLev) [2] which have theoretical guarantees for this task.", "startOffset": 123, "endOffset": 126}, {"referenceID": 1, "context": "2 Kernel Ridge Regression Next, we apply Dpp-Nystr\u00f6m to kernel ridge regression, comparing against uniform sampling (Unif) [5] and regularized leverage score sampling (RegLev) [2] which have theoretical guarantees for this task.", "startOffset": 176, "endOffset": 179}, {"referenceID": 1, "context": "For a fast, rougher approximation, we here compare to an approximation mentioned in [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 27, "context": "We also include AdapPartial (AdapPart) [28] that approximates AdapFull and is much more efficient, and Kmeans Nystr\u00f6m (Kmeans) [44] that empirically perform very well in kernel approximation.", "startOffset": 39, "endOffset": 43}, {"referenceID": 43, "context": "We also include AdapPartial (AdapPart) [28] that approximates AdapFull and is much more efficient, and Kmeans Nystr\u00f6m (Kmeans) [44] that empirically perform very well in kernel approximation.", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "References [1] R.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] O.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] E.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "Theorem 6 (Pemantle and Peres [33]).", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "It is known that a k-Dpp is a homogeneous strongly Rayleigh measure on {0, 1}N [4, 10], thus Theorem 6 applies to results obtained with k-Dpp.", "startOffset": 79, "endOffset": 86}, {"referenceID": 9, "context": "It is known that a k-Dpp is a homogeneous strongly Rayleigh measure on {0, 1}N [4, 10], thus Theorem 6 applies to results obtained with k-Dpp.", "startOffset": 79, "endOffset": 86}], "year": 2016, "abstractText": "The Nystr\u00f6m method has long been popular for scaling up kernel methods. Its theoretical guarantees and empirical performance rely critically on the quality of the landmarks selected. We study landmark selection for Nystr\u00f6m using Determinantal Point Processes (Dpps), discrete probability models that allow tractable generation of diverse samples. We prove that landmarks selected via Dpps guarantee bounds on approximation errors; subsequently, we analyze implications for kernel ridge regression. Contrary to prior reservations due to cubic complexity of Dpp sampling, we show that (under certain conditions) Markov chain Dpp sampling requires only linear time in the size of the data. We present several empirical results that support our theoretical analysis, and demonstrate the superior performance of Dpp-based landmark selection compared with existing approaches.", "creator": "LaTeX with hyperref package"}}}