{"id": "1603.06571", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "Bayesian Neural Word Embedding", "abstract": "recently, since several works in particularly the domain of online natural language processing presented commercially successful methods advocated for algorithms word speech embedding. among examples them, the tandem skip - gram ( sg ) with continuous negative sampling, known also as quantum word2vec, advanced the state - of - the - art of various linguistics tasks. in this paper, but we subsequently propose a scalable bayesian neural mapping word memory embedding program algorithm that \" can allegedly be beneficial somehow to enhance general equilibrium item temporal similarity tasks as well. running the algorithm systematically relies on a variational null bayes solution query for the ultimate sg resolution objective and while a detailed experimental step by step description of the sb algorithm is provided. we present recent experimental robust results that demonstrate the performance risks of evolving the proposed sg algorithm and who show it positively is competitive with the subsequent original optimal sg method.", "histories": [["v1", "Mon, 21 Mar 2016 16:32:06 GMT  (145kb)", "http://arxiv.org/abs/1603.06571v1", null], ["v2", "Sun, 5 Jun 2016 16:49:11 GMT  (156kb)", "http://arxiv.org/abs/1603.06571v2", null], ["v3", "Mon, 20 Feb 2017 20:45:33 GMT  (181kb)", "http://arxiv.org/abs/1603.06571v3", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["oren barkan"], "accepted": true, "id": "1603.06571"}, "pdf": {"name": "1603.06571.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "Recently, several works in the domain of natural language processing presented successful methods for word embedding. Among them, the Skip-gram (SG) with negative sampling, known also as Word2Vec, advanced the state-of-the-art of various linguistics tasks. In this paper, we propose a scalable Bayesian neural word embedding algorithm that can be beneficial to general item similarity tasks as well. The algorithm relies on a Variational Bayes solution for the SG objective and a detailed step by step description of the algorithm is provided. We present experimental results that demonstrate the performance of the proposed algorithm and show it is competitive with the original SG method.\nIndex terms: bayesian word2vec, bayesian skip-gram, neural word embedding, scalable bayesian word embedding, collaborative filtering, bayesian item2vec, item similarity, bayesian representation learning.\n1. Introduction\nRecent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].\nThese methods attempt to learn low dimensional representation for words that captures semantic and syntactic relations between them. Specifically, Skipgram (SG) with negative sampling , known also as Word2Vec [8], set new records in various linguistic tasks and its applications have been extended to other domains beyond NLP such as computer vision [1, 11], Collaborative Filtering (CF) and item similarity [12].\nIn this paper, we propose a scalable Bayesian neural embedding algorithm that in principle, can be applied to any dataset that is given as (sets) sequences of (items) words. Hence, the proposed method is not limited to the task of word embedding and applicable to\nThe proposed method is based on a Variational Bayes (VB) [4] solution to the SG objective that we name Bayesian SG (BSG). Different from SG that maps words to vectors, BSG maps words to densities in a latent space.\nThe VB approach has several additional advantages: First, it provides a stable and robust behavior. Second, it requires less effort in hyperparameter tuning. Third, it provides a measure of confidence in the embedding. Forth, it results in a better modeling for rare words. This is in contrast to point estimate solutions that are more sensitive to singularities and usually require significant amount of hyperparamter tuning.\nUnlike other previous works, we provide a fully detailed step by step description of the proposed algorithm, which is straightforward to implement and requires negligible amount of parameter tuning.\nThe rest of the paper is organized as follows: Section 2 overviews the SG method. In Section 3 we provide mathematical derivation of the BSG solution. In Section 4, we describe the BSG algorithm. In Section 5, we report experimental results.\n2. Related work \u2013 SG\nSG is a neural word embedding method that was introduced by Mikolov et. al in [8]. The method aims at estimating words representation that captures semantic and syntactic relations between a word to its surrounding words in a sentence. The rest of this section provides a brief overview of the SG method.\nGiven a sequence of words 1 ( )L i i w = from a finite\nvocabulary 1 { }l i i W w == , the Skip-gram objective aims at maximizing the following objective:\n1 , 0\n1 log ( | )\nL\ni j i\ni c j c j\np w w L + = \u2212 \u2264 \u2264 \u2260 \u2211 \u2211 (1)\nwhere c is defined as the context window size\nand ( | ) j i p w w is the softmax function:\nexp( )\n( | ) exp( )\nW\nT i j\nj i T T\ni k\nk I\nu v p w w\nu v \u2208\n= \u2211\n(2)\nwhere ( )m i u U\u2208 \u2282 \u211d and ( )m i v V\u2208 \u2282 \u211d are latent vectors that correspond to the target and context\nrepresentations for the word i w W\u2208 , respectively,\n{1,..., } W I l\u225c and the parameter m is chosen\nempirically and according to the size of the dataset.\nUsing Eq. (2) is impractical due to the computational\ncomplexity of ( | ) j i p w w\u2207 , which is a linear function of\nthe vocabulary size l that is usually in size of 5 610 10\u2212 .\n2.1 Negative sampling\nNegative sampling [8] is introduced in order to overcome the above computational problem by the replacement of the softmax function from Eq. (2) with\n1\n( | ) ( ) ( ) N T T\nj i i j i k\nk p w w u v u v\u03c3 \u03c3 = = \u2212\u220f (3)\nwhere ( ) 1/1 exp( )x x\u03c3 = + \u2212 , N is a parameter that\ndetermines the number of negative examples to be\nsampled per a positive example. A negative word k w is sampled from the unigram distribution raised to the 3/4rd power 3/4 ( ) uni p w , where ( ) uni p w is defined as the number of times w appear in the entire corpus divided\nby the total length (in words) of the corpus. This distribution was found to significantly outperform both the uniform and unigram distributions, empirically [2, 8, 9].\nThe latent representations U and V are then\nestimated by applying a stochastic optimization with respect to the objective in Eq. (1).\nIt worth noting that some versions of word embedding algorithms incorporates bias terms into Eq. (3) as follows\n1\n( | ) ( ) ( ) N T T\nj i i j i j i k i k\nk p w w u v b b u v b b\u03c3 \u03c3 = = + + \u2212 \u2212 \u2212\u220f\nThese biases often explain properties such as frequency of a word in the text [2, 7] or popularity of an item in the dataset [10]. In this work, we do not use biases, since in our experiments their contribution was found to be marginal.\n2.2 Data subsampling\nIn order to overcome the imbalance between rare and frequent words the following subsampling procedure is suggested [8]: Given the input words sequence, we discard each word w with a probability\n( | ) 1 ( ) p discard w f w\n\u03c1 = \u2212 where ( )f w is the\nfrequency of the word w and \u03c1 is a prescribed\nthreshold. This procedure is reported to accelerate the learning process and to improve the representation of rare words [8].\n2.3 Words representation and similarity\nSG produces two different representations i u and i v\nfor the word i w . In this work, we use i u as the final\nrepresentation for i w . Other options are to use i v , the\nadditive composition i i u v+ or the concatenation T\nT T\ni i u v  . The last two options are reported to produce\nsuperior representation [3].\nThe similarity between a pair of words , a b w w is computed by applying cosine similarity to their\ncorresponding representations , a b u u as follows\n( , ) T a b\na b\na b\nu u sim w w\nu u =\n3. Bayesian SG - Theory\nAs described in Section 2, SG produces point estimates for U and V . In this section, we propose a method\nfor deriving full distributions for U and V . Inspired\nby a recent CF work [10], we suggest a VB approximation for the true posteriors of U and V .\nWe assume that each target and context random vectors are independent and have normal priors with a zero mean and diagonal covariance with a precision parameter \u03c4 as follows\n: W i I\u2200 \u2208 1( ) (0, ) ii u p u N I\u03c4 \u2212= and 1( ) (0, ) ii v p v N I\u03c4 \u2212= .\nNote that different values of \u03c4 can be set to the priors over U and V . Furthermore, these hyperparameters\ncan be treated as random variables and be learnt from the data [10]. However, in this work we treat them as given hyperparameters that are identical for both U\nand V .\nWe define ( ) i C w as a multiset that contains the\nindices of the context words of i w in the corpus. Let\n{( , ) | ( )}P iI i j j C w= \u2208 and {( , ) | ( )}N iI i j j C w= \u2209 be the\npositive and negative sets, respectively. Note that P I is a\nmultiset as well and N I \u2019s size might be quadratic in the\n| |W . Therefore, we approximate N I by negative sampling as described in Section 2.1.\nLet D P N I I I= \u222a and define { | ( , ) }ij DD d i j I= \u2208 as a\nset, where : {1, 1} D d I \u2192 \u2212 is a random variable\n1 ( , )\n(( , )) 1 ( , )\np\nij\nN\ni j I d d i j\ni j I\n\u2208 = \n\u2212 \u2208 \u225c (4)\nthen, the likelihood of ij d is given by\n( | , ) ( ) T\nij i j ij i j p d u v d u v\u03c3= . Note that when applied to multisets, the operator \u222a is defined as the multiset sum and not as the multiset union.\nThe joint distribution of ,U V and the data D is\ngiven by\n( , )\n1 1\n( , )\n( , , ) ( | , ) ( ) ( )\n( | , ) ( ) ( )\n( ) (0, ) (0, )\np W W\ni i\nD W W\nij i j i i\ni j I i I i I\nT\nij i j u v\ni j I i I i I\np U V D p D U V p U p V\np d u v p u p v\nd u v N I N I\u03c3 \u03c4 \u03c4\n\u2208 \u2208 \u2208\n\u2212 \u2212\n\u2208 \u2208 \u2208\n= =\n=\u220f \u220f \u220f\n\u220f \u220f \u220f\n(5)\n3.1 Variational approximation\nWe aim at computing the posterior ( , | )p U V D .\nHowever, direct computation is hard. The posterior can be approximated using MCMC approaches such as Gibbs sampling or by using VB methods. In this work, we choose to apply VB approximation [4], since it was shown to converge faster to an accurate solution, empirically.\nLet U V\u03b8 = \u222a , VB approximates the posterior ( | )P D\u03b8 by finding a fully factorized distribution\n1\n1 1 1\n( ) ( , ) ( ) ( ) ( ) ( )\n( ) ( )\nl\ni i\ni\nl m m\nik ik\ni k k\nq q U V q U q V q u q v\nq u q v\n\u03b8 =\n= = =\n= = = =\n     \n\u220f\n\u220f \u220f \u220f (6)\nthat minimizes the KL divergence [4]\n( ) ( )\n( ) || ( | ) ( ) log ( | ) KL\nq D q P D q d\np D\n\u03b8 \u03b8 \u03b8 \u03b8 \u03b8\n\u03b8 = \u222b .\nTo this end, we define the following expression:\n( )\n( , ) ( ) ( ) log\n( )\n( | ) ( ) log ( ) log ( )\n( )\n( ) ( | ) log ( ) KL\np D L q q d\nq\np D q d q p D d\nq\nD q p D p D\n\u03b8 \u03b8 \u03b8\n\u03b8\n\u03b8 \u03b8 \u03b8 \u03b8 \u03b8\n\u03b8\n\u03b8 \u03b8\n=\n+ =\n\u2212 +\n\u222b\n\u222b \u222b\n\u225c\n(7)\nwhere the last transition in Eq. (7) is due to the fact q\nis a PDF. By rearranging Eq. (7) we get\n( )( ) ( | ) log ( ) ( )KLD q p D p D L q\u03b8 \u03b8 = \u2212 (8)\nBy examining Eq. (8), we notice that log ( )p D is\nindependent of q and hence, minimizing\n( )( ) ( | )KLD q p D\u03b8 \u03b8 is equivalent to maximizing ( )L q . It was shown [4] that ( )L q is maximized by an iterative\nprocedure that is guaranteed to converge to a local optimum. This is done by updating each of 'q s factors,\nsequentially and according to the following update rule\n( ) \\ * exp [log ( , )]\ni ui u q\nq p D const \u03b8 \u03b8= +E (9)\nwhere the update for *\niv q is performed by the\nreplacement of i u in Eq. (9) with i v .\nRecall that the term ( , ) ( , , )p D p U V D\u03b8 = in Eq. (9)\ncontains the likelihood ( | , )p D U V from Eq. (5), which\nis a product of sigmoid functions of U and V .\nTherefore, a conjugate relation between the likelihood and the priors does not hold and the distribution that is\nimplied by \\ [log ( , )] ui q p D \u03b8 \u03b8E in Eq. (9) does not belong to the exponential family.\nNext, we will show that by the introduction of an\nadditional parameter ij \u03be we are able to bring Eq. (9) to\na form that is recognized as the Gaussian distribution.\nWe start by lower bounding ( | , )p D U V using the\nfollowing logistic bound [13]:\n2 2log ( ) ( )( ) log ( )\n2\na a a \u03be \u03c3 \u03bb \u03be \u03be \u03c3 \u03be \u2212 \u2265 \u2212 \u2212 + (10)\nwhere\n1 1\n( ) ( ) 2 2 \u03bb \u03be \u03c3 \u03be \u03be\n  = \u2212 \n  . (11)\nBy applying the lower bound from Eq. (10) to log ( | )p D \u03b8 we get\n2\n( , )\nlog ( | ) log ( | ) log ( | , )\n( )( ) log ( ) 2\nD\nT\nij i j ij T T\nij i j j i ij ij\ni j I\np D p D p D U V\nd u v u v v u\n\u03be \u03be\u03b8 \u03b8\n\u03be \u03bb \u03be \u03be \u03c3 \u03be\n\u2208\n\u2265 = =\n\u2212 \u2212 \u2212 +\u2211\n(12)\nBy using Eq. (12) we can bound ( )L q as follows\n( , ) ( ) ( ) ( ) log\n( )\n( | ) ( ) ( ) log\n( )\np D L q L q q d\nq\np D p q d\nq\n\u03be\n\u03be\n\u03be\n\u03b8 \u03b8 \u03b8\n\u03b8\n\u03b8 \u03b8 \u03b8 \u03b8\n\u03b8\n\u2265 = =\u222b\n\u222b (13)\nFurthermore, it was shown [13] that the bound in Eq. (9) is tight when\n2 [ ] var( ) [ ] [ ]\nvar( ) i j j i\nT T T T T\nij q i j j i i j q i j q j i\nT T T i j u v v u\nu v v u u v u v v u\nu v\n\u03be\n\u00b5 \u00b5 \u00b5 \u00b5\n= = + =\n+\nE E E (14)\nwhere [ ] jv q j v\u00b5 \u0395\u225c and the last transition in Eq. (14)\nholds since i u and j v are independent. By assuming diagonal covariance matrices, the term var( ) T\ni j u v in Eq.\n(12) can be computed by\n1 1\n2 2 2 2 2 2\n1\nvar( ) var var( )\nik jk ik jk jk ik\nm m T i j ik jk ik jk\nk k\nm\nu v u v v u\nk\nu v u v u v\n\u03c3 \u03c3 \u03c3 \u00b5 \u03c3 \u00b5\n= =\n=\n  = = = \n \n+ +\n\u2211 \u2211\n\u2211 (15)\nFinally, by combining Eqs. (14) and (15) we get\n2 2 2 2\n1\n( )( ) ik ik jk jk\nm\nij u u v v\nk \u03be \u03c3 \u00b5 \u03c3 \u00b5 = = + +\u2211 . (16)\nTherefore, instead of maximizing ( )L q we can\nmaximize ( )L q\u03be and this is done by replacing the term\nlog ( , )p D\u03b8 from Eq. (9) with log ( , )p D\u03be \u03b8 as follows\n( ) \\ * exp [log ( , )]\ni ui u q\nq p D const \u03b8 \u03be \u03b8= +E (17)\nBy applying the natural logarithm to Eq. (17) we get\n\\\n\\ \\\n* log [log ( , )]\n[log ( | , )] [log ( , )]\n1\n2\n1 2 ( ) [ ] 2\ni ui\nu ui i\nj\nui\nui\nu q\nq q\nT i ij v\nj I\nT T i ij q j j i\nj I\nq p D const\np D U V p U V const\nu d\nu v v I u const\n\u03b8\n\u03b8 \u03b8\n\u03be\n\u03be\n\u03b8\n\u00b5\n\u03bb \u03be \u03c4\n\u2208\n\u2208\n= + =\n+ + =\n    \n   \u2212 + +   \n \n\u2211\n\u2211\nE\nE E\n\u0395\n(18)\nwhere { | ( , ) } iu D I j i j I= \u2208 and\n[ ] j j j\nT T\nq j j v v vv v \u00b5 \u00b5= \u2211 +\u0395 . (19)\nNote that in the last transition of Eq. (18) the const\nabsorbed all the terms that are independent of i u .\nBy a careful inspection of Eq. (18) we see that *\niu q is\nnormally distributed with the following natural parameters\n2 ( ) [ ]\n1\n2\ni\nui\ni j\nui\nT\nu ij q j j\nj I\nu ij v\nj I\nP v v I\nr d\n\u03bb \u03be \u03c4\n\u00b5\n\u2208\n\u2208\n = + \n=\n\u2211\n\u2211\n\u0395\n(20)\nwhere 1\ni iu u P\n\u2212= \u2211 is the precision matrix and\ni i iu u u r P \u00b5= is the means times precision vector. Note that the computation of *\niv q \u2019s parameters is symmetric\nand given by\n2 ( ) [ ]\n1\n2\ni\nvi\ni j\nvi\nT\nv ji q j j\nj I\nv ji u\nj I\nP u u I\nr d\n\u03bb \u03be \u03c4\n\u00b5\n\u2208\n\u2208\n = + \n=\n\u2211\n\u2211\n\u0395\n(21)\nMoreover, since the updates for 1{ }i l u iq = depends only on 1{ }i l v iq = and vice versa, they can be performed in parallel. This gives an alternating updates scheme that (under the assumption of a constant dataset) is guaranteed to converge to a local optimum [4]: First, update all 1{ }i l u iq = (in parallel), then update all 1{ }i l v iq = (in parallel) and repeat until convergence.\n3.2 Stochastic updates\nDue to data sampling, the effective dataset changes between the iterations and the optimization becomes stochastic. Since we do not want to ignore the information from previous steps, we need to figure out\na way to consider this information in our updates. A common practice is to apply updates in the spirit of the Robbins-Monro method [14]. This is performed by the introduction of an iteration dependent variable ( )k\u03b2\nthat controls the updates as follows\n( ) ( ) ( ) ( 1)\n( ) ( ) ( ) ( 1)\n(1 )\n(1 )\ni i i\ni i i\nk k k k\nu u u\nk k k k\nu u u\nP P P\nr r r\n\u03b2 \u03b2\n\u03b2 \u03b2\n\u2212\n\u2212\n= + \u2212\n= + \u2212 (22)\nIn practice, this means that during the runtime of the algorithm we need to keep the results from the previous iteration.\nRobbins and Monro showed several conditions for convergence, where one of them states that ( )k\u03b2 needs\nto satisfy:\n( )\n0\nk\nk\n\u03b2 \u221e = = \u221e\u2211 and ( ) 2 0 ( ) k k \u03b2 \u221e = < \u221e\u2211 . (23)\nTo this end, we suggest to use ( )k k \u03b3\u03b2 \u2212= with a decay\nparameter 0.5 1\u03b3< \u2264 as this ensures the conditions in\n(23) hold. We further suggest to set ( ) 1k\u03b2 = for the\nfirst few iterations, in order to avoid too early convergence. Specifically, in our implementation, we did not perform stochastic updates in the first 5 iterations.\n4. Bayesian SG - Algorithm\nIn this section, we provide a detailed description of the BSG algorithm that is based on Sections 2 and 3. The algorithm is described step by step in Fig. 1. The algorithm includes three main stages. The first stage is an initialization, then the algorithm iterates between data sampling and parameter updates till a convergence criterion is met or number of iterations is exceeded.\nNote that the application of BSG to general item similarity tasks is straightforward. The only requirement is that the data is given in the same format. Specifically, every sentence of words in the data is replaced with a sequence of items. Moreover, if the items are given as sets (where order does not matter), the subsampling procedure in step 5.1 should be followed by shuffling of the sequences. This results in a Bayesian version of the Item2Vec method [12].\n4.1 Stage 1 - initialization\nThe algorithm is given with the following hyperparameters: the target representation dimension m , the number of maximum iterations K , the maximal\nwindow size max c , the negative to positive ratio N \u2208\u2115 ,\nthe subsampling parameter \u03c1 , a stopping threshold \u03b5 ,\nthe decay parameter \u03b3 and the prior precision\nparameter \u03c4 . As described in Section 3, different values of \u03c4 can be learnt for U and V , however in our implementation, we chose to use a unique parameter \u03c4 .\nIn this stage, we further need to determine the effective set of words W to learn representations for.\nThis can be done by considering all words in the data that appear more than a prescribed number of times, or\nby considering the l most popular words. In this work, we stick with the latter. Then, every word w W\u2209 is\ndiscarded from the data (step 1).\nStep 2 initializes the target distributions\n1{ , }i i l u v iQ q q == parameters. Specifically, the means are drawn from the multivariate standard normal distribution and the covariance matrices are set to identity.\nIn step 3, we compute uni p according to the description in Section 2.1, then we raise it to the \u00be rd power.\nStep 4 updates k and K according to \u03ba . This ensures that stochastic updates are not performed in the first \u03ba iterations.\n4.2 Stage 2 \u2013 Data sampling\nAt the beginning of every iteration, we first subsample the data (step 5.1) as described in Section 2.2. Then, we follow the description in Section 3: for each\ninstance of the word i w in T , we sample a window size c from the uniform discrete distribution over the\nset max {1,..., }c and consider the c words to the left and\nto the right of i w as context words for i w . This results\nin a multiset ( ) i C w that contains the indices of the\ncontext words of i w (an index may appear multiple times). Then, we create a positive multiset of tuples\n{( , ) | ( )}P iI i j j C w= \u2208 (step 5.3).\nNext, for each tuple ( , ) P i j I\u2208 we sample N\nnegative examples ( , )i z such that ( , ) P i z I\u2209 . A negative\nword z w is sampled according to 3/ 4 ( ) uni z p w . We also\nupdate , , , , i j zu v v ij iz I I I d d accordingly (step 5.4).\nNote that the description of step 5.4 is na\u00efve. In practice, we suggest to save for each tuple a counter for the number of times it appears. This can be done by maintaining dictionary data structures that count positive and negative examples. This further avoids the need of maintaining 1{ , }i i l u v iI I = as multisets (list data structures) and replace them with set data structures.\nBayesian SG (BSG)\nInput: m - target representation dimension\nT - input text, given as sequence of sentences. Each sentence is a sequence of words \u03c4 - prior precision K - maximum number of iteration\nmax c - maximal window size\nl - number of most frequent words to be considered in the vocabulary \u03c1 - subsampling parameter\nN \u2013 negative to positive ratio \u03ba - number of iterations to apply without performing stochastic updates (in the beginning) \u03b5 - stopping threshold \u03b3 - decay parameter\nOutput:\n1{ , , , }i i i i l u v u v iQ \u00b5 \u00b5 == \u2211 \u2211 - parameters of the distributions 1{ , }i i l u v iQ q q ==\n1. Create a set 1 { }l i i W w == of the l most frequent words in T and discard all other words from T . 2. for 1i \u2190 to l 2.1 ~ (0, )\niu N I\u00b5 , ~ (0, ) iv N I\u00b5 , iu P I\u2190 , iv P I\u2190\n3. Compute 3/4 uni p over W using T as described in Section 2.1. 4. 1k \u03ba\u2190 \u2212 , K K \u03ba\u2190 \u2212 // first \u03ba iterations are performed without stochastic updates 5. repeat\n5.1. sub T \u2190 Subsample ( T ), as described in Section 2.2 5.2. for 1i \u2190 to l 5.2.1. ,\ni iu v I I\u03c6 \u03c6\u2190 \u2190\n5.2.2. i i\nprev u uP P\u2190 , i i prev v vP P\u2190 , i i prev u ur r\u2190 , i i prev v vr r\u2190 // save parameter values from the previous iteration\n5.3. Create P I based on sub T as described in Section 4.2 // positive sampling\n5.4. for ( , )i j in P I\n5.4.1. { } i iu u I I j\u2190 \u222a , { } j jv v I I i\u2190 \u222a , 1 ij d \u2190\n5.4.2. for 1n \u2190 to N // negative sampling\n5.4.2.1. Sample a negative word index z according to 3/ 4 ( ) uni z p w s.t. ( , ) p i z I\u2209\n5.4.2.2. { } i iu u I I z\u2190 \u222a , { } z zv v I I i\u2190 \u222a , 1 iz d \u2190 \u2212\n5.5. if 0k > then k \u03b3\u03b2 \u2212\u2190 else 1\u03b2 \u2190 // first \u03ba iterations are performed without stochastic updates\n5.6. parfor 1i \u2190 to l // parallel for loop\n5.6.1. Compute , i iu u P r using Eq. (20).\n5.6.2. (1 ) , (1 ) i i i i i i\nprev prev\nu u u u u uP P P r r r\u03b2 \u03b2 \u03b2 \u03b2\u2190 + \u2212 \u2190 + \u2212\n5.6.3. 1\ni iu u P\n\u2212\u03a3 \u2190\n5.6.4. i i iu u u r\u00b5 \u2190 \u03a3\n5.6.5. [ [ ]] i iu u diag diag\u03a3 \u2190 \u03a3\n5.7. Apply a symmetric version of step 5.6 to 1{ }i l v iq = parameters by using Eq. (21)\nuntil k K> or 2\n1 i i\nl prev\nu u\ni r r \u03b5 = \u2212 <\u2211 and 2 1 i i\nl prev\nv v\ni r r \u03b5 = \u2212 <\u2211\nFigure 1: The Bayesian SG algorithm\n4.3 Stage 3 \u2013 Parameter updates\nIn this stage we update the parameters of the distributions 1{ , }i i l u v iQ q q == . The updates are performed first for 1{ }i l u iq = (step 5.6) and then for 1{ }i l v iq = , in a symmetric manner (step 5.7). Moreover, each sequence of updates is performed in parallel.\nNote that step 5.6.1 involves the computation of ij \u03be ,\n( ) ij \u03bb \u03be and [ ]T q j j v v\u0395 that are given by Eqs. (11), (14)\nand (19), respectively.\nDue to data sampling the dataset is changed per iteration. Therefore, we apply stochastic updates (step 5.6.2). The stochastic updates are performed starting from the iteration 1\u03ba + . This is ensured by step 5.5. A crucial point to notice is the computation of the mean and the covariance: first, we compute the covariance matrix by the inversion of the precision matrix (step 5.6.3). This is performed by using Cholesky decomposition. Then, we extract the mean (step 5.6.4). Finally, we set all the off diagonal values\nin iu \u03a3 to zeros (step 5.6.5) while keeping iu P as is.\nThe algorithm stops if the convergence criterion is\nmet or number of iterations is exceeded (last line).\nIt worth noting that we can \u2018increase\u2019 the stochastic nature of the algorithm by increasing the frequency of the updates. This is done by dividing the data to minibatches and performing parameter updates per minibatch. This is similar to the update scheme of the original SG method.\n4.4 Similarity measures\nBSG maps words to normal distributions. In this work, we choose to use the distributions 1{ }i l u iq = for representing words. The similarity between a pair of\nwords , i j w w can be computed by the cosine similarity\nof their means , i ju u \u00b5 \u00b5 . By using the covariance, a confidence level can be further established. To this end, we define a random variable T\nij i j y u u= . Though the\ndistribution of ij y is not normal, it has the following\nmean and variance\n2 [ ]\nij i j\nij i j i i i j j j\nT\ny u u\nT T\ny u u u u u u u u tr\n\u00b5 \u00b5 \u00b5\n\u03c3 \u00b5 \u00b5 \u00b5 \u00b5\n= = \u03a3 \u03a3 + \u03a3 + \u03a3 . (24)\nHence, we choose to approximate ij y \u2019s distribution with 2\n( , ) ij ij ijy y y N \u00b5 \u03c3 . Then, 2 ijy \u03c3\u2212 can be used as a\nconfidence level of the similarity score.\nBSG further enables the applications of other types\nof similarities that involve the distributions 1{ }i l u iq = . For\nexample, we can approximate ( 1) ij p d = by\napproximating the marginalization\n2\n( 1) ( 1, , )\n( 1 | , ) ( ) ( )\n( ) ( ) ( )\n( ) ( ) 1 / 8\nij\nij\nij ij i j i j\nij i j i j i j\nT i j i j i j\ny\nij ij ij\ny\np d p d u u du du\np d u u p u p u du du\nu u p u p u du du\ny p y dy\n\u03c3\n\u00b5 \u03c3 \u03c3\n\u03c3 \u03c0\n= = = =\n= =\n\u2248\n   \u2248   +  \n\u222b\n\u222b\n\u222b\n\u222b\n(25)\nwhere ijy \u00b5 and 2 ijy \u03c3 are given by Eq. (24) and the last two transitions follow from the approximation in Eq. (24) and [15], respectively. Note that this approach is taken in [10] as well.\nAnother option is to apply a similarity measure that is based on the symmetric version of the KL divergence between two multivariate normal distributions\n( ) ( )( , ) || || i j i j j isymKL u u KL u u KL u u sim q q D q q D q q= \u2212 \u2212 (26)\nwhere ( )|| i jKL u u D q q has the following closed form\nsolution [4]\n( ) {\n}1 1\n1 || log log\n2\n( ) ( )\ni j j i\nj i j j i j i\nKL u u u u\nT T\nu u u u u u u\nD q q\ntr m\u00b5 \u00b5 \u00b5 \u00b5\u2212 \u2212\n= \u03a3 \u2212 \u03a3 +\n \u2212 \u03a3 \u2212 + \u03a3 \u03a3 \u2212 \n5. Experimental Setup and Results\nIn this section, we compare between the BSG and SG algorithms. We evaluate the algorithms on two different tasks: the word similarity task [16] and the word analogy task [2, 8].\nThe word similarity task [16] requires to score pairs of word according to their relatedness. For each pair, a ground truth similarity score is given. The scores are in the range of [0, 10], where 0 stands for totally unrelated words and 10 stands for very much related or identical words.\nThe similarity score we used for both models is the cosine similarity. Specifically, for BSG we observed no significant improvement, when applying the similarities from Eqs. (25) and (26), instead of the cosine similarity.\nIn order to compare between the BSG and SG models, we compute for each model the Spearman [17] and Kendall [18] rank correlation coefficients with respect to the ground truth.\nThe word analogy task [8] is essentially a completion\ntask: a bank of questions in the form of \u2018 a w is to b w as\nc w is to ?\u2019 is given, where the task is to replace ? with\nthe correct word d w . The questions are divided to syntactic questions such as \u2018onion is to onions as lion is to lions\u2019 and semantic questions, e.g. \u2018Berlin is to Germany as London is to England\u2019.\nThe method we use to answer the questions is by\nreporting the word d w that gives the highest cosine\nsimilarity score ?\n?\nT\nd\nd\nu u\nu u where ? b a c u u u u= \u2212 + . For\nthe BSG and SG models we used iu \u00b5 and i u as the\nrepresentation for the word i w , respectively.\n5.1 Datasets\nWe trained both models on the corpus from [5]. In order to accelerate the training process, we limited our vocabulary to the 30K most frequent words in the corpus and discarded all other words. Then, we randomly sampled a subset of 2.8M \u2018sentences\u2019 that results in a total text length of 66M words.\nThe word similarity evaluation dataset [16] contains 353 pairs of words that were scored by 13 \u2013 16 subjects according to their relatedness. For each pair, the final score is determined according to the scores average.\nThe word analogy evaluation dataset [2, 8] consists of 14 distinct groups of analogy questions, where each group contains a different number of questions. Both models were evaluated on the entire dataset that contains 14122 questions.\n5.2 Systems and parameters\nThe evaluated systems are BSG and SG. The same parameters configuration was set for both systems. Specifically, we set the target representation dimension 40m = , maximal window size max 4c = , subsampling\nTable 1 presents the Spearman and Kendall rank correlation coefficients for BSG and SG on the word similarity task [16]. We can see that BSG and SG perform approximately the same.\nTable 2 presents the percentage of correct answers for each model per questions group. We see that the models are competitive where BSG results in a slightly better total accuracy. Examining the results we can further notice that in most cases, BSG provides better results for syntactic analogies, whereas the opposite applies to semantic analogies.\nComparing our results with the literature [2, 8], we see that the accuracies that were obtained by both models are relatively low. This may be due to several reasons: First, we use a much smaller corpus of 66M vs. 6 billions in [2, 8]. Second, the target representation dimension we use is 40 vs. 100 \u2013 600 in [2, 8]. Third, our models were trained for 10 iterations, where the models in [2] ran for 50 \u2013 100 iterations. Therefore, we believe that the performance of our models can be improved significantly by increasing the representation dimension, the number of iterations and the amount of training data.\nIt is important to clarify that in this work, our goal is to show that BSG is an effective method for word embedding and provides competitive results when compared to the original SG method.\n6. Conclusion\nIn this paper, we introduced the BSG algorithm, which is a Bayesian version of the SG method. Specifically, BSG is based on a VB solution to the SG objective. We provide the mathematical derivation of the proposed solution as well as step by step algorithm that is straightforward to implement.\nWe further show that the BSG algorithm is not limited to the domain of word embedding and can be applied to general item similarity tasks as well.\nWe present experimental results on various linguistic\ntasks that show that BSG and SG are competitive.\nReferences\n[1] Frome A, Corrado GS, Shlens J, Bengio S, Dean J, Mikolov T. Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems 2013 (pp. 2121-2129).\n[2] Pennington J, Socher R, Manning CD. Glove: Global Vectors for Word Representation. In EMNLP 2014 Oct (Vol. 14, pp. 1532-1543).\n[3] Garten J, Sagae K, Ustun V, Dehghani M. Combining Distributed Vector Representations for Words. In Proceedings of NAACL-HLT 2015 Jun 5 (pp. 95-101).\n[4] Bishop CM. Pattern Recognition and Machine Learning, 2006.\n[5] Chelba C, Mikolov T, Schuster M, Ge Q, Brants T, Koehn P, Robinson T. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005. 2013 Dec 11.\n[6] Collobert R, Weston J. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning 2008 Jul 5 (pp. 160-167). ACM.\n[7] Mnih A, Hinton GE. A scalable hierarchical distributed language model. InAdvances in neural information processing systems 2009 (pp. 1081-1088).\n[8] Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems 2013 (pp. 3111-3119).\n[9] Mikolov T, Chen K, Corrado G, Dean J. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. 2013 Jan 16.\n[10] Paquet, U., Koenigstein, N. (2013, May). One-class collaborative filtering with random graphs. In Proceedings of the 22nd international conference on World Wide Web (pp. 999-1008).\n[11] Lazaridou A, Pham NT, Baroni M. Combining language and vision with a multimodal skip-gram model. arXiv preprint arXiv:1501.02598. 2015 Jan 12.\n[12] Barkan O. Koenigstein N. Item2Vec: Neural item embedding for collaborative filtering. arXiv: 1603.04259. 2016 Mar 14.\n[13] T. Jaakkola and M. Jordan. A variational approach to Bayesian logistic regression problems and their extensions. In Artificial Intelligence and Statistics, 1996.\n[14] Robbins H, Monro S. A stochastic approximation method. The annals of mathematical statistics. 1951 Sep 1:400-7.\n[15] MacKay DJ. The evidence framework applied to classification networks. Neural computation. 1992 Sep;4(5):720-36.\n[16] Finkelstein L, Gabrilovich E, Matias Y, Rivlin E, Solan Z, Wolfman G, Ruppin E. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web 2001 Apr 1 (pp. 406-414). ACM.\n[17] Spearman C. The proof and measurement of association between two things. The American journal of psychology. 1904 Jan 1;15(1):72-101.\n[18] Kendall MG. A new measure of rank correlation. Biometrika. 1938 Jun 1;30(1/2):81-93."}], "references": [{"title": "Devise: A deep visual-semantic embedding model", "author": ["A Frome", "GS Corrado", "J Shlens", "S Bengio", "J Dean", "T. Mikolov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J Pennington", "R Socher", "CD. Manning"], "venue": "In EMNLP 2014 Oct (Vol", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Combining Distributed Vector Representations for Words", "author": ["J Garten", "K Sagae", "V Ustun", "M. Dehghani"], "venue": "In Proceedings of NAACL-HLT 2015 Jun", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C Chelba", "T Mikolov", "M Schuster", "Q Ge", "T Brants", "P Koehn", "T. Robinson"], "venue": "arXiv preprint arXiv:1312.3005", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R Collobert", "J. Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning 2008 Jul", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "A scalable hierarchical distributed language model. InAdvances in neural information processing systems", "author": ["Mnih A", "Hinton GE"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems", "author": ["T Mikolov", "I Sutskever", "K Chen", "GS Corrado", "J. Dean"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["T Mikolov", "K Chen", "G Corrado", "J. Dean"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "One-class collaborative filtering with random graphs", "author": ["U. Paquet", "Koenigstein", "May"], "venue": "In Proceedings of the 22nd international conference on World Wide Web (pp. 999-1008)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A Lazaridou", "NT Pham", "M. Baroni"], "venue": "arXiv preprint arXiv:1501.02598", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Item2Vec: Neural item embedding for collaborative filtering", "author": ["N. Barkan O. Koenigstein"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "A variational approach to Bayesian logistic regression problems and their extensions", "author": ["T. Jaakkola", "M. Jordan"], "venue": "In Artificial Intelligence and Statistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "A stochastic approximation method", "author": ["H Robbins", "S. Monro"], "venue": "The annals of mathematical statistics", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1951}, {"title": "The evidence framework applied to classification networks", "author": ["MacKay DJ"], "venue": "Neural computation", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Placing search in context: The concept revisited", "author": ["L Finkelstein", "E Gabrilovich", "Y Matias", "E Rivlin", "Z Solan", "G Wolfman", "E. Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web 2001 Apr", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "The proof and measurement of association between two things", "author": ["C. Spearman"], "venue": "The American journal of psychology", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1904}, {"title": "A new measure of rank correlation. Biometrika", "author": ["Kendall MG"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1938}], "referenceMentions": [{"referenceID": 1, "context": "Introduction Recent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].", "startOffset": 142, "endOffset": 157}, {"referenceID": 4, "context": "Introduction Recent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].", "startOffset": 142, "endOffset": 157}, {"referenceID": 5, "context": "Introduction Recent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].", "startOffset": 142, "endOffset": 157}, {"referenceID": 6, "context": "Introduction Recent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].", "startOffset": 142, "endOffset": 157}, {"referenceID": 7, "context": "Introduction Recent progress in neural word embedding methods have advanced the state-of-the-art in the domain of natural language processing [2, 6, 7, 8, 9].", "startOffset": 142, "endOffset": 157}, {"referenceID": 6, "context": "Specifically, Skipgram (SG) with negative sampling , known also as Word2Vec [8], set new records in various linguistic tasks and its applications have been extended to other domains beyond NLP such as computer vision [1, 11], Collaborative Filtering (CF) and item similarity [12].", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "Specifically, Skipgram (SG) with negative sampling , known also as Word2Vec [8], set new records in various linguistic tasks and its applications have been extended to other domains beyond NLP such as computer vision [1, 11], Collaborative Filtering (CF) and item similarity [12].", "startOffset": 217, "endOffset": 224}, {"referenceID": 9, "context": "Specifically, Skipgram (SG) with negative sampling , known also as Word2Vec [8], set new records in various linguistic tasks and its applications have been extended to other domains beyond NLP such as computer vision [1, 11], Collaborative Filtering (CF) and item similarity [12].", "startOffset": 217, "endOffset": 224}, {"referenceID": 10, "context": "Specifically, Skipgram (SG) with negative sampling , known also as Word2Vec [8], set new records in various linguistic tasks and its applications have been extended to other domains beyond NLP such as computer vision [1, 11], Collaborative Filtering (CF) and item similarity [12].", "startOffset": 275, "endOffset": 279}, {"referenceID": 6, "context": "al in [8].", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "1 Negative sampling Negative sampling [8] is introduced in order to overcome the above computational problem by the replacement of the softmax function from Eq.", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "This distribution was found to significantly outperform both the uniform and unigram distributions, empirically [2, 8, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 6, "context": "This distribution was found to significantly outperform both the uniform and unigram distributions, empirically [2, 8, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 7, "context": "This distribution was found to significantly outperform both the uniform and unigram distributions, empirically [2, 8, 9].", "startOffset": 112, "endOffset": 121}, {"referenceID": 1, "context": "These biases often explain properties such as frequency of a word in the text [2, 7] or popularity of an item in the dataset [10].", "startOffset": 78, "endOffset": 84}, {"referenceID": 5, "context": "These biases often explain properties such as frequency of a word in the text [2, 7] or popularity of an item in the dataset [10].", "startOffset": 78, "endOffset": 84}, {"referenceID": 8, "context": "These biases often explain properties such as frequency of a word in the text [2, 7] or popularity of an item in the dataset [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 6, "context": "2 Data subsampling In order to overcome the imbalance between rare and frequent words the following subsampling procedure is suggested [8]: Given the input words sequence, we discard each word w with a probability", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "This procedure is reported to accelerate the learning process and to improve the representation of rare words [8].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "The last two options are reported to produce superior representation [3].", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "Inspired by a recent CF work [10], we suggest a VB approximation for the true posteriors of U and V .", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Furthermore, these hyperparameters can be treated as random variables and be learnt from the data [10].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "We start by lower bounding ( | , ) p D U V using the following logistic bound [13]:", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "Furthermore, it was shown [13] that the bound in Eq.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "A common practice is to apply updates in the spirit of the Robbins-Monro method [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "This results in a Bayesian version of the Item2Vec method [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 13, "context": "(24) and [15], respectively.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "Note that this approach is taken in [10] as well.", "startOffset": 36, "endOffset": 40}, {"referenceID": 14, "context": "We evaluate the algorithms on two different tasks: the word similarity task [16] and the word analogy task [2, 8].", "startOffset": 76, "endOffset": 80}, {"referenceID": 1, "context": "We evaluate the algorithms on two different tasks: the word similarity task [16] and the word analogy task [2, 8].", "startOffset": 107, "endOffset": 113}, {"referenceID": 6, "context": "We evaluate the algorithms on two different tasks: the word similarity task [16] and the word analogy task [2, 8].", "startOffset": 107, "endOffset": 113}, {"referenceID": 14, "context": "The word similarity task [16] requires to score pairs of word according to their relatedness.", "startOffset": 25, "endOffset": 29}, {"referenceID": 8, "context": "The scores are in the range of [0, 10], where 0 stands for totally unrelated words and 10 stands for very much related or identical words.", "startOffset": 31, "endOffset": 38}, {"referenceID": 15, "context": "8 In order to compare between the BSG and SG models, we compute for each model the Spearman [17] and Kendall [18] rank correlation coefficients with respect to the ground truth.", "startOffset": 92, "endOffset": 96}, {"referenceID": 16, "context": "8 In order to compare between the BSG and SG models, we compute for each model the Spearman [17] and Kendall [18] rank correlation coefficients with respect to the ground truth.", "startOffset": 109, "endOffset": 113}, {"referenceID": 6, "context": "The word analogy task [8] is essentially a completion task: a bank of questions in the form of \u2018 a w is to b w as", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "1 Datasets We trained both models on the corpus from [5].", "startOffset": 53, "endOffset": 56}, {"referenceID": 14, "context": "The word similarity evaluation dataset [16] contains 353 pairs of words that were scored by 13 \u2013 16 subjects according to their relatedness.", "startOffset": 39, "endOffset": 43}, {"referenceID": 1, "context": "The word analogy evaluation dataset [2, 8] consists of 14 distinct groups of analogy questions, where each group contains a different number of questions.", "startOffset": 36, "endOffset": 42}, {"referenceID": 6, "context": "The word analogy evaluation dataset [2, 8] consists of 14 distinct groups of analogy questions, where each group contains a different number of questions.", "startOffset": 36, "endOffset": 42}, {"referenceID": 14, "context": "3 Results Table 1 presents the Spearman and Kendall rank correlation coefficients for BSG and SG on the word similarity task [16].", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "Comparing our results with the literature [2, 8], we see that the accuracies that were obtained by both models are relatively low.", "startOffset": 42, "endOffset": 48}, {"referenceID": 6, "context": "Comparing our results with the literature [2, 8], we see that the accuracies that were obtained by both models are relatively low.", "startOffset": 42, "endOffset": 48}, {"referenceID": 1, "context": "6 billions in [2, 8].", "startOffset": 14, "endOffset": 20}, {"referenceID": 6, "context": "6 billions in [2, 8].", "startOffset": 14, "endOffset": 20}, {"referenceID": 1, "context": "100 \u2013 600 in [2, 8].", "startOffset": 13, "endOffset": 19}, {"referenceID": 6, "context": "100 \u2013 600 in [2, 8].", "startOffset": 13, "endOffset": 19}, {"referenceID": 1, "context": "Third, our models were trained for 10 iterations, where the models in [2] ran for 50 \u2013 100 iterations.", "startOffset": 70, "endOffset": 73}, {"referenceID": 14, "context": "TABLE 1 A COMPARISON BETWEEN BSG AND SG ON THE WORD SIMILARITY TASK [16]", "startOffset": 68, "endOffset": 72}, {"referenceID": 6, "context": "405 TABLE 2 A COMPARISON BETWEEN BSG AND SG ON THE WORD ANALOGY TASK [8]", "startOffset": 69, "endOffset": 72}], "year": 2016, "abstractText": "Recently, several works in the domain of natural language processing presented successful methods for word embedding. Among them, the Skip-gram (SG) with negative sampling, known also as Word2Vec, advanced the state-of-the-art of various linguistics tasks. In this paper, we propose a scalable Bayesian neural word embedding algorithm that can be beneficial to general item similarity tasks as well. The algorithm relies on a Variational Bayes solution for the SG objective and a detailed step by step description of the algorithm is provided. We present experimental results that demonstrate the performance of the proposed algorithm and show it is competitive with the original SG method.", "creator": "PScript5.dll Version 5.2.2"}}}