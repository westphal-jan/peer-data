{"id": "1603.07771", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2016", "title": "Neural Text Generation from Structured Data with Application to the Biography Domain", "abstract": "this paper vastly introduces quickly a neural model for constrained concept - to - text generation that closely scales to two large, rich domains. broadly we typically experiment with a startling new dataset of biographies vocabulary from wikipedia that is considering an order of magni - tude resources larger than existing learning resources with averages over 700k equivalent samples. however the dataset is also vastly different more diverse with approximately a 400k wikipedia vocab - ulary, compared to a distant few light hundred spare words for nasa weathergov or robocup. although our model builds upon current recent work on conditional tagged neural composite language mapping model mapping for text genera - tion. eager to deal with the large composite vocabulary, simply we extend these knowledge models to manually mix a visually fixed static vocabulary with copy search actions simultaneously that distinguish trans - target fer sample - specific words from moving the in - cache put database to the separately generated output termed sen - value tence. our standard neural model significantly out - performs a classical kneser - ney neural language model just adapted to create this constrained task induced by living nearly at 15 bleu.", "histories": [["v1", "Thu, 24 Mar 2016 22:40:00 GMT  (282kb,D)", "http://arxiv.org/abs/1603.07771v1", null], ["v2", "Thu, 22 Sep 2016 14:47:44 GMT  (341kb,D)", "http://arxiv.org/abs/1603.07771v2", "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016"], ["v3", "Fri, 23 Sep 2016 15:16:46 GMT  (341kb,D)", "http://arxiv.org/abs/1603.07771v3", "Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["r\u00e9mi lebret", "david grangier", "michael auli"], "accepted": true, "id": "1603.07771"}, "pdf": {"name": "1603.07771.pdf", "metadata": {"source": "CRF", "title": "Generating Text from Structured Data with Application to the Biography Domain", "authors": ["Remi Lebret", "David Grangier", "Michael Auli"], "emails": ["remi@lebret.ch", "grangier@fb.com", "michaelauli@fb.com"], "sections": [{"heading": null, "text": "1 Introduction\nConcept-to-text generation addresses the problem of rendering structured records into natural language (Reiter et al., 2000). A typical application is to generate a weather forecast based on a set of structured records of meteorological measurements. In contrast to previous work, we scale to the large and very diverse problem of generating biographies for personalities based on Wikipedia infoboxes. An infobox is a fact table describing a person, similar to a person subgraph in a knowledge base (Bollacker et al., 2008; Ferrucci, 2012). Similar generation applications include the generation of product descriptions based on a catalog of millions of items with dozens of attributes each.\nPrevious work experimented with datasets that\ncontain only a few tens of thousands of records such as Weathergov or the Robocup dataset, while our dataset contains over 700k biographies from Wikipedia. Furthermore, these datasets have a limited vocabulary of only about 350 words each, compared to over 400k words in our dataset.\nTo tackle this problem we introduce a statistical generation model conditioned on a Wikipedia infobox. We focus on the generation of the first sentence of a biography which requires the model to select among a large number of possible fields to generate an adequate output. Such diversity makes it difficult for classical count-based models to estimate probabilities of rare events due to data sparsity. We address this issue by parameterizing words and fields as vectors (embeddings), along with a neural language model operating on these embeddings (Bengio et al., 2003). This factorization allows us to scale to a large number of words and fields compared to Liang et al. (2009) and Kim and Mooney (2010) where the number of parameters grows as the product of the number of words and fields.\nMoreover, our approach does not restrict the relations between the field contents and the generated text. This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014).\nOur model exploits structured data both globally and locally. Global conditioning summarizes all information about a personality to understand high-level themes such as that the biography is about a scientist or an artist, while as local conditioning describes the previously generated tokens in terms of the their relationship to the infobox. We analyze the effectiveness of each and demonstrate their complementarity.\nar X\niv :1\n60 3.\n07 77\n1v 1\n[ cs\n.C L\n] 2\n4 M\nar 2\n01 6"}, {"heading": "2 Related Work", "text": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010). Generation is divided into modular, yet highly interdependent, decisions: (1) content planning defines which parts of the input fields or meaning representations should be selected; (2) sentence planning determines which selected fields are to be dealt with in each output sentence; and (3) surface realization generates those sentences.\nData-driven approaches have been proposed to automatically learn the individual modules. One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005). Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009). Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013).\nOur approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015).\nOur model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the Weathergov and Robocup tasks. Their architecture relies on LSTM units and an attention mechanism which reduces scalability compared to our simpler design."}, {"heading": "3 Language Modeling for Constrained Sentence generation", "text": "Conditional language models are a popular choice to generate sentences. We introduce a tableconditioned language model for constraining the sentence generation to include elements from fact tables."}, {"heading": "3.1 Language Model", "text": "Given a sentence s = w1, . . . , wT composed of T words from a vocabularyW , a language model estimates:\nP (s) =\nT\u220f\nt=1\nP (wt|w1, . . . , wt\u22121) . (1)\nLet ct = wt\u2212(n\u22121), . . . , wt\u22121 be the sequence of n\u2212 1 context words preceding wt \u2208 s. In an ngram language model, Equation 1 is approximated as\nP (s) \u2248 T\u220f\nt=1\nP (wt|ct) , (2)\nassuming an order n Markov property."}, {"heading": "3.2 Language Model Conditioned on Tables", "text": "As seen in Figure 1, a table consists of a set of field/value pairs, where values are sequences of\nwords. We therefore propose language models that are conditioned on these pairs.\nLocal conditioning. The table allows us to describe each word not only by its string (or index in the vocabulary) but also by a descriptor of its occurrence in the table. Let F define the set of all possible fields f . The occurrence of a word w in the table is described by a set of (field, position) pairs.\nzw = { (fi, pi) }m i=1 , (3)\nwhere m is the number of occurrences of w. Each pair (f, p) indicates that w occurs in field f at position p. In this scheme, most words are described by the empty set as they do not occur in the table. For example, the word linguistics in the table of Figure 1 is described as follows:\nzlinguistics = {(fields, 8); (known for, 4)}, (4)\nassuming words are lower-cased and commas are treated as separate tokens.\nConditioning both on the field type and the position within the field allows the model to encode field-specific regularities, e.g., a number token in a date field is likely followed by a month token; knowing that the number is the first token in the date field makes this even more likely.\nThe (field, position) description scheme of the table does not allow to express that a token terminates a field which can be useful to capture field transitions. For biographies, the last token of the name field is often followed by an introduction of the birth date like \u2018(\u2019 or \u2018was born\u2019. We hence extend our descriptor to a triplet that includes the position of the token counted from the end of the field:\nzw = { (fi, p + i , p \u2212 i ) }m i=1 , (5)\nwhere our example becomes:\nzlinguistics = {(fields, 8, 4); (known for, 4, 13)}.\nWe extend Equation 2 to use the above information as additional conditioning context when generating a sentence s:\nP (s|z) = T\u220f\nt=1\nP (wt|ct, zct) , (6)\nwhere zct = zwt\u2212(n\u22121) , . . . , zwt\u22121 are referred to as the local conditioning variables since they describe the local context (previous word) relations with the table.\nGlobal conditioning The set of fields available in a table often impacts the structure of the generation. For biographies, the fields used to describe a politician are different from the ones for an actor or an athlete. Knowing which fields are available in the table provides type information and helps to determine which fields should be mentioned, both of which greatly influence sentence structure. We introduce global conditioning on the fields gf as\nP (s|z, gf ) = T\u220f\nt=1\nP (wt|ct, zct , gf ). (7)\nSimilarly, global conditioning gw on the words occurring in the table is introduced:\nP (s|z, gf , gw) = T\u220f\nt=1\nP (wt|ct, zct , gf , gw). (8)\nWords provide information complementary to fields. For example, it may be hard to distinguish a basketball player from a hockey player by looking only at the field names, e.g. teams, league, position, weight and height, etc. However the actual field values such as team names, league name, player\u2019s position can help the model to give a better prediction. Here, gf \u2208 {0, 1}F and gw \u2208 {0, 1}W are binary indicators over fixed field and word vocabularies."}, {"heading": "3.3 Copy actions", "text": "So far we extended the model conditioning by features derived from the fact table. We now turn to using table information when scoring output words. In particular, knowing that a word appears in the table is valuable when considering to generate this word as output. Moreover, sentences which express facts from a given table often copy words from the table. We therefore extend our language model to predict special field tokens such as name 1 or name 2 which are subsequently replaced by the corresponding words from the table; we only include field tokens if the field token is actually in the table.\nOur model reads a table and defines an output domain W \u222a Q of vocabulary words W as well as all tokens in the table Q. This allows the generation of words which are not in the vocabulary. For instance Park-Rhodes in Figure 1 is unlikely to be part of W . However, Park-Rhodes will be included inQ as name 2 (since it is the second token\nof the name field) which allows our model to generate it. Often, the output space of each decision Q\u222aW is larger thanW ."}, {"heading": "4 A Neural Language Model Approach", "text": "A feed-forward neural language model (NLM) estimates P (wt|ct) in Equation 1 with a parametric function \u03c6\u03b8. This function is a composition of simple differentiable functions or layers, where \u03b8 refers to the learnable parameters of the network. Given a context input ct, it outputs a score for each next word wt \u2208 W , \u03c6\u03b8(ct) \u2208 R|W|. The probability distribution is then obtained by applying the softmax activation function:\nP (wt = w|ct) = exp(\u03c6\u03b8(ct, w))\u2211|W| i=1 exp(\u03c6\u03b8(ct, wi))\n(9)"}, {"heading": "4.1 Embeddings as inputs", "text": "A key aspect of neural language models is the use of word embeddings as inputs. Similar words have generally similar embeddings since they share latent features. Because the probability estimates are smooth functions of the continuous word embeddings, a small change in the features results in a small change in the probability estimates (Bengio et al., 2003). Therefore, the neural language model can achieve better generalization for unseen n-grams. Just as the discrete feature representations of words are mapped into continuous word embeddings, the discrete feature representations of tables can be mapped into continuous vector space.\nWord embeddings. Formally, the input layer maps each context word index to a continuous ddimensional vector. It relies on a parameter matrix W \u2208 R|W|\u00d7d to convert the input ct into n\u22121 vectors of dimension d:\nct = [ Wt\u2212(n\u22121); . . . ;Wt\u22121 ] \u2208 R(n\u22121)\u00d7d (10)\nW can be initialized randomly or with pre-trained word embeddings.\nTable embeddings. As described in Section 3.2, the language model is conditioned on elements from the table. Embedding matrices are therefore defined to model both local and global conditioning information. For local conditioning, we denote the maximum length of a sequence of words as l. Each field fj \u2208 F is associated with 2\u00d7 l vectors of d dimensions, the first l of those vectors embed\nall possible starting positions 1, . . . , l, and the remaining l vectors embed ending positions. This results in a parameter matrix Z \u2208 R|F|\u00d7(2\u00d7l)\u00d7d. For a given triplet (fj , p+i , p \u2212 i ), Zj,p+i and Zj,p\u2212i refer to the embedding vectors of the start and end position for field fj , respectively.\nFinally, global conditioning uses two parameter matrices Gf \u2208 R|F|\u00d7g and Gw \u2208 R|W|\u00d7g. Each row Gfj maps a table field fj into a vector of dimension g, while each row Gwt maps a word wt into a vector of the same dimension. In general, Gw shares its parameters withW , provided d = g.\nAggregating embeddings. We represent each occurence of a wordw as a triplet (field, start, end) where we have embeddings for the start and end position as described above. Often times a particular word w occurs multiple times in a table, e.g., \u2018linguistics\u2019 has two instances in Figure 1. In this case, we perform a component-wise max over the start embeddings of all instances of w to obtain the best features across all occurrences of w. We do the same for end position embeddings:\nzwt =\n[ max{Zj,p+i ,\u2200(fj , p + i , p \u2212 i ) \u2208 zwt};\nmax{Zj,p\u2212i ,\u2200(fj , p + i , p \u2212 i ) \u2208 zwt} ] (11)\nNote that a special no-field embedding is assigned to wt when the word is not associated with any fields.\nFor global conditioning, we define Fq \u2282 F as the set of all the fields in a given table q, and Q as the set of all words in q. We also perform max aggregation. This yields the vectors\ngf = max{Gf , \u2200f \u2208 Fq} , (12)\nand gw = max{Gw, \u2200w \u2208 Q} . (13)\nThe final context input is then the concatenation of these vectors:\nxct = [ ct; zct ; gf ; gw ] \u2208 Rd1 , (14)\nwith d1 = (n\u2212 1)\u00d7 (3\u00d7 d) + (2\u00d7 g). This input is mapped to a latent context representation using a linear operation followed by a hyperbolic tangent.\nhct = tanh(W 1xct + b 1)."}, {"heading": "4.2 In-vocabulary outputs", "text": "The representation of the context hct is then multiplied by a matrix with one row per word, this produces a real value score for each word in the vocabulary,\n\u03c6Wct =W outhct + b out \u2208 R|W| , (15) where W 1 \u2208 Rnhu\u00d7d1 , W out \u2208 R|W|\u00d7nhu, b1 \u2208 Rnhu, and bout \u2208 R|W| are learnable weights and biases and tanh denotes the component-wise hyperbolic tangent."}, {"heading": "4.3 Mixing outputs for better copying", "text": "Section 3.3 explains that each word w is also associated with zw, the set of fields in which it occurs, along with the position in that field. Similar to local conditioning, we represent each field and position pair (j, i) with an embedding Fj,i. These embeddings are then projected into the same space as the latent representation of context input hct . Using the max operation over the embedding dimension, each word is finally embedded into a unique vector:\nqw = max{tanh(W 2Fj,i+b2), \u2200Fj,i \u2208 zw} (16)\nwhere W 2 \u2208 Rnhu\u00d7d, and b2 \u2208 Rnhu are learnable weights and biases, and qw \u2208 Rnhu. A dot product with the context vector hct produces a real value score for each word w in the table,\n\u03c6Qct(w) = hct \u00b7 qwt . (17) Each word w \u2208 W\u222aQ receives a final score by summing the vocabulary score and the field score:\n\u03c6ct(w) = \u03c6 W ct (w) + \u03c6 Q ct(w) , (18)\nwhere \u03c6Qct(wt) = 0 when wt /\u2208 Q. The softmax function then maps the scores to a distribution over W \u222aQ, logP (w|ct) = \u03c6ct(w)\u2212log \u2211\nw\u2032\u2208W\u222aQ exp\u03c6ct(w\n\u2032) ."}, {"heading": "4.4 Training", "text": "The neural language model is trained to minimize the negative log-likelihood of a training sentence s with stochastic gradient descent (SGD; LeCun et al. 2012) :\nL\u03b8(s) = \u2212 T\u2211\nt=1\nlogP (wt|ct, zct , gf , gw) , (19)\nwith \u03b8 = { W ; Z; Gf ; Gw; F ; W 1; b1; W 2; b2; W out; bout } ."}, {"heading": "5 Experiments", "text": "Our neural network model (Section 4) is designed to generate sentences from tables for large-scale problems, where a diverse set of sentence types need to be generated. Biographies are therefore a good framework to evaluate our model, with Wikipedia offering a large and diverse dataset."}, {"heading": "5.1 Biography dataset", "text": "The corpus consists of 728,321 biography articles extracted from English Wikipedia (dump of September 2015). These biographies have been detected using \u201cWikiProject Biography\u201d.1 For each biography article, only the introduction section and the infobox are kept; we retain only articles for which an infobox exist. The resulting dataset has a vocabulary of 403k words. Introductions are split into sentences and tokenized with the Stanford CoreNLP toolkit (Manning et al., 2014). All numbers are mapped to a special token \u20180\u2019, except for years which are mapped to another special token \u2018XXXX\u2019. Infobox values have also been tokenized, templates for birth dates and death dates have been formatted in natural language.2 All tokens in introductions and infoboxes have been lower-cased. The final corpus has been divided into three sub-parts to provide training (80%), validation (10%) and test sets (10%). We will release this data with the cameraready."}, {"heading": "5.2 Baseline", "text": "Our baseline is an interpolated Kneser-Ney (KN) language model and we use the KenLM toolkit to train 5-gram models without pruning (Heafield et al., 2013). We equip the baseline with copy actions of words from tables to sentences (Section 3.3) by pre-processing words occurring in both as follows: Each copied word w is replaced by a special token reflecting its table descriptor zw (Equation 3). The introduction section of the table in Figure 1 may look as follows under this scheme: \u201cname 1 name 2 ( birthdate 1 birthdate 2 birthdate 3 \u2013 deathdate 1 deathdate 2 deathdate 3 ) was an english linguist , fields 3 pathologist , fields 10 scientist , mathematician , mystic and\n1See https://en.wikipedia.org/wiki/ Wikipedia:WikiProject_Biography\n2See https://en.wikipedia.org/wiki/ Template:Birth_date\nmycologist .\u201d At decoding time, we copy words from the tables when those special tokens are emitted."}, {"heading": "5.3 Training setup", "text": "For our neural models, we train 11-gram language models (n = 11) with a learning rate set to 0.0025. Table 2 describes the other hyper-parameters. We include all fields occurring at least 100 times in the training data in F , the set of fields. We include the 20, 000 most frequent words in the vocabulary. The other hyperparameters are set through validation, maximizing BLEU over a validation subset of 1, 000 sentences. Similarly, early stopping is applied: training ends when BLEU stops improving on the same validation subset. One should note that the maximum number of tokens in a field l = 10 means that we encode only 10 positions: for longer field values the final tokens are not dropped but their position is capped to 10. We initialize the word embeddings W from Hellinger PCA computed over the set of training\nbiographies. This representation has shown to be helpful for various applications (Lebret and Collobert, 2014)."}, {"heading": "5.4 Evaluation metrics", "text": "We use two different metrics to evaluate our models. Performance is first evaluated in terms of perplexity which is the standard metric for language modeling. We also measure the quality of the generated sentences with BLEU (Papineni et al., 2002) and report the mean and standard deviation for five models initialized with different seeds."}, {"heading": "6 Results", "text": "This section describes our results and discusses the impact of the different conditioning variables."}, {"heading": "6.1 The more, the better", "text": "The results (Table 1) show that more conditioning information helps to improve the performance of our models. We first discuss models without copy actions (the first three results) and then discuss models with copy actions (the remaining results). Note that the factorization of our models results in three different output domains which makes perplexity comparisons less straightforward: First, models without copy actions use a fixed output vocabulary of size |W|. Second, the Table KN model (marked by ?) uses a fixed vocabulary of approximate size |F|\u2217l+|W|. Third, all other Table NLM models (marked by \u2020) have the usual fixed output vocabulary plus a variable set Q which changes with each input table as described in Section 3.3.\nWithout copy actions. In terms of perplexity the (i) neural language model (NLM) is slightly better than an interpolated KN language model, and (ii) adding local conditioning on the field start and end position further improves performance. BLEU over the model generations is generally very low but there is a clear improvement when using local conditioning because it allows learning transitions between fields and links past model predictions to the table unlike KN or plain NLM.\nWith copy actions. For experiments with copy actions we use the full local conditioning (Equation 4) in the neural language models. Perplexity can only be compared between variants of Table NLM models as described above and improvements are less clear when adding features in terms of this measure. However, BLEU clearly improves when moving from Table KN to Table NLM and more features successively improve accuracy. Global conditioning on the fields improves the model by over 7 BLEU and adding words gives 1.3 points. This is a total improvement of nearly 15 BLEU over the Table Kneser-Ney baseline."}, {"heading": "6.2 Attention mechanism", "text": "Our model implements attention over input table fields. For each word w in the table, Equation (18)\nna m\ne\nbi rth\nda te\nbi rth\npl ac\ne\nna tio\nna lit\ny\noc cu\npa tio\nn\n1 2 1 2 3 1 2 1 1 2\n< s >\nnellie\nwong\n(\nborn\nseptember\n12\n,\n1934\n)\nis\nan\namerican\npoet\nand\nactivist\n.\nFigure 3: Visualization of attention scores for Nellie Wong\u2019s Wikipedia infobox. Rows represent the probability distribution over (field, position) pairs from the table after generating each word. The columns represent the conditioning context, e.g., the model takes n\u22121 words as context. The darker the color, the higher the probability.\ntakes the language model score \u03c6Wct and adds a bias \u03c6Qct . The bias is the dot-product between a representation of the table field in which w occurs and a representation of the context, Equation (17) that summarizes the previously generated fields and words.\nFigure 3 shows that this mechanism adds a large bias to continue a field if it has not generated all tokens from the table, e.g., it emits the word occurring in name 2 after generating name 1. It also nicely handles transitions between field types, e.g., the model adds a large bias to the words occurring in the occupation field after emitting the birth date."}, {"heading": "6.3 Sentence decoding", "text": "We use a standard beam search to explore a larger set of sentences compared to simple greedy search. This allows us to explore K times more paths which comes at a linear increase in the number of forward computation steps for our language model. We compare various beam settings for\nthe baseline Table KN and our Table NLM (Figure 2). The best validation BLEU can be obtained with a beam size of K = 5. Our model is also several times faster than the baseline, requiring only about 200 ms per sentence with K = 5. Beam search generates many n-gram lookups for Kneser-Ney which requires many random memory accesses; while neural models perform scoring through matrix-matrix products, an operation which is more local and can be performed in a block parallel manner where modern graphic processors shine (Kindratenko, 2014)."}, {"heading": "6.4 Qualitative analysis", "text": "Table 3 shows generations for different variants of our model based on the Wikipedia table in Figure 1. First of all, comparing the reference to the fact table reveals that our training data is not perfect. The birth month mentioned in the fact table and the first sentence of the Wikipedia article are different; this may have been introduced by one contributor editing the article and not keeping the information consistent.\nAll three versions of our model correctly generate the beginning of the sentence by copying the name, the birth date and the death date from the table. The model correctly uses the past tense since the death date in the table indicates that the person has passed away. Frederick Parker-Rhodes was a scientist, but this occupation is not directly mentioned in the table. The model without global\nconditioning can therefore not predict the right occupation, and it continues the generation with the most common occupation (in Wikipedia) for a person who has died. In contrast, the global conditioning over the fields helps the model to understand that this person was indeed a scientist. However, it is only with the global conditioning on the words that the model can infer the correct occupation, i.e., computer scientist."}, {"heading": "7 Conclusions", "text": "We have shown that our model can generate fluent descriptions of arbitrary people based on structured data. Local and global conditioning improves our model by a large margin and we outperform a Kneser-Ney language model by nearly 15 BLEU. Our task uses an order of magnitude more data than previous work and has a vocabulary that is three orders of magnitude larger.\nIn this paper, we have only focused on generating the first sentence and we will tackle the generation of longer biographies in future work. Furthermore, the current training loss function does not explicitly penalize the model from generating incorrect facts, e.g. predicting the wrong nationality or a wrong occupation is not currently considered worse than choosing the wrong determiner. A loss function that could assess factual accuracy would certainly improve sentence generation by avoiding such mistakes."}], "references": [{"title": "A simple domain-independent probabilistic approach to generation", "author": ["References G. Angeli", "P. Liang", "D. Klein."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Angeli et al\\.,? 2010", "shortCiteRegEx": "Angeli et al\\.", "year": 2010}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "International Conference on Learning Representations.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Collective content selection for concept-to-text generation", "author": ["R. Barzilay", "M. Lapata."], "venue": "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 331\u2013338. Association for", "citeRegEx": "Barzilay and Lapata.,? 2005", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2005}, {"title": "Aggregation via set partitioning for natural language generation", "author": ["R. Barzilay", "M. Lapata."], "venue": "Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational", "citeRegEx": "Barzilay and Lapata.,? 2006", "shortCiteRegEx": "Barzilay and Lapata.", "year": 2006}, {"title": "Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models", "author": ["A. Belz."], "venue": "Natural Language Engineering, 14(04):431\u2013455.", "citeRegEx": "Belz.,? 2008", "shortCiteRegEx": "Belz.", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "International Conference on Management of Data, pages 1247\u20131250. ACM.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Coral: Using natural language generation for navigational assistance", "author": ["R. Dale", "S. Geldof", "J.-P. Prost."], "venue": "Proceedings of the 26th Australasian computer science conference-Volume 16, pages 35\u201344. Australian Computer Society, Inc.", "citeRegEx": "Dale et al\\.,? 2003", "shortCiteRegEx": "Dale et al\\.", "year": 2003}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, vol-", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Content planner construction via evolutionary algorithms and a corpus-based fitness function", "author": ["P.A. Duboue", "K.R. McKeown."], "venue": "Proceedings of INLG 2002, pages 89\u201396.", "citeRegEx": "Duboue and McKeown.,? 2002", "shortCiteRegEx": "Duboue and McKeown.", "year": 2002}, {"title": "From captions to visual concepts and back", "author": ["Platt", "L.C. Zitnick", "G. Zweig."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.", "citeRegEx": "Platt et al\\.,? 2015", "shortCiteRegEx": "Platt et al\\.", "year": 2015}, {"title": "Introduction to this is watson", "author": ["David A Ferrucci."], "venue": "IBM Journal of Research and Development, 56(3.4):1\u20131.", "citeRegEx": "Ferrucci.,? 2012", "shortCiteRegEx": "Ferrucci.", "year": 2012}, {"title": "Generating multilingual descriptions from linguistically annotated owl ontologies: the naturalowl system", "author": ["D. Galanis", "I. Androutsopoulos."], "venue": "Proceedings of the Eleventh European Workshop on Natural Language Generation, pages 143\u2013146. As-", "citeRegEx": "Galanis and Androutsopoulos.,? 2007", "shortCiteRegEx": "Galanis and Androutsopoulos.", "year": 2007}, {"title": "Generation of biomedical arguments for lay readers", "author": ["N. Green."], "venue": "Proceedings of the Fourth International Natural Language Generation Conference, pages 114\u2013121. Association for Computational Linguistics.", "citeRegEx": "Green.,? 2006", "shortCiteRegEx": "Green.", "year": 2006}, {"title": "Surface realisation from knowledge-bases", "author": ["B. Gyawali", "C. Gardent."], "venue": "Proc. of ACL.", "citeRegEx": "Gyawali and Gardent.,? 2014", "shortCiteRegEx": "Gyawali and Gardent.", "year": 2014}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["K. Heafield", "I. Pouzyrevsky", "J.H. Clark", "P. Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690\u2013696, Sofia, Bulgaria, August.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Generative alignment and semantic parsing for learning from ambiguous supervision", "author": ["J. Kim", "R.J. Mooney."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 543\u2013551. Association for Com-", "citeRegEx": "Kim and Mooney.,? 2010", "shortCiteRegEx": "Kim and Mooney.", "year": 2010}, {"title": "Numerical Computations with GPUs", "author": ["Volodymyr Kindratenko."], "venue": "Springer.", "citeRegEx": "Kindratenko.,? 2014", "shortCiteRegEx": "Kindratenko.", "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel."], "venue": "arXiv preprint arXiv:1411.2539.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "A global model for concept-to-text generation", "author": ["I. Konstas", "M. Lapata."], "venue": "J. Artif. Int. Res., 48(1):305\u2013346, October.", "citeRegEx": "Konstas and Lapata.,? 2013", "shortCiteRegEx": "Konstas and Lapata.", "year": 2013}, {"title": "Word embeddings through hellinger pca", "author": ["R. Lebret", "R. Collobert."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 482\u2013490, Gothenburg, Sweden, April. Association for Com-", "citeRegEx": "Lebret and Collobert.,? 2014", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "Efficient backprop", "author": ["Y. A LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller."], "venue": "Neural networks: Tricks of the trade, pages 9\u201348. Springer.", "citeRegEx": "LeCun et al\\.,? 2012", "shortCiteRegEx": "LeCun et al\\.", "year": 2012}, {"title": "Learning semantic correspondences with less supervision", "author": ["P. Liang", "M.I. Jordan", "D. Klein."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing", "citeRegEx": "Liang et al\\.,? 2009", "shortCiteRegEx": "Liang et al\\.", "year": 2009}, {"title": "A probabilistic forestto-string model for language generation from typed lambda calculus expressions", "author": ["W. Lu", "H.T. Ng."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1611\u20131622. Association", "citeRegEx": "Lu and Ng.,? 2011", "shortCiteRegEx": "Lu and Ng.", "year": 2011}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["M.-T. Luong", "I. Sutskever", "Q. V Le", "O. Vinyals", "W. Zaremba."], "venue": "Proc. ACL, pages 11\u201319.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky."], "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment", "author": ["H. Mei", "M. Bansal", "M.R. Walter."], "venue": "Proceedings of Human Language Technologies: The 2016 Annual Conference of the North American Chapter of", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computational", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Trainable approaches to surface natural language generation and their application to conversational dialog systems", "author": ["A. Ratnaparkhi."], "venue": "Computer Speech & Language, 16(3):435\u2013455.", "citeRegEx": "Ratnaparkhi.,? 2002", "shortCiteRegEx": "Ratnaparkhi.", "year": 2002}, {"title": "Building natural language generation systems, volume 33", "author": ["E. Reiter", "R. Dale", "Z. Feng."], "venue": "MIT Press.", "citeRegEx": "Reiter et al\\.,? 2000", "shortCiteRegEx": "Reiter et al\\.", "year": 2000}, {"title": "Choosing words in computergenerated weather forecasts", "author": ["E. Reiter", "S. Sripada", "J. Hunter", "J. Yu", "I. Davy."], "venue": "Artificial Intelligence, 167(1):137\u2013169.", "citeRegEx": "Reiter et al\\.,? 2005", "shortCiteRegEx": "Reiter et al\\.", "year": 2005}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li."], "venue": "arXiv preprint arXiv:1503.02364.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Generating approximate geographic descriptions", "author": ["R. Turner", "S. Sripada", "E. Reiter."], "venue": "Empirical methods in natural language generation, pages 121\u2013 140. Springer.", "citeRegEx": "Turner et al\\.,? 2010", "shortCiteRegEx": "Turner et al\\.", "year": 2010}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan."], "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrk\u0161i\u0107", "PeiHao Su", "David Vandyke", "Steve Young."], "venue": "Proceedings of the 2015 Conference on Empirical", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Generation by inverting a semantic parser that uses statistical machine translation", "author": ["Y.W. Wong", "R.J. Mooney."], "venue": "HLT-NAACL, pages 172\u2013179.", "citeRegEx": "Wong and Mooney.,? 2007", "shortCiteRegEx": "Wong and Mooney.", "year": 2007}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio."], "venue": "Proceedings of The 32nd International Conference on Machine Learning, volume 37, July.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Attention with intention for a neural network conversation model", "author": ["K. Yao", "G. Zweig", "B. Peng."], "venue": "arXiv preprint arXiv:1510.08565.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "Concept-to-text generation addresses the problem of rendering structured records into natural language (Reiter et al., 2000).", "startOffset": 103, "endOffset": 124}, {"referenceID": 6, "context": "An infobox is a fact table describing a person, similar to a person subgraph in a knowledge base (Bollacker et al., 2008; Ferrucci, 2012).", "startOffset": 97, "endOffset": 137}, {"referenceID": 11, "context": "An infobox is a fact table describing a person, similar to a person subgraph in a knowledge base (Bollacker et al., 2008; Ferrucci, 2012).", "startOffset": 97, "endOffset": 137}, {"referenceID": 5, "context": "We address this issue by parameterizing words and fields as vectors (embeddings), along with a neural language model operating on these embeddings (Bengio et al., 2003).", "startOffset": 147, "endOffset": 168}, {"referenceID": 17, "context": "This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014).", "startOffset": 113, "endOffset": 135}, {"referenceID": 20, "context": "This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014).", "startOffset": 174, "endOffset": 200}, {"referenceID": 14, "context": "This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014).", "startOffset": 230, "endOffset": 257}, {"referenceID": 5, "context": "We address this issue by parameterizing words and fields as vectors (embeddings), along with a neural language model operating on these embeddings (Bengio et al., 2003). This factorization allows us to scale to a large number of words and fields compared to Liang et al. (2009) and Kim and Mooney (2010) where the number of parameters grows as the product of the number of words and fields.", "startOffset": 148, "endOffset": 278}, {"referenceID": 5, "context": "We address this issue by parameterizing words and fields as vectors (embeddings), along with a neural language model operating on these embeddings (Bengio et al., 2003). This factorization allows us to scale to a large number of words and fields compared to Liang et al. (2009) and Kim and Mooney (2010) where the number of parameters grows as the product of the number of words and fields.", "startOffset": 148, "endOffset": 304}, {"referenceID": 7, "context": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010).", "startOffset": 82, "endOffset": 191}, {"referenceID": 31, "context": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010).", "startOffset": 82, "endOffset": 191}, {"referenceID": 13, "context": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010).", "startOffset": 82, "endOffset": 191}, {"referenceID": 12, "context": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010).", "startOffset": 82, "endOffset": 191}, {"referenceID": 33, "context": "Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androutsopoulos, 2007; Turner et al., 2010).", "startOffset": 82, "endOffset": 191}, {"referenceID": 9, "context": "One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005).", "startOffset": 90, "endOffset": 143}, {"referenceID": 2, "context": "One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005).", "startOffset": 90, "endOffset": 143}, {"referenceID": 23, "context": "Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009).", "startOffset": 193, "endOffset": 213}, {"referenceID": 3, "context": "Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006).", "startOffset": 137, "endOffset": 164}, {"referenceID": 29, "context": "End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011).", "startOffset": 146, "endOffset": 217}, {"referenceID": 36, "context": "End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011).", "startOffset": 146, "endOffset": 217}, {"referenceID": 4, "context": "End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011).", "startOffset": 146, "endOffset": 217}, {"referenceID": 24, "context": "End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011).", "startOffset": 146, "endOffset": 217}, {"referenceID": 0, "context": "More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013).", "startOffset": 76, "endOffset": 145}, {"referenceID": 17, "context": "More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013).", "startOffset": 76, "endOffset": 145}, {"referenceID": 20, "context": "More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013).", "startOffset": 76, "endOffset": 145}, {"referenceID": 19, "context": "Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al.", "startOffset": 94, "endOffset": 200}, {"referenceID": 16, "context": "Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al.", "startOffset": 94, "endOffset": 200}, {"referenceID": 34, "context": "Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al.", "startOffset": 94, "endOffset": 200}, {"referenceID": 37, "context": "Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al.", "startOffset": 94, "endOffset": 200}, {"referenceID": 8, "context": ", 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al.", "startOffset": 29, "endOffset": 93}, {"referenceID": 1, "context": ", 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al.", "startOffset": 29, "endOffset": 93}, {"referenceID": 25, "context": ", 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al.", "startOffset": 29, "endOffset": 93}, {"referenceID": 32, "context": ", 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015).", "startOffset": 50, "endOffset": 106}, {"referenceID": 35, "context": ", 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015).", "startOffset": 50, "endOffset": 106}, {"referenceID": 38, "context": ", 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015).", "startOffset": 50, "endOffset": 106}, {"referenceID": 27, "context": "Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the Weathergov and Robocup tasks.", "startOffset": 29, "endOffset": 47}, {"referenceID": 5, "context": "Because the probability estimates are smooth functions of the continuous word embeddings, a small change in the features results in a small change in the probability estimates (Bengio et al., 2003).", "startOffset": 176, "endOffset": 197}, {"referenceID": 22, "context": "The neural language model is trained to minimize the negative log-likelihood of a training sentence s with stochastic gradient descent (SGD; LeCun et al. 2012) :", "startOffset": 135, "endOffset": 159}, {"referenceID": 26, "context": "Introductions are split into sentences and tokenized with the Stanford CoreNLP toolkit (Manning et al., 2014).", "startOffset": 87, "endOffset": 109}, {"referenceID": 15, "context": "Our baseline is an interpolated Kneser-Ney (KN) language model and we use the KenLM toolkit to train 5-gram models without pruning (Heafield et al., 2013).", "startOffset": 131, "endOffset": 154}, {"referenceID": 21, "context": "This representation has shown to be helpful for various applications (Lebret and Collobert, 2014).", "startOffset": 69, "endOffset": 97}, {"referenceID": 28, "context": "We also measure the quality of the generated sentences with BLEU (Papineni et al., 2002) and report the mean and standard deviation for five models initialized with different seeds.", "startOffset": 65, "endOffset": 88}, {"referenceID": 18, "context": "Beam search generates many n-gram lookups for Kneser-Ney which requires many random memory accesses; while neural models perform scoring through matrix-matrix products, an operation which is more local and can be performed in a block parallel manner where modern graphic processors shine (Kindratenko, 2014).", "startOffset": 288, "endOffset": 307}], "year": 2017, "abstractText": "This paper introduces a neural model for concept-to-text generation that scales to large, rich domains. We experiment with a new dataset of biographies from Wikipedia that is an order of magnitude larger than existing resources with over 700k samples. The dataset is also vastly more diverse with a 400k vocabulary, compared to a few hundred words for Weathergov or Robocup. Our model builds upon recent work on conditional neural language model for text generation. To deal with the large vocabulary, we extend these models to mix a fixed vocabulary with copy actions that transfer sample-specific words from the input database to the generated output sentence. Our neural model significantly outperforms a classical Kneser-Ney language model adapted to this task by nearly 15 BLEU.", "creator": "LaTeX with hyperref package"}}}