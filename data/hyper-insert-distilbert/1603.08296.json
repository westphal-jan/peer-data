{"id": "1603.08296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2016", "title": "The SVM Classifier Based on the Modified Particle Swarm Optimization", "abstract": "the problem of development error of interpreting the nonlinear svm experimental classifier based particularly on the uniformly modified arbitrary particle swarm database optimization has long been considered. assume this reconstructed algorithm carries out the simultaneous search of the kernel function template type, values independent of the kernel function value parameters and value of possibly the regularization assignment parameter for fitting the svm classifier. such an svm classifier provides the incredibly high quality of data classification. the unique idea principle of performing particles'{ \\ guillemotleft } class regeneration { \\ guillemotright } is put up on over the basis of the modified particle swarm optimization algorithm. at lowering the finite realization expense of understanding this idea, some particles change repeatedly their initial kernel function pattern type to approach the one which corresponds to presenting the sampled particle with normally the chosen best possible value of the resource classification accuracy. alternatively the offered evolved particle swarm optimization algorithm allows reducing the time expenditures employed for a development task of applying the svm null classifier. the results found of experimental evaluation studies confirm the efficiency of this approximation algorithm.", "histories": [["v1", "Mon, 21 Mar 2016 20:12:44 GMT  (623kb)", "http://arxiv.org/abs/1603.08296v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["l demidova", "e nikulchev", "yu sokolova"], "accepted": false, "id": "1603.08296"}, "pdf": {"name": "1603.08296.pdf", "metadata": {"source": "META", "title": "The SVM Classifier Based on the Modified Particle Swarm Optimization", "authors": ["Liliya Demidova", "Evgeny Nikulchev", "Yulia Sokolova"], "emails": [], "sections": [{"heading": null, "text": "16 | P a g e\nwww.ijacsa.thesai.org\nbased on the modified particle swarm optimization has been considered. This algorithm carries out the simultaneous search of the kernel function type, values of the kernel function parameters and value of the regularization parameter for the SVM classifier. Such SVM classifier provides the high quality of data classification. The idea of particles' \u00abregeneration\u00bb is put on the basis of the modified particle swarm optimization algorithm. At the realization of this idea, some particles change their kernel function type to the one which corresponds to the particle with the best value of the classification accuracy. The offered particle swarm optimization algorithm allows reducing the time expenditures for development of the SVM classifier. The results of experimental studies confirm the efficiency of this algorithm.\nKeywords\u2014particle swarm optimization; SVM-classifier; kernel function type; kernel function parameters; regularization parameter; support vectors\nI. INTRODUCTION\nCurrently, for the different classification problems in various applications the SVM algorithm (Support Vector Machines, SVM), which carries out training on precedents (\u00absupervised learning\u00bb), is successfully used. This algorithm includes in the group of boundary classification algorithms [1], [2].\nThe SVM classifiers by the SVM algorithm have been applied for credit risk analysis [3], medical diagnostics [4], handwritten character recognition [5], text categorization [6], information extraction [7], pedestrian detection [8], face detection [9], etc.\nThe main feature of the SVM classifier is using of the special function called the kernel, with which the experimental data set has been converted from the original space of characteristics into the higher dimension space with the construction of a hyperplane that separates classes. A herewith two parallel hyperplanes must be constructed on both sides of the separating hyperplane. These hyperplanes define borders of classes and have been situated at the maximal possible distance from each other. It has been assumed that the bigger distance between these parallel hyperplanes gives the better accuracy of the SVM classifier. Vectors of the classified objects\u2019 characteristics which are the nearest to the parallel hyperplanes are called support vectors. An example of the separating hyperplane building in the 2D space has been shown in Fig. 1.\nThe SVM classifier supposes an execution of training, testing, and classification. Satisfactory quality of training and testing allows using the resulting SVM classifier in the classification of new objects.\nTraining of the SVM classifier assumes solving a quadratic optimization problem [1]\u2013[3]. Using a standard quadratic problem solver for training the SVM classifier would involve solving a big quadratic programming problem even for a moderate sized data set. This can limit the size of problems which can be solved with the application of the SVM classifier. Nowdays methods like SMO [10, 11], chunking [12] and simple SVM [13], Pegasos [14] exist that iteratively compute the required solution and have a linear space complexity [15].\nA solution for the problem which has been connected with a choice of the optimal parameters\u2019 values of the SVM classifier represents essential interest. It is necessary to find the kernel function type, values of the kernel function parameters and value of the regularization parameter, which must be set by a user and shouldn't change [1], [2]. It is impossible to provide implementing of high-accuracy data classification with the use of the SVM classifier without adequate solution of this problem.\nLet values of the parameters of the SVM classifier be optimal, if high accuracy of classification has been achieved: numbers of error within training and test sets are minimal, moreover the number of errors within test set must not strongly differ from the number of errors within training set. It will allow excluding retraining of the SVM classifier.\n17 | P a g e\nwww.ijacsa.thesai.org\nIn the simplest case solution of this problem can be achieved by a search of the kernel function types, values of the kernel function parameters and value of the regularization parameter that demands significant computational expenses. A herewith for an assessment of classification quality, the indicators of classification accuracy, classification completeness, etc. can be used [3].\nIn most cases of the development of binary classifiers, it is necessary to work with the complex, multiple extremal, multiple parameter objective function.\nGradient methods are not suitable for search of the optimum of such objective function, but search algorithms of stochastic optimization, such as the genetic algorithm [16]\u2013 [18], the artificial bee colony algorithm [19], the particle swarm algorithm [20], [21], etc., have been used. earch of the optimal decision is carried out at once in all space of possible decisions.\nThe particle swarm algorithm (Particle Swarm Optimization, PSO algorithm), which is based on an idea of possibility to solve the optimization problems using modeling of animals\u2019 groups\u2019 behavior is the simplest algorithm of evolutionary programming because for its implementation it is necessary to be able to determine only value of the optimized function [20], [21].\nThe traditional approach to the application of the PSO algorithm consists of the repeated applications of the PSO algorithm for the fixed type of the kernel functions to choose optimal values of the kernel function parameters and value of the regularization parameter with the subsequent choice of the best type of the kernel function and values of the kernel function parameters and value of the regularization parameter corresponding to this kernel function type.\nAlong with the traditional approach to the application of the PSO algorithm a new approach, that implements the simultaneous search for the best type of the kernel function, values of the kernel function parameters and value of the regularization parameter, is offered [22]. Hereafter, particle swarm algorithms corresponding to traditional and modified approaches will be called as the traditional PSO algorithm and the modified PSO algorithm consequently.\nThe objective of this paper is to fulfill a comparative analysis of the traditional and modified particle swarm algorithms, applied for the development of the SVM classifier, both on the search time of the optimal parameters of the SVM classifier and the quality of data classification.\nThe rest of this paper is structured as follows. Section II presents the main stages of the SVM classifier development. Then, Section III details the proposed new approach for solving the problem of the simultaneous search of the kernel function type, values of the kernel function parameters and value of the regularization parameter for the SVM classifier. This approach is based on the application of the modified PSO algorithm. Experimental results comparing the traditional PSO algorithm to the modified PSO algorithm follow in Section IV. Finally, \u0441onclusions are drawn in Section V.\nII. THE SMV CLASSIFIER\nLet the experimental data set be a set in the form of\n)}(),...,{( 11 ss,yz,yz , in which each object Zzi  ( si ,1 ; s is the number of objects) is assigned to number  1;1 Yyi having a value of +1 or \u22121 depending on the class of object iz . A herewith it is assumed that every object iz is mapped to q - dimensional vector of numerical values of characteristics\n),,,( 21 qiiii zzzz  (typically normalized by values from the\ninterval [0, 1]) where liz is the numeric value of the l -th\ncharacteristic for the i -th object ( si ,1 , ql ,1 ) [22]\u2013[25]. It\nis necessary with use of special function ),(  zzi , which is\ncalled the kernel, to build the classifier YZF : , which\ncompares to the class with number from the set  1;1 Y some object from the set Z .\nTo build \u00abthe best\u00bb SVM classifier it is necessary to realize numerous repeated training and testing on the different randomly generated training and test sets with following determination of the best SVM classifier in terms of the highest possible classification quality provision. The test set contains the part of data from the experimental data set. The size of the test set must be equal to 1/10 \u2013 1/3 of the size of the experimental data set. The test set doesn't participate in the control of parameters of the SVM-classifier. This set is used for check of classifier\u2019s accuracy. The SVM classifier with satisfactory training and testing results can be used to classify new objects [22].\nThe separating hyperplane for the objects from the training\nset can be represented by equation 0, bzw , where w is a\nvector-perpendicular to the separating hyperplane; b is a parameter which corresponds to the shortest distance from the\norigin of coordinates to the hyperplane; ,w z is a scalar\nproduct of vectors w and z [1\u20133]. The condition\n1,1  bzw specifies a strip that separates the classes.\nThe wider the strip, the more confidently we can classify objects. The objects closest to the separating hyperplane, are exactly on the bounders of the strip.\nIn the case of linear separability of classes we can choose a hyperplane so that there is no any object from the training set between them, and then maximize the distance between the\nhyperplanes (width of the strip) 2 ,w w  , solving the\nproblem of quadratic optimization [1], [2]:\n\n  \n\n\n.,1,1),(\nmin,,\nSibzwy\nww\nii\n(1)\nThe problem of the separating hyperplane building can be reformulated as the dual problem of searching a saddle point of the Lagrange function, which reduces to the problem of quadratic programming, containing only dual variables [1], [2]:\n18 | P a g e\nwww.ijacsa.thesai.org\n    \n    \n\n\n\n\n\n\n \n\n\n \n\n,1, ,0\n0,\nmin,),( 2\n1\n)(\n1\n1 1\n1\nSiC\ny\nzzyy\nL\ni\nS\ni ii\nS\ni\nS\niii\nS\ni i\n\n\n\n\n  (2)\nwhere i is a dual variable; iz is the object of the training\nset; iy is a number (+1 or \u22121), which characterize the class of\nthe object iz from the experimental data set; ),(  zzi is a kernel function; C is a regularization parameter ( 0C  ); S is\na quantity of objects in the experimental data set; 1,i S .\nIn training of the SVM classifier it is necessary to determine the kernel function type ),(  zzi , values of the kernel parameters and value of the regularization parameter C , which allows finding a compromise between maximizing of the gap separating the classes and minimizing of the total error. A herewith typically one of the following functions is used as the kernel function ),(  zzi [1], [3], [26]:\n linear function:   zzzz ii ,),( ;\n polynomial function: dii zzzz 1),(),(   ;\n radial basis function:\n))2/(,(),( 2   zzzzexpzz iii ;\n sigmoid function: ),(),( 12   zzkkzz ii th ,\nwhere  zzi , is a scalar product of vectors iz and z ; d [ \u039dd  (by default 3d )],  [ 0 (by default 1 2 \n)], 2k [ 02 k (by default 12 k )] and 1k [ 01 k (by default 11 k )] are some of parameters; th is a hyperbolic tangent.\nThese kernel functions allow dividing the objects from different classes.\nAs a result of the SVM classifier training the support vectors must be determined. These vectors are closest to the hyperplane separating the classes and contain all information about the classes\u2019 separation. The main problem dealing with the training of the SVM classifier, is the lack of recommendations for the choice of value of the regularization parameter, the kernel function type and values of the kernel function parameters, which can provide the high accuracy of objects\u2019 classification. This problem can be solved with the use of various optimization algorithms, in particular with the use of the PSO algorithm.\nIII. THE MODIFIED PSO ALGORITHM\nIn the traditional PSO algorithm the n -dimensional search\nspace ( n is the number of parameters which are subject to\noptimization) is inhabited by a swarm of m agents-particles\n(elementary solutions). Position (location) of the i -th particle\nis determined by vector ),,,( 21 niiii xxxx  , which defines a set of values of optimization parameters. A herewith these parameters can be presented in an explicit form or even absent in analytical record of the objective function\n),...,,()( 21 nxxxfxf  of the optimization algorithm (for\nexample, the optimum is the minimum which must be achieved).\nThe particles must be situated randomly in the search space during the process of initialization. A herewith each i -th\nparticle ( mi ,1 ) has its own vector of speed n i Rv  which\ninfluence i -th particle ( mi ,1 ) coordinates\u2019 values in every single moment of time corresponding to some iteration of the PSO algorithm.\nThe coordinates of the i -th particle ( mi ,1 ) in the n -\ndimensional search space uniquely determine the value of the objective function ),,,()( 21 niiii xxxfxf  which is a certain solution of the optimization problem [20] \u2013 [22].\nFor each position of the n -dimensional search space where\nthe i -th particle ( mi ,1 ) was placed, the calculation of value\nof the objective function )( ixf is performed. A herewith each i -th particle remembers the best value of the objective function found personally as well as the coordinates of the position in the n -dimensional space corresponding to the value\nof the objective function. Moreover each i -th particle ( mi ,1\n) \u00abknows\u00bb the best position (in terms of achieving the optimum of the objective function) among all positions that had been \u00abexplored\u00bb by particles (due to it the immediate exchange of information is replicated by all the particles). At each iteration particles correct their velocity to, on the one hand, move closer to the best position which was found by the particle independently and, on the other hand, to get closer to the position which is the best globally at the current moment. After a number of iterations particles must come close to the best position (globally the best for all iterations). However, it is possible that some particles will stay somewhere in the relatively good local optimum.\nConvergence of the PSO algorithm depends on how velocity vector correction is performed. There are different\napproaches to implementation of velocity vector iv correction\nfor the i -th particle ( mi ,1 ) [20]. In the classical version of\nthe PSO algorithm correction of each j -th coordinate of\nvelocity vector ( nj ,1 ) of the i -th particle ( mi ,1 ) is made\nin accordance with formula [20]:\n)~(~~)\u02c6(\u02c6\u02c6 ji j i j i j i j i xxrxxrvv j   , (3)\nwhere j\niv is the j -th coordinate of velocity vector of the i -\nth particle; j\nix is the j -th coordinate of vector ix , defining the\nposition of the i -th particle; ji\u0445\u0302 is the j -th coordinate of the best position vector found by the i -th particle during its\n19 | P a g e\nwww.ijacsa.thesai.org\nexistence; j\u0445~ is the j -th coordinate of the globally best\nposition within the particles swarm in which the objective function has the optimal value; r\u0302 and r~ are random numbers in interval (0, 1), which introduce an element of randomness in the search process; \u0302 and ~ are personal and global\ncoefficients for particle acceleration which are constant and determine behavior and effectiveness of the PSO algorithm in general.\nWith personal and global acceleration coefficients in (3) random numbers r\u0302 and r~ must be scaled; a herewith the global acceleration coefficient ~ operates by the impact of the\nglobal best position on the speeds of all particles and the\npersonal acceleration coefficient \u0302 operates by the impact of\nthe personal best position on the velocity of some particle.\nCurrently different versions of the traditional PSO algorithm are known. In one of the most known canonical version of the PSO algorithm it is supposed to undertake the normalization of the acceleration coefficients \u0302 and ~ to\nmake the convergence of the algorithm not so much dependent on the choice of their values [20].\nA herewith correction of each j -th coordinate of the\nvelocity vector ( nj ,1 ) of the i -th particle ( mi ,1 ) is\nperformed in accordance with formula:\n)]~(~~)\u02c6(\u02c6\u02c6[ ji j i j i j i j i xxrxxrvv j   , (4)\nwhere  is a compression ratio;\n||/ 422 2   K ; (5)\n ~\u02c6  ( 4 ); (6)\nK is the some scaling coefficient, which takes values from the interval (0, 1).\nWhen using formula (4) for correction of velocity vector the convergence of the PSO algorithm is guaranteed and there is no need to control the particle velocity explicitly [20].\nLet the correction of velocity vector of the i -th particle (\nmi ,1 ) is executed in accordance with one of the formulas\n(3) or (4). The correction of the j -th position coordinate of the\ni -th particle ( mi ,1 ) can be executed in accordance with the\nformula:\nj\ni\nj\ni\nj\ni vxx  . (7)\nThen for each i -th particle ( mi ,1 ) the new value of the\nobjective function )( ixf can be calculated and the following check must be perfomed: whether a new position with\ncoordinates vector ix became the best among all positions in which the i -th particle has previously been placed. If new\nposition of the i -th particle is recognized to be the best at the current moment the information about it must be stored in a\nvector ix\u0302 ( mi ,1 ).\nA herewith value of the objective function )( ixf for this position must be remembered. Then among all new positions of the swarm particles the check of the globally best position must be carried out. If some new position is recognized as the best globally at the current moment, the information about it must be stored in a vector x~ . A herewith value of the objective\nfunction )( ixf for this position must be remembered.\nIn the case of the SVM classifier's development with the use of the PSO algorithm the swarm particles can be defined by vectors declaring their position in the search space and corded by the kernel function parameters and the regularization parameter: ),,( 21 iii Cxx , where i is a number of particle (\nmi ,1 ); 21, ii xx are the kernel function parameters of the i -\nth particle, [a herewith parameter 1 ix is equal to the kernel\nfunction parameters d ,  or 2k (depending on the kernel function type which corresponds to a swamp particle); parameter 2 ix is equal to the kernel function parameter 1k , if the swamp particle corresponds to the sigmoid type of the kernel function, otherwise this parameter is assumed to be\nzero]; iC is the regularization parameter.\nThen traditional approach to the application of the PSO algorithm in developing the SVM classifier must be concluded in numerous implementation of the PSO algorithm under the fixed kernel function type aiming to choose the optimal parameters values of the kernel function and value of the regularization parameter.\nAs result for each type T of the kernel function, participating in the search, the particle with the optimal combination of the parameters values ) ~ ,~,~ ( 21 Cxx providing\nhigh quality of classification will be defined.\nThe best type and the best values of the required parameters get out by results of the comparative analysis of the best particles received at realization of the PSO algorithm with the fixed kernel function type.\nAlong with the traditional approach to the application of the PSO algorithm in the development of the SVM classifier there is a new approach that implements a simultaneous search for the best kernel function type T ~\n, parameters\u2019 values 1~x and 2~x of the kernel function and value of the regularization\nparameter C ~ . At such approach each i -th particle in a swamp\n( mi ,1 ) defined by a vector which describes particle's\nposition in the search space: ),, ,( 21\niiii CxxT , where iT is the\nnumber of the kernel function type (for example, 1, 2, 3 \u2013 for polynomial, radial basis and sigmoid functions accordingly);\n20 | P a g e\nwww.ijacsa.thesai.org\nparameters ,1ix , 2 ix iC are defined as in the previous case. A herewith it is possible to \u00abregenerate\u00bb particle through\nchanging its coordinate iT on number of that kernel function type, for which particles show the highest quality of classification. In the case of particles\u2019 \u00abregeneration\u00bb the parameters\u2019 values change so that they corresponded to new type of the kernel function (taking into account ranges of change of their values). Particles which didn't undergo \u00abregeneration\u00bb, carry out the movement in own space of search of some dimension.\nThe number of particles taking part in \u00abregeneration\u00bb must be determined before start of algorithm. This number must be equal to 15% \u2013 25% of the initial number of particles. It will allow particles to investigate the space of search. A herewith they won't be located in it for a long time if their indicators of accuracy are the worst.\nThe offered modified PSO algorithm can be presented by the following consequence of steps.\nStep 1. To determine parameters of the PSO algorithm: number m of particles in a swamp, velocity coefficient K , personal and global velocity coefficients \u0302 and ~ , maximum\niterations number maxN of the PSO algorithm. To determine types T of kernel functions, which take part in the search ( 1T \u2013 polynomial function, 2T \u2013 radial basis function, 3T \u2013 sigmoid function) and ranges boundaries of the kernel function parameters and the regularization parameter C for the\nchosen kernel functions' types T : Tx1min , Tx1max , Tx2min , Tx2max , T\u0421min , T\u0421max ( 0 2 min  Tx and 02max  Tx for 1T and 2T ). To determine the particles\u2019 \u00abregeneration\u00bb percentage p .\nStep 2. To define equal number of particles for each kernel\ntype function T , included in search, to initialize coordinate iT\nfor each i -th particle ( mi ,1 ) (a herewith every kernel\nfunction type must be corresponded by equal number of\nparticles), other coordinates of the i -th particle ( mi ,1 ) must\nbe generated randomly from the corresponding ranges:\n],[ 1max 1 min 1 \u03a4\u03a4 i xxx  , ],[ 2 max 2 min 2 \u03a4\u03a4 i xxx  ( 0 2 ix under 1T and\n2T ), ],[ maxmin \u03a4\u03a4 i CCC  . To initialize random velocity vector\n),,( 321 iiii vvvv of the i -th particle ( mi ,1 ) ( 0 2 iv under 1T and 2T ). To establish initial position of the i -th particle (\nmi ,1 ) as its best known position )\u02c6,\u02c6,\u02c6,\u02c6 ( 21 iiii CxxT , to determine the best particle with coordinates\u2019 vector\n) ~ ,~,~ , ~ ( 21 CxxT from all the m particles, and to determine the\nbest particle for each kernel function type T , including in a search, with coordinates\u2019 vector ),,,( 21 TTT CxxT . Herewith\nnumber of executed iterations must be considered as 1.\nStep 3. To execute while the number of iterations is less\nthan the fixed number maxN :\n \u00abregeneration\u00bb of particles: to choose p % of particles\nwhich represent the lowest quality of classification from particles with coordinate TTi ~  ( mi ,1 ); to change coordinate iT (with the kernel function type) on T ~ ; to change values of the parameters iii Cxx ,, 21 of \u00abregenerated\u00bb particles to let them correspond to a new kernel function type T ~\n(within the scope of the corresponding ranges);\n correction of velocity vector ),,( 321 iiii vvvv and position\n),,( 21 iii Cxx of the i -th particle ( mi ,1 ) using formulas:\n\n \n\n\n \n,3)],(~~)\u02c6(\u02c6\u02c6[\n,2,1)],(~~)\u02c6(\u02c6\u02c6[\nj\u0421Cr\u0421\u0421rv\njxxrxxrv v\ni T ii j i\nj i jTj i j i j ij\ni \n\n(8)\nj i j i j i vxx  for 2,1j , (9)\n3iii vCC  , (10)\nwhere r\u0302 and r~ are random numbers in interval (0, 1), \nis a compression ratio calculated using the formula (5); a herewith formula (8) is the modification of formula (4): the coordinates\u2019 values TTT Cxx ,, 21 are used instead of the coordinates\u2019 values Cxx ~ ,~,~ 21 of the globally best particle;\n accuracy calculation of the SVM classifier with\nparameters\u2019 values ),,,( 21 iiii CxxT ( mi ,1 ) with aim to find the optimal combination ) ~ ,~,~, ~ ( 21 CxxT , which will\nprovide high quality of classification;\n increase of iterations number on 1.\nThe particle with the optimal combination of the\nparameters\u2019 values ) ~ ,~,~, ~ ( 21 CxxT which provides the highest\nquality of classification on chosen the function types will be defined after execution of the offered algorithm.\nAfter executing of the modified PSO algorithm it can be found out that all particles will be situated in the search space which corresponds to the kernel function with the highest classification quality because some particles in the modified PSO algorithm changed their coordinate, which is responsible for number of the kernel function. A herewith all other search spaces will turn out to be empty because all particles will \u00abregenerate\u00bb their coordinate with number of the kernel function type. In some cases (for small values of the iterations\u2019\nnumber maxN and for small value of the particles\u2019 \u00abregeneration\u00bb percentage p ) some particles will not\n\u00abregenerate\u00bb their kernel function type and will stay in their initial search space.\n21 | P a g e\nwww.ijacsa.thesai.org\nUsing of this approach in the application of the PSO algorithm in the problem of the SVM classifier development allows reducing the time required to construct the desired SVM classifier.\nQuality evaluation of the SVM classifier can be executed with the use of different classification quality indicators [3]. There are cross validation data indicator, accuracy indicator, classification completeness indicator and ROC curve analysis based indicator, etc.\nIV. EXPERIMENTAL STUDIES\nThe feasibility of the modified PSO algorithm using for the SVM classifier development was approved by test and real data. In the experiment for a particular data set the traditional PSO algorithm and the modified PSO algorithm were carried out. Comparison between these algorithms was executed using the found optimal parameters values of the SVM algorithm, classification accuracy and spent time.\nActual data used in the experimental researches was taken from Statlog project and from UCI machine learning library. Particularly, we used two data sets for medical diagnostics and one data set for credit scoring:\n breast cancer data set of The Department of Surgery at the University of Wisconsin, in which the total number of instances is 569 including 212 cases with the diagnosed cancer (class 1) and 357 cases without such diagnosis (class 2); a herewith each patient is described by 30 characteristics ( 30q ) and all information was\nobtained with the use of digital images (WDBC data set in the Table, the source is http://archive.ics.uci.edu/ml/; machine-learning-databases/breast-cancer-wisconsin/);\n heart disease data set, in which the total number of instances is 270 including 150 cases with the diagnosed heart disease (class 1) and 120 cases without such diagnosis (class 2); a herewith each patient is described by 13 characteristics ( 13q ) (Heart data set in the\nTable, the source is http:// archive.ics.uci.edu/ml/machine-learning-databases/ statlog/heart/; a herewith desease was found for 150 patients (class 1) and desease was not found for 120 patients (class 2));\n Australian consumer credit data set, in which the total number of instances is 690 including 382 creditworthy cases (class 1) and 308 default cases (class 2); a herewith each applicant is described by 14 characteristics ( 14q ) (Australian data set in the\nTable, the source is http://archive.ics.uci.edu/ml/ machine-learning-data bases/statlog/australian/).\nMoreover two testing data sets were used in experimental researches: Test [11] and \u041c\u041e\u0422\u041f12 (the source is http://machinelearning.ru/wiki/images/b/b2/ MOTP12_svm_example.rar).\nFor all data sets binary classification was performed.\nFor development of the SVM classifier the traditional and the modified PSO algorithms were used; a herewith the choice of the optimal values of the SVM classifier parameters was realized. The kernels with polynomial, radial basis and sigmoid functions were included in the search and the identical values of the PSO algorithm parameters and the identical ranges of values\u2019 change of the required SVM classifier parameters were established.\nThe short description of characteristics of each data set is provided in the Table. Here search results of the optimal values of parameters of the SVM classifier with the application of the traditional PSO algorithm and the modified PSO algorithm are presented (in the identical ranges of parameters\u2019 change and at the identical PSO algorithm parameters), number of error made during the training and testing of the SVM classifier and search time. For example, for WDBC data set with the use of the traditional and the modified PSO algorithms the kernel with radial basis function (number 2) was determined as the optimal. For the traditional PSO algorithm the optimal values of the kernel parameter and the regularization parameter are equal to 81.6 and 93.4C accordingly. For the modified PSO algorithm the optimal values of the kernel parameter and the regularization parameter are equal to 01.4 and\n83.9C accordingly.\nThe classification accuracy by the traditional PSO algorithm is equal to 99.12%, and the classification accuracy by the modified PSO algorithm is equal to 99.65%. A herewith the search time came to 10108 and 3250 seconds accordingly.\nFor Heart data set in the Figures 2 \u2013 4 the examples of position of the particles swarm in the D-2 search spaces and in the D-3 search space during initialization, at the 3-rd iteration and at the 12-th iteration (with the use of the modified PSO algorithm) are shown.\nThe kernels with polynomial, radial basis and sigmoid functions were included in the search. A herewith the following change ranges of values' parameters were set: 83  d , \u039dd (for polynomial function); 101.0  (for\nradial basis function); 1.010 2  k \u0438 101.0 1  k (for sigmoid function).\n23 | P a g e\nwww.ijacsa.thesai.org\nChange range for the regularization parameter C was determined as: 101.0 C . Moreover, the following values of parameters of the PSO algorithm were set: number m of\nparticles in a swarm equal to 600 (200 per each kernel\nfunction type); iterations\u2019 number 20max N ; personal and global velocity coefficients equal to 2\u02c6  and 5~ \naccordingly; the scaling coefficient 3.0K ; \u00abregeneration\u00bb\ncoefficient of particles %20p . Particles marked by asterisk\nbullets in the search spaces and the best position from the search space is marked by white round bullet. During realization of the modified PSO algorithm the swamp particles moves towards the best (optimal) position for the current iteration in the search space and demonstrate collective search of the optimal position. A herewith velocity and direction of each particle are corrected. Moreover \u00abregeneration\u00bb of particles takes place: some particles change own search space to space, in which particles show the best quality of classification.\nThus, during realization of the modified PSO algorithm there is a change of the particles\u2019 coordinates, which are\nresponsible for parameters of the kernel function ),(  zzi and the regularization parameter C . Besides, the type of the kernel function also changes. As a result the particles moves towards the united search space (in this case \u2013 the space corresponding to radial basis kernel function) leaving the space where they were initialized.\nIn the reviewed example only 7 particles didn\u2019t change their kernel function type after 20 iterations. Other particles situated near the best position responsible for the optimal solution in the search space (Figure 5).\nIt is visible from the Table that that as a result of search for the reviewed data sets both algorithms determined identical kernel function type as the optimal, similar values of the kernel function parameter and the regularization parameter, and also similar accuracy values of training and testing of the SVM classifier.\nBut the modified PSO algorithm is more effective, because it took less (more than 2\u20133 times) time for search than traditional one.\nV. CONCLUSION\nThe experimental results obtained on the base of the test data traditionally used to assess the classification quality, confirm the efficiency of the modified PSO algorithm. This algorithm allows choosing the best kernel function type, values of the kernel function parameters and value of the regularization parameter with the time expenditures which are significantly less, than in the case of the traditional PSO algorithm. A herewith high accuracy of classification is provided.\nThe obtained results had been reached thanks to \u00abregeneration\u00bb of particles in the modified PSO algorithm. Particles which participate in the \u00abregeneration\u00bb process change their kernel function type to the one which corresponds to the particle with the best value of the classification accuracy. Also, these articles change the accessory ranges of their parameters.\nFurther researches will have been devoted to the development of recommendations on the application of the modified PSO algorithm in the solution of the practical problems."}], "references": [{"title": "Choosing Multiple Parameters for Support Vector Machine", "author": ["O. Chapelle", "V. Vapnik", "O. Bousquet", "S. Mukherjee"], "venue": "Machine Learning, vol. 46, no. 1\u20133, pp. 131\u2013159, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Bio-Inspired Credit Risk Analysis", "author": ["L. Yu", "S. Wang", "K.K. Lai", "L. Zhou"], "venue": "Computational Intelligence with Support Vector Machines. Springer-Verlag,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Performance Evaluation of SVM and K- Nearest Neighbor Algorithm over Medical Data set", "author": ["J.S. Raikwal", "K. Saxena"], "venue": "International Journal of Computer Applications, vol. 50, no. 14, pp. 35\u201339, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning Algorithms for Classification: A Comparison on Handwritten Digit Recognition", "author": ["Y. LeCun", "L.D. Jackel", "L. Bottou", "C. Cortes at al."], "venue": "Neural Networks: The Statistical Mechanics Perspective, J. H. Oh, C. Kwon and S. Cho, Eds. World Scientific, 1995, pp. 261\u2013276.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Text Categorization with Support Vector Machines: Learning with Many Relevant Features", "author": ["T. Joachims"], "venue": "Lecture Notes in Computer Science, vol. 1398, pp. 137\u2013142, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "SVM Based Learning System For Information Extraction", "author": ["Y. Li", "K. Bontcheva", "H. Cunningham"], "venue": "Lecture Notes in Computer Science, vol. 3635, pp. 319\u2013339, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Pedestrian Detection Using Wavelet Templates", "author": ["M. Oren", "C. Papageorgious", "P. Sinha", "E. Osuna", "T. Poggio"], "venue": "1997 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 1997, pp. 193\u2013199.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Training Support Vector Machines: An Application to Face Detection", "author": ["E. Osuna", "R. Freund", "F. Girosi"], "venue": "1997 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition, 1997, pp. 130\u2013136.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Fast Training of Support Vector Machines Using Sequential Minimal Optimization", "author": ["J.C. Platt"], "venue": "Advances in Kernel Methods. Support Vector Learning, 1998, pp. 185\u2013208.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Improvements to the SMO Algorithm for SVM Regression", "author": ["S.K. Shevade", "S.S. Keerthi", "C. Bhattacharyya", "K.R.K. Murthy"], "venue": "IEEE Trans. on Neural Networks, vol. 11, no. 5, pp. 1188\u20131193, 2000.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Improved Training Algorithm for Support Vector Machines", "author": ["E. Osuna", "R. Freund", "F. Girosi"], "venue": "1997 IEEE Workshop Neural Networks for Signal Processing, 1997, pp. 24\u201326. (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 7, No. 2, 2016 24 | P a g e www.ijacsa.thesai.org", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1997}, {"title": "SSVM: a simple SVM algorithm", "author": ["S.V.N. Vishwanathan", "A. Smola", "N. Murty"], "venue": "Proceedings of the 2002 International Joint Conference on Neural Networks, vol. 3, pp. 2393-2398, 2002.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Pegasos: Primal Estimated sub-Gradient Solver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming, vol. 127, no. 1, pp. 3\u201330, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Messy genetic algorithms: Motivation, analysis, and first results", "author": ["D.E. Goldberg", "B. Korb", "K. Deb"], "venue": "Complex Systems, vol. 3, no. 5, pp. 493\u2013530, 1989.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1989}, {"title": "Genetic algorithms and support vector machines for time series classification", "author": ["D.R. Eads", "D. Hill", "S. Davis", "S.J. Perkins", "J. Ma at al."], "venue": "Proc. SPIE 4787 Applications and Science of Neural Networks, Fuzzy Systems, and Evolutionary Computation, vol. 74, 2002, p. 74.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Genetic algorithms for support vector machine model selection", "author": ["S. Lessmann", "R. Stahlbock", "S.F. Crone"], "venue": "2006 International Joint Conference on Neural Networks, 2006, pp. 3063\u20133069.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Artificial Bee Colony (ABC) Optimization Algorithm for Solving Constrained Optimization Problems", "author": ["D. Karaboga", "B. Basturk"], "venue": "Proc. of the 12th Intern. Fuzzy Systems Association world congress on Foundations of Fuzzy Logic and Soft Computing, 2007, pp. 789\u2013798.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Particle Swarm Optimisation: Classical and Quantum Perspectives", "author": ["J. Sun", "C.-H. Lai", "X.-J. Wu"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Particle swarm optimization", "author": ["R. Poli", "J. Kennedy", "T. Blackwell"], "venue": "Swarm Intelligence, vol. 1, no. 1, pp. 33\u201357, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Modification Of Particle Swarm Algorithm For The Problem Of The SVM Classifier Development", "author": ["L. Demidova", "Yu. Sokolova"], "venue": "2015 International Conference \"Stability and Control Processes\" (SCP), 2015, pp. 623\u2013627.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Use of Fuzzy Clustering Algorithms\u2019 Ensemble for SVM classifier Development", "author": ["L. Demidova", "Yu. Sokolova", "E. Nikulchev"], "venue": "International Review on Modelling and Simulations, vol. 8, no. 4, pp. 446\u2013457, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "SVM-Classifier Development With Use Of Fuzzy Clustering Algorithms' Ensemble On The Base Of Clusters' Tags' Vectors' Similarity Matrixes", "author": ["L. Demidova", "Yu. Sokolova"], "venue": "16th International Symposium on Advanced Intelligent Systems, 2015, pp. 889\u2013906.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Training Set Forming For SVM Algorithm With Use Of The Fuzzy Clustering Algorithms Ensemble On Base Of Cluster Tags Vectors Similarity Matrices", "author": ["L. Demidova", "Yu. Sokolova"], "venue": "2015 International Conference Stability and Control Processes (SCP), pp. 619\u2013622, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Support vector machines in R", "author": ["A. Karatzoglou", "D. Meyer", "K. Hornik"], "venue": "Research Report, WU Vienna,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "This algorithm includes in the group of boundary classification algorithms [1], [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "The SVM classifiers by the SVM algorithm have been applied for credit risk analysis [3], medical diagnostics [4], handwritten character recognition [5], text categorization [6], information extraction [7], pedestrian detection [8], face detection [9], etc.", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "The SVM classifiers by the SVM algorithm have been applied for credit risk analysis [3], medical diagnostics [4], handwritten character recognition [5], text categorization [6], information extraction [7], pedestrian detection [8], face detection [9], etc.", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "The SVM classifiers by the SVM algorithm have been applied for credit risk analysis [3], medical diagnostics [4], handwritten character recognition [5], text categorization [6], information extraction [7], pedestrian detection [8], face detection [9], etc.", "startOffset": 148, "endOffset": 151}, {"referenceID": 4, "context": "The SVM classifiers by the SVM algorithm have been applied for credit risk analysis [3], medical diagnostics [4], handwritten character recognition [5], text categorization [6], information extraction [7], pedestrian detection [8], face detection [9], etc.", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "The SVM classifiers by the SVM algorithm have been applied for credit risk analysis [3], medical diagnostics [4], handwritten character recognition [5], text categorization [6], information extraction [7], pedestrian detection [8], face detection [9], etc.", "startOffset": 201, "endOffset": 204}, {"referenceID": 6, "context": "The SVM classifiers by the SVM algorithm have been applied for credit risk analysis [3], medical diagnostics [4], handwritten character recognition [5], text categorization [6], information extraction [7], pedestrian detection [8], face detection [9], etc.", "startOffset": 227, "endOffset": 230}, {"referenceID": 7, "context": "The SVM classifiers by the SVM algorithm have been applied for credit risk analysis [3], medical diagnostics [4], handwritten character recognition [5], text categorization [6], information extraction [7], pedestrian detection [8], face detection [9], etc.", "startOffset": 247, "endOffset": 250}, {"referenceID": 0, "context": "Training of the SVM classifier assumes solving a quadratic optimization problem [1]\u2013[3].", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "Training of the SVM classifier assumes solving a quadratic optimization problem [1]\u2013[3].", "startOffset": 84, "endOffset": 87}, {"referenceID": 8, "context": "Nowdays methods like SMO [10, 11], chunking [12] and simple SVM [13], Pegasos [14] exist that iteratively compute the required solution and have a linear space complexity [15].", "startOffset": 25, "endOffset": 33}, {"referenceID": 9, "context": "Nowdays methods like SMO [10, 11], chunking [12] and simple SVM [13], Pegasos [14] exist that iteratively compute the required solution and have a linear space complexity [15].", "startOffset": 25, "endOffset": 33}, {"referenceID": 10, "context": "Nowdays methods like SMO [10, 11], chunking [12] and simple SVM [13], Pegasos [14] exist that iteratively compute the required solution and have a linear space complexity [15].", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Nowdays methods like SMO [10, 11], chunking [12] and simple SVM [13], Pegasos [14] exist that iteratively compute the required solution and have a linear space complexity [15].", "startOffset": 64, "endOffset": 68}, {"referenceID": 12, "context": "Nowdays methods like SMO [10, 11], chunking [12] and simple SVM [13], Pegasos [14] exist that iteratively compute the required solution and have a linear space complexity [15].", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "It is necessary to find the kernel function type, values of the kernel function parameters and value of the regularization parameter, which must be set by a user and shouldn't change [1], [2].", "startOffset": 183, "endOffset": 186}, {"referenceID": 1, "context": "can be used [3].", "startOffset": 12, "endOffset": 15}, {"referenceID": 13, "context": "Gradient methods are not suitable for search of the optimum of such objective function, but search algorithms of stochastic optimization, such as the genetic algorithm [16]\u2013 [18], the artificial bee colony algorithm [19], the particle swarm algorithm [20], [21], etc.", "startOffset": 168, "endOffset": 172}, {"referenceID": 15, "context": "Gradient methods are not suitable for search of the optimum of such objective function, but search algorithms of stochastic optimization, such as the genetic algorithm [16]\u2013 [18], the artificial bee colony algorithm [19], the particle swarm algorithm [20], [21], etc.", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": "Gradient methods are not suitable for search of the optimum of such objective function, but search algorithms of stochastic optimization, such as the genetic algorithm [16]\u2013 [18], the artificial bee colony algorithm [19], the particle swarm algorithm [20], [21], etc.", "startOffset": 216, "endOffset": 220}, {"referenceID": 17, "context": "Gradient methods are not suitable for search of the optimum of such objective function, but search algorithms of stochastic optimization, such as the genetic algorithm [16]\u2013 [18], the artificial bee colony algorithm [19], the particle swarm algorithm [20], [21], etc.", "startOffset": 251, "endOffset": 255}, {"referenceID": 18, "context": "Gradient methods are not suitable for search of the optimum of such objective function, but search algorithms of stochastic optimization, such as the genetic algorithm [16]\u2013 [18], the artificial bee colony algorithm [19], the particle swarm algorithm [20], [21], etc.", "startOffset": 257, "endOffset": 261}, {"referenceID": 17, "context": "The particle swarm algorithm (Particle Swarm Optimization, PSO algorithm), which is based on an idea of possibility to solve the optimization problems using modeling of animals\u2019 groups\u2019 behavior is the simplest algorithm of evolutionary programming because for its implementation it is necessary to be able to determine only value of the optimized function [20], [21].", "startOffset": 357, "endOffset": 361}, {"referenceID": 18, "context": "The particle swarm algorithm (Particle Swarm Optimization, PSO algorithm), which is based on an idea of possibility to solve the optimization problems using modeling of animals\u2019 groups\u2019 behavior is the simplest algorithm of evolutionary programming because for its implementation it is necessary to be able to determine only value of the optimized function [20], [21].", "startOffset": 363, "endOffset": 367}, {"referenceID": 19, "context": "Along with the traditional approach to the application of the PSO algorithm a new approach, that implements the simultaneous search for the best type of the kernel function, values of the kernel function parameters and value of the regularization parameter, is offered [22].", "startOffset": 269, "endOffset": 273}, {"referenceID": 0, "context": "A herewith it is assumed that every object i z is mapped to q dimensional vector of numerical values of characteristics ) , , , ( 2 1 q i i i i z z z z \uf04b \uf03d (typically normalized by values from the interval [0, 1]) where l i z is the numeric value of the l -th", "startOffset": 206, "endOffset": 212}, {"referenceID": 19, "context": "characteristic for the i -th object ( s i , 1 \uf03d , q l , 1 \uf03d ) [22]\u2013[25].", "startOffset": 62, "endOffset": 66}, {"referenceID": 22, "context": "characteristic for the i -th object ( s i , 1 \uf03d , q l , 1 \uf03d ) [22]\u2013[25].", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "The SVM classifier with satisfactory training and testing results can be used to classify new objects [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "The separating hyperplane for the objects from the training set can be represented by equation 0 , \uf03d \uf02bb z w , where w is a vector-perpendicular to the separating hyperplane; b is a parameter which corresponds to the shortest distance from the origin of coordinates to the hyperplane; , w z is a scalar product of vectors w and z [1\u20133].", "startOffset": 329, "endOffset": 334}, {"referenceID": 1, "context": "The separating hyperplane for the objects from the training set can be represented by equation 0 , \uf03d \uf02bb z w , where w is a vector-perpendicular to the separating hyperplane; b is a parameter which corresponds to the shortest distance from the origin of coordinates to the hyperplane; , w z is a scalar product of vectors w and z [1\u20133].", "startOffset": 329, "endOffset": 334}, {"referenceID": 0, "context": "In the case of linear separability of classes we can choose a hyperplane so that there is no any object from the training set between them, and then maximize the distance between the hyperplanes (width of the strip) 2 , w w \uf03c \uf03e , solving the problem of quadratic optimization [1], [2]:", "startOffset": 276, "endOffset": 279}, {"referenceID": 0, "context": "The problem of the separating hyperplane building can be reformulated as the dual problem of searching a saddle point of the Lagrange function, which reduces to the problem of quadratic programming, containing only dual variables [1], [2]:", "startOffset": 230, "endOffset": 233}, {"referenceID": 0, "context": "A herewith typically one of the following functions is used as the kernel function ) , ( \uf074 \uf06b z zi [1], [3], [26]:", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "A herewith typically one of the following functions is used as the kernel function ) , ( \uf074 \uf06b z zi [1], [3], [26]:", "startOffset": 103, "endOffset": 106}, {"referenceID": 23, "context": "A herewith typically one of the following functions is used as the kernel function ) , ( \uf074 \uf06b z zi [1], [3], [26]:", "startOffset": 108, "endOffset": 112}, {"referenceID": 17, "context": "The coordinates of the i -th particle ( m i , 1 \uf03d ) in the n dimensional search space uniquely determine the value of the objective function ) , , , ( ) ( 2 1 n i i i i x x x f x f \uf04b \uf03d which is a certain solution of the optimization problem [20] \u2013 [22].", "startOffset": 241, "endOffset": 245}, {"referenceID": 19, "context": "The coordinates of the i -th particle ( m i , 1 \uf03d ) in the n dimensional search space uniquely determine the value of the objective function ) , , , ( ) ( 2 1 n i i i i x x x f x f \uf04b \uf03d which is a certain solution of the optimization problem [20] \u2013 [22].", "startOffset": 248, "endOffset": 252}, {"referenceID": 17, "context": "for the i -th particle ( m i , 1 \uf03d ) [20].", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "velocity vector ( n j , 1 \uf03d ) of the i -th particle ( m i , 1 \uf03d ) is made in accordance with formula [20]:", "startOffset": 101, "endOffset": 105}, {"referenceID": 17, "context": "In one of the most known canonical version of the PSO algorithm it is supposed to undertake the normalization of the acceleration coefficients \uf06a\u0302 and \uf06a~ to make the convergence of the algorithm not so much dependent on the choice of their values [20].", "startOffset": 246, "endOffset": 250}, {"referenceID": 17, "context": "When using formula (4) for correction of velocity vector the convergence of the PSO algorithm is guaranteed and there is no need to control the particle velocity explicitly [20].", "startOffset": 173, "endOffset": 177}, {"referenceID": 1, "context": "Quality evaluation of the SVM classifier can be executed with the use of different classification quality indicators [3].", "startOffset": 117, "endOffset": 120}, {"referenceID": 9, "context": "Moreover two testing data sets were used in experimental researches: Test [11] and \u041c\u041e\u0422\u041f12 (the source is http://machinelearning.", "startOffset": 74, "endOffset": 78}], "year": 2016, "abstractText": "The problem of development of the SVM classifier based on the modified particle swarm optimization has been considered. This algorithm carries out the simultaneous search of the kernel function type, values of the kernel function parameters and value of the regularization parameter for the SVM classifier. Such SVM classifier provides the high quality of data classification. The idea of particles' \u00abregeneration\u00bb is put on the basis of the modified particle swarm optimization algorithm. At the realization of this idea, some particles change their kernel function type to the one which corresponds to the particle with the best value of the classification accuracy. The offered particle swarm optimization algorithm allows reducing the time expenditures for development of the SVM classifier. The results of experimental studies confirm the efficiency of this algorithm. Keywords\u2014particle swarm optimization; SVM-classifier; kernel function type; kernel function parameters; regularization parameter; support vectors", "creator": "Microsoft\u00ae Word 2010"}}}