{"id": "1603.09381", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2016", "title": "Clinical Information Extraction via Convolutional Neural Network", "abstract": "second we might report an acceptable implementation example of targeting a clinical information extraction tool that largely leverages redundant deep neural network signals to naturally annotate event spans directly and their visual attributes from plain raw clinical experience notes and specialized pathology reports. our approach uses context words and structures their inverse part - matching of - speech tags emphasize and shape biomedical information as features. then clearly we hire temporal ( or 1d ) convolutional neural network nodes to quickly learn hidden feature representations. finally, we extensively use multilayer perceptron ( mlp ) to formally predict unique event spans. the ml empirical value evaluation profile demonstrates that understanding our standardized approach significantly outperforms baselines.", "histories": [["v1", "Wed, 30 Mar 2016 20:57:07 GMT  (95kb)", "http://arxiv.org/abs/1603.09381v1", "arXiv admin note: text overlap witharXiv:1408.5882by other authors"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1408.5882by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["peng li", "heng huang"], "accepted": false, "id": "1603.09381"}, "pdf": {"name": "1603.09381.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jerryli1981@gmail.com", "heng@uta.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 3.\n09 38\n1v 1\n[ cs\n.L G\n] 3\n0 M\nar 2\n01 6"}, {"heading": "1 Introduction", "text": "In the past few years, there has been much interest in applying neural network based deep learning techniques to solve all kinds of natural language processing (NLP) tasks. From low level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015) and sentence relation modeling tasks such as paraphrase identification and question answering (Socher et al., 2011b; Iyyer et al., 2014; Yin and Schutze, 2015). Deep representation learning has demonstrated its importance for these tasks. All the tasks get performance improvement via learning either word level representations or sentence level representations.\nIn this work, we brought deep representation learning technologies to the clinical domain. Specifically, we focus on clinical information extraction, using clinical notes and pathology reports\nfrom the Mayo Clinic. Our system will identify event expressions consisting of the following components:\n\u2022 The spans (character offsets) of the expression in the raw text\n\u2022 Contextual Modality: ACTUAL, HYPOTHETICAL, HEDGED or GENERIC\n\u2022 Degree: MOST, LITTLE or N/A\n\u2022 Polarity: POS or NEG\n\u2022 Type: ASPECTUAL, EVIDENTIAL or N/A\nThe input of our system consists of raw clinical notes or pathology reports like below:\nApril 23, 2014: The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bolus on Friday even if there is slight nausea.\nAnd output annotations over the text that capture the key information such as event mentions and attributes. Table 1 illustrates the output of clinical information extraction in details.\nTo solve this task, the major challenge is how to precisely identify the spans (character offsets) of the event expressions from raw clinical notes. Traditional machine learning approaches usually build a supervised classifier with features generated by the Apache clinical Text Analysis and Knowledge Extraction System (cTAKES) 1. For example, BluLab system (Velupillai et al., 2015) extracted morphological(lemma), lexical(token), and syntactic(part-of-speech) features encoded from cTAKES. Although using the domain specific information extraction tools can improve the performance, learning how to use it well for\n1Apache cTAKES is a natural language processing system for extraction of information from electronic medical record clinical free-text\nclinical domain feature engineering is still very time-consuming. In short, a simple and effective method that only leverage basic NLP modules and achieves high extraction performance is desired to save costs.\nTo address this challenge, we propose a deep neural networks based method, especially convolution neural network (Collobert et al., 2011), to learn hidden feature representations directly from raw clinical notes. More specifically, one method first extract a window of surrounding words for the candidate word. Then, we attach each word with their part-of-speech tag and shape information as extra features. Then our system deploys a temporal convolution neural network to learn hidden feature representations. Finally, our system uses Multilayer Perceptron (MLP) to predict event spans. Note that we use the same model to predict event attributes."}, {"heading": "2 Constructing High Quality Training Dataset", "text": "The major advantage of our system is that we only leverage NLTK 2 tokenization and a POS tagger to preprocess our training dataset. When implementing our neural network based clinical information extraction system, we found it is not easy to construct high quality training data due to the noisy format of clinical notes. Choosing the proper tokenizer is quite important for span identification. After several experiments, we found \u201dRegexpTokenizer\u201d can match our needs. This tokenizer can generate spans for each token via sophisticated regular expression like below,\nn l t k . t o k e n i z e . RegexpT oken ize r\n2http://www.nltk.org\n( \u201d\\w+ |\\ $ [\\ d \\ . ] + | \\ S+\u201d )\nWe then use \u201dPerceptronTagger\u201d as our part-ofspeech tagger due to its fast tagging speed. Note that when extracting context words, please make sure you deploy the same tokenization module instead of just splitting strings by space."}, {"heading": "3 Neural Network Classifier", "text": "Event span identification is the task of extracting character offsets of the expression in raw clinical notes. This subtask is quite important due to the fact that the event span identification accuracy will affect the accuracy of attribute identification. We first run our neural network classifier to identify event spans. Then, given each span, our system tries to identify attribute values."}, {"heading": "3.1 Temporal Convolutional Neural Network", "text": "The way we use temporal convlution neural network for event span and attribute classification is similar with the approach proposed by (Collobert et al., 2011). Generally speaking, we can consider a word as represented by K discrete features w \u2208 D1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7DK , where DK is the dictionary for the kth feature. In our scenario, we just use three features such as token mention, pos tag and word shape. Note that word shape features are used to represent the abstract letter pattern of the word by mapping lower-case letters to \u201cx\u201d, upper-case to \u201cX\u201d, numbers to \u201cd\u201d, and retaining punctuation. We associate to each feature a lookup table. Given a word, a feature vector is then obtained by concatenating all lookup table outputs. Then a clinical snippet is transformed into a word embedding matrix. The matrix can be fed to further 1-dimension convolutional neural network\nand max pooling layers. Below we will briefly introduce core concepts of Convoluational Neural Network (CNN).\nTemporal Convolution\nTemporal Convolution applies one-dimensional convolution over the input sequence. The onedimensional convolution is an operation between a vector of weights m \u2208 Rm and a vector of inputs viewed as a sequence x \u2208 Rn. The vector m is the filter of the convolution. Concretely, we think of x as the input sentence and xi \u2208 R as a single feature value associated with the i-th word in the sentence. The idea behind the one-dimensional convolution is to take the dot product of the vector m with each m-gram in the sentence x to obtain another sequence c:\ncj = m T xj\u2212m+1:j . (1)\nUsually, xi is not a single value, but a ddimensional word vector so that x \u2208 Rd\u00d7n. There exist two types of 1d convolution operations. One was introduced by (Waibel et al., 1989) and also known as Time Delay Neural Networks (TDNNs). The other one was introduced by (Collobert et al., 2011). In TDNN, weights m \u2208 Rd\u00d7m form a matrix. Each row of m is convolved with the corresponding row of x. In (Collobert et al., 2011) architecture, a sequence of length n is represented as:\nx1:n = x1 \u2295 x2 \u00b7 \u00b7 \u00b7 \u2295 xn , (2)\nwhere \u2295 is the concatenation operation. In general, let xi:i+j refer to the concatenation of words xi,xi+1, . . . ,xi+j . A convolution operation involves a filter w \u2208 Rhk, which is applied to a window of h words to produce the new feature. For example, a feature ci is generated from a window of words xi:i+h\u22121 by:\nci = f(w \u00b7 xi:i+h\u22121 + b) , (3)\nwhere b \u2208 R is a bias term and f is a non-linear function such as the hyperbolic tangent. This filter is applied to each possible window of words in the sequence {x1:h,x2:h+1, . . . ,xn\u2212h+1:n} to produce the feature map:\nc = [c1, c2, . . . , cn\u2212h+1] , (4)\nwhere c \u2208 Rn\u2212h+1.\nWe also employ dropout on the penultimate layer with a constraint on \u21132-norms of the weight vector. Dropout prevents co-adaptation of hidden units by randomly dropping out a proportion p of the hidden units during forwardbackpropagation. That is, given the penultimate layer z = [c\u03021, . . . , c\u0302m], instead of using:\ny = w \u00b7 z+ b (5)\nfor output unit y in forward propagation, dropout uses:\ny = w \u00b7 (z \u25e6 r) + b , (6)\nwhere \u25e6 is the element-wise multiplication operator and r \u2208 Rm is a masking vector of Bernoulli random variables with probability p of being 1. Gradients are backpropagated only through the unmasked units. At test step, the learned weight vectors are scaled by p such that w\u0302 = pw, and w\u0302 is used to score unseen sentences. We additionally constrain l2-norms of the weight vectors by rescaling w to have ||w||2 = s whenever ||w||2 > s after a gradient descent step."}, {"heading": "4 Experimental Results", "text": ""}, {"heading": "4.1 Dataset", "text": "We use the Clinical TempEval corpus 3 as the evaluation dataset. This corpus was based on a set of 600 clinical notes and pathology reports from cancer patients at the Mayo Clinic. These notes were manually de-identified by the Mayo Clinic to replace names, locations, etc. with generic placeholders, but time expression were not altered. The notes were then manually annotated with times, events and temporal relations in clinical notes. These annotations include time expression types, event attributes and an increased focus on temporal relations. The event, time and temporal relation annotations were distributed separately from the text using the Anafora standoff format. Table 2 shows the number of documents, event expressions in the training, development and testing portions of the 2016 THYME data."}, {"heading": "4.2 Evaluation Metrics", "text": "All of the tasks were evaluated using the standard metrics of precision(P), recall(R) and F1:\n3http://alt.qcri.org/semeval2016/task12/index.php?id=data\nP = |S \u2229H|\n|S|\nR = |S \u2229H|\n|H|\nF1 = 2 \u00b7 P \u00b7R\nP +R\n(7)\nwhere S is the set of items predicted by the system and H is the set of items manually annotated by the humans. Applying these metrics of the tasks only requires a definition of what is considered an \u201ditem\u201d for each task. For evaluating the spans of event expressions, items were tuples of character offsets. Thus, system only received credit for identifying events with exactly the same character offsets as the manually annotated ones. For evaluating the attributes of event expression types, items were tuples of (begin, end, value) where begin and end are character offsets and value is the value that was given to the relevant attribute. Thus, systems only received credit for an event attribute if they both found an event with correct character offsets and then assigned the correct value for that attribute (Bethard et al., 2015)."}, {"heading": "4.3 Hyperparameters and Training Details", "text": "Objective Function\nWe want to maximize the likelihood of the correct class. This is equivalent to minimizing the negative log-likelihood (NLL). More specifically, the label y\u0302 given the inputs xh is predicted by a softmax classifier that takes the hidden state hj as input:\np\u0302\u03b8(y|xh) = softmax(W \u00b7 xh + b)\ny\u0302 = argmax y\np\u0302\u03b8(y|xh) (8)\nAfter that, the objective function is the negative log-likelihood of the true class labels yk:\nJ(\u03b8) = \u2212 1\nm\nm\u2211\nk=1\nlog p\u0302\u03b8(y k|xkh) +\n\u03bb 2 ||\u03b8||22 , (9)\nwhere m is the number of training examples and the superscript k indicates the kth example.\nHyperparameters We use Lasagne 4 deep learning framework. We first initialize our word representations using publicly available 300-dimensional Glove word vectors 5. We deploy CNN model with kernel width of 2, a filter size of 300, sequence length is 2 \u2217 windows size+1, number filters is seqlen\u2212kw+ 1, stride is 1, pool size is seqlen\u2212filter size+1, cnn activation function is tangent, MLP activation function is sigmoid. MLP hidden dimension is 50. We initialize CNN weights using a uniform distribution. Finally, by stacking a softmax function on top, we can get normalized log-probabilities. Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule (Duchi et al., 2011). The learning rate is set to 0.05. The mini-batch size is 100. The model parameters were regularized with a perminibatch L2 regularization strength of 10\u22124."}, {"heading": "4.4 Results and Discussions", "text": "Table 3 shows results on the event expression tasks. Our initial submits RUN 4 and 5 outperformed the memorization baseline on every metric on every task. The precision of event span identification is close to the max report. However, our system got lower recall. One of the main reason is that our training objective function is accuracyoriented. Table 4 shows results on the phase 2 subtask."}, {"heading": "5 Conclusions", "text": "In this paper, we introduced a new clinical information extraction system that only leverage deep neural networks to identify event spans and their attributes from raw clinical notes. We trained deep neural networks based classifiers to extract clinical event spans. Our method attached each word\n4https://github.com/Lasagne/Lasagne 5http://nlp.stanford.edu/projects/glove/\nto their part-of-speech tag and shape information as extra features. We then hire temporal convolution neural network to learn hidden feature representations. The entire experimental results demonstrate that our approach consistently outperforms the existing baseline methods on standard evaluation datasets.\nOur research proved that we can get competitive results without the help of a domain specific feature extraction toolkit, such as cTAKES. Also we only leverage basic natural language processing modules such as tokenization and part-ofspeech tagging. With the help of deep representation learning, we can dramatically reduce the cost of clinical information extraction system development."}], "references": [{"title": "Semeval-2015 task 6: Clinical tempeval", "author": ["Leon Derczynski", "Guergana Savova", "James Pustejovsky", "Marc Verhagen"], "venue": "In International Workshop on Semantic Evaluation", "citeRegEx": "Bethard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bethard et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["Iyyer et al.2014] Mohit Iyyer", "Jordan Boyd-Graber", "Leonardo Claudino", "Richard Socher", "Hal Daum\u00e9 III"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Iyyer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Iyyer et al\\.", "year": 2014}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Sys-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning"], "venue": "In Proceedings of the Conference on Empiri-", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Blulab: Temporal information extraction for the 2015 clinical tempeval challenge", "author": ["Danielle L Mowery", "Samir Abdelrahman", "Lee Christensen", "Wendy W Chapman"], "venue": "In International Workshop on Semantic", "citeRegEx": "Velupillai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Velupillai et al\\.", "year": 2015}, {"title": "Phoneme recognition using time-delay", "author": ["Toshiyuki Hanazawa", "Geoffrey Hinton", "Kiyohiro Shikano", "Kevin J Lang"], "venue": null, "citeRegEx": "Waibel et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Waibel et al\\.", "year": 1989}, {"title": "Multigrancnn: An architecture for general matching of text chunks on multiple levels of granularity", "author": ["Yin", "Schutze2015] Wenpeng Yin", "Hinrich Schutze"], "venue": "In Proceedings of th 53rd Annual Meeting of the Association", "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "From low level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al.", "startOffset": 114, "endOffset": 160}, {"referenceID": 5, "context": "From low level tasks such as language modeling, POS tagging, named entity recognition, and semantic role labeling (Collobert et al., 2011; Mikolov et al., 2013), to high level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al.", "startOffset": 114, "endOffset": 160}, {"referenceID": 8, "context": ", 2013), to high level tasks such as machine translation, information retrieval, semantic analysis (Kalchbrenner and Blunsom, 2013; Socher et al., 2011a; Tai et al., 2015) and sentence relation modeling tasks such as paraphrase identification and question answering (Socher et al.", "startOffset": 99, "endOffset": 171}, {"referenceID": 3, "context": ", 2015) and sentence relation modeling tasks such as paraphrase identification and question answering (Socher et al., 2011b; Iyyer et al., 2014; Yin and Schutze, 2015).", "startOffset": 102, "endOffset": 167}, {"referenceID": 9, "context": "For example, BluLab system (Velupillai et al., 2015) extracted morphological(lemma), lexical(token), and syntactic(part-of-speech) features encoded from cTAKES.", "startOffset": 27, "endOffset": 52}, {"referenceID": 1, "context": "To address this challenge, we propose a deep neural networks based method, especially convolution neural network (Collobert et al., 2011), to learn hidden feature representations directly from raw clinical notes.", "startOffset": 113, "endOffset": 137}, {"referenceID": 1, "context": "The way we use temporal convlution neural network for event span and attribute classification is similar with the approach proposed by (Collobert et al., 2011).", "startOffset": 135, "endOffset": 159}, {"referenceID": 10, "context": "One was introduced by (Waibel et al., 1989) and also known as Time Delay Neural Networks (TDNNs).", "startOffset": 22, "endOffset": 43}, {"referenceID": 1, "context": "The other one was introduced by (Collobert et al., 2011).", "startOffset": 32, "endOffset": 56}, {"referenceID": 1, "context": "In (Collobert et al., 2011) architecture, a sequence of length n is represented as:", "startOffset": 3, "endOffset": 27}, {"referenceID": 0, "context": "Thus, systems only received credit for an event attribute if they both found an event with correct character offsets and then assigned the correct value for that attribute (Bethard et al., 2015).", "startOffset": 172, "endOffset": 194}, {"referenceID": 2, "context": "Training is done through stochastic gradient descent over shuffled mini-batches with the AdaGrad update rule (Duchi et al., 2011).", "startOffset": 109, "endOffset": 129}], "year": 2016, "abstractText": "We report an implementation of a clinical information extraction tool that leverages deep neural network to annotate event spans and their attributes from raw clinical notes and pathology reports. Our approach uses context words and their partof-speech tags and shape information as features. Then we hire temporal (1D) convolutional neural network to learn hidden feature representations. Finally, we use Multilayer Perceptron (MLP) to predict event spans. The empirical evaluation demonstrates that our approach significantly outperforms baselines.", "creator": "LaTeX with hyperref package"}}}