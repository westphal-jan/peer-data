{"id": "1604.00117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2016", "title": "Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding", "abstract": "improving the goal of this short paper is aiming to use quantitative multi - task learning to reasonably efficiently accurately scale slot filling models accountable for natural speech language competence understanding to constantly handle multiple individually target given tasks or domains. the important key to scalability efficiency is of reducing practically the unnecessary amount of training capacity data resources needed to keep learn constructing a model for a valid new domain task. the long proposed constrained multi - processing task vocabulary model delivers better performance compatibility with drawing less data by also leveraging patterns that target it learns from the one other tasks. reducing the approach supports an open vocabulary, which sometimes allows the models to generalize to unseen words, which is particularly useful important when very little training data is used. a newly collected project crowd - sourced data binding set, covering four different domains, prototype is used to demonstrate the effectiveness of the information domain adaptation and open global vocabulary techniques.", "histories": [["v1", "Fri, 1 Apr 2016 03:24:32 GMT  (138kb,D)", "http://arxiv.org/abs/1604.00117v1", null], ["v2", "Wed, 10 Aug 2016 02:53:00 GMT  (174kb,D)", "http://arxiv.org/abs/1604.00117v2", "Interspeech 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aaron jaech", "larry heck", "mari ostendorf"], "accepted": false, "id": "1604.00117"}, "pdf": {"name": "1604.00117.pdf", "metadata": {"source": "CRF", "title": "Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding", "authors": ["Aaron Jaech", "Larry Heck", "Mari Ostendorf"], "emails": ["ajaech@uw.edu,", "larryheck@google.com,", "ostendor@uw.edu"], "sections": [{"heading": "1. Introduction", "text": "Slot filling models are a useful method for simple natural language understanding tasks, where information can be extracted from a sentence and used to perform some structured action. For example, dates, departure cities and destinations represent slots to fill in a flight booking task. This information is extracted from natural language queries leveraging typical context associated with each slot type. Researchers have been exploring datadriven approaches to learning models for automatic identification of slot information since the 90\u2019s, and significant advances have been made [1].\nOur paper builds on recent work on slot-filling using recurrent neural networks (RNNs) with a focus on the problem of training from minimal annotated data, taking an approach of sharing data from multiple tasks to reduce the amount of data for developing a new task.\nAs candidate tasks, we consider the actions that a user might perform via apps on their phone. Typically, a separate slot-filling model would be trained for each app. For example, one model understands queries about classified ads for cars [2] and another model handles queries about the weather [3]. As the number of apps increases, this approach becomes impractical due to the burden of collecting and labeling the training data for each model. In addition, using independent models for each task has high storage costs for mobile devices.\nAlternatively, a single model can be learned to handle all of the apps. This type of approach is known as multi-task learning and can lead to improved performance on all of the tasks due to information sharing between the different apps [?]. Multi-task learning in combination with neural networks has been show to be effective for natural language processing tasks [4]. When using RNNs for slot filling, almost all of the model parameters can\nbe shared between tasks. In our study, only the relatively small output layer, which consists of slot embeddings, is individual to each app. More sharing means that less training data per app can be used and there will still be enough data to effectively train the network. The multi-task approach has lower data requirements, which leads to a large cost savings and makes this approach scalable to large numbers of applications.\nThe shared representation that we build on leverages recent work on slot filling models that use neural network based approaches. Early neural network based papers propose feedforward [5] or RNN architectures [6, 7]. The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12]. The most recent papers use variations on LSTM sequence models, including encoder-decoder, external memory, or attention architectures [13, 14, 15]. The particular variant that we build on is a bidirectional LSTM, similar to [16, 11].\nOne highly desirable property of a good slot filling model is to generalize to previously unseen slot values. For instance, we should not expect that the model will see the names of all the cities during training time, especially when only a small amount of training data is used. We address the generalizability issue by incorporating the open vocabulary embeddings from Ling et al. into our model [17]. These embeddings work by using a character RNN to process a word one letter at a time. This way the model can learn to share parameters between different words that use the same morphemes. For example BBQ restaurants frequently use words like \u201csmokehouse\u201d, \u201csteakhouse\u201d, and \u201croadhouse\u201d in their names and \u201cBayside\u201d,\u201cBayview\u201d, and \u201cBaywood\u201d are all streets in San Francisco. Recognizing these patterns would be helpful in detecting a restaurant or street name slot, respectively.\nThe two main contributions of this work are the multi-task model and the use of the open vocabulary character-based embeddings, which together allow for scalable slot filling models. Our work on multi-task learning in slot filling differs from its previous use in [18] in that we allow for soft sharing between tasks instead of explicitly matching slots to each other across different tasks. A limitation of explicit slot matching is that two slots that appear to have the same underlying type, such as location-based slots, may actually use the slot information in different ways depending on the overall intent of the task. In our model, the sharing between tasks is done implicitly by the neural network. Our approach to handling words unseen in training data is different from the delexicalization proposed in [19] in that we do not require the vocabulary items associated with slots and values to be prespecified.\nThe proposed model is described in more detail in Section 2. The approach is assessed on a new data collection based\nar X\niv :1\n60 4.\n00 11\n7v 1\n[ cs\n.C L\n] 1\nA pr\n2 01\n6\non four apps, described in Section 3. The experiments described in Section 4 investigate how much data is needs to be collected for the n-th app using a multi-task model that leverages the data from the previous n\u22121 apps, with results compared against the single-task model that only utilizes the data from the n-th app. We conclude in Section 5 with a summary of the key findings and discussion of opportunities for future work."}, {"heading": "2. Model", "text": "Our model has a word embedding layer, followed by a bidirectional LSTM (bi-LSTM), and a softmax output layer. The bi-LSTM allows the model to use information from both the right and left contexts of each word when making predictions. We choose this architecture because similar models have been used in prior work on slot filling and have achieved good results [16, 11]. The LSTM gates are used as defined by Sak et al. including the use of the linear projection layer on the output of the LSTM [20]. The purpose of the projection layer is to produce a model with fewer parameters without reducing the number of LSTM memory cells.\nFor the multi-task model, the word embeddings and the biLSTM parameters are shared across tasks but each task has its own softmax layer. This means that if the multi-task model has half a million parameters, only a couple thousand of them are unique to each task and the other 99.5% are shared between all of the tasks.\nThe slot labels are encoded in BIO format [21] indicating if a word is the beginning, inside or outside any particular slot. Decoding is done greedily. If a label does not follow the BIO syntax rules, i.e. an inside tag must follow the appropriate begin tag, then it is replaced with the outside label. Evaluation is done using the CoNLL evaluation script [22] to calculate the F1 score. This is the standard way of evaluating slot-filling models in the literature.\nIn recent work on language modeling, a neural architecture that combined fixed word embeddings with character-based embeddings was found to to be useful for handling previously unseen words [23]. Based on that result, the embeddings in the open vocabulary model are a concatenation of the characterbased embeddings with fixed word embeddings. When an outof-vocabulary word is encountered, its character-based embedding is concatenated with the embedding for the unknown word token. The character-based embeddings are generted from a two layer bi-LSTM that processes each word one character at a time. The character-based word embedding is produced by concatenating the last states from each of the directional LSTM\u2019s in the second layer and passing them through a linear layer for dimensionality reduction."}, {"heading": "3. Data", "text": "Crowd-sourced data was collected simulating common use cases for four different apps: United Airlines, Airbnb, Greyhound bus service and OpenTable. The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant. In order to elicit natural language, crowd workers were instructed to simulate a conversation with a friend planning an activity as opposed to giving a command to the computer. Workers were prompted with a slot type/value pair and asked to form a reply to their friend using that information. The instructions were to not include any other potential slots in the sentence but this instruction was not always followed by the workers.\nSlot types were chosen to roughly correspond to form fields and UI elements, such as check boxes or dropdown menus, on the respective apps. The amount of data collected per app and the number of slot types is listed in Table 1. The slot types for each app are described in Table 2, and an example labeled sentence from each app is given in Table 3. One thing to notice is that the the number of slot types is relatively small when compared to the popular ATIS dataset that has over one hundred slot types [1]. In ATIS, separate slot types would be used for names of cities, states, or countries whereas in this data all of those would fall under a single slot for locations.\nSlot values were pulled from manually created lists of locations, dates and times, restaurants, etc. Values for prompting each rater were sampled from these lists. Workers were instructed to try and use different re-phrasings of the prompted values but most people used the prompted value verbatim. Occasionally, workers used an unprompted slot value that was not in the list.\nFor the word-level LSTM, the data was lower-cased and tokenized using a standard tokenizer. Spelling mistakes were not corrected. All digits were replaced by the \u2019#\u2019 character. Words that appear only once in the training data are replaced with an unknown word token. For the character-based word embeddings used in the open vocabulary model, no lower casing or digit replacement is done.\nDue to the way the OpenTable data was collected some slot values were over-represented leading to over fitting to those particular values. To correct this problem sentences that used the over-represented slot values had their values replaced by sampling from a larger list of potential values. The affected slot types are the ones for cuisine, restaurant names, and locations. This substitution made the OpenTable data more realistic as well as more similar to the other data that was collected.\nThe data we collected for the United Airlines app is an exception in a few ways: we collected four times as much data for this app than the other ones; workers were occasionally\nprompted with up to four slot type/value pairs; and workers were instructed to give commands to their device instead of simulating a conversation with a friend. For all of the other apps, workers were prompted to use a single slot type per sentence. We argue that having varying amounts of data for different apps is a realistic scenario.\nAnother possible source of data is the Air Travel Information Service (ATIS) data set collected in the early 1990\u2019s [1]. However, this data is sufficiently similar to the United collection, that it is not likely to add sufficient variety to improve the target domains. Further, it suffers from artifacts of data collected at a time with speech recognition systems had much higher error rates. The new data collected for this work fills a need raised in [24], which concluded that lack of data was an impediment to progress in slot filling."}, {"heading": "4. Experiments", "text": "The section describes two sets of experiments: the first is designed to test the effectiveness of the multi-task model and the second is designed to test the generalizability of the open vocabulary model. The scenario is that we already have n \u2212 1 models in place and we wish to discover how much data will be necessary to build a model for an additional application."}, {"heading": "4.1. Training and Model Configuration Details", "text": "The data is split to use 30% for training with 70% to be used for test data. The reason that a majority of the data is used for testing is that in the second experiment the results are reported separately for sentences containing out of vocabulary tokens and a large amount of data is needed to get a sufficient sample size. Hyperparameter tuning presents a challenge when operating in a low resource scenario. When there is barely enough data to train the model none can be spared for a validation set. We used data from the United app for hyperparameter tuning since it is the largest and assumed that the hyperparameter settings generalized to the other apps.\nTraining is done using stochastic gradient descent with minibatches of 25 sentences. The initial learning rate is 0.3 and is set to decay to 98% of its value every 100 minibatches. For the multi-task model, training proceeds by alternating between each of the tasks when selecting the next minibatch. All the parameters are initialized uniformly in the range [-0.1, 0.1]. Dropout is used for regularization on the word embeddings and on the outputs from each LSTM layer with the dropout probability set to 60% [25].\nFor the single-task model, the word embeddings are 60 dimensional and the LSTM is dimension 100 with a 70 dimensional projection layer on the LSTM. For the multi-task model, word embeddings are 200 dimensional, and the LSTM has 250 dimensions with a 170 dimensional projection layer. For the open vocabulary version of the model, the 200-dimensional input is a concatenation of 160-dimensional traditional word embeddings with 40-dimensional character-based word embed-\ndings. The character embedding layer is 15 dimensions, the first LSTM layer is 40 dimensions with a 20 dimensional projection layer, and the second LSTM layer is 130 dimensions."}, {"heading": "4.2. Multi-task Model Experiments", "text": "We compare a single-task model against the multi-task model for varying amounts of training data. In the multi-task model, the full amount of data is used for n \u2212 1 apps and the amount of data is allowed to vary only for the n-th application. These experiments use the traditional word embeddings with a closed vocabulary. Since the data for the United app is bigger than the other three apps combined, it is used as an anchor for the multi-task model. The other three apps alternate in the position of the n-th app. The data usage for the n-th app is varied while the other n\u2212 1 apps in each experiment use the full amount of available training data. The full amount of training data is different for each app. The data used for the n-th app is 200, 400, or 800 sentences or all available training data depending on the experiment. The test set remains fixed for all of the experiments even as part of the training data is discarded to simulate the low resource scenario.\nIn Figure 1 we show the single-task vs. multi-task model performance for each of three different applications. The multitask model outperforms the single-task model at all data sizes, and the relative performance increases as the size of the training data decreases. When only 200 sentences of training data are used, the performance of the multi-task model is about 60% better than the single-task model for both the Airbnb and Greyhound apps. The relative gain for the OpenTable app is 26%. Because the performance of the multi-task model decays much more slowly as the amount of training data is reduced, the multitask model can deliver the same performance with a considerable reduction in the amount of labeled data."}, {"heading": "4.3. Open Vocabulary Model Experiments", "text": "The open vocabulary model experiments test the ability of the model to handle unseen words in test time, which are particularly likely to occur when using a reduced amount of training data. In these experiments the open vocabulary model is compared against the fixed embedding model. The results are\nreported separately for the sentences that contain out of vocabulary tokens, since these are where the open vocabulary system is expected to have an advantage.\nFigure 2 gives the OOV rate for each app for varying amounts of training data. Since the vocabulary contains all words that appear at least twice in the multi-task training data, the OOV words here tend to be task-specific terminology. For example, the OpenTable task is the only one that has names of restaurants but names of cities are present in all four tasks so they tend to be covered better. The OOV rate dramatically increases when the size of the training data is less than 500 sentences. Since our goal is to operate in the regime of less than 500 sentences per task, handling OOVs is a priority.\nThe model used in these experiments is the multi-task model from the previous experiments. The only difference between the closed vocabulary and open vocabulary systems is that the closed vocabulary system uses the traditional word embeddings and the open vocabulary system uses the traditional word embeddings concatenated with character-based embeddings.\nTable 4 reports F1 scores on the test set for both the closed vocabulary and open vocabulary systems. The results differ between the different tasks, but none of the tasks benefit outright from the open vocabulary system. Looking only at the subset of sentences that contain an OOV token, the open vocabulary system delivers increased performance on the Airbnb and Greyhound tasks. These two are the most difficult apps out of the set of four and therefore had the most room for improvement. The United app is also all lower case and casing is an important clue for detecting proper nouns that the open vocabulary model takes advantage of.\nLooking a little deeper, in Figure 3 we show the breakdown in performance across individual slot types. Only those slot types which occur at least one hundred times in the test data are\nshown in this figure. The slot types that are above the diagonal saw a performance improvement using the open vocabulary model. The opposite is true for those that are below the diagonal. It appears that the open vocabulary system does worse on slots that express quantities, dates and times and better on slots with greater slot perplexity (i.e., greater variation in slot values) like ones relating to locations. The three slots where the open vocabulary model gave the biggest improvement are the Greyhound LeavingFrom and GoingTo slots along with the Airbnb Amenities slot. The three slots where the open vocabulary model did the worst relative to the closed vocabulary model are the Airbnb Price slot along with the Greyhound DiscountType and DepartDate slots. The Amenities slot is an example of a slot with higher perplexity (with options related to pets, availability of a gym, parking, fire extinguishers, proximity to attractions), and the DiscountType is one with lower perplexity (three options cover almost all cases). We hypothesize that the reason that the numerical slots are better under the closed vocabulary model is due to their relative simplicity and not an inability of the character embeddings to learn representations for numbers."}, {"heading": "5. Conclusions", "text": "In summary, we find that using a multi-task model with shared embeddings gives a large reduction in the minimum amount of data needed to train a slot-filling model for a new app. This translates into a cost savings for deploying slot filling models for new applications. The combination of the multi-task model with the open vocabulary embeddings increases the generalizability of the model especially when there are OOVs in the sentence. These two contributions allow for scalable slot filling models.\nFor future work, there are some improvements that could be made to the model such as the addition of an attentional mechanism to help with long distance dependencies [15], use of beam-search to improve decoding, and exploring unsupervised adaptation as in [19].\nAnother item for future work is to collect additional tasks to examine the scalability of the multi-task model beyond the four applications that were used in this work. Due to their extra depth, character-based methods usually require more data than word based models [26]. Since this paper uses limited data, the collection of additional tasks may significantly improve the performance of the open vocabulary model."}, {"heading": "6. References", "text": "[1] P. Price, \u201cEvaluation of spoken language systems: The ATIS do-\nmain,\u201d in Proc. of the DARPA Speech and Natural Language Workshop. Morgan Kaufmann, 1990, pp. 91\u201395.\n[2] H. Meng, S. Busayapongchai, J. Giass, D. Goddeau, L. Hethetingron, E. Hurley, C. Pao, J. Polifroni, S. Seneff, and V. Zue, \u201cWheels: A conversational system in the automobile classifieds domain,\u201d in Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, vol. 1, 1996, pp. 542\u2013545.\n[3] J. R. Glass and T. J. Hazen, \u201cTelephone-based conversational speech recognition in the JUPITER domain.\u201d in ICSLP, vol. 98, 1998, pp. 1327\u20131330.\n[4] R. Collobert and J. Weston, \u201cA unified architecture for natural language processing: Deep neural networks with multitask learning,\u201d in Proc. of the International Conference on Machine learning. ACM, 2008, pp. 160\u2013167.\n[5] A. Deoras and R. Sarikaya, \u201cDeep belief network based semantic taggers for spoken language understanding.\u201d in Proc. Interspeech, 2013, pp. 2713\u20132717.\n[6] G. Mesnil, X. He, L. Deng, and Y. Bengio, \u201cInvestigation of recurrent-neural-network architectures and learning methods for spoken language understanding.\u201d in Proc. Interspeech, 2013, pp. 3771\u20133775.\n[7] K. Yao, G. Zweig, M.-Y. Hwang, Y. Shi, and D. Yu, \u201cRecurrent neural networks for language understanding.\u201d in Proc. Interspeech, 2013, pp. 2524\u20132528.\n[8] P. Xu and R. Sarikaya, \u201cConvolutional neural network based triangular crf for joint intent detection and slot filling,\u201d in Proc. of the IEEE Automatic Speech Recognition and Understanding, 2013, pp. 78\u201383.\n[9] K. Yao, B. Peng, Y. Zhang, D. Yu, G. Zweig, and Y. Shi, \u201cSpoken language understanding using long short-term memory neural networks,\u201d in Proc. of the IEEE Spoken Language Technology Workshop, 2014, pp. 189\u2013194.\n[10] Y. Shi, K. Yao, H. Chen, Y.-C. Pan, M.-Y. Hwang, and B. Peng, \u201cContextual spoken language understanding using recurrent neural networks,\u201d in Proc. ICASSP, 2015, pp. 5271\u20135275.\n[11] G. Mesnil, Y. Dauphin, K. Yao, Y. Bengio, L. Deng, D. HakkaniTur, X. He, L. Heck, G. Tur, D. Yu et al., \u201cUsing recurrent neural networks for slot filling in spoken language understanding,\u201d IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 530\u2013539, 2015.\n[12] M. Sundermeyer, R. Schlu\u0308ter, and H. Ney, \u201cLstm neural networks for language modeling.\u201d in Proc. Interspeech, 2012, pp. 194\u2013197.\n[13] B. Peng, K. Yao, L. Jing, and K.-F. Wong, \u201cRecurrent neural networks with external memory for spoken language understanding,\u201d in Natural Language Processing and Chinese Computing. Springer, 2015, pp. 25\u201335.\n[14] G. Kurata, B. Xiang, B. Zhou, and M. Yu, \u201cLeveraging sentencelevel information with encoder LSTM for natural language understanding,\u201d arXiv preprint arXiv:1601.01530, 2016.\n[15] L. Dong and M. Lapata, \u201cLanguage to logical form with neural attention,\u201d arXiv preprint arXiv:1601.01280, 2016.\n[16] T. N. Vu, P. Gupta, H. Adel, and H. Schu\u0308tze, \u201cBi-directional recurrent neural network with ranking loss for spoken language understanding.\u201d 2016.\n[17] W. Ling, T. Lu\u0131\u0301s, L. Marujo, R. F. Astudillo, S. Amir, C. Dyer, A. W. Black, and I. Trancoso, \u201cFinding function in form: Compositional character models for open vocabulary word representation,\u201d arXiv preprint arXiv:1508.02096, 2015.\n[18] X. Li, Y.-Y. Wang, and G. Tu\u0308r, \u201cMulti-task learning for spoken language understanding with shared slots.\u201d in Proc. Interspeech, 2011.\n[19] M. Henderson, B. Thomson, and S. Young, \u201cRobust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation,\u201d in Prox. of the IEEE Spoken Language Technology Workshop, 2014, pp. 360\u2013365.\n[20] H. Sak, A. W. Senior, and F. Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling.\u201d in Proc. Interspeech, 2014, pp. 338\u2013342.\n[21] L. A. Ramshaw and M. P. Marcus, \u201cText chunking using transformation-based learning,\u201d 1995, pp. 82\u201394.\n[22] E. F. Tjong Kim Sang and S. Buchholz, \u201cIntroduction to the conll2000 shared task: Chunking,\u201d in Proc. of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning-Volume 7, 2000, pp. 127\u2013132.\n[23] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu, \u201cExploring the limits of language modeling,\u201d arXiv preprint arXiv:1602.02410, 2016.\n[24] G. Tur, D. Hakkani-Tur, and L. Heck, \u201cWhat is left to be understood in ATIS?\u201d in Proc. of the IEEE Spoken Language Technology Workshop (SLT), 2010, pp. 19\u201324.\n[25] W. Zaremba, I. Sutskever, and O. Vinyals, \u201cRecurrent neural network regularization,\u201d arXiv preprint arXiv:1409.2329, 2014.\n[26] R. K. Srivastava, K. Greff, and J. Schmidhuber, \u201cHighway networks,\u201d arXiv preprint arXiv:1505.00387, 2015."}], "references": [{"title": "Evaluation of spoken language systems: The ATIS domain", "author": ["P. Price"], "venue": "Proc. of the DARPA Speech and Natural Language Workshop. Morgan Kaufmann, 1990, pp. 91\u201395.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Wheels: A conversational system in the automobile classifieds domain", "author": ["H. Meng", "S. Busayapongchai", "J. Giass", "D. Goddeau", "L. Hethetingron", "E. Hurley", "C. Pao", "J. Polifroni", "S. Seneff", "V. Zue"], "venue": "Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on, vol. 1, 1996, pp. 542\u2013545.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Telephone-based conversational speech recognition in the JUPITER domain.", "author": ["J.R. Glass", "T.J. Hazen"], "venue": "in ICSLP, vol", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proc. of the International Conference on Machine learning. ACM, 2008, pp. 160\u2013167.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep belief network based semantic taggers for spoken language understanding.", "author": ["A. Deoras", "R. Sarikaya"], "venue": "in Proc. Interspeech,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding.", "author": ["G. Mesnil", "X. He", "L. Deng", "Y. Bengio"], "venue": "in Proc. Interspeech,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Recurrent neural networks for language understanding.", "author": ["K. Yao", "G. Zweig", "M.-Y. Hwang", "Y. Shi", "D. Yu"], "venue": "in Proc. Interspeech,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Convolutional neural network based triangular crf for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "Proc. of the IEEE Automatic Speech Recognition and Understanding, 2013, pp. 78\u201383.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["K. Yao", "B. Peng", "Y. Zhang", "D. Yu", "G. Zweig", "Y. Shi"], "venue": "Proc. of the IEEE Spoken Language Technology Workshop, 2014, pp. 189\u2013194.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Contextual spoken language understanding using recurrent neural networks", "author": ["Y. Shi", "K. Yao", "H. Chen", "Y.-C. Pan", "M.-Y. Hwang", "B. Peng"], "venue": "Proc. ICASSP, 2015, pp. 5271\u20135275.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani- Tur", "X. He", "L. Heck", "G. Tur", "D. Yu"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 530\u2013539, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Lstm neural networks for language modeling.", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "in Proc. Interspeech,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Recurrent neural networks with external memory for spoken language understanding", "author": ["B. Peng", "K. Yao", "L. Jing", "K.-F. Wong"], "venue": "Natural Language Processing and Chinese Computing. Springer, 2015, pp. 25\u201335.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Leveraging sentencelevel information with encoder LSTM for natural language understanding", "author": ["G. Kurata", "B. Xiang", "B. Zhou", "M. Yu"], "venue": "arXiv preprint arXiv:1601.01530, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Language to logical form with neural attention", "author": ["L. Dong", "M. Lapata"], "venue": "arXiv preprint arXiv:1601.01280, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Bi-directional recurrent neural network with ranking loss for spoken language understanding.", "author": ["T.N. Vu", "P. Gupta", "H. Adel", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["W. Ling", "T. Lu\u0131\u0301s", "L. Marujo", "R.F. Astudillo", "S. Amir", "C. Dyer", "A.W. Black", "I. Trancoso"], "venue": "arXiv preprint arXiv:1508.02096, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust dialog state tracking using delexicalised recurrent neural networks and unsupervised adaptation", "author": ["M. Henderson", "B. Thomson", "S. Young"], "venue": "Prox. of the IEEE Spoken Language Technology Workshop, 2014, pp. 360\u2013365.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling.", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "in Proc. Interspeech,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Text chunking using transformation-based learning", "author": ["L.A. Ramshaw", "M.P. Marcus"], "venue": "1995, pp. 82\u201394.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Introduction to the conll- 2000 shared task: Chunking", "author": ["E.F. Tjong Kim Sang", "S. Buchholz"], "venue": "Proc. of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning-Volume 7, 2000, pp. 127\u2013132.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "arXiv preprint arXiv:1602.02410, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "What is left to be understood in ATIS?", "author": ["G. Tur", "D. Hakkani-Tur", "L. Heck"], "venue": "in Proc. of the IEEE Spoken Language Technology Workshop (SLT),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Highway networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1505.00387, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Researchers have been exploring datadriven approaches to learning models for automatic identification of slot information since the 90\u2019s, and significant advances have been made [1].", "startOffset": 178, "endOffset": 181}, {"referenceID": 1, "context": "For example, one model understands queries about classified ads for cars [2] and another model handles queries about the weather [3].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "For example, one model understands queries about classified ads for cars [2] and another model handles queries about the weather [3].", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "Multi-task learning in combination with neural networks has been show to be effective for natural language processing tasks [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "Early neural network based papers propose feedforward [5] or RNN architectures [6, 7].", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "Early neural network based papers propose feedforward [5] or RNN architectures [6, 7].", "startOffset": 79, "endOffset": 85}, {"referenceID": 6, "context": "Early neural network based papers propose feedforward [5] or RNN architectures [6, 7].", "startOffset": 79, "endOffset": 85}, {"referenceID": 7, "context": "The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12].", "startOffset": 69, "endOffset": 83}, {"referenceID": 8, "context": "The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12].", "startOffset": 69, "endOffset": 83}, {"referenceID": 9, "context": "The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12].", "startOffset": 69, "endOffset": 83}, {"referenceID": 10, "context": "The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12].", "startOffset": 69, "endOffset": 83}, {"referenceID": 11, "context": "The focus shifted to RNN\u2019s with long-short term memory cells (LSTMs) [8, 9, 10, 11] after LSTMs were shown to be effective for other tasks [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "The most recent papers use variations on LSTM sequence models, including encoder-decoder, external memory, or attention architectures [13, 14, 15].", "startOffset": 134, "endOffset": 146}, {"referenceID": 13, "context": "The most recent papers use variations on LSTM sequence models, including encoder-decoder, external memory, or attention architectures [13, 14, 15].", "startOffset": 134, "endOffset": 146}, {"referenceID": 14, "context": "The most recent papers use variations on LSTM sequence models, including encoder-decoder, external memory, or attention architectures [13, 14, 15].", "startOffset": 134, "endOffset": 146}, {"referenceID": 15, "context": "The particular variant that we build on is a bidirectional LSTM, similar to [16, 11].", "startOffset": 76, "endOffset": 84}, {"referenceID": 10, "context": "The particular variant that we build on is a bidirectional LSTM, similar to [16, 11].", "startOffset": 76, "endOffset": 84}, {"referenceID": 16, "context": "into our model [17].", "startOffset": 15, "endOffset": 19}, {"referenceID": 17, "context": "Our approach to handling words unseen in training data is different from the delexicalization proposed in [19] in that we do not require the vocabulary items associated with slots and values to be prespecified.", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "We choose this architecture because similar models have been used in prior work on slot filling and have achieved good results [16, 11].", "startOffset": 127, "endOffset": 135}, {"referenceID": 10, "context": "We choose this architecture because similar models have been used in prior work on slot filling and have achieved good results [16, 11].", "startOffset": 127, "endOffset": 135}, {"referenceID": 18, "context": "including the use of the linear projection layer on the output of the LSTM [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "The slot labels are encoded in BIO format [21] indicating if a word is the beginning, inside or outside any particular slot.", "startOffset": 42, "endOffset": 46}, {"referenceID": 20, "context": "Evaluation is done using the CoNLL evaluation script [22] to calculate the F1 score.", "startOffset": 53, "endOffset": 57}, {"referenceID": 21, "context": "In recent work on language modeling, a neural architecture that combined fixed word embeddings with character-based embeddings was found to to be useful for handling previously unseen words [23].", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "One thing to notice is that the the number of slot types is relatively small when compared to the popular ATIS dataset that has over one hundred slot types [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "Another possible source of data is the Air Travel Information Service (ATIS) data set collected in the early 1990\u2019s [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 22, "context": "The new data collected for this work fills a need raised in [24], which concluded that lack of data was an impediment to progress in slot filling.", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "Dropout is used for regularization on the word embeddings and on the outputs from each LSTM layer with the dropout probability set to 60% [25].", "startOffset": 138, "endOffset": 142}, {"referenceID": 14, "context": "For future work, there are some improvements that could be made to the model such as the addition of an attentional mechanism to help with long distance dependencies [15], use of beam-search to improve decoding, and exploring unsupervised adaptation as in [19].", "startOffset": 166, "endOffset": 170}, {"referenceID": 17, "context": "For future work, there are some improvements that could be made to the model such as the addition of an attentional mechanism to help with long distance dependencies [15], use of beam-search to improve decoding, and exploring unsupervised adaptation as in [19].", "startOffset": 256, "endOffset": 260}, {"referenceID": 24, "context": "Due to their extra depth, character-based methods usually require more data than word based models [26].", "startOffset": 99, "endOffset": 103}], "year": 2017, "abstractText": "The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques.", "creator": "LaTeX with hyperref package"}}}