{"id": "1604.00861", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2016", "title": "Recurrent Neural Networks for Polyphonic Sound Event Detection in Real Life Recordings", "abstract": "in writing this paper we present an approach to polyphonic verbal sound event detection embodied in real at life recordings based on bi - directional accelerated long delay short spectral term memory ( customized blstm ) recurrent neural networks ( generic rnns ). a single multilabel template blstm modified rnn is trained thus to ideally map acoustic features comprised of possessing a fully mixture musical signal consisting partly of differentiated sounds from multiple classes, paired to binary electrical activity inventory indicators characteristic of each cognitive event class. our revised method is tested on a growing large standardized database of real - live life recordings, with matching 61 different classes ( e. g. music, hobby car, speech ) captured from 10 different everyday functional contexts. previously the previous proposed method outperforms previously previous cognitive approaches expectations by charging a large cost margin, broadly and later the results are further improved using newer data augmentation techniques. overall, specifically our system reports an interactive average f1 - score of currently 65. 5 % basis on 1 second video blocks measuring and 64. 1 7 % on single frames, a relative success improvement over previous binary state - of - the - thing art approach estimation of 6. 8 % correctly and 15. 79 1 % respectively.", "histories": [["v1", "Mon, 4 Apr 2016 13:54:09 GMT  (1918kb,D)", "http://arxiv.org/abs/1604.00861v1", "To appean in Proceedings of IEEE ICASSP 2016"]], "COMMENTS": "To appean in Proceedings of IEEE ICASSP 2016", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.NE", "authors": ["giambattista parascandolo", "heikki huttunen", "tuomas virtanen"], "accepted": false, "id": "1604.00861"}, "pdf": {"name": "1604.00861.pdf", "metadata": {"source": "CRF", "title": "RECURRENT NEURAL NETWORKS FOR POLYPHONIC SOUND EVENT DETECTION IN REAL LIFE RECORDINGS", "authors": ["Giambattista Parascandolo", "Heikki Huttunen", "Tuomas Virtanen"], "emails": [], "sections": [{"heading": null, "text": "detection in real life recordings based on bi-directional long short term memory (BLSTM) recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to map acoustic features of a mixture signal consisting of sounds from multiple classes, to binary activity indicators of each event class. Our method is tested on a large database of real-life recordings, with 61 classes (e.g. music, car, speech) from 10 different everyday contexts. The proposed method outperforms previous approaches by a large margin, and the results are further improved using data augmentation techniques. Overall, our system reports an average F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a relative improvement over previous state-of-the-art approach of 6.8% and 15.1% respectively.\nIndex Terms\u2014 Recurrent neural network, bidirectional LSTM, deep learning, polyphonic sound event detection"}, {"heading": "1. INTRODUCTION", "text": "Sound event detection (SED), also known as acoustic event detection, deals with the identification of sound events in audio recordings. The goal is to estimate start and end times of sound events, and to give a label for each event. Applications of SED include for example acoustic surveillance [1], environmental context detection [2] and automatic audio indexing [3].\nSED in single-source environment is called monophonic detection, which has been the major area of research in this field [4]. However, in a typical real environment it is uncommon to have only a single sound source emitting at a certain point in time; it is more likely that multiple sound sources are emitting simultaneously, thus resulting in an additive combination of sounds. Due to the presence of multiple and overlapping sounds, this problem is known as polyphonic detection, and the goal of such a SED system is to recognize for each sound event its category (e.g., music, car, speech), and its beginning and ending. This task is much more challenging than the monophonic detection problem, because the sounds are overlapping and the features extracted from the mixture do not match with features calculated from sounds in isolation. Moreover, the number of sources emitting at any given moment (polyphony) is unknown and potentially large.\nInitial approaches to polyphonic SED include traditional methods for speech recognition, such as the use of mel frequency cepstral coefficients (MFCCs) as features, with Gaussian mixture models (GMMs) combined with hidden Markov models (HMMs) [5, 6]. A different type of approach consists of extracting and matching the sounds in the input to templates in a dictionary of sounds. This\nTuomas Virtanen has been funded by the Academy of Finland, project no. 258708. The authors wish to acknowledge CSC IT Center for Science, Finland, for computational resources.\ncan be achieved through sound source separation techniques, such as non-negative matrix factorization (NMF) on time-frequency representations of the signals. NMF has been used in [7] and [8] to pre-process the signal creating a dictionary from single events, and later in [6] and [9] directly on the mixture, without learning from isolated sounds. The work in [9] was extended in [10] making learning feasible for long recordings by reducing the dictionary size.\nOther approaches are based on spectrogram analysis with image processing techniques, such as the work in [11] that studies polyphonic SED using generalized Hough transform over local spectrogram features.\nMore recent approaches based on neural networks have been quite successful. The best results to date in polyphonic SED for real life recordings have been achieved by feedforward neural networks (FNNs), in the form of multilabel time-windowed multi layer perceptrons (MLPs), trained on spectral features of the mixture of sounds [12], temporally smoothing the outputs for continuity.\nMotivated by the good performance shown by the FNN in [12], we propose to use a multilabel recurrent neural network (RNN) in the form of bi-directional long short-term memory (BLSTM) [13, 14] for polyphonic SED (Fig. 1). RNNs, contrarily to FNNs, can directly model the sequential information that is naturally present in audio. Their ability to remember past states can avoid the need for tailored postprocessing or smoothing steps. These networks have obtained excellent results on complex audio detection tasks, such as speech recognition [15] and onset detection [16] (multiclass), polyphonic\nTo appear in Proc. ICASSP2016, March 20-25, 2016, Shanghai, China c\u00a9 IEEE 2016\nar X\niv :1\n60 4.\n00 86\n1v 1\n[ cs\n.S D\n] 4\nA pr\n2 01\n6\npiano note transcription [17] (multilabel). The rest of the paper is structured as follows. Section 2 presents a short introduction to RNNs and long short-term memory (LSTM) blocks. We describe in Section 3 the features used and the proposed approach. Section 4 presents the experimental set-up and results on a database of real life recordings. Finally, we present our conclusions in Section 5."}, {"heading": "2. RECURRENT NEURAL NETWORKS", "text": ""}, {"heading": "2.1. Feedforward neural networks", "text": "In a feedforward neural network (FNN) all observations are processed independently of each other. Due to the lack of context information, FNNs may have difficulties processing sequential inputs such as audio, video and text. A fixed-size (causal or non-causal) window, concatenating the current feature vector with previous (and eventually future) feature vectors, is often used to provide context to the input. This approach however presents substantial shortcomings, such as increased dimensionality (imposing the need for more data, longer training time and larger models), and short fixed context available."}, {"heading": "2.2. Recurrent neural networks", "text": "Introducing feedback connections in a neural network can provide it with past context information. This network architecture is known as recurrent neural network (RNN). In an RNN, information from previous time steps can in principle circulate indefinitely inside the network through the directed cycles, where the hidden layers also act as memory. For a sequence of input vectors {x1, ...,xT }, a RNN computes a sequence of hidden activations {h1, ...,hT } and output vectors {y1, ...,yT } as\nht = F(Wxhxt +Whhht\u22121 + bh) (1)\nyt = G(Whyht + by) (2)\nfor all timesteps t = 1, ..., T , where the matrices W?? denote the weights connecting two layers, b? are bias terms, and F and G activation functions. In case of a deep network, with multiple hidden layers, the input to hidden layer j is the output of the previous hidden layer j \u2212 1.\nWhen instances from future timesteps are available, also future context can be provided to the network by using bi-directional RNN (BRNN) [18]. In a BRNN each hidden layer is split into two separate layers, one reads the training sequences forwards and the other one backwards. Once fully computed, the activations are then fed to the next layer, giving the network full and symmetrical context for both past and future instances of the input sequence."}, {"heading": "2.3. Long short-term memory", "text": "Standard RNNs, i.e., RNNs with simple recurrent connections in each hidden layer, may be difficult to train. One of the main reasons is the phenomenon called vanishing gradient problem [19], which makes the influence of past inputs decay exponentially over time.\nThe long short-term memory (LSTM) [13] architecture was proposed as a solution to this problem. The simple neurons with static self-connections, as in a standard RNN, are substituted by units called LSTM memory blocks (Fig. 2). An LSTM memory block is a subnet that contains one self-connected memory cell with its tanh input and output activation functions, and three gating neurons\u2014 input, forget and output\u2014with their corresponding multiplicative\nunits. Eq. 1, defining the hidden activation ht, is substituted by the following set of equations:\nit = \u03c3(W xixt +W hiht\u22121 +W cict\u22121 + b i) ft = \u03c3(W xfxt +W hfht\u22121 +W cfct\u22121 + b f) ct = ftct\u22121 + it tanh(W xcxt +W hcht\u22121 + b c) ot = \u03c3(W xoxt +W hoht\u22121 +W coct + b\no) ht = ot tanh(ct)\n(3)\nwhere ct, it, ft and ot are respectively the memory cell, input gate, forget gate and output gate activations, \u03c3 is the logistic function, W?? are the weight matrices and b? are bias terms.\nBy analogy, the memory cell c can be compared to a computer memory chip, and the input i, forget f and output o gating neurons represent write, reset and read operations respectively. All gating neurons represent binary switches but use the logistic function\u2014thus outputting in the range [0, 1]\u2014to preserve differentiability. Due to the multiplicative units, information can be stored over long time periods inside the cell.\nA bidirectional long short-term memory (BLSTM) [14] network is obtained by substituting the simple recurrent neurons in a BRNN with LSTM units. More details about LSTM, BLSTM and training algorithms can be found in [20]."}, {"heading": "3. METHOD", "text": "The proposed system receives as input a raw audio signal, extracts spectral features and then maps them to binary activity indicators of each event class using a BLSTM RNN (Fig. 1). Each step is described in further detail in this section."}, {"heading": "3.1. Feature extraction", "text": "The input to the system are raw audio signals. To account for different recording conditions, the amplitudes are normalized in each recording to lie in [\u22121, 1]. The signals are split into 50 millisecond frames with 50% overlap, and we calculate the log magnitudes within the 40 mel bands in each frame. We then normalize each frequency band by subtracting the mean value of each bin over all recordings and imposing unit variance (computing the constants on the training set), a standard procedure when working with neural networks.\nFor each recording we obtain a long sequence of feature vectors, which is then split into smaller sequences. We split every original sequence at three different scales, i.e., in non-overlapping length 10, length 25, and length 100 sequences (corresponding to lengths of\n0.25, 0.62 and 2.5 seconds respectively). This allows the network to more easily identify patterns at different timescales.\nEach frame has a target vector d associated, whose binary components dk indicate if a sound event from class k is present or not."}, {"heading": "3.2. Proposed neural network", "text": "We propose the use of multilabel BLSTM RNNs with multiple hidden layers to map the acoustic features to class activity indicator vectors. The output layer has logistic activation functions and one neuron for each class. We use Gaussian input noise injection and early stopping to reduce overfitting, halting the training if the cost on the validation set does not decrease for 20 epochs.\nThe output of the network at time t is a vector yt \u2208 [0, 1]L, where L is the number of classes. Its components yk can be interpreted as posterior probabilities that each class is active or inactive in frame xt. These outputs do not have to sum up to 1, since several classes might be active simultaneously. For this reason, contrarily to most multiclass approaches with neural networks, the outputs are not normalized by computing the softmax. Finally, the continuous outputs are thresholded to obtain binary indicators of class activities for each timestep.\nContrarily to [12], where the outputs are smoothed over time using a median filter on a 10-frame window, we do not apply any post-processing since the outputs from the RNN are already smooth."}, {"heading": "3.3. Data augmentation", "text": "As an additional measure to reduce overfitting, which easily arises in case the dataset is small compared to the network size, we also augment the training set by simple transformations. All transformations are applied directly to the extracted features in frequency domain.\n\u2022 Time stretching: we mimic the process of slightly slowing down or speeding up the recordings. To do this, we stretch the mel spectrogram in time using linear interpolation by factors slightly smaller or bigger than 1;\n\u2022 Sub-frame time shifting: we mimic small time shifts of the recordings\u2014at sub-frame scale\u2014linearly interpolating new feature frames in-between existing frames, thus retaining the same frame rate;\n\u2022 Blocks mixing: new recordings with equal or higher polyphony can be created by combining different parts of the signals within the same context. In frequency domain we directly achieve a similar result using the mixmax principle [21], overlapping blocks of the log mel spectrogram two at the time.\nSimilar techniques have been used in [22, 23]. The amount of augmentation performed depends on the scarcity of the data available and the difficulty of the task. For the experiments described in Section 4\u2014where specified\u2014we expanded the dataset using the aforementioned techniques by approximately 16 times. A 4-fold increase comes from the time stretching (using stretching coefficients of 0.7, 0.85, 1.2, 1.5), 3-fold increase from sub-frame time shifting and 9-fold increase from blocks mixing (mixing 2 blocks at the time, using 20 non-overlapping blocks of equal size for each context). We did not test other amounts or parameters of augmentations. In order to avoid extremely long training times, the augmented data was split in length 25 sequences only."}, {"heading": "4. EVALUATION", "text": ""}, {"heading": "4.1. Dataset", "text": "We evaluate the performance of the proposed method on a database consisting of recordings 10 to 30 minutes long, from ten real-life contexts [24]. The contexts are: basketball game, beach, inside a bus, inside a car, hallway, office, restaurant, shop, street and stadium with track and field events. Each context has 8 to 14 recordings, for a total of 103 recordings (1133 minutes). The recordings were acquired with a binaural microphone at 44.1 kHz sampling rate and 24-bit resolution. The stereo signals from the recordings are converted to mono by averaging the two channels into a single one. The sound events were manually annotated within 60 classes, including speech, applause, music, break squeak, keyboard; plus 1 class for rare or unknown events marked as unknown, for a total of 61 classes. All the events appear multiple times in the recordings; some of them are present in different contexts, others are context-specific. The average polyphony level\u2014i.e. the average number of events active simultaneously\u2014is 2.53, the distribution of polyphony levels across all recordings is illustrated in Fig. 3.\nThe database was split into training, validation and test set (about 60%, 20% and 20% of the data respectively) in a 5-fold manner. All results are presented as averages of the 5-fold cross validation results, with the same train/validation/test partitions used in previous experiments on the same dataset ([10, 12]). The hyperparameters of the network, e.g. the number and size of hidden layers, learning rate, etc., were chosen based on validation results of the first fold."}, {"heading": "4.2. Neural networks experiments", "text": "The network has an input layer with 40 units, each reading one component of the feature frames, and 4 hidden layers with 200 LSTM units each (100 reading the sequence forwards, 100 backwards). We train one network with the original data only, which is the same used in previous works, and one using the data augmentation techniques reported in Section 3.3 to further reduce overfitting. To compare the performance with standard LSTM layers, we also train a similar network architecture without bidirectional units on the same dataset without augmentation.\nThe network is initialised with uniformly distributed weights in [\u22120.1, 0.1] and trained using root mean squared error as a cost function. Training is done by back propagation through time (BPTT) [25]. The extracted features are presented as sequences clipped from the original data\u2014in sequences of 10, 25 and 100 frames\u2014in randomly ordered minibatches of 600 sequences, in order to allow parallel processing. After a mini-batch is processed the weights are updated using RMSProp [26]. The network is trained with a learning rate \u03b7 = 0.005, decay rate \u03c1 = 0.9 and Gaussian input noise of\n0.2 (hyperparameters chosen based on the validation set of the first fold). At test time we present the feature frames in sequences of 100 frames, and threshold the outputs with a fixed threshold of 0.5, i.e., we mark an event k as active if yk \u2265 0.5, otherwise inactive.\nFor each experiment and each fold we train 5 networks with different random initialisations, select the one that has the highest performance on the validation set and then use it to compute the results on the test data. The networks were trained on a GPU (Tesla K40t), with the open-source toolkit Currennt [27] modified to use RMSprop."}, {"heading": "4.3. Metrics", "text": "To evaluate the performance of the system we compute F1-score for each context in two ways: average of framewise F1-score (F1AvgFram) and average of F1-score in non-overlapping 1 second blocks (F11-sec) as proposed in [4], where each target class and prediction is marked as active on the whole block if it is active in at least one frame of the block. The overall scores are computed as the average of the average scores for each context."}, {"heading": "4.4. Results", "text": "In Table 1 we compare the average scores over all contexts for the FNN in [12] to our BLSTM and LSTM networks trained on the same data, and BLSTM network trained with the augmented data. The FNN uses the same features but at each timestep reads a concatenation of 5 input frames (the current frame and the two previous and two following frames). It has two hidden layers with 1600 hidden units each, downsampled to 800 with maxout activations.\nThe BLSTM network achieves better results than the FNN trained on the same data, improving the performance by relative 13.5% for the average framewise F1 and 4.3% for the 1 second block F1. The unidirectional LSTM network does not perform as well as the BLSTM network, but is still better than the FNN. The best results are obtained by the BLSTM network trained on the augmented dataset, which improves the performance over the FNN by relative 15.1% and 6.8% for the average framewise F1 and for the 1 second block F1 respectively.\nIn Table 2 we report the results for each context for the FNN in [12] (FNN), our BLSTM trained on the same data (BLSTM) and our\nBLSTM trained on the augmented data (BLSTM+DA). The results show that the proposed RNN, even without the regularisation from the data augmentation, outperforms the FNN in most of the contexts.\nThe F1-scores for different polyphony levels are approximately the same, showing that the method is quite robust even when several events are combined. It is interesting to notice that the RNNs have around 850K parameters each, compared to 1.65M parameters of the FNN trained with the same data. The RNNs make a more efficient and effective use of the parameters, due to the recurrent connections and the deeper structure with smaller layers."}, {"heading": "5. CONCLUSIONS", "text": "In this paper we proposed to use multilabel BLSTM recurrent neural networks for polyphonic sound event detection. RNNs can directly encode context information in the hidden layers and can learn the longer patterns present in the data. Data augmentation techniques effectively reduce overfitting, further improving performance. The presented approach outperforms the previous state-of-the-art FNN [12] tested on the same large database of real-life recordings, and has half as many parameters. The average improvement on the whole data set is 15.1% for the average framewise F1 and 6.8% for the 1 second block F1.\nFuture work will concentrate on finding novel data augmentation techniques. Concerning the model, further studies will develop on attention mechanisms and extending RNNs by coupling them with convolutional neural networks."}, {"heading": "6. REFERENCES", "text": "[1] Aki Ha\u0308rma\u0308, Martin F McKinney, and Janto Skowronek, \u201cAutomatic surveillance of the acoustic activity in our living environment,\u201d in IEEE International Conference on Multimedia and Expo (ICME), 2005.\n[2] Selina Chu, Shrikanth Narayanan, and CC Jay Kuo, \u201cEnvironmental sound recognition with time\u2013frequency audio features,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 6, pp. 1142\u20131158, 2009.\n[3] Min Xu, Changsheng Xu, Lingyu Duan, Jesse S Jin, and Suhuai Luo, \u201cAudio keywords generation for sports video analysis,\u201d ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 4, no. 2, pp. 11, 2008.\n[4] Toni Heittola, Annamaria Mesaros, Antti Eronen, and Tuomas Virtanen, \u201cContext-dependent sound event detection,\u201d EURASIP Journal on Audio, Speech, and Music Processing, vol. 2013, no. 1, pp. 1\u201313, 2013.\n[5] Annamaria Mesaros, Toni Heittola, Antti Eronen, and Tuomas Virtanen, \u201cAcoustic event detection in real life recordings,\u201d in 18th European Signal Processing Conference, 2010, pp. 1267\u2013 1271.\n[6] Toni Heittola, Annamaria Mesaros, Tuomas Virtanen, and Moncef Gabbouj, \u201cSupervised model training for overlapping sound events based on unsupervised source separation,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 8677\u20138681.\n[7] Satoshi Innami and Hiroyuki Kasai, \u201cNMF-based environmental sound source separation using time-variant gain features,\u201d Computers & Mathematics with Applications, vol. 64, no. 5, pp. 1333\u20131342, 2012.\n[8] Arnaud Dessein, Arshia Cont, and Guillaume Lemaitre, \u201cRealtime detection of overlapping sound events with non-negative matrix factorization,\u201d in Matrix Information Geometry, pp. 341\u2013371. Springer, 2013.\n[9] Onur Dikmen and Annamaria Mesaros, \u201cSound event detection using non-negative dictionaries learned from annotated overlapping events,\u201d in IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2013.\n[10] Annamaria Mesaros, Toni Heittola, Onur Dikmen, and Tuomas Virtanen, \u201cSound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 606\u2013618.\n[11] Jonathan Dennis, Huy Dat Tran, and Eng Siong Chng, \u201cOverlapping sound event recognition using local spectrogram features and the generalised hough transform,\u201d Pattern Recognition Letters, vol. 34, no. 9, pp. 1085\u20131093, 2013.\n[12] Emre Cakir, Toni Heittola, Heikki Huttunen, and Tuomas Virtanen, \u201cPolyphonic sound event detection using multi label deep neural networks,\u201d in IEEE International Joint Conference on Neural Networks (IJCNN), 2015.\n[13] Sepp Hochreiter and Ju\u0308rgen Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[14] Alex Graves and Ju\u0308rgen Schmidhuber, \u201cFramewise phoneme classification with bidirectional LSTM and other neural network architectures,\u201d Neural Networks, vol. 18, no. 5, pp. 602\u2013 610, 2005.\n[15] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6645\u20136649.\n[16] Florian Eyben, Sebastian Bo\u0308ck, Bjo\u0308rn Schuller, and Alex Graves, \u201cUniversal onset detection with bidirectional long short-term memory neural networks.,\u201d in International Society for Music Information Retrieval Conference (ISMIR), 2010, pp. 589\u2013594.\n[17] Sebastian Bo\u0308ck and Markus Schedl, \u201cPolyphonic piano note transcription with recurrent neural networks,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 121\u2013124.\n[18] Mike Schuster and Kuldip K Paliwal, \u201cBidirectional recurrent neural networks,\u201d IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.\n[19] Yoshua Bengio, Patrice Simard, and Paolo Frasconi, \u201cLearning long-term dependencies with gradient descent is difficult,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013 166, 1994.\n[20] Alex Graves, Marcus Liwicki, Santiago Ferna\u0301ndez, Roman Bertolami, Horst Bunke, and Ju\u0308rgen Schmidhuber, \u201cA novel connectionist system for unconstrained handwriting recognition,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, pp. 855\u2013868, 2009.\n[21] Arthur Na\u0301das, David Nahamoo, Michael Picheny, et al., \u201cSpeech recognition using noise-adaptive prototypes,\u201d IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 37, no. 10, pp. 1495\u20131503, 1989.\n[22] Jan Schlu\u0308ter and Thomas Grill, \u201cExploring data augmentation for improved singing voice detection with neural networks,\u201d in International Society for Music Information Retrieval Conference (ISMIR), 2015.\n[23] Brian McFee, Eric J. Humphrey, and Juan P. Bello, \u201cA software framework for musical data augmentation,\u201d in International Society for Music Information Retrieval Conference (ISMIR), 2015.\n[24] Toni Heittola, Annamaria Mesaros, Antti Eronen, and Tuomas Virtanen, \u201cAudio context recognition using audio event histograms,\u201d in Proc. of the 18th European Signal Processing Conference (EUSIPCO), 2010, pp. 1272\u20131276.\n[25] Paul J Werbos, \u201cBackpropagation through time: what it does and how to do it,\u201d Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.\n[26] Tijmen Tieleman and Geoffrey Hinton, \u201cLecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude,\u201d COURSERA: Neural Networks for Machine Learning, vol. 4, 2012.\n[27] Felix Weninger, \u201cIntroducing currennt: The munich opensource cuda recurrent neural network toolkit,\u201d Journal of Machine Learning Research, vol. 16, pp. 547\u2013551, 2015."}], "references": [{"title": "Automatic surveillance of the acoustic activity in our living environment", "author": ["Aki H\u00e4rm\u00e4", "Martin F McKinney", "Janto Skowronek"], "venue": "IEEE International Conference on Multimedia and Expo (ICME), 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Environmental sound recognition with time\u2013frequency audio features", "author": ["Selina Chu", "Shrikanth Narayanan", "CC Jay Kuo"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 17, no. 6, pp. 1142\u20131158, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio keywords generation for sports video analysis", "author": ["Min Xu", "Changsheng Xu", "Lingyu Duan", "Jesse S Jin", "Suhuai Luo"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 4, no. 2, pp. 11, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Context-dependent sound event detection", "author": ["Toni Heittola", "Annamaria Mesaros", "Antti Eronen", "Tuomas Virtanen"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing, vol. 2013, no. 1, pp. 1\u201313, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Acoustic event detection in real life recordings", "author": ["Annamaria Mesaros", "Toni Heittola", "Antti Eronen", "Tuomas Virtanen"], "venue": "18th European Signal Processing Conference, 2010, pp. 1267\u2013 1271.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised model training for overlapping sound events based on unsupervised source separation", "author": ["Toni Heittola", "Annamaria Mesaros", "Tuomas Virtanen", "Moncef Gabbouj"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 8677\u20138681.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "NMF-based environmental sound source separation using time-variant gain features", "author": ["Satoshi Innami", "Hiroyuki Kasai"], "venue": "Computers & Mathematics with Applications, vol. 64, no. 5, pp. 1333\u20131342, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Realtime detection of overlapping sound events with non-negative matrix factorization", "author": ["Arnaud Dessein", "Arshia Cont", "Guillaume Lemaitre"], "venue": "Matrix Information Geometry, pp. 341\u2013371. Springer, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Sound event detection using non-negative dictionaries learned from annotated overlapping events", "author": ["Onur Dikmen", "Annamaria Mesaros"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations", "author": ["Annamaria Mesaros", "Toni Heittola", "Onur Dikmen", "Tuomas Virtanen"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 606\u2013618.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Overlapping sound event recognition using local spectrogram features and the generalised hough transform", "author": ["Jonathan Dennis", "Huy Dat Tran", "Eng Siong Chng"], "venue": "Pattern Recognition Letters, vol. 34, no. 9, pp. 1085\u20131093, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Polyphonic sound event detection using multi label deep neural networks", "author": ["Emre Cakir", "Toni Heittola", "Heikki Huttunen", "Tuomas Virtanen"], "venue": "IEEE International Joint Conference on Neural Networks (IJCNN), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks, vol. 18, no. 5, pp. 602\u2013 610, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2013, pp. 6645\u20136649.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Universal onset detection with bidirectional long short-term memory neural networks", "author": ["Florian Eyben", "Sebastian B\u00f6ck", "Bj\u00f6rn Schuller", "Alex Graves"], "venue": "International Society for Music Information Retrieval Conference (ISMIR), 2010, pp. 589\u2013594.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Polyphonic piano note transcription with recurrent neural networks", "author": ["Sebastian B\u00f6ck", "Markus Schedl"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 121\u2013124.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673\u20132681, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013 166, 1994.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["Alex Graves", "Marcus Liwicki", "Santiago Fern\u00e1ndez", "Roman Bertolami", "Horst Bunke", "J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 5, pp. 855\u2013868, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Speech recognition using noise-adaptive prototypes", "author": ["Arthur N\u00e1das", "David Nahamoo", "Michael Picheny"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 37, no. 10, pp. 1495\u20131503, 1989.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1989}, {"title": "Exploring data augmentation for improved singing voice detection with neural networks", "author": ["Jan Schl\u00fcter", "Thomas Grill"], "venue": "International Society for Music Information Retrieval Conference (ISMIR), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A software framework for musical data augmentation", "author": ["Brian McFee", "Eric J. Humphrey", "Juan P. Bello"], "venue": "International Society for Music Information Retrieval Conference (ISMIR), 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio context recognition using audio event histograms", "author": ["Toni Heittola", "Annamaria Mesaros", "Antti Eronen", "Tuomas Virtanen"], "venue": "Proc. of the 18th European Signal Processing Conference (EUSIPCO), 2010, pp. 1272\u20131276.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1990}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Introducing currennt: The munich opensource cuda recurrent neural network toolkit", "author": ["Felix Weninger"], "venue": "Journal of Machine Learning Research, vol. 16, pp. 547\u2013551, 2015. 5", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Applications of SED include for example acoustic surveillance [1], environmental context detection [2] and automatic audio indexing [3].", "startOffset": 62, "endOffset": 65}, {"referenceID": 1, "context": "Applications of SED include for example acoustic surveillance [1], environmental context detection [2] and automatic audio indexing [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "Applications of SED include for example acoustic surveillance [1], environmental context detection [2] and automatic audio indexing [3].", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "SED in single-source environment is called monophonic detection, which has been the major area of research in this field [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "Initial approaches to polyphonic SED include traditional methods for speech recognition, such as the use of mel frequency cepstral coefficients (MFCCs) as features, with Gaussian mixture models (GMMs) combined with hidden Markov models (HMMs) [5, 6].", "startOffset": 243, "endOffset": 249}, {"referenceID": 5, "context": "Initial approaches to polyphonic SED include traditional methods for speech recognition, such as the use of mel frequency cepstral coefficients (MFCCs) as features, with Gaussian mixture models (GMMs) combined with hidden Markov models (HMMs) [5, 6].", "startOffset": 243, "endOffset": 249}, {"referenceID": 6, "context": "NMF has been used in [7] and [8] to pre-process the signal creating a dictionary from single events, and later in [6] and [9] directly on the mixture, without learning from isolated sounds.", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "NMF has been used in [7] and [8] to pre-process the signal creating a dictionary from single events, and later in [6] and [9] directly on the mixture, without learning from isolated sounds.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "NMF has been used in [7] and [8] to pre-process the signal creating a dictionary from single events, and later in [6] and [9] directly on the mixture, without learning from isolated sounds.", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "NMF has been used in [7] and [8] to pre-process the signal creating a dictionary from single events, and later in [6] and [9] directly on the mixture, without learning from isolated sounds.", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "The work in [9] was extended in [10] making learning feasible for long recordings by reducing the dictionary size.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "The work in [9] was extended in [10] making learning feasible for long recordings by reducing the dictionary size.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "Other approaches are based on spectrogram analysis with image processing techniques, such as the work in [11] that studies polyphonic SED using generalized Hough transform over local spectrogram features.", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "The best results to date in polyphonic SED for real life recordings have been achieved by feedforward neural networks (FNNs), in the form of multilabel time-windowed multi layer perceptrons (MLPs), trained on spectral features of the mixture of sounds [12], temporally smoothing the outputs for continuity.", "startOffset": 252, "endOffset": 256}, {"referenceID": 11, "context": "Motivated by the good performance shown by the FNN in [12], we propose to use a multilabel recurrent neural network (RNN) in the form of bi-directional long short-term memory (BLSTM) [13, 14] for polyphonic SED (Fig.", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "Motivated by the good performance shown by the FNN in [12], we propose to use a multilabel recurrent neural network (RNN) in the form of bi-directional long short-term memory (BLSTM) [13, 14] for polyphonic SED (Fig.", "startOffset": 183, "endOffset": 191}, {"referenceID": 13, "context": "Motivated by the good performance shown by the FNN in [12], we propose to use a multilabel recurrent neural network (RNN) in the form of bi-directional long short-term memory (BLSTM) [13, 14] for polyphonic SED (Fig.", "startOffset": 183, "endOffset": 191}, {"referenceID": 14, "context": "These networks have obtained excellent results on complex audio detection tasks, such as speech recognition [15] and onset detection [16] (multiclass), polyphonic", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "These networks have obtained excellent results on complex audio detection tasks, such as speech recognition [15] and onset detection [16] (multiclass), polyphonic", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "piano note transcription [17] (multilabel).", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "When instances from future timesteps are available, also future context can be provided to the network by using bi-directional RNN (BRNN) [18].", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "One of the main reasons is the phenomenon called vanishing gradient problem [19], which makes the influence of past inputs decay exponentially over time.", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "The long short-term memory (LSTM) [13] architecture was proposed as a solution to this problem.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "All gating neurons represent binary switches but use the logistic function\u2014thus outputting in the range [0, 1]\u2014to preserve differentiability.", "startOffset": 104, "endOffset": 110}, {"referenceID": 13, "context": "A bidirectional long short-term memory (BLSTM) [14] network is obtained by substituting the simple recurrent neurons in a BRNN with LSTM units.", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "More details about LSTM, BLSTM and training algorithms can be found in [20].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "The output of the network at time t is a vector yt \u2208 [0, 1], where L is the number of classes.", "startOffset": 53, "endOffset": 59}, {"referenceID": 11, "context": "Contrarily to [12], where the outputs are smoothed over time using a median filter on a 10-frame window, we do not apply any post-processing since the outputs from the RNN are already smooth.", "startOffset": 14, "endOffset": 18}, {"referenceID": 20, "context": "In frequency domain we directly achieve a similar result using the mixmax principle [21], overlapping blocks of the log mel spectrogram two at the time.", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "Similar techniques have been used in [22, 23].", "startOffset": 37, "endOffset": 45}, {"referenceID": 22, "context": "Similar techniques have been used in [22, 23].", "startOffset": 37, "endOffset": 45}, {"referenceID": 23, "context": "We evaluate the performance of the proposed method on a database consisting of recordings 10 to 30 minutes long, from ten real-life contexts [24].", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": "All results are presented as averages of the 5-fold cross validation results, with the same train/validation/test partitions used in previous experiments on the same dataset ([10, 12]).", "startOffset": 175, "endOffset": 183}, {"referenceID": 11, "context": "All results are presented as averages of the 5-fold cross validation results, with the same train/validation/test partitions used in previous experiments on the same dataset ([10, 12]).", "startOffset": 175, "endOffset": 183}, {"referenceID": 24, "context": "Training is done by back propagation through time (BPTT) [25].", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "After a mini-batch is processed the weights are updated using RMSProp [26].", "startOffset": 70, "endOffset": 74}, {"referenceID": 26, "context": "The networks were trained on a GPU (Tesla K40t), with the open-source toolkit Currennt [27] modified to use RMSprop.", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "To evaluate the performance of the system we compute F1-score for each context in two ways: average of framewise F1-score (F1AvgFram) and average of F1-score in non-overlapping 1 second blocks (F11-sec) as proposed in [4], where each target class and prediction is marked as active on the whole block if it is active in at least one frame of the block.", "startOffset": 218, "endOffset": 221}, {"referenceID": 11, "context": "In Table 1 we compare the average scores over all contexts for the FNN in [12] to our BLSTM and LSTM networks trained on the same data, and BLSTM network trained with the augmented data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "In Table 2 we report the results for each context for the FNN in [12] (FNN), our BLSTM trained on the same data (BLSTM) and our Table 1: Overall F1 scores, as average of individual contexts scores, for the FNN in [12] (FNN) compared to the proposed LSTM, BLSTM and BLSTM with data augmentation (BLSTM+DA).", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "In Table 2 we report the results for each context for the FNN in [12] (FNN), our BLSTM trained on the same data (BLSTM) and our Table 1: Overall F1 scores, as average of individual contexts scores, for the FNN in [12] (FNN) compared to the proposed LSTM, BLSTM and BLSTM with data augmentation (BLSTM+DA).", "startOffset": 213, "endOffset": 217}, {"referenceID": 11, "context": "FNN [12] 58.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "The presented approach outperforms the previous state-of-the-art FNN [12] tested on the same large database of real-life recordings, and has half as many parameters.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Table 2: Results for each context in the dataset for the FNN in [12] (FNN), and our approach without data augmentation (BLSTM) and with data augmentation (BLSTM+DA).", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "FNN [12] BLSTM BLSTM+DA FNN [12] BLSTM BLSTM+DA", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "FNN [12] BLSTM BLSTM+DA FNN [12] BLSTM BLSTM+DA", "startOffset": 28, "endOffset": 32}], "year": 2016, "abstractText": "In this paper we present an approach to polyphonic sound event detection in real life recordings based on bi-directional long short term memory (BLSTM) recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to map acoustic features of a mixture signal consisting of sounds from multiple classes, to binary activity indicators of each event class. Our method is tested on a large database of real-life recordings, with 61 classes (e.g. music, car, speech) from 10 different everyday contexts. The proposed method outperforms previous approaches by a large margin, and the results are further improved using data augmentation techniques. Overall, our system reports an average F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a relative improvement over previous state-of-the-art approach of 6.8% and 15.1% respectively.", "creator": "LaTeX with hyperref package"}}}