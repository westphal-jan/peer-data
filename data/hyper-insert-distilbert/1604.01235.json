{"id": "1604.01235", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Apr-2016", "title": "A new TAG Formalism for Tamil and Parser Analytics", "abstract": "tree wide adjoining tag grammar ( tag ) protocol is specifically suited uniquely for adding morph implementation rich and agglutinated languages not like english tamil due to adding its psycho linguistic knowledge features transparency and parse time dependency comparisons and adequate morph resolution. though tag and ltag vocabulary formalisms have been known unsuccessfully for compiling about recent 3 nearly decades, efforts on entirely designing tag layer syntax for tamil have not been entirely successful due to the complexity check of its linguistic specification mechanism and the rich morphology understanding of tamil as language. in demonstrating this paper we present out a minimalistic tag tree for tamil without knowing much apparent morphological considerations attached and also introduce a dictionary parser implementation with some obvious variations from the different xtag system", "histories": [["v1", "Tue, 5 Apr 2016 12:42:31 GMT  (728kb)", "http://arxiv.org/abs/1604.01235v1", "International Symposium for Dravidian Languages (iDravidian), co-located with ICON2014, Goa University, Dec 2014"]], "COMMENTS": "International Symposium for Dravidian Languages (iDravidian), co-located with ICON2014, Goa University, Dec 2014", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vijay krishna menon", "s rajendran", "m anand kumar", "k p soman"], "accepted": false, "id": "1604.01235"}, "pdf": {"name": "1604.01235.pdf", "metadata": {"source": "CRF", "title": "A new TAG Formalism for Tamil and Parser Analytics", "authors": ["Vijay Krishna Menon", "Rajendran S"], "emails": ["M_vijaykrishna@cb.amrita.edu", "rajushush@gmail.com", "m_anandkumar@cb.amrita.edu", "kp_soman@amrita.edu"], "sections": [{"heading": null, "text": "suited for morph rich and agglutinated languages like Tamil due to its psycho linguistic features and parse time dependency and morph resolution. Though TAG and LTAG formalisms have been known for about 3 decades, efforts on designing TAG Syntax for Tamil have not been entirely successful due to the complexity of its specification and the rich morphology of Tamil language. In this paper we present a minimalistic TAG for Tamil without much morphological considerations and also introduce a parser implementation with some obvious variations from the XTAG system."}, {"heading": "1 Overview", "text": "TAGs were proposed for language models earlier by Vijay Shankar and Aravind Joshi in (VijayShankar and Joshi, 1985). Unlike the Chomskian formalisms, the elementary objects manipulated by TAG are trees; structured objects and not strings. Such structured formalisms have properties that relate directly to strong generative capacity (structure descriptions), which is linguistically more relevant than string sets (weak generative capacity). So we call TAGs as a tree generating system rather than a string generating system. The set of all trees derived in a TAG constitute the object language. Hence, in order to describe the derivation of a tree\nin the object language, we will need to know about \u2018derivation trees\u2019. The derivation trees are important in both syntactic and semantic senses. TAGs also have some interesting linguistic properties. Lexicalization is one of the key motivations for the study of TAGs, both linguistic and formal. The lexical phenomena now explain many linguistic theories previously thought to be purely syntactic. So the information in lexicons, have increased both in amount and complexity. From the formal perspective, lexicalization allows us to associate every elementary structure (trees) with a lexicon (any word). The famous Greibach Normal Form (also Chomsky Normal Form or CNF) for CFGs is a kind of lexicalization. However it is a weak lexicalization, as the structure of the original grammar is not preserved and all rules cannot be lexicalised. Thus TAGs provide an edge to this errand over conventional CFGs"}, {"heading": "2 Formalism", "text": "TAGs were introduced by Joshi et al. (1975) and later Joshi (1985). It is known that tree adjoining languages (TALs) generate some strictly context sensitive languages and fall in the class of the so called \u2018mildly context sensitive\u2019 languages (Joshi et al, 1991). TALs properly contain context-free languages and are properly contained by indexed languages. We will introduce an overview of TAG\nand then move on to observe the lexicalization process.\nA tree-adjoining grammar (TAG), G consists of\na quintuple (\u2211, NT, I, A, S) where\ni. \u2211 is a finite set of terminal symbols. NT is a finite set of non-terminal symbols such\nthat (\u2211 . ii. S is a Sentential symbol such that .\niii. I is a finite set of trees called initial trees, with the following properties\na. Interior nodes are labelled by nonterminal symbols; b. The nodes on the frontier of all initial trees are labelled by terminals or non-\nterminals; non-terminals symbols on the frontier of any tree in I are marked for substitution which, by convention is a down arrow (\u2193);\niv. A is a finite set of trees called auxiliary trees, with the following properties\na. Interior nodes are labelled by nonterminal symbols; b. The nodes on the frontier of auxiliary trees are labelled by terminal symbols\nor non-terminal symbols. Nonterminal symbol on the frontier of trees in A are marked for substitution except for one node, called the foot node; by convention this is marked with an asterisk(*); the label of the foot node must be identical to the root node.\nIn lexicalised TAG, at least one frontier node must be labelled with a terminal symbol (the anchor) in all initial and auxiliary trees. The set I U A is called the set of elementary trees. If an elementary tree has its root labelled by non-terminal X, then it is called an X-type elementary tree.\nA tree built by combining the elementary trees is called derived tree or parse tree. We will now have to understand how the combinations of trees happen as to make a derived tree. There are 2 major composition operations adjoining and substitution.\nAdjoining (or adjunction, as it is alternately referred) builds a new tree from an auxiliary tree \u03b2 and a tree \u03b1 (\u03b1 is any tree initial auxiliary or derived). Adjunction has been illustrated in Fig 1. Let \u2018\u03b1\u2019 be a tree containing a non-substitution node labeled by X. The resulting tree, \u03b3, obtained by adjoining \u03b2 to \u03b1 at node n is structured as:\n The sub-tree of \u03b1 with root n is displaced by \u03b2, along with its root node n.\n The displace sub tree of \u03b1 will attach itself to \u03b2, replacing the foot node of \u03b2.\nSubstitution takes place only on non-terminal nodes in the frontier of a tree. Unlike normal adjunctions, substitutions are mandatory if the node is marked for it with a down arrow as explained above. When a node, say n, is substituted, the entire node is replaced by the initial tree that is substituted. Only initial trees or its derivatives may be used for substitution. By definition adjunctions on any node marked for substitution is not permitted. But adjunctions are possible on the root nodes of the trees already substituted replacing the marked node. This is illustrated in Fig 2 with a set of three initial trees. Substitution extents the targeted leaf node to complete a construct that requires addition of a single substring."}, {"heading": "2.1 Adjoining Constraints", "text": "Natural language specifics demands more precise ways of adjoining to be used with TAGs. Hence out of the original definition of adjoining, we can add on may constrains that may or may not be applied for an adjunction. Basically an auxiliary tree \u03b2 is adjoined at node n of \u03b1, if the root node of \u03b2 is also n (labelled by n) and no substitution is marked on node n of \u03b1. Now the newer constraints will take effect only on satisfaction of the above basic constraints. They are as follows:\n Selective Adjunction (SA (T), for short):\nonly elements of the set of auxiliary trees can be adjoined on the given node. The adjunction is not mandatory.  Null Adjunction1 (NA, for short): It disallows any adjunction on the given node.\n1 Null adjunction corresponds to a special case of selective adjunction SA (T) where T is a null set. i.e., .\n Obligatory Adjunction (OA (T), for short): the adjunction of any auxiliary tree in\nmust be mandatorily done on the given node. OA is used to indicate OA (A), which is a common type of Obligatory Adjunction.\nIf all constraints and substitution operation is withdrawn then the definition of TAG becomes synchronous with the one given in Joshi et al (1975). The latter two additions: constraints and substitution were found to be linguistically useful, hence added with TAGs. Substitution if we see is the main combinatory operation in CFGs. It was introduced by Vijay-Shankar et al (1985). The adjunction constraints allow TAGs to attain some much desired closure properties."}, {"heading": "2.2 Derivation Structures", "text": "When TAG grammar yields (generates) derived trees by derivation, the information to trace the history of such combination is not given. Unlike CFGs, the derived tree does not contain information as to which basic rules (in our case, elementary\ntrees) were used to construct it. Hence we re-\nquire a new object that gives us information regarding all operations and elementary trees used to build a derived tree. This structured object is called a derivation tree. It uniquely specifies what operation was used to combine which particular trees. Both adjunctions and substitutions are considered for derivation.\nConsider the example sentence \u201cYesterday a man saw Mary\u201d. This example has been adopted from Joshi and Schabes (1997). Fig 3 illustrates the derived tree for the above English sentence. But this tree does not give any relevant information regarding how it can be constructed. For this we define the derivation tree for the same sentence. Refer to Fig 4 where the necessary elementary trees required to derive the \u03b15 has been illustrated. Note that \u03b1 trees are initial trees and the \u03b2 ones are auxiliary. This convention will be prevailing\nFigure 3\nthroughout this paper whenever referring to TAG trees.\nNow the derivation tree for this example is shown in Fig 5. Along with exemplifying the process of building a derivation we also show how a proper lexicalization of TAG is achieved. All the elementary trees in the Fig 4 are properly and completely lexicalised with every elementary tree mapped to at least one lexicon. So every tree will have at least one anchor node.\nThe roots of all derivation trees are labelled by the name of an S-type initial tree. All child nodes are labelled by auxiliary trees which adjoined or initial trees which are substituted. The notion of tree address is used here to indicate where the composition happened. This will uniquely identify a node in a given tree. This address is referred to as the Gorn index; used for multiple array of purposes and is specifically important from an implementation point of view.\nThe Gorn index system starts with index 0 for the root node. For the 1st level children the numbering starts with 0.1 (or just 1) for the leftmost and increasing towards the right. For the 2nd level children say the child of the second leftmost child will be given 0.2.1 (or just 2.1) and so on. The system is simple and intuitive. Now if an adjunction takes place at this node of the tree, the derivation tree node labelled with the adjoining auxiliary tree will also carry the Gorn index 0.2.1, so we know exactly where the adjunction or substitution has occurred.\nFig 5 depicts the derivation of the derived tree given in Fig 3. Note that \u03b1saw is an S-type initial tree; most verb initial trees are expected to be so. Now the node \u03b1man (1) indicates a substitution of this tree at node 0.1 of \u03b1saw. In a deeper sense it\nmeans this tree replaced the node indexed 0.1 in tree \u03b1saw.\nThe case with \u03b1Mary is no different, except that it is substituted at for node 0.2.2. But \u03b2yesterday is an auxiliary tree and is adjoined at the root node of \u03b1saw as it contains the Gorn index pointing to the root. The main idea here is the Gorn indices given in a derivation tree\u2019s node, points to an address in its parent node\u2019s tree where the substitution or adjunction has been done. Further it also demonstrates how lower composition happens, like \u03b1a substituted on \u03b1man. Unlike as represented, substitutions need not be discriminated with dotted lines alone. The target node tree can solve the conflict by its type as in initial or auxiliary. Another counter intuitive fact is that adjoining happens even at the root node. But controlling adjunctions will help us control the grammars generative ability and restrict the constructs it creates. So every node in the derivation tree will have distinct indices for a given parent node. This way of representing derivation not only captures the syntactic structure of the target tree but also contains semantic dependencies. This has been demonstrated by Joshi and Rambow (1997); they were the first to investigate this property for TAG derivations. Later, Joshi and Rambow (2003) gave a dependency grammar based on TAG formalism. However we shall give a different picture of the same idea here. To illustrate this let us isolate the basic words of the above given example itself. Before we go into detail of this we will need to define dependency functions of each word with respect to the parts of speech (POS) of each word. Consider initially the verb saw. Now \u2018saw\u2019 is a transitive verb 2 , so it will have dependencies in 2 ways, one with its subject and the other\n2 Verbs that require a subject and an object of action are transitive verbs.\nwith the object. Hence the dependency function will look like this.\nThis show the dependencies of the transitive verb saw to depend on the subject as to who or what saw to the object as to saw whom or what. Logically this function looks like this for saw.\nThis is exactly what we get in the derivation; \u201cman saw Mary\u201d giving us the dependency function for saw to be saw (Man, Mary). All the other words will have dependencies too as well. As for the Noun man the function is different and addresses the number or specificity. That means that nouns have articles or adjectives that describe them. This is their dependency. The above derivation also gives man(a) which is the dependency function for the word. The dependencies of a word can be easily found from the children of the given node in a derivation tree.\nFrom the above insight, we must gather that saw in this example is not just transitive. That is to say it has a subject, an objects and an adverb. Thus the definition of the function should be having an extra parameter, one that specifies time in this case hence we have saw (Man, Mary, Yesterday). This property of TAG derivation greatly helps for representation of agglutinative languages, where the verbal inflection will depend on its subject or object or both. Subject verb agreements are crucial especially in Indian languages."}, {"heading": "2.3 Lexicalization", "text": "According to Joshi and Schabes, (1997), lexicalized grammars are of both linguistic and formal significance. In lexicalised TAG (LTAG), each elementary tree is systematically associated to a lexical item called anchor. The grammar should further define a lexicon 3 where every lexical item is associated with a finite number of structures, for which that item is the anchor. There are composition operations that describe the building of such structures. Such a grammar is called a lexicalised grammar.\n3 Lexicon is an ambiguous name. It will refer to a computer dictionary of all words or it can specifically refer to a single word, a terminal symbol of the grammar. Here it means a dictionary of plausible structures for an anchor.\nBefore we define lexicalization formally we need to refine some thought curves and ideologies referring to anchors, lexical items, lexicons and terminals. We shall give postulates on these, so they are easy to refer back in time when required. Though this postulations deviate from the original TAG definitions given by Vijay-Shankar (1987) and by Joshi and Schabes (1997). This was required for the model in which TAGs are used in Machine Translation and our implementation of it. These deviations do not disturb the formal properties in any manner but gives greater, intuitive, linguistic flexibility at the same time. It also allows for lexicalization easily, without loss of the generality of the normal grammar.\nThe lexicalization postulates are as follows.\n1. The anchor node is a lexical item. 2. The anchor nodes are labelled by special\nnon terminals that we call anchors\n3. The anchors represent direct terminal groups, such as POS classes. 4. Adjoining happen normally at anchor nodes unless otherwise specified. 5. All elementary trees carry exactly one anchor for every tree. 6. All trees are thus pseudo lexicalized. Dummy lexical node is inserted bellow the\nanchor node when needed.\n7. Anchor nodes are marked with diamond\nand lexical nodes with a script-L (\u2112). They exist as separate nodal entities.\nThese postulates will be further deliberated on when we see L-TAG Parsing implementation and analytics. Most of it will only make sense then as keeping these invariants has sped up parsing time complexity. Now we shall proceed to the formal definition of lexicalisation. A grammar is lexicalised if it consists of:\n A finite set of structures each associated with a lexical item; each lexical item will\nbe called the anchor of the corresponding structure;\n An operation or operations for composing structures. We require the anchor to be a non-empty lexical item. We shall define a Lexicon 4 which shall contain or map to a finite set of structures each associ-\n4 A lexical dictionary is at times referred to as just the Lexicon. Hence a Lexicon with L capitalized would refer to a lexical dictionary and lexicon with a lower case l would refer to a specific terminal symbol or a word.\nated with an anchor, called elementary structures. We will consider operations of combining two structures. These can be restricted as to yield languages of constant growth. The operations that we use will attribute these properties. From the above definition, we see some properties of lexicalised grammars.\nLemma 1: Lexicalised grammars are finitely ambiguous.\nA grammar is said to be finitely ambiguous if there is no sentence of finite length that can be analysed in an infinite number of ways. This can be seen holding true for lexicalised grammars. Consider any arbitrary sentence of finite length. Since the number words (lexicons) in this sentence are finite so is the number of structures necessary to analyse it. Now a finite number of structures can only be combined in finite number of ways, to produce finitely many structures. Therefore lexicalised grammars are finitely ambiguous.\nLemma 2: It is decidable whether or not a string is accepted by a lexicalised grammar.\nLexicalisation does not imply that all grammars be lexicalised. Given any grammar G stated in certain formalism, we can generate GL though not necessarily in the same formalism, which generates the same tree set, holding the lexical property, henceforth yielding the same string language. This is the lexicalisation phenomenon. From this facsimile the following statement holds true. Joshi and Schabes (1997) says, a formalism F can be lexicalised by another formalism F\u2019, if for any finitely ambiguous grammar G in F there is a grammar G\u2019 in F\u2019 such that G\u2019 is lexicalised and generate the same tree set.\nLemma 3: If G is a finitely ambiguous CFG which does not generate the empty string, then there is a lexicalised tree-adjoining grammar Glex generating the same language and tree set as G such that Glex can have no substitution node in any elementary tree.\nThe proof of the above constructivism is given by Joshi and Schabes (1997). It is proved by exemplification where in a grammar is defined satisfying the requirements of this lemma. The main thought for such construction is to separate the recursive part from the non-recursive part of the grammar. For every recursion an auxiliary tree is\ncreated; for rule of the type an auxiliary tree will be created."}, {"heading": "3 TAG Tamil Syntax Mapping", "text": "In the tree adjoining frame work, it is understood that each verb in syntactic lexicon selects one tree family. Here we propose to map tree frames for a finite classes of verbs (53 tree families according to one version) in English into Tamil. Tamil being a SOV language is different from English which is an SVO language in the tree configuration. English is both right branching (=head initial) and left-branching (=head-final) language, whereas Tamil is strongly left branching (head final) language. This difference between English and Tamil will be reflected in many structural configurations of English and Tamil. For example, thatcomplement (that-embedded clause) in English follows the matrix clause, where as in Tamil it normally precedes the matrix clause. Similarly English in the relative clause the head noun comes before the verb, whereas in Tamil the head noun comes after the verb. English is a prepositional language, but Tamil is the post-positional in nature. The post-positions are either suffix or separate particles. When we go by word-alignment, it may appear that the words are just reversed in Tamil when compared to the order of English. We hope to capture these configuration differences between English and Tamil and thereby find ways to map English sentence into Tamil. Here we will map the English tree configuration for a class of verbs with Tamil tree. We hope this will help us when go for transferring English tag configuration into Tamil tag configuration, for the sake of across-the-language NLP applications, for example machine translation. Here will cover up only important tree families into Tamil tree families."}, {"heading": "3.1 Intransitive: Tnx0V", "text": "This tree family is selected by verbs that do not require an object complement of any type. Adverbs, prepositional phrases and other adjuncts may adjoin on, but are not required for the sentences to be grammatical. 1,878 verbs select this family. For example the verbs like eat, sleep, dance, etc., select this tree as illustrated in Fig 6. These discussions are directly taken from Sarkar (2002) and not be repeated again and again\nas reference. Some example sentences: \u2018Al ate\u2019, \u2018Seth slept\u2019 and \u2018Hyun danced\u2019. Sarkar (2002)"}, {"heading": "3.2 Transitives: Tnx0Vnx1", "text": "This tree family is selected by verbs that require only an NP object complement. The NP's may be complex structures, including gerund NP's and NP's that take sentential complements. This does not include light verb constructions. 4,337 verbs select the transitive tree family. For example the verbs eat, dance, take, like, etc. take this tree; sentential examples are: \u2018Al ate an apple\u2019, \u2018Seth danced the tango\u2019, \u2018Hyun is taking an algorithms course\u2019 and \u2018Anoop likes the fact that the semester is finished\u2019. The Tamil and English mappings are given in Fig 6."}, {"heading": "3.3 Ditransitive with PP: Tnx0Vnx1pnx2", "text": "This tree family is selected by ditransitive verbs that take a noun phrase followed by a prepositional phrase. The preposition is not constrained in the syntactic lexicon. The preposition must be required and not optional - that is, the sentence must be ungrammatical with just the noun phrase (e.g. John put the table). No verbs, therefore, should select both this tree family and the transitive tree family. There are 5 verbs that select this tree family. For example the verbs ensconce, put, usher, etc. select this tree. Sentential examples: \u2018Mary ensconced herself on the sofa\u2019, \u2019He put the book on the table\u2019 and \u2019He ushered the patrons into the theater\u2019."}, {"heading": "4 Parsing Analytics and Examples", "text": "Vijay-Shankar and Joshi (1985) came up with O(n 6 ) CYK parser for TAGs. This was the first practical parser for the entire formalism. Later on [Schabes and Joshi, 1988] described their Earley type TAG parsing algorithm with extensions to various derived formalisms as well such as constraint TAGs. This was our guiding paper for the modified practical implementation we are to discuss. We have several modifications and deviations that we made to the actual parser. Furthermore, Joshi and Schabes (1997) have proposed a chart parsing algorithm which has been partially adopted by us and in combination came up with a multi-threaded implementation of the same for TAGs. The main benefit of an Earley parser is that it has a worst case complexity of O(n 6 ), which in most cases perform much better, as com-\npared to the same average complexity of the CYK parser. However it depends on the grammar definitions, and practically perform better of than in theory. Schabes and Joshi, (1988), starts with the original Earley\u2019s invariant and goes on to define one, for the trees, Preserving the actual perspective.\nSince our focus here about the parser is minimal we will not discuss detailed implementation of this parser or the algorithm. We are not using a statistical parser, which is why we will have multiple parses for a single sentence. Mostly this is due to lexicosyntactic ambiguities inherent in the Lan-\nguage specification and the grammar specification. The grammar was made with multilingual tasks in mind mainly machine translation."}, {"heading": "4.1 Tamil parse specifics and examples", "text": "The grammar used by the parser for Tamil contains over 120 trees correctly and is regularly pruned to reduce cross ambiguities. We have 25 initial and 95 auxiliary trees. Together they address most constructs of simple and direct sentences.\nThe parser accepts Parts of speech tagged sentences using the Penn Tag set for the same. If the sentence is within the construct range of the grammar, the parser immediate returns TAG derivations from which derived trees can be easily constructed. As mentioned before we are not currently dealing with morph analysis just to keep our focus on grammar and parsing. In the examples here the words are mostly surface forms with some chucks in it. Our objective is to prove the conformity of TAG syntax for Tamil in a broader sense. Two examples of the parse instances are illustrated bellow. The Tamil sentences has been Romanised for the sake of linguistic verification.\nExample 1: NiyUyArkkil na\u1e6daipeRRa yu.EsOpaN-A\u1e47ka\u1e37-ira\u1e6d\u1e6daiyar iRutip-pO\u1e6d\u1e6diyil liyA\u1e47\u1e6darpayas-jO\u1e6di veRRi peRRu pa\u1e6d\u1e6dattaik kaippaRRiyatu\nExample 2: MOtirattai tiru\u1e6diya vAliparai pOlIcAr tE\u1e6di varuki\u1e49\u1e5fa\u1e49ar\nThis sentence has multiple parses mainly due to a lexicosyntactic ambiguity. One prominent parse illustrated by Fig 8 and another parse illustrated by Fig 10."}, {"heading": "5 Conclusion and Further development", "text": "Evidently Tamil syntax can be effectively captured using TAG formalism. Even though we have not discussed morphological considerations here we maintain that we can very efficiently deal with it using TAGs. Feature based parsing is very easy as the dependencies are preserved with trees and separate dependency grammars are not required. Our main goal is to apply these techniques in multilingual applications predominantly in machine translation. The synchronised grammar methodology is well suited for such future endeavours."}], "references": [{"title": "The syntax of French de-N phrases", "author": ["A. Abeille", "O. Bonami", "D. Gordard", "J. Tseng"], "venue": "Proceedings of the HPSG04 Conference. Stanford: CSLI Publications", "citeRegEx": "Abeille et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abeille et al\\.", "year": 2004}, {"title": "Using lexicalized tree adjoining grammms for machine translation", "author": ["A. Abeille", "Y. Schabes", "A. Joshi"], "venue": "In Proceedings of the 12th International Conference on Computational Linguistics . Budapest", "citeRegEx": "Abeille et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Abeille et al\\.", "year": 1990}, {"title": "Government and Binding Theory and Minimalist Program", "author": ["N. Chomsky"], "venue": null, "citeRegEx": "Chomsky,? \\Q1995\\E", "shortCiteRegEx": "Chomsky", "year": 1995}, {"title": "The minimalist program", "author": ["N. Chomsky"], "venue": null, "citeRegEx": "Chomsky,? \\Q1995\\E", "shortCiteRegEx": "Chomsky", "year": 1995}, {"title": "An Introduction to Tree Adjoining Grammars. Mathematics of Language", "author": ["A. Joshi"], "venue": null, "citeRegEx": "Joshi,? \\Q1987\\E", "shortCiteRegEx": "Joshi", "year": 1987}, {"title": "How much context sensitivity is necessory for charectorizing structural descriptions. Natural Language Parsing Theoretical, Computational and Psychologica Perspectives", "author": ["A. Joshi"], "venue": null, "citeRegEx": "Joshi,? \\Q1985\\E", "shortCiteRegEx": "Joshi", "year": 1985}, {"title": "A Formalism for Dependency Grammar Based on Tree Adjoining Grammar", "author": ["A. Joshi", "O. Rambow"], "venue": null, "citeRegEx": "Joshi and Rambow,? \\Q2003\\E", "shortCiteRegEx": "Joshi and Rambow", "year": 2003}, {"title": "An Earley Type Parsing Algorithm for Tree Adjoining Grammars", "author": ["Y. Schabes", "A. Joshi"], "venue": "Proceedings of the 26th annual meeting on Association for Computational Linguistics (pp", "citeRegEx": "Schabes and Joshi,? \\Q1988\\E", "shortCiteRegEx": "Schabes and Joshi", "year": 1988}, {"title": "Synchronous Tree-Adjoining Grammars", "author": ["S.M. Schieber", "Y. Schabes"], "venue": "Proceedings of the 13th International Conference on Computational Linguistics", "citeRegEx": "Schieber and Schabes,? \\Q1990\\E", "shortCiteRegEx": "Schieber and Schabes", "year": 1990}, {"title": "A Study of Tree Adjoining Grammars, A PhD Thesis. Philadelphia: University of Pennsylvania", "author": ["K. Vijay-Shanker"], "venue": null, "citeRegEx": "Vijay.Shanker,? \\Q1988\\E", "shortCiteRegEx": "Vijay.Shanker", "year": 1988}], "referenceMentions": [{"referenceID": 4, "context": "TAGs were introduced by Joshi et al. (1975) and later Joshi (1985).", "startOffset": 24, "endOffset": 44}, {"referenceID": 4, "context": "TAGs were introduced by Joshi et al. (1975) and later Joshi (1985). It is known that tree adjoining languages (TALs) generate some strictly context sensitive languages and fall in the class of the so called \u2018mildly context sensitive\u2019 languages (Joshi et al, 1991).", "startOffset": 24, "endOffset": 67}, {"referenceID": 4, "context": "If all constraints and substitution operation is withdrawn then the definition of TAG becomes synchronous with the one given in Joshi et al (1975). The latter two additions: constraints and substitution were found to be linguistically useful, hence added with TAGs.", "startOffset": 128, "endOffset": 147}, {"referenceID": 4, "context": "If all constraints and substitution operation is withdrawn then the definition of TAG becomes synchronous with the one given in Joshi et al (1975). The latter two additions: constraints and substitution were found to be linguistically useful, hence added with TAGs. Substitution if we see is the main combinatory operation in CFGs. It was introduced by Vijay-Shankar et al (1985). The adjunction constraints allow TAGs to attain some much desired closure properties.", "startOffset": 128, "endOffset": 380}, {"referenceID": 4, "context": "This example has been adopted from Joshi and Schabes (1997). Fig 3 illustrates the derived tree for the above English sentence.", "startOffset": 35, "endOffset": 60}, {"referenceID": 4, "context": "This has been demonstrated by Joshi and Rambow (1997); they were the first to investigate this property for TAG derivations.", "startOffset": 30, "endOffset": 54}, {"referenceID": 4, "context": "This has been demonstrated by Joshi and Rambow (1997); they were the first to investigate this property for TAG derivations. Later, Joshi and Rambow (2003) gave a dependency grammar based on TAG formalism.", "startOffset": 30, "endOffset": 156}, {"referenceID": 4, "context": "According to Joshi and Schabes, (1997), lexicalized grammars are of both linguistic and formal significance.", "startOffset": 13, "endOffset": 39}, {"referenceID": 4, "context": "Though this postulations deviate from the original TAG definitions given by Vijay-Shankar (1987) and by Joshi and Schabes (1997). This was required for the model in which TAGs are used in Machine Translation and our implementation of it.", "startOffset": 104, "endOffset": 129}, {"referenceID": 4, "context": "Joshi and Schabes (1997) says, a formalism F can be lexicalised by another formalism F\u2019, if for any finitely ambiguous grammar G in F there is a grammar G\u2019 in F\u2019 such that G\u2019 is lexicalised and generate the", "startOffset": 0, "endOffset": 25}, {"referenceID": 4, "context": "The proof of the above constructivism is given by Joshi and Schabes (1997). It is proved by exemplification where in a grammar is defined satisfying the requirements of this lemma.", "startOffset": 50, "endOffset": 75}, {"referenceID": 7, "context": "Later on [Schabes and Joshi, 1988] described their Earley type TAG parsing algorithm with extensions to various derived formalisms as well such as constraint TAGs.", "startOffset": 9, "endOffset": 34}, {"referenceID": 4, "context": "Vijay-Shankar and Joshi (1985) came up with O(n 6 ) CYK parser for TAGs.", "startOffset": 18, "endOffset": 31}, {"referenceID": 4, "context": "Vijay-Shankar and Joshi (1985) came up with O(n 6 ) CYK parser for TAGs. This was the first practical parser for the entire formalism. Later on [Schabes and Joshi, 1988] described their Earley type TAG parsing algorithm with extensions to various derived formalisms as well such as constraint TAGs. This was our guiding paper for the modified practical implementation we are to discuss. We have several modifications and deviations that we made to the actual parser. Furthermore, Joshi and Schabes (1997) have proposed a chart parsing algorithm which has been partially adopted by us and in combination came up with a multi-threaded implementation of the same for TAGs.", "startOffset": 18, "endOffset": 505}, {"referenceID": 4, "context": "Schabes and Joshi, (1988), starts with the original Earley\u2019s invariant and goes on to define one, for the trees, Preserving the actual perspective.", "startOffset": 12, "endOffset": 26}], "year": 2016, "abstractText": "Tree adjoining grammar (TAG) is specifically suited for morph rich and agglutinated languages like Tamil due to its psycho linguistic features and parse time dependency and morph resolution. Though TAG and LTAG formalisms have been known for about 3 decades, efforts on designing TAG Syntax for Tamil have not been entirely successful due to the complexity of its specification and the rich morphology of Tamil language. In this paper we present a minimalistic TAG for Tamil without much morphological considerations and also introduce a parser implementation with some obvious variations from the XTAG", "creator": "Microsoft\u00ae Word 2010"}}}