{"id": "1604.01474", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Apr-2016", "title": "Self-Paced Multi-Task Learning", "abstract": "in extending this 2013 paper, we propose computing a novel multi - parameter task learning ( mtl ) learning framework, traditionally called self - paced multi - task distributed learning ( spmtl ). different concepts from previous works treating all core tasks separately and instances equally when given training, spmtl technology attempts learners to just jointly repeatedly learn through the tasks by taking explicit into consideration the complexities of both simple tasks learners and instances. this is perhaps inspired by the classical cognitive process of human brain that therefore often learns them from the easy to the real hard. we successively construct a compact ensemble spmtl formulation by proposing applying a proposed new task - flow oriented structured regularizer that can jointly prioritize the possible tasks components and the instances. instead thus it however can can be interpreted as assuming a conventional self - long paced neural learner architecture for mtl. running a simple naive yet effective algorithm theoretically is designed for optimizing the proposed objective factor function. eliminating an error corrected bound for selecting a simplified formulation is also analyzed theoretically. experimental results on pet toy and digital real - world datasets repeatedly demonstrate the effectiveness of selecting the second proposed approach, albeit compared roughly to the obsolete state - of - past the - art cognitive methods.", "histories": [["v1", "Wed, 6 Apr 2016 03:44:03 GMT  (211kb,D)", "http://arxiv.org/abs/1604.01474v1", null], ["v2", "Mon, 3 Apr 2017 02:28:32 GMT  (153kb,D)", "http://arxiv.org/abs/1604.01474v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["changsheng li", "junchi yan", "fan wei", "weishan dong", "qingshan liu", "hongyuan zha"], "accepted": true, "id": "1604.01474"}, "pdf": {"name": "1604.01474.pdf", "metadata": {"source": "CRF", "title": "Self-Paced Multi-Task Learning", "authors": ["Changsheng Li", "Fan Wei", "Junchi Yan", "Weishan Dong", "Qingshan Liu", "Hongyuan Zha"], "emails": ["dongweis}@cn.ibm.com.", "fanwei@stanford.edu.", "jcyan@sei.ecnu.edu.cn", "qsliu@nuist.edu.cn.", "zha@cc.gatech.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nMulti-task learning (MTL) is a learning paradigm, where multiple tasks such as classification or regression tasks, are jointly learnt. One basic assumption in MTL is that there exists common or related information among the tasks; learning such information can result in better generalization performance than independently learning each individual task [1]. It is particularly desirable to share such information across the tasks, when there are many related tasks but the available training data per task is limited. Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].\nMany multi-task learning methods have been proposed, which in general can be categorized into two classes [5], [6]. The first class assumes that all the tasks share a common yet low-rank feature representation. Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12]. The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15]. To combine the two classes of approaches, some works have attempted to simultaneously learn model parameter relation and feature representation in a unified framework.\nC. Li and W. Dong are with IBM Research-China, Beijing 100094, China. Email: {lcsheng, dongweis}@cn.ibm.com.\nF. Wei is with Department of Mathematics, Stanford University. E-mail: fanwei@stanford.edu.\nJ. Yan is with East China Normal University, Shanghai, China. E-mail: jcyan@sei.ecnu.edu.cn\nQ. Liu is with Nanjing University of Information Science and Technology, Nanjing 210014, China. Email: qsliu@nuist.edu.cn.\nH. Zha is with Georgia Institute of Technology, Atlanta, USA. E-mail: zha@cc.gatech.edu\n[16], [17], [18]. However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].\nHowever, the algorithms above treat all the tasks equally and all the instances per task equally during training. Such a learning process might not be optimal from the perspective of the human brain\u2019s cognitive process. For example, a student often starts with easier concepts (e.g. recognizing objects in simple scenes where an object is clearly visible) and builds up to more complex ones (e.g. cluttered images with occlusions). In the regime of MTL, not only do there exist \u2018easy\u2019 to \u2018hard\u2019 training instances, but also \u2018easy\u2019 to \u2018hard\u2019 tasks. For example, recognizing a monkey from the dataset consisting of a monkey and a tiger is a relatively \u2018easy\u2019 task, while recognizing a baboon from the dataset consisting of baboons and orangutans is a relatively \u2018hard\u2019 task. In the first task, an image of monkey with plain background is a relatively \u2018easy\u2019 positive instance, while an image of monkey with complex background is a relatively \u2018hard\u2019 positive instance. Hence it can be advantageous to consider jointly the complexities of both instances and tasks in the multi-task model. Recently, SeqMT [22] uses a heuristic algorithm to rank all the tasks, and learns the tasks sequentially for multi-task classification tasks. SeqMT fixes the order of tasks, sequentially ordering from the previously solved tasks to the next-to-be-solved task, as it assumes that information is transferred only between subsequent tasks. However, in a dynamic setting, such a predetermined order is not flexible and comprehensive. Therefore, it is desired for the algorithm to automatically adjust the order of the tasks as the learning proceeds iteratively. In addition, SeqMT ignores the \u2018easiness\u2019 and \u2018hardness\u2019 properties of the instances during learning.\nRecently, self-paced learning (SPL) [23] proposes a paradigm advocating that learning should be first done for \u2018simple\u2019 or \u2018easy\u2019 instances, and then gradually move to \u2018complex\u2019 or \u2018hard\u2019 instances, inspired by the cognitive process of humans. By taking advantage of an instance-oriented selfpaced regularizer, SPL can dynamically adjust the learning order of the instances. This enables SPL to suit with the dynamic learning model well. Self-paced learning has been empirically demonstrated to be helpful for avoiding bad local minima, especially in the presence of heavy noise and outliers [23], [24]. By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].\nIn this paper, we propose a novel MTL framework, called Self-Paced Multi-Task Learning (SPMTL), which aims at\nar X\niv :1\n60 4.\n01 47\n4v 1\n[ cs\n.L G\n] 6\nA pr\n2 01\n6\n2 building the connection between multi-task learning and selfpaced learning in a principled manner. The contributions are: \u2022 It is the first work, to our best knowledge, where a\nprincipled MTL model jointly takes into consideration the complexities of both the training instances and tasks. Our model can be interpreted as a self-paced MTL model to explore common information among the tasks. \u2022 We propose a new regularizer, which can set priorities for both the tasks and instances in each learning iteration, and use smooth weights for the tasks and instances priorities. To the best of our knowledge, this is also the first taskoriented self-paced regularizer in literature. \u2022 We design an effective learning algorithm for our model, and we explain theoretically its performance. Experimental results on the toy and real-world datasets demonstrate the effectiveness of the proposed approach."}, {"heading": "II. SELF-PACED MULTI-TASK LEARNING", "text": "Suppose we are given m learning tasks {Ti}mi=1. For the i-th task Ti, the training set Di consists of ni data points {(xij , yij)}nij=1 , where xij \u2208 Rd is the feature representation of the j-th instance and yij is its corresponding output, such as yij \u2208 R for regression and yij \u2208 {\u22121, 1} for binary classification problem. The total number of the training instances is n = \u2211m i=1 ni. The prediction model for the ith task is defined as f(pi,xij) = pTi xij . The objective of MTL is to derive optimal prediction models for all the tasks simultaneously. In practical applications, information is often shared in the same task group, and information in different task groups is also overlapped to some degree. Thus the MTL problem can be formulated by GO-MTL [21] as:\nmin U,V m\u2211 i=1 1 ni ni\u2211 j=1 L(yij ,vTi UTxij)+ \u03b1\u2016U\u20162F +\u03b2\u2016V\u20161, (1)\nwhere V = [v1, . . . ,vm] \u2208 Rk\u00d7m. \u03b1 \u2265 0 and \u03b2 \u2265 0 are two trade-off parameters. L(yij ,vTi UTxij) is the empirical loss on the training data points (xij , yij), and Uvi acts as the learned prediction model pi. U is a d \u00d7 k matrix with each column representing a basis. V is a k\u00d7m matrix whose columns contain the coefficients of the linear combination of the basis for the corresponding tasks.\nThe key idea of GO-MTL is that the weight vector of each task can be represented as a linear combination of a subset of k basis tasks (since vi is sparse, a subset of k basis tasks is used for representing the weight vector). Therefore, the tasks with the same basis can be seen as belonging to the same group, while the tasks whose basis are orthogonal are sure to belong to different groups. The partially overlapping of bases enables the algorithm to model those tasks which are not in the same group but still have something in common.\nIn the objective function (1), all the tasks and instances are treated equally for learning the model. However, as discussed earlier, there exists the distinction between \u2018easy\u2019 and \u2018hard\u2019 across both the tasks and instances in many real-world scenarios. Inspired by the cognitive process of human that often learns from the easy to the hard, it is beneficial to incorporate the easy-to-hard strategy into the process of MTL.\nIn addition, like many MTL formulations, problem (1) is not convex, thus having the risk of stuck in bad local minima during optimization, especially in the presence of noises and outliers. Previous works have demonstrated that adding prior information, such as the easy-to-hard strategy, can alleviate the problem of bad local minima [23]. In light of these points, we attempt to tailor the MTL process by simultaneously considering the complexities of both tasks and instances, which can be interpreted as a self-paced MTL framework. We propose a new objective function as follows:\nmin w,U,V m\u2211 i=1 1 ni ni\u2211 j=1 w (i) j L(yij ,v T i U Txij) + \u03b1\u2016U\u20162F\n+\u03b2\u2016V\u20161 + f(w, \u03bb, \u03b3) (2) s.t. w\n(i) j \u2208 [0, 1],\u2200j = 1, . . . ni, i = 1, . . . ,m,\nwhere w = [w(1)1 , . . . , w (1) n1 , w (2) 1 , . . . , w (2) n2 , . . . , w (m) nm ] \u2208 Rn denotes the importance weights imposed on all instances. Different from the objective function (1), the first term in (2) is a weighted loss term on all the instances. f(w, \u03bb, \u03b3) denotes the self-paced regularizer that determines the instances and tasks used for training. So far, many self-paced regularizers have been proposed for various tasks [23], [27], but almost all of them focus on imposing weights on the instance level. Thus, in order to simultaneously impose weights on the instance level and the task level, we propose a new self-paced regularizer as follows:\nf(w, \u03bb, \u03b3)=\u2212\u03bb m\u2211 i=1 ni\u2211 j=1 w (i) j +\u03b3 m\u2211 i=1 \u221a\u221a\u221a\u221a 1 ni ni\u2211 j=1 (w (i) j ) 2\n= \u2212\u03bb m\u2211 i=1 \u2016w(i)\u20161 + \u03b3 m\u2211 i=1 1 \u221a ni \u2016w(i)\u20162, (3)\nwhere w(i) = [w(i)1 , . . . , w (i) ni ] \u2208 [0, 1]ni , and thus w = [w(1), . . . ,w(m)]. \u03bb and \u03b3 are two self-paced parameters imposed on the instance level and the task level, respectively. There are two terms in Eq. (3): The first term is the negative l1-norm inherited from the conventional SPL, which favors selecting the easy over hard instances. The second term is an adaptive l2,1-norm of a matrix, which favors selecting the easy over hard tasks. We use 1\u221ani in the second term to avoid the task imbalance, when one task has so many data points that dominates the norm. We know that minimizing the l2,1 norm of a matrix can make the matrix sparse in rows or columns [7]; thus minimizing the second term in (3) makes the w(i)\u2019s corresponding to large empirical loss L (i.e., hard tasks) be close to or equal to zero vectors. In other words, this groupsparsity representation is expected to realize task selection from \u2018easy\u2019 ones to \u2018hard\u2019 ones. As the learning proceeds, \u2018harder\u2019 instances and tasks will be included in training by gradually increasing \u03bb and reducing \u03b3.\nPlugging (3) into (2), we obtain the final objective function:\n3 min w,U,V m\u2211 i=1 1 ni w(i)L\u0302(i) + \u03b1\u2016U\u20162F + \u03b2\u2016V\u20161\n\u2212\u03bb m\u2211 i=1 \u2016w(i)\u20161 + \u03b3 m\u2211 i=1 1 \u221a ni \u2016w(i)\u20162 (4)\ns.t. w(i) \u2208 [0, 1]ni ,\u2200i = 1, . . . ,m,\nwhere the vector L\u0302(i) = [L(i)1 , . . . ,L (i) ni ] T . In this paper, we focus on the regression tasks, and define L(i)j = L(yij ,vTi UTxij) = (yij \u2212 vTi UTxij)2. Note our method can be naturally applied to classification tasks by using a classification loss function L(i)j as in [28]."}, {"heading": "III. OPTIMIZATION", "text": "We solve the optimization problem (4) in an alternating fashion by the following three main steps. i) Optimize w with fixed U and V: the optimal w can be obtained by decomposing the optimization function into individual problems for each task Ti:\nmin w(i)\u2208[0,1]ni\nLi := 1\nni w(i)L\u0302(i)\u2212\u03bb\u2016w(i)\u20161+ \u03b3 \u221a ni \u2016w(i)\u20162. (5)\nFirst we assume L(i)1 \u2264 L (i) 2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 L (i) ni . Let p(i) =\u2211\nk0<j<k1\n(\u03bb\u2212 L (i) j\nni )2, and q(i) = \u2211 k0<j<k1 (\u03bb\u2212 L (i) j ni ). For each i\nand arbitrary k1 > k0, we define c\u2217(k0, k1), L(k0, k1), G\u2217i , S \u2217 i for later computation: 1)\nc\u2217(k0, k1) =  \u221a k0ni/(\u03b32 \u2212 nip(i)), if \u03b3 2 ni 6= p(i),( \u03bb\u2212 L(i)k0+1/ni )\u22121 , if \u03b3 2 ni = p(i) and \u03b3 2 ni < q(i),\n0, if \u03b3 2\nni =p(i)and \u03b3\n2\nni \u2265 q(i). 2) L(k0, k1) = \u2211k0 j=1 L(i)j ni \u2212 \u03bb(k0 + c\u2217(k0, k1)q(i)) +\n\u03b3\u221a ni\n\u221a k0 + c\u2217(k0, k1)2p(i).\n3) G\u2217i be the smallest j such that L (i) j \u2265 \u03bbni. 4) S\u2217i be the largest j such that L (i) j \u2264 ni\u03bb\u2212 \u221a ni\u03b3.\nThe following theorem gives the global optimum of (5).\nTheorem 1. Let k1 = G\u2217i , and k0 be obtained by optimizing\nk0 = arg min S\u2217i \u2264k0<k1 L(k0, k1) (6)\ns.t.  \u03b32 ni \u2212 p(i) \u2265 0, or positive if k0 > 0, (7) c\u2217(k0, k1)(\u03bb\u2212L(i)k0+1/ni) < 1, if k0 + 1<k1,(8) L(i)k0 ni + \u03b3 \u221a ni ( k0 + c \u2217(k0, k1) 2 p(i) )\u22121/2\n\u2264 \u03bb, if k0 + 1 < k1. (9)\nThen, the optimal w(i) is given by,\nw (i) j =  1, if j \u2264 k0, 0, if j \u2265 k1,\nc\u2217(k0, k1)(\u03bb\u2212 L(i)j ni ), if k0 < j < k1.\n(10)\nThus it takes only linear time O(ni) to compute w(i).\nProof of Theorem 1 is in Appendix at the end of the paper. ii) Optimize U with fixed w and V: This step is to learn the k basis tasks. The optimal U can be obtained by solving:\nmin U m\u2211 i=1 1 ni w(i)L\u0302(i) + \u03b1\u2016U\u20162F , (11)\nThe necessary optimality condition is that the derivative of (11) with respective to U is equal to zeros. Thus, we have\nm\u2211 i=1 ni\u2211 j=1 w (i) j ni xijx T ijUviv T i + \u03b1U = m\u2211 i=1 ni\u2211 j=1 w (i) j ni yijxijv T i ,\n\u21d2  m\u2211 i=1 ni\u2211 j=1 w (i) j ni (viv T i )\u2297 (xijxTij) + \u03b1I  vec(U) =\nm\u2211 i=1 ni\u2211 j=1 w (i) j ni yijvec(xijvTi ), (12)\nwhere \u2297 denotes the Kronecker product and vec(\u00b7) is an operator that reshapes a matrix of size d\u00d7 k into a vector of size dk\u00d71. Here we have used a property of Kronecker product that vec(ABC) = (CT\u2297A)vec(B). This is the standard form of system of linear equations that is full rank and thus has a unique solution. We can solve it using many efficient methods, such as LU decomposition. iii) Optimize V with fixed w and U: This step aims to learn the coefficient matrix. The optimization problem can be decomposed into individual problems for vi as:\nmin vi\n1\nni w(i)L\u0302(i) + \u03b2\u2016vi\u20161. (13)\nThe above problem is a l1 regularized optimization problem with 1niw\n(i)L\u0302(i) differentiable with respect to vi. To solve this problem, a variety of efficient methods, such as optimal projected gradient, can be used."}, {"heading": "IV. THEORETICAL ANALYSIS", "text": "Here we relax the two penalty terms \u03b1\u2016U\u20162F +\u03b2\u2016V\u20161 in our formulation (2) to a weaker constraint: U,V are bounded by a constant C (it can also be changed to bounded rank). This relaxation renders our following theoretical analysis approximate while more mathematically tractable and consumable. In fact, this simplification shares the same motivation for our raw formulation as intuitively we want U,V to be simple.\nThe true values are yrij = v rT i U rTxij . We observe yij = yrij+eij with eij independent Gaussian distribution with mean 0. In each round, to solve for U,V is to solve\nmin U,V bounded m\u2211 i=1 1 ni ni\u2211 j=1 w (i) j (yij\u2212v T i U Txij) 2. (14)\nLet the weighted error (or the \u2018average\u2019 error) be defined as\nER = \u221a\u221a\u221a\u221a m\u2211 i=1 1 ni ni\u2211 j=1 w (i) j (y r ij \u2212 vTi UTxij)2. (15)\n4 Theorem 2. Let the matrix W be [W]ij = ( w (i) j /ni )1/2 . Then\nER \u2264 2 \u221a\u221a\u221a\u221a\u2211 ij w (i) j ni e2ij = 2\u2016W E\u20162.\nwhere [E]ij = eij and is the entry-wise multiplication operator. Let [\u03a3]ij = Var(eij) and \u2211 ni = N . For each fixed > 0, with probability at least 1 \u2212 2Ne\u2212 2/4, ER \u2264 \u2016W \u03a3\u20162.\nProof. Let U\u2217,V\u2217 be the optimal solution. We have\nER = \u221a\u221a\u221a\u221a m\u2211 i=1 1 ni ni\u2211 j=1 w (i) j (yij \u2212 v\u2217Ti U\u2217 Txij \u2212 eij)2\n\u2264 \u221a\u221a\u221a\u221a m\u2211 i=1 1 ni ni\u2211 j=1 w (i) j (yij\u2212v\u2217Ti U\u2217 Txij)2 + \u2016W E\u20162\n\u2264 \u221a\u221a\u221a\u221a m\u2211 i=1 1 ni ni\u2211 j=1 w (i) j (yij \u2212 vrTi Ur Txij)2 + \u2016W E\u20162\n= \u221a\u221a\u221a\u221a m\u2211 i=1 1 ni ni\u2211 j=1 w (i) j e 2 ij + \u2016W E\u20162 = 2\u2016W E\u20162,\nThe last inequality holds because U\u2217,V\u2217 are the ones minimizing (14).\nTo prove the concentration inequality, we notice that\nPr 2 \u221a\u221a\u221a\u221a m\u2211\ni=1 ni\u2211 j=1 w (i) j ni e2ij \u2265 \u221a\u221a\u221a\u221a m\u2211 i=1 ni\u2211 j=1 w (i) j ni Var(eij)  \u2264Pr ( exists i, j : e2ij \u2265 2Var(eij)/4 ) .\nThis is because the event 2 \u221a\u2211m i=1 \u2211ni j=1 w (i) j ni e2ij \u2265\n\u221a\u2211m i=1 \u2211ni j=1 w (i) j ni Var(eij) is contained in the event that there exists i, j with e2ij \u2265 2Var(eij)/4. Since eij is distributed as N(0, \u03c3ij) where \u03c32ij = Var(eij), we have Pr(|eij | \u2265 \u03c3ij/2) \u2264 2 exp(\u2212 2/4). Therefore by the union bound, Pr ( exists ij : e2ij \u2265 2Var(eij)/4 ) \u2264\u2211\nij Pr ( e2ij \u2265 2Var(eij)/4 ) \u2264 2N exp(\u2212 2/4).\nFrom the above analysis we can see that, if we fix the set of the values of w(i)j and can exchange the weights between w (i) j and w(i)j\u2032 , then the upper bound for the \u2018average\u2019 error ER is minimized when the samples with higher noise variance have smaller weights. In other words, harder samples should be given smaller weights. And with probability exponentially small, the error is large."}, {"heading": "V. EXPERIMENTS", "text": "We conduct the experiments on one toy dataset and two realworld datasets to illustrate the effectiveness of our method. We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and\na recently proposed method, called Calibration [30].1 In addition, we also compare with single task learning (STL) where tasks are learned independently. Note that we do not compare with SeqMT [22] because it is hard to have a fair comparison since SeqMT is tailored to solve classification tasks, while our current model focuses on regression problems. For all the datasets, we randomly select the training instances from each task with different training ratios (10%, 20% and 30%) and use the rest of instances to form the testing set. We evaluate all the algorithms in terms of both root mean squared error (rMSE) and normalized mean squared error (nMSE), which are commonly used in multi-task learning problem [31], [32]. To have a fair comparison, we validate the regularization parameters of all the methods in the same search space on a subset of the training set, and use the optimal parameters to train the final models. The initial self-paced parameters \u03bb and \u03b3 are set such that more than half of tasks are selected, and then they are iteratively increased and decreased, respectively. We repeat each case 10 times and report the average results."}, {"heading": "A. Toy Example", "text": "We first describe the synthetic data generation procedure. Let there be 3 groups and each group has 10 tasks. There are 100 instances in each task; each instance is represented by a 15-dimensional feature vectors. We generate parameter vectors for 4 latent tasks, i.e., U in the proposed formulation, in 20 dimensions, with each entry drawn i.i.d. from a standard normal distribution. Based on U, we generate the first 10 tasks by linearly combining only the first two latent tasks. In a similar manner, the next 10 tasks are generated by linearly combining the second and the third latent tasks. Last 10 task are generated by linear combination of the last two latent tasks. All the coefficients of linear combinations, i.e., V, are drawn i.i.d. from a standard normal distribution. The instance xij is sampled from a standard Gaussian distribution, and the response is yij = vTi U\nTxij+\u03beij . To create tasks with different difficulty levels, we add different noise to tasks and instances by setting \u03beij = \u03c3i\u03b8j , where \u03c3i\u2019s are i.i.d. from a normal distribution N(0, 5), and \u03b8j is drawn i.i.d. from N(0, 1).\nWe first visualize w in (2) using 20% training data. Figure 1 (a), (b), and (c) show that in the earlier iterations, some \u2018hard\u2019 tasks are not selected for learning (\u2016wi\u20161 = 0). In later iterations, more tasks are included (0 < \u2016wi\u20161 \u2264 20). Similarly, instances in one task are also selected from \u2018easy\u2019 ones to \u2018hard\u2019 ones, as shown in Figure 1 (d), (e), and (f).\nWe also report the statistical results on this data in Table I. From these results we conclude that: (1) All multi-task learning methods outperform single-task learning methods, which proves the effectiveness of multi-task learning. (2) Our proposed self-paced multi-task learning method outperforms all other MTL methods. In addition, as can be seen from the objective functions (1) and (2), GO-MTL is a special case to our method: when all the entries in w are 1, our method is reduced to GO-MTL. This shows that our method can improve the prediction performance of the model by jointly considering the complexities of the tasks and the instances.\n1The codes of the compared methods are obtained from the corresponding authors.\n5\nTABLE II PERFORMANCE COMPARISON AMONG DIFFERENT METHODS ON THE OHSUMED DATASET IN TERMS OF RMSE AND NMSE.\nMeasure Training ratio STL DG-MTL Calibration MSMTFL GO-MTL SPMTL\nrMSE 10% 1.0690 0.8197 0.6346 0.7528 0.7762 0.6298 20% 0.8400 0.6961 0.6341 0.7519 0.6372 0.6140 30% 0.7474 0.6527 0.6140 0.7512 0.6105 0.5946 nMSE 10% 3.9915 2.1414 1.0719 1.4592 1.7331 1.0844 20% 2.3802 1.4228 1.0804 1.4584 1.1433 1.0286 30% 1.7118 1.2022 1.0134 1.4578 1.1433 0.9661\nTABLE III PERFORMANCE COMPARISON AMONG DIFFERENT METHODS ON THE ISOLET DATASET IN TERMS OF RMSE AND NMSE.\nMeasure Training ratio STL DG-MTL Calibration MSMTFL GO-MTL SPMTL\nrMSE 10% 6.1931 5.8498 5.8494 6.3696 5.9806 5.2505 20% 5.6947 5.4044 5.6002 5.9407 5.2275 4.9706 30% 5.4524 5.2210 5.3867 5.6384 5.0173 4.8697 nMSE 10% 0.6836 0.6098 0.6088 0.7239 0.6361 0.4902 20% 0.5773 0.5199 0.5578 0.6284 0.4859 0.4393 30% 0.5275 0.4837 0.5149 0.5640 0.4464 0.4205"}, {"heading": "B. Real-World Data Experiments", "text": "In this section, we conduct the experiments on two realworld datasets: OHSUMED and Isolet. The first dataset is a bibliographical online searching document collection. It consists of 53 queries, which are our tasks. Each query comes with multiple returned documents with labels indicating how relevant the returned document is to the query: \u201cdefinitely relevant\u201d, \u201cpossibly relevant\u201d, or \u201cnot relevant\u201d. These documents and their labels are our instances. There are in total 7,546 instances. The response is the relevance level (0,1,2). The second dataset is collected from 150 individuals each pronounces the English alphabet twice. Thus there are 52 samples from each individual. Each English letter corresponds to a label (1-26) and we treat the labels as regression values as [29]. The individuals are grouped into 5 groups by speaking similarity. Thus, we naturally have 5 tasks with each task corresponding to a group. There are 1560, 1560, 1560, 1558, and 1559 instances in the 5 tasks respectively.\nTables II and III report the performance measured by rMSE and nMSE for the OHSUMED dataset and the Isolet dataset, respectively. Our SPMTL outperforms all the compared methods with smaller error rates on both real datasets. This again demonstrates that our method is effective by introducing selfpaced learning scheme into multi-task learning regime.\nWe also test the effectiveness of considering either or both instance order and task order in our method on the OHSUMED dataset. By setting \u03b3 = 0 in (4), we only consider the complexities of instances. We call it Self-Paced Instance Weight Learning (SPIWL). We conduct the experiments on the 30% training data with the results shown in Figure 2. SPIWL performs better than GO-MTL, which suggests including the instances from \u2018easy\u2019 to \u2018hard\u2019 improves the performance. SPMTL outperforms SPIWL, which shows that learning \u2018easy\u2019\n5\n10 15 20 25 30 0 5 10\n15\n20\n(a) iter=1\n5\n10 15 20 25 30 0 5 10\n15\n20\n(b) iter=15\n5\n10 15 20 25 30 0 5 10\n15\n20\n(c) iter=30\n5\n10\n15\n20 0\n0.2\n0.4\n0.6\n0.8\n1\n(d) iter=1\n5\n10\n15\n20 0\n0.2\n0.4\n0.6\n0.8\n1\n(e) iter=15\n5\n10\n15\n20 0\n0.2\n0.4\n0.6\n0.8\n1\n(f) iter=30\nFig. 1. The visualization of the learned w. The rows in (a), (b), and (c) denote the corresponding task weights, i.e., the summation of the weights of all the instances in one task. The rows in (d), (e), and (f) denote the corresponding instance weights in the i-th task, here i=18. Dark blue denotes that the values are close to zero.\n0.58\n0.59\n0.6\n0.61\n0.62\nrM S\nE\nGO\u2212MTL SPIWL SPMTL\n(a) rMSE\n0.9\n0.95\n1\n1.05\n1.1\n1.15\nnM S\nE\nGO\u2212MTL SPIWL SPMTL\n(b) nMSE\nFig. 2. Effectiveness verification of considering task order and instance order on the OHSUMED dataset.\ntasks first and gradually adding \u2018hard\u2019 tasks into training can also be helpful for prediction."}, {"heading": "VI. CONCLUSION", "text": "We proposed a novel MTL algorithm, namely SPMTL. Unlike previous works that treat tasks and instances equally during learning, our model gradually include instances and tasks according to an easy-to-hard order. To achieve this goal, we propose a new task-oriented self-paced regularizer in our\n6 formulation. Experiments on both synthetic dataset and real datasets have verified the effectiveness of SPMTL."}, {"heading": "VII. APPENDIX", "text": "Proof of Theorem 1. Recall that L(i)1 \u2264 L (i) 2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 L (i) ni . The optimal solution of (5) should be w(i)j \u2265 w (i) j\u2032 for j < j\n\u2032, since otherwise we can swap the values for w(i)j and w (i) j\u2032 to decrease the value in (5). Thus w(i)1 \u2265 w (i) 2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 w (i) ni .\nSince w(i)j \u2208 [0, 1], we know that in the optimal solution,\neither \u2202Li(w\n(i) j )\n\u2202w (i) j\n= 0 or w(i)j = 0, 1. For a given j, we have\n\u2202Li\n\u2202w (i) j\n= L(i)j ni \u2212 \u03bb+ \u03b3w (i) j\u221a ni \u2211 s(w (i) s )2 . (16)\nIt is clear that w (i) j\u221a\u2211 s(w (i) s )2 \u2208 [0, 1]. If L (i) j ni \u2265 \u03bb, then \u2202Li \u2202w (i) j \u2265 0; thus Li is monotonely increasing in w (i) j . To minimize (5) we should let w(i)j = 0. Similarly, L(i)j ni \u2212 \u03bb \u2264 \u2212 \u03b3\u221ani implies\n\u2202Li \u2202w\n(i) j\n\u2264 0; Li is decreasing in w(i)j . Thus w (i) j = 1. Thus\nk0 \u2265 S\u2217i , k1 \u2264 G\u2217i by the monotonicity of w (i) j \u2019s.\nWhen k1 > k0+1, the rest of the w (i) j \u2019s satisify\n\u2202Li(w (i) j )\n\u2202w (i) j\n=\n0. Equation (16) tells us that the rest of the w(i)j \u2019s are proportional to L(i)j ni \u2212 \u03bb when fixing \u2211 s(w (i) s )2. Thus for some constant c to be found later, w(i)j = c(\u03bb \u2212 L (i) j /ni). Since recall w(i)j = 1 for j \u2264 k0 and w (i) j = 0 for j \u2265 k1, we\nhave \u2211 s(w (i) s )2 = k0+ \u2211 k0<j<k1\nc2(\u03bb\u2212L(i)j /ni)2. Plugging in back to (16), we have\n0 = L(i)j ni \u2212 \u03bb+ \u03b3c(\u03bb\u2212 L(i)j /ni) \u221a ni \u221a k0 + \u2211 k0<j<k1 c2(\u03bb\u2212 L(i)j /ni)2 .\nSince \u03bb\u2212 L(i)j /ni 6= 0 as S\u2217i < j < G\u2217i , we have c2\u03b32/ni = k0 + \u2211 k0<j<k1 c2(\u03bb\u2212 L(i)j /ni) 2. (17)\nIn order to solve c, we need \u2211 k0<j<k1\nc2(\u03bb \u2212 L(i)j /ni)2< \u03b32/ni if k0 > 0, and \u2211 k0<j<k1\nc2(\u03bb\u2212L(i)j /ni)2 \u2264 \u03b32/ni if k0 = 0. Thus c = c\u2217(k0, k1) by solving the above equation if k0 6= 0 or \u03b32/ni 6= \u2211 k0<j<k1\n(\u03bb\u2212L(i)j /ni)2. By the constraint 0 \u2264 w(i)j \u2264 1 and the monotonicity of w (i) j , we just need w (i) k0+1\n= c\u2217(k0, k1)(\u03bb\u2212 L(i)k0+1/ni) \u2264 1. By plugging in, the value in (5) is exactly L(k0, k1) defined before. Recall that we also need the constraints above for this set of solution to be valid, which are precisely the constraints (7), (8).\nWhen k0 = 0 and \u03b32/ni = \u2211 k0<j<k1\n(\u03bb \u2212 L(i)j /ni)2, we cannot obtain c from (17). In this case, we have\nL(k0, k1) = k0\u2211 j=1 L(i)j ni \u2212 \u03bbc \u2211 k0<j<k1 (\u03bb\u2212 L(i)j ni ) + \u03b32 ni c.\nL(k0, k1) is a linear function in c. To minimize it, we need the sign of \u2202L\u2202c = \u2212\u03bb \u2211 k0<j<k1 (\u03bb \u2212 L (i) j ni ) + \u03b3 2 ni . If the sign is non-negative, then L(k0, k1) is monotone increasing in c.\nThus we choose the minimum c, i.e., 0. If the sign is negative, then L(k0, k1) is monotone decreasing in c, and we pick the maximum c. Since w(i)j = c(\u03bb \u2212 L(i)j ni\n) \u2264 1, and L(i)j \u2019s are monotone, we have c\u2217(k0, k1) = 1/(\u03bb\u2212 L(i)k0+1/ni).\nThe optimal values for w(i) is determined by choosing the minimum of all the L(k0, k1) after plugging in c\u2217(k0, k1) under the constraints.\nIn the optimal solution w(i)j \u2019s have achieved the maximum value 1 for j \u2264 k0 implying \u2202Li\n\u2202w (i) j\n\u2264 0. Plugging in\nthe values for w(i), for j \u2264 k0 we have 0 \u2265 L(i)j ni \u2212\n\u03bb + \u03b3/ \u221a ni\u221a\nk0+ \u2211k1\u22121\nt=k0+1 c\u2217(k0,k1)\n2(\u03bb\u2212L(i)t /ni)2 . By monotonicity of\nL(i)j \u2019s, we just need to guarantee it holds for j = k0 + 1. Similarly, if w(i)j = 0 for j \u2265 k1, then\n\u2202Li \u2202w\n(i) j\n\u2265 0. Thus\nfor j \u2265 k1, we have \u2202Li \u2202w\n(i) j\n= L(i)j ni \u2212 \u03bb+ \u03b2\u00b70\u221a\u2211\ns(w (i) s )2 \u2265 0. By\nmonotonicity of L(i)j \u2019s, we have L(i)k1 ni \u2265 \u03bb. Notice that G\u2217i is defined to be the smallest j such that L(i)j ni \u2265 \u03bb and k1 \u2264 G\u2217i . We must have k1 = G\u2217i ."}], "references": [{"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine learning, vol. 28, no. 1, pp. 41\u201375, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "A multi-task learning formulation for predicting disease progression", "author": ["J. Zhou", "L. Yuan", "J. Liu", "J. Ye"], "venue": "KDD, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Boosted multi-task learning for face verification with applications to web image and video search", "author": ["X. Wang", "C. Zhang", "Z. Zhang"], "venue": "CVPR, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Predicting multiple attributes via relative multi-task learning", "author": ["L. Chen", "Q. Zhang", "B. Li"], "venue": "CVPR, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning with whom to share in multi-task feature learning", "author": ["Z. Kang", "K. Grauman", "F. Sha"], "venue": "ICML, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple task learning using iteratively reweighted least square", "author": ["J. Pu", "Y.-G. Jiang", "J. Wang", "X. Xue"], "venue": "IJCAI, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, vol. 73, no. 3, pp. 243\u2013272, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Tree-guided group lasso for multi-task regression with structured sparsity", "author": ["S. Kim", "E.P. Xing"], "venue": "2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic multi-task feature selection", "author": ["Y. Zhang", "D.-Y. Yeung", "Q. Xu"], "venue": "NIPS, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-task feature learning via efficient l 2, 1-norm minimization", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "UAI, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "An efficient projection for l1,\u221e regularization", "author": ["A. Quattoni", "X. Carreras", "M. Collins", "T. Darrell"], "venue": "ICML, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Trace norm regularization: Reformulations, algorithms, and multi-task learning", "author": ["T.K. Pong", "P. Tseng", "S. Ji", "J. Ye"], "venue": "SIAM Journal on Optimization, vol. 20, no. 6, pp. 3465\u20133489, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning multiple related tasks using latent independent component analysis", "author": ["J. Zhang", "Z. Ghahramani", "Y. Yang"], "venue": "NIPS, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "JMLR, vol. 6, pp. 1817\u20131853, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1817}, {"title": "A convex formulation for learning shared structures from multiple tasks", "author": ["J. Chen", "L. Tang", "J. Liu", "J. Ye"], "venue": "ICML, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-task learning with gaussian matrix generalized inverse gaussian model", "author": ["M. Yang", "Y. Li"], "venue": "ICML, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian max-margin multi-task learning with data augmentation", "author": ["C. Li", "J. Zhu", "J. Chen"], "venue": "ICML, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-task model and feature joint learning", "author": ["Y. Li", "X. Tian", "T. Liu", "D. Tao"], "venue": "IJCAI, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A dirty model for multi-task learning", "author": ["A. Jalali", "S. Sanghavi", "C. Ruan", "P.K. Ravikumar"], "venue": "NIPS, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Integrating low-rank and group-sparse structures for robust multi-task learning", "author": ["J. Chen", "J. Zhou", "J. Ye"], "venue": "KDD, 2011.  7", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning task grouping and overlap in multi-task learning", "author": ["A. Kumar", "H. Daume III"], "venue": "ICML, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Curriculum learning of multiple tasks", "author": ["A. Pentina", "V. Sharmanska", "C.H. Lampert"], "venue": "CVPR, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "NIPS, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Selfpaced learning with diversity", "author": ["L. Jiang", "D. Meng", "S.-I. Yu", "Z. Lan", "S. Shan", "A. Hauptmann"], "venue": "NIPS, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Multi-view self-paced learning for clustering", "author": ["C. Xu", "D. Tao", "C. Xu"], "venue": "IJCAI, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-paced learning for matrix factorization", "author": ["Q. Zhao", "D. Meng", "L. Jiang", "Q. Xie", "Z. Xu", "A.G. Hauptmann"], "venue": "AAAI, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "A selfpaced multiple-instance learning framework for co-saliency detection", "author": ["D. Zhang", "D. Meng", "C. Li", "L. Jiang", "Q. Zhao", "J. Han"], "venue": "ICCV, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Regularized multi-task learning", "author": ["T. Evgeniou", "M. Pontil"], "venue": "KDD, 2004.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-stage multi-task feature learning", "author": ["P. Gong", "J. Ye", "C. Zhang"], "venue": "JMLR, vol. 14, no. 1, pp. 2979\u20133010, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficient multi-task feature learning with calibration", "author": ["P. Gong", "J. Zhou", "W. Fan", "J. Ye"], "venue": "KDD, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "UAI, 2010.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorial multi-task learning: a bayesian nonparametric approach", "author": ["S. Gupta", "D. Phung", "S. Venkatesh"], "venue": "ICML, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "One basic assumption in MTL is that there exists common or related information among the tasks; learning such information can result in better generalization performance than independently learning each individual task [1].", "startOffset": 219, "endOffset": 222}, {"referenceID": 1, "context": "Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "Due to its empirical success, MTL has been applied to various domains, including disease modeling and prediction [2], web image and video search [3], relative attributes learning [4].", "startOffset": 179, "endOffset": 182}, {"referenceID": 4, "context": "Many multi-task learning methods have been proposed, which in general can be categorized into two classes [5], [6].", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "Many multi-task learning methods have been proposed, which in general can be categorized into two classes [5], [6].", "startOffset": 111, "endOffset": 114}, {"referenceID": 6, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 68, "endOffset": 71}, {"referenceID": 7, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 210, "endOffset": 214}, {"referenceID": 10, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 241, "endOffset": 245}, {"referenceID": 11, "context": "Algorithms in this class include convex multi-task feature learning [7], tree-guided group lasso [8], probabilistic multi-task feature selection [9], multi-task feature selection using l2,1 norm regularization [10], l1,\u221e norm regularization [11] and trace norm regularization [12].", "startOffset": 276, "endOffset": 280}, {"referenceID": 12, "context": "The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "The other class of methods assumes that the model parameters used by the tasks are related to each other [13], [14], [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 15, "context": "edu [16], [17], [18].", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "edu [16], [17], [18].", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "edu [16], [17], [18].", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].", "startOffset": 303, "endOffset": 307}, {"referenceID": 19, "context": "However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].", "startOffset": 309, "endOffset": 313}, {"referenceID": 20, "context": "However, the assumption that all the tasks share some common information is fairly strong, recent works propose task grouping or task outlier detecting, which assume that there exists common information only within a subset of tasks, or there exists outlier task having no relation with the other tasks [19], [20], [21].", "startOffset": 315, "endOffset": 319}, {"referenceID": 21, "context": "Recently, SeqMT [22] uses a heuristic algorithm to rank all the tasks, and learns the tasks sequentially for multi-task classification tasks.", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "Recently, self-paced learning (SPL) [23] proposes a paradigm advocating that learning should be first done for \u2018simple\u2019 or \u2018easy\u2019 instances, and then gradually move to \u2018complex\u2019 or \u2018hard\u2019 instances, inspired by the cognitive process of humans.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Self-paced learning has been empirically demonstrated to be helpful for avoiding bad local minima, especially in the presence of heavy noise and outliers [23], [24].", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "Self-paced learning has been empirically demonstrated to be helpful for avoiding bad local minima, especially in the presence of heavy noise and outliers [23], [24].", "startOffset": 160, "endOffset": 164}, {"referenceID": 24, "context": "By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].", "startOffset": 93, "endOffset": 97}, {"referenceID": 25, "context": "By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "By now, it has been successfully applied to many research areas, such as multi-view learning [25], matrix factorization [26], and multi-instance learning [27].", "startOffset": 154, "endOffset": 158}, {"referenceID": 20, "context": "Thus the MTL problem can be formulated by GO-MTL [21] as:", "startOffset": 49, "endOffset": 53}, {"referenceID": 22, "context": "Previous works have demonstrated that adding prior information, such as the easy-to-hard strategy, can alleviate the problem of bad local minima [23].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "w (i) j \u2208 [0, 1],\u2200j = 1, .", "startOffset": 10, "endOffset": 16}, {"referenceID": 22, "context": "So far, many self-paced regularizers have been proposed for various tasks [23], [27], but almost all of them focus on imposing weights on the instance level.", "startOffset": 74, "endOffset": 78}, {"referenceID": 26, "context": "So far, many self-paced regularizers have been proposed for various tasks [23], [27], but almost all of them focus on imposing weights on the instance level.", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": ", w (i) ni ] \u2208 [0, 1]i , and thus w = [w, .", "startOffset": 15, "endOffset": 21}, {"referenceID": 6, "context": "We know that minimizing the l2,1 norm of a matrix can make the matrix sparse in rows or columns [7]; thus minimizing the second term in (3) makes the w\u2019s corresponding to large empirical loss L (i.", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "w \u2208 [0, 1]i ,\u2200i = 1, .", "startOffset": 4, "endOffset": 10}, {"referenceID": 27, "context": "Note our method can be naturally applied to classification tasks by using a classification loss function L j as in [28].", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "min w(i)\u2208[0,1]ni Li := 1 ni wL\u0302\u2212\u03bb\u2016w\u20161+ \u03b3 \u221a ni \u2016w\u20162.", "startOffset": 9, "endOffset": 14}, {"referenceID": 4, "context": "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].", "startOffset": 74, "endOffset": 77}, {"referenceID": 20, "context": "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 28, "context": "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].", "startOffset": 99, "endOffset": 103}, {"referenceID": 29, "context": "We compare it with several state-of-the-art MTL methods, including DG-MTL [5], GO-MTL [21], MSMTFL [29], and a recently proposed method, called Calibration [30].", "startOffset": 156, "endOffset": 160}, {"referenceID": 21, "context": "Note that we do not compare with SeqMT [22] because it is hard to have a fair comparison since SeqMT is tailored to solve classification tasks, while our current model focuses on regression problems.", "startOffset": 39, "endOffset": 43}, {"referenceID": 30, "context": "We evaluate all the algorithms in terms of both root mean squared error (rMSE) and normalized mean squared error (nMSE), which are commonly used in multi-task learning problem [31], [32].", "startOffset": 176, "endOffset": 180}, {"referenceID": 31, "context": "We evaluate all the algorithms in terms of both root mean squared error (rMSE) and normalized mean squared error (nMSE), which are commonly used in multi-task learning problem [31], [32].", "startOffset": 182, "endOffset": 186}, {"referenceID": 28, "context": "Each English letter corresponds to a label (1-26) and we treat the labels as regression values as [29].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "Since w j \u2208 [0, 1], we know that in the optimal solution, either \u2202Li(w (i) j ) \u2202w (i) j = 0 or w j = 0, 1.", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "It is clear that w (i) j \u221a\u2211 s(w (i) s )2 \u2208 [0, 1].", "startOffset": 43, "endOffset": 49}], "year": 2017, "abstractText": "In this paper, we propose a novel multi-task learning (MTL) framework, called Self-Paced Multi-Task Learning (SPMTL). Different from previous works treating all tasks and instances equally when training, SPMTL attempts to jointly learn the tasks by taking into consideration the complexities of both tasks and instances. This is inspired by the cognitive process of human brain that often learns from the easy to the hard. We construct a compact SPMTL formulation by proposing a new task-oriented regularizer that can jointly prioritize the tasks and the instances. Thus it can be interpreted as a selfpaced learner for MTL. A simple yet effective algorithm is designed for optimizing the proposed objective function. An error bound for a simplified formulation is also analyzed theoretically. Experimental results on toy and real-world datasets demonstrate the effectiveness of the proposed approach, compared to the stateof-the-art methods.", "creator": "LaTeX with hyperref package"}}}