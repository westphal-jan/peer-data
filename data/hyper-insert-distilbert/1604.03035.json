{"id": "1604.03035", "review": {"conference": "HLT-NAACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2016", "title": "Learning Global Features for Coreference Resolution", "abstract": "there is compelling evidence here that coreference prediction would benefit from modeling correct global information predictions about other entity - mentioned clusters. yet, minimal state - being of - only the - art art global performance evaluation can be further achieved with systems accurately treating each data mention element prediction independently, which implies we attribute adequately to enhance the chronic inherent difficulty of continuously crafting specific informative cluster - level features. we ourselves instead typically propose to use recurrent automated neural networks ( rnns ) to learn latent, global representations of mathematical entity clusters directly from defining their particular mentions., we now show that such representations are especially useful for the prediction of pronominal referenced mentions, and graphs can be formally incorporated loosely into an end - source to - end binary coreference plan system that outperforms theoretically the operational state of the art without requiring any more additional search.", "histories": [["v1", "Mon, 11 Apr 2016 17:15:34 GMT  (667kb,D)", "http://arxiv.org/abs/1604.03035v1", "Accepted to NAACL 2016"]], "COMMENTS": "Accepted to NAACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sam wiseman", "alexander m rush", "stuart m shieber"], "accepted": true, "id": "1604.03035"}, "pdf": {"name": "1604.03035.pdf", "metadata": {"source": "CRF", "title": "Learning Global Features for Coreference Resolution", "authors": ["Sam Wiseman"], "emails": ["swiseman@seas.harvard.edu", "srush@seas.harvard.edu", "shieber@seas.harvard.edu"], "sections": [{"heading": "1 Introduction", "text": "While structured, non-local coreference models would seem to hold promise for avoiding many common coreference errors (as discussed further in Section 3), the results of employing such models in practice are decidedly mixed, and state-of-the-art results can be obtained using a completely local, mention-ranking system.\nIn this work, we posit that global context is indeed necessary for further improvements in coreference resolution, but argue that informative cluster, rather than mention, level features are very difficult to devise, limiting their effectiveness. Accordingly, we instead propose to learn representations of mention clusters by embedding them sequentially using a recurrent neural network (shown in Section 4). Our model has no manually defined cluster features, but\ninstead learns a global representation from the individual mentions present in each cluster. We incorporate these representations into a mention-ranking style coreference system.\nThe entire model, including the recurrent neural network and the mention-ranking sub-system, is trained end-to-end on the coreference task. We train the model as a local classifier with fixed context (that is, as a history-based model). As such, unlike several recent approaches, which may require complicated inference during training, we are able to train our model in much the same way as a vanilla mentionranking model.\nExperiments compare the use of learned global features to several strong baseline systems for coreference resolution. We demonstrate that the learned global representations capture important underlying information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015). Our final system improves over 0.8 points in CoNLL score over the current state of the art, and the improvement is statistically significant on all three CoNLL metrics."}, {"heading": "2 Background and Notation", "text": "Coreference resolution is fundamentally a clustering task. Given a sequence (xn)Nn=1 of (intra-document) mentions \u2013 that is, syntactic units that can refer or be referred to \u2013 coreference resolution involves partitioning (xn) into a sequence of clusters (X(m))Mm=1 such that all the mentions in any particular cluster\nar X\niv :1\n60 4.\n03 03\n5v 1\n[ cs\n.C L\n] 1\n1 A\npr 2\nX(m) refer to the same underlying entity. Since the mentions within a particular cluster may be ordered linearly by their appearance in the document,1 we will use the notation X(m)j to refer to the j\u2019th mention in the m\u2019th cluster.\nA valid clustering places each mention in exactly one cluster, and so we may represent a clustering with a vector z \u2208{1, . . . ,M}N , where zn =m iff xn is a member of X(m). Coreference systems attempt to find the best clustering z\u2217 \u2208 Z under some scoring function, with Z the set of valid clusterings.\nOne strategy to avoid the computational intractability associated with predicting an entire clustering z is to instead predict a single antecedent for each mention xn; because xn may not be anaphoric (and therefore have no antecedents), a \u201cdummy\u201d antecedent may also be predicted. The aforementioned strategy is adopted by \u201cmention-ranking\u201d systems (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013), which, formally, predict an antecedent y\u0302 \u2208Y(xn) for each mention xn, where Y(xn) = {1, . . . , n\u22121, }. Through transitivity, these decisions induce a clustering over the document.\nMention-ranking systems make their antecedent predictions with a local scoring function f(xn, y) defined for any mention xn and any antecedent y \u2208Y(xn). While such a scoring function clearly ignores much structural information, the mentionranking approach has been attractive for at least two reasons. First, inference is relatively simple and efficient, requiring only a left-to-right pass through a document\u2019s mentions during which a mention\u2019s antecedents (as well as ) are scored and the highest scoring antecedent is predicted. Second, from a linguistic modeling perspective, mention-ranking models learn a scoring function that requires a mention xn to be compatible with only one of its coreferent antecedents. This contrasts with mention-pair models (e.g., Bengtson and Roth (2008)), which score all pairs of mentions in a cluster, as well as with certain cluster-based models (see discussion in Culotta et al. (2007)). Modeling each mention as having a single antecedent is particularly advantageous for pronominal mentions, which we might like to model\n1We assume nested mentions are ordered by their syntactic heads.\nas linking to a single nominal or proper antecedent, for example, but not necessarily to all other coreferent mentions.\nAccordingly, in this paper we attempt to maintain the inferential simplicity and modeling benefits of mention ranking, while allowing the model to utilize global, structural information relating to z in making its predictions. We therefore investigate objective functions of the form\narg max y1,...,yN N\u2211 n=1 f(xn, yn) + g(xn, yn, z1:n\u22121) ,\nwhere g is a global function that, in making predictions for xn, may examine (features of) the clustering z1:n\u22121 induced by the antecedent predictions made through yn\u22121."}, {"heading": "3 The Role of Global Features", "text": "Here we motivate the use of global features for coreference resolution by focusing on the issues that may arise when resolving pronominal mentions in a purely local way. See Clark and Manning (2015) and Stoyanov and Eisner (2012) for more general motivation for using global models."}, {"heading": "3.1 Pronoun Problems", "text": "Recent empirical work has shown that the resolution of pronominal mentions accounts for a substantial percentage of the total errors made by modern mention-ranking systems. Wiseman et al. (2015) show that on the CoNLL 2012 English development set, almost 59% of mention-ranking precision errors and almost 24% of recall errors involve pronominal mentions. Martschat and Strube (2015) found a similar pattern in their comparison of mention-ranking, mention-pair, and latent-tree models.\nTo see why pronouns can be so problematic, consider the following passage from the \u201cBroadcast Conversation\u201d portion of the CoNLL development set (bc/msnbc/0000/018); below, we enclose mentions in brackets and give the same subscript to coclustered mentions. (This example is also shown in Figure 2.)\nDA: um and [I]1 think that is what\u2019s - Go ahead [Linda]2. LW: Well and uh thanks goes to [you]1 and to [the media]3 to help [us]4...So [our]4 hat is off to all of [you]5 as well.\nThis example is typical of Broadcast Conversation, and it is difficult because local systems learn to myopically link pronouns such as [you]5 to other instances of the same pronoun that are close by, such as [you]1. While this is often a reasonable strategy, in this case predicting [you]1 to be an antecedent of [you]5 would result in the prediction of an incoherent cluster, since [you]1 is coreferent with the singular [I]1, and [you]5, as part of the phrase \u201call of you,\u201d is evidently plural. Thus, while there is enough information in the text to correctly predict [you]5, doing so crucially depends on having access to the history of predictions made so far, and it is precisely this access to history that local models lack.\nMore empirically, there are non-local statistical regularities involving pronouns we might hope models could exploit. For instance, in the CoNLL training data over 70% of pleonastic \u201cit\u201d instances and over 74% of pleonastic \u201cyou\u201d instances follow (respectively) previous pleonastic \u201cit\u201d and \u201cyou\u201d instances. Similarly, over 78% of referential \u201cI\u201d instances and over 68% of referential \u201che\u201d instances corefer with previous \u201cI\u201d and \u201che\u201d instances, respectively.\nAccordingly, we might expect non-local models with access to global features to perform significantly better. However, models incorporating nonlocal features have a rather mixed track record. For instance, Bjo\u0308rkelund and Kuhn (2014) found that cluster-level features improved their results, whereas Martschat and Strube (2015) found that they did not. Clark and Manning (2015) found that incorporating cluster-level features beyond those involving the precomputed mention-pair and mention-ranking probabilities that form the basis of their agglomerative clustering coreference system did not improve performance. Furthermore, among recent, state-of-theart systems, mention-ranking systems (which are completely local) perform at least as well as their more structured counterparts (Durrett and Klein, 2014; Clark and Manning, 2015; Wiseman et al., 2015; Peng et al., 2015)."}, {"heading": "3.2 Issues with Global Features", "text": "We believe a major reason for the relative ineffectiveness of global features in coreference problems is that, as noted by Clark and Manning (2015), cluster-level features can be hard to define. Specif-\nically, it is difficult to define discrete, fixed-length features on clusters, which can be of variable size (or shape). As a result, global coreference features tend to be either too coarse or too sparse. Thus, early attempts at defining cluster-level features simply applied the coarse quantifier predicates all, none, most to the mention-level features defined on the mentions (or pairs of mentions) in a cluster (Culotta et al., 2007; Rahman and Ng, 2011). For example, a cluster would have the feature \u2018most-female=true\u2019 if more than half the mentions (or pairs of mentions) in the cluster have a \u2018female=true\u2019 feature.\nOn the other extreme, Bjo\u0308rkelund and Kuhn (2014) define certain cluster-level features by concatenating the mention-level features of a cluster\u2019s constituent mentions in order of the mentions\u2019 appearance in the document. For example, if a cluster consists, in order, of the mentions (the president, he, he), they would define a cluster-level \u201ctype\u201d feature \u2018C-P-P=true\u2019, which indicates that the cluster is composed, in order, of a common noun, a pronoun, and a pronoun. While very expressive, these concatenated features are often quite sparse, since clusters encountered during training can be of any size."}, {"heading": "4 Learning Global Features", "text": "To circumvent the aforementioned issues with defining global features, we propose to learn cluster-level feature representations implicitly, by identifying the state of a (partial) cluster with the hidden state of an RNN that has consumed the sequence of mentions composing the (partial) cluster. Before providing technical details, we provide some preliminary evidence that such learned representations capture important contextual information by displaying in Figure 1 the learned final states of all clusters in the CoNLL development set, projected using T-SNE (van der Maaten and Hinton, 2012). Each point in the visualization represents the learned features for an entity cluster and the head words of mentions are shown for representative points. Note that the model learns to roughly separate clusters by simple distinctions such as predominant type (nominal, proper, pronominal) and number (it, they, etc), but also captures more subtle relationships such as grouping geographic terms and long strings of pronouns."}, {"heading": "4.1 Recurrent Neural Networks", "text": "A recurrent neural network is a parameterized nonlinear function RNN that recursively maps an input sequence of vectors to a sequence of hidden states. Let (mj)Jj=1 be a sequence of J input vectors mj \u2208RD, and let h0 =0. Applying an RNN to any such sequence yields\nhj \u2190 RNN(mj ,hj\u22121;\u03b8) ,\nwhere \u03b8 is the set of parameters for the model, which are shared over time.\nThere are several varieties of RNN, but by far the most commonly used in natural-language processing is the Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997), particularly for language modeling (e.g., Zaremba et al. (2014)) and machine translation (e.g., Sutskever et al. (2014)), and we use LSTMs in all experiments."}, {"heading": "4.2 RNNs for Cluster Features", "text": "Our main contribution will be to utilize RNNs to produce feature representations of entity clusters which will provide the basis of the global term g. Recall that we view a cluster X(m) as a sequence of mentions (X(m)j ) J j=1 (ordered in linear document or-\nder). We therefore propose to embed the state(s) of X(m) by running an RNN over the cluster in order.\nIn order to run an RNN over the mentions we need an embedding function hc to map a mention to a real vector. First, following Wiseman et al. (2015) define \u03c6a(xn) : X \u2192 {0, 1}F as a standard set of local indicator features on a mention, such as its head word, its gender, and so on. (We elaborate on features below.) We then use a non-linear feature embedding hc to map a mention xn to a vector-space representation. In particular, we define\nhc(xn) , tanh(W c \u03c6a(xn) + bc) ,\nwhereW c and bc are parameters of the embedding. We will refer to the j\u2019th hidden state of the RNN corresponding to X(m) as h(m)j , and we obtain it according to the following formula\nh (m) j \u2190 RNN(hc(X (m) j ),h (m) j\u22121;\u03b8) ,\nagain assuming that h(m)0 =0. Thus, we will effectively run an RNN over each (sequence of mentions corresponding to a) cluster X(m) in the document, and thereby generate a hidden state h(m)j corresponding to each step of each cluster in the document. Concretely, this can be implemented by maintaining M RNNs \u2013 one for each cluster \u2013 that all share the parameters \u03b8. The process is illustrated in the top portion of Figure 2."}, {"heading": "5 Coreference with Global Features", "text": "We now describe how the RNN defined above is used within an end-to-end coreference system."}, {"heading": "5.1 Full Model and Training", "text": "Recall that our inference objective is to maximize the score of both a local mention ranking term as well as a global term based on the current clusters:\narg max y1,...,yN N\u2211 n=1 f(xn, yn) + g(xn, yn, z1:n\u22121)\nWe begin by defining the local model f(xn, y) with the two layer neural network of Wiseman et al. (2015), which has a specialization for the nonanaphoric case, as follows:\nf(xn, y) ,\n{ uT [ ha(xn) hp(xn,y) ] + u0 if y 6=\nvTha(xn) + v0 if y = .\nDA: um and [I]1 think that is what\u2019s - Go ahead [Linda]2. LW: Well and thanks goes to [you]1 and to [the media]3 to help [us]4...So [our]4 hat is off to all of [you]5...\nAbove, u and v are the parameters of the model, and ha and hp are learned feature embeddings of the local mention context and the pairwise affinity between a mention and an antecedent, respectively. These feature embeddings are defined similarly to hc, as\nha(xn) , tanh(W a \u03c6a(xn) + ba)\nhp(xn, y) , tanh(W p \u03c6p(xn, y) + bp) ,\nwhere \u03c6a (mentioned above) and \u03c6p are \u201craw\u201d (that is, unconjoined) features on the context of xn and on the pairwise affinity between mentions xn and antecedent y, respectively (Wiseman et al., 2015). Note that ha and hc use the same raw features; only their weights differ.\nWe now specify our global scoring function g based on the history of previous decisions. Define h (m) <n as the hidden state of cluster m before a decision is made for xn \u2013 that is, h (m) <n is the state of cluster m\u2019s RNN after it has consumed all mentions in the cluster preceding xn. We define g as\ng(xn, y,z1:n\u22121) ,\n{ hc(xn) Th (zy) <n if y 6=\nNA(xn) if y = ,\nwhere NA gives a score for assigning based on a non-linear function of all of the current hidden states:\nNA(xn) = q T tanh ( W s [ \u03c6a(xn)\u2211M m=1 h (m) <n ] + bs ) .\nSee Figure 2 for a diagram. The intuition behind the first case in g is that in considering whether y is a good antecedent for xn, we add a term to the score that examines how well xn matches with the mentions already inX(zy); this matching score is expressed via a dot-product.2 In the second case, when predicting that xn is non-anaphoric, we add the NA term to the score, which examines the (sum of) the current states h(m)<n of all clusters. This information is useful both because it allows the non-anaphoric score to incorporate information about potential antecedents, and because the occurrence of certain singleton-clusters often predicts the occurrence of future singleton-clusters, as noted in Section 3.\nThe whole system is trained end-to-end on coreference using backpropagation. For a given training document, let z(o) be the oracle mapping from mention to cluster, which induces an oracle clustering. While at training time we do have oracle clusters, we do not have oracle antecedents (y)Nn=1, so following past work we treat the oracle antecedent as latent (Yu and Joachims, 2009; Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013). We train with the following slack-rescaled, margin objective:\n2We also experimented with other non-linear functions, but dot-products performed best.\nN\u2211 n=1 max y\u0302\u2208Y(xn) \u2206(xn, y\u0302)(1 + f(xn, y\u0302) + g(xn, y\u0302,z (o))\n\u2212 f(xn, y`n)\u2212 g(xn, y`n, z(o))),\nwhere the latent antecedent y`n is defined as\ny`n , arg max y\u2208Y(xn):z(o)y =z(o)n f(xn, y) + g(xn, y, z (o))\nif xn is anaphoric, and is otherwise. The term \u2206(xn, y\u0302) gives different weight to different error types. We use a \u2206 with 3 different weights (\u03b11, \u03b12, \u03b13) for \u201cfalse link\u201d (FL), \u201cfalse new\u201d (FN), and \u201cwrong link\u201d (WL) mistakes (Durrett and Klein, 2013), which correspond to predicting an antecedent when non-anaphoric, when anaphoric, and the wrong antecedent, respectively.\nNote that in training we use the oracle clusters z(o). Since these are known a priori, we can precompute all the hidden states h(m)j in a document, which makes training quite simple and efficient. This approach contrasts in particular with the work of Bjo\u0308rkelund and Kuhn (2014) \u2014 who also incorporate global information in mention-ranking \u2014 in that they train against latent trees, which are not annotated and must be searched for during training. On the other hand, training on oracle clusters leads to a mismatch between training and test, which can hurt performance."}, {"heading": "5.2 Search", "text": "When moving from a strictly local objective to one with global features, the test-time search problem becomes intractable. The local objective requires O(n2) time, whereas the full clustering problem is NP-Hard. Past work with global features has used integer linear programming solvers for exact search (Chang et al., 2013; Peng et al., 2015), or beam search with (delayed) early update training for an approximate solution (Bjo\u0308rkelund and Kuhn, 2014). In contrast, we simply use greedy search at test time, which also requiresO(n2) time.3 The full algorithm\n3While beam search is a natural way to decrease search error at test time, it may fail to help if training involves a local margin objective (as in our case), since scores need not be calibrated across local decisions. We accordingly attempted to train various locally normalized versions of our model, but found that\nAlgorithm 1 Greedy search with global RNNs 1: procedure GREEDYCLUSTER(x1, . . . , xN ) 2: Initialize clusters X(1) . . . as empty lists, hidden states\nh(0), . . . as 0 vectors in RD , z as map from mention to cluster, and cluster counter M \u2190 0\n3: for n = 2 . . . N do 4: y\u2217 \u2190 argmax\ny\u2208Y(xn) f(xn, y) + g(xn, y, z1:n\u22121)\n5: m\u2190 zy\u2217 6: if y\u2217 = then 7: M \u2190M + 1 8: m\u2190M 9: append xn to X(m)\n10: zn \u2190 m 11: h(m) \u2190 RNN(hc(xn),h(m)) 12: return X(1), . . . , X(M)\nis shown in Algorithm 1. The greedy search algorithm is identical to a simple mention-ranking system, with the exception of line 11, which updates the current RNN representation based on the previous decision that was made, and line 4, which then uses this cluster representation as part of scoring."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Methods", "text": "We run experiments on the CoNLL 2012 English shared task (Pradhan et al., 2012). The task uses the OntoNotes corpus (Hovy et al., 2006), consisting of 3,493 documents in various domains and formats. We use the experimental split provided in the shared task. For all experiments, we use the Berkeley Coreference System (Durrett and Klein, 2013) for mention extraction and to compute features \u03c6a and \u03c6p.\nFeatures We use the raw BASIC+ feature sets described by Wiseman et al. (2015), with the following modifications:\n\u2022 We remove all features from \u03c6p that concatenate a feature of the antecedent with a feature of the current mention, such as bi-head features.\n\u2022 We add true-cased head features, a current speaker indicator feature, and a 2-character\nthey underperformed. We also experimented with training approaches and model variants that expose the model to its own predictions (Daume\u0301 III et al., 2009; Ross et al., 2011; Bengio et al., 2015), but found that these yielded a negligible performance improvement.\ngenre (out of {bc,bn,mz,nw,pt,tc,wb}) indicator to \u03c6p and \u03c6a.\n\u2022 We add features indicating if a mention has a substring overlap with the current speaker (\u03c6p and \u03c6a), and if an antecedent has a substring overlap with a speaker distinct from the current mention\u2019s speaker (\u03c6p).\n\u2022 We add a single centered, rescaled document position feature to each mention when learning hc. We calculate a mention xn\u2019s rescaled document position as 2n\u2212N\u22121N\u22121 .\nThese modifications result in there being approximately 14K distinct features in \u03c6a and approximately 28K distinct features in \u03c6p, which is far fewer features than has been typical in past work.\nFor training, we use document-size minibatches, which allows for efficient pre-computation of RNN states, and we minimize the loss described in Section 5 with AdaGrad (Duchi et al., 2011) (after clipping LSTM gradients to lie (elementwise) in (\u221210, 10)). We find that the initial learning rate chosen for AdaGrad has a significant impact on results, and we choose learning rates for each layer out of {0.1, 0.02, 0.01, 0.002, 0.001}.\nIn experiments, we set ha(xn), hc(xn), and h(m) to be \u2208R200, and hp(xn, y)\u2208R700. We use a single-layer LSTM (without \u201cpeep-hole\u201d connections), as implemented in the element-rnn library (Le\u0301onard et al., 2015). For regularization, we apply Dropout (Srivastava et al., 2014) with a rate of 0.4 before applying the linear weights u, and we also apply Dropout with a rate of 0.3 to the LSTM states before forming the dot-product scores.\nFollowing Wiseman et al. (2015) we use the costweights \u03b1 = \u30080.5, 1.2, 1\u3009 in defining \u2206, and we use their pre-training scheme as well. For final results, we train on both training and development portions of the CoNLL data. Scoring uses the official CoNLL 2012 script (Pradhan et al., 2014; Luo et al., 2014). Code for our system is available at https: //github.com/swiseman/nn_coref. The system makes use of a GPU for training, and trains in about two hours."}, {"heading": "6.2 Results", "text": "In Table 1 we present our main results on the CoNLL English test set, and compare with other recent stateof-the-art systems. We see a statistically significant improvement of over 0.8 CoNLL points over the previous state of the art, and the highest F1 scores to date on all three CoNLL metrics.\nWe now consider in more detail the impact of global features and RNNs on performance. For these experiments, we report MUC, B3, and CEAFe F1scores in Table 2 as well as errors broken down by mention type and by whether the mention is anaphoric or not in Table 3. Table 3 further partitions errors into FL, FN, and WL categories, which\nare defined in Section 5.1. We typically think of FL and WL as representing precision errors, and FN as representing recall errors.\nOur experiments consider several different settings. First, we consider an oracle setting (\u201cRNN, OH\u201d in tables), in which the model receives z (o) 1:n\u22121, the oracle partial clustering of all mentions preceding xn in the document, and is therefore not forced to rely on its own past predictions when predicting xn. This provides us with an upper bound on the performance achievable with our model. Next, we consider the performance of the model under a greedy inference strategy (RNN, GH), as in Algorithm 1. Finally, for baselines we consider the mention-ranking system (MR) of Wiseman et al. (2015) using our updated feature-set, as well as a non-local baseline with oracle history (Avg, OH), which averages the representations hc(xj) for all xj \u2208X(m), rather than feed them through an RNN; errors are still backpropagated through the hc representations during learning.\nIn Table 3 we see that the RNN improves performance overall, with the most dramatic improve-\nments on non-anaphoric pronouns, though errors are also decreased significantly for non-anaphoric nominal and proper mentions that follow at least one mention with the same head. While WL errors also decrease for both these mention-categories under the RNN model, FN errors increase. Importantly, the RNN performance is significantly better than that of the Avg baseline, which barely improves over mention-ranking, even with oracle history. This suggests that modeling the sequence of mentions in a cluster is advantageous. We also note that while RNN performance degrades in both precision and recall when moving from the oracle history upperbound to a greedy setting, we are still able to recover a significant portion of the possible performance improvement."}, {"heading": "6.3 Qualitative Analysis", "text": "In this section we consider in detail the impact of the g term in the RNN scoring function on the two error categories that improve most under the RNN model (as shown in Table 3), namely, pronominal WL errors and pronominal FL errors. We consider an example from the CoNLL development set in each category on which the baseline MR model makes an error but the greedy RNN model does not.\nThe example in Figure 3 involves the resolution of the ambiguous pronoun \u201chis,\u201d which is bracketed and in bold in the figure. Whereas the baseline MR model incorrectly predicts \u201chis\u201d to corefer with the closest gender-consistent antecedent \u201cJustin\u201d \u2014 thus making a WL error \u2014 the greedy RNN model\ncorrectly predicts \u201chis\u201d to corefer with \u201cMr. Kaye\u201d in the previous sentence. (Note that \u201cthe official\u201d also refers to Mr. Kaye). To get a sense of the greedy RNN model\u2019s decision-making on this example, we color the mentions the greedy RNN model has predicted to corefer with \u201cMr. Kaye\u201d in green, and the mentions it has predicted to corefer with \u201cJustin\u201d in blue. (Note that the model incorrectly predicts the initial \u201cI\u201d mentions to corefer with \u201cJustin.\u201d) Letting X(1) refer to the blue cluster, X(2) refer to the green cluster, and xn refer to the ambiguous mention \u201chis,\u201d we further shade each mention xj in X(1) so that its intensity corresponds to hc(xn)Th (1) <k, where k= j+ 1; mentions in X(2) are shaded analogously. Thus, the shading shows how highly g scores the compatibility between \u201chis\u201d and a cluster X(i) as each of X(i)\u2019s mentions is added. We see that when the initial \u201cJustin\u201d mentions are added to X(1) the g-score is relatively high. However, after \u201cThe company\u201d is correctly predicted to corefer with \u201cJustin,\u201d the score of X(1) drops, since companies are generally not coreferent with pronouns like \u201chis.\u201d\nFigure 4 shows an example (consisting of a telephone conversation between \u201cA\u201d and \u201cB\u201d) in which the bracketed pronoun \u201cIt\u2019s\u201d is being used pleonastically. Whereas the baseline MR model predicts \u201cIt\u2019s\u201d to corefer with a previous \u201cit\u201d \u2014 thus making a FL error \u2014 the greedy RNN model does not. In Figure 4 the final mention in three preceding clusters is shaded so its intensity corresponds to the magnitude of the gradient of the NA term in g with respect to that mention. This visualization resembles the \u201csaliency\u201d technique of Li et al. (2016), and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score.\nWe see that the potential antecedent \u201cS-Bahn\u201d has a large gradient, but also that the initial, obviously pleonastic use of \u201cit\u2019s\u201d has a large gradient,\nwhich may suggest that earlier, easier predictions of pleonasm can inform subsequent predictions."}, {"heading": "7 Related Work", "text": "In addition to the related work noted throughout, we add supplementary references here. Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013; Wiseman et al., 2015). Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently, those that learn a latent tree of mentions (Fernandes et al., 2012; Bjo\u0308rkelund and Kuhn, 2014; Martschat and Strube, 2015).\nThere have also been structured approaches that merge the mention-ranking and mention-pair ideas in some way. For instance, Rahman and Ng (2011) rank clusters rather than mentions; Clark and Manning (2015) use the output of both mention-ranking and mention pair systems to learn a clustering.\nThe application of RNNs to modeling (the trajectory of) the state of a cluster is apparently novel, though it bears some similarity to the recent work of Dyer et al. (2015), who use LSTMs to embed the state of a transition based parser\u2019s stack."}, {"heading": "8 Conclusion", "text": "We have presented a simple, state of the art approach to incorporating global information in an end-to-end coreference system, which obviates the need to define global features, and moreover allows for simple (greedy) inference. Future work will examine improving recall, and more sophisticated approaches to global training."}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge the support of a Google Research Award."}], "references": [{"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio et al.2015] Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Understanding the Value of Features for Coreference Resolution", "author": ["Bengtson", "Roth2008] Eric Bengtson", "Dan Roth"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bengtson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bengtson et al\\.", "year": 2008}, {"title": "Learning structured perceptrons for coreference Resolution with Latent Antecedents and Non-local Features", "author": ["Bj\u00f6rkelund", "Kuhn2014] Anders Bj\u00f6rkelund", "Jonas Kuhn"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "End-to-end coreference resolution via hypergraph partitioning", "author": ["Cai", "Strube2010] Jie Cai", "Michael Strube"], "venue": "In 23rd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Cai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2010}, {"title": "A Constrained Latent Variable Model for Coreference Resolution", "author": ["Chang et al.2013] Kai-Wei Chang", "Rajhans Samdani", "Dan Roth"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Chang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2013}, {"title": "Entity-centric coreference resolution with model stacking", "author": ["Clark", "Manning2015] Kevin Clark", "Christopher D. Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Clark et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2015}, {"title": "First-order Probabilistic Models for Coreference Resolution", "author": ["Culotta et al.2007] Aron Culotta", "Michael Wick", "Robert Hall", "Andrew McCallum"], "venue": "In Human Language Technology Conference of the North American Chapter of the Association of Computa-", "citeRegEx": "Culotta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Culotta et al\\.", "year": 2007}, {"title": "Search-based structured prediction", "author": ["John Langford", "Daniel Marcu"], "venue": "Machine Learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Specialized Models and Ranking for Coreference Resolution", "author": ["Denis", "Baldridge2008] Pascal Denis", "Jason Baldridge"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Denis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Denis et al\\.", "year": 2008}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Easy Victories and Uphill Battles in Coreference Resolution", "author": ["Durrett", "Klein2013] Greg Durrett", "Dan Klein"], "venue": "In Proceedings of the 2013 Confer-", "citeRegEx": "Durrett et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2013}, {"title": "A Joint Model for Entity Analysis: Coreference, Typing, and Linking", "author": ["Durrett", "Klein2014] Greg Durrett", "Dan Klein"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Durrett et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2014}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Latent Structure Perceptron with Feature Induction for Unrestricted Coreference Resolution", "author": ["C\u0131\u0301cero Nogueira Dos Santos", "Ruy Luiz Milidi\u00fa"], "venue": "In Joint Conference on EMNLP and CoNLL-Shared", "citeRegEx": "Fernandes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fernandes et al\\.", "year": 2012}, {"title": "Coreference Resolution in a Modular", "author": ["Haghighi", "Klein2010] Aria Haghighi", "Dan Klein"], "venue": "Entitycentered Model. In The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Haghighi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2010}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Ontonotes: the 90% Solution", "author": ["Hovy et al.2006] Eduard Hovy", "Mitchell Marcus", "Martha Palmer", "Lance Ramshaw", "Ralph Weischedel"], "venue": "In Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers,", "citeRegEx": "Hovy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hovy et al\\.", "year": 2006}, {"title": "Statistical Significance Tests for Machine Translation Evaluation", "author": ["Philipp Koehn"], "venue": "In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Koehn.,? \\Q2004\\E", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Error-driven Analysis of Challenges in Coreference Resolution", "author": ["Kummerfeld", "Dan Klein"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kummerfeld et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2013}, {"title": "rnn: Recurrent Library for Torch", "author": ["Yand Waghmare", "Sagar ad Wang", "Jin-Hwa Kim"], "venue": "arXiv preprint arXiv:1511.07889", "citeRegEx": "L\u00e9onard et al\\.,? \\Q2015\\E", "shortCiteRegEx": "L\u00e9onard et al\\.", "year": 2015}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Li et al.2016] Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": "In NAACL HLT", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Latent structures for coreference resolution", "author": ["Martschat", "Strube2015] Sebastian Martschat", "Michael Strube"], "venue": null, "citeRegEx": "Martschat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martschat et al\\.", "year": 2015}, {"title": "Analyzing and visualizing coreference resolution errors", "author": ["Thierry G\u00f6ckel", "Michael Strube"], "venue": "In NAACL HLT,", "citeRegEx": "Martschat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martschat et al\\.", "year": 2015}, {"title": "Toward Conditional Models of Identity Uncertainty with Application to Proper Noun Coreference", "author": ["McCallum", "Wellner2003] Andrew McCallum", "Ben Wellner"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "McCallum et al\\.,? \\Q2003\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2003}, {"title": "Identifying Anaphoric and Non-anaphoric Noun Phrases to Improve Coreference Resolution", "author": ["Ng", "Cardie2002] Vincent Ng", "Claire Cardie"], "venue": "In Proceedings of the 19th international conference on Computational linguistics-Volume", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "A joint framework for coreference resolution and mention head detection", "author": ["Peng et al.2015] Haoruo Peng", "Kai-Wei Chang", "Dan Roth"], "venue": "In Proceedings of the 19th Conference on Computational Natural Language Learning (CoNLL),", "citeRegEx": "Peng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Peng et al\\.", "year": 2015}, {"title": "Joint unsupervised coreference resolution with markov logic", "author": ["Poon", "Domingos2008] Hoifung Poon", "Pedro M. Domingos"], "venue": "In 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Poon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Poon et al\\.", "year": 2008}, {"title": "Conll-2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes", "author": ["Alessandro Moschitti", "Nianwen Xue", "Olga Uryupina", "Yuchen Zhang"], "venue": "In Joint Conference on EMNLP and CoNLL-Shared", "citeRegEx": "Pradhan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2012}, {"title": "Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation", "author": ["Xiaoqiang Luo", "Marta Recasens", "Eduard Hovy", "Vincent Ng", "Michael Strube"], "venue": "In Proceedings of the Association", "citeRegEx": "Pradhan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pradhan et al\\.", "year": 2014}, {"title": "Supervised Models for Coreference Resolution", "author": ["Rahman", "Ng2009] Altaf Rahman", "Vincent Ng"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2Volume", "citeRegEx": "Rahman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rahman et al\\.", "year": 2009}, {"title": "Narrowing the modeling gap: A cluster-ranking approach to coreference resolution", "author": ["Rahman", "Ng2011] Altaf Rahman", "Vincent Ng"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "Rahman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rahman et al\\.", "year": 2011}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross et al.2011] St\u00e9phane Ross", "Geoffrey J. Gordon", "Drew Bagnell"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "A Machine Learning Approach to Coreference Resolution of Noun Phrases", "author": ["Soon et al.2001] Wee Meng Soon", "Hwee Tou Ng", "Daniel Chung Yong Lim"], "venue": null, "citeRegEx": "Soon et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Soon et al\\.", "year": 2001}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Easy-first Coreference Resolution", "author": ["Stoyanov", "Eisner2012] Veselin Stoyanov", "Jason Eisner"], "venue": "In COLING,", "citeRegEx": "Stoyanov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Stoyanov et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Visualizing non-metric similarities in multiple maps", "author": ["van der Maaten", "Geoffrey E. Hinton"], "venue": "Machine Learning,", "citeRegEx": "Maaten et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2012}, {"title": "Learning anaphoricity and antecedent ranking features for coreference resolution", "author": ["Wiseman et al.2015] Sam Wiseman", "Alexander M. Rush", "Stuart M. Shieber", "Jason Weston"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Wiseman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2015}, {"title": "Learning Structural SVMs with Latent Variables", "author": ["Yu", "Joachims2009] Chun-Nam John Yu", "Thorsten Joachims"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Yu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2009}, {"title": "Recurrent neural network regularization. CoRR, abs/1409.2329", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 37, "context": "information that can help resolve difficult pronominal mentions, which remain a persistent source of errors for modern coreference systems (Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Wiseman et al., 2015; Martschat and Strube, 2015).", "startOffset": 139, "endOffset": 242}, {"referenceID": 6, "context": ", Bengtson and Roth (2008)), which score all pairs of mentions in a cluster, as well as with certain cluster-based models (see discussion in Culotta et al. (2007)).", "startOffset": 141, "endOffset": 163}, {"referenceID": 37, "context": "Wiseman et al. (2015) show that on the CoNLL 2012 English development set, almost 59% of mention-ranking precision errors and almost 24% of recall errors involve pronominal mentions.", "startOffset": 0, "endOffset": 22}, {"referenceID": 37, "context": "Wiseman et al. (2015) show that on the CoNLL 2012 English development set, almost 59% of mention-ranking precision errors and almost 24% of recall errors involve pronominal mentions. Martschat and Strube (2015) found a similar pattern in their comparison of mention-ranking, mention-pair, and latent-tree models.", "startOffset": 0, "endOffset": 211}, {"referenceID": 37, "context": "Furthermore, among recent, state-of-theart systems, mention-ranking systems (which are completely local) perform at least as well as their more structured counterparts (Durrett and Klein, 2014; Clark and Manning, 2015; Wiseman et al., 2015; Peng et al., 2015).", "startOffset": 168, "endOffset": 259}, {"referenceID": 25, "context": "Furthermore, among recent, state-of-theart systems, mention-ranking systems (which are completely local) perform at least as well as their more structured counterparts (Durrett and Klein, 2014; Clark and Manning, 2015; Wiseman et al., 2015; Peng et al., 2015).", "startOffset": 168, "endOffset": 259}, {"referenceID": 6, "context": "Thus, early attempts at defining cluster-level features simply applied the coarse quantifier predicates all, none, most to the mention-level features defined on the mentions (or pairs of mentions) in a cluster (Culotta et al., 2007; Rahman and Ng, 2011).", "startOffset": 210, "endOffset": 253}, {"referenceID": 38, "context": ", Zaremba et al. (2014)) and machine translation (e.", "startOffset": 2, "endOffset": 24}, {"referenceID": 35, "context": ", Sutskever et al. (2014)), and we use LSTMs in all experiments.", "startOffset": 2, "endOffset": 26}, {"referenceID": 37, "context": "First, following Wiseman et al. (2015) define \u03c6a(xn) : X \u2192 {0, 1}F as a standard set of local indicator features on a mention, such as its head word, its gender, and so on.", "startOffset": 17, "endOffset": 39}, {"referenceID": 37, "context": "We begin by defining the local model f(xn, y) with the two layer neural network of Wiseman et al. (2015), which has a specialization for the nonanaphoric case, as follows:", "startOffset": 83, "endOffset": 105}, {"referenceID": 37, "context": "where \u03c6a (mentioned above) and \u03c6p are \u201craw\u201d (that is, unconjoined) features on the context of xn and on the pairwise affinity between mentions xn and antecedent y, respectively (Wiseman et al., 2015).", "startOffset": 177, "endOffset": 199}, {"referenceID": 13, "context": "While at training time we do have oracle clusters, we do not have oracle antecedents (y)n=1, so following past work we treat the oracle antecedent as latent (Yu and Joachims, 2009; Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013).", "startOffset": 157, "endOffset": 249}, {"referenceID": 4, "context": "While at training time we do have oracle clusters, we do not have oracle antecedents (y)n=1, so following past work we treat the oracle antecedent as latent (Yu and Joachims, 2009; Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013).", "startOffset": 157, "endOffset": 249}, {"referenceID": 4, "context": "Past work with global features has used integer linear programming solvers for exact search (Chang et al., 2013; Peng et al., 2015), or beam search with (delayed) early update training for an approximate solution (Bj\u00f6rkelund and Kuhn, 2014).", "startOffset": 92, "endOffset": 131}, {"referenceID": 25, "context": "Past work with global features has used integer linear programming solvers for exact search (Chang et al., 2013; Peng et al., 2015), or beam search with (delayed) early update training for an approximate solution (Bj\u00f6rkelund and Kuhn, 2014).", "startOffset": 92, "endOffset": 131}, {"referenceID": 27, "context": "We run experiments on the CoNLL 2012 English shared task (Pradhan et al., 2012).", "startOffset": 57, "endOffset": 79}, {"referenceID": 16, "context": "The task uses the OntoNotes corpus (Hovy et al., 2006), consist-", "startOffset": 35, "endOffset": 54}, {"referenceID": 37, "context": "Features We use the raw BASIC+ feature sets described by Wiseman et al. (2015), with the following modifications:", "startOffset": 57, "endOffset": 79}, {"referenceID": 31, "context": "We also experimented with training approaches and model variants that expose the model to its own predictions (Daum\u00e9 III et al., 2009; Ross et al., 2011; Bengio et al., 2015), but found that these yielded a negligible performance improvement.", "startOffset": 110, "endOffset": 174}, {"referenceID": 0, "context": "We also experimented with training approaches and model variants that expose the model to its own predictions (Daum\u00e9 III et al., 2009; Ross et al., 2011; Bengio et al., 2015), but found that these yielded a negligible performance improvement.", "startOffset": 110, "endOffset": 174}, {"referenceID": 25, "context": "02 Peng et al. (2015) - - 72.", "startOffset": 3, "endOffset": 22}, {"referenceID": 25, "context": "02 Peng et al. (2015) - - 72.22 - - 60.50 - - 56.37 63.03 Wiseman et al. (2015) 76.", "startOffset": 3, "endOffset": 80}, {"referenceID": 17, "context": "05 under the bootstrap resample test (Koehn, 2004)) compared with Wiseman et al.", "startOffset": 37, "endOffset": 50}, {"referenceID": 24, "context": "We compare against recent state of the art systems, including (in order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and Wiseman et al.", "startOffset": 156, "endOffset": 175}, {"referenceID": 24, "context": "We compare against recent state of the art systems, including (in order) Bjorkelund and Kuhn (2014), Martschat and Strube (2015), Clark and Manning (2015), Peng et al. (2015), and Wiseman et al. (2015). F1 gains are significant (p < 0.", "startOffset": 156, "endOffset": 202}, {"referenceID": 17, "context": "05 under the bootstrap resample test (Koehn, 2004)) compared with Wiseman et al. (2015) for all metrics.", "startOffset": 38, "endOffset": 88}, {"referenceID": 9, "context": "For training, we use document-size minibatches, which allows for efficient pre-computation of RNN states, and we minimize the loss described in Section 5 with AdaGrad (Duchi et al., 2011) (after clipping LSTM gradients to lie (elementwise) in (\u221210, 10)).", "startOffset": 167, "endOffset": 187}, {"referenceID": 19, "context": "We use a single-layer LSTM (without \u201cpeep-hole\u201d connections), as implemented in the element-rnn library (L\u00e9onard et al., 2015).", "startOffset": 104, "endOffset": 126}, {"referenceID": 33, "context": "For regularization, we apply Dropout (Srivastava et al., 2014) with a rate of 0.", "startOffset": 37, "endOffset": 62}, {"referenceID": 35, "context": "Following Wiseman et al. (2015) we use the costweights \u03b1 = \u30080.", "startOffset": 10, "endOffset": 32}, {"referenceID": 37, "context": "Finally, for baselines we consider the mention-ranking system (MR) of Wiseman et al. (2015) using our updated feature-set, as well as a non-local baseline with oracle history (Avg, OH), which averages the representations hc(xj) for all xj \u2208X(m), rather than feed them through an RNN; errors are still backpropagated through the hc representations during learning.", "startOffset": 70, "endOffset": 92}, {"referenceID": 20, "context": "This visualization resembles the \u201csaliency\u201d technique of Li et al. (2016), and it attempts to gives a sense of the contribution of a (preceding) cluster in the calculation of the NA score.", "startOffset": 57, "endOffset": 74}, {"referenceID": 32, "context": "Unstructured approaches to coreference typically divide into mention-pair models, which classify (nearly) every pair of mentions in a document as coreferent or not (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008), and mention-ranking models, which select a single antecedent for each anaphoric mention (Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al.", "startOffset": 164, "endOffset": 229}, {"referenceID": 6, "context": "Structured approaches typically divide between those that induce a clustering of mentions (McCallum and Wellner, 2003; Culotta et al., 2007; Poon and Domingos, 2008; Haghighi and Klein, 2010; Stoyanov and Eisner, 2012; Cai and Strube, 2010), and, more recently,", "startOffset": 90, "endOffset": 240}, {"referenceID": 13, "context": "those that learn a latent tree of mentions (Fernandes et al., 2012; Bj\u00f6rkelund and Kuhn, 2014; Martschat and Strube, 2015).", "startOffset": 43, "endOffset": 122}, {"referenceID": 12, "context": "tory of) the state of a cluster is apparently novel, though it bears some similarity to the recent work of Dyer et al. (2015), who use LSTMs to embed the state of a transition based parser\u2019s stack.", "startOffset": 107, "endOffset": 126}], "year": 2016, "abstractText": "There is compelling evidence that coreference prediction would benefit from modeling global information about entity-clusters. Yet, state-of-the-art performance can be achieved with systems treating each mention prediction independently, which we attribute to the inherent difficulty of crafting informative clusterlevel features. We instead propose to use recurrent neural networks (RNNs) to learn latent, global representations of entity clusters directly from their mentions. We show that such representations are especially useful for the prediction of pronominal mentions, and can be incorporated into an end-to-end coreference system that outperforms the state of the art without requiring any additional search.", "creator": "LaTeX with hyperref package"}}}