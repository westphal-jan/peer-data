{"id": "1604.03901", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2016", "title": "Single-Image Depth Perception in the Wild", "abstract": "this paper directly studies single - cycle image depth gradient perception algorithms in the wild, subjective i. e., so recovering depth from a flawed single image is taken together in unconstrained settings. additionally we introduce a massive new dataset \" high depth allocation in the wild \" consisting of statistical images configured in that the wild annotated inputs with relative depth between distinct pairs of reasonably random cue points. we all also propose a fast new sampling algorithm that only learns easy to easily estimate metric depth using weighted annotations of relative depth. compared to the state of the box art, our recent algorithm coverage is inherently simpler and performs is better. improved experiments show that our newer algorithm, actually combined jointly with other existing rgb - d data matrices and our new relative depth annotations, significantly improves single - process image depth perception in the wild.", "histories": [["v1", "Wed, 13 Apr 2016 18:19:35 GMT  (4819kb,D)", "http://arxiv.org/abs/1604.03901v1", null], ["v2", "Fri, 6 Jan 2017 16:05:35 GMT  (9305kb,D)", "http://arxiv.org/abs/1604.03901v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["weifeng chen", "zhao fu", "dawei yang", "jia deng"], "accepted": true, "id": "1604.03901"}, "pdf": {"name": "1604.03901.pdf", "metadata": {"source": "CRF", "title": "Single-Image Depth Perception in the Wild", "authors": ["Weifeng Chen", "Zhao Fu", "Dawei Yang", "Jia Deng"], "emails": ["wfchen@umich.edu", "zhaofu@umich.edu", "ydawei@umich.edu", "jiadeng@umich.edu"], "sections": [{"heading": null, "text": "Deep Network with Pixel-wise Prediction Metric Depth\nRGB-D Data Relative Depth Annotations\ntrain\nInput Image\nFigure 1: We crowdsource annotations of relative depth and train a deep network to recover depth from a single image taken in unconstrained settings (\u201cin the wild\u201d)."}, {"heading": "1 Introduction", "text": "Depth from a single RGB image is a fundamental problem in vision. Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10]. But such advances have yet to broadly impact higher-level tasks. One reason is that many higher-level tasks must operate on images \u201cin the wild\u201d\u2014images taken with no constraints on cameras, locations, scenes, and objects\u2014but the RGB-D datasets used to train and evaluate image-to-depth systems are constrained in one way or another.\nCurrent RGB-D datasets were collected by depth sensors [4, 5], which are limited in range and resolution, and often fail on specular or transparent objects [11]. In addition, because there is no Flickr for RGB-D images, researchers have to manually capture the images. As a result, current RGB-D datasets are limited in the diversity of scenes. For example, NYU depth [4] consists entirely of indoor scenes with no human presence; KITTI [5] consists entirely of road scenes captured from a car; Make3D [3, 12] consists entirely of outdoor scenes of the Stanford campus (Figure. 4). While\nar X\niv :1\n60 4.\n03 90\n1v 1\n[ cs\n.C V\n] 1\n3 A\npr 2\n01 6\nthese datasets are pivotal in driving research, it is unclear whether systems trained on them can generalize to images in the wild.\nIs it possible to collect ground-truth depth for images in the wild? Using depth sensors in unconstrained settings is not yet feasible. Crowdsourcing seems viable, but humans are not good at estimating metric depth, or 3D metric structure in general [13]. In fact, metric depth from a single image is fundamentally ambiguous: a tree behind a house can be slightly bigger but further away, or slightly smaller but closer\u2014the absolute depth difference between the house and the tree cannot be uniquely determined. Furthermore, even in cases where humans can estimate metric depth, it is unclear how to elicit the values from them.\nBut humans are better at judging relative depth [13]: \u201cIs point A closer than point B?\u201d is often a much easier question for humans. Recent work by Zoran et al. [14] shows that it is possible to learn to estimate metric depth using only annotations of relative depth. Although such metric depth estimates are only accurate up to monotonic transformations, they may well be sufficiently useful for high-level tasks, especially for occlusion reasoning. The seminal results by Zoran et al. point to two fronts for further progress: (1) collecting a large amount of relative depth annotations for images in the wild and (2) improving the algorithms that learn from annotations of relative depth.\nIn this paper, we make contributions on both fronts. Our first contribution is a new dataset called \u201cDepth in the Wild\u201d (DIW). It consists of 495K diverse images, each annotated with randomly sampled points and their relative depth. We sample one pair of points per image to minimize the redundancy of annotation. To the best of our knowledge this is the first large-scale dataset consisting of images in the wild with relative depth annotations. We demonstrate that this dataset can be used as an evaluation benchmark as well as a training resource 1.\nOur second contribution is a new algorithm for learning to estimate metric depth using only annotations of relative depth. Our algorithm not only significantly outperforms that of Zoran et al. [14], but is also simpler. The algorithm of Zoran et al. [14] first learns a classifier to predict the ordinal relation between two points in an image. Given a new image, this classifier is repeatedly applied to predict the ordinal relations between a sparse set of point pairs (mostly between the centers of neighboring superpixels). The algorithm then reconstructs depth from the predicted ordinal relations by solving a constrained quadratic optimization that enforces additional smoothness constraints and reconciles potentially inconsistent ordinal relations. Finally, the algorithm estimates depth for all pixels assuming a constant depth within each superpixel.\nIn contrast, our algorithm consists of a single deep network that directly predicts pixel-wise depth (Fig. 1). The network takes an entire image as input and consists of off-the-shelf components (convolution, max pooling, downsampling, upsampling) and can be trained only with annotations of relative depth. The novelty of our approach lies in the combination of two ingredients: (1) a multiscale deep network that produces pixel-wise prediction of metric depth and (2) a loss function using relative depth. Experiments show that our method produces pixel-wise depth that is more accurately ordered, outperforming not only the method by Zoran et al. [14] but also the state-of-the-art image-todepth system by Eigen et al. [8] trained with ground-truth metric depth. Furthermore, combing our new algorithm, our new dataset, and existing RGB-D data significantly improves single-image depth estimation in the wild."}, {"heading": "2 Related Work", "text": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5]. Existing Kinect-based datasets are limited to indoor scenes; existing LIDARbased datasets are biased towards scenes of man-made structures [3, 5]. In contrast, our dataset covers a much wider variety of scenes; it can be easily expanded with large-scale crowdsourcing and the virually umlimited Internet images.\nIntrinsic Images in the Wild: Our work draws inspiration from Intrinsic Images in the Wild [18], a seminal work that crowdsources annotations of relative reflectance on unconstrained images. Our work differs in goals as well as in several design decisions. First, we sample random points instead of centers of superpixels, because unlike reflectance, it is unreasonable to assume a constant depth within\n1Dataset website: http://www-personal.umich.edu/~wfchen/depth-in-the-wild.\na superpixel. Second, we sample only one pair of points per image instead of many to maximize the value of human annotations.\nDepth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24]. The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26]. But the networks in these previous works, with the exception of [14], were trained exclusively using ground-truth metric depth, whereas our approach uses relative depth.\nOur work is inspired by that of Zoran et al. [14], which proposes to use a deep network to repeatedly classify pairs of points sampled based on superpixel segmentation, and to reconstruct per-pixel metric depth by solving an additional optimization problem. Our approach is different: it consists of a single deep network trained end-to-end that directly predicts per-pixel metric depth; there is no intermediate classification of ordinal relations and as a result no optimization needed to resolve inconsistencies.\nLearning with Ordinal Relations: Several recent works [27, 28] have used the ordinal relations from the Intrinsic Images in the Wild dataset [18] to estimate surface refletance. Similar to Zoran et al. [14], Zhou et al. [27] first learn a deep network to classify the ordinal relations between pair of points and then make them globally consistent through energy minimization.\nNarihira et al. [28] learn a \u201clightness potential\u201d network that takes an image patch and predicts the metric reflectance of the center pixel. But this network is applied to only a sparse set of pixels. Although in principle this lightness potential network can be applied to every pixel to produce pixel-wise reflectance, doing so would be quite expensive. Making it fully convolutional (as the authors mentioned in [28]) only solves it partially: as long as the lightness potential network has downsampling layers, which is the case in [28], the final output will be downsampled accordingly. Additional resolution augmentation (such as the \u201cshift and stitch\u201d approach [29]) is thus needed. In contrast, our approach completely avoids such issues and directly outputs pixel-wise estimates.\nBeyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34]."}, {"heading": "3 Dataset Construction", "text": "We gather images from Flickr. We use random query keywords sampled from an English dictionary and exclude artificial images such as drawings and clip arts. To collect annotations of relative depth, we present a crowd worker an image and two highlighted points (Fig. 2), and ask \u201cwhich point is closer, point 1, point 2, or hard to tell?\u201d The worker presses a key to respond.\nHow Many Pairs? How many pairs of points should we query per image? We sample just one per image because this maximizes the amount of information from human annotators. Consider the other\nextreme\u2014querying all possible pairs of points in the same image. This is wasteful because pairs of points in close proximity are likely to have the same relative depth. In other words, querying one more pair from the same image may add less information than querying one more pair from a new image. Thus querying only one pair per image is more cost-effective.\nWhich Pairs? Which two points should we query given an image? The simplest way would be to sample two random points from the 2D plane. But this results in a severe bias that can be easily exploited: if an algorithm simply classifies the lower point in the image to be closer in depth, it will agree with humans 85.8% of the time (Fig. 3). Although this bias is natural, it makes the dataset less useful as a benchmark.\nAn alternative is to sample two points uniformly from a random horizontal line, which makes it impossible to use the y image coordinate as a cue. But we find yet another bias: if an algorithm simply classifies the point closer to the center of the image to be closer in depth, it will agree with humans 71.4% of the time. This leads to a third approach: uniformly sample two symmetric points with respect to the center from a random horizontal line (the middle column of Fig. 5). With the symmetry enforced, we are not able to find a simple yet effective rule based purely on image coordinates: the left point is almost equally likely (50.03%) to be closer than the right one.\nOur final dataset consists of a roughly 50-50 combination of unconstrained pairs and symmetric pairs, which strikes a balance between the need for representing natural scene statistics and the need for performance differentiation.\nProtocol and Results: We crowdsource the annotations using Amazon Mechanical Turk (AMT). To remove spammers, we insert into all tasks gold-standard images verified by ourselves, and reject workers whose accumulative accuracy on the gold-standard images is below 85%. We assign each query (an image and a point pair) to two workers, and add the query to our dataset if both workers can tell the relative depth and agree with each other; otherwise the query is discarded. Under this protocol, the chance of adding a wrong answer to our dataset is less than 1% as measured on the gold-standard images.\nWe processed 1.24M images on AMT and obtained 0.5M valid answers (both workers can tell the relative depth and agree with each other). Among the valid answers, 261K are for unconstrained pairs and 240K are for symmetric pairs. For unconstrained pairs, It takes a median of 3.4 seconds for a worker to decide, and two workers agree on the relative depth 52% of the time; for symmetric pairs, the numbers are 3.8s and 32%. These numbers suggest that the symmetric pairs are indeed harder. Fig. 5 presents examples of different kinds of queries."}, {"heading": "4 Learning with Relative Depth", "text": "How do we learn to predict metric depth given only annotations of relative depth? Zoran et al. [14] first learn a classifier to predict ordinal relations between centers of superpixels, and then reconcile\nunconstrained pairs\nthe relations to recover depth using energy minimization, and then interpolate within each superpixel to produce per-pixel depth.\nWe take a simpler approach. The idea is that any image-to-depth algorithm would have to compute a function that maps an image to pixel-wise depth. Why not represent this function as a neural network and learn it from end to end? We just need two ingredients: (1) a network design that outputs the same resolution as the input, and (2) a way to train the network with annotations of relative depth.\nNetwork Design: Networks that output the same resolution as the input are aplenty, including the recent designs for depth estimation [8, 35] and those for semantic segmentation [36] and edge detection [37]. A common element of these designs is processing and passing information across multiple scales.\nIn this work, we use a variant of the recently introduced \u201chourglass\u201d network, which has been used to achieve state-of-the-art results on human pose estimation [38]. Fig. 6 illustrates the design, which consists of a series of convolutions (using a variant of the inception [39] module) and downsampling, followed by a series of convolutions and upsampling, interleaved with skip connections that add back features from high resolutions. The symmetric shape of the network resembles a \u201chourglass\u201d, hence the name. We refer the reader to [38] for comparing the design to related work. For our purpose, this particular choice is not essential, as the various designs mainly differ in how information from different scales is dispersed and aggregated, and it is possible that all of them can work equally well for our task.\nLoss Function: How do we train the network using only ordinal annotations? All we need is a loss function that encourages the predicted depth map to agree with the ground-truth ordinal relations. Specifically, consider a training image I and its K queries R = {(ik, jk, rk)}, k = 1, . . . ,K, where ik is the location of the first point in the k-th query, jk is the location of the second point in the k-th query, and rk \u2208 {+1,\u22121, 0} is the ground-truth depth relation between ik and jk: closer (+1), further (\u22121), and equal (0). Let z be the predicted depth map and zik , zjk be the depths at point ik and jk. We define a loss function\nL(I,R, z) = K\u2211 k=1 \u03c8k(I, ik, jk, r, z), (1)\nwhere \u03c8k(I, ik, jk, z) is the loss for the k-th query\n\u03c8k(I, ik, jk, z) =  log (1 + exp(\u2212zik + zjk)) , rk = +1log (1 + exp(zik \u2212 zjk)) , rk = \u22121(zik \u2212 zjk)2, rk = 0. (2) This is essentially a ranking loss: it encourages a small difference between depths if the ground-truth relation is equality; otherwise it encourages a large difference.\nNovelty of Our Approach: Our novelty lies in the combination of a deep network that does pixelwise prediction and a ranking loss placed on the pixel-wise prediction. A deep network that does pixel-wise prediction is not new, nor is a ranking loss. But to the best of our knowledge, such a combination has not been proposed before, and in particular not for estimating depth."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 NYU Depth", "text": "We evaluate our method using the NYU Depth dataset [4], which consists of indoor scenes with ground-truth Kinect depth. We use the same setup as that of Zoran et al. [14]: point pairs are sampled from the training images (the subset of NYU Depth consisting of 795 images with semantic labels) using superpixel segmentation and their ground-truth ordinal relations are generated by comparing the ground-truth Kinect depth; the same procedure is applied to the test set to generate the point pairs for evaluation (around 3K pairs per image). We use the same training data and test data as Zoran et al. [14].\nAs the system by Zoran et al. [14], our network predicts one of the three ordinal relations on the test pairs: equal (=), closer (<), or farther (>). We report WKDR, the weighted disagreement rate\nbetween the predicted ordinal relations and ground-truth ordinal relations 2. We also report WKDR= (disagreement rate on pairs whose ground-truth relations are =) and WKDR 6= (disagreement rate on pairs whose ground-truth relations are < or >).\nSince two ground-truth depths are almost never exactly the same, there needs to be a relaxed definition of equality. Zoran et al. [14] define two points to have equal depths if the ratio between their groundtruth depths is within a pre-determined range. Our network predicts an equality relation if the depth difference is smaller than a threshold \u03c4 . The choice of this threshold will result in different values for the error metrics (WKDR, WKDR=, WKDR 6=): if \u03c4 is too small, most pairs will be predicted to be unequal and the error metric on equality relations (WKDR=) will be large; if \u03c4 is too big, most pairs will be predicted to be equal and the error metric on inequality relations (WKDR 6=) will be large. We choose the threshold \u03c4 that minimizes the maximum of the three error metrics on a validation set held out from the training set.\nTab. 2 compares our network (ours) versus that of Zoran et al. [14]. Our network is trained with the same data 3 but outperforms [14] on all three metrics.\nFollowing [14], we also compare with the state-of-art image-to-depth system by Eigen et al. [8], which is trained on pixel-wise ground-truth metric depth from the full NYU Depth training set (220K images). To compare fairly, we give our network access to the full NYU Depth training set. In addition, we remove the limit of 800 point pairs per training image placed by Zoran et al and use all available pairs. The results in Tab. 2 show that our network (ours_full) achieves superior performance in estimating depth ordering. Granted, this comparison is not entirely fair because [8] is not optimized for predicting ordinal relations. But this comparison is still significant in that it shows that we can train on only relative depth and rival the state-of-the-art system in estimating depth up to monotonic transformations.\nIn Figure. 8 we show qualitative results on the same example images used by Zoran et al. [14]. We see that although imperfect, the recovered metric depth by our method is overall reasonable and qualitatively similar to that by the state-of-art system [8] trained on ground-truth metric depth.\nMetric Error Measures Our network is trained with relative depth, so it is unsurprising that it does well in estimating depth up to ordering. But how good is the estimated depth in terms of metric error? We thus evaluate conventional error measures such as RMSE (the root mean squared error), which compares the absolute depth values to the ground truths. Because our network is trained only on relative depth and does not know the range of the ground-truth depth values, to make these error measures meaningful we normalize the depth predicted by our network such that the mean and standard deviation are the same as those of the mean depth map of the training set. Tab. 2 reports the results. We see that under these metric error measures our network still outperforms the method of Zoran et al. [14]. In addition, while our metric error is worse than the current state-of-the-art, it is comparable to some of the earlier methods (e.g. [1]) that have access to ground-truth metric depth.\n2WKDR stands for \u201cWeighted Kinect Disagreement Rate\u201d; the weight is set to 1 as in [14] 3The code released by Zoran et al. [14] indicates that they train with a random subset of 800 pairs per image\ninstead of the all pairs. We follow the same procedure and only use a random subset of 800 pairs per image.\nSuperpixel Sampling versus Random Sampling. To compare with the method by Zoran et al. [14], we train our network using the same point pairs, which are pairs of centers of superpixels (Fig. 9). But is superpixel segmentation necessary? In other words, can we simply train with randomly sampled points?\nTo answer this question, we train our network with randomly sampled points. We constrain the distance between the two points to be between 13 and 19 pixels (out of a 320\u00d7240 image) such that the distance is similar to that between the centers of neighboring superpixels. The results are included in Tab. 2. We see that using 3.3k pairs per image (rand_3K) already achieves comparable performance to the method by Zoran et al. [14]. Using twice or four times as many pairs (rand_6K, rand_12K) can further improve performance and significantly outperform the method by Zoran et al. [14].\nIt is worth noting that in all these experiments the test pairs are still from superpixels, so training on random pairs incurs a mismatch between training and testing distributions. Yet we can still achieve comparable performance despite this mismatch. This shows that our method can indeed operate without superpixel segmentation."}, {"heading": "5.2 Depth in the Wild", "text": "In this section we experiment on our new Depth in the Wild (DIW) dataset. We split the dataset into 421K training images and 74K test images 4.\nWe report the WHDR (Weighted Human Disagreement Rate) 5 of 5 methods in Tab. 3: (1) the state-of-the-art system by Eigen et al. [8] trained on full NYU Depth; (2) our network trained on full NYU Depth (Ours_Full); (3) our network pre-trained on full NYU Depth and fine-tuned on DIW (Ours_NYU_DIW); (4) our network trained from scratch on DIW (Ours_DIW); (5) a baseline method that uses only the location of the query points: classify the lower point to be closer or guess randomly if the two points are at the same height (Query_Location_Only).\nWe see that the best result is achieved by pre-training on NYU Depth and fine-tuning on DIW. Training only on NYU Depth (Ours_NYU and Eigen) does not work as well, which is expected because NYU Depth only has indoor scenes. Training from scratch on DIW achieves slightly better performance than those trained on only NYU Depth despite using much less supervision. Pre-training on NYU Depth and fine-tuning on DIW leaverages all available data and achieves the best performance. As shown in Fig. 10, the quality of predicted depth is notably better with fine-tuning on DIW, especially for outdoor scenes. These results suggest that it is promising to combine existing RGB-D datasets and crowdsourced annotations to significantly advance the state-of-the art in single-image depth estimation."}, {"heading": "6 Conclusions", "text": "We have studied single-image depth perception in the wild, recovering depth from a single image taken in unconstrained settings. We have introduced a new dataset consisting of images in the wild annotated with relative depth between pairs of random points. We have proposed a new algorithm that learns to estimate metric depth using annotations of relative depth. We have shown that our algorithm outperforms prior art. In addition, we have shown that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild."}], "references": [{"title": "Depthtransfer: Depth extraction from video using nonparametric sampling", "author": ["K. Karsch", "C. Liu", "S.B. Kang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic photo pop-up", "author": ["D. Hoiem", "A.A. Efros", "M. Hebert"], "venue": "ACM Transactions on Graphics (TOG), vol. 24, no. 3, pp. 577\u2013584, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Make3d: Learning 3d scene structure from a single still image", "author": ["A. Saxena", "M. Sun", "A. Ng"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, no. 5, pp. 824\u2013840, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "Computer Vision\u2013ECCV 2012, pp. 746\u2013760, Springer, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Vision meets robotics: The kitti dataset", "author": ["A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "The International Journal of Robotics Research, p. 0278364913491297, 2013. 4.38% of images are duplicates downloaded using different query keywords. We have removed test images that have duplicates in the training set. All weights are 1. Also a pair of points can only have two possible ordinal relations (farther or closer) for DIW. 9  Input Eigen Ours_NYU_DIW Input Eigen Ours_NYU_DIW Figure 10: Qualitative results on our Depth in the Wild (DIW) dataset by our method and the method of Eigen et al. [8].", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional neural fields for depth estimation from a single image", "author": ["F. Liu", "C. Shen", "G. Lin"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5162\u20135170, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Pulling things out of perspective", "author": ["L. Ladicky", "J. Shi", "M. Pollefeys"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pp. 89\u201396, IEEE, 2014. 10", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 2650\u20132658, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Coupled depth learning", "author": ["M.H. Baig", "L. Torresani"], "venue": "arXiv preprint arXiv:1501.04537, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs", "author": ["B. Li", "C. Shen", "Y. Dai", "A. van den Hengel", "M. He"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1119\u20131127, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving the kinect by cross-modal stereo", "author": ["W.W.-C. Chiu", "U. Blanke", "M. Fritz"], "venue": "BMVC, vol. 1, p. 3, Citeseer, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning depth from single monocular images", "author": ["A. Saxena", "S.H. Chung", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, pp. 1161\u20131168, 2005.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "The visual perception of 3-d shape from multiple cues: Are observers capable of perceiving metric structure", "author": ["J.T. Todd", "J.F. Norman"], "venue": "Perception & Psychophysics, vol. 65, no. 1, pp. 31\u201347, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning ordinal relationships for mid-level vision", "author": ["D. Zoran", "P. Isola", "D. Krishnan", "W.T. Freeman"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 388\u2013396, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A category-level 3d object dataset: Putting the kinect to work", "author": ["A. Janoch", "S. Karayev", "Y. Jia", "J.T. Barron", "M. Fritz", "K. Saenko", "T. Darrell"], "venue": "Consumer Depth Cameras for Computer Vision, pp. 141\u2013165, Springer, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Sun rgb-d: A rgb-d scene understanding benchmark suite", "author": ["S. Song", "S.P. Lichtenberg", "J. Xiao"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 567\u2013576, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A large dataset of object scans", "author": ["S. Choi", "Q.-Y. Zhou", "S. Miller", "V. Koltun"], "venue": "arXiv preprint arXiv:1602.02481, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Intrinsic images in the wild", "author": ["S. Bell", "K. Bala", "N. Snavely"], "venue": "ACM Transactions on Graphics (TOG), vol. 33, no. 4, p. 159, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Shape, illumination, and reflectance from shading", "author": ["J.T. Barron", "J. Malik"], "venue": "TPAMI, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "3-d depth reconstruction from a single still image", "author": ["A. Saxena", "S.H. Chung", "A.Y. Ng"], "venue": "International journal of computer vision, vol. 76, no. 1, pp. 53\u201369, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "From shading to local shape", "author": ["Y. Xiong", "A. Chakrabarti", "R. Basri", "S.J. Gortler", "D.W. Jacobs", "T. Zickler"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 37, no. 1, pp. 67\u201379, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Direction matters: Depth estimation with a surface normal classifier", "author": ["C. Hane", "L. Ladicky", "M. Pollefeys"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 381\u2013389, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Single image depth estimation from predicted semantic labels", "author": ["B. Liu", "S. Gould", "D. Koller"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pp. 1253\u20131260, IEEE, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Scene intrinsics and depth from a single image", "author": ["E. Shelhamer", "J. Barron", "T. Darrell"], "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 37\u201344, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Monocular object instance segmentation and depth ordering with cnns", "author": ["Z. Zhang", "A.G. Schwing", "S. Fidler", "R. Urtasun"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 2614\u20132622, 2015. 11", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards unified depth and semantic prediction from a single image", "author": ["P. Wang", "X. Shen", "Z. Lin", "S. Cohen", "B. Price", "A.L. Yuille"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2800\u20132809, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning data-driven reflectance priors for intrinsic image decomposition", "author": ["T. Zhou", "P. Krahenbuhl", "A.A. Efros"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 3469\u20133477, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning lightness from human judgement on relative reflectance", "author": ["T. Narihira", "M. Maire", "S.X. Yu"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pp. 2965\u20132973, IEEE, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "arXiv preprint arXiv:1312.6229, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Relative attributes", "author": ["D. Parikh", "K. Grauman"], "venue": "Computer Vision (ICCV), 2011 IEEE International Conference on, pp. 503\u2013510, IEEE, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Attribute and simile classifiers for face verification", "author": ["N. Kumar", "A.C. Berg", "P.N. Belhumeur", "S.K. Nayar"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on, pp. 365\u2013372, IEEE, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning globally-consistent local distance functions for shape-based image retrieval and classification", "author": ["A. Frome", "Y. Singer", "F. Sha", "J. Malik"], "venue": "Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pp. 1\u20138, IEEE, 2007.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Z. Cao", "T. Qin", "T.-Y. Liu", "M.-F. Tsai", "H. Li"], "venue": "Proceedings of the 24th international conference on Machine learning, pp. 129\u2013136, ACM, 2007.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimizing search engines using clickthrough data", "author": ["T. Joachims"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 133\u2013142, ACM, 2002.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2002}, {"title": "Depth map prediction from a single image using a multiscale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, pp. 2366\u20132374, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3431\u2013 3440, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Holistically-nested edge detection", "author": ["S. Xie", "Z. Tu"], "venue": "CoRR, vol. abs/1504.06375, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["A. Newell", "K. Yang", "J. Deng"], "venue": "arXiv preprint arXiv:1603.06937, 2016.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1\u20139, 2015.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Im2depth: Scalable exemplar based depth transfer", "author": ["M.H. Baig", "V. Jagadeesh", "R. Piramuthu", "A. Bhardwaj", "W. Di", "N. Sundaresan"], "venue": "Applications of Computer Vision (WACV), 2014 IEEE Winter Conference on, pp. 145\u2013152, IEEE, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014. 12", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 68, "endOffset": 77}, {"referenceID": 1, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 68, "endOffset": 77}, {"referenceID": 2, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 68, "endOffset": 77}, {"referenceID": 3, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 4, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 5, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 6, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 7, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 8, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 9, "context": "Recent years have seen rapid progress thanks to data-driven methods [1, 2, 3], in particular, deep neural networks trained on large RGB-D datasets [4, 5, 6, 7, 8, 9, 10].", "startOffset": 147, "endOffset": 169}, {"referenceID": 3, "context": "Current RGB-D datasets were collected by depth sensors [4, 5], which are limited in range and resolution, and often fail on specular or transparent objects [11].", "startOffset": 55, "endOffset": 61}, {"referenceID": 4, "context": "Current RGB-D datasets were collected by depth sensors [4, 5], which are limited in range and resolution, and often fail on specular or transparent objects [11].", "startOffset": 55, "endOffset": 61}, {"referenceID": 10, "context": "Current RGB-D datasets were collected by depth sensors [4, 5], which are limited in range and resolution, and often fail on specular or transparent objects [11].", "startOffset": 156, "endOffset": 160}, {"referenceID": 3, "context": "For example, NYU depth [4] consists entirely of indoor scenes with no human presence; KITTI [5] consists entirely of road scenes captured from a car; Make3D [3, 12] consists entirely of outdoor scenes of the Stanford campus (Figure.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "For example, NYU depth [4] consists entirely of indoor scenes with no human presence; KITTI [5] consists entirely of road scenes captured from a car; Make3D [3, 12] consists entirely of outdoor scenes of the Stanford campus (Figure.", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "For example, NYU depth [4] consists entirely of indoor scenes with no human presence; KITTI [5] consists entirely of road scenes captured from a car; Make3D [3, 12] consists entirely of outdoor scenes of the Stanford campus (Figure.", "startOffset": 157, "endOffset": 164}, {"referenceID": 11, "context": "For example, NYU depth [4] consists entirely of indoor scenes with no human presence; KITTI [5] consists entirely of road scenes captured from a car; Make3D [3, 12] consists entirely of outdoor scenes of the Stanford campus (Figure.", "startOffset": 157, "endOffset": 164}, {"referenceID": 12, "context": "Crowdsourcing seems viable, but humans are not good at estimating metric depth, or 3D metric structure in general [13].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "But humans are better at judging relative depth [13]: \u201cIs point A closer than point B?\u201d is often a much easier question for humans.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "[14] shows that it is possible to learn to estimate metric depth using only annotations of relative depth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14], but is also simpler.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] first learns a classifier to predict the ordinal relation between two points in an image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] but also the state-of-the-art image-todepth system by Eigen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] trained with ground-truth metric depth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 86, "endOffset": 101}, {"referenceID": 14, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 86, "endOffset": 101}, {"referenceID": 15, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 86, "endOffset": 101}, {"referenceID": 16, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 86, "endOffset": 101}, {"referenceID": 2, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 111, "endOffset": 117}, {"referenceID": 4, "context": "RGB-D Datasets: Prior work on constructing RGB-D datasets has relied on either Kinect [4, 15, 16, 17] or LIDAR [3, 5].", "startOffset": 111, "endOffset": 117}, {"referenceID": 2, "context": "Existing Kinect-based datasets are limited to indoor scenes; existing LIDARbased datasets are biased towards scenes of man-made structures [3, 5].", "startOffset": 139, "endOffset": 145}, {"referenceID": 4, "context": "Existing Kinect-based datasets are limited to indoor scenes; existing LIDARbased datasets are biased towards scenes of man-made structures [3, 5].", "startOffset": 139, "endOffset": 145}, {"referenceID": 17, "context": "Intrinsic Images in the Wild: Our work draws inspiration from Intrinsic Images in the Wild [18], a seminal work that crowdsources annotations of relative reflectance on unconstrained images.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 5, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 6, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 7, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 8, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 9, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 11, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 18, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 18, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 19, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 20, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 21, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 22, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 23, "context": "Depth from a Single Image: Image-to-depth is a long-standing problem with a large body of literature [1, 6, 7, 8, 9, 10, 12, 19, 19, 20, 21, 22, 23, 24].", "startOffset": 101, "endOffset": 152}, {"referenceID": 3, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 66, "endOffset": 72}, {"referenceID": 4, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 66, "endOffset": 72}, {"referenceID": 5, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 7, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 9, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 13, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 24, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 25, "context": "The recent convergence of deep neural networks and RGB-D datasets [4, 5] has led to major advances [6, 8, 10, 14, 25, 26].", "startOffset": 99, "endOffset": 121}, {"referenceID": 13, "context": "But the networks in these previous works, with the exception of [14], were trained exclusively using ground-truth metric depth, whereas our approach uses relative depth.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "[14], which proposes to use a deep network to repeatedly classify pairs of points sampled based on superpixel segmentation, and to reconstruct per-pixel metric depth by solving an additional optimization problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Learning with Ordinal Relations: Several recent works [27, 28] have used the ordinal relations from the Intrinsic Images in the Wild dataset [18] to estimate surface refletance.", "startOffset": 54, "endOffset": 62}, {"referenceID": 27, "context": "Learning with Ordinal Relations: Several recent works [27, 28] have used the ordinal relations from the Intrinsic Images in the Wild dataset [18] to estimate surface refletance.", "startOffset": 54, "endOffset": 62}, {"referenceID": 17, "context": "Learning with Ordinal Relations: Several recent works [27, 28] have used the ordinal relations from the Intrinsic Images in the Wild dataset [18] to estimate surface refletance.", "startOffset": 141, "endOffset": 145}, {"referenceID": 13, "context": "[14], Zhou et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] first learn a deep network to classify the ordinal relations between pair of points and then make them globally consistent through energy minimization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] learn a \u201clightness potential\u201d network that takes an image patch and predicts the metric reflectance of the center pixel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Making it fully convolutional (as the authors mentioned in [28]) only solves it partially: as long as the lightness potential network has downsampling layers, which is the case in [28], the final output will be downsampled accordingly.", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "Making it fully convolutional (as the authors mentioned in [28]) only solves it partially: as long as the lightness potential network has downsampling layers, which is the case in [28], the final output will be downsampled accordingly.", "startOffset": 180, "endOffset": 184}, {"referenceID": 28, "context": "Additional resolution augmentation (such as the \u201cshift and stitch\u201d approach [29]) is thus needed.", "startOffset": 76, "endOffset": 80}, {"referenceID": 29, "context": "Beyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34].", "startOffset": 135, "endOffset": 139}, {"referenceID": 30, "context": "Beyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34].", "startOffset": 158, "endOffset": 162}, {"referenceID": 31, "context": "Beyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34].", "startOffset": 180, "endOffset": 184}, {"referenceID": 32, "context": "Beyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34].", "startOffset": 207, "endOffset": 215}, {"referenceID": 33, "context": "Beyond intrinsic images, ordinal relations have been used widely in computer vision and machine learning, including object recognition [30], face recognition [31], image retrieval [32], and learning to rank [33, 34].", "startOffset": 207, "endOffset": 215}, {"referenceID": 13, "context": "[14] first learn a classifier to predict ordinal relations between centers of superpixels, and then reconcile", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Network Design: Networks that output the same resolution as the input are aplenty, including the recent designs for depth estimation [8, 35] and those for semantic segmentation [36] and edge detection [37].", "startOffset": 133, "endOffset": 140}, {"referenceID": 34, "context": "Network Design: Networks that output the same resolution as the input are aplenty, including the recent designs for depth estimation [8, 35] and those for semantic segmentation [36] and edge detection [37].", "startOffset": 133, "endOffset": 140}, {"referenceID": 35, "context": "Network Design: Networks that output the same resolution as the input are aplenty, including the recent designs for depth estimation [8, 35] and those for semantic segmentation [36] and edge detection [37].", "startOffset": 177, "endOffset": 181}, {"referenceID": 36, "context": "Network Design: Networks that output the same resolution as the input are aplenty, including the recent designs for depth estimation [8, 35] and those for semantic segmentation [36] and edge detection [37].", "startOffset": 201, "endOffset": 205}, {"referenceID": 37, "context": "In this work, we use a variant of the recently introduced \u201chourglass\u201d network, which has been used to achieve state-of-the-art results on human pose estimation [38].", "startOffset": 160, "endOffset": 164}, {"referenceID": 38, "context": "6 illustrates the design, which consists of a series of convolutions (using a variant of the inception [39] module) and downsampling, followed by a series of convolutions and upsampling, interleaved with skip connections that add back features from high resolutions.", "startOffset": 103, "endOffset": 107}, {"referenceID": 37, "context": "We refer the reader to [38] for comparing the design to related work.", "startOffset": 23, "endOffset": 27}, {"referenceID": 38, "context": "Figure 7: The variant of the Inception Module [39] used in our model.", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "We evaluate our method using the NYU Depth dataset [4], which consists of indoor scenes with ground-truth Kinect depth.", "startOffset": 51, "endOffset": 54}, {"referenceID": 13, "context": "[14]: point pairs are sampled from the training images (the subset of NYU Depth consisting of 795 images with semantic labels) using superpixel segmentation and their ground-truth ordinal relations are generated by comparing the ground-truth Kinect depth; the same procedure is applied to the test set to generate the point pairs for evaluation (around 3K pairs per image).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14], our network predicts one of the three ordinal relations on the test pairs: equal (=), closer (<), or farther (>).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "4% Zoran [14] 43.", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "7% Eigen(A) [8] 37.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "7% Eigen(V) [8] 34.", "startOffset": 12, "endOffset": 15}, {"referenceID": 13, "context": "43 Zoran [14] 1.", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "54 Eigen(A) [8] 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "19 Eigen(V) [8] 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 25, "context": "12 Wang [26] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "22 Liu [6] 0.", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "23 Li [10] 0.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "23 Karsch [1] 1.", "startOffset": 10, "endOffset": 13}, {"referenceID": 39, "context": "35 Baig [40] 1.", "startOffset": 8, "endOffset": 12}, {"referenceID": 7, "context": "Details for each metric can be found in [8].", "startOffset": 40, "endOffset": 43}, {"referenceID": 7, "context": "[8], one using AlexNet (Eigen(A)) and one using VGGNet (Eigen(V)).", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] define two points to have equal depths if the ratio between their groundtruth depths is within a pre-determined range.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Our network is trained with the same data 3 but outperforms [14] on all three metrics.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "Following [14], we also compare with the state-of-art image-to-depth system by Eigen et al.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "[8], which is trained on pixel-wise ground-truth metric depth from the full NYU Depth training set (220K images).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Granted, this comparison is not entirely fair because [8] is not optimized for predicting ordinal relations.", "startOffset": 54, "endOffset": 57}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We see that although imperfect, the recovered metric depth by our method is overall reasonable and qualitatively similar to that by the state-of-art system [8] trained on ground-truth metric depth.", "startOffset": 156, "endOffset": 159}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1]) that have access to ground-truth metric depth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "WKDR stands for \u201cWeighted Kinect Disagreement Rate\u201d; the weight is set to 1 as in [14] The code released by Zoran et al.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "[14] indicates that they train with a random subset of 800 pairs per image instead of the all pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8], and the method of Zoran et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "All depth maps except ours are directly from [14].", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "Figure 9: Point pairs generated through superpixel segmentation [14] (left) versus point pairs generated through random sampling with distance constraints (right).", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "[14], we train our network using the same point pairs, which are pairs of centers of superpixels (Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Method Eigen(V) [8] Ours_Full Ours_NYU_DIW Ours_DIW Query_Location_Only WHDR 25.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "[8] (VGGNet [41] version)", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "[8] (VGGNet [41] version)", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "[8] trained on full NYU Depth; (2) our network trained on full NYU Depth (Ours_Full); (3) our network pre-trained on full NYU Depth and fine-tuned on DIW (Ours_NYU_DIW); (4) our network trained from scratch on DIW (Ours_DIW); (5) a baseline method that uses only the location of the query points: classify the lower point to be closer or guess randomly if the two points are at the same height (Query_Location_Only).", "startOffset": 0, "endOffset": 3}], "year": 2017, "abstractText": "This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset \u201cDepth in the Wild\u201d consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild. Deep Network with Pixel-wise Prediction Metric Depth RGB-D Data Relative Depth Annotations", "creator": "LaTeX with hyperref package"}}}