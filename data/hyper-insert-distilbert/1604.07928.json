{"id": "1604.07928", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2016", "title": "Distributed Flexible Nonlinear Tensor Factorization", "abstract": "tensor factorization evolution is an important approach particular to developing multiway data data analysis. compared with popular multilinear representation methods, 3d nonlinear optics tensor factorization function models normally are able to capture more complex relationships stored in data. however, they traditionally are computationally typically expensive and likely incapable of exploiting entirely the data and sparsity. to overcome these dynamic limitations, we simply propose a new tensor tensor spatial factorization model. subsequently the model employs a gaussian process ( gp ) to capture the complex nonlinear relationships. internally the gp genome can be projected horizontally to match arbitrary mesh sets of 3d tensor attribute elements, and thus ourselves can avoid causing the aforementioned expensive image computation of the standardized kronecker product plot and is able essentially to flexibly incorporate naturally meaningful entries for training. overall furthermore, to scale speeds up the optical model bound to large xml data, we thus develop already a tightly distributed local variational inference algorithm known in mapreduce framework. proceed to attain this eventual end, and we derive a tractable and tight - variational graph evidence lower memory bound ( elbo ) that strongly enables efficient parallel matrix computations and high quality inference inferences. especially in my addition, we could design a non - blocking key - value map - reduce scheme that can partially prevent the infinitely costly data collection shuffling and fully use the specialized memory - protected cache search mechanism in fast mapreduce detection systems implementing such as pure spark. experiments demonstrate the advantages of showcasing our method over maintaining existing approaches in terms accused of showing both numerical predictive performance benefits and rapid computational efficiency. moreover, our approach shows a promising modelling potential step in the application of optimal click - through - rate ( ctr ) prediction capabilities for online map advertising.", "histories": [["v1", "Wed, 27 Apr 2016 04:18:32 GMT  (572kb)", "https://arxiv.org/abs/1604.07928v1", "Gaussian process, tensor factorization, multidimensional arrays, large scale, spark, map-reduce"], ["v2", "Sun, 22 May 2016 00:00:23 GMT  (596kb)", "http://arxiv.org/abs/1604.07928v2", "Gaussian process, tensor factorization, multidimensional arrays, large scale, spark, map-reduce"]], "COMMENTS": "Gaussian process, tensor factorization, multidimensional arrays, large scale, spark, map-reduce", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.DC stat.ML", "authors": ["shandian zhe", "kai zhang 0001", "pengyuan wang", "kuang-chih lee", "zenglin xu", "yuan qi", "zoubin ghahramani"], "accepted": true, "id": "1604.07928"}, "pdf": {"name": "1604.07928.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Pengyuan Wang", "Zoubin Ghahraman"], "emails": ["szhe@purdue.edu", "kzhang980@gmail.com", "pengyuan@yahoo-inc.com", "kclee@yahoo-inc.com", "zlxu@uestc.edu.cn", "alanqi@cs.purdue.edu", "zoubin@eng.cam.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 4."}, {"heading": "1 Introduction", "text": "Tensors, or multidimensional arrays, are generalizations of matrices (from binary interactions) to high-order interactions between multiple entities. For example, we can extract a three-mode tensor (user, advertisement, context) from online advertising data. To analyze tensor data, people usually turn to factorization approaches that use a set of latent factors to represent each entity and model how the latent factors interact with each other to generate tensor elements. Classical tensor factorization models include Tucker [24] and CANDECOMP/PARAFAC (CP) [8] decompositions, which have been widely used in real-world applications. However, because they all assume a multilinear interaction between the latent factors, they are unable to capture more complex, nonlinear relationships. Recently, Xu et al. [25] proposed Infinite Tucker decomposition (InfTucker), which generalizes the Tucker model to infinite feature space using a Tensor-variate Gaussian process (TGP) and thus is powerful to model intricate nonlinear interactions. However, InfTucker and its variants [28, 29] are computationally expensive, because the Kronecker product between the covariances of all the modes requires the TGP to model the entire tensor structure. In addition, they may suffer from the extreme sparsity of real-world tensor data, i.e., when the proportion of the nonzero entries is extremely low. As is often the case, most of the zero elements in real tensors are meaningless: they simply indicate missing or unobserved entries. Incorporating all of them in the training process may affect the factorization quality and lead to biased predictions.\nTo address these issues, in this paper we propose a distributed, flexible nonlinear tensor factorization model, which has several important advantages. First, it can capture highly nonlinear interactions in the tensor, and is flexible enough to incorporate arbitrary subset of (meaningful) tensorial entries for the training. This is achieved by placing Gaussian process priors over tensor entries, where the input is constructed by concatenating the latent factors from each mode and the intricate relationships are captured by using the kernel function. By using such a construction, the covariance function is then free of the Kronecker-product structure, and as a result users can freely choose any subset of tensor elements for the training process and incorporate prior domain knowledge. For example, one can choose a combination of balanced zero and nonzero elements to overcome the learning bias. Second, the tight variational evidence lower bound (ELBO) we derived using functional derivatives and convex conjugates subsumes optimal variational posteriors, thus evades inefficient, sequential E-M updates and enables highly efficient, parallel computations as well as improved inference quality. Moreover, the new bound allows us to develop a distributed, gradient-based optimization algorithm. Finally, we develop a simple yet very efficient procedure to avoid the data shuffling operation, a major performance bottleneck in the (key-value) sorting procedure in MAPREDUCE. That is, rather than sending out key-value pairs, each mapper simply calculates and sends a global gradient vector without keys. This keyvalue-free procedure is general and can effectively prevent massive disk IOs and fully exploit the memory cache mechanism in fast MAPREDUCE systems, such as SPARK.\nEvaluation using small real-world tensor data have fully demonstrated the superior prediction accuracy of our model in comparison with InfTucker and other state-ofthe-art. On large tensors with millions of nonzero elements, our approach is signifi-\ncantly better than, or at least as good as two popular large-scale nonlinear factorization methods based on TGP: one uses hierarchical modeling to perform distributed infinite Tucker decomposition [28]; the other further enhances InfTucker by using Dirichlet process mixture prior over the latent factors and employs an online learning scheme [29]. Our method also outperforms GigaTensor [11], a typical large-scale CP factorization algorithm, by a large margin. In addition, our method achieves faster training speed and enjoys almost linear scalability on the number of computational nodes. We apply our model to CTR prediction for online advertising and achieves a significant, 20% improvement over the popular logistic regression and linear SVM approaches."}, {"heading": "2 Background", "text": "We first introduce the background knowledge. For convenience, we will use the same notations in [25]. Specifically, we denote a K-mode tensor by M \u2208 Rd1\u00d7...\u00d7dK , where the k-th mode is of dimension dk. The tensor entry at location i (i = (i1, . . . , iK)) is denoted by mi. To introduce Tucker decomposition, we need to generalize matrixmatrix products to tensor-matrix products. Specifically, a tensor W \u2208 Rr1\u00d7...\u00d7rK can multiply with a matrix U \u2208 Rs\u00d7t at mode k when its dimension at mode-k is consistent with the number of columns in U, i.e., rk = t. The product is a new tensor, with size r1 \u00d7 . . . \u00d7 rk\u22121 \u00d7 s \u00d7 rk+1 \u00d7 . . . \u00d7 rK . Each element is calculated by (W \u00d7k U)i1...ik\u22121jik+1...iK = \u2211rk ik=1 wi1...iKujik .\nThe Tucker decomposition model uses a latent factor matrix Uk \u2208 Rdk\u00d7rk in each mode k and a core tensor W \u2208 Rr1\u00d7...\u00d7rK and assumes the whole tensor M is generated by M = W\u00d71U(1)\u00d72 . . .\u00d7KU(K). Note that this is a multilinear function of W and {U1, . . . ,UK}. It can be further simplified by restricting r1 = r2 = . . . = rK and the off-diagonal elements of W to be 0. In this case, the Tucker model becomes CANDECOMP/PARAFAC (CP).\nThe infinite Tucker decomposition (InfTucker) generalizes the Tucker model to infinite feature space via a tensor-variate Gaussian process (TGP) [25]. Specifically, in a probabilistic framework, we assign a standard normal prior over each element of the core tensor W , and then marginalize out W to obtain the probability of the tensor given the latent factors:\np(M|U(1), . . . ,U(K)) = N (vec(M);0,\u03a3(1) \u2297 . . .\u2297 \u03a3(K)) (1)\nwhere vec(M) is the vectorized whole tensor,\u03a3(k) = U(k)U(k) \u22a4\nand\u2297 is the Kroneckerproduct. Next, we apply the kernel trick to model nonlinear interactions between the latent factors: Each row ukt of the latent factors U\n(k) is replaced by a nonlinear feature transformation \u03c6(ukt ) and thus an equivalent nonlinear covariance matrix \u03a3(k) = k(U(k),U(k)) is used to replace U(k)U(k) \u22a4\n, where k(\u00b7, \u00b7) is the covariance function. After the nonlinear feature mapping, the original Tucker decomposition is performed in an (unknown) infinite feature space. Further, since the covariance of vec(M) is a function of the latent factors U = {U(1), . . . ,U(K)}, Equation (1) actually defines a Gaussian process (GP) on tensors, namely tensor-variate GP (TGP) [25],\nwhere the input are based on U . Finally, we can use different noisy models p(Y|M) to sample the observed tensor Y . For example, we can use Gaussian models and Probit models for continuous and binary observations, respectively."}, {"heading": "3 Model", "text": "Despite being able to capture nonlinear interactions, InfTucker may suffer from the extreme sparsity issue in real-world tensor data sets. The reason is that its full covariance is a Kronecker-product between the covariances over all the modes\u2014{\u03a3(1), . . . ,\u03a3(K)} (see Equation (1)). Each \u03a3(k) is of size dk \u00d7 dk and the full covariance is of size \u220f\nk dk \u00d7 \u220f\nk dk. Thus TGP is projected onto the entire tensor with respect to the latent factors U , including all zero and nonzero elements, rather than a (meaningful) subset of them. However, the real-world tensor data are usually extremely sparse, with a huge number of zero entries and a tiny portion of nonzero entries. On one hand, because most zero entries are meaningless\u2014they are either missing or unobserved, using them can adversely affect the tensor factorization quality and lead to biased predictions; on the other hand, incorporating numerous zero entries into GP models will result in large covariance matrices and high computational costs. Although Zhe et al. [28, 29] proposed to improve the scalability by modeling subtensors instead, the sampled subtensors can still be very sparse. Even worse, because subtensors are typically restricted to a small dimension due to the efficiency considerations, it is often possible to encounter one that does not contain any nonzero entry. This may further incur numerical instabilities in model estimation.\nTo address these issues, we propose a flexible Gaussian process tensor factorization model. While inheriting the nonlinear modeling power, our model disposes of the Kronecker-product structure in the full covariance and can therefore select an arbitrary subset of tensor entries for training.\nSpecifically, given a tensor M \u2208 Rd1\u00d7...\u00d7dK , for each tensor entry mi (i = (i1, . . . , iK)), we construct an input xi by concatenating the corresponding latent factors from all the modes: xi = [u (1) i1 , . . . ,u (K) iK\n], where u(k)ik is the ik-th row in the latent factor matrix U(k) for mode k. We assume that there is an underlying function f : R \u2211K j=1 dj \u2192 R such that mi = f(xi) = f([u (1) i1 , . . . ,u (K) iK\n]). This function is unknown and can be complex and nonlinear. To learn the function, we assign a Gaussian process prior over f : for any set of tensor entries S = {i1, . . . , iN}, the function values fS = {f(xi1), . . . , f(xiN )} are distributed according to a multivariate Gaussian distribution with mean 0 and covariance determined by XS = {xi1 , . . . ,xiN }:\np(fS |U) = N (fS |0, k(XS ,XS))\nwhere k(\u00b7, \u00b7) is a (nonlinear) covariance function.\nBecause k(xi,xj) = k([u (1) i1 , . . . ,u (K) iK ], [u (1) j1 , . . . ,u (K) jK\n]), there is no Kroneckerproduct structure constraint and so any subset of tensor entries can be selected for training. To prevent the learning process to be biased toward zero, we can use a set of entries with balanced zeros and nonzeros. Furthermore, useful domain knowledge can also be incorporated to select meaningful entries for training. Note, however, that if we\nstill use all the tensor entries and intensionally impose the Kronecker-product structure in the full covariance, our model is reduced to InfTucker. Therefore, from the modeling perspective, the proposed model is more general.\nWe further assign a standard normal prior over the latent factors U . Given the selected tensor entries m = [mi1 , . . . ,miN ], the observed entries y = [yi1 , . . . , yiN ] are sampled from a noise model p(y|m). In this paper, we deal with both continuous and binary observations. For continuous data, we use the Gaussian model, p(y|m) = N (y|m, \u03b2\u22121I) and the joint probability is\np(y,m,U) = \u220fK\nt=1 N (vec(U(t))|0, I)N (m|0, k(XS ,XS))N (y|m, \u03b2 \u22121I) (2)\nwhere S = [i1, . . . , iN ]. For binary data, we use the Probit model in the following manner. We first introduce augmented variables z = [z1, . . . , zN ] and then decompose the Probit model into p(zj |mij ) = N (zj |mij , 1) and p(yij |zj) = 1(yij = 0)1(zj \u2264 0) + 1(yij = 1)1(zj > 0) where 1(\u00b7) is the indicator function. Then the joint probability is\np(y, z,m,U) = \u220fK\nt=1 N (vec(U(t))|0, I)N (m|0, k(XS ,XS))N (z|m, I)\n\u00b7 \u220f\nj 1(yij = 0)1(zj \u2264 0) + 1(yij = 1)1(zj > 0). (3)"}, {"heading": "4 Distributed Variational Inference", "text": "Real-world tensor data often comprise a large number of entries, say, millions of nonzeros and billions of zeros. Even by only using nonzero entries for training, exact inference of the proposed model may still be intractable. This motivates us to develop a distributed variational inference algorithm, presented as follows."}, {"heading": "4.1 Tractable Variational Evidence Lower Bound", "text": "Since the GP covariance term \u2014 k(XS ,XS) (see Equations (2) and (3)) intertwines all the latent factors, exact inference in parallel is difficult. Therefore, we first derive a tractable variational evidence lower bound (ELBO), following the sparse Gaussian process framework by Titsias [23]. The key idea is to introduce a small set of inducing points B = {b1, . . . ,bp} and latent targets v = {v1, . . . , vp} (p \u226a N ). Then we augment the original model with a joint multivariate Gaussian distribution of the latent tensor entries m and targets v,\np(m,v|U ,B) = N (\n[\nm\nv\n]\n;\n[\n0\n0\n]\n,\n[\nKSS KSB KBS KBB\n]\n)\nwhere KSS = k(XS ,XS), KBB = k(B,B), KSB = k(XS ,B) and KBS = k(B,XS). We use Jensen\u2019s inequality and conditional Gaussian distributions to construct the ELBO. Using a very similar derivation to [23], we can obtain a tractable\nELBO for our model on continuous data, log ( p(y,U|B) ) \u2265 L1 ( U ,B, q(v) ) , where\nL1 ( U ,B, q(v) ) = log(p(U)) +\n\u222b\nq(v) log p(v|B)\nq(v) dv\n+ \u2211\nj\n\u222b\nq(v)Fv(yij , \u03b2)dv. (4)\nHere p(v|B) = N (v|0,KBB), q(v) is the variational posterior for the latent targets v andFv(\u00b7j , \u2217) = \u222b log ( N (\u00b7j |mij , \u2217) ) N (mij |\u00b5j , \u03c3 2 j )dmij , where\u00b5j = k(xij ,B)K \u22121 BBv and \u03c32j = \u03a3(j, j) = k(xij ,xij ) \u2212 k(xij ,B)K \u22121 BBk(B,xij ). Note that L1 is decomposed into a summation of terms involving individual tensor entries ij(1 \u2264 j \u2264 N). The additive form enables us to distribute the computation across multiple computers.\nFor binary data, we introduce a variational posterior q(z) and make the meanfield assumption that q(z) = \u220f\nj q(zj). Following a similar derivation to the continuous case, we can obtain a tractable ELBO for binary data, log ( p(y,U|B) ) \u2265 L2 ( U ,B, q(v), q(z) ) , where\nL2 ( U ,B, q(v), q(z) ) = log(p(U)) +\n\u222b\nq(v) log( p(v|B)\nq(v) )dv\n+ \u2211\nj q(zj) log(\np(yij |zj)\nq(zj) ) +\n\u2211\nj\n\u222b\nq(v)\n\u222b\nq(zj)Fv(zj, 1)dzjdv. (5)\nOne can simply use the standard Expectation-maximization (EM) framework to optimize (4) and (5) for model inference, i.e., the E step updates the variational posteriors {q(v), q(z)} and the M step updates the latent factors U , the inducing points B and the kernel parameters. However, the sequential E-M updates can not fully exploit the paralleling computing resources. Due to the strong dependencies between the E step and the M step, the sequential E-M updates may take a large number of iterations to converge. Things become worse for binary case: in the E step, the updates of q(v) and q(z) are also dependent on each other, making a parallel inference even less efficient."}, {"heading": "4.2 Tight and Parallelizable Variational Evidence Lower Bound", "text": "In this section, we further derive tight(er) ELBOs that subsume the optimal variational posteriors for q(v) and q(z). Thereby we can avoid the sequential E-M updates to perform decoupled, highly efficient parallel inference. Moreover, the inference quality is very likely to be improved using tighter bounds. Due to the space limit, we only present key ideas and results here. Detailed discussions are given in Section 1 of the supplementary material. Tight ELBO for continuous tensors. We take functional derivative of L1 with respect to q(v) in (4). By setting the derivative to zero, we obtain the optimal q(v) (which is a Gaussian distribution) and then substitute it into L1, manipulating the terms, we achieve the following tighter ELBO.\nTheorem 4.1. For continuous data, we have\nlog ( p(y,U|B) ) \u2265 L\u22171(U ,B) = 1\n2 log |KBB| \u2212\n1 2 log |KBB + \u03b2A1| \u2212 1 2 \u03b2a2 \u2212 1 2 \u03b2a3\n+ \u03b2\n2 tr(K\u22121BBA1)\u2212\n1\n2\nK \u2211\nk=1\n\u2016U(k)\u20162F + 1\n2 \u03b22a\u22a44 (KBB + \u03b2A1)\n\u22121a4 + N\n2 log(\n\u03b2\n2\u03c0 ),\n(6)\nwhere \u2016 \u00b7 \u2016F is Frobenius norm, and\nA1 = \u2211\nj k(B,xij )k(xij ,B), a2 =\n\u2211\nj y2ij ,\na3 = \u2211\nj k(xij ,xij ), a4 =\n\u2211\nj k(B,xij )yij .\nTight ELBO for binary tensors. The binary case is more difficult because q(v) and q(z) are coupled together (see (5)). We use the following steps: we first fix q(z) and plug the optimal q(v) in the same way as the continuous case. Then we obtain an intermediate ELBO L\u03022 that only contains q(z). However, a quadratic term in L\u03022 , 1 2 (KBS\u3008z\u3009) \u22a4(KBB +A1) \u22121(KBS\u3008z\u3009), intertwines all {q(zj)}j in L\u03022, making it infeasible to analytically derive or parallelly compute the optimal {q(zj)}j . To overcome this difficulty, we exploit the convex conjugate of the quadratic term to introduce an extra variational parameter \u03bb to decouple the dependences between {q(zj)}j . After that, we are able to derive the optimal {q(zj)}j using functional derivatives and to obtain the following tight ELBO.\nTheorem 4.2. For binary data, we have\nlog ( p(y,U|B) ) \u2265 L\u22172(U ,B,\u03bb) = 1\n2 log |KBB| \u2212\n1 2 log |KBB +A1| \u2212 1 2 a3\n+ \u2211\nj\nlog ( \u03a6((2yij \u2212 1)\u03bb \u22a4k(B,xij )) )\n\u2212 1\n2 \u03bb\u22a4KBB\u03bb+\n1 2 tr(K\u22121BBA1)\n\u2212 1\n2\nK \u2211\nk=1\n\u2016U(k)\u20162F (7)\nwhere \u03a6(\u00b7) is the cumulative distribution function of the standard Gaussian.\nAs we can see, due to the additive forms of the terms in L\u22171 and L \u2217 2, such as A1, a2, a3 and a4, the computation of the tight ELBOs and their gradients can be efficiently performed in parallel. The derivation of the full gradient is given in Section 2 of the supplementary material."}, {"heading": "4.3 Distributed Inference on Tight Bound", "text": ""}, {"heading": "4.3.1 Distributed Gradient-based Optimization", "text": "Given the tighter ELBOs in (6) and (7), we develop a distributed algorithm to optimize the latent factors U , the inducing points B, the variational parameters \u03bb (for binary\ndata) and the kernel parameters. We distribute the computations over multiple computational nodes (MAP step) and then collect the results to calculate the ELBO and its gradient (REDUCE step). A standard routine, such as gradient descent and L-BFGS, is then used to solve the optimization problem.\nFor binary data, we further find that \u03bb can be updated with a simple fixed point iteration:\n\u03bb(t+1) = (KBB +A1) \u22121(A1\u03bb (t) + a5) (8)\nwhere a5 = \u2211 j k(B,xij )(2yij \u2212 1) N ( k(B,xij ) \u22a4 \u03bb (t)|0,1 )\n\u03a6 (\n(2yij\u22121)k(B,xij ) \u22a4\u03bb(t)\n) .\nApparently, the updating can be efficiently performed in parallel (due to the additive structure of A1 and a5). Moreover, the convergence is guaranteed by the following lemma. The proof is given in Section 3 of the supplementary material.\nLemma 4.3. Given U and B, we have L\u22172(U ,B,\u03bb t+1) \u2265 L\u22172(U ,B,\u03bb t) and the fixed point iteration (8) always converges.\nIn our experience, the fixed-point iterations are much more efficient than general search strategies (such as line-search) to identity an appropriate step length along the gradient direction. To use it, before we calculate the gradients with respect to U and B, we first optimize \u03bb using the fixed point iteration (in an inner loop). In the outer control, we then employ gradient descent or L-BFGS to optimize U and B. This will lead to an even tighter bound for our model: L\u2217\u22172 (U ,B) = max\u03bb L \u2217 2(U ,B,\u03bb) = maxq(v),q(z) L2(U ,B, q(v), q(z)). Empirically, this converges must faster than feeding the optimization algorithms with \u2202\u03bb, \u2202U and \u2202B altogether."}, {"heading": "4.3.2 Key-Value-Free MAPREDUCE", "text": "In this section we present the detailed design of MAPREDUCE procedures to fulfill our distributed inference. Basically, we first allocate a set of tensor entries St on each MAPPER t such that the corresponding components of the ELBO and the gradients are calculated. Then the REDUCER aggregates local results from each MAPPER to obtain the integrated, global ELBO and gradient.\nWe first consider the standard (key-value) design. For brevity, we take the gradient computation for the latent factors as an example. For each tensor entry i on a MAPPER, we calculate the corresponding gradients {\u2202u(1)i1 , . . . \u2202u (K) iK } and then send out the keyvalue pairs {(k, ik) \u2192 \u2202u (k) ik\n}k, where the key indicates the mode and the index of the latent factors. The REDUCER aggregates gradients with the same key to recover the full gradient with respect to each latent factor.\nAlthough the (key-value) MAPREDUCE has been successfully applied in numerous applications, it relies on an expensive data shuffling operation: the REDUCE step has to sort the MAPPERS\u2019 output by the keys before aggregation. Since the sorting is usually performed on disk due to significant data size, intensive disk I/Os and network communications will become serious computational overheads. To overcome this deficiency, we devise a key-value-free MAP-REDUCE scheme to avoid on-disk data shuffling operations. Specifically, on each MAPPER, a complete gradient vector is maintained for\nall the parameters, including U , B and the kernel parameters. However, only relevant components of the gradient, as specified by the tensor entries allocated to this MAPPER, will be updated. After updates, each MAPPER will then send out the full gradient vector, and the REDUCER will simply sum them up together to obtain a global gradient vector without having to perform any extra data sorting. Note that a similar procedure can also be used to perform the fixed point iteration for \u03bb (in binary tensors).\nEfficient MAPREDUCE systems, such as SPARK [27], can fully optimize the nonshuffling MAP and REDUCE, where most of the data are buffered in memory and disk I/Os are circumvented to the utmost; by contrast, the performance with data shuffling degrades severely [6]. This is verified in our evaluations: on a small tensor of size 100\u00d7 100\u00d7 100, our key-value-free MAPREDUCE gains 30 times speed acceleration over the traditional key-value process. Therefore, our algorithm can fully exploit the memory-cache mechanism to achieve fast inference."}, {"heading": "4.4 Algorithm Complexity", "text": "Suppose we use N tensor entries for training, with p inducing points and T MAPPER, the time complexity for each MAPPER node is O( 1\nT p2N). Since p \u226a N is a fixed\nconstant (p = 100 in our experiments), the time complexity is linear in the number of tensor entries. The space complexity for each MAPPER node is O( \u2211K\nj=1 mjrj +\np2+ N T K), in order to store the latent factors, their gradients, the covariance matrix on inducing points, and the indices of the latent factors for each tensor entry. Again, the space complexity is linear in the number of tensor entries. In comparison, InfTucker utilizes the Kronecker-product properties to calculate the gradients and has to perform eigenvalue decomposition of the covariance matrices in each tensor mode. Therefor it has a higher time and space complexity (see [25] for details) and is not scalable to large dimensions."}, {"heading": "5 Related work", "text": "Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17]. To deal with big data, several distributed factorization algorithms have been recently developed, such as GigaTensor [11] and DFacTo [4]. Despite the widespread success of these methods, their underlying multilinear factorization structure may limit their capability to capture more complex, nonlinear relationship in real-world applications. Infinite Tucker decomposition [25], and its distributed or online extensions [28, 29] address this issue by modeling tensors or subtensors via a tensor-variate Gaussian process (TGP). However, these methods may suffer from the extreme sparsity in real-world tensor data, because the Kronecker-product structure in the covariance of TGP requires modeling the entire tensor space no matter the elements are meaningful (non-zeros) or not. By contrast, our flexible GP factorization model eliminates the Kroneckerproduct restriction and can model an arbitrary subset of tensor entries. In theory, all such nonlinear factorization models belong to the random function prior models [14] for exchangeable multidimensional arrays.\nOur distributed variational inference algorithm is based on sparse GP [16], an efficient approximation framework to scale up GP models. Sparse GP uses a small set of inducing points to break the dependency between random function values. Recently, Titsias [23] proposed a variational learning framework for sparse GP, based on which Gal et al. [7] derived a tight variational lower bound for distributed inference of GP regression and GPLVM [13]. The derivation of the tight ELBO in our model for continuous tensors is similar to [7]. However, the gradient calculation is substantially different, because the input to our GP factorization model is the concatenation of the latent factors. Many tensor entries may partly share the same latent factors, causing a large amount of key-value pair to be sent during the distributed gradient calculation. This will incur an expensive data shuffling procedure that takes place on disk. To improve the computational efficiency, we develop a non-key-value MAP-REDUCE to avoid data shuffling and fully exploit the memory-cache mechanism in efficient MAPREDUCE systems. This strategy is also applicable to other MAP-REDUCE based learning algorithms. In addition to continuous data, we also develop a tight ELBO for binary data on optimal variational posteriors. By introducing p extra variational parameters with convex conjugates (p is the number of inducing points), our inference can be performed efficiently in a distributed manner, which avoids explicit optimization on a large number of variational posteriors for the latent tensor entries and inducing targets. Our method can also be useful for GP classification problem."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Evaluation on Small Tensor Data", "text": "For evaluation, we first compared our method with various existing tensor factorization methods. To this end, we used four small real datasets where all methods are computationally feasible: (1) Alog, a real-valued tensor of size 200 \u00d7 100 \u00d7 200, representing a three-way interaction (user, action, resource) in a file access log. It contains 0.33% nonzero entries.(2) AdClick, a real-valued tensor of size 80 \u00d7 100 \u00d7 100, describing (user, publisher, advertisement) clicks for online advertising. It contains 2.39% nonzero entries. (3) Enron, a binary tensor extracted from the Enron email dataset (www.cs.cmu.edu/\u02dc./enron/) depicting the three-way relationship (sender, receiver, time). It contains 203 \u00d7 203 \u00d7 200 elements, of which 0.01% are nonzero. (4) NellSmall, a binary tensor extracted from the NELL knowledge base (rtw.ml.cmu.edu/rtw/resources), of size 295 \u00d7 170 \u00d7 94. It depicts the knowledge predicates (entity, relationship, entity). The data set contains 0.05% nonzero elements.\nWe compared with CP, nonnegative CP (NN-CP) [19], high order SVD (HOSVD) [12], Tucker, infinite Tucker (InfTucker) Xu et al. [25] and its extension (InfTuckerEx) which uses the Dirichlet process mixture (DPM) prior to model latent clusters and local TGP to perform scalable, online factorization [29]. Note that InfTucker and InfTuckerEx are nonlinear factorization approaches.\nFor testing, we used the same setting as in [29]. All the methods were evaluated via a 5-fold cross validation. The nonzero entries were randomly split into 5 folds: 4 folds\nwere used for training and the remaining non-zero entries and 0.1% zero entries were used for testing so that the number of non-zero entries is comparable to the number of zero entries. By doing this, zero and nonzero entries are treated equally important in testing, and so the evaluation will not be dominated by large portion of zeros. For InfTucker and InfTuckerEx, we carried out extra cross-validations to select the kernel form (e.g., RBF, ARD and Matern kernels) and the kernel parameters. For InfTuckerEx, we randomly sampled subtensors and tuned the learning rate following [29]. For our model, the number of inducing points was set to 100, and we used a balanced training set generated as follows: in addition to nonzero entries, we randomly sampled the same number of zero entries and made sure that they would not overlap with the testing zero elements.\nOur model used ARD kernel and the kernel parameters were estimated jointly with the latent factors. Thus, the expensive parameter selection procedure was not needed. We implemented our distributed inference algorithm with two optimization frameworks, gradient descent and L-BFGS (denoted by Ours-GD and Ours-LBFGS respectively). For a comprehensive evaluation, we also examined CP on balanced training entries generated in the same way as our model, denoted by CP-2. The mean squared error (MSE) is used to evaluate predictive performance on Alog and Click and area-under-curve (AUC) on Enron and Nell. The averaged results from the 5-fold cross validation are reported.\nOur model achieves a higher prediction accuracy than InfTucker, and a better or comparable accuracy than InfTuckerEx (see Figure 1). A t-test shows that our model outperforms InfTucker significantly (p < 0.05) in almost all situations. Although InfTuckerEx uses the DPM prior to improve factorization, our model still obtains significantly better predictions on Alog and AdClick and comparable or better performance on Enron and NellSmall. This might be attributed to the flexibility of our model in using balanced training entries to prevent the learning bias (toward numerous zeros). Similar improvements can be observed from CP to CP-2. Finally, our model outperforms all the remaining methods, demonstrating the advantage of our nonlinear factorization approach."}, {"heading": "6.2 Scalability Analysis", "text": "To examine the scalability of the proposed distributed inference algorithm, we used the following large real-world datasets: (1) ACC, A real-valued tensor describing threeway interactions (user, action, resource) in a code repository management system [29]. The tensor is of size 3K\u00d7150\u00d730K , where 0.009% are nonzero. (2) DBLP: a binary tensor depicting a three-way bibliography relationship (author, conference, keyword) [29]. The tensor was extracted from DBLP database and contains 10K \u00d7 200\u00d7 10K elements, where 0.001% are nonzero entries. (3) NELL: a binary tensor representing the knowledge predicates, in the form of (entity, entity, relationship) [28]. The tensor size is 20K \u00d7 12.3K \u00d7 280 and 0.0001% are nonzero.\nThe scalability of our distributed inference algorithm was examined with regard to the number of machines on ACC dataset. The number of latent factors was set to 3. We ran our algorithm using the gradient descent. The results are shown in Figure 2(a). The Y-axis shows the reciprocal of the running time multiplied by a constant\u2014which\ncorresponds to the running speed. As we can see, the speed of our algorithm scales up linearly to the number of machines."}, {"heading": "6.3 Evaluation on Large Tensor Data", "text": "We then compared our approach with three state-of-the-art large-scale tensor factorization methods: GigaTensor [11], Distributed infinite Tucker decomposition (DinTucker) [28], and InfTuckerEx [29]. Both GigaTensor and DinTucker are developed on HADOOP, while InfTuckerEx uses online inference. Our model was implemented on SPARK. We ran Gigatensor, DinTucker and our approach on a large YARN cluster and InfTuckerEx on a single computer.\nWe set the number of latent factors to 3 for ACC and DBLP data set, and 5 for NELL data set. Following the settings in [29, 28], we randomly chose 80% of nonzero entries for training, and then sampled 50 test data sets from the remaining entries. For ACC and DBLP, each test data set comprises 200 nonzero elements and 1, 800 zero elements; for NELL, each test data set contains 200 nonzero elements and 2, 000 zero elements. The running of GigaTensor was based on the default settings of the software package. For DinTucker and InfTuckerEx, we randomly sampled subtensors for distributed or online inference. The parameters, including the number and size of the subtensors and the learning rate, were selected in the same way as [29]. The kernel form and parameters were chosen by a cross-validation on the training tensor. For our model, we used the same setting as in the small data. We set 50 MAPPERS for GigaTensor, DinTucker and our model.\nFigure 2(b)-(d) shows the predictive performance of all the methods. We observe that our approach consistently outperforms GigaTensor and DinTucker on all the three datasets; our approach outperforms InfTuckerEx on ACC and DBLP and is slightly worse than InfTuckerEx on NELL. Note again that InfTuckerEx uses DPM prior to en-\nhance the factorization while our model doesn\u2019t; finally, all the nonlinear factorization methods outperform GigaTensor, a distributed CP factorization algorithm by a large margin, confirming the advantages of nonlinear factorizations on large data. In terms of speed, our algorithm is much faster than GigaTensor and DinTucker. For example, on DBLP dataset, the average per-iteration running time were 1.45, 15.4 and 20.5 minutes for our model, GigaTensor and DinTucker, respectively. This is not surprising, because (1) our model uses the data sparsity and can exclude numerous, meaningless zero elements from training; (2) our algorithm is based on SPARK, a more efficient MAPREDUCE system than HADOOP; (3) our algorithm gets rid of data shuffling and can fully exploit the memory-cache mechanism of SPARK."}, {"heading": "6.4 Application on Click-Through-Rate Prediction", "text": "In this section, we report the results of applying our nonlinear tensor factorization approach on Click-Through-Rate (CTR) prediction for online advertising.\nWe used the online ads click log from a major Internet company, from which we extracted a four mode tensor (user, advertisement, publisher, page-section). We used the first three days\u2019s log on May 2015, trained our model on one day\u2019s data and used it to predict the click behaviour on the next day. The sizes of the extracted tensors for the three days are 179K \u00d7 81K \u00d7 35 \u00d7 355, 167K \u00d7 78K \u00d7 35 \u00d7 354 and 213K \u00d7 82K\u00d737\u00d7354 respectively. These tensors are very sparse (2.7\u00d710\u22128% nonzeros on average). In other words, the observed clicks are very rare. However, we do not want our prediction completely bias toward zero (i.e., non-click); otherwise, ads ranking and recommendation will be infeasible. Thus we sampled non-clicks of the same quantity as the clicks for training and testing. Note that training CTR prediction models with comparable clicks and non-click samples is common in online advertising systems [2]. The number of training and testing entries used for the three days are (109K, 99K), (91K, 103K) and (109K, 136K) respectively.\nWe compared with popular methods for CTR prediction, including logistic regression and linear SVM, where each tensor entry is represented by a set of binary features according to the indices of each mode in the entry.\nThe results are reported in Table 1, in terms of AUC. It shows that our model improves logistic regression and linear SVM by a large margin, on average 20.7% and 20.8% respectively. Therefore, although we have not incorporated side features, such as user profiles and advertisement attributes, our tentative experiments have shown a promising potential of our model on CTR prediction task."}, {"heading": "7 Conclusion", "text": "In this paper, we have proposed a new nonlinear and flexible tensor factorization model. By disposing of the Kronecker-product covariance structure, the model can properly exploit the data sparsity and is flexible to incorporate any subset of meaningful tensor entries for training. Moreover, we have derived a tight ELBO for both continuous and binary problems, based on which we further developed an efficient distributed variational inference algorithm in MAPREDUCE framework. In the future, we will consider applying asynchronous inference on the tight ELBO, such as [20], to further improve the scalability of our model."}, {"heading": "1 Tight Variational Evidence Lower Bound", "text": "The naive variational evidence lower bound (ELBO) derived from the sparse Gaussian process framework (see Section 4.1 of the main paper) is given by\nL1(U ,B, q(v)) = log(p(U)) +\n\u222b\nq(v) log p(v|B)\nq(v) dv\n+ \u2211\nj\n\u222b\nq(v)Fv(yij , \u03b2)dv (9)\nfor continuous tensor and\nL2(U ,B, q(v), q(z)) = log(p(U)) +\n\u222b\nq(v) log( p(v|B)\nq(v) )dv\n+ \u2211\nj q(zj) log(\np(yij |zj)\nq(zj) ) +\n\u2211\nj\n\u222b\nq(v)\n\u222b\nq(zj)Fv(zj , 1)dzjdv (10)\nfor binary tensor, where Fv(\u00b7j , \u2217) = \u222b log ( N (\u00b7j |mij , \u2217) ) N (mij |\u00b5j , \u03c3 2 j )dmij and p(v|B) = N (v|0,KBB). Our goal is to further obtain a tight ELBO that subsumes the optimal variational posterior (i.e., q(v) and q(z)) so as to prevent the sequential E-M procedure for efficient parallel training and to improve the inference quality."}, {"heading": "1.1 Continuous Tensor", "text": "First, let us consider the continuous data. Given U and B, we use functional derivatives [3] to calculate the optimal q(v). The functional derivative of L1 with respect to q(v) is given by\n\u03b4L1(q)\n\u03b4q(v) = log\np(v|B)\nq(v) \u2212 1 +\n\u2211\nj Fv(yij , \u03b2).\nBecause q(v) is a probability density function, we use Lagrange multipliers to impose the constraint and obtain the optimal q(v) by solving\n\u03b4 ( L1(q) + \u03bb( \u222b q(v)dv \u2212 1) )\n\u03b4q(v) = 0,\n\u2202 ( L1(q) + \u03bb( \u222b q(v)dv \u2212 1) )\n\u2202\u03bb = 0.\nThough simple algebraic manipulations, we can obtain the optimal q(v) to be the following form\nq\u2217(v) = N (v|\u00b5,\u039b),\nwhere\n\u00b5 = \u03b2KBB(KBB+\u03b2KBSKSB) \u22121KBSy, \u039b = KBB(KBB+\u03b2KBSKSB) \u22121KBB.\nNow substituting q(v) in L1 with N (v|\u00b5,\u039b), we obtain the tight ELBO presented in Theorem 4.1 of the main paper:\nlog ( p(y,U|B) ) \u2265 L\u22171(U ,B) = 1\n2 log |KBB| \u2212\n1 2 log |KBB + \u03b2A1| \u2212 1 2 \u03b2a2 \u2212 1 2 \u03b2a3\n+ \u03b2\n2 tr(K\u22121BBA1)\u2212\n1\n2\nK \u2211\nk=1\n\u2016U(k)\u20162F + 1\n2 \u03b22a\u22a44 (KBB + \u03b2A1) \u22121a4\n+ N\n2 log(\n\u03b2\n2\u03c0 ), (11)\nwhere \u2016 \u00b7 \u2016F is Frobenius norm, and\nA1 = \u2211\nj k(B,xij )k(xij ,B), a2 =\n\u2211\nj y2ij ,\na3 = \u2211\nj k(xij ,xij ), a4 =\n\u2211\nj k(B,xij )yij ."}, {"heading": "1.2 Binary Tensor", "text": "Next, let us look at the binary data. The case for binary tensors is more complex, because we have the additional variational posterior q(z) = \u220f\nj q(zj). Furthermore, q(v) and q(z) are coupled in the original ELBO (see (10)). To eliminate q(v) and q(z), we use the following steps. We first fix q(z), calculate the optimal q(v) and plug it into L2 (this is similar to the continuous case) to obtain an intermediate bound,\nL\u03022(q(z),U ,B) = max q(v) L2(q(v), q(z),U ,B)\n= 1\n2 log |KBB| \u2212\n1 2 log |KBB +A1| \u2212 1 2 \u2211 j \u3008z2j \u3009 \u2212 1 2 a3 + 1 2 tr(K\u22121BBA1)\n\u2212 N\n2 log(2\u03c0) +\n1 2 (KBS\u3008z\u3009) \u22a4(KBB +A1) \u22121)(KBS\u3008z\u3009)\n+ \u2211\nj\n\u222b\nq(zj) log( p(yij |zj)\nq(zj) )dzj \u2212\n1\n2\n\u2211K\nk=1 \u2016U(k)\u20162F (12)\nwhere \u3008\u00b7\u3009 denotes the expectation under the variational posteriors. Note that L\u03022 has a similar form to L\u22171 in (11).\nNow we consider to calculate the optimal q(z) for L\u03022. To this end, we calculate the functional derivative of L\u03022 with respect to each q(zj):\n\u03b4L\u03022\n\u03b4q(zj) = log\np(yij |zj)\nq(zj) \u2212 1\u2212\n1 2 z2j + cjj\u3008zj\u3009zj + \u2211\nt6=j\nctj\u3008zt\u3009zj .\nwhere ctj = k(xit ,B)(KBB+A1) \u22121k(B,xij ) and p(yij |zj) = 1\n( (2yij \u22121)zj \u2265 0 ) .\nSolving \u03b4L\u03022 \u03b4q(zj) being 0 with Lagrange multipliers, we find that the optimal q(zj) is a truncated Gaussian,\nq\u2217(zj) \u221d N (zj |cjj\u3008zj\u3009+ \u2211\nt6=j\nctj\u3008zt\u3009, 1)1 ( (2yij \u2212 1)zj \u2265 0 ) .\nThis expression is unfortunately not analytical. Even if we can explicitly update each q(zj), the updating will depend on all the other variational posteriors {q(zt)}t6=j , making distributed calculation very difficult. This arises from the quadratic term 12 (KBS\u3008z\u3009) \u22a4 (KBB +A1) \u22121(KBS\u3008z\u3009) in (12), which couples all {\u3008zj\u3009}j .\nTo resolve this issue, we introduce an extra variational parameter \u03bb to decouple the dependencies between {\u3008zj\u3009}j using the following lemma.\nLemma 1.1. For any symmetric positive definite matrix E,\n\u03b7 \u22a4E\u22121\u03b7 \u2265 2\u03bb\u22a4\u03b7\u2212 \u03bb\u22a4E\u03bb. (13)\nThe equality is achieved when \u03bb = E\u22121\u03b7.\nProof. Define the function f(\u03b7) = \u03b7\u22a4E\u22121\u03b7 and it is easy to see that f(\u03b7) is convex because E\u22121 \u227b 0. Then using the convex conjugate, we have f(\u03b7) \u2265 \u03bb\u22a4\u03b7 \u2212 g(\u03bb) and g(\u03bb) \u2265 \u03b7\u22a4\u03bb \u2212 f(\u03b7). Then by maximizing \u03b7\u22a4\u03bb \u2212 f(\u03b7), we can obtain g(\u03bb) =\n1 4\u03bb \u22a4E\u03bb. Thus, f(\u03b7) \u2265 \u03bb\u22a4\u03b7\u2212 14\u03bb \u22a4E\u03bb. Since \u03bb is a free parameter, we can use 2\u03bb to replace \u03bb and obtain the inequality (13). Further, we can verify that when \u03bb = E\u22121\u03b7 the equality is achieved.\nWe now apply the inequality on the term 12 (KBS\u3008z\u3009) \u22a4(KBB +A1) \u22121KBS\u3008z\u3009 in (12). Note that the quadratic term regarding all {zj} now vanishes, and instead a linear term \u03bb\u22a4KBS\u3008z\u3009 is introduced so that these annoying dependencies between {zj}j are eliminated. We therefore obtain a more friendly intermediate ELBO,\nL\u03032(U ,B, q(z),\u03bb) = 1\n2 log |KBB| \u2212\n1 2 log |KBB +A1| \u2212 1 2 \u2211 j \u3008z2j \u3009 \u2212 1 2 a3\n+ 1\n2 tr(K\u22121BBA1)\u2212\nN\n2 log(2\u03c0) +\n\u2211\nj \u03bb \u22a4k(B,xij )\u3008zj\u3009 \u2212\n1 2 \u03bb \u22a4(KBB +A1)\u03bb\n+ \u2211\nj\n\u222b\nq(zj) log( p(yij |zj)\nq(zj) )dzj \u2212\n1\n2\nK \u2211\nk=1\n\u2016U(k)\u20162F . (14)\nThe functional derivative with respect to q(zj) is then given by\n\u03b4L\u03032\n\u03b4q(zj) = log\np(yij |zj)\nq(zj) \u2212 1\u2212\n1 2 z2j + \u03bb \u22a4k(B,xij )zj .\nNow solving \u03b4L\u03032 \u03b4q(zj) = 0, we see that the optimal variational posterior has an analytical form:\nq\u2217(zj) \u221d N (zj |\u03bb \u22a4k(B, xij ), 1)1\n( (2yij \u2212 1)zj \u2265 0 ) .\nPlugging each q\u2217(zj) into (14), we finally obtain the tight ELBO as presented in Theorem 4.2 of the main paper:\nlog ( p(y,U|B) ) \u2265 L\u22172(U ,B,\u03bb) = 1\n2 log |KBB| \u2212\n1 2 log |KBB +A1| \u2212 1 2 a3\n+ \u2211\nj\nlog ( \u03a6((2yij \u2212 1)\u03bb \u22a4k(B,xij )) )\n\u2212 1\n2 \u03bb\u22a4KBB\u03bb+\n1 2 tr(K\u22121BBA1)\n\u2212 1\n2\nK \u2211\nk=1\n\u2016U(k)\u20162F . (15)"}, {"heading": "2 Gradients of the Tight ELBO", "text": "In this section, we present how to calculate the gradients of the tight ELBOs in (11) and (15) with respect to the latent factors U , the inducing points B and the kernel parameters.\nLet us first consider the tight ELBO for continuous data. Because U , B and the kernel parameters are all inside the terms involving the kernel functions, such as KBB and A1, we calculate the gradients with respect to these terms first and then use the chain rule to calculate the gradients with respect to U and B and the kernel parameters.\nSpecifically, we consider the derivatives with respect to KBB , A1, a3 and a4. Using matrix derivatives and algebras [15], we obtain\ndL\u22171 = 1\n2 tr ( (K\u22121BB \u2212 (KBB + \u03b2A1) \u22121)dKBB )\n\u2212 \u03b2\n2 tr ( (KBB + \u03b2A1) \u22121dA1 )\n\u2212 \u03b2\n2 da3 \u2212\n\u03b2 2 tr(K\u22121BBA1K \u22121 BBdKBB) + \u03b2 2tr(a\u22a44 (KBB + \u03b2A1) \u22121da4)\n+ \u03b2\n2 tr(K\u22121BBdA1)\u2212\n1 2 \u03b22tr ( (KBB + \u03b2A1) \u22121a4a \u22a4 4 (KBB + \u03b2A1) \u22121dKBB )\n\u2212 1\n2 \u03b23tr\n(\n(KBB + \u03b2A1) \u22121a4a \u22a4 4 (KBB + \u03b2A1) \u22121dA1 ) . (16)\nNext, we calculate the derivatives dKBB , dA1, da3 and da4, which depend on the specific kernel function form used in the model. For example, if we use the linear kernel, dKBB = 2B\u22a4dB and dA1 = \u2211N j=1 k(B,xij )(xijdB \u22a4 +dxijB \u22a4)+ (dBx\u22a4ij + Bdx\u22a4ij )k(xij ,B) where xij = [u (1) ij1 , . . . ,u (K) ijK\n]. Note that because A1, a3 and a4 all have additive structures which involve individual tensor entry ij (1 \u2264 j \u2264 N ) and the major computation of the derivatives in (16) also involve similar summations, the computation of the final gradients with respect to U and B and the kernel parameters can easily be performed in parallel.\nThe gradient calculation for the tight ELBOs for binary tensors is very similar to the continuous case. Specifically, we obtain\ndL\u22172 = 1\n2 tr ( K\u22121BB \u2212 (KBB +A1) \u22121dKBB )\n\u2212 1\n2 tr ( (KBB +A1) \u22121dA1 )\n\u2212 1\n2 da3 \u2212\n1 2 tr(K\u22121BBA1K \u22121 BBdKBB) + 1 2 tr(K\u22121BBdA1)\u2212 1 2 tr(\u03bb\u03bb\u22a4dKBB)\n+\nN \u2211\nj=1\n(2yij \u2212 1) N ( \u03bb\u22a4k(B,xij )|0, 1 )\n\u03a6 ( (2yij \u2212 1)\u03bb \u22a4k(B,xij )\n)\u03bb \u22a4dk(B,xij ). (17)\nWe can then calculate the derivatives dKBB , dA1, da3 and each dk(B,xij )(1 \u2264 j \u2264 N) and then apply the chain rule to calculate the gradient with respect to U , B and the kernel parameters."}, {"heading": "3 Fixed Point Iteration for \u03bb", "text": "In this section, we give the convergence proof of the fixed point iteration of the variational parameters \u03bb in the tight ELBO for binary tensors. While \u03bb can be jointly optimized via gradient based approaches with U , B and the kernel parameters, we empirically find that combining this fixed point iteration can converge much faster. The fixed point iteration is given by\n\u03bb(t+1) = (KBB +A1) \u22121(A1\u03bb (t) + a5) (18)\nwhere\nA1 = \u2211\nj k(B,xij )k(xij ,B),\na5 = \u2211\nj\nk(B,xij )(2yij \u2212 1) N ( k(B,xij ) \u22a4\u03bb(t)|0, 1 )\n\u03a6 ( (2yij \u2212 1)k(B,xij ) \u22a4\u03bb(t)\n) .\nWe now show that the fixed point iteration not only always converges, but also improves the ELBO in (15) after every update of \u03bb (see Lemma 4.3 in the main paper).\nSpecifically, given U and B, from Section 1 we have\nL\u22172 ( \u03bb(t) ) = maxq(z) L\u03032 ( \u03bb(t), q(z) ) = L\u03032 ( \u03bb(t), q \u03bb(t) (z) )\nwhere q \u03bb(t) (z) is the optimal variational posterior: q \u03bb(t)\n(z) = \u220f\nj q\u03bb(t)(zj) and q\u03bb(t)(zj) \u221d\nN (zj |k(B,xij ) \u22a4\u03bb(t), 1)1\n( (2yij \u2212 1)zj \u2265 0 ) .\nNow let us fix q \u03bb(t) (z) and derive the optimal \u03bb by solving \u2202L\u03032 \u2202\u03bb = 0. We then\nobtain the update of \u03bb: \u03bb(t+1) = (KBB + A1)\u22121 ( \u2211 j k(B,xij )\u3008zj\u3009 ) where \u3008zj\u3009 is the expectation of the optimal variational posterior of zj given \u03bb (t), i.e., q \u03bb(t) (zj). Obviously, we have\nL\u03032 ( \u03bb(t), q \u03bb(t) (z) ) \u2264 L\u03032 ( \u03bb(t+1), q \u03bb(t) (z) ) .\nFurther, because L\u22172(\u03bb (t)) = L\u03032\n(\n\u03bb(t), q \u03bb(t) (z) ) and\nL\u03032 ( \u03bb (t+1), q \u03bb(t) (z) ) \u2264 L\u03032 ( \u03bb (t+1), q \u03bb(t+1) (z) ) = L\u22172(\u03bb (t+1))\nwe conclude that L\u22172(\u03bb (t)) \u2264 L\u22172(\u03bb (t+1)). Now, we plug the fact that \u3008zj\u3009 = w (t) j + k(B,xij ) \u22a4\u03bb(t) given q \u03bb(t) (zj) into the calculation of \u03bb\n(t+1), merge and arrange the terms. We then obtain the fixed point iteration for \u03bb in (18). Finally since L\u22172 is upper bounded by the log model evidence, the fixed point iteration always converges."}], "references": [{"title": "Scalable tensor factorizations for incomplete data", "author": ["E. Acar", "D.M. Dunlavy", "T.G. Kolda", "M. Morup"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Laser: A scalable response prediction platform for online advertising", "author": ["D. Agarwal", "B. Long", "J. Traupman", "D. Xin", "L. Zhang"], "venue": "In Proceedings of the 7th ACM international conference on Web search and data mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Pattern recognition and machine learning. springer", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Dfacto: Distributed factorization of tensors", "author": ["J.H. Choi", "S. Vishwanathan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Probabilistic models for incomplete multidimensional arrays. AISTATS", "author": ["W. Chu", "Z. Ghahramani"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Optimizing shuffle performance in spark. University of California, Berkeley-Department of Electrical Engineering and Computer", "author": ["A. Davidson", "A. Or"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Distributed variational inference in sparse gaussian process regression and latent variable models", "author": ["Y. Gal", "M. van der Wilk", "C. Rasmussen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Foundations of the PARAFAC procedure: Model and conditions for an\u201dexplanatory\u201dmulti-mode factor analysis", "author": ["R.A. Harshman"], "venue": "UCLA Working Papers in Phonetics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1970}, {"title": "Hierarchical multilinear models for multiway data", "author": ["P. Hoff"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Zero-truncated poisson tensor factorization for massive binary tensors", "author": ["C. Hu", "P. Rai", "L. Carin"], "venue": "In UAI", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Gigatensor: scaling tensor analysis up by 100 times-algorithms and discoveries", "author": ["U. Kang", "E. Papalexakis", "A. Harpale", "C. Faloutsos"], "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "A multilinear singular value decomposition", "author": ["L.D. Lathauwer", "B.D. Moor", "J. Vandewalle"], "venue": "SIAM J. Matrix Anal. Appl,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["N.D. Lawrence"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Random function priors for exchangeable arrays with applications to graphs and relational data", "author": ["J.R. Lloyd", "P. Orbanz", "Z. Ghahramani", "D.M. Roy"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Old and new matrix algebra useful for statistics", "author": ["T.P. Minka"], "venue": "See www. stat. cmu. edu/minka/papers/matrix. html", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "A unifying view of sparse approximate gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Scalable probabilistic tensor factorization for binary and count data", "author": ["P. Rai", "C. Hu", "M. Harding", "L. Carin"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Scalable Bayesian low-rank decomposition of incomplete multiway tensors", "author": ["P. Rai", "Y. Wang", "S. Guo", "G. Chen", "D. Dunson", "L. Carin"], "venue": "In Proceedings of the 31th International Conference on Machine Learning (ICML)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Non-negative tensor factorization with applications to statistics and computer vision", "author": ["A. Shashua", "T. Hazan"], "venue": "In Proceedings of the 22th International Conference on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Asynchronous distributed learning of topic models", "author": ["P. Smyth", "M. Welling", "A.U. Asuncion"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Provable sparse tensor decomposition. arXiv preprint arXiv:1502.01425", "author": ["W. Sun", "J. Lu", "H. Liu", "G. Cheng"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Modelling relational data using bayesian clustered tensor factorization. In Advances in neural information processing", "author": ["I. Sutskever", "J.B. Tenenbaum", "R.R. Salakhutdinov"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Variational learning of inducing variables in sparse gaussian processes", "author": ["M.K. Titsias"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Some mathematical notes on three-mode factor analysis", "author": ["L. Tucker"], "venue": "Psychometrika, 31,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1966}, {"title": "Infinite Tucker decomposition: Nonparametric Bayesian models for multiway data analysis", "author": ["Z. Xu", "F. Yan", "Y. Qi"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML)", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Bayesian conditional tensor factorizations for high-dimensional classification", "author": ["Y. Yang", "D. Dunson"], "venue": "Journal of the Royal Statistical Society B, revision submitted", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "In Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Dintucker: Scaling up gaussian process models on multidimensional arrays with billions of elements", "author": ["S. Zhe", "Y. Qi", "Y. Park", "I. Molloy", "S. Chari"], "venue": "arXiv preprint arXiv:1311.2663", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": "Classical tensor factorization models include Tucker [24] and CANDECOMP/PARAFAC (CP) [8] decompositions, which have been widely used in real-world applications.", "startOffset": 53, "endOffset": 57}, {"referenceID": 7, "context": "Classical tensor factorization models include Tucker [24] and CANDECOMP/PARAFAC (CP) [8] decompositions, which have been widely used in real-world applications.", "startOffset": 85, "endOffset": 88}, {"referenceID": 24, "context": "[25] proposed Infinite Tucker decomposition (InfTucker), which generalizes the Tucker model to infinite feature space using a Tensor-variate Gaussian process (TGP) and thus is powerful to model intricate nonlinear interactions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "However, InfTucker and its variants [28, 29] are computationally expensive, because the Kronecker product between the covariances of all the modes requires the TGP to model the entire tensor structure.", "startOffset": 36, "endOffset": 44}, {"referenceID": 27, "context": "cantly better than, or at least as good as two popular large-scale nonlinear factorization methods based on TGP: one uses hierarchical modeling to perform distributed infinite Tucker decomposition [28]; the other further enhances InfTucker by using Dirichlet process mixture prior over the latent factors and employs an online learning scheme [29].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Our method also outperforms GigaTensor [11], a typical large-scale CP factorization algorithm, by a large margin.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "For convenience, we will use the same notations in [25].", "startOffset": 51, "endOffset": 55}, {"referenceID": 24, "context": "The infinite Tucker decomposition (InfTucker) generalizes the Tucker model to infinite feature space via a tensor-variate Gaussian process (TGP) [25].", "startOffset": 145, "endOffset": 149}, {"referenceID": 24, "context": ",U}, Equation (1) actually defines a Gaussian process (GP) on tensors, namely tensor-variate GP (TGP) [25],", "startOffset": 102, "endOffset": 106}, {"referenceID": 27, "context": "[28, 29] proposed to improve the scalability by modeling subtensors instead, the sampled subtensors can still be very sparse.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "Therefore, we first derive a tractable variational evidence lower bound (ELBO), following the sparse Gaussian process framework by Titsias [23].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "Using a very similar derivation to [23], we can obtain a tractable", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "Efficient MAPREDUCE systems, such as SPARK [27], can fully optimize the nonshuffling MAP and REDUCE, where most of the data are buffered in memory and disk I/Os are circumvented to the utmost; by contrast, the performance with data shuffling degrades severely [6].", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "Efficient MAPREDUCE systems, such as SPARK [27], can fully optimize the nonshuffling MAP and REDUCE, where most of the data are buffered in memory and disk I/Os are circumvented to the utmost; by contrast, the performance with data shuffling degrades severely [6].", "startOffset": 260, "endOffset": 263}, {"referenceID": 24, "context": "Therefor it has a higher time and space complexity (see [25] for details) and is not scalable to large dimensions.", "startOffset": 56, "endOffset": 60}, {"referenceID": 23, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 80, "endOffset": 83}, {"referenceID": 18, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 4, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 21, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 0, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 8, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 25, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 17, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 20, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 9, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 16, "context": "5 Related work Classical tensor factorization models include Tucker [24] and CP [8], based on which many excellent works have been proposed [19, 5, 22, 1, 9, 26, 18, 21, 10, 17].", "startOffset": 140, "endOffset": 177}, {"referenceID": 10, "context": "To deal with big data, several distributed factorization algorithms have been recently developed, such as GigaTensor [11] and DFacTo [4].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "To deal with big data, several distributed factorization algorithms have been recently developed, such as GigaTensor [11] and DFacTo [4].", "startOffset": 133, "endOffset": 136}, {"referenceID": 24, "context": "Infinite Tucker decomposition [25], and its distributed or online extensions [28, 29] address this issue by modeling tensors or subtensors via a tensor-variate Gaussian process (TGP).", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "Infinite Tucker decomposition [25], and its distributed or online extensions [28, 29] address this issue by modeling tensors or subtensors via a tensor-variate Gaussian process (TGP).", "startOffset": 77, "endOffset": 85}, {"referenceID": 13, "context": "In theory, all such nonlinear factorization models belong to the random function prior models [14] for exchangeable multidimensional arrays.", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "Our distributed variational inference algorithm is based on sparse GP [16], an efficient approximation framework to scale up GP models.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "Recently, Titsias [23] proposed a variational learning framework for sparse GP, based on which Gal et al.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "[7] derived a tight variational lower bound for distributed inference of GP regression and GPLVM [13].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[7] derived a tight variational lower bound for distributed inference of GP regression and GPLVM [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "The derivation of the tight ELBO in our model for continuous tensors is similar to [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 18, "context": "We compared with CP, nonnegative CP (NN-CP) [19], high order SVD (HOSVD) [12], Tucker, infinite Tucker (InfTucker) Xu et al.", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "We compared with CP, nonnegative CP (NN-CP) [19], high order SVD (HOSVD) [12], Tucker, infinite Tucker (InfTucker) Xu et al.", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "[25] and its extension (InfTuckerEx) which uses the Dirichlet process mixture (DPM) prior to model latent clusters and local TGP to perform scalable, online factorization [29].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "(3) NELL: a binary tensor representing the knowledge predicates, in the form of (entity, entity, relationship) [28].", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "3 Evaluation on Large Tensor Data We then compared our approach with three state-of-the-art large-scale tensor factorization methods: GigaTensor [11], Distributed infinite Tucker decomposition (DinTucker) [28], and InfTuckerEx [29].", "startOffset": 145, "endOffset": 149}, {"referenceID": 27, "context": "3 Evaluation on Large Tensor Data We then compared our approach with three state-of-the-art large-scale tensor factorization methods: GigaTensor [11], Distributed infinite Tucker decomposition (DinTucker) [28], and InfTuckerEx [29].", "startOffset": 205, "endOffset": 209}, {"referenceID": 27, "context": "Following the settings in [29, 28], we randomly chose 80% of nonzero entries for training, and then sampled 50 test data sets from the remaining entries.", "startOffset": 26, "endOffset": 34}, {"referenceID": 1, "context": "Note that training CTR prediction models with comparable clicks and non-click samples is common in online advertising systems [2].", "startOffset": 126, "endOffset": 129}], "year": 2016, "abstractText": "Tensor factorization is a powerful tool to analyse multi-way data. Compared with traditional multi-linear methods, nonlinear tensor factorization models are capable of capturing more complex relationships in the data. However, they are computationally expensive and may suffer severe learning bias in case of extreme data sparsity. To overcome these limitations, in this paper we propose a distributed, flexible nonlinear tensor factorization model. Our model can effectively avoid the expensive computations and structural restrictions of the Kronecker-product in existing TGP formulations, allowing an arbitrary subset of tensorial entries to be selected to contribute to the training. At the same time, we derive a tractable and tight variational evidence lower bound (ELBO) that enables highly decoupled, parallel computations and high-quality inference. Based on the new bound, we develop a distributed inference algorithm in the MAPREDUCE framework, which is key-value-free and can fully exploit the memory cache mechanism in fast MAPREDUCE systems such as SPARK. Experimental results fully demonstrate the advantages of our method over several state-of-the-art approaches, in terms of both predictive performance and computational efficiency. Moreover, our approach shows a promising potential in the application of Click-Through-Rate (CTR) prediction for online advertising.", "creator": "LaTeX with hyperref package"}}}