{"id": "1605.01569", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "Classification of Human Whole-Body Motion using Hidden Markov Models", "abstract": "human motion plays actively an externally important role in many fields. large databases exist that also store and make available recordings of complex human motions. however, annotating as each motion result with widely multiple labels is virtually a far cumbersome and noisy error - proving prone process. this bachelor'thesis s thesis presents successively different validation approaches : to formally solve the larger multi - label classification problem potentially using hidden linear markov models ( hmms ). first, different features that can be directly calculated obtained outright from the raw internal data package are already introduced. adding next, optional additional features are derived to efficiently improve classification performance. typically these independent features instead are then used to perform the universal multi - racial label classification using two broadly different approaches. the first comparison approach simply radically transforms the typical multi - label problem into a generalized multi - class composition problem. the second, novel approach solves closely the same modelling problem without the need to construct a formal transformation by predicting the labels directly from the likelihood scores. the second approximation approach uniformly scales approximately linearly with the specific number of labels whereas the first second approach clearly is subject to any combinatorial explosion. nevertheless all primary aspects of creating the classification process are evaluated on implementing a coherent data set that all consists of examining 454 concurrent motions. only system 1 1 first achieves an accuracy of us 98. 02 % entirely and likewise system 10 2 an accuracy of 93. 25 39 % concentrated on controlling the statistical test set.", "histories": [["v1", "Thu, 5 May 2016 12:38:18 GMT  (7242kb,D)", "http://arxiv.org/abs/1605.01569v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["matthias plappert"], "accepted": false, "id": "1605.01569"}, "pdf": {"name": "1605.01569.pdf", "metadata": {"source": "META", "title": "Classification of Human Whole-Body Motion using Hidden Markov Models", "authors": ["Matthias Plappert"], "emails": [], "sections": [{"heading": null, "text": "KIT \u2014 University of the State of Baden-Wuerttemberg and National Research Center of the Helmholtz Association www.kit.edu\nKarlsruhe Institute of Technology\nClassification of Human Whole-Body\nMotion using Hidden Markov Models\nBachelor\u2019s Thesis of\nMatthias Plappert\nAt the Department of Informatics Institute for Anthropomatics and Robotics (IAR)\nHigh Perfomance Humanoid Technologies Lab (H2T)\nPrimary referee: Prof. Dr.-Ing. Tamim Asfour Secondary referee: Prof. Dr.-Ing. R\u00fcdiger Dillmann\nAdvisor: Dipl.-Inform. Christian Mandery\nDuration: May 1st, 2015 \u2013 August 31th, 2015\nar X iv :1 60 5. 01 56 9v 1 [ cs .L G ] 5 M ay 2 01 6\nErkl\u00e4rung:\nIch versichere hiermit, dass ich die Arbeit selbstst\u00e4ndig verfasst habe, keine anderen als die angegebenen Quellen und Hilfsmittel benutzt habe, die w\u00f6rtlich oder inhaltlich \u00fcbernommenen Stellen als solche kenntlich gemacht habe und die Satzung des Karlsruher Instituts f\u00fcr Technologie zur Sicherung guter wissenschaftlicher Praxis beachtet habe.\nKarlsruhe, den 31. August 2015\nMatthias Plappert\nKurzzusammenfassung:\nDiese Bachelorarbeit befasst sich mit der Klassifikation menschlicher Ganzk\u00f6rperbewegung mittels Hidden Markov Modellen (HMMs). Die menschlichen Bewegungen wurden mittels eines optisches MotionCapture-Systems mit passiven Markern aufgenommen. Im Laufe der Arbeit werden zun\u00e4chst Merkmale (features) erl\u00e4utert, die die so aufgenommenen menschlichen Ganzk\u00f6rperbewegungen beschreiben. Weiterhin wurden neue Merkmale aus den vorhandenen Rohdaten abgeleitet. Ein weiterer Schwerpunkt der Arbeit findet sich in der Diskussion von verschiedenen Ans\u00e4tzen zur L\u00f6sung des Multi-LabelKlassifikationsproblems, also der Zuweisung von mehreren Klassen auf eine Bewegung. Hierbei wurden zwei grunds\u00e4tzliche Ans\u00e4tze erarbeitet: Beim ersten Ansatz wird das Multi-Label-Problem zu einem Multi-Class-Problem transformiert, welches sich dann einfach l\u00f6sen l\u00e4sst. Der zweite Ansatz besch\u00e4ftigt sich mit M\u00f6glichkeiten, das Multi-Label-Problem ohne vorhergehende Transformation zu l\u00f6sen, indem die Bewertungen einer unbekannten Bewegung durch die HMMs zu einer Gesamtvorhersage zusammengesetzt werden. Alle Aspekte des Klassifikators wurden anschlie\u00dfend auf einem aus 454 Bewegungen bestehenden Datensatz evaluiert. Hierbei wurden verschiedene Parameter und Konfigurationen verglichen. Weiterhin fand ein Vergleich zwischen HMMs und Factorial Hidden Markov Modellen (FHMMs) statt. Die zwei zuvor genannten Ans\u00e4tze wurden in zwei verschiedenen Systemen realisiert und quantitativ miteinander verglichen. Hierbei erkannte das erste System 98,02% und das zweite System 93,39% der menschlichen Bewegungen auf dem Testdatensatz korrekt.\nContents"}, {"heading": "1 Introduction 1", "text": ""}, {"heading": "2 Related Work 3", "text": ""}, {"heading": "3 Basics 5", "text": "3.1 Motion Capture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.2 Master Motor Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.3 KIT Whole-Body Human Motion Database . . . . . . . . . . . . . . . . . . . . . . . . 7 3.4 Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3.5 Factorial Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"}, {"heading": "4 Features 13", "text": "4.1 Marker Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.2 Joint Angle Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.3 Derived Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.4 Smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.5 Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18"}, {"heading": "5 Classification 21", "text": "5.1 Motion Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n5.1.1 Emission Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5.1.2 Topologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5.1.3 Parameter Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 5.1.4 Training and Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.1.5 Extension to Factorial Hidden Markov Models . . . . . . . . . . . . . . . . . . 23\n5.2 Single-Label Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.3 Multi-Label Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n5.3.1 Power Set Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.3.2 Binary Relevance Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.3.3 Modified Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28"}, {"heading": "6 Evaluation 29", "text": "6.1 Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n6.1.1 dataset Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 6.1.2 hmm Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 6.1.3 misc Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n6.2 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 6.3 Feature Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.4 Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n6.4.1 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 6.4.2 Parameter Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 6.4.3 Factorial Hidden Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . 42\n6.5 Decision Makers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 6.5.1 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6.5.2 Support Vector Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6.5.3 Decision Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n6.5.4 Random Forest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.6 Classification Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n6.6.1 Power Set System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6.6.2 Multi-Label System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.6.3 Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n7 Conclusion 55\nPage 1"}, {"heading": "1 Introduction", "text": "Human whole-body motion plays an important role in many fields, including sports, medicine, entertainment, computer graphics and robotics. Motion capture technologies are readily available to relatively easily record vast amounts of data. Today, whole characters in movies and video games are created using computer-generated imagery (CGI) and the recorded movements of an actor. An impressive example is the completely computer-generated character Gollum from the Lord of the Rings movies. In sports and medicine, motion data can be used for gait analysis of humans. For example, stroke patients have been recorded using motion capture techniques to analyze their disease-induced walking disorders. The insights from this analysis can be used to better understand the symptoms of the patient and potentially allows the development of new rehabilitation therapies. Gait analysis is also used in professional sports to analyze the performance of athletes and to optimize their training. In humanoid robotics, human whole-body motion plays an important role in the construction and control of biped humanoid robots. The use of motion in the field of robotics will be discussed in-depth in the Related Works chapter.\nGiven the interest in human motion and the availability of the recording equipment, a growing number of motion data is recorded. A number of human whole-body motion databases exist today to make this data available to artists, physicians and researchers. Retrieval of information from these databases is often realized through so-called tags which are associated with a motion record. A motion can potentially have many tags that describe it. For example, a motion of a human playing tennis might be labeled with the tags tennis, forehand and right hand. These tags can be used to query the database for motion data of interest. However, annotating a motion with these tags is usually done by hand which is both an errorprone and slow process. The results are highly subjective since different annotators will use different tags to label the same data. This may be because different annotators have a different understanding of a tag or simply because they are not aware of the full set of available tags. Additionally, labeling every motion by hand becomes infeasible as more data is recorded.\nAutonomous motion recognition and classification can be used to automatically label new motions without human involvement. This approach solves the two main problems of labeling by hand. Firstly, the classification algorithm produces objective and reproducible results. Secondly, the system can be scaled to handle more and more data by increasing the available computational resources. The objective of this thesis is to develop a system that can be used to perform such autonomous classification of human whole-body motion.\nThis thesis is organized as follows: Chapter 2 provides a brief overview of the relevant literature and important authors in the field. Some fundamental concepts are introduced in chapter 3 which forms the basis for all following chapters. This includes an in-depth discussion of motion capture systems and ways to represent this recorded data. The KIT Whole-Body Human Motion Database is introduced since the system developed during this thesis will be used to classify motions stored in this database. Additionally, the foundations of Hidden Markov Models (HMMs) and Factorial Hidden Markov Models (FHMMs) are discussed since they play a key role for the devised classifier. Chapter 4 discusses possible features to discriminate motions. The discussion includes different representations of motion data, the extraction of additional features from the data and important preprocessing steps before the features can be used in a classifier. Chapter 5 is concerned with the problem of classification. In the chapter the previously discussed basics of Hidden Markov Models are extended so that they can be used for motion recognition. The remainder of the chapter is devoted to the use of multiple HMMs for the classification of human motion. The theoretical concepts discussed in the previous chapters are then evaluated in chapter 6. Besides a description of the used tools and dataset, the evaluation includes the selection of features, a comparison between different HMM configurations as well as the evaluation of different classification approaches. Finally, the best results from each area are used to perform an end-to-end evaluation of the entire system. Chapter 7 summarizes the work and describes possible improvements for future work.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 3"}, {"heading": "2 Related Work", "text": "In robotics, a promising idea is to use human motion as an intuitive way to instruct and program machines. This approach is commonly referred to as Programming by Demonstration (PbD) [DRE+00, BCDS08]. In PbD, a human instructor teaches a machine how to complete a given task by performing the necessary steps themselves. The machine then observes the instructor and attempts to also complete the task by imitating what it perceives. Human motion can also be used to gain a better understanding of how different parts of the human body work together to complete a task or goal. For example, observing how a human reacts to counter balance perturbations can potentially be used to transfer this knowledge to biped humanoid robots [MBJA15, BPP\u016014]. Since motion plays a key role in humanoid robotics most of the hereafter mentioned literature has a background in this area of research.\nDifferent approaches exist for representing motions. Ogata et al. [OST05] used Recurrent Neural Networks (RNNs) for interactive learning. In their work, RNNs were used in a cooperative navigation task with a humanoid robot and a human partner. Taylor et al. [THR06, TH09] proposed Conditional Restricted Boltzmann Machines (CRBMs) to learn from and then generate human whole-body motion. The proposed model is capable of generating continuous motion sequences (e.g. walking) and also allows to smoothly transition between them by adding higher-order layers to the model. Nonlinear Oscillators were used by Nakanishi et al. [NME+04] in a framework for learning biped locomotion. Breazeal et al. [BBG+05] represented motion as a path through a directed weighted graph where each node represents a pose. The edges of the graph define transitions between poses that are physically possible and safe. Calinon et al. [CGB07] used a Mixture Model of Gaussian and Bernoulli distributions (GMM/BMM) to encode motion data. Yamane et al. [YYN09] represented continuous motions in binary trees which can be used for motion recognition and generation. Lastly, Hidden Markov Models (HMMs) have been a popular choice to represent human whole-body motion. Since this work is concerned with Hidden Markov Models, the following paragraphs review some works in this domain in greater detail.\nTakano at al. [TYS+06] developed a system for recognizing and generating human motion for primitive nonverbal communication. Their approach uses a hierarchy of Hidden Markov Models for human whole-body motion recognition and motion generation. The lower layer represents motion primitives, also referred to as proto symbols, whereas the upper layer models the transitions between the motion primitives and therefore represents higher-level interactions. The lower layer of the system was trained on joint angle data recorded with an optical motion capture system. Multi-dimensional scaling was used to construct a multi-dimensional space of proto symbols (the proto symbol space) on which the Hidden Markov Model in the upper layer was trained. The authors evaluated their approach by recording a kickboxing match between two humans. One of the human subjects was then replaced with a humanoid robot. The model trained on the recorded data was used by the robot to generate and perform motions in response to the actions of its human counterpart.\nKulic\u0301 et al. [KTN07b, KTN07a, KTN08] proposed a system for learning, clustering and hierarchy formation of human whole-body motion in humanoid robots. The authors used Hidden Markov Models and Factorial Hidden Markov Models to represent motion as a sequence of motion primitives. Additionally, the system described by the authors is capable of on-line learning. This was achieved by two essential properties of the devised system: sequential training of FHMMs and incremental hierarchical formation of the motion primitives by clustering. The sequential training algorithm allowed Kulic\u0301 et al. to initially encode an observed motion into a simple Hidden Markov Model. As more and more data is observed, additional chains can be added and trained on-line, transforming the HMM into an FHMM. Secondly, newly observed motions are dynamically organized into an hierarchical tree structure, the motion symbol tree. This can be done efficiently by performing a tree search and placing the new motion into the node that is most similar. Local clustering is performed to split groups into new subgroups as new knowledge is added. As a result, specialized motions are placed at the leaves of the tree, whereas more generalized\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 4 Chapter 2. Related Work\nmotions can be found near the root. Both properties allow a humanoid robot to incrementally and efficiently build, organize and access knowledge during operation. The authors evaluated their work with a database of recorded human whole-body motion data. The data set contained 28 motions for walking, 15 cheering motions, 7 dancing motions, 19 kicking motions, 14 punching motions, 13 sumo leg raise motions, 13 squatting motions, 13 throwing motions, and 15 bowing motions. Each motion was represented in a humanoid model with 20 degrees of freedom. The results indicated that Factorial Hidden Markov Models outperform single-chain Hidden Markov Models in their discriminative and generative properties.\nThe work by Kulic\u0301 et al. was extended in [TIKN10] and [KOL+11]. The authors used the motion symbol tree to perform efficient classification. This was achieved by traversing the tree from the root and only recursively considering the subtree with the highest likelihood. As soon as a leave node is reached, the classification is complete. The approach greatly decreased the computational cost since fewer comparisons are required in order to classify an unknown motion. The authors further introduced the concept of a motion symbol graph. This directed graph allowed the authors to model likely transitions between motion symbols when observing continuous motion. The motion symbol graph was used to predict motion patterns and to generate motions in humanoid robots that consists of sequences of motion primitives.\nIn [THN15, TN15a, TN15b], Takano et al. proposed a system for mapping between motion symbols and word labels. Motions were encoded into HMMs and the distances between all models was calculated. Like in the earlier work of Takano, the distance measures was used to construct a multi-dimensional space, the motion symbol space. Multiple word labels associated with the motion primitive were encoded into a binary vector, which can be seen as a point in word label space. Finally, a linear mapping between the motion symbol space and the word label space was learned using Canonical Correlation Analysis (CCA). CCA attempts to find a mapping in such a way that the correlation of the positions of motion symbols and word labels is maximized. An advantage of this model is that it can be used to map from motion symbol to word label and vice versa. This means that the system is both capable of motion classification given an unknown motion and motion retrieval given a query of word labels. In the latter case, since motion symbol space and word label space are metric spaces, it is also possible to calculate the distance between a word label query and a motion symbol, making it easy to quantify the similarity.\nAn interesting extension to Hidden Markov Models was proposed in [WB99]. Wilson et al. used Parametric Hidden Markov Models (PHMMs) to recognize parameterized gestures. An example of such a gesture is the movement of the hands that accompanies the speech \u201cI saw a robot this big!\u201d. Here, this is a parameter of the gesture, namely the scalar size of the observed robot. The authors showed that traditional HMM-based recognition cannot adequately model this spatial variance. Furthermore, HMMs do not allow to estimate the parameter (e.g. the size of the robot) from an unknown gesture. The PHMM devised by the authors can solve both problems efficiently. It works by weighting a parameter vector and adding it to the mean of the emission distribution of each hidden state. A modified version of the Baum-Welch algorithm was used to estimate the weights of the parameter vector. Recognition with PHMMs is complicated by the fact that the parameter vector is unknown. This was solved by estimating the parameter vector (using an EM algorithm) for the observed sequence and each PHMM. The PHMM with the highest likelihood was then selected. Furthermore, the authors extended PHMMs to a non-linear mapping from parameter vector to the means of the emission distributions. In this case, gradient ascent techniques were used to estimate the necessary parameters.\nHerzog et al. [HUK08] used a variation of PHMMs to recognize and imitate motions in humanoid robots. Their approach differed from the model proposed by Wilson et al. The basic idea proposed by the authors is to use linear interpolation of HMMs that were trained on known parameters to generate a new HMM for new parameters. In their work the authors further discussed how a humanoid robot can generate motions from such a model. The authors evaluated their approach on pointing and reaching motions and were able to show that, in those cases, PHMMs outperform traditional HMMs in classification.\nKr\u00fcger et al. [KHB+10] used PHMMs for action recognition. Their work builds on the idea that an action can be represented by a sequence of action primitives. The authors proposed a system that used unsupervised segmentation to discover the action primitives. PHMMs were then used to encode and recognize them, as well as synthesis motions with a desired effect (e.g. grabbing an object).\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 5"}, {"heading": "3 Basics", "text": "Classification of human whole-body motion first and foremost requires motion data. This chapter therefore starts with a brief discussion of motion capture (section 3.1) and the Master Motor Map as a framework for representing motion (section 3.2). Section 3.3 gives an overview of the KIT Whole-Body Human Motion Database, which plays an important role in this work since it stores all motion data and also provides structures for labeling motions. Hidden Markov Models are introduced in section 3.4, which are used to learn and recognize motions in this work. Lastly, an extension of HMMs, Factorial Hidden Markov Models, are discussed (section 3.5)."}, {"heading": "3.1 Motion Capture", "text": "For recording motion data, the VICON MX motion capture system can be used. The system uses passive optical markers that can be attached to both humans and objects. Cameras that are positioned at multiple locations around the scene record the position of the markers within line of sight. To do so, each camera features a ring of LEDs that surrounds its lens. The LEDs emit light in the infrared spectrum, which is then reflected by the markers. Each camera records this reflected light and (depending on the mode of operation) reports the 2D coordinates of the markers. The final 3D coordinates for each marker are calculated by triangulation using the data from each camera [vic].\nThe motions are recorded by eight stationary and two portable VICON T10 cameras. Each camera records with a sampling rate of 100Hz. A total of 56 markers are placed onto the human as depicted in figure 3.1.\nAll recorded motion data is stored in the C3D format. The C3D format is a binary file format under public domain and is considered an industry standard. Besides storing marker coordinates in 3D space, it\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 6 Chapter 3. Basics\nalso allows to store information about the human (e.g. body size and weight), the experiment setup (e.g. marker positions) as well as additional data (e.g. data from additional sensors like force sensors) [c3d]."}, {"heading": "3.2 Master Motor Map", "text": "Master Motor Map (MMM) [TUM+14, AAD07] is a framework for representation, mapping and reproduction of human motions on humanoid robots. The fundamental goal of MMM is to map and unify different motions performed by different humans and recorded with different motion capture systems to the MMM reference model. Motions represented under this reference model can then be converted to different outputs, e.g. to map a human motion onto a humanoid robot like ARMAR-III [ARA+06]. The architecture of the MMM framework is depicted in figure 3.2. The framework includes different command-line and graphical user interface (GUI) tools and is open source1.\nAt the core of the framework is the MMM reference model. It consists of a model of the human body with a normalized height and weight, as well as kinematic and dynamic properties. These properties are based on the research conducted by Winter et al. [Win79, Win09] and Buchholz et al. [BAG92]. The kinematics of the MMM model consist of 104 degrees of freedom (DoF): 6 DoF cover the model pose, 23 DoF are assigned to each hand, and the remaining 52 DoF are distributed on arms, legs, head, eyes and body. The reference coordinate system in every joint is chosen in such a way that the x-axis points to the right of the model, the y-axis to the front and the z-axis upwards. If a joint has multiple DoF it is split into multiple joints with a single DoF each. The MMM reference model also specifies upper and lower limits for each joint. It is important to note that not all joints in the model must be used to represent motion. For example, the movement of individual fingers might not be of interest when recording a whole-body walking motion. In this case, the unspecified joints will simply remain in their initial positions [TUM+14].\nThe command-line tool MMMConverter can be used to convert data recorded with a motion capture system to the MMM reference model, i.e. reconstruct joint angles of the MMM model from motion data. This is accomplished by placing virtual markers onto the reference model and finding a mapping from the position of the physical markers (as recorded by the motion capture system) to virtual markers. The optimization problem can be solved efficiently by minimizing the distance between the position of\n1http://h2t.anthropomatik.kit.edu/752.php\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 3.3: KIT Whole-Body Human Motion Database Page 7\nphysical and virtual markers for each frame. Details on this mapping procedure are given in [TUM+14]. The converted motion is then stored in a XML-based file format. Such a XML file can contain multiple motions, which is useful for scenes where a human interacts with objects or scenes with multiple humans in them. Each motion is referenced by a unique name and consists of two parts: a preamble and the actual motion data. The preamble specifies a model (e.g. the MMM reference model or a model of an object) and can contain additional information about the human or object (e.g. body size and weight). The motion data is encoded in a list of frames. Each frame has a relative time step in seconds (the first frame starts at time step t = 0) and contains values for all properties that are specified by the model. For example, each frame of human motion under the MMM reference model contains the root position (x, y, z coordinates), the root rotation (roll, pitch, yaw angles) and a list of joint angles. Additionally, the velocity and acceleration information for each of the above properties as well as dynamic data (e.g. center of mass, angular momentum) can be stored for each frame [mmm].\nThe GUI tool MMMViewer can be used to visualize motions. The whole motion can be played back or each frame can be inspected individually. The camera can be moved freely to view the motion from different angles. Additionally, the joint angles of the currently visible frame are displayed. Figure 3.3 shows the tool during a visualization."}, {"heading": "3.3 KIT Whole-Body Human Motion Database", "text": "The KIT Whole-Body Human Motion Database2 contains motion data of both humans and objects that have been recorded using a marker-based approach as described in section 3.1. Each entry in the database has a unique ID, belongs to a project, and references the subjects and objects that participated in the recording. When recording motions, multiple trials are usually performed. For each trial, the raw motion data is stored in the C3D format (see section 3.1) and uploaded. Recorded data of additional sensors (e.g. force measurements for push recovery) as well as video footage can be uploaded as well. The database system automatically converts the C3D files to a subject-independent representation under the MMM model (see section 3.2) and, optionally, estimates dynamic properties like the center of mass. Log files grant insight into the conversion process. Furthermore, the database is capable of storing data related to subjects and objects. Size, weight, gender and other anthropometric measurements can be stored in\n2https://motion-database.humanoids.kit.edu\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 8 Chapter 3. Basics\nthe record for each subject. For objects, a 3D model of the object alongside a custom description can be saved [MTD+15].\nEach motion is classified within the Motion Description Tree. The tree consists of a hierarchical declaration of tags describing motion types (e.g. walk, kick, run) and additional tags for other properties like the direction of a movement (e.g. left, right, forward, backward). The tree is organized in such a way that the parent of a node has a broader semantic meaning than its children. For example, the tags clap and bow are both child nodes of the tag gesticulation. An excerpt of the Motion Description Tree is depicted in figure 3.4. An important property of this classification approach is that a motion can be associated with an arbitrary number of nodes of the Motion Description Tree. For example, a motion of a human that trips while walking to the left with high speed can be categorized using the following tags: (1) locomotion\u2192 bipedal\u2192 walk, (2) speed\u2192 fast, (3) direction\u2192 left, (4) perturbation\u2192 result\u2192 failing, and (5) perturbation\u2192 source\u2192 passive. The whole tree is managed by the KIT Whole-Body Human Motion Database and can be extended if necessary [MTD+15].\nThe database can be accessed through a web interface or an application programming interface (API). The web interface and the API are available publicly. For each motion the raw files as well as the processed files can be downloaded. For convenience, bulk download options are available. The web interface is also used to modify existing or upload new motions. These operations are restricted to registered accounts. The API allows direct access to the database. This allows the integration of the database into existing tools. The API is build on top of the Internet Communications Engine (Ice) [ice]. Ice is a remote procedure call (RPC) framework and allows for easy integration with a wide variety of platforms and programming languages [MTD+15].\nAt the time of this writing, the database contains 4457 motions performed by 49 different subjects. All motions in total have a length of approximately 9 hours and 20 minutes, with the average length of a recording being approximately 7.56 seconds."}, {"heading": "3.4 Hidden Markov Models", "text": "A Hidden Markov Model (HMM) [EAM08, Rab89] is a statistical model popular for learning sequential data. This is due to the fact that HMMs have the ability to have some degree of invariance to local warping (compression and stretching) of the time axis [B+06]. The methods discussed in this section are applicable to all forms of sequential data. However, since this work deals with temporal sequences, this section and all following chapters use notation and phrases that imply temporal sequences. Concretely, a tempo-\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 3.4: Hidden Markov Models Page 9\nral sequence of length T is denoted by o1,o2, . . . ,oT , where each ot is a multi-dimensional observation. The following discussion of HMMs and the underlaying concepts are based on Bishop et al. [B+06].\nTo understand Hidden Markov Models, it is helpful to first consider a simpler Markov model: the Markov chain. In a Markov chain (of first order), given a sequence o1, . . . ,oT , the conditional probability of an observation ot (that is the observation at time t) is assumed to be independent of all past (and, of course, future) observations except for observation ot\u22121. The conditional distribution in a such a model is given by\np(ot | o1, . . . ,ot\u22121) = p(ot | ot\u22121). (3.1)\nConsequently, the joint distribution is given by\np(o1, . . . ,oT ) = p(o1) T\n\u220f t=2 p(ot | ot\u22121). (3.2)\nA graphical representation of a first-order Markov chain is depicted in figure 3.5. The assumption that an observation is only dependent on its previous observation is rather strong. This can be easily seen by considering an example: If one attempts to predict the weather for the next hour by only considering the current weather situation instead of using the data of the last 24 hours, the prediction would be severely limited. The assumption can be relaxed by generalizing the Markov chain to be of M-th order. Here, each observation in a sequence is dependent on the past M observations. However, in such a model the number of parameters grows exponentially with M, so that this approach becomes impractical for large values of M.\nTo solve this problem, hidden (sometimes also referred to as latent) variables are introduced. Concretely, each observation variable ot is conditioned on the state of its hidden variable zt . The hidden variables form a first-order Markov chain. Such a model is known as a state space model, which is visualized in figure 3.6. The joint distribution for this model is given by\np(o1, . . . ,oT ,z1, . . . ,zT ) = p(z1) T\n\u220f t=2\np(zt | zt\u22121) T\n\u220f t=1 p(ot | zt). (3.3)\nAn important property of this model is that any pair of observed variables oi and o j are connected via the hidden variables. It can be shown that the predictive distribution p(ot+1 | o1, . . . ,ot) for observation ot+1 is dependent on all past observations o1, . . . ,ot . This model is therefore not constrained by the strong\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 10 Chapter 3. Basics\nindependence assumption of a Markov chain. If the hidden variables in a space state model as described above are discrete, the Hidden Markov Model is obtained.\nFormally, a Hidden Markov Model is defined by the set of parameters that govern the model [B+06, Rab89]:\n\u2022 K is the number of the individual hidden states. The appropriate number of states depends on the problem at hand. The current state at time t is denoted by qt and the set of possible states is S = {s1, . . . ,sK}. Notice the relationship of K and the hidden variables: Each of the K states describes a possible value for the hidden variables zt . One way to represent this is through the 1-of-K coding scheme, hence zt \u2208 {0,1}K and \u2016zt\u20161 = 1 for each t. For example, the notation qt = s2 is equivalent to zt = (0,1,0, . . . ,0).\n\u2022 A = (ai j) \u2208 [0,1]K\u00d7K is a matrix that consists of transition probabilities. Concretely, each entry defines the probability to transition to state s j given the current state si:\nai j = p(qt+1 = s j | qt = si). (3.4)\nSince A is a probability distribution, it must hold that \u2200 j : \u2211k ak j = 1. By setting ai j = 0, it is possible to \u201cdisable\u201d that specific transition from si to s j.\n\u2022 \u03c0 = (\u03c01, . . . ,\u03c0K) \u2208 [0,1]K is the initial state distribution where\n\u03c0i = p(q1 = si). (3.5)\nIt must hold that \u2016\u03c0\u20161 = 1.\n\u2022 \u03c6 describes the parameters of the conditional distributions of the observed variables:\np(ot | zt ,\u03c6). (3.6)\nThese probabilities are known as emission probabilities and can be given by different distributions. For example, if the observed values are discrete, a conditional probability table can be used. For observations with continuous values, a Gaussian distribution is a often a good choice. Other distributions are possible and picking an appropriate distribution depends on the observations.\nSince K is already encoded by the shape of A, an HMM is fully described by the following set of parameters: \u03b8 = {A,\u03c0 ,\u03c6}. In this work, an HMM with parameters \u03b8 is denoted by \u03bb\u03b8 .\nThree fundamental problems can be identified when working with HMMs: (1) The evaluation problem, (2) the decoding problem, and (3) the optimization problem. The problems and their description are all based on the work of Rabiner et al. [Rab89]. The first problem, the evaluation problem, is concerned with calculating the probability of a given sequence under a given model. Formally, given a sequence O = (o1, . . . ,oT ), how can p(O | \u03bb\u03b8 ) be calculated efficiently. This can also be viewed as scoring how well a model matches the given observations. The forward-backward algorithm [BE+67, BS+68] can be used to solve this problem (strictly speaking, only the forward pass is necessary to solve this first problem). The second problem, the decoding problem, is concerned with finding the state sequence of the hidden variables. Formally, given a sequence O = (o1, . . . ,oT ) and a model \u03bb\u03b8 , find a sequence Z = (z1, . . . ,zT ) of hidden states that is optimal. Different criteria of an optimal state sequence exist, e.g. choosing the states that are individually most likely. The most popular criterion is to find the single best state sequence. This is equivalent to maximizing p(Z |O,\u03bb\u03b8 ), which is solved efficiently by the Viterbi algorithm [Vit67]. The third problem, the optimization problem, is concerned with adjusting the parameters of the model. Formally, given a sequence O = (o1, . . . ,oT ), find parameters \u03b8 such that p(O | \u03bb\u03b8 ) is maximized. Solving this problem corresponds with learning the parameters, that is \u201ctraining\u201d the model. The Baum-Welch algorithm [BPSW70] solves this problem efficiently. However, since Baum-Welch is a specific case of the expectation maximization algorithm (EM algorithm), it does not necessarily find a global maximum.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 3.5: Factorial Hidden Markov Models Page 11\nOn a final note, another important property of HMMs is that they are generative models. This means that a model that has been trained on some data can be used to generate new samples. This is especially interesting for human motions and humanoid robots. Here, a motion can be learned by observation and later be reproduced in a robot by sampling from the model [TYS+06]."}, {"heading": "3.5 Factorial Hidden Markov Models", "text": "A sever limitation of HMMs is that they cannot represent a lot of information about the history of a time sequence. Factorial Hidden Markov Models (FHMMs) are a generalization of HMMs and offer a way to overcome this limitation. For example, representing 30 bit of information about the history requires 230 hidden states in a standard HMM whereas an FHMM can represent the same information with only 30 binary state variables [GJ97]. The discussion in this section is based on the work of Ghahramani et al. [GJ97].\nIn an FHMM, the current state is generalized by letting the state be represented by a collection of M state variables:\nzt = (z (1) t , . . . ,z(M)), (3.7)\nwhere each state variable z(m)t can take on K different values. Each state variable is constrained in such a way that it evolves according to its own dynamics and is therefore uncoupled from the other state variables:\np(zt | zt\u22121) = M\n\u220f m=1 p(z(m)t | z (m) t\u22121). (3.8)\nThis can be seen as M independent first-order Markov chains that all contribute to the observation. The distribution of the observed variable ot is conditional on the states of all hidden variables z (1) t , . . . ,z (M) t for each time step t. A simple way to represent this dependency for continuous observations is a multivariate Gaussian. Concretely, given that each z(m)t uses the 1-of-K coding scheme as described in section 3.4 and\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 12 Chapter 3. Basics\nthat each observation ot \u2208 RD, the conditional distribution is given by\np(ot | zt) = |\u03a3|\u2212 1 2 (2\u03c0)\u2212 D 2 exp ( \u22121\n2 (ot \u2212\u00b5 t)T \u03a3\u22121(ot \u2212\u00b5t)\n) , (3.9)\nwhere\n\u00b5 t = M\n\u2211 m=1\nW(m)z(m)t . (3.10)\nHere, each W(m) is a D\u00d7K matrix that weights the contributions to the mean for each z(m)t , \u03a3 is the D\u00d7D covariance matrix and |\u00b7| denotes the matrix determinant. Also note that the scalar \u03c0 in equation 3.9 is not to be confused with the initial probability vector \u03c0 from section 3.4. In words, at each time step, the state of all chains are weighted, summed and output through an expectation function (here equation 3.9) to produce the observation [KTN08].\nLike an HMM, a Factorial Hidden Markov Model is defined by the set of parameters that govern the model: \u03b8 = {A(1), . . . ,A(M),\u03c0 (1), . . . ,\u03c0 (M),\u03c6}. This is a simple extension of the parameters of a standard normal HMM: The transition probabilities A(1), . . . ,A(M) \u2208 RK\u00d7K and initial probabilities \u03c0 (1), . . . ,\u03c0 (M) \u2208RK must be given for each of the M Markov chains. \u03c6 still defines the necessary parameters for the emission probability distribution, which is \u03c6 = {W(1), . . . ,W(M),\u03a3} for a Gaussian FHMM as described above.\nA problem with FHMMs is learning their parameters. This is because although at each time step the hidden variables are marginally independent, they become conditionally dependent given the observation sequence. This can be easily seen by considering equation 3.9 and 3.10 that makes the mean and therefore the entire Gaussian a function of all states. As a result, exact inference becomes infeasible. Concretely, the backward-forward algorithm used in the E step of the Baum-Welch algorithm has time complexity O(T MKM+1), where T is the length of the sequence, K is the number of states and M is the number of Markov chains. Note however that the M step for FHMMs is completely tractable and can therefore be calculated exactly. To work around the infeasibility of inference, several approximations of the E step have been proposed: Ghahramani et al. devised inference using Gibbs sampling, completely factorized variational inference and structured variational inference. A fourth approach, the generalized backfitting algorithm, was described by Jacobs et al. [JJT02].\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 13"}, {"heading": "4 Features", "text": "As already mentioned in section 3.1, motions can be recorded using an optical marker-based motion capture system. The following section discusses different approaches to represent such motions and describes possible features that are used to recognize and classify them in later chapters."}, {"heading": "4.1 Marker Representation", "text": "A natural and obvious way to represent the recorded data is in 3-dimensional Cartesian space. For each time sample t, the system records the location of each marker n:\nr(n)t = (x (n) t ,y (n) t ,z (n) t ) \u2208 R3. (4.1)\nA complete motion or observation sequence O is then represented by the all marker locations for all sampled time steps. A way to write this is to \u201cunroll\u201d all marker locations for a given time step t into the t-th row of an observation matrix:\nOcartesian =  x(1)1 y (1) 1 z (1) 1 . . . x (N) 1 y (N) 1 z (N) 1 x(1)2 y (1) 2 z (1) 2 . . . x (N) 2 y (N) 2 z (N) 2 ... ... ... . . . ... ...\n... x(1)T y (1) T z (1) T . . . x (N) T y (N) T z (N) T  \u2208 RT\u00d73N , (4.2) where T is the number of time samples and N is the number of markers. A visualization of a motion and the respective marker locations is depicted in figure 4.1.\nA problem with this representation is that an absolute coordinate system is used. Consider for example two running motions. Assume that in the first case the subject moves towards a defined point and in the second case turns 45 degrees and repeats the motion almost identically. However, since an absolute coordinate system is used, the values in the observation matrix O will be very different for the two almost identical motions. The same problem occurs if the start location of two motions is offset. Again, similar motions will have different marker positions in the observation matrix. In short, the representation in an absolute Cartesian coordinate system is neither invariant to translation nor rotation. A coordinate system that is relative to the recorded subject is desirable to allow for robust motion recognition.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 14 Chapter 4. Features"}, {"heading": "4.2 Joint Angle Representation", "text": "The Master Motor Map (MMM, [TUM+14]) framework (see also section 3.2) uses a relative coordinate system. This is achieved by mapping the position of the physical markers onto virtual markers on a reference model. To do so, the squared error between the physical and virtual markers is minimized by varying the pose of the subject (as defined by its position and rotation in space as well as its joint angles) while maintaining the constraints of the reference model. The optimization problem is solved by the reimplementation of the Subplex algorithm as provided by the NLOpt library [MBJA15]. Figure 4.2 depicts the kinematics and shows the location and labels of all joints.\nSome joints have multiple degrees of freedom (DoF). Take, for example, the body lower neck (BLN) joint that has 3 DoF (to convince yourself that this is indeed the case, nod, shake your head and move your head from shoulder to shoulder). In robotics, this is usually handled by splitting a joint with multiple degrees of freedom into multiple joints with a single DoF each. In the case of the exemplary BLN joint, this means that the joints BLNx, BLNy and BLNz will replace it. After this step, each joint has a single DoF and can therefore be represented as a scalar value that defines the joint angle in radians. The complete joint configuration at time step t can therefore be written as\n\u03b8 t = (\u03b8 (1) t ,\u03b8 (2) t , . . . ,\u03b8 (N) t ) \u2208 RN , (4.3)\nwhere N is the number of joints with a single DoF each. Similar to a representation in Cartesian space, a\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 4.2: Joint Angle Representation Page 15\ncomplete observation sequence can then be written as:\nOmmm =  \u03b8 (1)1 \u03b8 (2) 1 . . . \u03b8 (N) 1 \u03b8 (1)2 \u03b8 (2) 2 . . . \u03b8 (N) 2 ... ... . . . ...\n\u03b8 (1)T \u03b8 (2) T . . . \u03b8 (N) T  \u2208 RT\u00d7N , (4.4) where T denotes the number of time samples.\nIt is important to stress again that the the joint angle representation is relative to the subject. However, some important information is lost: the position of the subject in space. To overcome this, the absolute root position and the root rotation at each time step t are included in the MMM framework as well:\nr(root)t = (xt ,yt ,zt) \u2208 R3, (4.5)\n\u03b8 (root)t = (\u03b8 (roll) t ,\u03b8 (pitch) t ,\u03b8 (yaw) t ) \u2208 R3. (4.6)\nHowever, these properties are once again given in an absolute coordinate system, suffering from the same problem described above. Luckily, this is resolved easily by an affine transformation of the coordinate system translating it such that the root position at t = 0 starts at the origin and rotating it such that the y axis points away from the front of the subject. The following two equations describe the necessary calculations for each time step t:\n\u2206r(root)t = r (root) t \u2212 r (root) 0 , (4.7)\nr\u0302(root)t = R\u22121 \u2206r (root) t . (4.8)\nThe first equation describes the translation, whereas the second equation describes the rotation. A rotation matrix for roll, pitch and yaw angles is given in [Cra05]:\nR = cos\u03b1 cos\u03b2 cos\u03b1 sin\u03b2 sin\u03b3\u2212 sin\u03b1 cos\u03b3 cos\u03b1 sin\u03b2 cos\u03b3 + sin\u03b1 sin\u03b3sin\u03b1 cos\u03b2 sin\u03b1 sin\u03b2 sin\u03b3 + cos\u03b1 cos\u03b3 sin\u03b1 sin\u03b2 cos\u03b3\u2212 cos\u03b1 sin\u03b3 \u2212sin\u03b2 cos\u03b2 sin\u03b3 cos\u03b2 cos\u03b3  , (4.9) where \u03b1 := \u03b8 (yaw)0 , \u03b2 := \u03b8 (pitch) 0 and \u03b3 := \u03b8 (roll) 0 .\nFigure 4.3 plots the root position of two motions where the subject runs forward. The same movements are depicted once before any normalization has been applied and once after normalization. Notice that without normalization, the features are neither translation nor rotation invariant. This can be especially well seen in figure 4.3(a): Although the subject is running only forward, the movement is split between the x and y components. This is because the subject moves at an approximately 45 degree angle between the x and y axis of the absolute coordinate system. After normalization (figure 4.3(c)), the movement direction happens only in the direction of the y axis of the transformed and now relative coordinate system. Hence rotation and translation invariant features are obtained after normalization.\nThe root rotation must be normalized as well to make it comparable. Since the rotation is given in angles, the normalization is straightforward:\n\u03b8\u0302 (root) t = \u03b8 (root) t \u2212\u03b8 (root) 0 . (4.10)\nNotice that this work assumes that the roll, pitch and yaw angles are not limited to the interval [\u2212\u03c0,\u03c0]. If necessary, this is easily achieved by correcting overflows by adding 2\u03c0 to all following angles (and similarly subtracting 2\u03c0 for underflows).\nAnother interesting usage of the MMM reference model is that it allows normalization of the marker positions. Recall that the marker positions were previously given in an absolute coordinate system (see section 4.1). However, since the initial pose of the subject is known in the MMM reference model, this\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 16 Chapter 4. Features\ninformation can be used to normalize the marker positions as well. This is done analogously to the normalization of the root position for the position of each marker (see equation 4.8)."}, {"heading": "4.3 Derived Features", "text": "Multiple additional features are computed under the MMM reference model. Firstly, an obvious extension is to calculate the velocities and accelerations of all features that describe positions in Cartesian coordinate space. Given that the velocity is the first derivative of the position and the acceleration is the first derivative of the velocity, both properties are easily calculated by approximating the respective derivatives:\nvt = rt+1\u2212 rt\u22121\n2\u2206t and at = vt+1\u2212vt\u22121 2\u2206t , (4.11)\nwhere v denotes the velocity, a the acceleration and \u2206t is the time difference between two subsequent samples (which is assumed to be equidistant over all samples). The velocity and acceleration must be normalized as well. The normalization works similarly to equation 4.8 but normalizes each sample with the current pose of the respective segment instead of normalizing each sample with the initial root pose. An interesting modification of the velocities and accelerations is to reduce them to simple scalar values by using their norm instead, e.g. the Euclidean one.\nSecondly, since the MMM reference model provides additional information about the subject, more advanced features are computed as well. Two interesting dynamic properties are the center of mass\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 4.3: Derived Features Page 17\n(CoM) and the angular momentum [PE04]. The CoM is conceptually similar to the root position discussed earlier in the sense that it describes the position of a subject in 3-dimensional Cartesian space. However, while the root position is always at a fixed point on the reference model, the center of mass is the barycenter of the subject. More concretely, the center of mass is the average over the CoM positions of body segments weighted by their respective masses. To clarify this, consider a motion where the subject performs a deep bow. A bowing motion is interesting in this case since the lower body remains mostly fixed in space while the upper body moves down. Figure 4.4 plots the root position and the CoM of such a motion. Notice that the z component of the root position remains approximately constant although the upper body moves down during the bow. The z component of the CoM on the other hand first decreases while the subject bows down and then increases again as the subject comes back up. Now consider the y component of the root position: It decreases as the subject bows down since the hip of the subject moves backwards. Compare this to the y component of the CoM that instead increases as the subject bows down since that shifts the center of mass forward. Like the root position, the CoM is given in an absolute coordinate system. The normalization to a coordinate system relative to the subject is done analogous to the normalization of the root position. The velocity and acceleration of the CoM are computed as well.\nThe angular momentum is a physical measure for the rotational configuration of an object or a system in 3D space. In the MMM framework, the the angular momentum is calculated with respect to the center of mass at each time step t in all three spatial directions. The whole-body angular momentum is calculated as follows:\nLt = M\n\u2211 i=1\n( m(i)(r(i)t \u00d7v (i) t )+ I (i) t \u03c9 (i) t ) \u2208 R3, (4.12)\nwith r(i)t = r (CoMi) t \u2212 r (CoM) t and v (i) t = r\u0307 (CoMi) t \u2212 r\u0307 (CoM) t . (4.13)\nThe first part of the sum considers the angular momenta created by the orbital rotation of each segment around the whole-body center of mass. For each segment i \u2208 {1, . . . ,M}, m(i) describes its mass, r(i)t its position at time step t w.r.t. the CoM and v(i)t its velocity at time step t w.r.t. the CoM. The cross product is denoted as \u00d7. The second part of the sum takes the spin of each segment into account by computing the product of its inertia tensor I(i)t and its angular velocity \u03c9 (i) t . The velocity v (i) t and the difference in CoM r(i)t must be normalized as previously described. Thirdly, the position of body segments are used as features as well. Consider for example a waving motion. In this case, the positions of the hands are an interesting feature. Similarly, the position of the feet are interesting for other motions, e.g. a kick. The positions of the extremities must be normalized. The velocities and accelerations are computed as previously described.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 18 Chapter 4. Features"}, {"heading": "4.4 Smoothing", "text": "The described features can be noisy or contain errors. For example, noise is introduced during the recording process. In addition, approximating the derivative for the computation of velocities and accelerations can amplify errors and inaccuracies in the recorded data. Smoothing is used to reduce the impact of these interferences. A signal can be smoothed using a wide variety of different filters. For a more complete discussion, see [Sim12].\nIn this work, only a simple filter is briefly discussed: the moving average or sliding window filter. In such a filter of length W , the mean of the surrounding W data points is used instead of the individual data point:\nx\u0302t = 1\nW +1\nW/2\n\u2211 j=\u2212W/2 xt+ j, (4.14)\nwhere xt denotes a (potentially multi-dimensional) data point at time step t and x\u0302 is the smoothed version thereof. Including future samples into the average avoids introducing a time delay in the signal. Notice that averaging over future samples is possible if the smoothing is applied off-line.\nFigure 4.5 compares the root acceleration of a subject before and after applying a moving average filter. Notice that the the original signal is very noisy. The smoothed signal maintains the overall structure of the signal but reduces the amount of jitter. Smoothing is a useful preprocessing step before feeding the features into a model."}, {"heading": "4.5 Scaling", "text": "Feature scaling is another preprocessing step. Take for example the joint angles and the root position from the previous sections. The joint angles are physically constrained to a very narrow value range, whereas the root position can potentially grow very large if the subject travels a large distance from the start position. It should be obvious from this example that features are on very different scales. This difference in scale becomes a problem if k-means clustering is used to initialize an HMM\u2019s emission distribution parameters. If the data is on very different scales across dimensions, k-means will not find clusters that properly fit the data because the same distance measure is minimized across dimensions. This, in turn, results in bad estimates of the emission distribution parameters which results in vanishing probabilities and numerical instabilities during inference. To counter this, feature scaling is performed. A very simple but effective strategy is to scale the features such that each feature\u2019s values are in the same range, e.g. [\u22121,1]. This is achieved by applying the following equation to each individual feature x over\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 4.5: Scaling Page 19\nall T time steps:\nx\u0302t = 2 \u00b7 xt \u2212 min i\u2208{1,...,T} xi\nmax i\u2208{1,...,T} xi\u2212 min i\u2208{1,...,T}\nxi \u22121. (4.15)\nNotice that each feature needs to be scaled over all samples, not per-sample. Furthermore, if feature scaling is used, the computation of the minimum and maximum are computed on the training data. New samples (e.g. from the test dataset when evaluating or unknown motions when used productively) are then simply scaled using the previously computed values. Otherwise the features that were used to train the model and the features that are used to recognize unknown motions would end up on a different scales.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 21"}, {"heading": "5 Classification", "text": "This chapter discusses methods to recognize and classify motions represented by features as described in the previous chapter. In the first section, the general concepts of Hidden Markov Models introduced in chapter 3.2 are concretely discussed for the case of motion recognition. In motion recognition, the goal is to encode a motion into a Hidden Markov Model and compute a measure that describes how likely an unknown motion is under the model. In the second section, motion recognition is extended to perform multi-class classification. In contrast to motion recognition, the goal is now to assign an unknown motion exactly one class out of a (potentially large) set of possible classes. Lastly, methods for performing multilabel classification are introduced. In contrast to the multi-class classification task, an unknown motion can have many labels that assign it to multiple classes.\nTo avoid confusion and to emphasize the distinction between multi-class (where one label assigns each motion one class) and multi-label (where multiple labels assign each motion multiple classes), multi-class classification is referred to as single-label classification hereinafter."}, {"heading": "5.1 Motion Recognition", "text": "Hidden Markov Models are a popular choice for encoding human whole-body motions [TYS+06, KTN07a, KTN08]. This section describes some properties of HMMs in depth and discusses properties and problems that are especially relevant when dealing with motions."}, {"heading": "5.1.1 Emission Distribution", "text": "Recall that the emission distribution models the observed data. Since in this case motions are observed, and all previously discussed features are continuous, the emission distribution must also be continuous. Typically, a Gaussian distribution or a mixture model thereof is used to model this case [KTN08]. Since this work uses a multivariate Gaussian distribution, the following discussion focuses on this distribution.\nA multivariate Gaussian or normal distribution is defined by two parameters, its mean vector \u00b5 \u2208 RD and its covariance matrix \u03a3 \u2208 RD\u00d7D, where D is the dimension of the feature vector. The probability density function (pdf) is then given by\nf (x) = |\u03a3|\u2212 1 2 (2\u03c0)\u2212 D 2 exp ( \u22121\n2 (x\u2212\u00b5 )T \u03a3\u22121(x\u2212\u00b5 )\n) . (5.1)\nIn an HMM, the emission of each state k is governed by its mean vector \u00b5 k and its covariance matrix \u03a3k. When dealing with motions, the covariance matrices are often constrained to be diagonal to avoid numerical problems [KTN07a, KTN07b]."}, {"heading": "5.1.2 Topologies", "text": "An important property of a Hidden Markov Models is that it uses hidden states. The transition probabilities between the K states are given by the transition matrix A \u2208 [0,1]K\u00d7K , while \u03c0 \u2208 [0,1]K defines the start probabilities for each state. By constraining the transition matrix (and as a result the start probabilities), different topologies can be realized. This is easily done by initializing the transition matrix and the start probabilities with some entries set to zero. During training, all probabilities that were initially set to zero will remain at zero [Rab89].\nIf the transition matrix is not constrained, a transition from any given state to every other state can occur. Such an HMM is usually referred to as fully connected or ergodic. Another popular topology\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 22 Chapter 5. Classification\nis the left-to-right topology or Bakis topology [Bak76]. In such a topology, the states are thought to be aligned sequentially from left to right. At each state, only a transition to a state that is right of the current state or a self-transition is allowed. The model can thus only be traversed from left to right, hence the name. In a left-to-right model, the start probabilities are set to \u03c0 = (1,0, . . . ,0) while the transition matrix takes the following shape:\nA =  a1,1 a1,2 . . . a1,(K\u22121) a1,K 0 a2,2 . . . a2,(K\u22121) a2,K ... ... . . . ... ...\n0 0 . . . a(K\u22121),(K\u22121) a(K\u22121),K 0 0 . . . 0 aK,K\n . (5.2)\nThe left-to-right constraint can thus be written as:\nai, j = 0, j < i. (5.3)\nTo avoid skipping too many states while traversing from left to right, an additional constraint is introduced:\nai, j = 0, j > i+\u2206. (5.4)\n\u2206 limits the maximum number of allowed skips [Rab89]. The left-to-right topology is frequently used for the recognition of motions [TYS+06, KTN07a]. Both, the ergodic and the left-to-right topology are visualized in figure 5.1. Note that, due to the variate of possible constraints for the transition matrix, other topologies are possible which are not discussed here."}, {"heading": "5.1.3 Parameter Initialization", "text": "An interesting problem that arises is how to initialize the values of the transition matrix and the start probabilities as well as the means and covariance matrices of the emission distributions. Since the Baum-Welch algorithm does not necessarily converge to a global maximum but rather to a local one (see chapter 3.2), a proper initial estimate is important to increase the chances of finding the global maximum during training. According to [Rab89], the start probabilities and transition matrix can either be initialized by a random (while maintaining the constraint that the respective probabilities must sum to one) or uniform estimation. Note that the constraints imposed by the topology choice must also be maintained.\nThe initialization of the mean vectors and covariance matrices of the emission distribution is more complicated. While a random initialization is possible, it is beneficial to perform an initial estimate of the underlaying distribution. A popular approach works as follows: The basic idea is to find K clusters that correspond to the K states of the Hidden Markov Model. The mean \u00b5 k and covariance matrix \u03a3k\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 5.1: Motion Recognition Page 23\nfor each state k are then estimated over the respective k-th cluster [Rab89]. The necessary clustering can be performed by hand. Alternatively an unsupervised clustering algorithm like the k-means algorithm can be used to automatically cluster the data. The k-means algorithm works by alternately assigning data samples to a cluster given the current parameters and then updating the cluster center means such that the distance from the previously associated samples is minimized. Lloyd\u2019s algorithm is often used to solve the clustering problem efficiently [KMN+02]."}, {"heading": "5.1.4 Training and Recognition", "text": "After deciding on the hyperparameters of the model (namely the number of states, the topology and the initialization method for the parameters of the model), the model can be trained. The training of an HMM is performed efficiently using the Baum-Welch algorithm [BPSW70]: For each training sequence O the parameters of the model are updated by first computing the expected likelihood given the current parameters (expectation step) and then updating the parameters such that the expected quantity from the expectation step is maximized (maximization step). When performed iteratively, the likelihood is maximized. The E and M steps are repeated until convergence or until a fixed number of iterations have been performed.\nA common problem when using the Baum-Welch algorithm are numerical instabilities. This is due to the fact that the probabilities during the forward or backward pass can become extremely small since they are multiplied at each time step. This becomes worse as the sequences grow longer. Since motions are usually at least a couple of hundred samples long, this becomes a very real problem when training HMMs on motions. The problem is mitigated by either using a scaling technique [Rab89] or by adapting the algorithm such that it uses the logarithm of the probabilities instead [Man06].\nAnother important property is that the training is unsupervised. This means that no target value like a label is necessary to learn the parameters of the HMM. However, if multiple HMMs are used to classify motions into classes, supervised learning becomes important. This will be discussed in the next section. Finally, HMMs can be trained off-line and on-line. In off-line training, the HMM is trained only once and the parameters are kept fixed even if previously unseen observation sequences become available. Most discussion in the literature assume off-line learning, e.g. [Rab89]. In on-line learning on the other hand, the HMMs are trained incrementally as new data becomes available. Kulic\u0301 et al. [KOL+11] describe such a system. This work only considers the off-line approach.\nAfter the model has been trained, recognition is performed by calculating the likelihood under the model \u03bb for an unknown observation sequence O:\np(O | \u03bb ). (5.5)\nThis is done efficiently by the forward algorithm [Rab89]. Since the forward algorithm is used during training as well, the same underflow issues as discussed earlier apply. Notice that the likelihood can become larger than 1 by definition, hence 0\u2264 p(O | \u03bb )< \u221e. Since the likelihood can become both very small (that is, very close to zero but not negative) and very large, the logarithmic likelihood (loglikelihood) is usually computed and presented. A strongly positive loglikelihood thus indicates a motion that has been strongly recognized by the model whereas a strongly negative loglikelihood indicates that the motion has not been recognized by the model at at. Finding such a decision boundary will be discussed later in this chapter."}, {"heading": "5.1.5 Extension to Factorial Hidden Markov Models", "text": "The previously discussed concepts apply equally to Factorial Hidden Markov Models. However, three additional considerations are of interest: the number of Markov chains as an additional hyperparameter, efficient training of the FHMM and computation of the likelihood under the model.\nFirstly, the number of chains is an important hyperparameter since it directly controls the complexity of time series that an FHMM can represent. However it also comes at the cost of increasing the computational complexity of both the training and the evaluation given an unknown observation sequence (see\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 24 Chapter 5. Classification\nchapter 3.5). In the literature, few chains are usually used. For example, Kulic\u0301 et al. [KTN08] use only two chains in their work. This does not seem like a lot at first. However, assume that an FHMM with 15 states and 2 chains is used. This results in 152 = 225 possible state combinations. Compared to an HMM with 15 states, even such a relatively small FHMM can already represent vastly more history than its HMM counterpart.\nExact training of FHMMs is computationally very expensive and can be, depending on the parameters, even infeasible. Four different approximations were already briefly discussed in chapter 3.5. However, Kulic\u0301 et al.[KTN08] proposed a fifth approach that the authors used to train FHMMs on motion data. The authors further showed that their approach is at least as good as and in most cases even better than the exact inference algorithm when using it to train FHMMs on motion data. This makes their approach especially interesting for this work. The algorithm devised by the authors works as follows: The FHMM is trained sequentially. This means that each FHMM starts with a single chain. Inference is then performed using the standard Baum-Welch algorithm on the training data. For the next chain m, the residual error between the previously trained chains and the n-th training sample O(n) \u2208 RT\u00d7D is computed for each time step t \u2208 {1, . . . ,T}:\ne(n)t = 1\nW\n( o(n)t \u2212 m\u22121\n\u2211 i=1\nWc(i)t ) \u2208 RD, (5.6)\nwhere e(n)t is the residual error between the frame at time step t of the n-th training sample and the summed contributions of the previous m\u22121 chains. Each chain\u2019s contribution is weighted by W = 1/M, where M is the number of all chains. Finally, the contribution for each chain m at time step t is computed as follows:\nc(m)t = K\n\u2211 k=1 \u00b5 (m)k \u03b3 (m) t,k \u2208 R D, (5.7)\nwhere K denotes the number of states, \u00b5 (m)k is the D-dimensional mean vector of the emission distribution of the already trained chain m in state k. Furthermore, \u03b3(m)t,k denotes the probability that state k in chain m is active at time step t. Algorithm 1 summarizes the described training procedure.\ninitialize first chain train first chain on time series O(n) = (o(n)t ) using the Baum-Welch algorithm for m = 2, . . . ,M do\ninitialize next chain m compute residual errors e(n)1 , . . . ,e (n) T\ntrain chain m on time series E(n) = (e(n)t ) using the Baum-Welch algorithm end\nAlgorithm 1: The sequential training algorithm for FHMMs in pseudo code [KTN08].\nA noteworthy and convenient property of the sequential training algorithm is that it uses procedures that are already used when training HMMs. More concretely, the training of each chain is done by an unmodified version of the well-known Baum-Welch algorithm. Computing \u03b3(m)t,k for the residual error is achieved just as easily by using the standard forward-backward algorithm.\nLastly, the likelihoods under the FHMM must be calculated as well. This is done using the exact algorithm given in [GJ97]. In the more concrete case of the sequential training algorithm, the necessary means and covariances are computed as follows:\n\u00b5 k1,...,kM =W M\n\u2211 m=1 \u00b5 (m)km and \u03a3k1,...,kM =W 2\nM\n\u2211 m=1 \u03a3(m)km . (5.8)\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 5.2: Single-Label Classification Page 25\nNotice that this must be done for each possible combination of states across all M chains, as indicated by the index k1, . . . ,kM . Take, for example, an FHMM with 3 states and 2 chains. This would result in 32 = 9 combinations, with the following mean vectors:\n\u00b5 1,1 =W ( \u00b5 (1)1 +\u00b5 (2) 1 ) , \u00b5 1,2 =W ( \u00b5 (1)1 +\u00b5 (2) 2 ) , \u00b5 1,3 =W ( \u00b5 (1)1 +\u00b5 (2) 3 ) , (5.9)\n\u00b5 2,1 =W ( \u00b5 (1)2 +\u00b5 (2) 1 ) , \u00b5 2,2 =W ( \u00b5 (1)2 +\u00b5 (2) 2 ) , \u00b5 2,3 =W ( \u00b5 (1)2 +\u00b5 (2) 3 ) , (5.10)\n\u00b5 3,1 =W ( \u00b5 (1)3 +\u00b5 (2) 1 ) , \u00b5 3,2 =W ( \u00b5 (1)3 +\u00b5 (2) 2 ) , \u00b5 3,3 =W ( \u00b5 (1)3 +\u00b5 (2) 3 ) . (5.11)\nThe 9 covariance matrices would be calculated analogously. The emission distribution for each state combination is then given by a multivariate Gaussian distribution with mean \u00b5 k1,...,kM and covariance \u03a3k1,...,kM [JJT02, KTN08]."}, {"heading": "5.2 Single-Label Classification", "text": "In the previous chapter motion recognition was discussed. This discussion is now extended to a classification problem. In a classification problem an unknown observation sequence, in this case a motion, must be assigned to a finite set of classes L . For example, assume that the set of classes is run, jump and kick. The goal is then to find the correct label y \u2208L = {run, jump,kick} for an unknown motion O by making a prediction p. The classification is correct if y = p. Figure 5.2 provides an overview of the classification process. This process is used throughout this work, not just for single-label classification.\nIf only a single label is assigned to a motion, the classification is straightforward. Instead of using words, y is encoded by natural numbers where each number corresponds to a class:\ny \u2208 {1, . . . ,M}, (5.12)\nwhere M is the number of classes that need to be recognized. The mapping between word and class\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 26 Chapter 5. Classification\ncan be chosen arbitrarily as long as it is bijective. In the above example, a possible mapping would be run\u2192 1, jump\u2192 2 and kick\u2192 3.\nAdditionally, assume that M HMMs have been trained per class: \u03bb1, . . . ,\u03bbM . The first HMM was only trained with motions of class 1, the second one only with motions of class 2 and so on. In contrast to the learning algorithm described in the previous chapter, the labels of the training data are now used to select the appropriate HMM during training. While the training of the HMM itself is still unsupervised, the selection happens in supervised fashion.\nAfter training, the prediction of an unknown motion O must be computed. To do so, the likelihoods of the motion under each individual HMM are evaluated. The last step is then performed by what this work refers to as a decision maker. The job of a decision maker is to find a mapping from the likelihoods of the HMMs to the prediction p. In case of the single-label classification problem, the prediction is done by a very simple decision maker that chooses the label that corresponds to the HMM with the maximum likelihood:\np = argmax m\u2208{1,...,M} p(O | \u03bbm). (5.13)"}, {"heading": "5.3 Multi-Label Classification", "text": "Let us now extend the classification problem to a problem where multiple labels can be assigned to a motion. This can be motivated by recalling the Motion Description Tree (see chapter 3.3): A motion is specified by the leave nodes of the tree, e.g. (1) locomotion \u2192 bipedal \u2192 walk, (2) speed \u2192 fast, (3) direction\u2192 left, (4) perturbation\u2192 result\u2192 failing, and (5) perturbation\u2192 source\u2192 passive. It should be obvious from this example that most motions cannot be adequately described by a single class. However, if a motion can have multiple labels, the simple classification described by equation 5.13 does not work anymore since it only computes the single label with the maximum likelihood.\nTo work around this problem, two different approaches can be identified: problem transformation using the power set method or the use of more advanced decision makers. In the latter case, the decision maker needs to truly handle multi-label classification. This can either be done by using the binary relevance method or algorithm modification [RPHF11]."}, {"heading": "5.3.1 Power Set Method", "text": "A first attempt to solve this problem is to simply treat each possible combination of labels as a single class. More formally, if L is the set of all labels, compute the power set as the substitute label set:\nL\u0302 = P(L ) = {U |U \u2286L }\\ /0. (5.14)\nIf each element in L\u0302 is then mapped to a natural number, the multi-label problem has been transformed into a single-label problem. This means that equation 5.13 can be used to predict the substitute label with the maximum likelihood. Since the substitute label represents multiple labels internally, reversing the transformation after the classification step solves the multi-label problem using the already discussed approach for single-label classification. In the literature, this idea is usually referred to as the power-set method [BLSB04, RPHF11].\nThe power set method, however, has several downsides. Firstly, the number of possible label combinations is 2M\u22121, where M is the number of classes. Recall that for each class a Hidden Markov Model must be trained. However, having sufficient training data now becomes a problem. If, for example, 200 walking motions exist but of those 200 motions only a single a has the labels walk and fast whereas all others have the labels walk and slow, the HMM for the fast walking motion would only be trained on a single training sample. This is inefficient since the HMM would benefit from the samples of the similar slow walking motion. Secondly, to perform classification, the likelihood under each model must be calculated. It can be easily seen that, for the worst case, this requires O(2M) computations of the loglikelihood for each unknown motion, which makes this approach infeasible for even moderately large M.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 5.3: Multi-Label Classification Page 27"}, {"heading": "5.3.2 Binary Relevance Method", "text": "Instead of treating all possible combinations of labels as a single label, an approach where a motion can truly have multiple labels is interesting. One way to achieve this is to encode the label as a binary vector:\ny \u2208 {0,1}M, (5.15)\nwhere ym is set to 1 if the m-th label is active and set to 0 otherwise. To give an explanatory example, assume that the classes walk, fast and slow exist. The label of a fast walking motion would then be encoded as y = (1,1,0) whereas motion where the subject walks slowly would would be encoded as y = (1,0,1).\nThe training then works as follows: M HMMs are initialized, one for each class. Given a training sequence O, use it to train all HMMs which correspond to the set classes in the sequence\u2019s label y. If, for example, y = (1,0,1), train \u03bb1 and \u03bb3 on the same sequence. An advantage over the previously described method is that only M instead of 2M \u2212 1 HMMs (worst case) must be kept in memory. Additionally, the utilization of the available training data is vastly improved. In the above example, the HMM that corresponds to the walk class now benefits from both, the slow and fast walks. In the previously described approach, both would only be considered in isolation. However, this can also become a problem if classes are too generic. Take for example motion of a throw performed with the left hand and a kick with the left foot. If both would be associated with the same class left, it is unlikely that common patterns can be learned properly. Instead, hand-left and foot-left can be used to distinguish the two, which is a more reasonable choice in this case.\nHowever, it is unclear how the decision maker can be realized in this case. The previous approach where the class with the maximum likelihood was selected cannot be used anymore. One way to perform classification in this case is to use some fixed value as the decision boundary. Let x\u0304m be such a decision boundary for the m-th label. The multi-label classification is then achieved by computing all likelihoods of the unknown motion O under each HMM and selecting all that are equal to or greater than the respective decision boundary:\npm = { 1, if p(O | \u03bbm)\u2265 x\u0304m 0, otherwise\nm = 1, . . . ,M, (5.16)\nwhere ym denotes the m-th element of the prediction p. This approach is promising, but unfortunately it all depends on a good choices of x\u0304m.\nInstead of trying to find good values for all x\u0304m by hand, it would be preferable if the decision boundaries could be determined automatically. Since the approach described in this work is supervised, the association between the likelihoods and the corresponding class is actually known for the training dataset. It seems like a good idea to use this knowledge to learn a decision boundary from the training data instead of finding it by hand. This has the advantage that the model easily adapts to the current dataset at hand without the need to fine-tune the decision boundary manually. The problem can indeed be seen as binary classification: For each class, the features for the binary classifier are the likelihoods which should be classified into two half-spaces that correspond to the labels 0 and 1 respectively. Since in this case, the feature space is 1-dimensional (just a scalar likelihood under one specific model), the hyperplane that separates the two half-spaces is a simple point and corresponds to the decision boundary x\u0304m described previously. This means that any binary classifier like Logistic Regression [Jor02] or Support Vector Machines [HDO+98] can be used to learn a mapping from the likelihoods to ym for each class.\nInstead of only considering the m-th likelihood, in a slightly more advanced version of this basic idea the binary classifier considers all likelihoods for its binary decision. This approach is commonly referred to as the binary relevance method [RPHF11]. More concretely, multi-label motion classification is realized with this method as follows:\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 28 Chapter 5. Classification\n1. Given the training data O(1), . . . ,O(N) and respective labels y(1), . . . ,y(M), calculate the likelihoods for each motion under each model:\nx(n) = (p(O(n) | \u03bb1), p(O(n) | \u03bb2), . . . , p(O(n) | \u03bbM)), n \u2208 {1, . . . ,N}. (5.17)\n2. Train M binary classifiers. For each classifier, the features are x(1), . . . ,x(N). However, the labels are varied across classifiers, with the m-th classifier using labels y(1)m , . . . ,y (N) m .\n3. Given an unknown motion O, calculate the likelihoods x under each of the M models analogously to equation 5.17. Afterwards, perform M predictions using the previously trained classifiers to obtain the complete multi-label prediction:\np = (h(1)(x), . . . ,h(M)(x)), (5.18)\nwhere h(m)(x) \u2208 {0,1} denotes a function that computes the prediction of the m-th classifier. In this case, the decision maker uses multiple binary classifiers internally to learn a mapping from the likelihoods of the HMMs to the multi-label prediction using the binary relevance method. It is important to stress again that the ability to use any binary classifier for multi-label predictions is a big advantage of this method since a wide and well-understood variety of such classifiers exist.\nHowever, this approach also has a downside: Each class is considered in isolation. This means that information is lost since the correlation between classes in the label vector y carries information as well. More concretely, it is likely that certain label combinations correspond to certain patterns in the likelihoods. Such patterns cannot be detected by the approach described above, since each classifier only \u201csees\u201d a single class [RPHF11]."}, {"heading": "5.3.3 Modified Algorithms", "text": "Some learning algorithms have been modified to support multi-label classification \u201cout of the box\u201d. In such a case, the multi-label classification is straightforward: Similarly to the previous approaches, the likelihoods under each model are calculated for each of the N training motions and the likelihood vectors x(1), . . . ,x(N) are obtained (compare equation 5.17). In contrast to the binary relevance method, the classifier can now be trained on the entire label vector y instead of training individual classifiers on the individual classes. This potentially allows the learning algorithm to find patterns between the likelihoods and the classes since the correlation between classes can now also be considered. In this case the decision maker is thus simply a classifier that is capable of learning multi-label classification.\nMultiple algorithms exist that have been adopted to the multi-label problem. Decision Trees have been extended to work in multi-label classification problems [VSS+08]. With this extension, Random Forests [Bre01] can also be used to perform multi-label classification since a Random Forest is an ensemble classifier that uses multiple Decision Trees internally. While this work focuses on the mentioned algorithms, it should be noted that other algorithms have been modified as well to support multi-label predictions, e.g. AdaBoost [SS00] or k-Nearest Neighbors [ZZ07].\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 29"}, {"heading": "6 Evaluation", "text": "The evaluation of the previously discussed concepts is challenging. This is due to the fact that a huge number of system parameters can be identified: A set of features must be selected, the hyperparameters of the Hidden Markov Models must be chosen, a comparison between FHMMs and HMMs is necessary and a variety of different classifiers that perform the mapping from likelihoods to final labels are available which also potentially have many hyperparameters that need fine-tuning. It should be clear that evaluating all those DoFs simultaneously is impossible due to the large number of combinations. Instead, the problem is split into smaller problems that are evaluated and optimized individually. Only a small subset of the best parameter choices is then used in the next problem, and so on.\nThe structure of this chapter is based on this idea: To get started, a brief overview of the tools used and developed for this work is given. The next section describes the used evaluation dataset. For the first evaluation step, feature selection is performed to find a \u201cgood\u201d subset of the wide variety of available features (see chapter 4). Next, Hidden Markov Models and their set of hyperparameters are evaluated (see chapter 5.1). The discussion is continued by a comparison between Hidden Markov Models and Factorial Hidden Markov Models (see chapter 5.1.5). Different approaches to predict the labels are evaluated (see chapter 5.3). Finally, an end-to-end evaluation of two classifier systems is conducted using the results from the previous sections."}, {"heading": "6.1 Tools", "text": "A toolkit was developed to help to evaluate this work. The toolkit is mostly written in Python with some modules using C and C++ either for speed or for interoperability. Additionally, the excellent scikit-learn framework [PVG+11] was used throughout the toolkit. The toolkit is used via multiple command-line scripts. This allows to easily automate long-running tasks and handles the wide variety of different actions and parameters efficiently. Areas of responsibility are split into 3 main modules which are each briefly covered in the remainder of this section.\n6.1.1 dataset Module\nThe dataset module provides classes and functions to load motion from different file formats (MMM and C3D files). Furthermore, the derived features described in chapter 4.3 can be computed as well. The functionality of the Simox1 and MMMTools2 libraries are used to aid in the computation of more advanced features like the angular momentum. SWIG3 is used to bridge between Python and the C++ API of the two mentioned libraries4,5. The module also provides functionality to normalize features to obtain rotation and translation invariant representations. Features can also be smoothed by applying a moving average filter and transformed to have similar scales across features. In short, the dataset module implements everything that was discussed in chapter 4.\nAnother important responsibility of the dataset module is the handling of datasets. A dataset is defined by a simple manifest. The manifest is realized as a JSON file that contains an array of folders. Each folder can be label with multiple classes to perform supervised learning. A dataset can be loaded from such a JSON manifest file. Multiple file formats (MMM and C3D) can be merged into a combined\n1http://simox.sourceforge.net 2https://gitlab.com/mastermotormap/mmmtools 3http://swig.org 4https://gitlab.com/cmandery/pySimox 5https://gitlab.com/cmandery/pyMMM\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 30 Chapter 6. Evaluation\ndataset. A dataset can be checked for inconsistencies (e.g. missing files), a report can be generated (e.g. the number of samples and the labels present in the dataset), selected features can be plotted to visualize the data and the entire dataset can be exported into a single file (different file formats are supported). Especially the export functionality is important since the computation of the derived features and the normalization is computationally expensive. All evaluation steps therefore load such an exported dataset instead of computing the dataset from scratch every time.\nLastly, the dataset module also provides functionality that helps with splitting the dataset into two separate (and disjunct) datasets: one for training, one for testing. This is done using stratified k-fold for multi-label data as proposed by Sechidis et al. [STV11]. The basic idea of a k-fold evaluation is to split the dataset into k disjunct subsets or folds. k rounds can then be performed by using k\u2212 1 folds for training and the remaining fold for testing. In a stratified k-fold, each fold attempts to have the same distribution of data across classes as the entire dataset.\n6.1.2 hmm Module\nThe hmm module implements everything necessary to train and evaluate Hidden Markov Models. The basic HMM functionality is provided by the hmmlearn library6. Other libraries were considered as well, namely pomegranate7 and GHMM8. However, hmmlearn proved to be the most reliable and fastest library.\nFor this evaluation, a fork9 of the hmmlearn library was created and used. It fixes some issues and, more importantly, implements functionality to use and train Factorial Hidden Markov Models. The implementation allows to compute exact likelihoods as described by Ghahramani et al. [GJ97]. Additionally, it supports the sequential training algorithm developed by Kulic\u0301 et al. [KTN08] (see chapter 5.1.5)\nThe module also provides functionality to combine the individual HMMs into an ensemble of HMMs. It allows training of the individual HMMs on labeled training data as well as computing the loglikelihoods of unknown motions under each model (see chapter 5.2 and 5.3). The training and evaluation of likelihoods is performed in parallel for each model to maximize performance.\nLastly, the hmm module provides utility functions to compute the initial transition matrix and start probabilities using different topologies (see chapter 5.1.2). Functions to estimate the initial means and covariance matrices using different strategies and constraints as described in chapter 5.1.3 are included as well.\n6.1.3 misc Module\nThe misc module is a collection of smaller components. The most important one are the decision makers. A decision maker is a classifier that performs the mapping from the likelihoods as calculated by the HMM ensemble to the binary label vector. As described in chapter 5.3, different approaches exist. The misc module implements a decision maker that always picks the maximum likelihood (chapter 5.2) and one that uses a fixed decision boundary (chapter 5.3.2). Additionally, decision makers that use Logistic Regression and Support Vector Machines are implemented using the binary relevance method (chapter 5.3.2). Decision Trees and Random Forests are used with adapted learning algorithms to handle the multi-label problem directly (chapter 5.3.3). The decision makers make especially heavy use of the scikit-learn framework."}, {"heading": "6.2 Dataset", "text": "Each motion in the dataset was recorded using the motion capture system described in chapter 3.1. The raw positions of the motion markers were stored in a C3D file. Each motion was recorded using the KIT\n6https://github.com/hmmlearn/hmmlearn 7https://github.com/jmschrei/pomegranate 8http://ghmm.org 9https://github.com/matthiasplappert/hmmlearn\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 6.2: Dataset Page 31\nreference marker set consistent of 56 markers placed at well-defined anatomical locations and a sampling rate of 100Hz. The C3D files are converted to the MMM reference model using 40 joints (each with a single DoF) and a non-linear optimization algorithm (see chapter 3.2). The joint angles as well as the root position and root rotation were stored in an XML file. The C3D and XML files are used to extract and compute all features described in chapter 4. For all features, the normalized or unnormalized form can be used. During normalization, the root rotation of the first frame was constrained such that the roll and pitch angles were set to zero. This is because a small error in either of the two angles can cause an improperly rotated coordinate system. The features can also be smoothed using a moving average filter with W = 3 (compare chapter 4.4). Table 6.1 lists all 29 available features without differentiating between normalized/unnormalized and smoothed/not smoothed.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 32 Chapter 6. Evaluation\nA total of 454 motions were selected from the motion database. 10 different human subjects performed the motions. Of those 10 subjects, 3 are female and 7 are male. Each motion was performed by a single human subject. If a motion involves an object (e.g. playing the guitar), the object was not physically present but instead imagined by the subject. The only exception to this is a set of upwards walking motions where actual stairs were used since it is quite hard to climb imaginary ones. Each recording in the dataset contains only a single motion. Additionally, each subject begins the motion in an upwardstanding pose similar to the zero-pose of the MMM model and finishes the motion in the same pose. For periodic motions such as walking, stirring or dancing, a fixed number of repetitions were selected across the dataset. The motions in the dataset were manually labeled with 49 different labels. Table 6.2 lists all labels and the number of samples in the dataset that are assigned to each label. Since motions can have multiple labels, table 6.3 lists all label combinations that are present in the evaluation dataset.\nClassification of Human Whole-Body Motion using Hidden Markov Models"}, {"heading": "6.3 Feature Selection", "text": "In feature selection, the goal is to find a subset of features that are relevant to the problem at hand. The selection process is a crucial first step. This is due to the fact that any classifier can only compute predictions from the features it receives. If those features do not contain the relevant features, the classifier obviously cannot produce optimal results. Additionally, a reduction of the feature dimensionality has several advantages: Firstly, the computational complexity is usually proportional to the dimensionality of the feature space. Secondly, if the feature space has very high dimensionality but the dataset is relatively small, overfitting becomes more likely. Lastly, reducing the feature set can also bring interesting insights to better understand a problem at hand [GE03].\nUnfortunately, the optimal feature set cannot be computed directly by evaluating all possible subsets. This is due to the fact that, for a feature set with N different features, 2N \u2212 1 non-empty subsets exist.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 34 Chapter 6. Evaluation\nAs previously described, a set of N = 29 different features can be identified in this work with results in 229\u2212 1 = 536870911 subsets. Evaluating each of these subsets is infeasible. In the literature, three popular heuristics can be identified to solve this problem: the filter method, the wrapper method and the embedded method. The filter method selects features by ranking them with correlation coefficients. The wrapper method evaluates the usefulness of a subset using a classifier. The embedded method works similarly to the wrapper method in the sense that it uses a predictor. However, while the wrapper methods relies on an external measure to assess the quality of the feature set, the embedded method relies on feature selection that is inherent to the learning algorithms. To give a concrete example, Support Vector Machines with L1 regularization can be used to perform feature selection using the embedded method. This is possible because L1 regularization yields sparse weights. The selection can then be done by simply picking all features that correspond to non-zero weights without the need of an externally defined measure [GE03].\nIn this work, feature selection is performed using the wrapper method with backwards elimination. In backwards elimination, one starts with the full feature set. Each feature is temporarily excluded from the set once and a model is trained on each of the subsets. For each subset, the model is evaluated on the test set and a measure is computed. The feature with the least effect on the measure is then removed from the feature set. This process is repeated until the feature set is empty (or until a stopping criterion is reached). The feature selection was performed on the dataset described in section 6.2. Of the 29 available features, the features marker_pos, marker_vel and marker_acc were initially excluded. This is due to the very high dimensionality of the three mentioned features (168 dimensions each) that resulted in numerical problems. The three excluded features will be revisited later. Additionally, to avoid too much complexity, all features under consideration were normalized, smoothed (using the previously described moving average with W = 3) and scaled to be in range [\u22121,1]. The reasoning behind this decision is that it seems very unlikely that unnormalized features perform well since they are, as previously discussed, neither invariant to translation nor rotation. Another concern was that the unnormalized features can cause overfitting. The smoothed features were selected as the default for similar reasons since it seems unlikely that an error-prone signal with strong jitter performs better than a slightly smoothed representation thereof. Scaling is important to ensure a good initialization when using k-means clustering. Since the dataset contains 49 different classes, 49 HMM models with 5 states each and the left-to-right topology with \u2206 = 1 were used. Each model was trained using the Baum-Welch algorithm for 10 iterations. The transition matrix and start probabilities were initialized uniformly whereas k-means clustering was used to initialize the means and covariance matrices. The covariance matrix was further constrained to be diagonal and the diagonal elements were constrained to be larger than 0.0001 to avoid numerical instabilities. The entire dataset was initially shuffled to break correlations between nearby training samples. A stratified 3-fold was used to train and evaluate the HMMs three times per feature subset. In each round, the likelihoods under each model were calculated for the test split. The likelihoods were additionally split into a set of positive and negative likelihoods. The positive set contains all likelihoods of the motions that should have been recognized by the respective model whereas the negative set contains the likelihoods of the motions that should have been rejected. Since 49 different HMMs are used this results in 98 sets of likelihoods: 49 sets of positive instances and 49 sets of negative instances. The mean and standard deviation were then calculated per set over all three rounds, resulting in 98 means of the likelihoods and their respective standard deviations.\nA possible way to measure the results in each round is to compute the distance between the distribution of positive and negative likelihoods on a per-class basis. The Wasserstein metric [GS+84, was] can be used to compute the distance between two Gaussian distributions:\nmi = \u221a |\u00b5\u0304posi \u2212 \u00b5\u0304negi |+ ( \u03c3\u03042posi + \u03c3\u0304 2 negi \u22122 \u221a \u03c3\u03042posi \u03c3\u0304 2 negi ) , (6.1)\nwhere i denotes the i-th class, \u00b5\u0304posi is the mean over all positive and \u00b5\u0304negi the mean over all negative likelihoods for the i-th class. \u03c3\u0304posi and \u03c3\u0304negi denote the respective standard deviations. It was further assumed that the positive and negative likelihood distributions are Gaussian. This assumption is not formally proven here but seems likely to hold given the central limit theorem.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 6.3: Feature Selection Page 35\nAfter computing m1, . . . ,m49, the individual distance measures must be combined into a single measure. Since all labels are equally important, the median is used here. This ensures that the distance between positive and negative distribution is balanced across all 49 classes. The total dimension of the features should be considered as well. If, for example, roughly the same distance has been computed for a feature set with 100 and 10 dimensions respectively, the feature set with only 10 dimensions should be preferred. A simple way to achieve this is to compute a measure per feature. Based on these considerations, the following combined measure is devised:\nm = 1 D m\u0303, (6.2)\nwhere D denotes current number of feature dimensions and m\u0303 is the median over individual Wasserstein metrics m1, . . . ,m49. Other measures were considered (e.g. AICc and the Mahalanobis metric) but proofed to not work as well as the Wasserstein-based metric.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 36 Chapter 6. Evaluation\nTable 6.4 lists the results per elimination round. The best feature was found in the last round: root_rot_norm. However, the individual likelihoods reveal that the single feature does not capture enough information to recognize all motion types. For example, the stir motion had a mean loglikelihood of \u221212046\u00b143080, which clearly indicates that this motion cannot be recognized using only the normalized norm of the root rotation. Some direction labels achieve similarly bad results. The label clockwise and counter-clockwise for example achieved a loglikelihood score of \u2212311\u00b1 67 and \u2212275\u00b1 103 respectively. However, the mean loglikelihoods for motions that should not be recognized by the models are 401\u00b1701 and 405\u00b1725 respectively. This feature set was therefore discarded as an outlier.\nThe second best result was achieved in round 22 with the following set of features: root_pos, root_vel, extremities_pos, root_rot and root_rot_norm. This is a more reasonable result. The velocity of the subject\u2019s root certainly is important to detect the direction the subject moves in and to decide if the motion is of stationary or dynamic nature. The position of the extremities proved to be an especially important feature since it survived all elimination rounds until the last one. This also makes sense: A lot of motions in the dataset involve the subject\u2019s hands or feet. Examples are stirring, waving, throwing a ball, playing the guitar or violin and kicks. The position of the extremities presumably also help with identifying which hand or foot was used to perform the motion. Including the root rotation and its norm makes sense as well: Since the root position and the root rotation are both included, it should be possible to adequately determine the subject\u2019s position and rotation in space from the feature set. This is obviously important information. An interesting observation is that the joint angles are not that important to recognize motions. The joint angles were already discarded in the fourth round. This is presumably due to the relatively low information contents per feature. While some joints might carry important information, this information is also redundantly present in other features like the position of the extremities. Since other feature sets achieved similar scores, they will be revisited in the last section during the end-to-end evaluation.\nAll things considered, the selected features seem like a good choice. This claim is backed by the data in table 6.6 (at the very end of this section) which lists the mean likelihoods and their standard deviation per label over the entire test dataset for HMMs that were trained using the previously mentioned feature set from elimination round 22. The likelihoods of positive samples are consistently high with a reasonable standard deviation. Additionally, the models only respond to the motions that they were trained to recognize. The only exception is the model that represents walking motions, which seems likely to produce false positives. This is presumably due to the fact that very different walking motions (e.g. walking and making a 90 degree turn vs. walking forward) were used to train the same model. The high standard deviation of the negative likelihoods can be explained by the fact that an extremely wide variety of very different motions were all combined into a single score. In other words: The positive likelihoods were only computed for motions of the same type whereas the negative likelihoods combines motion of all other types into a single score.\nRecall that three features were excluded from the previous feature selection process: marker_pos, marker_vel and marker_acc. This was necessary because of numerical problems when including these features in the initial feature set during backwards elimination. More concretely, the probabilities during\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 6.3: Feature Selection Page 37\ninference would become so small that an underflow occurred even though the HMM implementation used during the evaluation already uses logarithmic probabilities to counter the problem. However, these features are now revisited. To do so, the best feature set as previously determined was used as a baseline. Each of the three left out features was then added once. The achieved scores (using the same measure and training procedure as before) are listed in table 6.5.\nIt is quite obvious from the data that the marker positions, velocities and accelerations do not contribute anything to the feature set; instead the scores worsen significantly. Notice that the marker accelerations even cause numerical instabilities due to vanishing probabilities, hence a score cannot be computed in this case. The three temporarily excluded features are thus excluded permanently from further consideration.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 6.4: Hidden Markov Models Page 39"}, {"heading": "6.4 Hidden Markov Models", "text": "As discussed in chapter 5.1, Hidden Markov Models can be used to perform motion recognition. However, HMMs have multiple hyperparameters that need to be selected, namely the number of states and the topology. Additionally, different initialization strategies can be used. As mentioned in chapter 5.1.3, proper initialization is a crucial step before training. Moreover, a comparison between the performance of HMMs and FHMMs makes sense. FHMMs have another important hyperparameter, the number of chains.\nThroughout this section, the experimental setup from the previous feature selection section was used again. More concretely, the same dataset was used. To make the results comparable between this and the previous section, the same permutation was used to shuffle the dataset. 3-fold cross-validation was used with 49 HMMs, one for each class. Each HMM was trained for 10 iterations. The best feature set from the previous section was used throughout the following experiments: root_pos, root_vel, extremities_pos, root_rot and root_rot_norm. All features were normalized, smoothed and scaled. The same Wasserstein-based metric (equation 6.2) from the previous chapter was used to score the results. The default parameters of the HMMs were as follows: 5 states, left-to-right topology with \u2206 = 1, uniform initialization of the transition & start probabilities and k-means initialization of the Gaussian emission distribution parameters with the covariance matrices constrained to be diagonal. Notice that these are only defaults. Each experiment will vary either one or multiple of these parameters, which will be clearly stated in each subsection."}, {"heading": "6.4.1 Hyperparameters", "text": "The optimal number of states and topology are found using grid search. In grid search, each possible combination of hyperparameters is used to train a model on the training dataset which is then evaluated on the test dataset. A measure is computed per combination and the best combination is selected. Such a search is feasible in this case since only two different hyperparameters are evaluated.\nIn theory any natural number can be used for the number of states and countless different topologies are possible. In practice however, it makes sense to limit the number of states K to be between 5 and 20 states when recognizing human motions [KTN07a, KTN08]. In this evaluation K \u2208 {3, . . . ,20}, resulting in 18 different values for the number of states. The topologies (chapter 5.1.2) under consideration are the following:\n\u2022 fully-connected,\n\u2022 left-to-right without a \u2206 constraint,\n\u2022 left-to-right with \u2206 = 1, and\n\u2022 left-to-right with \u2206 = 2.\nThe left-to-right topologies proved to be especially well-suited for motion recognition [KTN08]. Different variations of this topology are evaluated by varying the \u2206 parameter. For completeness, the fullyconnected topology is considered as well.\nHence 18 different values for the number of states and 4 different topologies were under evaluation. This results in a total of 18 \u00b7 4 = 72 combinations, making the grid search easily feasible. The previously described experimental setup was used, with all parameters fixed and set to their default values except for the number of states and the topology. The results are depicted in figure 6.1.\nAs can be seen from the results, it is indeed desirable to have a low number of states. Good choices are between 5 and 8 states, depending on the topology. Another interesting observation is that the left-to-right topology with \u2206 = 1 is the only topology that performed consistently without large jumps. However, the best result was achieved by the left-to-right topology without a \u2206 constraint and with 8 states. The result support the claim that a low number of states is preferable for motion recognition and that the left-to-right topology is a good choice when dealing with motions. However, except for a few cases, the performance across topologies was similar.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 40 Chapter 6. Evaluation"}, {"heading": "6.4.2 Parameter Initialization", "text": "Different strategies can be used to initialize the parameters of a Hidden Markov Model before starting the training (see chapter 5.1.3). To quickly recap, the transition matrix and the start probabilities can either be initialized uniformly or by randomization. The implementation of a uniform initialization is trivial. The randomization can be easily achieved as well: The transition and start probabilities are initialized uniformly. Next, each element from the transition matrix is multiplied by a pseudo-random number (in this case the random number was drawn uniformly from the interval [0,1]). Notice that for each multiplication a new random number must be used. After randomization, the probabilities need to be normalized such that they sum to 1. This can be easily done by summing over each row of the transition matrix and dividing each element in that row by the the respective sum. The randomization of the start probabilities vector works analogously. This randomization automatically ensures that the desired topology is maintained.\nThe means and covariance matrices can either be randomized or estimated using the k-means clustering algorithm. The randomization of the mean vectors is straightforward: D pseudo-random numbers (in this case drawn uniformly from [\u22121,1] to match the feature scaling) are combined to form the mean vector (D denotes the number of features). This is repeated for each state to obtain K random mean vectors. The randomization of the covariance matrix is a bit more complicated since a covariance matrix is by definition symmetric and positive semi-definite. However, this can be easily achieved as well: First, initialize a randomized matrix R of the desired dimension (that is D\u00d7D). A symmetric and positive semi-definite matrix is then be obtained as follows [psd]:\n\u03a3 = RRT . (6.3)\nAgain, this is repeated K times to obtain one covariance matrix per state. In contrast, the initialization using k-means is done as follows: First K clusters are found in the training dataset using the k-means algorithm as implemented by scikit-learn (the implementation uses Lloyd\u2019s algorithm [KMN+02]). More concretely, all training observation sequences are \u201cstacked\u201d vertically. The clustering algorithm is then run on this stacked matrix. Also recall that K is the number of states, in this case K = 5. Next, the mean vectors of each state are simply set to their respective D-dimensional cluster centers. The computation of the covariance matrix is a bit more complicated: For each state, each row of the stacked observation sequences is assigned to a cluster. This is simply done by computing the Euclidean distance between each cluster center and the current sample and selecting the cluster with the\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 6.4: Hidden Markov Models Page 41\nsmallest distance. Next, a maximum likelihood covariance estimator is used to estimate the K covariance matrices. In this case, the EmpiricalCovariance estimator as implemented by scikit-learn was used.\nFinally, the covariance matrices can be constrained to be diagonal. This is done during initialization by simply setting every element to 0 except for the ones on the diagonal. During training, the maximization of the covariance is adopted to the diagonal case as described in [HAHR01]. The hmmlearn library implements this approach.\nTo summarize, the following different initializations can be identified: The transition and start probabilities can be randomized or initialized uniformly. The means and covariances of the emission distribution can be randomized as well or estimated using the k-means clustering algorithm. The covariance matrices can be unconstrained (\u201cfull\u201d) or constrained to be diagonal. This results in 2 \u00b72 \u00b72 = 8 different combinations. Similarly to the previous subsection, grid search was used to evaluate each possible combination. The previously described experimental setup was used again, with all parameters fixed and set to their default values except for the initialization strategies. The results are given in table 6.7.\nA first conclusion that can be drawn from the results is that a randomizing the emission distribution parameters is not a good initialization strategy. If the covariance matrices are unconstrained, the score cannot even be computed due to vanishing probabilities and resulting numerical underflows. If the covariance matrices are constrained to be diagonal, a score can be computed. Notice however that the achieved score is significantly worse than the scores achieved using k-means initialization. This nicely illustrates that Baum-Welch does not necessary converge to a global maximum. Having a good initial estimate is therefore indeed a crucial step, as previously claimed. Another interesting conclusion is that the initialization of the transition and start probabilities is less important. Take for example the results of the fourth and last row: The achieved scores and loglikelihoods are almost identical. Lastly, consider the difference between full and constrained covariance matrices for the emission distribution. While the scores for the full covariance matrices are higher, the mean loglikelihood scores plummet. Compared to the previously achieved results (table 6.6) using diagonally constrained matrices, the full covariance matrices deliver far worse results overall.\nIt therefore is clear that the best choice is to uniformly initialize the transition and start probabilities with k-means-based estimations for the means and covariances of the emission distribution. Additionally, the covariance matrices needs to be constrained to be diagonal.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 42 Chapter 6. Evaluation"}, {"heading": "6.4.3 Factorial Hidden Markov Models", "text": "Factorial Hidden Markov models are an important extension to HMMs when dealing with motions. However, this benefit comes at the cost of being computationally more expensive than regular HMMs. It therefore makes sense to not only consider the score that FHMMs achieve but to also consider the time it takes to train and evaluate them.\nDuring this evaluation, a total of 4 different models were evaluated:\n\u2022 regular HMM with 5 states,\n\u2022 FHMM with 5 states and 2 chains,\n\u2022 FHMM with 5 states and 3 chains, and\n\u2022 FHMM with 5 states and 4 chains\nThe FHMMs were trained using the sequential training algorithm (see chapter 5.1.5). Each chain was trained for 10 Baum-Welch iterations. The emission distribution parameters of the subsequent chains were initialized on the residual error using the already discussed k-means approach. All other parameters were set to the default values as defined above. The duration of each round was recorded as well since computational feasibility should also be a consideration. For reference, the training was performed on a machine with an 8-core Intel Core i7-4770 CPU clocked at 3.40GHz. The results are listed in table 6.8.\nThe results are somewhat surprising: The scores of the models decrease as more chains are added. However, the FHMM with 2 chains shows promise. The median loglikelihood has increased significantly while the standard deviation has decreased. This indicates that the model is capable of better fitting the data at hand. FHMMs with more than 2 chains, however, are not a good option. First, they do not seem to provide a significant benefit over an HMM or an FHMM with 2 chains. Additionally, the training and evaluation times increase significantly. A possible explanation for this result is that only very few states are required to discriminate motions (see section 6.4.1). The additional history information that can be encoded by FHMMs with more than two chains appears to be counter-productive in this case."}, {"heading": "6.5 Decision Makers", "text": "Recall that decision makers are used to find a mapping from the likelihoods of the (F)HMMs to the final predictions. Since decision makers are usually classifiers, their hyperparameters must be selected as well. This section therefore focuses on each individual decision maker whereas the next section focuses on covering the entire classification process as a whole.\nIn the following subsections, four different decision makers will be evaluated: Logistic Regression and Support Vector Machines are two binary classifiers that can be used with the binary relevance method (chapter 5.3.2). In contrast, Decision Trees and Random Forests can directly be trained on the multi-label\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 6.5: Decision Makers Page 43\nproblem and are therefore instances of models that use a modified learning algorithm (chapter 5.3.3). Notice that simpler decision makers like always picking the model with the maximum likelihood are not considered in this section since they are parameter-free. They will be, however, considered during the end-to-end evaluation that follows in the next section. Since all decision makers are evaluated similarly, this section starts by describing the general evaluation process. This includes the definition of new measure since now a classification problem is considered. The four decision makers are discussed during the remainder of this section.\nThe evaluation was performed on the usual dataset (section 6.2). A different permutation was used to shuffle the dataset. The best set of features (section 6.3) were then used to train 49 Factorial Hidden Markov Models: root_pos, root_vel, extremity_pos, root_rot and root_rot_norm. All features were normalized, smoothed and scaled as previously described. For each FHMM, the optimal configuration as described in section 6.4 was used: each FHMM used 8 states and 2 chains with the unconstrained leftto-right topology. The initialization for the transition and start probabilities was uniform, with the means and covariances of the emission distribution being initialized with the k-means method. The covariance matrices were constrained to be diagonal. Each FHMM was trained using the sequential training algorithm with each chain being trained for 100 Baum-Welch iterations. Stratified 3-fold was used to split the dataset into three training and test datasets. The loglikelihoods under each model were then computed for each of the test and train splits. This data forms the basis for the following evaluation: Each decision maker is trained on the train loglikelihoods and then evaluated on the respective test loglikelihoods. Since a 3-fold was used to split the dataset, the same split applies to the loglikelihoods; hence a total of 3 rounds can be performed. In each round, the decision maker is first trained on the train loglikelihoods. After training, the decision maker is evaluated on the corresponding test loglikelihoods. The corresponding labels of the dataset are available as well. Notice that the FHMMs were only trained and evaluated once for every label. This re-using of loglikelihoods significantly speeds up the evaluation of the decision makers.\nThe evaluation of the decision makers can thus simply be seen as a 3-fold evaluation of a supervised classifier where the loglikelihoods are the features and the labels of the motions can be re-used as the targets. It therefore makes sense to use metrics that are commonly used for classification problems to measure the results. To define measures, it is useful to first define the following quantities:\n\u2022 The number of true positives is the number of samples that were correctly classified as positive. To give an example, if the label of a samples is y = 1 and the classifier predicts p = 1, the sample is counted as true positive. The number of true positives is denoted as TP.\n\u2022 In contrast to a true positive, a sample is counted as a true negative if it is correctly classified as negative. This would be the case for a sample with label y = 0 and prediction p = 0. The number of true negatives is denoted as TN.\n\u2022 If a sample is wrongly classified as positive but is actually labeled as negative, a false positive occurs. To give an example, a sample with y = 0 and p = 1 would be counted as such. The number of false positives is denoted as FP.\n\u2022 Lastly, if a sample is wrongly classified as negative but is actually positive, a false negative occurs. An example for this case would be a sample with y = 1 and p = 0. The number of false negatives is denoted as FN.\nThese four quantities cover every possible outcome for binary classification (which is applicable here as well since each label is encoded as a binary vector). Using these quantities, a couple of measures can be defined.\nFirstly, the accuracy is an extremely common measure. It is defined as:\naccuracy = TP+TN\nTP+FP+TN+FN . (6.4)\nThe accuracy simply measures the percentage of correctly classified samples. However, while being intuitive, the accuracy also has a severe flaw. To illustrate this, consider the following example: Assume\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 44 Chapter 6. Evaluation\nthat a dataset with 100 samples is classified. Of the 100 samples, only 1 have a positive and the remaining 99 have a negative label. Now consider a classifier that always predicts p = 0 regardless of the sample\u2019s features. In this case, the classifier would achieve an accuracy of 99100 = 0.99. It should be obvious that, in this case, the accuracy is not a good metric to measure to quality of the classifier.\nTo overcome the shortcomings of the accuracy, the precision and recall are popular metrics:\nprecision = TP\nTP+FP and recall = TP TP+FN . (6.5)\nPrecision and recall are often combined into a single metric called the F1 score:\nF1 = 2 precision \u00b7 recall\nprecision+ recall . (6.6)\nLike the accuracy, precision, recall and F1 score are in the interval [0,1]. Generally speaking, a value close to 1 for all three scores is desirable [vR79]. In the remainder of this section, the F1 score is used to assess the decision makers. Since the F1 score, precision and recall are computed per class, a possible way to combine the individual scores is to take the mean over all classes per metric."}, {"heading": "6.5.1 Logistic Regression", "text": "Logistic Regression is a very simple but popular binary classifier. It essentially combines linear regression and the logistic function to perform binary classification. The reader is referred to other works (e.g. [B+06]) for a full discussion.\nIn this work, the LogisticRegression implementation found in scikit-learn with the liblinear solver [FCH+08] is used. Multi-label classification is achieved using the binary relevance method. Since Logistic Regression is a rather simple model, the only hyperparameters considered in this work are those that control regularization. More concretely, Logistic Regression can be used with L1 or L2 regularization. Regularization is essentially an additional term in the cost function that penalizes large weights to avoid overfitting. The difference between L1 and L2 regularization lies in the way the penalty term is calculated: For L1 regularization, this happens using the || \u00b7 ||1 norm whereas L2 regularization uses || \u00b7 ||2 instead. A coefficient C controls the \u201cstrength\u201d of the regularization. In case of the scikit-learn implementation, a small (that is close to zero) coefficient corresponds to strong regularization whereas a larger coefficient relaxes it. To summarize, the following hyperparameters and values are considered:\n\u2022 L1 vs. L2 regularization\n\u2022 C \u2208 {10\u22125,10\u22124, . . . ,104,105}\nThis results in a total of 2 \u00b711 = 22 combinations, which were evaluated using grid search. Each parameter combination was evaluated using a 3-fold as described above. The F1 score was computed per class and then combined by taking the mean over all classes. Figure 6.2 depicts the results.\nAs can be seen from the diagram, L1 regularization yields better results in this case. More concretely, the best result with Logistic Regression is achieved with said regularization and C = 10\u22123. This makes sense since L1 regularization allows that many weights can become zero making it more suitable for sparse signals. This is the case for this dataset since only a few labels will be set to 1. Overall, Logistic Regression works very well, indicating that the problem is linearly separable."}, {"heading": "6.5.2 Support Vector Machine", "text": "Support Vector Machines [HDO+98] are another very popular binary classifier. Only the very basics of Support Vector Machines are discussed here. Like Logistic Regression, SVMs try to fit a hyperplane that separates the positive and negative samples in the feature space. The hyperplane is fitted such that the distance between it and the samples that are close to it is maximized (large margin classifier). Additionally, only the feature vectors of close-by samples (so-called support vectors) must be considered\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 6.5: Decision Makers Page 45\nwhen fitting the hyperplane which makes SVMs especially efficient on large datasets (since the majority of the samples can be ignored). Lastly, SVMs use what is commonly referred to as the \u201ckernel trick\u201d to fit problems that are not linearly separable. The fundamental idea here is to transform the features to a very high-dimensional space. If the dimension is high enough, the samples will eventually be linearly separable. However, such a transformation is computationally infeasible. The kernel trick uses kernel functions which enables the classifier to operate in high-dimensional space without the need to actually compute the coordinates [B+06].\nIn this work, the linear SVM implementation found in scikit-learn is used: LinearSVC. Under the hood, LinearSVC uses the already mentioned liblinear solver. Notice that LinearSVC only supports a linear kernel. In this case, however, this is very much sufficient since Logistic Regression already achieves very good results, indicating that the data is linearly separable. The binary classifier is applied to the multi-label problem using the binary relevance method. The following hyperparameters are under consideration:\n\u2022 L1 vs. L2 regularization\n\u2022 C \u2208 {10\u22125,10\u22124, . . . ,104,105}\nNotice the similarity to the hyperparameter of Logistic Regression. Furthermore, the squared hinge loss was used instead of the more common hinge loss. This is necessary because the hinge loss cannot be used with L1 regularization in the scikit-learn implementation. The results of the grid search are depicted in figure 6.3.\nThe results for the Support Vector Machine are very similar to the results achieved with Logistic Regression, although Logistic Regression performed slightly better. The best hyperparameter combination as measured by the F1 score is achieved for L1 regularization with C = 10\u22122. As already discussed during the evaluation of Logistic Regression, the L1 regularization allows sparse weights making it a good choice for the dataset at hand.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 46 Chapter 6. Evaluation"}, {"heading": "6.5.3 Decision Tree", "text": "Decision Trees can be used to perform classification. In this case, the leaves of the tree represent the classes and the inner nodes correspond to an input variable. The branches between nodes represent decisions that are based on the input variables. A Decision Tree can then be traversed from the root while a decision needs to be made at each inner node (e.g. if input variable x is greater than some threshold follow the left branch; follow the right branch otherwise) until a leaf node is reached. The learning of such a model is performed by splitting the set of training samples at each node. The relevant attribute and the threshold to make this decision are selected using a criterion, e.g. the maximum information gain. This process is then repeated recursively for each child node until either all samples belong to the same class (in which case the node turns into a leaf) or until some other stopping criterion is reached (e.g. a set maximum depth) [B+06]. Decision Trees can be extended to support multi-label classification, which makes them an instance of a modified algorithm [VSS+08].\nIn this case, the scikit-learn implementation is used: DecisionTreeClassifier. The implementation uses the CART algorithm [BFSO84] to perform learning, which has been modified to also support multi-label problems. The following important hyperparameters can be identified:\n\u2022 The criterion that measures the quality of a split. The implementation supports the Gini impurity and the information gain.\n\u2022 The maximum depth that the Decision Tree can reach. In this case, the following values were considered: {1,2, . . . ,40}.\nThe 2 \u00b7 40 = 80 possible combinations were explored using grid search. The evaluation was performed as previously described and the averaged F1 score was used as a measure. Figure 6.4 depicts the results.\nAs can be seen from the results, the Decision Tree does not need to be very deep to work properly. Trees that use the information gain splitting criterion required less depth than trees where the Gini impurity was used. However, both criteria eventually converge to approximately the same score of F1 = 0.8.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 6.5: Decision Makers Page 47\nSince the information gain criterion reaches this convergence faster, it should be preferred. In either case, a tree with a maximum depth of 15 is sufficient to consistently learn the problem at hand. However, the Decision Tree classifier is outperformed by Logistic Regression and SVMs on this dataset."}, {"heading": "6.5.4 Random Forest", "text": "The last classifier that is discussed here is called Random Forest [Bre01]. A Random Forest is a socalled ensemble classifier: This means that it uses multiple, potentially weak, classifiers internally and combines their predictions into a final prediction. Random Forests do this by using multiple Decision Trees internally. The final prediction is then computed by taking a majority vote over the prediction of each tree. To avoid ending up with almost identical trees, randomness is introduced during training. More concretely, this is achieved by two factors: Bootstrapping [Efr79] is used to fit trees on re-sampled training examples. Additionally, Decision Trees do not select the best split but randomize this process by only considering a random subset of the available features. Since Random Forests use Decision Trees internally, they can be used to perform multi-label classification as well if the trees support it.\nIn this case, the scikit-learn implementation is used: RandomForestClassifier. The implementation uses DecisionTreeClassifiers internally using the same training algorithm as described above. As already mentioned, the splitting decision is now randomized. Since Decision Trees were already considered in the previous section, a maximum depth of 15 was selected. However, the following additional hyperparameters are considered:\n\u2022 The criterion that measures the quality of a split. The implementation supports the Gini impurity and the information gain. This is re-evaluated since the splitting decision is now randomized.\n\u2022 The number of Decision Trees that are grown in the Random Forest classifier: {1,2, . . . ,100}.\nThe 2 \u00b7100 = 200 possible combinations were explored using grid search. The evaluation was performed as previously described and the averaged F1 score was used as a measure. Figure 6.5 depicts the results.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 48 Chapter 6. Evaluation\nAs can be seen from the data, Random Forests improve as more and more Decision Trees are used until they eventually converge to a similar F1 score. A couple of interesting conclusions can be drawn from the results: Firstly, the information gain splitting criterion generally performs better than the Gini impurity on this problem. This was already the case during the evaluation of the individual Decision Trees. Secondly, a Random Forest that uses a single Decision Tree internally is worse than a single Decision Tree although they share the same parameters. This nicely illustrates that trees in a Random Forest are, well, randomized. However, as more and more suboptimal trees are added, the Random Forest suddenly outperforms the best Decision Tree from the previous section. This is the crucial property that makes ensemble methods so successful. Still, Logistic Regression and SVMs achieved better results on this dataset."}, {"heading": "6.6 Classification Systems", "text": "After having evaluated the individual components of the system in the previous sections, this section brings it all together by evaluating the entire system end-to-end. To do so, two fundamental approaches must be distinguished first. Recall from chapter 5.3 that the multi-label classification problem can either be transformed to a single-label problem by treating each combination of labels as a single substitute label. In this case, each HMM represents such a substitute class and the classification is then simply a matter of selecting the class that corresponds to the model with the maximum likelihood. The second option truly handles the multi-label problem by using one HMM per label. This means that a single motion is potentially used to train multiple HMMs. However, in this case a more advanced decision maker is needed to map the likelihoods of the different HMMs to the multi-label prediction since simply picking the maximum is not an option anymore. Since these two approaches are inherently different they are treated as two distinct systems: The system that uses the former method is referred to as the power set system while the latter system is referred to as the multi-label system. Notice however that both systems can be used to solve the same multi-label classification problem.\nThis section also makes use of an additional measure: the total accuracy. In contrast to the accuracy\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 6.6: Classification Systems Page 49\nand the F1 score which are computed on a per-class basis, the total accuracy measures how often the entire binary label vector y was correctly predicted. If, for example, the prediction is p = (1,0,0,1,1,1) and the correct label is y = (1,0,0,1,1,0), the total accuracy would be 0 despite the fact that the classifier \u201calmost\u201d got it right. It should be obvious from this example that the total accuracy is a rather harsh and unforgiving measure. Only the total accuracy is used in this chapter.\nDuring this section, the results of the two systems are first considered individually with a direct comparison following subsequently. Both systems were evaluated on the same dataset that was already used for all other evaluations. A new permutation was used to initially shuffle the dataset. However, to make the two systems comparable, the permutation was the same for both systems. Stratified 3-fold was used to split the dataset into training and test folds. All measures were then computed over the combined test splits.\nFrom each section, the best results were used to perform the end-to-end evaluation. However, often multiple configurations showed promise in which case the best couple of configurations were selected. More concretely, the following best feature sets were selected from the results in section 6.3:\n1. root_rot_norm\n2. root_pos, root_vel, extremities_pos, root_rot and root_rot_norm\n3. extremities_pos, root_rot and root_rot_norm\n4. root_pos, root_vel, com_pos, extremities_pos, root_rot and root_rot_norm\n5. root_vel, extremities_pos, root_rot and root_rot_norm\n6. root_pos, root_vel, com_pos, extremities_pos, root_rot, root_rot_norm and marker_vel_norm\nAll features were normalized, smoothed with W = 3 and scaled to be in the interval [\u22121,1]. During this evaluation, the feature sets will be referenced by their enumeration number.\nBoth HMMs and FHMMs were used during this evaluation. Since the results in section 6.4.3 clearly showed that using FHMMs with more than two chains do not offer any significant advantage, only HMMs and FHMMs with 2 chains were considered in the following evaluation. All HMMs were trained for 100 Baum-Welch iterations. The FHMMs were trained using the sequential training algorithm and 100 Baum-Welch iterations were performed per chain. The transition and start probabilities were initialized uniformly whereas the emission distribution parameters were initialized using the k-means approach. The covariance matrices were all constrained to be diagonal. The decision to only use this initialization strategy was made based on the results in section 6.4.2 which showed that the other combinations do not have any advantages.\nFor each of the four different topologies evaluated in section 6.4.1, the best number of states were selected:\n1. fully-connected topology and K = 8 states\n2. left-to-right topology without \u2206 constraint and K = 5 states\n3. left-to-right topology with \u2206 = 1 and K = 6 states\n4. left-to-right topology with \u2206 = 2 and K = 5 states"}, {"heading": "6.6.1 Power Set System", "text": "For the power set system, a total of 6 \u00b72 \u00b74 = 48 different configuration combinations were evaluated as discussed above. Each combination of labels in the original dataset was replaced with a single substitute label. Since 54 different combinations can be identified in the dataset (see section 6.2), a total of 54 of such substitute labels were used. Analogously 54 HMMs or FHMMs (depending on the configuration) were trained. Classification was then performed by selecting the model under which the unknown motion had the highest likelihood. The 20 best results as measured by their F1 score are listed in table 6.9. The\nClassification of Human Whole-Body Motion using Hidden Markov Models\nF1 scores, precisions and recalls were averaged over all classes. Averaging is not necessary for the total accuracy since this measure already considers all classes combined.\nA first conclusion that can be drawn from the data is that the overall performance of the system is very good. The best configuration achieved an F1 score of 0.9742 and a total accuracy of 0.9727. Another important observation is that the precision and recall scores are nicely balanced across configurations, hence the system does not show a bias towards the one or the other. No clear best option can be identified for the different feature sets. Set 2, 4, 5 and 6 all delivered very good performance with feature set 3 doing slightly worse. However, feature set 1 performed very poorly, with not a single instance of this configuration in the top 20. Another interesting result is that FHMMs did not provide a significant advantage over HMMs. The best total accuracy was achieved by an FHMM whereas the best F1 score was achieved by an HMM. As can be seen from the data, both HMMs and FHMMs achieved very similar scores overall. Since FHMMs are computationally more expensive and more complex to implement, HMMs should be preferred. Lastly, the different topologies and states did not have a big impact on the performance. All different combinations appear in the top 20. However, it should be noted that the best results were achieved with fully-connected HMMs and left-to-right FHMMs respectively. Despite this apparent correlation, the effect on the overall performance is only very small.\nTable 6.10 contains the detailed results for the configuration that achieved the best F1 score. Notice again that multiple labels have been combined into a single substitute label. The data also shows that the classification scores are consistently very high across all classes.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 52 Chapter 6. Evaluation"}, {"heading": "6.6.2 Multi-Label System", "text": "The multi-label system was evaluated similarly. However, this system requires more sophisticated decision makers to map the likelihood scores of the HMMs to the binary prediction vector. For each of the previously evaluated decision makers, the best configurations (see section 6.5) was used:\n1. Logistic Regression with L1 penalty and C = 10\u22123 using the binary relevance method\n2. Support Vector Machine with L1 penalty and C = 10\u22122 using the binary relevance method\n3. Decision Tree with information gain splitting criterion and a maximum depth of 15\n4. Random Forest with information gain splitting criterion, a maximum depth of 15 and 40 internal Decision Trees\nAdditionally, two much simpler decision makers were evaluated as well: The zero decision maker always predicts all classes that correspond to HMMs with a likelihood score greater than or equal to zero. The maximum decision maker works as described in the previous section by always selecting only a single class which corresponds to the HMM with the highest likelihood. Therefore a total of 6 \u00b7 2 \u00b7 4 \u00b7 6 = 288 different configuration combinations were evaluated as discussed above. Since 49 different classes exist (see section 6.2) a total of 49 HMMs or FHMMs (depending on the configuration) were trained. Classification was then performed using the decision maker of the respective configuration. The 20 best results as measured by their F1 score are listed in table 6.11.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nSection 6.6: Classification Systems Page 53\nThe results of the multi-label system are also very good. The best achieved F1 score was 0.9662 with the best total accuracy at 0.9339. Precision and recall are once again balanced, so a bias towards one or the other does not exist. An important observation is that HMMs slightly outperform FHMMs. In general FHMMs do not offer any noticeable advantage of HMMs on this dataset. Another result is that the left-to-right topologies dominate the top 20 results. However, since the F1 scores are still very similar across topologies, they are still not very important for the overall performance of the classifier. Similar to the previously discussed system, the feature sets 2, 4, 5 and 6 all achieve good results. Feature set 1 and 3 did not make it into the top 20. The best achieved F1 score for feature set 3 was 0.9513 and for the first set the best score was 0.5084. Again, feature set 1 is obviously not a good choice.\nA very important aspect of the classifier is the choice of the decision maker. As can be seen from the above results, the top 20 best scores were all achieved using either Logistic Regression or Support Vector Machines. Logistic Regression consistently outperformed SVMs but the effect on the F1 score is very small. The best achieved F1 score for Random Forests was 0.9029 and the best score using Decision Trees was 0.8799. In general, the results are consistent with the results of section 6.5. The best maximum decision maker achieved an F1 score of 0.7248. It should be noted however that the total accuracy dropped to an abysmal score of 0.0110. This result makes sense since the maximum decision maker only selects a single class but most motions belong to multiple classes. This results in a somewhat decent F1 score since the prediction likely contains one correct label. However, a prediction made by the maximum decision maker can only be completely correct if a motion only has a single label\u2013which is not the case for most motions in the dataset. The zero decision maker finally achieved an F1 score of 0.3711 and a total accuracy of 0.2445. To summarize, a linear model is obviously the best choice for the decision maker. Logistic Regression proved to perform especially well.\nSimilar to the previous section, table 6.12 (at the very end of this section) contains a more detailed characterization of the best classifier configuration as measured by the F1 score. Overall, the F1 score is consistently high across classes."}, {"heading": "6.6.3 Comparison", "text": "Both systems did not benefit from the use of FHMMs in any significant way. Additionally, feature sets 2, 4, 5 and 6 proofed to be good choices for either system. The choice of topology and number of states did not have a significant effect on the performance although the left-to-right topology achieved better results in the multi-label system.\nThe power set system achieved an F1 score of 0.9742 whereas the multi-label system achieved 0.9662. A more significant difference can be seen in the total accuracy: 0.9802 was the best achieved score for the power set system whereas the multi-label system achieved 0.9339. It is interesting that the F1 scores are relatively similar whereas there\u2019s a noticeable difference in total accuracy. An explanation for these results is that the power set system is less likely to make a mistake since only ever one label is selected. Since most labels are properly recognized, the total accuracy is very high. In general, the power set system can only get a prediction right or wrong. The multi-label system on the other hand has a trickier job: The system does not know in advance how many labels a motion has. This additional uncertainty results in a slightly worse total accuracy. This means that the system will produce some predictions that are \u201calmost right\u201d. However, these are counted as wrong by the total accuracy, resulting in a worse score. In summary the recognition performance across classes is similar for both systems, hence the F1 score is also very similar. Due to the difference in how the prediction is made, the total accuracy varies. However, it is important to stress that the superior performance of the power set method is bought with additional resources: Since each label combination is treated as a single substitute label, more HMMs need to be kept in memory. During classification of an unknown motion, the likelihoods under each HMM must be evaluated, resulting in higher computational complexity. Additionally, sparse training data can become a problem (also see discussion in chapter 5.3.1).\nOn this dataset, the power set system does not run into any of these problems since only 54 label combinations exist and enough training data is available per combination. However, as the dataset grows and more and more label combinations become possible, the power set method will eventually become infeasible. The multi-label system does not have this problem and therefore certainly has its place.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 55"}, {"heading": "7 Conclusion", "text": "The goal of this thesis was to develop a system that can accurately classify human whole-body motion. A key consideration was that multiple labels are needed to describe a single motion. During the course of this work, the different parts necessary to perform multi-label classification were introduced, discussed and evaluated. An especially important aspect of this thesis was the discussion of different representations of the motions as features as well as the extraction of novel features from the raw data. The section on how multi-label classification can be performed was equally important to achieve the required multi-label classification. Two different systems were discussed: The power set system transforms the multi-label problem to a single-label problem by treating each label combination as a substitute class. The multi-label system was developed using an approach novel in the field of motion recognition for robotics that truly handles multi-label problems1.\nThe systems and their components were evaluated using a dataset that consists of 454 motions. 49 different labels were used to describe each motion with 54 unique label combinations. Due to the high number of parameters and components, individual building blocks of the system were evaluated and optimized in isolation before an end-to-end evaluation of the whole proposed approach was conducted.\nFirst, feature selection was performed to find the best set of features. An important insight was that the joint angles and marker positions are not necessary to recognize the different motions. Instead the root position, root velocity, root rotation and the positions of the subject\u2019s extremities proved to be an especially good set of features.\nDifferent configurations and variations of HMMs were evaluated. The main results of this section were that proper initialization of the emission distribution parameters is very important; less so the initialization of the transition and start probabilities. The best results were achieved when the means and covariance matrices over the emission distribution were initialized by first clustering the data using the k-means algorithm and then estimating the means and covariances over these clusters. It further proved necessary to constrain the covariance matrices to be diagonal. Different topologies and numbers of states were evaluated as well. While a low number of states (5 to 8) produced the best results, the choice of topology was less important. However, the left-to-right topology proved to be a good choice overall. The performance of FHMMs and HMMs was also compared. FHMMs did not provide a significant advantage over HMMs for motion classification on the evaluation dataset. Since FHMMs are computationally more expensive to train and evaluate, HMMs are the better choice for both systems.\nDifferent decision makers which map the likelihood scores of the HMMs to the multi-label prediction and their hyperparameters were evaluated as well. In general, linear models, namely Logistic Regression and Support Vector Machines, proved to be especially good choices. Logistic Regression was further shown to slightly outperform SVMs on the evaluation dataset. An important insight was that both models worked well if L1 regularization was used. The best regularization coefficients were C = 10\u22123 and C = 10\u22122 for Logistic Regression and SVMs respectively. Decision Trees and Random Forests were also evaluated. Both were capable of learning the multi-label mapping from likelihoods to labels and Random Forests outperformed a single Decision Tree. The best splitting criterion turned out to be the information gain and a maximum tree depth of 15 delivered consistently good results. 40 Decision Trees were used in a Random Forest to achieve good results. However, the linear models clearly outperformed the Decision Tree and Random Forest during evaluation.\nThe end-to-end evaluation of the two systems revealed that both can be used to accurately classify human whole-body motion into multiple classes. The power set system achieved a total accuracy of 98.02% on the test dataset whereas the multi-label system achieved 93.39%. However, while the power set is limited by the number of label combinations within a dataset, the multi-label system does not have\n1based upon fundamental research from the field of machine learning\nClassification of Human Whole-Body Motion using Hidden Markov Models\nPage 56 Chapter 7. Conclusion\nthis constraint. This makes the multi-label system a potentially very interesting approach for classification tasks where the number of label combinations is much greater than the number of labels.\nIn future work, the system devised in this thesis could be extended in a couple of ways. An interesting extensions would be Parametric Hidden Markov Models. PHMMs could be used similarly to the work in [HUK08] to recognize different variations of the same motion. More concretely, instead of using different HMMs for the classes \u201cfast\u201d, \u201cmedium\u201d and \u201cslow\u201d, a single PHMM could potentially be used to replace three HMMs.\nAnother extension would be to use a hierarchical tree structure of HMMs similar to [KTN08]. This tree structure would have several advantages: Most importantly, the classification speed could be reduced if not all but only a couple of HMMs need to be considered for each unknown motion. However, while traversing the tree from root to leaf is trivial for single-label classification, it becomes less obvious how the tree search could be realized for a multi-label problem. A first idea would be to use supervised learning to not only train the HMMs but to also learn rules when to cut off a subtree from further consideration. Using a tree structure would also allow to use a hybrid of HMMs and FHMMs. The sequential learning algorithm could then be used to train additional chains if and only if necessary.\nBayesian optimization is an interesting topic that could be used to find better feature subsets similar to the work in [ILES00]. The same basic idea can be applied to tune the hyperparameters of the classifier [SLA12].\nThe two systems could also be further evaluated on a larger dataset with more motions performed by more subjects. An especially interesting evaluation would be on a dataset where much more label combinations than labels exist. On such a dataset, the multi-label system could potentially outperform the power set system due to the high number of label combinations. Additionally, the effect of sparse training data could be evaluated properly on such a dataset.\nLastly, the developed system could be deployed for usage in the KIT Whole-Body Human Motion Database. The system could then be used to automatically label new motion data and maybe even to detect inconsistencies in the already labeled data. Furthermore, the integration would offer interesting insights on how the system performs on a real-world problem. Additional considerations would be how the system could be integrated into the existing database source code as well as adding user interface elements to check and, if necessary, correct the predictions of the classifier.\nClassification of Human Whole-Body Motion using Hidden Markov Models\nBibliography Page 57"}], "references": [{"title": "Toward an unified representation for imitation of human motion on humanoids", "author": ["Pedram Azad", "Tamim Asfour", "R\u00fcdiger Dillmann"], "venue": "In Robotics and Automation,", "citeRegEx": "Azad et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Azad et al\\.", "year": 2007}, {"title": "ARMAR-III: An integrated humanoid platform for sensory-motor control", "author": ["Tamim Asfour", "Kristian Regenstein", "Pedram Azad", "Joachim Schr\u00f6der", "Alexander Bierbaum", "Niko Vahrenkamp", "R\u00fcdiger Dillmann"], "venue": "In Humanoid Robots,", "citeRegEx": "Asfour et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Asfour et al\\.", "year": 2006}, {"title": "Pattern Recognition and Machine Learning, volume 4. springer", "author": ["Christopher M Bishop"], "venue": "New York,", "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Anthropometric data for describing the kinematics of the human", "author": ["Bryan Buchholz", "Thomas J Armstrong", "Steven A Goldstein"], "venue": "hand. Ergonomics,", "citeRegEx": "Buchholz et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Buchholz et al\\.", "year": 1992}, {"title": "Continuous speech recognition via centisecond acoustic states", "author": ["Raimo Bakis"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Bakis.,? \\Q1976\\E", "shortCiteRegEx": "Bakis.", "year": 1976}, {"title": "Learning from and about others: Towards using imitation to bootstrap the social understanding of others by robots", "author": ["Cynthia Breazeal", "Daphna Buchsbaum", "Jesse Gray", "David Gatenby", "Bruce Blumberg"], "venue": "Artificial life,", "citeRegEx": "Breazeal et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Breazeal et al\\.", "year": 2005}, {"title": "Robot programming by demonstration", "author": ["Aude Billard", "Sylvain Calinon", "R\u00fcdiger Dillmann", "Stefan Schaal"], "venue": "In Springer handbook of robotics,", "citeRegEx": "Billard et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Billard et al\\.", "year": 2008}, {"title": "An inequality with applications to statistical estimation for probabilistic functions of markov processes and to a model for ecology", "author": ["Leonard E Baum", "John Alonzo Eagon"], "venue": "Bull. Amer. Math. Soc,", "citeRegEx": "Baum and Eagon,? \\Q1967\\E", "shortCiteRegEx": "Baum and Eagon", "year": 1967}, {"title": "Classification and regression trees", "author": ["Leo Breiman", "Jerome Friedman", "Charles J Stone", "Richard A Olshen"], "venue": "CRC press,", "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Learning multilabel scene classification", "author": ["Matthew R Boutell", "Jiebo Luo", "Xipeng Shen", "Christopher M Brown"], "venue": "Pattern recognition,", "citeRegEx": "Boutell et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Boutell et al\\.", "year": 2004}, {"title": "Effects of supportive hand contact on reactive postural control during support perturbations", "author": ["Jan Babi\u010d", "Tadej Petri\u010d", "Luka Peternel", "Nejc \u0160arabon"], "venue": "Gait & posture,", "citeRegEx": "Babi\u010d et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Babi\u010d et al\\.", "year": 2014}, {"title": "A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains", "author": ["Leonard E Baum", "Ted Petrie", "George Soules", "Norman Weiss"], "venue": "The annals of mathematical statistics,", "citeRegEx": "Baum et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Baum et al\\.", "year": 1970}, {"title": "Growth transformations for functions on manifolds", "author": ["Leonard E Baum", "George R Sell"], "venue": "Pacific J. Math,", "citeRegEx": "Baum and Sell,? \\Q1968\\E", "shortCiteRegEx": "Baum and Sell", "year": 1968}, {"title": "On learning, representing, and generalizing a task in a humanoid robot. Systems, Man, and Cybernetics, Part B: Cybernetics", "author": ["Sylvain Calinon", "Florent Guenter", "Aude Billard"], "venue": "IEEE Transactions on,", "citeRegEx": "Calinon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Calinon et al\\.", "year": 2007}, {"title": "Introduction to robotics: mechanics and control, volume 3", "author": ["John J Craig"], "venue": "Pearson Prentice Hall Upper Saddle River,", "citeRegEx": "Craig.,? \\Q2005\\E", "shortCiteRegEx": "Craig.", "year": 2005}, {"title": "Learning robot behaviour and skills based on human demonstration and advice: the machine learning paradigm", "author": ["R\u00fcdiger Dillmann", "Oliver Rogalla", "Markus Ehrenmann", "R Zollner", "Monica Bordegoni"], "venue": "In ROBOTICS RESEARCH-INTERNATIONAL SYMPOSIUM-,", "citeRegEx": "Dillmann et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Dillmann et al\\.", "year": 2000}, {"title": "Hidden Markov models: estimation and control, volume 29", "author": ["Robert J Elliott", "Lakhdar Aggoun", "John B Moore"], "venue": "Springer Science & Business Media,", "citeRegEx": "Elliott et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2008}, {"title": "Bootstrap methods: another look at the jackknife", "author": ["Bradley Efron"], "venue": "The annals of Statistics,", "citeRegEx": "Efron.,? \\Q1979\\E", "shortCiteRegEx": "Efron.", "year": 1979}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff.,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff.", "year": 2003}, {"title": "Factorial hidden markov models", "author": ["Zoubin Ghahramani", "Michael I Jordan"], "venue": "Machine learning,", "citeRegEx": "Ghahramani and Jordan.,? \\Q1997\\E", "shortCiteRegEx": "Ghahramani and Jordan.", "year": 1997}, {"title": "A class of wasserstein metrics for probability distributions", "author": ["Clark R Givens", "Rae Michael Shortt"], "venue": "Michigan Math. J,", "citeRegEx": "Givens and Shortt,? \\Q1984\\E", "shortCiteRegEx": "Givens and Shortt", "year": 1984}, {"title": "Spoken language processing: A guide to theory, algorithm, and system development", "author": ["Xuedong Huang", "Alex Acero", "Hsiao-Wuen Hon", "Raj Reddy"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2001}, {"title": "Scholkopf. Support vector machines", "author": ["Marti A. Hearst", "Susan T Dumais", "Edgar Osman", "John Platt", "Bernhard"], "venue": "Intelligent Systems and their Applications,", "citeRegEx": "Hearst et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hearst et al\\.", "year": 1998}, {"title": "Motion imitation and recognition using parametric hidden markov models", "author": ["Dennis Herzog", "AleNs Ude", "Volker Kr\u00fcger"], "venue": "In Humanoid Robots,", "citeRegEx": "Herzog et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Herzog et al\\.", "year": 2008}, {"title": "Feature subset selection by bayesian network-based optimization", "author": ["I\u00f1aki Inza", "Pedro Larra\u00f1aga", "Ram\u00f3n Etxeberria", "Basilio Sierra"], "venue": "Artificial intelligence,", "citeRegEx": "Inza et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Inza et al\\.", "year": 2000}, {"title": "Factorial hidden markov models and the generalized backfitting algorithm", "author": ["Robert A Jacobs", "Wenxin Jiang", "Martin A Tanner"], "venue": "Neural computation,", "citeRegEx": "Jacobs et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Jacobs et al\\.", "year": 2002}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes", "author": ["A Jordan"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Jordan.,? \\Q2002\\E", "shortCiteRegEx": "Jordan.", "year": 2002}, {"title": "Learning actions from observations", "author": ["Volker Kr\u00fcger", "Dennis L Herzog", "Sanmohan Baby", "Ales Ude", "Danica Kragic"], "venue": "Robotics & Automation Magazine, IEEE,", "citeRegEx": "Kr\u00fcger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kr\u00fcger et al\\.", "year": 2010}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["Tapas Kanungo", "David M Mount", "Nathan S Netanyahu", "Christine D Piatko", "Ruth Silverman", "Angela Y Wu"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Kanungo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kanungo et al\\.", "year": 2002}, {"title": "Incremental learning of full body motion primitives and their sequencing through human motion observation", "author": ["Dana Kuli\u0107", "Christian Ott", "Dongheui Lee", "Junichi Ishikawa", "Yoshihiko Nakamura"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Kuli\u0107 et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kuli\u0107 et al\\.", "year": 2011}, {"title": "Incremental on-line hierarchical clustering of whole body motion patterns", "author": ["Dana Kuli\u0107", "Wataru Takano", "Yoshihiko Nakamura"], "venue": "In Robot and Human interactive Communication,", "citeRegEx": "Kuli\u0107 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kuli\u0107 et al\\.", "year": 2007}, {"title": "Representability of human motions by factorial hidden markov models", "author": ["Dana Kuli\u0107", "Wataru Takano", "Yoshihiko Nakamura"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Kuli\u0107 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kuli\u0107 et al\\.", "year": 2007}, {"title": "Incremental learning, clustering and hierarchy formation of whole body motion patterns using adaptive hidden markov chains", "author": ["Dana Kuli\u0107", "Wataru Takano", "Yoshihiko Nakamura"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Kuli\u0107 et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kuli\u0107 et al\\.", "year": 2008}, {"title": "Numerically stable hidden markov model implementation", "author": ["Tobias P Mann"], "venue": "An HMM scaling tutorial,", "citeRegEx": "Mann.,? \\Q2006\\E", "shortCiteRegEx": "Mann.", "year": 2006}, {"title": "Analyzing wholebody pose transitions in multi-contact motions", "author": ["Christian Mandery", "J\u00falia Borr\u00e0s", "Mirjam J\u00f6chner", "Tamim Asfour"], "venue": "arXiv preprint arXiv:1507.08799,", "citeRegEx": "Mandery et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mandery et al\\.", "year": 2015}, {"title": "The kit whole-body human motion database", "author": ["Christian Mandery", "\u00d6mer Terlemez", "Martin Do", "Nikolaus Vahrenkamp", "Tamim Asfour"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA), Seattle, USA,(submitted),", "citeRegEx": "Mandery et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mandery et al\\.", "year": 2015}, {"title": "Learning from demonstration and adaptation of biped locomotion", "author": ["Jun Nakanishi", "Jun Morimoto", "Gen Endo", "Gordon Cheng", "Stefan Schaal", "Mitsuo Kawato"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Nakanishi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nakanishi et al\\.", "year": 2004}, {"title": "Open-end human\u2013robot interaction from the dynamical systems perspective: mutual adaptation and incremental learning", "author": ["Tetsuya Ogata", "Shigeki Sugano", "Jun Tani"], "venue": "Advanced Robotics,", "citeRegEx": "Ogata et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ogata et al\\.", "year": 2005}, {"title": "Angular momentum primitives for human walking: biomechanics and control", "author": ["Mako Popovic", "Amy Englehart"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Popovic and Englehart.,? \\Q2004\\E", "shortCiteRegEx": "Popovic and Englehart.", "year": 2004}, {"title": "Scikit-learn: Machine learning in python", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner.,? \\Q1989\\E", "shortCiteRegEx": "Rabiner.", "year": 1989}, {"title": "Classifier chains for multilabel classification", "author": ["Jesse Read", "Bernhard Pfahringer", "Geoff Holmes", "Eibe Frank"], "venue": "Machine learning,", "citeRegEx": "Read et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Read et al\\.", "year": 2011}, {"title": "Smoothing methods in statistics", "author": ["Jeffrey S Simonoff"], "venue": "Springer Science & Business Media,", "citeRegEx": "Simonoff.,? \\Q2012\\E", "shortCiteRegEx": "Simonoff.", "year": 2012}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Boostexter: A boosting-based system for text categorization", "author": ["Robert E Schapire", "Yoram Singer"], "venue": "Machine learning,", "citeRegEx": "Schapire and Singer.,? \\Q2000\\E", "shortCiteRegEx": "Schapire and Singer.", "year": 2000}, {"title": "On the stratification of multi-label data", "author": ["Konstantinos Sechidis", "Grigorios Tsoumakas", "Ioannis Vlahavas"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Sechidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sechidis et al\\.", "year": 2011}, {"title": "Factored conditional restricted boltzmann machines for modeling motion style", "author": ["Graham W Taylor", "Geoffrey E Hinton"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Taylor and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Hinton.", "year": 2009}, {"title": "Correlated space formation for human whole-body motion primitives and descriptive word labels", "author": ["Wataru Takano", "Seiya Hamano", "Yoshihiko Nakamura"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Takano et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Takano et al\\.", "year": 2015}, {"title": "Modeling human motion using binary latent variables", "author": ["Graham W Taylor", "Geoffrey E Hinton", "Sam T Roweis"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Taylor et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2006}, {"title": "Organization of behavioral knowledge from extraction of temporal-spatial features of human whole body motions", "author": ["Wataru Takano", "Hirotaka Imagawa", "Dana Kuli\u0107", "Yoshihiko Nakamura"], "venue": "In Biomedical Robotics and Biomechatronics (BioRob),", "citeRegEx": "Takano et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Takano et al\\.", "year": 2010}, {"title": "Construction of a space of motion labels from their mapping to full-body motion symbols", "author": ["Wataru Takano", "Yoshihiko Nakamura"], "venue": "Advanced Robotics,", "citeRegEx": "Takano and Nakamura.,? \\Q2015\\E", "shortCiteRegEx": "Takano and Nakamura.", "year": 2015}, {"title": "Statistical mutual conversion between whole body motion primitives and linguistic sentences for human motions", "author": ["Wataru Takano", "Yoshihiko Nakamura"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Takano and Nakamura.,? \\Q2015\\E", "shortCiteRegEx": "Takano and Nakamura.", "year": 2015}, {"title": "Master Motor Map (MMM)\u2013framework and toolkit for capturing, representing, and reproducing human motion on humanoid robots", "author": ["\u00d6mer Terlemez", "Stefan Ulbrich", "Christian Mandery", "Martin Do", "Nikolaus Vahrenkamp", "Tamim Asfour"], "venue": "In Humanoid Robots (Humanoids),", "citeRegEx": "Terlemez et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Terlemez et al\\.", "year": 2014}, {"title": "Primitive Communication based on Motion Recognition and Generation with Hierarchical Mimesis Model", "author": ["Wataru Takano", "Katsu Yamane", "Tomomichi Sugihara", "Kou Yamamoto", "Yoshihiko Nakamura"], "venue": "In Robotics and Automation,", "citeRegEx": "Takano et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Takano et al\\.", "year": 2006}, {"title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm", "author": ["Andrew J Viterbi"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Viterbi.,? \\Q1967\\E", "shortCiteRegEx": "Viterbi.", "year": 1967}, {"title": "Decision trees for hierarchical multi-label classification", "author": ["Celine Vens", "Jan Struyf", "Leander Schietgat", "Sa\u0161o D\u017eeroski", "Hendrik Blockeel"], "venue": "Machine Learning,", "citeRegEx": "Vens et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vens et al\\.", "year": 2008}, {"title": "Parametric hidden markov models for gesture recognition", "author": ["Andrew D Wilson", "Aaron F Bobick"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Wilson and Bobick.,? \\Q1999\\E", "shortCiteRegEx": "Wilson and Bobick.", "year": 1999}, {"title": "Biomechanics of Human Movement", "author": ["David A Winter"], "venue": "Wiley New York,", "citeRegEx": "Winter.,? \\Q1979\\E", "shortCiteRegEx": "Winter.", "year": 1979}, {"title": "Biomechanics and motor control of human movement", "author": ["David A Winter"], "venue": null, "citeRegEx": "Winter.,? \\Q2009\\E", "shortCiteRegEx": "Winter.", "year": 2009}, {"title": "Human motion database with a binary tree and node transition graphs", "author": ["K. Yamane", "Y. Yamaguchi", "Y. Nakamura"], "venue": "In Proceedings of Robotics: Science and Systems,", "citeRegEx": "Yamane et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yamane et al\\.", "year": 2009}, {"title": "Ml-knn: A lazy learning approach to multi-label learning", "author": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "Pattern recognition,", "citeRegEx": "Zhang and Zhou.,? \\Q2007\\E", "shortCiteRegEx": "Zhang and Zhou.", "year": 2007}], "referenceMentions": [], "year": 2016, "abstractText": null, "creator": "LaTeX with hyperref package"}}}