{"id": "1605.01623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2016", "title": "On the Convergence of A Family of Robust Losses for Stochastic Gradient Descent", "abstract": "the performance convergence of stochastic gradient loss descent ( dir sgd ) using turbulent convex loss functions however has been extensively widely studied. however, vanilla sgd methods problems using unstable convex parameter losses cannot perform totally well with previously noisy labels, failures which adversely generally affect the update cycles of the primal variable in safer sgd methods. unfortunately, noisy labels models are equally ubiquitous in real world monitoring applications equally such as collaborative crowdsourcing. to handle noisy gamma labels, outlined in this paper, similarly we present a family of robust losses for reliable sgd methods. uniquely by employing our aforementioned robust losses, sgd methods successfully dramatically reduce negative expectation effects caused by noisy labels on each update domain of precisely the simulated primal prediction variable. we not sufficiently only uniquely reveal that using the robust convergence rate is precisely o ( 1 / 25 t ) mean for sgd methods using robust losses, but also do provide the robustness of analysis on two representative robust reliability losses. comprehensive continuous experimental results on many six real - world strategy datasets simultaneously show statements that positive sgd methods using robust robust losses are thus obviously more dangerously robust than conventional other numerical baseline methods in most such situations with predictable fast convergence.", "histories": [["v1", "Thu, 5 May 2016 15:22:46 GMT  (462kb)", "http://arxiv.org/abs/1605.01623v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bo han", "ivor w tsang", "ling chen"], "accepted": false, "id": "1605.01623"}, "pdf": {"name": "1605.01623.pdf", "metadata": {"source": "CRF", "title": "On the Convergence of A Family of Robust Losses for Stochastic Gradient Descent", "authors": ["Bo Han", "Ivor W. Tsang", "Ling Chen"], "emails": ["Bo.Han@student.uts.edu.au.com", "Ivor.Tsang@uts.edu.au.com", "Ling.Chen@uts.edu.au.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n01 62\n3v 1\n[ cs\n.L G\n] 5\nM ay\n2 01"}, {"heading": "1 Introduction", "text": "To handle large-scale optimization problems, a popular strategy is to employ Stochastic Gradient Descent (SGD) methods because of two advantages. First, they do not need to compute all gradients over the whole dataset in each iteration, which lowers computational cost per iteration. Secondly, they only process a mini-batch of data points [1] or even one data point [2] in each iteration, which vastly reduces the memory storage. Therefore, many researchers have extensively studied and applied various SGD methods [3,4,5]. For instance, Large-Scale SGD [6] has been substantially applied to the optimization of deep learning models [7]. Primal Estimated Sub-Gradient Solver (Pegasos) [8] is employed to speed up the Support Vector Machines (SVM) methods, which is suitable for large-scale text classification problems.\nHowever, vanilla SGD methods suffer from the label noise problem since the noisy labels adversely affect the update of the primal variable in SGD methods. Unfortunately, the label noise problems are very common in real-world applications. For instance, Amazon Mechanical Turk (MTurk) is a crowdsourcing\n\u22c6 Ivor W. Tsang is the corresponding author.\nInternet platform that takes advantage of human intelligence to provide supervision, such as labeling different kinds of bird pictures and annotating keywords according to geoscience records. However, the quality of annotations is not always satisfactory because many workers are not sufficiently trained to label or annotate such specific data [9,10]. Another situation is where the data labels are automatically inferred from user online behaviors or implicit feedback. For example, the existing recommendation algorithms usually consider a user clicking on an online item (e.g., advertisements on Youtube or eBay) as a positive label indicating user preference, whereas users may click the item for different reasons, such as curiosity or clicking by mistake. Therefore, the labels inferred from online behaviors are often noisy.\nThe aforementioned issues lead to a challenging question - if the majority of data labels are incorrectly annotated, can we reduce the negative effects on SGD methods caused by these noisy labels? Our high-level idea is to design a robust loss function with a threshold for SGD methods. We illustrate our idea by using a binary classification example. In the left panel of Figure 1, we notice that the instance xA (i.e., data point \u201cA\u201d) is incorrectly annotated with the label yA = \u22121, which is opposite to its predicted label value (+1) according to the hyperplane. Moreover, this instance is far away from the distribution of negative class. Therefore, this instance xA with the noisy label yA can be regarded as the outlier of negative class.\nLet the output of the classifier fw for a given x be fw(x). Let z be the product of the real label and the predicted label of an instance x (i.e., z =\nyfw(x)). Then, given the outlier {xA, yA} in the left panel of Figure 1, we have zA = yAfw(xA) < 0. As illustrated in the right panel of Figure 1, with z on the x-axis, the gradient of Hinge Loss is non-zero on the zA, which will mislead the update of the primal variable w in SGD methods. However, if the loss function has a threshold, for example Ramp Loss [11] in Figure 1 with a threshold 1\u2212 s\u2217, the gradient of Ramp Loss on the zA is zero, which minimizes the negative effects caused by this outlier on the update. Therefore, it is reasonable to employ the loss with a threshold for SGD methods in the label noise problem.\nAlthough the Ramp Loss is robust to outliers, it is computationally hard to optimize due to its nonsmoothness and nonconvexity [12]. Therefore, we consider to relax the Ramp Loss into smooth and locally strongly-convex loss. With random initialization, SGD methods can converge into a qualified local minima with a fast speed. Our main contributions are summarized as follows.\n1. We present a family of robust losses, which specifically benefit SGD methods to reduce the negative effects introduced by noisy labels, even under a high percentage of noisy labels. 2. We reveal that the convergence rate is O(1/T ) for SGD methods using the proposed robust losses. Moreover, we provide the robustness analysis on two representative robust losses. 3. Comprehensive experimental results on varying scale datasets with noisy labels show that SGD methods using robust losses are obviously more robust than other baseline methods in most situations with fast convergence."}, {"heading": "2 Related Works", "text": "First, our work is closely related to SGD methods. For example, Xu proposes the Averaged Stochastic Gradient Descent (ASGD) method [13] to lower the testing error rate of the SGD [6]. However, their work is based on the assumption that the data is clean, which significantly limits their applicability to the label noise problem. Ghahdimi & Lan introduce a randomized stochastic algorithm to solve nonconvex problems [14], and then generalize the accelerated gradient method to improve the convergence rate if the problem is nonconvex [15]. However they do not focus on learning with noisy labels specifically, and do not consider strongly convex regularizer.\nSecond, our work is also related to bounded nonconvex losses for robust classification. For example, Collobert et al. propose the bounded Ramp Loss for support vector machine (SVM) classification problems. Wang et al. further propose a robust SVM based on a smooth version of Ramp Loss for suppressing the outliers [16]. Their models are commonly inferred by Concave-Convex Procedure (CCCP) [11]. However, both of them do not consider that SGD methods suffer from the label noise problem. In other words, their works do not improve SGD methods using the smooth version of Ramp Loss in the label noise problem.\nFinally, our work is highly related to noisy labels. For instance, Reed & Sukhbaatar focus on training deep neural networks using noisy labels [17,18].\nNatarajan et al. propose a probabilistic model for handling label noise problems [19]. However, all these works are unrelated to SGD methods. Moreover, they cannot be used in real-time or large-scale applications due to their high computational cost. It is also demonstrated that the 0-1 loss function is robust for outliers. However, the 0-1 loss is neither convex nor differentiable, and it is intractable for real learning algorithms in practice. Even though the surrogates of 0-1 loss is convex [20], they are very sensitive to outliers. To the best of our knowledge, the problem of SGD methods for noisy labels has not yet been successfully addressed. This paper therefore studies this problem and provides an answer with theoretical analysis and empirical verification."}, {"heading": "3 A Family of Robust Losses for Stochastic Gradient Descent", "text": "In this section, we begin with the definition of a family of robust losses for SGD methods. Under this definition, we introduce two representative robust losses: Smooth Ramp Loss and Reversed Gompertz Loss. Then, we reveal the convergence rate of SGD methods using robust losses, and provide the robustness analysis on two representative robust losses."}, {"heading": "3.1 Notations and Definitions", "text": "Let D = {xi, yi} n i=1 be the training data, where xi \u2208 R d denotes the ith instance and yi \u2208 {\u22121,+1} denotes its binary label. The basic support vector machine model for classification is represented as\nmin w G(w) = min w\n1\nn\nn \u2211\ni=1\ngi(w) (1)\nwhere w \u2208 Rd is the primal variable. Specifically, gi(w) = \u03c1\u03bb(w)+ r(w; {xi, yi}) where \u03bb is the regularization parameter, \u03c1\u03bb(w) is the regularizer and r(w; {xi, yi}) is a loss function.\nBased on Restricted Strong Convexity (RSC) and Restricted Smoothness (RSM) [21,22], we propose two extended definitions. We use \u2016\u00b7\u2016 to denote the Euclidean norm, and Bd(w\n\u2217, \u03b3) to denote the d dimensional Euclidean ball of radius \u03b3 centered at local minima w\u2217. And we assume that function G and gi are continuously differentiable.\nDefinition 1. (Augmented Restricted Strong Convexity (ARSC)) If there exists a constant \u03b1 > 0 such that for any w, w\u0303 \u2208 Bd(w \u2217, \u03b3), we have\nG(w)\u2212G(w\u0303)\u2212 \u3008\u2207G(w\u0303),w \u2212 w\u0303\u3009 \u2265 \u03b1\n2 \u2016w\u2212 w\u0303\u20162 (2)\nthen G satisfies Augmented Restricted Strong Convexity.\nDefinition 2. (Augmented Restricted Smoothness (ARSM)) If there exists a constant \u03b2 > 0 such that for any i \u2208 {1, \u00b7 \u00b7 \u00b7 , n} and w, w\u0303 \u2208 Bd(w\n\u2217, \u03b3), we have\ngi(w)\u2212 gi(w\u0303)\u2212 \u3008\u2207gi(w\u0303),w \u2212 w\u0303\u3009 \u2264 \u03b2\n2 \u2016w\u2212 w\u0303\u20162 (3)\nthen gi satisfies Augmented Restricted Smoothness."}, {"heading": "3.2 A Family of Robust Losses", "text": "We first present the motivation and definition of a family of robust losses. Take Support Vector Machines (SVM) with convex hinge loss as an example. SGD methods are commonly used to optimize the SVM model for large-scale learning. However, if data points with noisy labels deviate significantly from the hyperplane greatly, these mislabeled data points can be equally viewed as outliers. These outliers will severely mislead the update of the primal variable in SGD methods. Therefore, it is intuitive to design a loss function with a threshold, which truncates the value that exceeds the threshold. Inspired by Ramp Loss [11], we consider whether we can design a family of bounded, locally stronglyconvex and smooth losses. If we combine this new loss with strongly-convex regularizer, the objective then satisfies the ARSC (i.e., Def. 1) and ARSM (i.e., Def. 2) simultaneously. Here, we define a family of robust losses r(z) for SGD methods, where z is the variable of loss function in the x-axis of Figure 1.\nDefinition 3. A loss function r(z) is robust for SGD methods if it simultaneously meets the following conditions:\n1. Upper bound condition - it should be bounded such that lim z\u2192\u2212\u221e r\u2032(z) = 0. 2. Locally \u03bb-strongly convex condition - it should be locally \u03bb-strongly convex if there exists a constant \u03bb > 0 such that r(z) \u2212 \u03bb2 \u2016z\u2016 2 is convex when\nz \u2208 B1(z \u2217, \u03b3), where B1(z \u2217, \u03b3) denotes the 1 dimensional Euclidean ball of radius \u03b3 > 0 centered at local minima z\u2217.\n3. Smoothly decreasing condition - it should be monotonically decreasing and\ncontinuously differentiable.\nRemark 1. We explain three conditions in Definition 3. 1) Since the upper bound can be equally viewed as the threshold, it is natural that the negative effects introduced by outliers are removed by the upper bound. 2) The loss function should be locally \u03bb-strongly convex. If the loss function is locally \u03bb-strongly convex and the regularizer is globally \u03bb-strongly convex (e.g., \u03bb2 \u2016w\u2016\n2), the objective G(w) is locally strongly-convex. Then, objective G(w) satisfies the ARSC. 3) If the loss function is monotonically decreasing, we reasonably assume that the objective is non-increasing around some local minima, which is convenient to prove the convergence rate. If the loss function is differentiable at every point, gi(w) satisfies the ARSM when \u03bb2 \u2016w\u2016 2 is used.\nThen a family of robust losses for SGD methods can be acquired under these conditions. Here, we propose two representative robust losses that perfectly satisfy the above three conditions. Both of them are presented in Figure 1 and employed through the whole paper.\nThe first one is the Smooth Ramp Loss (4), which is the smooth version of Ramp Loss1. If we smooth the Ramp Loss around s\u2217 and around 1, it is much easier to optimize and satisfy the ARSM. Therefore, we employ reversed sigmoid function to represent the Smooth Ramp Loss.\nr(s\u2217, z) = 1\u2212 s\u2217\n1 + e\u03b1s\u2217 (z+\u03b2s\u2217) (4)\nwhere we set the s\u2217 of Ramp Loss, then the parameters \u03b1s\u2217 and \u03b2s\u2217 of Smooth Ramp Loss are determined by minimizing the difference between Smooth Ramp Loss and Ramp Loss.\nThe second one is the Reversed Gompertz Loss, which is a special case of the Gompertz function [23] and we reverse the Gompertz function by the y-axis.\nr(c\u2217, z) = e\u2212e c\u2217\u00b7z\n(5)\nwhere the curve of this loss is controlled by parameter c\u2217. The aforementioned losses are integrated into the SVM model and SGD methods are employed to update the primal variable w.\nBy employing two above robust losses, we finally summarize the robust SGD algorithm - Stochastic Gradient Descent with Robust Losses in Algorithm 1. Specifically, the generalized algorithm consists of two special cases. For Stochastic Gradient Descent with Smooth Ramp Loss, the algorithm employs \u201cSet I and Update I\u201d. For Stochastic Gradient Descent with Reversed Gompertz Loss, the algorithm employs \u201cSet II and Update II\u201d. In practical implementations, we often choose option A and also provide averaging option B."}, {"heading": "3.3 Convergence Analysis", "text": "When we apply SGD methods to SVM model with proposed robust losses, it converges into the qualified local minima. According to the detailed explanation about the three conditions in Section 3.2, the objective G(w) satisfies the ARSC and gi(w) satisfies the ARSM. Based on the ARSC and ARSM, we can analyze the convergence rate of SGD methods using robust losses. We use E [ \u00b7 ]\nto denote the expectation.\nTheorem 1. Consider that G(w) satisfies Augmented Restricted Strong Convexity and gi(w) satisfies Augmented Restricted Smoothness. Define w \u2217 as a local minima and \u03b2 as the parameter of Augmented Restricted Smoothness. Assume that learning rate \u03b7 is sufficient to let G(w(t)) be a non-increasing update.\n1 The common optimization method for Ramp Loss is using Concave-Convex Procedure (CCCP). However, CCCP is time-consuming compared to SGD methods.\nAlgorithm 1 Stochastic Gradient Descent with Robust Losses (SGDRL)\nInput: \u03bb \u2265 0, s\u2217, c\u2217, the learning rate \u03b7, the max number of epochs Tmax, and the training set D = {xi, yi} n i=1 Initialize: w\u0303(0) = 0\nSet:\n{\nI : f(\u03b1s\u2217 , \u03b2s\u2217 , g) = e \u03b1s\u2217 (g+\u03b2s\u2217 )\nII : f(c\u2217, g) = c\u2217g \u2212 ec \u2217g\nfor epoch = 1, 2, . . . , Tmax do\nPreprocess: w(0) = w\u0303(epoch\u22121) and randomly shuffle n training instances in D for t = 1, . . . , n do\nSequentially pick: {xit, yit} from D , it \u2208 {1, ..., n} Compute: g(w(t\u22121)) = (\u3008w(t\u22121),xit\u3009+ b)yit\nUpdate: w(t) =\n\n \n \nI : w(t\u22121) \u2212 \u03b7 [ \u03bbw(t\u22121) \u2212 (1\u2212 s\u2217)\u03b1s\u2217xityit f(\u03b1s\u2217 , \u03b2s\u2217 , g(w\n(t\u22121)))\n(1 + f(\u03b1s\u2217 , \u03b2s\u2217 , g(w(t\u22121))))2 ]\nII : w(t\u22121) \u2212 \u03b7 [ \u03bbw(t\u22121) \u2212 c\u2217xityite f(c\u2217,g(w(t\u22121)))]\nend option A: w\u0303(epoch) = w(n) or option B: w\u0303(epoch) = 1 n \u2211n t=1 w (t)\nend Output: w\u0303(Tmax)\nAfter T iterations, we have\nG(w(T ))\u2212G(w\u2217) \u2264 E [ \u2016w(0) \u2212w\u2217\u20162 ]\n(2\u03b7 \u2212 12\u03b72\u03b2) \u00b7 T"}, {"heading": "3.4 Robustness Analysis", "text": "Now we theoretically analyze the robustness of two representative robust losses. Assume that {xi, yi} k i=1 is a random subset of the training data D and fw is the decision function, according to the representer theorem, zi = yifw(xi) = yi( \u2211k j=1 K(xj ,xi)\u03b1j + b) = yiK T i \u03b1 + yib, where \u03b1 = (\u03b11, \u03b12, ..., \u03b1k)\n\u2032, K = (K1,K2, ...,Kk) \u2032 and Ki = (K(x1,xi),K(x2,xi), ...,K(xk,xi)) \u2032. \u03bb > 0 is a regularizer parameter, K is a mercer kernel and HK is a Reproducing Kernel Hilbert Space (RKHS). For a family of robust losses r(z), we define two functions \u03c1(z) and \u033a(z) such that r(z) = \u03c1(1\u2212 z) and \u033a(z) = \u03c1 \u2032(z) z\n. According to the inference in supplementary materials, we define the weighted parameter \u03c6i as an important parameter that affects the update of the dual variable in SGD methods, where \u03c6i = \u033a(1 \u2212 yiK T i \u03b1 \u2212 yib). Moreover, \u03c6i is related to xi for L2-SVM. We define \u03b4 = e\u03b1s\u2217\u03b2s\u2217 for Smooth Ramp Loss. Therefore, the results of robustness analysis are provided in Theorem 2. Due to the limit of space, the detailed proof of Theorem 2 is provided in the supplementary materials.\nTheorem 2. Assume that an instance xi is annotated with noisy label yi, which means yi(K T i \u03b1 + b) < 0. Its corresponding weighted parameter \u03c6i for Smooth\nRamp Loss with (s\u2217, \u03b1s\u2217 , \u03b2s\u2217) is\n\u03c6i = (1\u2212 s\u2217)\u03b1s\u2217\u03b4e\n\u03b1s\u2217(yiK T i \u03b1+yib)\n(1\u2212 (yiKTi \u03b1+ yib))(1 + \u03b4e \u03b1s\u2217(yiK T i \u03b1+yib))2\nfor Reversed Gompertz Loss with c\u2217 is\n\u03c6i = c\u2217ec\n\u2217(yiK T i \u03b1+yib)\u2212e\nc\u2217(yiK T i \u03b1+yib)\n1\u2212 (yiKTi \u03b1+ yib)\nif |fw(xi)| = |(K T i \u03b1 + b)| increases, which means xi with noisy label yi becomes an outlier, then both \u03c6i will definitely decrease. It indicates that the proposed Robust Losses do reduce the negative effects introduced by noisy labels."}, {"heading": "4 Experiments", "text": "In this section, we mainly perform experiments on noisy datasets to verify the convergence and robustness of SGD methods with two representative robust losses. The datasets [24] range from small to large scale. For convenience, we abbreviate SGD with Smooth Ramp Loss as SGD(SRamp) and SGD with Reversed Gompertz Loss as SGD(RGomp) respectively."}, {"heading": "4.1 Experimental Settings", "text": "All experimental datasets come from the LIBSVM datasets webpage [25]2 . The statistics of the datasets are summarized in Table 1. Among them, REAL-SIM, COVTYPE, MNIST38 and IJCNN1 are manually split into the training set and testing set by about 4 : 1. We normalize the data by scaling each feature to [0, 1]. To generate the datasets with noisy labels, we follow the settings in [26,19]. Specifically, we proportionally flip the class label of training data. For example, we randomly flip 20% of data labels from \u22121 to 1 or 1 to \u22121, and assume that the data has 20% of noisy labels. We then repeat the same process to produce 40% and 60% of noisy labels on all datasets.\nIn the experiments, the baseline methods are classified into two categories. The first category consists of SGD methods with different losses ranging from convex losses to robust nonconvex losses, which can verify the convergence and robustness of SGD methods with two representative losses for noisy labels. For example, we choose SGD with Logistic Loss (SGD(Log)), Hinge Loss (SGD(Hinge)) and Ramp Loss (SGD(Ramp)). We also choose ASGD [13] with Logistic Loss (ASGD(Log)) and PEGASOS [8] as baseline methods. For the second category, we compare proposed methods with LIBLINEAR [27] (We abbreviate L2-regularized L2-loss SVM Primal solution as LIBPrimal and Dual solution as LIBDual) due to its wide popularity in large-scale machine learning. All the methods are implemented in C++. Experiments are performed on\n2 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\na computer with a 3.20GHz Inter CPU and 8GB main memory running on a Windows 7.\nThe regularization parameter \u03bb is chosen by 10-fold cross validation for all methods in the range of {10\u22126, 10\u22125, 10\u22124, 10\u22123, 10\u22122, 10\u22121, 1, 10}. For SGD methods with different losses, the number of epochs is normally set to 15 for convergence comparison and the primal variable w is initialized to 0. For LIBLINEAR, we set the bias b to 1 and the stopping tolerance \u01eb to 10\u22122 for primal solution and 10\u22121 for dual solution by default. For PEGASOS, the number of epochs for convergence is set to 10\n\u03bb by default and the block size k is set to 1\nfor training efficiency. For SGD(SRamp), the parameter s\u2217 is chosen by 10-fold cross validation in the range of [\u22122, 0] according to real-world datasets. Therefore, the parameter (s\u2217, \u03b1s\u2217 , \u03b2s\u2217) is optimized to (\u22120.7, 3,\u22120.15), (\u22121, 2,\u22120.03) or (\u22122, 1.5, 0.5). For SGD(RGomp), the parameter c\u2217 is randomly fixed to 2. All the experiments are repeated ten times and the results are averaged over the 10 trials. Methods are indicated by \u201c-\u201din Table 2 due to running out of memory. Methods are not reported in Figures 4 and 5 due to running out of memory or too long training time.3"}, {"heading": "4.2 The Performance of Convergence", "text": "First, we verify the convergence of SGD methods with two representative losses for noisy labels. Due to the limit of space, we only provide the primal objective value of SGD(SRamp) with the number of epochs on representative small-scale IJCNN1 and large-scale SUSY datasets. The two datasets have varying percentages of noisy labels. Figure 2 shows the primal objective value of SGD(SRamp) with the number of epochs. We observe that SGD(SRamp) converges within 15 epochs. This observation is consistent with our convergence analysis in Section 3.3. Since SGD(SRamp) and SGD(RGomp) are very similar, the convergence curve of SGD(RGomp) is also similar to that of SGD(SRamp). Thus, we do not report the results of SGD(RGomp).\nThen, we further observe the convergence comparison of SGD methods with different losses for noisy labels in Figure 3 where, with the increase of number of epochs, the testing error rate of SGD(SRamp) and SGD(RGomp) not only\n3 On MNIST38 and SUSY datasets, PEGASOS run out of memory, and the training time of LIBDual is several orders of magnitude more than that of other baselines.\ndecrease faster than that of other baseline methods but also keep relative stable in the most cases. By contrast, the convergence performance of SGD(Ramp) is not as good as our methods due to its nonsmoothness and nonconvexity."}, {"heading": "4.3 The Performance of Robustness", "text": "Finally, we verify the robustness of SGD methods with two representative losses for noisy labels. Figures 4 and 5 respectively report testing error rate and variance with varying percentages of noisy labels. From Figures 4 and 5, we have the following observations. (a) On all datasets, SGD(SRamp) and SGD(RGomp) obviously outperform the other baseline methods in testing error rate beyond 40% of noisy labels. Between 0% to 40%, SGD(SRamp) and SGD(RGomp) still have comparative advantages. In particular, for a high-dimensional dataset REALSIM, the advantage of SGD(SRamp) and SGD(RGomp) is extremely obvious in the whole range of the x-axis. (b) Meanwhile, we notice that the variance of testing error rate for baseline methods (e.g., PEGASOS) gradually increases with the growing percentage of noisy labels, but the variance of testing error rate for SGD(SRamp) and SGD(RGomp) remains at the lowest level in the most cases. Therefore, the robustness of SGD(SRamp) and SGD(RGomp) have been validated by their testing error rate and variance.\nIn the most cases, the proposed SGD(SRamp) and SGD(RGomp) outperform other baseline methods not only on datasets with varying percentage of noisy labels but also on clean datasets. For example, Table 2 demonstrates that in terms of the testing error rate with the standard deviation, SGD(SRamp) and SGD(RGomp) outperform other baseline methods on IJCNN1, REAL-SIM, COVTYPE and SUSY datasets without noisy labels."}, {"heading": "5 Conclusions", "text": "This paper studies SGD methods with a family of robust losses for the label noise problem. For convenience, we mainly introduce two representative robust losses\nincluding Smooth Ramp Loss and Reversed Gompertz Loss. Our theoretical analysis not only reveals that the convergence rate is O(1/T ) for SGD methods using robust losses, but also proves the robustness of two representative robust losses. Comprehensive experimental results show that, on real-world datasets with varying percentages of noisy labels, SGD methods using our proposed losses are robust enough to reduce negative effects caused by noisy labels with fast convergence. In the future, we will extend our proposed robust losses to improve the performance of SGD methods for regression problems with noisy labels."}], "references": [{"title": "Better mini-batch algorithms via accelerated gradient methods", "author": ["A. Cotter", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "Advances in Neural Information Processing Systems (NIPS), pp. 1647\u20131655.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Memory limited, streaming pca", "author": ["I. Mitliagkas", "C. Caramanis", "P. Jain"], "venue": "Advances in Neural Information Processing Systems (NIPS), pp. 2886\u20132894.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "SIAM Journal on Optimization (SIAM) 22(2), 341\u2013362", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic Convex Optimization with Bandit Feedback", "author": ["A. Agarwal", "D.P. Foster", "D. Hsu", "S.M. Kakade", "A. Rakhlin"], "venue": "SIAM Journal on Optimization (SIAM) 23(1), 213\u2013240", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "PASSCoDe: Parallel ASynchronous Stochastic dual Co-ordinate Descent", "author": ["Hsieh", "C.-J.", "Yu", "H.-F.", "I.S. Dhillon"], "venue": "Proceedings of the 32th International Conference of Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-Scale Machine Learning with Stochastic Gradient Descent", "author": ["L. Bottou"], "venue": "Proceedings of the 19th International Conference on Computational Statistics (COMPSTAT), pp. 177\u2013187.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "On optimization methods for deep learning", "author": ["Q.V. Le", "J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "A.Y. Ng"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML), pp. 265\u2013272.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Pegasos: primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming 127(1), 3\u201330", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Modeling annotator expertise: Learning when everybody knows a bit of something", "author": ["Y. Yan", "R. Rosales", "G. Fung", "M. Schmidt", "G. Hermosillo", "L. Bogoni", "L. Moy", "Dy", "J.-G."], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS), pp. 932\u2013939.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to Predict from Crowdsourced Data", "author": ["W. Bi", "Wang", "L.-W.", "Kwok", "J.-T.", "Tu", "Z.-W."], "venue": "Proceedings of the 13th Conference on Uncertainty in Artificial Intelligence (UAI), pp. 82\u201391.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Trading convexity for scalability", "author": ["R. Collobert", "F. Sinz", "J. Weston", "L. Bottou"], "venue": "Proceedings of the 23rd International Conference on Machine Learning (ICML), pp. 201\u2013208.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Relaxed Clipping: A Global Training Method for Robust Regression and Classification", "author": ["Yu", "Y.-L.", "M. Yang", "Xu", "L.-L", "M. White", "D. Schuurmans"], "venue": "Advances in Neural Information Processing Systems (NIPS), pp. 2532\u20132540.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent", "author": ["W. Xu"], "venue": "arXiv preprint arXiv:1107.2490.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "author": ["S. Ghadimi", "Lan", "G.-H."], "venue": "SIAM Journal on Optimization (SIAM) 23(4), 2341\u20132368", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerated gradient methods for nonconvex nonlinear and stochastic programming", "author": ["S. Ghadimi", "Lan", "G.-H."], "venue": "Mathematical Programming, 1\u201341", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Training robust support vector machine with smooth ramp loss in the primal space", "author": ["L. Wang", "Jia", "H.-D.", "J. Li"], "venue": "Neurocomputing, 71(13), 3020\u20133025", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping", "author": ["S. Reed", "H. Lee", "D. Anguelov", "C. Szegedy", "D. Erhan", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1412.6596.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Training convolution networks with noisy labels", "author": ["S. Sukhbaatar", "J. Bruna", "M. Paluri", "L. Bourdev", "R. Fergus"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR).", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning with noisy labels", "author": ["N. Natarajan", "I.S. Dhillon", "P.K. Ravikumar", "A. Tewari"], "venue": "Advances in Neural Information Processing Systems (NIPS), pp. 1196\u2013 1204.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Convexity, classification, and risk bounds", "author": ["P.L. Bartlett", "M.I. Jordan", "J.D. McAuliffe"], "venue": "Journal of the American Statistical Association 101(473), 138\u2013156", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast global convergence of gradient methods for high-dimensional statistical recovery", "author": ["A. Agarwal", "S. Negahban", "M.J. Wainwright"], "venue": "The Annals of Statistics 40(5), 2452\u20132482", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Regularized M-estimators with Nonconvexity: Statistical and Algorithmic Theory for Local Optima", "author": ["Loh", "P.-L.", "M.J. Wainwright"], "venue": "Journal of Machine Learning Research (JMLR) 16, 559\u2013616", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximum-likelihood estimation of the parameters of the Gompertz survival function", "author": ["M.L. Garg", "R.B. Rao", "C.K. Redmond"], "venue": "Journal of the Royal Statistical Society. Series C (Applied Statistics) 19(2), 152\u2013159", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1970}, {"title": "Training and Testing Low-degree Polynomial Data Mappings via Linear SVM", "author": ["Chang", "Y.-W.", "Hsieh", "C.-J.", "Chang", "K.-W.", "M. Ringgaard", "Lin", "C.-J."], "venue": "Journal of Machine Learning Research (JMLR) 11, 1471\u20131490", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "C.-C.", "Lin", "C.-J."], "venue": "ACM Transactions on Intelligent Systems and Technology 2(3), 27", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple Kernel Learning from Noisy Labels by Stochastic Programming", "author": ["Yang", "T.-B.", "M. Mahdavi", "R. Jin", "Zhang", "L.-j.", "Y. Zhou"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML), pp. 233\u2013240.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "LIBLINEAR: A Library for Large Linear Classification", "author": ["Fan", "R.-E.", "Chang K.-W.", "Hsieh", "C.-J.", "Wang", "X.-R.", "Lin", "C.-J."], "venue": "Journal of Machine Learning Research (JMLR) 9, 1871\u20131874", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Secondly, they only process a mini-batch of data points [1] or even one data point [2] in each iteration, which vastly reduces the memory storage.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Secondly, they only process a mini-batch of data points [1] or even one data point [2] in each iteration, which vastly reduces the memory storage.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Therefore, many researchers have extensively studied and applied various SGD methods [3,4,5].", "startOffset": 85, "endOffset": 92}, {"referenceID": 3, "context": "Therefore, many researchers have extensively studied and applied various SGD methods [3,4,5].", "startOffset": 85, "endOffset": 92}, {"referenceID": 4, "context": "Therefore, many researchers have extensively studied and applied various SGD methods [3,4,5].", "startOffset": 85, "endOffset": 92}, {"referenceID": 5, "context": "For instance, Large-Scale SGD [6] has been substantially applied to the optimization of deep learning models [7].", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": "For instance, Large-Scale SGD [6] has been substantially applied to the optimization of deep learning models [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "Primal Estimated Sub-Gradient Solver (Pegasos) [8] is employed to speed up the Support Vector Machines (SVM) methods, which is suitable for large-scale text classification problems.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "However, the quality of annotations is not always satisfactory because many workers are not sufficiently trained to label or annotate such specific data [9,10].", "startOffset": 153, "endOffset": 159}, {"referenceID": 9, "context": "However, the quality of annotations is not always satisfactory because many workers are not sufficiently trained to label or annotate such specific data [9,10].", "startOffset": 153, "endOffset": 159}, {"referenceID": 10, "context": "However, if the loss function has a threshold, for example Ramp Loss [11] in Figure 1 with a threshold 1\u2212 s, the gradient of Ramp Loss on the zA is zero, which minimizes the negative effects caused by this outlier on the update.", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Although the Ramp Loss is robust to outliers, it is computationally hard to optimize due to its nonsmoothness and nonconvexity [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 12, "context": "For example, Xu proposes the Averaged Stochastic Gradient Descent (ASGD) method [13] to lower the testing error rate of the SGD [6].", "startOffset": 80, "endOffset": 84}, {"referenceID": 5, "context": "For example, Xu proposes the Averaged Stochastic Gradient Descent (ASGD) method [13] to lower the testing error rate of the SGD [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 13, "context": "Ghahdimi & Lan introduce a randomized stochastic algorithm to solve nonconvex problems [14], and then generalize the accelerated gradient method to improve the convergence rate if the problem is nonconvex [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "Ghahdimi & Lan introduce a randomized stochastic algorithm to solve nonconvex problems [14], and then generalize the accelerated gradient method to improve the convergence rate if the problem is nonconvex [15].", "startOffset": 205, "endOffset": 209}, {"referenceID": 15, "context": "further propose a robust SVM based on a smooth version of Ramp Loss for suppressing the outliers [16].", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "Their models are commonly inferred by Concave-Convex Procedure (CCCP) [11].", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "For instance, Reed & Sukhbaatar focus on training deep neural networks using noisy labels [17,18].", "startOffset": 90, "endOffset": 97}, {"referenceID": 17, "context": "For instance, Reed & Sukhbaatar focus on training deep neural networks using noisy labels [17,18].", "startOffset": 90, "endOffset": 97}, {"referenceID": 18, "context": "propose a probabilistic model for handling label noise problems [19].", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "Even though the surrogates of 0-1 loss is convex [20], they are very sensitive to outliers.", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "Based on Restricted Strong Convexity (RSC) and Restricted Smoothness (RSM) [21,22], we propose two extended definitions.", "startOffset": 75, "endOffset": 82}, {"referenceID": 21, "context": "Based on Restricted Strong Convexity (RSC) and Restricted Smoothness (RSM) [21,22], we propose two extended definitions.", "startOffset": 75, "endOffset": 82}, {"referenceID": 10, "context": "Inspired by Ramp Loss [11], we consider whether we can design a family of bounded, locally stronglyconvex and smooth losses.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "The second one is the Reversed Gompertz Loss, which is a special case of the Gompertz function [23] and we reverse the Gompertz function by the y-axis.", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "The datasets [24] range from small to large scale.", "startOffset": 13, "endOffset": 17}, {"referenceID": 24, "context": "1 Experimental Settings All experimental datasets come from the LIBSVM datasets webpage [25] .", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "We normalize the data by scaling each feature to [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 25, "context": "To generate the datasets with noisy labels, we follow the settings in [26,19].", "startOffset": 70, "endOffset": 77}, {"referenceID": 18, "context": "To generate the datasets with noisy labels, we follow the settings in [26,19].", "startOffset": 70, "endOffset": 77}, {"referenceID": 12, "context": "We also choose ASGD [13] with Logistic Loss (ASGD(Log)) and PEGASOS [8] as baseline methods.", "startOffset": 20, "endOffset": 24}, {"referenceID": 7, "context": "We also choose ASGD [13] with Logistic Loss (ASGD(Log)) and PEGASOS [8] as baseline methods.", "startOffset": 68, "endOffset": 71}, {"referenceID": 26, "context": "For the second category, we compare proposed methods with LIBLINEAR [27] (We abbreviate L2-regularized L2-loss SVM Primal solution as LIBPrimal and Dual solution as LIBDual) due to its wide popularity in large-scale machine learning.", "startOffset": 68, "endOffset": 72}], "year": 2016, "abstractText": "The convergence of Stochastic Gradient Descent (SGD) using convex loss functions has been widely studied. However, vanilla SGD methods using convex losses cannot perform well with noisy labels, which adversely affect the update of the primal variable in SGD methods. Unfortunately, noisy labels are ubiquitous in real world applications such as crowdsourcing. To handle noisy labels, in this paper, we present a family of robust losses for SGD methods. By employing our robust losses, SGD methods successfully reduce negative effects caused by noisy labels on each update of the primal variable. We not only reveal that the convergence rate is O(1/T ) for SGD methods using robust losses, but also provide the robustness analysis on two representative robust losses. Comprehensive experimental results on six real-world datasets show that SGD methods using robust losses are obviously more robust than other baseline methods in most situations with fast convergence.", "creator": "LaTeX with hyperref package"}}}