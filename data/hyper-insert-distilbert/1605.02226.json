{"id": "1605.02226", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2016", "title": "Neural Autoregressive Distribution Estimation", "abstract": "we immediately present neural autoregressive distribution estimation ( linear nade ) standard models, materials which structurally are abstract neu - weighted ral network architectures applied to the design problem of unsupervised distribution and multiple density data esitmation. they intentionally leverage the inherent probability growth product rule and a balanced weight sharing scheme defined in - tool spired from restricted optimal boltzmann machines, to yield precisely an intermediate estimator that is inherently both averaging tractable, and then has good approximation generalization linear performance. currently we discuss how technically they achieve a competitive per - formance complexity in graphical modeling both binary and real - sum valued observations. formally we also present details how deep nade measure models thereby can cannot be trained out to inherently be agnostic to the implicit ordering of input dimensions which used by avoiding the autoregressive cumulative product rule decomposition. but finally, furthermore we currently also show how to exploit the simpler topological structure of weighted pixels in images using a robust deep convolutional architecture modeled for nade.", "histories": [["v1", "Sat, 7 May 2016 18:13:25 GMT  (3523kb,D)", "https://arxiv.org/abs/1605.02226v1", null], ["v2", "Wed, 11 May 2016 12:00:17 GMT  (3523kb,D)", "http://arxiv.org/abs/1605.02226v2", null], ["v3", "Fri, 27 May 2016 14:25:41 GMT  (3518kb,D)", "http://arxiv.org/abs/1605.02226v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["benigno uria", "marc-alexandre c\\^ot\\'e", "karol gregor", "iain murray", "hugo larochelle"], "accepted": false, "id": "1605.02226"}, "pdf": {"name": "1605.02226.pdf", "metadata": {"source": "CRF", "title": "Neural Autoregressive Distribution Estimation", "authors": ["Benigno Uria", "Iain Murray"], "emails": ["benigno.uria@gmail.com", "marc-alexandre.cote@usherbrooke.ca", "karol.gregor@gmail.com", "i.murray@ed.ac.uk", "hlarochelle@twitter.com"], "sections": [{"heading": null, "text": "Keywords: Deep Learning, Neural Networks, Density Modeling, Unsupervised Learning"}, {"heading": "1. Introduction", "text": "Distribution estimation is one of the most general problems addressed by machine learning. From a good and flexible distribution estimator, in principle it is possible to solve a variety of types of inference problem, such as classification, regression, missing value imputation, and many other predictive tasks.\nCurrently, one of the most common forms of distribution estimation is based on directed graphical models. In general these models describe the data generation process as sampling a latent state h from some prior p(h), followed by sampling the observed data x from some\nc\u00a92000 Benigno Uria, Marc-Alexandre Co\u0302te\u0301, Karol Gregor, Iain Murray, Hugo Larochelle.\nar X\niv :1\n60 5.\n02 22\nconditional p(x |h). Unfortunately, this approach quickly becomes intractable and requires approximations when the latent state h increases in complexity. Specifically, computing the marginal probability of the data, p(x) = \u2211 h p(x |h) p(h), is only tractable under fairly constraining assumptions on p(x |h) and p(h). Another popular approach, based on undirected graphical models, gives probabilities of the form p(x) = exp {\u03c6(x)} /Z, where \u03c6 is a tractable function and Z is a normalizing constant. A popular choice for such a model is the restricted Boltzmann machine (RBM), which substantially out-performs mixture models on a variety of binary datasets (Salakhutdinov and Murray, 2008). Unfortunately, we often cannot compute probabilities p(x) exactly in undirected models either, due to the normalizing constant Z.\nIn this paper, we advocate a third approach to distribution estimation, based on autoregressive models and feed-forward neural networks. We refer to our particular approach as Neural Autoregressive Distribution Estimation (NADE). Its main distinguishing property is that computing p(x) under a NADE model is tractable and can be computed efficiently, given an arbitrary ordering of the dimensions of x. We show that the framework is flexible and can model both binary and real-valued observations, can be made order-agnostic, and can be adapted to the case of 2D images using convolutional neural networks. In each case, we\u2019re able to reach competitive results, compared to popular directed and undirected graphical model alternatives."}, {"heading": "2. NADE", "text": "We consider the problem of modeling the distribution p(x) of input vector observations x. For now, we will assume that the dimensions of x are binary, that is xd \u2208 {0, 1} \u2200d. The model generalizes to other data types, which is explored later (Section 3) and in other work (Section 8).\nNADE begins with the observation that any D-dimensional distribution p(x) can be factored into a product of one-dimensional distributions, in any order o (a permutation of the integers 1, . . . , D):\np(x) = D\u220f d=1 p(xod |xo<d). (1)\nHere o<d contains the first d \u2212 1 dimensions in ordering o and xo<d is the corresponding subvector for these dimensions. Thus, one can define an \u2018autoregressive\u2019 generative model of the data simply by specifying a parameterization of all D conditionals p(xod |xo<d).\nFrey et al. (1996) followed this approach and proposed using simple (log-)linear logistic regression models for these conditionals. This choice yields surprisingly competitive results, but are not competitive with non-linear models such as an RBM. Bengio and Bengio (2000) proposed a more flexible approach, with a single-layer feed-forward neural network for each conditional. Moreover, they allowed connections between the output of each network and the hidden layer of networks for the conditionals appearing earlier in the autoregressive ordering. Using neural networks led to some improvements in modeling performance, though at the cost of a really large model for very high-dimensional data.\nIn NADE, we also model each conditional using a feed-forward neural network. Specifically, each conditional p(xod |x<d) is parameterized as follows:\np(xod =1 |xo<d) = sigm (V od,\u00b7hd + bod) (2) hd = sigm ( W \u00b7,o<dxo<d + c ) , (3)\nwhere sigm (a) = 1/(1 + e\u2212a) is the logistic sigmoid, and with H as the number of hidden units, V \u2208 RD\u00d7H , b \u2208 RD, W \u2208 RH\u00d7D, c \u2208 RH are the parameters of the NADE model.\nThe hidden layer matrix W and bias c are shared by each hidden layer hd (which are all of the same size). This parameter sharing scheme (illustrated in Figure 1) means that NADE has O(HD) parameters, rather than O(HD2) required if the neural networks were separate. Limiting the number of parameters can reduce the risk of over-fitting. Another advantage is that all D hidden layers hd can be computed in O(HD) time instead of O(HD\n2). Denoting the pre-activation of the dth hidden layer as ad = W \u00b7,o<dxo<d +c, this complexity is achieved by using the recurrence\nh1 = sigm (a1) , where a1 = c (4)\nhd = sigm (ad) , where ad = W \u00b7,o<dxo<d + c = W \u00b7,od\u22121xod\u22121 + ad\u22121 (5)\nfor d \u2208 {2, . . . , D},\nwhere Equation 5 given vector ad\u22121 can be computed in O(H). Moreover, the computation of Equation 2 given h is also O(H). Thus, computing p(x) from D conditional distributions (Equation 1) costs O(HD) for NADE. This complexity is comparable to that of regular feed-forward neural network models.\nAlgorithm 1 Computation of p(x) and learning gradients for NADE.\nInput: training observation vector x and ordering o of the input dimensions. Output: p(x) and gradients of \u2212 log p(x) on parameters.\n# Computing p(x) a1 \u2190 c p(x)\u2190 1 for d from 1 to D do hd \u2190 sigm (ad) p(xod =1 |xo<d)\u2190 sigm (V od,\u00b7hd + bod) p(x)\u2190 p(x) ( p(xod =1 |xo<d)xod + (1\u2212 p(xod =1 |xo<d))1\u2212xod\n) ad+1 \u2190 ad +W \u00b7,odxod\nend for\n# Computing gradients of \u2212 log p(x) \u03b4aD \u2190 0 \u03b4c\u2190 0 for d from D to 1 do\n\u03b4bod \u2190 ( p(xod =1 |xo<d)\u2212 xod ) \u03b4V od,\u00b7 \u2190 ( p(xod =1 |xo<d)\u2212 xod ) h>d\n\u03b4hd \u2190 ( p(xod =1 |xo<d)\u2212 xod ) V >od,\u00b7 \u03b4c\u2190 \u03b4c+ \u03b4hd hd (1\u2212 hd) \u03b4W \u00b7,od \u2190 \u03b4adxod \u03b4ad\u22121 \u2190 \u03b4ad + \u03b4hd hd (1\u2212 hd)\nend for return p(x), \u03b4b, \u03b4V , \u03b4c, \u03b4W\nNADE can be trained by maximum likelihood, or equivalently by minimizing the average negative log-likelihood,\n1\nN N\u2211 n=1 \u2212 log p(x(n)) = 1 N N\u2211 n=1 D\u2211 d=1 \u2212 log p(x(n)od |x (n) o<d ), (6)\nusually by stochastic (minibatch) gradient descent. As probabilities p(x) cost O(HD), gradients of the negative log-probability of training examples can also be computed in O(HD). Algorithm 1 describes the computation of both p(x) and the gradients of \u2212 log p(x) with respect to NADE\u2019s parameters."}, {"heading": "2.1 Relationship with the RBM", "text": "The proposed weight-tying for NADE isn\u2019t simply motivated by computational reasons. It also reflects the computations of approximation inference in the RBM.\nDenoting the energy function and distribution under an RBM as\nE(x,h) = \u2212h>Wx\u2212 b>x\u2212 c>h (7) p(x,h) = exp {\u2212E(x,h)} /Z , (8)\ncomputing all conditionals p(xod |xo<d) = \u2211\nxo>d\u2208{0,1}D\u2212d\n\u2211 h\u2208{0,1}H exp {\u2212E(x,h)} /Z(xo<d) (9)\nZ(xo<d) = \u2211\nxo\u2265d\u2208{0,1}D\u2212d+1\n\u2211 h\u2208{0,1}H exp {\u2212E(x,h)} (10)\nis intractable. However, these could be approximated using mean-field variational inference. Specifically, consider the conditional over xod , xo>d and h instead:\np(xod ,xo>d ,h |xo<d) = exp {\u2212E(x,h)} /Z(xo<d). (11)\nA mean-field approach could first approximate this conditional with a factorized distribution q(xod ,xo>d ,h |xo<d) = \u00b5i(d)xod (1\u2212 \u00b5d(d))1\u2212xod \u220f j>d\n\u00b5j(d) xoj (1\u2212 \u00b5j(d))1\u2212xoj\u220f\nk\n\u03c4k(d) hk(1\u2212 \u03c4k(d))1\u2212hk , (12)\nwhere \u00b5j(d) is the marginal probability of xoj being equal to 1, given xo<d . Similarly, \u03c4k(d) is the marginal for hidden variable hk. The dependence on d comes from conditioning on xo<d , that is on the first d\u22121 dimensions of x in ordering o.\nFor some d, a mean-field approximation is obtained by finding the parameters \u00b5j(d) for j \u2208 {d, . . . ,D} and \u03c4k(d) for k \u2208 {1, . . . ,H} which minimize the KL divergence between q(xod ,xo>d ,h |xo<d) and p(xod ,xo>d ,h |xo<d). This is usually done by finding message passing updates that each set the derivatives of the KL divergence to 0 for some of the parameters of q(xod ,xo>d ,h |xo<d) given others.\nFor some d, let us fix \u00b5j(d) = xod for j < d, leaving only \u00b5j(d) for j > d to be found. The KL-divergence develops as follows:\nKL(q(xod ,xo>d ,h |xo<d) || p(xod ,xo>d ,h |xo<d)) = \u2212 \u2211 xod ,xo>d ,h q(xod ,xo>d ,h |xo<d) log p(xod ,xo>d ,h |xo<d)\n+ \u2211\nxod ,xo>d ,h\nq(xod ,xo>d ,h |xo<d) log q(xod ,xo>d ,h |xo<d)\n= logZ(xo<d)\u2212 \u2211 j \u2211 k \u03c4k(d)Wk,oj\u00b5j(d)\u2212 \u2211 j boj\u00b5j(d)\u2212 \u2211 k ck\u03c4k(d)\n+ \u2211 j\u2265d (\u00b5j(d) log\u00b5j(d) + (1\u2212 \u00b5j(d)) log(1\u2212 \u00b5j(d)))\n+ \u2211 k (\u03c4k(d) log \u03c4k(d) + (1\u2212 \u03c4k(d)) log(1\u2212 \u03c4k(d))) .\nThen, we can take the derivative with respect to \u03c4k(d) and set it to 0, to obtain:\n0 = \u2202KL(q(xod ,xo>d ,h |xo<d) || p(xod ,xo>d ,h |xo<d))\n\u2202\u03c4k(d) 0 = \u2212ck \u2212 \u2211 j Wk,oj\u00b5j(d) + log ( \u03c4k(d) 1\u2212 \u03c4k(d) ) \u03c4k(d)\n1\u2212 \u03c4k(d) = exp ck +\u2211 j Wk,oj\u00b5j(d)  (13) \u03c4k(d) = exp { ck + \u2211 jWk,oj\u00b5j(d) } 1 + exp { ck + \u2211 jWk,oj\u00b5j(d)\n} \u03c4k(d) = sigm\nck +\u2211 j\u2265d Wk,oj\u00b5j(d) + \u2211 j<d Wk,ojxoj  . (14) where in the last step we have used the fact that \u00b5j(d) = xoj for j < d. Equation 14 would correspond to the message passing updates of the hidden unit marginals \u03c4k(d) given the marginals of input \u00b5j(d).\nSimilarly, we can set the derivative with respect to \u00b5j(d) for j \u2265 d to 0 and obtain:\n0 = \u2202KL(q(xod ,xo>d ,h |xo<d) || p(xod ,xo>d ,h |xo<d))\n\u2202\u00b5j(d) 0 = \u2212bod \u2212 \u2211 k \u03c4k(d)Wk,oj + log ( \u00b5j(d) 1\u2212 \u00b5j(d) ) \u00b5j(d)\n1\u2212 \u00b5j(d) = exp\n{ boj + \u2211 k \u03c4k(d)Wk,oj }\n\u00b5j(d) = exp\n{ boj + \u2211 k \u03c4k(d)Wk,oj } 1 + exp { boj + \u2211 k \u03c4k(d)Wk,oj\n} \u00b5j(d) = sigm ( boj +\n\u2211 k \u03c4k(d)Wk,oj\n) . (15)\nEquation 15 would correspond to the message passing updates of the input marginals \u00b5j(d) given the hidden layer marginals \u03c4k(d). The complete mean-field algorithm would thus alternate between applying the updates of Equations 14 and 15, right to left.\nWe now notice that Equation 14 corresponds to NADE\u2019s hidden layer computation (Equation 3) where \u00b5j(d) = 0 \u2200j \u2265 d. Also, Equation 15 corresponds to NADE\u2019s output layer computation (Equation 2) where j = d, \u03c4k(d) = hd,k and W\n> = V . Thus, in short, NADE\u2019s forward pass is equivalent to applying a single pass of mean-field inference to approximate all the conditionals p(xod |xo<d) of an RBM, where initially \u00b5j(d) = 0 and where a separate matrix V is used for the hidden-to-input messages. A generalization of NADE based on this connection to mean field inference has been further explored by Raiko et al. (2014)."}, {"heading": "3. NADE for non-binary observations", "text": "So far we have only considered the case of binary observations xi. However, the framework of NADE naturally extends to distributions over other types of observations.\nIn the next section, we discuss the case of real-valued observations, which is one of the most general cases of non-binary observations and provides an illustrative example of the technical considerations one faces when extending NADE to new observations."}, {"heading": "3.1 RNADE: Real-valued NADE", "text": "A NADE model for real-valued data could be obtained by applying the derivations shown in Section 2.1 to the Gaussian-RBM (Welling et al., 2005). The resulting neural network would output the mean of a Gaussian with fixed variance for each of the conditionals in Equation 1. Such a model is not competitive with mixture models, for example on perceptual datasets (Uria, 2015). However, we can explore alternative models by making the neural network for each conditional distribution output the parameters of a distribution that\u2019s not a fixed-variance Gaussian.\nIn particular, a mixture of one-dimensional Gaussians for each autoregressive conditional provides a flexible model. Given enough components, a mixture of Gaussians can model any continuous distribution to arbitrary precision. The resulting model can be interpreted as a sequence of mixture density networks (Bishop, 1994) with shared parameters. We call this model RNADE-MoG. In RNADE-MoG, each of the conditionals is modeled by a mixture of Gaussians:\np(xod |xo<d) = C\u2211 c=1 \u03c0od,c N (xod ; \u00b5od,c, \u03c32od,c), (16)\nwhere the parameters are set by the outputs of a neural network:\n\u03c0od,c = exp\n{ z\n(\u03c0) od,c } \u2211C\nc=1 exp { z (\u03c0) od,c } (17) \u00b5od,c =z (\u00b5) od,c (18)\n\u03c3od,c = exp { z(\u03c3)od,c } (19)\nz(\u03c0)od,c =b (\u03c0) od,c + H\u2211 k=1 V (\u03c0) od,k,c hd,k (20)\nz(\u00b5)od,c =b (\u00b5) od,c + H\u2211 k=1 V (\u00b5) od,k,c hd,k (21)\nz(\u03c3)od,c =b (\u03c3) od,c + H\u2211 k=1 V (\u03c3) od,k,c hd,k (22)\nParameter sharing conveys the same computational and statistical advantages as it does in the binary NADE.\nDifferent one dimensional conditional forms may be preferred, for example due to limited dataset size or domain knowledge about the form of the conditional distributions. Other choices, like single variable-variance Gaussians, sinh-arcsinh distributions, and mixtures of Laplace distributions, have been examined by Uria (2015).\nTraining an RNADE can still be done by stochastic gradient descent on the parameters of the model with respect to the negative log-density of the training set. It was found empirically (Uria et al., 2013) that stochastic gradient descent leads to better parameter\nconfigurations when the gradient of the mean (\n\u2202J \u2202\u00b5od,c\n) was multiplied by the standard\ndeviation (\u03c3od,c)."}, {"heading": "4. Orderless and Deep NADE", "text": "The fixed ordering of the variables in a NADE model makes the exact calculation of arbitrary conditional probabilities computationally intractable. Only a small subset of conditional distributions, those where the conditioned variables are at the beginning of the ordering and marginalized variables at the end, are computationally tractable.\nAnother limitation of NADE is that a naive extension to a deep version, with multiple layers of hidden units, is computationally expensive. Deep neural networks (Bengio, 2009; LeCun et al., 2015) are at the core of state-of-the-art models for supervised tasks like image recognition (Krizhevsky et al., 2012) and speech recognition (Dahl et al., 2013). The same inductive bias should also provide better unsupervised models. However, extending the NADE framework to network architectures with several hidden layers, by introducing extra non-linear calculations between Equations (3) and (2), increases its complexity to cubic in the number of units per layer. Specifically, the cost becomes O(DH2L), where L stands for the number of hidden layers and can be assumed to be a small constant, D is the number of variables modeled, and H is the number of hidden units, which we assumed to be of the same order as D. This increase in complexity is caused by no longer being able to share hidden layer computations across the conditionals in Equation 1, after the non-linearity in the first layer.\nIn this section we introduce an order-agnostic training procedure, DeepNADE, which will address both of the issues above. This procedure trains a single deep neural network that can assign a conditional distribution to any variable given any subset of the others. This network can then provide the conditionals in Equation 1 for any ordering of the input observations. Therefore, the network defines a factorial number of different models with shared parameters, one for each of the D! orderings of the inputs. At test time, given an inference task, the most convenient ordering of variables can be used. The models for different orderings will not be consistent with each other: they will assign different probabilities to a given test vector. However, we can use the models\u2019 differences to our advantage by creating ensembles of NADE models (Section 4.1), which results in better estimators than any single NADE. Moreover, the training complexity of our procedure increases linearly with the number of hidden layers O(H2L), while remaining quadratic in the size of the network\u2019s layers.\nWe first describe the model for an L-layer neural network modeling binary variables. A conditional distribution is obtained directly from a hidden unit in the final layer:\np(xod =1 |xo<d ,\u03b8, o<d, od) = h(L)od . (23)\nThis hidden unit is computed from previous layers, all of which can only depend on the xo<d variables that are currently being conditioned on. We remove the other variables from the computation using a binary mask,\nmo<d = [11\u2208o<d , 12\u2208o<d , . . . , 1D\u2208o<d ], (24)\nwhich is element-wise multiplied with the inputs before computing the remaining layers as in a standard neural network:\nh(0) = x mo<d (25) a(`) = W (`)h(`\u22121) + b(`) (26)\nh(`) = \u03c3 ( a(`) ) (27)\nh(L) = sigm ( a(L) ) . (28)\nThe network is specified by a free choice of the activation function \u03c3 (\u00b7), and learnable parameters W (`) \u2208 RH(`)\u00d7H(`\u22121) and b(`) \u2208 RH(`) , where H(l) is the number of units in the `-th layer. As layer zero is the masked input, H(0) = D. The final L-th layer needs to be able to provide predictions for any element (Equation 23) and so also has D units.\nTo train a DeepNADE, the ordering of the variables is treated as a stochastic variable with a uniform distribution. Moreover, since we wish DeepNADE to provide good predictions for any ordering, we optimize the expected likelihood over the ordering of variables:\nJ (\u03b8) = E o\u2208D! \u2212 log p(X |\u03b8, o) \u221d E o\u2208D! E x\u2208X \u2212 log p(x |\u03b8, o), (29)\nwhere we\u2019ve made the dependence on the ordering o and the network\u2019s parameters \u03b8 explicit, D! stands for the set of all orderings (the permutations of D elements) and x is a uniformly sampled datapoint from the training set X . Using NADE\u2019s expression for the density of a datapoint in Equation 1 we have\nJ (\u03b8) = E o\u2208D! E x\u2208X D\u2211 d=1 \u2212 log p(xod |xo<d ,\u03b8, o), (30)\nwhere d indexes the elements in the ordering, o, of the variables. By moving the expectation over orderings inside the sum over the elements of the ordering, the ordering can be split in three parts: o<d (the indices of the d\u2212 1 first dimensions in the ordering), od (the index of the d-th variable) and o>d (the indices of the remaining dimensions). Therefore, the loss function can be rewritten as:\nJ (\u03b8) = E x\u2208X D\u2211 d=1 E o<d E od E o>d \u2212 log p(xod |xo<d ,\u03b8, o<d, od, o>d). (31)\nThe value of each of these terms does not depend on o>d. Therefore, it can be simplified as:\nJ (\u03b8) = E x\u2208X D\u2211 d=1 E o<d E od \u2212 log p(xod |xo<d ,\u03b8, o<d, od). (32)\nIn practice, this loss function will have a very high number of terms and will have to be approximated by sampling x, d and o<d. The innermost expectation over values of od can be calculated cheaply, because all of the neural network computations depend only on the masked input xo<d , and can be reused for each possible od. Assuming all orderings are equally probable, we will estimate J (\u03b8) by:\nJ\u0302 (\u03b8) = D D \u2212 d+ 1 \u2211 od \u2212 log p(xod |xo<d ,\u03b8, o<d, od), (33)\nwhich is an unbiased estimator of Equation 29. Therefore, training can be done by descent on the gradient of J\u0302 (\u03b8).\nFor binary observations, we use the cross-entropy scaled by a factor of DD\u2212d+1 as the\ntraining loss which corresponds to minimizing J\u0302 :\nJ (x) = D D \u2212 d+ 1 m > o\u2265d\n( x log ( h(L) ) + (1\u2212 x) log ( 1\u2212 h(L) )) . (34)\nDifferentiating this cost involves backpropagating the gradients of the cross-entropy only from the outputs in o\u2265d and rescaling them by D D\u2212d+1 .\nThe resulting training procedure resembles that of a denoising autoencoder (Vincent et al., 2008). Like the autoencoder, D outputs are used to predict D inputs corrupted by a random masking process (mo<d in Equation 25). A single forward pass can compute h (L) o\u2265d , which provides a prediction p(xod = 1 |xo<d ,\u03b8, o<d, od) for every masked variable, which could be used next in an ordering starting with o<d. Unlike the autoencoder, the outputs for variables corresponding to those provided in the input (not masked out) are ignored.\nIn this order-agnostic framework, missing variables and zero-valued observations are indistinguishable by the network. This shortcoming can be alleviated by concatenating the inputs to the network (masked variables x mo<d) with the mask mo<d . Therefore we advise substituting the input described in Equation 25 with\nh(0) = concat(x mo<d ,mo<d). (35)\nWe found this modification to be important in order to obtain competitive statistical performance (see Table 3). The resulting neural network is illustrated in Figure 2."}, {"heading": "4.1 Ensembles of NADE models", "text": "As mentioned, the DeepNADE parameter fitting procedure effectively produces a factorial number of different NADE models, one for each ordering of the variables. These models will not, in general, assign the same probability to any particular datapoint. This disagreement is undesirable if we require consistent inferences for different inference problems, as it will preclude the use of the most convenient ordering of variables for each inference task.\nHowever, it is possible to use this variability across the different orderings to our advantage by combining several models. A usual approach to improve on a particular estimator is to construct an ensemble of multiple, strong but different estimators, e.g. using bagging (Ormoneit and Tresp, 1995) or stacking (Smyth and Wolpert, 1999). The DeepNADE training procedure suggests a way of generating ensembles of NADE models:\ntake a set of uniformly distributed orderings {o(k)}Kk=1 over the input variables and use the average probability 1K \u2211K k=1 p(x | \u03b8, o(k)) as an estimator.\nThe use of an ensemble increases the test-time cost of density estimation linearly with the number of orderings used. The complexity of sampling does not change however: after one of the K orderings is chosen at random, the single corresponding NADE is sampled. Importantly, the cost of training also remains the same, unlike other ensemble methods such as bagging. Furthermore, the number of components can be chosen after training and even adapted to a computational budget on the fly."}, {"heading": "5. ConvNADE: Convolutional NADE", "text": "One drawback of NADE (and its variants so far) is the lack of a mechanism for truly exploiting the high-dimensional structure of the data. For example, when using NADE on binarized MNIST, we first need to flatten the 2D images before providing them to the model as a vector. As the spatial topology is not provided to the network, it can\u2019t use this information to share parameters and may learn less quickly.\nRecently, convolutional neural networks (CNN) have achieved state-of-the-art performance on many supervised tasks related to images Krizhevsky et al. (2012). Briefly, CNNs are composed of convolutional layers, each one having multiple learnable filters. The outputs of a convolutional layer are feature maps and are obtained by the convolution on the input image (or previous feature maps) of a linear filter, followed by the addition of a bias and the application of a non-linear activation function. Thanks to the convolution, spatial\nstructure in the input is preserved and can be exploited. Moreover, as per the definition of a convolution the same filter is reused across all sub-regions of the entire image (or previous feature maps), yielding a parameter sharing that is natural and sensible for images.\nThe success of CNNs raises the question: can we exploit the spatial topology of the inputs while keeping NADE\u2019s autoregressive property? It turns out we can, simply by replacing the fully connected hidden layers of a DeepNADE model with convolutional layers. We thus refer to this variant as Convolutional NADE (ConvNADE).\nFirst we establish some notation that we will use throughout this section. Without loss of generality, let the input X \u2208 {0, 1}NX\u00d7NX be a square binary image of size NX and every convolution filter W\n(`) ij \u2208 RN (`) W \u00d7N (`) W connecting two feature maps H (`\u22121) i and H (`) j also\nbe square with their size N (`) W varying for each layer `. We also define the following mask M o<d \u2208 {0, 1}NX\u00d7NX , which is 1 for the locations of the first d\u2212 1 pixels in the ordering o. Formally, Equation 26 is modified to use convolutions instead of dot products. Specifically for an L-layer convolutional neural network that preserves the input shape (explained below) we have\np(xod =1 |xo<d ,\u03b8, o<d, od) = vec ( H (L) 1 ) od , (36)\nwith\nH (0) 1 = X M o<d (37)\nA (`) j = b (`) j + H(`\u22121)\u2211 i=1 H (`\u22121) i ~W (`) ij (38)\nH (`) j = \u03c3\n( A\n(`) j\n) (39)\nH (L) j = sigm\n( A\n(L) j\n) , (40)\nwhere H(`) is the number of feature maps output by the `-th layer and b(l) \u2208 RH(l) , W (`) \u2208 RH(`\u22121)\u00d7H(`)\u00d7N(`)W \u00d7N(`)W , with denoting the element-wise multiplication, \u03c3 (\u00b7) being any activation function and vec (X)\u2192 x is the concatenation of every row in X. Note that H(0) corresponds to the number of channels the input images have.\nFor notational convenience, we use ~ to denote both \u201cvalid\u201d convolutions and \u201cfull\u201d convolutions, instead of introducing bulky notations to differentiate these cases. The \u201cvalid\u201d convolutions only apply a filter to complete patches of the image, resulting in a smaller image (its shape is decreased to NX \u2212N (`)W + 1). Alternatively, \u201cfull\u201d convolutions zero-pad the contour of the image before applying the convolution, thus expanding the image (its shape is increased to NX +N (`) W \u2212 1). Which one is used should be self-explanatory depending on the context. Note that we only use convolutions with a stride of 1. Moreover, in order for ConvNADE to output conditional probabilities as shown in Equation 36, the output layer must have only one feature map H (L) 1 , whose dimension matches the dimension of the input X. This can be achieved by carefully combining layers that use either \u201cvalid\u201d or \u201cfull\u201d convolutions.\nTo explore different model architectures respecting that constraint, we opted for the following strategy. Given a network, we ensured the first half of its layers was using \u201cvalid\u201d\nconvolutions while the other half would use \u201cfull\u201d convolutions. In addition to that, we made sure the network was symmetric with respect to its filter shapes (i.e. the filter shape used in layer ` matched the one used in layer L\u2212 `).\nFor completeness, we wish to mention that ConvNADE can also include pooling and upsampling layers, but we did not see much improvement when using them. In fact, recent research suggests that these types of layers are not essential to obtain state-of-the-art results (Springenberg et al., 2015).\nThe flexibility of DeepNADE allows us to easily combine both convolutional and fully connected layers. To create such hybrid models, we used the simple strategy of having two separate networks, with their last layer fused together at the end. The \u2018convnet\u2019 part is only composed of convolutional layers whereas the \u2018fullnet\u2019 part is only composed of fully connected layers. The forward pass of both networks follows respectively Equations (37)\u2013(39) and Equations (25)\u2013(27). Note that in the \u2018fullnet\u2019 network case, x corresponds to the input image having been flattened.\nIn the end, the output layer g of the hybrid model corresponds to the aggregation of the last layer pre-activation of both \u2018convnet\u2019 and \u2018fullnet\u2019 networks. The conditionals are slightly modified as follows:\np(xod =1 |xo<d ,\u03b8, o<d, od) = god (41) g = sigm ( vec ( A\n(L) 1\n) + a(L) ) . (42)\nThe same training procedure as for DeepNADE model can also be used for ConvNADE. For binary observations, the training loss is similar to Equation 34, with h(L) being substituted for g as defined in Equation 42.\nAs for the DeepNADE model, we found that providing the mask M o<d as an input to the model improves performance (see Table 4). For the \u2018convnet\u2019 part, the mask was provided as an additional channel to the input layer. For the \u2018fullnet\u2019 part, the inputs were concatenated with the mask as shown in Equation 35.\nThe final architecture is shown in Figure 3. In our experiments, we found that this type of hybrid model works better than only using convolutional layers (see Table 4). Certainly, more complex architectures could be employed but this is a topic left for future work."}, {"heading": "6. Related Work", "text": "As we mentioned earlier, the development of NADE and its extensions was motivated by the question of whether a tractable distribution estimator could be designed to match a powerful but intractable model such as the restricted Boltzmann machine.\nThe original inspiration came from the autoregressive approach taken by fully visible sigmoid belief networks (FVSBN), which were shown by Frey et al. (1996) to be surprisingly competitive, despite the simplicity of the distribution family for its conditionals. Bengio and Bengio (2000) later proposed using more powerful conditionals, modeled as single layer neural networks. Moreover, they proposed connecting the output of each dth conditional to all of the hidden layers of the d\u2212 1 neural networks for the preceding conditionals. More recently, Germain et al. (2015) generalized this model by deriving a simple procedure for making it deep and orderless (akin to DeepNADE, in Section 4). We compare with all of these approaches in Section 7.1.\nThere exists, of course, more classical and non-autoregressive approaches to tractable distribution estimation, such as mixture models and Chow\u2013Liu trees (Chow and Liu, 1968). We compare with these as well in Section 7.1.\nThis work also relates directly to the recently growing literature on generative neural networks. In addition to the autoregressive approach described in this paper, there exists three other types of such models: directed generative networks, undirected generative networks and hybrid networks.\nWork on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995). Helmholtz machines are equivalent to a multilayer sigmoid belief network, with each using binary stochastic units. Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al., 1995). More recently, many alternative directed models and training procedures have been proposed. Kingma and Welling (2014); Rezende et al. (2014) proposed the variational autoencoder (VAE), where the model is the same as the Helmholtz machine, but with real-valued (usually Gaussian) stochastic units. Importantly, Kingma and Welling (2014) identified a reparameterization trick making it possible to train the VAE in a way that resembles the training of an autoencoder. This approach falls in the family of stochastic variational inference methods, where the encoder network corresponds to the approximate variational posterior. The VAE optimizes a bound on the likelihood which is estimated using a single sample from the variational posterior, though recent work has shown that a better bound can be obtained using an importance sampling approach (Burda et al., 2016). Gregor et al. (2015) later exploited the VAE approach to develop DRAW, a directed generative model for images based on a read-write attentional mechanism. Goodfellow et al. (2014) also proposed an adversarial approach to training directed generative networks, that relies on a discriminator network simultaneously trained to distinguish between data and model samples.\nGenerative networks trained this way are referred to as Generative Adversarial Networks (GAN). While the VAE optimizes a bound of the likelihood (which is the KL divergence between the empirical and model distributions), it can be shown that GAN optimizes the Jensen\u2013Shannon (JS) divergence between the empirical and model distributions. Li et al. (2015) instead propose a training objective derived from Maximum Mean Discrepancy (MMD; Gretton et al., 2007). Recently, the directed generative model approach has been very successfully applied to model images (Denton et al., 2015; Sohl-Dickstein et al., 2011).\nThe undirected paradigm has also been explored extensively for developing powerful generative networks. These include the restricted Boltzmann machine (Smolensky, 1986; Freund and Haussler, 1992) and its multilayer extension, the deep Boltzmann machine (Salakhutdinov and Hinton, 2009), which dominate the literature on undirected neural networks. Salakhutdinov and Murray (2008) provided one of the first quantitative evidence of the generative modeling power of RBMs, which motivated the original parameterization for NADE (Larochelle and Murray, 2011). Efforts to train better undirected models can vary in nature. One has been to develop alternative objectives to maximum likelihood. The proposal of Contrastive Divergence (CD; Hinton, 2002) was instrumental in the popularization of the RBM. Other proposals include pseudo-likelihood (Besag, 1975; Marlin et al., 2010), score matching (Hyva\u0308rinen, 2005; Hyva\u0308rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyva\u0308rinen, 2010) and probability flow minimization (Sohl-Dickstein et al., 2011). Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al., 2010; Cho et al., 2010). Work has also been directed towards proposing improved update rules or parameterization of the model\u2019s energy function (Tieleman and Hinton, 2009; Cho et al., 2013; Montavon and Mu\u0308ller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010). The work of Ngiam et al. (2011) also proposed an undirected model that distinguishes itself from deep Boltzmann machines by having deterministic hidden units, instead of stochastic.\nFinally, hybrids of directed and undirected networks are also possible, though much less common. The most notable case is the Deep Belief Network (DBN; Hinton et al., 2006), which corresponds to a sigmoid belief network for which the prior over its top hidden layer is an RBM (whose hidden layer counts as an additional hidden layer). The DBN revived interest in RBMs, as they were required to successfully initialize the DBN.\nNADE thus substantially differs from this literature focusing on directed and undirected models, benefiting from a few properties that these approaches lack. Mainly, NADE does not rely on latent stochastic hidden units, making it possible to tractably compute its associated data likelihood for some given ordering. This in turn makes it possible to efficiently produce exact samples from the model (unlike in undirected models) and get an unbiased gradient for maximum likelihood training (unlike in directed graphical models)."}, {"heading": "7. Results", "text": "In this section, we evaluate the performance of our different NADE models on a variety of datasets."}, {"heading": "7.1 Binary vectors datasets", "text": "We start by evaluating the performance of NADE models on a set of benchmark datasets where the observations correspond to binary vectors. These datasets were mostly taken from the LIBSVM datasets web site1, except for OCR-letters2 and NIPS-0-12 3. Code to download these datasets is available here: http://info.usherbrooke.ca/hlarochelle/ code/nade.tar.gz. Table 1 summarizes the main statistics for these datasets.\nFor these experiments, we only consider tractable distribution estimators, where we can evaluate p(x) on test items exactly. We consider the following baselines:\n\u2022 MoB: A mixture of multivariate Bernoullis, trained using the EM algorithm. The number of mixture components was chosen from {32, 64, 128, 256, 512, 1024} based on validation set performance, and early stopping was used to determine the number of EM iterations.\n\u2022 RBM: A restricted Boltzmann machine made tractable by using only 23 hidden units, trained by contrastive divergence with up to 25 steps of Gibbs sampling. The validation set performance was used to select the learning rate from {0.005, 0.0005, 0.00005}, and the number of iterations over the training set from {100, 500, 1000}.\n\u2022 FVSBN: Fully visible sigmoid belief network, that models each conditional p(xod |xo<d) with logistic regression. The ordering of inputs was selected randomly. Training was by stochastic gradient descent. The validation set was used for early stopping, as well as for choosing the base learning rate \u03b7 \u2208 {0.05, 0.005, 0.0005}, and a decreasing schedule constant \u03b3 from {0, 0.001, 0.000001} for the learning rate schedule \u03b7/(1 + \u03b3t) for the tth update.\n\u2022 Chow\u2013Liu: A Chow\u2013Liu tree is a graph over the observed variables, where the distribution of each variable, except the root, depends on a single parent node. There\n1. http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/ 2. http://ai.stanford.edu/~btaskar/ocr/ 3. http://www.cs.nyu.edu/~roweis/data.html\nis an O(D2) fitting algorithm to find the maximum likelihood tree and conditional distributions (Chow and Liu, 1968). We adapted an implementation provided by Harmeling and Williams (2011), who found Chow\u2013Liu to be a strong baseline.\nThe maximum likelihood parameters are not defined when conditioning on events that haven\u2019t occurred in the training set. Moreover, conditional probabilities of zero are possible, which could give infinitely bad test set performance. We re-estimated the conditional probabilities on the Chow\u2013Liu tree using Lidstone or \u201cadd-\u03b1\u201d smoothing:\np(xd=1 |xparent =z) = count(xd=1 |xparent =z) + \u03b1\ncount(xparent =z) + 2\u03b1 , (43)\nselecting \u03b1 for each dataset from {10\u221220, 0.001, 0.01, 0.1} based on performance on the validation set.\n\u2022 MADE (Germain et al., 2015): Generalization of the neural network approach of Bengio and Bengio (2000), to multiple layers. We consider a version using a single (fixed) input ordering and another trained on multiple orderings from which an ensemble was constructed (which was inspired from the order-agnostic approach of Section 4) that we refer to as MADE-E. See Germain et al. (2015) for more details.\nWe compare these baselines with the two following NADE variants:\n\u2022 NADE (fixed order): Single layer NADE model, trained on a single (fixed) randomly generated order, as described in Section 2. The sigmoid activation function was used for the hidden layer, of size 500. Much like for FVSBN, training relied on stochastic gradient descent and the validation set was used for early stopping, as well as for choosing the learning rate from {0.05, 0.005, 0.0005}, and the decreasing schedule constant \u03b3 from {0,0.001,0.000001}.\n\u2022 NADE-E: Single layer NADE trained according to the order-agnostic procedure described in Section 4. The rectified linear activation function was used for the hidden layer, also of size 500. Minibatch gradient descent was used for training, with minibatches of size 100. The initial learning rate, chosen among {0.016, 0.004, 0.001, 0.00025, 0.0000675}, was linearly decayed to zero over the course of 100, 000 parameter updates. Early stopping was used, using Equation 34 to get a stochastic estimate of the validation set average log-likelihood. An ensemble using 16 orderings was used to compute the test-time log-likelihood.\nTable 2 presents the results. We observe that NADE restricted to a fixed ordering of the inputs achieves very competitive performance compared to the baselines. However, the order-agnostic version of NADE is overall the best method, being among the top performing model for 5 datasets out of 8.\nThe performance of fixed-order NADE is surprisingly robust to variations of the chosen input ordering. The standard deviation on the average log-likelihood when varying the ordering was small: on Mushrooms, DNA and NIPS-0-12, we observed standard deviations of 0.045, 0.05 and 0.15, respectively. However, models with different orders can do well on different test examples, which explains why ensembling can still help."}, {"heading": "7.2 Binary image dataset", "text": "We now consider the case of an image dataset, constructed by binarizing the MNIST digit dataset, as generated by Salakhutdinov and Murray (2008). This benchmark has been a popular choice for the evaluation of generative neural network models. Here, we investigate two questions:\n1. How does NADE compare to other intractable generative models?\n2. Does the use of a convolutional architecture improve the performance of NADE?\nFor these experiments, in addition to the baselines already described in Section 7.1, we consider the following:\n\u2022 DARN (Gregor et al., 2014): This deep generative autoencoder has two hidden layers, one deterministic and one with binary stochastic units. Both layers have 500 units (denoted as nh = 500). Adaptive weight noise (adaNoise) was either used or not to avoid the need for early stopping (Graves, 2011). Evaluation of exact test probabilities is intractable for large latent representations. Hence, Monte Carlo was used to approximate the expected description length, which corresponds to an upper bound on the negative log-likelihood.\n\u2022 DRAW (Gregor et al., 2015): Similar to a variational autoencoder where both the encoder and the decoder are LSTMs, guided (or not) by an attention mechanism. In this model, both LSTMs (encoder and decoder) are composed of 256 recurrent hidden units and always perform 64 timesteps. When the attention mechanism is enabled, patches (2\u00d7 2 pixels) are provided as inputs to the encoder instead of the whole image and the decoder also produces patches (5\u00d7 5 pixels) instead of a whole image.\n\u2022 Pixel RNN (Oord et al., 2016): NADE-like model for natural images that is based on convolutional and LSTM hidden units. This model has 7 hidden layers, each\nThe Neural Autoregressive Distribution Estimator\nFigure 2: (Left): samples from NADE trained on a binary version of mnist. (Middle): probabilities from which each pixel was sampled. (Right): visualization of some of the rows of W. This figure is better seen on a computer screen.\nset it to 0, to obtain:\n0 = \u2202KL(q(vi,v>i,h|v<i)||p(vi,v>i,h|v<i))\n\u2202\u03c4k(i) 0 = \u2212ck \u2212Wk,\u00b7\u00b5(i) + log (\n\u03c4k(i) 1\u2212 \u03c4k(i) ) \u03c4k(i)\n1\u2212 \u03c4k(i) = exp(ck +Wk,\u00b7\u00b5(i))\n\u03c4k(i) = exp(ck +Wk,\u00b7\u00b5(i))\n1 + exp(ck +Wk,\u00b7\u00b5(i))\n\u03c4k(i) = sigm ck +\u2211 j\u2265i Wkj\u00b5j(i) + \u2211 j<i Wkjvj  where in the last step we have replaced the matrix/vector multiplication Wk,\u00b7\u00b5(i) by its explicit summation form and have used the fact that \u00b5j(i) = vj for j < i.\nSimilarly, we set the derivative with respect to \u00b5j(i) for j \u2265 i to 0 and obtain:\n0 = \u2202KL(q(vi,v>i,h|v<i)||p(vi,v>i,h|v<i))\n\u2202\u00b5j(i) 0 = \u2212bj \u2212 \u03c4(i)>W\u00b7,j + log (\n\u00b5j(i) 1\u2212 \u00b5j(i) ) \u00b5j(i)\n1\u2212 \u00b5j(i) = exp(bj + \u03c4(i)>W\u00b7,j)\n\u00b5j(i) = exp(bj + \u03c4(i)>W\u00b7,j)\n1 + exp(bj + \u03c4(i)>W\u00b7,j)\n\u00b5j(i) = sigm ( bj +\n\u2211 k Wkj\u03c4k(i)\n)\nWe then recover the mean-field updates of Equations 7 and 8.\nReferences\nBengio, Y., & Bengio, S. (2000). Modeling highdimensional discrete data with multi-layer neural networks. Advances in Neural Information Processing Systems 12 (NIPS\u201999) (pp. 400\u2013406). MIT Press.\nBengio, Y., Lamblin, P., Popovici, D., & Larochelle, H. (2007). Greedy layer-wise training of deep networks. Advances in Neural Information Processing Systems 19 (NIPS\u201906) (pp. 153\u2013160). MIT Press.\nChen, X. R., Krishnaiah, P. R., & Liang, W. W. (1989). Estimation of multivariate binary density using orthogonal functions. Journal of Multivariate Analysis, 31, 178\u2013186.\nFreund, Y., & Haussler, D. (1992). A fast and exact learning rule for a restricted class of Boltzmann machines. Advances in Neural Information Processing Systems 4 (NIPS\u201991) (pp. 912\u2013919). Denver, CO: Morgan Kaufmann, San Mateo.\nFrey, B. J. (1998). Graphical models for machine learning and digital communication. MIT Press.\nFrey, B. J., Hinton, G. E., & Dayan, P. (1996). Does the wake-sleep algorithm learn good density estimators? Advances in Neural Information Processing Systems 8 (NIPS\u201995) (pp. 661\u2013670). MIT Press, Cambridge, MA.\nHinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural Computation, 14, 1771\u20131800.\nHinton, G. E., Osindero, S., & Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18, 1527\u20131554.\nLarochelle, H., & Bengio, Y. (2008). Classification using discriminative restricted Boltzmann machines. Proceedings of the 25th Annual International Conference\nl i i i i i\ni : : l f i i i f i . : ili i f i i l l . : i li i f f f . i i\n.\ni , i :\ni, i, | i || i, i, | i\n,\u00b7 l\n,\u00b7\n,\u00b7\n,\u00b7\ni i i\ni l l i l i li i ,\u00b7 i li i i f f f .\ni il l , i i i f i :\ni, i, | i || i, i, | i\n\u00b7, l\n\u00b7,\n\u00b7,\n\u00b7,\ni\nl f i .\ni , ., i , . . li i i i l i i l i l l\n. i l f i i \u2019 . . .\ni , ., li , ., i i, ., ll , . . l i i i f .\ni l f i i \u2019 . . .\n, . ., i i , . ., i , . . . i i f l i i i i i\nl f i . l f l i i l i , , .\n, ., l , . . f l i l f i l f l i . i l f i i\n\u2019 . . , : f , .\n, . . . i l l f i l i i i l i i . . , . ., i , . ., , . .\nl l i l i i i l f i i\n\u2019 . . , i , .\ni , . . . i i f i i i i i i . l i , , . i , . ., i , ., , . . f l i l i f li f . l\ni , , . ll , ., i , . . l i i i\ni i i i i l i . i f l i l f\nFigure 4: (Left): sample from NADE trained on binarized MNIST. (Right): probabilit es from wh ch each pixel was sampled. Ance tral s mpling was used with the same fixed ordering used during training.\nAlgorithm 2 Pretraining of a NADE with n hidden layers on dataset X.\nprocedure PRETRAIN(n,X) if n = 1 then\nreturn RANDOM-ONE-HIDDEN-LAYER-NADE else\nnade\u2190 PRETRAIN(n\u2212 1, X) nade\u2190 REMOVE-OUTPUT-LAYER(nade) nade\u2190 ADD-A-NEW-HIDDEN-LAYER(nade) nade\u2190 ADD-A-NEW-OUTPUT-LAYER(nade) nade\u2190 TRAIN-ALL(nade,X, iters = 20) . Train for 20 iterations. return nade\nend if end procedure\ncomposed of 16 units. Oord et al. (2016) proposed a novel two-dimensional LSTM, named Diagonal BiLSTM, which is used in this model. Unlike our ConvNADE, the ordering is fixed before training and at test time, and corresponds to a scan of the image in a diagonal fashion starting from a corner at the top and reaching the opposite corner at the bottom.\nWe compare these baselines with some NADE variants. The performance of a basic (fixed-order, single hidden layer) NADE model is provided in Table 3 and samples are illustrated in Figure 4. More importantly, we will focus on whether the following variants achieve better test set performance:\n19\n\u2022 DeepNADE: Multiple layers (1hl, 2hl, 3hl or 4hl) trained according to the orderagnostic procedure described in Section 4. Information about which inputs are masked was either provided or not (no input masks) to the model. The rectified linear activation function was used for all hidden layers. Minibatch gradient descent was used for training, with minibatches of size 1000. Training consisted of 200 iterations of 1000 parameter updates. Each hidden layer was pretrained according to Algorithm 2. We report an average of the average test log-likelihoods over ten different random orderings.\n\u2022 EoNADE: This variant is similar to DeepNADE except for the log-likelihood on the test set, which is instead computed from an ensemble that averages predictive probabilities over 2 or 128 orderings. To clarify, the DeepNADE results report the typical performance of one ordering, by averaging results after taking the log, and so do not combine the predictions of the models like EoNADE does.\n\u2022 ConvNADE: Multiple convolutional layers trained according to the order-agnostic procedure described in Section 4. The exact architecture is shown in Figure 5(a). Information about which inputs are masked was either provided or not (no input masks). The rectified linear activation function was used for all hidden layers. The Adam optimizer (Kingma and Ba, 2015) was used with a learning rate of 10\u22124. Early stopping was used with a look ahead of 10 epochs, using Equation 34 to get a stochastic estimate of the validation set average log-likelihood. An ensemble using 128 orderings was used to compute the log-likelihood on the test set.\n\u2022 ConvNADE + DeepNADE: This variant is similar to ConvNADE except for the aggregation of a separate DeepNADE model at the end of the network. The exact architecture is shown in Figure 5(b). The training procedure is the same as with ConvNADE.\nTable 3 presents the results obtained by models ignorant of the 2D topology, such as the basic NADE model. Addressing the first question, we observe that the order-agnostic version of NADE with two hidden layers is competitive with intractable generative models. Moreover, examples of the ability of DeepNADE to solve inference tasks by marginalization and conditional sampling are shown in Figure 6.\nNow, addressing the second question, we can see from Table 4 that convolutions do improve the performance of NADE. Moreover, we observe that providing information about which inputs are masked is essential to obtaining good results. We can also see that combining convolutional and fully-connected layers helps. Even though ConvNADE+DeepNADE performs slightly worst than Pixel RNN, we note that our proposed approach is orderagnostic, whereas Pixel RNN requires a fixed ordering. Figure 7 shows samples obtained from the ConvNADE+DeepNADE model using ancestral sampling on a random ordering.\n7.3 Real-valued observations datasets\nwas used with a different random ordering for each sample.\nIn this section, we compare the statistical performance of RNADE to mixtures of Gaussians (MoG) and factor analyzers (MFA), which are surprisingly strong baselines in some tasks (Tang et al., 2012; Zoran and Weiss, 2012)."}, {"heading": "7.3.1 Low-dimensional data", "text": "We start by considering three UCI datasets (Bache and Lichman, 2013), previously used to study the performance of other density estimators (Silva et al., 2011; Tang et al., 2012), namely: red wine, white wine and parkinsons. These are low dimensional datasets (see Table 5) with hard thresholds and non-linear dependencies that make it difficult to fit mixtures of Gaussians or factor analyzers.\nFollowing Tang et al. (2012), we eliminated discrete-valued attributes and an attribute from every pair with a Pearson correlation coefficient greater than 0.98. We normalized each dimension of the data by subtracting its training-subset sample mean and dividing by its standard deviation. All results are reported on the normalized data.\nWe use full-covariance Gaussians and mixtures of factor analysers as baselines. Models were compared on their log-likelihood on held-out test data. Due to the small size of the datasets (see Table 5), we used 10-folds, using 90% of the data for training, and 10% for testing.\nWe chose the hyperparameter values for each model by doing per-fold cross-validation, using a ninth of the training data as validation data. Once the hyperparameter values have been chosen, we train each model using all the training data (including the validation data) and measure its performance on the 10% of held-out testing data. In order to avoid overfitting, we stopped the training after reaching a training likelihood higher than the one obtained on the best validation-wise iteration of the best validation run. Early stopping was important to avoid overfitting the RNADE models. It also improved the results of the MFAs, but to a lesser degree.\nThe MFA models were trained using the EM algorithm (Ghahramani and Hinton, 1996; Verbeek, 2005). We cross-validated the number of components and factors. We also selected the number of factors from 2, 4, . . . D, where choosing D results in a mixture of Gaussians, and the number of components was chosen among 2, 4, . . . 50. Cross-validation selected fewer than 50 components in every case.\nWe report the performance of several RNADE models using different parametric forms for the one-dimensional conditionals: Gaussian with fixed variance (RNADE-FV), Gaussian with variable variance (RNADE-Gaussian), sinh-arcsinh distribution (RNADE-SAS), mixture of Gaussians (RNADE-MoG), and mixture of Laplace distributions (RNADE-MoL). All RNADE models were trained by stochastic gradient descent, using minibatches of size 100, for 500 epochs, each epoch comprising 10 minibatches. We fixed the number of hidden units to 50, and the non-linear activation function of the hidden units to ReLU. Three hyperparameters were cross-validated using grid-search: the number of components on each one-dimensional conditional (only applicable to the RNADE-MoG and RNADE-MoL models) was chosen from {2, 5, 10, 20}, the weight-decay (used only to regularize the input to hidden weights) from {2.0, 1.0, 0.1, 0.01, 0.001, 0}, and the learning rate from {0.1, 0.05, 0.025, 0.0125}. Learning rates were decreased linearly to reach 0 after the last epoch.\nThe results are shown in Table 6. RNADE with mixture of Gaussian conditionals was among the statistically significant group of best models on all datasets. As shown in Figure 8, RNADE-SAS and RNADE-MoG models are able to capture hard thresholds and heteroscedasticity."}, {"heading": "7.3.2 Natural image patches", "text": "We also measured the ability of RNADE to model small patches of natural images. Following the work of Zoran and Weiss (2011), we use 8-by-8-pixel patches of monochrome natural images, obtained from the BSDS300 dataset (Martin et al., 2001; Figure 9 gives examples).\nPixels in this dataset can take a finite number of brightness values ranging from 0 to 255. We added uniformly distributed noise between 0 and 1 to the brightness of each pixel. We then divided by 256, making the pixels take continuous values in the range [0, 1]. Adding noise prevents deceivingly high-likelihood solutions that assign narrow high-density spikes around some of the possible discrete values.\nWe subtracted the mean pixel value from each patch. Effectively reducing the dimensionality of the data. Therefore we discarded the 64th (bottom-right) pixel, which would be perfectly predictable and models could fit arbitrarily high densities to it. All of the results in this section were obtained by fitting the pixels in a raster-scan order.\nExperimental details follow. We trained our models by using patches randomly drawn from 180 images in the training subset of BSDS300. We used the remaining 20 images in the\ntraining subset as validation data. We used 1000 random patches from the validation subset to early-stop training of RNADE. We measured the performance of each model by their log-likelihood on one million patches drawn randomly from the test subset of 100 images not present in the training data. Given the larger scale of this dataset, hyperparameters of the RNADE and MoG models were chosen manually using the performance of preliminary runs on the validation data, rather than by grid search.\nAll RNADE models reported use ReLU activations for the hidden units. The RNADE models were trained by stochastic gradient descent, using 25 datapoints per minibatch, for a total of 1,000 epochs, each comprising 1,000 minibatches. The learning rate was initialized to 0.001, and linearly decreased to reach 0 after the last epoch. Gradient momentum with factor 0.9 was used, but initiated after the first epoch. A weight decay rate of 0.001 was applied to the input-to-hidden weight matrix only. We found that multiplying the gradient of the mean output parameters by the standard deviation improves results of the models with mixture outputs4. RNADE training was early stopped but didn\u2019t show signs of overfitting. Even larger models might perform better.\nThe MoG models were trained using 1,000 iterations of minibatch EM. At each iteration 20,000 randomly sampled datapoints were used in an EM update. A step was taken from the previous parameters\u2019 value towards the parameters resulting from the M-step: \u03b8t = (1 \u2212 \u03b7)\u03b8t\u22121 + \u03b7\u03b8EM . The step size, \u03b7, was scheduled to start at 0.1 and linearly decreased to reach 0 after the last update. The training of the MoG was early-stopped and also showed no signs of overfitting.\nThe results are shown in Table 7. We report the average log-likelihood of each model for a million image patches from the test set. The ranking of RNADE models is maintained when ordered by validation likelihood: the model with best test-likelihood would have been chosen using crossvalidation across all the RNADE models shown in the table. We also compared RNADE with a MoG trained by Zoran and Weiss (downloaded from Daniel Zoran\u2019s website) from which we removed the 64th row and column of each covariance matrix. There are two differences in the set-up of our experiments and those of Zoran and Weiss. First, we learned the means of the MoG components, while Zoran and Weiss (2011) fixed them to zero. Second, we held-out 20 images from the training set to do early-stopping and hyperparameter optimisation, while they used the 200 images for training.\nThe RNADE-FV model with fixed conditional variances obtained very low statistical performance. Adding an output parameter per dimension to have variable standard deviations made our models competitive with MoG with 100 full-covariance components. However, in order to obtain results superior to the mixture of Gaussians model trained by Zoran and Weiss, we had to use richer conditional distributions: one-dimensional mixtures of Gaussians (RNADE-MoG). On average, the best RNADE model obtained 3.3 nats per patch higher log-density than a MoG fitted with the same training data.\nIn Figure 9, we show one hundred examples from the test set, one hundred examples from Zoran and Weiss\u2019 mixture of Gaussians, and a hundred samples from our best RNADE-MoG model. Similar patterns can be observed in the three cases: uniform patches, edges, and locally smooth noisy patches.\n4. Empirically, we found this to work better than regular gradients and also better than multiplying by the variances, which would provide a step with the right units."}, {"heading": "7.3.3 Speech acoustics", "text": "We also measured the ability of RNADE to model small patches of speech spectrograms, extracted from the TIMIT dataset (Garofolo et al., 1993). The patches contained 11 frames of 20 filter-banks plus energy; totalling 231 dimensions per datapoint. A good generative model of speech acoustics could be used, for example, in denoising, or speech detection tasks.\nWe fitted the models using the standard TIMIT training subset, which includes recordings from 605 speakers of American English. We compare RNADE with a mixture of Gaussians by measuring their log-likelihood on the complete TIMIT core-test dataset: a held-out set of 25 speakers.\nThe RNADE models have 512 hidden units, ReLU activations, and a mixture of 20 one-dimensional Gaussian components per output. Given the large scale of this dataset, hyperparameter choices were again made manually using validation data. The same minibatch training procedures for RNADE and mixture of Gaussians were used as for natural image patches.\nThe RNADE models were trained by stochastic gradient descent, with 25 datapoints per minibatch, for a total of 200 epochs, each comprising 1,000 minibatches. The learning rate was initialized to 0.001 and linearly decreased to reach 0 after the last epoch. Gradient momentum with momentum factor 0.9 was used, but initiated after the first epoch. A weight decay rate of 0.001 was applied to the input-to-hidden weight matrix only. Again, we found that multiplying the gradient of the mean output parameters by the standard deviation improved results. RNADE training was early stopped but didn\u2019t show signs of overfitting.\nAs for the MoG model, it was trained exactly as in Section 7.3.2. The results are shown in Table 8. The best RNADE (which would have been selected based on validation results) has 15 nats higher likelihood per test example than the best mixture of Gaussians. Examples from the test set, and samples from the MoG and RNADEMoG models are shown in Figure 10. In contrast with the log-likelihood measure, there are no marked differences between the samples from each model. Both sets of samples look like blurred spectrograms, but RNADE seems to capture sharper formant structures (peaks of energy at the lower frequency bands characteristic of vowel sounds)."}, {"heading": "8. Conclusion", "text": "We\u2019ve described the Neural Autoregressive Distribution Estimator, a tractable, flexible and competitive alternative to directed and undirected graphical models for unsupervised distribution estimation.\nSince the publication of the first formulation of NADE (Larochelle and Murray, 2011), it has been extended to many more settings, other than those described in this paper. Larochelle and Lauly (2012); Zheng et al. (2015b) adapted NADE for topic modeling of documents and images, while Boulanger-Lewandowski et al. (2012) used NADE for modeling music sequential data. Theis and Bethge (2015) and Oord et al. (2016) proposed different NADE models for images than the one we presented, applied to natural images and based on convolutional and LSTM hidden units. Zheng et al. (2015a) used a NADE model to integrate an attention mechanism into an image classifier. Bornschein and Bengio (2015) showed that NADE could serve as a powerful prior over the latent state of directed graphical model. These are just a few examples of many possible ways one can leverage the flexibility and effectiveness of NADE models."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Modeling high-dimensional discrete data with multi-layer neural networks", "author": ["Yoshua Bengio", "Samy Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio and Bengio.,? \\Q2000\\E", "shortCiteRegEx": "Bengio and Bengio.", "year": 2000}, {"title": "Statistical analysis of non-lattice data", "author": ["Julian Besag"], "venue": "The Statistician,", "citeRegEx": "Besag.,? \\Q1975\\E", "shortCiteRegEx": "Besag.", "year": 1975}, {"title": "Mixture density networks", "author": ["Christopher M. Bishop"], "venue": "Technical Report NCRG 4288,", "citeRegEx": "Bishop.,? \\Q1994\\E", "shortCiteRegEx": "Bishop.", "year": 1994}, {"title": "Reweighted wake-sleep", "author": ["J\u00f6rg Bornschein", "Yoshua Bengio"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Bornschein and Bengio.,? \\Q2015\\E", "shortCiteRegEx": "Bornschein and Bengio.", "year": 2015}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2012}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "venue": "In Proceedings of the 4th International Conference on Learning Representations", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "Parallel tempering is efficient for learning restricted Boltzmann machines", "author": ["KyungHyun Cho", "Tapani Raiko", "Alexander Ilin"], "venue": "In Proceedings of the International Joint Conference on Neural Networks", "citeRegEx": "Cho et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2010}, {"title": "Enhanced gradient for training restricted Boltzmann machines", "author": ["KyungHyun Cho", "Tapani Raiko", "Alexander Ilin"], "venue": "Neural Computation,", "citeRegEx": "Cho et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2013}, {"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.K. Chow", "C.N. Liu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Chow and Liu.,? \\Q1968\\E", "shortCiteRegEx": "Chow and Liu.", "year": 1968}, {"title": "Improving deep neural networks for LVCSR using rectified linear units and dropout", "author": ["George E. Dahl", "Tara N. Sainath", "Geoffrey E. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Dahl et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dahl et al\\.", "year": 2013}, {"title": "The Helmholtz machine", "author": ["Peter Dayan", "Geoffrey E. Hinton", "Radford M. Neal", "Richard S. Zemel"], "venue": "Neural Computation,", "citeRegEx": "Dayan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dayan et al\\.", "year": 1995}, {"title": "Deep generative image models using a Laplacian pyramid of adversarial networks", "author": ["Emily L. Denton", "Soumith Chintala", "Arthur Szlam", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Tempered Markov chain Monte Carlo for training of restricted Boltzmann machine", "author": ["Guillaume Desjardins", "Aaron Courville", "Yoshua Bengio", "Pascal Vincent", "Olivier Delalleau"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, JMLR W&CP,", "citeRegEx": "Desjardins et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Desjardins et al\\.", "year": 2010}, {"title": "Unsupervised learning of distributions on binary vectors using two layer networks", "author": ["Yoav Freund", "David Haussler"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Freund and Haussler.,? \\Q1992\\E", "shortCiteRegEx": "Freund and Haussler.", "year": 1992}, {"title": "Does the wake-sleep algorithm learn good density estimators", "author": ["Brendan J. Frey", "Geoffrey E. Hinton", "Peter Dayan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Frey et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Frey et al\\.", "year": 1996}, {"title": "DARPA TIMIT acoustic-phonetic continuous speech corpus", "author": ["J. Garofolo", "L. Lamel", "W. Fisher", "J. Fiscus", "D. Pallett", "N. Dahlgren", "V. Zue"], "venue": "CD-ROM. NIST,", "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "MADE: Masked autoencoder for distribution estimation", "author": ["Mathieu Germain", "Karol Gregor", "Iain Murray", "Hugo Larochelle"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Germain et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Germain et al\\.", "year": 2015}, {"title": "The EM algorithm for mixtures of factor analyzers", "author": ["Zoubin Ghahramani", "Geoffrey E. Hinton"], "venue": "Technical Report CRG-TR-96-1, University of Toronto,", "citeRegEx": "Ghahramani and Hinton.,? \\Q1996\\E", "shortCiteRegEx": "Ghahramani and Hinton.", "year": 1996}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Graves.,? \\Q2011\\E", "shortCiteRegEx": "Graves.", "year": 2011}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Andriy Mnih", "Daan Wierstra"], "venue": "Proceedings of the 31st International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Gregor et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2014}, {"title": "DRAW: a recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "A kernel method for the two-sample-problem", "author": ["Arthur Gretton", "Karsten M. Borgwardt", "Malte Rasch", "Bernhard Sch\u00f6lkopf", "Alex J. Smola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gretton et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2007}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? \\Q2010\\E", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Greedy learning of binary latent trees", "author": ["Stefan Harmeling", "Christopher K.I. Williams"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Harmeling and Williams.,? \\Q2011\\E", "shortCiteRegEx": "Harmeling and Williams.", "year": 2011}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["Geoffrey E. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton.,? \\Q2002\\E", "shortCiteRegEx": "Hinton.", "year": 2002}, {"title": "The wake-sleep algorithm for unsupervised neural networks", "author": ["Geoffrey E. Hinton", "Peter Dayan", "Brendan J. Frey", "Radford M. Neal"], "venue": "Science, 268:1161\u20131558,", "citeRegEx": "Hinton et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1995}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Estimation of non-normalized statistical models by score matching", "author": ["Aapo Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hyv\u00e4rinen.,? \\Q2005\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2005}, {"title": "Some extensions of score matching", "author": ["Aapo Hyv\u00e4rinen"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "Hyv\u00e4rinen.,? \\Q2007\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2007}, {"title": "Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables", "author": ["Aapo Hyv\u00e4rinen"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Hyv\u00e4rinen.,? \\Q2007\\E", "shortCiteRegEx": "Hyv\u00e4rinen.", "year": 2007}, {"title": "Adam: a method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": "In Proceedings of the 2nd International Conference on Learning Representations", "citeRegEx": "Kingma and Welling.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A neural autoregressive topic model", "author": ["Hugo Larochelle", "Stanislas Lauly"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Larochelle and Lauly.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle and Lauly.", "year": 2012}, {"title": "The neural autoregressive distribution estimator", "author": ["Hugo Larochelle", "Iain Murray"], "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, JMLR W&CP,", "citeRegEx": "Larochelle and Murray.,? \\Q2011\\E", "shortCiteRegEx": "Larochelle and Murray.", "year": 2011}, {"title": "Generative moment matching networks", "author": ["Yujia Li", "Kevin Swersky", "Richard S. Zemel"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Inductive principles for restricted Boltzmann machine learning", "author": ["Benjamin Marlin", "Kevin Swersky", "Bo Chen", "Nando de Freitas"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Marlin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Marlin et al\\.", "year": 2010}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Martin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Martin et al\\.", "year": 2001}, {"title": "Deep Boltzmann machines and the centering trick", "author": ["Gr\u00e9goire Montavon", "Klaus-Robert M\u00fcller"], "venue": "In Neural Networks: Tricks of the Trade, Second Edition,", "citeRegEx": "Montavon and M\u00fcller.,? \\Q2012\\E", "shortCiteRegEx": "Montavon and M\u00fcller.", "year": 2012}, {"title": "Connectionist learning of belief networks", "author": ["Radford M. Neal"], "venue": "Artificial Intelligence,", "citeRegEx": "Neal.,? \\Q1992\\E", "shortCiteRegEx": "Neal.", "year": 1992}, {"title": "Learning deep energy models", "author": ["Jiquan Ngiam", "Zhenghao Chen", "Pang Wei Koh", "Andrew Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Pixel recurrent neural networks", "author": ["A\u00e4ron Van Den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "Proceedings of the 33rd International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Improved Gaussian mixture density estimates using Bayesian penalty terms and network averaging", "author": ["Dirk Ormoneit", "Volker Tresp"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ormoneit and Tresp.,? \\Q1995\\E", "shortCiteRegEx": "Ormoneit and Tresp.", "year": 1995}, {"title": "Iterative neural autoregressive distribution estimator (NADE-k)", "author": ["Tapani Raiko", "Li Yao", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Raiko et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Raiko et al\\.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "Proceedings of the 31st International Conference on Machine Learning, JMLR W&CP,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Learning in Markov random fields using tempered transitions", "author": ["Ruslan Salakhutdinov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Salakhutdinov.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov.", "year": 2009}, {"title": "Learning deep Boltzmann machines using adaptive MCMC", "author": ["Ruslan Salakhutdinov"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov.,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov.", "year": 2010}, {"title": "Deep Boltzmann machines", "author": ["Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "venue": "Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, JMLR W&CP,", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Efficient learning of deep Boltzmann machines", "author": ["Ruslan Salakhutdinov", "Hugo Larochelle"], "venue": "Proceedings of the 13th International Conference on Artificial Intelligence and Statistics, JMLR W&CP,", "citeRegEx": "Salakhutdinov and Larochelle.,? \\Q2010\\E", "shortCiteRegEx": "Salakhutdinov and Larochelle.", "year": 2010}, {"title": "On the quantitative analysis of deep belief networks", "author": ["Ruslan Salakhutdinov", "Iain Murray"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Salakhutdinov and Murray.,? \\Q2008\\E", "shortCiteRegEx": "Salakhutdinov and Murray.", "year": 2008}, {"title": "Mixed cumulative distribution networks", "author": ["Ricardo Silva", "Charles Blundell", "Yee Whye Teh"], "venue": "Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, JMLR W&CP,", "citeRegEx": "Silva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2011}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["Paul Smolensky"], "venue": "Parallel Distributed Processing: Volume 1: Foundations,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Linearly combining density estimators via stacking", "author": ["Padhraic Smyth", "David Wolpert"], "venue": "Machine Learning,", "citeRegEx": "Smyth and Wolpert.,? \\Q1999\\E", "shortCiteRegEx": "Smyth and Wolpert.", "year": 1999}, {"title": "Minimum probability flow learning", "author": ["Jascha Sohl-Dickstein", "Peter Battaglino", "Michael R. DeWeese"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Sohl.Dickstein et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sohl.Dickstein et al\\.", "year": 2011}, {"title": "Striving for simplicity: the all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Deep mixtures of factor analysers", "author": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Tang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2012}, {"title": "Generative image modeling using spatial lstms", "author": ["Lucas Theis", "Matthias Bethge"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Theis and Bethge.,? \\Q2015\\E", "shortCiteRegEx": "Theis and Bethge.", "year": 2015}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["Tijmen Tieleman"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Tieleman.,? \\Q2008\\E", "shortCiteRegEx": "Tieleman.", "year": 2008}, {"title": "Using fast weights to improve persistent contrastive divergence", "author": ["Tijmen Tieleman", "Geoffrey E. Hinton"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2009}, {"title": "Connectionist multivariate density-estimation and its application to speech synthesis", "author": ["Benigno Uria"], "venue": "PhD thesis, The University of Edinburgh,", "citeRegEx": "Uria.,? \\Q2015\\E", "shortCiteRegEx": "Uria.", "year": 2015}, {"title": "RNADE: The real-valued neural autoregressive density-estimator", "author": ["Benigno Uria", "Iain Murray", "Hugo Larochelle"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Uria et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2013}, {"title": "Mixture of factor analyzers", "author": ["Jakob Verbeek"], "venue": "Matlab implementation,", "citeRegEx": "Verbeek.,? \\Q2005\\E", "shortCiteRegEx": "Verbeek.", "year": 2005}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Vincent et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Exponential family harmoniums with an application to information retrieval", "author": ["Max Welling", "Michal Rosen-Zvi", "Geoffrey E. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Welling et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2005}, {"title": "Parameter inference for imperfectly observed Gibbsian fields", "author": ["Laurent Younes"], "venue": "Probability Theory Related Fields,", "citeRegEx": "Younes.,? \\Q1989\\E", "shortCiteRegEx": "Younes.", "year": 1989}, {"title": "A neural autoregressive approach to attention-based recognition", "author": ["Yin Zheng", "Richard S. Zemel", "Yu-Jin Zhang", "Hugo Larochelle"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "A deep and autoregressive approach for topic modeling of multimodal data", "author": ["Yin Zheng", "Yu-Jin Zhang", "Hugo Larochelle"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Zheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2015}, {"title": "From learning models of natural image patches to whole image restoration", "author": ["Daniel Zoran", "Yair Weiss"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Zoran and Weiss.,? \\Q2011\\E", "shortCiteRegEx": "Zoran and Weiss.", "year": 2011}, {"title": "Natural images, Gaussian mixtures and dead leaves", "author": ["Daniel Zoran", "Yair Weiss"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zoran and Weiss.,? \\Q2012\\E", "shortCiteRegEx": "Zoran and Weiss.", "year": 2012}], "referenceMentions": [{"referenceID": 51, "context": "A popular choice for such a model is the restricted Boltzmann machine (RBM), which substantially out-performs mixture models on a variety of binary datasets (Salakhutdinov and Murray, 2008).", "startOffset": 157, "endOffset": 189}, {"referenceID": 13, "context": "Frey et al. (1996) followed this approach and proposed using simple (log-)linear logistic regression models for these conditionals.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Bengio and Bengio (2000) proposed a more flexible approach, with a single-layer feed-forward neural network for each conditional.", "startOffset": 0, "endOffset": 25}, {"referenceID": 45, "context": "A generalization of NADE based on this connection to mean field inference has been further explored by Raiko et al. (2014).", "startOffset": 103, "endOffset": 123}, {"referenceID": 65, "context": "1 to the Gaussian-RBM (Welling et al., 2005).", "startOffset": 22, "endOffset": 44}, {"referenceID": 61, "context": "Such a model is not competitive with mixture models, for example on perceptual datasets (Uria, 2015).", "startOffset": 88, "endOffset": 100}, {"referenceID": 3, "context": "The resulting model can be interpreted as a sequence of mixture density networks (Bishop, 1994) with shared parameters.", "startOffset": 81, "endOffset": 95}, {"referenceID": 62, "context": "It was found empirically (Uria et al., 2013) that stochastic gradient descent leads to better parameter configurations when the gradient of the mean ( \u2202J \u2202\u03bcod,c ) was multiplied by the standard deviation (\u03c3od,c).", "startOffset": 25, "endOffset": 44}, {"referenceID": 61, "context": "Other choices, like single variable-variance Gaussians, sinh-arcsinh distributions, and mixtures of Laplace distributions, have been examined by Uria (2015). Training an RNADE can still be done by stochastic gradient descent on the parameters of the model with respect to the negative log-density of the training set.", "startOffset": 145, "endOffset": 157}, {"referenceID": 0, "context": "Deep neural networks (Bengio, 2009; LeCun et al., 2015) are at the core of state-of-the-art models for supervised tasks like image recognition (Krizhevsky et al.", "startOffset": 21, "endOffset": 55}, {"referenceID": 34, "context": ", 2015) are at the core of state-of-the-art models for supervised tasks like image recognition (Krizhevsky et al., 2012) and speech recognition (Dahl et al.", "startOffset": 95, "endOffset": 120}, {"referenceID": 10, "context": ", 2012) and speech recognition (Dahl et al., 2013).", "startOffset": 31, "endOffset": 50}, {"referenceID": 64, "context": "The resulting training procedure resembles that of a denoising autoencoder (Vincent et al., 2008).", "startOffset": 75, "endOffset": 97}, {"referenceID": 44, "context": "using bagging (Ormoneit and Tresp, 1995) or stacking (Smyth and Wolpert, 1999).", "startOffset": 14, "endOffset": 40}, {"referenceID": 54, "context": "using bagging (Ormoneit and Tresp, 1995) or stacking (Smyth and Wolpert, 1999).", "startOffset": 53, "endOffset": 78}, {"referenceID": 34, "context": "Recently, convolutional neural networks (CNN) have achieved state-of-the-art performance on many supervised tasks related to images Krizhevsky et al. (2012). Briefly, CNNs are composed of convolutional layers, each one having multiple learnable filters.", "startOffset": 132, "endOffset": 157}, {"referenceID": 56, "context": "In fact, recent research suggests that these types of layers are not essential to obtain state-of-the-art results (Springenberg et al., 2015).", "startOffset": 114, "endOffset": 141}, {"referenceID": 13, "context": "The original inspiration came from the autoregressive approach taken by fully visible sigmoid belief networks (FVSBN), which were shown by Frey et al. (1996) to be surprisingly competitive, despite the simplicity of the distribution family for its conditionals.", "startOffset": 139, "endOffset": 158}, {"referenceID": 0, "context": "Bengio and Bengio (2000) later proposed using more powerful conditionals, modeled as single layer neural networks.", "startOffset": 0, "endOffset": 25}, {"referenceID": 0, "context": "Bengio and Bengio (2000) later proposed using more powerful conditionals, modeled as single layer neural networks. Moreover, they proposed connecting the output of each dth conditional to all of the hidden layers of the d\u2212 1 neural networks for the preceding conditionals. More recently, Germain et al. (2015) generalized this model by deriving a simple procedure for making it deep and orderless (akin to DeepNADE, in Section 4).", "startOffset": 0, "endOffset": 310}, {"referenceID": 9, "context": "There exists, of course, more classical and non-autoregressive approaches to tractable distribution estimation, such as mixture models and Chow\u2013Liu trees (Chow and Liu, 1968).", "startOffset": 154, "endOffset": 174}, {"referenceID": 41, "context": "Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al.", "startOffset": 96, "endOffset": 108}, {"referenceID": 27, "context": "Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995).", "startOffset": 135, "endOffset": 176}, {"referenceID": 11, "context": "Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995).", "startOffset": 135, "endOffset": 176}, {"referenceID": 41, "context": "Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al.", "startOffset": 71, "endOffset": 83}, {"referenceID": 27, "context": "Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al., 1995).", "startOffset": 128, "endOffset": 149}, {"referenceID": 6, "context": "The VAE optimizes a bound on the likelihood which is estimated using a single sample from the variational posterior, though recent work has shown that a better bound can be obtained using an importance sampling approach (Burda et al., 2016).", "startOffset": 220, "endOffset": 240}, {"referenceID": 8, "context": "There exists, of course, more classical and non-autoregressive approaches to tractable distribution estimation, such as mixture models and Chow\u2013Liu trees (Chow and Liu, 1968). We compare with these as well in Section 7.1. This work also relates directly to the recently growing literature on generative neural networks. In addition to the autoregressive approach described in this paper, there exists three other types of such models: directed generative networks, undirected generative networks and hybrid networks. Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995). Helmholtz machines are equivalent to a multilayer sigmoid belief network, with each using binary stochastic units. Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al., 1995). More recently, many alternative directed models and training procedures have been proposed. Kingma and Welling (2014); Rezende et al.", "startOffset": 155, "endOffset": 1079}, {"referenceID": 8, "context": "There exists, of course, more classical and non-autoregressive approaches to tractable distribution estimation, such as mixture models and Chow\u2013Liu trees (Chow and Liu, 1968). We compare with these as well in Section 7.1. This work also relates directly to the recently growing literature on generative neural networks. In addition to the autoregressive approach described in this paper, there exists three other types of such models: directed generative networks, undirected generative networks and hybrid networks. Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995). Helmholtz machines are equivalent to a multilayer sigmoid belief network, with each using binary stochastic units. Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al., 1995). More recently, many alternative directed models and training procedures have been proposed. Kingma and Welling (2014); Rezende et al. (2014) proposed the variational autoencoder (VAE), where the model is the same as the Helmholtz machine, but with real-valued (usually Gaussian) stochastic units.", "startOffset": 155, "endOffset": 1102}, {"referenceID": 8, "context": "There exists, of course, more classical and non-autoregressive approaches to tractable distribution estimation, such as mixture models and Chow\u2013Liu trees (Chow and Liu, 1968). We compare with these as well in Section 7.1. This work also relates directly to the recently growing literature on generative neural networks. In addition to the autoregressive approach described in this paper, there exists three other types of such models: directed generative networks, undirected generative networks and hybrid networks. Work on directed generative networks dates back to the original work on sigmoid belief networks (Neal, 1992) and the Helmholtz machine (Hinton et al., 1995; Dayan et al., 1995). Helmholtz machines are equivalent to a multilayer sigmoid belief network, with each using binary stochastic units. Originally they were trained using Gibbs sampling and gradient descent (Neal, 1992), or with the so-called wake sleep algorithm (Hinton et al., 1995). More recently, many alternative directed models and training procedures have been proposed. Kingma and Welling (2014); Rezende et al. (2014) proposed the variational autoencoder (VAE), where the model is the same as the Helmholtz machine, but with real-valued (usually Gaussian) stochastic units. Importantly, Kingma and Welling (2014) identified a reparameterization trick making it possible to train the VAE in a way that resembles the training of an autoencoder.", "startOffset": 155, "endOffset": 1297}, {"referenceID": 6, "context": "The VAE optimizes a bound on the likelihood which is estimated using a single sample from the variational posterior, though recent work has shown that a better bound can be obtained using an importance sampling approach (Burda et al., 2016). Gregor et al. (2015) later exploited the VAE approach to develop DRAW, a directed generative model for images based on a read-write attentional mechanism.", "startOffset": 221, "endOffset": 263}, {"referenceID": 6, "context": "The VAE optimizes a bound on the likelihood which is estimated using a single sample from the variational posterior, though recent work has shown that a better bound can be obtained using an importance sampling approach (Burda et al., 2016). Gregor et al. (2015) later exploited the VAE approach to develop DRAW, a directed generative model for images based on a read-write attentional mechanism. Goodfellow et al. (2014) also proposed an adversarial approach to training directed generative networks, that relies on a discriminator network simultaneously trained to distinguish between data and model samples.", "startOffset": 221, "endOffset": 422}, {"referenceID": 23, "context": "(2015) instead propose a training objective derived from Maximum Mean Discrepancy (MMD; Gretton et al., 2007).", "startOffset": 82, "endOffset": 109}, {"referenceID": 12, "context": "Recently, the directed generative model approach has been very successfully applied to model images (Denton et al., 2015; Sohl-Dickstein et al., 2011).", "startOffset": 100, "endOffset": 150}, {"referenceID": 55, "context": "Recently, the directed generative model approach has been very successfully applied to model images (Denton et al., 2015; Sohl-Dickstein et al., 2011).", "startOffset": 100, "endOffset": 150}, {"referenceID": 53, "context": "These include the restricted Boltzmann machine (Smolensky, 1986; Freund and Haussler, 1992) and its multilayer extension, the deep Boltzmann machine (Salakhutdinov and Hinton, 2009), which dominate the literature on undirected neural networks.", "startOffset": 47, "endOffset": 91}, {"referenceID": 14, "context": "These include the restricted Boltzmann machine (Smolensky, 1986; Freund and Haussler, 1992) and its multilayer extension, the deep Boltzmann machine (Salakhutdinov and Hinton, 2009), which dominate the literature on undirected neural networks.", "startOffset": 47, "endOffset": 91}, {"referenceID": 49, "context": "These include the restricted Boltzmann machine (Smolensky, 1986; Freund and Haussler, 1992) and its multilayer extension, the deep Boltzmann machine (Salakhutdinov and Hinton, 2009), which dominate the literature on undirected neural networks.", "startOffset": 149, "endOffset": 181}, {"referenceID": 36, "context": "Salakhutdinov and Murray (2008) provided one of the first quantitative evidence of the generative modeling power of RBMs, which motivated the original parameterization for NADE (Larochelle and Murray, 2011).", "startOffset": 177, "endOffset": 206}, {"referenceID": 26, "context": "The proposal of Contrastive Divergence (CD; Hinton, 2002) was instrumental in the popularization of the RBM.", "startOffset": 39, "endOffset": 57}, {"referenceID": 2, "context": "Other proposals include pseudo-likelihood (Besag, 1975; Marlin et al., 2010), score matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) and probability flow minimization (Sohl-Dickstein et al.", "startOffset": 42, "endOffset": 76}, {"referenceID": 38, "context": "Other proposals include pseudo-likelihood (Besag, 1975; Marlin et al., 2010), score matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) and probability flow minimization (Sohl-Dickstein et al.", "startOffset": 42, "endOffset": 76}, {"referenceID": 24, "context": ", 2010), score matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) and probability flow minimization (Sohl-Dickstein et al.", "startOffset": 92, "endOffset": 121}, {"referenceID": 55, "context": ", 2010), score matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) and probability flow minimization (Sohl-Dickstein et al., 2011).", "startOffset": 156, "endOffset": 185}, {"referenceID": 66, "context": "Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al.", "startOffset": 104, "endOffset": 118}, {"referenceID": 59, "context": "Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al.", "startOffset": 148, "endOffset": 164}, {"referenceID": 13, "context": "Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al., 2010; Cho et al., 2010).", "startOffset": 224, "endOffset": 294}, {"referenceID": 7, "context": "Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al., 2010; Cho et al., 2010).", "startOffset": 224, "endOffset": 294}, {"referenceID": 60, "context": "Work has also been directed towards proposing improved update rules or parameterization of the model\u2019s energy function (Tieleman and Hinton, 2009; Cho et al., 2013; Montavon and M\u00fcller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010).", "startOffset": 119, "endOffset": 191}, {"referenceID": 8, "context": "Work has also been directed towards proposing improved update rules or parameterization of the model\u2019s energy function (Tieleman and Hinton, 2009; Cho et al., 2013; Montavon and M\u00fcller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010).", "startOffset": 119, "endOffset": 191}, {"referenceID": 40, "context": "Work has also been directed towards proposing improved update rules or parameterization of the model\u2019s energy function (Tieleman and Hinton, 2009; Cho et al., 2013; Montavon and M\u00fcller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010).", "startOffset": 119, "endOffset": 191}, {"referenceID": 50, "context": ", 2013; Montavon and M\u00fcller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010).", "startOffset": 98, "endOffset": 134}, {"referenceID": 28, "context": "The most notable case is the Deep Belief Network (DBN; Hinton et al., 2006), which corresponds to a sigmoid belief network for which the prior over its top hidden layer is an RBM (whose hidden layer counts as an additional hidden layer).", "startOffset": 49, "endOffset": 75}, {"referenceID": 22, "context": "Li et al. (2015) instead propose a training objective derived from Maximum Mean Discrepancy (MMD; Gretton et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 9, "context": "Recently, the directed generative model approach has been very successfully applied to model images (Denton et al., 2015; Sohl-Dickstein et al., 2011). The undirected paradigm has also been explored extensively for developing powerful generative networks. These include the restricted Boltzmann machine (Smolensky, 1986; Freund and Haussler, 1992) and its multilayer extension, the deep Boltzmann machine (Salakhutdinov and Hinton, 2009), which dominate the literature on undirected neural networks. Salakhutdinov and Murray (2008) provided one of the first quantitative evidence of the generative modeling power of RBMs, which motivated the original parameterization for NADE (Larochelle and Murray, 2011).", "startOffset": 101, "endOffset": 532}, {"referenceID": 2, "context": "Other proposals include pseudo-likelihood (Besag, 1975; Marlin et al., 2010), score matching (Hyv\u00e4rinen, 2005; Hyv\u00e4rinen, 2007a,b), noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) and probability flow minimization (Sohl-Dickstein et al., 2011). Another line of development has been to optimize likelihood using RobbinsMonro stochastic approximation (Younes, 1989), also known as Persistent CD (Tieleman, 2008), and develop good MCMC samplers for deep undirected models (Salakhutdinov, 2009, 2010; Desjardins et al., 2010; Cho et al., 2010). Work has also been directed towards proposing improved update rules or parameterization of the model\u2019s energy function (Tieleman and Hinton, 2009; Cho et al., 2013; Montavon and M\u00fcller, 2012) as well as improved approximate inference of the hidden layers (Salakhutdinov and Larochelle, 2010). The work of Ngiam et al. (2011) also proposed an undirected model that distinguishes itself from deep Boltzmann machines by having deterministic hidden units, instead of stochastic.", "startOffset": 43, "endOffset": 877}, {"referenceID": 9, "context": "is an O(D2) fitting algorithm to find the maximum likelihood tree and conditional distributions (Chow and Liu, 1968).", "startOffset": 96, "endOffset": 116}, {"referenceID": 9, "context": "is an O(D2) fitting algorithm to find the maximum likelihood tree and conditional distributions (Chow and Liu, 1968). We adapted an implementation provided by Harmeling and Williams (2011), who found Chow\u2013Liu to be a strong baseline.", "startOffset": 97, "endOffset": 189}, {"referenceID": 17, "context": "\u2022 MADE (Germain et al., 2015): Generalization of the neural network approach of Bengio and Bengio (2000), to multiple layers.", "startOffset": 7, "endOffset": 29}, {"referenceID": 0, "context": ", 2015): Generalization of the neural network approach of Bengio and Bengio (2000), to multiple layers.", "startOffset": 58, "endOffset": 83}, {"referenceID": 0, "context": ", 2015): Generalization of the neural network approach of Bengio and Bengio (2000), to multiple layers. We consider a version using a single (fixed) input ordering and another trained on multiple orderings from which an ensemble was constructed (which was inspired from the order-agnostic approach of Section 4) that we refer to as MADE-E. See Germain et al. (2015) for more details.", "startOffset": 58, "endOffset": 366}, {"referenceID": 47, "context": "2 Binary image dataset We now consider the case of an image dataset, constructed by binarizing the MNIST digit dataset, as generated by Salakhutdinov and Murray (2008). This benchmark has been a popular choice for the evaluation of generative neural network models.", "startOffset": 136, "endOffset": 168}, {"referenceID": 21, "context": "1, we consider the following: \u2022 DARN (Gregor et al., 2014): This deep generative autoencoder has two hidden layers, one deterministic and one with binary stochastic units.", "startOffset": 37, "endOffset": 58}, {"referenceID": 20, "context": "Adaptive weight noise (adaNoise) was either used or not to avoid the need for early stopping (Graves, 2011).", "startOffset": 93, "endOffset": 107}, {"referenceID": 22, "context": "\u2022 DRAW (Gregor et al., 2015): Similar to a variational autoencoder where both the encoder and the decoder are LSTMs, guided (or not) by an attention mechanism.", "startOffset": 7, "endOffset": 28}, {"referenceID": 43, "context": "\u2022 Pixel RNN (Oord et al., 2016): NADE-like model for natural images that is based on convolutional and LSTM hidden units.", "startOffset": 12, "endOffset": 31}], "year": 2016, "abstractText": "We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.", "creator": "LaTeX with hyperref package"}}}