{"id": "1605.02536", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2016", "title": "Random Fourier Features for Operator-Valued Kernels", "abstract": "partly devoted further to automatic multi - task learning and structured automated output learning, operator - valued kernels only provide a flexible tool to build operational vector - valued functions in the context of reproducing kernel hilbert spaces. bothering to scale up with these automated methods, we extend over the highly celebrated unique random fourier feature calculations methodology trying to get using an approximation variant of operator - valued component kernels. we lastly propose a strict general principle evaluation for operator - valued operational random local fourier feature construction yet relying prominently on compiling a partial generalization of bochner's bernstein theorem proof for translation - polynomial invariant operator - valued mercer polynomial kernels. we finally prove the optimal uniform convergence of generating the kernel approximation mechanics for approximate bounded and unbounded operator random linear fourier coefficient features using appropriate bernstein matrix concentration integral inequality. an experimental proof - of - concept compiler shows again the quality of testing the approximation and reduces the efficiency of the parameter corresponding linear models on the example analytic datasets.", "histories": [["v1", "Mon, 9 May 2016 11:36:40 GMT  (383kb,D)", "https://arxiv.org/abs/1605.02536v1", "32 pages, 6 figures"], ["v2", "Mon, 23 May 2016 16:35:41 GMT  (383kb,D)", "http://arxiv.org/abs/1605.02536v2", "32 pages, 6 figures"], ["v3", "Tue, 24 May 2016 12:59:58 GMT  (383kb,D)", "http://arxiv.org/abs/1605.02536v3", "32 pages, 6 figures"]], "COMMENTS": "32 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["romain brault", "florence d'alch\\'e-buc", "markus heinonen"], "accepted": false, "id": "1605.02536"}, "pdf": {"name": "1605.02536.pdf", "metadata": {"source": "CRF", "title": "Random Fourier Features for Operator-Valued Kernels", "authors": ["Romain Brault", "Florence d\u2019Alch\u00e9-Buc", "Markus Heinonen"], "emails": ["ro.brault@telecom-paristech.fr", "florence.dalche@telecom-paristech.fr", "markus.o.heinonen@aalto.fi"], "sections": [{"heading": null, "text": "Devoted to multi-task learning and structured output learning, operator-valued kernels provide a flexible tool to build vector-valued functions in the context of Reproducing Kernel Hilbert Spaces. To scale up these methods, we extend the celebrated Random Fourier Feature methodology to get an approximation of operatorvalued kernels. We propose a general principle for Operator-valued Random Fourier\n\u2217ro.brault@telecom-paristech.fr \u2020florence.dalche@telecom-paristech.fr \u2021markus.o.heinonen@aalto.fi\nar X\niv :1\nFeature construction relying on a generalization of Bochner\u2019s theorem for translationinvariant operator-valued Mercer kernels. We prove the uniform convergence of the kernel approximation for bounded and unbounded operator random Fourier features using appropriate Bernstein matrix concentration inequality. An experimental proof-of-concept shows the quality of the approximation and the efficiency of the corresponding linear models on example datasets."}, {"heading": "1 Introduction", "text": "Multi-task regression (Micchelli and Pontil, 2005), structured classification (Dinuzzo et al., 2011), vector field learning (Baldassarre et al., 2012) and vector autoregression (Sindhwani et al., 2013; Lim et al., 2015) are all learning problems that boil down to learning a vector while taking into account an appropriate output structure. A pdimensional vector-valued model can account for couplings between the outputs for improved performance in comparison to p independent scalar-valued models. In this paper we are interested in a general and flexible approach to efficiently implement and learn vector-valued functions, while allowing couplings between the outputs. To achieve this goal, we turn to shallow architectures, namely the product of a (nonlinear) feature matrix \u03a6\u0303(x) and a parameter vector \u03b8 such that f\u0303(x) = \u03a6\u0303(x)\u2217\u03b8, and combine two appealing methodologies: Operator-Valued Kernel Regression and Random Fourier Features.\nOperator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; A\u0301lvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions. As in the scalar case, operator-valued kernels (OVKs) are used to build Reproducing Kernel Hilbert Spaces (RKHS) in which representer theorems apply as for ridge regression or other appropriate loss functional. In these cases, learning a model in the RKHS boils down to learning a function of the form f(x) = \u2211n i=1 K(x, xi)\u03b1i where x1, . . . , xn are the training input data and each \u03b1i, i = 1, . . . , n is a vector of the output space Y and each K(x, xi), an operator on vectors of Y . However, OVKs suffer from the same drawback as classic kernel machines: they scale poorly to very large datasets because they are very demanding in terms of memory and computation. Therefore, focusing on the case Y = Rp, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines. The RFF approach linearizes a shift-invariant kernel model by generating explicitly an approximated feature map \u03c6\u0303. RFFs has been shown to be efficient on large datasets and further improved by efficient matrix computations of FastFood (Le et al., 2013), and is considered as one of the best large scale implementations of kernel methods, along with Ny\u0308strom approaches (Yang et al., 2012).\nIn this paper, we propose general Random Fourier Features for functions in vector-\nvalued RKHS. Here are our contributions: (1) we define a general form of Operator Random Fourier Feature (ORFF) map for shift-invariant operator-valued kernels, (2) we construct explicit operator feature maps for a simple bounded kernel, the decomposable kernel, and more complex unbounded kernels curl-free and divergence-free kernels, (3) the corresponding kernel approximation is shown to uniformly converge towards the target kernel using appropriate Bernstein matrix concentration inequality, for both bounded and unbounded operator-valued kernels and (4) we illustrate the theoretical approach by a few numerical results.\nThe paper is organized as follows. In section 1.2, we recall Random Fourier Feature and Operator-valued kernels. In section 2, we use extension of Bochner\u2019s theorem to propose a general principle of Operator Random Fourier Features and provide examples for decomposable, curl-free and divergence-free kernels. In section 3, we present a theorem of uniform convergence for bounded and unbounded ORFFs (proof is given in appendix B) and the conditions of its application. Section 4 shows an numerical illustration on learning linear ORFF-models. Section 5 concludes the paper. The main proofs of the paper are presented in Appendix."}, {"heading": "1.1 Notations", "text": "The euclidean inner product in Rd is denoted \u3008\u00b7, \u00b7\u3009 and the euclidean norm is denoted \u2016\u00b7\u2016. The unit pure imaginary number \u221a \u22121 is denoted i. B(Rd) is the Borel \u03c3-algebra on Rd. For a function f : Rd \u2192 R, if dx is the Lebesgue measure on Rd, we denote F [f ] its Fourier transform defined by:\n\u2200x \u2208 Rd,F [f ] (x) = \u222b Rd e\u2212i\u3008\u03c9,x\u3009f(\u03c9)d\u03c9.\nThe inverse Fourier transform of a function g is defined as F\u22121 [g] (\u03c9) = \u222b Rd ei\u3008x,\u03c9\u3009f(\u03c9)dx.\nIt is common to define the Fourier transform of a (positive) measure \u00b5 by F [\u00b5] (x) = \u222b Rd e\u2212i\u3008\u03c9,x\u3009d\u00b5(\u03c9).\nIf X and Y are two vector spaces, we denote by F(X ;Y) the vector space of functions f : X \u2192 Y and C(X ;Y) \u2282 F(X ;Y) the subspace of continuous functions. If H is an Hilbert space we denote its scalar product by \u3008., .\u3009H and its norm by \u2016.\u2016H. We set L(H) = L(H;H) to be the space of linear operators from H to itself. If W \u2208 L(H), Ker W denotes the nullspace, Im W the image and W \u2217 \u2208 L(H) the adjoint operator (transpose in the real case)."}, {"heading": "1.2 Background", "text": "Random Fourier Features: we first consider scalar-valued functions. Denote k : Rd \u00d7 Rd \u2192 R a positive definite kernel on Rd. A kernel k is said to be shift-invariant for the addition if for any a \u2208 Rd, \u2200(x, x\u2032) \u2208 Rd\u00d7Rd, k(x\u2212 a, z\u2212 a) = k(x, z). Then, we define k0 : Rd \u2192 R the function such that k(x, z) = k0(x \u2212 z). k0 is called the signature of kernel k. Bochner theorem is the theoretical result that leads to the Random Fourier Features.\nTheorem 1.1 (Bochner\u2019s theorem1). Every positive definite complex valued function is the Fourier transform of a non-negative measure. This implies that any positive definite, continuous and shift-invariant kernel k is the Fourier transform of a non-negative measure \u00b5:\nk(x, z) = k0(x\u2212 z) = \u222b Rd e\u2212i\u3008\u03c9,x\u2212z\u3009d\u00b5(\u03c9). (1)\nWithout loss of generality for the Random Fourier methodology, we assume that \u00b5 is a probability measure, i.e. \u222b Rd d\u00b5(\u03c9) = 1. Then we can write eq. (1) as an expectation\nover \u00b5: k0(x \u2212 z) = E\u00b5 [ e\u2212i\u3008\u03c9,x\u2212z\u3009 ] . Both k and \u00b5 are real-valued, and the imaginary part is null if and only if \u00b5(\u03c9) = \u00b5(\u2212\u03c9). We thus only write the real part:\nk(x, z) = E\u00b5[cos\u3008\u03c9, x\u2212 z\u3009] = E\u00b5 [cos\u3008\u03c9, z\u3009 cos\u3008\u03c9, x\u3009+ sin\u3008\u03c9, z\u3009 sin\u3008\u03c9, x\u3009] .\nLet \u2295D\nj=1 xj denote the Dm-length column vector obtained by stacking vectors xj \u2208 Rm. The feature map \u03c6\u0303 : Rd \u2192 R2D defined as\n\u03c6\u0303(x) = 1\u221a D D\u2295 j=1 ( cos \u3008x, \u03c9j\u3009 sin \u3008x, \u03c9j\u3009 ) , \u03c9j \u223c \u00b5 (2)\nis called a Random Fourier Feature map. Each \u03c9j, j = 1, . . . , D is independently sampled from the inverse Fourier transform \u00b5 of k0. This Random Fourier Feature map provides the following Monte-Carlo estimator of the kernel:\nK\u0303(x, z) = \u03c6\u0303(x)\u2217\u03c6\u0303(z), (3)\nthat is proven to uniformly converge towards the true kernel described in eq. (1). The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015). Finally, it is important to notice that Random Fourier Feature approach\n1See Rudin (1994).\nonly requires two steps before learning: (1) define the inverse Fourier transform of the given shift-invariant kernel, (2) compute the randomized feature map using the spectral distribution \u00b5. For the Gaussian kernel k(x \u2212 z) = exp(\u2212\u03b3\u2016x\u2212 z\u20162), the spectral distribution \u00b5(\u03c9) is Gaussian Rahimi and Recht (2007).\nOperator-valued kernels: we now turn to vector-valued functions and consider vectorvalued Reproducing Kernel Hilbert spaces (vv-RKHS) theory. The definitions are given for input space X \u2282 Cd and output space Y \u2282 Cp. We will define operator-valued kernel as reproducing kernels following the presentation of Carmeli et al. (2010). Given X and Y , a map K : X \u00d7 X \u2192 L(Y) is called a Y-reproducing kernel if\nN\u2211 i,j=1 \u3008K(xi, xj)yj, yi\u3009 \u2265 0,\nfor all x1, . . . , xN in X , all y1, . . . , yN in Y and N \u2265 1. Given x \u2208 X , Kx : Y \u2192 F(X ;Y) denotes the linear operator whose action on a vector y is the function Kxy \u2208 F(X ;Y) defined by (Kxy)(z) = K(z, x)y, \u2200z \u2208 X .\nAdditionally, given a Y-reproducing kernelK, there is a unique Hilbert spaceHK \u2282 F(X ;Y) satisfying Kx \u2208 L(Y ;HK), \u2200x \u2208 X and f(x) = K\u2217xf, \u2200x \u2208 X ,\u2200f \u2208 HK , where K\u2217x : HK \u2192 Y is the adjoint of Kx. The space HK is called the (vector-valued) Reproducing Kernel Hilbert Space associated with K. The corresponding product and norm are denoted by \u3008., .\u3009K and \u2016.\u2016K , respectively. As a consequence (Carmeli et al., 2010) we have:\nK(x, z) = K\u2217xKz \u2200x, z \u2208 X HK = span {Kxy | \u2200x \u2208 X , \u2200y \u2208 Y}\nAnother way to describe functions ofHK consists in using a suitable feature map.\nProposition 1.1 (Carmeli et al. (2010)). Let H be a Hilbert space and \u03a6 : X \u2192 L(Y ;H), with \u03a6x , \u03a6(x). Then the operatorW : H \u2192 F(X ;Y) defined by (Wg)(x) = \u03a6\u2217xg, \u2200g \u2208 H,\u2200x \u2208 X is a partial isometry fromH onto the reproducing kernel Hilbert spaceHK with reproducing kernel\nK(x, z) = \u03a6\u2217x\u03a6z, \u2200x, z \u2208 X ."}, {"heading": "W \u2217W is the orthogonal projection onto", "text": "Ker W\u22a5 = span {\u03a6xy | \u2200x \u2208 X , \u2200y \u2208 Y} .\nThen \u2016f\u2016K = inf {\u2016g\u2016H | \u2200g \u2208 H, Wg = f}.\nWe call \u03a6 a feature map, W a feature operator andH a feature space.\nIn this paper, we are interested on finding feature maps of this form for shift-invariant Rp-Mercer kernels using the following definitions. A reproducing kernel K on Rd is a Rp-Mercer provided that HK is a subspace of C(Rd;Rp). It is said to be a shiftinvariant kernel or a translation-invariant kernel for the addition if K(x + a, z + a) = K(x, z), \u2200(x, z, a) \u2208 X 3. It is characterized by a function K0 : X \u2192 L(Y) of completely positive type such that K(x, z) = K0(\u03b4), with \u03b4 = x\u2212 z."}, {"heading": "2 Operator-valued Random Fourier Features", "text": ""}, {"heading": "2.1 Spectral representation of shift-invariant vector-valued Mercer kernels", "text": "The goal of this work is to build approximated matrix-valued feature map for shiftinvariant Rp-Mercer kernels, denoted with K, such that any function f \u2208 HK can be approximated by a function f\u0303 defined by:\nf\u0303(x) = \u03a6\u0303(x)\u2217\u03b8\nwhere \u03a6\u0303(x) is a matrix of size (m \u00d7 p) and \u03b8 is an m-dimensional vector. We propose a randomized approximation of such a feature map using a generalization of the Bochner theorem for operator-valued functions. For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)). In a few words, Pontryagin duality deals with functions on locally compact Abelian groups, and allows to define their Fourier transform in a very general way. For sake of simplicity, we instantiate the general results of Carmeli et al. (2010); Zhang et al. (2012) for our case of interest of X = Rd and Y = Rp. The following proposition extends Bochner\u2019s theorem to any shift-invariant Rp-Mercer kernel.\nProposition 2.1 (Operator-valued Bochner\u2019s theorem2). A continuous function K from Rd \u00d7 Rd to L(Rp) is a shift-invariant reproducing kernel if and only if \u2200x, z \u2208 Rd, it is the Fourier transform of a positive operator-valued measureM : B(Rd)\u2192 L+(Rp):\nK(x, z) = \u222b Rd e\u2212i\u3008x\u2212z,\u03c9\u3009dM(\u03c9),\nwhereM belongs to the set of all the L+(Rp)-valued measures of bounded variation on the \u03c3-algebra of Borel subsets of Rd.\n2Equation (36) in Zhang et al. (2012).\nHowever it is much more convenient to use a more explicit result that involves realvalued (positive) measures. The following proposition instantiates Prop. 13 in Carmeli et al. (2010) to matrix-valued operators.\nProposition 2.2 (Carmeli et al. (2010)). Let \u00b5 be a positive measure on Rd and A : Rd \u2192 L(Rp) such that \u3008A(.)y, y\u2032\u3009 \u2208 L1(Rd, d\u00b5) for all y, y\u2032 \u2208 Rp and A(\u03c9) \u2265 0 for \u00b5-almost all \u03c9. Then, for all \u03b4 \u2208 Rd, \u2200`,m \u2208 {1, . . . , p},\nK0(\u03b4)`m = \u222b Rd e\u2212i\u3008\u03b4,\u03c9\u3009A(\u03c9)`md\u00b5(\u03c9) (4)\nis the kernel signature of a shift-invariant Rp-Mercer kernel K such that K(x, z) = K0(x \u2212 z). In other terms, each real-valued function K0(\u00b7)`m is the Fourier transform of A(\u00b7)`mp\u00b5(\u00b7) where p\u00b5(\u03c9) = d\u00b5d\u03c9 is the Radon-Nikodym derivative of the measure \u00b5, which is also called the density of the measure \u00b5. Any shift-invariant kernel is of the above form for some pair (A(\u03c9), \u00b5(\u03c9)).\nThis theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, \u00b5 is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1). Now we introduce the following proposition that directly is a direct consequence of proposition 2.2.\nProposition 2.3 (Feature map). Given the conditions of proposition 2.2, we defineB(\u03c9) such that A(\u03c9) = B(\u03c9)B(\u03c9)\u2217. Then the function \u03a6 : Rp \u2192 L2(Rd, \u00b5;Rp) defined for all x \u2208 Rp by\n\u2200y \u2208 Rp, (\u03a6xy) (\u03c9) = ei\u3008x,\u03c9\u3009B(\u03c9)\u2217y (5)\nis a feature map of the shift-invariant kernelK, i.e. it satisfies for all x, z in Rd, \u03a6\u2217x\u03a6z = K(x, z).\nThus, to define an approximation of a given operator-valued kernel, we need an inversion theorem that provides an explicit construction of the pair A(\u03c9), \u00b5(\u03c9) from the kernel signature. Proposition 14 in Carmeli et al. (2010), instantiated to Rp-Mercer kernel gives the solution.\nProposition 2.4 (Carmeli et al. (2010)). Let K be a shift-invariant Rp-Mercer kernel. Suppose that \u2200z \u2208 Rd,\u2200y, y\u2032 \u2208 Rp, \u3008K0(.)y, y\u2032\u3009 \u2208 L1(Rd, dx) where dx denotes the Lebesgue measure. Define C such that \u2200\u03c9 \u2208 Rd,\u2200`,m \u2208 {1, . . . , p},\nC(\u03c9)`m = \u222b Rd ei\u3008\u03b4,\u03c9\u3009K0(\u03b4)`md\u03b4. (6)\nThen\ni) C(\u03c9) is an non-negative matrix for all \u03c9 \u2208 Rd,\nii) \u3008C(.)y, y\u2032\u3009 \u2208 L1(Rd, d\u03c9) for all y, y\u2032 \u2208 Rp,\niii) for all \u03b4 \u2208 Rd,\u2200`,m \u2208 {1, . . . , p},\nK0(\u03b4)`m = \u222b Rd e\u2212i\u3008\u03b4,\u03c9\u3009C(\u03c9)`md\u03c9.\nFrom eq. (4) and eq. (6), we can write the following equality concerning the matrixvalued kernel signature K0, coefficient by coefficient: \u2200\u03b4 \u2208 Rd,\u2200i, j \u2208 {1, . . . , p},\u222b\nRd e\u2212i\u3008\u03b4,\u03c9\u3009C(\u03c9)ijd\u03c9 = \u222b Rd e\u2212i\u3008\u03b4,\u03c9\u3009A(\u03c9)ijd\u00b5(\u03c9).\nWe then conclude that the following equality holds almost everywhere for \u03c9 \u2208 Rd: C(\u03c9)ij = A(\u03c9)ijp\u00b5(\u03c9) where p\u00b5(\u03c9) = d\u00b5d\u03c9 . Without loss of generality we assume that\u222b Rd d\u00b5(\u03c9) = 1 and thus, \u00b5 is a probability distribution. Note that this is always possible through an appropriate normalization of the kernel. Then p\u00b5 is the density of \u00b5. The proposition 2.2 thus results in an expectation:\nK0(x\u2212 z) = E\u00b5[e\u2212i\u3008x\u2212z,\u03c9\u3009A(\u03c9)] (7)"}, {"heading": "2.2 Construction of Operator Random Fourier Feature", "text": "Given a Rp-Mercer shift-invariant kernel K on Rd, we build an Operator-Valued Random Fourier Feature (ORFF) map in three steps:\n1) compute C : Rd \u2192 L(Rp) from eq. (6) by using the inverse Fourier transform (in the sense of proposition 2.4) of K0, the signature of K;\n2) find A(\u03c9), p\u00b5(\u03c9) and compute B(\u03c9) such that A(\u03c9) = B(\u03c9)B(\u03c9)\u2217;\n3) build an randomized feature map via Monte-Carlo sampling from the probability measure \u00b5 and the application B."}, {"heading": "2.3 Monte-Carlo approximation", "text": "Let \u2295D\nj=1 Xj denote the block matrix of size rD \u00d7 s obtained by stacking D matrices X1, . . . , XD of size r \u00d7 s. Assuming steps 1 and 2 have been performed, for all j = 1, . . . , n, we find a decomposition A(\u03c9j) = B(\u03c9j)B(\u03c9j)\u2217 either by exhibiting a general analytical closed-form or using a numerical decomposition. Denote p\u00d7p\u2032 the dimension\nof the matrix B(\u03c9). We then propose a randomized matrix-valued feature map: \u2200x \u2208 Rd,\n\u03a6\u0303(x) = 1\u221a D D\u2295 j=1 \u03a6x(\u03c9j), \u03c9j \u223c \u00b5\n= 1\u221a D D\u2295 j=1 e\u2212i\u3008x,\u03c9j\u3009B(\u03c9j) \u2217, \u03c9j \u223c \u00b5.\n(8)\nThe corresponding approximation for the kernel is then: \u2200x, z \u2208 Rd\nK\u0303(x, z) = \u03a6\u0303(x)\u2217\u03a6\u0303(z)\n= 1\nD \u2211D j=1 e\u2212i\u3008x,\u03c9j\u3009B(\u03c9j)e i\u3008z,\u03c9j\u3009B(\u03c9j) \u2217\n= 1\nD \u2211D j=1 e\u2212i\u3008x\u2212z,\u03c9j\u3009A(\u03c9j).\nThe Monte-Carlo estimator \u03a6\u0303(x)\u2217\u03a6\u0303(z) converges in probability to K(x, z) when D tends to infinity. Namely,\nK\u0303(x, z) = \u03a6\u0303(x)\u2217\u03a6\u0303(z) p.\u2212\u2212\u2212\u2192 D\u2192\u221e E\u00b5 [ e\u2212i\u3008x\u2212z,\u03c9\u3009A(\u03c9) ] = K(x, z)\nAs for the scalar-valued kernel, a real-valued matrix-valued function has a real matrixvalued Fourier transform if A(\u03c9) is even with respect to \u03c9. Taking this point into account, we define the feature map of a real matrix-valued kernel as\n\u03a6\u0303(x) = 1\u221a D D\u2295 j=1 ( cos \u3008x, \u03c9j\u3009B(\u03c9j)\u2217 sin \u3008x, \u03c9j\u3009B(\u03c9j)\u2217 ) , \u03c9j \u223c \u00b5.\nThe kernel approximation becomes\n\u03a6\u0303(x)\u2217\u03a6\u0303(z) = 1\nD D\u2211 j=1 cos \u3008x,\u03c9j\u3009 cos \u3008z,\u03c9j\u3009A(\u03c9j) sin \u3008x,\u03c9j\u3009 sin \u3008z,\u03c9j\u3009A(\u03c9j)+\n= 1\nD D\u2211 j=1 cos \u3008x\u2212 z, \u03c9j\u3009A(\u03c9j).\nIn the following, we give an explicit construction of ORFFs for three well-known RpMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al. (2012) for vector field learning. All these kernels are defined using a scalar-valued shift-invariant Mercer kernel k :\nRd\u00d7Rd \u2192 R whose signature is denoted k0. A usual choice is to choose k as a Gaussian kernel with k0(\u03b4) = exp ( \u2212\u2016\u03b4\u2016 2\n2\u03c32\n) , which gives \u00b5 = N (0, \u03c3\u22122I) (Huang et al., 2013) as\nits inverse Fourier transform.\nDefinition 2.1 (Decomposable kernel). Let A be a (p\u00d7 p) positive semi-definite matrix. K defined as \u2200(x, z) \u2208 Rd \u00d7 Rd, K(x, z) = k(x, z)A is a Rp-Mercer shift-invariant reproducing kernel.\nMatrix A encodes the relationships between the outputs coordinates. If a graph coding for the proximity between tasks is known, then it is shown in Evgeniou et al. (2005); Baldassarre et al. (2010) that A can be chosen equal to the pseudo inverse L\u2020 of the graph Laplacian, and then the `2 norm in HK is a graph-regularizing penalty for the outputs (tasks). When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015). Another interesting property of the decomposable kernel is its universality. A reproducing kernel K is said universal if the associated RKHSHK is dense in the space C(X ,Y).\nExample 2.1 (ORFF for decomposable kernel).\nCdec(\u03c9)`m = \u222b X ei\u3008\u03b4,\u03c9\u3009k0(\u03b4)A`md\u03b4 = A`mF\u22121 [k0] (\u03c9)\nHence, Adec(\u03c9) = A and pdec\u00b5 (\u03c9) = F\u22121 [k0] (\u03c9).\nORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p). Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlstro\u0308m et al., 2013). These kernels discussed in Fuselier (2006) allow encoding input-dependent similarities between vector-fields.\nDefinition 2.2 (Curl-free and Div-free kernel). We have d = p. The divergence-free kernel is defined as\nKdiv(x, z) = Kdiv0 (\u03b4) = (\u2207\u2207\u2217 \u2212\u2206I)k0(\u03b4)\nand the curl-free kernel as\nKcurl(x, z) = Kcurl0 (\u03b4) = \u2212\u2207\u2207\u2217k0(\u03b4),\nwhere \u2207\u2207\u2217 is the Hessian operator and \u2206 is the Laplacian operator.\nAlthough taken separately these kernels are not universal, a convex combination of the curl-free and divergence-free kernels allows to learn any vector field that satisfies the Helmholtz decomposition theorem (Macedo and Castro, 2008; Baldassarre et al., 2012). For the divergence-free and curl-free kernel we use the differentiation properties of the Fourier transform.\nExample 2.2 (ORFF for curl-free kernel:). \u2200`,m \u2208 {1, . . . , p},\nCcurl(\u03c9)`m = \u2212F\u22121 [ \u2202\n\u2202\u03b4`\n\u2202\n\u2202\u03b4m k0\n] (\u03c9)\n= \u03c9`\u03c9mF\u22121 [k0] (\u03c9)\nHence,Acurl(\u03c9) = \u03c9\u03c9\u2217 and pcurl\u00b5 (\u03c9) = F\u22121 [k0] (\u03c9). We can obtain directly: Bcurl(\u03c9) = \u03c9.\nFor the divergence-free kernel we first compute the Fourier transform of the Laplacian of a scalar kernel using differentiation and linearity properties of the Fourier transform. We denote \u03b4{`=m} as the Kronecker delta which is 1 if ` = m and zero otherwise.\nExample 2.3 (ORFF for divergence-free kernel:).\nCdiv(\u03c9)`m = F\u22121 [ \u2202\n\u2202\u03b4`\n\u2202\n\u2202\u03b4m k0 \u2212 \u03b4{`=m}\u2206k0 ] = F\u22121 [ \u2202\n\u2202\u03b4`\n\u2202\n\u2202\u03b4m k0\n] \u2212 \u03b4{`=m}F\u22121 [\u2206k0]\n= (\u03b4{`=m} \u2212 \u03c9`\u03c9m)\u2016\u03c9\u201622F \u22121 [k0] ,\nsince\nF\u22121 [\u2206k0(\u03b4)] = p\u2211\nk=1\nF\u22121 [ \u2202\n\u2202\u03b4k k0\n] = \u2212\u2016\u03c9\u201622F \u22121 [k0] .\nHence Adiv(\u03c9) = I\u2016\u03c9\u201622 \u2212 \u03c9\u03c9\u2217 and pdiv\u00b5 (\u03c9) = F\u22121 [k0] (\u03c9). Here, Bdiv(\u03c9) has to be obtained by a numerical decomposition such as Cholesky or SVD."}, {"heading": "3 Uniform error bound on ORFF approximation", "text": "We are now interested on measuring how close the approximation K\u0303(x, z) = \u03a6\u0303(x)\u2217\u03a6\u0303(z) is close to the target kernel K(x, z) for any x, z in a compact set C. If A is a real matrix, we denote \u2016A\u20162 its spectral norm, defined as the square root of the largest eigenvalue of\nA. For x and z in some compact C \u2282 Rd, we consider: F (x\u2212 z) = K\u0303(x, z)\u2212K(x, z) and study how the uniform norm\n\u2016F\u2016\u221e = sup x,z\u2208C \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2\n(9)\nbehaves according to D. Figure 1 empirically shows convergence of three different OVK approximations for x, z from the compact [\u22121, 1]4 using an increasing number of sample points D. The log-log plot shows that all three kernels have the same convergence rate, up to a multiplicative factor.\nIn order to bound the error with high probability, we turn to concentration inequalities devoted to random matrices (Boucheron et al.). In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225\n2 = \u2016A\u20162 \u2225\u2225\u2225k\u0303(x, z)\u2212 k(x, z)\u2225\u2225\u2225 This theorem and its proof are presented in corollary A.1.1.\nMore interestingly, we propose a new bound for Operator Random Fourier Feature approximation in the general case. It relies on three main ideas: (i) Matrix concentration inequality for random matrices has to be used instead of concentration inequality for (scalar) random variables, (ii) Instead of using Hoeffding inequality as in the scalar case (proof of Rahimi and Recht (2007)) but for matrix concentration (Mackey et al., 2014) we use a refined inequality such as the Bernstein matrix inequality (Ahlswede and Winter; Boucheron et al.; Tropp, 2012), also used for the scalar case in (Sutherland and Schneider, 2015), (iii) we propose a general theorem valid for random matrices with bounded norms (case for decomposable kernel ORFF approximation) as well as with unbounded norms (curl and divergence-free kernels). For the latter, we notice that their norms behave as subexponential random variables (Koltchinskii et al., 2013). Before introducing the new theorem, we give the definition of the Orlicz norm and subexponential random variables.\nDefinition 3.1 (Orlicz norm). We follow the definition given by Koltchinskii et al. (2013). Let \u03c8 : R+ \u2192 R+ be a non-decreasing convex function with \u03c8(0) = 0. For a random variable X on a measured space (\u2126, T (\u2126), \u00b5),\n\u2016X\u2016\u03c8 , inf {C > 0 | E[\u03c8 (|X|/C)] \u2264 1} .\nHere, the function \u03c8 is chosen as \u03c8(u) = \u03c8\u03b1(u) where \u03c8\u03b1(u) , eu \u03b1 \u2212 1. When \u03b1 = 1, a random variable with finite Orlicz norm is called a subexponential variable because its tails decrease at least exponentially fast.\nTheorem 3.1. Let C be a compact subset of Rd of diameter l. Let K be a shift-invariant Rp-Mercer kernel on Rd, K0 its signature and p\u00b5(\u00b7)A(\u00b7) the inverse Fourier transform of the kernel\u2019s signature (in the sense of proposition 2.4) where p\u00b5 is the density of a probability measure \u00b5 considering appropriate normalization. Let D be a positive integer and \u03c91, . . . , \u03c9D, i.i.d. random vectors drawn according to the probability law \u00b5. For x, z \u2208 C, we recall\nK\u0303(x, z) = D\u2211 j=1 cos\u3008x\u2212 z, \u03c9j\u3009A(\u03c9j).\nWe note for all j \u2208 {1, . . . , D},\nFj(x\u2212 z) = 1\nD ( D\u2211 j=1 cos\u3008x\u2212 z, \u03c9j\u3009A(\u03c9j)\u2212K(x, z) )\nand F (x\u2212 z) = K\u0303(x, z)\u2212K(x, z). \u2016F\u2016\u221e denotes the infinite norm of F (x\u2212 z) on the compact C as introduced in eq. (9). If one can define the following terms (bD,m, \u03c32p) \u2208 R3+:\nbD = sup x,z\u2208C D \u2225\u2225\u2225\u2225\u2225E\u00b5 [ D\u2211 j=1 (Fj(x\u2212 z))2 ]\u2225\u2225\u2225\u2225\u2225\n2\n,\nm = 4 ( \u2016\u2016A(\u03c9)\u20162\u2016\u03c81 + sup\nx,z\u2208C \u2016K(x, z)\u2016\n) , \u03c9 \u223c \u00b5,\n\u03c32p = E\u00b5 [ \u2016\u03c9\u201622\u2016A(\u03c9)\u2016 2 2 ] .\nThen for all in R+,\nP {\u2016F\u2016\u221e \u2265 } \u2264 Cd ( \u03c3pl ) 2 1+2/d exp ( \u2212 2D 8(d+2)(bD+ u\u0304D6 ) ) if u\u0304D \u2264 2(e\u22121)bD exp ( \u2212 D\n(d+2)(e\u22121)u\u0304D\n) otherwise,\nwhere u\u0304D = 2m log ( 2 3 2 ( m bD )2) and Cd = p (( d 2 ) \u2212d d+2 + ( d 2 ) 2 d+2 ) 2 6d+2 d+2 .\nWe detail the proof of the theorem in appendix B. It follows the usual scheme derived in Rahimi and Recht (2007) and Sutherland and Schneider (2015) and involves Bernstein concentration inequality for unbounded symmetric matrices (theorem B.1)."}, {"heading": "3.1 Application to some operator-valued kernel", "text": "To apply theorem 3.1 to operator-valued kernels, we need to ensure that all the constants exist. In the following, we first show how to bound the constant term bD. Then we exhibit the upper bounds for the three operator-valued kernels we took as examples. Eventually, we ensure that the random variable \u2016A(\u03c9)\u2016 has a finite Orlicz norm with \u03c8 = \u03c81 in these three cases.\nBounding the term bD(\u03b4): Proposition 3.1. Define the matrix V\u00b5[A(\u03c9)] as follows: for all `,m \u2208 {1, . . . , p},\nV\u00b5[A(\u03c9)]`m = p\u2211 r=1 Cov\u00b5[A(\u03c9)`r, A(\u03c9)rm]\nFor a given \u03b4 = x\u2212 z, define:\nbD(\u03b4) = D \u2225\u2225\u2225\u2225\u2225E\u00b5 [ D\u2211 j=1 (Fj(\u03b4)) 2 ]\u2225\u2225\u2225\u2225\u2225 2 .\nThen we have:\nbD(\u03b4) \u2264 1\n2 \u2225\u2225(K0(2\u03b4) +K0(0))E\u00b5[A(\u03c9)]\u2212 2K0(\u03b4)2\u2225\u22252 + \u2016V\u00b5[A(\u03c9)]\u20162. The proof uses trigonometry properties and various properties of the moments and is given in appendix C. Now, we compute the upper bound given by proposition 3.1 for the three kernels we have taken as examples.\ni) Decomposable kernel: notice that in the case of the Gaussian decomposable kernel, i.e. A(\u03c9) = A, K0(\u03b4) = Ak0(\u03b4), k0(\u03b4) \u2265 0 and k0(\u03b4) = 1, then we have:\nbD(\u03b4) \u2264 1\n2 (1 + k0(2\u03b4))\u2016A\u20162 + k0(\u03b4) 2\nii) curl-free and div-free kernels: recall that in this case p = d. For the (Gaussian) curl-free kernel, A(\u03c9) = \u03c9\u03c9\u2217 where \u03c9 \u2208 Rd \u223c N (0, \u03c3\u22122Id) thus E\u00b5[A(\u03c9)] = Id/\u03c3 2 and V\u00b5[A(\u03c9)] = (d+ 1)Id/\u03c34 (see Petersen et al. (2008)). Hence,\nbD(\u03b4) \u2264 1\n2 \u2225\u2225\u2225\u2225 1\u03c32K0(2\u03b4)\u2212 2K0(\u03b4)2 \u2225\u2225\u2225\u2225\n2\n+ (d+ 1)\n\u03c34\nEventually for the Gaussian divergence-free kernel, A(\u03c9) = I\u2016\u03c9\u201622 \u2212 \u03c9\u03c9\u2217, thus E\u00b5[A(\u03c9)] = Id(d \u2212 1)/\u03c32 and V\u00b5[A(\u03c9)] = d(4d \u2212 3)Id/\u03c34 (see Petersen et al. (2008)). Hence,\nbD(\u03b4) \u2264 1\n2 \u2225\u2225\u2225\u2225(d\u2212 1)\u03c32 K0(2\u03b4)\u2212 2K0(\u03b4)2 \u2225\u2225\u2225\u2225\n2\n+ d(4d\u2212 3)\n\u03c34\nAn empirical illustration of these bounds is shown in fig. 6.\nComputing the Orlicz norm: For a random variable with strictly monotonic moment generating function (MGF), one can characterize its \u03c81 Orlicz norm by taking the functional inverse of the MGF evaluated at 2. In other words\n\u2016X\u2016\u22121\u03c81 = MGF(x) \u22121 X (2).\nFor the Gaussian curl-free and divergence-free kernel\u2225\u2225Adiv(\u03c9)\u2225\u2225 2 = \u2225\u2225Acurl(\u03c9)\u2225\u2225 2 = \u2016\u03c9\u201622\nwhere \u03c9 \u223c N (0, Id/\u03c32), hence \u2016A(\u03c9)\u20162 \u223c \u0393(p/2, 2/\u03c32). The MGF of this gamma distribution is MGF(x)\u22121(t) = (1\u2212 2t/\u03c32)\u2212(p/2). Eventually\n\u2225\u2225\u2225\u2225Adiv(\u03c9)\u2225\u2225 2 \u2225\u2225\u22121 \u03c81 = \u2225\u2225\u2225\u2225Acurl(\u03c9)\u2225\u2225 2 \u2225\u2225\u22121 \u03c81 = \u03c32\n2\n( 1\u2212 4\u2212 1 p ) ."}, {"heading": "4 Learning with ORFF", "text": "In practise, the previous bounds are however too large to find a safe value for D. In the following, numerical examples of ORFF-approximations are presented."}, {"heading": "4.1 Penalized regression with ORFF", "text": "Once we have an approximated feature map, we can use it to provide a feature matrix of size p\u2032D\u00d7 p with matrix B(\u03c9) of size p\u00d7 p\u2032 such that A(\u03c9) = B(\u03c9)B(\u03c9)\u2217. A function f \u2208 HK is then approximated by a linear model\nf\u0303(x) = \u03a6\u0303(x)\u2217\u03b8, where \u03b8 \u2208 Rp\u2032D.\nLet S = {(xi, yi) \u2208 Rd \u00d7 Rp, i = 1, . . . , N} be a collection of i.i.d training samples. Given a local loss function L : S \u2192 R+ and a `2 penalty, we minimize\nL(\u03b8) = 1 N N\u2211 i=1 L ( \u03a6\u0303(xi) \u2217\u03b8, yi ) + \u03bb\u2016\u03b8\u201622, (10)\ninstead of minimizing L(f) = 1 N \u2211N i=1 L(f(xi), yi) + \u03bb\u2016f\u2016 2 HK . To find a minimizer of the optimization problem eq. (10) many optimization algorithms are available. For instance, in large-scale context, a stochastic gradient descent algorithm would be be suitable: we can adapt the algorithm to the kind of kernel/problematic. We investigate two optimization algorithms: a Stein equation solver appropriate for the decomposable kernel and a (stochastic) gradient descent for non-decomposable kernels (e.g. the curlfree and divergence-free kernels).\nClosed form for the decomposable kernel: for the real decomposable kernelK0(\u03b4) = k(\u03b4)A when L(y, y\u2032) = \u2016y \u2212 y\u2032\u201622 (Kernel Ridge regression in HK), the learning problem described in eq. (10) can be re-written in terms of matrices to find the unique minimizer \u0398\u2217, where vec(\u0398) = \u03b8 such that \u03b8 is a p\u2032D vector and \u0398 a p\u2032 \u00d7D matrix. If \u03c6\u0303 is a feature map (\u03c6\u0303(X) is a matrix of size D \u00d7N ) for the scalar kernel k0, then\n\u03a6\u0303(x)\u2217\u03b8 = (\u03c6\u0303(x)\u2217 \u2297B)\u03b8 = B\u0398\u03c6\u0303(x)\nand \u03b8\u2217 = arg min\n\u0398\u2208Rp\u2032\u00d7D\n\u2225\u2225\u2225B\u0398\u03c6\u0303(X)\u2212 Y \u2225\u2225\u22252 F + \u03bb\u2016\u0398\u20162F . (11)\nThis is a convex optimization problem and a sufficient condition is:\n\u03c6\u0303(X)\u03c6\u0303(X)\u2217\u0398\u2217B \u2217B \u2212 \u03c6\u0303(X)Y \u2217B + \u03bb\u0398\u2217 = 0,\nwhich is a Stein equation.\nGradient computation for the general case. When it is not possible or desirable to use Stein\u2019s equations solver one can apply a (stochastic) gradient descent algorithm. The gradient computation for and `2-loss applied to ORFF model is briefly recalled in appendix D.1."}, {"heading": "4.2 Numerical illustration", "text": "We present a few experiments to complete the theoretical contribution and illustrate the behavior of ORFF-regression. Other experimentalresults with noisy output data are shown in appendix D.2.\nDatasets: the first dataset is the handwritten digits recognition dataset MNIST3 We select a training set of 12000 images and a test set of 10000 images. The inputs are images represented as a vector xi \u2208 [0, 255]784 and the targets are integers between 0 and 9. First we scaled the inputs such that they take values in [\u22121, 1]784. Then we binarize the targets such that each number is represented by a unique binary vector of length 10. To predict classes, we use simplex coding method presented in Mroueh et al. (2012). The intuition behind simplex coding is to project the binarized labels of dimension p onto the most separated vectors on the hypersphere of dimension p \u2212 1. For ORFF we can encode directly this projection in the B matrix of the decomposable kernel K0(\u03b4) = BB\u2217k0(\u03b4) where k0 is a Gaussian kernel. For OVK we project the binarized targets on the simplex as a preprocessing step, before learning with the kernel K0(\u03b4) = Ipk0(\u03b4), where k0 is a also Gaussian kernel.\n3available at http://yann.lecun.com/exdb/mnist.\nThe second dataset corresponds to a 2D-vector field with structure. We generated a scalar field as a mixture of five Gaussians located at [0, 0], [0, 1], [0,\u22121], with positive values and at [\u22121, 0], [1, 0] with negative values. The curl-free field has been generated by taking the gradient of the scalar-field, and the divergence-free field by taking the orthogonal of the curl-free field. These 2D-datasets are depicted in fig. 7.\nApproximation: We trained both an ORFF and an OVK model on the handwritten digits recognition dataset (MNIST) with a decomposable Gaussian kernel with signature K0(\u03b4) = exp(\u2212\u2016\u03b4\u2016/\u03c32)A. To find a solution of the optimization problem described in eq. (11), we use off-the-shelf solver4 able to handle Stein\u2019s equation. For both methods we choose \u03c3 = 20 and use a 2-fold cross validation on the training set to select the optimal \u03bb. First, fig. 2 shows the running time comparison between OVK and ORFF models using D = 1000 Fourier features against the number of datapoints N . The loglog plot shows ORFF scaling better than the OVK w.r.t the number of points. Second, fig. 3 shows the test prediction error versus the number of ORFFs D, when using N = 1000 training points. As expected, the ORFF model converges toward the OVK model when the number of features increases.\nIndependent (RFF) prediction vs Structured prediction on vector fields: we perform a similar experiment over a simulated dataset designed for learning a 2D-vector field with structure. Figure 4 reports the Mean Squared Error versus the number of ORFF D. For this experiment we use a Gaussian curl-free kernel and tune its \u03c3 hyperparameter as well as the \u03bb on a grid. The curl-free ORFF outperforms classic RFFs by tending more quickly towards the noise level. Figure 5 shows the computation time between curl-ORFF and curl-OVK indicating that the OVK solution does not scale to large datasets, while ORFF scales well with when the number of data increases. When N > 104 exact OVK is not able to be trained in reasonable time (> 1000 seconds)."}, {"heading": "5 Conclusion", "text": "We introduced a general and versatile framework for operator-valued kernel approximation with Operator Random Fourier Features. We showed the uniform convergence of these approximations by proving a matrix concentration inequality for bounded and unbounded ORFFs. The complexity in time of these approximations together with the linear learning algorithm make this implementation scalable with the number of data and therefore interesting compared to OVK regression. The numerical illustration shows the behavior expected from theory. ORFFs are especially a very promising approach in vector field learning or on noisy datasets. Another appealing direction is to use this archi-\n4 http://ta.twi.tudelft.nl/nw/users/gijzen/IDR.html\ntecture to automatically learn operator-valued kernels by learning a mixture of ORFFs in order to choose appropriate kernels, a working direction closely related to the recent method called \u201cAlacarte\u201d (Yang et al., 2015) based on the very efficient \u201cFastFood\u201d method (Le et al., 2013) for scalar kernels. Finally this work opens the door to building deeper architectures by stacking vector-valued functions while keeping a kernel view for large datasets."}, {"heading": "A Reminder on Random Fourier Feature in the scalar case", "text": "Rahimi and Recht (2007) proved the uniform convergence of Random Fourier Feature (RFF) approximation for a scalar shift invariant kernel.\nTheorem A.1 (Uniform error bound for RFF, Rahimi and Recht (2007)). Let C be a compact of subset of Rd of diameter l. Let k a shift invariant kernel, differentiable with a bounded first derivative and \u00b5 its normalized inverse Fourier transform. Let D the dimension of the Fourier feature vectors. Then, for the mapping \u03c6\u0303 described in section 2, we have :\nP {\nsup x,z\u2208C \u2225\u2225\u2225k\u0303(x, z)\u2212 k(x, z)\u2225\u2225\u2225 2 \u2265 } \u2264 28 ( d\u03c3l )2 exp ( \u2212 2D 4(d+ 2) ) (12)\nFrom theorem A.1, we can deduce the following corollary about the uniform convergence of the ORFF approximation of the decomposable kernel.\nCorollary A.1.1 (Uniform error bound for decomposable ORFF). Let C be a compact of subset of Rd of diameter l. Kdec is a decomposable kernel built from a p\u00d7p semi-definite matrixA and k, a shift invariant and differentiable kernel whose first derivative is bounded. Let k\u0303 the Random Fourier approximation for the scalar-valued kernel k. We recall that: for a given pair (x, z) \u2208 C, K\u0303(x, z) = \u03a6\u0303(x)\u2217\u03a6\u0303(z) = Ak\u0303(x, z) and K0(x\u2212 z) = AE\u00b5[k\u0303(x, z)].\nP {\nsup x,z\u2208C \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2 \u2265 } \u2264 28 ( d\u03c3\u2016A\u20162l )2 exp ( \u2212\n2D\n4\u2016A\u201622(d+ 2) ) Proof. The proof directly extends A.1 given by Rahimi and Recht (2007). Since\nsup x,z\u2208C \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2 = sup x,z\u2208C \u2016A\u20162. \u2223\u2223\u2223K\u0303(x, z)\u2212 k(x, z)\u2223\u2223\u2223\nand then, taking \u2032 = \u2016A\u20162 gives the following result for all positive \u2032:\nP {\nsup x,z\u2208C \u2225\u2225\u2225A(K\u0303(x, z)\u2212 k(x, z))\u2225\u2225\u2225 2 \u2265 \u2032 } \u2264 28 ( d\u03c3\u2016A\u20162l \u2032 )2 exp ( \u2212\n\u20322D\n4\u2016A\u201622(d+ 2)\n)\nPlease note that a similar corollary could have been obtained for the recent result of Sutherland and Schneider (2015) who refined the bound proposed by Rahimi and Recht by using a Bernstein concentration inequality instead of the Hoeffding inequality."}, {"heading": "B Proof of the uniform error bound for ORFF approxima-", "text": "tion\nThis section present a proof of theorem 3.1.\nWe note \u03b4 = x \u2212 z, K\u0303(x, z) = \u03a6\u0303(x)\u2217\u03a6\u0303(z), K\u0303j(x, z) = \u03a6\u0303j(x)\u2217\u03a6\u0303j(z) and K0(\u03b4) = K(x, z). For sake of simplicity, we use the following notation:\nF (\u03b4) = F (x\u2212 z) = K\u0303(x, z)\u2212K(x, z) Fj(\u03b4) = Fj(x\u2212 z) = (K\u0303j(x, z)\u2212K(x, z))/D\nCompared to the scalar case, the proof follows the same scheme as the one described in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015) but requires to consider matrix norms and appropriate matrix concentration inequality. The main feature of theorem 3.1 is that it covers the case of bounded ORFF as well as unbounded ORFF: in the case of bounded ORFF, a Bernstein inequality for matrix concentration such that the one proved in Mackey et al. (2014) (Corollary 5.2) or the formulation of Tropp (2012) recalled in Koltchinskii et al. (2013) is suitable. However some kernels like the curl and the div-free kernels do not have bounded \u2016Fj\u2016 but exhibit Fj with subexponential tails. Therefore, we will use a Bernstein matrix concentration inequality adapted for random matrices with subexponential norms (Koltchinskii et al. (2013)).\nB.1 Epsilon-net Let DC = {x\u2212 z | x, z \u2208 C} with diameter at most 2l where l is the diameter of C. Since C is supposed compact, so is DC . It is then possible to find an -net covering DC with at most T = (4l/r)d balls of radius r.\nLet us call \u03b4i, i = 1, . . . , T the center of the i-th ball, also called anchor of the -net. Denote LF the Lipschitz constant of F . Let \u2016.\u2016 be the `2 norm on L(Rp), that is the spectral norm. Now let use introduce the following lemma:\nLemma B.0.1. \u2200\u03b4 \u2208 DC , if (1): LF \u2264 2r and (2): \u2016F (\u03b4i)\u2016 \u2264 2 ,for all 0 < i < T , then \u2016F (\u03b4)\u2016 \u2264 .\nProof. \u2016F (\u03b4)\u2016 = \u2016F (\u03b4)\u2212 F (\u03b4i) + F (\u03b4i)\u2016 \u2264 \u2016F (\u03b4)\u2212 F (\u03b4i)\u2016+\u2016F (\u03b4i)\u2016, for all 0 < i < T . Using the Lipschitz continuity of F we have \u2016F (\u03b4)\u2212 F (\u03b4i)\u2016 \u2264 LF \u2016\u03b4 \u2212 \u03b4i\u2016 \u2264 rLF hence \u2016F (\u03b4)\u2016 \u2264 rLF + \u2016F (\u03b4i)\u2016.\nTo apply the lemma, we must bound the Lipschitz constant of the matrix-valued function F (condition (1)) and \u2016F (\u03b4i)\u2016, for all i = 1, . . . , T as well (condition (2)).\nB.2 Regularity condition We first establish that \u2202\u2202\u03b4EK\u0303(\u03b4) = E \u2202 \u2202\u03b4 K\u0303(\u03b4). Since K\u0303 is a finite dimensional matrix-valued function, we verify the integrability coefficient-wise, following Sutherland and Schneider (2015)\u2019s demonstration. Namely, without loss of generality we show [\n\u2202\n\u2202\u03b4 EK\u0303(\u03b4) ] lm = E \u2202 \u2202\u03b4 [ K\u0303(\u03b4) ] lm\nwhere [A]lm denotes the l-th row and m-th column element of the matrix A.\nProposition B.1 (Differentiation under the integral sign). Let X be an open subset of Rd and \u2126 be a measured space. Suppose that the function f : X \u00d7 \u2126\u2192 R verifies the following conditions:\n\u2022 f(x, \u03c9) is a measurable function of \u03c9 for each x in X .\n\u2022 For almost all \u03c9 in \u2126, the derivative \u2202f(x, \u03c9)/\u2202xi exists for all x in X .\n\u2022 There is an integrable function \u0398 : \u2126\u2192 R such that |\u2202f(x, \u03c9)/\u2202xi| \u2264 \u0398(\u03c9) for all x in X .\nThen \u2202\n\u2202xi \u222b \u2126 f(x, \u03c9)d\u03c9 = \u222b \u2126 \u2202 \u2202xi f(x, \u03c9)d\u03c9.\nDefine the function G\u0303i,l,mx,y (t, \u03c9) : R \u00d7 \u2126 \u2192 R by G\u0303i,l,mx,y (t, \u03c9) = [ K\u0303(x+ tei \u2212 y) ] lm = [ G\u0303ix,y(t, \u03c9) ] lm ,\nwhere ei is the i-th standard basis vector. Then G\u0303i,l,mx,y is integrable w.r.t. \u03c9 since\u222b \u2126 G\u0303i,l,mx,y (t, \u03c9)d\u03c9 = E [ K\u0303(x+ tei \u2212 y) ] lm = [K(x+ tei \u2212 y)]lm <\u221e.\nAdditionally for any \u03c9 in \u2126, \u2202/\u2202tG\u0303i,l,mx,y (t, \u03c9) exists and satisfies\nE \u2223\u2223\u2223\u2223 \u2202\u2202t G\u0303i,l,mx,y (t, \u03c9) \u2223\u2223\u2223\u2223 = E \u2223\u2223\u2223\u2223\u2223\u2223 1D D\u2211 j=1 A(\u03c9)lm ( sin\u3008y, \u03c9j\u3009 \u2202 \u2202t sin(\u3008x, \u03c9j\u3009+ t\u03c9ij) + cos\u3008y, \u03c9j\u3009 \u2202 \u2202t cos(\u3008x, \u03c9j\u3009+ t\u03c9ij) )\u2223\u2223\u2223\u2223\u2223\u2223 = E\n\u2223\u2223\u2223\u2223\u2223\u2223 1D D\u2211 j=1 A(\u03c9)lm (\u03c9ji sin\u3008y, \u03c9j\u3009 sin(\u3008x, \u03c9j\u3009+ t\u03c9ji)\u2212 \u03c9ji cos\u3008y, \u03c9j\u3009 cos(\u3008x, \u03c9j\u3009+ t\u03c9ji)) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 E  1 D D\u2211 j=1 |A(\u03c9)lm\u03c9ji sin\u3008y, \u03c9j\u3009 sin(\u3008x, \u03c9j\u3009+ t\u03c9ji)|+ |A(\u03c9)lm\u03c9ji cos\u3008y, \u03c9j\u3009 cos(\u3008x, \u03c9j\u3009+ t\u03c9ji)|\n \u2264 E  1 D D\u2211 j=1 2|A(\u03c9)lm\u03c9ji|\n . Hence\nE \u2223\u2223\u2223\u2223 \u2202\u2202tG\u0303ix,y(t, \u03c9) \u2223\u2223\u2223\u2223 \u2264 2E [|\u03c9 \u2297A(\u03c9)|] .\nwhich is assumed to exist since in finite dimensions all norms are equivalent and E\u00b5 [ \u2016\u03c9\u20162\u2016A(\u03c9)\u20162 ] is assume\nto exists. Thus applying proposition B.1 we have [ \u2202 \u2202\u03b4i EK\u0303(\u03b4) ] lm = E \u2202\u2202\u03b4i [ K\u0303(\u03b4) ] lm The same holds for y by symmetry. Combining the results for each component xi and for each element lm, we get that \u2202\u2202\u03b4EK\u0303(\u03b4) = E \u2202\u2202\u03b4 K\u0303(\u03b4).\nB.3 Bounding the Lipschitz constant Since F is differentiable, LF = \u2225\u2225\u2202F \u2202\u03b4 (\u03b4 \u2217) \u2225\u2225 where \u03b4\u2217 = arg max\u03b4\u2208DC\u2225\u2225\u2202F\u2202\u03b4 (\u03b4)\u2225\u2225.\nE\u00b5,\u03b4\u2217 [ L2f ] = E\u00b5,\u03b4\u2217 \u2225\u2225\u2225\u2225\u2225\u2202K\u0303\u2202\u03b4 (\u03b4\u2217)\u2212 \u2202K0\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u2225\u2225 2\n\u2264 E\u03b4\u2217 E\u00b5 \u2225\u2225\u2225\u2225\u2225\u2202K\u0303\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u2225\u2225 2 \u2212 2 \u2225\u2225\u2225\u2225\u2202K0\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u2225E\u00b5 \u2225\u2225\u2225\u2225\u2225\u2202K\u0303\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225\u2202K0\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u22252 \nUsing Jensen\u2019s inequality \u2225\u2225\u2225E\u00b5 \u2202K\u0303\u2202\u03b4 (\u03b4\u2217)\u2225\u2225\u2225 \u2264 E\u00b5\u2225\u2225\u2225\u2202K\u0303\u2202\u03b4 (\u03b4\u2217)\u2225\u2225\u2225 and \u2202\u2202\u03b4EK\u0303(\u03b4) = E \u2202\u2202\u03b4 K\u0303(\u03b4). Since K\u0303 (see ap-\npendix B.2), E\u00b5 \u2202K\u0303\u2202\u03b4 (\u03b4 \u2217) = \u2202\u2202\u03b4E\u00b5K\u0303(\u03b4 \u2217) = \u2202K0\u2202\u03b4 (\u03b4 \u2217) thus\nE\u00b5,\u03b4\u2217 [ L2f ] \u2264 E\u03b4\u2217 E\u00b5 \u2225\u2225\u2225\u2225\u2225\u2202K\u0303\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u2225\u2225 2 \u2212 2 \u2225\u2225\u2225\u2225\u2202K0\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u22252 + \u2225\u2225\u2225\u2225\u2202K0\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u22252 \n= E\u00b5,\u03b4\u2217 \u2225\u2225\u2225\u2225\u2225\u2202K\u0303\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u2225\u2225 2 \u2212 E\u03b4\u2217 \u2225\u2225\u2225\u2225\u2202K0\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u22252\n\u2264 E\u00b5,\u03b4\u2217 \u2225\u2225\u2225\u2225\u2225\u2202K\u0303\u2202\u03b4 (\u03b4\u2217) \u2225\u2225\u2225\u2225\u2225 2\n= E\u00b5,\u03b4\u2217 \u2225\u2225\u2225\u2225 \u2202\u2202\u03b4\u2217 cos\u3008\u03b4\u2217, \u03c9\u3009A(\u03c9) \u2225\u2225\u2225\u22252 = E\u00b5,\u03b4\u2217\u2016\u2212\u03c9 sin(\u3008\u03b4\u2217, \u03c9\u3009)\u2297A(\u03c9)\u20162\n\u2264 E\u00b5 [ \u2016\u03c9\u20162\u2016A(\u03c9)\u20162 ] , \u03c32p\nEventually applying Markov\u2019s inequality yields\nP { LF \u2265\n2r\n} = P { L2F \u2265 ( 2r )2} \u2264 \u03c32p ( 2r )2 . (13)\nB.4 Bounding F on a given anchor point \u03b4i To bound \u2016F (\u03b4i)\u20162, Hoeffding inequality devoted to matrix concentration Mackey et al. (2014) can be applied. We prefer here to turn to tighter and refined inequalities such as Matrix Bernstein inequalities (Sutherland and Schneider (2015) also pointed that for the scalar case).\nIf we had bounded ORFF, we could use the following Bernstein matrix concentration inequality proposed in Ahlswede and Winter; Tropp (2012); Koltchinskii et al. (2013).\nTheorem B.1 (Bounded non-commutative Bernstein concentration inequality). Verbatim from Theorem 3 of Koltchinskii et al. (2013), consider a sequence (Xj)Dj=1 of D independent Hermitian (here symmetric) p \u00d7 p random matrices that satisfy EXj = 0 and suppose that for some constant U > 0, \u2016Xj\u2016 \u2264 U for each index j. Denote BD =\n\u2225\u2225E[X21 + . . . X2D]\u2225\u2225. Then, for all \u2265 0, P\n \u2225\u2225\u2225\u2225\u2225\u2225 D\u2211 j=1 Xj \u2225\u2225\u2225\u2225\u2225\u2225 \u2265  \u2264 p exp ( \u2212\n2\n2BD + 2U /3\n)\nHowever, to cover the general case including unbounded ORFFs like curl and div-free ORFFs, we choose a version of Bernstein matrix concentration inequality proposed in Koltchinskii et al. (2013) that allow to consider matrices are not uniformly bounded but have subexponential tails.\nTheorem B.2 (Unbounded non-commutative Bernstein concentration inequality). Verbatim from Theorem 4 of Koltchinskii et al. (2013). Let X1, . . . , XD be independent Hermitian p\u00d7 p random matrices, such that EXj = 0 for all j = 1, . . . , D. Let \u03c8 = \u03c81. Define\nF(D) , D\u2211 j=1 Xj and BD , \u2225\u2225\u2225\u2225\u2225\u2225E  D\u2211 j=1 X2j \u2225\u2225\u2225\u2225\u2225\u2225. Suppose that,\nM = 2 max 1\u2264j\u2264D \u2016\u2016Xj\u2016\u2016\u03c8\nLet \u03b4 \u2208 ] 0; 2e\u22121 [ and\nU\u0304 ,M log\n( 2\n\u03b4\nM2 B2D + 1 ) Then, for U\u0304 \u2264 (e\u2212 1)(1 + \u03b4)BD,\nP {\u2225\u2225F(D)\u2225\u2225 \u2265 } \u2264 2p exp(\u2212 2\n2(1 + \u03b4)BD + 2 U\u0304/3\n) (14)\nand for U\u0304 > (e\u2212 1)(1 + \u03b4)BD, P {\u2225\u2225F(D)\u2225\u2225 \u2265 } \u2264 2p exp(\u2212\n(e\u2212 1)U\u0304\n) . (15)\nTo use this theorem, we set: Xj = Fj(\u03b4i). We have indeed: E\u00b5[Fj(\u03b4i)] = 0 since K\u0303(\u03b4i) is the Monte-Carlo approximation of K0(\u03b4i) and the matrices Fj(\u03b4i) are symmetric. We assume we can bound all the Orlicz norms of the Fj(\u03b4i) = 1D (K\u0303j(\u03b4i)\u2212K0(\u03b4i)). Please note that in the following we use a constant m such that m = DM ,\nm = 2D max 1\u2264j\u2264D \u2016\u2016Fj(\u03b4i)\u2016\u2016\u03c8\n\u2264 2 max 1\u2264j\u2264D \u2225\u2225\u2225\u2225\u2225\u2225K\u0303j(\u03b4i)\u2225\u2225\u2225\u2225\u2225\u2225 \u03c8 + 2\u2016\u2016K0(\u03b4i)\u2016\u2016\u03c8\n< 4 max 1\u2264j\u2264D \u2016\u2016A(\u03c9j)\u2016\u2016\u03c8 + 4\u2016K0(\u03b4i)\u2016\nThen U\u0304 can be re-written using m and D:\nU\u0304 = m\nD log\n( 2\n\u03b4\nm2 b2D + 1 ) We define: u\u0304 = DU\u0304 and bD = DBD. Then, we get: for U\u0304 \u2264 (e\u2212 1)(1 + \u03b4)BD,\nP {\u2016F (\u03b4i)\u2016 \u2265 } \u2264 2p exp ( \u2212 D 2\n2(1 + \u03b4)bD + 2 u\u0304/3\n) (16)\nand for U\u0304 > (e\u2212 1)(1 + \u03b4)BD, P {\u2016F (\u03b4i)\u2016 \u2265 } \u2264 2p exp ( \u2212 D\n(e\u2212 1)u\u0304\n) . (17)\nB.5 Union bound Now take the union bound over the centers of the -net:\nP { \u2217\u22c3 i=1 \u2016F (\u03b4i)\u2016 \u2265 2 } \u2264 4Tp exp ( \u2212\n2D\n8((1+\u03b4)bD+ 2 6 u\u0304)\n) if U\u0304D \u2264 (e\u2212 1)(1 + \u03b4)BD\nexp ( \u2212 D2(e\u22121)u\u0304 ) otherwise.\n(18)\nB.5.1 Optimizing over r Combining eq. (18) and eq. (13) and taking \u03b4 = 1 < 2/(e\u2212 1) yields\nP {\nsup \u03b4\u2208DC\n\u2016F (\u03b4)\u2016 \u2264 } \u2265 1\u2212 \u03ba1r\u2212d \u2212 \u03ba2r2,\nwith\n\u03ba2 = 4\u03c3 2 p \u22122 and \u03ba1 = 2p(4l)d exp ( \u2212\n2D\n16(bD+ 6 u\u0304D)\n) if U\u0304D \u2264 2(e\u2212 1)BD\nexp ( \u2212 D2(e\u22121)u\u0304D ) otherwise.\nwe choose r such that d\u03ba1r\u2212d\u22121 \u2212 2\u03ba2r = 0, i.e. r = ( d\u03ba1 2\u03ba2 ) 1 d+2 . Eventually let\nC \u2032d =\n(( d\n2\n) \u2212d d+2\n+\n( d\n2\n) 2 d+2 ) the bound becomes\nP {\nsup \u03b4\u2208DC \u2225\u2225\u2225F\u0303i(\u03b4)\u2225\u2225\u2225 \u2265 } \u2264 C \u2032d\u03ba 2d+21 \u03ba dd+22 = C \u2032d ( 4\u03c32p \u22122) dd+2 2p(4l)d exp ( \u2212\n2D\n16(BD+ 6 U\u0304D)\n) if U\u0304D \u2264 2(e\u2212 1)BD\nexp ( \u2212 D\n2(e\u22121)U\u0304D\n) otherwise\n 2 d+2\n= pC \u2032d2 2+4d+2d d+2\n( \u03c3pl ) 2d d+2 exp ( \u2212\n2\n8(d+2)(BD+ 6 U\u0304D)\n) if U\u0304D \u2264 2(e\u2212 1)BD\nexp ( \u2212\n(d+2)(e\u22121)U\u0304D\n) otherwise\n= pC \u2032d2 6d+2 d+2\n( \u03c3pl ) 2 1+2/d exp ( \u2212\n2\n8(d+2)(BD+ 6 U\u0304D)\n) if U\u0304D \u2264 2(e\u2212 1)BD\nexp ( \u2212\n(d+2)(e\u22121)U\u0304D\n) otherwise.\nConclude the proof by taking Cd = C \u2032d2 6d+2 d+2 ."}, {"heading": "C Application of the bounds to decomposable, curl-free, divergence-", "text": "free kernels\nProposition C.1 (Bounding the term bD). Define the random matrix V\u00b5[A(\u03c9)] as follows: `,m \u2208 {1, . . . , p}, V\u00b5[A(\u03c9)]`m = \u2211p r=1Cov\u00b5[(A(\u03c9)`r, A(\u03c9)rm]. For a given \u03b4 = x\u2212 z, with the previous notations\nbD = D \u2225\u2225\u2225\u2225\u2225\u2225E\u00b5  D\u2211 j=1 ( F\u0303j(\u03b4) )2\u2225\u2225\u2225\u2225\u2225\u2225 2 ,\nwe have: bD \u2264 1 2 ( \u2225\u2225(K0(2\u03b4) +K0(0))E\u00b5[A(\u03c9)]\u2212 2K0(\u03b4)2\u2225\u22252 + 2\u2016V\u00b5[A(\u03c9)]\u20162).\nProof. We fix \u03b4 = x \u2212 z. For sake of simplicity, we note: BD = \u2225\u2225E\u00b5 [F1(\u03b4)2 + . . .+ FD(\u03b4)2]\u2225\u22252 and we have bD = DBD, with the notations of the theorem. Then\nBD = \u2225\u2225\u2225\u2225\u2225\u2225E\u00b5  D\u2211 j=1 1 D2 (K\u0303j(\u03b4)\u2212K0(\u03b4))2 \u2225\u2225\u2225\u2225\u2225\u2225 2\n= 1\nD2 \u2225\u2225\u2225\u2225\u2225\u2225 D\u2211 j=1 E\u00b5 [( K\u0303j(\u03b4) 2 \u2212 K\u0303j(\u03b4)K0(\u03b4)\u2212K0(\u03b4)K\u0303j(\u03b4) +K0(\u03b4)2 )]\u2225\u2225\u2225\u2225\u2225\u2225\n2\n.\nAs K0(\u03b4)\u2217 = K0(\u03b4) and A(\u03c9j)\u2217 = A(\u03c9j), then K\u0303j(\u03b4)\u2217 = K\u0303j(\u03b4), we have\nBD = 1\nD2 \u2225\u2225\u2225\u2225\u2225\u2225 \u2211 j E\u00b5 [ K\u0303j(\u03b4) 2 \u2212 2K\u0303j(\u03b4)K0(\u03b4) +K0(\u03b4)2 ]\u2225\u2225\u2225\u2225\u2225\u2225\n2\n.\nFrom the definition of K\u0303j , E\u00b5[K\u0303j(\u03b4)] = K0(\u03b4) which leads to\nBD = 1\nD2 \u2225\u2225\u2225\u2225\u2225\u2225 D\u2211 j=1 E\u00b5 ( K\u0303j(\u03b4) 2 \u2212K0(\u03b4)2 )\u2225\u2225\u2225\u2225\u2225\u2225\n2\nNow we omit the j index since all vectors \u03c9j are identically distributed and consider a random vector \u03c9 \u223c \u00b5:\nBD = 1 D2 \u2225\u2225DE\u00b5 [(cos\u3008\u03c9, \u03b4\u3009)2A(\u03c9)2]\u2212K0(\u03b4)2\u2225\u22252\nA trigonometry property gives us: (cos\u3008\u03c9, \u03b4\u3009)2 = 12 (cos\u3008\u03c9, 2\u03b4\u3009+ cos\u3008\u03c9, 0\u3009)\nBD = 1\nD2 \u2225\u2225\u2225\u2225D2 E\u00b5 [(cos\u3008\u03c9, 2\u03b4\u3009+ cos\u3008\u03c9, 0\u3009)A(\u03c9)2]\u2212K0(\u03b4)2 \u2225\u2225\u2225\u2225\n2\n= 1\n2D \u2225\u2225\u2225\u2225E\u00b5 [(cos\u3008\u03c9, 2\u03b4\u3009+ cos\u3008\u03c9, 0\u3009)A(\u03c9)2]\u2212 2DK0(\u03b4)2 \u2225\u2225\u2225\u2225\n2\n(19)\nMoreover, we write the expectation of a matrix product, coefficient by coefficient, as: \u2200`,m \u2208 {1, . . . , p},\nE\u00b5 [( cos\u3008\u03c9, 2\u03b4\u3009A(\u03c9)2 )] `m = \u2211 r E\u00b5 [cos\u3008\u03c9, 2\u03b4\u3009A(\u03c9)]`r E\u00b5 [A(\u03c9)]rm + Cov\u00b5[cos\u3008\u03c9, 2\u03b4\u3009A(\u03c9)`r, A(\u03c9)rm]\nE\u00b5 [( cos\u3008\u03c9, 2\u03b4\u3009A(\u03c9)2 )] = E\u00b5[cos\u3008\u03c9, 2\u03b4\u3009A(\u03c9)]E\u00b5[A(\u03c9)] + \u03a3cos\n= K0(2\u03b4)E\u00b5[A(\u03c9)] + \u03a3cos\nwhere the random matrix \u03a3cos is defined by: \u03a3cos`m = \u2211 rCov\u00b5[cos\u3008\u03c9, 2\u03b4\u3009A(\u03c9)`r, A(\u03c9)rm]. Similarly, we get:\nE\u00b5 [ cos\u3008\u03c9, 0\u3009A(\u03c9)2 ] = K0(0)E\u00b5 [A(\u03c9)] + \u03a3cos.\nHence, we have:\nBD = 1\n2D \u2225\u2225(K0(2\u03b4) +K0(0))E\u00b5[A(\u03c9)]\u2212 2K0(\u03b4)2 + 2\u03a3cos\u2225\u22252 \u2264 1\n2D [\u2225\u2225(K0(2\u03b4) +K0(0))E\u00b5[A(\u03c9)]\u2212 2K0(\u03b4)2\u2225\u22252 + 2\u2016V\u00b5[A(\u03c9)]\u20162] , using \u2016\u03a3cos\u20162 \u2264 \u2016V\u00b5[A(\u03c9)]\u20162, where V\u00b5[A(\u03c9)] = E\u00b5[(A(\u03c9) \u2212 E\u00b5[A(\u03c9)])\n2] and for all `,m \u2208 {1, . . . , p}, V\u00b5[A(\u03c9)]`m = \u2211p r=1Cov\u00b5[A(\u03c9)`r, A(\u03c9)rm].\nFor the three kernels of interest, we illustrate this bound in fig. 6."}, {"heading": "D Additional information and results", "text": "D.1 Implementation detail For each \u03c9j \u223c \u00b5, let B(\u03c9j) be a p by p\u2032 matrix such that B(\u03c9j)B(\u03c9j)\u2217 = A(\u03c9j). In practice, making a prediction y = h(x) using directly the formula h(x) = \u03a6\u0303(x)\u2217\u03b8 is prohibitive. Indeed, if \u03a6(x) = \u2295D j=1 exp(\u2212i\u3008x, \u03c9j\u3009)B(\u03c9j)\u2217B(\u03c9j), it would cost O(Dp\u2032p) operation to make a prediction, since \u02dc\u03a6(x) is a Dp\u2032 by p matrix.\nD 100 101 102 103 104 105\nEmpirical Upper Bound\nVariance 10-4 10-3 10-2 10-1 100 101 102 E rr or\nDecomposable kernel\nD\n100 101 102 103 104\nEmpirical Upper Bound\nVariance 10-4 10-3 10-2 10-1 100 101 102 E rr or\nCurl-free kernel\nD.2 Simulated dataset"}], "references": [{"title": "Kernels for vector-valued functions: a review", "author": ["M.A. \u00c1lvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "\u00c1lvarez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "\u00c1lvarez et al\\.", "year": 2012}, {"title": "On the equivalence between quadrature rules and random features", "author": ["F. Bach"], "venue": null, "citeRegEx": "Bach.,? \\Q2015\\E", "shortCiteRegEx": "Bach.", "year": 2015}, {"title": "Vector field learning via spectral filtering", "author": ["L. Baldassarre", "L. Rosasco", "A. Barla", "A. Verri"], "venue": "editors, ECML/PKDD,", "citeRegEx": "Baldassarre et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baldassarre et al\\.", "year": 2010}, {"title": "Multi-output learning via spectral filtering", "author": ["L. Baldassarre", "L. Rosasco", "A. Barla", "A. Verri"], "venue": "Machine Learning,", "citeRegEx": "Baldassarre et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baldassarre et al\\.", "year": 2012}, {"title": "Vector valued reproducing kernel hilbert spaces and universality", "author": ["C. Carmeli", "E. De Vito", "A. Toigo", "V. Umanit\u00e0"], "venue": "Analysis and Applications,", "citeRegEx": "Carmeli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Carmeli et al\\.", "year": 2010}, {"title": "Learning output kernels with block coordinate descent", "author": ["F. Dinuzzo", "C. Ong", "P. Gehler", "G. Pillonetto"], "venue": "In Proc. of the 28th Int. Conf. on Machine Learning,", "citeRegEx": "Dinuzzo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dinuzzo et al\\.", "year": 2011}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Evgeniou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2005}, {"title": "A course in abstract harmonic analysis", "author": ["G.B. Folland"], "venue": "CRC press,", "citeRegEx": "Folland.,? \\Q1994\\E", "shortCiteRegEx": "Folland.", "year": 1994}, {"title": "Lsmr: An iterative algorithm for sparse least-squares problems", "author": ["D.C.-L. Fong", "M. Saunders"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Fong and Saunders.,? \\Q2011\\E", "shortCiteRegEx": "Fong and Saunders.", "year": 2011}, {"title": "Refined Error Estimates for Matrix-Valued Radial Basis Functions", "author": ["E. Fuselier"], "venue": "PhD thesis,", "citeRegEx": "Fuselier.,? \\Q2006\\E", "shortCiteRegEx": "Fuselier.", "year": 2006}, {"title": "Random features for kernel deep convex network", "author": ["P.-S. Huang", "L. Deng", "M. Hasegawa-Johnson", "X. He"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Huang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Using the fisher kernel method to detect remote protein homologies", "author": ["T. Jaakkola", "M. Diekhans", "D. Haussler"], "venue": "In ISMB,", "citeRegEx": "Jaakkola et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1999}, {"title": "A remark on low rank matrix recovery and noncommutative bernstein type inequalities. In From Probability to Statistics and Back: High-Dimensional Models and Processes\u2013A Festschrift in Honor of Jon A", "author": ["V. Koltchinskii"], "venue": "Institute of Mathematical Statistics,", "citeRegEx": "Koltchinskii,? \\Q2013\\E", "shortCiteRegEx": "Koltchinskii", "year": 2013}, {"title": "Fastfood - computing hilbert space expansions in loglinear time", "author": ["Q.V. Le", "T. Sarl\u00f3s", "A.J. Smola"], "venue": "In Proceedings of ICML 2013, Atlanta,", "citeRegEx": "Le et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Le et al\\.", "year": 2013}, {"title": "Operator-valued kernel-based vector autoregressive models for network inference", "author": ["N. Lim", "F. d\u2019Alch\u00e9-Buc", "C. Auliac", "G. Michailidis"], "venue": "Machine Learning,", "citeRegEx": "Lim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lim et al\\.", "year": 2015}, {"title": "Learning div-free and curl-free vector fields by matrix-valued kernels", "author": ["Y. Macedo", "R. Castro"], "venue": "Technical report,", "citeRegEx": "Macedo and Castro.,? \\Q2008\\E", "shortCiteRegEx": "Macedo and Castro.", "year": 2008}, {"title": "Matrix concentration inequalities via the method of exchangeable pairs", "author": ["L. Mackey", "M.I. Jordan", "R. Chen", "B. Farrel", "J. Tropp"], "venue": "The Annals of Probability,", "citeRegEx": "Mackey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mackey et al\\.", "year": 2014}, {"title": "On learning vector-valued functions", "author": ["C.A. Micchelli", "M.A. Pontil"], "venue": "Neural Computation,", "citeRegEx": "Micchelli and Pontil.,? \\Q2005\\E", "shortCiteRegEx": "Micchelli and Pontil.", "year": 2005}, {"title": "Matrix-valued kernels for shape deformation analysis", "author": ["M. Micheli", "J. Glaunes"], "venue": "Technical report, Arxiv report,", "citeRegEx": "Micheli and Glaunes.,? \\Q2013\\E", "shortCiteRegEx": "Micheli and Glaunes.", "year": 2013}, {"title": "Multiclass learning with simplex coding", "author": ["Y. Mroueh", "T. Poggio", "L. Rosasco", "J.-j. Slotine"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mroueh et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mroueh et al\\.", "year": 2012}, {"title": "The matrix cookbook", "author": ["K.B. Petersen", "M.S. Pedersen"], "venue": "Technical University of Denmark,", "citeRegEx": "Petersen and Pedersen,? \\Q2008\\E", "shortCiteRegEx": "Petersen and Pedersen", "year": 2008}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In NIPS 20,", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "Scalable matrix-valued kernel learning for high-dimensional nonlinear multivariate regression and granger causality", "author": ["V. Sindhwani", "H.Q. Minh", "A. Lozano"], "venue": "In Proc. of UAI\u201913,", "citeRegEx": "Sindhwani et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sindhwani et al\\.", "year": 2013}, {"title": "Idr (s): A family of simple and fast algorithms for solving large nonsymmetric systems of linear equations", "author": ["P. Sonneveld", "M.B. van Gijzen"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Sonneveld and Gijzen.,? \\Q2008\\E", "shortCiteRegEx": "Sonneveld and Gijzen.", "year": 2008}, {"title": "Optimal rates for random fourier features", "author": ["B. Sriperumbudur", "Z. Szabo"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sriperumbudur and Szabo.,? \\Q2015\\E", "shortCiteRegEx": "Sriperumbudur and Szabo.", "year": 2015}, {"title": "On the error of random fourier features", "author": ["D.J. Sutherland", "J.G. Schneider"], "venue": "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Sutherland and Schneider.,? \\Q2015\\E", "shortCiteRegEx": "Sutherland and Schneider.", "year": 2015}, {"title": "User-friendly tail bounds for sums of random matrices", "author": ["J.-A. Tropp"], "venue": "Comput. Math.,", "citeRegEx": "Tropp.,? \\Q2012\\E", "shortCiteRegEx": "Tropp.", "year": 2012}, {"title": "Modeling magnetic fields using gaussian processes", "author": ["N. Wahlstr\u00f6m", "M. Kok", "T. Sch\u00f6n", "F. Gustafsson"], "venue": "In in Proc. of the 38th ICASSP,", "citeRegEx": "Wahlstr\u00f6m et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wahlstr\u00f6m et al\\.", "year": 2013}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["T. Yang", "Y.-F. Li", "M. Mahdavi", "R. Jin", "Z. Zhou"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2012}, {"title": "A la carte - learning fast kernels", "author": ["Z. Yang", "A.J. Smola", "L. Song", "A.G. Wilson"], "venue": "CoRR, abs/1412.6493,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "A la carte - learning fast kernels", "author": ["Z. Yang", "A.G. Wilson", "A.J. Smola", "L. Song"], "venue": "In Proc. of AISTATS", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Refinement of operator-valued reproducing kernels", "author": ["H. Zhang", "Y. Xu", "Q. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Compared to the scalar case, the proof follows the same scheme as the one described in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015) but requires to consider matrix norms and appropriate matrix concentration inequality. The main feature of theorem 3.1 is that it covers the case of bounded ORFF as well as unbounded ORFF: in the case of bounded ORFF, a Bernstein inequality for matrix concentration such that the one proved in Mackey et al", "author": ["Koltchinskii"], "venue": null, "citeRegEx": "Koltchinskii,? \\Q2012\\E", "shortCiteRegEx": "Koltchinskii", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "1 Introduction Multi-task regression (Micchelli and Pontil, 2005), structured classification (Dinuzzo et al.", "startOffset": 37, "endOffset": 65}, {"referenceID": 5, "context": "1 Introduction Multi-task regression (Micchelli and Pontil, 2005), structured classification (Dinuzzo et al., 2011), vector field learning (Baldassarre et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 3, "context": ", 2011), vector field learning (Baldassarre et al., 2012) and vector autoregression (Sindhwani et al.", "startOffset": 31, "endOffset": 57}, {"referenceID": 22, "context": ", 2012) and vector autoregression (Sindhwani et al., 2013; Lim et al., 2015) are all learning problems that boil down to learning a vector while taking into account an appropriate output structure.", "startOffset": 34, "endOffset": 76}, {"referenceID": 14, "context": ", 2012) and vector autoregression (Sindhwani et al., 2013; Lim et al., 2015) are all learning problems that boil down to learning a vector while taking into account an appropriate output structure.", "startOffset": 34, "endOffset": 76}, {"referenceID": 17, "context": "Operator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; \u00c1lvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions.", "startOffset": 24, "endOffset": 96}, {"referenceID": 4, "context": "Operator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; \u00c1lvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions.", "startOffset": 24, "endOffset": 96}, {"referenceID": 0, "context": "Operator-Valued Kernels (Micchelli and Pontil, 2005; Carmeli et al., 2010; \u00c1lvarez et al., 2012) extend the classic scalar-valued kernels to vector-valued functions.", "startOffset": 24, "endOffset": 96}, {"referenceID": 21, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 13, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 29, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 24, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 1, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 25, "context": "Therefore, focusing on the case Y = R, we propose to approximate OVKs by extending a methodology called Random Fourier Features (RFFs) (Rahimi and Recht, 2007; Le et al., 2013; Yang et al., 2014; Sriperumbudur and Szabo, 2015; Bach, 2015; Sutherland and Schneider, 2015) so far developed to speed up scalar-valued kernel machines.", "startOffset": 135, "endOffset": 270}, {"referenceID": 13, "context": "RFFs has been shown to be efficient on large datasets and further improved by efficient matrix computations of FastFood (Le et al., 2013), and is considered as one of the best large scale implementations of kernel methods, along with N\u00ffstrom approaches (Yang et al.", "startOffset": 120, "endOffset": 137}, {"referenceID": 28, "context": ", 2013), and is considered as one of the best large scale implementations of kernel methods, along with N\u00ffstrom approaches (Yang et al., 2012).", "startOffset": 123, "endOffset": 142}, {"referenceID": 21, "context": "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015).", "startOffset": 128, "endOffset": 152}, {"referenceID": 21, "context": "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015).", "startOffset": 128, "endOffset": 232}, {"referenceID": 21, "context": "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015). Finally, it is important to notice that Random Fourier Feature approach 1See Rudin (1994).", "startOffset": 128, "endOffset": 264}, {"referenceID": 21, "context": "The dimension D governs the precision of this approximation whose uniform convergence towards the target kernel can be found in Rahimi and Recht (2007) and in more recent papers with some refinements Sutherland and Schneider (2015); Sriperumbudur and Szabo (2015). Finally, it is important to notice that Random Fourier Feature approach 1See Rudin (1994).", "startOffset": 128, "endOffset": 355}, {"referenceID": 21, "context": "For the Gaussian kernel k(x \u2212 z) = exp(\u2212\u03b3\u2016x\u2212 z\u2016), the spectral distribution \u03bc(\u03c9) is Gaussian Rahimi and Recht (2007).", "startOffset": 93, "endOffset": 117}, {"referenceID": 4, "context": "We will define operator-valued kernel as reproducing kernels following the presentation of Carmeli et al. (2010). Given X and Y , a map K : X \u00d7 X \u2192 L(Y) is called a Y-reproducing kernel if", "startOffset": 91, "endOffset": 113}, {"referenceID": 4, "context": "As a consequence (Carmeli et al., 2010) we have: K(x, z) = K\u2217 xKz \u2200x, z \u2208 X HK = span {Kxy | \u2200x \u2208 X , \u2200y \u2208 Y} Another way to describe functions ofHK consists in using a suitable feature map.", "startOffset": 17, "endOffset": 39}, {"referenceID": 4, "context": "As a consequence (Carmeli et al., 2010) we have: K(x, z) = K\u2217 xKz \u2200x, z \u2208 X HK = span {Kxy | \u2200x \u2208 X , \u2200y \u2208 Y} Another way to describe functions ofHK consists in using a suitable feature map. Proposition 1.1 (Carmeli et al. (2010)).", "startOffset": 18, "endOffset": 230}, {"referenceID": 4, "context": "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)).", "startOffset": 44, "endOffset": 66}, {"referenceID": 4, "context": "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)).", "startOffset": 44, "endOffset": 278}, {"referenceID": 4, "context": "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)). In a few words, Pontryagin duality deals with functions on locally compact Abelian groups, and allows to define their Fourier transform in a very general way. For sake of simplicity, we instantiate the general results of Carmeli et al. (2010); Zhang et al.", "startOffset": 44, "endOffset": 523}, {"referenceID": 4, "context": "For this purpose, we build upon the work of Carmeli et al. (2010) that introduced the Fourier representation of shift-invariant Operator-Valued Mercer Kernels on locally compact Abelian groups X using the general framework of Pontryagin duality (see for instance Folland (1994)). In a few words, Pontryagin duality deals with functions on locally compact Abelian groups, and allows to define their Fourier transform in a very general way. For sake of simplicity, we instantiate the general results of Carmeli et al. (2010); Zhang et al. (2012) for our case of interest of X = R and Y = R.", "startOffset": 44, "endOffset": 544}, {"referenceID": 31, "context": "2Equation (36) in Zhang et al. (2012).", "startOffset": 18, "endOffset": 38}, {"referenceID": 4, "context": "13 in Carmeli et al. (2010) to matrix-valued operators.", "startOffset": 6, "endOffset": 28}, {"referenceID": 4, "context": "13 in Carmeli et al. (2010) to matrix-valued operators. Proposition 2.2 (Carmeli et al. (2010)).", "startOffset": 6, "endOffset": 95}, {"referenceID": 4, "context": "This theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, \u03bc is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1).", "startOffset": 26, "endOffset": 48}, {"referenceID": 4, "context": "This theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, \u03bc is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1). Now we introduce the following proposition that directly is a direct consequence of proposition 2.2. Proposition 2.3 (Feature map). Given the conditions of proposition 2.2, we defineB(\u03c9) such that A(\u03c9) = B(\u03c9)B(\u03c9)\u2217. Then the function \u03a6 : R \u2192 L(R, \u03bc;R) defined for all x \u2208 R by \u2200y \u2208 R, (\u03a6xy) (\u03c9) = ei\u3008x,\u03c9\u3009B(\u03c9)\u2217y (5) is a feature map of the shift-invariant kernelK, i.e. it satisfies for all x, z in R, \u03a6x\u03a6z = K(x, z). Thus, to define an approximation of a given operator-valued kernel, we need an inversion theorem that provides an explicit construction of the pair A(\u03c9), \u03bc(\u03c9) from the kernel signature. Proposition 14 in Carmeli et al. (2010), instantiated to R-Mercer kernel gives the solution.", "startOffset": 26, "endOffset": 868}, {"referenceID": 4, "context": "This theorem is proved in Carmeli et al. (2010). When p = 1 one can always assumeA is reduced to the scalar 1, \u03bc is still a bounded positive measure and we retrieve the Bochner theorem applied to the scalar case (footnote 1). Now we introduce the following proposition that directly is a direct consequence of proposition 2.2. Proposition 2.3 (Feature map). Given the conditions of proposition 2.2, we defineB(\u03c9) such that A(\u03c9) = B(\u03c9)B(\u03c9)\u2217. Then the function \u03a6 : R \u2192 L(R, \u03bc;R) defined for all x \u2208 R by \u2200y \u2208 R, (\u03a6xy) (\u03c9) = ei\u3008x,\u03c9\u3009B(\u03c9)\u2217y (5) is a feature map of the shift-invariant kernelK, i.e. it satisfies for all x, z in R, \u03a6x\u03a6z = K(x, z). Thus, to define an approximation of a given operator-valued kernel, we need an inversion theorem that provides an explicit construction of the pair A(\u03c9), \u03bc(\u03c9) from the kernel signature. Proposition 14 in Carmeli et al. (2010), instantiated to R-Mercer kernel gives the solution. Proposition 2.4 (Carmeli et al. (2010)).", "startOffset": 26, "endOffset": 960}, {"referenceID": 14, "context": "In the following, we give an explicit construction of ORFFs for three well-known RMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al.", "startOffset": 156, "endOffset": 184}, {"referenceID": 13, "context": "In the following, we give an explicit construction of ORFFs for three well-known RMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al.", "startOffset": 271, "endOffset": 296}, {"referenceID": 2, "context": "In the following, we give an explicit construction of ORFFs for three well-known RMercer and shift-invariant kernels: the decomposable kernel introduced in Micchelli and Pontil (2005) for multi-task regression and the curl-free and the divergence-free kernels studied in Macedo and Castro (2008); Baldassarre et al. (2012) for vector field learning.", "startOffset": 297, "endOffset": 323}, {"referenceID": 10, "context": "A usual choice is to choose k as a Gaussian kernel with k0(\u03b4) = exp ( \u2212 2 2\u03c32 ) , which gives \u03bc = N (0, \u03c3\u22122I) (Huang et al., 2013) as its inverse Fourier transform.", "startOffset": 110, "endOffset": 130}, {"referenceID": 5, "context": "When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015).", "startOffset": 172, "endOffset": 236}, {"referenceID": 22, "context": "When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015).", "startOffset": 172, "endOffset": 236}, {"referenceID": 14, "context": "When no prior knowledge is available, A can be set to the empirical covariance of the output training data or learned with one of the algorithms proposed in the literature (Dinuzzo et al., 2011; Sindhwani et al., 2013; Lim et al., 2015).", "startOffset": 172, "endOffset": 236}, {"referenceID": 3, "context": "If a graph coding for the proximity between tasks is known, then it is shown in Evgeniou et al. (2005); Baldassarre et al.", "startOffset": 80, "endOffset": 103}, {"referenceID": 2, "context": "(2005); Baldassarre et al. (2010) that A can be chosen equal to the pseudo inverse L\u2020 of the graph Laplacian, and then the `2 norm in HK is a graph-regularizing penalty for the outputs (tasks).", "startOffset": 8, "endOffset": 34}, {"referenceID": 15, "context": "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p).", "startOffset": 141, "endOffset": 219}, {"referenceID": 3, "context": "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p).", "startOffset": 141, "endOffset": 219}, {"referenceID": 18, "context": "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p).", "startOffset": 141, "endOffset": 219}, {"referenceID": 18, "context": "Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlstr\u00f6m et al.", "startOffset": 46, "endOffset": 73}, {"referenceID": 27, "context": "Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlstr\u00f6m et al., 2013).", "startOffset": 109, "endOffset": 133}, {"referenceID": 2, "context": "ORFF for curl-free and div-free kernels: Curl-free and divergence-free kernels provide an interesting application of operator-valued kernels (Macedo and Castro, 2008; Baldassarre et al., 2012; Micheli and Glaunes, 2013) to vector field learning, for which input and output spaces have the same dimensions (d = p). Applications cover shape deformation analysis (Micheli and Glaunes, 2013) and magnetic fields approximations (Wahlstr\u00f6m et al., 2013). These kernels discussed in Fuselier (2006) allow encoding input-dependent similarities between vector-fields.", "startOffset": 167, "endOffset": 492}, {"referenceID": 15, "context": "Although taken separately these kernels are not universal, a convex combination of the curl-free and divergence-free kernels allows to learn any vector field that satisfies the Helmholtz decomposition theorem (Macedo and Castro, 2008; Baldassarre et al., 2012).", "startOffset": 209, "endOffset": 260}, {"referenceID": 3, "context": "Although taken separately these kernels are not universal, a convex combination of the curl-free and divergence-free kernels allows to learn any vector field that satisfies the Helmholtz decomposition theorem (Macedo and Castro, 2008; Baldassarre et al., 2012).", "startOffset": 209, "endOffset": 260}, {"referenceID": 25, "context": "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2 = \u2016A\u20162 \u2225\u2225\u2225k\u0303(x, z)\u2212 k(x, z)\u2225\u2225\u2225 This theorem and its proof are presented in corollary A.", "startOffset": 218, "endOffset": 281}, {"referenceID": 24, "context": "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2 = \u2016A\u20162 \u2225\u2225\u2225k\u0303(x, z)\u2212 k(x, z)\u2225\u2225\u2225 This theorem and its proof are presented in corollary A.", "startOffset": 218, "endOffset": 281}, {"referenceID": 16, "context": "It relies on three main ideas: (i) Matrix concentration inequality for random matrices has to be used instead of concentration inequality for (scalar) random variables, (ii) Instead of using Hoeffding inequality as in the scalar case (proof of Rahimi and Recht (2007)) but for matrix concentration (Mackey et al., 2014) we use a refined inequality such as the Bernstein matrix inequality (Ahlswede and Winter; Boucheron et al.", "startOffset": 298, "endOffset": 319}, {"referenceID": 26, "context": ", 2014) we use a refined inequality such as the Bernstein matrix inequality (Ahlswede and Winter; Boucheron et al.; Tropp, 2012), also used for the scalar case in (Sutherland and Schneider, 2015), (iii) we propose a general theorem valid for random matrices with bounded norms (case for decomposable kernel ORFF approximation) as well as with unbounded norms (curl and divergence-free kernels).", "startOffset": 76, "endOffset": 128}, {"referenceID": 25, "context": "; Tropp, 2012), also used for the scalar case in (Sutherland and Schneider, 2015), (iii) we propose a general theorem valid for random matrices with bounded norms (case for decomposable kernel ORFF approximation) as well as with unbounded norms (curl and divergence-free kernels).", "startOffset": 49, "endOffset": 81}, {"referenceID": 19, "context": "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2 = \u2016A\u20162 \u2225\u2225\u2225k\u0303(x, z)\u2212 k(x, z)\u2225\u2225\u2225 This theorem and its proof are presented in corollary A.", "startOffset": 176, "endOffset": 200}, {"referenceID": 19, "context": "In the case of the decomposable kernel, the answer to that question can be directly obtained as a consequence of the uniform convergence of RFFs in the scalar case obtained by Rahimi and Recht (2007) and other authors (Sutherland and Schneider, 2015; Sriperumbudur and Szabo, 2015) since in this case, \u2225\u2225\u2225K\u0303(x, z)\u2212K(x, z)\u2225\u2225\u2225 2 = \u2016A\u20162 \u2225\u2225\u2225k\u0303(x, z)\u2212 k(x, z)\u2225\u2225\u2225 This theorem and its proof are presented in corollary A.1.1. More interestingly, we propose a new bound for Operator Random Fourier Feature approximation in the general case. It relies on three main ideas: (i) Matrix concentration inequality for random matrices has to be used instead of concentration inequality for (scalar) random variables, (ii) Instead of using Hoeffding inequality as in the scalar case (proof of Rahimi and Recht (2007)) but for matrix concentration (Mackey et al.", "startOffset": 176, "endOffset": 801}, {"referenceID": 12, "context": "For the latter, we notice that their norms behave as subexponential random variables (Koltchinskii et al., 2013). Before introducing the new theorem, we give the definition of the Orlicz norm and subexponential random variables. Definition 3.1 (Orlicz norm). We follow the definition given by Koltchinskii et al. (2013). Let \u03c8 : R+ \u2192 R+ be a non-decreasing convex function with \u03c8(0) = 0.", "startOffset": 86, "endOffset": 320}, {"referenceID": 21, "context": "It follows the usual scheme derived in Rahimi and Recht (2007) and Sutherland and Schneider (2015) and involves Bernstein concentration inequality for unbounded symmetric matrices (theorem B.", "startOffset": 39, "endOffset": 63}, {"referenceID": 21, "context": "It follows the usual scheme derived in Rahimi and Recht (2007) and Sutherland and Schneider (2015) and involves Bernstein concentration inequality for unbounded symmetric matrices (theorem B.", "startOffset": 39, "endOffset": 99}, {"referenceID": 19, "context": "To predict classes, we use simplex coding method presented in Mroueh et al. (2012). The intuition behind simplex coding is to project the binarized labels of dimension p onto the most separated vectors on the hypersphere of dimension p \u2212 1.", "startOffset": 62, "endOffset": 83}], "year": 2016, "abstractText": "Devoted to multi-task learning and structured output learning, operator-valued kernels provide a flexible tool to build vector-valued functions in the context of Reproducing Kernel Hilbert Spaces. To scale up these methods, we extend the celebrated Random Fourier Feature methodology to get an approximation of operatorvalued kernels. We propose a general principle for Operator-valued Random Fourier \u2217ro.brault@telecom-paristech.fr \u2020florence.dalche@telecom-paristech.fr \u2021markus.o.heinonen@aalto.fi 1 ar X iv :1 60 5. 02 53 6v 3 [ cs .L G ] 2 4 M ay 2 01 6 Feature construction relying on a generalization of Bochner\u2019s theorem for translationinvariant operator-valued Mercer kernels. We prove the uniform convergence of the kernel approximation for bounded and unbounded operator random Fourier features using appropriate Bernstein matrix concentration inequality. An experimental proof-of-concept shows the quality of the approximation and the efficiency of the corresponding linear models on example datasets.", "creator": "LaTeX with hyperref package"}}}