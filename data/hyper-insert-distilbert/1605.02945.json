{"id": "1605.02945", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2016", "title": "The Yahoo Query Treebank, V. 1.0", "abstract": "a description engine and log annotation style guidelines specifically for searching the yahoo data webscope release trailer of query treebank, version 1. 0, may 2016.", "histories": [["v1", "Tue, 10 May 2016 11:29:28 GMT  (15kb)", "http://arxiv.org/abs/1605.02945v1", "Co-released with the Webscope Dataset (L-28) and with Pinter et al., Syntactic Parsing of Web Queries with Question Intent, NAACL-HLT 2016"], ["v2", "Wed, 11 May 2016 17:20:26 GMT  (15kb)", "http://arxiv.org/abs/1605.02945v2", "Co-released with the Webscope Dataset (L-28) and with Pinter et al., Syntactic Parsing of Web Queries with Question Intent, NAACL-HLT 2016"]], "COMMENTS": "Co-released with the Webscope Dataset (L-28) and with Pinter et al., Syntactic Parsing of Web Queries with Question Intent, NAACL-HLT 2016", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["yuval pinter", "roi reichart", "idan szpektor"], "accepted": false, "id": "1605.02945"}, "pdf": {"name": "1605.02945.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["me@yuvalpinter.com", "roiri@ie.technion.ac.il", "idan@yahoo-inc.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n02 94\n5v 1\n[ cs\n.C L\n] 1\n0 M\nay 2\n01 6\nThe Yahoo Query Treebank, V. 1.0 May 2016\nYuval Pinter Yahoo Research\nme@yuvalpinter.com\nRoi Reichart Yahoo Research & Technion IIT\nroiri@ie.technion.ac.il\nIdan Szpektor Yahoo Research\nidan@yahoo-inc.com"}, {"heading": "1 General", "text": "This dataset release accompanies Pinter et al. (2016) which describes the motivation and grammatical theory. Please cite that paper when referencing the dataset.\nThe dataset may be accessed via the Yahoo Webscope homepage1 under Linguistic Data as dataset L-28. The description in Section 2 is included within the dataset as a Readme.\nThe dataset is sure to have annotation errors which are not covered by the special cases specified in this document. Please approach the first author for any corrections and they will appear in the next release. See Section 4 for known errors."}, {"heading": "2 Dataset Description", "text": "User queries annotated for syntactic dependency parsing, version 1.0."}, {"heading": "2.1 Full description", "text": "This dataset contains two files:\nydata-search-parsed-queries-dev-v1 0.txt 1,000 queries, 5,344 tokens ydata-search-parsed-queries-test-v1 0.txt 4,000 queries, 26,015 tokens\nThese files differ in their level of annotation, but share the schema. They contain tab-delimited lines, each representing a single token in a Web query. The tokens in each query are given sequentially, and queries are given in order of an arbitrarily-selected numeric ID (with no empty lines between queries). The field schema is detailed in Table 1.\nAn example query is brought in Table 2. These lines represent a query, whose ID in the \u2018test\u2019 set is 183, and whose raw form is charter school graduate early. It is interpretable thus: the query is composed of two syntactic segments: [charter school]\n1 http://webscope.sandbox.yahoo.com\n[graduate early]. In the first segment, the syntactic root is the noun school, and the noun charter modifies it in a nominal compound modifier relation. The second segment is rooted by the verb graduate, modified by the adverb early in an adverbial modifier relation. A dependency tree corresponding to this query is produced in Figure 1."}, {"heading": "2.2 Linguistic pre-processing notes", "text": "All queries were tokenized using the ClearNLP tokenizer for English (Choi and McCallum, 2013) 2 and were not spell-corrected or filtered for adult content. For Excel-friendliness, initial quotes were replaced with backticks. When using offthe-shelf processing tools, we recommend to rereplace them in pre-processing."}, {"heading": "3 Annotation Guidelines", "text": "The parse tree annotations followed the segmentation annotation. Both phases included supervision over automatic part-of-speech tagging. In general, parse tree annotators were instructed to adhere to the Stanford Dependency guidelines (De Marneffe and Manning, 2008)3 with the nec-\n2Version 2.0.1, from www.clearnlp.com 3Version 3.5.2, from http://nlp.stanford.edu/software/dependencies_manual.pdf\nessary caveats that come with non-standard text prone to errors.\nFollowing are some selected issues which can be seen in the test dataset. Section 3.1 describes issues relating to segmentation tagging, and the subsequent subsections address issues of dependency edge attachment."}, {"heading": "3.1 Segmentation Ambiguity", "text": "Noun Strings The most difficult segmentation decisions have been in cases of long strings of nouns, both common and proper. The guideline is based on judgment, asking whether the phrase can stand as a constituent in a coherent sentence. For example, in query 116 words 1-3 are big mac cost, a phrase considered clunky as opposed to the much-preferred cost of (a) big mac, resulting in the decision to segment it as [big mac] [cost]. A more clear-cut case is the segmentation decision in q.284: [first day of missed period] [symptoms], where theoretically day could be tagged as a modifier phrase head of symptoms, but no imaginable well-formed sentence would use this formation as a constituent.\nSometimes reasonable semantics forced us to conclude towards a segmentation decision over an unlikely (but syntactically well-formed) singlesegment constituent. For example, in q.271 [fanfiction] [guest reviews] we assume (and support this decision by running our own search) that the searcher was looking for guest reviews on fanfiction website platforms rather than guest reviews written about pieces of fanfiction. Likewise, in q.3013 [questionnaire] [assisted suicide] we treated assisted as an adjective relating to suicide rather than a main verb in a sentence describing an unlikely scenario.\nMissing Auxiliary Verbs Another issue was certain constructions which may become a sen-\ntence by the addition of a copula or auxiliary verb. The guidelines stated that if the post-copular sequence describes an attribute of the semantic subject (pre-copula), it is considered the head of an adjunct phrase and shares its segment (e.g. q.2916 [paragraph describing a family], q.91.w.14: [battery cables not tight]). If it is construed as a sentence missing an auxiliary, it is segmented away from the subject (e.g. q.475 [lab] [throwing up blood ?]). In a way, this is an explicit application of the constituency test described earlier."}, {"heading": "3.2 Proper Names\u2019 Internal Structure", "text": "The dataset contains many instances of product names followed by numbers denoting model (e.g. query 2347: how to replace crank sensor on 95 saab cs9000. The guidelines in these cases were to place the phrase head as the last alphabetic word (here, saab) and depend the post-modifier (cs9000) on it as an npadvmod.\nLong proper names with complex internal structures were left as their underlying structures. E.g. query 3252: the1 call of the2 wild is parsed with call as the phrase head with a det preceding and a prep following, further decomposed into pobj(of,wild) and det(wild,the2). However, more trivial internal structures were flattened into the common proper name representation: q.3260, jeeves & wooster: nn(wooster,jeeves), nn(wooster,&). Nominal compounds were tagged as successive NNPs, e.g. q.3413, what cameras do they use in planet NNP earth NNP: nn(earth,planet)."}, {"heading": "3.3 Truncated Sentences", "text": "Many of the queries in the dataset are in fact sentences aborted mid-way. Some because the query is a sentence-completion question (e.g. query 970 a major result of the european age of exploration was) and some for more opaque reasons (e.g.\nq.3828 why are russians so). In both these types, where the root of the sentence would normally lie in the missing complement, the root was assigned to the token nearest to it in the assumed full graph (was and so, respectively), with the other dependents collapsed unto it. The same goes for any phrase which is truncated before its grammatical head (e.g. q.3840) or mandatory complement (e.g. q.1861)."}, {"heading": "3.4 Foreign Languages", "text": "The dataset contains several non-English queries that are treated as nonce (part-of-speech tag = \u2018FW\u2019, dependency relation = \u2018dep\u2019). Their parse tree is, as a rule, a flat tree headed by the final word (proper name convention). E.g. q.3299, q.3319. Other cases where foreign words are tagged as \u2018FW\u2019 is when they function metalinguistically. E.g. q.3472, what does baka FW mean in japanese."}, {"heading": "3.5 Grammatical Errors", "text": "By the nature of Web Queries being written in real time by users possessing diverse proficiency and competence, the dataset contains many grammatical errors (as well as typos \u2013 see Section 3.6). These were not corrected during pre-processing in order to maintain the authenticity of the data. The guidelines in such cases, unless re-segmentation was in order, called to retain as much of the intended structure as possible. For example, in query 3274, word 4 part is meant to be parts and as such is tagged as a plural noun. In some cases, different parts of the sentence were fused together due to incorrect grammar. The solution was to represent the fused token by the head of the intended phrase if it is fully contained within (e.g. q.3233, dogs for dog \u2019s), or ignore a dependent if there is structural crossover (e.g. in q.3590, my sister in laws husband, the possessive \u2019s was in effect excluded from the tree).\nAnother common case was auxiliary deletion common in ESL writing, e.g. query 3949 why you study in university. Here we opted again for the intended meaning as a sentence missing the auxiliary do rather than treating the full query as a constituent (adverbial clause, clausal complement, etc.) of a deleted governing predicate."}, {"heading": "3.6 Typographical Errors", "text": "In general, obvious typos were treated as the intended word whether they resulted in a legal En-\nglish word or not. E.g. q.3 w.13, trianlgle NN, or q.3786 w.5 if VBZ (for the intended is). Sometimes, typos result in word merge, in which case they were either POS-tagged as XX (e.g. q.252 w.3) or by the head if they can be construed as a coherent phrase (e.g. q.3115 w.1 searchwhere, which is probably a mis-concatenation of a meta-linguistic search with the first intended term where, and so analyzed as if it were the latter alone).\nSometimes, extremely creative tokenization is employed by users. Behold the glory that is query 1814: green chemistry.in day today life. We parsed day today as if it were a noun phrase, and gave up representing the preposition in altogether."}, {"heading": "3.7 BE-sentences", "text": "Sentences with forms of the verb BE are often ambiguous between attributive sentences (where the BE-verb acts as copula to the head of the following phrase) and proper essential statements where BE is the main verb. We tended to go for the former in case of ambiguity, excepting for very clear cases of essence (e.g. query 3264 the free market is a myth: root(ROOT,is)) and, of course, where the following can only be a clausal or prepositional complement (e.g. q.799 trouble is you think you have time, q.3593 what is on sheldons shirt in season 1 episode 4)."}, {"heading": "4 Known Annotation Errors", "text": ""}, {"heading": "4.1 Segmentation Errors", "text": "Query ID V 1.0 Correct 194 1,3 1 304 1,3 1 325 1,3 1 362 1,3 1 425 1,4 1 847 1,3 1 911 1,4 1 3779 1,5 1 1147 1 1,3 1812 1 1,4 2784 1 1,2,3 2883 1 1,7 2912 1 1,5,7 3348 1 1,2 3366 1 1,2,3,4"}, {"heading": "4.2 Attachment Errors", "text": "Query.Token V 1.0 Correct 3153.6 2 7 3153.7 6 2\nCredits\nSegmentation tagged by Bettina Bolla, Avihai Mejer, Yuval Pinter, Roi Reichart, and Idan Szpektor. Parsing tagged by Shir Givoni and Yuval Pinter."}], "references": [{"title": "Transition-based dependency parsing with selectional branching", "author": ["Choi", "McCallum2013] Jinho D Choi", "Andrew McCallum"], "venue": "ACL", "citeRegEx": "Choi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2013}, {"title": "Stanford typed dependencies manual", "author": ["De Marneffe", "Christopher D Manning"], "venue": "Technical report,", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Syntactic parsing of web queries with question intent", "author": ["Pinter et al.2016] Yuval Pinter", "Roi Reichart", "Idan Szpektor"], "venue": "In Proceedings of NAACLHLT,", "citeRegEx": "Pinter et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pinter et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "This dataset release accompanies Pinter et al. (2016) which describes the motivation and grammatical theory.", "startOffset": 33, "endOffset": 54}], "year": 2017, "abstractText": "This dataset release accompanies Pinter et al. (2016) which describes the motivation and grammatical theory. Please cite that paper when referencing the dataset. The dataset may be accessed via the Yahoo Webscope homepage1 under Linguistic Data as dataset L-28. The description in Section 2 is included within the dataset as a Readme. The dataset is sure to have annotation errors which are not covered by the special cases specified in this document. Please approach the first author for any corrections and they will appear in the next release. See Section 4 for known errors.", "creator": "LaTeX with hyperref package"}}}