{"id": "1605.03661", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-May-2016", "title": "Learning Representations for Counterfactual Inference", "abstract": "observational studies issues are rising in importance due immensely to the increasing widespread wealth accumulation pattern of mental data overlooked in traditional fields affecting such. as optimal healthcare, autism education, employment protection and environmental ecology. we shall consider explaining the unique task of answering counterfactual questions correctly such as, \" would this patient have lower blood iron sugar because had actually she received a different psychotic medication? \". we specifically propose opening a crucial new algorithmic framework for deep counterfactual inference which brings together ideas from geographical domain adaptation and numerical representation learning. in heavy addition to attaining a theoretical justification, we perform an empirical comparison with previous approaches wanting to apply causal learning inference from conventional observational data. our deep learning algorithm significantly clearly outperforms the experimental previous state - of - the - art.", "histories": [["v1", "Thu, 12 May 2016 02:59:40 GMT  (193kb,D)", "http://arxiv.org/abs/1605.03661v1", "To appear in ICML 2016"], ["v2", "Wed, 8 Jun 2016 17:04:07 GMT  (194kb,D)", "http://arxiv.org/abs/1605.03661v2", "Appearing in ICML 2016"]], "COMMENTS": "To appear in ICML 2016", "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.LG", "authors": ["fredrik d johansson", "uri shalit", "david sontag"], "accepted": true, "id": "1605.03661"}, "pdf": {"name": "1605.03661.pdf", "metadata": {"source": "META", "title": "Learning Representations for Counterfactual Inference", "authors": ["Fredrik D. Johansson", "Uri Shalit", "David Sontag"], "emails": ["FREJOHK@CHALMERS.SE", "SHALIT@CS.NYU.EDU", "DSONTAG@CS.NYU.EDU"], "sections": [{"heading": "1. Introduction", "text": "Inferring causal relations is a fundamental problem in the sciences and commercial applications. The problem of causal inference is often framed in terms of counterfactual questions (Lewis, 1973; Rubin, 1974; Pearl, 2009) such as \u201cWould this patient have lower blood sugar had she received a different medication?\u201d, or \u201cWould the user have clicked on this ad had it been in a different color?\u201d. In this paper we propose a method to learn representations suited for counterfactual inference, and show its efficacy in both simulated and real world tasks.\nWe focus on counterfactual questions raised by what are known as observational studies. Observational studies are studies where interventions and outcomes have been recorded, along with appropriate context. For example,\n* Equal contribution\nconsider an electronic health record dataset collected over several years, where for each patient we have lab tests and past diagnoses, as well as data relating to their diabetic status, and the causal question of interest is which of two existing anti-diabetic medications A or B is better for a given patient. Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We believe machine learning will be called on more and more to help make better decisions in these fields, and that researchers should be careful to pay attention to the ways in which these studies differ from classic supervised learning, as explained in Section 2 below.\nIn this work we draw a connection between counterfactual inference and domain adaptation. We use this connection to introduce a form of regularization by enforcing similarity between the distributions of representations learned for populations with different interventions. For example, the representations for patients who received medication A versus those who received medication B. In Section 3 we give several methods for learning such representations. In Section 4 we show that our methods approximately minimize an upper bound on a regret term in the counterfactual regime. The general method is outlined in Figure 1.\nOur work has commonalities with recent work on learning fair representations (Zemel et al., 2013; Louizos et al., 2015) and on learning representations suited for transfer learning (Ben-David et al., 2007; Gani et al., 2015). In all these cases the learned representation exhibits a degree of invariance to specific aspects of the data: either an identity of a certain group such as racial minorities for fair representations, or the identity of the data source for domain adaptation, or, in the case of counterfactual learning, the type of intervention enacted in each population.\nIn machine learning, counterfactual questions typically arise in problems where there is a learning agent which\nar X\niv :1\n60 5.\n03 66\n1v 1\n[ st\nat .M\nL ]\n1 2\nM ay\nperforms actions, and receives feedback or reward for that choice without knowing what would be the feedback for other possible choices. This is sometimes referred to as bandit feedback (Beygelzimer et al., 2010). This setup comes up in diverse areas, for example off-policy evaluation in reinforcement learning (Sutton & Barto, 1998), learning from \u201clogged implicit exploration data\u201d (Strehl et al., 2010) or \u201clogged bandit feedback\u201d (Swaminathan & Joachims, 2015), and in understanding and designing complex real world ad-placement systems (Bottou et al., 2013). Note that while in contextual bandit or robotics applications the researcher typically knows the method underlying the action choice (e.g. the policy in reinforcement learning), in observational studies we usually do not have control or even a full understanding of the mechanism which chooses which actions are performed and which feedback or reward is revealed. For instance, for anti-diabetic medication, more affluent patients might be insensitive to the price of a drug, while less affluent patients could bring this into account in their choice.\nGiven that we do not know beforehand the particulars determining the choice of action, the question remains, how can we learn from data which course of action would have better outcomes. By bringing together ideas from representation learning and domain adaptation, our method offers a novel way to leverage increasing computation power and the rise of large datasets to tackle consequential questions of causal inference.\nThe contributions of our paper are as follows. First, we show how to formulate the problem of counterfactual inference as a domain adaptation problem, and more specifically a covariate shift problem. Second, we derive new families of representation algorithms for counterfactual inference: one family is based on linear models and variable selection, and the other is based on deep learning of representations (Bengio et al., 2013). Finally, we show that learning representations that encourage similarity (also called balance) between the treatment and control populations leads to better counterfactual inference; this is in contrast to many methods which attempt to create balance by re-weighting samples (e.g., Bang & Robins, 2005; Dud\u0131\u0301k et al., 2011; Austin, 2011; Swaminathan & Joachims, 2015). We show the merit of learning balanced representations both theoretically in Theorem 1, and empirically in a set of experiments across three datasets."}, {"heading": "2. Problem setup", "text": "Let T be the set of potential interventions or actions we wish to consider, X the set of contexts, and Y the set of possible outcomes. For example, for a patient x \u2208 X the set T of interventions of interest might be two different treatments, and the set of outcomes might be Y = [0, 200]\nindicating blood sugar levels in mg/dL. For an ad slot on a webpage x, the set of interventions T might be all possible ads in the inventory that fit that slot, while the potential outcomes could be Y = {click, no click}. For a context x (e.g. patient, webpage), and for each potential intervention t \u2208 T , let Yt(x) \u2208 Y be the potential outcome for x. The fundamental problem of causal inference is that only one potential outcome is observed for a given context x: even if we give the patient one medication and later the other, the patient is not in exactly the same condition. In the machine learning literature this type of partial feedback is often called \u201cbandit feedback\u201d. The model described above is known in statistics as the Rubin-Neyman causal model (Rubin, 1974; 2011).\nWe are interested in the case of a binary action set T = {0, 1}, where action 1 is often known as the \u201ctreatment\u201d and action 0 is the \u201ccontrol\u201d. In this case the quantity Y1(x) \u2212 Y0(x) is of high interest: it is known as the individualized treatment effect (ITE) for context x (van der Laan & Petersen, 2007; Weiss et al., 2015). Knowing this quantity enables choosing the best of the two actions when confronted with the choice, for example choosing the best treatment for a specific patient. However, the fact that we only have access to the outcome of one of the two actions prevents the ITE from being known. Another commonly sought after quantity is the average treatment effect, ATE = Ex\u223cp(x)[ITE(x)] for a population with distribution p(x). In the binary action setting, we refer to the observed and unobserved outcomes as the factual outcome yF (x), and counterfactual outcome yCF (x) respectively.\nA common approach for estimating the ITE is by direct modelling: given n samples {(xi, ti, yFi )}ni=1, where yFi = ti \u00b7Y1(xi)+(1\u2212ti)Y0(xi), learn a function h : X\u00d7T \u2192 Y such that h(xi, ti) \u2248 yFi . The estimated transductive ITE is then:\n\u02c6ITE(xi) = { yFi \u2212 h(xi, 1\u2212 ti), ti = 1. h(xi, 1\u2212 ti)\u2212 yFi , ti = 0.\n(1)\nWhile in principle any function fitting model might be used for estimating the ITE (Prentice, 1976; Gelman & Hill, 2006; Chipman et al., 2010; Wager & Athey, 2015; Weiss et al., 2015), it is important to note how this task differs from standard supervised learning. The problem is as follows: the observed sample consists of the set P\u0302F = {(xi, ti)}ni=1. However, calculating the ITE requires inferring the outcome on the set P\u0302CF = {(xi, 1\u2212 ti)}ni=1. We call the set P\u0302F \u223c PF the empirical factual distribution, and the set P\u0302CF \u223c PCF the empirical counterfactual distribution, respectively. Because PF and PCF need not be equal, the problem of causal inference by counterfactual prediction might require inference over a different distribution than the one from which samples are given. In\nmachine learning terms, this means that the feature distribution of the test set differs from that of the train set. This is a case of covariate shift, which is a special case of domain adaptation (Daume III & Marcu, 2006; Jiang, 2008; Mansour et al., 2009). A somewhat similar connection was noted in Scho\u0308lkopf et al. (2012) with respect to covariate shift, in the context of a very simple causal model.\nSpecifically, we have that PF (x, t) = P (x) \u00b7 P (t|x) and PCF (x, t) = P (x) \u00b7 P (\u00act|x). The difference between the observed (factual) sample and the sample we must perform inference on lies precisely in the treatment assignment mechanism, P (t|x). For example, in a randomized control trial, we typically have that t and x are independent. In the contextual bandit setting, there is typically an algorithm which determines the choice of the action t given the context x. In observational studies, which are the focus of this work, the treatment assignment mechanism is not under our control and in general will not be independent of the context x. Therefore, in general, the counterfactual distribution will be different from the factual distribution."}, {"heading": "3. Balancing counterfactual regression", "text": "We propose to perform counterfactual inference by amending the direct modeling approach, taking into account the fact that the learned estimator h must generalize from the factual distribution to the counterfactual distribution.\nOur method, illustrated in Figure 1, learns a representation \u03a6 : X \u2192 Rd, (either using a deep neural network, or by feature selection and feature re-weighting), and a function h : Rd\u00d7T \u2192 R, such that the learned representation trades off three objectives: (1) enabling low-error prediction of the observed outcomes over the factual representation, (2) enabling low-error prediction of unobserved counterfactuals by taking into account relevant factual outcomes, and (3) the distribution of treatment and control populations is similar or balanced.\nWe accomplish low-error prediction by the usual means\nof error minimization over an observed training set and regularization in order to enable good generalization error. We accomplish the second objective by an error term that encourages counterfactual predictions to be close to the nearest observed outcome from the respective treatment or control set. Finally, we accomplish the third objective by minimizing the so-called discrepancy distance, introduced by Mansour et al. (2009), which is a hypothesis class dependent distance measure specifically tailored for domain adaptation applications. For hypothesis space H, we denote the discrepancy distance by discH. See Section 4 for the formal definition and motivation. Other discrepancy measures such as Maximum Mean Discrepancy (Gretton et al., 2012) could also be used for this purpose. We experimented with this measure, and will report the results in future work.\nIntuitively, representations that reduce the discrepancy between the treatment and control populations prevent the learner from using \u201cunreliable\u201d aspects of the data when trying to generalize from the factual to counterfactual domains. For example, if in our sample almost no men ever received medication A, inferring how men would react to medication A is highly prone to error and a more conservative use of the gender feature might be warranted.\nLet X = {xi}ni=1, T = {ti}ni=1, and Y F = {yFi }ni=1 denote the observed units, treatment assignments and factual outcomes respectively. We assumeX is a metric space with a metric d. Let j(i) \u2208 arg minj\u2208{1...n} s.t. tj=1\u2212ti d(xj , xi) be the nearest neighbor of xi among the group that received the opposite treatment from unit i. Note that the nearest neighbor is computed once, in the input space, and does not change with the representation \u03a6. The objective we minimize over representations \u03a6 and hypotheses h \u2208 H is\nBH,\u03b1,\u03b3(\u03a6, h) = 1\nn n\u2211 i=1 |h(\u03a6(xi), ti)\u2212 yFi |+ (2)\n\u03b1 discH(P\u0302 F \u03a6 , P\u0302 CF \u03a6 ) +\n\u03b3\nn n\u2211 i=1 |h(\u03a6(xi), 1\u2212 ti)\u2212 yFj(i)| ,\nwhere \u03b1, \u03b3 > 0 are hyperparameters to control the strength of the imbalance penalties, and disc is the discrepancy measure defined in 4.1. When the hypothesis class H is the class of linear functions, the term discH(P\u0302F\u03a6 , P\u0302 CF \u03a6 ) has a closed form brought in 4.1 below, and h(\u03a6, ti) = h>[\u03a6(xi) ti]. For more complex hypothesis spaces there is in general no exact closed form for discH(P\u0302F\u03a6 , P\u0302 CF \u03a6 ).\nOnce the representation \u03a6 is learned, we fit a final hypothesis minimizing a regularized squared loss objective on the factual data. Our algorithm is summarized in Algorithm 1. Note that our algorithm involves two minimization procedures.\nIn Section 4 we motivate our method, by showing that\nAlgorithm 1 Balancing counterfactual regression\n1: Input: X,T, Y F ;H,N ;\u03b1, \u03b3, \u03bb 2: \u03a6\u2217, g\u2217 = arg min\n\u03a6\u2208N ,g\u2208H BH,\u03b1,\u03b3(\u03a6, g) (2)\n3: h\u2217 = arg minh\u2208H 1 n \u2211n i=1(h(\u03a6, ti)\u2212 yFi )2 +\u03bb\u2016h\u2016H 4: Output: h\u2217,\u03a6\u2217\nour method of learning representations minimizes an upper bound on the regret error over the counterfactual distribution, using results of Cortes & Mohri (2014)."}, {"heading": "3.1. Balancing variable selection", "text": "A na\u0131\u0308ve way of balancing the input distributions of treatment and control groups is to only consider features that are already well balanced, i.e. features which have a similar distribution over both treatment and control sets, while discarding unbalanced features. In some cases however, imbalanced features can be highly predictive, and should not be discarded. A middle-ground is to restrict the influence of imbalanced features on the predicted outcome. We build on this idea by learning a sparse re-weighting of the features that minimizes the bound in Theorem 1. The reweighting determines the influence of a feature by trading off its predictive capabilities and its balance.\nWe implement the re-weighting as a diagonal matrix W , forming the representation \u03a6(x) = Wx, with diag(W ) subject to a simplex constraint to achieve sparsity. LetN = {x 7\u2192 Wx : W = diag(w), wi \u2208 [0, 1], \u2211 i wi = 1} denote the space of such representations. WithN defined and Hl the space of linear hypotheses, we can apply Algorithm 1.\nHere, because the hypothesis spaceHl is linear, disc(\u03a6) is a function of the distance between the weighted population means, see Section 4.1. With p = E[t], c = p\u2212 1/2, nt =\u2211n i=1 ti, \u00b51 = 1 nt \u2211n i:ti=1 xi, and \u00b50 analogously defined,\ndiscHl(XW ) = c+ \u221a c2 + \u2016W (p\u00b51 \u2212 (1\u2212 p)\u00b50)]\u201622\nIt is plain to see that, in order to minimize the discrepancy, features k that differ a lot between treatment groups will receive a smaller weight wk. Minimizing the overall objective B, however, involves a trade-off between maximizing balance and predictive accuracy. We train the variable reweighting model using alternating stochastic sub-gradient descent in W and h."}, {"heading": "3.2. Deep neural networks", "text": "Deep neural networks have been shown to successfully learn good representations of high-dimensional data in many different tasks (Bengio et al., 2013). Here we show\nthat they can be used for counterfactual inference and, crucially, for accommodating imbalance penalties.\nWe propose a modification of the standard feed-forward architecture with fully connected layers, see Figure 2. The first dr hidden layers are used to learn a representation \u03a6(x) of the input x. The output of the dr:th layer is used to calculate the discrepancy discH(P\u0302F\u03a6 , P\u0302 CF \u03a6 ). The do layers following the first dr layers take as additional input the treatment assignment ti and generate a prediction h([\u03a6(xi), ti]) of the outcome."}, {"heading": "3.3. Non-linear hypotheses and individual effect", "text": "It is important to note that both in the case of variable reweighting, and for neural nets with a single linear outcome layer, do = 0, the hypothesis space H comprises linear functions of [\u03a6, t]. Hence the discrepancy, discH(\u03a6) can be expressed in closed-form. A less desirable consequence of using linear hypotheses is that the models cannot capture difference in the individual treatment effect, as the predicted outcome can be written as \u03b2>\u03a6(x) + \u03b2tt + \u03b20 for some vector \u03b2 \u2208 Rd and scalars \u03b2t, \u03b20. To obtain truly individual estimates of treatment effect we need to consider interactions between \u03a6(x) and t. This can for example be done by (polynomial) feature expansion, or in the case of neural networks, by adding non-linear layers after the concatenation [\u03a6(x), t]. For both approaches, however, we pay the price of no longer having a closed form expression for discH(P\u0302 F \u03a6 , P\u0302 CF \u03a6 ). In practice, we find in our experiments that using a non-linear outcome model, together with the linear discrepancy, gives good results in practice."}, {"heading": "4. Theory", "text": "In this section we derive an upper bound on the relative counterfactual generalization error of a representation function \u03a6. The bound only uses quantities we can measure directly from the available data. In the previous section we gave several methods for learning representations which approximately minimize the upper bound.\nRecall that for an observed context or instance xi \u2208 X with observed treatment ti \u2208 {0, 1}, the two potential outcomes are Y0(xi), Y1(xi) \u2208 Y , of which we observe the factual outcome yFi = tiY1(xi) + (1 \u2212 ti)Y0(xi). Let\n(x1, t1, y F 1 ), . . . , (xn, tn, y F n ) be a sample from the factual distribution. Similarly, let (x1, 1 \u2212 t1, yCF1 ), . . . , (xn, 1 \u2212 tn, y CF n ) be the counterfactual sample. Note that while we know the factual outcomes yFi , we do not know the counterfactual outcomes yCFi . Let \u03a6 : X \u2192 Rd be a representation function, and let R(\u03a6) denote its range. Denote by P\u0302F\u03a6 the empirical distribution over the representations and treatment assignments (\u03a6(x1), t1), . . . , (\u03a6(xn), tn), and similarly P\u0302CF\u03a6 the empirical distribution over the representations and counterfactual treatment assignments (\u03a6(x1), 1 \u2212 t1), . . . , (\u03a6(xn), 1 \u2212 tn). Let Hl be the hypothesis set of linear functions \u03b2 : R(\u03a6)\u00d7 {0, 1} \u2192 Y . Definition 1 (Mansour et al. 2009). Given a hypothesis set H and a loss functionL, the empirical discrepancy between the empirical distributions P\u0302F\u03a6 and P\u0302 CF \u03a6 is:\ndiscH(P\u0302F\u03a6 , P\u0302 CF \u03a6 ) =\nmax \u03b2,\u03b2\u2032\u2208H \u2223\u2223\u2223Ex\u223cP\u0302F\u03a6 [L(\u03b2(x), \u03b2\u2032(x))]\u2212 Ex\u223cP\u0302CF\u03a6 [L(\u03b2(x), \u03b2\u2032(x))]\u2223\u2223\u2223 , where L is a loss function L : Y \u00d7 Y \u2192 R with weak Lipschitz constant \u00b5 relative toH 1. Note that the discrepancy is defined with respect to a hypothesis class and a loss function, and is therefore very useful for obtaining generalization bounds involving different distributions. Throughout this section we always have L denote the squared loss. We prove the following, based on Cortes & Mohri (2014): Theorem 1. For a sample {(xi, ti, yFi )}ni=1, xi \u2208 X , ti \u2208 {0, 1} and yi \u2208 Y , and a given representation function \u03a6 : X \u2192 Rd, let P\u0302F\u03a6 = (\u03a6(x1), t1), . . . , (\u03a6(xn), tn), P\u0302CF\u03a6 = (\u03a6(x1), 1\u2212t1), . . . , (\u03a6(xn), 1\u2212tn). We assume thatX is a metric space with metric d, and that the potential outcome functions Y0(x) and Y1(x) are Lipschitz continuous with constants K0 and K1 respectively, such that d(xa, xb) \u2264 c =\u21d2 |Yt(xa)\u2212 Yt(xb)| \u2264 Kt \u00b7 c for t = 0, 1.\nLet Hl \u2282 Rd+1 be the space of linear functions \u03b2 : X \u00d7 {0, 1} \u2192 Y , and for \u03b2 \u2208 Hl, let LP (\u03b2) = E(x,t,y)\u223cP [L(\u03b2(x, t), y)] be the expected loss of \u03b2 over distribution P . Let r = max ( E(x,t)\u223cPF [\u2016[\u03a6(x), t]\u20162] ,E(x,t)\u223cPCF [\u2016[\u03a6(x), t]\u20162]\n) be the maximum expected radius of the distributions. For \u03bb > 0, let \u03b2\u0302F (\u03a6) = arg min\u03b2\u2208Hl LP\u0302F\u03a6 (\u03b2) + \u03bb\u2016\u03b2\u2016 2 2, and \u03b2\u0302CF (\u03a6) similarly for P\u0302CF\u03a6 , i.e. \u03b2\u0302 F (\u03a6) and \u03b2\u0302CF (\u03a6) are the ridge regression solutions for the factual and counterfactual empirical distributions, respectively.\nLet y\u0302Fi (\u03a6, h) = h >[\u03a6(xi), ti] and y\u0302CFi (\u03a6, h) = h>[\u03a6(xi), 1 \u2212 ti] be the outputs of the hypothesis h \u2208 Hl over the representation \u03a6(xi) for the factual and counterfactual settings of ti, respectively. Finally, for each i \u2208\n1For the case where L is the squared loss we can show that if we assume \u2016\u03a6(x)\u20162 \u2264 m and |y| \u2264 M , and that the hypothesis setH is that of linear functions with norm bounded by m/\u03bb, then \u00b5 \u2264 2M(1 +m2/\u03bb).\n{1 . . . n}, let j(i) \u2208 arg minj\u2208{1...n} s.t. tj=1\u2212ti d(xj , xi) be the nearest neighbor in X of xi among the group that received the opposite treatment from unit i. Let di,j \u2261 d(xi, xj).\nThen for both Q = PF and Q = PCF we have:\n\u03bb\n\u00b5r (LQ(\u03b2\u0302F (\u03a6))\u2212 LQ(\u03b2\u0302CF (\u03a6)))2 \u2264\ndiscHl(P\u0302 F \u03a6 , P\u0302 CF \u03a6 )+ (3)\nmin h\u2208Hl\n1\nn n\u2211 i=1 ( |y\u0302Fi (\u03a6, h)\u2212 yFi |+ |y\u0302CFi (\u03a6, h)\u2212 yCFi | ) \u2264\n(4)\ndiscHl(P\u0302 F \u03a6 , P\u0302 CF \u03a6 )+\nmin h\u2208Hl\n1\nn n\u2211 i=1 ( |y\u0302Fi (\u03a6, h)\u2212 yFi |+ |y\u0302CFi (\u03a6, h)\u2212 yFj(i)| ) +\n(5) K0 n \u2211 i:ti=1 di,j(i) + K1 n \u2211 i:ti=0 di,j(i). (6)\nThe proof is in the supplemental material.\nTheorem 1 gives, for all fixed representations \u03a6, a bound on the relative error for a ridge regression model fit on the factual outcomes and evaluated on the counterfactual, as compared with ridge regression had it been fit on the unobserved counterfactual outcomes. It does not take into account how \u03a6 is obtained, and applies even if h(\u03a6(x), t) is not convex in x, e.g. if \u03a6 is a neural net. Since the bound in the theorem is true for all representations \u03a6, we can attempt to minimize it over \u03a6, as done in Algorithm 1.\nThe term on line (4) of the bound includes the unknown counterfactual outcomes yCFi . It measures how well could we in principle fit the factual and counterfactual outcomes together using a linear hypothesis over the representation \u03a6. For example, if the dimension d of the representation is greater than the number of samples n, and in addition if there exist constants b and such that |yFi \u2212 yCFi \u2212 b| \u2264 , then this term is upper bounded by . In general however, we cannot directly control its magnitude.\nThe term on line (3) measures the discrepancy between the factual and counterfactual distributions over the representation \u03a6. In 4.1 below, we show that this term is closely related to the norm of the difference between the mean of the representation of the control group and the mean of the representation of the treatment group. A representation for which the means of the treatment and control are close (small value of (3)), but which at the same time allows for a good prediction of the factuals and counterfactuals (small value of (4)), is guaranteed to yield structural risk minimizers with similar generalization errors between factual and counterfactual.\nWe further show that the term on line (4), which cannot be evaluated since we do not know yCFi , can be upper bounded by a sum of the terms on lines (5) and (12). The term (5) includes two empirical data fitting terms: |y\u0302Fi (\u03a6, v)\u2212yFi | and |y\u0302CFi (\u03a6, v)\u2212yFj(i)|. The first is simply fitting the observed factual outcomes using a linear function over the representation \u03a6. The second term is a form of nearest-neighbor regression, where the counterfactual outcomes for a treated (resp. control) instance are fit to the most similar factual outcome among the control (resp. treatment) set, where similarity is measured in the original space X . Finally, the term on line (12), is the only quantity which is independent of the representation \u03a6. It measures the average distance between each treated instance to the nearest control, and vice-versa, scaled by the Lipschitz constants of the true treated and control outcome functions. This term will be small when: (a) the true outcome functions Y0(x) and Y1(x) are relatively smooth, and (b) there is overlap between the treated and control groups, leading to small average nearest neighbor distance across the groups. It is well-known that when there is not much overlap between treated and control, causal inference in general is more difficult since the extrapolation from treated to control and vice-versa is more extreme (Rosenbaum, 2009).\nThe upper bound in Theorem 1 suggests the following approach for counterfactual regression. First minimize the terms (3) and (5) as functions of the representation \u03a6. Once \u03a6 is obtained, perform a ridge regression on the factual outcomes using the representations \u03a6(x) and the treatment assignments as input. The terms in the bound ensure that \u03a6 would have a good fit for the data (term (5)), while removing aspects of the treatment and control which create a large discrepancy term (3)). For example, if there is a feature which is much more strongly associated with the treatment assignment than with the outcome, it might be advisable to not use it (Pearl, 2011)."}, {"heading": "4.1. Linear discrepancy", "text": "A straightforward calculation shows that for a class Hl of linear hypotheses,\ndisc Hl\n(P,Q) = \u2016\u00b52(P )\u2212 \u00b52(Q)\u20162 .\nHere, \u2016A\u20162 is the spectral norm of A and \u00b52(P ) = Ex\u223cP [xx>] is the second-order moment of x \u223c P . In the special case of counterfactual inference, P and Q differ only in the treatment assignment. Specifically,\ndisc(P\u0302F\u03a6 , P\u0302 CF \u03a6 ) =\n\u2225\u2225\u2225\u2225[0d,d vv> 2p\u2212 1 ]\u2225\u2225\u2225\u2225\n2\n(7)\n= p\u2212 1 2 +\n\u221a (2p\u2212 1)2\n4 + \u2016v\u201622 (8)\nwhere v = E(x,t)\u223cP\u0302F\u03a6 [\u03a6(x) \u00b7 t] \u2212 E(x,t)\u223cP\u0302F\u03a6 [\u03a6(x) \u00b7 (1 \u2212 t)] and p = E[t]. Let \u00b51(\u03a6) = E(x,t)\u223cP\u0302F\u03a6 [\u03a6(x)|t = 1] and \u00b50(\u03a6) = E(x,t)\u223cP\u0302F\u03a6 [\u03a6(x)|t = 0] be the treatment and control means in \u03a6 space. Then v = p \u00b7 \u00b51(\u03a6) \u2212 (1 \u2212 p) \u00b7 \u00b50(\u03a6), exactly the difference in means between the treatment and control groups, weighted by their respective sizes. As a consequence, minimizing the discrepancy with respect to linear hypotheses constitutes matching the means in the feature space."}, {"heading": "5. Related work", "text": "Counterfactual inference for determining causal effects in observational studies has been studied extensively in statistics, economics, epidemiology and sociology (Morgan & Winship, 2014; Robins et al., 2000; Rubin, 2011; Chernozhukov et al., 2013) as well as in machine learning (Langford et al., 2011; Bottou et al., 2013; Swaminathan & Joachims, 2015).\nNon-parametric methods do not attempt to model the relation between the context, intervention, and outcome. The methods include nearest-neighbor matching, propensity score matching, and propensity score re-weighting (Rosenbaum & Rubin, 1983; Rosenbaum, 2002; Austin, 2011).\nParametric methods, on the other hand, attempt to concretely model the relation between the context, intervention, and outcome. These methods include any type of regression including linear and logistic regression (Prentice, 1976; Gelman & Hill, 2006), random forests (Wager & Athey, 2015) and regression trees (Chipman et al., 2010).\nDoubly robust methods combine aspects of parametric and non-parametric methods, typically by using a propensity score weighted regression (Bang & Robins, 2005; Dud\u0131\u0301k et al., 2011). They are especially of use when the treatment assignment probability is known, as is the case for off-policy evaluation or learning from logged bandit data. Once the treatment assignment probability has to be estimated, as is the case in most observational studies, their efficacy might wane considerably (Kang & Schafer, 2007).\nThere are few approaches that transform or select covariates to achieve balance. Tian et al. (2014) presented one such method, modeling the interactions between treatment and covariates."}, {"heading": "6. Experiments", "text": "We evaluate the two variants of our algorithm proposed in Section 3 with focus on two questions: 1) What is the effect of imposing imbalance regularization on representations? 2) How do our methods fare against established methods for counterfactual inference? We refer to the variable se-\nlection method of Section 3.1 as Balancing Linear Regression (BLR) and the neural network approach as BNN for Balancing Neural Network.\nWe report the RMSE of the estimated individual treatment effect, denoted ITE , and the absolute error in estimated average treatment effect, denoted ATE , see Section 2. Further, following Hill (2011), we report the Precision in Estimation of Heterogeneous Effect (PEHE), PEHE =\u221a\n1 n \u2211n i=1 (y\u03021(xi)\u2212 y\u03020(xi)\u2212 (Y1(xi)\u2212 Y0(xi)))\n2. Unlike for ITE, obtaining a good (small) PEHE requires accurate estimation of both the factual and counterfactual responses, not just the counterfactual.\nStandard methods for hyperparameter selection, including cross-validation, are unavailable when training counterfactual models on real-world data, as there are no samples from the counterfactual outcome. In our experiments however, all outcomes are simulated, and we have access to counterfactual samples. To avoid fitting parameters to the test set, we generate multiple repeated experiments, each with a different outcome function. We then pick hyperparameters once, for all models (and baselines), based on a held-out set of experiments. While this method is generally not available in practice, using the same parameter settings for all repeated experiments gives an indication of the robustness of the approach.\nThe neural network architectures used for all experiments consist of fully-connected ReLU layers. Each model is trained using RMSProp, with a small l2 weight decay, \u03bb = 10\u22123. We evaluate two architectures. BNN-4-0 consists of 4 ReLU representation-only layers and a single linear output layer, dr = 4, do = 0. BNN-2-2 consists of 2 ReLU representation-only layers, 2 ReLU output layers after the treatment has been added, and a single linear output layer, dr = 2, do = 2, see Figure 2. These architectures were not fitted to data, but picked apriori. For the IHDP data we use layers of 25 hidden units each. For the News data representation layers have 400 units and output layers 200 units. The nearest neighbor term, see Section 3, did not improve empirical performance, and was omitted for the BNN models. For the neural network models, the hypothesis and the representation were fit jointly.\nLinear models are widely used for counterfactual inference, because of their interpretability and statistical properties. Therefore, we use several different linear models for comparison, including ordinary linear regression (OLS) and doubly robust linear regression (DR) (Bang & Robins, 2005). We also include LASSO (Tibshirani, 1996) as a strong competitor for our variable selection approach. Here, variables are first selected using LASSO and then used to fit a ridge regression (LASSO + RIDGE). We pick regularization parameters based on a held out sample, as\nfor the proposed models. For DR, we estimate propensity scores using logistic regression and clip weights at 100. For the News dataset (see below), we perform the logistic regression on the first 100 principal components of the data.\nBayesian Additive Regression Trees (BART) (Chipman et al., 2010) is a non-linear regression model which has been used successfully for counterfactual inference in the past (Hill, 2011). We compare our results to BART using the implementation provided in the BayesTree R-package2. Like (Hill, 2011), we do not attempt to tune the parameters, but use the default. Finally, we include a standard feed-forward neural network, trained with 4 hidden layers, to predict the factual outcome based on X and t, without a penalty for imbalance. We refer to this as NN-4."}, {"heading": "6.1. Simulation based on real data \u2013 IHDP", "text": "Hill (2011) introduced a semi-simulated dataset based on the Infant Health and Development Program (IHDP). The IHDP data uses covariates from a real randomized experiment, studying the effect of high-quality child care and home visits on future cognitive test scores. The experiment proposed by Hill (2011) uses a simulated log-linear outcome to provide ground truth, and artificially introduces imbalance between treatment and control subjects by removing a subset of the treatment population. In total, the dataset consists of 747 subjects (139 treated, 608 control), each represented by 25 covariates measuring properties of the child and their mother. For details, see Hill (2011). We run 100 repeated experiments for hyperparameter selection and an additional 1000 for evaluation, all with the nonlinear response surface implemented as setting \u201cB\u201d in the NPCI package 3."}, {"heading": "6.2. Simulation based on real data \u2013 News", "text": "We introduce a new dataset, simulating the opinions of a media consumer exposed to multiple news items. Each item is consumed either on a mobile device or on desktop. The units are different news items represented by word counts xi \u2208 NV , and the outcome yF (xi) \u2208 R is the readers experience of xi. The intervention t \u2208 {0, 1} represents the viewing device, desktop (t = 0) or mobile (t = 1). We assume that the consumer prefers to read about certain topics on mobile. To model this, we train a topic model on a large set of documents and let z(x) \u2208 Rk represent the topic distribution of news item x. We define two centroids in topic space, zc1 (mobile), and z c 0 (desktop), and let the readers opinion of news item x on device t be determined\n2https://cran.r-project.org/package= BayesTree\n3https://github.com/vdorie/npci\nby the similarity between z(x) and zct ,\nyF (xi) = C ( z(x)>zc0 + ti \u00b7 z(x)>zc1 ) + ,\nwhere C is a scaling factor and \u223c N (0, 1). Here, we let the mobile centroid, zc1 be the topic distribution of a randomly sampled document, and zc0 be the average topic representation of all documents. We further assume that the assignment of a news item x to a device t \u2208 {0, 1} is biased towards the device preferred for that item. We model this using the softmax function,\np(t = 1 | x) = e \u03ba\u00b7z(x)>zc1\ne\u03ba\u00b7z(x) >zc0 + e\u03ba\u00b7z(x) >zc1\nwhere \u03ba \u2265 0 determines the strength of the bias. Note that \u03ba = 0 implies a completely random device assignment.\nWe sample n = 5000 news items and outcomes according to this model, based on 50 LDA topics, trained on documents from the NY Times corpus (downloaded from UCI 4). The data available to the algorithms are the raw word counts, from a vocabulary of k = 3477 words, selected as union of the most 100 probable words in each topic. We set the scaling parameters to C = 50, \u03ba = 10. Figure 3 shows a 2D visualization of the outcome and device assignments for a random sample of 500 documents. Note that the device assignment becomes increasingly random, and the outcome lower, further away from the centroids. The distances are computed in the original space. The experiment was repeated 50 times."}, {"heading": "6.3. Results", "text": "The results of the IHDP and News experiments are presented in Table 1 and Table 2 respectively. We see that, in general, the non-linear methods perform better in terms of individual prediction (ITE, PEHE). Further, we see that our proposed balancing neural network BNN-2-2 performs\n4https://archive.ics.uci.edu/ml/datasets/ Bag+of+Words\nthe best on both datasets in terms of estimating the ITE and PEHE, and is competitive on average treatment effect, ATE. Particularly noteworthy is the comparison with the network without balance penalty, NN-4. These results indicate that our proposed regularization can help avoid overfitting the representation to the factual outcome. Figure 4 plots the performance of BNN-2-2 for various imbalance penalties \u03b1. The valley in the region \u03b1 = 1, and the fact that we don\u2019t experience a loss in performance for smaller values of \u03b1, show that the penalizing imbalance in the representation \u03a6 has the desired effect.\nFor the linear methods, we see that the two variable selection approaches, our proposed BLR method and LASSO + RIDGE, work the best in terms of estimating ITE. We would like to emphasize that LASSO + RIDGE is a very strong baseline and it\u2019s exciting that our theory-guided method is competitive with this approach. On News, BLR and LASSO + RIDGE perform equally well yet again, although this time with qualitatively different results, as they do not select the same variables. Interestingly, BNN-4-0, BLR and LASSO + RIDGE all perform better on News than the standard neu-\nral network, NN-4. The performance of BART on News is likely hurt by the dimensionality of the dataset, and could improve with hyperparameter tuning."}, {"heading": "7. Conclusion", "text": "As machine learning is becoming a major tool for researchers and policy makers across different fields such as healthcare and economics, causal inference becomes a crucial issue for the practice of machine learning. In this paper we focus on counterfactual inference, which is a widely applicable special case of causal inference. We cast counterfactual inference as a type of domain adaptation problem, and derive a novel way of learning representations suited for this problem.\nOur models rely on a novel type of regularization criteria: learning balanced representations, representations which have similar distributions among the treated and untreated populations. We show that trading off a balancing criterion with standard data fitting and regularization terms is both practically and theoretically prudent.\nOpen questions which remain are how to generalize this method for cases where more than one treatment is in question, deriving better optimization algorithms and using richer discrepancy measures."}, {"heading": "Acknowledgements", "text": "DS and US were supported by NSF CAREER award #1350965."}, {"heading": "A. Proof of Theorem 1", "text": "We use a result implicit in the proof of Theorem 2 of Cortes & Mohri (2014), for the case where H is the set of linear hypotheses over a fixed representation \u03a6. Cortes & Mohri (2014) state their result for the case of domain adaptation: in our case, the factual distribution is the so-called \u201csource domain\u201d, and the counterfactual distribution is the \u201ctarget domain\u201d. Theorem A1. [Cortes & Mohri (2014)] Using the notation and assumptions of Theorem 1, for both Q = PF and Q = PCF : \u03bb\n\u00b5r (LQ(\u03b2\u0302F (\u03a6))\u2212 LQ(\u03b2\u0302CF (\u03a6)))2 \u2264\ndiscHl(P\u0302 F \u03a6 , P\u0302 CF \u03a6 )+\nmin h\u2208Hl\n1\nn ( n\u2211 i=1 |y\u0302Fi (\u03a6, h)\u2212 yFi |+ |y\u0302CFi (\u03a6, h)\u2212 yCFi | ) (9)\nIn their work, Cortes & Mohri (2014) assume the H is a reproducing kernel Hilbert space (RKHS) for a universal kernel, and they do not consider the role of the representation \u03a6. Since the RKHS hypothesis space they use is much stronger than the linear space Hl, it is often reasonable to assume that the second term in the bound 9 is small. We however cannot make this assumption, and therefore we wish to explicitly bound the term minh\u2208Hl 1 n (\u2211n i=1 |y\u0302Fi (\u03a6, h)\u2212 yFi |+ |y\u0302CFi (\u03a6, h)\u2212 yCFi | ) , while using the fact that we have control over the representation \u03a6. Lemma 1. Let {(xi, ti, yFi )}ni=1, xi \u2208 X , ti \u2208 {0, 1} and yFi \u2208 Y \u2286 R. We assume that X is a metric space with metric d, and that there exist two function Y0(x) and Y1(x) such that yFi = tiY1(xi) + (1\u2212 ti)Y0(xi), and in addition we define yCFi = (1 \u2212 ti)Y1(xi) + tiY0(xi). We further assume that the functions Y0(x) and Y1(x) are Lipschitz continuous with constants K0 and K1 respectively, such that d(xa, xb) \u2264 c =\u21d2 |Yt(xa) \u2212 Yt(xb)| \u2264 Ktc. Define j(i) \u2208 arg minj\u2208{1...n} s.t. tj=1\u2212ti d(xj , xi) to be the nearest neighbor of xi among the group that received the opposite treatment from unit i, for all i \u2208 {1 . . . n}. Let di,j = d(xi, xj)\nFor any b \u2208 Y and h \u2208 H:\n|b\u2212 yCFi | \u2264 |b\u2212 yFj(i)|+K1\u2212tidi,j(i)\nProof. By the triangle inequality, we have that:\n|b\u2212 yCFi | \u2264 |b\u2212 yFj(i)|+ |y F j(i) \u2212 y CF i |.\nBy the Lipschitz assumption on Y1\u2212ti , and since d(xi, xj(i)) \u2264 di,j(i), we obtain that\n|yFj(i)\u2212y CF i | = |Y1\u2212ti(xj(i))\u2212Y1\u2212ti(xi)| \u2264 di,j(i)K1\u2212ti .\nBy definition yCFi = Y1\u2212ti(xi). In addition, by definition of j(i), we have tj(i) = 1 \u2212 ti, and therefore yFj(i) = Y1\u2212ti(xj(i)), proving the equality. The inequality is an immediate consequence of the Lipschitz property.\nWe restate Theorem 1 and prove it. Theorem 1. For a sample {(xi, ti, yFi )}ni=1, xi \u2208 X , ti \u2208 {0, 1} and yi \u2208 Y , and a given representation function \u03a6 : X \u2192 Rd, let P\u0302F\u03a6 = (\u03a6(x1), t1), . . . , (\u03a6(xn), tn), P\u0302CF\u03a6 = (\u03a6(x1), 1\u2212t1), . . . , (\u03a6(xn), 1\u2212tn). We assume thatX is a metric space with metric d, and that the potential outcome functions Y0(x) and Y1(x) are Lipschitz continuous with constants K0 and K1 respectively, such that d(xa, xb) \u2264 c =\u21d2 |Yt(xa)\u2212 Yt(xb)| \u2264 Kt \u00b7 c for t = 0, 1.\nLet Hl \u2282 Rd+1 be the space of linear functions \u03b2 : X \u00d7 {0, 1} \u2192 Y , and for \u03b2 \u2208 Hl, let LP (\u03b2) = E(x,t,y)\u223cP [L(\u03b2(x, t), y)] be the expected loss of \u03b2 over distribution P . Let r = max ( E(x,t)\u223cPF [\u2016[\u03a6(x), t]\u20162] ,E(x,t)\u223cPCF [\u2016[\u03a6(x), t]\u20162]\n) be the maximum expected radius of the distributions. For \u03bb > 0, let \u03b2\u0302F (\u03a6) = arg min\u03b2\u2208Hl LP\u0302F\u03a6 (\u03b2) + \u03bb\u2016\u03b2\u2016 2 2, and \u03b2\u0302CF (\u03a6) similarly for P\u0302CF\u03a6 , i.e. \u03b2\u0302 F (\u03a6) and \u03b2\u0302CF (\u03a6) are the ridge regression solutions for the factual and counterfactual empirical distributions, respectively.\nLet y\u0302Fi (\u03a6, h) = h >[\u03a6(xi), ti] and y\u0302CFi (\u03a6, h) = h>[\u03a6(xi), 1 \u2212 ti] be the outputs of the hypothesis h \u2208 Hl over the representation \u03a6(xi) for the factual and counterfactual settings of ti, respectively. Finally, for each i \u2208 {1 . . . n}, let j(i) \u2208 arg minj\u2208{1...n} s.t. tj=1\u2212ti d(xj , xi) be the nearest neighbor in X of xi among the group that received the opposite treatment from unit i. Let di,j \u2261 d(xi, xj).\nThen for both Q = PF and Q = PCF we have:\n\u03bb\n\u00b5r (LQ(\u03b2\u0302F (\u03a6))\u2212 LQ(\u03b2\u0302CF (\u03a6)))2 \u2264 (10)\ndiscHl(P\u0302 F \u03a6 , P\u0302 CF \u03a6 )+\nmin h\u2208Hl\n1\nn n\u2211 i=1 ( |y\u0302Fi (\u03a6, h)\u2212 yFi |+ |y\u0302CFi (\u03a6, h)\u2212 yCFi | ) \u2264\n(11)\ndiscHl(P\u0302 F \u03a6 , P\u0302 CF \u03a6 )+\nmin h\u2208Hl\n1\nn n\u2211 i=1 ( |y\u0302Fi (\u03a6, h)\u2212 yFi |+ |y\u0302CFi (\u03a6, h)\u2212 yFj(i)| ) +\nK0 n \u2211 i:ti=1 di,j(i) + K1 n \u2211 i:ti=0 di,j(i). (12)\nProof. Inequality (10) is immediate by Theorem A1. In order to prove inequality (11), we apply Lemma 1, setting b = y\u0302CFi and summing over the i."}], "references": [{"title": "An introduction to propensity score methods for reducing the effects of confounding in observational studies", "author": ["Austin", "Peter C"], "venue": "Multivariate behavioral research,", "citeRegEx": "Austin and C.,? \\Q2011\\E", "shortCiteRegEx": "Austin and C.", "year": 2011}, {"title": "Doubly robust estimation in missing data and causal inference models", "author": ["Bang", "Heejung", "Robins", "James M"], "venue": null, "citeRegEx": "Bang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bang et al\\.", "year": 2005}, {"title": "Analysis of representations for domain adaptation", "author": ["nando"], "venue": "Advances in neural information processing systems,", "citeRegEx": "nando,? \\Q2007\\E", "shortCiteRegEx": "nando", "year": 2007}, {"title": "Representation learning: A review and new perspectives", "author": ["Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pierre"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Beygelzimer", "Alina", "Langford", "John", "Li", "Lihong", "Reyzin", "Lev", "Schapire", "Robert E"], "venue": "arXiv preprint arXiv:1002.4058,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2010}, {"title": "Bayesian additive regression trees", "author": ["Chipman", "Hugh A", "George", "Edward I", "McCulloch", "Robert E. Bart"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Chipman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chipman et al\\.", "year": 2010}, {"title": "Domain adaptation and sample bias correction theory and algorithm for regression", "author": ["Cortes", "Corinna", "Mohri", "Mehryar"], "venue": "Theoretical Computer Science,", "citeRegEx": "Cortes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2014}, {"title": "Domain adaptation for statistical classifiers", "author": ["Daume III", "Hal", "Marcu", "Daniel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "III et al\\.,? \\Q2006\\E", "shortCiteRegEx": "III et al\\.", "year": 2006}, {"title": "Doubly robust policy evaluation and learning", "author": ["Dud\u0131\u0301k", "Miroslav", "Langford", "John", "Li", "Lihong"], "venue": "arXiv preprint arXiv:1103.4601,", "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dud\u0131\u0301k et al\\.", "year": 2011}, {"title": "Domain-adversarial training of neural networks", "author": ["Gani", "Yaroslav", "Ustinova", "Evgeniya", "Ajakan", "Hana", "Germain", "Pascal", "Larochelle", "Hugo", "Laviolette", "Fran\u00e7ois", "Marchand", "Mario", "Lempitsky", "Victor"], "venue": "arXiv preprint arXiv:1505.07818,", "citeRegEx": "Gani et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gani et al\\.", "year": 2015}, {"title": "Data analysis using regression and multilevel/hierarchical models", "author": ["Gelman", "Andrew", "Hill", "Jennifer"], "venue": null, "citeRegEx": "Gelman et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gelman et al\\.", "year": 2006}, {"title": "A kernel twosample test", "author": ["Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Bayesian nonparametric modeling for causal inference", "author": ["Hill", "Jennifer L"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Hill and L.,? \\Q2011\\E", "shortCiteRegEx": "Hill and L.", "year": 2011}, {"title": "A literature survey on domain adaptation of statistical classifiers", "author": ["Jiang", "Jing"], "venue": "URL: http://sifaka. cs. uiuc. edu/jiang4/domainadaptation/survey,", "citeRegEx": "Jiang and Jing.,? \\Q2008\\E", "shortCiteRegEx": "Jiang and Jing.", "year": 2008}, {"title": "Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data", "author": ["Kang", "Joseph DY", "Schafer", "Joseph L"], "venue": "Statistical science,", "citeRegEx": "Kang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2007}, {"title": "Doubly robust policy evaluation and learning", "author": ["Langford", "John", "Li", "Lihong", "Dud\u0131\u0301k", "Miroslav"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Langford et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2011}, {"title": "Causation. The journal of philosophy", "author": ["Lewis", "David"], "venue": null, "citeRegEx": "Lewis and David.,? \\Q1973\\E", "shortCiteRegEx": "Lewis and David.", "year": 1973}, {"title": "The variational fair auto encoder", "author": ["Louizos", "Christos", "Swersky", "Kevin", "Li", "Yujia", "Welling", "Max", "Zemel", "Richard"], "venue": "arXiv preprint arXiv:1511.00830,", "citeRegEx": "Louizos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2015}, {"title": "Domain adaptation: Learning bounds and algorithms", "author": ["Mansour", "Yishay", "Mohri", "Mehryar", "Rostamizadeh", "Afshin"], "venue": "arXiv preprint arXiv:0902.3430,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Counterfactuals and causal inference", "author": ["Morgan", "Stephen L", "Winship", "Christopher"], "venue": null, "citeRegEx": "Morgan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Morgan et al\\.", "year": 2014}, {"title": "Invited commentary: understanding bias amplification", "author": ["Pearl", "Judea"], "venue": "American journal of epidemiology,", "citeRegEx": "Pearl and Judea.,? \\Q2011\\E", "shortCiteRegEx": "Pearl and Judea.", "year": 2011}, {"title": "Use of the logistic model in retrospective studies", "author": ["Prentice", "Ross"], "venue": "Biometrics, pp. 599\u2013606,", "citeRegEx": "Prentice and Ross.,? \\Q1976\\E", "shortCiteRegEx": "Prentice and Ross.", "year": 1976}, {"title": "Marginal structural models and causal inference in epidemiology", "author": ["Robins", "James M", "Hernan", "Miguel Angel", "Brumback", "Babette"], "venue": "Epidemiology, pp", "citeRegEx": "Robins et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Robins et al\\.", "year": 2000}, {"title": "Design of Observational Studies", "author": ["Rosenbaum", "Paul R"], "venue": "Springer Science & Business Media,", "citeRegEx": "Rosenbaum and R.,? \\Q2009\\E", "shortCiteRegEx": "Rosenbaum and R.", "year": 2009}, {"title": "The central role of the propensity score in observational studies for causal effects", "author": ["Rosenbaum", "Paul R", "Rubin", "Donald B"], "venue": null, "citeRegEx": "Rosenbaum et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Rosenbaum et al\\.", "year": 1983}, {"title": "Estimating causal effects of treatments in randomized and nonrandomized studies", "author": ["Rubin", "Donald B"], "venue": "Journal of educational Psychology,", "citeRegEx": "Rubin and B.,? \\Q1974\\E", "shortCiteRegEx": "Rubin and B.", "year": 1974}, {"title": "Causal inference using potential outcomes", "author": ["Rubin", "Donald B"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rubin and B.,? \\Q2011\\E", "shortCiteRegEx": "Rubin and B.", "year": 2011}, {"title": "On causal and anticausal learning", "author": ["B. Sch\u00f6lkopf", "D. Janzing", "J. Peters", "E. Sgouritsa", "K. Zhang", "J. Mooij"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2012}, {"title": "Learning from logged implicit exploration data", "author": ["Strehl", "Alex", "Langford", "John", "Li", "Lihong", "Kakade", "Sham M"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Strehl et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2010}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Batch learning from logged bandit feedback through counterfactual risk minimization", "author": ["Swaminathan", "Adith", "Joachims", "Thorsten"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Swaminathan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Swaminathan et al\\.", "year": 2015}, {"title": "A simple method for estimating interactions between a treatment and a large number of covariates", "author": ["Tian", "Lu", "Alizadeh", "Ash A", "Gentles", "Andrew J", "Tibshirani", "Robert"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Regression shrinkage and selection via the lasso", "author": ["Tibshirani", "Robert"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "citeRegEx": "Tibshirani and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani and Robert.", "year": 1996}, {"title": "Causal effect models for realistic individualized treatment and intention to treat rules", "author": ["van der Laan", "Mark J", "Petersen", "Maya L"], "venue": "The International Journal of Biostatistics,", "citeRegEx": "Laan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Laan et al\\.", "year": 2007}, {"title": "Estimation and inference of heterogeneous treatment effects using random forests", "author": ["Wager", "Stefan", "Athey", "Susan"], "venue": "arXiv preprint arXiv:1510.04342,", "citeRegEx": "Wager et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wager et al\\.", "year": 2015}, {"title": "Machine learning for treatment assignment: Improving individualized risk attribution", "author": ["Weiss", "Jeremy C", "Kuusisto", "Finn", "Boyd", "Kendrick", "Lui", "Jie", "Page", "David C"], "venue": "American Medical Informatics Association Annual Symposium,", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Learning fair representations", "author": ["Zemel", "Rich", "Wu", "Yu", "Swersky", "Kevin", "Pitassi", "Toni", "Dwork", "Cynthia"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Zemel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zemel et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 36, "context": "Our work has commonalities with recent work on learning fair representations (Zemel et al., 2013; Louizos et al., 2015) and on learning representations suited for transfer learning (Ben-David et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 17, "context": "Our work has commonalities with recent work on learning fair representations (Zemel et al., 2013; Louizos et al., 2015) and on learning representations suited for transfer learning (Ben-David et al.", "startOffset": 77, "endOffset": 119}, {"referenceID": 9, "context": ", 2015) and on learning representations suited for transfer learning (Ben-David et al., 2007; Gani et al., 2015).", "startOffset": 69, "endOffset": 112}, {"referenceID": 4, "context": "This is sometimes referred to as bandit feedback (Beygelzimer et al., 2010).", "startOffset": 49, "endOffset": 75}, {"referenceID": 28, "context": "This setup comes up in diverse areas, for example off-policy evaluation in reinforcement learning (Sutton & Barto, 1998), learning from \u201clogged implicit exploration data\u201d (Strehl et al., 2010) or \u201clogged bandit feedback\u201d (Swaminathan & Joachims, 2015), and in understanding and designing complex real world ad-placement systems (Bottou et al.", "startOffset": 171, "endOffset": 192}, {"referenceID": 3, "context": "Second, we derive new families of representation algorithms for counterfactual inference: one family is based on linear models and variable selection, and the other is based on deep learning of representations (Bengio et al., 2013).", "startOffset": 210, "endOffset": 231}, {"referenceID": 8, "context": "Finally, we show that learning representations that encourage similarity (also called balance) between the treatment and control populations leads to better counterfactual inference; this is in contrast to many methods which attempt to create balance by re-weighting samples (e.g., Bang & Robins, 2005; Dud\u0131\u0301k et al., 2011; Austin, 2011; Swaminathan & Joachims, 2015).", "startOffset": 275, "endOffset": 367}, {"referenceID": 35, "context": "In this case the quantity Y1(x) \u2212 Y0(x) is of high interest: it is known as the individualized treatment effect (ITE) for context x (van der Laan & Petersen, 2007; Weiss et al., 2015).", "startOffset": 132, "endOffset": 183}, {"referenceID": 5, "context": "While in principle any function fitting model might be used for estimating the ITE (Prentice, 1976; Gelman & Hill, 2006; Chipman et al., 2010; Wager & Athey, 2015; Weiss et al., 2015), it is important to note how this task differs from standard supervised learning.", "startOffset": 83, "endOffset": 183}, {"referenceID": 35, "context": "While in principle any function fitting model might be used for estimating the ITE (Prentice, 1976; Gelman & Hill, 2006; Chipman et al., 2010; Wager & Athey, 2015; Weiss et al., 2015), it is important to note how this task differs from standard supervised learning.", "startOffset": 83, "endOffset": 183}, {"referenceID": 18, "context": "This is a case of covariate shift, which is a special case of domain adaptation (Daume III & Marcu, 2006; Jiang, 2008; Mansour et al., 2009).", "startOffset": 80, "endOffset": 140}, {"referenceID": 18, "context": "This is a case of covariate shift, which is a special case of domain adaptation (Daume III & Marcu, 2006; Jiang, 2008; Mansour et al., 2009). A somewhat similar connection was noted in Sch\u00f6lkopf et al. (2012) with respect to covariate shift, in the context of a very simple causal model.", "startOffset": 119, "endOffset": 209}, {"referenceID": 11, "context": "Other discrepancy measures such as Maximum Mean Discrepancy (Gretton et al., 2012) could also be used for this purpose.", "startOffset": 60, "endOffset": 82}, {"referenceID": 17, "context": "Finally, we accomplish the third objective by minimizing the so-called discrepancy distance, introduced by Mansour et al. (2009), which is a hypothesis class dependent distance measure specifically tailored for domain adaptation applications.", "startOffset": 107, "endOffset": 129}, {"referenceID": 3, "context": "Deep neural networks have been shown to successfully learn good representations of high-dimensional data in many different tasks (Bengio et al., 2013).", "startOffset": 129, "endOffset": 150}, {"referenceID": 18, "context": "Definition 1 (Mansour et al. 2009).", "startOffset": 13, "endOffset": 34}, {"referenceID": 22, "context": "Counterfactual inference for determining causal effects in observational studies has been studied extensively in statistics, economics, epidemiology and sociology (Morgan & Winship, 2014; Robins et al., 2000; Rubin, 2011; Chernozhukov et al., 2013) as well as in machine learning (Langford et al.", "startOffset": 163, "endOffset": 248}, {"referenceID": 15, "context": ", 2013) as well as in machine learning (Langford et al., 2011; Bottou et al., 2013; Swaminathan & Joachims, 2015).", "startOffset": 39, "endOffset": 113}, {"referenceID": 5, "context": "These methods include any type of regression including linear and logistic regression (Prentice, 1976; Gelman & Hill, 2006), random forests (Wager & Athey, 2015) and regression trees (Chipman et al., 2010).", "startOffset": 183, "endOffset": 205}, {"referenceID": 8, "context": "Doubly robust methods combine aspects of parametric and non-parametric methods, typically by using a propensity score weighted regression (Bang & Robins, 2005; Dud\u0131\u0301k et al., 2011).", "startOffset": 138, "endOffset": 180}, {"referenceID": 31, "context": "Tian et al. (2014) presented one such method, modeling the interactions between treatment and covariates.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Bayesian Additive Regression Trees (BART) (Chipman et al., 2010) is a non-linear regression model which has been used successfully for counterfactual inference in the past (Hill, 2011).", "startOffset": 42, "endOffset": 64}, {"referenceID": 5, "context": "\u2020 (Chipman et al., 2010)", "startOffset": 2, "endOffset": 24}, {"referenceID": 5, "context": "\u2020 (Chipman et al., 2010)", "startOffset": 2, "endOffset": 24}], "year": 2016, "abstractText": "Observational studies are rising in importance due to the widespread accumulation of data in fields such as healthcare, education, employment and ecology. We consider the task of answering counterfactual questions such as, \u201cWould this patient have lower blood sugar had she received a different medication?\u201d. We propose a new algorithmic framework for counterfactual inference which brings together ideas from domain adaptation and representation learning. In addition to a theoretical justification, we perform an empirical comparison with previous approaches to causal inference from observational data. Our deep learning algorithm significantly outperforms the previous state-of-the-art.", "creator": "LaTeX with hyperref package"}}}