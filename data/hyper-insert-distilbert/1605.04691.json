{"id": "1605.04691", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2016", "title": "On Avoidance Learning with Partial Observability", "abstract": "we study a framework formula where agents primarily have to avoid aversive signals. for the agents are given only partial information, in quite the form of vague features that are are projections typical of certain task cognitive states. additionally, the progressive agents have appeared to partially cope directly with non - trivial determinism, defined as representing unpredictability on the approximate way only that multiple actions correctly are truly executed. hence the preferred goal procedure of each bounded agent is to define explicitly its behavior based on feature - action bound pairs : that reliably avoid unnecessary aversive signals. we study employing a computational learning complexity algorithm, ultimately called behavioral a - adaptive learning, that exhibits explicit fixpoint convergence, where the probable belief variables of which the allowed feature - action pairs eventually becomes continuously fixed. a - learning is parameter - free and easy to implement.", "histories": [["v1", "Mon, 16 May 2016 09:26:53 GMT  (237kb,D)", "http://arxiv.org/abs/1605.04691v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom j ameloot"], "accepted": false, "id": "1605.04691"}, "pdf": {"name": "1605.04691.pdf", "metadata": {"source": "CRF", "title": "On Avoidance Learning with Partial Observability", "authors": ["Tom J. Ameloot"], "emails": [], "sections": [{"heading": null, "text": "Contents"}, {"heading": "1 Introduction 1", "text": ""}, {"heading": "2 Related Work 6", "text": ""}, {"heading": "3 Fundamental Notions 7", "text": "3.1 Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.2 Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9"}, {"heading": "4 Avoidance Learning 11", "text": "4.1 A-learning Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 4.3 Fairness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15"}, {"heading": "5 Simple Grid Navigation 16", "text": "5.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 5.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18"}, {"heading": "6 Conclusion and Further Work 20", "text": ""}, {"heading": "1 Introduction", "text": "The main aim of this paper is to let agents solve tasks by ultimately avoiding aversive signals forever. This approach entails an interesting and perhaps quite\n\u2217T.J. Ameloot is a Postdoctoral Fellow of the Research Foundation \u2013 Flanders (FWO).\nar X\niv :1\n60 5.\n04 69\n1v 1\n[ cs\n.A I]\n1 6\nM ay\n2 01\nstrong guarantee on the agent performance. The motivation is partly to understand how animals are successful in solving problems, like navigation (GevaSagiv et al., 2015), with limited sensory information and unpredictable effects in the environment. The animal should find food or return home before it gets lost or becomes exhausted.\nWe study a general framework in which agents need to avoid problems in tasks. If the agent encounters a problem, an aversive signal is received. This way the agent could learn to avoid the problem, by avoiding the usage of actions and action-sequences that lead to aversive signals. The general idea is sketched in Figure 1.1. Before we discuss our approach, we first briefly discuss two important ingredients of the framework, namely, partial information and non-determinism.\nPartial information and non-determinism First, we assume that the agent is given only partial information, as follows: each encountered task state is projected to a set of features. This is a propositional representation, where each feature is a true/false question posed about the state (van Otterlo, 2009). The number of features determines the granularity by which states can be perceived by agents. Therefore, the behavior of the agent will be based on feature-action associations, and not on (direct) state-action associations.1 Each application can choose its own features and its own way of computing them. Examples of features are: detected edges in images, impulses through sensors, temporal events over streams, AND-OR combinations thereof, etc. In this paper, we assume that tasks have only a finite number of features, although there could still be many features. Perhaps not surprisingly, theoretical investigations show how hard it is to solve tasks under partial information, see e.g. (Lusena et al., 2001; Roy et al., 2005; Chatterjee and Chmel\u0301\u0131k, 2015).2\nSecond, we allow tasks to be non-deterministic. This means that the effect of some action-applications to states can not be predicted. In this paper, we assume that non-determinism is an inherent property of tasks. Although partial information also limits the reasoning within the agent, and therefore generally prevents accurate predictions, it remains a separate assumption to allow tasks themselves to be non-deterministic. For example, one may consider tasks in\n1If the same feature is used by several states, this may be seen as a form of generalization over those states. In this paper, features are used directly, and we do not perform a (second) generalization step over the observed features.\n2Part of the motivation for this paper is also to reason about the feature design for solving tasks. An example is given in Section 5.\nwhich features actually provide complete information, and where the agent could still struggle with non-determinism.3\nStrategies The focus of this paper to understand agents based on their behavior in tasks, which could be a useful way to understand intelligence in general (Pfeifer and Scheier, 1999). As remarked earlier, in this paper, agent behavior will be based on feature-action associations. Conceptually, we may think of the agent as having a set P of possibly allowed feature-action pairs, and whenever the agent encounters a task state s, the agent (thinks it) is allowed to perform all actions a for which there is a feature f observed in state s such that (f, a) \u2208 P . We also refer to P as a policy.\nWe say that a set P of feature-action pairs constitutes a strategy for a start state if P will never lead to an aversive signal when starting from that start state. We note that it is not always sufficient for the states near the aversive signals to steer away from them, because sometimes the agent may get trapped in a zone of the state space that does not immediately give aversive signals but from which it is impossible to reliably escape the aversive signals. The agent should avoid such zones, which could require that the agent anticipates aversive signals from very early on.\nOur aim in this paper is to reason about the existence of such successful strategies for classes of tasks, and to discuss an algorithm to find such strategies automatically. A main challenge throughout this study is posed by the compression of state information into features and the uncontrollable outcomes due to non-determinism.\nReward-based value estimation seems unsuitable Before presenting more details of our algorithm, we first argue that algorithms based on (numerical) reward-based value estimation do not always appear suitable for reliably finding problem-avoiding strategies.\nOn the theory side, convergence proofs of value estimation algorithms often require the learning step-size to decrease over time, see e.g. (Jaakkola et al., 1994; Watkins and Dayan, 1992). Intuitively, convergence of the estimated values arises because the decreasing learning step-size makes it harder and harder for the agent to learn as time progresses. However, we would like to avoid putting such limits on the agent, because: (1) it is useful to also study more flexible agents because they might sometimes better describe real-world agents; (2) in practice it might be difficult to estimate in what exact way the learning step-size should decrease; and, (3) also in practice, there are no guarantees on what the estimated values will eventually be after a certain amount of time has passed, because the estimates depend strongly on random fluctuations during task performance (due to non-determinism).\nIn practice, a non-decreasing step-size, although potentially useful to model flexible agents that keep learning from their latest experiences (Sutton and Barto, 1998), can lead to problems of its own. We illustrate this with the example task shown in Figure 1.2a. There is a start state 1, and two actions a and b that lead back to state 1. We assume complete information for now, i.e., state 1 is presented completely to the agent as a feature with the same information, namely, the identity of state 1. Suppose that the state-action pair\n3We will see an example of this situation a bit later in the Introduction.\n(1, a) always gives reward +1. But for the pair (1, b), the reward could be either +5 or \u221210. Although the pair (1, b) is clearly preferable over the pair (1, a) in case of positive reward, there is the risk of incurring a strong negative reward. The negative reward represents an aversive signal. In the perspective of strategies from above, note that (1, a) constitutes a strategy: constantly executing action a in state 1 leads to an avoidance of aversive signals forever.\nBut the agent will not necessarily learn to avoid (1, b) when a hidden task mechanism could periodically deceive the agent by issuing higher rewards under action b. Concretely, let n be a strictly positive natural number. To represent the outcome of action b, suppose that we constantly give reward +5 during the first n times (1, b) is applied; the next n times we give \u221210; the following n times we again give +5, and so on. We call this the n-swap semantics. For each outcome, the empirical probability would be 0.5: indeed, the observed frequency of each outcome converges to 0.5 as we perform more applications of action b. We can choose n arbitrarily large; this does not change the empirical probability of each outcome.\nWithout the restriction on learning step-size, it seems that value estimation algorithms can get into trouble on the above setting because we can set n so large that after a while the agent starts to believe that the outcome would remain fixed. For example, we could start with reward +5 for the pair (1, b) during the first n applications, and the agent starts believing that the reward really is +5. Then come the next n applications, where we repeatedly give reward \u221210, and the agent starts believing that the reward really is \u221210. We can swap the two outcomes forever, each for a period of n applications, and the agent will never make up its mind about the behavior of action b in state 1. This effect is illustrated in Figure 1.3.\nAlthough the above example is very simple, real-world tasks could still exhibit problems similar to the n-swap semantics. Even if such problems are identified and understood, perhaps there are no good solutions for them as the problems might be outside the range of control for the agent. In this paper we would like to learn to avoid the aversive signals forever, even under quite adversary semantics of tasks like the n-swap semantics.\nAvoidance learning In the example of Figure 1.2a, we would like the agent to make up its mind more quickly that action b leads to aversive signals. An idea is to let the agent (monotonically) increase its estimate of the value of a feature-action pair. We should immediately observe, however, that this idea will not work when feedback remains to be modeled as reward, as in the example: once the outcome of (1, b) is observed to be +5; then remembering +5 would lead to a preference of (1, b) over (1, a), causing a reward-seeking agent to (accidentally) encounter negative rewards, i.e., aversive signals, indefinitely under n-swap semantics.\nFortunately, the idea of increasing estimates seems to work when feedback is modeled with aversive signals, even in face of non-determinism. Indeed, Heger (1994) has previously proposed a learning algorithm in tasks where actions have numeric costs, representing aversive signals. By repeatedly remembering the highest observed cost for a state-action pair (with the max-operator), and by choosing actions to minimize such costs, the agent learns to steer away from high costs. We would like to further elaborate this idea and how it relates to the notion of aversion-avoiding strategies mentioned above.\nIn our framework, we only explicitly model aversive signals, as boolean flags: the flag \u201ctrue\u201d would mean that an aversive signal is present. This leads to a framework that is conceptually neat and computationally efficient. Because a policy is either successful in avoiding aversive signals forever, or it is not, the choice of a boolean model aligns well with our motivation to study the relationship between learning and successful strategies. To illustrate, the example of Figure 1.2a would be represented by Figure 1.2b, where only the aversive signal is explicitly represented. In general, the boolean flags will act like borders, to\ndemarcate undesirable areas in the state space. Reward is now only implicit: by using a strategy, as mentioned earlier, the agent can stay away from the aversive signals forever.\nIn the above setting with explicit aversive signals, we describe an avoidance learning algorithm, called A-learning, in which the agent repeatedly flags feature-action pairs that lead to aversive signals, or, as an effect thereof, to states for which all proposed actions are flagged (based on the observed features). Intuitively, the flags indicate \u201cdanger\u201d. On the example of Figure 1.2b, A-learning flags (1, b) at the first occurrence of an aversive signal under action b; and, importantly, the strategy (1, a) is never flagged.4 There is no second chance for changing the agents mind. This gives one of the strongest convergence notions in learning, namely, fixpoint convergence, where the agent eventually stops changing its mind about the outcome of actions.\nIf there really is a strategy, avoidance learning will carve out a subset of good feature-action pairs from the mass of all feature-action pairs. This way, it seems that avoidance learning could be useful in making the agent eventually avoid aversive signals forever. This provides the guaranteed agent performance we would like to better understand, as remarked at the beginning of the Introduction.\nMeaning of optimality In this paper we view an agent as being optimal if it can (learn to) avoid aversive signals forever. There is no explicit concept of reward. Depending on the setting, or application, aversive signals can originate from diverse sources and together they can describe a very detailed image of what the agent is allowed to do, and what the agent is not allowed to do. One obtains a rich conceptual setting for reasoning about agent performance.\nFor example, suppose a robotic agent should learn to move boxes in a storehouse as fast as possible. We could emit an aversive signal when the robot goes beyond a (reasonable) time limit. Any other constraints, perhaps regarding battery usage, can be combined with the first constraint by adding more signals.\nOutline This paper is organized as follows. We discuss related work in Section 2. We introduce fundamental concepts like tasks, and strategies, in Section 3. We present and analyze our avoidance learning algorithm in Section 4. One of our results is that if there is a strategy for a start state then the algorithm will preserve the strategy. This mechanism can be used to materialize strategies if they exist. To better understand the nature of strategies, we prove the existence of strategies for a family of grid navigation tasks in Section 5."}, {"heading": "2 Related Work", "text": "The idea of avoiding aversive signals, or problems in general, is related to safe reinforcement learning (Garc\u0301\u0131a and Ferna\u0301ndez, 2015). There, the goal is essentially to perform reinforcement learning, often based on approximation techniques for optimizing numerical reward, with the addition of avoiding certain\n4In our description of A-learning (Section 4), the flagged feature-action pairs are removed from the agent\u2019s memory.\nproblematic areas in the task state space. An example could be to train a robot for navigation tasks but while avoiding damage to the robot as much as possible. In the current paper, feedback to the agent consists of the aversive signals. Reward becomes more implicit, as it lies in the avoidance of aversive signals. Therefore, the viewpoint in this paper is that the agent is called optimal when it eventually succeeds in avoiding all aversive signals forever; there is no notion of optimizing reward. The approach is related to a trend identified by Garc\u0301\u0131a and Ferna\u0301ndez (2015), namely, the modification of the optimality criterion.\nThe work by Heger (1994) is closely related to our work. The framework by Heger (1994) provides feedback to the agent in the form of numerical cost signals, which, from the perspective of this paper, could be seen as aversive signals. Similar to our n-swapping example in the Introduction (Figure 1.2a), Heger (1994) provides other examples to motivate that estimation of expected values is not suitable for reliably deciding actions. The learning algorithm proposed by Heger (1994) maps each state-action pair to the worst outcome (or cost), by means of the max-operator. By remembering the highest incurred cost for a state-action pair, the agent in some sense learns about \u201cwalls\u201d in the state space that constrain its actions towards lower costs. The avoidance learning algorithm discussed in this paper (Section 4) is similar in spirit to the one by Heger (1994). A deviation, however, is that we assume here a boolean interpretation of aversive signals, which leads to a neat and computationally efficient framework. We additionally identify the concept of strategies, under which the agent can avoid aversive signals forever. Our interest lies in understanding such avoidance strategies and their relationship to the avoidance learning algorithm. Moreover, we also focus on partial information, by letting the agent only observe features instead of full states."}, {"heading": "3 Fundamental Notions", "text": ""}, {"heading": "3.1 Tasks", "text": "For a set X, let P(X) denote the powerset of X, i.e., the set of all subsets of X. A task is a tuple T = (S,B,A, F, \u03b4, \u03d5,\u2126) where\n\u2022 S is a nonempty set of states;\n\u2022 B \u2286 S is a finite subset of start states;5\n\u2022 A is a nonempty finite set of actions;\n\u2022 F is a nonempty finite set of features;\n\u2022 \u03b4 : S \u00d7A\u2192 P(S) is the transition function;\n\u2022 \u03d5 : S \u2192 P(F ) is the feature function; and\n\u2022 \u2126 \u2286 S \u00d7A is the set of aversive signals,\nwhere all states s \u2208 S are reachable in the sense that there is a sequence s0, a0, s1, a1, . . . , sn with s0 \u2208 B, sn = s, and si \u2208 \u03b4(si\u22121, ai\u22121) for each i \u2208 {1 .. n}.\n5The symbol B stands for \u201cbegin\u201d.\nThe function \u03b4 maps each pair (s, a) \u2208 S \u00d7 A to a set of possible successor states, representing non-determinism. The function \u03d5 associates a set of features to each state; an agent interacting with the task can only observe states through features and can therefore not directly observe states. The meaning of a pair (s, a) \u2208 \u2126 is that the agent could witness an aversive signal when performing action a in state s.6\nExample 3.1. We define an example task T = (S,B,A, F, \u03b4, \u03d5,\u2126) as follows: S = {1, 2}; B = S; A = {a}; F = {f, g}; regarding \u03b4, we define\n\u03b4(1, a) = {1}, \u03b4(2, a) = {2};\nregarding \u03d5, we define\n\u03d5(1) = {f}, \u03d5(2) = {g};\nand, we define \u2126 = {(1, a)}. The task is depicted in Figure 3.1.\nRemark 3.2 (All features, one successor state). Note that the function \u03d5 maps each state to a set of features. Similarly, the function \u03b4 maps each state-action pair to a set of successor states. However, an agent interacts with each function in a different way, as follows. For a state s, we assume that an agent can always observe all features in \u03d5(s) simultaneously. This way, the function \u03d5 may be viewed as being deterministic. In contrast, for a state-action pair (s, a), we select only one successor state from \u03b4(s, a) to proceed with the task.\nThe function \u03d5 remains deterministic throughout this paper. The framework still allows us to consider tasks in which the agent can sometimes observe a certain feature and sometimes it can not. Thereto we can define richer states, in which, say, the status of sensors is stored; if a state s says that a sensor is broken, then \u03d5(s) could omit the feature that would otherwise be generated by the sensor.\n6In a fair task, if the agent would infinitely often perform action a in state s, then the agent witnesses an aversive signal infinitely often during the application of a at state s, but this signal could sometimes be omitted. See also Section 4.3.\nRemark 3.3 (Modeling flexibility). Our definition of task resembles that of a standard Markov decision process (Sutton and Barto, 1998), but we have added features and aversive signals. There can be many features, actions, and start states. And we allow an infinite number of states."}, {"heading": "3.2 Strategies", "text": "Since the agent may only see features, and not states directly, agent behavior has to be based on feature-action associations.\nLet T = (S,B,A, F, \u03b4, \u03d5,\u2126) be a task. A policy for T is a total function \u03c0 : F \u2192 P(A). We allow features to be mapped to empty sets of actions. If the task is understood from the context, for a state s \u2208 S we define\nact(s, \u03c0) = \u22c3\nf \u2208 \u03d5(s) \u03c0(f),\ni.e., act(s, \u03c0) is the set of all actions that are proposed by the policy \u03c0 based on the features in s. We say that a state s is blocked in \u03c0 if act(s, \u03c0) = \u2205, i.e., the policy does not propose actions for s.\nRemark 3.4 (Features as actors). For a state s, we do not view \u03d5(s) as an atomic signature to which actions should be associated. Instead, the definition of act(s, \u03c0) indicates that each feature in \u03d5(s) may independently propose its own actions, regardless of what is proposed by other features. All proposed actions are collected into a set, by means of the union-operator.7 Therefore, features are little actors that become active at appropriate times and that suggest to the agent what actions are (supposedly) allowed. This viewpoint resembles the way that an individual neuron (or a small group of neurons) in the brain could represent a distinct concept and could be individually linked to actions (Potjans et al., 2011; Fre\u0301maux et al., 2013).8 It is the goal of the learning algorithm (Section 4) to remove feature-action associations that lead to aversive signals or, as a result of such removals, to blocked states.\nWe now consider the following definition:\nDefinition 3.5. A policy \u03c0 is called a strategy for a start state s0 \u2208 B if\n1. act(s0, \u03c0) 6= \u2205;\n2. \u2200s \u2208 S, \u2200a \u2208 act(s, \u03c0),\n(a) \u2200s\u2032 \u2208 \u03b4(s, a) we have act(s\u2032, \u03c0) 6= \u2205; and, (b) (s, a) /\u2208 \u2126.\n7It would in general not be possible to replace this with an intersection-operator, because then there might be some unreliable features to which no action can be associated. Under an intersection-operator, such empty sets will destroy the proposals contributed by reliable features.\n8So, features constitute the agent\u2019s mind, like neurons constitute an organism\u2019s brain. Likely there are only finitely many neurons, so the assumption of having a finite number of features and a finite number of actions (or decisions) appears natural.\nIn words: a policy is a strategy for a start state s0 if the policy acts upon s0; and, for any states upon which the policy acts, the reached successor states can also be acted upon, and the policy never causes aversive signals. Intuitively, to use a strategy, for each encountered state s we first select some (arbritary) feature f \u2208 \u03d5(s) that satisfies \u03c0(f) 6= \u2205, and we subsequently select an arbitrary action a \u2208 \u03c0(f).\nRemark 3.6 (Global viewpoint). The definition of strategy demands properties in a global fashion, possibly also for states that would not be explored when strictly following the strategy. This condition however ensures that learning algorithms can never have negative experiences when they perform actions suggested by the strategy; see Section 4.\nSuppose \u03c0 is a strategy, and let f be a feature with \u03c0(f) 6= \u2205. Intuitively, the definition of strategy says that f is a reliable feature, in the sense that every time we see it, we may safely perform all actions in \u03c0(f), without the risk of encountering blocked states and aversive signals. This is related to the Markov assumption (Sutton and Barto, 1998), because we do not have to remember any features that were seen during previous time steps, and we may instead choose actions based on just f by itself.\nExample 3.7. Consider the task from Example 3.1. There is no strategy for start state 1, but there is a strategy \u03c0 for start state 2 defined as: \u03c0(f) = \u2205 and \u03c0(g) = {a}.\nThe following property illustrates that strategies are resilient to adding new features. In practical applications, this means that the addition of new kinds of features will not destroy previously existing strategies.9\nProposition 3.8. Let T1 = (S1, B1, A1, F1, \u03b41, \u03d51,\u21261) be a task. Let V be a set of features that is disjoint from F1. Let T2 = (S2, B2, A2, F2, \u03b42, \u03d52,\u21262) be another task that is almost the same as T1 except that F2 = F1 \u222a V and for each state s the constraint \u03d52(s) \u2229 F1 = \u03d51(s) holds.10 Let s0 be a start state, and suppose that a policy \u03c0 : F1 \u2192 P(A1) is a strategy for s0 in T1. Then \u03c0 is also a strategy for s0 in T2.\nProof. We show that the conditions of strategy in Definition 3.5 are satisfied for \u03c0 in T2. To better show which task is involved, for a state s and a task index i \u2208 {1, 2}, we write\nact i(s, \u03c0) = \u22c3\nf\u2208\u03d5i(s) \u03c0(f).\nWhen \u03c0 is used in T2, we assume \u03c0(f) = \u2205 for each f \u2208 V . Above we have also assumed that A1 = A2.\nCondition 1 Since \u03c0 is a strategy for s0 in T1, we have act1(s0, \u03c0) 6= \u2205. This implies there is some f \u2208 \u03d51(s0) with \u03c0(f) 6= \u2205. Since \u03d51(s0) \u2286 \u03d52(s0) by assumption, we obtain act2(s0, \u03c0) 6= \u2205.\n9For example, in a robotics application, new features could be the result of adding new sensor types to the robot.\n10This means that T2 uses the features of F1 in the same way as T1.\nCondition 2 Let s be a state and assume there is some action a \u2208 act2(s, \u03c0). First we argue that a \u2208 act1(s, \u03c0). There must be a feature f \u2208 \u03d52(s) with a \u2208 \u03c0(f). But since \u03c0 only knows features in F1, we have f \u2208 \u03d52(s)\u2229F1 = \u03d51(s). Hence, a \u2208 act1(s, \u03c0).\nWe first handle Condition 2a. Let s\u2032 \u2208 \u03b4(s, a). Because a \u2208 act1(s, \u03c0) and \u03c0 satisfies Condition 2a in T1, we know act1(s\n\u2032, \u03c0) 6= \u2205. So there is a feature f \u2032 \u2208 \u03d51(s\u2032) with \u03c0(f \u2032) 6= \u2205. Since \u03d51(s\u2032) \u2286 \u03d52(s\u2032), we know a \u2208 act2(s\u2032, \u03c0) 6= \u2205, as desired.\nNow we handle Condition 2b. Because a \u2208 act1(s, \u03c0) and \u03c0 satisfies Condition 2b in T1, we know (s, a) /\u2208 \u21261. Since \u21262 = \u21261, we obtain (s, a) /\u2208 \u21262, as desired."}, {"heading": "4 Avoidance Learning", "text": "We present and study an avoidance learning algorithm, and its relationship to the concept of strategy introduced in Section 3.2."}, {"heading": "4.1 A-learning Algorithm", "text": "Algorithm 4.1 is an avoidance learning algorithm. The algorithm describes how the agent interacts with the task, and how feature-action combinations are forgotten as the direct or indirect result of aversive signals. Some aspects of the interaction are not under control of the agent, in particular how a successor state is chosen by means of function \u03b4, and how features are derived from states by means of function \u03d5. We now provide more discussion of the algorithm. Henceforth, we will refer to Algorithm 4.1 as A-learning.\nThe essential product of A-learning is a set P \u2286 F \u00d7 A that represents the allowed feature-action pairs; the symbol P stands for \u201cpossibilities\u201d. At any time, the set P uniquely defines a policy \u03c0 as follows: for each f \u2208 F , we define \u03c0(f) = {a \u2208 A | (f, a) \u2208 P}. Regarding notation, for any state s, we write act(s, P ) to denote the set act(s, \u03c0) of proposed actions, where \u03c0 is the unique policy defined by P .\nWe now explain the steps of A-learning in more detail.\n\u2022 Line 1 initializes P with all feature-action pairs. We will gradually remove pairs if they lead to \u2126 or to blocked states (that are created by removals of the first kind).\n\u2022 Line 2 selects a random start state. The control flow of the algorithm is redirected here each time we want to restart the task. But we never re-initialize P .\nTask restarts may be requested by A-learning itself (see below), or externally by the training framework in which A-learning is running.\n\u2022 Line 3 requests a task restart in case the chosen start state is blocked. This allows more exploration from the other start states. As we will see later in Theorem 4.5(1), if no actions remain for a start state then this start state has no strategy.\n\u2022 At Line 6, the algorithm enters a learning loop. The loop is only exited to satisfy task restart requests, at Line 7.\nAlgorithm 4.1: Avoidance learning (A-learning)\nData: Task T = (S,B,A, F, \u03b4, \u03d5,\u2126).\n1 P := F \u00d7A; 2 s := choose from B;\n3 if act(s, P ) = \u2205 then 4 request restart (see below);\n5 end\n6 while true do\n7 if restart requested then 8 go to Line 2;\n9 end\n10 a := choose from act(s, P ); 11 s\u2032 := choose from \u03b4(s, a); 12 if (s, a) \u2208 \u2126 or act(s\u2032, P ) = \u2205 then 13 P := P \\ (\u03d5(s)\u00d7 {a}); 14 request restart;"}, {"heading": "15 end", "text": "16 s := s\u2032;"}, {"heading": "17 end", "text": "\u2022 At Line 10, we choose an action a to apply to current state s based on the set act(s, P ) of still allowed actions. At Line 11, we are subsequently given a successor state s\u2032, chosen arbitrarily from \u03b4(s, a).\n\u2022 Next, at Line 12, we check whether we have encountered \u2126 or if successor state s\u2032 is blocked. In either case we exclude from P the feature-action pairs that caused us to apply action a in state s (Line 13), and we restart the task (Line 14).\n\u2022 If we do not encounter \u2126 and state s\u2032 is not blocked, then we proceed with the while loop (Line 16).\nNote that in general there are multiple runs of A-learning on a task, because of the choice on action selection and the choice on successor state. Each run of A-learning is infinitely long. Nonetheless, there is always an eventual fixpoint on the set P because after the initialization we only remove feature-action pairs. There are only a finite number of possible feature-action pairs, although there could be many. When the run is clear from the context, we write P \u2217 to denote the fixpoint of P obtained in that run.\nFor conceptual convenience, we can divide each run of A-learning into trials by using the task restarts as dividers: whenever we execute Line 2, the previous trial ends and the next trial begins. Each trial is thus a sequence s0, a0, s1, a1, . . . , sn, where s0 is a start state, sn is the last state of the trial, and si \u2208 \u03b4(si\u22121, ai\u22121) for each i \u2208 {1 .. n}.11\n11We only use explicit task restarts (Line 2) to divide runs into trials, and not the encounter\nRemark 4.1 (No stopping condition). There is no stopping condition in the algorithm because in general we may not be able to detect when the agent has explored the task sufficiently to be successful at avoiding aversive signals.\nRemark 4.2 (Greediness). We would like to emphasize that A-learning is always greedy in avoiding \u2126. This is an important deviation from the -greedy exploration principle (Sutton and Barto, 1998), where at each time step the agent chooses a random action with small probability \u2208 [0, 1]. We do not use that mechanism here because otherwise the agent keeps running the risk of encountering aversive signals (Garc\u0301\u0131a and Ferna\u0301ndez, 2015).\nRemark 4.3 (Internal task restarts). The reason for requesting a task restart at Line 14 is that sometimes the agent could become stuck in a zone of the state space where there are only blocked states or aversive signals. In that case, if we want the agent to start removing feature-action pairs to prevent future aversive signals, we should first transport the agent to a zone in the state space without blocked states and aversive signals. For example, in a robot navigation problem, the robot could learn to avoid pits, but once it enters a pit it can perhaps not reliably escape without the help of an external supervisor.\nRemark 4.4 (Memory efficiency). Algorithm 4.1 explicitly stores the allowed feature-action pairs in a set P . This is an intuitive perspective for the theory developed in this paper. However, in practice it may sometimes be more efficient to store the opposite information, namely, the removed feature-action pairs. This way all allowed feature-action pairs can still be uniquely recovered. Using the analogy of a planar map, where aversive signals are borders between neutral zones on the one hand and undesirable zones on the other hand, there could be a decreased memory usage in storing only the border (i.e., the removed featureaction pairs) if the borders are simple shapes instead of irregular shapes with many protrusions."}, {"heading": "4.2 Results", "text": "The following theorem helps to understand what A-learning computes.\nTheorem 4.5. For all tasks T = (S,B,A, F, \u03b4, \u03d5,\u2126), for each s0 \u2208 B, for each run of A-learning, where P \u2217 denotes the fixpoint,\n1. if there is a strategy for s0 then act(s0, P \u2217) 6= \u2205.\n2. if act(s0, P \u2217) 6= \u2205 then every trial for s0 after the fixpoint avoids blocked\nstates and \u2126.\nProof. We consider the two properties separately.\nProperty 1 Suppose there is a strategy \u03c0 for s0. We show that the featureaction pairs of \u03c0 are preserved in P \u2217, so that act(s0, \u03c0) 6= \u2205 would imply act(s0, P\n\u2217) 6= \u2205. Towards a contradiction, suppose that A-learning removes a pair (f, a) from P where a \u2208 \u03c0(f); let (f, a) be the first such pair that is removed. The removal has happened as follows: we reach a state s with f \u2208 \u03d5(s) of start states. This means that in principle we allow si \u2208 B for some or all i \u2208 {1 .. n}.\nand we perform a, and either the successor state s\u2032 \u2208 \u03b4(s, a) is blocked or we receive an aversive signal. We discuss each case in turn.\nLet P denote the remaining feature-action pairs just before we remove (f, a). Note that a \u2208 \u03c0(f) and f \u2208 \u03d5(s) together imply a \u2208 act(s, \u03c0). \u2022 Suppose that s\u2032 is blocked. Since \u03c0 is a strategy, by condition 2a of\nDefinition 3.5, we have assumed act(s\u2032, \u03c0) 6= \u2205. So, there is a feature f \u2032 \u2208 \u03d5(s\u2032) and an action a\u2032 \u2208 \u03c0(f \u2032). Since (f, a) is the first pair of \u03c0 that is removed, we still have (f \u2032, a\u2032) \u2208 P . But then a\u2032 \u2208 act(s\u2032, P ), and s\u2032 is actually not blocked; we have found a contradiction.\n\u2022 Suppose that an aversive signal was received when applying a to s, which implies (s, a) \u2208 \u2126. This immediately contradicts the assumption that \u03c0 satisfies condition 2b of Definition 3.5.\nProperty 2 Suppose act(s0, P \u2217) 6= \u2205. Towards a contradiction, suppose that after the fixpoint there is a trial for start state s0 where we encounter a state s and we perform an action a such that either the successor state is blocked or we receive an aversive signal. Suppose we conceptually halt the offending trial at the first encountered problem. We have followed a path:\ns0 a0\u2212\u2192 s1 a1\u2212\u2192 . . . sn\u22121 an\u22121\u2212\u2212\u2212\u2192 sn = s an=a\u2212\u2212\u2212\u2192 s\u2032,\nfor some s\u2032 \u2208 \u03b4(s, a). We have ai \u2208 act(si, P \u2217) for each i \u2208 {0 .. n}. We note in particular that a \u2208 act(s, P \u2217). Next we distinguish two cases, depending on the type of problem.\n\u2022 Suppose that act(s\u2032, P \u2217) = \u2205. Then A-learning now removes \u03d5(s) \u00d7 {a} from P . But then we will no longer propose action a for state s, which was previously allowed by the fixpoint. Then P \u2217 would be an invalid fixpoint, which is a contradiction.\n\u2022 Suppose that an aversive signal is received when applying a to s, which implies (s, a) \u2208 \u2126. We make a similar reasoning as in the previous case: A-learning removes \u03d5(s) \u00d7 {a} from P . Again P \u2217 would be an invalid fixpoint.\nRemark 4.6 (Strategies and eventual success). Suppose that a task has a strategy for each start state. In that case, Theorem 4.5 tells us that every run of A-learning will eventually avoid blocked states and aversive signals. The agent therefore makes a transition from first discovering the strategies to later exploiting the strategies.\nThe opposite is not necessarily true: there are tasks for which exist runs that eventually avoid blocked states and aversive signals, but without there being a strategy in the sense of Definition 3.5. This is illustrated by the task in Figure 4.1. Consider a run where the first application of action b in state 1 results in an aversive signal, and after which we immediately restart the task. In that run, there is no further exploration to state 2, which causes (f, a) \u2208 P \u2217; hence, act(1, P \u2217) = {a} 6= \u2205. However, note that if the internal restart request at Line 14 of Algorithm 4.1 would sometimes not be handled immediately, but a few steps later, then some runs will not preserve the pair (f, a).\nRemark 4.7 (Usage of A-learning). The insights of Theorem 4.5 could be used as follows. First, although proving that a strategy exists helps in understanding guarantees on the agent performance, programming the strategy by hand could be tedious and time-consuming. So, Property 1 could be used to materialize strategies once they are proven to exist.\nSecond, if one does not know whether a strategy exists, Property 2 could be used to perform a preliminary search for strategies. Although the discovered strategies might not be easily interpreted, they could serve as inspiration for a theoretical study of strategies for the tasks at hand. A practical consideration, however, is that it might not be possible to efficiently detect the fixpoint, i.e., typically one does not know if a fixpoint has been reached when A-learning has not removed feature-action pairs for a while."}, {"heading": "4.3 Fairness", "text": "So far we have silently allowed all possible runs of A-learning. For example, we did not explicitly demand that the agent actually must receive an aversive signal when applying an action a to a state s where (s, a) \u2208 \u2126. The aversive signal could also be omitted. This brings us to the topic of fairness (Francez, 1986).\nIntuitively, for this paper, fairness would mean that there is sufficient exploration of the task. A practical application of A-learning (Algorithm 4.1) could take the following fairness assumptions into account:\n\u2022 if we execute Line 2 infinitely often then we choose each start state infinitely often;\n\u2022 to fully learn the task from each start state, we infinitely often issue external task restarts at Line 7; those restarts are not requested by A-learning itself;\n\u2022 at Line 10, if we encounter the same pair of a state s and set P infinitely often then we choose each action a \u2208 act(s, P ) infinitely often;\n\u2022 at Line 11, if we apply action a infinitely often to state s then each successor state s\u2032 \u2208 \u03b4(s, a) is visited infinitely often from an application of a to s;\n\u2022 at Line 12, if we perform action a in state s infinitely often, where (s, a) \u2208 \u2126, then the agent should infinitely often receive an aversive signal when applying a to s;\nThe only aspect of fairness that can be directly influenced by the agent itself, is the action selection at Line 10. For this purpose, a random number generator can be used to select random indices in an array-representation of the proposed actions.12\nRemark 4.8 (Fairness not required). Note that Theorem 4.5 also works for unfair runs. Every run has a fixpoint on P , whether the run is fair or not. But by exploring fewer states, or by issuing fewer aversive signals, an unfair run essentially makes it easier for the agent to avoid aversive signals. This way, some feature-action pairs could remain forever, even though a more fair exploration of the task could have removed them.\nAlso, because the notion of strategy in Definition 3.5 is rather strong, it is not possible for a fair run or an unfair run to confront the agent with a situation that leads to the failure of a strategy. The agent will never be disappointed in the exploitation of the strategy."}, {"heading": "5 Simple Grid Navigation", "text": "We study a simple class of grid navigation problems."}, {"heading": "5.1 Definitions", "text": "Let Z denote the set of integers. For any two points p1, p2 \u2208 Z \u00d7 Z, denoting p1 = (x1, y1) and p2 = (x2, y2), we recall the definition of L1-distance between p1 and p2:\nd1(p1, p2) = abs (x2 \u2212 x1) + abs (y2 \u2212 y1) . A simple grid navigation problem is a quintuple G = (Width,Height ,Starts,\nTargets, \u03c4), where\n\u2022 Width \u2208 N and Height \u2208 N are the dimensions of a terrain;\n\u2022 Starts \u2286 {0 ..Width} \u00d7 {0 ..Height} is a set of start locations;\n\u2022 Targets \u2286 {0 ..Width} \u00d7 {0 ..Height} is a set of possible target locations; and,\n\u2022 \u03c4 \u2208 N is a time limit,\nwith the following assumptions,\n\u2022 \u2200p, q \u2208 Targets, we assume d1(p, q) < \u03c4 ; and,\n\u2022 \u2200p \u2208 Starts,\u2200q \u2208 Targets, we assume d1(p, q) < \u03c4 . 12One has to assume that the random number generator is fair in selecting all indices\ninfinitely often if we let the system run forever.\nThe intuition is that at the beginning of a session we select a start location p \u2208 Starts and an initial active target location q \u2208 Targets and we should navigate from p to q within time \u03c4 . Whenever we reach the active target location q we choose another target location q\u2032 \u2208 Targets and we should now navigate from q to q\u2032 within time \u03c4 . This relocation of the active target may be repeated an arbitrary number of times. But at any moment we may also begin a new session, in which we again choose a start location and initial target location. There are infinitely many sessions. The available actions are: left, right, up, down, left-up, left-down, right-up, right-down, and wait. Importantly: failure to respect the time \u03c4 results in an aversive signal; we aim to eventually avoid such aversive signals.\nFor a location (x, y) \u2208 N \u00d7 N and an action a, we now define the possible successor locations that result from the application of a to (x, y); we denote this set as move(x, y, a). A set of multiple possible successors is used to represent non-determinism. An empty set of of successors is used to say that the action would lead outside the considered terrain. We assume the following actions to be deterministic: left, right, up, and down. The other, \u201cdiagonal\u201d, actions are non-deterministic. For example, for each (x, y) \u2208 Z\u00d7 Z,\nmove(x, y, left) = {(x\u2212 1, y)},\nmove(x, y, left-up) = {(x\u2212 1, y \u2212 1), (x\u2212 1, y), (x, y \u2212 1)}, and, move(x, y,wait) = {(x, y)}. We make the assumption that the direction of the positive Y-axis corresponds to \u201cdownward\u201d.\nWe now define the task structure task(G) = (S,B,A, F, \u03b4, \u03d5,\u2126) that corresponds to the above grid problem G. Here it will be convenient to view states and features as structured objects, with components; for an object x with a component y, we write x.y to access the component.\n\u2022 the set S consists of all triples s with components agent, target, and time, satisfying the following constraints: s.agent and s.target are both in the set {0 ..Width} \u00d7 {0 ..Height}, and s.time \u2208 {0 .. \u03c4};\n\u2022 the set B consists of those states s where s.agent \u2208 Starts, s.target \u2208 Targets, and s.time = \u03c4 ;\n\u2022 A = {left, right, up, down, left-up, left-down, right-up, right-down, wait};\n\u2022 the set F consists of all pairs f with components offset and time, satisfying the constraints: f .offset \u2208 {\u2212Width ..Width} \u00d7 {\u2212Height ..Height} and f .time \u2208 {0 .. \u03c4};13\n\u2022 the transition function \u03b4 is described by Algorithm 5.1; for a state s \u2208 S and action a \u2208 A, the set \u03b4(s, a) consists of all states that could possibly be returned by Algorithm 5.1 upon receiving input (s, a);\n13These features represent an agent-centric perspective, in which the relative offset to the target is stored (see below).\n\u2022 regarding \u03d5, for each s \u2208 S, we define \u03d5(s) = {f} where f \u2208 F is the single feature for which f .offset = s.target\u2212 s.agent and f .time = s.time; and,\n\u2022 \u2126 = {(s, a) \u2208 S \u00d7A | s.time = 0}.\nAlgorithm 5.1: Action application for grid navigation (Section 5)\ninput : (1) current state s \u2208 S (2) action a \u2208 A\noutput: successor state s\u2032 \u2208 S 1 (x1, y1) := s.agent; 2 (x2, y2) := choose from move(x1, y1, a); 3 if (x2, y2) \u2208 {0 ..Width} \u00d7 {0 ..Height} then 4 s\u2032.agent := (x2, y2); 5 else 6 s\u2032.agent := (x1, y1); 7 end 8 if s\u2032.agent = s.target then 9 s\u2032.target := choose from Targets;\n10 s\u2032.time := \u03c4 ;"}, {"heading": "11 else", "text": "12 s\u2032.target := s.target; 13 s\u2032.time := max(0, s.time\u2212 1);"}, {"heading": "14 end", "text": ""}, {"heading": "5.2 Results", "text": "Proposition 5.1. For each grid problem G, there is a strategy for each start state in task(G). Proof. Denote task(G) = (S,B,A, F, \u03b4, \u03d5,\u2126). We define one policy that is a strategy for all start states.\nFirst, we define an auxiliary set V \u2286 F to consist of all features f for which |f .offset|1 < f .time, where |(x, y)|1 = abs (x) + abs (y) is the L1-norm of a point (x, y). Intuitively, such features indicate that the deterministic distance from the agent location to the target location \u2013 where we only use the actions left, right, up, and down \u2013 can be bridged within the remaining time.\nWe now define a policy \u03c0. For all f \u2208 F \\ V we define \u03c0(f) = \u2205, and for each f \u2208 V , denoting f .offset = (x, y), we define\n\u03c0(f) =  {left} if x < 0; {right} if x > 0; {up} if x = 0 and y < 0; {down} if x = 0 and y > 0; {wait} x = 0 and y = 0.\nAs mentioned earlier, we define downwards as the direction of the positive Yaxis. The case where \u03c0(f) = {wait} occurs when the agent is located at the target.14\nLet s0 \u2208 B. We show that \u03c0 is a strategy for s0, according to Definition 3.5.\nCondition 1 of Definition 3.5 We show that act(s0, \u03c0) 6= \u2205. By assumption on s0, we have s0.agent \u2208 Starts, s0.target \u2208 Targets, and s0.time = \u03c4 . By using the distance assumptions on locations in G, we obtain d1(s0.agent, s0.target) < \u03c4 . Letting f be the single feature in \u03d5(s0), we see that |f .offset|1 < \u03c4 = f .time, which implies that f \u2208 V . Hence \u03c0(f) 6= \u2205, which implies act(s0, \u03c0) 6= \u2205.\nCondition 2a of Definition 3.5 Let s \u2208 S. Suppose there is some action a \u2208 act(s, \u03c0). Let f denote the single feature of s. We have a \u2208 \u03c0(f), which implies f \u2208 V .\nLet s\u2032 \u2208 \u03b4(s, a). We must show that act(s\u2032, \u03c0) 6= \u2205. Let f \u2032 be the single feature of s\u2032. We will show that f \u2032 \u2208 V , which implies \u03c0(f \u2032) 6= \u2205, and further that act(s\u2032, \u03c0) 6= \u2205. Based on Algorithm 5.1, we reason about what has happened during the application of action a to state s.\n\u2022 Suppose the if-test at Line 8 succeeds, i.e., the agent reaches the target location. Then s\u2032.time = \u03c4 , and\nd1(s \u2032.agent, s\u2032.target) = d1(s.target, s \u2032.target)\n< \u03c4,\nwhere we use the distance assumption between target locations. Overall, |f \u2032.offset|1 < f \u2032.time; hence, f \u2032 \u2208 V .\n\u2022 Suppose the if-test at Line 8 does not succeed, i.e., the agent did not yet reach the target location. It must be that a 6= wait, because otherwise f .offset = (0, 0), which implies s\u2032.agent = s.agent = s.target, and the test at Line 8 would have succeeded (see previous case). So, a \u2208 {left, right, up, down}. First, we observe that\nd1(s \u2032.agent, s\u2032.target) < d1(s.agent, s.target).\nIndeed, this property holds because (1) the locations s.agent and s.target = s\u2032.target are inside the convex terrain; (2) the action a is given deterministic movement semantics (i.e., there is precisely one outcome), causing s\u2032.agent to be both inside the terrain and strictly closer to s.target = s\u2032.target.\nSecond, we also observe that\nf \u2032.time = f .time\u2212 1,\nsince s\u2032.time = max(0, s.time \u2212 1) by definition and s.time > 0 (which follows from f \u2208 V ).\n14Algorithm 5.1 implies that the situation where f .offset = (0, 0) only occurs when the agent reaches some target location and the next target location is the same as the old target location.\nOverall, we may now write\n|f \u2032.offset|1 < |f .offset|1 \u2264 f .time\u2212 1 = f \u2032.time.\nIn the second line we have used f \u2208 V . We conclude that f \u2032 \u2208 V .\nCondition 2b of Definition 3.5 Let s \u2208 S. Suppose there is some action a \u2208 act(s, \u03c0), which implies that the single feature f of s must be in V . By definition of V , we have |f .offset|1 < f .time. Hence, s.time > 0, which implies (s, a) /\u2208 \u2126, as desired. Remark 5.2 (Richness in strategy). The policy defined in the proof of Proposition 5.1 is in general not the maximal strategy, in the sense that the policy could be extended with more actions than currently specified. For instance, if the time limit is high then the agent can randomly wander around before it becomes sufficiently urgent to reach a target location. The agent may also use the diagonal actions, like left-up, if the time limit is not violated under either of the three outcomes.\nRemark 5.3 (Extendability). It is possible to extend the above setting of grid navigation to richer state representations, by including for example the locations of additional objects (that do not influence the agent). If this new information would be communicated to the agent with a set of features that is disjoint from the set of old features in Section 5.1, then Proposition 3.8 tells us that the strategy described in the proof of Proposition 5.1 is still valid."}, {"heading": "6 Conclusion and Further Work", "text": "We have used the notion of strategies to reason about the successful avoidance of aversive signals in tasks. We have shown that our avoidance learning algorithm always preserves those strategies. Now we discuss some interesting topics for further work.\nFeature detectors In this paper we have considered a framework in which features are essentially black boxes, in the sense that we do not assume anything about the way that they are computed. Hence, we do not know how features are related to the task environment. It would be interesting to develop more detailed insights into how features can be designed, to ensure that strategies, or similarly successful policies, are possible.\nIn particular, it seems fascinating to explore possible connections between our framework and neuron-like models, where features would be represented by neurons or by small groups of neurons. It is currently an open question whether or not feature learning in the brain is a completely unsupervised process (Fre\u0301maux and Gerstner, 2015), i.e., it is not known whether feature creation is influenced by rewarding or aversive signals. So, in a general theory, it might be valid to consider feature learning as a separate, unsupervised, module. This approach could lead to a conceptually simple framework of agent behavior and feature detection simultaneously. Concretely, the approach could enable the results in this paper to be linked to various feature detector algorithms.\nThe challenge of new features In this paper we have assumed that the set of features is fixed at the beginning of the learning process. This could be suitable for many applications, as there is no fixed limit on how many features there are, as long as there are finitely many. But it seems intriguing to introduce new features while the agent is performing the task. In the technical approach of this paper, however, a newly inserted feature likely proposes wrong actions if we would initially associate all actions to the feature. In general we still insist that aversive signals are avoided, and therefore the wrong actions need to be unlearned as soon as possible.\nA way to soften the introduction of new features, could be to reintroduce reward into the framework. Concretely, a feature f may only propose an action a if the pair (f, a) has been observed to be correlated to reward, either directly, or transitively by means of eligibility traces (Sutton and Barto, 1998). This idea introduces a threshold for proposing actions. Of course any feature-action pairs introduced in this way could still lead to aversive signals. For example, there could be spurious features (e.g. features that randomly appear) to which no actions should be linked, or perhaps the rewarding signals contradict the aversive signals, or some actions that give reward could also give aversive signals (as in the example of the Introduction). To resolve priority issues, one could view avoidance learning as having the highest precedence, where reward is used as a softer ranking mechanism on the allowed actions.\nPossibly, an agent that keeps learning new features will keep making mistakes. How to cope with new features therefore seems a relevant question. The answers could perhaps also help to understand animal behavior and consciousness. Thereto one could consider other notions of success than the avoidance of aversive signals investigated in this paper."}], "references": [{"title": "POMDPs under probabilistic semantics", "author": ["K. Chatterjee", "M. Chme\u013a\u0131k"], "venue": "Artificial Intelligence, 221:46 \u2013 72.", "citeRegEx": "Chatterjee and Chme\u013a\u0131k,? 2015", "shortCiteRegEx": "Chatterjee and Chme\u013a\u0131k", "year": 2015}, {"title": "Fairness", "author": ["N. Francez"], "venue": "Springer-Verlag New York, Inc.", "citeRegEx": "Francez,? 1986", "shortCiteRegEx": "Francez", "year": 1986}, {"title": "Neuromodulated spike-timing-dependent plasticity, and theory of three-factor learning rules", "author": ["N. Fr\u00e9maux", "W. Gerstner"], "venue": "Frontiers in Neural Circuits.", "citeRegEx": "Fr\u00e9maux and Gerstner,? 2015", "shortCiteRegEx": "Fr\u00e9maux and Gerstner", "year": 2015}, {"title": "Reinforcement learning using a continuous time actor-critic framework with spiking neurons", "author": ["N. Fr\u00e9maux", "H. Sprekeler", "W. Gerstner"], "venue": "PLoS Computational Biology, 9(4):e1003024.", "citeRegEx": "Fr\u00e9maux et al\\.,? 2013", "shortCiteRegEx": "Fr\u00e9maux et al\\.", "year": 2013}, {"title": "A comprehensive survey on safe reinforcement learning", "author": ["J. Gar\u0107\u0131a", "F. Fern\u00e1ndez"], "venue": "Journal of Machine Learning Research, 16:1437\u20131480.", "citeRegEx": "Gar\u0107\u0131a and Fern\u00e1ndez,? 2015", "shortCiteRegEx": "Gar\u0107\u0131a and Fern\u00e1ndez", "year": 2015}, {"title": "Spatial cognition in bats and rats: from sensory acquisition to multiscale maps and navigation", "author": ["M. Geva-Sagiv", "L. Las", "Y. Yovel", "N. Ulanovsky"], "venue": "Nature Reviews Neuroscience, 16(4):94\u2013108.", "citeRegEx": "Geva.Sagiv et al\\.,? 2015", "shortCiteRegEx": "Geva.Sagiv et al\\.", "year": 2015}, {"title": "Consideration of risk in reinforcement learning", "author": ["M. Heger"], "venue": "Proceedings of the Eleventh International Conference on Machine Learning, pages 105\u2013 111.", "citeRegEx": "Heger,? 1994", "shortCiteRegEx": "Heger", "year": 1994}, {"title": "On the convergence of stochastic iterative dynamic programming algorithms", "author": ["T. Jaakkola", "M. Jordan", "S. Singh"], "venue": "Neural Computation, 6(6).", "citeRegEx": "Jaakkola et al\\.,? 1994", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1994}, {"title": "Nonapproximability results for partially observable markov decision processes", "author": ["C. Lusena", "J. Goldsmith", "M. Mundhenk"], "venue": "Journal of Artificial Intelligence Research, 14:83\u2013103.", "citeRegEx": "Lusena et al\\.,? 2001", "shortCiteRegEx": "Lusena et al\\.", "year": 2001}, {"title": "Understanding Intelligence", "author": ["R. Pfeifer", "C. Scheier"], "venue": "The MIT Press.", "citeRegEx": "Pfeifer and Scheier,? 1999", "shortCiteRegEx": "Pfeifer and Scheier", "year": 1999}, {"title": "An imperfect dopaminergic error signal can drive temporal-difference learning", "author": ["W. Potjans", "M. Diesmann", "A. Morrison"], "venue": "PLoS Computational Biology, 7(5):e1001133.", "citeRegEx": "Potjans et al\\.,? 2011", "shortCiteRegEx": "Potjans et al\\.", "year": 2011}, {"title": "Finding approximate POMDP solutions through belief compression", "author": ["N. Roy", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research, 23:1 \u2013 40.", "citeRegEx": "Roy et al\\.,? 2005", "shortCiteRegEx": "Roy et al\\.", "year": 2005}, {"title": "Reinforcement Learning, An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "The MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "The logic of adaptive behavior", "author": ["M. van Otterlo"], "venue": null, "citeRegEx": "Otterlo,? \\Q2009\\E", "shortCiteRegEx": "Otterlo", "year": 2009}, {"title": "Learning from delayed rewards", "author": ["C. Watkins"], "venue": "PhD thesis, Cambridge University.", "citeRegEx": "Watkins,? 1989", "shortCiteRegEx": "Watkins", "year": 1989}, {"title": "Q-learning", "author": ["C. Watkins", "P. Dayan"], "venue": "Machine Learning, 8(3\u20134):279\u2013 292. 22", "citeRegEx": "Watkins and Dayan,? 1992", "shortCiteRegEx": "Watkins and Dayan", "year": 1992}], "referenceMentions": [{"referenceID": 8, "context": "(Lusena et al., 2001; Roy et al., 2005; Chatterjee and Chme\u013a\u0131k, 2015).", "startOffset": 0, "endOffset": 69}, {"referenceID": 11, "context": "(Lusena et al., 2001; Roy et al., 2005; Chatterjee and Chme\u013a\u0131k, 2015).", "startOffset": 0, "endOffset": 69}, {"referenceID": 0, "context": "(Lusena et al., 2001; Roy et al., 2005; Chatterjee and Chme\u013a\u0131k, 2015).", "startOffset": 0, "endOffset": 69}, {"referenceID": 9, "context": "Strategies The focus of this paper to understand agents based on their behavior in tasks, which could be a useful way to understand intelligence in general (Pfeifer and Scheier, 1999).", "startOffset": 156, "endOffset": 183}, {"referenceID": 7, "context": "(Jaakkola et al., 1994; Watkins and Dayan, 1992).", "startOffset": 0, "endOffset": 48}, {"referenceID": 15, "context": "(Jaakkola et al., 1994; Watkins and Dayan, 1992).", "startOffset": 0, "endOffset": 48}, {"referenceID": 12, "context": "In practice, a non-decreasing step-size, although potentially useful to model flexible agents that keep learning from their latest experiences (Sutton and Barto, 1998), can lead to problems of its own.", "startOffset": 143, "endOffset": 167}, {"referenceID": 14, "context": "3: We have simulated the Q-learning algorithm (Watkins, 1989; Watkins and Dayan, 1992) on the example task shown in Figure 1.", "startOffset": 46, "endOffset": 86}, {"referenceID": 15, "context": "3: We have simulated the Q-learning algorithm (Watkins, 1989; Watkins and Dayan, 1992) on the example task shown in Figure 1.", "startOffset": 46, "endOffset": 86}, {"referenceID": 6, "context": "Indeed, Heger (1994) has previously proposed a learning algorithm in tasks where actions have numeric costs, representing aversive signals.", "startOffset": 8, "endOffset": 21}, {"referenceID": 4, "context": "The idea of avoiding aversive signals, or problems in general, is related to safe reinforcement learning (Gar\u0107\u0131a and Fern\u00e1ndez, 2015).", "startOffset": 105, "endOffset": 133}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion.", "startOffset": 49, "endOffset": 77}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion. The work by Heger (1994) is closely related to our work.", "startOffset": 49, "endOffset": 157}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion. The work by Heger (1994) is closely related to our work. The framework by Heger (1994) provides feedback to the agent in the form of numerical cost signals, which, from the perspective of this paper, could be seen as aversive signals.", "startOffset": 49, "endOffset": 219}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion. The work by Heger (1994) is closely related to our work. The framework by Heger (1994) provides feedback to the agent in the form of numerical cost signals, which, from the perspective of this paper, could be seen as aversive signals. Similar to our n-swapping example in the Introduction (Figure 1.2a), Heger (1994) provides other examples to motivate that estimation of expected values is not suitable for reliably deciding actions.", "startOffset": 49, "endOffset": 449}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion. The work by Heger (1994) is closely related to our work. The framework by Heger (1994) provides feedback to the agent in the form of numerical cost signals, which, from the perspective of this paper, could be seen as aversive signals. Similar to our n-swapping example in the Introduction (Figure 1.2a), Heger (1994) provides other examples to motivate that estimation of expected values is not suitable for reliably deciding actions. The learning algorithm proposed by Heger (1994) maps each state-action pair to the worst outcome (or cost), by means of the max-operator.", "startOffset": 49, "endOffset": 615}, {"referenceID": 4, "context": "The approach is related to a trend identified by Gar\u0107\u0131a and Fern\u00e1ndez (2015), namely, the modification of the optimality criterion. The work by Heger (1994) is closely related to our work. The framework by Heger (1994) provides feedback to the agent in the form of numerical cost signals, which, from the perspective of this paper, could be seen as aversive signals. Similar to our n-swapping example in the Introduction (Figure 1.2a), Heger (1994) provides other examples to motivate that estimation of expected values is not suitable for reliably deciding actions. The learning algorithm proposed by Heger (1994) maps each state-action pair to the worst outcome (or cost), by means of the max-operator. By remembering the highest incurred cost for a state-action pair, the agent in some sense learns about \u201cwalls\u201d in the state space that constrain its actions towards lower costs. The avoidance learning algorithm discussed in this paper (Section 4) is similar in spirit to the one by Heger (1994). A deviation, however, is that we assume here a boolean interpretation of aversive signals, which leads to a neat and computationally efficient framework.", "startOffset": 49, "endOffset": 1000}, {"referenceID": 12, "context": "Our definition of task resembles that of a standard Markov decision process (Sutton and Barto, 1998), but we have added features and aversive signals.", "startOffset": 76, "endOffset": 100}, {"referenceID": 10, "context": "This viewpoint resembles the way that an individual neuron (or a small group of neurons) in the brain could represent a distinct concept and could be individually linked to actions (Potjans et al., 2011; Fr\u00e9maux et al., 2013).", "startOffset": 181, "endOffset": 225}, {"referenceID": 3, "context": "This viewpoint resembles the way that an individual neuron (or a small group of neurons) in the brain could represent a distinct concept and could be individually linked to actions (Potjans et al., 2011; Fr\u00e9maux et al., 2013).", "startOffset": 181, "endOffset": 225}, {"referenceID": 12, "context": "This is related to the Markov assumption (Sutton and Barto, 1998), because we do not have to remember any features that were seen during previous time steps, and we may instead choose actions based on just f by itself.", "startOffset": 41, "endOffset": 65}, {"referenceID": 12, "context": "This is an important deviation from the -greedy exploration principle (Sutton and Barto, 1998), where at each time step the agent chooses a random action with small probability \u2208 [0, 1].", "startOffset": 70, "endOffset": 94}, {"referenceID": 4, "context": "We do not use that mechanism here because otherwise the agent keeps running the risk of encountering aversive signals (Gar\u0107\u0131a and Fern\u00e1ndez, 2015).", "startOffset": 118, "endOffset": 146}, {"referenceID": 1, "context": "This brings us to the topic of fairness (Francez, 1986).", "startOffset": 40, "endOffset": 55}, {"referenceID": 2, "context": "It is currently an open question whether or not feature learning in the brain is a completely unsupervised process (Fr\u00e9maux and Gerstner, 2015), i.", "startOffset": 115, "endOffset": 143}, {"referenceID": 12, "context": "Concretely, a feature f may only propose an action a if the pair (f, a) has been observed to be correlated to reward, either directly, or transitively by means of eligibility traces (Sutton and Barto, 1998).", "startOffset": 182, "endOffset": 206}], "year": 2016, "abstractText": "We study a framework where agents have to avoid aversive signals. The agents are given only partial information, in the form of features that are projections of task states. Additionally, the agents have to cope with non-determinism, defined as unpredictability on the way that actions are executed. The goal of each agent is to define its behavior based on featureaction pairs that reliably avoid aversive signals. We study a learning algorithm, called A-learning, that exhibits fixpoint convergence, where the belief of the allowed feature-action pairs eventually becomes fixed. A-learning is parameter-free and easy to implement.", "creator": "LaTeX with hyperref package"}}}