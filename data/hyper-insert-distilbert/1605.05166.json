{"id": "1605.05166", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2016", "title": "Digital Stylometry: Linking Profiles Across Social Networks", "abstract": "\u00ab there simply is an ever growing online number usage of users who with accounts on multiple social twitter media and facebook networking gaming sites. hence consequently, there is increasing interest in matching user accounts and profiles placed across fundamentally different social networks in order to create aggregate peer profiles ahead of key users. in this paper, we present behavioral models aiming for digital dictionary stylometry, below which moreover is a simpler method for matching fellow users often through stylometry programming inspired techniques. we routinely experimented with linguistic, temporal, chemical and combined existing temporal - linguistic models for matching user network accounts, using standard linguistic and novel techniques. quickly using these publicly available empirical data, achieving our best model, a combined virtual temporal - linguistic one, was able simply to correctly all match the accounts of 31 % of 5, 612 thousand distinct chat users across three twitter blogs and the facebook.", "histories": [["v1", "Tue, 17 May 2016 13:47:24 GMT  (132kb,D)", "http://arxiv.org/abs/1605.05166v1", "SocInfo'15, Beijing, China. In proceedings of the 7th International Conference on Social Informatics (SocInfo 2015). Beijing, China"]], "COMMENTS": "SocInfo'15, Beijing, China. In proceedings of the 7th International Conference on Social Informatics (SocInfo 2015). Beijing, China", "reviews": [], "SUBJECTS": "cs.SI cs.AI cs.CL cs.IR", "authors": ["soroush vosoughi", "helen zhou", "deb roy"], "accepted": false, "id": "1605.05166"}, "pdf": {"name": "1605.05166.pdf", "metadata": {"source": "CRF", "title": "Digital Stylometry: Linking Profiles Across Social Networks", "authors": ["Soroush Vosoughi", "Helen Zhou"], "emails": ["soroush@mit.edu,", "hlzhou@mit.edu,", "dkroy@media.mit.edu"], "sections": [{"heading": null, "text": "Keywords: Stylometry, Profile Matching, Social Networks, Linguistic, Temporal."}, {"heading": "1 Introduction", "text": "Stylometry is defined as, \u201dthe statistical analysis of variations in literary style between one writer or genre and another\u201d. It is a centuries-old practice, dating back the early Renaissance. It is most often used to attribute authorship to disputed or anonymous documents. Stylometry techniques have also successfully been applied to other, non-linguistic fields, such as paintings and music. The main principles of stylometry were compiled and laid out by the philosopher Wincenty Lutosawski in 1890 in his work \u201dPrincipes de stylomtrie\u201d [10].\nToday, there are millions of users with accounts and profiles on many different social media and networking sites. It is not uncommon for users to have multiple accounts on different social media and networking sites. With so many networking, emailing, and photo sharing sites on the Web, a user often accumulates an abundance of account profiles. There is an increasing focus from the academic and business worlds on aggregating user information across different sites, allowing for the development of more complete user profiles. There currently exist several businesses that focus on this task [20,19,13]. These businesses use the aggregate profiles for advertising, background checks or customer service related tasks. Moreover, profile matching across social networks, can assist the growing field of social media rumor detection [15,21,23,24], since many malicious ar X\niv :1\n60 5.\n05 16\n6v 1\n[ cs\n.S I]\n1 7\nM ay\n2 01\n2 rumors are spread on different social media platforms by the same people, using different accounts and usernames.\nMotivated by traditional stylometry and the growing interest in matching user accounts across Internet services, we created models for Digital Stylometry, which fuses traditional stylometry techniques with big-data driven social informatics methods used commonly in analyzing social networks. Our models use linguistic and temporal activity patterns of users on different accounts to match accounts belonging to the same person. We evaluated our models on 11, 224 accounts belonging to 5, 612 distinct users on two of the largest social media networks, Twitter and Facebook. The only information that was used in our models were the time and the linguistic content of posts by the users. We intentionally did not use any other information, especially the potentially personally identifiable information that was explicitly provided by the user, such as the screen name, birthday or location. This is in accordance with traditional stylometry techniques, since people could misstate, omit, or lie about this information. Also, we wanted to show that there are implicit clues about the identities of users in the content (language) and context (time) of the users\u2019 interactions with social networks that can be used to link their accounts across different services.\nOther than the obvious technical goal, the purpose of this paper is to shed light on the relative ease with which seemingly innocuous information can be used to track users across social networks, even when signing up on different services using completely different account and profile information (such as name and birthday). This paper is as much of a technical contribution, as it is a warning to users who increasingly share a large part of their private lives on these services.\nThe rest of this paper is structured as follows. In the next sections we will review related work on linking profiles, followed by a description of our data collection and annotation efforts. After that, we discuss the linguistic, temporal and combined temporal-linguistic models developed for linking user profiles. Finally, we discuss and summarize our findings and contributions and discuss possible paths for future work."}, {"heading": "2 Related Work", "text": "There are several recent works that attempt to match profiles across different Internet services. Some of these works utilize private user data, while some, like ours, use publicly available data. An example of a work that uses private data is Balduzzi et al. [2]. They use data from the Friend Finder system (which includes some private data) provided by various social networks to link users across services. Though one can achieve a relatively high level of success by using private data to link user accounts, we are interested in using only publicly available data for this task. In fact, as mentioned earlier, we do not even consider publicly available information that could explicitly identify a user, such as names, birthdays and locations.\n3 Several methods have been proposed for matching user profiles using public data [11,26,22,16,14,27,9,7]. These works differ from ours in two main aspects. First, in some of these works, the ground truth data is collected by assuming that all profiles that have the same screen name are from the same users [9,7]. This is not a valid assumption. In fact, it has been suggested that close to 20% of accounts with the same screen name in Twitter and Facebook are not matching [6]. Second, almost all of these works use features extracted from the user profiles [11,26,22,16,14,27,9]. Our work, on the other hand, is blind to the profile information and only utilizes users\u2019 activity patterns (linguistic and temporal) to match their accounts across different social networks. Using profile information to match accounts is contrary to the best practices of stylometry since it assumes and relies on the honesty, consistency and willingness of the users to explicitly share identifiable information about themselves (such as location)."}, {"heading": "3 Data Collection and Datasets", "text": "For the purposes of this paper, we focused on matching accounts between two of the largest social networks: Twitter and Facebook. In order to proceed with our study, we needed a sizeable (few thousand) number of English speaking users with accounts on both Twitter and Facebook. We also needed to know the precise matching between the Twitter and Facebook accounts for our ground truth.\nTo that end, we crawled publicly available, English-language, Google Plus accounts using the Google Plus API 1 and scraped links to the users\u2019 other social media profiles. (Note that one of the reasons why we used Twitter and Facebook is that they were two of the most common sites linked to on Google Plus). We used a third party social media site (i.e., Google Plus), one that was not used in our analysis to compile our ground truth in order to limit selection bias in our data collection.\nWe discarded all users who did not link to an account for both Twitter and Facebook and those whose accounts on either of these sites were not public. We then used the APIs of Twitter2 and Facebook3 to collect posts made by the users on these sites. We only collected the linguistic content and the date and time at the which the posts were made. For technical and privacy reasons, we did not collect any information from the profile of the users, such as the location, screen name, or birthday.\nOur analysis focused on activity of users for one whole year, from February 1st, 2014 to February 1st, 2015. Since we can not reliably model the behaviour patterns of users with scarce data, users with less than 20 posts in that time period on either site were discarded. Overall, we collected a dataset of 5, 612 users with each having a Facebook and Twitter account for a total of 11, 224 accounts.\n1 https://developers.google.com/+/web/api/rest/ 2 https://dev.twitter.com/rest/public 3 https://developers.facebook.com/docs/public feed\n4 Figure 1 shows the distribution of the number of posts per user for Twitter and Facebook for our collected dataset. In the figure, the data for the number of posts has been divided into 500 bins. For the Twitter data, each bin corresponds to 80 tweets, while for the Facebook data, it corresponds to 10 posts. Table 1 shows some statistics about the data collected, including the average number of posts per user for each of the sites.\nTable 1: Statistics about the number of posts by the users of the 5, 612 accounts collected from Twitter and Facebook.\nTwitter Facebook\nMean 1,535 155 Median 352 54 Maximum 39,891 4,907 Minimum 20 20\n(a) Twitter data. (b) Facebook data.\nFig. 1: Distribution of number of posts per user for Twitter and Facebook, from our collected dataset."}, {"heading": "4 Models", "text": "We developed several linguistic, temporal and combined temporal-linguistic models for our task. These models take as input a user, u, from one of the sites (i.e., Twitter or Facebook) and a list of N users from the other service, where one of the N users, u\u2032, is the same as u. The models then provide a ranking among candidate matches between u and each of the N users. We used two criteria to evaluate our models:\n5 \u2013 Accuracy: percentage of cases when a model\u2019s top ranked candidate is u\u2032. \u2013 Average Rank: the average rank of u\u2032 within the ranked list of candidates\ngenerated by a model.\nA baseline random choice ranker would have an accuracy of 1/N , and an average rank of N/2 (since u\u2032 may appear anywhere in the list of N items)."}, {"heading": "4.1 Linguistic Models", "text": "A valuable source of information in matching user accounts, one used in traditional stylometry tasks, is the way in which people use language. A speaker or writer\u2019s choice of words depends on many factors, including the rules of grammar, message content and stylistic considerations. There is a great variety of possible ways to compare the language patterns of two people. However, first we need a method for modelling the language of a given user. Below we explain how this is done.\nLanguage Models Most statistical language models do not attempt to explicitly model the complete language generation process, but rather seek a compact model that adequately explains the observed linguistic data. Probabilistic models of language assign probabilities to word sequences w1 . . . w`, and as such the likelihood of a corpus can be used to fit model parameters as well as characterize model performance.\nN-gram language modelling [4,8,12] is an effective technique that treats words as samples drawn from a distribution conditioned on other words, usually the immediately preceding n \u2212 1 words, in order to capture strong local word dependencies. The probability of a sequence of ` words, written compactly as w`1 is Pr(w`1) and can be factored exactly as\nPr(w`1) = Pr(w1) \u220f\u0300 i=2 Pr(wi|wi\u221211 )\nHowever, parameter estimation in this full model is intractable, as the number of possible word combinations grows exponentially with sequence length. N-gram models address this with the approximation P\u0303r(wi|wi\u22121i\u2212n+1) \u2248 Pr(wi|w i\u22121 1 ) using only the preceding n \u2212 1 words for context. A bigram model (n = 2) uses the preceding word for context, while a unigram model (n = 1) does not use any context.\nFor this work, we used unigram models in Python, utilizing some components from NLTK [3]. Probability distributions were calculated using Witten-Bell smoothing [8]. Rather than assigning word wi the maximum likelihood probability estimate pi = ci N , where ci is the number of observations of word wi and N is the total number of observed tokens, Witten-Bell smoothing discounts the probability of observed words to p\u2217i = ci N+T where T is the total number of observed word types. The remaining Z words in the vocabulary that are unobserved (i.e. where ci = 0) are given by p \u2217 i = T Z(N+T ) .\n6 We experimented with two methods for measuring the similarity between ngram language models. In particular, we tried approaches based on KL-divergence and perplexity [5]. We also tried two methods that do not rely on n-gram models, cosine similarity of TF-IDF vectors [17], as well as our own novel method, called the confusion model.\nThe performance of each method is shown in Table 2. Note that all methods outperform the random baseline in both accuracy and average rank by a great margin. Below we explain each of these metrics.\nKL-divergence The first metric used for measuring the distance between the language of two user accounts is the Kullback-Leibler (KL) divergence [5] between the unigram probability distribution of the corpus corresponding to the two accounts. The KL-divergence provides an asymmetric measure of dissimilarity between two probability distribution functions p and q and is given by:\nKL(p||q) = \u222b p(x)ln p(x)\nq(x)\nWe can modify the equation to prove a symmetric distance between distributions:\nKL2(p||q) = KL(p||q) + KL(q||p)\nPerplexity For this method, the similarity metric is the perplexity [5] of the unigram language model generated from one account, p and evaluated on another account, q. Perplexity is given as:\nPP (p, q) = 2H(p,q)\nwhere H(p, q) is the cross-entropy [5] between distributions of the two accounts p and q. More similar models lead to smaller perplexity. As with KLdivergence, we can make perplexity symmetric:\n7 PP2(p, q) = PP (p, q) + PP (q, p)\nThis method outperformed the KL-divergence method in terms of average rank but not accuracy (see Table 2).\nTF-IDF Perhaps the relatively low accuracies of perplexity and KL-divergence measures should not be too surprising. These measures are most sensitive to the variations in frequencies of most common words. For instance, in its most straightforward implementation, the KL-divergence measure would be highly sensitive to the frequency of the word \u201cthe\u201d. Although this problem might be mitigated by the removal of stop words and applying topic modelling to the texts, we believe that this issue is more nuanced than that.\nDifferent social media (such as Twitter and Facebook) are used by people for different purposes, and thus Twitter and Facebook entries by the same person are likely to be thematically different. So it is likely that straightforward comparison of language models would be inefficient for this task.\nOne possible solution for this problem is to look at users\u2019 language models not in isolation, but in comparison to the languages models of everyone else. In other words, identify features of a particular language model that are characteristic to its corresponding user, and then use these features to estimate similarity between different accounts. This is a task that Term Frequency-Inverse Document Frequency, or TF-IDF, combined with cosine similarity, can manage.\nTF-IDF is a method of converting text into numbers so that it can be represented meaningfully by a vector [17]. TF-IDF is the product of two statistics, TF or Term Frequency and IDF or Inverse Document Frequency. Term Frequency measures the number of times a term (word) occurs in a document. Since each document will be of different size, we need to normalize the document based on its size. We do this by dividing the Term Frequency by the total number of terms.\nTF considers all terms as equally important, however, certain terms that occur too frequently should have little effect (for example, the term \u201cthe\u201d). And conversely, terms that occur less in a document can be more relevant. Therefore, in order to weigh down the effects of the terms that occur too frequently and weigh up the effects of less frequently occurring terms, an Inverse Document Frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely. Generally speaking, the Inverse Document Frequency is a measure of how much information a word provides, that is, whether the term is common or rare across all documents.\nUsing TF-IDF, we derive a vector from the corpus of each account. We measure the similarity between two accounts using cosine similarity:\nSimilarity(d1, d2) = d1 \u00b7 d2\n||d1|| \u00d7 ||d2||\n8 Here, d1 \u00b7 d2 is the dot product of two documents, and ||d1|| \u00d7 ||d2|| is the product of the magnitude of the two documents. Using TD-IDF and cosine similarity, we achieved significantly better results than the last two methods, with an accuracy of 0.21 and average rank of 999.\nConfusion Model TF-IDF can be thought of as a heuristic measure of the extent to which different words are characteristic of a user. We came up with a new, theoretically motivated measure of \u201cbeing characteristic\u201d for words. We considered the following setup :\n1. The whole corpus of the 11, 224 Twitter and Facebook accounts was treated as one long string; 2. For each token in the string, we know the user who produced it. Imagine that we removed this information and are now making a guess as to who the user was. This will give us a probability distribution over all users; 3. Now imagine that we are making a number of the following samples: randomly selecting a word from the string, taking the true user, TU for this word and a guessed user, GU from correspondent probability distribution. Intuitively, the more often a particular pair, TU = U1, GU = U2 appear together, the stronger is the similarity between U1 and U2; 4. We then use mutual information to measure the strength of association. In this case, it will be the mutual information [5] between random variables, TU = U1 and GU = U2. This mutual information turns out to be proportional to the probabilities of U1 and U2 in the dataset, which is undesirable for a similarity measure. To correct for this, we divide it by the probabilities of U1 and U2;\nWe call this model the confusion model, as it evaluated the probability that U1 will be confused for U2 on the basis of a single word. The expression for the similarity value according to the model is S \u00d7 log(S), where S is:\nS = \u2211 w p(w)p(U1|w)p(U2|w)\nNote that if U1 = U2, the words contributing most to the sum will be ordered by their \u201cdegree of being characteristic\u201d. The values, p(w) and p(u|w) have to be estimated from the corpus. To do that, we assumed that the corpus was produced using the following auxiliary model:\n1. For each token, a user is selected from a set of users by multinomial distribution; 2. A word is selected from a multinomial distribution of words for this user to produce the token.\nWe used Dirichlet distributions [1] as priors over multinomials. This method outperforms all other methods with an accuracy of 0.27 and average rank of 859.\n9"}, {"heading": "4.2 Temporal Models", "text": "Another valuable source of information in matching user accounts, is the activity patterns of users. A measure of activity is the time and the intensity at which users utilize a social network or media site. All public social networks, including publicly available Twitter and Facebook data, make this information available. Previous research has shown temporal information (and other contextual information, such as spatial information) to be correlated with the linguistic activities of people [18,25].\nWe extracted the following discrete temporal features from our corpus: month (12 bins), day of month (31 bins), day of week (7 bins) and hour (24 bins). We chose these features to capture fine to coarse-level temporal patterns of user activity. For example, commuting to work is a recurring pattern linked to a time of day, while paying bills is more closely tied to the day of the month, and vacations are more closely tied to the month.\nWe treated each of these bins as a word, so that we could use the same methods used in the last section to measure the similarity between the temporal activity patterns of pairs of accounts (this will also help greatly for creating the combined model, explained in the next section). In other word, the 12 bins in month were set to w1 . . . w12, the 31 bins in day of month to w13 . . . w43, the 7 bins in day of week to w44 . . . w50, and the 24 bins in time were set to w51 . . . w74. Thus, we had a corpus of 74 words.\nFor example, a post on Friday, August 5th at 2 AM would be translated to {w8, w17, w48, w53}, corresponding to August, 5th, Friday, 2 AM respectively. Since we are only using unigram models, the order of words does not matter. As with the language models described in the last section, all of the probability distributions were calculated using Witten-Bell smoothing. We used the same four methods as in the last section to create our temporal models.\nTable 3 shows the performance of each of these models. Although the performance of the temporal models were not as strong as the linguistic ones, they all vastly outperformed the baseline. Also, note that here as with the linguistic models, the confusion model greatly outperformed the other models.\n10"}, {"heading": "4.3 Combined Models", "text": "Finally, we created a combined temporal-linguistic model. Since both the linguistic and the temporal models were built using the same framework, it was fairly simple to combine the two models. The combined model was created by merging the linguistic and temporal corpora and vocabularies. (Recall that we treated temporal features as words). We then experimented with the same four methods as in the last two sections to create our combined models.\nTable 4 shows the performance of each of these models. Across the board, the combined models outperformed their corresponding linguistic and temporal models, though the difference with the linguistic models were not as great. These results suggest that at some level the temporal and the linguistic \u201dstyles\u201d of users provide non-overlapping cues about the identity of said users. Also, note that as with the linguistic and temporal models, our combined confusion model outperformed the other combined models.\nAnother way to evaluate the performance of the different combined models is through the rank-statistics plot. This is shown in Figure 2. The figure shows the distribution of the ranks of the 5, 612 users for different combined models. The x-axis is the rank percentile (divided into bins of 5%), the y-axis is the percentage of the users that fall in each bin. For example, for the confusion model, 69% (3880) of the 5, 612 users were correctly linked between Twitter and Facebook when looking at the top 5% (281) of the predictions by the model. From the figure, you can clearly see that the confusion model is superior to the other models, with TF-IDF a close second. You can also see from the figure that the rank plot for the random baseline is a horizontal line, with each rank percentile bin having 5% of the users (5% because the rank percentiles were divided into bins of 5%)."}, {"heading": "5 Evaluation Against Humans", "text": "Matching profiles across social networks is a hard task for humans. It is a task on par with detecting plagiarism, something a non-trained person (or sometimes\n11\neven a trained person) cannot easily accomplish. (Hence the need for the development of the field of stylometry in early Renaissance.) Be that as it may, we wanted to evaluate our model against humans to make sure that it is indeed outperforming them.\nWe designed an experiment to compare the performance of human judges to our best model, the temporal-linguistic confusion model. The task had to be simple enough so that human judges could attempt it with ease. For example, it would have been ludicrous to ask the judges to sort 11, 224 accounts into 5, 612 matching pairs.\nThus, we randomly selected 100 accounts from distinct users from our collection of 11, 224 accounts. A unique list of 10 candidate accounts was created for each of the 100 accounts. Each list contained the correct matching account mixed in with 9 other randomly selected accounts. The judges were then presented with the 100 accounts one at a time and asked to pick the correct matching account from the list of 10 candidate accounts. For simplicity, we did not ask the judges to do any ranking other than picking the one account that they thought matched the original account. We then measured the accuracy of the judges based on how many of the 100 accounts they correctly matched. We had our model do the exact same task with the same dataset. A random baseline model would have a one in ten chance of getting the correct answer, giving it an accuracy of 0.10.\n12\nWe had a total of 3 English speaking human judges from Amazon Mechanical Turk (which is an tool for crowd-sourcing of human annotation tasks) 4. For each task, the judges were shown the link to one of the 100 account, and its 10 corresponding candidate account links. The judges were allowed to explore each of the accounts as much as they wanted to make their decision (since all these accounts were public, there were no privacy concerns).\nTable 5 shows the performance of each of the three human judges, our model and the random baseline. Since the task is so much simpler than pairing 11, 224 accounts, our combined confusion model had a much greater accuracy than reported in the last section. With an accuracy of 0.86, our model vastly outperformed even the best human judge, at 0.69. Overall, our model beat the average human performance by 0.26 (0.86 to 0.60 respectively) which is a 43% relative (and 26% absolute) improvement."}, {"heading": "6 Discussion and Conclusions", "text": "Motivated by the growing interest in matching user account across different social media and networking sites, in this paper we presented models for Digital Stylometry, which is a method for matching users through stylometry inspired techniques. We used temporal and linguistic patterns of users to do the matching.\nWe experimented with linguistic, temporal, and combined temporal-linguistic models using standard and novel techniques. The methods based on our novel confusion model outperformed the more standard ones in all cases. We showed that both temporal and linguistic information are useful for matching users, with the best temporal model performing with an accuracy of .10 and the best linguistic model performing with an accuracy of 0.27. Even though the linguistic models vastly outperformed the temporal models, when combined the temporallinguistic models outperformed both with an accuracy of 0.31. The improvement in the performance of the combined models suggests that although temporal information is dwarfed by linguistic information, in terms of its contribution to\n4 https://www.mturk.com/\n13\ndigital stylometry, it nonetheless provides non-overlapping information with the linguistic data.\nOur models were evaluated on 5, 612 users with a total of 11, 224 accounts on Twitter and Facebook combined. In contrast to other works in this area, we did not use any profile information in our matching models. The only information that was used in our models were the time and the linguistic content of posts by the users. This is in accordance with traditional stylometry techniques (since people could lie or misstate this information). Also, we wanted to show that there are implicit clues about the identity of users in the content (language) and context (time) of the users\u2019 interactions with social networks that can be used to link their accounts across different services.\nIn addition to the technical contributions (such as our confusion model), we hope that this paper is able to shed light on the relative ease with which seemingly innocuous information can be used to track users across social networks, even when signing up on different services using completely different account and profile information. In the future, we hope to extend this work to other social network sites, and to incorporate more sophisticated techniques, such as topic modelling and opinion mining, into our models.\nAcknowledgments We would like to thank Ivan Sysoev for his help with developing the confusion model. We would also like to thank William Powers for sharing his insights on user privacy in the age of social networks. Finally, thanks to all the human annotators for their help with the evaluation of our models."}], "references": [{"title": "A primer on statistical distributions", "author": ["N. Balakrishnan", "V.B. Nevzorov"], "venue": "John Wiley & Sons", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Abusing social networks for automated user profiling", "author": ["M. Balduzzi", "C. Platzer", "T. Holz", "E. Kirda", "D. Balzarotti", "C. Kruegel"], "venue": "Recent Advances in Intrusion Detection. pp. 422\u2013441. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Natural language processing with Python", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": "O\u2019Reilly Media, Inc.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Statistical language learning", "author": ["E. Charniak"], "venue": "MIT press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & Sons", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "On the reliability of profile matching across large online social networks", "author": ["O. Goga", "P. Loiseau", "R. Sommer", "R. Teixeira", "K.P. Gummadi"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 1799\u20131808. ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Identifying users across social tagging systems", "author": ["T. Iofciu", "P. Fankhauser", "F. Abel", "K. Bischoff"], "venue": "ICWSM", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition", "author": ["D. Jurafsky", "J.H. Martin"], "venue": "Prentice Hall, 2nd edn.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "What your friends tell others about you: Low cost linkability of social network profiles", "author": ["S. Labitzke", "I. Taranu", "H. Hartenstein"], "venue": "Proc. 5th International ACM Workshop on Social Network Mining and Analysis, San Diego, CA, USA", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Studying user footprints in different online social networks", "author": ["A. Malhotra", "L. Totti", "W. Meira Jr", "P. Kumaraguru", "V. Almeida"], "venue": "Proceedings of the 2012 International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2012). pp. 1065\u20131070. IEEE Computer Society", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Foundations of statistical natural language processing", "author": ["C.D. Manning", "H. Sch\u00fctze"], "venue": "MIT press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Entity matching in online social networks", "author": ["O. Peled", "M. Fire", "L. Rokach", "Y. Elovici"], "venue": "Social Computing (SocialCom), 2013 International Conference on. pp. 339\u2013344. IEEE", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Rumor has it: Identifying misinformation in microblogs", "author": ["V. Qazvinian", "E. Rosengren", "D.R. Radev", "Q. Mei"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp. 1589\u20131599. Association for Computational Linguistics", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "User profile matching in social networks", "author": ["E. Raad", "R. Chbeir", "A. Dipanda"], "venue": "Network-Based Information Systems (NBiS), 2010 13th International Conference on. pp. 297\u2013304. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Mining of massive datasets", "author": ["A. Rajaraman", "J.D. Ullman"], "venue": "Cambridge University Press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Grounding language models in spatiotemporal context", "author": ["B.C. Roy", "S. Vosoughi", "D. Roy"], "venue": "Fifteenth Annual Conference of the International Speech Communication Association", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Rumor detection on twitter", "author": ["T. Takahashi", "N. Igata"], "venue": "Soft Computing and Intelligent Systems (SCIS) and 13th International Symposium on Advanced Intelligent Systems (ISIS), 2012 Joint 6th International Conference on. pp. 452\u2013457. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "User identification across multiple social networks", "author": ["J. Vosecky", "D. Hong", "V.Y. Shen"], "venue": "Networked Digital Technologies, 2009. NDT\u201909. First International Conference on. pp. 360\u2013365. IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatic detection and verification of rumors on Twitter", "author": ["S. Vosoughi"], "venue": "Ph.D. thesis, Massachusetts Institute of Technology", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A human-machine collaborative system for identifying rumors on twitter", "author": ["S. Vosoughi", "D. Roy"], "venue": "ICDM workshop on Event Analytics using Social Media Data", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Enhanced twitter sentiment classification using contextual information. In: Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. pp. 16\u201324", "author": ["S. Vosoughi", "H. Zhou", "D. Roy"], "venue": "Association for Computational Linguistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Socialsearch: enhancing entity search with social network matching", "author": ["You", "G.w.", "Hwang", "S.w.", "Z. Nie", "J.R. Wen"], "venue": "Proceedings of the 14th International Conference on Extending Database Technology. pp. 515\u2013519. ACM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Connecting corresponding identities across communities", "author": ["R. Zafarani", "H. Liu"], "venue": "ICWSM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 12, "context": "Moreover, profile matching across social networks, can assist the growing field of social media rumor detection [15,21,23,24], since many malicious ar X iv :1 60 5.", "startOffset": 112, "endOffset": 125}, {"referenceID": 16, "context": "Moreover, profile matching across social networks, can assist the growing field of social media rumor detection [15,21,23,24], since many malicious ar X iv :1 60 5.", "startOffset": 112, "endOffset": 125}, {"referenceID": 18, "context": "Moreover, profile matching across social networks, can assist the growing field of social media rumor detection [15,21,23,24], since many malicious ar X iv :1 60 5.", "startOffset": 112, "endOffset": 125}, {"referenceID": 19, "context": "Moreover, profile matching across social networks, can assist the growing field of social media rumor detection [15,21,23,24], since many malicious ar X iv :1 60 5.", "startOffset": 112, "endOffset": 125}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Several methods have been proposed for matching user profiles using public data [11,26,22,16,14,27,9,7].", "startOffset": 80, "endOffset": 103}, {"referenceID": 21, "context": "Several methods have been proposed for matching user profiles using public data [11,26,22,16,14,27,9,7].", "startOffset": 80, "endOffset": 103}, {"referenceID": 17, "context": "Several methods have been proposed for matching user profiles using public data [11,26,22,16,14,27,9,7].", "startOffset": 80, "endOffset": 103}, {"referenceID": 13, "context": "Several methods have been proposed for matching user profiles using public data [11,26,22,16,14,27,9,7].", "startOffset": 80, "endOffset": 103}, {"referenceID": 11, "context": "Several methods have been proposed for matching user profiles using public data [11,26,22,16,14,27,9,7].", "startOffset": 80, "endOffset": 103}, {"referenceID": 22, "context": "Several methods have been proposed for matching user profiles using public data [11,26,22,16,14,27,9,7].", "startOffset": 80, "endOffset": 103}, {"referenceID": 8, "context": "Several methods have been proposed for matching user profiles using public data [11,26,22,16,14,27,9,7].", "startOffset": 80, "endOffset": 103}, {"referenceID": 6, "context": "Several methods have been proposed for matching user profiles using public data [11,26,22,16,14,27,9,7].", "startOffset": 80, "endOffset": 103}, {"referenceID": 8, "context": "First, in some of these works, the ground truth data is collected by assuming that all profiles that have the same screen name are from the same users [9,7].", "startOffset": 151, "endOffset": 156}, {"referenceID": 6, "context": "First, in some of these works, the ground truth data is collected by assuming that all profiles that have the same screen name are from the same users [9,7].", "startOffset": 151, "endOffset": 156}, {"referenceID": 5, "context": "In fact, it has been suggested that close to 20% of accounts with the same screen name in Twitter and Facebook are not matching [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 9, "context": "Second, almost all of these works use features extracted from the user profiles [11,26,22,16,14,27,9].", "startOffset": 80, "endOffset": 101}, {"referenceID": 21, "context": "Second, almost all of these works use features extracted from the user profiles [11,26,22,16,14,27,9].", "startOffset": 80, "endOffset": 101}, {"referenceID": 17, "context": "Second, almost all of these works use features extracted from the user profiles [11,26,22,16,14,27,9].", "startOffset": 80, "endOffset": 101}, {"referenceID": 13, "context": "Second, almost all of these works use features extracted from the user profiles [11,26,22,16,14,27,9].", "startOffset": 80, "endOffset": 101}, {"referenceID": 11, "context": "Second, almost all of these works use features extracted from the user profiles [11,26,22,16,14,27,9].", "startOffset": 80, "endOffset": 101}, {"referenceID": 22, "context": "Second, almost all of these works use features extracted from the user profiles [11,26,22,16,14,27,9].", "startOffset": 80, "endOffset": 101}, {"referenceID": 8, "context": "Second, almost all of these works use features extracted from the user profiles [11,26,22,16,14,27,9].", "startOffset": 80, "endOffset": 101}, {"referenceID": 3, "context": "N-gram language modelling [4,8,12] is an effective technique that treats words as samples drawn from a distribution conditioned on other words, usually the immediately preceding n \u2212 1 words, in order to capture strong local word dependencies.", "startOffset": 26, "endOffset": 34}, {"referenceID": 7, "context": "N-gram language modelling [4,8,12] is an effective technique that treats words as samples drawn from a distribution conditioned on other words, usually the immediately preceding n \u2212 1 words, in order to capture strong local word dependencies.", "startOffset": 26, "endOffset": 34}, {"referenceID": 10, "context": "N-gram language modelling [4,8,12] is an effective technique that treats words as samples drawn from a distribution conditioned on other words, usually the immediately preceding n \u2212 1 words, in order to capture strong local word dependencies.", "startOffset": 26, "endOffset": 34}, {"referenceID": 2, "context": "For this work, we used unigram models in Python, utilizing some components from NLTK [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "Probability distributions were calculated using Witten-Bell smoothing [8].", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "In particular, we tried approaches based on KL-divergence and perplexity [5].", "startOffset": 73, "endOffset": 76}, {"referenceID": 14, "context": "We also tried two methods that do not rely on n-gram models, cosine similarity of TF-IDF vectors [17], as well as our own novel method, called the confusion model.", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "KL-divergence The first metric used for measuring the distance between the language of two user accounts is the Kullback-Leibler (KL) divergence [5] between the unigram probability distribution of the corpus corresponding to the two accounts.", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "Perplexity For this method, the similarity metric is the perplexity [5] of the unigram language model generated from one account, p and evaluated on another account, q.", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "where H(p, q) is the cross-entropy [5] between distributions of the two accounts p and q.", "startOffset": 35, "endOffset": 38}, {"referenceID": 14, "context": "TF-IDF is a method of converting text into numbers so that it can be represented meaningfully by a vector [17].", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "In this case, it will be the mutual information [5] between random variables, TU = U1 and GU = U2.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "We used Dirichlet distributions [1] as priors over multinomials.", "startOffset": 32, "endOffset": 35}, {"referenceID": 15, "context": "Previous research has shown temporal information (and other contextual information, such as spatial information) to be correlated with the linguistic activities of people [18,25].", "startOffset": 171, "endOffset": 178}, {"referenceID": 20, "context": "Previous research has shown temporal information (and other contextual information, such as spatial information) to be correlated with the linguistic activities of people [18,25].", "startOffset": 171, "endOffset": 178}], "year": 2016, "abstractText": "There is an ever growing number of users with accounts on multiple social media and networking sites. Consequently, there is increasing interest in matching user accounts and profiles across different social networks in order to create aggregate profiles of users. In this paper, we present models for Digital Stylometry, which is a method for matching users through stylometry inspired techniques. We experimented with linguistic, temporal, and combined temporal-linguistic models for matching user accounts, using standard and novel techniques. Using publicly available data, our best model, a combined temporal-linguistic one, was able to correctly match the accounts of 31% of 5, 612 distinct users across Twitter and Facebook.", "creator": "LaTeX with hyperref package"}}}