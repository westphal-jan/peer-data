{"id": "1605.06083", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2016", "title": "Stereotyping and Bias in the Flickr30K Dataset", "abstract": "this an untested assumption behind pioneering the crowdsourced link descriptions algorithm of the binary images given in discussing the flickr30k dataset ( ronald young et al., 2014 ) is that instead they \" probably focus virtually only on the information extracted that can be clearly obtained continuously from making the sd image alone \" ( min hodosh et al., roberts 2013, 127 p. 859 ). this scientific paper presents some evidence finding against this simple assumption, arguments and provides a hierarchical list of biases and various unwarranted conflicting inferences that can be found out in the revised flickr30k sd dataset. finally, independently it likewise considers methods to ultimately find examples upstream of these, and discusses altogether how heavily we should ideally deal with the stereotype - driven descriptions in presumably future applications.", "histories": [["v1", "Thu, 19 May 2016 19:17:23 GMT  (128kb,D)", "http://arxiv.org/abs/1605.06083v1", "In: Proceedings of the Workshop on Multimodal Corpora (MMC-2016), pages 1-4. Editors: Jens Edlund, Dirk Heylen and Patrizia Paggio"]], "COMMENTS": "In: Proceedings of the Workshop on Multimodal Corpora (MMC-2016), pages 1-4. Editors: Jens Edlund, Dirk Heylen and Patrizia Paggio", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["emiel van miltenburg"], "accepted": false, "id": "1605.06083"}, "pdf": {"name": "1605.06083.pdf", "metadata": {"source": "CRF", "title": "Stereotyping and Bias in the Flickr30K Dataset", "authors": ["Emiel van Miltenburg"], "emails": ["emiel.van.miltenburg@vu.nl"], "sections": [{"heading": null, "text": "Keywords: image annotation, stereotypes, bias, Flickr30K"}, {"heading": "1. Introduction", "text": "The Flickr30K dataset (Young et al., 2014) is a collection of over 30,000 images with 5 crowdsourced descriptions each. It is commonly used to train and evaluate neural network models that generate image descriptions (e.g. (Vinyals et al., 2015)). An untested assumption behind the dataset is that the descriptions are based on the images, and nothing else. Here are the authors (about the Flickr8K dataset, a subset of Flickr30K):\n\u201cBy asking people to describe the people, objects, scenes and activities that are shown in a picture without giving them any further information about the context in which the picture was taken, we were able to obtain conceptual descriptions that focus only on the information that can be obtained from the image alone.\u201d (Hodosh et al., 2013, p. 859)\nWhat this assumption overlooks is the amount of interpretation or recontextualization carried out by the annotators. Let us take a concrete example. Figure 1 shows an image from the Flickr30K dataset.\nFigure 1: Image 8063007 from the Flickr30K dataset.\nThis image comes with the five descriptions below. All but the first one contain information that cannot come from the image alone. Relevant parts are highlighted in bold:\n1. A blond girl and a bald man with his arms crossed are standing inside looking at each other. 2. A worker is being scolded by her boss in a stern lecture.\n3. A manager talks to an employee about job performance. 4. A hot, blond girl getting criticized by her boss. 5. Sonic employees talking about work.\nWe need to understand that the descriptions in the Flickr30K dataset are subjective descriptions of events. This can be a good thing: the descriptions tell us what are the salient parts of each image to the average human annotator. So the two humans in Figure 1 are relevant, but the two soap dispensers are not. But subjectivity can also result in stereotypical descriptions, in this case suggesting that the male is more likely to be the manager, and the female is more likely to be the subordinate. Rashtchian et al. (2010) do note that some descriptions are speculative in nature, which they say hurts the accuracy and the consistency of the descriptions. But the problem is not with the lack of consistency here. Quite the contrary: the problem is that stereotypes may be pervasive enough for the data to be consistently biased. And so language models trained on this data may propagate harmful stereotypes, such as the idea that women are less suited for leadership positions. This paper aims to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices. I will build on earlier work on linguistic bias in general (Beukeboom, 2014), providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences. Finally, I will discuss several methods to analyze the data in order to detect biases.1"}, {"heading": "2. Stereotype-driven descriptions", "text": "Stereotypes are ideas about how other (groups of) people commonly behave and what they are likely to do. These ideas guide the way we talk about the world. I distinguish two kinds of verbal behavior that result from stereotypes: (i) linguistic bias, and (ii) unwarranted inferences. The former is discussed in more detail by Beukeboom (2014), who defines linguistic bias as \u201ca systematic asymmetry in word choice as a function of the social category to which the target belongs.\u201d So this bias becomes visible through the distribution of terms used to describe entities in a particular\n1The Flickr30K data also contains examples where annotators judge the subjects of the images on their looks. E.g. description #4 above calling the girl in the image hot. Analyzing this judgmental language goes beyond the scope of this paper.\nar X\niv :1\n60 5.\n06 08\n3v 1\n[ cs\n.C L\n] 1\n9 M\nay 2\n01 6\ncategory. Unwarranted inferences are the result of speculation about the image; here, the annotator goes beyond what can be glanced from the image and makes use of their knowledge and expectations about the world to provide an overly specific description. Such descriptions are directly identifiable as such, and in fact we have already seen four of them (descriptions 2\u20135) discussed earlier."}, {"heading": "2.1. Linguistic bias", "text": "Generally speaking, people tend to use more concrete or specific language when they have to describe a person that does not meet their expectations. Beukeboom (2014) lists several linguistic \u2018tools\u2019 that people use to mark individuals who deviate from the norm. I will mention two of them.2\nAdjectives One well-studied example (Stahlberg et al., 2007; Romaine, 2001) is sexist language, where the sex of a person tends to be mentioned more frequently if their role or occupation is inconsistent with \u2018traditional\u2019 gender roles (e.g. female surgeon, male nurse). Beukeboom also notes that adjectives are used to create \u201cmore narrow labels [or subtypes] for individuals who do not fit with general social category expectations\u201d (p. 3). E.g. tough woman makes an exception to the \u2018rule\u2019 that women aren\u2019t considered to be tough. Negation can be used when prior beliefs about a particular social category are violated, e.g. The garbage man was not stupid. See also (Beukeboom et al., 2010).\nThese examples are similar in that the speaker has to put in additional effort to mark the subject for being unusual. But they differ in what we can conclude about the speaker, especially in the context of the Flickr30K data. Negations are much more overtly displaying the annotator\u2019s prior beliefs. When one annotator writes that A little boy is eating pie without utensils (image 2659046789), this immediately reveals the annotator\u2019s normative beliefs about the world: pie should be eaten with utensils. But when another annotator talks about a girls basketball game (image 8245366095), this cannot be taken as an indication that the annotator is biased about the gender of basketball players; they might just be helpful by providing a detailed description. In section 3 I will discuss how to establish whether or not there is any bias in the data regarding the use of adjectives."}, {"heading": "2.2. Unwarranted inferences", "text": "Unwarranted inferences are statements about the subject(s) of an image that go beyond what the visual data alone can tell us. They are based on additional assumptions about the world. After inspecting a subset of the Flickr30K data, I have grouped these inferences into six categories (image examples between parentheses):\nActivity We\u2019ve seen an example of this in the introduction, where the \u2018manager\u2019 was said to be talking about job performance and scolding [a worker] in a stern lecture (8063007). Ethnicity Many dark-skinned individuals are called African-American regardless of whether the picture has been taken in the USA or not (4280272). And people who\n2Examples given are also due to (Beukeboom, 2014).\nlook Asian are called Chinese (1434151732) or Japanese (4834664666). Event In image 4183120 (Figure 2), people sitting at a gym are said to be watching a game, even though there could be any sort of event going on. But since the location is so strongly associated with sports, crowdworkers readily make the assumption. Goal Quite a few annotations focus on explaining the why of the situation. For example, in image 3963038375 a man is fastening his climbing harness in order to have some fun. And in an extreme case, one annotator writes about a picture of a dancing woman that the school is having a special event in order to show the american culture on how other cultures are dealt with in parties (3636329461). This is reminiscent of the Stereotypic Explanatory Bias (Sekaquaptewa et al., 2003, SEB), which refers to \u201cthe tendency to provide relatively more explanations in descriptions of stereotype inconsistent, compared to consistent behavior\u201d (Beukeboom et al., 2010, p. 5). So in theory, odd or surprising situations should receive more explanations, since a description alone may not make enough sense in those cases, but it is beyond the scope of this paper to test whether or not the Flickr30K data suffers from the SEB. Relation Older people with children around them are commonly seen as parents (5287405), small children as siblings (205842), men and women as lovers (4429660), groups of young people as friends (36979). Status/occupation Annotators will often guess the status or occupation of people in an image. Sometimes these guesses are relatively general (e.g. college-aged people being called students in image 36979), but other times these are very specific (e.g. a man in a workshop being called a graphics designer, 5867606)."}, {"heading": "3. Detecting stereotype-driven descriptions", "text": "In order to get an idea of the kinds of stereotype-driven descriptions that are in the Flickr30K dataset, I made a browser-based annotation tool that shows both the images and their associated descriptions.3 You can simply leaf through the images by clicking \u2018Next\u2019 or \u2018Random\u2019 until you find an interesting pattern.\n3Code and data is available on GitHub: https://github. com/evanmiltenburg/Flickr30K-Image-Viewer\n(asian|white|black|African-American|skinned) baby. I found two false positives, indicated with FP."}, {"heading": "3.1. Ethnicity/race", "text": "One interesting pattern is that the ethnicity/race of babies doesn\u2019t seem to be mentioned unless the baby is black or asian. In other words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased? We don\u2019t know whether or not an entity belongs to a particular social class (in this case: ethnic group) until it is marked as such. But we can approximate the proportion by looking at all the images where the annotators have used a marker (in this case: adjectives like black, white, asian), and for those images count how many descriptions (out of five) contain a marker. This gives us an upper bound that tells us how often ethnicity is indicated by the annotators. Note that this upper bound lies somewhere between 20% (one description) and 100% (5 descriptions). Figure 1 presents count data for the ethnic marking of babies. It includes two false positives (talking about a white baby stroller rather than a white baby). In the Asian group there is an additional complication: sometimes the mother gets marked rather than the baby. E.g. An Asian woman holds a baby girl. I have counted these occurrences as well. The numbers in Table 1 are striking: there seems to be a real, systematic difference in ethnicity marking between the groups. We can take one step further and look at all the 697 pictures with the word \u2018baby\u2019 in it. If there turn out to be disproportionately many white babies, this strengthens the conclusion that the dataset is biased.4\n4Of course this extra step does constitute an additional annotation effort, and it is fairly difficult to automate; one would have to train a classifier for each group that needs to be checked.\nI have manually categorized each of the baby images. There are 504 white, 66 asian, and 36 black babies. 73 images do not contain a baby, and 18 images do not fall into any of the other categories. While this does bring down the average number of times each category was marked, it also increases the contrast between white babies (who get marked in less than 1% of the images) and asian/black babies (who get marked much more often). A next step would be to see whether these observations also hold for other age groups, i.e. children and adults.3"}, {"heading": "3.2. Other methods", "text": "It may be difficult to spot patterns by just looking at a collection of images. Another method is to tag all descriptions with part-of-speech information, so that it becomes possible to see e.g. which adjectives are most commonly used for particular nouns. One method readers may find particularly useful is to leverage the structure of Flickr30K Entities (Plummer et al., 2015). This dataset enriches Flickr30K by adding coreference annotations, i.e. which phrase in each description refers to the same entity in the corresponding image. I have used this data to create a coreference graph by linking all phrases that refer to the same entity. Following this, I applied Louvain clustering (Blondel et al., 2008) to the coreference graph, resulting in clusters of expressions that refer to similar entities. Looking at those clusters helps to get a sense of the enormous variation in referring expressions. To get an idea of the richness of this data, here is a small sample of the phrases used to describe beards (cluster 268): a scruffy beard; a thick beard; large white beard; a bubble beard; red facial hair; a braided beard; a flaming red beard. In this case, \u2018red facial hair\u2019 really stands out as a description; why not choose the simpler \u2018beard\u2019 instead?5"}, {"heading": "4. Discussion", "text": "In the previous section, I have outlined several methods to manually detect stereotypes, biases, and odd phrases. Because there are many ways in which a phrase can be biased, it is difficult to automatically detect bias from the data. So how should we deal with stereotype-driven descriptions?\nNeutralizing stereotypes for production One way to move forward might be to work with multilingual data. Elliott et al. (2015) propose a model that generates image descriptions given data from multiple languages, in their case German and English. Multilingual, or better: multicultural data might force models to put less emphasis on features that are only salient to annotators from one particular country. Stereotypes and interpretation While stereotypes might be a problem for production, further study of cultural stereotyping might be beneficial to systems that have to interpret human descriptions and determine likely referents of those descriptions. E.g. knowing that baseball player probably refers to a male baseball player is very useful. Levels of describing an image There is a large body of work in art, information science, library science and related fields dedicated to the description and categorization\n5Code and data is available on GitHub: https://github. com/evanmiltenburg/Flickr30k-clusters\nof images (Shatford, 1986; Jaimes and Chang, 1999). A common thread is that we can divide image description into multiple levels or stages, starting from concrete physical attributes up to abstract contextual information. These levels build on each other; we first have to recognize separate entities before we can reason about their relation. But recent neural network models like (Vinyals et al., 2015) do not match this procedure. Rather, they are trained to create a direct mapping between images and their descriptions. With this paper, I hope to have shown that the Flickr30K dataset is layered, reflecting not only the physical contents of the images, but also whether the images match the everyday expectations of the crowd. An interesting challenge would be for image description models to learn separate representations for both layers: the perceptual and the contextual. Representativeness My argument here is not that we should explicitly remove bias from crowdsourced descriptions of images. This may result in normalising the data into a form that is less representative of actual human descriptions. I do, however, contend that we should accept that crowdsourced descriptions of images are biased. Acknowledging this fact is an important step towards designing models that can accommodate data based on a mixture of facts and stereotypes about the world."}, {"heading": "5. Conclusion", "text": "This paper provided a taxonomy of stereotype-driven descriptions in the Flickr30K dataset. I have divided these descriptions into two classes: linguistic bias and unwarranted inferences. The former corresponds to the annotators\u2019 choice of words when confronted with an image that may or may not match their stereotypical expectancies. The latter corresponds to the tendency of annotators to go beyond what the physical data can tell us, and expand their descriptions based on their past experiences and knowledge of the world. Acknowledging these phenomena is important, because on the one hand it helps us think about what is learnable from the data, and on the other hand it serves as a warning: if we train and evaluate language models on this data, we are effectively teaching them to be biased. I have also looked at methods to detect stereotype-driven descriptions, but due to the richness of language it is difficult to find an automated measure. Depending on whether your goal is production or interpretation, it may either be useful to suppress or to emphasize biases in human language. Finally, I have discussed stereotyping behavior as the addition of a contextual layer on top of a more basic description. This raises the question what kind of descriptions we would like our models to produce."}, {"heading": "6. Acknowledgments", "text": "Thanks to Piek Vossen and Antske Fokkens for discussion, and to Desmond Elliott and an anonymous reviewer for comments on an earlier version of this paper. This research was supported by the Netherlands Organization for Scientific Research (NWO) via the Spinoza-prize awarded to Piek Vossen (SPI 30-673, 2014-2019)."}, {"heading": "7. Bibliographical references", "text": "Beukeboom, C. J., Finkenauer, C., and Wigboldus, D. H.\n(2010). The negation bias: when negations signal stereotypic expectancies. Journal of personality and social psychology, 99(6):978. Beukeboom, C. J. (2014). Mechanisms of linguistic bias: How words reflect and maintain stereotypic expectancies. In J. Laszlo, et al., editors, Social cognition and communication, volume 31, pages 313\u2013330. Psychology Press. Author\u2019s pdf: http://dare.ubvu.vu.nl/ handle/1871/47698. Blondel, V. D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008. Elliott, D., Frank, S., and Hasler, E. (2015). Multilingual image description with neural sequence models. CoRR, abs/1510.04709. Hodosh, M., Young, P., and Hockenmaier, J. (2013). Framing image description as a ranking task: Data, models and evaluation metrics. JAIR, pages 853\u2013899. Jaimes, A. and Chang, S.-F. (1999). Conceptual framework for indexing visual information at multiple levels. In Electronic Imaging, pages 2\u201315. International Society for Optics and Photonics. Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C., Hockenmaier, J., and Lazebnik, S. (2015). Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE ICCV, pages 2641\u20132649. Rashtchian, C., Young, P., Hodosh, M., and Hockenmaier, J. (2010). Collecting image annotations using amazon\u2019s mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk, pages 139\u2013147. Association for Computational Linguistics. Romaine, S. (2001). A corpus-based view of gender in british and american english. Gender Across Languages: The linguistic representation of women and men, 1:153\u2013 175. Sekaquaptewa, D., Espinoza, P., Thompson, M., Vargas, P., and von Hippel, W. (2003). Stereotypic explanatory bias: Implicit stereotyping as a predictor of discrimination. Journal of Experimental Social Psychology, 39(1):75\u201382. Shatford, S. (1986). Analyzing the subject of a picture: a theoretical approach. Cataloging & classification quarterly, 6(3):39\u201362. Stahlberg, D., Braun, F., Irmen, L., and Sczesny, S. (2007). Representation of the sexes in language. Social communication, pages 163\u2013187. Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings of CVPR, pages 3156\u20133164. Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. (2014). From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. TACL, 2:67\u201378."}], "references": [{"title": "The negation bias: when negations signal stereotypic expectancies", "author": ["C.J. Beukeboom", "C. Finkenauer", "D.H. Wigboldus"], "venue": "Journal of personality and social psychology,", "citeRegEx": "Beukeboom et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Beukeboom et al\\.", "year": 2010}, {"title": "Mechanisms of linguistic bias: How words reflect and maintain stereotypic expectancies", "author": ["C.J. Beukeboom"], "venue": "Social cognition and communication,", "citeRegEx": "Beukeboom,? \\Q2014\\E", "shortCiteRegEx": "Beukeboom", "year": 2014}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "Guillaume", "J.-L", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of statistical mechanics: theory and experiment,", "citeRegEx": "Blondel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blondel et al\\.", "year": 2008}, {"title": "Multilingual image description with neural sequence models. CoRR, abs/1510.04709", "author": ["D. Elliott", "S. Frank", "E. Hasler"], "venue": null, "citeRegEx": "Elliott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "metrics. JAIR,", "citeRegEx": "Hodosh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hodosh et al\\.", "year": 2013}, {"title": "Conceptual framework for indexing visual information at multiple levels. In Electronic Imaging, pages 2\u201315. International Society for Optics and Photonics", "author": ["A. Jaimes", "Chang", "S.-F"], "venue": null, "citeRegEx": "Jaimes et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jaimes et al\\.", "year": 1999}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "In Proceedings of the IEEE ICCV,", "citeRegEx": "Plummer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2015}, {"title": "Collecting image annotations using amazon\u2019s mechanical turk", "author": ["C. Rashtchian", "P. Young", "M. Hodosh", "J. Hockenmaier"], "venue": "In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk,", "citeRegEx": "Rashtchian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rashtchian et al\\.", "year": 2010}, {"title": "A corpus-based view of gender in british and american english. Gender Across Languages: The linguistic representation of women and men", "author": ["S. Romaine"], "venue": null, "citeRegEx": "Romaine,? \\Q2001\\E", "shortCiteRegEx": "Romaine", "year": 2001}, {"title": "Stereotypic explanatory bias: Implicit stereotyping as a predictor of discrimination", "author": ["D. Sekaquaptewa", "P. Espinoza", "M. Thompson", "P. Vargas", "W. von Hippel"], "venue": "Journal of Experimental Social Psychology,", "citeRegEx": "Sekaquaptewa et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sekaquaptewa et al\\.", "year": 2003}, {"title": "Analyzing the subject of a picture: a theoretical approach", "author": ["S. Shatford"], "venue": "Cataloging & classification quarterly,", "citeRegEx": "Shatford,? \\Q1986\\E", "shortCiteRegEx": "Shatford", "year": 1986}, {"title": "Representation of the sexes in language", "author": ["D. Stahlberg", "F. Braun", "L. Irmen", "S. Sczesny"], "venue": "Social communication,", "citeRegEx": "Stahlberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2007}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they \u201cfo-", "startOffset": 99, "endOffset": 119}, {"referenceID": 13, "context": "The Flickr30K dataset (Young et al., 2014) is a collection of over 30,000 images with 5 crowdsourced descriptions each.", "startOffset": 22, "endOffset": 42}, {"referenceID": 12, "context": "(Vinyals et al., 2015)).", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "I will build on earlier work on linguistic bias in general (Beukeboom, 2014), providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences.", "startOffset": 59, "endOffset": 76}, {"referenceID": 6, "context": "Rashtchian et al. (2010) do note that some descriptions are speculative in nature, which they say hurts the accuracy and the consistency of the descriptions.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "The former is discussed in more detail by Beukeboom (2014), who defines linguistic bias as \u201ca systematic asymmetry in word choice as a function of the social category to which the target belongs.", "startOffset": 42, "endOffset": 59}, {"referenceID": 1, "context": "Beukeboom (2014) lists several linguistic \u2018tools\u2019 that people use to mark individuals who deviate from the norm.", "startOffset": 0, "endOffset": 17}, {"referenceID": 11, "context": "Adjectives One well-studied example (Stahlberg et al., 2007; Romaine, 2001) is sexist language, where the sex of a person tends to be mentioned more frequently if their role or occupation is inconsistent with \u2018traditional\u2019 gender roles (e.", "startOffset": 36, "endOffset": 75}, {"referenceID": 8, "context": "Adjectives One well-studied example (Stahlberg et al., 2007; Romaine, 2001) is sexist language, where the sex of a person tends to be mentioned more frequently if their role or occupation is inconsistent with \u2018traditional\u2019 gender roles (e.", "startOffset": 36, "endOffset": 75}, {"referenceID": 0, "context": "See also (Beukeboom et al., 2010).", "startOffset": 9, "endOffset": 33}, {"referenceID": 1, "context": "Examples given are also due to (Beukeboom, 2014).", "startOffset": 31, "endOffset": 48}, {"referenceID": 6, "context": "One method readers may find particularly useful is to leverage the structure of Flickr30K Entities (Plummer et al., 2015).", "startOffset": 99, "endOffset": 121}, {"referenceID": 2, "context": "Following this, I applied Louvain clustering (Blondel et al., 2008) to the coreference graph, resulting in clusters of expressions that refer to similar entities.", "startOffset": 45, "endOffset": 67}, {"referenceID": 3, "context": "Elliott et al. (2015) propose a model that generates image descriptions given data from multiple languages, in their case German and English.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "of images (Shatford, 1986; Jaimes and Chang, 1999).", "startOffset": 10, "endOffset": 50}, {"referenceID": 12, "context": "But recent neural network models like (Vinyals et al., 2015) do not match this procedure.", "startOffset": 38, "endOffset": 60}], "year": 2016, "abstractText": "In: Proceedings of the Workshop on Multimodal Corpora: Computer vision and language processing (MMC-2016), pages 1\u20134. Workshop held: 24 May 2016, collocated with LREC 2016, Portoro\u017e, Slovenia. Proceedings available at: http://www.lrec-conf.org/proceedings/lrec2016/workshops/ LREC2016Workshop-MCC-2016-proceedings.pdf An untested assumption behind the crowdsourced descriptions of the images in the Flickr30K dataset (Young et al., 2014) is that they \u201cfocus only on the information that can be obtained from the image alone\u201d (Hodosh et al., 2013, p. 859). This paper presents some evidence against this assumption, and provides a list of biases and unwarranted inferences that can be found in the Flickr30K dataset. Finally, it considers methods to find examples of these, and discusses how we should deal with stereotype-driven descriptions in future applications.", "creator": "LaTeX with hyperref package"}}}