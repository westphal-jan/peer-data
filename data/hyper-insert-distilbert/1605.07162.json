{"id": "1605.07162", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2016", "title": "Pure Exploration of Multi-armed Bandit Under Matroid Constraints", "abstract": "we study specifically the pure exploration problem subject belonging to a matroid constraint ( best - basis ) in getting a real stochastic constraint multi - game armed bandit trap game. ` in guessing a best - basis of instance, we are given $ n $ stochastic arms } with unknown reward distributions, presumably as well as a matroid $ \\ total mathcal { if m } $ = over for the sampled arms. essentially let the weight of { an arm be the mean of each its unspecified reward distribution. finally our operational goal exercise is basically to individually identify a numerical basis of $ \\ mathcal { \u2264 m } $ with the predicted maximum total nominal weight, using then as unusually few samples elsewhere as possible.", "histories": [["v1", "Mon, 23 May 2016 19:51:42 GMT  (39kb)", "http://arxiv.org/abs/1605.07162v1", "To appear in COLT 2016"], ["v2", "Tue, 24 May 2016 19:20:41 GMT  (50kb)", "http://arxiv.org/abs/1605.07162v2", "Accepted for presentation at Conference on Learning Theory (COLT) 2016"], ["v3", "Wed, 25 May 2016 16:03:23 GMT  (39kb)", "http://arxiv.org/abs/1605.07162v3", "Accepted for presentation at Conference on Learning Theory (COLT) 2016"]], "COMMENTS": "To appear in COLT 2016", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["lijie chen", "anupam gupta", "jian li"], "accepted": false, "id": "1605.07162"}, "pdf": {"name": "1605.07162.pdf", "metadata": {"source": "CRF", "title": "Pure Exploration of Multi-armed Bandit Under Matroid Constraints", "authors": ["Lijie Chen", "Anupam Gupta", "Jian Li"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n07 16\n2v 1\n[ cs\n.L G\n] 2\n3 M\nThe problem is a significant generalization of the best arm identification problem and the top-k arm identification problem, which have attracted significant attentions in recent years. We study both the exact and PAC versions of Best-Basis, and provide algorithms with nearlyoptimal sample complexities for these versions. Our results generalize and/or improve on several previous results for the top-k arm identification problem and the combinatorial pure exploration problem when the combinatorial constraint is a matroid."}, {"heading": "1 Introduction", "text": "The stochastic multi-armed bandit is a classical model for characterizing the exploration-exploitation tradeoff in many decision-making problems in stochastic environments. The popular objectives include maximizing the cumulative sum of rewards, or minimizing the cumulative regret (see e.g., [CBL06, BCB12]). However, in many application domains, the exploration phase and the evaluation phase are separated. The decision-maker can perform a pure-exploration phase to identify an optimal (or nearly optimal) solution, and then keep exploiting this solution. Such problems arise in application domains such as medical trials [Rob85, AB10], communication network [AB10], crowdsourcing [ZCL14, CLTL15]. In particular, the problem of identifying the single best arm in a stochastic bandit game has been has received considerable attention in recent years [AB10, EDMM06, MT04, JMNB14, KKS13, CL15]. The generalization to identifying the top-k arms has also been studied extensively [GGL12, KTAS12, KK13, KCG14, ZCL14, CLTL15]. Since these problems are closely related to the problem we study in the paper, we formally define it as follows.\n\u2217Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University, Beijing, China. Research supported in part by the National Basic Research Program of China grants 2015CB358700, 2011CBA00300, 2011CBA00301, and the National NSFC grants 61033001, 61361136003.\n\u2020Department of Computer Science, Carnegie Mellon University, Pittsburgh, USA. Research partly supported by NSF awards CCF-1016799 and CCF-1319811.\nProblem 1. (Best-k-Arm) There are n unknown distributions D1,D2, . . . ,Dn, all supported on [0, 1]. Let the mean of Di be \u00b5i. At each step we choose a distribution and get an i.i.d. sample from the distribution. Our goal is to find the k distributions with the largest means (exactly or approximately), with probability at least 1\u2212 \u03b4, using as few samples as possible.\nThe distributions above are also called arms in the multi-armed bandit literature. We denote the kth largest mean by \u00b5[k]. In addition, we assume \u00b5[k] and \u00b5[k+1] are different (so the optimal top-k answer is unique).\nIn certain applications such as online ad allocations, there is a natural combinatorial constraint over the set of arms, and we can only choose a subset of arms subject to the given constraint (Best-k-Arm simply involves a cardinality constraint). Motivated by such applications, Chen et al. [CLK+14] introduced the combinatrial pure exploration problem. They considered the general setting with arbitrary combinatorial constraint, and propose several algorithms. In this paper, we consider the same problem under a matroid constraint, one of the most popular combinatorial constraint. The matroid constraint was also discuss in length in [CLK+14].\nThe notion of matroid (see Section 2 for the definition) is an abstraction of many combinatorial structures, including the sets of linearly independent vectors in a given set of vectors, the sets of spanning forests in an undirected graph and many others. We note that Best-k-Arm is a special case of a matroid constraint, since all subsets of size of at most k form a uniform matroid. Now, we formally define the matroid pure exploration bandit problem as follows.\nDefinition 1.1. (Best-Basis) In a Best-Basis instance S = (S,M), we are given a set S of n arms. Each arm a \u2208 S is associated with an unknown reward distribution Da, supported on [0, 1], with mean \u00b5a (which is unknown as well). Without loss of generality, we assume all arms have distinct means.\nWe are also given a matroid M = (S,I) with ground set identified with the set S of arms. The weight function \u00b5 : S \u2192 R+ simply sets the weight of a to be the mean of Da; i.e., \u00b5(a) = \u00b5a for all a \u2208 S. The weights are initially unknown, and are only learned by sampling arms. Our goal is to find a basis (a.k.a. a maximal independent set) of the matroid with the maximum total weight/cost (exactly or approximately), with probability at least 1\u2212 \u03b4, using as few samples as possible.\nBesides including Best-k-Arm as a special case, the Best-Basis problem also captures the following natural problems, motivated by various applications.\n1. Suppose we have m disjoint groups G1, . . . , Gm of arms, and we would like to pick the best ki arms from group Gi (where kis are given integers). This is exactly the best-basis problem for a partition matroid. Note that PAC version of the problem cannot be modeled as a disjoint collection of best-k-problems.\nThe special case where ki = 1 has been studied in [GGLB11, BWV12] (under the fixed budget setting). They are motivated by a clinical problem with m subpopulations, where one would like to decide the best ki treatments from the options available for subjects from each subpopulation.\n2. Beside the above constraints for the groups, we may have an additional global constraint on the total number of arms we can choose. This is a special case of a laminar matroid.\n3. An application mentioned in [CLK+14] is the following. Consider a network where the delay of the links are stochastic. A network routing system wants to build a minimum spanning tree to connect all nodes, where the weight of each edges are expected delay of that link. A spanning tree is a basis in a graphical matroid.\n4. Consider a set of workers and a set of tasks. Each worker is able to do only a subset of tasks (which defines a worker-task bipartite graph). Each worker must be assigned to one task (so we need to build a matching between the workers and the tasks) and the reward of a task is stochastic. We would like to identify the set of tasks that can be completed by the set of workers and have maximum total reward. This combinatorial structure (over the subsets of tasks) is a transversal matroid. This problem (or variants) may find applications in crowdsourcing or online advertisement.\nThere are two natural formulations of the Best-Basis problem: in one, we need to identify the unique optimal basis with a certain confidence, and in some others we can settle for an approximate optimal basis (the PAC setting). We now formally define these problems, and present our results."}, {"heading": "1.1 Identifying the Exact Optimal basis", "text": "Definition 1.2. (Exact-Basis) Given a Best-Basis instance S = (S,M) and a confidence level \u03b4 > 0, the goal is to output the optimal basis of M (one that maximizes \u2211 a\u2208I \u00b5a) with probability at least 1\u2212 \u03b4, using as few samples as possible.\nWithout loss of generality, assume that matroid M has no isolated elements (i.e., elements that are included in every basis) and no loops (i.e., elements that belong to no basis), since we can always include or ignore them without affecting the solution. We use OPT(M) to denote the optimal basis (as well as the optimal total weight) for matroid M. For a subset of elements F \u2286 S, let MF denote the restriction of M to F , and M/F denote the contraction of M by F (see Definition 2.3). Note that OPT(M/{e}) + \u00b5(e) is the optimal cost among all bases including e. Naturally, the sample complexity of an algorithm for Exact-Basis depends on the parameters of the problem instance. In particular, we need to define the following gap parameter.\nDefinition 1.3 (Gap). Given a matroid M = (S,I) with cost function \u00b5 : S \u2192 R+, such that all costs are distinct, define the gap of an element e \u2208 S to be\n\u2206M,\u00b5e := { OPT(M)\u2212 OPT(MS\\{e}) e \u2208 OPT(M) OPT(M)\u2212 (OPT(M/{e}) + \u00b5(e)) e 6\u2208 OPT(M)\nIntuitively, for an element e \u2208 OPT(M), its gap is the loss if we do not select e, whereas for an element e /\u2208 OPT(M), its gap is the loss if we are forced to select e. Since we assume that elements have distinct weights, \u2206e > 0 for all arms e. We note that Definition 1.3 is the same as the gap definition in [CLK+14] and generalizes the gaps defined for the Best-k-Arm problem used in [KTAS12] (in Best-k-Arm, the gap of an arm e to be \u2206e = \u00b5e \u2212 \u00b5[k+1] if e is a top-k arm, and \u2206e = \u00b5[k] \u2212 \u00b5e otherwise). Chen et al. [CLK+14] obtained an algorithm with sample complexity\n( \u2211\ne\u2208S\n\u2206\u22122e (ln \u03b4 \u22121 + lnn+ ln \u2211 e\u2208S \u2206\u22122e )\n) ,\nwhen specialized to Exact-Basis. 1 We improve upon their result by proving the following theorem.\nTheorem 1.4 (Main Result for Exact Identification). There is an algorithm for Exact-Basis, that returns the optimal basis for S, with probability at least 1\u2212 \u03b4, and uses at most\nO\n( \u2211\ne\u2208S\n\u2206\u22122e (ln \u03b4 \u22121 + ln k + ln ln\u2206\u22121e )\n)\nsamples. Here, k = rank(M) is the size of a basis of M.\nObserve that the dependence is now on the rank of the matroid k, rather than the number of elements n which may be much larger than k. Moreover, the dependence on \u2206e is doubly logarithmic.\nFor the special case of the k-uniform matroid, the problem becomes the Best-k-Arm problem, for which the current state-of-the-art is O( \u2211n i=1\u2206 \u22122 [i] (ln \u03b4 \u22121 + ln \u2211n i=1 \u2206 \u22122 [i] )), obtained by [KTAS12]. Theorem 1.4 improves upon this result for the typical case when ln \u2211n\ni=1\u2206 \u22122 [i] is larger than ln k.\nTheorem 1.4 also matches the recent upper bound of O( \u2211n\ni=2\u2206 \u22122 [i] (ln ln\u2206 \u22121 [i] + ln \u03b4 \u22121)) for Best-\n1-Arm, due to Karnin et al. [KKS13] and Jamieson et al. [JMNB14]. Chen et al. [CLK+14] proved an \u2126( \u2211\ne\u2208S \u2206 \u22122 e ln \u03b4 \u22121) lower bound for the problem. Moreover, Kalyanakrishnan et al. [KTAS12] showed an \u2126(n\u03b5\u22122(ln \u03b4\u22121 + ln k)) lower bound for a PAC version (the Explore-k metric, see Section 1.2) of Best-k-Arm. Indeed, in their lower bound instances, all arms have gap \u2206e = \u03b5. If we apply our exact algorithm on those instances, the sample complexity is O(n\u03b5\u22122(ln \u03b4\u22121 + ln k + ln ln \u03b5\u22121)). Hence, the first two terms of our upper bound are probably necessary in light of the above lower bounds."}, {"heading": "1.2 The PAC setting", "text": "Next we discuss our results for the PAC setting. Several notions of approximation were used for the special case of Best-k-Arm, when we return a set I of k arms. Kalyanakrishnan et al. [KTAS12] required that the mean of every arm in I be at least \u00b5[k] \u2212 \u03b5 (The Explore-k metric). Zhou et al. [ZCL14] required that the average mean 1k \u2211 e\u2208I \u00b5e of I be at least 1 k \u2211k i=1 \u00b5[i] \u2212 \u03b5; we call such a solution an average-\u03b5-optimal solution. Finally, Cao et al. [CLTL15] proposed a stronger metric that required the mean of the ith arm in I be at least \u00b5[i] \u2212 \u03b5, for all i \u2208 [k]. This notion, which we call elementwise-\u03b5-optimality extends to general matroids: we need that ith largest arm in our solution is at least the ith largest mean in the optimal solution minus \u03b5.\nIn this paper we introduce the stronger notion of an \u03b5-optimal solution.\nDefinition 1.5. (PAC-Basis and \u03b5-optimality) We are given a matroid M = (S,I) with cost function \u00b5 : S \u2192 R+. We say a basis I is \u03b5-optimal (with respect to \u00b5), if I is an optimal solution for the modified cost function \u00b5I,\u03b5, defined as follows:\n\u00b5I,\u03b5(e) = { \u00b5(e) + \u03b5 for e \u2208 I \u00b5(e) for e 6\u2208 I.\n1Their algorithm works for arbitrary combinatorial constraint. The sample complexity depends on a width parameter of the constraint, which is roughly the number of elements needed to be exchanged from one feasible solution to another. The width can be as large as n. For a matroid, the width is 2.\nIn other words, if we add \u03b5 to each element in I, I would become an optimal solution.\nThe proof of the following proposition can be found in the appendix.\nProposition 1.6. For a Best-Basis instance, an \u03b5-optimal solution is also elementwise-\u03b5-optimal. The converse is not necessarily true.\nTheorem 1.7 (Main Result for PAC Setting). There is an algorithm for PAC-Basis which returns an \u03b5-optimal solution for S = (S,M), with probability at least 1\u2212 \u03b4, and uses at most\nO(n\u03b5\u22122 \u00b7 (ln k + ln \u03b4\u22121))\nsamples, where k = rank(M).\nThis theorem generalizes and strengthens the results in [KTAS12, CLTL15], in which the same sample complexity was obtained for Best-k-Arm under Explore-k and elementwise-\u03b5-optimality metrics, respectively. In fact, this sample complexity is optimal, since an \u2126(n\u03b5\u22122(ln k + ln \u03b4\u22121)) lower bound is known for Explore-k for the special case of Best-k-Arm, due to [KTAS12]."}, {"heading": "1.2.1 Average-\u03b5-optimality", "text": "We also consider the weaker notion of average-\u03b5-optimality, which may suffice for certain applications. For this definition, we give another algorithm with a lower sample complexity.\nDefinition 1.8. (PAC-Basis-Avg) Given a matroid M = (S,I) with cost function \u00b5 : S \u2192 R +. Suppose k = rank(M). We say a basis I is an average-\u01eb-optimal solution (w.r.t. \u00b5), if: 1 k \u2211 e\u2208I \u00b5(e) \u2265 1kOPT(M)\u2212 \u03b5.\nTheorem 1.9. There is an algorithm for PAC-Basis-Avg, which can return an average-\u01eb-optimal solution for S, with probability at least 1\u2212 \u03b4, and its sample complexity is at most\nO (( n \u00b7 (1 + ln \u03b4\u22121/k) + (ln \u03b4\u22121 + k)(ln k ln ln k + ln \u03b4\u22121 ln ln \u03b4\u22121) ) \u03b5\u22122 ) .\nIn particular, when k ln \u03b4\u22121 \u2264 O(n0.99) and \u03b4 \u2265 \u2126(exp(\u2212n0.49)), the sample complexity is\nO(n\u03b5\u22122(1 + ln \u03b4\u22121/k)).\n[ZCL14] obtained matching upper and lower bounds of \u2126(n\u03b5\u22122(1 + ln \u03b4\u22121/k)) for Best-k-Arm under the average metric. Our result matches their result when \u03b4 is not extremely small and k is not very close to n. Obtaining tight upper and lower bounds for all range of parameters is left as an interesting open question."}, {"heading": "1.2.2 Prior and Our Techniques", "text": "Several prior algorithms for the PAC versions of Best-1-Arm and Best-k-Arm (e.g., [KKS13, ZCL14, EDMM02]) were elimination-based, roughly using the following framework: In the rth round, we sample each remaining arm Qr times, 2 and eliminate all arms whose empirical means\n2Typically, Qr increases exponentially with r.\nfall below a certain threshold. This threshold can be either a percentile, as in [EDMM02, ZCL14] or an \u03b5-optimal arm obtained by some PAC algorithm, such as in [KKS13]. After eliminating some arms, we proceed to the next round. Small variations to this procedure are possible, e.g., if the number of remaining arms is not much larger than k, we can directly use the na\u0308\u0131ve uniform sampling algorithm. A main difference in prior works is in their analysis, due to the different PAC-optimality metrics. However, we cannot easily extend this framework to either PAC-Basis or PAC-BasisAvg, since it is not clear how to eliminate even a small constant fraction of arms while ensuring that the optimal value for the remaining set does not drop. Indeed, due to the combinatorial structure of the matroid, we cannot perform elimination based solely on fixed thresholds.\nWe resolve the issue by applying a sampling-and-pruning technique developed by Karger, and used by Karger, Klein, and Tarjan in their expected linear-time randomized algorithm for minimum spanning tree. Here is the high-level idea, in the context of the PAC-Basis problem. We pick a random subset F by including each arm independently with some small constant probability p, and recursively find an \u03b5/3-optimal basis I for the subset F . The key idea is that this basis I can be used to eliminate a significant proportion of arms, while ensuring that the remaining set still contains a desirable solution. Hence, after eliminating those arms, we can recurse on the remaining arms. Unlike the previous algorithms which eliminate arms based on a single threshold, we perform the elimination based on the solution I of a random subset. We feel this extension of the sampling and pruning technique to bandit problems will find other applications.\nAnother popular approach for pure exploration problems is based on upper or lower confidence bounds (UCB or LUCB) (see e.g., [KTAS12, CLK+14]). While being very flexible and easy to apply, the analysis of all such bounds inevitably requires a union bound of all rounds (which is at least n), thus incurring a log n factor, which is worse than the optimal log k factor that we obtain."}, {"heading": "1.3 Other Related Work", "text": "The problem of identifying the single best arm, a very special case of our problem, has been studied extensively. For the PAC version of the problem, 3 Even-Dar et al. [EDMM02] provided an algorithm with sample complexity O(n\u03b5\u22122 \u00b7 ln \u03b4\u22121), which is also optimal. For the exact version, Mannor and Tsitsiklis [MT04] proved a lower bound of \u2126( \u2211n i=2 \u2206 \u22122 [i] ln \u03b4 \u22121). [Far64] showed a lower bound of \u2126(\u2206\u22122[2] ln ln\u2206 \u22121 [2] ) even if there are only two arms. Karnin et al. [KKS13] obtained an upper bound of O( \u2211n\ni=2\u2206 \u22122 [i] (ln ln\u2206 \u22121 [i] + ln \u03b4 \u22121)), matching Farrell\u2019s lower bound for two arms.\nJamieson et al. [JMNB14] obtained the same result using a UCB-like algorithm. Very Recently, Chen and Li [CL15] provided a new lower bound of \u2126( \u2211n i=2 \u2206 \u22122 [i] ln lnn) and an improved upper\nbound of O ( \u2206\u22122[2] ln ln\u2206 \u22121 [2] + \u2211n i=2 \u2206 \u22122 [i] ln \u03b4 \u22121 + \u2211n i=2 \u2206 \u22122 [i] ln lnmin(n,\u2206 \u22121 [i] ) ) .\nIn all aforementioned results, we require that the (PAC or exact) algorithm returns a correct answer with probability at least 1\u2212 \u03b4. This is called the fixed confidence setting in the literature. Another popular setting is the fixed budget setting, in which the total number of samples is subject to a given budget constraint, and we would like to minimize the failure probability (see e.g., [BWV13, GGL12, KKS13, CLK+14]). Some prior work ([AB10, BWV13, ABL13]) also considered the objective of making the expected simple regret at most \u03b5 (i.e., 1k ( \u2211k i=1 \u00b5[i] \u2212 E[ \u2211 a\u2208T \u00b5a]) \u2264 \u03b5), which is a somewhat weaker objective.\n3 Since the solution only contains one arm, all different notions of PAC optimality mentioned in Section 1.2 are equivalent.\nThere is a large body of work on minimizing the cumulative regret in online multi-armed bandit games with various combinatorial constraints in different feedback settings (see e.g., [CBL06, BCB12, CBL12, ABL13, CWY13] and the references therein). In an online bandit game, there are T rounds. In the tth round, we can play a combinatorial subset Si of arms. The goal is to minimize T \u2211 a\u2208OPT \u00b5a \u2212 \u2211T t=1 \u2211 a\u2208St\n\u00b5a. We note that it is possible to obtain an expected simple regret of \u03b5 for Best-Basis, with at most O(n\u03b5\u22122) samples, using the semi-bandit regret bound in [ABL13]. In particular, they provided an online mirror descent algorithm and showed a cumulative regret of\u221a knT in the semi-bandit feedback setting (i.e., we can only observe the rewards from the arms we played ), where k is the maximum cardinality of a feasible set. By setting T = nk\u22121\u03b5\u22122, we get a cumulative regret of n/\u03b5. If we uniformly randomly pick a solution from {St}t\u2208[T ], we can see that Et\u2208[T ] 1 k ( \u2211 a\u2208OPT \u00b5a \u2212 \u2211 a\u2208St \u00b5a) \u2264 \u03b5. One drawback of their algorithm is that it needs to solve a convex program over the matroid polytope, which can be computationally expensive, while our algorithm is purely combinatorial and very easy to implement.\nIn recent and concurrent work, Gabillon et al. [GLG+16] proposed a new complexity notion for the general combinatorial pure exploration problem, and developed new algorithms in both fixed budget and the fixed confidence setting. They showed that in some cases, the sample complexity of their algorithms is better than that of [CLK+14]. While the current implementations of their algorithm have an exponential running time, even for general matroid constraints, it is an interesting problem to get more efficient algorithms, and to combine their notion of complexity with our techniques."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Useful Facts about Matroids", "text": "While there are many equivalent definitions for matroids, we find this one most convenient.\nDefinition 2.1 (Matroid). A matroid M(S,I) consists of a finite set S (called the ground set), and a non-empty family I of subsets of S (with sets in I being called independent sets), satisfying the following:\ni. Any subset of an independent set is an independent set. ii. Given two sets I, J \u2208 I, if |I| > |J |, there exists element e \u2208 I \\ J such that J \u222a {e} \u2208 I.\nFor convenience, we often write I \u2208 M instead of I \u2208 I to denote that I is an independent set of M. An independent set is maximal if it is not a proper subset of another independent set; a maximal independent set is called a basis.\nDefinition 2.2 (Rank). Given matroid M(S,I) and set A \u2286 S, the rank of A, denoted by rankM(A), is the cardinality of a maximal independent subset contained in A.\nWhen M is clear from context, we merely write rank(A). All bases of a matroid have the same cardinality. We use rank(M), instead of rank(S), to denote the cardinality of every basis of M. We often need to work with the set of independent sets restricted to a subset of elements. Sometimes we can determine to include some elements as a partial solution, we need to work the the rest of the matroid, conditioning on the partial solution. We need the definitions of matroid restrictions and matroid contractions to formalize the above situations.\nDefinition 2.3 (Matroid restrictions and contractions). Let M(S,I) be a matroid. For A \u2286 S, we define the restriction of M to A as follows: MA is also a matroid with ground set A; an independent set of M which is a subset of A is an independent of MA. The contraction of A is defined as follows: M/A is the matroid with ground set S\u2032 = {e \u2208 S | rank({e} \u222a A) > rank(A)}, and the independent set family I \u2032 = {I \u2286 S\u2032 | rank(I \u222a A) = |I|+ rank(A)}.\nBoth MA and M/A are indeed matroids. Sometimes, we may also write M|A and M/A to avoid successive subscripts. In our paper, we only need to contract an independent set A \u2208 I. In this case, rank(A) = |A|, and the definition simplifies to the following: a set I (disjoint from A) is independent in M/A, if I \u222aA is independent in M.\nDefinition 2.4 (Isolated Elements and Loops). For a matroid M = (S,I) and element e \u2208 S, we say e is an isolated element, if it is contained in all bases of M (or equivalently, rank(S) > rank(S \\ {e})). We say e is a loop if it belongs to no basis of M.\nClearly, since the mean of each arm is nonnegative, we can directly select all isolated elements and contract out these elements. Also, we can simply ignore those loops. From now on, we can assume without loss of generality that there is no isolated element or loop in M.\nDefinition 2.5 (Block). Let M(S,I) be a matroid. Given a subset A \u2282 S and an element e such that e 6\u2208 A, we say A blocks e, if rankM(A \u222a {e}) = rankM(A).\nIntuitively, e is blocked by A if adding e is not useful in increasing the cardinality of the maximal independent set in A. Note that, if A \u2286 B, e 6\u2208 B and A blocks e, then clearly B also blocks e, due to the submodularity of rank: rank(B \u2229 {e}) \u2212 rank(B) \u2264 rank(A \u2229 {e}) \u2212 rank(A). We have the following lemma characterizing when a subset A blocks an element e.\nLemma 2.6. If A blocks e, every basis I of MA blocks e.\nProof. Since A blocks e, rankM(A\u222a{e}) = rankM(A). Consider a basis I of MA. rankM(I\u222a{e}) \u2264 rankM(A \u222a {e}) = rankM(A) = rankM(I). Hence, I blocks e as well.\nThen we define what is an optimal solution for a matroid with respect to a cost function \u00b5.\nDefinition 2.7. Given a matroid M(S,I), and an injective cost/weight function \u00b5 : S \u2192 R+, let \u00b5(I) := \u2211 e\u2208I \u00b5(e) denote the total weight of elements in the independent set I \u2208 M . We say I is an optimal basis (with respect to \u00b5) if \u00b5(I) has the maximum value among all independent sets in I. We define OPT\u00b5(M) = maxI\u2208I \u00b5(I). With slight abuse of notation, we may also use OPT\u00b5(M) to denote the optimal basis. When \u00b5 is clear from the context, we simply write OPT(M).\nFrom now on, we assume the cost of each element is distinct. It is well known that the optimal basis OPT(M) is unique (under the distinctness assumption) and can be obtained by a simple greedy algorithm: We first sort the elements in the decreasing order of their cost. Then, we attempt to add the elements greedily one by one in this order, to the current solution, which is initially empty.\nWe are given matroid M = (S,I) with cost function \u00b5 : S \u2192 R+. For a subset A \u2286 S, we define\nA\u2265a\u00b5 := {e \u2208 A | \u00b5(e) \u2265 a}.\nWe define A>a\u00b5 , A \u2264a \u00b5 , A <a \u00b5 similarly. Sometimes we omit the subscript \u00b5 if it is clear from the context. Finally, the following characterizations of optimal solutions for M all follow from the greedy procedure.\nLemma 2.8. For a matroid M(S,I), cost function \u00b5 : S \u2192 R+ and basis I \u2208 I, the following statements are equivalent:\ni. I is an optimal basis for M with respect to \u00b5. ii. For any e \u2208 I, S>\u00b5(e) does not block e. iii. For any e \u2208 S \\ I, I\u2265\u00b5(e) blocks e. iv. For any r \u2208 R, I\u2265r is a basis in MS\u2265r ."}, {"heading": "2.2 Uniform Sampling", "text": "The following na\u0308\u0131ve uniform sampling procedure will be used frequently.\nAlgorithm 1: UniformSample (S, \u03b5, \u03b4)\nData: Arm set S, error bound \u03b5, confidence level \u03b4. Result: For each arm a, output the empirical mean \u00b5\u0302a.\n1 For each arm a \u2208 S, sample it \u03b5\u22122 ln(2 \u00b7 \u03b4\u22121)/2 times. Let \u00b5\u0302a be the empirical mean.\nThe following lemma for Algorithm 1, is an immediate consequence of Proposition A.1.1.\nLemma 2.9. For each arm a \u2208 S, we have that Pr [|\u00b5a \u2212 \u00b5\u0302a| \u2265 \u03b5] \u2264 \u03b4."}, {"heading": "3 An Optimal PAC Algorithm for the PAC-Basis Problem", "text": "In this section, we prove Theorem 1.7 by presenting an algorithm for PAC-Basis with optimal sample complexity. The algorithm is also a useful subprocedure for both Exact-Basis and PACBasis-Avg."}, {"heading": "3.1 Notation", "text": "We first introduce an analogue of Lemma 2.8 for \u03b5-optimal solutions.\nLemma 3.1. For a matroid M = (S,I) with cost function \u00b5 : S \u2192 R+, and a basis I, the following statements are equivalent:\n1. I is \u03b5-optimal for M with respect to \u00b5. 2. For any e \u2208 S \\ I, I\u2265\u00b5(e)\u2212\u03b5 blocks e. 3. For any r \u2208 R, let Dr = (S \\ I)\u2265r+\u03b5 \u222a I\u2265r. I\u2265r is a basis in MDr .\nProof. Apply Lemma 2.8 with the cost function \u00b5I,\u03b5, as defined in Definition 1.5.\nDefinition 3.2 (\u03b5-Approximation Subset). Given a matroid M = (S,I) and cost function \u00b5 : S \u2192 R +, let A \u2286 B be two subsets of S. We say A is an \u03b5-approximate subset of B if there exists an independent set I \u2208 MA such that I is \u03b5-optimal for MB with respect to the cost function \u00b5. Lemma 3.3. Suppose A is an \u03b5-approximate subset of B, and I \u2208 MA is \u03b5-optimal for MB. For any e \u2208 B \\ A, I\u2265\u00b5(e)\u2212\u03b5 blocks e and A\u2265\u00b5(e)\u2212\u03b5 blocks e.\nProof. I\u2265\u00b5(e)\u2212\u03b5 blocks e because Lemma 3.1(2). I\u2265\u00b5(e)\u2212\u03b5 is an independent set of A\u2265\u00b5(e)\u2212\u03b5, so A\u2265\u00b5(e)\u2212\u03b5 blocks e as well.\nThen we show that \u201cis an \u03b5-approximate subset of\u201d is a transitive relation.\nLemma 3.4. Let A \u2286 B \u2286 C. Suppose A is an \u03b51-approximate subset of B, and B is an \u03b52approximate subset of C. Then A is an (\u03b51 + \u03b52)-approximate subset of C.\nProof. Let I \u2208 MA be \u03b51-optimal for MB . We prove it is (\u03b51 + \u03b52)-optimal for MC . For any element e \u2208 B \\ A, I\u2265\u00b5(e)\u2212\u03b51 blocks e. So I\u2265\u00b5(e)\u2212(\u03b51+\u03b52) blocks e as well. For e \u2208 C \\ B, we have B\u2265\u00b5(e)\u2212\u03b52 blocks e, by Lemma 3.3. Set r = \u00b5(e) \u2212 (\u03b51 + \u03b52). Using Lemma 3.1(3) with Dr = (B \\ I)\u2265\u00b5(e)\u2212\u03b52 \u222a I\u2265\u00b5(e)\u2212(\u03b51+\u03b52), we can see that I\u2265\u00b5(e)\u2212(\u03b51+\u03b52) is a basis in MDr . Clearly B\u2265\u00b5(e)\u2212\u03b52 \u2286 Dr. So Dr blocks e, which implies I\u2265\u00b5(e)\u2212(\u03b51+\u03b52) blocks e. Hence, by Lemma 3.1, I is (\u03b51 + \u03b52)-optimal for MC ."}, {"heading": "3.2 Na\u0308\u0131ve Uniform Sampling Algorithm", "text": "We start with a na\u0308\u0131ve uniform sampling algorithm, which samples each arm enough times to ensure that with high probability the empirical means are all within \u03b5/2 from the true means, and then outputs the optimal solution with respect to the empirical means. The algorithm is a useful procedure in our final algorithm.\nAlgorithm 2: Na\u0308\u0131ve-I (S, \u03b5, \u03b4) Data: A PAC-Basis instance S = (S,M), with rank(M) = k, approximation error \u03b5,\nconfidence level \u03b4. Result: A basis I in M.\n1 \u00b5\u0302 \u2190 UniformSample(S, \u03b5/2, \u03b4/|S|) 2 Return The optimal solution I with respect to the empirical means.\nLemma 3.5. The Na\u0308\u0131ve-I (S, \u03b5, \u03b4) algorithm outputs an \u03b5-optimal solution for S with probability at least 1\u2212 \u03b4. The number of samples is O(|S|\u03b5\u22122 \u00b7 (ln \u03b4\u22121 + ln |S|)).\nProof. By Lemma 2.9 and a simple union bound, we have |\u00b5e \u2212 \u00b5\u0302e| \u2264 \u03b5/2 simultaneously for all arms e \u2208 S with probability 1\u2212 \u03b4. Conditioning on that event, let I be the returned basis. For an arm e 6\u2208 I, we have I\u2265\u00b5\u0302e\u00b5\u0302 blocks e. Note that for all arm a \u2208 I, if \u00b5\u0302a \u2265 \u00b5\u0302e, we must have \u00b5a \u2265 \u00b5e\u2212\u03b5. Hence, I\u2265\u00b5\u0302e\u00b5\u0302 \u2286 I \u2265\u00b5e\u2212\u03b5 \u00b5 . So I \u2265\u00b5e\u2212\u03b5 \u00b5 blocks e. Then we have I is \u03b5-optimal by Lemma 3.1. The sample complexity follows from the algorithm statement."}, {"heading": "3.3 Sampling and Pruning", "text": "Our optimal PAC algorithm applies the sampling and pruning technique, initially developed in the celebrated work of Karger, Klein and Tarjan [KKT95]. They used the technique to obtain an expected linear-time algorithm for computing the minimum spanning tree.\nWe first describe the high level idea from [KKT95], which will be instructive for our later development. Suppose we want to find the maximum spanning tree (MST). We first construct a subgraph F by sampling each edge with probability p; this subgraph may not be connected, so we solve the maximum-weight spanning forest I of F . The key idea is this: we can use I to prune a lot \u201cuseless\u201d\nedges in the original graph. Formally, an edge e = (u, v) is useless if edges with larger cost in I can connect u and v: this is because the cheapest edge in a cycle does not belong to the MST). (In other words, e is useless if it is blocked by I>\u00b5(e).) Having removed these useless edges, we again recurse on the remaining graph, which now has much fewer edges, to find the MST. A crucial ingredient of the analysis in [KKT95] is to show that I can indeed prune a lot of edges.\nA proof from [KKT95, Kar98] or [MR10, pp. 299-300] shows that an optimal solution from a random subset can help us prune a substantial amount of elements.\nLemma 3.6. ([KKT95, Lemma 2.1 and Remark 2.3]) Given a matroid M = (S,I) with an injective cost function \u00b5 : S \u2192 R+, sample a subset F of S by selecting each element independently with probability p. An element e \u2208 S is called F -good if F>\u00b5(e) does not block e, else it is F -bad. If the r.v. X denotes the number of F -good elements in S, then X is stochastically dominated by NegBin(rank(M); p).\nWe also introduce a lemma which shows an \u03b5-optimal solution I in F can be used to eliminate some sub-optimal arms.\nLemma 3.7. For a matroid M = (S,I) with cost function \u00b5 : S \u2192 R+, Let F \u2286 S be a subset, and I be an \u03b1-optimal basis for MF for some \u03b1 > 0. If an element e \u2208 S \\ I is F -bad, I\u2265\u00b5(e)\u2212\u03b1 blocks e.\nProof. As e is F -bad, F\u2265\u00b5(e) blocks e. Let r = \u00b5(e)\u2212 \u03b1, and\nD = (F \\ I)\u2265r+\u03b1 \u222a I\u2265r = (F \\ I)\u2265\u00b5(e) \u222a I\u2265\u00b5(e)\u2212\u03b1.\n(In other words, we first add \u03b1 to the cost of every element in I, then consider all element with cost at least \u00b5(e) in F ). Then by Lemma 2.8(4) and the fact I is \u03b1-optimal for MF , I\u2265\u00b5(e)\u2212\u03b1 is maximal for MD (in fact, it is optimal for MD w.r.t. the modified cost function). Clearly F\u2265\u00b5(e) \u2286 D, so D blocks e as well. Hence I\u2265\u00b5(e)\u2212\u03b1 also blocks e, by Lemma 2.6."}, {"heading": "3.4 Our Optimal PAC Algorithm", "text": "Now, we present our algorithm for the PAC case, which is based on the sampling-and-pruning technique discussed above. Let p = 0.01. The algorithm runs as follows: If the number of arms |S| is sufficiently small, we simply run the na\u0308\u0131ve uniform sampling algorithm. Otherwise, we sample a subset F of S by selecting each arm with probability p independently, and recurse on the subinstance SF = (F,MF ) to find an \u03b1-optimal solution I, where \u03b1 = \u03b5/3. Next, we uniformly sample each arm in S by calling UniformSample (S, \u03bb, \u03b4 \u00b7p/8k), where \u03bb = \u03b5/12. Then, we use I to eliminate those sub-optimal arms in S \\ I. More precisely, a sub-optimal arm e is blocked by the arms of I with empirical values larger than \u00b5\u0302e \u2212 \u03b1\u2212 2\u03bb. Finally, we invoke the algorithm recursively on the remaining arms to find an \u03b1-optimal solution, which we output as the final result. The pseudo-code can be found in Algorithm 3.\nNote that UniformSample (step 6) is the only step in which we take samples from the arms. Also note that in both recursive calls we set the approximation error to be \u03b1 = \u03b5/3. Effectively, this makes sure that an arms surviving in deeper recursive call are sampled more times. This feature is shared by other elimination-based method, such as [EDMM02, ZCL14]. However, the way we choose which arms should be eliminated is quite different.\nAlgorithm 3: PAC-SamplePrune (S, \u03b5, \u03b4) Data: A PAC-Basis instance S = (S,M), with rank(M) = k, approximation error \u03b5,\nconfidence level \u03b4. Result: A basis I in M.\n1 if |S| \u2264 2p\u22122 \u00b7max(4 \u00b7 ln 8\u03b4\u22121, k) then 2 Return Na\u0308\u0131ve-I (S, \u03b5, \u03b4) 3 4 Sample a subset F \u2286 S by choosing each element with probability p independently. 5 \u03b1 \u2190 \u03b5/3, \u03bb \u2190 \u03b5/12 6 I \u2190 PAC-SamplePrune(SF = (F,MF ), \u03b1, \u03b4/8) 7 \u00b5\u0302 \u2190 UniformSample (S, \u03bb, \u03b4 \u00b7 p/8k) 8 S\u2032 \u2190 I \u222a {e \u2208 S \\ I | I\u2265\u00b5\u0302e\u2212\u03b1\u22122\u03bb\u00b5\u0302 does not block e} 9 Return PAC-SamplePrune (SS\u2032 = (S\u2032,MS\u2032), \u03b1, \u03b4/4)"}, {"heading": "3.5 Analysis of the sample complexity", "text": "In this subsection, we analyze PAC-SamplePrune and prove Theorem 1.7.\nTheorem 1.7 (rephrased)Given a PAC-Basis instance S = (S,M), Algorithm PAC-SamplePrune (S, \u03b5, \u03b4) returns an \u03b5-optimal solution, with probability at least 1\u2212 \u03b4, and uses at most\nO(n\u03b5\u22122 \u00b7 (ln k + ln \u03b4\u22121))\nsamples. Here k = rank(M), and n = |S|. Let c1, c2 be two constants to be specified later. We will prove by induction on |S| that with probability at least 1\u2212 \u03b4, PAC-SamplePrune (S = (S,M), \u03b5, \u03b4) returns an \u03b5-optimal solution, using at most c1 \u00b7 (|S|\u03b5\u22122 \u00b7 (ln k + ln \u03b4\u22121 + c2)) samples. Remember that p = 0.01. We first consider the simple case where |S| is not much larger than k. When |S| \u2264 2p\u22122 \u00b7max(4 \u00b7 ln 8\u03b4\u22121, k), we have that ln |S| = O(ln \u03b4\u22121+ln k). So the number of samples of Na\u0308\u0131ve-I is O(|S|\u03b5\u22122 \u00b7 (ln |S| + ln \u03b4\u22121)) = O(|S|\u03b5\u22122 \u00b7 (ln k + ln \u03b4\u22121)); by Lemma 3.5, the returned basis is \u03b5-optimal with probability at least 1\u2212 \u03b4. Hence the theorem holds in this case. Now consider the case where |S| > 2p\u22122 \u00b7 max(4 \u00b7 ln 8\u03b4\u22121, k), and inductively assume that the theorem is true for all instances of size smaller than |S|. We first need the following lemma, which describes the good events that happen with high probability.\nLemma 3.8. Let O be the unique optimal solution for S = (S,M). With probability at least 1\u2212 \u03b4/2, the following statements hold simultaneously.\n1. |F | \u2264 2p \u00b7 |S| (F is obtained in Line 3). 2. There are at most p \u00b7 |S| F -good elements in S. 3. |\u00b5e \u2212 \u00b5\u0302e| \u2264 \u03bb, for all elements e \u2208 O \u222a I (I is obtained in Line 5). 4. I is an \u03b1-optimal solution for F .\nProof. Let n = |S|. By Corollary A.2, we have that\nPr[|F | > 2pn] = Pr[Bin(n, p) > 2pn] \u2264 e\u2212pn/3 \u2264 \u03b4/8,\nfor n \u2265 8p\u22122(ln 8\u03b4\u22121). Moreover, let X be the r.v. denoting the number of F -good elements in S. By Lemma 3.6, X is dominated by NegBin(k; p), and hence\nPr[X > pn] \u2264 Pr[NegBin(k; p) > pn] = Pr[Bin(pn, p) < k] \u2264 Pr[Bin(pn, p) < 1 2 p2n] \u2264 e\u2212 18p2n \u2264 \u03b4/8.\nThe second inequality holds since p2n \u2265 2k, while the last inequality is due to 18p2n \u2265 ln \u03b4\u22121+ln 8. In addition, by Lemma 2.9 and a trivial union bound over all arms in O \u222a I, the third statement holds with probability at least 1\u2212 (p \u00b7 \u03b4/8k) \u00b7 (2k) \u2265 1\u2212 \u03b4/8. Finally, conditioning on the first statement, we have |F | < |S|, and hence by the induction hypothesis, with probability at least 1\u2212 \u03b4/8, I is an \u03b1-optimal solution for F . Putting them together, all four statements hold with probability at least 1\u2212 \u03b4/8 \u00b7 4 = 1\u2212 \u03b4/2.\nNow let E denote the event that all statements in Lemma 3.8 hold. We show each F -bad element in S \\ I has a constant probability to be eliminated.\nLemma 3.9. Conditioning on E, for an F -bad element e \u2208 S \\ I, Pr[e \u2208 S\u2032] \u2264 \u03b4 \u00b7 p/8k.\nProof. Conditioning on E , I is \u03b1-optimal for F . Hence, by Lemma 3.7, for an F -bad element e \u2208 S \\ I, I\u2265\u00b5e\u2212\u03b1\u00b5 blocks e. By Lemma 3.5, |\u00b5\u0302e \u2212 \u00b5e| \u2264 \u03bb with probability 1 \u2212 p \u00b7 \u03b4/8k. Moreover, conditioning on E , we have |\u00b5\u0302a \u2212\u00b5a| \u2264 \u03bb for every element a \u2208 I (by Lemma 3.8.3). Consequently, I\u2265\u00b5e\u2212\u03b1\u00b5 \u2286 I\u2265\u00b5\u0302e\u2212\u03b1\u22122\u03bb\u00b5\u0302 , which implies that I \u2265\u00b5\u0302e\u2212\u03b1\u22122\u03bb \u00b5\u0302 blocks e. By the definition of S\n\u2032, this means e /\u2208 S\u2032. Hence, we have Pr[e \u2208 S\u2032 | E ] \u2264 \u03b4 \u00b7 p/8k.\nNow, we show that with high probability, S\u2032 is a 2\u03b5/3-approximate subset of S, and the size of S\u2032 is much smaller than |S|.\nLemma 3.10. Conditioning on E, |S\u2032| \u2264 2p|S|, and S\u2032 is an (\u03b1 + 4\u03bb)-approximate subset of S, with probability 1\u2212 \u03b4/4.\nProof. Conditioned on event E , there are at most p \u00b7 |S| F -good elements in S (by Lemma 3.8.2). If X denotes the number of F -bad elements in S \\ I which remain in S\u2032, Lemma 3.9 implies E[X] \u2264 \u03b4 \u00b7 (p/8k) \u00b7 |S \\ I| \u2264 \u03b4 \u00b7 p/8 \u00b7 |S|. By Markov\u2019s inequality, we have Pr[X \u2265 0.5p|S|] \u2264 Pr[X \u2265 4 \u00b7 \u03b4\u22121E[X]] \u2264 \u03b4/4. So there are at most |I| + 1.5p \u00b7 |S| \u2264 k + 1.5p|S| \u2264 2p|S| elements in S\u2032 with probability at least 1\u2212 \u03b4/4. For the second part, observe that O \u222a S\u2032 \u2286 S is a 0-approximate subset of S, so by Lemma 3.4, it suffices to show S\u2032 is an (\u03b1 + 4\u03bb)-approximate subset for O \u222a S\u2032. Still conditioned on event E , for all arms e \u2208 I \u222a O, we have |\u00b5e \u2212 \u00b5\u0302e| \u2264 \u03bb. So for an arm e \u2208 O \\ S\u2032, we have I\u2265\u00b5\u0302e\u2212\u03b1\u22122\u03bb\u00b5\u0302 blocks e (otherwise, e should be included in S\u2032), which implies I\u2265\u00b5e\u2212\u03b1\u22124\u03bb\u00b5 blocks e. Since I \u2286 S\u2032, we can see S\u2032 is an (\u03b1+ 4\u03bb)-approximate subset of S\u2032 \u222aO by Definition 3.2.\nFinally, we are ready to prove Theorem 1.7.\nProof of Theorem 1.7. Let EG be the intersection of the event E , the event that Lemma 3.10 holds and the event that PAC-SamplePrune (line 8) outputs correctly. Conditioning on event E , |S\u2032| < |S|, so by the induction hypothesis, the last event happens with probability at least 1 \u2212 \u03b4/4. Hence, Pr[EG] \u2265 1\u2212 \u03b4/2 \u2212 \u03b4/4 \u2212 \u03b4/4 = 1\u2212 \u03b4. We condition our following argument on EG. First we show the algorithm is correct. By Lemma 3.10, S\u2032 is an (\u03b1 + 4\u03bb)-approximate subset of S, and the returned basis J is an \u03b1-optimal solution of S\u2032, hence also an \u03b1-approximate subset of\nS\u2032. By the \u201ctransitivity\u201d property of Lemma 3.4, J is an (\u03b1 + \u03b1 + 4\u03bb)-approximate subset of S. This is an \u03b5-optimal solution of S since \u03b1+ \u03b1+ 4\u03bb = \u03b5.\nBy Lemma 3.8 and Lemma 4.1, we have |F | \u2264 2p \u00b7 |S| and |S\u2032| \u2264 2p \u00b7 |S|. By the induction hypothesis, the total number of samples in both recursive calls (line 6 and line 8) can be bounded by c1 \u00b7 4p|S|\u03b5\u22122(ln \u03b4\u22121 + ln k + c2) \u00b7 9 \u2264 36c1p \u00b7 |S|\u03b5\u22122(ln \u03b4\u22121 + ln k + c2). The number of samples incurred by UniformSample (line 7) can be bounded by\n|S|\u03bb\u22122 \u00b7 (ln \u03b4\u22121 + ln 16 + ln p\u22121 + ln k)/2 \u2264 72|S|\u03b5\u22122 \u00b7 (ln \u03b4\u22121 + ln 16 + ln p\u22121 + ln k).\nNow, let c2 = ln 16 + ln p \u22121, which is a constant. Then the total number of samples is bounded by\n(36p \u00b7 c1 + 72)|S|\u03b5\u22122 \u00b7 (ln \u03b4\u22121 + ln k + c2).\nSetting c1 = max(120, c0), and plugging in p = 0.01, we can see the above quantity is bounded by c1 \u00b7 |S|\u03b5\u22122 \u00b7 (ln \u03b4\u22121 + ln k + c2), which completes the proof.\nAlgorithm 4: Exact-ExpGap (S, \u03b5, \u03b4) Data: An Exact-Basis instance S = (S,M), with rank(M) = k, approx. error \u03b5,\nconfidence level \u03b4. Result: A basis I in M.\n1 relim \u2190 1, rsele \u2190 1 2 while True do 3 Scur \u2190 the arm set of Mcur 4 nopt \u2190 rank(Mcur), nbad \u2190 |Scur| \u2212 nopt 5 if nopt \u2264 nbad then 6 if nopt = 0 then break 7 r \u2190 relim 8 \u03b5r \u2190 2\u2212r/4, \u03b4r \u2190 \u03b4/100r3, relim \u2190 relim + 1 9 I \u2190 PAC-SamplePrune(Scur = (Scur,Mcur), \u03b5r, \u03b4r)\n10 \u00b5\u0302 \u2190 UniformSample(I, \u03b5r/2, \u03b4r/nopt) 11 \u00b5\u0302 \u2190 UniformSample(Scur \\ I, \u03b5r, \u03b4r/nopt) 12 Snew \u2190 I \u222a {e \u2208 Scur \\ I | I\u2265\u00b5\u0302e+1.5\u03b5r\u00b5\u0302 does not block e in Mcur} 13 Mcur \u2190 Mcur|Snew 14 else 15 if nbad = 0 then 16 Ans \u2190 Ans \u222a Scur 17 break 18 r \u2190 rsele 19 \u03b5r \u2190 2\u2212r/4, \u03b4r \u2190 \u03b4/100r3, rsele \u2190 rsele + 1 20 \u00b5\u0302 \u2190 UniformSample(Scur, \u03b5r, \u03b4r/|Scur|) 21 U \u2190 {e \u2208 Scur | (Scur \\ {e})\u2265\u00b5\u0302e\u22122\u03b5r\u00b5\u0302 does not block e in Mcur} 22 Ans \u2190 Ans \u222a U 23 Mcur \u2190 Mcur/U 24\n25 Return Ans"}, {"heading": "4 An Algorithm for the Exact-Basis Problem", "text": "We now turn to the Exact-Basis problem, and prove Theorem 1.4. If we denote the unique optimal basis by OPT, and let BAD be the set of all other arms in S \\OPT, our goal for the Exact-Basis problem is to find this set OPT with confidence 1\u2212 \u03b4 using as few samples as possible. Our algorithm Exact-ExpGap is based on our previous PAC result for PAC-Basis, and also borrow some idea from the Exponential-Gap-Eliminating algorithm by [KKS13]. It will run in rounds. In each round, it either tries to eliminate some arms in BAD (we call such a round an eliminationround), or adds some arms from OPT into our solution and removes them from further consideration (we call such a round a selection-round). Let us give some details about these two kinds of rounds. Let Mcur be the current matroid defined over the remaining arms, nopt be the number of remaining arms in OPT, and nbad be the number of remaining arms in BAD.\n1. (elimination-round) When nopt \u2264 nbad, we are in an elimination-round. In the rth eliminationround, first we find an \u03b5r-optimal solution I for the current matroid Mcur by calling PACSamplePrune (Mcur, \u03b5r, \u03b4r) (i.e., the PAC algorithm from Section 3) with \u03b5r = 2\u2212r/4 and \u03b4r = \u03b4/100r\n3. We sample each arm in I by calling UniformSample(I, \u03b5r/2, \u03b4r/nopt) to estimate their means. We do the same for arms Scur \\ I by calling UniformSample(Scur \\ I, \u03b5r, \u03b4r/nopt). Note the confidence parameter is not low enough to give accurate estimations for all arms in Scur \\ I with high probability: that would require us reducing the parameter to \u03b4r/|Scur \\ I|. However, we will be satisfied with being accurate only for arms in OPT \\ I with probability \u03b4r.\nFinally, we use I to eliminate some sub-optimal arms. In particular, an arm e should be eliminated if I\u2265\u00b5\u0302e+1.5\u03b5r\u00b5\u0302 blocks e, where \u00b5\u0302 is the cost function defined by the empirical means obtained from the above UniformSample procedures.\n2. (selection-round) When nbad < nopt, we are in a selection-round. In the r th selection-round,\nwe sample all the arms in Mcur by calling UniformSample(Scur, \u03b5r, \u03b4r/|Scur|). We then select into our solution Ans those elements e which are not blocked by all other elements in Mcur with larger empirical means, even if we slightly decrease e\u2019s empirical mean by 2\u03b5r. Having contracted these selected arms, we proceed to the next round.\nFinally, the algorithm terminates when either nopt = 0 or nbad = 0. The pseudo-code is given as Algorithm 4."}, {"heading": "4.1 Analysis of the algorithm", "text": "Now, we prove the main theorem of this section by analyzing the correctness and sample complexity of Exact-ExpGap.\nTheorem 1.4 (rephrased) Given an Exact-Basis instance S(S,M), Exact-ExpGap(S, \u03b5, \u03b4) returns the optimal basis of M, with probability at least 1\u2212 \u03b4, and uses at most\nO\n( \u2211\ne\u2208S\n\u2206\u22122e (ln \u03b4 \u22121 + ln k + ln ln\u2206\u22121e )\n)\nsamples. Here, k = rank(M) is the size of a basis of M.\nWe first recall that the gap of an element e (throughout this section, we only consider the cost function \u00b5 for gap) is defined to be\n\u2206Me :=   \nOPT(M)\u2212 OPT(MS\\{e}), e \u2208 OPT and e is not isolated; +\u221e, e is isolated; OPT(M)\u2212 OPT(M/{e})\u2212 \u00b5e, e 6\u2208 OPT.\nNote that we extend the definition to the isolated elements, since the restrictions and contractions may result in such element (note that no loop is introduced during the process). We also need the following equivalent definition (the equivalence follows from Lemma 2.8), which may be convenient in some case:\n\u2206Me := { max{w \u2208 R | (S \\ {e})>\u00b5e\u2212w does not block e} for e \u2208 OPT; max{w \u2208 R | S\u2265\u00b5e+w blocks e} for e 6\u2208 OPT\n(1)\nFirst, we prove that our algorithm returns the optimal basis with high probability. In the following lemma, We specify a few events on which we condition our later discussion.\nLemma 4.1. With probability at least 1\u2212 \u03b4/5, all of the following statements hold: 1. In all elimination-rounds, PAC-SamplePrune (line 9) returns correctly. 2. In all elimination-rounds, for all element u \u2208 I, |\u00b5u \u2212 \u00b5\u0302u| < \u03b5r/2. 3. In all elimination-rounds, for all element u \u2208 OPT(Mcur), |\u00b5u \u2212 \u00b5\u0302u| < \u03b5r. 4. In all selection-rounds, for all element u \u2208 Scur, |\u00b5u \u2212 \u00b5\u0302u| < \u03b5r.\nWe use E to denote the event that all above statements are true.\nProof. In the rth elimination-round, the specification of the failure probabilities of PAC-SamplePrune and UniformSample imply the first three statements hold with probability 1 \u2212 3\u03b4r. In the rth selection-round, the last statement holds with probability 1 \u2212 \u03b4r. A trivial union bound over all rounds gives\nPr[\u00acE ] \u2264 +\u221e\u2211\nr=1\n(3\u03b4r + \u03b4r) =\n+\u221e\u2211\nr=1\n4\u03b4/100r3 \u2264 \u03b4/5,\nand the lemma follows immediately.\nLemma 4.2. Conditioning on E, the subset Ans returned by the algorithm is the optimal basis OPT.\nProof. We condition on E in the following discussion. We show that the algorithm only deletes arms from BAD in every elimination-round, and it only selects arms from OPT in every selection-round. We say a round is correct if it satisfies the above requirements. We prove all rounds are correct by induction. Consider a round, and suppose all previous rounds are correct. Hence, at the beginning of the current round, Ans clearly is a subset of OPT, and OPT(Mcur) = OPT \\Ans. There are two cases:\nIf the current round is an elimination-round, consider an arm u \u2208 OPT(Mcur) = OPT\\Ans. We can see that I>\u00b5u\u00b5 does not block u in Mcur, by the characterization of the matroid optimal solutions in Lemma 2.8.2. Since |\u00b5\u0302u \u2212\u00b5u| < \u03b5r for all u \u2208 OPT(Mcur), and |\u00b5\u0302e \u2212\u00b5e| < \u03b5r/2 for all e \u2208 I, we also have I\u2265\u00b5\u0302u+1.5\u03b5r\u00b5\u0302 does not block u in Mcur. Hence u \u2208 Snew, and it is not eliminated.\nNext, consider a selection-round and an arm u \u2208 BAD \u2229 Scur. By the induction hypothesis, in the beginning of the round, u 6\u2208 OPT(Mcur). Then, we can see u is blocked by (Scur \\ {u})\u2265\u00b5u\u00b5 in Mcur by Lemma 2.8.3. Again, for all arms e in Scur, |\u00b5e \u2212 \u00b5\u0302e| < \u03b5r, so u is also blocked by (Scur \\ {u})\u2265\u00b5\u0302u\u22122\u03b5r\u00b5\u0302 in Mcur. Hence u 6\u2208 U , and is not selected into Ans. Finally, if the algorithms returns, we have |Ans| = |OPT| = rank(M). Since Ans \u2286 OPT, it must be the case that Ans = OPT."}, {"heading": "4.1.1 Analysis of Sample Complexity", "text": "To analyze the sample complexity, we need some additional notation. Let nropt (resp. n r bad) denote nopt (resp. nbad) at the beginning of r th elimination-round (resp. selection-round). Also, let Srelim denote the arm set of Mcur at the end of the rth elimination-round, and Srsele denote the arm set of Mcur at the end of the rth selection-round. We partition the arms in OPT and BAD based on their gaps, as follows:\nOPTs = {u \u2208 OPT | 2\u2212s \u2264 \u2206u < 2\u2212s+1}, BADs = {u \u2208 BAD | 2\u2212s \u2264 \u2206u < 2\u2212s+1}.\nMoreover, we define OPTr,s := S r sele \u2229 OPTs, i.e., the set of arms in OPTs not selected in the rth selection-round\u2014recall that in a selection-round we aim to select those arms into OPT. Similarly, define BADr,s := S r elim \u2229 BADs as the set of arms in BADs not eliminated the rth eliminationround\u2014again, in an elimination-round we aim to delete those arms in BAD.\nVery roughly speaking, the sth round is dedicated to deal with those arms with gap roughly O(2\u2212s) (namely, an arm in OPTs is likely to be selected in the s\nth selection-round and an arm in BADs is likely to be eliminated in the sth elimination-round). Now, we prove a crucial lemma, which states that all elements in OPTs should be selected in or before the s\nth selection-round, and the number of remaining elements in BADs should drop exponentially after the s th elimination-round.\nLemma 4.3. Conditioning on event E, with probability at least 1\u2212 4\u03b4/5, we have\n|OPTr,s| = 0 and |BADr,s| \u2264 1\n8 |BADr\u22121,s| for all 1 \u2264 s \u2264 r.\nProving Lemma 4.3 requires some preparations. All the following arguments are conditioned on event E . We first prove a useful lemma which roughly states that if we select some elements in OPT and remove some elements in BAD, the gap of the remaining instance does not decrease.\nLemma 4.4. For two subsets A,B of S such that A \u2286 OPT \u2286 B, consider the matroid M\u2032 = (M|B)/A. Let S\u2032 be its ground set. For all element u \u2208 S\u2032, we have that \u2206M\u2032u \u2265 \u2206Mu .\nProof. By the definition of matroid contraction and the fact that A is independent, we can see for any subset U \u2286 S\u2032, U is independent in M\u2032 iff U \u222a A is independent in M. We also have rankM\u2032(S\n\u2032) = rankM(S)\u2212 |A| and OPT \\A is the unique optimal solution for M\u2032. Now, let u \u2208 S\u2032. Suppose u \u2208 OPT(M\u2032) = OPT \\ A. Suppose for contradiction that \u2206M\u2032u < \u2206Mu . Then we have a basis I contained in S\u2032 \\ {u} in M\u2032 such that\n\u00b5(I) = \u00b5(OPT \\ A)\u2212\u2206M\u2032u > \u00b5(OPT \\ A)\u2212\u2206Mu .\nBut this means I \u222aA is a basis contained in S \\ {u} in M such that \u00b5(I \u222aA) > OPT(M) \u2212\u2206Mu , contradicting to the definition of \u2206Mu . Note that a non-isolated element in M may become isolated in M\u2032 (for which \u2206M\u2032u = +\u221e). Then, we consider the case u 6\u2208 OPT(M\u2032) = OPT \\ A. The argument is quite similar. Suppose for contradiction that \u2206M \u2032\nu < \u2206 M u . This means that there exists a basis I in M\u2032 such that u \u2208 I and\n\u00b5(I) > OPT(M\u2032) \u2212 \u2206Mu = \u00b5(OPT \\ A) \u2212 \u2206Mu . But this means A \u222a I is a basis in M such that \u00b5(A \u222a I) > OPT\u2212\u2206Mu . Since u \u2208 (A \u222a I), this contradicts the definition of \u2206Mu .\nProof of Lemma 4.3. We first prove |OPTr,s| = 0 for r \u2265 s. Suppose we are at the beginning of the rth selection-round. Let A = Ans and B = Scur \u222a Ans. We can see A \u2286 OPT \u2286 B and Mcur = (M|B)/A. For any arm u \u2208 OPTr\u22121,s such that s \u2264 r, we have \u2206u \u2265 2\u2212s \u2265 2\u2212r \u2265 4\u03b5r. By Lemma 4.4, we have \u2206Mcuru \u2265 \u2206Mu \u2265 4\u03b5r, which means (Scur \\ {u})>\u00b5u\u22124\u03b5r\u00b5 does not block u. Note that conditioning on E , |\u00b5e \u2212 \u00b5\u0302e| < \u03b5r for all e \u2208 Scur. This implies that (Scur \\ {u})\u2265\u00b5\u0302u\u22122\u03b5r\u00b5\u0302 does not block u as well. So u \u2208 U (U is defined in line 21) and consequently |OPTr,s| = 0. Now, we prove the second part of the lemma. We claim that for 1 \u2264 s \u2264 r, we have that\nPr[|BADr,s| \u2264 1\n8 |BADr\u22121,s|] \u2265 1\u2212 8\u03b4r (2)\nThe lemma follows directly from the claim by taking a union bound over all s, r such that 1 \u2264 s \u2264 r: +\u221e\u2211\nr=1\nr\u2211\ns=1\n8\u03b4r =\n+\u221e\u2211\nr=1\n8\u03b4/100r2 \u2264 4\u03b4/5.\nWhat remains to prove is the claim (Inequality (2)). Suppose we are at the beginning of the rth elimination-round. Let A = Ans and B = Scur \u222a Ans. Conditioning on event E , we can see that A \u2286 OPT \u2286 B and Mcur = (M|B)/A. For any arm u \u2208 BADr\u22121,s such that s \u2264 r, we have \u2206u \u2265 2\u2212s \u2265 2\u2212r \u2265 4\u03b5r. By Lemma 4.4, we have \u2206Mcuru \u2265 \u2206Mu \u2265 4\u03b5r. So (Scur)\u2265\u00b5u+4\u03b5r\u00b5 blocks u in Mcur by the definition of \u2206Mcuru . As I is \u03b5r-optimal for Mcur, we also have u is blocked by I\u2265\u00b5u+3\u03b5r\u00b5 in Mcur. This implies that u /\u2208 I. Since we have |\u00b5u \u2212 \u00b5\u0302u| < \u03b5r with probability 1\u2212 \u03b4r, combining with the fact that |\u00b5e \u2212 \u00b5\u0302e| < \u03b5r/2 for all e \u2208 I (guaranteed by E), u is blocked by I>\u00b5\u0302u+1.5\u03b5r\u00b5\u0302 with probability 1 \u2212 \u03b4r. This implies that u 6\u2208 BADr,s (u should be eliminated in line 12). From the above, we can see that\nE[|BADr,s|] \u2264 \u03b4r|BADr\u22121,s|.\nBy Markov inequality, we have Pr[|BADr,s| \u2265 18 |BADr\u22121,s|] \u2264 8\u03b4r, which concludes the proof.\nFinally, everything is in place to prove Theorem 1.4.\nProof of Theorem 1.4. Let EG be the intersection of event E and the event that Lemma 4.3 holds. By Lemma 4.1 and Lemma 4.3, Pr[EG] \u2265 1\u2212 \u03b4. Now we condition on this event. The correctness has been proved in Lemma 4.2. We only need to bound the sample complexity of Exact-ExpGap.\nWe first consider the number samples taken by the UniformSample procedure. We handle the samples taken by PAC-SamplePrune later. Now, we bound the total number of samples taken from\narms in OPTs in all selection-round s. Notice that we can safely ignore all samples on arms in BAD since nopt \u2265 nbad. By Lemma 4.3, |OPTr,s| = 0 for r \u2265 s. So it can be bounded as:\nO\n( s\u2211\nr=1\n|OPTr\u22121,s| \u00b7 (lnnopt + ln \u03b4\u22121r )\u03b5\u22122r\n) \u2264 O ( s\u2211\nr=1\n|OPTs| \u00b7 (ln k + ln \u03b4\u22121 + ln r)\u03b5\u22122r\n)\n\u2264 O ( |OPTs| \u00b7 (ln k + ln \u03b4\u22121 + ln s) \u00b7 4s ) .\nNext, we consider the number of samples from elimination-round s. In an elimination-round, since nopt \u2264 nbad, we only need to bound the number of samples from BAD. The total number of samples taken from arms in BADs in the first s elimination-rounds can be bounded as:\nO\n( s\u2211\nr=1\n|BADr\u22121,s| \u00b7 (ln |Scur|+ ln \u03b4\u22121r )\u03b5\u22122r\n) \u2264 O ( |BADs| \u00b7 (ln k + ln \u03b4\u22121 + ln s) \u00b7 4s )\nThe inequality holds since |Scur| \u2264 nopt + nbad \u2264 2nopt \u2264 2k in a selection-round. Now, we bound the number of samples from the remaining rounds. Since |BADr,s| \u2264 18 |BADr\u22121,s| when r \u2265 s, we have:\nO\n( +\u221e\u2211\nr=s+1\n|BADr\u22121,s| \u00b7 (ln |Scur|+ ln \u03b4\u22121r )\u03b5\u22122r\n) = O ( +\u221e\u2211\nr=s+1\n1\n8r\u2212s \u00b7 |BADs| \u00b7 (ln k + ln \u03b4\u22121 + ln r)\u03b5\u22122r\n)\n= O ( |BADs| \u00b7 (ln k + ln \u03b4\u22121 + ln s) \u00b7 4s )\nPutting them together, we can see the number of samples incurred by UniformSample is bounded by:\nO\n( +\u221e\u2211\ns=1\n(|BADs|+ |OPTs|) \u00b7 (ln k + ln \u03b4\u22121 + ln s) \u00b7 4s ) ,\nwhich simplifies to O (\u2211\ne\u2208S \u2206 \u22122 e (ln k + ln \u03b4 \u22121 + ln ln\u2206\u22121e ) ) . Finally, we consider the number of\nsamples taken by PAC-SamplePrune. Noticing nopt = rank(Mcur), the number of samples is O(|Scur|(ln nopt + ln \u03b4\u22121r )\u03b5\u22122r ). So PAC-SamplePrune does not affect the sample complexity, and we finish our proof."}, {"heading": "5 An Algorithm for PAC-Basis-Avg", "text": "In this section we prove Theorem 1.9 by providing an algorithm with the desired sample complexity."}, {"heading": "5.1 Building Blocks", "text": ""}, {"heading": "1. Na\u0308\u0131ve Uniform Sampling Algorithm:", "text": "We first investigate how many samples the uniform sampling algorithm needs in order to find an average-\u01eb-optimal solution. The uniform sampling procedure in Na\u0308\u0131ve-II has a different parameter\nfrom that in Na\u0308\u0131ve-I.\nAlgorithm 5: Na\u0308\u0131ve-II (S, \u03b5, \u03b4) Data: A PAC-Basis-Avg instance S = (S,M), with rank(M) = k, approximation error \u03b5,\nconfidence level \u03b4. Result: A basis in M.\n1 Sample each arm e \u2208 S for Q0 = 2\u03b5\u22122 \u00b7 ln 2(|S| k )\n\u03b4 /k times. Let \u00b5\u0302e be its empirical mean. 2 Return The optimal solution I with respect to the empirical means.\nThe following lemma shows the performance of the above algorithm. The proof is fairly standard and can be found in the appendix.\nLemma 5.1. Let I be the output of Na\u0308\u0131ve-II(S, \u03b5, \u03b4). With probability 1\u2212 \u03b4, I is average-\u01eb-optimal for S. The total number of samples is at most\nO ( |S| \u00b7 ( ln\n|S| k + ln \u03b4\u22121 k\n) \u03b5\u22122 ) ."}, {"heading": "2. Elimination Procedure:", "text": "The following procedure Elimination is our main ingredient. Roughly speaking, it can help us to eliminate a constant fraction of the remaining arms while preserving the value of the optimal solution.\nThe idea of the procedure is similar to PAC-SamplePrune in Section 3. It runs as follows. It first samples a random subset F by choosing each element with probability p independently. Then it finds a \u01eb/5-optimal solution I for F using PAC-SamplePrune. Afterwards, we estimate the means of all arms in I by calling UniformSample(I, \u03b1, \u03b4/6k). This guarantees the additive errors for all arms in I are at most \u03b1 = \u03b5/5, with probability at least 1\u2212 \u03b4/6. Then we take a uniform number Q0 = \u03b2\n\u22122max(ln 6\u03b4/k, ln 200/2) of samples from the other arms. Next, we use I to eliminate those sub-optimal arms in S \\ I based on their empirical means. More precisely, an arm e should be eliminated if it is blocked by the arms of I with empirical means larger than the empirical mean of e minus 3\u03b5/5. The pseudo-code can be found in Algorithm 6.\nAlgorithm 6: Elimination(S, \u03b5, \u03b4) Data: A PAC-Basis-Avg instance S = (S,M), with rank(M) = k, approximation error \u03b5,\nconfidence level \u03b4. Result: A remained subset S\u2032 \u2286 S.\n1 \u03bb \u2190 \u03b5/5, \u03b1 \u2190 \u03b5/5, \u03b2 \u2190 \u03b5/5 2 Let p \u2190 100\u00b7(k+ln \u03b4\u22121+ln 6)N 3 Sample a random subset F by choosing each element in S with probability p independently. 4 I \u2190 PAC-SamplePrune(SF = (F,MF ), \u03bb, \u03b4/6) 5 \u00b5\u0302 \u2190 UniformSample(I, \u03b1, \u03b4/6k) 6 Sample each arm e \u2208 (S \\ I) for Q0 = \u03b2\u22122 max(ln 6\u03b4/k, ln 200/2) times. Let \u00b5\u0302e be its empirical mean. 7 Return S\u2032 = I \u222a {e \u2208 S \\ I | I\u2265\u00b5\u0302e\u2212\u03bb\u2212\u03b1\u2212\u03b2\u00b5\u0302 does not block e}\nWe first specify the event we condition our analysis on. The proof of the following lemma is almost identical to that for Lemma 3.8 and can be found in the appendix.\nLemma 5.2. With probability at least 1\u2212 2\u03b4/3, the following statements hold: 1. |F | \u2264 2pN = O(k + ln \u03b4\u22121). 2. The number of F -good elements in S is at most 0.04N . 3. I is \u03bb-optimal for F . 4. |\u00b5e \u2212 \u00b5\u0302e| \u2264 \u03b1 for all elements e \u2208 I.\nDenote the event described in the previous Lemma by E . In the following we condition on event E . For notational convenience, we define the average value of a subset.\nDefinition 5.3. Given a matroid M = (S,I) and \u00b5 : S \u2192 R+ be a cost function, for any subset A \u2286 S, define the average value of A to be\nvalM,\u00b5(A) := { OPT\u00b5(MA)/rank(A) rank(A) = rank(S), \u2212\u221e rank(A) < rank(S).\nWhen M or \u00b5 is clear from the context, we omit it for convenience.\nThe following lemma summarizes the properties of Elimination, which roughly says that we can eliminate a significant portion of arms while the value does not drop by much.\nLemma 5.4. Let N = |S|. Suppose N \u2265 100 \u00b7 (k + ln \u03b4\u22121 + ln 6) and Elimination(S, \u03b5, \u03b4) returns S\u2032. With probability at least 1\u2212 \u03b4, the following statements hold:\n1. val(S\u2032) \u2265 val(S)\u2212 \u03b5. 2. |S\u2032| \u2264 0.1|S|. 3. It takes O (( (1 + ln \u03b4\u22121/k)|S| + (k + ln \u03b4\u22121)(ln k + ln \u03b4\u22121) ) \u03b5\u22122 ) samples.\nProof. Let O be the optimal solution of S, and O\u0303 = O \\ I. We prove the first claim by showing val\u00b5(I \u222a (O\u2229S\u2032)) \u2265 val\u00b5(O)\u2212\u03b5 = val\u00b5(S)\u2212\u03b5. Now, we fix the set I found in step 4 (which satisfies E). For each arm u \u2208 O\u0303, set \u2206u = min{w \u2208 R | I\u2265\u00b5u\u2212w\u00b5 blocks u}. Note that for fixed I, \u2206u is a fixed number for each u. As u \u2208 O\u0303 \u2286 O, we must have \u2206u \u2265 0. Let \u03b3 = \u03bb+ 2\u03b1+ \u03b2. For each u \u2208 O, let \u03b7u = max(0,\u2206u \u2212 \u03b3) if u \u2208 O\u0303, otherwise let \u03b7u = 0. Note that \u03b7u is a fixed number, if we fix I. For each u \u2208 O, we also define the random variables Xu as follows:\nXu = { 0 if u \u2208 S\u2032; \u03b7u if u \u2208 O \\ S\u2032.\nNote that the randomness of Xu is only due to step 6 (which may cause u \u2208 S\u2032 or u /\u2208 S\u2032). We define X = 1k \u2211 u\u2208O Xu. Now, we show Pr[X > \u03b2] \u2264 \u03b4/6. Note those random variables are independent. We first show that for each u \u2208 O, Pr[Xu = \u03b7u] \u2264 exp(\u2212\u03b72u \u00b7 2Q0). It trivially holds for the case \u03b7u = 0. So we only need to consider the case \u2206u > \u03b3. Suppose u 6\u2208 S\u2032. So, we have I\u2265\u00b5\u0302u\u2212\u03bb\u2212\u03b1\u2212\u03b2\u00b5\u0302 blocks u. Since |\u00b5e \u2212 \u00b5\u0302e| \u2264 \u03b1 for all arms e \u2208 I, we have I\u2265\u00b5\u0302u\u2212\u03bb\u22122\u03b1\u2212\u03b2\u00b5 = I \u2265\u00b5\u0302u\u2212\u03b3 \u00b5 blocks u as well (recall \u03b3 = \u03bb + 2\u03b1 + \u03b2). By the definition of \u2206u, we have \u00b5\u0302u \u2212 \u03b3 \u2264 \u00b5u \u2212 \u2206u, which means \u00b5\u0302u \u2212 \u00b5u < \u03b3 \u2212\u2206u = \u2212\u03b7u. By Proposition A.1.1, we have Pr[\u00b5\u0302u \u2212 \u00b5u < \u2212\u03b7u] \u2264 exp(\u2212\u03b72u \u00b7 2Q0). So Pr[Xu = \u03b7u] \u2264 Pr[\u00b5\u0302u \u2212 \u00b5u < \u2212\u03b7u] \u2264 exp(\u2212\u03b72u \u00b7 2Q0).\nThen we can apply Proposition A.3 with t = 2Q0 and obtain\nPr[X > \u03b2] < exp(\u2212\u03b22 \u00b7 kQ0) \u2264 \u03b4/6.\nNow, we show X \u2264 \u03b2 implies val\u00b5(I \u222a (O \u2229S\u2032)) \u2265 val\u00b5(O)\u2212 \u03b5. We define a new cost function \u00b5\u0303 on I \u222aO as follows:\n\u00b5\u0303(e) = { \u00b5e e \u2208 (I \u222aO) \u2229 S\u2032 \u00b5e \u2212\u2206e e \u2208 O\u0303 \\ S\u2032.\nNote that I \u2286 S\u2032, hence \u00b5\u0303 is well-defined for all elements in I \u222aO. For all element u \u2208 O\u0303 \\ S\u2032, by the definition of \u2206u, I \u2265\u00b5\u0303(u) \u00b5\u0303 = I \u2265\u00b5u\u2212\u2206u \u00b5 blocks u. Hence, we have\nOPT\u00b5\u0303(I \u222aO) = OPT\u00b5\u0303((I \u222aO) \u2229 S\u2032).\nBy the definition of \u00b5\u0303, we can see that\nOPT\u00b5((I \u222aO) \u2229 S\u2032) \u2265 OPT\u00b5\u0303((I \u222aO) \u2229 S\u2032) = OPT\u00b5\u0303(I \u222aO) \u2265 OPT\u00b5(I \u222aO)\u2212 \u2211\nu\u2208O\u0303\\S\u2032\n\u2206u.\nThe first inequality is due to the fact that \u2206u > 0 for all u \u2208 O\u0303. The last inequality is due to the fact that the total value we added is no more than\n\u2211 u\u2208O\u0303\\S\u2032 \u2206u. Note that \u2211 u\u2208O\u0303\\S\u2032 \u2206u \u2264 \u2211 u\u2208O(Xu+\u03b3)\nby the definition of Xu. Hence X \u2264 \u03b2 implies \u2211\nu\u2208O\u0303\\S\u2032 \u2206u \u2264 (\u03b2 + \u03b3) \u00b7 k \u2264 k \u00b7 \u03b5, which further implies\nval\u00b5(S \u2032) \u2265 val\u00b5(I \u222a (O \u2229 S\u2032)) \u2265 val\u00b5(I \u222aO)\u2212 \u03b5 = val\u00b5(S)\u2212 \u03b5.\nThis proves the first claim.\nNext, we prove the second claim. Let NB be the number of F -bad elements in S \\ I. Then by Lemma 5.2 and N \u2265 100k, NB \u2265 N\u22120.04N\u2212k \u2265 0.95N . Let e be an F -bad element in S\\I. F\u2265\u00b5e\u00b5 blocks e. Since I is \u03bb-optimal for F , we have I\u2265\u00b5e\u2212\u03bb\u00b5 blocks e by Lemma 3.7. By Proposition A.1.1, with probability at least 1\u22122 exp(\u22122Q0\u03b22) \u2265 1\u2212 1100 , |\u00b5e\u2212\u00b5\u0302e| \u2264 \u03b2. In that case, since |\u00b5u\u2212\u00b5\u0302u| \u2264 \u03b1 for all elements u \u2208 I, we have that I\u2265\u00b5e\u2212\u03bb\u00b5 \u2286 I\u2265\u00b5\u0302e\u2212\u03bb\u2212\u03b1\u2212\u03b2\u00b5\u0302 , which means I \u2265\u00b5\u0302e\u2212\u03bb\u2212\u03b1\u2212\u03b2 \u00b5\u0302 blocks e. Hence, e 6\u2208 S\u2032. Then we can see that for each F -bad element in S \\I, with probability \u2264 1100 , it is in S\u2032. All these events are independent. So let Y denote the number of the remaining F -bad elements in S \\ I. By the above argument, we can see Y is stochastically dominated by Bin(NB , 1100 ). Then applying Corollary A.2, we can obtain that\nPr[Y > 0.05N ] \u2264 Pr[Bin(NB , 1/100) > 0.05N ] \u2264 Pr[Bin(N, 1/100) > 0.05N ] \u2264 exp(\u221242 \u00b7 0.01N/3) \u2264 exp(\u22120.01N) \u2264 \u03b4/6.\nSo with probability at least 1 \u2212 2\u03b4/3 \u2212 2\u03b4/6 = 1 \u2212 \u03b4, Y \u2264 0.05N , and |S\u2032| \u2264 N \u2212 (NB \u2212 Y ) \u2264 N \u2212 (0.95N \u2212 0.05N) \u2264 0.1|S|, which proves the second claim. Finally, we examine the sample complexity. Since |F | = O(k+ln \u03b4\u22121), we can see that the procedure PAC-SamplePrune(F, \u03bb, \u03b4/6) takes O((k + ln \u03b4\u22121)(ln k + ln \u03b4\u22121)\u03b5\u22122) samples by Theorem 1.7. By Lemma 2.9, UniformSample(I, \u03b1, \u03b4/6k) takes O(k(ln k + ln \u03b4\u22121)\u03b5\u22122) samples. Finally, the sampling step (line 6) incurs O((|S|+ln \u03b4\u22121/k)\u03b5\u22122) samples. Summing them together, we can see the sample complexity of Elimination is as claimed."}, {"heading": "5.2 Main Algorithm", "text": "In this section, we present our main algorithm for finding an average-\u01eb-optimal solution.\nThe algorithm runs as follows: When |S| is small enough, we just invoke Na\u0308\u0131ve-II and Lemma 5.1 can guarantee that we find an average-\u01eb-optimal solution. Otherwise, we proceed in rounds. In the rth round, we invoke Elimination((Sr,MSr), \u03b5r, \u03b4r), where Sr is the remaining set of arms, \u03b5r = \u03b5/2r+1 and \u03b4r = \u03b4/2\nr+1, until the number of arms is smaller than a certain number. Elimination guarantees that the number of arms drops exponentially, hence the number of samples is dominated by the first call to Elimination. In the end, we invoke Na\u0308\u0131ve-II on the remaining arms to find the final solution. The pseudo-code can be found in Algorithm 7.\nAlgorithm 7: AvgPAC-RecurElim (S, \u03b5, \u03b4) Data: A PAC-Basis-Avg instance S = (S,M), with rank(M) = k, approximation error \u03b5,\nconfidence level \u03b4. Result: A basis in M.\n1 if |S|/k \u2264 10 OR ln \u03b4\u22121 > k ln |S|k then 2 Return Na\u0308\u0131ve-II (S, \u03b5, \u03b4) 3 r \u2190 1, Sr \u2190 S 4 while True do 5 \u03b4r \u2190 \u03b4/2r+1, \u03b5r \u2190 \u03b5/2r+1 6 If |Sr| \u2264 (ln \u03b4\u22121r + k + ln 6)(100 + ln k + ln \u03b4\u22121r ) Break 7 Sr+1 \u2190 Elimination(SSr = (Sr,MSr), \u03b5r, \u03b4r) 8 r \u2190 r + 1 9 Return I =Na\u0308\u0131ve-II (SSr = (Sr,MSr), \u03b5/2, \u03b4/2)\nNow, we prove Theorem 1.9.\nTheorem 1.9 (rephrased) Given a PAC-Basis-Avg instance S = (S,M), AvgPAC-RecurElim (S, \u03b5, \u03b4) returns an average-\u01eb-optimal solution, with probability at least 1 \u2212 \u03b4, and its sample complexity is at most\nO (( n \u00b7 (1 + ln \u03b4\u22121/k) + (ln \u03b4\u22121 + k)(ln k ln ln k + ln \u03b4\u22121 ln ln \u03b4\u22121) ) \u03b5\u22122 ) ,\nin which n = |S| and k = rank(M).\nProof. Note that when nk \u2264 10 or ln \u03b4\u22121 > k ln nk , Na\u0308\u0131ve-II(S, \u03b5, \u03b4) returns a correct solution and takes O(n(1 + ln \u03b4\u22121/k)) samples. So from now on we assume ln \u03b4\u22121 \u2264 k ln nk and n > 10k. Note that by a simple union bound, with probability at least 1 \u2212 \u03b4, all calls to Elimination return correctly, and the last call to Na\u0308\u0131ve-II also returns correctly. Denote this event as E and the following argument is conditioned on E . First we show the correctness. Suppose there are t rounds in total. We can see that\nval(St) \u2265 val(S1)\u2212 t\u22121\u2211\ni=1\n\u03b5/2i+1 \u2265 val(S)\u2212 \u03b5/2.\nClearly, val(I) \u2265 val(St)\u2212 \u03b5/2. Hence val(I) \u2265 val(S)\u2212 \u03b5, which means I is average-\u01eb-optimal for S.\nNow, we bound the sample complexity. Elimination is only called during the first t\u22121 rounds, and in which we have |Sr| > (ln \u03b4\u22121r +k+ln 6)(100+ln k+ln \u03b4\u22121r ) \u2265 100(ln \u03b4\u22121r +k+ln 6). By Lemma 5.4, the sample complexity for round r is O (( (1 + ln \u03b4\u22121r /k)|Sr| + (k + ln \u03b4\u22121r )(ln k + ln \u03b4\u22121r ) ) \u03b5\u22122r ) = O ( (1 + ln \u03b4\u22121r /k)|Sr|\u03b5\u22122r ) .\nSince |Sr| \u2264 |S1| \u00b7 0.1r\u22121 for 1 \u2264 r \u2264 t and \u03b5r = \u03b5/2r+1, we can bound the total number of samples of the first t\u2212 1 rounds by\nO (\u2211t\u22121\nr=1 4r+1 \u00b7 (1 + (ln \u03b4\u22121 + r)/k) \u00b7 n \u00b7 0.1r\u22121\u03b5\u22122\n) = O ( n(1 + ln \u03b4\u22121/k)\u03b5\u22122 ) .\nFinally, consider the last round t. We have |St| \u2264 (ln \u03b4\u22121t + k + ln 6)(100 + ln k + ln \u03b4\u22121t ) = O ( (ln \u03b4\u22121+ t+k)(ln k+ln \u03b4\u22121+ t) ) . Since St \u2265 k, we can see that t \u2264 O(ln nk ). Now we distinguish two cases:\n1. n \u2264 k3: In this case, t = O(ln k) and |St| \u2264 O ( (ln \u03b4\u22121 + k)(ln k + ln \u03b4\u22121) ) . By Lemma 5.1,\nthe sample complexity for Na\u0308\u0131ve-II (SSt , \u03b5/2, \u03b4/2) can be bounded by\nO (( (ln \u03b4\u22121 + k)(ln k + ln \u03b4\u22121) \u00b7 ln (ln \u03b4 \u22121 + k)(ln k + ln \u03b4\u22121)\nk + |St|/k \u00b7 ln \u03b4\u22121\n) \u03b5\u22122\n)\n\u2264O (( (ln \u03b4\u22121 + k)(ln k + ln \u03b4\u22121) \u00b7 (ln ln k + ln ln \u03b4\u22121) + |St|/k \u00b7 ln \u03b4\u22121 ) \u03b5\u22122 ) \u2264O (( (ln \u03b4\u22121 + k)(ln k ln ln k + ln \u03b4\u22121 ln ln \u03b4\u22121) + n/k \u00b7 ln \u03b4\u22121 ) \u03b5\u22122 ) .\n2. n \u2265 k3: In this case, t \u2264 O(lnn). Recalling that ln \u03b4\u22121 \u2264 k ln |St|k \u2264 O(n0.4). The sample complexity for Na\u0308\u0131ve-II (SSt , \u03b5/2, \u03b4/2) can be bounded by\nO (( n0.4 \u00b7 n0.4 \u00b7 lnn+ |St|/k \u00b7 ln \u03b4\u22121 ) \u03b5\u22122 ) \u2264 O (( n+ n/k \u00b7 ln \u03b4\u22121 ) \u03b5\u22122 ) .\nPutting them together, we can see the total sample complexity is\nO (( n(1 + ln \u03b4\u22121/k) + (ln \u03b4\u22121 + k)(ln k ln ln k + ln \u03b4\u22121 ln ln \u03b4\u22121) ) \u03b5\u22122 ) .\nThis completes the proof of the theorem."}, {"heading": "6 Future Work", "text": "In this paper, we present nearly-optimal algorithms for both the exact and PAC versions of the pure-exploration problem subject to a matroid constraint in a stochastic multi-armed bandit game: given a set of arms with a matroid constraint on them, pick a basis of the matroid whose weight (the sum of expectations over arms in this basis) is as large as possible, with high probability.\nAn immediate direction for investiation is to extend our results to other polynomial-time-computable combinatorial constraints: s-t paths, matchings (or more generally, the intersection of two matroids), etc. The model also extends to NP-hard combinatorial constraints, but there we would likely compare our solution against \u03b1-approximate solutions, instead of the optimal solution. Considering non-linear functions of the means is another natural next step. Yet another, perhaps more challenging, direction is to consider stochastic optimization problems, where the solution may depend on other details of the distributions than just the means."}, {"heading": "A Preliminaries in Probability", "text": "We first introduce the following versions of the standard Chernoff-Hoeffding bounds.\nProposition A.1. Let Xi(1 \u2264 i \u2264 n) be n independent random variables with values in [0, 1]. Let X = 1n \u2211n i=1Xi. The following statements hold:\n1. For every t > 0, we have that\nPr[X \u2212 E[X] \u2265 t] \u2264 exp(\u22122t2n), and\nPr[X \u2212 E[X] \u2264 \u2212t] \u2264 exp(\u22122t2n).\n2. For any \u01eb > 0, we have that\nPr[X < (1\u2212 \u01eb)E[X]] \u2264 exp(\u2212\u01eb2nE[X]/2), and\nPr[X > (1 + \u01eb)E[X]] \u2264 exp(\u2212\u01eb2nE[X]/3).\nApplying the above Proposition, we can get useful upper bound for the binomial distribution.\nCorollary A.2. Suppose the random variable X follows the binomial distribution Bin(n, p), i.e., Pr[X = k] = ( n k ) pk(1\u2212 p)n\u2212k for k \u2208 {0, 1, . . . , n}. It holds that for any \u01eb > 0,\nPr[X < (1\u2212 \u01eb)pn] \u2264 exp(\u2212\u01eb2pn/2), and\nPr[X > (1 + \u01eb)pn] \u2264 exp(\u2212\u01eb2pn/3).\nWe also need the following Chernoff-type concentration inequality (see Proposition A.4. in [ZCL14]).\nProposition A.3. Let Xi(1 \u2264 i \u2264 k) be independent random variables. Each Xi takes value ai (ai \u2265 0) with probability at most exp(\u2212a2i t) for some t \u2265 0, and 0 otherwise. Let X = 1k \u2211k i=1 Xi. For every \u01eb > 0, when t \u2265 2 \u01eb2 , we have that\nPr[X > \u01eb] < exp(\u2212\u01eb2tk/2).\nWe need to introduce the definition of the negative binomial distributions (see e.g., [MR10, pp.446]).\nDefinition A.4. Let X1,X2, . . . ,Xn be i.i.d. random variables with the common distribution being the geometric distribution with parameter p. The random variable X = X1+X2+ . . .+Xn denotes the number of coin flips (each one has probability p to be HEAD) needed to obtain n HEADS. The random variable X has the negative binomial distribution with parameters n and p, denote as X \u223c NegBin(n; p).\nLemma A.5. Pr[NegBin(n; p) > r] = Pr[Bin(r; p) < n].\nProof. Consider the event NegBin(n; p) > r. By the definition of NegBin(n; p), it is equivalent to the event that during the first r coin flips, there are less than n HEADS. The lemma follows immediately.\nDefinition A.6. (stochastic dominance) We say a random variable X stochastically dominates another random variable Y if for all r \u2208 R, we have Pr[X > r] \u2265 Pr[Y > r]."}, {"heading": "B Missing Proofs", "text": "Proof of Proposition 1.6. Let I be an \u03b5-optimal solution. We show it is also elementwise-\u03b5-optimal. Let oi be the arm with the i th largest mean in OPT and ai be the arm with the i th largest mean in I. Suppose for contradiction that \u00b5(ai) < \u00b5(oi) \u2212 \u03b5 for some i \u2208 [k] where k = rank(S). Now, consider the sorted list of the arms according to the modified cost function \u00b5I,\u03b5. The arm ai is ranked after oi and all oj with j < i. Let P be the set of all arms with mean no less than oi with respect to \u00b5I,\u03b5. Clearly, rank(P ) \u2265 i. So the greedy algorithm should select at least i elements in P , while I only has at most i\u2212 1 elements in P , contradicting the optimality of I with respect to \u00b5I,\u03b5.\nFor the second part, take a Best-k-Arm (k = 2) instance with four arms: \u00b5(a1) = 0.91, \u00b5(a2) = 0.9, \u00b5(a3) = 0.89, \u00b5(a4) = 0.875. The set {a3, a4} is elementwise-0.3-optimal, but not 0.3-optimal.\nProof of Lemma 5.1. First consider a basis U in M (hence |U | = k). We apply Proposition A.1.1 to all samples taken from the arms in U :\nPr [\u2223\u2223\u2223 1\nk\n\u2211\nu\u2208U\n\u00b5u \u2212 1\nk\n\u2211\nu\u2208U\n\u00b5\u0302u \u2223\u2223\u2223 > \u01eb/2 ] \u2264 2 exp(\u2212\u01eb2/2 \u00b7Q0 \u00b7 k) \u2264 \u03b4/ (|S| k ) .\nNote that there are at most (|S| k ) distinct bases. Hence, by a union bound over all bases, with\nprobability 1\u2212 \u03b4, we have \u2223\u2223 1 k \u2211 u\u2208U \u00b5u \u2212 1k \u2211 u\u2208U \u00b5\u0302u \u2223\u2223 \u2264 \u01eb/2, for all basis U . Let O = OPT(M). Then we have: 1k \u2211 u\u2208I \u00b5u \u2265 1k \u2211 u\u2208I \u00b5\u0302u\u2212\u01eb/2 \u2265 1k \u2211 u\u2208O \u00b5\u0302u\u2212\u01eb/2 \u2265 1k \u2211 u\u2208O \u00b5u\u2212 \u01eb, which means I is average-\u01eb-optimal for S. Finally, using the fact that (|S| k ) \u2264 ( e|S| k )k , the sample complexity can be easily verified.\nProof of Lemma 5.2. We show that each claim happens with probability \u2265 1\u2212 \u03b4/6. First, by Corollary A.2, we have Pr[|F | > 2pN ] \u2264 exp(\u2212pN/3) \u2264 \u03b4/6. Next, let the number of F -good elements in S be X and A = 0.04N . By Lemma 3.6, X is stochastically dominated by NegBin(k; p). Then, by Lemma A.5 and pA = 125pN \u2265 4k, we know Pr[X > A] \u2264 Pr[NegBin(k; p) > A] = Pr[Bin(A, p) < k] \u2264 Pr[Bin(A, p) < pA/4] . By Corollary A.2,\nPr[Bin(A, p) < pA/4] \u2264 exp(\u22129/16 \u00b7 pA/2) \u2264 exp(\u2212pA/4) \u2264 \u03b4/6.\nSo we have Pr[X > 0.04N ] \u2264 \u03b4/6. Also, by Theorem 1.7, I is a \u03bb-optimal solution for F with probability 1\u2212\u03b4/6. Finally, by Lemma 2.9 and a simple union bound, we have |\u00b5e\u2212 \u00b5\u0302e| \u2264 \u03b1 for all elements e \u2208 I with probability 1\u2212\u03b4/6."}], "references": [{"title": "Best arm identification in multi-armed bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In COLT-23th Conference on Learning Theory-2010, pages 13\u2013p,", "citeRegEx": "Audibert and Bubeck.,? \\Q2010\\E", "shortCiteRegEx": "Audibert and Bubeck.", "year": 2010}, {"title": "Regret in online combinatorial optimization", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck", "G\u00e1bor Lugosi"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Audibert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2013}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "arXiv preprint arXiv:1204.5721,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Multiple identifications in multi-armed bandits", "author": ["S\u00e9bastien Bubeck", "Tengyao Wang", "Nitin Viswanathan"], "venue": "arXiv preprint arXiv:1205.3181,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Multiple identifications in multi-armed bandits", "author": ["S\u00e9ebastian Bubeck", "Tengyao Wang", "Nitin Viswanathan"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Bubeck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2013}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Cambridge university press,", "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Combinatorial bandits", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2012}, {"title": "On the optimal sample complexity for best arm identification", "author": ["Lijie Chen", "Jian Li"], "venue": "arXiv preprint arXiv:1511.03774,", "citeRegEx": "Chen and Li.,? \\Q2015\\E", "shortCiteRegEx": "Chen and Li.", "year": 2015}, {"title": "Combinatorial pure exploration of multi-armed bandits", "author": ["Shouyuan Chen", "Tian Lin", "Irwin King", "Michael R Lyu", "Wei Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "On top-k selection in multi-armed bandits and hidden bipartite graphs", "author": ["Wei Cao", "Jian Li", "Yufei Tao", "Zhize Li"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Combinatorial multi-armed bandit: General framework and applications", "author": ["Wei Chen", "Yajun Wang", "Yang Yuan"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "Pac bounds for multi-armed bandit and markov decision processes. In Computational Learning Theory, pages 255\u2013270", "author": ["Eyal Even-Dar", "Shie Mannor", "Yishay Mansour"], "venue": null, "citeRegEx": "Even.Dar et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2002}, {"title": "Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems", "author": ["Eyal Even-Dar", "Shie Mannor", "Yishay Mansour"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Even.Dar et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Even.Dar et al\\.", "year": 2006}, {"title": "Asymptotic behavior of expected sample size in certain one sided tests", "author": ["RH Farrell"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Farrell.,? \\Q1964\\E", "shortCiteRegEx": "Farrell.", "year": 1964}, {"title": "Best arm identification: A unified approach to fixed budget and fixed confidence", "author": ["Victor Gabillon", "Mohammad Ghavamzadeh", "Alessandro Lazaric"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2012}, {"title": "Multi-bandit best arm identification", "author": ["Victor Gabillon", "Mohammad Ghavamzadeh", "Alessandro Lazaric", "S\u00e9bastien Bubeck"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gabillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Improved learning complexity in combinatorial pure exploration bandits", "author": ["Victor Gabillon", "Alessandro Lazaric", "Mohammad Ghavamzadeh", "Ronald Ortner", "Peter Bartlett"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Gabillon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2016}, {"title": "lil\u2019ucb: An optimal exploration algorithm for multi-armed bandits", "author": ["Kevin Jamieson", "Matthew Malloy", "Robert Nowak", "S\u00e9bastien Bubeck"], "venue": "COLT,", "citeRegEx": "Jamieson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jamieson et al\\.", "year": 2014}, {"title": "Random sampling and greedy sparsification for matroid optimization problems", "author": ["David R Karger"], "venue": "Mathematical Programming,", "citeRegEx": "Karger.,? \\Q1998\\E", "shortCiteRegEx": "Karger.", "year": 1998}, {"title": "On the complexity of best arm identification in multi-armed bandit models", "author": ["Emilie Kaufmann", "Olivier Capp\u00e9", "Aur\u00e9lien Garivier"], "venue": "arXiv preprint arXiv:1407.4443,", "citeRegEx": "Kaufmann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2014}, {"title": "Information complexity in bandit subset selection", "author": ["Emilie Kaufmann", "Shivaram Kalyanakrishnan"], "venue": "In Conference on Learning Theory, pages 228\u2013251,", "citeRegEx": "Kaufmann and Kalyanakrishnan.,? \\Q2013\\E", "shortCiteRegEx": "Kaufmann and Kalyanakrishnan.", "year": 2013}, {"title": "Almost optimal exploration in multiarmed bandits", "author": ["Zohar Karnin", "Tomer Koren", "Oren Somekh"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "Karnin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karnin et al\\.", "year": 2013}, {"title": "A randomized linear-time algorithm to find minimum spanning trees", "author": ["David R Karger", "Philip N Klein", "Robert E Tarjan"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Karger et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Karger et al\\.", "year": 1995}, {"title": "Pac subset selection in stochastic multi-armed bandits", "author": ["Shivaram Kalyanakrishnan", "Ambuj Tewari", "Peter Auer", "Peter Stone"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "Kalyanakrishnan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kalyanakrishnan et al\\.", "year": 2012}, {"title": "The sample complexity of exploration in the multiarmed bandit problem", "author": ["Shie Mannor", "John N Tsitsiklis"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}, {"title": "Some aspects of the sequential design of experiments", "author": ["Herbert Robbins"], "venue": "In Herbert Robbins Selected Papers,", "citeRegEx": "Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Robbins.", "year": 1985}, {"title": "Optimal pac multiple arm identification with applications to crowdsourcing", "author": ["Yuan Zhou", "Xi Chen", "Jian Li"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Zhou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "We study the pure exploration problem subject to a matroid constraint (Best-Basis) in a stochastic multi-armed bandit game. In a Best-Basis instance, we are given n stochastic arms with unknown reward distributions, as well as a matroid M over the arms. Let the weight of an arm be the mean of its reward distribution. Our goal is to identify a basis of M with the maximum total weight, using as few samples as possible. The problem is a significant generalization of the best arm identification problem and the top-k arm identification problem, which have attracted significant attentions in recent years. We study both the exact and PAC versions of Best-Basis, and provide algorithms with nearlyoptimal sample complexities for these versions. Our results generalize and/or improve on several previous results for the top-k arm identification problem and the combinatorial pure exploration problem when the combinatorial constraint is a matroid.", "creator": "LaTeX with hyperref package"}}}