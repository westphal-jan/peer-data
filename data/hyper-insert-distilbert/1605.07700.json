{"id": "1605.07700", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Learning Purposeful Behaviour in the Absence of Rewards", "abstract": "artificial intelligence is furthermore commonly defined as representing the ability essential to achieve goals in serving the behavioral world. in assessing the reinforcement learning model framework, foraging goals are encoded as sensory reward functions signal that guide agent behaviour, and the conditional sum count of observed received rewards equally provide a notion of progress. however, some domains have no such reward signal, or have a special reward derived signal presenting so randomly sparse tasks as to altogether appear absent. when without reward neural feedback, agent behaviour recognition is typically random, often dithering aimlessly scattered and always lacking intentionality. in this paper we present an explicit algorithm capable of learning consciously purposeful behaviour in the absence mechanism of rewards. the algorithm proceeds normally by constructing temporally extended actions ( rewards options ), through the spatial identification of when purposes that occur are \" just out of target reach \" wary of approaching the participating agent's current behaviour. these purposes establish intrinsic goals for the agent to potentially learn, ultimately resulting in a fragmented suite of cryptic behaviours generated that broadly encourage the intelligent agent to blindly visit to different parts of about the state space. likewise moreover, even the potential approach context is surprisingly particularly richly suited for settings situations where rewards are very strategically sparse, paths and such behaviours together can eventually help in the exploration required of extending the stimulus environment until optimal reward is observed.", "histories": [["v1", "Wed, 25 May 2016 01:33:34 GMT  (181kb,D)", "http://arxiv.org/abs/1605.07700v1", "Extended version of the paper presented at the workshop entitled Abstraction in Reinforcement Learning, at the 33rd International Conference on Machine Learning, New York, NY, USA, 2016"]], "COMMENTS": "Extended version of the paper presented at the workshop entitled Abstraction in Reinforcement Learning, at the 33rd International Conference on Machine Learning, New York, NY, USA, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["marlos c machado", "michael bowling"], "accepted": false, "id": "1605.07700"}, "pdf": {"name": "1605.07700.pdf", "metadata": {"source": "META", "title": "Learning Purposeful Behaviour in the Absence of Rewards", "authors": ["Marlos C. Machado", "Michael Bowling"], "emails": ["MACHADO@UALBERTA.CA", "MBOWLING@UALBERTA.CA"], "sections": [{"heading": "1. Introduction", "text": "Reinforcement learning (RL) has been successful in generating agents capable of intelligent behaviour in initially unknown environments; with accomplishments such as surpassing human-level performance in Backgammon (Tesauro, 1995), helicopter flight (Ng et al., 2004), and general competency in dozens of Atari 2600 games (Mnih et al., 2015). Such successes are achieved by algorithms that maximize the expected cumulative sum of rewards, which can be seen as a measure of progress towards the\nExtended version of the paper presented at the workshop entitled Abstraction in Reinforcement Learning, at the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. Copyright 2016 by the author(s).\ndesired goal. The goal is sometimes easily defined through rewards, such as \u00b11 signals encoding win/loss in games or \u22121 signals informing the agent something undesirable occurred (e.g., a robot bumping into a wall).\nWe are interested in the setting where the reward signal is uninformative or even absent. In the uninformative setting, the paucity of reward usually leads to dithering typical of -greedy exploration approaches. This effect is particularly pronounced when the agent operates at a fine time scale, as is common of video game platforms (Bellemare et al., 2013). In the complete absence of reward, it is unclear what intelligent behaviour should even constitute. Intrinsic motivation-based approaches (Singh et al., 2004; Oudeyer et al., 2007; Barto, 2013) offer a solution in the form of an intrinsic reward signal, and some authors have proposed agents undergoing developmental periods in which they are not concerned with maximizing extrinsic reward but in acquiring reusable options autonomously learned from intrinsic rewards (Singh et al., 2004). However, here we instead consider the notion of a purposeful agent: one that can commit to a behaviour for an extended period of time.\nTo construct purposeful agents, we appeal to the options framework (Sutton et al., 1999). Options extend the mathematical framework of reinforcement learning and Markov decision processes (MDPs) to allow agents to take temporally extended actions to accomplish subgoals. While this extension is an extremely powerful idea for allowing reasoning at different levels of abstraction, automatically discovering options (e.g., by identifying subgoals) is an openproblem in the literature. Generally, options are designed by practitioners who identify meaningful subgoals that can be used as stepping-stones to solve a complex task. Besides using options to divide a task in to subgoals, we advocate one can also use options to add decisiveness to agents in an environment in which rewards are not available, and that this is a better choice than aimless exploration.\nIn this paper we introduce an algorithm capable of learning purposeful behaviour in the absence of rewards. Our approach discovers options by identifying purposes that are achievable by the agent. These purposes are turned into intrinsic subgoals through to an intrinsic reward function,\nar X\niv :1\n60 5.\n07 70\n0v 1\n[ cs\n.L G\n] 2\n5 M\nay 2\n01 6\nresulting in a suite of behaviours that encourage the agent to visit different parts of the state space. These options are particularly useful in the absence of rewards. As an example, when the agent observes a change in the environment through its feature representation, it tries to learn a policy capable of reproducing that change. Also, when such an option is added to the agent\u2019s action set, the agent now can move farther in the state-space, with some events that were rare now becoming frequent and events that were \u201cimpossible\u201d now becoming \u201cjust\u201d infrequent.\nIn this early paper we introduce the main ideas that underly our algorithm, including concepts such as \u201cpurpose\u201d. We also provide an algorithm for option discovery with linear function approximation, in contrast to most approaches for option discovery that rely on tabular representations. We show that in any finite MDP our learned options are guaranteed to have at least one state which will cause option termination. Finally, we apply our approach to a simple domain, showing it can reach states further from the starting state, thus exhibiting intentionality in its behaviour."}, {"heading": "2. Background", "text": "In this section we introduce the reinforcement learning (RL) problem setting and the options framework. We also discuss the problem of option discovery and some approaches that try to address it. As a convention, we indicate random variables by capital letters (e.g., St,Rt), vectors by bold letters (e.g., \u03b8), functions by lowercase letters (e.g., v), and sets by calligraphic font (e.g., S , A)."}, {"heading": "2.1. Reinforcement Learning and Options", "text": "In the RL framework (Sutton & Barto, 1998; Szepesva\u0301ri, 2010) an agent aims at maximizing some notion of cumulative reward by taking actions in an environment; these actions may affect the next state the agent will be as well as all subsequent rewards it will experience. It is generally assumed the tasks of interest satisfy the Markov property, being called Markov decision processes (MDPs). An MDP is formally defined as a 5-tuple \u3008S,A, r, p, \u03b3\u3009. At time t the agent is in the state st \u2208 S where it takes an action at \u2208 A that leads to the next state st+1 \u2208 S according to the transition probability kernel p(s\u2032|s, a), which encodes Pr(St+1 = s\n\u2032|St = s,At = a). The agent also observes a reward Rt+1 \u223c r(s, a, s\u2032). The agent\u2019s goal is to learn a policy \u03c0 : S \u00d7 A \u2192 [0, 1] that maximizes the expected discounted return Gt . = E\u03c0 [\u2211\u221e k=0 \u03b3 kRt+k+1|st ] , where \u03b3 \u2208 [0, 1) is known as the discount factor. We are interested in settings where the reward signalRt is uniform across the environment.\nWhen learning to maximize Gt it is common to learn an action-value function defined as q\u03c0(s, a) . = E\u03c0[Gt|St =\ns,At = a]. However, in large problems it may be infeasible to learn q\u03c0 exactly for each state-action pair. To tackle this issue agents often learn an approximate value function: q\u03c0(s, a) \u2248 q\u03c0(s, a;\u03b8). A common approach is to approximate these values using linear function approximation where q\u03c0(s, a;\u03b8) . = \u03b8>\u03c6(s, a), in which \u03b8 denotes the vector of weights and \u03c6(s, a) denotes a static feature representation of the state s when taking action a. This can also be done through non-linear function approximation methods such as neural networks (e.g., Tesauro, 1995, Mnih et al., 2015). Note that generally \u03b8 has much smaller number of parameters than the number of states in S .\nThe standard RL framework is focused on MDPs, in which actions last a single time step. Nevertheless, it is convenient to have agents encoding higher levels of abstraction, which also facilitate the learning process if properly defined (Dietterich, 1998; Sutton et al., 1999). Sutton et al. extended the RL framework by introducing temporally extended actions called options. Intuitively, options are higher-level actions that are extended through several time steps. Formally, an option o is defined as 3-tuple o = \u3008I, $, T \u3009 where I \u2208 S denotes the initiation set, $ : A \u00d7 S \u2192 [0, 1] denotes the option\u2019s policy, and T \u2208 S denotes the termination set. After initiated, actions are selected according to$ until the agent reaches a state in T . Originally, Sutton et al. defined a function \u03b2 : S \u2192 [0, 1] to encode the probability of an option terminating at a given state. In this paper we define \u03b2 to be the characteristic function of the set T : \u03b2(s) = 1 for all s \u2208 T and \u03b2(s) = 0 for all s /\u2208 T , hence we overload the notation. Options generalize MDPs to semiMarkov decision processes (SMDPs) in which actions take variable amounts of time. Options are also useful when addressing the problem of exploration because they can move agents farther in the state-space."}, {"heading": "2.2. Option Discovery", "text": "The potential of options to dramatically affect learning by improving exploration is well known (e.g., McGovern & Sutton, 1998; McGovern & Barto, 2001; Kulkarni et al., 2016). Nevertheless, most works that benefit from options do not discover them, but assume they are provided or that there is a hardwired notion of interestingness (reward) that can be used to discover options e.g., salient events (Singh et al., 2004).\nThe works that investigate how to discover options can be clustered in three different categories. The most common approach is to try to identify subgoal states through some heuristic such as visitation frequency (McGovern & Barto, 2001), graph-related metrics such as betweenness (S\u0327ims\u0327ek & Barto, 2008), or graph partitioning metrics (Menache et al., 2002; Mannor et al., 2004; S\u0327ims\u0327ek et al., 2005). Some authors have also tackled the problem of option dis-\ncovery by trying to identify common subpolicies (Thrun & Schwartz, 1994; Pickett & Barto, 2002), while others proposed methods based on the frequency of the change of state variables (Hengst, 2002).\nThe works on option discovery generally operate in a tabular setting where you can have states uniquely defined. Consequently, metrics such as frequency of visitation and transition graphs can be used for option discovery. Automatically discovering options in large state-spaces where function approximation is required is still a challenge. Our work presents an approach for option discovery in settings with linear function approximation, which has a much larger applicability. Few works tackled option discovery with function approximation. Those that did generally simplified the problem with additional assumptions such as knowledge of subgoal states (Konidaris & Barreto, 2009) or that one can control the interface between regions of the MDP (Hengst, 2002).\nThe proposals of not depending on a reward signal to discover meaningful behaviour (S\u0327ims\u0327ek & Barto, 2004), and of looking at the different rates of changes in the agent\u2019s feature representation (Hengst, 2002) are related to our work. This relationship will be clearer in the next section."}, {"heading": "3. Option Discovery for Purposeful Agents", "text": "Approaches based on intrinsic motivation and novelty are some of the ways to circumvent the absence of rewards in the environment. Schmidhuber (2010) summarizes several works based on intrinsic motivation, which he defines as algorithms that maximize a reward signal derived from the agent\u2019s learning progress. Lehman & Stanley (2011) have advocated that agents should drop feedback they receive from the environment even in more traditional settings such as search, maximizing novelty instead. Both ideas are related to our work. We discover options based on novelty assuming no extrinsic rewards are available (Lehman & Stanley, 2011). These options are based on a very loose notion of a model of the environment, aiming at learning how to change principal components of a compressed environment representation (Schmidhuber, 2010).\nOur algorithm is based on four different concepts, namely: (i) storing the changes seen between two different time steps, (ii) clustering correlated changes in order to extract a purpose, (iii) learning policies capable of reproducing desired purposes, and (iv) transforming these policies into options that can be used to move the agent farther in the state space. After these steps a new set of options giving different purposes to the agent is discovered. These options guide the agent to different parts of the state space, which may lead to identifying new purposes. When such steps are used iteratively we create a self-reinforcing loop. We\ndiscuss each concept individually while introducing the algorithm. The algorithm we introduce uses a binary feature representation, but it is extensible to more general settings.\nThe agent initially follows some default policy (possible random) for a given number of time steps, using all actions available on its action set. While following such policy, at every time step the agent stores in a dataset D the difference between the feature representation of its current observation \u03c6(st) and the representation of its previous observation \u03c6(st\u22121), i.e.: D \u2190 D \u222a { ( \u03c6(st) \u2212 \u03c6(st\u22121) ) }. It is important to stress that while storing changes one can easily see those that stand out, such as features that rarely change. Storing the features is less informative than the current transition because it is harder for the agent to identify when a feature really changed. Moreover, storing differences allow us to clearly identify different directions in the change, something that is useful in the next steps.\nAfter a pre-defined number of time steps the agent stops storing transitions to identify purposes in the observed behaviour. It clusters together features that change together, avoiding correlated changes to generate the same purpose. Formally, such step consists in reducing the dimensionality of D through singular value decomposition (SVD): D = U\u03a3V \u2217. The SVD generates a lower rank representation of the transitions stored in D. Such low rank representation consists of eigenvalues and eigenvectors. The eigenvectors encode the principal components of D while the eigenvalues weight them according to how much that eigenvector is important to reconstruct D. Each eigenvector can be seen as encoding a different purpose for the agent because all features that are somehow correlated are collapsed to a single eigenvector, explaining a direction of variation of the observed transitions. We call the eigenvectors obtained from the dataset of transitions eigenpurposes. Definition 3.1 (Eigenpurpose). Given a matrix D of transitions where each row encodes the difference between two consecutive observations, i.e. \u03c6(st) \u2212 \u03c6(st\u22121), and having Vi denoting the i-th row of matrix V ; each eigenvector (V \u2217)i obtained from the singular value decomposition traditionally defined asD = U\u03a3V \u2217 is called an eigenpurpose.\nThe following example provides an intuition about the importance of eigenpurposes. Imagine that an agent, by chance, leaves a building. By doing it the value of several of its features change (e.g. lighting, temperature, soil). When we collapse all these changes with the SVD, instead of having a feature encoding \u201ctemperature increase\u201d, other encoding \u201cchange of lighting\u201d, and so on, we have only an eigenpurpose encoding \u201coutside the building\u201d.\nOnce eigenpurposes have been identified, the agent learns policies capable of maximizing their occurrence. In order to learn a policy that maximizes an eigenpurpose we need to define a proper reward function. Such reward ri,t\nis defined as the similarity between the observed transition and the eigenpurpose of interest ei, ri,t = ei> ( \u03c6(st) \u2212\n\u03c6(st\u22121) ) . Because SVD does not encode signs, we learn how to maximize eigenpurposes in both possible directions, i.e. the agent also learns a second policy that maximizes \u2212ri,t. These policies are called eigenbehaviours. Definition 3.2 (Eigenbehaviour). A policy \u03c0 : S \u2192 A is called an eigenbehaviour if it is the optimal policy that maximizes the occurrence of an eigenpurpose (Definition 3.1) in the original MDP augumented by the action \u22a5 that terminates the option; i.e. \u03c0(s) = arg maxa max\u03c0 q\u03c0(s, a) in the MDP \u3008S,A \u222a {\u22a5}, (V >)j ( \u03c6(st)\u2212 \u03c6(st\u22121) ) , p, \u03b3\u3009.\nThe algorithm used to learn eigenbehaviours is not predefined, nor the order in which they are learned.\nWe can naturally construct an option from the learned eigenbehaviour. To do so, we need to define the set of states for which the eigenbehaviour is effective (initiation set) and its termination condition. We define the initiation set of an option as the set of states s in which, after learning, q\u03c0(s, a) > 0 for at least one action a \u2208 A. Intuitively this corresponds to every state in which the policy can still make progress towards the eigenpurpose. The set of terminal states for this option is the complement of the initiation set, i.e. S \\ I. Once such options are discovered we can add them to the agents action set, which allows the agent to repeat the described process with a policy that uses the discovered options to collect new data.\nNotice that eigenvalues loosely encode how frequent each eigenpurpose was observed. Therefore, the eigenbehaviours corresponding to the lower eigenvalues encode purposes observed less frequently. Because of that, once an option for a \u201crare\u201d purpose is discovered, this purpose will no longer be \u201crare\u201d since a single decision (taking the option) is now capable of reproducing this rare event. We may also increase the likelihood of observing other unlikely \u201cpurposes\u201d since a single action now moves the agent much farther in the state-space. This can help agents to explore environments in which rewards are very sparse, guiding the agent until a reward signal is observed.\nThe described algorithm is formally presented in Algorithm 1. An additional detail not discussed yet is that one can decide to learn how to maximize only a subset of the discovered eigenpurposes. In this work we did not evaluate the impact of different approaches. Here we propose a simple eigenvalue threshold \u03ba that determines which eigenpurposes are interesting. We select all eigenpurposes that have a correspondent eigenvalue greater than \u03ba, which we interpret as discarding noise.\nOur constructed options represent a \u201cpurpose\u201d, which can be thought of as reaching states in the options termination\nAlgorithm 1 Purposeful Option Discovery (POD) Input: A {Action set}\n\u03ba {Noise threshold} nI > 0 {Number of iterations} nR > 0 {Num. of random steps per iteration}\nOutput: \u2126 {Option set} 1: \u2126\u2190 \u2205 2: for i\u2190 1 to nI do 3: D \u2190 \u2205 4: for j \u2190 1 to nR do 5: Observe \u03c6(s) 6: Take an action a \u2208 A or an option o \u2208 \u2126 7: if option o was taken then 8: while s /\u2208 To and j < nR do 9: Take an action a following $o 10: Observe \u03c6(s\u2032) 11: D \u2190 D \u222a ( \u03c6(s\u2032)\u2212 \u03c6(s)\n) 12: Observe \u03c6(s) 13: j \u2190 j + 1 14: end while 15: else 16: Observe \u03c6(s\u2032) 17: D \u2190 D \u222a ( \u03c6(s\u2032)\u2212 \u03c6(s)\n) 18: end if 19: end for 20: U,\u03a3, V \u2190 SVD(D) 21: for all j such that \u03a3j > \u03ba 22: Learn policy \u03c0j that max. ( V > ) j ( \u03c6(s\u2032)\u2212 \u03c6(s)\n) 23: Learn policy \u03c0k that max. ( \u2212V > ) j ( \u03c6(s\u2032)\u2212\u03c6(s)\n) 24: Ij \u2190 {s|s \u2208 S,\u2203a \u2208 A : q\u03c0j (s, a) > 0} 25: Ik \u2190 {s|s \u2208 S,\u2203a \u2208 A : q\u03c0k(s, a) > 0} 26: \u2126\u2190 \u2126 \u222a \u3008Ij , \u03c0j ,S \\ Ij\u3009 \u222a \u3008Ik, \u03c0k,S \\ Ik\u3009 27: end for all 28: end for\nset, which we can show is guaranteed to be non-empty.\nFinally, it is important to guarantee that there is at least one state that satisfies the discovered purposes, or in other words, that the termination set of an option is not empty.\nTheorem 3.1 (Option\u2019s Termination). Consider an option o = \u3008Io, \u03c0o, To\u3009 discovered with Algorithm 1 where \u03b3 < 1. Then To is nonempty.\nProof intuition. Consider the state with the largest potential value. From this state the agent must terminate either due to the discount factor or due to the termination action in a state with lesser or equal potential value. The cumulative reward received is the difference in potential and so the expected value of the state must be non-positive. The complete proof is in the Appendix."}, {"heading": "4. Experimental Evaluation", "text": "We performed an empirical validation in a hand-crafted domain which allows us to clearly illustrate our algorithm\u2019s features, namely:\n\u2022 At each new iteration of the algorithm the discovered options become increasingly more complex.\n\u2022 More complex options are able to move the agent farther away in the state space.\n\u2022 As the agent moves farther away with newly discovered options, it observes new eigenpurposes, discovering more options, what creates a self-reinforcing loop.\nWe evaluated our algorithm in a random walk in a toy domain consisting of moving around a ring of length 4096 with deterministic transitions. The agent starts at the x coordinate 0 and at every time step it chooses between going right or going left. We use linear function approximation with the two\u2019s complement binary encoding as representation (12 bits long). When the agent goes left on the state 0 it goes to the state \u22121 (0000 0000 00002 \u2192 1111 1111 11112). Similarly, going right in state 2047 transitions to\u22122048. There are no rewards in this environment.\nAll the evaluations were made in the same setting. The selected \u201cnoise\u201d parameter \u03ba was set to 1 and the environment, when learning the eigenbehaviours, had a discount factor \u03b3 = 0.99. The policies were obtained through value iteration with 100 iterations. Each round of our algorithm consisted of 1, 000 time steps in which the agent collected transitions to discover eigenpurposes. We ran six rounds, with round zero having only primitive actions available. The agent\u2019s default policy was to select uniformly random among the currently available actions, or options.\nWe first evaluated our results in agents with full observability, in which the agent perceives states as described above (Figure 1a, and Table 1). By looking at the average length of the discovered options we see that options become increasingly complex with each iteration. This added complexity allows the agent to move to farther states, as evidenced by the increasing distance between farthest point and the agent\u2019s starting state at that iteration (Max Dist. from Start). The improvement is particularly clear when comparing to a sample random walk on primitive actions (Figure 1a). Note that, despite options constructed in later iterations still use only primitive actions, they present more complex behaviours. This is different than typical option discovery methods, which construct hierarchies of options.\nWe also evaluated a setting in which the agent had partial observability. In this setting the agent does not observe the three least significant bits encoding the state. This collapses several states together and makes it much harder\nfor the agent to observe progress. Interestingly, the same behavioural pattern as in the full observability experiment emerges. Agents still come up with options of the type \u201cflip the i-th bit\u201d once they discover these purposes. The unique difference is that fewer options are discovered at each iteration, due to fewer observable eigenpurposes."}, {"heading": "5. Conclusion", "text": "In this paper we introduced a new algorithm capable of discovering options without any feedback in the form of rewards from the environment. Our algorithm discovers options that reproduce purposes extracted from the states seen by an agent acting randomly. We presented experimental results showing how the discovered options greatly improve exploration by introducing decisiveness on the agents, avoiding the traditional aimless dithering. We also showed evidences that our approach may work well with partial observability.\nAs future work, we plan to investigate how this algorithm behaves in more complex environments, such as the Arcade Learning Environment (Bellemare et al., 2013). We can evaluate our algorithm on these large domains because it is amenable to function approximation, differently from most other approaches for option discovery. Naturally, we then have to be able to learn a policy, using the discovered options, to maximize the discounted sum of rewards. Some of our preliminary results using interrupting options do seem promising. However, when applying this algorithm to larger domains we face a challenge not discussed here: the exploding number of eigenpurposes (and consequently discovered options), indicating that proper sampling techniques must be further evaluated. Finally, it is important to have coevolving action and representation abstractions: higher levels of action abstraction should drive the agent to improve its representation of the world and once the agent has a better representation of the world, better action abstractions should become available. This is a topic that is not commonly investigated but that needs to be addressed in the future, maybe our algorithm can be the first step towards that direction."}, {"heading": "Acknowledgements", "text": "The authors thank Richard S. Sutton and Marc G. Bellemare for insightful discussions that helped improve this work, and Csaba Szepesva\u0301ri for point out the Neumann series we used in our proof. This work was supported by grants from Alberta Innovates Technology Futures and the Alberta Innovates Centre for Machine Learning (AICML)."}], "references": [{"title": "Intrinsic Motivation and Reinforcement Learning", "author": ["Barto", "Andrew G"], "venue": "In Intrinsically Motivated Learning in Natural and Artificial Systems,", "citeRegEx": "Barto and G.,? \\Q2013\\E", "shortCiteRegEx": "Barto and G.", "year": 2013}, {"title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Using Relative Novelty to Identify Useful Temporal Abstractions in Reinforcement Learning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2004\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2004}, {"title": "Skill Characterization Based on Betweenness", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Barto", "Andrew G"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2008}, {"title": "Identifying Useful Subgoals in Reinforcement Learning by Local Graph Partitioning", "author": ["\u015eim\u015fek", "\u00d6zg\u00fcr", "Wolfe", "Alicia P", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "\u015eim\u015fek et al\\.,? \\Q2005\\E", "shortCiteRegEx": "\u015eim\u015fek et al\\.", "year": 2005}, {"title": "The MAXQ Method for Hierarchical Reinforcement Learning", "author": ["Dietterich", "Thomas G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Dietterich and G.,? \\Q1998\\E", "shortCiteRegEx": "Dietterich and G.", "year": 1998}, {"title": "Discovering Hierarchy in Reinforcement Learning with HEXQ", "author": ["Hengst", "Bernhard"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Hengst and Bernhard.,? \\Q2002\\E", "shortCiteRegEx": "Hengst and Bernhard.", "year": 2002}, {"title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", "author": ["Kulkarni", "Tejas D", "Narasimhan", "Karthik R", "Saeedi", "Ardavan", "Tenenbaum", "Joshua B"], "venue": "ArXiv e-prints,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Abandoning Objectives: Evolution Through the Search for Novelty Alone", "author": ["Lehman", "Joel", "Stanley", "Kenneth O"], "venue": "Evolutionary Computation,", "citeRegEx": "Lehman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lehman et al\\.", "year": 2011}, {"title": "Dynamic Abstraction in Reinforcement Learning via Clustering", "author": ["Mannor", "Shie", "Menache", "Ishai", "Hoze", "Amit", "Klein", "Uri"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Mannor et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mannor et al\\.", "year": 2004}, {"title": "Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density", "author": ["McGovern", "Amy", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "McGovern et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 2001}, {"title": "Roles of Macroactions in Accelerating Reinforcement Learning", "author": ["McGovern", "Amy", "Sutton", "Richard S"], "venue": "Technical report, University of Massachusetts,", "citeRegEx": "McGovern et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McGovern et al\\.", "year": 1998}, {"title": "QCut - Dynamic Discovery of Sub-goals in Reinforcement Learning", "author": ["Menache", "Ishai", "Mannor", "Shie", "Shimkin", "Nahum"], "venue": "In Proceedings of the European Conference on Machine Learning (ECML),", "citeRegEx": "Menache et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Menache et al\\.", "year": 2002}, {"title": "Human-level control through deep reinforcement learning", "author": ["Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Dharshan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dharshan et al\\.", "year": 2015}, {"title": "Autonomous Inverted Helicopter Flight via Reinforcement Learning", "author": ["Ng", "Andrew Y", "Coates", "Adam", "Diel", "Mark", "Ganapathi", "Varun", "Schulte", "Jamie", "Tse", "Ben", "Berger", "Eric", "Liang"], "venue": "In Proceedings of the International Symposium on Experimental Robotics (ISER),", "citeRegEx": "Ng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2004}, {"title": "Intrinsic Motivation Systems for Autonomous Mental Development", "author": ["P.Y. Oudeyer", "F. Kaplan", "Hafner", "VV"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "Oudeyer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Oudeyer et al\\.", "year": 2007}, {"title": "PolicyBlocks: An Algorithm for Creating Useful Macro-Actions in Reinforcement Learning", "author": ["Pickett", "Marc", "Barto", "Andrew G"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Pickett et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pickett et al\\.", "year": 2002}, {"title": "Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990-2010)", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2010}, {"title": "Intrinsically Motivated Reinforcement Learning", "author": ["Singh", "Satinder P", "Barto", "Andrew G", "Chentanez", "Nuttapong"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Singh et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2004}, {"title": "Reinforcement Learning: An Introduction", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning", "author": ["Szepesv\u00e1ri", "Csaba"], "venue": null, "citeRegEx": "Szepesv\u00e1ri and Csaba.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Csaba.", "year": 2010}, {"title": "Temporal Difference Learning and TDGammon", "author": ["Tesauro", "Gerald"], "venue": "Comm. of the ACM,", "citeRegEx": "Tesauro and Gerald.,? \\Q1995\\E", "shortCiteRegEx": "Tesauro and Gerald.", "year": 1995}, {"title": "Finding Structure in Reinforcement Learning", "author": ["Thrun", "Sebastian", "Schwartz", "Anton"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Thrun et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1994}], "referenceMentions": [{"referenceID": 14, "context": "Reinforcement learning (RL) has been successful in generating agents capable of intelligent behaviour in initially unknown environments; with accomplishments such as surpassing human-level performance in Backgammon (Tesauro, 1995), helicopter flight (Ng et al., 2004), and general competency in dozens of Atari 2600 games (Mnih et al.", "startOffset": 250, "endOffset": 267}, {"referenceID": 1, "context": "This effect is particularly pronounced when the agent operates at a fine time scale, as is common of video game platforms (Bellemare et al., 2013).", "startOffset": 122, "endOffset": 146}, {"referenceID": 18, "context": "Intrinsic motivation-based approaches (Singh et al., 2004; Oudeyer et al., 2007; Barto, 2013) offer a solution in the form of an intrinsic reward signal, and some authors have proposed agents undergoing developmental periods in which they are not concerned with maximizing extrinsic reward but in acquiring reusable options autonomously learned from intrinsic rewards (Singh et al.", "startOffset": 38, "endOffset": 93}, {"referenceID": 15, "context": "Intrinsic motivation-based approaches (Singh et al., 2004; Oudeyer et al., 2007; Barto, 2013) offer a solution in the form of an intrinsic reward signal, and some authors have proposed agents undergoing developmental periods in which they are not concerned with maximizing extrinsic reward but in acquiring reusable options autonomously learned from intrinsic rewards (Singh et al.", "startOffset": 38, "endOffset": 93}, {"referenceID": 18, "context": ", 2007; Barto, 2013) offer a solution in the form of an intrinsic reward signal, and some authors have proposed agents undergoing developmental periods in which they are not concerned with maximizing extrinsic reward but in acquiring reusable options autonomously learned from intrinsic rewards (Singh et al., 2004).", "startOffset": 295, "endOffset": 315}, {"referenceID": 20, "context": "To construct purposeful agents, we appeal to the options framework (Sutton et al., 1999).", "startOffset": 67, "endOffset": 88}, {"referenceID": 20, "context": "Nevertheless, it is convenient to have agents encoding higher levels of abstraction, which also facilitate the learning process if properly defined (Dietterich, 1998; Sutton et al., 1999).", "startOffset": 148, "endOffset": 187}, {"referenceID": 7, "context": "The potential of options to dramatically affect learning by improving exploration is well known (e.g., McGovern & Sutton, 1998; McGovern & Barto, 2001; Kulkarni et al., 2016).", "startOffset": 96, "endOffset": 174}, {"referenceID": 18, "context": ", salient events (Singh et al., 2004).", "startOffset": 17, "endOffset": 37}, {"referenceID": 12, "context": "The most common approach is to try to identify subgoal states through some heuristic such as visitation frequency (McGovern & Barto, 2001), graph-related metrics such as betweenness (\u015eim\u015fek & Barto, 2008), or graph partitioning metrics (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005).", "startOffset": 236, "endOffset": 300}, {"referenceID": 9, "context": "The most common approach is to try to identify subgoal states through some heuristic such as visitation frequency (McGovern & Barto, 2001), graph-related metrics such as betweenness (\u015eim\u015fek & Barto, 2008), or graph partitioning metrics (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005).", "startOffset": 236, "endOffset": 300}, {"referenceID": 4, "context": "The most common approach is to try to identify subgoal states through some heuristic such as visitation frequency (McGovern & Barto, 2001), graph-related metrics such as betweenness (\u015eim\u015fek & Barto, 2008), or graph partitioning metrics (Menache et al., 2002; Mannor et al., 2004; \u015eim\u015fek et al., 2005).", "startOffset": 236, "endOffset": 300}, {"referenceID": 1, "context": "As future work, we plan to investigate how this algorithm behaves in more complex environments, such as the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 136, "endOffset": 160}], "year": 2016, "abstractText": "Artificial intelligence is commonly defined as the ability to achieve goals in the world. In the reinforcement learning framework, goals are encoded as reward functions that guide agent behaviour, and the sum of observed rewards provide a notion of progress. However, some domains have no such reward signal, or have a reward signal so sparse as to appear absent. Without reward feedback, agent behaviour is typically random, often dithering aimlessly and lacking intentionality. In this paper we present an algorithm capable of learning purposeful behaviour in the absence of rewards. The algorithm proceeds by constructing temporally extended actions (options), through the identification of purposes that are \u201cjust out of reach\u201d of the agents current behaviour. These purposes establish intrinsic goals for the agent to learn, ultimately resulting in a suite of behaviours that encourage the agent to visit different parts of the state space. Moreover, the approach is particularly suited for settings where rewards are very sparse, and such behaviours can help in the exploration of the environment until reward is observed.", "creator": "LaTeX with hyperref package"}}}