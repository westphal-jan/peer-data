{"id": "1605.07891", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2016", "title": "Query Expansion with Locally-Trained Word Embeddings", "abstract": "continuous space word embeddings potentially have many received in a great or deal bit of attention in the developing natural knowledge language processing and theoretical machine learning communities for describing their ability needed to independently model discrete term sense similarity and other knowledge relationships. we study the use of term relatedness agents in resolving the context of retrieval query expansion for geographical information system retrieval. we here demonstrate that word embeddings define such products as deep word2vec * and glove, when trained globally, underperform corpus and query specific vocabulary embeddings need for digital retrieval tasks. these statistical results suggest that with other tasks benefiting individuals from extensive global embeddings may also also benefit from local memory embeddings.", "histories": [["v1", "Wed, 25 May 2016 14:09:00 GMT  (424kb,D)", "http://arxiv.org/abs/1605.07891v1", "ACL 2016, to appear"], ["v2", "Thu, 23 Jun 2016 00:46:06 GMT  (441kb,D)", "http://arxiv.org/abs/1605.07891v2", null]], "COMMENTS": "ACL 2016, to appear", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["fernando diaz 0001", "bhaskar mitra", "nick craswell"], "accepted": true, "id": "1605.07891"}, "pdf": {"name": "1605.07891.pdf", "metadata": {"source": "CRF", "title": "Query Expansion with Locally-Trained Word Embeddings", "authors": ["Fernando Diaz", "Bhaskar Mitra", "Nick Craswell"], "emails": ["fdiaz@microsoft.com", "bmitra@microsoft.com", "nickr@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Continuous space embeddings such as word2vec (Mikolov et al., 2013b) or GloVe (Pennington et al., 2014a) project terms in a vocabulary to a dense, lower dimensional space. Recent results in the natural language processing community demonstrate the effectiveness of these methods for analogy and word similarity tasks. In general, these approaches provide global representations of words. Each word has a fixed representation, regardless of any discourse context. While a global representation provides some advantages, language use can be very topic-specific. For example, ambiguous terms can easily be disambiguated given local information in immediately surrounding words (Harris, 1954; Yarowsky, 1993). Indeed, the window-based training of word2vec style algorithms exploits this distributional property.\nA global word embedding, even when trained using local windows, risks capturing\nonly coarse representations or those dominant in the corpus. While a particular embedding may be appropriate for a specific word within a sentence-length context globally, it may be entirely inappropriate within a specific topic. Gale et al. refer to this as the \u2018one sense per discourse\u2019 property (Gale et al., 1992). Previous work by Yarowsky demonstrates that this property can be successfully combined with information from nearby terms (Yarowsky, 1995). Our work applies this approach for word2vec-style training in the context word similarity, instead of word senses.\nFor any task that requires per-topic analysis, it is crucial to apply methods that work well in the context of each topic. Indeed, it is difficult to imagine a task that would not benefit from an understanding of local topical structure. Our work focuses on a query expansion, an information retrieval task where we can study different lexical similarity methods with an extrinsic evaluation metric (i.e. retrieval metrics).\nWe propose that embeddings be learned on topically-constrained corpora, instead of large topically-unconstrained corpora. In a retrieval scenario, this amounts to retraining an embedding on documents related to the topic of the query. We present local embeddings which capture the nuances of topic-specific language better than global embeddings. Indeed, there is substantial evidence that global methods underperform local methods for information retrieval tasks such as query expansion (Xu and Croft, 1996), cluster-based retrieval (Tombros and van Rijsbergen, 2001; Tombros et al., 2002; Willett, 1985), and term clustering (Attar and Fraenkel, 1977). We demonstrate that the same holds true when using word embeddings for text retrieval. ar X iv :1\n60 5.\n07 89\n1v 1\n[ cs\n.I R\n] 2\n5 M\nay 2\n01 6"}, {"heading": "2 Motivation", "text": "For the purpose of motivating our approach, we will restrict ourselves to word2vec although other methods behave similarly (Levy and Goldberg, 2014). The training procedure for word2vec involves discriminatively training a neural network to predict a word given small set of context words. More formally, given a target word w and observed context c, the instance loss is defined as,\n`(w, c) = log \u03c3(\u03c6(w) \u00b7 \u03c8(c)) + \u03b7 \u00b7 Ew\u223c\u03b8C [log \u03c3(\u2212\u03c6(w) \u00b7 \u03c8(w))]\nwhere \u03c6 : V \u2192 <k projects a term into a kdimensional embedding space, \u03c8 : Vm \u2192 <k projects a set of m terms into a k-dimensional embedding space. The parameter \u03b7 controls the sampling of random negative terms. These matrices are trained over a set of contexts sampled from a large corpus and minimize the expected loss,\nLc = Ew,c\u223cpc [`(w, c)] (1)\nwhere pc(w, c) is the distribution of wordcontext pairs in the training corpus and can be estimated from corpus statistics.\nWhile using corpus statistics may make sense absent any other information, oftentimes we know that our analysis will be topically constrained. For example, we might be analyzing the \u2018sports\u2019 documents in a collection. The language in this domain is more specialized and the distribution over word-context pairs is unlikely to be similar to pc(w, c). In fact, prior work in information retrieval suggests that documents on subtopics in a collection have very different unigram distributions compared to the whole corpus (Cronen-Townsend et al., 2002). Let pt(w, c) be the probability of observing a word-context pair conditioned on the topic t.\nThe expected loss under this distribution is (Shimodaira, 2000),\nLt = Ew,c\u223cpc [ pt(w, c)\npc(w, c) `(w, c)\n] (2)\nIn general, if our corpus consists of sufficiently diverse data (e.g. Wikipedia), the support of\npt(w, c) is much smaller than and contained in that of pc(w, c). The loss, `, of a context that occurs more frequently in the topic, will be amplified by the importance weight \u03c9 = pt(w,c)pc(w,c) . Because topics require specialized language, this is likely to occur; at the same time, these contexts are likely to be underemphasized in training a model according to Equation 1.\nIn order to quantify this, we took a topic from a TREC ad hoc retrieval collection (see Section 5 for details) and computed the importance weight for each term occurring in the set of on-topic documents. The histogram of weights \u03c9 is presented in Figure 1. While larger probabilities are expected since the size of a topic-constrained vocabulary is smaller, there are a non-trivial number of terms with much larger importance weights. If the loss, `(w), of a word2vec embedding is worse for these words with low pc(w), then we expect these errors to be exacerbated for the topic.\nOf course, these highly weighted terms may have a low value for pt(w) but a very high value relative to the corpus. We can adjust the weights by considering the pointwise KullbackLeibler divergence for each word w,\nDw(pt\u2016pc) = pt(w) log pt(w)\npc(w) (3)\nWords which have a much higher value of pt(w) than pc(w) and have a high absolute value of pt(w) will have high pointwise KL\ndivergence. Figure 2 shows the divergences for the top 100 most frequent terms in pt(w). The higher ranked terms, which are also good query expansion candidates, tend to have much higher probabilities than found in pc(w). If the loss on those words is large, this may result in poor embeddings for the most important words for the topic.\nA dramatic change in distribution between the corpus and the topic has implications for performance precisely because of the objective used by word2vec (i.e. Equation 1). The training emphasizes word-context pairs occurring with high frequency in the corpus. We will demonstrate that, even with heuristic downsampling of frequent terms in word2vec, these techniques result in suboptimal performance for specific topics.\nThus far, we have sketched out why using the corpus distribution for a specific topic may result in undesirable outcomes. However, it is even unclear that pt(w|c) = pc(w|c). In fact, we suspect that pt(w|c) 6= pc(w|c) because of the \u2018one sense per discourse\u2019 claim (Gale et al., 1992). We can qualitatively observe the difference in pc(w|c) and pt(w|c) by training two word2vec models: the first on the large, generic Gigaword corpus and the second on a topically-constrained subset of the gigaword. We present the most similar terms to \u2018cut\u2019 using both a global embedding and a topicspecific embedding in Figure 3. In this case, the topic is \u2018gasoline tax\u2019. As we can see, the \u2018tax cut\u2019 sense of \u2018cut\u2019 is emphasized in the\ntopic-specific embedding."}, {"heading": "3 Local Word Embeddings", "text": "The previous section described several reasons why a global embedding may result in overgeneral word embeddings. In order to perform topic-specific training, we need a set of topicspecific documents. In information retrieval scenarios users rarely provide the system with examples of topic-specific documents, instead providing a small set of keywords.\nFortunately, we can use information retrieval techniques to generate a query-specific set of topical documents. Specifically, we adopt a language modeling approach to do so (Croft and Lafferty, 2003). In this retrieval model, each document is represented as a maximum likelihood language model estimated from document term frequencies. Query language models are estimated similarly, using term frequency in the query. A document score then, is the Kullback-Leibler divergence between the query and document language models,\nD(pq\u2016pd) = \u2211 w\u2208V pq(w) log pq(w) pd(w) (4)\nDocuments whose language models are more similar to the query language model will have a lower KL divergence score. For consistency with prior work, we will refer to this as the query likelihood score of a document.\nThe scores in Equation 4 can be passed through a softmax function to derive a multi-\nnomial over the entire corpus (Lavrenko and Croft, 2001),\np(d) = exp(\u2212D(pq\u2016pd))\u2211 d\u2032 exp(\u2212D(pq\u2016pd\u2032))\n(5)\nRecall in Section 2 that training a word2vec model weights word-context pairs according to the corpus frequency. Our query-based multinomial, p(d), provides a weighting function capturing the documents relevant to this topic. Although an estimation of the topicspecific documents from a query will be imprecise (i.e. some nonrelevant documents will be scored highly), the language use tends to be consistent with that found in the known relevant documents (Lavrenko and Croft, 2001). We train an local word embedding by sampling documents from p(d) instead of uniformly from the corpus."}, {"heading": "4 Query Expansion with Word Embeddings", "text": "When using language models for retrieval, query expansion involves estimating an alternative to pq. Specifically, when each expansion term is associated with a weight, we normalize these weights to derive the expansion language model, pq+ . This language model is then interpolated with the original query model,\np1q(w) = \u03bbpq(w) + (1\u2212 \u03bb)pq+(w) (6)\nThis interpolated language model can then be used with Equation 4 to rank documents (Abdul-Jaleel et al., 2004). We will refer to this as the expanded query score of a document.\nNow we turn to using word embeddings for query expansion. Let U be an |V| \u00d7 k term embedding matrix. If q is a |V| \u00d7 1 column term vector for a query, then the expansion term weights are UUTq. We then take the top k terms, normalize their weights, and compute pq+(w).\nWe consider the following alternatives for U. The first approach is to use a globally trained model by sampling documents uniformly. The second approach, which we propose in this paper, is to sample documents according to p(d) and retrain the word2vec model from scratch."}, {"heading": "5 Methods", "text": ""}, {"heading": "5.1 Data", "text": "To evaluate the different retrieval strategies described in Section 3, we use the following datasets. Two newswire datasets, trec12 and robust, consist of the newswire documents and associated queries from TREC ad hoc retrieval evaluations. The trec12 corpus consists of Tipster disks 1 and 2; and the robust corpus consists of Tipster disks 4 and 5. Our third dataset, web, consists of the Clue Web 2009 Category B Web corpus. For the Web corpus, we only retain documents with a Waterloo spam rank above 70.1 We present corpus statistics in Table 1.\nWe consider several publicly available global embeddings. We use four GloVe embeddings of different dimensionality trained on the union of Wikipedia and Gigaword documents.2 We use one publicly available word2vec embedding trained on Google News documents.3 We also trained a global embedding for trec12 and robust using the entire corpus. Instead of training a global embedding on the large web collection, we use a GloVe embedding trained on Common Crawl data.4\nWe train local embeddings using one of three retrieval sources. First, we consider documents retrieved from the target corpus of the query (i.e. trec12, robust, or web). We also consider training a local embedding by performing a retrieval on large auxiliary corpora. We use the Gigaword corpus as a large auxiliary news corpus. We hypothesize that retrieving from a larger news corpus will provide\n1https://plg.uwaterloo.ca/~gvcormac/ clueweb09spam/\n2http://nlp.stanford.edu/data/glove.6B.zip 3https://code.google.com/archive/p/\nword2vec/ 4http://nlp.stanford.edu/data/glove.840B. 300d.zip\nsubstantially more local training data than a target retrieval. We also use a Wikipedia snapshot from December 2014. We hypothesize that retrieving from a large, clean corpus will provide cleaner language than in target domains such as the web. Table 1 shows the relative magnitude of these auxiliary corpora compared to the target corpora.\nAll corpora in Table 1 were stopped using the SMART stopword list5 and stemmed using the Krovetz algorithm (Krovetz, 1993). We used the indri implementation for indexing and retrieval.6"}, {"heading": "5.2 Evaluation", "text": "We consider several standard retrieval evaluation metrics, including NDCG@10 and interpolated precision at standard recall points (Ja\u0308rvelin and Keka\u0308la\u0308inen, 2002; van Rijsbergen, 1979). NDCG@10 provides insight into performance specifically at higher ranks. An interpolated precision recall graph describes system performance throughout the entire ranked list."}, {"heading": "5.3 Training", "text": "All retrieval experiments were conducted by performing 10-fold cross-validation across queries. Specifically, we cross-validate the number of expansion terms, k \u2208 {5, 10, 25, 50, 100, 250, 500}, and interpolation weight, \u03bb \u2208 [0, 1]. For local word2vec training, we cross-validate the learning rate \u03b1 \u2208 {10\u22121, 10\u22122, 10\u22123}.\nAll word2vec training used the publicly available word2vec cbow implementation.7 When training the local models, we sampled 1000 documents from p(d) with replacement. To compensate for the much smaller corpus size, we ran word2vec training for 80 iterations. Local word2vec models use a fixed embedding dimension of 400 although other choices did not significantly affect our results. Unless otherwise noted, default parameter settings were used.\nIn our experiments, expanded queries rescore the top 1000 documents from an initial query likelihood retrieval. Previous results\n5http://jmlr.csail.mit.edu/papers/volume5/ lewis04a/a11-smart-stop-list/english.stop\n6http://www.lemurproject.org/indri/ 7https://code.google.com/p/word2vec/\nhave demonstrated that this approach results in performance nearly identical with an expanded retrieval at a much lower cost (Diaz, 2015). Because publicly available embeddings may have tokenization inconsistent with our target corpora, we restricted the vocabulary of candidate expansion terms to those occurring in the initial retrieval. If a candidate term was not found in the vocabulary of the embedding matrix, we searched for the candidate in a stemmed version of the embedding vocabulary. In the event that the candidate term was still not found after this process, we removed it from consideration."}, {"heading": "6 Results", "text": "We present results for retrieval experiments in Table 2. We find that embedding-based query expansion outperforms our query likelihood baseline across all conditions. When using the global embedding, the news corpora benefit from the various embeddings in different situations. Interestingly, for trec12, using an embedding trained on the target corpus significantly outperforms all other global embeddings, despite use substantially less data to estimate the model. While one reason for this performance may be due to the embedding having a tokenization consistent with the target corpus, another reason may come from the fact that the corpus is more representative of the target documents than other embeddings which rely on online news or are mixed with non-news content. To some extent this supports our desire to move training closer to the target distribution.\nAcross all conditions, local embeddings significantly outperform global embeddings for query expansion. For our two news collections, estimating the local model using a retrieval from the larger Gigaword corpus led to substantial improvements. This effect is almost certainly due to the Gigaword corpus being similar in writing style to the target corpus but, at the same time, providing significantly more relevant content. As a result, the local embedding is trained using a larger variety of topical material than if it were to use a retrieval from the smaller target corpus. An embedding trained with a retrieval from Wikipedia tended to perform\nworse most likely because the language is dissimilar from news content. Our web collection, on the other hand, benefitted more from embeddings trained using retrievals from the general Wikipedia corpus. The Gigaword corpus was less useful here because news-style language is almost certainly not representative of general web documents.\nFigure 4 presents interpolated precisionrecall curves comparing the baseline, the best global query expansion method, and the best local query expansion method. Interestingly, although global methods achieve strong performance for NDCG@10, these improvements over the baseline are not reflected in our precision-recall curves. Local methods, on the other hand, almost always strictly dominate both the baseline and global expansion across all recall levels.\nThe results support the hypothesis that local embeddings provide better similarity measures than global embeddings for query expansion. In order to understand why, we first compare the performance differences between local\nand global embeddings. Figure 2 suggests that we should adopt a local embedding when the local unigram language model deviates from the corpus language model. To test this, we computed the KL divergence between the local unigram distribution, \u2211 d p(w|d)p(d), and the corpus unigram language model (CronenTownsend et al., 2002). We hypothesize that, when this value is high, the global embedding will be inferior to the local embedding. Therefore, we tested the rank correlation between this KL divergence and the relative performance of the local embedding with respect to the global embedding. These correlations are presented in Table 3. Unfortunately, we find that the correlation is low, although it is positive across collections.\nWe can also qualitatively analyze the differences in behavior of the embeddings. If we have access to the set of documents labeled relevant to a query, then we can compute the frequency of terms in this set and consider those terms with high frequency (after stopping and stemming) to be good query expansion can-\ndidates. We can then visualize where these terms lie in the global and local embeddings. In Figure 5, we present a two-dimensional projection (van der Maaten and Hinton, 2008) of terms for the query \u2018ocean remote sensing\u2019, with those good candidates highlighted. Our projection includes the top 50 candidates by frequency and a sample of terms occurring in the query likelihood retrieval. We notice that, in the global embedding, the good candidates are spread out amongst poorer candidates. By contrast, the local embedding clusters the candidates in general but also situates them closely around the query. As a result, we suspect that the similar terms extracted from the local embedding are more likely to include these good candidates."}, {"heading": "7 Discussion", "text": "The success of local embeddings on this task should alarm natural language processing researchers using global embeddings as a representational tool. For one, the approach of learning from vast amounts of data is only effective if the data is appropriate for the task at hand. And, when provided, much smaller high-quality data can provide much better performance. Beyond this, our results suggest that the approach of estimating global representations, while computationally convenient, may overlook insights possible at query time, or evaluation time in general. A similar local embedding approach can be adopted for any natural language processing task where topical locality is expected and can be estimated. Although we used a query to re-weight the corpus in our experiments, we could just as easily use alternative contextual information in other tasks.\nglobal\nDespite these strong results, we believe there are still some open questions in this work. First, although local embeddings provide effectiveness gains, they can be quite inefficient compared to one-time global embedding computation. We believe there is opportunity to improve the efficiency by considering offline computation of local embeddings at a coarser level than queries but more specialized than the corpus. If the retrieval algorithm is able to select the appropriate embedding at query time, we can avoid training the local embedding. Second, although our secondary experiments present some support for our intuition, the results are not strong enough to provide a solid explanation. Further theoretical and empirical analysis is necessary."}, {"heading": "8 Related Work", "text": "Topical adaptation of models The shortcomings of learning a single global vector representation, especially for polysemic words, have been pointed out before (Reisinger and Mooney, 2010b). The problem can be addressed by training a global model with multiple vector embeddings per word (Reisinger and Mooney, 2010a; Huang et al., 2012). The number of senses for each word may be fixed (Neelakantan et al., 2015), or determined using class labels (Trask et al., 2015). However, to the best of our knowledge, this is the first time that training topic-specific word embeddings has been explored.\nSeveral methods exist in the language modeling community for topic-dependent adaptation of language models (Bellegarda, 2004). These can lead to performance improvements in tasks such as machine translation (Zhao et al., 2004) and speech recognition (Nanjo and Kawahara, 2004). Topic-specific data may be gathered in advance, by identifying corpus of topic-specific documents. It may also be gathered during the discourse, using multiple hypotheses from N-best lists as a source of topicspecific language. Then a topic-specific language model is trained (or the global model is adapted) online using the topic-specific training data. A topic-dependent model may be combined with the global model using linear interpolation (Iyer and Ostendorf, 1999) or other more sophisticated approaches (Federico, 1996; Kuhn and De Mori, 1990). Similarly to the adaptation work, we use topicspecific documents to train a topic-specific model. In our case the documents come from a first round of retrieval for the user\u2019s current query, and the word embedding model is trained based on sentences from the topicspecific document set. Unlike the past work, we do not focus on interpolating the local and global models, although this is a promising area for future work. In the current study we focus on a direct comparison between the local-only and global-only approach, for improving retrieval performance.\nWord embeddings for IR Information Retrieval has a long history of learning representations of words that are low-dimensional\ndense vectors. These approaches can be broadly classified into two families based on whether they are learnt based on a termdocument matrix or term-cooccurence data. Using the term-document matrix for embedding leads to several well-studied approaches such as LSA (Deerwester et al., 1990), PLSA (Hofmann, 1999), and LDA (Blei et al., 2003; Wei and Croft, 2006). The performance of these models varies depending on the task, for example they are known to perform poorly for retrieval tasks unless combined with lexical features (Atreya and Elkan, 2011). Term-cooccurence based embeddings, such as word2vec (Mikolov et al., 2013b; Mikolov et al., 2013a) and (Pennington et al., 2014b), have recently been remarkably popular for many natural language processing and logical reasoning tasks. However, there are relatively less known successful applications of these models in IR. Ganguly et. al. (Ganguly et al., 2015) used the word similarity in the word2vec embedding space as a way to estimate term transformation probabilities in a language modelling setting for retrieval. More recently, Nalisnick et. al. (Nalisnick et al., 2016) proposed to model document aboutness by computing the similarity between all pairs of query and document terms using dual embedding spaces. Both these approaches estimate the semantic relatedness between two terms as the cosine distance between them in the embedding space(s). We adopt a similar notion of term relatedness but focus on demonstrating improved retrieval performance using locally trained embeddings."}, {"heading": "9 Conclusion", "text": "We have demonstrated a simple and effective method for performing query expansion with word embeddings. Importantly, our results highlight the value of locally-training word embeddings in a query-specific manner. The strength of these results suggests that other research adopting global embedding vectors should consider local embeddings as a potentially superior representation. Although embedding techniques like word2vec have been referred to as the \u2018Sriracha sauce of deep learning\u2019, we contend that perhaps a dish may taste better with its own unique sauce."}], "references": [{"title": "Latent semantic indexing (lsi) fails for trec collections", "author": ["Atreya", "Elkan2011] Avinash Atreya", "Charles Elkan"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Atreya et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Atreya et al\\.", "year": 2011}, {"title": "Local feedback in full-text retrieval systems", "author": ["Attar", "Fraenkel1977] R. Attar", "A.S. Fraenkel"], "venue": "J. ACM,", "citeRegEx": "Attar et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Attar et al\\.", "year": 1977}, {"title": "Statistical language model adaptation: review and perspectives", "author": ["Jerome R Bellegarda"], "venue": "Speech communication,", "citeRegEx": "Bellegarda.,? \\Q2004\\E", "shortCiteRegEx": "Bellegarda.", "year": 2004}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Language Modeling for Information Retrieval", "author": ["Croft", "Lafferty2003] W. Bruce Croft", "John Lafferty"], "venue": null, "citeRegEx": "Croft et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Croft et al\\.", "year": 2003}, {"title": "Predicting query performance", "author": ["Yun Zhou", "W. Bruce Croft"], "venue": "In SIGIR \u201902: Proceedings of the 25th annual international ACM SIGIR conference on Research", "citeRegEx": "CronenTownsend et al\\.,? \\Q2002\\E", "shortCiteRegEx": "CronenTownsend et al\\.", "year": 2002}, {"title": "Indexing by latent semantic analysis", "author": ["Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman"], "venue": "Journal of the American Society of Information", "citeRegEx": "Deerwester et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Condensed list relevance models", "author": ["Fernando Diaz"], "venue": "In Proceedings of the 2015 International Conference on The Theory of Information Retrieval,", "citeRegEx": "Diaz.,? \\Q2015\\E", "shortCiteRegEx": "Diaz.", "year": 2015}, {"title": "Bayesian estimation methods for n-gram language model adaptation", "author": ["Marcello Federico"], "venue": "In Spoken Language,", "citeRegEx": "Federico.,? \\Q1996\\E", "shortCiteRegEx": "Federico.", "year": 1996}, {"title": "One sense per discourse", "author": ["Gale et al.1992] William A. Gale", "Kenneth W. Church", "David Yarowsky"], "venue": "In Proceedings of the Workshop on Speech and Natural Language,", "citeRegEx": "Gale et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Gale et al\\.", "year": 1992}, {"title": "Word embedding based generalized language model for information retrieval", "author": ["Dwaipayan Roy", "Mandar Mitra", "Gareth J.F. Jones"], "venue": "In Proceedings of the 38th International ACM SIGIR", "citeRegEx": "Ganguly et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ganguly et al\\.", "year": 2015}, {"title": "Probabilistic latent semantic indexing", "author": ["Thomas Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Hofmann.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann.", "year": 1999}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["Huang et al.2012] Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the As-", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Modeling long distance dependence in language: topic mixtures versus dynamic cache models", "author": ["Iyer", "Ostendorf1999] R.M. Iyer", "M. Ostendorf"], "venue": "Speech and Audio Processing, IEEE Transactions on,", "citeRegEx": "Iyer et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 1999}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["J\u00e4rvelin", "Kek\u00e4l\u00e4inen2002] Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": null, "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Viewing morphology as an inference process", "author": ["Robert Krovetz"], "venue": "Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Krovetz.,? \\Q1993\\E", "shortCiteRegEx": "Krovetz.", "year": 1993}, {"title": "A cache-based natural language model for speech recognition", "author": ["Kuhn", "De Mori1990] Roland Kuhn", "Renato De Mori"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions", "citeRegEx": "Kuhn et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kuhn et al\\.", "year": 1990}, {"title": "Relevance based language models", "author": ["Lavrenko", "Croft2001] Victor Lavrenko", "W. Bruce Croft"], "venue": "In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information", "citeRegEx": "Lavrenko et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lavrenko et al\\.", "year": 2001}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improving document ranking with dual word embeddings", "author": ["Bhaskar Mitra", "Nick Craswell", "Rich Caruana"], "venue": "In Proc. WWW. International World Wide Web Conferences Steering Commit-", "citeRegEx": "Nalisnick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nalisnick et al\\.", "year": 2016}, {"title": "Language model and speaking rate adaptation for spontaneous presentation speech recognition", "author": ["Nanjo", "Kawahara2004] Hiroaki Nanjo", "Tatsuya Kawahara"], "venue": "Speech and Audio Processing, IEEE Transactions on,", "citeRegEx": "Nanjo et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Nanjo et al\\.", "year": 2004}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space. arXiv preprint arXiv:1504.06654", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": null, "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proc. EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A mixture model with sharing for lexical semantics", "author": ["Reisinger", "Mooney2010a] Joseph Reisinger", "Raymond Mooney"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Multiprototype vector-space models of word meaning", "author": ["Reisinger", "Mooney2010b] Joseph Reisinger", "Raymond J Mooney"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American", "citeRegEx": "Reisinger et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reisinger et al\\.", "year": 2010}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["Hidetoshi Shimodaira"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Shimodaira.,? \\Q2000\\E", "shortCiteRegEx": "Shimodaira.", "year": 2000}, {"title": "Query-sensitive similarity measures for the calculation of interdocument relationships", "author": ["Tombros", "C.J. van Rijsbergen"], "venue": "In CIKM", "citeRegEx": "Tombros et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Tombros et al\\.", "year": 2001}, {"title": "The effectiveness of query-specific hierarchic clustering in information retrieval", "author": ["Robert Villa", "C.J. Van Rijsbergen"], "venue": "Inf. Process. Manage.,", "citeRegEx": "Tombros et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tombros et al\\.", "year": 2002}, {"title": "sense2vec-a fast and accurate method for word sense disambiguation in neural word embeddings", "author": ["Trask et al.2015] Andrew Trask", "Phil Michalak", "John Liu"], "venue": "arXiv preprint arXiv:1511.06388", "citeRegEx": "Trask et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Trask et al\\.", "year": 2015}, {"title": "Visualizing high-dimensional data using t-sne", "author": ["van der Maaten", "Geoffrey E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "LDA-based document models for ad-hoc retrieval", "author": ["Wei", "Croft2006] Xing Wei", "W. Bruce Croft"], "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development", "citeRegEx": "Wei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wei et al\\.", "year": 2006}, {"title": "Query-specific automatic document classification", "author": ["Peter Willett"], "venue": "In International Forum on Information and Documentation,", "citeRegEx": "Willett.,? \\Q1985\\E", "shortCiteRegEx": "Willett.", "year": 1985}, {"title": "Query expansion using local and global document analysis", "author": ["Xu", "Croft1996] Jinxi Xu", "W. Bruce Croft"], "venue": "In Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information", "citeRegEx": "Xu et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Xu et al\\.", "year": 1996}, {"title": "One sense per collocation", "author": ["David Yarowsky"], "venue": "In Proceedings of the Workshop on Human Language Technology,", "citeRegEx": "Yarowsky.,? \\Q1993\\E", "shortCiteRegEx": "Yarowsky.", "year": 1993}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["David Yarowsky"], "venue": "In Proceedings of the 33rd Annual Meeting on Association", "citeRegEx": "Yarowsky.,? \\Q1995\\E", "shortCiteRegEx": "Yarowsky.", "year": 1995}, {"title": "Language model adaptation for statistical machine translation with structured query models", "author": ["Zhao et al.2004] Bing Zhao", "Matthias Eck", "Stephan Vogel"], "venue": "In Proceedings of the 20th International Conference on Compu-", "citeRegEx": "Zhao et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 36, "context": "immediately surrounding words (Harris, 1954; Yarowsky, 1993).", "startOffset": 30, "endOffset": 60}, {"referenceID": 9, "context": "refer to this as the \u2018one sense per discourse\u2019 property (Gale et al., 1992).", "startOffset": 56, "endOffset": 75}, {"referenceID": 37, "context": "Previous work by Yarowsky demonstrates that this property can be successfully combined with information from nearby terms (Yarowsky, 1995).", "startOffset": 122, "endOffset": 138}, {"referenceID": 30, "context": "sion (Xu and Croft, 1996), cluster-based retrieval (Tombros and van Rijsbergen, 2001; Tombros et al., 2002; Willett, 1985), and term clustering (Attar and Fraenkel, 1977).", "startOffset": 51, "endOffset": 122}, {"referenceID": 34, "context": "sion (Xu and Croft, 1996), cluster-based retrieval (Tombros and van Rijsbergen, 2001; Tombros et al., 2002; Willett, 1985), and term clustering (Attar and Fraenkel, 1977).", "startOffset": 51, "endOffset": 122}, {"referenceID": 28, "context": "The expected loss under this distribution is (Shimodaira, 2000),", "startOffset": 45, "endOffset": 63}, {"referenceID": 9, "context": "In fact, we suspect that pt(w|c) 6= pc(w|c) because of the \u2018one sense per discourse\u2019 claim (Gale et al., 1992).", "startOffset": 91, "endOffset": 110}, {"referenceID": 15, "context": "All corpora in Table 1 were stopped using the SMART stopword list5 and stemmed using the Krovetz algorithm (Krovetz, 1993).", "startOffset": 107, "endOffset": 122}, {"referenceID": 7, "context": "in performance nearly identical with an expanded retrieval at a much lower cost (Diaz, 2015).", "startOffset": 80, "endOffset": 92}, {"referenceID": 5, "context": "To test this, we computed the KL divergence between the local unigram distribution, \u2211 d p(w|d)p(d), and the corpus unigram language model (CronenTownsend et al., 2002).", "startOffset": 138, "endOffset": 167}, {"referenceID": 12, "context": "The problem can be addressed by training a global model with multiple vector embeddings per word (Reisinger and Mooney, 2010a; Huang et al., 2012).", "startOffset": 97, "endOffset": 146}, {"referenceID": 23, "context": "number of senses for each word may be fixed (Neelakantan et al., 2015), or determined using class labels (Trask et al.", "startOffset": 44, "endOffset": 70}, {"referenceID": 31, "context": ", 2015), or determined using class labels (Trask et al., 2015).", "startOffset": 42, "endOffset": 62}, {"referenceID": 2, "context": "Several methods exist in the language modeling community for topic-dependent adaptation of language models (Bellegarda, 2004).", "startOffset": 107, "endOffset": 125}, {"referenceID": 38, "context": "These can lead to performance improvements in tasks such as machine translation (Zhao et al., 2004) and speech recognition (Nanjo and Kawahara, 2004).", "startOffset": 80, "endOffset": 99}, {"referenceID": 8, "context": "or other more sophisticated approaches (Federico, 1996; Kuhn and De Mori, 1990).", "startOffset": 39, "endOffset": 79}, {"referenceID": 6, "context": "Using the term-document matrix for embedding leads to several well-studied approaches such as LSA (Deerwester et al., 1990), PLSA (Hofmann, 1999), and LDA (Blei et al.", "startOffset": 98, "endOffset": 123}, {"referenceID": 11, "context": ", 1990), PLSA (Hofmann, 1999), and LDA (Blei et al.", "startOffset": 14, "endOffset": 29}, {"referenceID": 10, "context": "(Ganguly et al., 2015) used the word similarity in the word2vec embedding space as a way to estimate term transformation probabilities in a language modelling setting for retrieval.", "startOffset": 0, "endOffset": 22}, {"referenceID": 21, "context": "(Nalisnick et al., 2016) proposed to model document aboutness by computing the similarity between all pairs of query and document terms using dual embedding spaces.", "startOffset": 0, "endOffset": 24}], "year": 2016, "abstractText": "Continuous space word embeddings have received a great deal of attention in the natural language processing and machine learning communities for their ability to model term similarity and other relationships. We study the use of term relatedness in the context of query expansion for information retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when trained globally, underperform corpus and query specific embeddings for retrieval tasks. These results suggest that other tasks benefiting from global embeddings may also benefit from local embeddings.", "creator": "LaTeX with hyperref package"}}}