{"id": "1605.09066", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2016", "title": "Distributed Asynchronous Dual Free Stochastic Dual Coordinate Ascent", "abstract": "in doing this paper, clearly we propose new distributed asynchronous automatic dual - free coordinate network ascent method ( named asy - independent df \u00d7 sdca ), and efficiently provide together the proof of possible convergence variation rate for two cases : the designated individual loss defect is specially convex posterior and the assumed individual loss information is appropriately non - convex but typically its expected local loss is genuinely convex. stochastic dual coordinate distance ascent ( sdca ) model is a popular comparison method before and often has better performances relative than stochastic gradient descent attack methods in adequately solving regularized independent convex loss - minimization convergence problems. dual - peak free stochastic dual alternative coordinate ascent method is a natural variation procedure of sdca, and too can be generally applied to non - convex problem when its automated dual function problem method is becoming meaningless. basically we independently extend dual - free optimal stochastic dual coordinate ascent method to the distributed mode with considering the indifference star valley network in preparing this paper.", "histories": [["v1", "Sun, 29 May 2016 21:33:07 GMT  (7kb)", "http://arxiv.org/abs/1605.09066v1", null], ["v2", "Wed, 27 Jul 2016 03:29:57 GMT  (509kb)", "http://arxiv.org/abs/1605.09066v2", null], ["v3", "Sat, 19 Nov 2016 20:32:35 GMT  (511kb)", "http://arxiv.org/abs/1605.09066v3", null], ["v4", "Fri, 27 Oct 2017 03:06:07 GMT  (386kb)", "http://arxiv.org/abs/1605.09066v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhouyuan huo", "heng huang"], "accepted": false, "id": "1605.09066"}, "pdf": {"name": "1605.09066.pdf", "metadata": {"source": "CRF", "title": "Distributed Asynchronous Stochastic Dual Coordinate Ascent without Duality", "authors": ["Zhouyuan Huo"], "emails": ["zhouyuan.huo@mavs.uta.edu", "heng@uta.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 5.\n09 06\n6v 1\n[ cs\n.L G\n] 2\n1 Introduction\nWe consider the following \u21132-norm regularized loss minimization problem:\nmin w\u2208Rd\nP (w) = 1\nn\nn\u2211\ni=1\n\u03c6i(w) + \u03bb\n2 \u2016w\u20162 . (1)\nMany optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18]. Experimental results in [18] verify that SDCA method enjoys strong theoretical convergence guarantee properties and often has better performances than stochastic gradient descent (SGD) based methods. In [6], the paper points out that SDCA is a variation of SGD method, and its update is based on an unbiased estimate of gradient. Unlike most of the SGD methods which solve primal problem directly, as its name indicates, SDCA is derived by considering a dual problem of (1). However, the dual problem of \u03c6i is meaningless sometimes. In [13], a variation of SDCA was proposed and applied to problems in which individual \u03c6i is non-convex.\nRecently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17]. There are mainly two architectures in distributed system: one is shared-memory architecture, and the other one is distributed-memory\narchitecture. In this paper, we only consider distributed-memory architecture. In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when \u03c6i is smooth and convex.\nIn this paper, we propose a Distributed Asynchronous Dual-Free Coordinate Ascent (Asy-df SDCA) method. The corresponding convergence analysis is provided on two different assumptions: one is that \u03c6i is L-smooth and convex, and the other one is that \u03c6i is L-smooth and non-convex, but the average of \u03c6i is strongly convex.\n2 Asynchronous Dual Free Stochastic Dual Coordinate Ascent Method\nDetails of our proposed Distributed Asynchronous Dual-Free Coordinate Ascent method (Asy-df SDCA) are described in Algorithms (1) and (2). Algorithm (1) presents the pseudo code of Asy-df SDCA on each worker node. \u03b1i \u2208 Rd, i \u2208 {1, \u00b7 \u00b7 \u00b7 , n} denotes pseudo-dual vector for each sample, and they are maintained by workers. We assume datasets are evenly distributed in K workers, and there are nk samples in worker k. Algorithm (1) summarizes the pseudo code on server node. Parameter w is maintained in the server, and vi represents update received from workers in each iteration.\nAlgorithm 1 Asy-df SDCA (Worker k)\nInitialize \u03b10,0i \u2208 R d, i \u2208 {1, \u00b7 \u00b7 \u00b7 , nk} for s = 1, 2, \u00b7 \u00b7 \u00b7 , S do for t = 1, 2, \u00b7 \u00b7 \u00b7 , nk do\nPull ws,t\u2212\u03c4 from server. Randomly select sample i from {1, \u00b7 \u00b7 \u00b7 , nk}; v s,t i = \u2207\u03c6i(w\ns,t\u2212\u03c4 ) + \u03b1s,t\u2212\u03c4i Update \u03b1s,ti = \u03b1 s,t\u22121 i \u2212 \u03bb\u03b7nv s,t i\nPush vs,ti to server. end for\nend for\nAlgorithm 2 Asy-df SDCA (Server)\nInitialize w0,0 \u2208 Rd. for s = 1, 2, \u00b7 \u00b7 \u00b7 , S do\nfor t = 1, 2, \u00b7 \u00b7 \u00b7 , n do Receive vs,ti from worker. Update ws,t = ws,t\u22121 \u2212 \u03b7vs,ti end for ws+1,0 = ws,n\nend for\n3 Convergence Analysis\nWe provide convergence analysis of our proposed method on two different cases: (1) \u03c6i is L-smooth and convex, and (2) \u03c6i is L-smooth and non-convex, but the average of \u03c6i is strongly convex.\n3.1 Convex Case\nFor further analysis, in this section, we make the following assumptions for problem (1). All of them are common assumptions in the theoretical analysis of distributed methods and stochastic gradient method.\nAssumption 1 We assume the following conditions hold:\n\u2022 \u03c6i is L-smooth,\n\u2016\u2207\u03c6(x) \u2212\u2207\u03c6i(y)\u2016 2 \u2264 L2\u2016x\u2212 y\u20162 . (2)\n\u2022 \u03c6i is convex,\n\u03c6i(x) \u2265 \u03c6i(y) +\u2207\u03c6i(y) T (x \u2212 y) . (3)\n\u2022 Time delay \u03c4 is no larger than \u2206.\nFollowing the above assumptions, we know that our method is able to have linear convergence rate with the following theorem.\nTheorem 1 When the above assumptions satisfy, and let w\u2217 be the minimizer of P (w), \u03b1\u2217i = \u2212\u2207\u03c6i(w \u2217). If \u03b7 \u2264 1 2L+n\u03bb+4L\u2206 , then we have\nE\n[\n\u2016ws,0 \u2212 w\u2217\u20162 + 1 2\u03bbL\nn\u2211\ni=1\n\u2016\u03b1s,0i \u2212 \u03b1 \u2217 i \u2016 2\n]\n\u2264 e\u2212\u03b7\u03bbs [\n\u2016w0,0 \u2212 w\u2217\u20162 + 1 2\u03bbL\nn\u2211\ni=1\n\u2016\u03b10,0i \u2212 \u03b1 \u2217 i \u2016 2\n]\n. (4)\n3.2 Non-Convex Case\nFor further analysis, in this section, we make the following assumptions for problem (1).\nAssumption 2 We assume the following conditions holds:\n\u2022 \u03c6i is L-smooth,\n\u2016\u2207\u03c6(x) \u2212\u2207\u03c6i(y)\u2016 2 \u2264 L2\u2016x\u2212 y\u20162 . (5)\n\u2022 \u03c6i is non-convex, and the average of \u03c6i is \u03b3-strongly convex,\n1\nn\nn\u2211\ni=1\n\u03c6i(x) \u2265 1\nn\nn\u2211\ni=1\n\u03c6i(y) + 1\nn\nn\u2211\ni=1\n\u2207\u03c6i(y) T (x \u2212 y) +\n\u03b3 2 \u2016x\u2212 y\u20162 . (6)\n\u2022 Time delay \u03c4 is no larger than \u2206.\nFollowing the above assumptions, we know that our method is able to have linear convergence rate with the following theorem.\nTheorem 2 When the above assumptions satisfy, and let w\u2217 be the minimizer of P (w), \u03b1\u2217i = \u2212\u2207\u03c6i(w\n\u2217). If \u03b7 satisfies that: (\n\u22062 + 2\u03b3\u22062\n\u03b3 \u2212 \u03bb\n)\n\u03b72 +\n( 2\u2206+ 1\n\u03b3 \u2212 \u03bb +\nn\n2L2\n)\n\u03b7 \u2212 1\n2L2 \u2264 0 , (7)\nthen we have the final conclusion:\nE\n[\n1 \u03b3\u2212\u03bb \u2016ws,0 \u2212 w\u2217\u20162 + 1 2\u03bbL2n\nn\u2211\ni=1\n\u2016\u03b1s,0i \u2212 \u03b1 \u2217 i \u2016 2\n]\n\u2264 e\u2212\u03b7\u03bbs [ 1\n\u03b3\u2212\u03bb \u2016w0,0 \u2212 w\u2217\u20162 + 1 2\u03bbL2n\nn\u2211\ni=1\n\u2016\u03b10,0i \u2212 \u03b1 \u2217 i \u2016 2\n]\n. (8)\nReferences\n[1] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems, pages 873\u2013881, 2011.\n[2] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646\u20131654, 2014.\n[3] Mingyi Hong. A distributed, asynchronous and incremental algorithm for nonconvex optimization: An admm based approach. arXiv preprint arXiv:1412.6058, 2014.\n[4] Zhouyuan Huo and Heng Huang. Asynchronous stochastic gradient descent with variance reduction for non-convex optimization. arXiv preprint arXiv:1604.03584, 2016.\n[5] Martin Jaggi, Virginia Smith, Martin Taka\u0301c, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I Jordan. Communication-efficient distributed dual coordinate ascent. In Advances in Neural Information Processing Systems, pages 3068\u20133076, 2014.\n[6] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315\u2013323, 2013.\n[7] John Langford, Alexander Smola, and Martin Zinkevich. Slow learners are fast. arXiv preprint arXiv:0911.0491, 2009.\n[8] Mu Li, David G Andersen, Alex J Smola, and Kai Yu. Communication efficient distributed machine learning with the parameter server. In Advances in Neural Information Processing Systems, pages 19\u201327, 2014.\n[9] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2719\u20132727, 2015.\n[10] Ji Liu, Stephen J Wright, and Srikrishna Sridhar. An asynchronous parallel randomized kaczmarz algorithm. arXiv preprint arXiv:1401.4780, 2014.\n[11] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693\u2013701, 2011.\n[12] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. arXiv preprint arXiv:1309.2388, 2013.\n[13] Shai Shalev-Shwartz. Sdca without duality. arXiv preprint arXiv:1502.06177, 2015.\n[14] Shai Shalev-Shwartz and Tong Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems, pages 378\u2013385, 2013.\n[15] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss. The Journal of Machine Learning Research, 14(1):567\u2013599, 2013.\n[16] Martin Taka\u0301c\u030c, Avleen Bijral, Peter Richta\u0301rik, and Nathan Srebro. Mini-batch primal and dual methods for svms. arXiv preprint arXiv:1303.2314, 2013.\n[17] Martin Taka\u0301c, Peter Richta\u0301rik, and Nathan Srebro. Distributed mini-batch sdca. arXiv preprint arXiv:1507.08322, 2015.\n[18] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057\u20132075, 2014.\n[19] Tianbao Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems, pages 629\u2013637, 2013.\n[20] Ruiliang Zhang and James Kwok. Asynchronous distributed admm for consensus optimization. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1701\u20131709, 2014.\n[21] Ruiliang Zhang, Shuai Zheng, and James T Kwok. Fast distributed asynchronous sgd with variance reduction. arXiv preprint arXiv:1508.01633, 2015.\n[22] Shen-Yi Zhao and Wu-Jun Li. Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee. 2016.\nA Proof of Theorem 1\nLemma 3 Assuming that each \u03c6i is L-smooth and convex, for every w, we have:\n1\nn\nn\u2211\ni=1\n\u2016\u2207\u03c6i(w) \u2212\u2207\u03c6i(w \u2217)\u20162 \u2264 2L\n(\nP (w)\u2212 P (w\u2217)\u2212 \u03bb\n2 \u2016w \u2212 w\u2217\u20162\n)\n(9)\nThis lemma was proved in [13].\nProof 1 (Proof of Theorem 1) As per Lemma 3, we know that:\nE[\u2016 \u2207\u03c6i(w t\u2212\u03c4 ) + \u03b1\u2217i \u2016 2] \u2264 E[\u2016 \u2207\u03c6i(w t\u2212\u03c4 )\u2212\u2207\u03c6i(w t\u22121) +\u2207\u03c6i(w t\u22121)\u2212\u2207\u03c6i(w \u2217)\u20162]\n\u2264 2E[\u2016 \u2207\u03c6i(w t\u2212\u03c4 )\u2212\u2207\u03c6i(w t\u22121)\u20162] + 2E[\u2016 \u2207\u03c6i(w t\u22121)\u2212\u2207\u03c6i(w \u2217)\u20162]\n\u2264 4L\n(\nP (wt\u2212\u03c4 )\u2212 P (wt\u22121)\u2212 \u03bb\n2 E[\u2016wt\u2212\u03c4 \u2212 wt\u22121\u20162]\n)\n+ 4L\n(\nP (wt\u22121)\u2212 P (w\u2217)\u2212 \u03bb\n2 E[\u2016wt\u22121 \u2212 w\u2217\u20162]\n)\n\u2264 4L(P (wt\u2212\u03c4 )\u2212 P (w\u2217))\u2212 2\u03bbLE[\u2016wt\u22121 \u2212 w\u2217\u20162] (10)\nE[(wt\u22121 \u2212 w\u2217)T vti ] = E[(w t\u22121 \u2212 wt\u2212\u03c4 + wt\u2212\u03c4 \u2212 w\u2217)T vti ]\n= \u03b7\nt\u22121\u2211\nj=t\u2212\u03c4+1\nE[(vjij ) T vti ] + (w t\u2212\u03c4 \u2212 w\u2217)\u2207P (wt\u2212\u03c4 )\n\u2265 \u03b7\nt\u22121\u2211\nj=t\u2212\u03c4+1\nE[(vjij ) T vti ] + P (w t\u2212\u03c4 )\u2212 P (w\u2217) (11)\nwhere the final inequality follows from convexity of P (w). Let\u2019s define Ct = caAt + cbBt, and take expectation over i:\nE[Ct] = E\n[\nca(1\u2212 \u03b7\u03bb)At\u22121 + ca\u03b7\u03bb\u2016\u2207\u03c6i(w t\u2212\u03c4 ) + \u03b1\u2217i \u2016 2 \u2212 ca\u03b7\u03bb(1 \u2212 \u03b2)\u2016v t i\u2016 2\n+ cbBt\u22121 \u2212 2cb\u03b7(w t\u22121 \u2212 w\u2217)T vti + cb\u03b7 2\u2016vti\u2016 2\n]\n\u2264 E\n[\nca(1\u2212 \u03b7\u03bb)At\u22121 + ca\u03b7\u03bb\n(\n4L(P (wt\u2212\u03c4 )\u2212 P (w\u2217))\u2212 2\u03bbLE[\u2016wt\u22121 \u2212 w\u2217\u20162]\n)\n\u2212 ca\u03b7\u03bb(1 \u2212 \u03b2)\u2016v t i\u2016 2 + cbBt\u22121 + cb\u03b7 2\u2016vti\u2016 2\n\u2212 2cb\u03b7\n\n\u03b7\nt\u22121\u2211\nj=t\u2212\u03c4+1\nE[(vjij ) T vti ] + P (w t\u2212\u03c4 )\u2212 P (w\u2217)\n\n\n]\n\u2264 ca(1 \u2212 \u03b7\u03bb)E[At\u22121] + (cb \u2212 2ca\u03b7L\u03bb 2)E[Bt\u22121] + ( cb\u03b7 2 \u2212 ca\u03b7\u03bb(1 \u2212 \u03b2) + cb\u2206\u03b7 2 ) E[\u2016vti\u2016 2]\n+ (4ca\u03b7\u03bbL \u2212 2cb\u03b7) ( P (wt\u2212\u03c4 )\u2212 P (w\u2217) ) + cb\u03b7 2\nt\u22121\u2211\nj=t\u2212\u03c4+1\nE[\u2016vjij\u2016 2] (12)\nSumming over E[Ct], we have:\nn\u2211\nt=1\nE[Ct] \u2264\nn\u2211\nt=1\n( ca(1\u2212 \u03b7\u03bb)E[At\u22121] + (cb \u2212 2ca\u03b7L\u03bb 2)E[Bt\u22121] )\n+ n\u2211\nt=1\n(4ca\u03b7\u03bbL \u2212 2cb\u03b7) ( P (wt\u2212\u03c4 )\u2212 P (w\u2217) )\n+\nn\u2211\nt=1\n( cb\u03b7 2 \u2212 ca\u03b7\u03bb(1 \u2212 \u03b2) + 2cb\u2206\u03b7 2 ) E[\u2016vti\u2016 2] (13)\nWe denote\ncb \u2212 2ca\u03b7L\u03bb 2 = cb(1\u2212 \u03b7\u03bb) (14)\ncb\u03b7 2 \u2212 ca\u03b7\u03bb(1 \u2212 \u03b2) + 2cb\u2206\u03b7 2 \u2264 0 . (15)\nTherefore, if cb = 2ca\u03bbL and \u03b7 \u2264 12L+n\u03bb+4L\u2206 , we have:\nn\u2211\nt=1\nE[Ct] \u2264 (1\u2212 \u03b7\u03bb) n\u2211\nt=1\nE[Ct \u2212 1]\n\u2264\nn\u2211\nt=2\nE[Ct \u2212 1] + (1\u2212 \u03b7\u03bb)E[C0] (16)\nThus\nE[Cn] \u2264 (1\u2212 \u03b7\u03bb)E[C0] (17)\nBecause E[Cn] = E[Cs+1,0] and E[C0] = E[Cs,0], we have:\nE[Cs,0] \u2264 (1\u2212 \u03b7\u03bb)E[Cs\u22121,0]\n\u2264 (1\u2212 \u03b7\u03bb)sC0,0 \u2264 e\u2212\u03b7\u03bbsC0,0 (18)\nLet ca = 12L\u03bb and cb = 1, then we have the final conclusion:\nE\n[\n\u2016ws,0 \u2212 w\u2217\u20162 + 1 2\u03bbL\nn\u2211\ni=1\n\u2016\u03b1s,0i \u2212 \u03b1 \u2217 i \u2016 2\n]\n\u2264 e\u2212\u03b7\u03bbs [\n\u2016w0,0 \u2212 w\u2217\u20162 + 1 2\u03bbL\nn\u2211\ni=1\n\u2016\u03b10,0i \u2212 \u03b1 \u2217 i \u2016 2\n]\n(19)\nB Proof of Theorem 2\nProof 2 (Proof of Theorem 2) Let w\u2217 be the minimizer ofP (w) and let\u03b1\u2217i = \u2212\u2207\u03c6i(w \u2217).\nu s,t i = \u2207\u03c6i(w s,t\u22121) + \u03b1s,t\u22121i (20)\nv s,t i = \u2207\u03c6i(w s,t\u2212\u03c4 ) + \u03b1s,t\u2212\u03c4i (21)\nBecause \u03b1i will be not updated in the process from t\u2212 \u03c4 to t\u2212 1, so \u03b1 s,t\u2212\u03c4 i = \u03b1 s,t\u22121 i . In an epoch s, we use wt to denote ws,t, \u03b1ti to denote \u03b1 s,t i , At, Bt, Ct to denote As,t, Bs,t, Cs,t.\nAt = 1\nn\nn\u2211\ni=1\n\u2016\u03b1ti \u2212 \u03b1 \u2217 i \u2016 2 (22)\nBt = \u2016w t \u2212 w\u2217\u20162 (23)\nLet \u03b2 = \u03b7\u03bbn, so in iteration t, \u03b1ti = (1\u2212 \u03b2)\u03b1 t\u22121 i + \u03b2(\u2212\u2207\u03c6i(w t\u2212\u03c4 )).\nAt \u2212At\u22121 = 1\nn \u2016\u03b1ti \u2212 \u03b1 \u2217 i \u2016 2 \u2212\n1 n \u2016\u03b1t\u22121i \u2212 \u03b1 \u2217 i \u2016 2\n= 1\nn \u2016(1\u2212 \u03b2)(\u03b1t\u22121i \u2212 \u03b1 \u2217 i ) + \u03b2(\u2212\u2207\u03c6i(w t\u2212\u03c4 )\u2212 \u03b1\u2217i )\u2016\n2 \u2212 1\nn \u2016\u03b1t\u22121i \u2212 \u03b1 \u2217 i \u2016 2\n= 1\nn\n(\n(1\u2212 \u03b2)\u2016\u03b1t\u22121i \u2212 \u03b1 \u2217 i \u2016 2 + \u03b2\u2016 \u2212 \u2207\u03c6i(w t\u2212\u03c4 )\u2212 \u03b1\u2217i \u2016 2\n\u2212 \u03b2(1 \u2212 \u03b2)\u2016\u03b1t\u22121i +\u2207\u03c6i(w t\u2212\u03c4 )\u20162 \u2212 \u2016\u03b1t\u22121i \u2212 \u03b1 \u2217 i \u2016 2\n)\n= \u03b2\nn\n(\n\u2212\u2016\u03b1t\u22121i \u2212 \u03b1 \u2217 i \u2016 2 + \u2016\u2207\u03c6i(w t\u2212\u03c4 ) + \u03b1\u2217i \u2016 2 \u2212 (1 \u2212 \u03b2)\u2016vti\u2016 2\n)\n= \u03b7\u03bb\n(\n\u2212\u2016\u03b1t\u22121i \u2212 \u03b1 \u2217 i \u2016 2 + \u2016\u2207\u03c6i(w t\u2212\u03c4 ) + \u03b1\u2217i \u2016 2 \u2212 (1\u2212 \u03b2)\u2016vti\u2016 2\n)\n(24)\nIn addition,\nBt \u2212 Bt\u22121 = \u2016w t \u2212 w\u2217\u20162 \u2212 \u2016wt\u22121 \u2212 w\u2217\u20162\n= \u2016wt\u22121 \u2212 \u03b7vti \u2212 w \u2217\u20162 \u2212 \u2016wt\u22121 \u2212 w\u2217\u20162 = \u22122\u03b7(wt\u22121 \u2212 w\u2217)T vti + \u03b7 2\u2016vti\u2016 2 (25)\n\u2016\u2207\u03c6i(w t\u2212\u03c4 ) + \u03b1\u2217i \u2016 2 = \u2016\u2207\u03c6i(w t\u2212\u03c4 )\u2212\u2207\u03c6i(w t\u22121) +\u2207\u03c6i(w t\u22121)\u2212\u2207\u03c6i(w \u2217)\u20162\n\u2264 2\u2016\u2207\u03c6i(w t\u2212\u03c4 )\u2212\u2207\u03c6i(w t\u22121)\u20162 + 2\u2016\u2207\u03c6i(w t\u22121)\u2212\u2207\u03c6i(w \u2217)\u20162 \u2264 2L2\u2016wt\u2212\u03c4 \u2212 wt\u22121\u20162 + 2L2\u2016wt\u22121 \u2212 w\u2217\u20162\n\u2264 2L2\u2016\nt\u22121\u2211\nj=t\u2212\u03c4+1\n(wj \u2212 wj\u22121)\u20162 + 2L2\u2016wt\u22121 \u2212 w\u2217\u20162\n\u2264 2L2\u2206\nt\u22121\u2211\nj=t\u2212\u03c4+1\n\u2016wj \u2212 wj\u22121\u20162 + 2L2\u2016wt\u22121 \u2212 w\u2217\u20162\n\u2264 2\u2206\u03b72L2 t\u22121\u2211\nj=t\u2212\u03c4+1\n\u2016vjij\u2016 2 + 2L2\u2016wt\u22121 \u2212 w\u2217\u20162 (26)\nwhere the first and fourth inequality follows from that \u2016 n\u2211\ni=1\nai\u2016 2 \u2264 n\nn\u2211\ni=1\n\u2016ai\u2016 2. The\nsecond inequality follows that \u03c6i is L-smooth. \u2206 is the upper bound of time delay.\n(wt\u22121 \u2212 w\u2217)T vti = (w t\u22121 \u2212 wt\u2212\u03c4 + wt\u2212\u03c4 \u2212 w\u2217)T vti\n= (wt\u22121 \u2212 wt\u2212\u03c4 )T vti \ufe38 \ufe37\ufe37 \ufe38\nT1\n+(wt\u2212\u03c4 \u2212 w\u2217)T vti \ufe38 \ufe37\ufe37 \ufe38\nT2\n(27)\nBecause E[vti ] = \u2207P (w t\u2212\u03c4 ), we have:\nE[T2] = E[(w t\u2212\u03c4 \u2212 w\u2217)T vti ]\n= (wt\u2212\u03c4 \u2212 w\u2217)T\u2207P (wt\u2212\u03c4 )\n\u2265 \u03b3\u2016wt\u2212\u03c4 \u2212 w\u2217\u20162\n\u2265 \u03b3( 1\n2 \u2016wt\u22121 \u2212 w\u2217\u20162 \u2212 \u2016wt\u2212\u03c4 \u2212 wt\u22121\u20162)\n\u2265 \u03b3\n2 \u2016wt\u22121 \u2212 w\u2217\u20162 \u2212\u2206\u03b3\u03b72\nt\u22121\u2211\nj=t\u2212\u03c4+1\n\u2016vjij\u2016 2 (28)\nwhere the first inequality follows from the strong convexity of P (w),\n(w\u2217 \u2212 wt\u2212\u03c4 )T\u2207P (wt\u2212\u03c4 ) \u2265 P (wt\u2212\u03c4 )\u2212 P (w\u2217) + \u03b3\n2 \u2016wt\u2212\u03c4 \u2212 w\u2217\u20162 (29)\nP (wt\u2212\u03c4 )\u2212 P (w\u2217) \u2265 \u03b3\n2 \u2016wt\u2212\u03c4 \u2212 w\u2217\u20162 (30)\nThe second inequality follows from the inequality,\n\u2016wt\u22121 \u2212 w\u2217\u20162 = \u2016wt\u22121 \u2212 wt\u2212\u03c4 + wt\u2212\u03c4 \u2212 w\u2217\u20162\n\u2264 2\u2016wt\u22121 \u2212 wt\u2212\u03c4\u20162 + 2\u2016wt\u2212\u03c4 \u2212 w\u2217\u20162 (31)\nT1 = (w t\u22121 \u2212 wt\u2212\u03c4 )T vti\n= (\nt\u22121\u2211\nj=t\u2212\u03c4+1\n(wj \u2212 wj\u22121))T vti\n= \u03b7 t\u22121\u2211\nj=t\u2212\u03c4+1\n(vjij ) T vti (32)\nWe define Ct = caAt + cbBt, and take expectation over i,\nE[Ct] = E\n[\nca ( At\u22121 + \u03b7\u03bb ( \u2212\u2016\u03b1t\u22121i \u2212 \u03b1 \u2217 i \u2016 2 + \u2016\u2207\u03c6i(w t\u2212\u03c4 ) + \u03b1\u2217i \u2016 2 \u2212 (1\u2212 \u03b2)\u2016vti\u2016 2 ))\n+ cb ( Bt\u22121 \u2212 2\u03b7(w t\u22121 \u2212 w\u2217)T vti + \u03b7 2\u2016vti\u2016\n2 ) ]\n= E\n[\nca(1\u2212 \u03b7\u03bb)At\u22121 + ca\u03b7\u03bb\u2016\u2207\u03c6i(w t\u2212\u03c4 ) + \u03b1\u2217i \u2016 2 \u2212 ca\u03b7\u03bb(1 \u2212 \u03b2)\u2016v t i\u2016 2\n+ cbBt\u22121 \u2212 2cb\u03b7(w t\u22121 \u2212 w\u2217)T vti + cb\u03b7 2\u2016vti\u2016 2\n]\n\u2264 E\n[\nca(1\u2212 \u03b7\u03bb)At\u22121 + ca\u03b7\u03bb\n 2\u2206\u03b72L2 t\u22121\u2211\nj=t\u2212\u03c4+1\n\u2016vjij\u2016 2 + 2L2\u2016wt\u22121 \u2212 w\u2217\u20162\n\n\n\u2212 ca\u03b7\u03bb(1 \u2212 \u03b2)\u2016v t i\u2016 2 + cbBt\u22121 + cb\u03b7 2\u2016vti\u2016 2\n\u2212 2cb\u03b7\n\n \u03b3\n2 \u2016wt\u22121 \u2212 w\u2217\u20162 \u2212\u2206\u03b3\u03b72\nt\u22121\u2211\nj=t\u2212\u03c4+1\n\u2016vjij\u2016 2 + \u03b7\nt\u22121\u2211\nj=t\u2212\u03c4+1\n(vjij ) T vti\n\n\n]\n\u2264 ca(1 \u2212 \u03b7\u03bb)E[At\u22121] + (2ca\u03b7\u03bbL 2 + cb \u2212 cb\u03b7\u03b3)E[Bt\u22121] + ( \u2212ca\u03b7\u03bb(1 \u2212 \u03b2) + cb\u03b7 2 + cb\u2206\u03b7 2 ) E[\u2016vti\u2016 2]\n+ (2ca\u2206\u03bbL 2\u03b73 + 2cb\u2206\u03b3\u03b7 3 + cb\u03b7 2)\nt\u22121\u2211\nj=t\u2212\u03c4+1\nE[\u2016vjij\u2016 2] (33)\nwhere the first equality follows from E[\u2016\u03b1t\u22121i \u2212 \u03b1 \u2217 i \u2016 2] = At\u22121.\nSumming over E[Ct] from t = 1 to n, we obtain:\nn\u2211\nt=1\nE[Ct] \u2264\nn\u2211\nt=1\n( ca(1\u2212 \u03b7\u03bb)E[At\u22121] + (2ca\u03b7\u03bbL 2 + cb \u2212 cb\u03b7\u03b3)E[Bt\u22121] )\n(34)\n+\nn\u2211\nt=1\n( 2ca\u03bb\u2206 2L2\u03b73 + 2cb\u03b3\u2206 2\u03b73 + 2cb\u2206\u03b7 2 + cb\u03b7 2 \u2212 ca\u03b7\u03bb(1 \u2212 \u03b2) ) E[\u2016vti\u2016 2]\nIf two following inequalities hold\n2ca\u03b7\u03bbL 2 + cb \u2212 cb\u03b7\u03b3 = cb(1 \u2212 \u03b7\u03bb) (35)\n2ca\u03bb\u2206 2L2\u03b72 + 2cb\u03b3\u2206 2\u03b72 + 2cb\u2206\u03b7 + cb\u03b7 \u2212 ca\u03bb(1 \u2212 n\u03bb\u03b7) \u2264 0 (36)\nthen n\u2211\nt=1\nE[Ct] \u2264 (1\u2212 \u03b7\u03bb)\nn\u2211\nt=1\nE[Ct\u22121]\n\u2264 n\u2211\nt=2\nE[Ct\u22121] + (1\u2212 \u03b7\u03bb)E[C0] (37)\nBecause E[Cn] = E[Cs+1,0] and E[C0] = E[Cs,0], thus,\nE[Cs,0] \u2264 (1\u2212 \u03b7\u03bb)E[Cs\u22121,0]\n\u2264 (1\u2212 \u03b7\u03bb)sC0,0 \u2264 e\u2212\u03b7\u03bbsC0,0 (38)\nWe set ca = 12\u03bbL2 and cb = 1 \u03b3\u2212\u03bb . If \u03b7 = 0, the left-side value in (36) is \u2212ca\u03bb, and it is smaller than 0. Thus there exists a value \u03b7 > 0 to make the inequality (36) hold. Thus, we have the final conclusion. If \u03b7 satisfies the following inequality:\n(\n\u22062 + 2\u03b3\u22062\n\u03b3 \u2212 \u03bb\n)\n\u03b72 +\n( 2\u2206+ 1\n\u03b3 \u2212 \u03bb +\nn\n2L2\n)\n\u03b7 \u2212 1\n2L2 \u2264 0 , (39)\nthen\nE\n[\n1 \u03b3\u2212\u03bb \u2016ws,0 \u2212 w\u2217\u20162 + 1 2\u03bbL2n\nn\u2211\ni=1\n\u2016\u03b1s,0i \u2212 \u03b1 \u2217 i \u2016 2\n]\n\u2264 e\u2212\u03b7\u03bbs [ 1\n\u03b3\u2212\u03bb \u2016w0,0 \u2212 w\u2217\u20162 + 1 2\u03bbL2n\nn\u2211\ni=1\n\u2016\u03b10,0i \u2212 \u03b1 \u2217 i \u2016 2\n]\n. (40)"}], "references": [{"title": "Distributed delayed stochastic optimization", "author": ["Alekh Agarwal", "John C Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A distributed, asynchronous and incremental algorithm for nonconvex optimization: An admm based approach", "author": ["Mingyi Hong"], "venue": "arXiv preprint arXiv:1412.6058,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Asynchronous stochastic gradient descent with variance reduction for non-convex optimization", "author": ["Zhouyuan Huo", "Heng Huang"], "venue": "arXiv preprint arXiv:1604.03584,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Martin Jaggi", "Virginia Smith", "Martin Tak\u00e1c", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Slow learners are fast", "author": ["John Langford", "Alexander Smola", "Martin Zinkevich"], "venue": "arXiv preprint arXiv:0911.0491,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Communication efficient distributed machine learning with the parameter server", "author": ["Mu Li", "David G Andersen", "Alex J Smola", "Kai Yu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Asynchronous parallel stochastic gradient for nonconvex optimization", "author": ["Xiangru Lian", "Yijun Huang", "Yuncheng Li", "Ji Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "An asynchronous parallel randomized kaczmarz algorithm", "author": ["Ji Liu", "Stephen J Wright", "Srikrishna Sridhar"], "venue": "arXiv preprint arXiv:1401.4780,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": "arXiv preprint arXiv:1309.2388,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Sdca without duality", "author": ["Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1502.06177,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Accelerated mini-batch stochastic dual coordinate ascent", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Mini-batch primal and dual methods for svms", "author": ["Martin Tak\u00e1\u010d", "Avleen Bijral", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1303.2314,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Distributed mini-batch sdca", "author": ["Martin Tak\u00e1c", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1507.08322,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["Tianbao Yang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Asynchronous distributed admm for consensus optimization", "author": ["Ruiliang Zhang", "James Kwok"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Fast distributed asynchronous sgd with variance reduction", "author": ["Ruiliang Zhang", "Shuai Zheng", "James T Kwok"], "venue": "arXiv preprint arXiv:1508.01633,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 14, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 1, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 11, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 12, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 13, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 15, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 17, "context": "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].", "startOffset": 77, "endOffset": 107}, {"referenceID": 17, "context": "Experimental results in [18] verify that SDCA method enjoys strong theoretical convergence guarantee properties and often has better performances than stochastic gradient descent (SGD) based methods.", "startOffset": 24, "endOffset": 28}, {"referenceID": 5, "context": "In [6], the paper points out that SDCA is a variation of SGD method, and its update is based on an unbiased estimate of gradient.", "startOffset": 3, "endOffset": 6}, {"referenceID": 12, "context": "In [13], a variation of SDCA was proposed and applied to problems in which individual \u03c6i is non-convex.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 20, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 10, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 7, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 0, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 2, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 9, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 3, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 19, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 6, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 18, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 4, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 16, "context": "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].", "startOffset": 155, "endOffset": 204}, {"referenceID": 18, "context": "In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when \u03c6i is smooth and convex.", "startOffset": 3, "endOffset": 14}, {"referenceID": 4, "context": "In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when \u03c6i is smooth and convex.", "startOffset": 3, "endOffset": 14}, {"referenceID": 16, "context": "In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when \u03c6i is smooth and convex.", "startOffset": 3, "endOffset": 14}, {"referenceID": 0, "context": "References [1] Alekh Agarwal and John C Duchi.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Mingyi Hong.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Zhouyuan Huo and Heng Huang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Martin Jaggi, Virginia Smith, Martin Tak\u00e1c, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I Jordan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Rie Johnson and Tong Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] John Langford, Alexander Smola, and Martin Zinkevich.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Mu Li, David G Andersen, Alex J Smola, and Kai Yu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] Ji Liu, Stephen J Wright, and Srikrishna Sridhar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Mark Schmidt, Nicolas Le Roux, and Francis Bach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Shai Shalev-Shwartz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] Shai Shalev-Shwartz and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Martin Tak\u00e1\u010d, Avleen Bijral, Peter Richt\u00e1rik, and Nathan Srebro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] Martin Tak\u00e1c, Peter Richt\u00e1rik, and Nathan Srebro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] Lin Xiao and Tong Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Tianbao Yang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Ruiliang Zhang and James Kwok.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] Ruiliang Zhang, Shuai Zheng, and James T Kwok.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "i=1 \u2016\u2207\u03c6i(w) \u2212\u2207\u03c6i(w )\u2016 \u2264 2L ( P (w)\u2212 P (w)\u2212 \u03bb 2 \u2016w \u2212 w\u2016 ) (9) This lemma was proved in [13].", "startOffset": 86, "endOffset": 90}], "year": 2017, "abstractText": "In this paper, we propose new Distributed Asynchronous Dual-Free Coordinate Ascent method (Asy-df SDCA), and provide the proof of convergence rate for two cases: the individual loss is convex and the individual loss is non-convex but its expected loss is convex. Stochastic Dual Coordinate Ascent (SDCA) model is a popular method and often has better performances than stochastic gradient descent methods in solving regularized convex loss minimization problems. Dual-Free Stochastic Dual Coordinate Ascent method is a variation of SDCA, and can be applied to non-convex problem when its dual problem is meaningless. We extend Dual-Free Stochastic Dual Coordinate Ascent method to the distributed mode with considering the star network in this paper.", "creator": "LaTeX with hyperref package"}}}