{"id": "1605.09432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2016", "title": "Evaluating Crowdsourcing Participants in the Absence of Ground-Truth", "abstract": "given a supervised / supervisory semi - supervised learning theory scenario suitable where multiple annotators available are commonly available, as we consider the problem when of introducing identification sequences of adversarial responses or unreliable annotators.", "histories": [["v1", "Mon, 30 May 2016 22:05:36 GMT  (1126kb,D)", "http://arxiv.org/abs/1605.09432v1", "4 pages, 5 figures, Workshop on Human Computation for Science and Computational Sustainability, NIPS 2012, Lake Tahoe, NV. 7 Dec 2012"]], "COMMENTS": "4 pages, 5 figures, Workshop on Human Computation for Science and Computational Sustainability, NIPS 2012, Lake Tahoe, NV. 7 Dec 2012", "reviews": [], "SUBJECTS": "cs.HC cs.LG", "authors": ["ramanathan subramanian", "romer rosales", "glenn fung", "jennifer dy"], "accepted": false, "id": "1605.09432"}, "pdf": {"name": "1605.09432.pdf", "metadata": {"source": "CRF", "title": "Evaluating Crowdsourcing Participants in the Absence of Ground-Truth", "authors": ["Ramanathan Subramanian", "R\u00f3mer Rosales"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Data can be acquired, shared, and processed by an increasingly larger number of entities, in particular people. The distributed nature of this phenomenon has contributed to the development of many crowdsourcing projects. This scenario is prevalent in most forms of expert/non-expert group opinion and rating tasks (including many forms of internet or on-line user behavior), where a key element is the aggregation of observations-opinions from multiple sources.\nThis data acquisition setting has created a number of interesting problems and opportunities for machine learning and data modeling among other disciplines. Efficiently aggregating and obtaining knowledge from multiple distributed sources is far from being a solved problem. From a machine learning point of view, we treat observations and opinions as data points and participant label/annotations respectively. In this sense, multiple participants (annotators) may assign conflicting labels to the same data point. Clearly, participant expertise varies and some data points may be considered easier for certain participants relative to others. Thus, the selection of and trust placed on participants plays a crucial role on crowdsourcing outcomes.\nThis paper describes an approach for learning to label/annotate by modeling participant expertise, and focuses on a novel approach to identify unhelpful or adversarial participants. The paper presents an approach for participant (also referred to as annotators) evaluation based on a multi-annotator statistical model and provides experimental validation of the approach with respect to number of adversaries, degree of adversarial behavior, and effect on the classification accuracy."}, {"heading": "2 Problem Setup", "text": "Let the available data collected from multiple annotators be given by a set of points X = {x1, . . . ,xN} drawn independently from an input distribution. Some of these data points may have been labeled by one or more annotators. We denote by y(t)i the label for the i-th data point given by annotator t and let Y = {y (t) i }it. We let xi and y (t) i be random variables in an input and label space respectively and introduce additional random variables Z = {z1, . . . , zN} to represent the true but not observed label for the data points. Annotators may label certain data points more reliably than others because of the properties of the data point itself. For example, a noisy data point maybe more difficult to label by all annotators, but also a data point\nar X\niv :1\n60 5.\n09 43\n2v 1\n[ cs\n.H C\n] 3\n0 M\nay 2\n01 6\nmay be more familiar to certain annotators and thus the annotator-specific accuracy would vary. Thus, we let the annotation y(t), provided by labeler t, depend on the true unknown label z but also on the input x.\nThis is in contrast with the related work in [1,2] where annotators are assumed uniformly accurate/inaccurate irrespective of the data point being labeled. We instead follow the model introduced in [3] where the dependency x\u2192 y(t) \u2190 z is explicit. This can be summarized by the graphical model in Fig. 1-Left. We focus on binary classification and use a Bernoulli distribution to model the ability of the annotators to provide the correct label: p(y(t)i |xi, zi) = (1\u2212 \u03b7t(x)) |y(t)i \u2212zi|\u03b7t(x)\n1\u2212|y(t)i \u2212zi|, where the probability of success \u03b7t(x) is a function of x, thus allowing the annotator accuracy to depend on the input data point coordinates. We let the true label depend on x via a logistic regression model for p(zi = 1|xi)."}, {"heading": "3 Evaluating Annotators", "text": "Consider any single data point. If we knew the ground-truth, then we could straightforwardly evaluate the annotator accuracy. What if we do not have access to the ground-truth (it does not exist or is expensive to obtain)? The following proposed distribution provides a way to evaluate an annotator without reliance on ground-truth. The basic idea is to evaluate a conditional distribution measuring how predictable an annotator label is conditioned on other annotators:\np(y(k)|{y(t\\k)},x) = p({y (t)}|x)\np({y(t\\k)}|x) =\n\u2211 z p({y(t)}|z,x)p(z|x)\u2211\nz p({y(t\\k)}|z,x)p(z|x) (1)\nWe note that if the ground-truth is given (along with the input data), the annotators are mutually independent and p(y(k)|{y(t\\k)},x) = p(y(k)|x), as expected. A related way to understand Eq. 1 is as a measure of how much an annotator\u2019s label, for a given data-point, can be correctly inferred from that of other annotators."}, {"heading": "4 Experiments", "text": "We start with two medical multi-annotator datasets: (1) A breast cancer dataset where mammograms have been labeled independently by three doctors. Ground-truth has been obtained through a biopsy, not available to the algorithm nor the participant doctors. (2) A concept inference dataset where non-experts read a passage of text, from de-identified electronic medical records, to assess whether the patient has atrial fibrillation, a type of heart arrhythmia. Ground-truth was given by expert assessment. The datasets contained 75 and 1057 datapoints respectively. For a more controlled experiment, for both datasets we additionally generated M helpful/adversarial annotators. The labels given by these were generated as the true labels but flipped with\nprobability pa (thus, pa effectively determines the nature of the annotator). For all experiments we trained the multi-labeler model described in Sec. 2 to build the distributions required by Eq. 1.\nIn order to illustrate the properties of the approach, Fig. 1 shows breast cancer data points labeled by various annotators plotted against the value of p(y(k)|{y(t\\k)},x) for some specific annotator k (Eq. 1). In Fig. 1- center we flipped the label of the first annotator with p = 0.4 causing the conditional probability of the labels given by this annotator to decrease, as expected. In Fig. 1-right a fourth adversarial annotator with pa = 0.4 was introduced. This annotator obtains a lower conditional probability value comparatively, indicating that it is relatively more difficult to predict its label compared with the original three annotators.\nWe additionally measured the combined impact of the number of adversarial annotators and the strength of pa. For this we defined an adversarial score for an annotator as the sum (over data points) of the negative log conditional probabilities defined by Eq. 1. This is comparable with the negative log-likelihood measure frequently utilized in learning tasks. Thus, if an annotator\u2019s labels are difficult to infer based on the rest of the annotator labels, then the conditional probabilities above will be generally low. The lower these probabilities the larger the score; thus, a large score is indicative of an annotator that is less trustworthy.\nFig. 2 shows this score while varying pa and the number of helpful/adversarial annotators. We noted various properties. The scores of the non-adversarial annotators remained fairly constant. This indicates that the scoring scheme is robust to the number of adversaries and the strength of pa and could be used for detecting adversaries. The score curves for the adversaries are symmetric about pa = 0.5, which is consistent with the notion that a completely random annotator corresponds to the worst case (independent) adversary since no information can be gained from its labels.\nWe additionally tested this approach with various UCI datasets. Since these datasets do not employ multiple annotators, all the non-adversarial annotators (blue) were generated at random based on the true label, while the adversarial annotators (red) were generated as before. Results, shown in Fig. 3 are consistent with the previous experiments and further confirm the benefits of the approach in a more controlled setting.\nFinally, Figs. 4-5 show how the effect of adversarial annotators and the value of pa can impact the performance (AUC) of the model. We can observe that the AUC decreases considerably only when a large proportion of the adversarial annotators consistently flip the correct label. This indicates that the model can remain accurate under a wide range of adversarial influence."}], "references": [{"title": "Learning from crowds", "author": ["V.C. Raykar"], "venue": "J. Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Evaluating non-expert annotations for natural language tasks", "author": ["R. Snow"], "venue": "In Proc. EM-NLP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Modeling annotator expertise: Learning when everybody knows a bit of something", "author": ["Y. Yan"], "venue": "In Proc. Int. Conference on Artificial Intelligence and Statistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "This is in contrast with the related work in [1,2] where annotators are assumed uniformly accurate/inaccurate irrespective of the data point being labeled.", "startOffset": 45, "endOffset": 50}, {"referenceID": 1, "context": "This is in contrast with the related work in [1,2] where annotators are assumed uniformly accurate/inaccurate irrespective of the data point being labeled.", "startOffset": 45, "endOffset": 50}, {"referenceID": 2, "context": "We instead follow the model introduced in [3] where the dependency x\u2192 y \u2190 z is explicit.", "startOffset": 42, "endOffset": 45}], "year": 2016, "abstractText": "Data can be acquired, shared, and processed by an increasingly larger number of entities, in particular people. The distributed nature of this phenomenon has contributed to the development of many crowdsourcing projects. This scenario is prevalent in most forms of expert/non-expert group opinion and rating tasks (including many forms of internet or on-line user behavior), where a key element is the aggregation of observations-opinions from multiple sources.", "creator": "LaTeX with hyperref package"}}}