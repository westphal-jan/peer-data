{"id": "1605.09458", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2016", "title": "Training Auto-encoders Effectively via Eliminating Task-irrelevant Input Variables", "abstract": "auto - critical encoders are often nowadays used as hardware building blocks of inherently deep network architecture classifier architecture to manually learn intermediate feature value extractors, but task - context irrelevant signal information uncertainty in the dominant input data parameters may lead to increasingly bad extractors functioning and result in poor generalization performance gain of filtering the network. in this paper, via dropping the task - irrelevant input variables the characteristic performance of auto - encoders presumably can be obviously improved. suppose specifically, an importance - importance based variable function selection method estimation is proposed there to be aim simply at finding primarily the task - irrelevant input variables precisely and dropping them. it algorithm firstly alternately estimates irrelevant importance parameters of each variable, and then drops the data variables successively with importance value lower weight than a threshold. in so order to obtain that better performance, the method error can be precisely employed simultaneously for choosing each layer of six stacked auto - encoders. promising experimental results show unexpectedly that : when well combined with our method the stacked mode denoising auto - encoders achieves significantly improved test performance dependence on only three challenging datasets.", "histories": [["v1", "Tue, 31 May 2016 00:58:47 GMT  (324kb,D)", "http://arxiv.org/abs/1605.09458v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["hui shen", "dehua li", "hong wu", "zhaoxiang zang"], "accepted": false, "id": "1605.09458"}, "pdf": {"name": "1605.09458.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Hui Shen", "Dehua Li", "Hong Wu", "Zhaoxiang Zang"], "emails": ["shenhui@hust.edu.cn", "lidehua1946@sina.com;", "wu@hust.edu.cn", "zxzang@gmail.com"], "sections": [{"heading": null, "text": "Keywords: feature learning; deep learning; neural network; auto-encoder; stacked autoencoders; variable selection; feature selection; unsupervised training\nBiographical notes: Hui Shen is currently a PhD student in School of Automation, Huazhong University of Science and Technology,China. She obtained her bachelor\u2019s in Wuhan Polytechnic University, China.And got her master\u2019s in Huazhong University of Science and Technology, China. Her main fields of interest are neural network,deep learning and machine learning.\nDeHua Li is a professor in School of Automation, Huazhong University of Science and Technology,China.He got his bachelor\u2019s from Wuhan University in 1970. And he spent one year as senior visiting scholar in AI department of University of Edinburgh, UK. His research interests including AI, neotic science, and machine learning.\nHong Wu obtained his bachelor\u2019s in computer science and technology from Wuhan University,China. And got his master\u2019s in Huazhong University of Science and Technology,China. Now he is a PhD student of the School of Automation, Huazhong University of Science and Technology, China. His main research fields are neural network, deep learning and machine learning.\nZhaoxiang Zang is an associate professor in College of Computer and Information Technology, China Three Gorges University, Yichang Hubei, China, and he is member of Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering, China Three Gorges University, China. He obtained his master\u2019s and PhD from Huazhong University of Science and Technology,China.His research interests mainly in the fields of machine learning,computational intelligence and computer game intelligence."}, {"heading": "1 Introduction", "text": "Neural networks are widely applied in various fields, such as oil exploration in Liu et al. (2011), speech\nrecognition in Arisoy et al. (2015), temperature control in Admuthe and Chile (2015), and so on. As a kind of neural network, deep neural network has become an increasingly popular research field. And training auto-\nar X\niv :1\n60 5.\n09 45\n8v 1\n[ cs\n.L G\n] 3\n1 M\nay 2\nencoders to learn useful feature extractors to inisitalize a deep neural network is a widely used approach. An autoencoder is comprised by an encoder and a decoder. Given an input example, the encoder, which consists of a group of feature extractors, produces features that constitute an abstract representation or code of the example, while the decoder reconstructs the example from the code. Training an auto-encoder is to minimize the difference between the input example and its reconstruction. Details of auto-encoders and their applications in deep learning can be found in Hinton et al. (2006); Bengio (2013); Ciresan et al. (2012); Bengio et al. (2013). Auto-encoders are usually implemented with neural networks, but over-complete (higher dimensional hidden layer than the input layer) and unconstrained autoencoders may learn identical mapping, which results in useless features. Therefore, many regularized autoencoders were proposed to learn good feature extractors, such as sparse auto-encoders (Lee et al., 2008; Boureau et al., 2008; Ng, 2011), contractive auto-encoders (CAEs) (Rifai et al., 2011a,b,c), denoising auto-encoders (DAEs) (Vincent et al., 2008, 2010), marginalized denoising autoencoders (mDAEs) (Chen et al., 2012, 2014), and so on.\nBecause of the unsupervised training, auto-encoders attempts to capture everything in input data, including task-irrelevant information if there exits(Vincent et al., 2010). However, learning task-irrelevant information may waste the computation resources and the capability of networks, and is easy to cause overfitting. Therefore, eliminating task-irrelevant information becomes one of the ways to obtain better performance and reduce computation cost. Bengio et al. explored supervised pretraining (Bengio et al., 2007), and they concluded that partially supervised pre-training (alternately perform supervised and unsupervised training of an autoencoder) can lead auto-encoders to learn better feature extractors when much task-irrelevant information is contained in training data. Shon et al. proposed pointwise gated Boltzmann machines (PGBM) (Sohn et al., 2013). They used a hidden layer containing twogroup units to model the foreground and background respectively, then took the foreground group to extract task-relevant features. PGMB achieves the state-ofthe-art performance on several benchmark datasets. Inspired by human attention, Wang et al. proposed the attentional neural network (aNN) (Wang et al., 2014). They used a segmentation module to iteratively segment foreground from noisy input via a feedback loop, and employed a deep network on the foreground for classification. aNN also achieves the state-of-the-art performance on one benchmark dataset.\nIn this work, we try to drop the task-irrelevant variables by performing variable selection on input of auto-encoders and then execute unsupervised learning on the remaining variables. We introduce an importance-based variable selection method that evaluate importance of each variable and drop the variables with low importance.For obtaining better classification performance, the method is employed for\neach layer of stacked auto-encoders, not only on the raw input for the first layer, which is different from methods mentioned above. The experiment results show that it helps stacked DAE (SDAE) achieve significantly improved performance on three challenging benchmark datasets.\nThe rest of this paper is organized as following. Preliminaries about DAE and SDAE are provided in section 2. The proposed variable selection method is described in detail in section 3. Experiments and results are reported in section 4, and conclusions are made in section 5. We will use following notations through out the paper. \u03c7 is a training dataset. Each element of \u03c7 contains a input example x and a target label r such that x \u2208 [0, 1]M (or RM ) and r \u2208 {1, . . . ,K}. For a vector x, xd is its d-th component. For the sake of simplicity, we use \u03c7 to denote the training dataset for an auto-encoder, no matter where the auto-encoder is located in a stacked auto-encoders."}, {"heading": "2 Preliminaries", "text": "Auto-encoders(AEs) are often employed as building blocks of deep networks. An AE is a neural network that composed of three layers,an input layer, a hidden layer and an output layer. It tries to recover the input data x from the hidden representation h at the output layer. The motivation is that, if the input data can be reconstructed well enough, then it can be said that the hidden feature is a discription of the input data. An AE is also can be seen as a network that consists of an encoder and a decoder. Processing from input layer to hidden layer can be seen as an encoder, while from hidden layer to output layer a decoder.\nDuring training,it firstly maps the the input data x to hidden representation h by the encoder:\nhq = fq (x) = sf ( wTq x+ bq ) ,\nwhere wTq is a weight vector, bq is a bias and sf is the activation function of the encoder, typically the sigmoid functionsf (z) = 1/ (1 + e\n\u2212z). Then reconstruct input yd from the hidden representation h through the decoder:\nyd = gd (h) = sg ( wTd h+ cd ) ,\nwhere cd is a bias and sg is the activation function of the decoder, which can be sigmoid function for binary input or an identity for continuous input.\nExplicitly, the reconstruction y should approach to original input x as much as possible, this can be measured by the reconstruction error L (x,y) which is typically computed via squared error or cross-entropy. Which one to be chosen depends on the activation function of the decoder.\nIf sg is an identity, i.e.sg (z) = z, then\nL (x,y) = ||x\u2212 y||2.\nIf sg is simoid function, i.e.sg (z) = 1/ (1 + e \u2212z), then\nL (x,y) = \u2212 d=M\u2211 d=1 xdlog (yd) + (1\u2212 xd) log (1\u2212 yd) ,(1)\nwhere yd is depend on the model parameters wq,wd,bq and cdwhen input data x is given.\nThen the auto-encoder is trained to learn model parameters that minimize the reconstruction error L (x,y). Gradient descent can be utilized to optimized the error function during training. When the training is completed, the output layer with the weights of hidden to output are dropped and the learned representation feature holds in the hidden layer, which can be used for classification or used as the input of an other autoencoder to learn more abstract feature.\nHowever, an AE may learn identity, which leads to obtain no useful featrues. In order to prevent this situation, AEs often utilize the configuration called \u201cbottleneck\u201d of which the quantity of hidden units lower than input units. An other approch is adding regular terms on the objective function to constrain the weights. Otherwise, using disturbed input data for training is also an effective means, like denoising autoencoder.\nDenoising auto-encoder(DAE) is a variant of standard auto-encoder. It attempts to reconstruct the input x from the encoded representation h of noisy input x\u0303 via a decoder. By disturbing the input x, denoising auto-encoder tries to learn robust features that can successfully recover the perturbed values to reconstruct the original input data. If a DAE can recover the original input data from the code of corruped input data, it can be said that the DAE has leanred robust and stable features.\nStacked denoising auto-encoder(SDAE) is formed by stacking multiple single-layer DAEs for learning more abstract representations. SDAE can be used to effectively pre-train deep networks. In the process of pre-training by SDAE, the hidden features learned by lower-layer DAE are used as inputs for training next (upper-layer) DAE, and the encoders of DAEs are used to initialize weights in the deep network. See Vincent et al. (2010) for details."}, {"heading": "3 Importance-based Variable Selection", "text": "According to equation (1), AEs belong to unsupervised learning without considering the label information. The hidden representation of an autoecoder is a description of the whole input data. AEs do not identify useful or unuseful information for classification. they attempt to capture all the information of the input, not only task-relevant information, but also task-irrelevant information if there exists. However, learning taskirrelevant information may waste computation resources and even cause over-fitting. Therefore, task-irrelevant information contained in input data should be reduced or eliminated for obtaining better performance.\nTo address this issue, an importance-based variable selection method is proposed to find the task-irrelevant variables and drop them. Briefly, the method is to evaluate the importance of each variable to classification and drop the variables with importance lower than a threshold. We exploit the sensitivity of the discriminant hyperplane to a variable to evaluate the importance of the variable. we argue that, the variables with higher sensitivity are more important for classification, these variables also possess higher importance value and should be reserved, while those variables with low importance(lower than a threshold) should be dropped. The details are described as follows.\nWe employ a trained Multinominal Logistic Regression(MLR) model as a pre-classifier to help us determine the importance of each input variable to classification. Multinominal Logistic Regression (MLR) is a simple log-linear classifier, and can be easily analyzed. Given an example x, the MLR computes the posterior probability of each hypothesis via a softmax function, and takes the one with biggest posterior probability as prediction. see Bishop (2006); Hosmer Jr et al. (2013) for MLR in detail. We briefly introduce the softmax function, from which the importance notation can be deduced.\nThe softmax function can be written as\n\u03c3i (x) = exp ( wTi x+ bi ) / [ K\u2211 c=1 exp ( wTc x+ bc )] where wi and bi are parameters, and i \u2208 {1, . . . ,K} is the index of class. The predicted class of x is obtained by y = arg maxi \u03c3i (x).\nThe softmax function computes the estimated probability of the class label for a given input x. Now we consider any two classes of class i and class j. We suppose that, the discriminant hyperplane between class i and class j is consisted of the points that with equivalent estimated probabilities of the two labels. Let\nexp ( wTi x+ bi )\u2211K c=1 exp (w T c x+ bc) = exp ( wTj x+ bj )\u2211K c=1 exp (w T c x+ bc) ,\nthen we obtain the discriminant hyperplane:\n(wi \u2212wj)T x+ (bi \u2212 bj) = 0\nAfter normalization, discriminant function between class i and class j can be written as\nfi,j (x) = [ (wi \u2212wj)T x+ (bi \u2212 bj) ] /\u2016wi \u2212wj\u20162\nIn other words, all x that satisfy fi,j (x) = 0 form a discriminant hyperplane between class i and class j. We denote the discriminant hyperplane as Hi,j . The unit normal vector of Hi,j can be written as\nvi,j = wi \u2212wj \u2016wi \u2212wj\u20162 . (2)\nDefine the sensitivity of fi,j to an input variable as |\u2202fij\u2202xd |, which reflects the influence of the variable to fi,j .\nAlgorithm 1 Importance-based Variables Selection Input: training dataset \u03c7 = { x(t), r(t) }N t=1\n, importance threshold cth Output: variables mask \u03b1 1: \u03b1\u2190 (1 (true))Md=1 2: loop\n3: Train a new pre-classifier MLR with the masked training data { \u03b1 x(t), r(t) }N t=1\n. 4: if stop criterion then 5: return \u03b1 6: else 7: Update \u03b1 according to (2), (3), (4), and (5). 8: end if 9: end loop\nBecause fi,j is a linear function, |\u2202fij\u2202xd | = |vi,j,d| where vi,j,d is the d-th component of vi,j . By normalizing the sensitivities of fi,j with \u2016vi,j\u2016\u221e (the infinity norm of vi,j), we can define the importance of the d-th variable to fi,j as\nsi,j,d = |vi,j,d| \u2016vi,j\u2016\u221e = |vi,j,d| maxk|vi,j,k| . (3)\nWe argue that if a variable has low importances to all the discriminant functions then it can be identified as task-irrelevant variable. On the other hand, a variable is identified as task-relevant variable if it has unignorable importance to any fi,j . Therefore, we define the importance of the d-th variable to the classification as\ncd = max i,j 6=i si,j,d. (4)\ncd is the maximum of importances of the d-th variable across all discriminant functions.\nConsequently, task-irrelevant variables, each of which has low importance (below a threshold) to classification, can be discarded in the unsupervised training of autoencoder. In order to facilitate computation, we use a variable mask \u03b1 to represent the binary task-relevances of input variables. \u03b1 is defined as\n\u03b1 = (1 (cd > cth)) M d=1 , (5)\nwhere cth is a importance threshold and 1 (\u00b7) is the indicator function so that it takes 1 if the condition in the brackets is true and 0 otherwise. Mask components corresponding to task-irrelevant variables will take 0.\nIn practice, since there might be cross-correlation between variables in input variable set, it is not easy to find out all task-irrelevant variables through training a MLR with full input variable set. A iterative method can be employed to find out task-irrelevant variables gradually, and in each iteration a new pre-classifier MLR is trained to dropping a few variables from input variable set.\nAlgorithm 1 is called Importance-based Variable Selection (IVS), and describes how to find task-irrelevant variables in detail. In line 1, variable mask \u03b1 is initialized\nby assigning 1 to each component, which means all input variables will be used in the first MLR training. For each iteration, a new pre-classifier MLR is trained with masked training data in line 3, where means component-wise multiplication or Haddamard product. In order to prevent the MLR training from overfitting, model selection can be done by using a validation dataset to early stop the training in line 3. The variable mask \u03b1 is updated in line 7 based on the well-trained MLR. This iterative procedure will stop under conditions such as exceeding maximum iterations, no more task-irrelevant variables found, no better classification performance obtained on validation set and so on. Once the variable mask \u03b1 is obtained, task-irrelevant variables indicated by \u03b1 will be dropped in the following unsupervised training for auto-encoders.\nAlgorithm 1 can be employed for each higher layer of stacked auto-encoders, therefore complex task-irrelevant information not eliminated on low level can be removed gradually on higher layers. From a model selection point of view, eliminating task-irrelevant variables can reduce complexity of networks therefore obtain better performance."}, {"heading": "4 Experiments and Results", "text": "In our experiments, we combined the proposed method with DAE Vincent et al. (2010), which is called DAEIVS, and compared performances produced by stacked DAE-IVS (SDAE-IVS), stacked DAE (SDAE) Vincent et al. (2010), PGBM Sohn et al. (2013), and aNN Wang et al. (2014) . The baseline was SDAE trained with standard training strategy described in Vincent et al. (2010). We tested different depth of SDAE and SDAEIVS from 1 layer to 3 layers. Each auto-encoder had 1000 hidden units and used tied weights. Both encoder and decoder used sigmoid function and took cross-entropy loss as reconstruction error. The outputs of feature extractors learned by a layer were used as input variables of upper layer. Multinominal Logistic Regression layer was added on both top of SDAE and SDAE-IVS to perform supervised fine-tuning. All training processes used stochastic gradient descent for parameter learning.\nOur datasets were several variants of MNIST dataset for recognizing images of handwritten digits Larochelle et al. (2007), including MNIST with random background(bgrand) or with image background (bg-img) and the combination of rotated digits with image background (rot-bg-img). Each dataset was split into three subsets: a training set (10000 examples) for pre-training and finetuning the parameters, a validation set (2000 examples) for model selection and a testing set (50000 examples) on which the classification performance were reported. The hyper-parameters were chosen on the validation set, including the learning rate for pre-classification in Algorithm 1 (candidate set [0.01, 0.02, 0.05, 0.1]), the importance threshold (candidate set [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]), learning rate for pre-training\nand fine-tuning (candidate set [0.01, 0.05, 0.1, 0.2]), Gaussian noise standard deviation in SDAE and SDAEIVS (candidate set [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]), and pre-training epochs (candidate set [60, 120, 180, 240, 300]). The loop in Algorithm 1 was stopped when no more task-irrelevant variables found and no better classification performance obtained on validation set."}, {"heading": "4.1 Effect of importance-based variable selection", "text": "During Applying algorithm1, we can compute the importance value of each variable on each dataset(equation (2), (3) and (4)). Examples and visualization of the importance of variable for each dataset are shown in Figure 1. In importance image, the lighter and the darker areas correspond to the\nvariables with higher importance and lower importance respectively. It can be seen that variables with high importance concentrate on the central area where digits mainly occupy. Different from other datasets, the highimportance area of rot-bg-img looks like a disc because of the rotation of digits. We take the variables with importance value higher than a threshold as the task-\nrelevant variable, otherwise as the task-irrelevant ones (equation (5)). Visualizations of the task-irrelevant and the task-relevant patterns (weights of feature extractors learned by the first layer of SDAE-IVS) are showed in Figure 2, where the threshold is set by experience. Although there exists misclassification, most taskirrelevant patterns describe the background image, and\nmost task-relevant patterns describe the foreground digit. The algorithm 1 is an iterate process, and taskirrelevant variables are dropped in each iteration. This can indirectly improve the signal to noise ratio related to classification. Figure 3 shows that not only the number of variables is decreased (to 36%), but also the classification performance of the pre-classifier is improved. Similar results can also be found on different layers of SDAE-IVS trained on different datasets.\nLet VDAE\u2212IV S and V DAE be the number of taskrelevant feature extractor learned by the first layer of SDAE-IVS and SDAE respectively. We used the the ratio of VDAE\u2212IV S to VDAE to measure the effectiveness improvement of learning useful feature extractors. As shown in Figure 4, all these curves are above 1, which means that DAE-IVS could learn more task-relevant feature extractor than DAE. With the importance threshold increasing, many actual task-relevant feature extractors were also dropped, thus these curves get lower. However, they are still above 1. Note that we do not use the curves to optimize the threshold of importance, we just concentrate on illustrating that feature selection is beneficial to training AE.For showing the effect of feature selection, we try to reconstruct the lower-layer data through the decoders of AEs by using just the task-relevant features in higher-layer. By observing the reconstruction result, we can see whether our algorithm effectively eliminates the task-irrelevant variables and preserves the task-relevant ones. In Figure 5, we show the reconstructions of raw data produced by SDEA and SDAE-IVS with different depth on different datasets. The reconstructions are clearer after dropping taskirrelevant variables, and the background information is significantly suppressed."}, {"heading": "4.2 Classification performance comparison", "text": "In Table 1, we show the test classification error rate of produced by SDAE and SDAE-IVS with different depth. It can be seen that in each depth the performance produced by SDAE-IVS significantly outperforms the performance of SDAE. These results suggest that our method can effectively help auto-encoders learn more and better task-relevant feature extractor so as to get better task performance."}, {"heading": "5 Conclusion", "text": "Auto-encoders attempt to capture as much as possible of information in the input data and have to expend part of its capacity to learn task-irrelevant information if there exists. More importantly, task-irrelevant information may lead the eventual classification to overfitting resulting in bad performance.\nThe proposed method is a simple and effective variable selection method to deal with this problem. Through several rounds of variable selection, the remaining input variables are fed into an auto-encoder\nto learn feature extractors. Because this method is employed for each layer of stacked auto-encoders, it not only eliminates task-irrelevant information, but also prunes the deep network in a certain degree so as to efficiently control the model complexity to obtain better performance. Experimental results show that the method can efficiently drop task-irrelevant variables and helps the auto-encoders learn more and better feature extractor. It helps SDAE achieve significant improvements on classification performances.\nIn the future, we will explore some variable selection method that deduced from non-linear classification models, expecting to help stacked auto-encoders get better performance."}, {"heading": "Acknowledgment", "text": "This work was partially supported by the Doctoral Startup Foundation of China Three Gorges University (Grant No. KJ2013B064), Natural Science Foundation of Hubei (Grant Nos. 2015CFB336) and National Natural Science Foundation of China (Grant No. 61502274)."}], "references": [{"title": "Neuro-fuzzybased hybrid controller for stable temperature of liquid in heat exchanger", "author": ["S. Admuthe", "R. Chile"], "venue": "International Journal of Computational Science and Engineering, 10(1):220 \u2013 230.", "citeRegEx": "Admuthe and Chile,? 2015", "shortCiteRegEx": "Admuthe and Chile", "year": 2015}, {"title": "Bidirectional recurrent neural network language models for automatic speech recognition", "author": ["E. Arisoy", "A. Sethy", "B. Ramabhadran", "S. Chen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 5421\u2013", "citeRegEx": "Arisoy et al\\.,? 2015", "shortCiteRegEx": "Arisoy et al\\.", "year": 2015}, {"title": "Deep learning of representations: Looking forward", "author": ["Y. Bengio"], "venue": "Dediu, A.-H., Mart\u00c3n-Vide, C., Mitkov, R., and Truthe, B., editors, Statistical Language and Speech Processing, volume 7978 of Lecture Notes in Computer Science, pages 1\u201337.", "citeRegEx": "Bengio,? 2013", "shortCiteRegEx": "Bengio", "year": 2013}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1798\u20131828.", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "H. Larochelle", "D. Popovici", "U. Montreal"], "venue": "In NIPS, pages 153\u2013160.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "springer.", "citeRegEx": "Bishop,? 2006", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "Sparse feature learning for deep belief networks", "author": ["Boureau", "Y.-l", "Cun", "Y. L"], "venue": null, "citeRegEx": "Boureau et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Boureau et al\\.", "year": 2008}, {"title": "Marginalized denoising auto-encoders for nonlinear representations", "author": ["M. Chen", "K.Q. Weinberger", "F. Sha", "Y. Bengio"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1476\u20131484.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["M. Chen", "Z. Xu", "K. Weinberger", "F. Sha"], "venue": "arXiv preprint arXiv:1206.4683.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Multi-column deep neural networks for image classification", "author": ["D. Ciresan", "U. Meier", "J. Schmidhuber"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3642\u20133649. IEEE.", "citeRegEx": "Ciresan et al\\.,? 2012", "shortCiteRegEx": "Ciresan et al\\.", "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W."], "venue": "Neural computation, 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Applied logistic regression, volume 398", "author": ["D.W. Hosmer Jr", "S. Lemeshow", "R.X. Sturdivant"], "venue": "John Wiley & Sons.", "citeRegEx": "Jr et al\\.,? 2013", "shortCiteRegEx": "Jr et al\\.", "year": 2013}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 473\u2013480. ACM.", "citeRegEx": "Larochelle et al\\.,? 2007", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "Advances in neural information processing systems, pages 873\u2013 880.", "citeRegEx": "Lee et al\\.,? 2008", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "An intelligent oil reservoir identification approach by deploying quantum levenberg-marquardt neural network and rough set", "author": ["N. Liu", "F. Zheng", "K. Xia"], "venue": "International Journal of Computational Science and Engineering, 6(1-2):76 \u2013 85.", "citeRegEx": "Liu et al\\.,? 2011", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Sparse autoencoder", "author": ["A. Ng"], "venue": "CS294A Lecture notes, 72.", "citeRegEx": "Ng,? 2011", "shortCiteRegEx": "Ng", "year": 2011}, {"title": "The manifold tangent classifier", "author": ["S. Rifai", "Y.N. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller"], "venue": "Advances in Neural Information Processing Systems, pages 2294\u20132302.", "citeRegEx": "Rifai et al\\.,? 2011a", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Higher order contractive auto-encoder", "author": ["S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 645\u2013 660. Springer.", "citeRegEx": "Rifai et al\\.,? 2011b", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 833\u2013840.", "citeRegEx": "Rifai et al\\.,? 2011c", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Learning and selecting features jointly with point-wise gated boltzmann machines", "author": ["K. Sohn", "G. Zhou", "C. Lee", "H. Lee"], "venue": "International Conference on Machine Learning, pages 217\u2013225.", "citeRegEx": "Sohn et al\\.,? 2013", "shortCiteRegEx": "Sohn et al\\.", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A."], "venue": "The Journal of Machine Learning Research, 11:3371\u20133408.", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Attentional neural network: Feature selection using cognitive feedback", "author": ["Q. Wang", "J. Zhang", "S. Song", "Z. Zhang"], "venue": "ArXiv e-prints.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Neural networks are widely applied in various fields, such as oil exploration in Liu et al. (2011), speech recognition in Arisoy et al.", "startOffset": 81, "endOffset": 99}, {"referenceID": 0, "context": "(2011), speech recognition in Arisoy et al. (2015), temperature control in Admuthe and Chile (2015), and so on.", "startOffset": 30, "endOffset": 51}, {"referenceID": 0, "context": "(2015), temperature control in Admuthe and Chile (2015), and so on.", "startOffset": 31, "endOffset": 56}, {"referenceID": 13, "context": "Therefore, many regularized autoencoders were proposed to learn good feature extractors, such as sparse auto-encoders (Lee et al., 2008; Boureau et al., 2008; Ng, 2011), contractive auto-encoders (CAEs) (Rifai et al.", "startOffset": 118, "endOffset": 168}, {"referenceID": 6, "context": "Therefore, many regularized autoencoders were proposed to learn good feature extractors, such as sparse auto-encoders (Lee et al., 2008; Boureau et al., 2008; Ng, 2011), contractive auto-encoders (CAEs) (Rifai et al.", "startOffset": 118, "endOffset": 168}, {"referenceID": 15, "context": "Therefore, many regularized autoencoders were proposed to learn good feature extractors, such as sparse auto-encoders (Lee et al., 2008; Boureau et al., 2008; Ng, 2011), contractive auto-encoders (CAEs) (Rifai et al.", "startOffset": 118, "endOffset": 168}, {"referenceID": 3, "context": "Details of auto-encoders and their applications in deep learning can be found in Hinton et al. (2006); Bengio (2013); Ciresan et al.", "startOffset": 81, "endOffset": 102}, {"referenceID": 2, "context": "(2006); Bengio (2013); Ciresan et al.", "startOffset": 8, "endOffset": 22}, {"referenceID": 2, "context": "(2006); Bengio (2013); Ciresan et al. (2012); Bengio et al.", "startOffset": 8, "endOffset": 45}, {"referenceID": 2, "context": "(2006); Bengio (2013); Ciresan et al. (2012); Bengio et al. (2013). Auto-encoders are usually implemented with neural networks, but over-complete (higher dimensional hidden layer than the input layer) and unconstrained autoencoders may learn identical mapping, which results in useless features.", "startOffset": 8, "endOffset": 67}, {"referenceID": 21, "context": "Because of the unsupervised training, auto-encoders attempts to capture everything in input data, including task-irrelevant information if there exits(Vincent et al., 2010).", "startOffset": 150, "endOffset": 172}, {"referenceID": 4, "context": "explored supervised pretraining (Bengio et al., 2007), and they concluded that partially supervised pre-training (alternately perform supervised and unsupervised training of an autoencoder) can lead auto-encoders to learn better feature extractors when much task-irrelevant information is contained in training data.", "startOffset": 32, "endOffset": 53}, {"referenceID": 19, "context": "proposed pointwise gated Boltzmann machines (PGBM) (Sohn et al., 2013).", "startOffset": 51, "endOffset": 70}, {"referenceID": 22, "context": "proposed the attentional neural network (aNN) (Wang et al., 2014).", "startOffset": 46, "endOffset": 65}, {"referenceID": 15, "context": "Gradient descent can be utilized to optimized the error function during training. When the training is completed, the output layer with the weights of hidden to output are dropped and the learned representation feature holds in the hidden layer, which can be used for classification or used as the input of an other autoencoder to learn more abstract feature. However, an AE may learn identity, which leads to obtain no useful featrues. In order to prevent this situation, AEs often utilize the configuration called \u201cbottleneck\u201d of which the quantity of hidden units lower than input units. An other approch is adding regular terms on the objective function to constrain the weights. Otherwise, using disturbed input data for training is also an effective means, like denoising autoencoder. Denoising auto-encoder(DAE) is a variant of standard auto-encoder. It attempts to reconstruct the input x from the encoded representation h of noisy input x\u0303 via a decoder. By disturbing the input x, denoising auto-encoder tries to learn robust features that can successfully recover the perturbed values to reconstruct the original input data. If a DAE can recover the original input data from the code of corruped input data, it can be said that the DAE has leanred robust and stable features. Stacked denoising auto-encoder(SDAE) is formed by stacking multiple single-layer DAEs for learning more abstract representations. SDAE can be used to effectively pre-train deep networks. In the process of pre-training by SDAE, the hidden features learned by lower-layer DAE are used as inputs for training next (upper-layer) DAE, and the encoders of DAEs are used to initialize weights in the deep network. See Vincent et al. (2010) for details.", "startOffset": 69, "endOffset": 1720}, {"referenceID": 5, "context": "see Bishop (2006); Hosmer Jr et al.", "startOffset": 4, "endOffset": 18}, {"referenceID": 5, "context": "see Bishop (2006); Hosmer Jr et al. (2013) for MLR in detail.", "startOffset": 4, "endOffset": 43}, {"referenceID": 18, "context": "In our experiments, we combined the proposed method with DAE Vincent et al. (2010), which is called DAEIVS, and compared performances produced by stacked DAE-IVS (SDAE-IVS), stacked DAE (SDAE) Vincent et al.", "startOffset": 61, "endOffset": 83}, {"referenceID": 18, "context": "In our experiments, we combined the proposed method with DAE Vincent et al. (2010), which is called DAEIVS, and compared performances produced by stacked DAE-IVS (SDAE-IVS), stacked DAE (SDAE) Vincent et al. (2010), PGBM Sohn et al.", "startOffset": 61, "endOffset": 215}, {"referenceID": 18, "context": "(2010), PGBM Sohn et al. (2013), and aNN Wang et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 15, "context": "(2013), and aNN Wang et al. (2014) .", "startOffset": 18, "endOffset": 35}, {"referenceID": 15, "context": "(2013), and aNN Wang et al. (2014) . The baseline was SDAE trained with standard training strategy described in Vincent et al. (2010). We tested different depth of SDAE and SDAEIVS from 1 layer to 3 layers.", "startOffset": 18, "endOffset": 134}, {"referenceID": 12, "context": "Our datasets were several variants of MNIST dataset for recognizing images of handwritten digits Larochelle et al. (2007), including MNIST with random background(bgrand) or with image background (bg-img) and the combination of rotated digits with image background (rot-bg-img).", "startOffset": 97, "endOffset": 122}], "year": 2016, "abstractText": "Auto-encoders are often used as building blocks of deep network classifier to learn feature extractors, but task-irrelevant information in the input data may lead to bad extractors and result in poor generalization performance of the network. In this paper,via dropping the task-irrelevant input variables the performance of auto-encoders can be obviously improved .Specifically, an importance-based variable selection method is proposed to aim at finding the task-irrelevant input variables and dropping them.It firstly estimates importance of each variable,and then drops the variables with importance value lower than a threshold. In order to obtain better performance, the method can be employed for each layer of stacked auto-encoders. Experimental results show that when combined with our method the stacked denoising auto-encoders achieves significantly improved performance on three challenging datasets.", "creator": "LaTeX with hyperref package"}}}