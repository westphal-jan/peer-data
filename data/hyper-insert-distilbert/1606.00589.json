{"id": "1606.00589", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2016", "title": "Single-Model Encoder-Decoder with Explicit Morphological Representation for Reinflection", "abstract": "advanced morphological reinflection is the task of generating a given target form given basically a favorable source form, a weak source tag and a natural target slave tag. we propose a large new way line of modeling using this task with neural encoder - assisted decoder conversion models. our mature approach reduces the largest amount of efficiently required standardized training among data output for such this architecture and immediately achieves state - of - the - art art results, making encoder - decoder models applicable prior to processing morphological xml reinflection even for various low - end resource living languages. secondly we further present and a new automatic encoding correction method for the outputs based on edit validation trees.", "histories": [["v1", "Thu, 2 Jun 2016 08:57:14 GMT  (231kb,D)", "http://arxiv.org/abs/1606.00589v1", "Accepted at ACL 2016"]], "COMMENTS": "Accepted at ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["katharina kann", "hinrich sch\u00fctze"], "accepted": true, "id": "1606.00589"}, "pdf": {"name": "1606.00589.pdf", "metadata": {"source": "CRF", "title": "Single-Model Encoder-Decoder with Explicit Morphological Representation for Reinflection", "authors": ["Katharina Kann"], "emails": ["kann@cis.lmu.de"], "sections": [{"heading": "1 Introduction", "text": "Morphological analysis and generation of previously unseen word forms is a fundamental problem in many areas of natural language processing (NLP). Its accuracy is crucial for the success of downstream tasks like machine translation and question answering. Accordingly, learning morphological inflection patterns from labeled data is an important challenge.\nThe task of morphological reinflection (MRI) consists of producing an inflected form for a given source form, source tag and target tag. A special case is morphological inflection (MI), the task of finding an inflected form for a given lemma and target tag. An English example is \u201ctree\u201d+PLURAL\u2192 \u201ctrees\u201d. Prior work on MI and MRI includes machine learning models and models that exploit the paradigm structure of the language (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015).\nIn this work, we propose the neural encoderdecoder MED \u2013 Morphological Encoder-Decoder \u2013 a character-level sequence-to-sequence attention model that is a language-independent solution for\nMRI. In contrast to prior work, we train a single model that is trained on all source to target mappings of the language that are attested in the training set. This radically reduces the amount of training data needed for the encoder-decoder because most MRI patterns occur in many source-target tag pairs. In our model design, what is learned for one pair can be transferred to others.\nThe key enabler for this single-model approach is a novel representation we use for MRI. We encode the input as a single sequence of (i) the morphological tags of the source form, (ii) the morphological tags of the target form and (iii) the sequence of letters of the source form. The output is the sequence of letters of the target form. As the decoder produces each letter, the attention mechanism can focus on the input letter sequence for parts of the output that simply copy the input. For other parts of the output, e.g., an inflectional ending that is predicted using the target tags, the attention mechanism can focus on the target morphological tags. In more complex cases, simultaneous attention can be paid to subsequences of all three input types \u2013 source tags, target tags and input letter sequence. We can train a single generic encoder-decoder per language on this representation that can handle all tag pairs, thus making it possible to make efficient use of the available training data. MED outperformed other systems on the SIGMORPHON16 shared task1 for all ten languages that were covered (Kann and Schu\u0308tze, 2016; Cotterell et al., 2016).\nWe also present POET \u2013 Prefer Observed Edit Trees \u2013 a new generic method for correcting the output of an MRI system. The combination of MED and POET is state-of-the-art or close to it on a CELEX-based evaluation of MRI even though this evaluation makes it difficult to exploit gener-\n1ryancotterell.github.io/ sigmorphon2016/\nar X\niv :1\n60 6.\n00 58\n9v 1\n[ cs\n.C L\n] 2\nJ un\n2 01\n6\nalizations across tag pairs."}, {"heading": "2 Model Description", "text": "Neural network model. Our model is based on the network architecture proposed by Bahdanau et al. (2014) for machine translation.2 They describe the model in detail; unless we explicitly say so in the description of our model below, we use the same network configuration as Bahdanau et al. (2014).\nBahdanau et al. (2014)\u2019s model is an extension of the recurrent neural network (RNN) encoderdecoder developed by Cho et al. (2014) and Sutskever et al. (2014). The encoder of the latter consists of an RNN that reads an input sequence of vectors x and encodes it into a fixed-length context vector c, computing hidden states ht and c by\nht = f(xt, ht\u22121), c = q(h1, ..., hTx) (1)\nwith nonlinear functions f and q. The decoder is trained to predict each output yt dependent on c and previous predictions y1, ..., yt\u22121:\np(y) = Ty\u220f t=1 p(yt|{y1, ..., yt\u22121}, c) (2)\nwith y = (y1, ..., yTy) and each conditional probability being modeled with an RNN as\np(yt|{y1, ..., yt\u22121}, c) = g(yt\u22121, st, c) (3)\nwhere g is a nonlinear function and st is the hidden state of the RNN.\nBahdanau et al. (2014) proposed an attentionbased extension of this model that allows different vectors ct for each step by automatic learning of an alignment model. Additionally, they made the encoder bidirectional: each hidden state hj at time step j does not only depend on the preceding, but also on the following input:\nhj = [\u2212\u2192 hTj ; \u2190\u2212 hTj ]T (4)\nThe formula for p(y) changes as follows:\np(y) = Ty\u220f t=1 p(yt|{y1, ..., yt\u22121}, x) (5)\n= g(yt\u22121, st, ct) (6)\n2Our implementation of MED is based on github.com/mila-udem/blocks-examples/ tree/master/machine_translation.\nwith st being an RNN hidden state for time t and ct being the weighted sum of the annotations (h1, ..., hTx) produced by the encoder, using the attention weights. Further descriptions can be found in (Bahdanau et al., 2014).\nThe final model is a multilayer network with a single maxout (Goodfellow et al., 2013) hidden layer that computes the conditional probability of each element in the output sequence (a letter in our case, (Pascanu et al., 2014)). As MRI is less complex than machine translation, we reduce the number of hidden units and embedding size. After initial experiments, we fixed the hyperparameters of our system and did not further adapt them to a specific task or language. Encoder and decoder RNNs have 100 hidden units each. For training, we use stochastic gradient descent, Adadelta (Zeiler, 2012) and a minibatch size of 20. We initialize all weights in the encoder, decoder and the embeddings except for the GRU weights in the decoder with the identity matrix as well as all biases with zero (Le et al., 2015). We train all models for 20,000 iterations. We settled on this number in early experimentation because training usually converged before that limit.\nMED is an ensemble of five RNN encoderdecoders. The final decision is made by majority voting. In case of a tie, the answer is chosen randomly among the most frequent predictions.\nInput and output format. We define the alphabet \u03a3lang as the set of characters used in the application language. As each morphological tag consists of one or more subtags, e.g. \u201cnumber\u201c or \u201ccase\u201c, we further define \u03a3src and \u03a3trg as the set of morphological subtags seen during training as part of the source tag and target tag, respectively. Let Sstart and Send be predefined start and end symbols. Then each input of our system is of the format Sstart\u03a3src+\u03a3trg+\u03a3lang+Send. In the same way, we define the output format as Sstart\u03a3lang+Send.\nA sample input for German is <w> IN=pos=ADJ IN=case=GEN IN=num=PL OUT=pos=ADJ OUT=case=ACC OUT=num=PL i s o l i e r t e r </w>. The system should produce the corresponding output <w> i s o l i e r t e </w>. The high-level structure of MED can be seen in Figure 1.\nPOET. We now describe POET (Prefer Observed Edit Trees), a new generic method for correcting the output of an MRI system. We use it in combination with MED in this paper, but it can in\nprinciple be applied to any MRI system. An edit tree e(\u03c3, \u03c4) specifies a transformation from a source string \u03c3 to a target string \u03c4 (Chrupa\u0142a, 2008). To compute e(\u03c3, \u03c4), we first determine the longest common substring (LCS) (Gusfield, 1997) between \u03c3 and \u03c4 and then recursively model the prefix and suffix pairs of the LCS. If the length of LCS is zero for (\u03c3, \u03c4), then e(\u03c3, \u03c4) is simply the substitution operation that replaces \u03c3 with \u03c4 . Figure 2 shows an example.\nLet X be a training set for MRI. For each pair (s, t) of tags, we define:\nEs,t={e\u2032|\u2203x\u2208X : e\u2032=e(x), s=S(x), t=T (x)}\nwhere S(x) and T (x) are source and target tags of x and e(x) is e(\u03c3(x), \u03c4(x)), the edit tree that transforms the source form into the target form.\nLet \u03c1 be a target form predicted by the MRI system for the source form \u03c3 and let s and t be source and target tags. POET does not change \u03c1 if e(\u03c3, \u03c1) \u2208 Es,t. Otherwise it replaces \u03c1 with \u03c4 :\n\u03c4 := { \u03c4 \u2032 if e(\u03c3, \u03c4 \u2032) \u2208 Es,t, |\u03c1, \u03c4 \u2032| = 1 \u03c1 else\nwhere |\u03c1, \u03c4 \u2032| is the Levenshtein distance. If there are several forms \u03c4 \u2032 with edit distance 1, we select the one with the most frequent edit tree. Ties are broken randomly.\nWe observed that MED sometimes makes errors that are close to the target, but differ by one\nedit operation. Those errors are often not covered by edit trees that are observed in the training data whereas the correct form is. Thus, substituting a form not supported by an observed edit tree with a close one that is supported promises to reduce the error rate.\nThe effectiveness of POET depends on a training set that is large enough to cover the possible edit trees that can occur in reinflection in a language. Thus, if the training set is not large enough in this respect, then POET will not be beneficial."}, {"heading": "3 Experiments", "text": "We compare MED with the three models of Dreyer et al. (2008) as well as with two recently proposed models: (i) discriminative string transduction (Durrett and DeNero, 2013; Nicolai et al., 2015), the SIGMORPHON16 baseline, and (ii) Faruqui et al. (2015)\u2019s encoder-decoder model.3 We call the latter MODEL*TAG as it requires training as many models as there are target tags.\nWe evaluate MED on two MRI tasks: CELEX and SIGMORPHON16.\nCELEX. This task is based on complete inflection tables for German extracted from CELEX. For this experiment we follow Dreyer et al. (2008). We use four pairs of morphological tags and corresponding word forms from the German part of the CELEX morphological database. The 4 different transduction tasks are: 13SIA\u2192 13SKE, 2PIE\u2192 13PKE, 2PKE \u2192 z and rP \u2192 pA.4 An example for this task would be to produce the output gesteuert (target tag pA) for the source steuert (source tag rP). To do so, the system has to learn that the prefix ge-, which is used for many participles in German, has to be added to the beginning of the original word form.\nWe use the same data splits as Dreyer et al. (2008), dividing the original 2500 samples for each tag into five folds, each consisting of 500 training and 1000 development and 1000 test samples. We train a separate model for each fold and report exact match accuracy, averaged over the five folds, as our final result.\n3For our experiments we ran the code available at github.com/mfaruqui/morph-trans. We used the enc-dec-attn model as overall results for the CELEX task were better than with the sep-morph model.\n413SIA=1st/3rd sg. ind. past; 13SKE=1st/3rd sg. subjunct. pres.; 2PIE=2nd pl. ind. pres.; 13PKE=1st/3rd pl. subjunct. pres.; 2PKE=2nd. pl. subjunct. pres.; z=infinitive; rP=imperative pl.; pA=past part.\nSIGMORPHON16. This task covers eight languages and does not provide complete paradigms, but only a set of quadruples, each consisting of word form, source tag, target tag and target form. The main difference to CELEX is that the number of tag pairs is large, resulting in much less training data per tag pair. The number of tag pairs varies by language with Georgian being an extreme case; it has 28 tag pairs in dev that appear less than 10 times in train. For each language, we have around 12,800 training and 1600 development samples. We report exact match accuracy on the development set, as the final test data of the shared task is not publically available yet."}, {"heading": "4 Results", "text": "Table 1 gives CELEX results. MED+POET is better than prior work on one task, close in performance on two and worse by a small amount on the third. Unlike Dreyer et al. (2008)\u2019s models, MED does not use any hand-crafted features. MED\u2019s results are weakest on 13SIA. Typical errors on this task include epenthesis (e.g., zirkle vs. zirkele) and irregular verbs (e.g., abhing vs. abha\u0308ngte).\nFor SIGMORPHON16, Table 2 shows that MED outperforms the baseline for all eight languages. Absolute performance and variance is probably influenced by type of morphology (e.g., templatic vs. agglutinative), regularity of the language, number of different tag pairs and other factors. MED performs well even for complex and diverse languages like Arabic, Finnish, Navajo and Turkish, suggesting that the type of attentionbased encoder-decoder we use \u2013 single-model, using an explicit morphological representation \u2013 is a good choice for MRI.\nWe do not compare to MODEL*TAG here because it requires training a large number of individual networks. This is a disadvantage compared to MED both in terms of the number of models that need to be trained and in terms of the effective use of the small number of training examples that are available per tag pair.\nPOET improves the results for all tag pairs for CELEX. However, initial experiments indicated that it is not effective for SIGMORPHON16 because its training sets are not large enough."}, {"heading": "5 Analysis", "text": "The main innovation of our work is that MED learns a single model of all MRI patterns of a language and thus can transfer what it has learned from one tag pair to another tag pair. Using CELEX, we now analyze how much our design contributes to better performance by conducting two experiments in which we gradually decrease the training set in two different ways. (i) Large general training set. We only reduce the number of training examples available for a tag pair (s, t) and retain all other training examples. (ii) Small training set. We reduce the number of training examples available for all tag pairs, not just for one.\nA typical example of the large general training set scenario is that familiar second person forms are rare in genres like encyclopedia and news. So a training set derived from these genres will be large, but it will have very few tag pairs whose target tag is familiar second person.\nA typical example of the small training set scenario is that we are dealing with a low-resource language.\nIn the following two experiments, we only reduce the training set and do not change the test set.\nLarge general training set. We iteratively halve the training data for 2PIE \u2192 13PKE until only 6.25% or 32 samples are left. Figure 3 shows that MED performs well even if only 6.25% of the training examples for the tag pair remain. In contrast, MODEL*TAG struggles to generalize correctly. This is due to the fact that we train one single model for all tags, so it can learn from other tags and transfer what it has learned to the tag pair that has a small training set.\nSmall training set. Figure 4 shows results for reducing the training data equally for all tags. MED performs much better than the baseline for less than 50% of the training data. This can be explained by the fact that MED learns from all given data at once and thus is able to learn common patterns that apply across different tag pairs."}, {"heading": "6 Related Work", "text": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; De\u0301jean, 1998) and different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero,\n2013; Eskander et al., 2013; Nicolai et al., 2015). Chrupa\u0142a (2008) defined edit trees and Chrupa\u0142a (2008) and Mu\u0308ller et al. (2015) use them for morphological tagging and lemmatization.\nIn the last years, RNN encoder-decoder models and RNNs in general were applied to several NLP tasks. For example, they proved to be useful for machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al., 2015) and speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013).\nMED bears some resemblance to Faruqui et al. (2015)\u2019s work. However, they train one network for every tag pair; this can negatively impact performance for low-resource languages and in general when training data are limited. In contrast, we train a single model for each language. This radically reduces the amount of training data needed for the encoder-decoder because most MRI patterns occur in many tag pairs, so what is learned for one can be transferred to others. To be able to model all tag pairs of the language together, we introduce an explicit morphological representation that enables the attention mechanism of the encoder-decoder to generalize MRI patterns across tag pairs."}, {"heading": "7 Conclusion and Future Work", "text": "We have presented MED, a language independent neural sequence-to-sequence mapping approach, and POET, a method based on edit trees for correcting the output of an MRI system. MED obtains results comparable to state-of-the-art systems for CELEX and establishes the state-of-the-art for SIGMORPHON16. POET improves results further for large training sets. Our analysis showed that MED outperforms a neural encoder-decoder baseline system by a large margin, especially for small training sets.\nIn future work, we would like to make POET less dependent on the source tag and thus increase its accuracy for small training sets. Second, we will look into ways of taking advantage of additional information sources including unlabeled corpora."}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge the financial support of Siemens for this research."}], "references": [{"title": "Semi-supervised learning of morphological paradigms and lexicons", "author": ["Malin Ahlberg", "Markus Forsberg", "Mans Hulden."], "venue": "Proc. of EACL.", "citeRegEx": "Ahlberg et al\\.,? 2014", "shortCiteRegEx": "Ahlberg et al\\.", "year": 2014}, {"title": "Paradigm classification in supervised learning of morphology", "author": ["Malin Ahlberg", "Markus Forsberg", "Mans Hulden."], "venue": "Proc. of NAACL.", "citeRegEx": "Ahlberg et al\\.,? 2015", "shortCiteRegEx": "Ahlberg et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Towards a machinelearning architecture for lexical functional grammar parsing", "author": ["Grzegorz Chrupa\u0142a."], "venue": "Ph.D. thesis, Dublin City University.", "citeRegEx": "Chrupa\u0142a.,? 2008", "shortCiteRegEx": "Chrupa\u0142a.", "year": 2008}, {"title": "The SIGMORPHON 2016 shared task\u2014 morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "Proc. of the 2016 Meeting of SIGMORPHON.", "citeRegEx": "Cotterell et al\\.,? 2016", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Morphemes as necessary concept for structures discovery from untagged corpora", "author": ["Herv\u00e9 D\u00e9jean."], "venue": "Proc. of the Joint Conferences on New Methods in Language Processing and CoNLL.", "citeRegEx": "D\u00e9jean.,? 1998", "shortCiteRegEx": "D\u00e9jean.", "year": 1998}, {"title": "Latent-variable modeling of string transductions with finite-state methods", "author": ["Markus Dreyer", "Jason R Smith", "Jason Eisner."], "venue": "Proc. of EMNLP.", "citeRegEx": "Dreyer et al\\.,? 2008", "shortCiteRegEx": "Dreyer et al\\.", "year": 2008}, {"title": "A non-parametric model for the discovery of inflectional paradigms from plain text using graphical models over strings", "author": ["Markus Dreyer."], "venue": "Ph.D. thesis, Johns Hopkins University, Baltimore, MD.", "citeRegEx": "Dreyer.,? 2011", "shortCiteRegEx": "Dreyer.", "year": 2011}, {"title": "Supervised learning of complete morphological paradigms", "author": ["Greg Durrett", "John DeNero."], "venue": "Proc. of HLT-NAACL.", "citeRegEx": "Durrett and DeNero.,? 2013", "shortCiteRegEx": "Durrett and DeNero.", "year": 2013}, {"title": "Automatic extraction of morphological lexicons from morphologically annotated corpora", "author": ["Ramy Eskander", "Nizar Habash", "Owen Rambow."], "venue": "Proc. of EMNLP.", "citeRegEx": "Eskander et al\\.,? 2013", "shortCiteRegEx": "Eskander et al\\.", "year": 2013}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "arXiv preprint arXiv:1512.06110.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Maxout networks", "author": ["Ian Goodfellow", "David Warde-farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio."], "venue": "Proc. of ICML.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "Proc of. ICASSP.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Algorithms on strings, trees and sequences: computer science and computational biology", "author": ["Dan Gusfield."], "venue": "Cambridge university press.", "citeRegEx": "Gusfield.,? 1997", "shortCiteRegEx": "Gusfield.", "year": 1997}, {"title": "Word segmentation by letter successor varieties", "author": ["Margaret A Hafer", "Stephen F Weiss."], "venue": "Information storage and retrieval, 10(11):371\u2013385.", "citeRegEx": "Hafer and Weiss.,? 1974", "shortCiteRegEx": "Hafer and Weiss.", "year": 1974}, {"title": "From phoneme to morpheme", "author": ["Zellig S Harris."], "venue": "Language, 31(2):190\u2013222.", "citeRegEx": "Harris.,? 1955", "shortCiteRegEx": "Harris.", "year": 1955}, {"title": "MED: The LMU system for the SIGMORPHON 2016 shared task on morphological reinflection", "author": ["Katharina Kann", "Hinrich Sch\u00fctze."], "venue": "Proc. of the 2016 Meeting of SIGMORPHON.", "citeRegEx": "Kann and Sch\u00fctze.,? 2016", "shortCiteRegEx": "Kann and Sch\u00fctze.", "year": 2016}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton."], "venue": "arXiv preprint arXiv:1504.00941.", "citeRegEx": "Le et al\\.,? 2015", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Joint lemmatization and morphological tagging with lemming", "author": ["Thomas M\u00fcller", "Ryan Cotterell", "Alexander Fraser."], "venue": "Proc. of EMNLP.", "citeRegEx": "M\u00fcller et al\\.,? 2015", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2015}, {"title": "Inflection generation as discriminative string transduction", "author": ["Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak."], "venue": "Proc. of NAACL.", "citeRegEx": "Nicolai et al\\.,? 2015", "shortCiteRegEx": "Nicolai et al\\.", "year": 2015}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. of ICLR.", "citeRegEx": "Pascanu et al\\.,? 2014", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Proc. of NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proc. of NIPS.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "Prior work on MI and MRI includes machine learning models and models that exploit the paradigm structure of the language (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015).", "startOffset": 121, "endOffset": 179}, {"referenceID": 8, "context": "Prior work on MI and MRI includes machine learning models and models that exploit the paradigm structure of the language (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015).", "startOffset": 121, "endOffset": 179}, {"referenceID": 21, "context": "Prior work on MI and MRI includes machine learning models and models that exploit the paradigm structure of the language (Ahlberg et al., 2015; Dreyer, 2011; Nicolai et al., 2015).", "startOffset": 121, "endOffset": 179}, {"referenceID": 18, "context": "MED outperformed other systems on the SIGMORPHON16 shared task1 for all ten languages that were covered (Kann and Sch\u00fctze, 2016; Cotterell et al., 2016).", "startOffset": 104, "endOffset": 152}, {"referenceID": 5, "context": "MED outperformed other systems on the SIGMORPHON16 shared task1 for all ten languages that were covered (Kann and Sch\u00fctze, 2016; Cotterell et al., 2016).", "startOffset": 104, "endOffset": 152}, {"referenceID": 2, "context": "Our model is based on the network architecture proposed by Bahdanau et al. (2014) for machine translation.", "startOffset": 59, "endOffset": 82}, {"referenceID": 2, "context": "Our model is based on the network architecture proposed by Bahdanau et al. (2014) for machine translation.2 They describe the model in detail; unless we explicitly say so in the description of our model below, we use the same network configuration as Bahdanau et al. (2014). Bahdanau et al.", "startOffset": 59, "endOffset": 274}, {"referenceID": 2, "context": "Our model is based on the network architecture proposed by Bahdanau et al. (2014) for machine translation.2 They describe the model in detail; unless we explicitly say so in the description of our model below, we use the same network configuration as Bahdanau et al. (2014). Bahdanau et al. (2014)\u2019s model is an extension of the recurrent neural network (RNN) encoderdecoder developed by Cho et al.", "startOffset": 59, "endOffset": 298}, {"referenceID": 2, "context": "Our model is based on the network architecture proposed by Bahdanau et al. (2014) for machine translation.2 They describe the model in detail; unless we explicitly say so in the description of our model below, we use the same network configuration as Bahdanau et al. (2014). Bahdanau et al. (2014)\u2019s model is an extension of the recurrent neural network (RNN) encoderdecoder developed by Cho et al. (2014) and Sutskever et al.", "startOffset": 59, "endOffset": 406}, {"referenceID": 2, "context": "Our model is based on the network architecture proposed by Bahdanau et al. (2014) for machine translation.2 They describe the model in detail; unless we explicitly say so in the description of our model below, we use the same network configuration as Bahdanau et al. (2014). Bahdanau et al. (2014)\u2019s model is an extension of the recurrent neural network (RNN) encoderdecoder developed by Cho et al. (2014) and Sutskever et al. (2014). The encoder of the latter consists of an RNN that reads an input sequence of vectors x and encodes it into a fixed-length context vector c, computing hidden states ht and c by", "startOffset": 59, "endOffset": 434}, {"referenceID": 2, "context": "Bahdanau et al. (2014) proposed an attentionbased extension of this model that allows different vectors ct for each step by automatic learning of an alignment model.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "Further descriptions can be found in (Bahdanau et al., 2014).", "startOffset": 37, "endOffset": 60}, {"referenceID": 12, "context": "The final model is a multilayer network with a single maxout (Goodfellow et al., 2013) hidden layer that computes the conditional probability of each element in the output sequence (a letter in our case, (Pascanu et al.", "startOffset": 61, "endOffset": 86}, {"referenceID": 22, "context": ", 2013) hidden layer that computes the conditional probability of each element in the output sequence (a letter in our case, (Pascanu et al., 2014)).", "startOffset": 125, "endOffset": 147}, {"referenceID": 25, "context": "For training, we use stochastic gradient descent, Adadelta (Zeiler, 2012) and a minibatch size of 20.", "startOffset": 59, "endOffset": 73}, {"referenceID": 19, "context": "We initialize all weights in the encoder, decoder and the embeddings except for the GRU weights in the decoder with the identity matrix as well as all biases with zero (Le et al., 2015).", "startOffset": 168, "endOffset": 185}, {"referenceID": 4, "context": "An edit tree e(\u03c3, \u03c4) specifies a transformation from a source string \u03c3 to a target string \u03c4 (Chrupa\u0142a, 2008).", "startOffset": 92, "endOffset": 108}, {"referenceID": 15, "context": "To compute e(\u03c3, \u03c4), we first determine the longest common substring (LCS) (Gusfield, 1997) between \u03c3 and \u03c4 and then recursively model the prefix and suffix pairs of the LCS.", "startOffset": 74, "endOffset": 90}, {"referenceID": 9, "context": "(2008) as well as with two recently proposed models: (i) discriminative string transduction (Durrett and DeNero, 2013; Nicolai et al., 2015), the SIGMORPHON16 baseline, and (ii) Faruqui et al.", "startOffset": 92, "endOffset": 140}, {"referenceID": 21, "context": "(2008) as well as with two recently proposed models: (i) discriminative string transduction (Durrett and DeNero, 2013; Nicolai et al., 2015), the SIGMORPHON16 baseline, and (ii) Faruqui et al.", "startOffset": 92, "endOffset": 140}, {"referenceID": 7, "context": "We compare MED with the three models of Dreyer et al. (2008) as well as with two recently proposed models: (i) discriminative string transduction (Durrett and DeNero, 2013; Nicolai et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 7, "context": "We compare MED with the three models of Dreyer et al. (2008) as well as with two recently proposed models: (i) discriminative string transduction (Durrett and DeNero, 2013; Nicolai et al., 2015), the SIGMORPHON16 baseline, and (ii) Faruqui et al. (2015)\u2019s encoder-decoder model.", "startOffset": 40, "endOffset": 254}, {"referenceID": 7, "context": "For this experiment we follow Dreyer et al. (2008). We use four pairs of morphological tags and corresponding word forms from the German part of the CELEX morphological database.", "startOffset": 30, "endOffset": 51}, {"referenceID": 7, "context": "We use the same data splits as Dreyer et al. (2008), dividing the original 2500 samples for each tag into five folds, each consisting of 500 training and 1000 development and 1000 test samples.", "startOffset": 31, "endOffset": 52}, {"referenceID": 7, "context": "Results of (Dreyer et al., 2008)\u2019s model are from their paper; backoff: ngrams+x model; lat-class: ngrams+x+latent class model; lat-region: ngrams+x+latent class+latent region model; baseline: SIGMORPHON16 baseline.", "startOffset": 11, "endOffset": 32}, {"referenceID": 7, "context": "Unlike Dreyer et al. (2008)\u2019s models, MED does not use any hand-crafted features.", "startOffset": 7, "endOffset": 28}, {"referenceID": 17, "context": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D\u00e9jean, 1998) and different approaches for MRI (Ahlberg et al.", "startOffset": 63, "endOffset": 114}, {"referenceID": 16, "context": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D\u00e9jean, 1998) and different approaches for MRI (Ahlberg et al.", "startOffset": 63, "endOffset": 114}, {"referenceID": 6, "context": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D\u00e9jean, 1998) and different approaches for MRI (Ahlberg et al.", "startOffset": 63, "endOffset": 114}, {"referenceID": 0, "context": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D\u00e9jean, 1998) and different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015).", "startOffset": 148, "endOffset": 241}, {"referenceID": 9, "context": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D\u00e9jean, 1998) and different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015).", "startOffset": 148, "endOffset": 241}, {"referenceID": 10, "context": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D\u00e9jean, 1998) and different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015).", "startOffset": 148, "endOffset": 241}, {"referenceID": 21, "context": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D\u00e9jean, 1998) and different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015).", "startOffset": 148, "endOffset": 241}, {"referenceID": 0, "context": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D\u00e9jean, 1998) and different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). Chrupa\u0142a (2008) defined edit trees and Chrupa\u0142a (2008) and M\u00fcller et al.", "startOffset": 149, "endOffset": 259}, {"referenceID": 0, "context": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D\u00e9jean, 1998) and different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). Chrupa\u0142a (2008) defined edit trees and Chrupa\u0142a (2008) and M\u00fcller et al.", "startOffset": 149, "endOffset": 298}, {"referenceID": 0, "context": "Earlier work on morphology includes morphological segmentation (Harris, 1955; Hafer and Weiss, 1974; D\u00e9jean, 1998) and different approaches for MRI (Ahlberg et al., 2014; Durrett and DeNero, 2013; Eskander et al., 2013; Nicolai et al., 2015). Chrupa\u0142a (2008) defined edit trees and Chrupa\u0142a (2008) and M\u00fcller et al. (2015) use them for morphological tagging and lemmatization.", "startOffset": 149, "endOffset": 323}, {"referenceID": 3, "context": "machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al.", "startOffset": 20, "endOffset": 85}, {"referenceID": 23, "context": "machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al.", "startOffset": 20, "endOffset": 85}, {"referenceID": 2, "context": "machine translation (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014), parsing (Vinyals et al.", "startOffset": 20, "endOffset": 85}, {"referenceID": 24, "context": ", 2014), parsing (Vinyals et al., 2015) and speech recognition (Graves and Schmidhuber, 2005; Graves et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 13, "context": ", 2015) and speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013).", "startOffset": 31, "endOffset": 82}, {"referenceID": 14, "context": ", 2015) and speech recognition (Graves and Schmidhuber, 2005; Graves et al., 2013).", "startOffset": 31, "endOffset": 82}, {"referenceID": 11, "context": "MED bears some resemblance to Faruqui et al. (2015)\u2019s work.", "startOffset": 30, "endOffset": 52}], "year": 2016, "abstractText": "Morphological reinflection is the task of generating a target form given a source form, a source tag and a target tag. We propose a new way of modeling this task with neural encoder-decoder models. Our approach reduces the amount of required training data for this architecture and achieves state-of-the-art results, making encoder-decoder models applicable to morphological reinflection even for lowresource languages. We further present a new automatic correction method for the outputs based on edit trees.", "creator": "TeX"}}}