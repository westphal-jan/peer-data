{"id": "1606.01151", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2016", "title": "Privacy Protection for Natural Language Records: Neural Generative Models for Releasing Synthetic Twitter Data", "abstract": "understanding redaction risk has been the second most common psychological approach to protecting valuable text data, and but treating synthetic verbal data nonetheless presents a highly potentially more reliable alternative tools for disclosure control. by dramatically producing new textual sample values which closely follow the original sample distribution but don't contain actually real expected values, quality privacy versus protection can be improved while utility prevented from providing the data for specific viewing purposes is maintained. we extend the basic synthetic data structure approach to natural language by actually developing fully a simplified neural generative model for such data. we find that the synthetic models outperform our simple redaction, on both comparative inference risk capability and intrinsic utility.", "histories": [["v1", "Fri, 3 Jun 2016 15:43:15 GMT  (385kb,D)", "https://arxiv.org/abs/1606.01151v1", null], ["v2", "Fri, 14 Oct 2016 03:34:37 GMT  (558kb,D)", "http://arxiv.org/abs/1606.01151v2", null], ["v3", "Fri, 13 Oct 2017 22:14:38 GMT  (1269kb,D)", "http://arxiv.org/abs/1606.01151v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander g ororbia ii", "fridolin linder", "joshua snoke"], "accepted": false, "id": "1606.01151"}, "pdf": {"name": "1606.01151.pdf", "metadata": {"source": "CRF", "title": "Using Neural Generative Models to Release Synthetic Twitter Corpora with Reduced Stylometric Identifiability of Users", "authors": ["Alexander G. Ororbia II", "Fridolin Linder", "Joshua Snoke"], "emails": [], "sections": [{"heading": null, "text": "We present a method for generating synthetic versions of Twitter data using neural generative models. The goal is to protect individuals in the source data from stylometric re-identification attacks while still releasing data that carries research value. To generate tweet corpora that maintain user-level word distributions, our proposed approach augments powerful neural language models with local parameters that weight user-specific inputs. We compare our work to two standard text data protection methods: redaction and iterative translation. We evaluate the three methods on risk and utility. We define risk following the stylometric models of re-identification, and we define utility based on two general language measures and two common text analysis tasks. We find that neural models are able to significantly lower risk over previous methods at the cost of some utility. More importantly, we show that the risk utility trade-off depends on how the neural model\u2019s logits (or the unscaled preactivation values of the output layer) are scaled. This work presents promising results for a new tool addressing the problem of privacy for free text and sharing social media data in a way that respects privacy and is ethically responsible."}, {"heading": "1 Introduction", "text": "This paper presents a method to produce synthetic tweet corpora that can be released in place of the original tweets, with the goals of minimizing the identifiability of users and maintaining user specific word distributions as well as the aggregate, analytical usefulness of the original corpus. In\nPreliminary work. Under review by AISTATS 2018. Do not distribute.\nthe field of data privacy, these two goals (colloquially known as minimizing risk and maximizing utility, see [10]) form the basis for measuring the effectiveness of any protection method. Risk and utility always reside in tension to each other which means that protection methods offer the certain levels of utility for a certain levels of risk. Ideally, we want a variety of protection methods (or a way of tuning current methods) to produce different levels of risk and utility in order to allow data providers to release data as they see fit.\nWhen dealing with unstructured text documents, privacy risk for users has been measured through stylometric models designed to identify anonymized authors based on the text features. These models have been shown to be successful in the task of author attribution in document collections such as the Federalists papers [21], online blogs [22], and even executable binaries [7]. Recent work has also shown similar promise in short texts such as tweets (which exhibit high levels of linkability), despite the small character limit [1].\nOur concern in this paper is a stylometric attack situation involving a researcher collecting and releasing a corpus of tweets, which is quite common in the social sciences. It may be possible that this corpus includes tweets which are no longer publicly available, either from users self-deleting their content or Twitter\u2019s policy actions (e.g. violations, length of time). As some others have pointed out [35], Twitter states in their developer guidelines that users have the right to be forgotten [33], such that if tweets are deleted, researchers will not use or release those tweets further. In addition to this, a number of papers are calling for stronger ethical standards when publishing social media data [35, 36, 28]. The primary privacy risk for users comes from information disclosed through the sharing of previous deleted tweets or through new information generated from analyses, e.g identifying hate speech [5]. In this case, if users were identified and linked to their tweets it would entail a privacy breach since person-specific information would be made public beyond what they had intended to share.\nWhile most of the stylometry literature has focused on developing attack models, some methods do exist to protect the text from stylometric attacks. These include semi-automatic methods that can obscure individual authors [14] or ap-\nar X\niv :1\n60 6.\n01 15\n1v 3\n[ cs\n.C L\n] 1\n3 O\nct 2\n01 7\nproaches such as iterative translation [27, 19], which have been shown to offer little protection in some cases [6]. In addition to risk concerns, redaction or translation are blunt instruments that do not take into consideration the original language of the text and may remove features which are vital for researchers. We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29]. In particular, neural architectures offer the attractive option of greatly reducing the amount of human involvement required in extracting the properties of language and (word) distributional information from the original documents. From a utility standpoint, generating tweets for release that capture the language of the original texts should outperform redaction or translation. On the risk side, neural models will not precisely replicate the original tweets (it would be severe overfitting if they did), so risk should be reduced by the noise added in the new text generation process.\nComparing synthetic corpora generated by our neural models with those produced through redaction or translation (procedures for protecting text data against stylometric attacks), our experimental results show that data generated by neural models can offer an improved risk utility tradeoff. We further show that the risk utility trade-off can be managed by the scaling of the neural model\u2019s output logits (using a coefficient commonly called temperature, which we will refer to moving forward), whereas previous methods did not offer such a clear \"knob\". Since many researchers currently collect and share Twitter data, this work offers a promising, simple way to allow such data to be shared while improving privacy for those whose data are collected."}, {"heading": "2 Problem Formulation", "text": "Our proposed method addresses two fundamental needs which are at tension with each other when releasing data for research under privacy constraints. First, the released data should reduce the risk, or in this case, identifiability, as much as possible for the individuals in the data. Second, the released data should preserve as much research value (i.e., utility) of the original data as possible. In the following sections we define how we measure these two outcomes."}, {"heading": "2.1 Re-identification attack model", "text": "Assuming the released tweet corpus is \"anonymized\", i.e. the real usernames are removed and replaced with an identifier to connect the tweets in the corpus, identification can still occur through a stylometric attack, which links publicly available texts composed by the users with the unlabeled tweets in the corpus. We also assume the tweets do not contain metadata, which would need to be handled with additional consideration in order to protect individuals\u2019 privacy. We emulate the stylometric attack by using an approach akin\nto that of [22] and [1]. We assume a release data set, R, and an attack data set, A, with matching sets of k users, U = {u1, u2, ..., uk}. For each user in R and A, there are a set of corresponding tweets:\ntRij , i \u2208 1...nRj & j \u2208 1...k\ntAij , i \u2208 1...nAj & j \u2208 1...k\nR represents a theoretical tweet corpus collected and released using anonymized user labels. The attacker who wishes to identify users in R collects A, which has labeled tweets for the users of interest. In a realistic scenario, the attacker likely does not know the exact set of users in the target data (or is not interested in disclosing all of their identities), but by assuming the user sets of R and A are the same, we place an upper bound on the attacker\u2019s knowledge of users in the release data. This allows us to estimate the identification percentage across all users in the sample.\nFor each user in A, we train classifiers on A to predict each user versus all other users. We utilize a variety of feature sets and models for this task. The aim of this paper, however, is not to test stylometric attack models but rather to test the methods of protection. As a result, we report results using features and models that have been used in previous studies as well as basic uni-grams and bi-grams. The top identifying feature set is used in the experimental results of section 4.3 while the rest are reported in the supplementary material. Table 1 displays the five different feature sets used for the attack models. We use the uni-grams and bi-grams as simple common sense feature sets, and we used the feature sets from [1] and [22] because they were both shown to perform quite well for re-identification.\nFor each of these feature sets we built four models: regularized least squares (RLSC), support vector machines (SVM), naive Bayes (NB), and k-nearest neighbors (KNN). These models were used in [22, 1]. For RLSC, SVM, and KNN, we use row and feature normalization following in line with [22]. As with the feature sets, it is highly likely an attacker would try multiple different models to achieve re-identification, so we test multiple models in order to calculate an estimate of the risk under different attacks. Again, the best identifying model and feature set combination is used in the experimental results in section 4.3 (the rest are given in the supplementary material). We also note that an attacker who tried different models and feature sets would not know which model performs the best (since he/she does not have access to the true target variables), but would more likely estimate model generalization through personal crossvalidation. By finding which feature set/model combination poses a large risk for our given data set, we validate our protection methods on more than single attack.\nFor each trained model and feature set, we test the models using the data in R, producing a ranking of user matches from highest to lowest. Our overall identification risk can be\nsummarized as the percentage of users with correct matches in the top x (e.g. 1, 5, 10) most likely users. For example, if x is 1, this is the percentage of most likely users which match the true identity. We also fix the number of test samples for each user (nRj ) in R at 99 tweets in order to have comparable testing sets for each user. As suggested by [22] we collapse the feature vectors for each user in R to the mean for testing. This allows us to gain a single ranking of user classifications rather than a set for each tweet. We do not collapse feature vectors during training, except in the case of the KNN, where we collapse vectors to the centroid for each user to improve computational ability."}, {"heading": "2.2 Evaluating research utility", "text": "We measure the utility or usefulness of the released tweet corpus in two ways: the general word distributions and similarity of results from commonly used text analyses. Utility is always compared to the baseline, i.e. the original, unaltered data, since the maximal utility would come from a researcher having access to the original tweets.\nFor distributional similarity, we use the cosine similarity of the uni-grams and bi-grams between the original and each altered corpus, defined as:\nCosSim(x, y) = \u03a3ixiyi\u221a \u03a3ix2i \u221a \u03a3iy2i\nwhere x and y are term frequency vectors for a user in the original and the altered corpus. Because the data has user labels, we calculate the similarity for each user and take the average across all users as our utility measure. We do this because each user in the corpus may have specific interests or ways of communicating, which a researcher would want to utilize. Our protection method should preserve not only the overall word distribution, but it should preserve each user distribution as best as possible. A high average user cosine similarity implies the protection method has preserved the overall language of each user.\nFor model-specific assessment, we consider the results from\ntwo common types of analyses. The first was a classification task to predict tweets that reference the band \u201cOne Direction\u201d, either by name (\u2019one direction\u2019), hashtag (\u2019#onedirection\u2019), or handle (\u2019onedirection\u2019). This was chosen due to the relatively high incidence rate (\u223c 20%) compared to other terms. We labeled tweets with these references and trained a model on the baseline data to predict the label given the rest of the terms in the tweet. Using cross-validation we got a baseline estimate of the out-of-sample F1-score, which combines precision and recall.\nF1-score = precision \u2217 recall precision+ recall\nWe then tested the cross-validated model on the different release data sets, to get F1-scores for each of the altered corpora. We expect the perturbation, either from redaction, translation, or synthesis to decrease the accuracy of the model, so we can use these results as an analysis specific utility measure. Generally speaking, the better the release data maintains the feature distribution of the baseline data, the closer the resulting F1-score would be to the baseline F1-score. Another way to think about this is that if the altered corpus carried the same distributional properties as the original corpus, we would expect similar performance on a test set.\nFor the second, model-specific measure, we analyze how well the different protection methods preserve sentiment by user, using the \u201cVader\u201d sentiment model, specifically developed to measure sentiment in social media data [13]. For each release, we generate a vector of user sentiment scores, representing the average compound sentiment (across tweets) for each user. The utility measure is then the cosine similarity between the vector for each release and the baseline\u2019s vector. While this is similar in nature to the general word distribution measures, building sentiment models is a very common analysis task using Twitter data. If we were working with numerical data this would be similar to comparing the covariance matrices versus comparing a specific regression model. While the former tells us overall how the datasets compare, the second gives us a specific\ninsight into how the protection methods affect our inference models."}, {"heading": "3 Protection method", "text": ""}, {"heading": "3.1 A Multi-User Conditional Synthesis Model", "text": "In this paper, we aim to model the unknown data generating process underlying an observed tweet corpus,R, conditional on the set of users, U . If able to accurately approximate this generative distribution, we can sample at the character level and release new synthetic tweets in place of the originals. Motivated by recent successes in function approximation through the use of parametrized neural architectures [11, 9, 30], we propose an architecture for the task of text synthesis. In particular, to better capture temporal and structural information inherent in the data, given a finite dictionary of symbols D, we assume that for input sequences c = (ct, ct\u22121, . . . , ct\u2212m) the distribution of interest is p(ct|j, c<t), where ct is a 1-of-|D| encoding of a symbol (such as a character) at time t indexed in D. c<t is the historical context preceding the current symbol of length m\u22121 and j is the index of the user to be associated with this encoded symbol. By learning a good generative model for this conditional distribution, we are able to generate valid symbol sequence samples representative of specific, target users. In the architecture proposed in this paper, this can be easily done by clamping the model\u2019s \u201cuser\u201d units to a particular index.\nIn order to learn from and generate sequences of symbols, one must account for the ordering of symbols. To this end, we design a recurrent neural architecture that specifically learns to approximate p(ct|j, c<t). This architecture can be viewed as processing two parallel streams of encodings\u2013 a stream of symbols and a corresponding stream of user indices (injectively mapped). The model predicts the symbol at time t + 1 using a vector summary of the past (i.e., the last hidden state ht\u22121), the currently observed token at t, and knowledge of the current user ej (also a 1-of-k binary encoding corresponding to the user uj). The task is to fit this model to the target corpus R that we want to synthesize. The architecture, unfolded over time, is shown in Figure 1.\nIn contrast to building a separate model for each user in the data-set, our model shares its \u201clanguage model\u201d parameters across the multiple views of the data, similar in spirit to the hybrid architectures of [26, 23, 24] or the log-linear models of [17]. In this way, we may construct a single model that efficiently learns user-specific parameters (contained within a single |H|x|R| matrix) jointly with the general language model parameters (which aggregates across users).\nInterestingly enough, our specialized neural architecture also addresses a recent problem found in neural conversation agents [34]. This issue centers around \u201ccoherence\u201d, where a trained neural model has no sense of identity or\nself (even at the crudest level). For example, if asked the question, \u201cAre you married?\u201d, the model responds with \u201cNo!\u201d, but when followed up with the question \u201cWhat is your wife\u2019s name?\u201d, the model might respond with, \u201cCynthia\u201d. While this issue is more prominent in sequence-to-sequence modeling tasks (as in question-answering, dialogue modeling), we also argue that in synthetic data-generation, where samples often come with meta-data, having a model that preserves local information, such as user identity, is crucial. We note that other work has attempted to address the coherence/consistency problem in neural models [31, 18]. However, our approach is notably simpler in representing user-local information and differs in problem context compared to sequence-to-sequence mapping models.\nFormally, with model parameters \u0398 = (Wuser,Win,Wrec,Wout) (biases have been omitted for clarity), we calculate hidden and output states via the following set of equations:\nht = \u03c6hid(Winc +Wuserej +Wrecht\u22121) (1) ot = \u03c6out(Woutht) (2)\nwhere ej is the one-hot encoding of a user index j and (\u03c6hid, \u03c6out) are the activation functions for input-to-hidden and hidden-to-output layers respectively. While we present the simplest formulation of our model above (the simple Elman-RNN), further improvement in bits-per-character (BPC) was found when experimenting with more complex gated models, unified under the Differential State Framework [25]. Variations we experimented with included the concrete model proposed in [25], the Delta-RNN, and a Gated Recurrent Unit (GRU) [8]. In this paper, for \u03c6out we choose to parametrize the posterior as a maximum-entropy classifier, defined as follows:\nP (w,ht) = P\u0398(w|ht) = exp (wTWoutht/\u03c4)\u2211\nw\u2032 exp ((w \u2032)TWoutht/\u03c4)\n,\n(3)\nwhere \u03c4 , the temperature meta-parameter, controls the smoothness of the output probability distribution. When \u03c4 is increased (i.e., \u03c4 \u2192 \u221e), the output classes tend towards equal probabilities, yielding more random samples, increasing the risk of mistakes but lowering the probability of generating highly similar text snippets (which can sometimes lead to strange generation paths). Lowering \u03c4 (i.e., \u03c4 \u2192 0+) sharpens the posterior distribution, making the probability of the target class (or the class that model assigns highest probability to) closer to 1. This means that the model will be more driven by the frequently observed terms of the original data sample in what it generates, but its samples will be made with higher confidence. At too low of temperatures we may only obtain high frequency samples, chopping off the tail of the observed term distribution.\nWe expect that raising the temperature will lower the distributional similarity of users to the original data (harming utility), but will lower the identification risk.\nNote that this single-hidden layer model is \u201cdeep in time\u201d and can be easily made to be deep in structure (i.e., stacking more hidden layers). However, each hidden layer should sport a set of skip-connections to the user input layer, so that way all layer activations of the model are biased towards user-local distributed representations. Parameters of the model are fit to the data via stochastic gradient descent using truncated back-propagation through time to calculate parameter gradients (wherem is used to control the length of the window, or number of steps back in time). The objective is to minimize the negative log likelihood of the predictive posterior over the sequence:\nL(c) = \u2212 m\u2211 t=1 log p(ct|ot\u22121) (4)\nThe parameters used to model individual users are also modified as part of the back-propagation-based procedure. Note that these learnt user \u201cembeddings\u201d could be useful for auxiliary tasks, such as clustering user representations together for similarity-search-based applications.\nTo generate samples from the neural model, we simply make use of the model\u2019s efficient inference procedure, similar to that in [12]. Specifically, by clamping the input units corresponding to a desired target user index and feeding in a \u201cnull\u201d vector (or vector of all zeroes) as initial input, we may sample from its output probabilities and ultimately generate synthetic symbol sequences for individuals by feeding in a sample of model\u2019s predicted output back in as input for the next step. A sequence is continuously generated until either a simple end-token is generated (in our case this was a token in the set [., !?], or simple punctuation) or an upper bound on the character limit is reached (this is particularly useful for Twitter data, which caps the total characters)."}, {"heading": "3.2 Previous protection methods", "text": "Two other protections method were applied to the release data set, redaction and iterative translation. For redaction, we simply removed all hashtags and handles present in the corpus, assuming that many users frequently repeat hashtags and handles which makes these features best used for identifying a user. The iterative translation was based on [19] and worked by first translating the original (redacted) corpus into Arabic and then back into English using the Google Translate API1. Arabic was chosen because it was found to offer the highest level of protection in [19]. The translation was performed on top of the redacted data because the hashtags and handles would likely to not change throughout the iterative translation. If left in, we would expect them to weaken the translation-based approach allow for users to be more easily identified."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Dataset", "text": "The dataset we experiment with consists of tweets collected by [3] for a study on German twitter users. Twitter users were randomly sampled (by generating uniformly random Twitter ids and selecting based on profile-level selection criteria). For the evaluation here, we only select tweets classified as English by Twitter\u2019s language detection algorithm. The dataset was then divided into two parts, as described in section 2.1. The attack dataset contained 386,684 tweets across 627 users and the release data set contained 62,073 tweets across the same 627 users.\n1cloud.google.com/translate"}, {"heading": "4.2 Experimental Design", "text": "We produced 15 total neural generated synthetic data sets using three model variations and five temperature settings as described in section 3.1. The three variations of our neural synthesis model were the Elman RNN, the DeltaRNN, and the GRU. Optimization of parameters was carried out through stochastic gradient descent using the ADAM [15] adaptive learning rate scheme (step size of \u03bb = 0.002 was found to be sufficient in preliminary experiments). All models were trained for 150 epochs, with updates calculated using mini-batches of 32 samples. Since tweets are bounded, we were able to use full back-propagation through time (without any truncation). Regularization consisted of layer normalization [2], with exact details of which preactivations within each architecture the normalization was applied to provided in the supplementary material. For each of the three, five synthetic corpora were produced for each of the temperatures {0.25, 0.5, 1, 1.25, 1.75}.\nThe re-identification risk was estimated following the description given in section 2.1, with the five feature sets and four classification models, resulting in 20 risk estimates. The utility was estimated using the four measures detailed in section 2.2: two general distributional measures and two analysis specific measures. The two general measures were average user uni-gram and bi-gram cosine similarity between the baseline data and each altered data set, with a high similarity implying most users have similar word distributions between the baseline and altered data sets. The two model-specific measures were a classification task to\npredict a reference to the band \u201cOne Direction\u201d and a sentiment analysis task. High utility implied similarity of results between the original and altered/synthesized corpora."}, {"heading": "4.3 Experimental Results", "text": "We first consider just the risk results for the baseline, redacted, translated, and the three neural models (at temperature 1). From the five feature sets we found that the bi-grams had the best performance, and within that feature set the SVM model performed the best (highest percentage of users identified as top 1 most likely in the baseline). We found that for 82% of users, the most likely predicted candidate was the true user. This shows a very significant risk of re-identification in the unaltered data. These results show comparable re-identification levels compared to previous work [22, 1], which supports our risk models.\nFigure 2 shows the results for each of the four models with the bi-gram feature set. It shows the percentage of users correctly identified in the top n predicted users. We see that for three of the four models, the classifiers do quite well at identifying the true user among the most likely. We also see that all the protection methods do indeed decrease the risk, but at varying levels. Generally the Elman RNN models has the lowest risk, while either the redacted or GRU have the highest risk (out of the set of protection methods). A full plot of the 20 risk results (5 feature sets x 4 models) is provided in the supplementary material.\nWe can visualize these results along with the utility by using\nrisk utility mapping, shown in figure 3 (full results also given in table 2). There are four plots in the figure, each showing a risk utility map for each of our four utility measures. In all cases, utility ranges from zero to one with zero being the lowest utility and one being the highest. Similarly, risk ranges from zero to one with zero being the lowest risk and one being the highest. An optimal protection method would produce utility of one and risk of zero (top left corner). We expect methods to move roughly along a curve from the bottom left corner (no utility, no risk) to the top right corner\n(most utility, most risk). We can use the baseline release as the maximum utility, maximum risk case, since it implies releasing the original data completely unaltered. And as expected, the baseline resides in the top right corner of the risk utility maps.\nConsidering first the general distributional measures, we look at the average user uni-gram and bi-gram cosine similarities (third and fourth panels). For the uni-grams we see that this expected relationship between risk and utility for\nthe neural models, with both risk and utility increasing as the temperature decreases. Interestingly, we see this trend change when the temperature gets very small (0.25). This is because for lower temperatures we are chopping off more from the tails of the user word distributions, which implies both lower risk due to increased rarity of identifying words and also lower utility due to less distributional similarity. For uni-grams, the redacted and translation methods fall below the line formed by the neural methods, which implies they offer a worse risk utility trade-off. For the bi-grams, the translation approach is comparable to the neural methods while the redacted sits above the line. It appears, in both cases, that the Delta-RNN has the highest line, offering the best risk utility trade-off.\nLooking at the two model-specific measures (first and second panels), we see the neural methods perform quite well for the classification task. The redacted and translated sit well below the line, implying a better risk utility trade-off from the neural methods. In this case, at very low temperatures the risk still decreases but the utility actually improves, moving the line towards the top left corner. While this is only one example, this is a very promising result. The sentiment results look fairly similar to the uni-grams and bigrams, which makes sense because it is a dictionary based task. Both the redacted and translated perform well, sitting above the line formed by the neural methods, but overall every method offers very high utility. While still interesting, the sentiment task gives us less ability to differentiate between the protection methods.\nThe temperature plays an important role in the risk utility trade-off, with higher temperatures providing less risk and lower temperatures generally providing more utility. This happens because at lower temperatures the more frequently observed terms are more likely to be generated, thus producing tweets that look more like the original language. Table 3 shows example tweets from the Delta-RNN for each of the temperatures. It is easy to see that the tweets become less like actual English at higher temperatures (though some may wonder if Twitter is ever actual English). Particularly the spelling (since we are building character-level generative models) and the grammar gets worse with higher temperatures."}, {"heading": "5 Discussion", "text": "This paper presents a novel method using neural models for reducing the risk from stylometric attacks on text data while preserving the original language. We show that a more informed risk utility trade-off is possible over previous methods such as redaction and translation. Particularly we find that the neural methods can reduce the risk far beyond the other methods, at some cost of utility. In some cases, such as uni-gram distributions or the classification task, the neural methods offered a better trade-off, while in the case of bi-grams or the sentiment task they did not. Either way, when using the temperature (of the output distribution) as a \"knob\" to control the risk and utility, the neural models allow us to set the appropriate levels. Redaction on the other hand, while maintaining high utility, still allowed over 50% re-identification of the users which may be unacceptably high. By testing a variety of attack models and feature sets, we showed that our protection methods are robust to more than one attack. We found a similar level of re-identification risk for the baseline corpus as was shown in previous papers, validating our stylometric models.\nThe use of temperature to control the risk and utility seems an important feature for future work. Additionally, we see a different curve for the temperature when using different models. For example, the GRU carried higher risk even for higher temperatures, whereas the Delta-RNN and RNN changed quite a bit as the temperature lowered. In all cases, the desired risk utility trade-off is important, and giving a range of output possibilities is key. In application, a data provider would need to assess the desired levels of risk and utility, and would need tools to match these levels.\nIn order to further improve the performance of the neural synthesis models proposed in this paper, future work should include reformulating our models such that neural variational inference [16] may be employed. This would facilitate the learning of richer probabilistic language models that might capture yet more complex features of the original corpora distributions one might want to synthesize. Furthermore, operating under such a Bayesian framework would allow us to easily integrate better text-specific prior distributions, such as the piecewise-constant distribution [29], easing the learning of difficult, multi-modal distributions."}], "references": [{"title": "Stylometric linkability of tweets", "author": ["M. ALMISHARI", "D. KAAFAR", "E. OGUZ", "G. TSUDIK"], "venue": "In Proceedings of the 13th Workshop on Privacy in the Electronic Society (2014),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Are governments more responsive to voters in issues they own? a comparative study of the quality of political representation using social media data", "author": ["P. BARBERA", "J. B\u00d8LSTAD"], "venue": "Presented at Polmeth", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "AND JAU- VIN, C. A neural probabilistic language model", "author": ["Y. BENGIO", "R. DUCHARME", "P. VINCENT"], "venue": "Journal of machine learning research 3,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Cyber hate speech on twitter: An application of machine classification and statistical modeling for policy and decision making", "author": ["P. BURNAP", "M.L. WILLIAMS"], "venue": "Policy & Internet 7,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Translate once, translate twice, translate thrice and attribute: Identifying authors and machine translation tools in translated text", "author": ["A. CALISKAN", "R. GREENSTADT"], "venue": "In Semantic Computing (ICSC),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "When coding style survives compilation: De-anonymizing programmers from executable binaries", "author": ["A. CALISKAN-ISLAM", "F. YAMAGUCHI", "E. DAUBER", "R. HARANG", "K. RIECK", "R. GREENSTADT", "A. NARAYANAN"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "A recurrent latent variable model for sequential data. In Advances in neural information processing systems", "author": ["J. CHUNG", "K. KASTNER", "L. DINH", "K. GOEL", "A.C. COURVILLE", "Y. BENGIO"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Disclosure risk vs. data utility: The ru confidentiality map", "author": ["G.T. DUNCAN", "S.A. KELLER-MCNULTY", "S.L. STOKES"], "venue": "Citeseer", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Generative adversarial nets", "author": ["I. GOODFELLOW", "J. POUGET-ABADIE", "M. MIRZA", "B. XU", "D. WARDE-FARLEY", "S. OZAIR", "A. COURVILLE", "Y. BENGIO"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. GRAVES"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A parsimonious rule-based model for sentiment analysis of social media text", "author": ["C.J. HUTTO", "GILBERT", "E. Vader"], "venue": "In Eighth international AAAI conference on weblogs and social media", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Obfuscating document stylometry to preserve author anonymity", "author": ["G. KACMARCIK", "M. GAMON"], "venue": "In Proceedings of the COLING/ACL on Main conference poster sessions", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Adam: A method for stochastic optimization", "author": ["KINGMA D", "BA"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["D.P. KINGMA", "M. WELLING"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. LE", "T. MIKOLOV"], "venue": "arXiv preprint arXiv:1405.4053", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A persona-based neural conversation model", "author": ["J. LI", "M. GALLEY", "C. BROCKETT", "J. GAO", "B. DOLAN"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "The best way to a strong defense is a strong offense: Mitigating deanonymization attacks via iterative language translation", "author": ["N. MACK", "J. BOWERS", "H. WILLIAMS", "G. DOZIER", "J. SHELTON"], "venue": "International Journal of Machine Learning and Computing", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["T. MIKOLOV", "M. KARAFI\u00c1T", "L. BURGET", "J. CERNOCK\u1ef2", "S. KHUDANPUR"], "venue": "In Interspeech (2010),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "On the feasibility of internet-scale author identification", "author": ["A. NARAYANAN", "H. PASKOV", "N.Z. GONG", "J. BETHEN- COURT", "E. STEFANOV", "E.C.R. SHIN", "D. SONG"], "venue": "In Security and Privacy (SP),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Learning a deep hybrid model for semi-supervised text classification", "author": ["A.G. ORORBIA II", "C.L. GILES", "D. REITTER"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Online semi-supervised learning with deep hybrid boltzmann machines and denoising autoencoders", "author": ["A.G. ORORBIA II", "C.L. GILES", "D. REITTER"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Learning simpler language models with the differential state framework", "author": ["A.G. ORORBIA II", "T. MIKOLOV", "D. REITTER"], "venue": "Neural Computation", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "Online learning of deep hybrid architectures for semisupervised categorization. In Machine Learning and Knowledge Discovery in Databases (Proceedings", "author": ["A.G. ORORBIA II", "D. REITTER", "J. WU", "C.L. GILES"], "venue": "ECML PKDD 2015),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Can pseudonymity really guarantee privacy", "author": ["J.R. RAO", "P ROHATGI"], "venue": "In USENIX Security Symposium", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "Ethical research standards in a world of big data", "author": ["C.M. RIVERS", "B.L. LEWIS"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Piecewise latent variables for neural variational text processing", "author": ["I.V. SERBAN", "A. ORORBIA II", "J. PINEAU", "A. COURVILLE"], "venue": "In Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2017}, {"title": "Ladder variational autoencoders", "author": ["C.K. S\u00d8NDERBY", "T. RAIKO", "L. MAAL\u00d8E", "S.K. S\u00d8NDERBY", "O. WINTHER"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. SORDONI", "M. GALLEY", "M. AULI", "C. BROCKETT", "Y. JI", "M. MITCHELL", "NIE", "GAO J.-Y", "B. DOLAN"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems", "author": ["I. SUTSKEVER", "O. VINYALS", "LE", "Q. V"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "A neural conversational model", "author": ["VINYALS O", "LE"], "venue": "arXiv preprint arXiv:1506.05869", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Towards an ethical framework for publishing twitter data in social research: Taking into account users\u2019 views, online context and algorithmic estimation", "author": ["M.L. WILLIAMS", "P. BURNAP", "L. SLOAN"], "venue": "Sociology", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2017}, {"title": "but the data is already public\u201d: on the ethics of research in facebook", "author": ["M. ZIMMER"], "venue": "Ethics and information technology 12,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "the field of data privacy, these two goals (colloquially known as minimizing risk and maximizing utility, see [10]) form the basis for measuring the effectiveness of any protection method.", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "These models have been shown to be successful in the task of author attribution in document collections such as the Federalists papers [21], online blogs [22], and even executable binaries [7].", "startOffset": 154, "endOffset": 158}, {"referenceID": 5, "context": "These models have been shown to be successful in the task of author attribution in document collections such as the Federalists papers [21], online blogs [22], and even executable binaries [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 0, "context": "Recent work has also shown similar promise in short texts such as tweets (which exhibit high levels of linkability), despite the small character limit [1].", "startOffset": 151, "endOffset": 154}, {"referenceID": 30, "context": "As some others have pointed out [35], Twitter states in their developer guidelines that users have the right to be forgotten [33], such that if tweets are deleted, researchers will not use or release those tweets further.", "startOffset": 32, "endOffset": 36}, {"referenceID": 30, "context": "In addition to this, a number of papers are calling for stronger ethical standards when publishing social media data [35, 36, 28].", "startOffset": 117, "endOffset": 129}, {"referenceID": 31, "context": "In addition to this, a number of papers are calling for stronger ethical standards when publishing social media data [35, 36, 28].", "startOffset": 117, "endOffset": 129}, {"referenceID": 24, "context": "In addition to this, a number of papers are calling for stronger ethical standards when publishing social media data [35, 36, 28].", "startOffset": 117, "endOffset": 129}, {"referenceID": 3, "context": "g identifying hate speech [5].", "startOffset": 26, "endOffset": 29}, {"referenceID": 11, "context": "These include semi-automatic methods that can obscure individual authors [14] or apar X iv :1 60 6.", "startOffset": 73, "endOffset": 77}, {"referenceID": 23, "context": "proaches such as iterative translation [27, 19], which have been shown to offer little protection in some cases [6].", "startOffset": 39, "endOffset": 47}, {"referenceID": 16, "context": "proaches such as iterative translation [27, 19], which have been shown to offer little protection in some cases [6].", "startOffset": 39, "endOffset": 47}, {"referenceID": 4, "context": "proaches such as iterative translation [27, 19], which have been shown to offer little protection in some cases [6].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29].", "startOffset": 198, "endOffset": 217}, {"referenceID": 17, "context": "We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29].", "startOffset": 198, "endOffset": 217}, {"referenceID": 28, "context": "We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29].", "startOffset": 198, "endOffset": 217}, {"referenceID": 21, "context": "We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29].", "startOffset": 198, "endOffset": 217}, {"referenceID": 25, "context": "We address this issue by taking a neuralbased approach, inspired by the promising results reported so far in training neural architectures as simple generative models of natural language processing [4, 20, 32, 25, 29].", "startOffset": 198, "endOffset": 217}, {"referenceID": 18, "context": "We emulate the stylometric attack by using an approach akin to that of [22] and [1].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "We emulate the stylometric attack by using an approach akin to that of [22] and [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 0, "context": "We use the uni-grams and bi-grams as simple common sense feature sets, and we used the feature sets from [1] and [22] because they were both shown to perform quite well for re-identification.", "startOffset": 105, "endOffset": 108}, {"referenceID": 18, "context": "We use the uni-grams and bi-grams as simple common sense feature sets, and we used the feature sets from [1] and [22] because they were both shown to perform quite well for re-identification.", "startOffset": 113, "endOffset": 117}, {"referenceID": 18, "context": "These models were used in [22, 1].", "startOffset": 26, "endOffset": 33}, {"referenceID": 0, "context": "These models were used in [22, 1].", "startOffset": 26, "endOffset": 33}, {"referenceID": 18, "context": "KNN, we use row and feature normalization following in line with [22].", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "As suggested by [22] we collapse the feature vectors for each user in R to the mean for testing.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "For the second, model-specific measure, we analyze how well the different protection methods preserve sentiment by user, using the \u201cVader\u201d sentiment model, specifically developed to measure sentiment in social media data [13].", "startOffset": 221, "endOffset": 225}, {"referenceID": 8, "context": "Motivated by recent successes in function approximation through the use of parametrized neural architectures [11, 9, 30], we propose an architecture for the task of text synthesis.", "startOffset": 109, "endOffset": 120}, {"referenceID": 6, "context": "Motivated by recent successes in function approximation through the use of parametrized neural architectures [11, 9, 30], we propose an architecture for the task of text synthesis.", "startOffset": 109, "endOffset": 120}, {"referenceID": 26, "context": "Motivated by recent successes in function approximation through the use of parametrized neural architectures [11, 9, 30], we propose an architecture for the task of text synthesis.", "startOffset": 109, "endOffset": 120}, {"referenceID": 22, "context": "In contrast to building a separate model for each user in the data-set, our model shares its \u201clanguage model\u201d parameters across the multiple views of the data, similar in spirit to the hybrid architectures of [26, 23, 24] or the log-linear models of [17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 19, "context": "In contrast to building a separate model for each user in the data-set, our model shares its \u201clanguage model\u201d parameters across the multiple views of the data, similar in spirit to the hybrid architectures of [26, 23, 24] or the log-linear models of [17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 20, "context": "In contrast to building a separate model for each user in the data-set, our model shares its \u201clanguage model\u201d parameters across the multiple views of the data, similar in spirit to the hybrid architectures of [26, 23, 24] or the log-linear models of [17].", "startOffset": 209, "endOffset": 221}, {"referenceID": 14, "context": "In contrast to building a separate model for each user in the data-set, our model shares its \u201clanguage model\u201d parameters across the multiple views of the data, similar in spirit to the hybrid architectures of [26, 23, 24] or the log-linear models of [17].", "startOffset": 250, "endOffset": 254}, {"referenceID": 29, "context": "Interestingly enough, our specialized neural architecture also addresses a recent problem found in neural conversation agents [34].", "startOffset": 126, "endOffset": 130}, {"referenceID": 27, "context": "We note that other work has attempted to address the coherence/consistency problem in neural models [31, 18].", "startOffset": 100, "endOffset": 108}, {"referenceID": 15, "context": "We note that other work has attempted to address the coherence/consistency problem in neural models [31, 18].", "startOffset": 100, "endOffset": 108}, {"referenceID": 21, "context": "While we present the simplest formulation of our model above (the simple Elman-RNN), further improvement in bits-per-character (BPC) was found when experimenting with more complex gated models, unified under the Differential State Framework [25].", "startOffset": 241, "endOffset": 245}, {"referenceID": 21, "context": "Variations we experimented with included the concrete model proposed in [25], the Delta-RNN, and a Gated Recurrent Unit (GRU) [8].", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "To generate samples from the neural model, we simply make use of the model\u2019s efficient inference procedure, similar to that in [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 16, "context": "The iterative translation was based on [19] and worked by first translating the original (redacted) corpus into Arabic and then back into English using the Google Translate API1.", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "Arabic was chosen because it was found to offer the highest level of protection in [19].", "startOffset": 83, "endOffset": 87}, {"referenceID": 1, "context": "The dataset we experiment with consists of tweets collected by [3] for a study on German twitter users.", "startOffset": 63, "endOffset": 66}, {"referenceID": 12, "context": "Optimization of parameters was carried out through stochastic gradient descent using the ADAM [15] adaptive learning rate scheme (step size of \u03bb = 0.", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "comparable re-identification levels compared to previous work [22, 1], which supports our risk models.", "startOffset": 62, "endOffset": 69}, {"referenceID": 0, "context": "comparable re-identification levels compared to previous work [22, 1], which supports our risk models.", "startOffset": 62, "endOffset": 69}, {"referenceID": 13, "context": "should include reformulating our models such that neural variational inference [16] may be employed.", "startOffset": 79, "endOffset": 83}, {"referenceID": 25, "context": "Furthermore, operating under such a Bayesian framework would allow us to easily integrate better text-specific prior distributions, such as the piecewise-constant distribution [29], easing the learning of difficult, multi-modal distributions.", "startOffset": 176, "endOffset": 180}], "year": 2017, "abstractText": "We present a method for generating synthetic versions of Twitter data using neural generative models. The goal is to protect individuals in the source data from stylometric re-identification attacks while still releasing data that carries research value. To generate tweet corpora that maintain user-level word distributions, our proposed approach augments powerful neural language models with local parameters that weight user-specific inputs. We compare our work to two standard text data protection methods: redaction and iterative translation. We evaluate the three methods on risk and utility. We define risk following the stylometric models of re-identification, and we define utility based on two general language measures and two common text analysis tasks. We find that neural models are able to significantly lower risk over previous methods at the cost of some utility. More importantly, we show that the risk utility trade-off depends on how the neural model\u2019s logits (or the unscaled preactivation values of the output layer) are scaled. This work presents promising results for a new tool addressing the problem of privacy for free text and sharing social media data in a way that respects privacy and is ethically responsible.", "creator": "LaTeX with hyperref package"}}}