{"id": "1606.02228", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2016", "title": "Systematic evaluation of CNN advances on the ImageNet", "abstract": "nowadays the paper group systematically studies the unique impact functions of finding a range suite of specific recent quantitative advances advanced in pure cnn oriented architectures and relational learning methods on reaching the sequential object categorization ( ilsvrc ) specification problem. the evalution tests the unitary influence characteristic of the following important choices mechanisms of the architecture : simple non - unitary linearity ( local relu, elu, parameter maxout, compatibility with new batch normalization ), pooling response variants ( stochastic, max, net average, mixed ), network width, statistical classifier design ( convolutional, random fully - block connected, fuzzy spp ), image of pre - task processing, and of learning data parameters : learning growth rate, batch flow size, cleanliness assurance of sequencing the compiled data, etc.", "histories": [["v1", "Tue, 7 Jun 2016 17:38:06 GMT  (2747kb,D)", "http://arxiv.org/abs/1606.02228v1", "Submitted to CVIU Special Issue on Deep Learning"], ["v2", "Mon, 13 Jun 2016 13:48:39 GMT  (2751kb,D)", "http://arxiv.org/abs/1606.02228v2", "Submitted to CVIU Special Issue on Deep Learning. Updated dataset quality experiment"]], "COMMENTS": "Submitted to CVIU Special Issue on Deep Learning", "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["dmytro mishkin", "nikolay sergievskiy", "jiri matas"], "accepted": false, "id": "1606.02228"}, "pdf": {"name": "1606.02228.pdf", "metadata": {"source": "CRF", "title": "Systematic evaluation of CNN advances on the ImageNet", "authors": ["Dmytro Mishkin", "Nikolay Sergievskiy", "Jiri Matas"], "emails": [], "sections": [{"heading": null, "text": "The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatability with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc.\nThe performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the \u201ddeficit\u201d is small suggesting independence of their benefits.\nWe show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.\nKeywords: CNN, benchmark, non-linearity, pooling, ImageNet"}, {"heading": "1. Introduction", "text": "Deep convolution networks have become the mainstream method for solving various computer vision tasks, such as image classification [1], object detection [1, 2], semantic segmentation [3], image retrieval [4], tracking [5], text detection [6], stereo matching [7], and many other.\nBesides two classic works on training neural networks \u2013 [8] and [9], which are still highly relevant, there is very little guidance or theory on the plethora of design choices and hyper-parameter settings of CNNs with the consequent that researchers proceed by trial-and-error experimentation and architecture copying, sticking to established net types. With good results in ImageNet competition,\nPreprint submitted to Computer Vision and Image Understanding June 8, 2016\nar X\niv :1\n60 6.\n02 22\n8v 1\n[ cs\n.N E\n] 7\nJ un\nthe AlexNet [10], VGGNet [11] and GoogLeNet(Inception) [12] have become the de-facto standard.\nImprovements of many components of the CNN architecture like the nonlinearity type, pooling, structure and learning have been recently proposed. First applied in the ILSVRC [1] competition, they have been adopted in different research areas.\nThe contributions of the recent CNN improvements and their interaction have not been systematically evaluated. We survey the recent developments and perform a large scale experimental study that considers the choice of nonlinearity, pooling, learning rate policy, classifier design, network width, batch normalization [13]. We did not include ResNets [14] \u2013 a recent development achieving excellent results \u2013 since they have been well covered in papers [15, 16, 17, 18].\nThere are three main contributions of the paper. First, we survey and present baseline results for a wide variety of architectures and design choices both alone and in combination. Based on large-scale evaluation, we provide novel recommendations and insights about construction deep convolutional network. Second, we present ImageNet-128px as fast (24 hours of training AlexNet on GTX980) and reliable benchmark \u2013 the relative order of results for popular architectures does not change compared to common image size 224x224 or even 300x300 pixels. Last, but not least, the benchmark is fully reproducible and all scripts and data are available online1.\nThe paper is structured as follows. In Section 2.1 we explain and validate experiment design. In Section 3, the influence of the a range of hyper-parameters is evaluated in isolation. The related literature is review the corresponding in experiment sections. Section 4 is devoted to the combination of best hyperparameter setting and to \u201csqueezing-the-last-percentage-points\u201d for a given architecture recommendation. The paper is concluded in Section 5."}, {"heading": "2. Evaluation", "text": "Standard CaffeNet parameters and architecture are shown in Table 2. The full list of tested attributes is given in Table 1."}, {"heading": "2.1. Evaluation framework", "text": "All tested networks were trained on the 1000 object category classification problem on the ImageNet dataset [1]. The set consists of a 1.2M image training set, a 50K image validation set and a 100K image test set. The test set is not used in the experiments. The commonly used pre-processing includes image rescaling to 256xN, where N \u2265 256, and then cropping a random 224x224 square [10, 19]. The setup achieves good results in classification, but training a network of this size takes several days even on modern GPUs. We thus\n1https://github.com/ducha-aiki/caffenet-benchmark\npropose to limit the image size to 144xN where N \u2265 128 (denoted as ImageNet128px). For example, the CaffeNet [20] is trained within 24 hours using NVIDIA GTX980 on ImageNet-128px."}, {"heading": "2.1.1. Architectures", "text": "The input size reduction is validated by training CaffeNet, GoogLeNet and VGGNet on both the reduced and standard image sizes. The results are shown in Figure 1. The reduction of the input image size leads to a consistent drop in top-1 accuracy around 6% for all there popular architectures and does not change their relative order (VGGNet > GoogLeNet > CaffeNet) or accuracy difference.\nIn order to decrease the probability of overfitting and to make experiments less demanding in memory, another change of CaffeNet is made. A number of filters in fully-connected layers 6 and 7 were reduced by a factor of two, from 4096 to 2048. The results validating the resolution reduction are presented in Figure 1.\nThe parameters and architecture of the standard CaffeNet are shown in Table 2. For experiments we used CaffeNet with 2x thinner fully-connected layers, named as CaffeNet128-FC2048. The architecture can be denoted as 96C11/4\u2192MP3/2\u2192 192G2C5/2\u2192MP3/2\u2192 384G2C3\u2192 384C3\u2192 256G2C3 \u2192 MP3/2 \u2192 2048C3 \u2192 2048C1 \u2192 1000C1. Here we used fully-convolutional\nnotation for fully-connected layers, which are equivalent when image input size is fixed to 128x128 px. The default activation function is ReLU and it is put after every convolution layer, except the last 1000-way softmax classifier."}, {"heading": "2.1.2. Learning", "text": "SGD with momentum 0.9 is used for learning, the initial learning rate is set to 0.01, decreased by a factor of ten after each 100K iterations until learning stops after 320K iterations. The L2 weight decay for convolutional weights is set to 0.0005 and it is not applied to bias. The dropout [21] with probability 0.5 is used before the two last layers. All the networks were initialized with LSUV [22]. Biases are initialized to zero. Since the LSUV initialization works under assumption of preserving unit variance of the input, pixel intensities were scaled by 0.04, after subtracting the mean of BGR pixel values (104 117 124)."}, {"heading": "3. Single experiments", "text": "This section is devoted to the experiments with a single hyper-parameter or design choice per experiment."}, {"heading": "3.1. Activation functions", "text": ""}, {"heading": "3.1.1. Previous work", "text": "The activation functions for neural networks are a hot topic, many functions have been proposed since the ReLU discovery [23]. The first group are related to ReLU, i.e. LeakyReLU [24] and Very Leaky ReLU [25], RReLU [26],PReLU [27] and its generalized version \u2013 APL [28], ELU [29]. Others are based on different ideas, e.g. maxout [30], MBA [31], etc. However, to our best knowledge only a small fraction of this activation functions have been evaluated on ImageNetscale dataset. And when they have, e.g. ELU, the network architecture used in the evaluation was designed specifically for the experiment and is not commonly used."}, {"heading": "3.1.2. Experiment", "text": "We have tested the most popular activation functions and all those with available or trivial implementations: ReLU, tanh, sigmoid, VLReLU, RReLU, PReLU, ELU, linear, maxout, APL, SoftPlus. Formulas and references are given in Table 3. We have selected APL and maxout with two linear pieces. Maxout is tested in two modifications: MaxW \u2013 having the same effective network width, which doubles the number of parameters and computation costs because of the two linear pieces, and MaxS \u2013 having same computational complexity - with\u221a\n2 thinner each piece. Besides this, we have tested \u201doptimally scaled\u201d tanh, proposed by LeCun [8]. We have also tried to train sigmoid [32] network, but the initial loss never decreased. Finally, as proposed by Swietojanski et.al [33], we have tested combination of ReLU for first layers and maxout for the last layers of the network.\nResults are shown in Figure 2. The best single performing activation function similar in complexity to ReLU is ELU. The parametric PReLU performed on par. The performance of the centered softplus is the same as for ELU. Surprisingly, Very Leaky ReLU, popular for DCGAN networks [34] and for small datasets, does not outperforms vanilla ReLU. Interesting, the network with no\nnon-linearity has respectable performance \u2013 38.9% top-1 accuracy on ImageNet, not much worse than tanh-network.\nThe Swietojanski et.al [33] hypothesis about maxout power in the final layers is confirmed and combined ELU (after convolutional layers) + maxout (after fully connected layers) shows the best performance among non-linearities with speed close to ReLU. Wide maxout outperforms the rest of the competitors at a higher computational cost."}, {"heading": "3.2. Pooling", "text": ""}, {"heading": "3.2.1. Previous work", "text": "Pooling, combined with striding, is a common way to archive a degree of invariance together with a reduction of spatial size of feature maps. The most popular options are max pooling and average pooling. Among the recent advances are: Stochastic pooling [35], LP-Norm pooling [36] and Tree-Gated pooling [37]. Only the authors of the last paper have tested their pooling on ImageNet.\nThe pooling receptive field is another design choice. Krizhevskiy etal. [10] claimed superiority of overlapping pooling with 3x3 window size and stride 2, while VGGNet [11] uses a non-overlapping 2x2 window."}, {"heading": "3.2.2. Experiment", "text": "We have tested (see Table 4) average, max, stochastic and proposed by Lee et al [37] sum of average and max pooling, and skipping pooling at all, replacing it with strided convolutions proposed by Springenberd et al. [38]. We have also tried Tree and Gated poolings [37], but we encountered convergence problems and the results were strongly depend on the input image size. We do not know if it is a problem of the implementation, or the method itself and therefore omitted the results.\nThe results are shown in Figure 3, left. Stochastic pooling had very bad results. In order to check if it was due to extreme randomization by the stochastic\npooling and dropout, we trained network without the dropout. This decreased accuracy even more. The best results were obtained by a combination of max and average pooling. Our guess is that max pooling brings selectivity and invariance, while average pooling allows using gradients of all filters, instead of throwing away 3/4 of information as done by non-overlapping 2x2 max pooling.\nThe second experiment is about the receptive field size. The results are shown in Figure 3, right. Overlapping pooling is inferior to a non-overlapping 2x2 window, but wins if zero-padding is done. This can be explained by the fact that better results are obtained for larger outputs; 3x3/2 pooling leads to 3x3 spatial size of pool5 feature map, 2x2/2 leads to 4x4 pool5, while 3x3/2 + 1 \u2013 to 5x5. This observation means there is a speed \u2013 performance trade-off."}, {"heading": "3.3. Learning rate policy", "text": "Learning rate is one of the most important hyper-parameters which influences the final CNN performance. Surprisingly, the most commonly used learning rate decay policy is \u201dreduce learning rate 10x, when validation error stops decreasing\u201d adopted with no parameter search. While this works well in practice, such lazy policy can be sub-optimal. We have tested four learning rate policies: step, quadratic and square root decay (used for training GoogLeNet by BVLC [20]), and linear decay. The actual learning rate dynamics are shown in Figure 4, left. The validation accuracy is shown in the right. Linear decay gives the best results."}, {"heading": "3.4. Image pre-processing", "text": ""}, {"heading": "3.4.1. Previous work", "text": "The commonly used input to CNN is raw RGB pixels and the commonly adopted recommendation is not to use any pre-processing. There has not been much research on the optimal colorspace or pre-processing techniques for CNN. Rachmadi and Purnama [39] explored different colorspaces for vehicle color identification, Dong et.al [40] compared YCrCb and RGB channels for image superresolution, Graham [41] extracted local average color from retina images in winning solution to the Kaggle competition."}, {"heading": "3.4.2. Experiment", "text": "The pre-processing experiment is divided in two parts. First, we have tested popular handcrafted image pre-processing methods and colorspaces. Since all transformations were done on-the-fly, we first tested if calculation of the mean pixel and variance over the training set can be replaced with applying batch normalization to input images. It decreases final accuracy by 0.3% and can be seen as baseline for all other methods. We have tested HSV, YCrCb, Lab, RGB and single-channel grayscale colorspaces. Results are shown in Figure 5. The experiment confirms that RGB is the best suitable colorspace for CNNs. Labbased network has not improved the initial loss after 10K iterations. Removing color information from images costs from 5.8% to 5.2% of the accuracy, for\nOpenCV RGB2Gray and learned decolorization resp. Global [42] and local (CLAHE [43]) histogram equalizations hurt performance as well.\nSecond, we let the network to learn a transformation via 1x1 convolution, so no pixel neighbors are involved. The mini-networks architectures are described in Table 6. The learning process is joint with the main network and can be seen as extending the CaffeNet architecture with several 1x1 convolutions at the input. The best performing network gave 1.4% absolute accuracy gain without a significant computational cost."}, {"heading": "3.5. Batch normalization", "text": "Batch normalization [13] (BN) is a recent method tha t solves the gradient exploding/vanishing problem and guarantees near-optimal learning regime for the layer following the batch normalized one. Following [22], we first tested different options where to put BN \u2013 before or after the non-linearity. Results presented in Table 7 are surprisingly contradictory: CaffeNet architecture prefers ConvReLU-BN-Conv, while GoogLeNet \u2013 Conv-BN-ReLU-Conv placement. Moreover, results for GoogLeNet are inferior to the plain network. The difference to [13] is that we have not changed any other parameters except using BN, while in the original paper, authors decreased regularization (both weight decay and dropout), changed the learning rate decay policy and applied an additional training set re-shuffling. Also, GoogLeNet behavior seems different to CaffeNet and VGGNet w.r.t. to other modification, see Section 4.\nFor the next experiment with BN and activations, we selected placement after non-linearity. Results are shown in Figure 6. Batch normalization washes out differences between ReLU-family variants, so there is no need to use the\nmore complex variants. Sigmoid with BN outperforms ReLU without it, but, surprisingly, tanh with BN shows worse accuracy than sigmoid with BN."}, {"heading": "3.6. Classifier design", "text": ""}, {"heading": "3.6.1. Previous work", "text": "The CNN architecture can be seen as integration of feature detector and which is following by a classifier. Ren et. al. [44] proposed to consider convolutional layers of the AlexNet as an feature extractor and fully-connected layers as 2-layer MLP as a classifier. They argued that 2 fully-connected layers are not the optimal design and explored various architectures instead. But they considered only pre-trained CNN or HOGs as feature extractor, so explored mostly transfer learning scenario, when the most of the network weights are frozen. Also, they explored architectures with additional convolution layers, which can be seen not as better classifier, but as an enhancement of the feature extractor.\nThere is three the most popular approaches to classifier design. First \u2013 final layer of the feature extractor is max pooling layer and the classifier is a one or two layer MLP, as it is done in LeNet [45], AlexNet [10] and VGGNet [11]. Second \u2013 spatial pooling pyramid layer [46] instead of pooling layer, followed by two layer MLP. And the third architecture consist of average pooling layer, squashing spatial dimensions, followed by softmax classifier without any feature transform. This variant is used in GoogLeNet [12] and ResNet [14]."}, {"heading": "3.6.2. Experiment", "text": "We have explored following variants: default 2-layer MLP, SPPNet with 2 and 3 pyramid levels, removing pool5-layer, treating fully-connected layers as convolutional, which allows to use zero-padding, therefore increase effective number of training examples for this layer, averaging features before softmax layer or averaging spatial predictions of the softmax layer [47]. The results are shown in the Figure 7. The best results are get, when predictions are averaged over all spatial positions and MLP layers are treated as convolution - with zero padding. The advantage of the SPP over standard max pooling is less pronounced."}, {"heading": "3.7. Batch size and learning rate", "text": "The mini-batch size is always a trade-off between computation efficiency \u2013 because GPU architecture prefers it large enough \u2013 and accuracy; early work by Wilson and Martinez [48] shows superiority of the online training to batchtraining. Here we explore the influence of mini-batch size on the final accuracy. Experiments show that keeping a constant learning rate for different mini-batch sizes has a negative impact on performance. We also have tested the heuristic proposed by Krizhevskiy [49] which suggests to keep the product of mini-batch size and learning rate constant. Results are shown in Figure8. The heuristics works, but large (512 and more) mini-batch sizes leads to quite significant decrease in performance. On the other extreme, online training (mini-batch with single example) does not bring accuracy gains over 64 or 256, but significantly slows down the training wall-clock time."}, {"heading": "3.8. Network width", "text": "All the advances in ImageNet competition so far were caused by architectural improvement. To the best of our knowledge, there is no study about network width \u2013 final accuracy dependence. Canziani et.al [50] did a comparative analysis of the ImageNet winner in terms of accuracy, number of parameters and computational complexity, but it is a comparison of the different architectures. In this subsection we evaluate how far one can get by increasing CaffeNet width, with no other changes. The results are shown in Figure 9. The original architecture is close to optimal in accuracy per FLOPS sense: a decrease in the number of filters leads to a quick and significant accuracy drop, while making the network thicker brings gains, but it saturates quickly. Making the network thicker more than 3 times leads to a very limited accuracy gain."}, {"heading": "3.9. Input image size", "text": "The input image size, as it brings additional information and training samples for convolution filters, plays a very important role. Our initial experiment, showed in Figure 1 indicates that CaffeNet, trained on 227x227 images can compete with much more complex GoogLeNet architecture, trained on smaller\nimages. So the obvious question is what is the dependence between image size and final accuracy.\nWe have performed an experiment with different input image sizes: 96, 128, 180 and 224 pixels wide. The results are presented in Figure 10. The bad news are that while accuracy depends on image size linearly, the needed computations grow quadratically, so it is a very expensive way to a performance gain. In the second part of experiment, we kept the spatial output size of the pool1 layer fixed while changing the input image size. To archieve this, we respectively change the stride and filter size of the conv1 layer. Results show that the gain from a large image size mostly (after some minimum value) comes from the larger spatial size of deeper layers than from the unseen image details."}, {"heading": "3.10. Dataset size and noisy labels", "text": ""}, {"heading": "3.10.1. Previous work", "text": "The performance of the current deep neural network is highly dependent on the dataset size. Unfortunately, not much research has been published on this topic. In DeepFace [51], the authors shows that dataset reduction from 4.4M to 1.5M leads to a 1.74% accuracy drop. Similar dependence is shown by Schroff et.al [52] but on an extra-large dataset: decreasing the dataset size from 260M\nto 2.6M leads to accuracy drop in 10%. But these datasets are private and the experiments are not reproducible. Another important property of a dataset is the cleanliness of the data. For example, an estimate of human accuracy on ImageNet is 5.1% for top-5 [1]. To create the ImageNet, each image was voted on by ten different people [1]."}, {"heading": "3.10.2. Experiment", "text": "We explore the dependency between the accuracy and the dataset size/cleanliness on ImageNet. For the dataset size experiment, 200, 400, 600, 800 thousand examples were random chosen from a full training set. For each reduced dataset, a CaffeNet is trained from scratch. For the cleanliness test, we replaced the labes to a random incorrect one for 5%, 10%, 15% and 32% of the examples. The labels are fixed, unlike the recent work on disturbing labels as a regularization method [53].\nThe results are shown in Figure 11 which clearly shows that bigger (and more diverse) dataset brings an improvement. There is a minimum size below which performance quickly degrades. Less clean data outperforms more noisy ones: a clean dataset with 400K images performs on par with 1.2M dataset with 800K correct images."}, {"heading": "3.11. Bias in convolution layers", "text": "We conducted a simple experiment on the importance of the bias in the convolution and fully-connected layers. First, the network is trained as usual, for the second \u2013 biases are initialized with zeros and the bias learning rate is set to zero. The network without biases shows 2.6% less accuracy than the default \u2013 see Table 8."}, {"heading": "4. Best-of-all experiments", "text": "Finally, we test how all the improvements, which do not increase the computational cost, perform together. We combine: the learned colorspace transform F, ELU as non-linearity for convolution layers and maxout for fully-connected layers, linear learning rate decay policy, average plus max pooling. The improvements are applied to CaffeNet128, CaffeNet224, VGGNet128 and GoogleNet128.\nThe first three demonstrated consistent performance growth (see Figure 12), while GoogLeNet performance degraded, as it was found for batch normalization. Possibly, this is due to the complex and optimized structure of the\nGoogLeNet network. Unfortunately, the cost of training VGGNet224 is prohibitive, one month of GPU time, so we have not subjected it to the tests yet."}, {"heading": "5. Conclusions", "text": "We have compared systematically a set of recent CNN advances on large scale dataset. We have shown that benchmarking can be done at an affordable time and computation cost. A summary of recommendations:\n\u2022 use ELU non-linearity without batchnorm or ReLU with it. \u2022 apply a learned colorspace transformation of RGB. \u2022 use the linear learning rate decay policy. \u2022 use a sum of the average and max pooling layers. \u2022 use mini-batch size around 128 or 256. If this is too big for your GPU,\ndecrease the learning rate proportionally to the batch size. \u2022 use fully-connected layers as convolutional and average the predictions for\nthe final decision. \u2022 when investing in increasing training set size, check if a plateau has not\nbeen reach. \u2022 cleanliness of the data is more important then the size. \u2022 if you cannot increase the input image size, reduce the stride in the con-\nsequent layers, it has roughly the same effect. \u2022 if your network has a complex and highly optimized architecture, like e.g.\nGoogLeNet, be careful with modifications."}, {"heading": "Acknowledgements", "text": "The authors were supported by The Czech Science Foundation Project GACR P103/12/G084 and CTU student grant SGS15/155/OHK3/2T/13.\nTable 9: Results of all tests on ImageNet-128px\nGroup Name Names acc [%] Baseline 47.1 Non-linearity Linear 38.9\ntanh 40.1 VReLU 46.9 APL2 47.1 ReLU 47.1 RReLU 47.8 maxout (MaxS) 48.2 PReLU 48.5 ELU 48.8 maxout (MaxW) 51.7\nBatch Normalization (BN) before non-linearity 47.4 after non-linearity 49.9 BN + Non-linearity Linear 38.4 tanh 44.8 sigmoid 47.5 maxout (MaxS) 48.7 ELU 49.8 ReLU 49.9 RReLU 50.0 PReLU 50.3 Pooling stochastic, no dropout 42.9 average 43.5 stochastic 43.8 Max 47.1 strided convolution 47.2 max+average 48.3 Pooling window size 3x3/2 47.1 2x2/2 48.4 3x3/2 pad=1 48.8 Learning rate decay policy step 47.1 square 48.3 square root 48.3 linear 49.3 Colorspace & Pre-processing OpenCV grayscale 41.3 grayscale learned 41.9 histogram equalized 44.8 HSV 45.1 YCrCb 45.8 CLAHE 46.7 RGB 47.1 Classifier design pooling-FC-FC-clf 47.1 SPP2-FC-FC-clf 47.1 pooling-C3-C1-clf-maxpool 47.3 SPP3-FC-FC-clf 48.3 pooling-C3-C1-avepool-clf 48.9 C3-C1-clf-avepool 49.1 pooling-C3-C1-clf-avepool 49.5 Percentage of noisy data 5% 45.8 10% 44.7 15% 43.7 32% 40.1 Dataset size 1200K 47.1 800K 43.8 600K 42.5 400K 39.3 200K 30.5 Network width 4 \u221a\n2 56.5 4 56.3 2 \u221a\n2 55.2 2 53.3\u221a\n2 50.6 1 47.1 1/ \u221a\n2 46.0 1/2 41.6 1/2 \u221a\n2 31.8 1/4 25.6\nBatch size BS=1024, lr=0.04 46.5 BS=1024, lr=0.01 41.9 BS=512, lr=0.02 46.9 BS=512, lr=0.01 45.5 BS=256, lr=0.01 47.0 BS=128, lr=0.005 47.0 BS=128, lr=0.01 47.2 BS=64, lr=0.0025 47.5 BS=64, lr=0.01 47.1 BS=32, lr=0.00125 47.0 BS=32, lr=0.01 46.3 BS=1, lr=0.000039 47.4 Bias without 44.5 with 47.1 Architectures CaffeNet128 47.1 CaffeNet128All 53.0 CaffeNet224 56.5 CaffeNet224All 61.3 VGGNet16-128 65.1 VGGNet16-128All 68.2 GoogLeNet128 61.9 GoogLeNet128All 60.6 GoogLeNet224 67.9 17"}, {"heading": "6. References", "text": ""}], "references": [{"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision (IJCV) 88 (2) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Effiicient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "in: Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, Springer-Verlag, London, UK, UK", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Neural Networks: Tricks of the Trade: Second Edition", "author": ["Y. Bengio"], "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: F. Pereira, C. Burges, L. Bottou, K. Weinberger (Eds.), Advances in Neural Information Processing Systems 25, Curran Associates, Inc.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for largescale visual recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "in: Proceedings of ICLR", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "in: CVPR 2015", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "in: D. Blei, F. Bach (Eds.), Proceedings of the 32nd International Conference on Machine Learning (ICML- 15), JMLR Workshop and Conference Proceedings", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "All you need is a good init", "author": ["D. Mishkin", "J. Matas"], "venue": "in: Proceedings of ICLR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "in: G. J. Gordon, D. B. Dunson (Eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11), Vol. 15, Journal of Machine Learning Research - Workshop and Conference Proceedings", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Train you very own deep convolutional network", "author": ["B. Graham"], "venue": "train-you-very-own-deep-convolutional-network", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "in: Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "vol. 1, MIT Press, Cambridge, MA, USA", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1986}, {"title": "Investigation of maxout networks for speech recognition", "author": ["P. Swietojanski", "J. Li", "J.-T. Huang"], "venue": "in: Proceedings of ICASSP", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "in: Proceedings of ICLR Workshop", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Kaggle diabetic retinopathy detection competition report", "author": ["B. Graham"], "venue": "Tech. rep. ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Image enhancement by histogram transformation", "author": ["R. Hummel"], "venue": "Computer Graphics and Image Processing 6 (2) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1016}, {"title": "Graphics gems iv", "author": ["K. Zuiderveld"], "venue": "Academic Press Professional, Inc., San Diego, CA, USA", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1994}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "in: Proceedings of the IEEE, Vol. 86", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1998}, {"title": "The general inefficiency of batch training for gradient descent learning", "author": ["D.R. Wilson", "T.R. Martinez"], "venue": "Neural Netw. 16 (10) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1016}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["F. Schroff", "D. Kalenichenko", "J. Philbin"], "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep convolution networks have become the mainstream method for solving various computer vision tasks, such as image classification [1], object detection [1, 2], semantic segmentation [3], image retrieval [4], tracking [5], text detection [6], stereo matching [7], and many other.", "startOffset": 132, "endOffset": 135}, {"referenceID": 0, "context": "Deep convolution networks have become the mainstream method for solving various computer vision tasks, such as image classification [1], object detection [1, 2], semantic segmentation [3], image retrieval [4], tracking [5], text detection [6], stereo matching [7], and many other.", "startOffset": 154, "endOffset": 160}, {"referenceID": 1, "context": "Deep convolution networks have become the mainstream method for solving various computer vision tasks, such as image classification [1], object detection [1, 2], semantic segmentation [3], image retrieval [4], tracking [5], text detection [6], stereo matching [7], and many other.", "startOffset": 154, "endOffset": 160}, {"referenceID": 2, "context": "Besides two classic works on training neural networks \u2013 [8] and [9], which are still highly relevant, there is very little guidance or theory on the plethora of design choices and hyper-parameter settings of CNNs with the consequent that researchers proceed by trial-and-error experimentation and architecture copying, sticking to established net types.", "startOffset": 56, "endOffset": 59}, {"referenceID": 3, "context": "Besides two classic works on training neural networks \u2013 [8] and [9], which are still highly relevant, there is very little guidance or theory on the plethora of design choices and hyper-parameter settings of CNNs with the consequent that researchers proceed by trial-and-error experimentation and architecture copying, sticking to established net types.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "the AlexNet [10], VGGNet [11] and GoogLeNet(Inception) [12] have become the de-facto standard.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "the AlexNet [10], VGGNet [11] and GoogLeNet(Inception) [12] have become the de-facto standard.", "startOffset": 25, "endOffset": 29}, {"referenceID": 6, "context": "the AlexNet [10], VGGNet [11] and GoogLeNet(Inception) [12] have become the de-facto standard.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "First applied in the ILSVRC [1] competition, they have been adopted in different research areas.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "We survey the recent developments and perform a large scale experimental study that considers the choice of nonlinearity, pooling, learning rate policy, classifier design, network width, batch normalization [13].", "startOffset": 207, "endOffset": 211}, {"referenceID": 0, "context": "Evaluation framework All tested networks were trained on the 1000 object category classification problem on the ImageNet dataset [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "The commonly used pre-processing includes image rescaling to 256xN, where N \u2265 256, and then cropping a random 224x224 square [10, 19].", "startOffset": 125, "endOffset": 133}, {"referenceID": 8, "context": "The dropout [21] with probability 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 9, "context": "All the networks were initialized with LSUV [22].", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Previous work The activation functions for neural networks are a hot topic, many functions have been proposed since the ReLU discovery [23].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "LeakyReLU [24] and Very Leaky ReLU [25], RReLU [26],PReLU [27] and its generalized version \u2013 APL [28], ELU [29].", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "maxout [30], MBA [31], etc.", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "Besides this, we have tested \u201doptimally scaled\u201d tanh, proposed by LeCun [8].", "startOffset": 72, "endOffset": 75}, {"referenceID": 13, "context": "We have also tried to train sigmoid [32] network, but the initial loss never decreased.", "startOffset": 36, "endOffset": 40}, {"referenceID": 14, "context": "al [33], we have tested combination of ReLU for first layers and maxout for the last layers of the network.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "al [33] hypothesis about maxout power in the final layers is confirmed and combined ELU (after convolutional layers) + maxout (after fully connected layers) shows the best performance among non-linearities with speed close to ReLU.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "[10] claimed superiority of overlapping pooling with 3x3 window size and stride 2, while VGGNet [11] uses a non-overlapping 2x2 window.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[10] claimed superiority of overlapping pooling with 3x3 window size and stride 2, while VGGNet [11] uses a non-overlapping 2x2 window.", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "[38].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "al [40] compared YCrCb and RGB channels for image superresolution, Graham [41] extracted local average color from retina images in winning solution to the Kaggle competition.", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "Global [42] and local (CLAHE [43]) histogram equalizations hurt performance as well.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "Global [42] and local (CLAHE [43]) histogram equalizations hurt performance as well.", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "Batch normalization Batch normalization [13] (BN) is a recent method tha t solves the gradient exploding/vanishing problem and guarantees near-optimal learning regime for the layer following the batch normalized one.", "startOffset": 40, "endOffset": 44}, {"referenceID": 9, "context": "Following [22], we first tested different options where to put BN \u2013 before or after the non-linearity.", "startOffset": 10, "endOffset": 14}, {"referenceID": 7, "context": "The difference to [13] is that we have not changed any other parameters except using BN, while in the original paper, authors decreased regularization (both weight decay and dropout), changed the learning rate decay policy and applied an additional training set re-shuffling.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "First \u2013 final layer of the feature extractor is max pooling layer and the classifier is a one or two layer MLP, as it is done in LeNet [45], AlexNet [10] and VGGNet [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "First \u2013 final layer of the feature extractor is max pooling layer and the classifier is a one or two layer MLP, as it is done in LeNet [45], AlexNet [10] and VGGNet [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 5, "context": "First \u2013 final layer of the feature extractor is max pooling layer and the classifier is a one or two layer MLP, as it is done in LeNet [45], AlexNet [10] and VGGNet [11].", "startOffset": 165, "endOffset": 169}, {"referenceID": 6, "context": "This variant is used in GoogLeNet [12] and ResNet [14].", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "Batch size and learning rate The mini-batch size is always a trade-off between computation efficiency \u2013 because GPU architecture prefers it large enough \u2013 and accuracy; early work by Wilson and Martinez [48] shows superiority of the online training to batchtraining.", "startOffset": 203, "endOffset": 207}, {"referenceID": 21, "context": "In DeepFace [51], the authors shows that dataset reduction from 4.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "al [52] but on an extra-large dataset: decreasing the dataset size from 260M", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "1% for top-5 [1].", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "To create the ImageNet, each image was voted on by ten different people [1].", "startOffset": 72, "endOffset": 75}], "year": 2016, "abstractText": "The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatability with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the \u201ddeficit\u201d is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.", "creator": "LaTeX with hyperref package"}}}