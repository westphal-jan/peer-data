{"id": "1606.02617", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Efficient Estimation of k for the Nearest Neighbors Class of Methods", "abstract": "the k intrinsic nearest neighbors ( possibly knn ) estimation method has allegedly received much attention indoors in the past decades, recently where some theoretical security bounds on testing its performance were finally identified and analyzed where practical modeling optimizations were also proposed largely for making it work fairly consistently well in several high dimensional finite spaces and on increasingly large datasets. resultant from countless large experiments of the past research it became considered widely eventually accepted that predicting the value of uncertainty k has a first significant impact on the numerical performance requirements of programming this method. however, the widely efficient optimization of this robust parameter has not received way so much attention nationally in literature. today, the most common convenient approach : is to cross - validate or ultimately bootstrap instead this conditional value function for covering all neighboring values in question. this approach usually forces distances to be recomputed many other times, seldom even if efficient methods, are never used. hence, estimating merely the optimal properties k can not become expensive even on other modern systems. frequently, applying this circumstance seemingly leads inadvertently to a sparse online manual search of k. whereas in this paper we want systematically to consistently point - out that a systematic efficient and thorough estimation of estimation the parameter k can be performed efficiently. the discussed parameter approach relies on choosing large matrices, clearly but nowadays we want to argue, that in future practice a higher state space using complexity is often much even less of a problem than repetetive distance computations.", "histories": [["v1", "Wed, 8 Jun 2016 16:11:53 GMT  (248kb,D)", "http://arxiv.org/abs/1606.02617v1", "Technical Report, 16p, alternative source:this http URL"], ["v2", "Mon, 13 Jun 2016 11:34:59 GMT  (248kb,D)", "http://arxiv.org/abs/1606.02617v2", "Technical Report, 16p, alternative source:this http URL"]], "COMMENTS": "Technical Report, 16p, alternative source:this http URL", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aleksander lodwich", "faisal shafait", "thomas breuel"], "accepted": false, "id": "1606.02617"}, "pdf": {"name": "1606.02617.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Aleksander Lodwich", "Faisal Shafait", "Thomas Breuel"], "emails": [], "sections": [{"heading": null, "text": "Keywords: nearest neighbor, optimization, benchmarking"}, {"heading": "1 Introduction", "text": "Since the introduction of the k Nearest Neighbor (kNN) method by Fix and Hodges in 1951 [1] a lot of different variants of it have appeared in order to make it suitable to different scenarios. The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]). An overview over the state-of-the-art in nearest neighbor techniques is given in [11].\nThe continuing richness of investigation work into nearest neighbor can be explained with the omnipresence of CBR (Case Based Reasoning) type of problems [12] or just from the practical point of view of its massive parallelizability or simply populartiy. After all nearest neighbor is conceptually easy to understand - many students get to learn the nearest neighbor as the first classifier.\nar X\niv :1\n60 6.\n02 61\n7v 1\n[ cs\n.L G\n] 8\nJ un\n2 01\nAll of the beforehand mentioned advances evolve around the question of how to optimize the kNN\u2019s distance measure for retrieving. The method\u2019s apparent laziness might be the reason why a fast preoptimization of k has not been paid a lot attention to.\nThe proper choice of k is an important factor for achieving maximum performance of a kNN. However, as we will show, conventional k optimization via cross-validation or bootstrapping is slow and promises potential for being sped up. Therefore, we will devote this paper to the concept of fast k optimization.\nThere is work addressing this issue in an alternative way by introducing incremental kNN classifiers based on different types of trees. This class of nearest neighbors attempts to eliminate the influence of k by choosing the right k ad hoc. The rationale behind this method is that for classification tasks the exact majority of a specific class label is not necessarily interesting. It is only interesting when nearest neighbor is used as a density estimator. In other cases it is generally enough to feel safe about which class label rules the nearest set. This class of nearest neighbor starts by polling a minimal amount of nearest neighbors. Then it analyzes the labels and if the retrieved collection of labels is indecisive it will poll more nearest neighbors until it considers the collection decisive enough.\nNaturally, the method does not scale to small values of k, because e.g. a k = 1 will never be indecisive. This is a problem because we know from experiments that small k are often optimal. Incremental nearest neighbors have their strength in very large databases where typical queries do not need to compute all relative distances. During a lifetime of a large database some distances might not be computed at all. The lazy distance computation make incremental nearest neighbor methods ideal candidates for real time tasks operating on large volatile data. However, in a cross validation setup which needs to compute all distances incremental kNNs cannot play out their strengths. Because of the restrictions and intended use we exempted incremental methods from investigation in this paper.\nWe organize this paper in three parts. In the first part we will study the options to make the estimation of k as fast as possible, in the second part we will experimentally compare the result with the conventional approach and in the last part we will draw a conclusion."}, {"heading": "2 Stating the Problem", "text": "The kNN is commonly considered a classifier from the area of supervised learning theory. In this theory there exists a training function T that delivers a model m based on a set of options o, a matrix of example values V and a vector of labels l of respective size (m = T (o, V, l)). In turn, the model m is used in a classification function C that is presented with a matrix of new examples V \u2032 and its task is to deliver a vector of new labels l\u2032 (l\u2032 = C(m,V \u2032)). T and C must be related but there is no restriction on what the model can be. It can be a set of complex items, a matrix, a vector or just a single value, indeed.\nFrom the perspective of a human the purpose of a model is to make predictions about the future. In order to be able to do this the human brain requires a simplification of the world. Only with the simplification of the world to a reduced number of variables and rules it can compute future states faster than they occur. In case of kNN the model m consists of the data and of the smoothing parameter k. This means, that the function T is an identity function between model and parameters. This conflicts with the common notion of a model because the production of models commonly implies reduction. However, the collected data already are a reduction of the world! From a practical point of view they can be considered as representative model states of the world and the data in the model is used to predict the class variable of a new vector before it is actually recorded. This fits perfectly well with the original notion of models. However, the model is only fixed, when k is fixed.\nAccording to the framework, fixing the model (getting the right value for k) is the job of the function T . Many training techniques in machine learning use optimization strategies developed for real numbers and open parameter spaces. This is not suitable for k as it is an integer value and has known left and right limits: an ideal candidate for full search.\nMost frameworks for pattern recognition offer macro optimization for the remaing model parameters that express themself in the initial training options o. The structural compatibility of the kNN with the pattern recognition frameworks\u2019 macro optimization functions seduces the users very often to macro optimize k. The consequence of this is that kNN must compute distances repetetively as it cannot assume that specific vectors will simply exchange their role between training and testing in the future. In case of kNN this is exceptionally regrettable.\nWhat does macro optimization mean for the computational complexity? Here we assume - but without loose of generality - that the dataset V is of size n (nrows in matrix V ) and can be exactly divided into f equally sized partitions ready to be rearranged into f different train and test setups. Since everything is being recomputed the computational complexity for this kind of cross-validation\nof k for a brute kNN is O ( k\u0302 \u00b7 f\u22121f \u00b7 n 2 ) . k\u0302 is the size of the tested range of k\nand since we are considering full search we accept that k\u0302 depends on the size n of the dataset and the number of folds f . This means that the k\u0302 = n \u00b7 f\u22121f . The scan of nearest sets yields a partial complexity of O( f\u22121f \u00b7 n 2). Hence, for full\nsearch the total complexity is O (( f\u22121 f )2 \u00b7 n3 + f\u22121f \u00b7 n 2 ) .\nIn order to reduce this high complexity it is necessary to optimize kNN within the train function T as it has all necessary information about the relationships among the examples and the labels. This means that T (o = {k}, V, l) should become T (o = {i}, V, l) where i is a vector of partition indices of the kind (0, 0, 0, ..., 1, 1, 1, ..., f, f, f...)T .\nNow, the train function T can utilize the fact that no new data will arrive during the training and all possible distance requests can be computed in advance. Distances within the same partition need not be computed as they will\nbe never requested. These fields can be set to infinity (alternatively they can be filled up with the largest value found in the matrix + 1). The lower triangle is symmetrical to the upper triangle of the matrix because vectors between two points have the same norm. The distance matrix D has a structure as shown in figure 1. The size of D is n\u00d7n\u00d72. By D(column, row)1 we mean the component distance and by D(column, row)2 we mean the associated label.\nAlone this redesign causes the complexity of the distance computations to get reduced to O (\nf\u22121 f \u00b7 n2 2\n) . However this benefit is achieved at the expense of\nhigher memory use. The brute kNN has a space complexity of O(n), now the space complexity has risen to O(n2).\nThe next step is to sort the vectors horizontally according to their distance. Although collecting the k best solutions would be faster for a single run it means for a range of k that you effectively obtain the insertion sort. Since there exist faster sorting algorithms we choose to sort but by using a different algorithm.\nThe fastest algorithm for doing so is the quick sort. Its average complexity is O (n log(n)). In the worst case scenario the sorting complexity of this method is O ( n2 ) . In that case n rows will be sorted with O ( n2 ) . This means\nthat the worst time complexity so far is O (\nf\u22121 f \u00b7 n2 2 + n 3 ) and average case is\nO (\nf\u22121 f \u00b7 n2 2 + n 2 log(n)\n) .\nIn order to obtain nearest neighbors for each vector indexed by the row a counting matrix M := Nn\u00d7s is initialized with zeros. s is the number of symbols or classes. For each row in D and M and for the columns k = 1.. f\u22121f n in D the counters for the specific class label is increased. More precisely, for every row r = 1..n and for every tested k the counters M (D(k, r)2, r) are updated\nby 1. The complexity of this operation is O (\nf\u22121 f \u00b7 n\n2 )\n. For the overall method this adds up to O (\n3 2 f\u22121 f \u00b7 n\n2 )\n. In parallel the level of correct classification must\nbe computed because after every modification of M the state for the smaller k is lost. Therefore a matrix A := N f\u22121 f n\u00d7f for recording the number of correct classfications is required. How is this number computed? At every round k of the nearest neighbor candidate computations Mk contains in each of its rows a vector that tells how many labels of specific kind are in the nearest neighbors set. The classification label is l\u2032rk = argmax\ns Mkr. The complexity for this operation is O(n\n2s).\nThis simple method is ambiguous by nature, as there can be many labels that are represented by the same amount of vectors in a nearby set. Computer implementations prefer to return the symbol with the smalest coding. However, it is possible to have a shadow matrix S := Rn\u00d7s that is the sum of the distances observed for each class label in the set so far. The rule for computing S is the same as for M with the difference that instead of adding ones to the matrix you add distances. When symbol frequency is ambiguous (argmax returns more than one value) it is possible to use S to find which samples are closer overall. Because of the specific interest into fast k optimization the simple argmax processing is used.\nNow, every l\u2032rk is compared for equality with lr (ground truth) and the binary result is added to A(k, ir). The argmax\nk Af will return f best k. k\n\u2217 is obtained\nby averaging. Considering all parts of the algorithm together the overall complexity is O (\n3 2 f\u22121 f \u00b7 n\n2 + n2 log(n) + n2s ) \u2248 O ( n2 log(n) )"}, {"heading": "3 Experiments", "text": "The method for fast k computation (AutokNN) was tested against three other algorithms from the ANN library 1.1[16]: brute, kd-tree and bd-tree kNN with default settings. The AutokNN and its competitors performed a complete cross validation run on the ad, diabetes, gene, glass, heart, heartc, horse, ionosphere, iris, mushrooms, soybean, STATLOG australian, STATLOG german, STATLOG heart, STATLOG SAT, STATLOG segment STATLOG shuttle, STATLOG vehicle, thyroid, waveform and wine datasets with 3, 5, 10 and 20 folds. The goal of the experiment was the measurement of the time required to complete the full course of testing different k.\nThe data was separated into stratified partitions which were used in different configurations in order to obtain a training and a testing set. The AutoKNN computes the classification results for all k while the other algorithms are bound to use a logarithmic search. By logarithmic search the following schema is meant: k \u2208 {1, 2, 3, 4, 5, 6, 7, 8, 10, 100, 200, ..., 1000, ..., f\u22121f n}. This schema is practically motivated and rational under the assumption that the influence of additional labels on the result diminishes with higher values of k. Practical consideration is primarily test time. Example: while AutokNN required 15,35s for a complete scan based on the ad dataset, on same data exact brute kNN needed 353,7s in logarithmic mode and 19595,7s in full mode. The use of the logarithmic mode makes results with exactly the same values impossible. However, the differences in resulting k and thus in accuracy were absolutely negligible so that the results are directly comparable nonetheless.\nWe added the experiment times for all databases up to a total for each cross validation size. The results are shown in the figure and the table under 2.\nTime measurements were performed on a AMD Phenom II 965 with 8GB of RAM with a Linux 2.6.35 kernel. The algorithms are implemented in C/C++ and were compiled with gcc 4.4.5 with O3 option. Only core algorithm operation was measured and all time for additional I/O was ignored. For best comparability, ANN library sources were statically included."}, {"heading": "4 Discussion and Conclusion", "text": "The Nearest Neighbor approach is considered user friendly and is frequently used for data mining, classification and regression tasks. It is embedded into many automatic environments that make use of kNN\u2019s flexibility. Although kNN has been used, analyzed and advanced for almost six decades a repeating question can not be answered by current literature: What is the fastest way to estimate the right value for k and what are the expenses for doing so.\nThe approach chosen here is to move the k esimation away from the meta framework right into the training function T . The advantage of this is that additional information about the data can be made. This additional information allows to precompute the distances among all vectors without waste and to reuse them numerous times. From this design change which is known to practitioners\nbut not discussed in literature a reduction in time complexity can be observed\nfrom O (( f\u22121 f )2 \u00b7 n3 + f\u22121f \u00b7 n 2 ) to O ( 3 2 f\u22121 f \u00b7 n 2 + n2 log(n) + n2s ) in average\ncase. The experiments show, that this has significant impact on the speed of the k estimation task. The comparison between kd-tree kNN and the proposed approach proves moreover that having a better time complexity saves practically more time than an efficient distance measure for this task.\nThe cost of this improvement is a higher space complexity (now O(n2)). In order to esimtate the practical impact of this complexity exchange we studied the contents of the UCI repository [17]. The UCI should be a reasonable crossover of the problems people face in real life.\nOut of 162 datasets we found that 90% of them have less than 50K examples, 80% of them have less than 10K examples and half of the UCI\u2019s datasets has less than 1000 examples (for exact distribution see Fig. 3). These sizes can be easily handled on higher class commodity computers.\nThis leads to the conclusion that turning in space complexity for time complexity is a good choice most of the time. Future implementations should offer an integrated k searching. The results also show that the so found values for k can be transfered not only to other exact kNN but also to approximate kNN working on kd-tree and bd-tree models.\nSample Sizes of Datasets in the UCI Repository"}, {"heading": "5 Acknowledgments", "text": "This work has been made possible through the funding of the PaREn (Pattern Recognition and Engineering [18]) project by the BMBF (Federal Ministry of Education and Research, Germany)."}, {"heading": "19. Janick V. Frasch and Aleksander Lodwich and Faisal Shafait and Thomas M.", "text": "Breuel A Bayes-true data generator for evaluation of supervised and unsupervised learning methods. Pattern Recognition Letters, volume 32:11, p.1523-1531, 2011, doi: 10.1016/j.patrec.2011.04.010\nAPPENDIX\nA Influence of k on the kNN Performance\nThe following diagrams are results from kNN based on natural and synthetic datasets. Synthetic datasets were obtained using WGKS [19] The standard deviation was estimated based on a 10x cross-validation. The diagrams are non linear. Sections of little change are compressed, hence x-axis are discontinuous.\n1 5 10 15 20 25 30 35 40 45 50 60 70 80 90 100 0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\nkNN Performance Heart Database (10x-validation)\nmean(acc)\nk\nAc cu\nra cy\n1 5 10 15 20 25 30 40 50 60 70 80 90 100 110 120 130140 150 160 170 180 189 0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\nkNNPerformance Glass Database (10x-validation)\nmean(acc)\nk\nA cc\nur ac\ny\n1 2 3 4 5 10 20 30 40 50 60 70 80 100 120 140 155 0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\nkNN Performance Wine Dataset (10x-validation)\nmean(acc)\nk\nAc cu\nra cy\n1 2 3 4 5 10 20 30 50 100 150 200 300 400 500 600 700 1000 1250 1500 1750 2000 2500 3000 3500 4000 4250 4300 4350 4400 4450 4460 4469\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\nkNN Performance Waveform Dataset (10x-validation)\nmean\nk\nA cc\nur ac\ny\n1 2 3 4 10 20 30 40 50 60 70 80 90 100 6438 0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nkNN Performance Thyroid Database (10x-validation)\nmean\nk\nAc cu\nra cy\n1 2 3 4 5 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 250 300 350 400 500 600 650 700 710 720 730 740 747\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\nkNN Performance Statlog's Vehicle Database (10x-validation)\nmean\nk\nAc cu\nra cy\n1 20 40 60 80 100 160 200 260 320 380 440 500 700 900 1200 1600 2000\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\n1.20\nkNN Performance Statlog's Shuttle Database (10x-validation)\nmean\nk\nAc cu\nra cy\n1 20 40 60 80 100 120 140 200 300 400 500 600 1000 1400 1800 2001\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\n1.20\nkNN Performance Statlog's Segment Database (10x-validation)\nmean\nk\nAc cu\nra cy\n1 20 40 100 200 400 600 800 1000 2000 3000 3700 3900 4500 5100 5300 5500 5700\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nkNN Performance Statlog's SAT Database (10x-validation)\nmean\nk\nAc cu\nra cy\n1 2 3 4 5 6 7 8 9 10 20 30 40 50 100 884 0.60\n0.62\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\nkNN Performance Statlog's German Finance Database (10x-validation)\nmean\nk\nAc cu\nra cy\n1 2 3 4 5 10 20 30 40 50 60 70 80 90 100 200 300 400 500 598 0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\nkNN Performance Statlog's Australian Financial Dataset (10x-validation)\nmean\nk\nAc cu\nra cy\n1 10 20 30 40 50 60 70 80 90 100 120 140 160 180 200 220 240 260 280 300 350 400 450 500 550 595\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nkNN Performance Soybean Dataset (10x-validation)\nmean\nk\nAc cu\nra cy\n1 100 200 300 400 500 600 700 800 900 1000 1100 1250 1400 1600 1700 3000 5000 6500 7278\n0.00\n0.20\n0.40\n0.60\n0.80\n1.00\n1.20\nkNN Performance Mushrooms Database (10x-validation)\nmean\nk\nAc cu\nra cy\n1 10 20 30 40 50 60 70 80 90 100 200 300 400 500 600 700 800 900 1000 1500 2000 3000 4000 5000 6000 7000 8000 9000 9999\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nkNN Performance Random 16.67% of MNIST60k (10x-validation)\nmean\nk\nAc cu\nra cy\n1 10 20 30 40 50 60 70 80 90 100 200 300 400 500 600 700 800 900 1000 1500 2000 3000 4000 5000 6000 7000 8000 8958\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nkNN Performance MNIST10K (10x-validation)\nmean\nk\nAc cu\nra cy\n1 2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100110120129 0.00\n0.20\n0.40\n0.60\n0.80\n1.00\n1.20\nkNN Performance Iris Dataset (10x-validation)\nmean\nk\nAc cu\nra cy\n1 2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100150200307 0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nkNN Performance Ionosphere Dataset (10x-validation)\nmean\nk\nAc cu\nra cy\n1 10 20 30 40 50 60 70 80 90 100 110 120 130 140 150 160 170 180 190 200 210 220 230 240 250 260 270 280 290 300 317\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\nkNN Performance Horse Dataset (10x-validation)\nmean\nk\nAc cu\nra cy\n1 2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100 150 200 250 260\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nkNN Performance Heart C Dataset (10x-validation)\nmean\nk\nAc cu\nra cy\n1 2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100 200 300 400 500 600 700 800 900 1000 1200 1500 2834\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nkNN Performance Gene Dataset (10x-validation)\nmean\nk\nA cc\nur ac\ny\n1 2 3 4 5 6 7 8 9 10 20 30 40 50 60 70 80 90 100 150 200 250 300 350 400 450 500 550 600 650 681\n0.50\n0.60\n0.70\n0.80\n0.90\n1.00\nkNN Performance Diabetes Dataset (10x-validation)\nmean\nk\nAc cu\nra cy\n1 3 5 7 9 15 25 40 60 80 100 200 400 600 800 1000 2000\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nkNN Performance Ad Dataset (10x-validation)\nmean\nk\nAc cu\nra cy\n1 41 81 121 161 201 241 281 321 361 401 441 481 4001 5501 5541 5581 5621 5661 5701 5741 5781 5821 5861 5901 5941 5981\n0.640\n0.660\n0.680\n0.700\n0.720\n0.740\n0.760\n0.780\n0.800\nkNN Performance WGKS 2D 20% (10x-validation)\nmean\nk\nA cc\nur ac\ny\n1 21 41 61 81 101 121 141 161 181 201 3001 4001 5001 5501 5801 5821 5841 5861 5881 5901 5921 5941 5961 5981\n0.500\n0.550\n0.600\n0.650\n0.700\n0.750\n0.800\n0.850\n0.900\n0.950\nkNN Performance WGKS 2D 10% (10x-validation)\nmean\nk\nAc cu\nra cy\n1 21 41 61 81 101 121 141 161 181 1981 3981 4781 4801 4821 4841 4861 4881 4901 4921 4941 4961 4981\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nkNN Performance WGKS 4D 30% (10x-validation)\nmean\nk\nAc cu\nra cy\n1 21 41 61 81 101 4501 4521 4541 4561 4581 4601 4621 4641 4661 4681 4701 4721 4741 4761 4781 4801 4821 4841 4861 4881 4901 4921 4941 4961 4981\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nkNN Performance WGKS 4D 20% (10x-validation)\nmean\nk\nAc cu\nra cy\n1 21 41 61 81 101 4501 4521 4541 4561 4581 4601 4621 4641 4661 4681 4701 4721 4741 4761 4781 4801 4821 4841 4861 4881 4901 4921 4941 4961 4981\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nkNN Performance WGKS 4D 10% (10x-validation)\nmean\nk\nAc cu\nra cy\n1 21 41 61 81 101 5901 5921 5941 5961 5981 0.54\n0.56\n0.58\n0.60\n0.62\n0.64\n0.66\n0.68\n0.70\n0.72\nkNN Performance WGKS 2D 30% (10x-validation)\nmean\nk\nAc cu\nra cy"}], "references": [{"title": "Discriminatory analysis, nonparametric discrimination: Consistency properties", "author": ["E. Fix", "J.L. Hodges"], "venue": "Technical Report 4, USAF School of Aviation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1951}, {"title": "Adaptive Metric nearest Neighbor Classification", "author": ["Carlotta Domeniconi", "Dimitrios Gunopulos", "Jing Peng"], "venue": "CVPR, Vol. 1,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Adaptive Kernel Metric Nearest Neighbor Classification", "author": ["Jing Peng", "Douglas R. Heisterkamp", "H.K. Dai"], "venue": "ICPR, Vol. 3,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Learning Weighted Distances for Relevance Feedback in Image Retrieval", "author": ["T. Deselaers", "R. Paredes", "E. Vidal", "H. Ney"], "venue": "ICPR", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "On Packing R-trees", "author": ["I. Kamel", "C. Faloutsos"], "venue": "Proceedings of the 2nd Conference on Information and Knowledge Management (CIKM), Washington DC", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Introduction to Algorithms", "author": ["Thomas H. Cormen", "Charles E. Leiserson", "Ronald L. Rivest"], "venue": "ch. 10, MIT Press and McGraw-Hill", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "An index structure for highdimensional data", "author": ["S. Berchtold", "D. Keim", "H.-P. Kriegel. The X-tree"], "venue": "In Proceedings of 22th International Conference on Very Large Databases (VLDB96), pp 2839, Morgan Kaufmann", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "An Efficient Technique for Nearest-Neighbor Query Processing on the SPY-TEC", "author": ["Dong-Ho Lee", "Hyoung-Joo Kim"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Prototype Selection for the Nearest-Neighbor Rule Through Proximity Graphs", "author": ["Jose Salvador-Sanchez", "Filiberto Pla", "Francesco J. Ferri"], "venue": "PRL, Vol. 18,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Selection of the optimal prototype subset for 1-NN classification PRL", "author": ["U. Lipowezky"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "The Omnipresence of Case-Based Reasoning in Science and Application", "author": ["David W. Aha"], "venue": "Journal on Knowledge-Based Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Secure K-NN Algorithm for Distributed Databases", "author": ["B. Young", "R. Bhatnagar"], "venue": "University of Cincinnati", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Privacy Preserving Nearest Neighbor Search", "author": ["M. Shaneck", "K. Yongdae", "V. Kumar"], "venue": "Dept. of Computer Science Minneapolis", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Privately Computing a Distributed k-nn Classifier", "author": ["M. Kantarcoglu", "C. Clifton"], "venue": "LNCS, Vol. 3202, 279\u2013290", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Approximate Nearest Neighbor Library 1.1, Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland", "author": ["D.M. Mount"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "A Bayes-true data generator for evaluation of supervised and unsupervised learning methods", "author": ["Janick V. Frasch", "Aleksander Lodwich", "Faisal Shafait", "Thomas M. Breuel"], "venue": "Pattern Recognition Letters, volume 32:11,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Since the introduction of the k Nearest Neighbor (kNN) method by Fix and Hodges in 1951 [1] a lot of different variants of it have appeared in order to make it suitable to different scenarios.", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 78, "endOffset": 81}, {"referenceID": 2, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 154, "endOffset": 157}, {"referenceID": 6, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 166, "endOffset": 169}, {"referenceID": 7, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 179, "endOffset": 182}, {"referenceID": 8, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 221, "endOffset": 224}, {"referenceID": 9, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 225, "endOffset": 229}, {"referenceID": 11, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 286, "endOffset": 290}, {"referenceID": 12, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 291, "endOffset": 295}, {"referenceID": 13, "context": "The most notable improvements were done in terms of adaptive distance metrics [2][3][4], fast access via space partitioning (packed R* trees [5], kd-trees[6], X-trees[7], SPY-TEC [8]), knowledge base (prototype) pruning ([9],[10]) or classification based on sensitive distributed data ([13],[14],[15]).", "startOffset": 296, "endOffset": 300}, {"referenceID": 10, "context": "The continuing richness of investigation work into nearest neighbor can be explained with the omnipresence of CBR (Case Based Reasoning) type of problems [12] or just from the practical point of view of its massive parallelizability or simply populartiy.", "startOffset": 154, "endOffset": 158}, {"referenceID": 14, "context": "1[16]: brute, kd-tree and bd-tree kNN with default settings.", "startOffset": 1, "endOffset": 5}], "year": 2017, "abstractText": "The k Nearest Neighbors (kNN) method has received much attention in the past decades, where some theoretical bounds on its performance were identified and where practical optimizations were proposed for making it work fairly well in high dimensional spaces and on large datasets. From countless experiments of the past it became widely accepted that the value of k has a significant impact on the performance of this method. However, the efficient optimization of this parameter has not received so much attention in literature. Today, the most common approach is to cross-validate or bootstrap this value for all values in question. This approach forces distances to be recomputed many times, even if efficient methods are used. Hence, estimating the optimal k can become expensive even on modern systems. Frequently, this circumstance leads to a sparse manual search of k. In this paper we want to point out that a systematic and thorough estimation of the parameter k can be performed efficiently. The discussed approach relies on large matrices, but we want to argue, that in practice a higher space complexity is often much less of a problem than repetetive distance computations.", "creator": "LaTeX with hyperref package"}}}