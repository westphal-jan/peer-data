{"id": "1606.02647", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2016", "title": "Safe and Efficient Off-Policy Reinforcement Learning", "abstract": "in running this chapter work, we take critically a substantially fresh look at some somewhat old and emerging new algorithms selected for employing off - policy, return - indexed based constructive reinforcement supervised learning. expressing at these in a much common form, we critically derive a common novel algorithm, retrace ( $ \\ lambda $ ), with three desired properties : ( method 1 ) low internal variance ; ( 2 ) safety, as it safely uses most samples being collected completely from any behaviour policy, whatever its degree limitations of \" off - policyness \" ; accountability and ( stage 3 ) service efficiency, as as it freely makes the available best use only of small samples commonly collected from near on - policy behaviour monitoring policies. we independently analyse the contractive nature of the related intrinsic operator solutions under performing both strict off - list policy negative policy evaluation and posterior control parameter settings and derive independent online, sample - based algorithms.. to advance our knowledge, this is the first return - based nonlinear off - key policy flow control algorithm converging # a. parameter s. to $ q ^ * $ search without forcing the glie assumption ( unlike greedy strategy in eliminating the limit consistent with random infinite exploration ). as a corollary, we actually prove the convergence convergence of watkins'calculus q ( $ \\ lambda $ ), above which was still previously an open problem. first we illustrate explaining the desired benefits paradox of retrace ( $ \\ lambda $ ) on a standard memory suite reminiscent of traditional atari jones 2600 games.", "histories": [["v1", "Wed, 8 Jun 2016 17:34:13 GMT  (150kb,D)", "http://arxiv.org/abs/1606.02647v1", null], ["v2", "Mon, 7 Nov 2016 21:26:31 GMT  (184kb,D)", "http://arxiv.org/abs/1606.02647v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["r\u00e9mi munos", "tom stepleton", "anna harutyunyan", "marc g bellemare"], "accepted": true, "id": "1606.02647"}, "pdf": {"name": "1606.02647.pdf", "metadata": {"source": "CRF", "title": "Safe and efficient off-policy reinforcement learning", "authors": ["R\u00e9mi Munos", "Tom Stepleton", "Anna Harutyunyan", "Marc G. Bellemare"], "emails": ["munos@google.com", "stepleton@google.com", "anna.harutyunyan@vub.ac.be", "bellemare@google.com"], "sections": [{"heading": null, "text": "One fundamental trade-off in reinforcement learning lies in the definition of the update target: should one estimate Monte Carlo returns or bootstrap from an existing Q-function? Return-based methods (where return refers to the sum of discounted rewards \u2211 t \u03b3\ntrt) offer some advantages over value bootstrap methods: they are better behaved when combined with function approximation, and quickly propagate the fruits of exploration (Sutton, 1996). On the other hand, value bootstrap methods are more readily applied to off-policy data, a common use case. In this paper we show that learning from returns need not be at cross-purposes with off-policy learning.\nWe start from the recent work of Harutyunyan et al. (2016), who show that naive off-policy policy evaluation, without correcting for the \u201coff-policyness\u201d of a trajectory, still converges to the desired Q\u03c0 value function provided the behavior \u00b5 and target \u03c0 policies are not too far apart (the maximum allowed distance depends on the \u03bb parameter). Their Q\u03c0(\u03bb) algorithm learns from trajectories generated by \u00b5 simply by summing discounted off-policy corrected rewards at each time step. Unfortunately, the assumption that \u00b5 and \u03c0 are close is restrictive, as well as difficult to uphold in the control case, where the target policy is always greedy with respect to the current Q-function. In that sense this algorithm is not safe: it does not handle the case of arbitrary \u201coff-policyness\u201d.\nAlternatively, the Tree-backup (TB) (\u03bb) algorithm (Precup et al., 2000) tolerates arbitrary target/behavior discrepancies by scaling information (here called traces) from future temporal differences by the product of target policy probabilities. TB(\u03bb) is not efficient in the \u201cnear on-policy\u201d case (similar \u00b5 and \u03c0), though, as traces may be cut prematurely, blocking learning from full returns.\nIn this work, we express several off-policy, return-based algorithms in a common form. From this we derive an improved algorithm, Retrace(\u03bb), which is both safe and efficient, enjoying convergence guarantees for off-policy policy evaluation and \u2013 more importantly \u2013 for the control setting.\nRetrace(\u03bb) can learn from full returns retrieved from past policy data, as in the context of experience replay (Lin, 1993), which has returned to favour with advances in deep reinforcement learning (Mnih\nar X\niv :1\n60 6.\n02 64\n7v 1\n[ cs\n.L G\n] 8\nJ un\net al., 2015; Schaul et al., 2016). Off-policy learning is also desirable for exploration, since it allows the agent to deviate from the target policy currently under evaluation.\nTo the best of our knowledge, this is the first online return-based off-policy control algorithm which does not require the GLIE (Greedy in the Limit with Infinite Exploration) assumption (Singh et al., 2000). In addition, we provide as a corollary the first proof of convergence of Watkins\u2019 Q(\u03bb) (see, e.g., Watkins, 1989; Sutton and Barto, 1998).\nFinally, we illustrate the significance of Retrace(\u03bb) in a deep learning setting by applying it to the suite of Atari 2600 games provided by the Arcade Learning Environment (Bellemare et al., 2013)."}, {"heading": "1 Notation", "text": "We consider an agent interacting with a Markov Decision Process (X ,A, \u03b3, P, r). X is a finite state space, A the action space, \u03b3 \u2208 [0, 1) the discount factor, P the transition function mapping stateaction pairs (x, a) \u2208 X \u00d7A to distributions over X , and r : X \u00d7A \u2192 [\u2212RMAX, RMAX] is the reward function. For notational simplicity we will consider a finite action space, but the case of infinite \u2013 possibly continuous \u2013 action space can be handled by the Retrace(\u03bb) algorithm as well. A policy \u03c0 is a mapping from X to a distribution over A. A Q-function Q maps each state-action pair (x, a) to a value in R; in particular, the reward r is a Q-function. For a policy \u03c0 we define the operator P\u03c0:\n(P\u03c0Q)(x, a) := \u2211 x\u2032\u2208X \u2211 a\u2032\u2208A P (x\u2032 |x, a)\u03c0(a\u2032 |x\u2032)Q(x\u2032, a\u2032).\nThe value function for a policy \u03c0, Q\u03c0 , describes the expected discounted sum of rewards associated with following \u03c0 from a given state-action pair. Using operator notation, we write this as\nQ\u03c0 := \u2211 t\u22650 \u03b3t(P\u03c0)tr. (1)\nThe Bellman operator T \u03c0 is T \u03c0Q := r + \u03b3P\u03c0Q (2)\nand its fixed point is Q\u03c0 , i.e. T \u03c0Q\u03c0 = Q\u03c0 = (I \u2212 \u03b3P\u03c0)\u22121r. The Bellman optimality operator introduces a maximization over the set of policies:\nT Q := r + \u03b3max \u03c0 P\u03c0Q. (3)\nIts fixed point is Q\u2217, the unique optimal value function (Puterman, 1994). It is this quantity that we will seek to obtain when we talk about the \u201ccontrol setting\u201d.\nReturn-based Operators: The \u03bb-return extension (Sutton, 1988) of both (2) and (3) considers exponentially weighted sums of n-step returns:\nT \u03c0\u03bb Q := (1\u2212 \u03bb) \u2211 n\u22650 \u03bbn [(T \u03c0)nQ] = Q+ (I \u2212 \u03bb\u03b3P\u03c0)\u22121(T \u03c0Q\u2212Q),\nwhere T \u03c0Q \u2212 Q is the Bellman residual of Q for policy \u03c0, with T Q \u2212 Q replacing T \u03c0 \u2212 Q for (3). Examination of the above shows that Q\u03c0 is also the fixed point of T \u03c0\u03bb . At one extreme (\u03bb = 0) we have the Bellman operator T \u03c0\u03bb=0Q = T \u03c0Q, while at the other (\u03bb = 1) we have the policy evaluation operator T \u03c0\u03bb=1Q = Q\u03c0 which can be estimated using Monte Carlo methods (Sutton and Barto, 1998). Intermediate values of \u03bb trade off estimation bias with sample variance (Kearns and Singh, 2000).\nWe seek to evaluate a target policy \u03c0 using trajectories drawn from a behaviour policy \u00b5. If \u03c0 = \u00b5, we are on-policy; otherwise, we are off-policy. We will consider trajectories of the form:\nx0 = x, a0 = a, r0, x1, a1, r1, x2, a2, r2, . . .\nwith at \u223c \u00b5(\u00b7|xt), rt = r(xt, at) and xt+1 \u223c P (\u00b7|xt, at). We denote by Ft this sequence up to time t, and write E\u00b5 the expectation with respect to both \u00b5 and the MDP transition probabilities. Throughout, we write \u2016 \u00b7 \u2016 for supremum norm."}, {"heading": "2 Off-Policy Algorithms", "text": "We are interested in two related off-policy learning problems. In the policy evaluation setting, we are given a fixed policy \u03c0 whose value Q\u03c0 we wish to estimate from sample trajectories drawn from a behaviour policy \u00b5. In the control setting, we consider a sequence of policies that depend on our own sequence of Q-functions (such as \u03b5-greedy policies), and seek to approximate Q\u2217.\nThe general form that we consider for comparing several return-based off-policy algorithms is:\nRQ(x, a) := Q(x, a) + E\u00b5 \u2211 t\u22650 \u03b3t ( t\u220f s=1 cs )( rt + \u03b3E\u03c0Q(xt+1, \u00b7)\u2212Q(xt, at) ) , (4) for some non-negative coefficients (cs), where we write E\u03c0Q(x, \u00b7) := \u2211 a \u03c0(a|x)Q(x, a) and write\n( \u220ft s=1 cs) = 1 when t = 0. By extension of the idea of eligibility traces (Sutton and Barto, 1998), we informally call the coefficients (cs) the traces of the operator.\nImportance sampling (IS): cs = \u03c0(as|xs)\u00b5(as|xs) . Importance sampling is the simplest way to correct for the discrepancy between \u00b5 and \u03c0 when learning from off-policy returns (Precup et al., 2000, 2001; Geist and Scherrer, 2014). The off-policy correction uses the product of the likelihood ratios between \u03c0 and \u00b5. Notice that the RQ operator (4) defined with this choice of (cs) yields Q\u03c0 for any Q. For Q = 0 we recover the basic IS estimate \u2211 t\u22650 \u03b3 t (\u220ft s=1 cs ) rt, thus (4) can be seen as a variance reduction technique (with a baseline Q). It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u00b5(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u00b5(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015).\nOff-policy Q\u03c0(\u03bb) and Q\u2217(\u03bb): cs = \u03bb. A recent alternative proposed by Harutyunyan et al. (2016) introduces an off-policy correction based on a Q-baseline (instead of correcting the probability of the sample path like in IS). This approach, called Q\u03c0(\u03bb) and Q\u2217(\u03bb) for policy evaluation and control, respectively, corresponds to the choice cs = \u03bb. It offers the advantage of avoiding the blow-up of the variance of the product of ratios encountered with IS. Interestingly, this operator contracts around Q\u03c0 provided that \u00b5 and \u03c0 are sufficiently close to each other. Defining \u03b5 := maxx \u2016\u03c0(\u00b7|x)\u2212\u00b5(\u00b7|x)\u20161 the amount of \u201coff-policyness\u201d, the authors prove that the operator defined by (4) with cs = \u03bb is a contraction mapping around Q\u03c0 for \u03bb < 1\u2212\u03b3\u03b3\u03b5 , and around Q\n\u2217 for the worst case of \u03bb < 1\u2212\u03b32\u03b3 . Unfortunately, Q\u03c0(\u03bb) requires knowledge of \u03b5, and the condition for Q\u2217(\u03bb) is very conservative. Neither Q\u03c0(\u03bb), nor Q\u2217(\u03bb) are safe as they do not guarantee convergence for arbitrary \u03c0 and \u00b5.\nTree-backup (TB) (\u03bb): cs = \u03bb\u03c0(as|xs). The TB(\u03bb) algorithm of Precup et al. (2000) corrects for the target/behaviour discrepancy by multiplying each term of the sum by the product of target policy probabilities. The corresponding operator defines a contraction mapping (not only in expectation but also for any sample trajectory) for any policies \u03c0 and \u00b5, which makes it a safe algorithm. However, this algorithm is not efficient in the near on-policy case (where \u00b5 and \u03c0 are similar) as it unnecessarily cuts the traces, preventing it to make use of full returns: we need not discount stochastic on-policy transitions (as shown by Harutyunyan et al.\u2019s results about Q\u03c0).\nRetrace(\u03bb): cs = \u03bbmin (\n1, \u03c0(as|xs)\u00b5(as|xs)\n) . Our contribution is an algorithm \u2013 Retrace(\u03bb) \u2013 that takes\nthe best of the three previous algorithms. Retrace(\u03bb) uses the importance sampling ratio truncated at 1. Compared to IS, it does not suffer from the variance explosion of the product of importance sampling ratios. Now, similarly to Q\u03c0(\u03bb) and unlike TB(\u03bb), it does not cut the traces in the onpolicy case, making it possible to benefit from the full returns. In the off-policy case, the traces are safely cut, similarly to TB(\u03bb). In particular, min ( 1, \u03c0(as|xs)\u00b5(as|xs) ) \u2265 \u03c0(as|xs): Retrace(\u03bb) does not cut the traces as much as TB(\u03bb).\nIn the subsequent sections, we will show the following:\n\u2022 The Retrace(\u03bb) operator is a \u03b3-contraction around Q\u03c0 , for arbitrary policies \u00b5 and \u03c0, \u2022 Taking cs to be no greater than the ratio \u03c0/\u00b5 is sufficient to guarantee this property, \u2022 Under mild assumptions, the control version of Retrace(\u03bb), where \u03c0 is replaced by a se-\nquence of increasingly greedy policies, is also a contraction, and\n\u2022 The online Retrace(\u03bb) algorithm converges a.s. to Q\u2217 in the control case. In the control case, convergence does not require the GLIE assumption.\n\u2022 As a corollary, we prove the convergence of Watkins\u2019s Q(\u03bb) to Q\u2217."}, {"heading": "3 Analysis of Retrace(\u03bb)", "text": "We will in turn analyse both off-policy policy evaluation and control settings. We will show that R is a contraction mapping in both settings (under a mild additional assumption for the control case)."}, {"heading": "3.1 Policy Evaluation", "text": "Consider a fixed target policy \u03c0. For ease of exposition we consider a fixed behaviour policy \u00b5, noting that our result extends to the setting of sequences of behaviour policies (\u00b5k : k \u2208 N). Our first result states the \u03b3-contraction of the operator (4) defined by any set of non-negative coefficients cs = cs(as,Fs) (in order to emphasize that cs can be a function of the whole history Fs) under the assumption that cs \u2264 \u03c0(as|xs)\u00b5(as|xs) .\nTheorem 1. The operator R defined by (4) has a unique fixed point Q\u03c0 . Furthermore, if for each as \u2208 A and each history Fs we have cs = cs(as,Fs) \u2208 [ 0, \u03c0(as|xs)\u00b5(as|xs) ] , then for any Q-function Q\n\u2016RQ\u2212Q\u03c0\u2016 \u2264 \u03b3\u2016Q\u2212Q\u03c0\u2016.\nThe following lemma will be useful in proving Theorem 1 (proof in the appendix).\nLemma 1. The difference betweenRQ and its fixed point Q\u03c0 is\nRQ(x, a)\u2212Q\u03c0(x, a) = E\u00b5 [ \u2211 t\u22651 \u03b3t ( t\u22121\u220f i=1 ci )([ E\u03c0[(Q\u2212Q\u03c0)(xt, \u00b7)]\u2212 ct(Q\u2212Q\u03c0)(xt, at) ])] .\nProof (Theorem 1). The fact that Q\u03c0 is the fixed point of the operator R is obvious from (4) since Ext+1\u223cP (\u00b7|xt,at) [ rt + \u03b3E\u03c0Q\u03c0(xx+1, \u00b7) \u2212 Q\u03c0(xt, at) ] = (T \u03c0Q\u03c0 \u2212 Q\u03c0)(xt, at) = 0, since Q\u03c0 is the fixed point of T \u03c0 . Now, from Lemma 1, and defining \u2206Q := Q\u2212Q\u03c0 , we have RQ(x, a)\u2212Q\u03c0(x, a) = \u2211 t\u22651 \u03b3tEx1:t,a1:t [( t\u22121\u220f i=1 ci )([ E\u03c0\u2206Q(xt, \u00b7)\u2212 ct\u2206Q(xt, at) ])]\n= \u2211 t\u22651 \u03b3tEx1:t,a1:t\u22121 [( t\u22121\u220f i=1 ci )([ E\u03c0\u2206Q(xt, \u00b7)\u2212 Eat [ct(at,Ft)\u2206Q(xt, at)|Ft] ])] = \u2211 t\u22651 \u03b3tEx1:t,a1:t\u22121 [( t\u22121\u220f i=1 ci )\u2211 b ( \u03c0(b|xt)\u2212 \u00b5(b|xt)ct(b,Ft) ) \u2206Q(xt, b) ] .\nNow since \u03c0(a|xt) \u2212 \u00b5(a|xt)ct(b,Ft) \u2265 0, we have that RQ(x, a) \u2212 Q\u03c0(x, a) =\u2211 y,b wy,b\u2206Q(y, b), i.e. a linear combination of \u2206Q(y, b) weighted by non-negative coefficients:\nwy,b := \u2211 t\u22651 \u03b3tEx1:t,a1:t\u22121 [( t\u22121\u220f i=1 ci )( \u03c0(b|xt)\u2212 \u00b5(b|xt)ct(b,Ft) ) I{xt = y} ] .\nThe sum of those coefficients is:\u2211 y,b wy,b = \u2211 t\u22651 \u03b3tEx1:t,a1:t\u22121 [( t\u22121\u220f i=1 ci )\u2211 b ( \u03c0(b|xt)\u2212 \u00b5(b|xt)ct(b,Ft) )]\n= \u2211 t\u22651 \u03b3tEx1:t,a1:t\u22121 [( t\u22121\u220f i=1 ci ) Eat [1\u2212 ct(at,Ft)|Ft] ] = \u2211 t\u22651 \u03b3tEx1:t,a1:t [( t\u22121\u220f i=1 ci ) (1\u2212 ct) ]\n= E\u00b5 [\u2211 t\u22651 \u03b3t ( t\u22121\u220f i=1 ci ) \u2212 \u2211 t\u22651 \u03b3t ( t\u220f i=1 ci )] = \u03b3C \u2212 (C \u2212 1),\nwhere C := E\u00b5 [\u2211 t\u22650 \u03b3 t (\u220ft i=1 ci )] . Since C \u2265 1, we have that \u2211 y,b wy,b \u2264 \u03b3. Thus RQ(x, a) \u2212 Q\u03c0(x, a) is a sub-convex combination of \u2206Q(y, b) weighted by non-negative coefficients wy,b which sum to (at most) \u03b3, thusR is a \u03b3-contraction mapping around Q\u03c0 .\nRemark 1. Notice that the coefficient C in the proof of Theorem 1 depends on (x, a). If we let \u03b7(x, a) := 1\u2212 (1\u2212 \u03b3)E\u00b5 [\u2211 t\u22650 \u03b3 t( \u220ft s=1 cs) ] , then we have shown that\n|RQ(x, a)\u2212Q\u03c0(x, a)| \u2264 \u03b7(x, a)\u2016Q\u2212Q\u03c0\u2016. Thus \u03b7(x, a) \u2208 [0, \u03b3] is a (x, a)-specific contraction coefficient, which is \u03b3 when c1 = 0 (the trace is cut immediately) and can be close to zero when learning from full returns (ct \u2248 1 for all t)."}, {"heading": "3.2 Control", "text": "In the control setting, the single target policy \u03c0 is replaced by a sequence of policies which depend on Qk. While most prior work has focused on strictly greedy policies, here we consider the larger class of increasingly greedy sequences. We now make this notion precise. Definition 1. We say that a sequence of policies (\u03c0k : k \u2208 N) is increasingly greedy w.r.t. a sequence (Qk : k \u2208 N) of Q-functions if the following property holds for all k:\nP\u03c0k+1Qk+1 \u2265 P\u03c0kQk+1.\nIntuitively, this means that each \u03c0k+1 is at least as greedy as the previous policy \u03c0k for Qk+1. Many natural sequences of policies are increasingly greedy, including \u03b5k-greedy policies (with nonincreasing \u03b5k) and softmax policies (with non-increasing temperature). See proofs in the appendix.\nWe will assume that cs = cs(as,Fs) = c(as, xs) is Markovian, in the sense that it depends on xs, as (as well as the policies \u03c0 and \u00b5) only but not on the full past history. This allows us to define the (sub)-probability transition operator\n(P c\u00b5Q)(x, a) := \u2211 x\u2032 \u2211 a\u2032 p(x\u2032|x, a)\u00b5(a\u2032|x\u2032)c(a\u2032, x\u2032)Q(x\u2032, a\u2032).\nFinally, an additional requirement to the convergence in the control case, we assume thatQ0 satisfies T \u03c00Q0 \u2265 Q0 (this can be achieved by a pessimistic initialization Q0 = \u2212RMAX/(1\u2212 \u03b3)). Theorem 2. Consider an arbitrary sequence of behaviour policies (\u00b5k) (which may depend on (Qk)) and a sequence of target policies (\u03c0k) that are increasingly greedy w.r.t. the sequence (Qk):\nQk+1 = RkQk, where the return operator Rk is defined by (4) for \u03c0k and \u00b5k and a Markovian cs = c(as, xs) \u2208 [0, \u03c0(as|xs)\u00b5(as|xs) ]. Assume the target policies \u03c0k are \u03b5k-away from the greeedy policies w.r.t. Qk, in the sense that T \u03c0kQk \u2265 T Qk \u2212 \u03b5k\u2016Qk\u2016e, where e is the vector with 1-components. Further suppose that T \u03c00Q0 \u2265 Q0. Then for any k \u2265 0, \u2016Qk+1 \u2212Q\u2217\u2016 \u2264 \u03b3\u2016Qk \u2212Q\u2217\u2016+ \u03b5k\u2016Qk\u2016. In consequence, if \u03b5k \u2192 0 then Qk \u2192 Q\u2217.\nSketch of Proof (The full proof is in the appendix). Using P c\u00b5k , the Retrace(\u03bb) operator rewrites RkQ = Q+ \u2211 t\u22650 \u03b3t(P c\u00b5k)t(T \u03c0kQ\u2212Q) = Q+ (I \u2212 \u03b3P c\u00b5k)\u22121(T \u03c0kQ\u2212Q).\nWe now lower- and upper-bound the term Qk+1 \u2212Q\u2217. Upper bound on Qk+1 \u2212 Q\u2217. We prove that Qk+1 \u2212 Q\u2217 \u2264 Ak(Qk \u2212 Q\u2217) with Ak := \u03b3(I \u2212 \u03b3P c\u00b5k)\u22121 [ P\u03c0k \u2212 P c\u00b5k ] . Since ct \u2208 [0, \u03c0(at|xt)\u00b5(at|xt) ] we deduce that Ak has non-negative elements, whose sum over each row, is at most \u03b3. Thus\nQk+1 \u2212Q\u2217 \u2264 \u03b3\u2016Qk \u2212Q\u2217\u2016e. (5)\nLower bound on Qk+1 \u2212Q\u2217. Using the fact that T \u03c0kQk \u2265 T \u03c0 \u2217 Qk \u2212 \u03b5k\u2016Qk\u2016e we have\nQk+1 \u2212Q\u2217 \u2265 Qk+1 \u2212 T \u03c0kQk + \u03b3P\u03c0 \u2217 (Qk \u2212Q\u2217)\u2212 \u03b3\u03b5k\u2016Qk\u2016e\n= \u03b3P c\u00b5k(I \u2212 \u03b3P c\u00b5k)\u22121(T \u03c0kQk \u2212Qk) + \u03b3P\u03c0 \u2217 (Qk \u2212Q\u2217)\u2212 \u03b5k\u2016Qk\u2016e. (6)\nLower bound on T \u03c0kQk \u2212Qk. Since (\u03c0k) is increasingly greedy, we have\nT \u03c0k+1Qk+1 \u2212Qk+1 \u2265 T \u03c0kQk+1 \u2212Qk+1 = r + (\u03b3P\u03c0k \u2212 I)RQk = Bk(T \u03c0kQk \u2212Qk), (7)\nwhereBk := \u03b3[P\u03c0k\u2212P c\u00b5k ](I\u2212\u03b3P c\u00b5k)\u22121. Since P\u03c0k\u2212P c\u00b5k and (I\u2212\u03b3P c\u00b5k)\u22121 are non-negative matrices, so is Bk. Thus\nT \u03c0kQk \u2212Qk \u2265 Bk\u22121Bk\u22122 . . . B0(T \u03c00Q0 \u2212Q0) \u2265 0,\nsince we assumed T\u03c00Q0 \u2212Q0 \u2265 0. Thus, (6) implies that\nQk+1 \u2212Q\u2217 \u2265 \u03b3P\u03c0 \u2217 (Qk \u2212Q\u2217)\u2212 \u03b5k\u2016Qk\u2016e.\nand combining the above with (5) we deduce \u2016Qk+1 \u2212 Q\u2217\u2016 \u2264 \u03b3\u2016Qk \u2212 Q\u2217\u2016 + \u03b5k\u2016Qk\u2016. When \u03b5\u2192 0, we further deduce that Qk are bounded, thus Qk \u2192 Q\u2217."}, {"heading": "3.3 Online algorithms", "text": "So far we have analysed the contraction properties of the expected R operators. We now describe online algorithms which can learn from sample trajectories. We analyze the algorithms in the every visit form (Sutton and Barto, 1998), which is the more practical generalization of the first-visit form. In this section, we will only consider the Retrace(\u03bb) algorithm defined with the coefficient c = \u03bbmin(1, \u03c0/\u00b5). For that c, let us rewrite the operator P c\u00b5 as \u03bbP\u03c0\u2227\u00b5, where P\u03c0\u2227\u00b5Q(x, a) := \u2211 y \u2211 b min(\u03c0(b|y), \u00b5(b|y))Q(y, b), and write the Retrace operator RQ = Q + (I \u2212 \u03bb\u03b3P\u03c0\u2227\u00b5)\u22121(T \u03c0Q \u2212 Q). We focus on the control case, noting that a similar (and more general) result can be derived for policy evaluation.\nTheorem 3. Consider a sequence of sample trajectories, with the kth trajectory x0, a0, r0, x1, a1, r1, . . . generated by following \u00b5k: at \u223c \u00b5k(\u00b7|xt). For each (x, a) along this trajectory, with s the time of first occurrence of (x, a), update\nQk+1(x, a)\u2190 Qk(x, a) + \u03b1k \u2211 t\u2265s \u03b4\u03c0kt t\u2211 j=s \u03b3t\u2212j ( t\u220f i=j+1 ci ) I{xj , aj = x, a}, (8)\nwhere \u03b4\u03c0kt := rt + \u03b3E\u03c0kQk(xt+1, \u00b7) \u2212 Qk(xt, at), \u03b1k = \u03b1k(xs, as). We consider the Retrace(\u03bb) algorithm where ci = \u03bbmin ( 1, \u03c0(ai|xi)\u00b5(ai|xi) ) . Assume that (\u03c0k) are increasingly greedy w.r.t. (Qk) and are each \u03b5k-away from the greedy policies (\u03c0Qk), i.e. maxx \u2016\u03c0k(\u00b7|x)\u2212\u03c0Qk(\u00b7|x)\u20161 \u2264 \u03b5k, with \u03b5k \u2192 0. Assume that P\u03c0k and P\u03c0k\u2227\u00b5k asymptotically commute: limk \u2016P\u03c0kP\u03c0k\u2227\u00b5k \u2212P\u03c0k\u2227\u00b5kP\u03c0k\u2016 = 0. Assume further that (1) all states and actions are visited infinitely often: \u2211 t\u22650 P{xt, at = x, a} \u2265 D > 0, (2) the sample trajectories are finite in terms of the second moment of their lengths Tk: E\u00b5kT 2k <\u221e, (3) the stepsizes obey the usual Robbins-Munro conditions. Then Qk \u2192 Q\u2217 a.s.\nThe proof extends similar convergence proofs of TD(\u03bb) by Bertsekas and Tsitsiklis (1996) and of optimistic policy iteration by Tsitsiklis (2003), and is provided in the appendix. Notice that compared to Theorem 2 we do not assume that T \u03c00Q0\u2212Q0 \u2265 0 here. However, we make the additional (rather technical) assumption that P\u03c0k and P\u03c0k\u2227\u00b5k commute at the limit. This is satisfied for example when the probability assigned by the behavior policy \u00b5k(\u00b7|x) to the greedy action \u03c0Qk(x) is independent of x. Examples include \u03b5-greedy policies, or more generally mixtures between the greedy policy \u03c0Qk and an arbitrary distribution \u00b5 (see Lemma 5 in the appendix for the proof):\n\u00b5k(a|x) = \u03b5 \u00b5(a|x)\n1\u2212 \u00b5(\u03c0Qk(x)|x) I{a 6= \u03c0Qk(x)}+ (1\u2212 \u03b5)I{a = \u03c0Qk(x)}. (9)\nNotice that the mixture coefficient \u03b5 needs not go to 0."}, {"heading": "4 Discussion of the results", "text": ""}, {"heading": "4.1 Choice of the trace coefficients cs", "text": "Theorems 1 and 2 ensure convergence to Q\u03c0 and Q\u2217 for any trace coefficient cs \u2208 [0, \u03c0(as|xs)\u00b5(as|xs) ]. However, to make the best choice of cs, we need to consider the speed of convergence, which depends on both (1) the variance of the online estimate, which indicates how many online updates are required in a single iteration ofR, and (2) the contraction coefficient ofR. Variance The variance of the estimate strongly depends on the variance of the product trace (c1 . . . ct), which is not an easy quantity to control in general, as the (cs) are usually not independent. However, assuming independence and stationarity of (cs), we have that V (\u2211 t \u03b3 tc1 . . . ct )\nis at least \u2211 t \u03b3\n2tV(c)t, which is finite only if V(c) < 1/\u03b32. Thus, an important requirement for a numerically stable algorithm is for V(c) to be as small as possible, and certainly no more than 1/\u03b32. This rules out importance sampling (for which c \u221d \u03c0(a|x)\u00b5(a|x) , and V(c|x) \u221d \u2211 a \u00b5(a|x) (\u03c0(a|x) \u00b5(a|x)\u22121 )2 =\u2211\na \u03c0(at|xt)2 \u00b5(at|xt) \u22121, which may be larger than 1/\u03b3 2 for some \u03c0 and \u00b5), and is the reason we take cs \u2264 1.\nContraction speed The contraction coefficient \u03b7 \u2208 [0, \u03b3] of R (see Remark 1) depends on how much the traces have been cut, and should be as small as possible (since it takes log(1/\u03b5)/ log(1/\u03b7) iterations of R to obtain an \u03b5-approximation). It is smallest when the traces are not cut at all (i.e. if cs = 1 for all s, R is the policy evaluation operator which produces Q\u03c0 in a single iteration). Indeed, when the traces are cut, we do not benefit from learning from full returns (in the extreme, c1 = 0 and R reduces to the Bellman operator with \u03b7 = \u03b3). Although (cs) should be as large as possible, they probably should not be larger than 1, or the update rule would consider the future to be more important than the present. A reasonable trade-off between low variance (when cs are small) and high contraction speed (when cs are large) is given by Retrace(\u03bb), for which we provde the convergence of the online algorithm.\nIf we relax the assumption that the trace is Markovian (in which case only the result for policy evaluation has been proven so far) we could trade off a low trace at some time for a possibly largerthan-1 trace at another time, as long as their product is less than 1. A possible choice could be\nct = \u03bbmin ( 1 c1 . . . ct\u22121 , \u03c0(at|xt) \u00b5(at|xt) ) . (10)"}, {"heading": "4.2 Other topics of discussion", "text": "No GLIE assumption. The crucial point of Theorem 2 is that convergence to Q\u2217 occurs for arbitrary behaviour policies. Thus the online result in Theorem 3 does not require the behaviour policies to become greedy in the limit of infinite exploration (i.e. GLIE assumption, Singh et al., 2000). We believe Theorem 3 provides the first convergence result to Q\u2217 for a \u03bb-return (with \u03bb > 0) algorithm that does not require this (hard to satisfy) assumption.\nProof of Watkins\u2019 Q(\u03bb). As a corollary of Theorem 3 when selecting our target policies \u03c0k to be greedy w.r.t. Qk (i.e. \u03b5k = 0), we deduce that Watkins\u2019 Q(\u03bb) (e.g., Watkins, 1989; Sutton and Barto, 1998) converges a.s. to Q\u2217 (under the assumption that \u00b5k commutes asymptotically with the greedy policies, which is satisfied for e.g. \u00b5k defined by (9)). We believe this is the first such proof.\nIncreasingly greedy policies The assumption that the sequence of target policies (\u03c0k) is increasingly greedy w.r.t. the sequence of (Qk) is more general that just considering greedy policies w.r.t. (Qk) (which is Watkins\u2019s Q(\u03bb)), and may be more efficient as well. Indeed, using non-greedy target policies \u03c0k can speed up convergence as the traces will not be cut as frequently. Of course, in order to converge to Q\u2217, we eventually need the target policies (and not the behaviour policies, as mentioned above) to become greedy in the limit (i.e. \u03b5k \u2192 0 as defined in Theorem 2). Comparison to Q\u03c0(\u03bb). Unlike Retrace(\u03bb), Q\u03c0 does not need to know the behaviour policy \u00b5. However, it fails to converge when \u00b5 is far from \u03c0. Retrace(\u03bb) uses its knowledge of \u00b5 (for the chosen actions) to cut the traces and safely handle arbitrary policies \u03c0 and \u00b5.\nComparison to TB(\u03bb). Similarly to Q\u03c0 , TB(\u03bb) does not need the knowledge of the behaviour policy \u00b5. But as a consequence, TB(\u03bb) is not able to benefit from possible near on-policy situations, cutting traces unnecessarily when \u03c0 and \u00b5 are close.\nContinuous action space. Let us mention that Theorems 1 and 2 extend to the case of (measurable) continuous or infinite action spaces. The trace coefficients will make use of the densities min(1, d\u03c0/d\u00b5) instead of the probabilities min(1, \u03c0/\u00b5). This would not be possible with TB(\u03bb).\nOpen questions include: (1) Removing the technical assumption that P\u03c0k and P\u03c0k\u2227\u00b5k asymptotically commute, (2) Relaxing the Markov assumption in the control case in order to allow trace coefficients ct of the form (10)."}, {"heading": "5 Experimental Results", "text": "To validate our theoretical results, we employ Retrace(\u03bb) in an experience replay (Lin, 1993) setting, where sample transitions are stored within a large but bounded replay memory and subsequently replayed as if they were new experience. Naturally, older data in the memory is usually drawn from a policy which differs from the current policy, offering an excellent point of comparison for the algorithms presented in Section 2.\nOur agent adapts the DQN architecture of Mnih et al. (2015) to replay short sequences from the memory (details in Appendix F) instead of single transitions. The Q-function target for a sample sequence xt, at, rt, \u00b7 \u00b7 \u00b7 , xt+k is\n\u2206Q(xt, at) = k\u22121\u2211 s=t \u03b3s\u2212t ( s\u220f i=t+1 ci )[ r(xs, as) + \u03b3E\u03c0Q(xs+1, \u00b7)\u2212Q(xs, as) ] .\nWe compare our algorithms\u2019 performance on 60 different Atari 2600 games in the Arcade Learning Environment (Bellemare et al., 2013) using Bellemare et al.\u2019s inter-algorithm score distribution. Inter-algorithm scores are normalized so that 0 and 1 respectively correspond to the worst and best score for a particular game, within the set of algorithms under comparison. If g \u2208 {1, . . . , 60} is a game and zg,a the inter-algorithm score on g for algorithm a, then the score distribution function is f(x) := |{g : zg,a \u2265 x}|/60. Roughly, a strictly higher curve corresponds to a better algorithm. Across values of \u03bb, \u03bb = 1 performs best, save for Q\u2217 where \u03bb = 0.5 obtains slightly superior performance. However, Q\u2217 diverges for larger \u03bb values (see Figure 1, left), and yields poor performance for smaller ones. Both Retrace and TB(\u03bb) achieve dramatically higher performance than\nQ-Learning early on and maintain their advantage throughout. Compared to TB(\u03bb), Retrace(\u03bb) offers a narrower but still marked advantage, being the best performer on 30 games; TB(\u03bb) claims 15 of the remainder. Per-game performance details appear in Table 2 in Appendix F."}, {"heading": "A Proof of Lemma 1", "text": "Proof (Lemma 1). Let \u2206Q := Q\u2212Q\u03c0 . We begin by rewriting (4):\nRQ(x, a) = \u2211 t\u22650 \u03b3tE\u00b5 [( t\u220f s=1 cs )( rt + \u03b3 [ E\u03c0Q(xt+1, \u00b7)\u2212 ct+1Q(xt+1, at+1) )]] .\nSince Q\u03c0 is the fixed point ofR, we have Q\u03c0(x, a) = RQ\u03c0(x, a) = \u2211 t\u22650 \u03b3tE\u00b5 [( t\u220f s=1 cs )( rt + \u03b3 [ E\u03c0Q\u03c0(xt+1, \u00b7)\u2212 ct+1Q\u03c0(xt+1, at+1) )]] ,\nfrom which we deduce that\nRQ(x, a)\u2212Q\u03c0(x, a) = \u2211 t\u22650 \u03b3tE\u00b5 [( t\u220f s=1 cs )( \u03b3 [ E\u03c0\u2206Q(xt+1, \u00b7)\u2212 ct+1\u2206Q(xt+1, at+1) ])]\n= \u2211 t\u22651 \u03b3tE\u00b5 [( t\u22121\u220f s=1 cs )([ E\u03c0\u2206Q(xt, \u00b7)\u2212 ct\u2206Q(xt, at) ])] .\nB Increasingly greedy policies\nRecall the definition of an increasingly greedy sequence of policies. Definition 2. We say that a sequence of policies (\u03c0k) is increasingly greedy w.r.t. a sequence of functions (Qk) if the following property holds for all k:\nP\u03c0k+1Qk+1 \u2265 P\u03c0kQk+1.\nIt is obvious to see that this property holds if all policies \u03c0k are greedy w.r.t. Qk. Indeed in such case, T \u03c0k+1Qk+1 = T Qk+1 \u2265 T \u03c0Qk+1 for any \u03c0. We now prove that this property holds for \u03b5k-greedy policies (with non-increasing (\u03b5k)) as well as soft-max policies (with non-decreasing (\u03b2k)), as stated in the two lemmas below.\nOf course not all policies satisfy this property (a counter-example being \u03c0k(a|x) := arg mina\u2032 Qk(x, a\n\u2032)). Lemma 2. Let (\u03b5k) be a non-increasing sequence. Then the sequence of policies (\u03c0k) which are \u03b5k-greedy w.r.t. the sequence of functions (Qk) is increasingly greedy w.r.t. that sequence.\nProof. From the definition of an \u03b5-greedy policy we have: P\u03c0k+1Qk+1(x, a) = \u2211 y p(y|x, a) [ (1\u2212 \u03b5k+1) max b Qk+1(y, b) + \u03b5k+1 1 A \u2211 b Qk+1(y, b) ]\n\u2265 \u2211 y p(y|x, a) [ (1\u2212 \u03b5k) max b Qk+1(y, b) + \u03b5k 1 A \u2211 b Qk+1(y, b) ]\n\u2265 \u2211 y p(y|x, a) [ (1\u2212 \u03b5k)Qk+1(y, arg max b Qk(y, b)) + \u03b5 1 A \u2211 b Qk+1(y, b) ] = P\u03c0kQk+1,\nwhere we used the fact that \u03b5k+1 \u2264 \u03b5k.\nLemma 3. Let (\u03b2k) be a non-decreasing sequence of soft-max parameters. Then the sequence of policies (\u03c0k) which are soft-max (with parameter \u03b2k) w.r.t. the sequence of functions (Qk) is increasingly greedy w.r.t. that sequence.\nProof. For any Q and y, define \u03c0\u03b2(b) = e \u03b2Q(y,b)\u2211\nb\u2032 e \u03b2Q(y,b\u2032) and f(\u03b2) =\n\u2211 b \u03c0\u03b2(b)Q(y, b). Then we have\nf \u2032(\u03b2) = \u2211 b [ \u03c0\u03b2(b)Q(y, b)\u2212 \u03c0\u03b2(b) \u2211 b\u2032 \u03c0\u03b2(b \u2032)Q(y, b\u2032) ] Q(y, b)\n= \u2211 b \u03c0\u03b2(b)Q(y, b) 2 \u2212 (\u2211 b \u03c0\u03b2(b)Q(y, b) )2\n= Vb\u223c\u03c0\u03b2 [ Q(y, b) ] \u2265 0.\nThus \u03b2 7\u2192 f(\u03b2) is a non-decreasing function, and since \u03b2k+1 \u2265 \u03b2k, we have\nP\u03c0k+1Qk+1(x, a) = \u2211 y p(y|x, a) \u2211 b e\u03b2k+1Qk+1(y,b)\u2211 b\u2032 e \u03b2k+1Qk+1(y,b\u2032) Qk+1(y, b)\n\u2265 \u2211 y p(y|x, a) \u2211 b e\u03b2kQk+1(y,b)\u2211 b\u2032 e \u03b2kQk+1(y,b\u2032) Qk+1(y, b)\n= P\u03c0kQk+1(x, a)."}, {"heading": "C Proof of Theorem 2", "text": "As mentioned in the main text, since cs is Markovian, we can define the (sub)-probability transition operator\n(P c\u00b5Q)(x, a) := \u2211 x\u2032 \u2211 a\u2032 p(x\u2032|x, a)\u00b5(a\u2032|x\u2032)c(a\u2032, x\u2032)Q(x\u2032, a\u2032).\nThe Retrace(\u03bb) operator then writes RkQ = Q+ \u2211 t\u22650 \u03b3t(P c\u00b5k)t(T \u03c0kQ\u2212Q) = Q+ (I \u2212 \u03b3P c\u00b5k)\u22121(T \u03c0kQ\u2212Q).\nProof. We now lower- and upper-bound the term Qk+1 \u2212Q\u2217.\nUpper bound on Qk+1 \u2212Q\u2217. Since Qk+1 = RkQk, we have\nQk+1 \u2212Q\u2217 = Qk \u2212Q\u2217 + (I \u2212 \u03b3P c\u00b5k)\u22121 [ T \u03c0kQk \u2212Qk ] = (I \u2212 \u03b3P c\u00b5k)\u22121 [ T \u03c0kQk \u2212Qk + (I \u2212 \u03b3P c\u00b5k)(Qk \u2212Q\u2217)]\n= (I \u2212 \u03b3P c\u00b5k)\u22121 [ T \u03c0kQk \u2212Q\u2217 \u2212 \u03b3P c\u00b5k(Qk \u2212Q\u2217)]\n= (I \u2212 \u03b3P c\u00b5k)\u22121 [ T \u03c0kQk \u2212 T Q\u2217 \u2212 \u03b3P c\u00b5k(Qk \u2212Q\u2217)]\n\u2264 (I \u2212 \u03b3P c\u00b5k)\u22121 [ \u03b3P\u03c0k(Qk \u2212Q\u2217)\u2212 \u03b3P c\u00b5k(Qk \u2212Q\u2217)]\n= \u03b3(I \u2212 \u03b3P c\u00b5k)\u22121 [ P\u03c0k \u2212 P c\u00b5k ] (Qk \u2212Q\u2217), = Ak(Qk \u2212Q\u2217), (11)\nwhere Ak := \u03b3(I \u2212 \u03b3P c\u00b5k)\u22121 [ P\u03c0k \u2212 P c\u00b5k ] .\nNow let us prove that Ak has non-negative elements, whose sum over each row is at most \u03b3. Let e be the vector with 1-components. By rewriting Ak as \u03b3 \u2211 t\u22650 \u03b3\nt(P c\u00b5k)t(P\u03c0k \u2212P c\u00b5k) and noticing that\n(P\u03c0k \u2212 P c\u00b5k)e(x, a) = \u2211 x\u2032 \u2211 a\u2032 p(x\u2032|x, a)[\u03c0k(a\u2032|x\u2032)\u2212 c(a\u2032, x\u2032)\u00b5k(a\u2032|x\u2032)] \u2265 0, (12)\nit is clear that all elements of Ak are non-negative. We have Ake = \u03b3 \u2211 t\u22650 \u03b3t(P c\u00b5k)t [ P\u03c0k \u2212 P c\u00b5k ] e\n= \u03b3 \u2211 t\u22650 \u03b3t(P c\u00b5k)te\u2212 \u2211 t\u22650 \u03b3t+1(P c\u00b5k)t+1e\n= e\u2212 (1\u2212 \u03b3) \u2211 t\u22650 \u03b3t(P c\u00b5k)te\n\u2264 \u03b3e, (13) (since \u2211 t\u22650 \u03b3\nt(P c\u00b5k)te \u2265 e). Thus Ak has non-negative elements, whose sum over each row, is at most \u03b3. We deduce from (11) that Qk+1 \u2212 Q\u2217 is upper-bounded by a sub-convex combination of components of Qk \u2212Q\u2217; the sum of their coefficients is at most \u03b3. Thus\nQk+1 \u2212Q\u2217 \u2264 \u03b3\u2016Qk \u2212Q\u2217\u2016e. (14)\nLower bound on Qk+1 \u2212Q\u2217. We have Qk+1 = Qk + (I \u2212 \u03b3P c\u00b5k)\u22121(T \u03c0kQk \u2212Qk)\n= Qk + \u2211 i\u22650 \u03b3i(P c\u00b5k)i(T \u03c0kQk \u2212Qk)\n= T \u03c0kQk + \u2211 i\u22651 \u03b3i(P c\u00b5k)i(T \u03c0kQk \u2212Qk)\n= T \u03c0kQk + \u03b3P c\u00b5k(I \u2212 \u03b3P c\u00b5k)\u22121(T \u03c0kQk \u2212Qk). (15)\nNow, from the definition of \u03b5k we have T \u03c0kQk \u2265 T Qk \u2212 \u03b5k\u2016Qk\u2016 \u2265 T \u03c0 \u2217 Qk \u2212 \u03b5k\u2016Qk\u2016, thus\nQk+1 \u2212Q\u2217 = Qk+1 \u2212 T \u03c0kQk + T \u03c0kQk \u2212 T \u03c0 \u2217 Qk + T \u03c0 \u2217 Qk \u2212 T \u03c0 \u2217 Q\u2217\n\u2265 Qk+1 \u2212 T \u03c0kQk + \u03b3P\u03c0 \u2217 (Qk \u2212Q\u2217)\u2212 \u03b5k\u2016Qk\u2016e\nUsing (15) we derive the lower bound:\nQk+1 \u2212Q\u2217 \u2265 \u03b3P c\u00b5k(I \u2212 \u03b3P c\u00b5k)\u22121(T \u03c0kQk \u2212Qk) + \u03b3P\u03c0 \u2217 (Qk \u2212Q\u2217)\u2212 \u03b5k\u2016Qk\u2016. (16)\nLower bound on T \u03c0kQk \u2212Qk. By hypothesis, (\u03c0k) is increasingly greedy w.r.t. (Qk), thus T \u03c0k+1Qk+1 \u2212Qk+1 \u2265 T \u03c0kQk+1 \u2212Qk+1\n= T \u03c0kRQk \u2212RQk = r + (\u03b3P\u03c0k \u2212 I)RQk = r + (\u03b3P\u03c0k \u2212 I) [ Qk + (I \u2212 \u03b3P c\u00b5k)\u22121(T \u03c0kQk \u2212Qk)\n] = T \u03c0kQk \u2212Qk + (\u03b3P\u03c0k \u2212 I)(I \u2212 \u03b3P c\u00b5k)\u22121(T \u03c0kQk \u2212Qk) = \u03b3 [ P\u03c0k \u2212 P c\u00b5k ] (I \u2212 \u03b3P c\u00b5k)\u22121(T \u03c0kQk \u2212Qk)\n= Bk(T \u03c0kQk \u2212Qk), (17) where Bk := \u03b3[P\u03c0k \u2212 P c\u00b5k ](I \u2212 \u03b3P c\u00b5k)\u22121. Since P\u03c0k \u2212 P c\u00b5k has non-negative elements (as proven in (12)) as well as (I \u2212 \u03b3P c\u00b5k)\u22121, then Bk has non-negative elements as well. Thus T \u03c0kQk \u2212Qk \u2265 Bk\u22121Bk\u22122 . . . B0(T \u03c00Q0 \u2212Q0) \u2265 0, since we assumed T\u03c00Q0 \u2212Q0 \u2265 0. Thus (16) implies that\nQk+1 \u2212Q\u2217 \u2265 \u03b3P\u03c0 \u2217 (Qk \u2212Q\u2217)\u2212 \u03b5k\u2016Qk\u2016.\nand combining the above with (14) we deduce \u2016Qk+1 \u2212Q\u2217\u2016 \u2264 \u03b3\u2016Qk \u2212Q\u2217\u2016+ \u03b5k\u2016Qk\u2016.\nNow assume that \u03b5k \u2192 0. We first deduce that Qk is bounded. Indeed as soon as \u03b5k < (1 \u2212 \u03b3)/2, we have\n\u2016Qk+1\u2016 \u2264 \u2016Q\u2217\u2016+ \u03b3\u2016Qk \u2212Q\u2217\u2016+ 1\u2212 \u03b3\n2 \u2016Qk\u2016 \u2264 (1 + \u03b3)\u2016Q\u2217\u2016+\n1 + \u03b3\n2 \u2016Qk\u2016.\nThus lim sup \u2016Qk\u2016 \u2264 1+\u03b31\u2212(1+\u03b3)/2\u2016Q \u2217\u2016. Since Qk is bounded, we deduce that lim supQk = Q\u2217."}, {"heading": "D Proof of Theorem 3", "text": "We first prove convergence of the general online algorithm. Theorem 4. Consider the algorithm\nQk+1(x, a) = (1\u2212 \u03b1k(x, a))Qk(x, a) + \u03b1k(x, a)(RkQk(x, a) + \u03c9k(x, a) + \u03c5k(x, a)), (18) and assume that (1) \u03c9k is a centered, Fk-measurable noise term of bounded variance, and (2) \u03c5k is bounded from above by \u03b8k(\u2016Qk\u2016+ 1), where (\u03b8k) is a random sequence that converges to 0 a.s. Then, under the same assumptions as in Theorem 3, we have that Qk \u2192 Q\u2217 almost surely.\nProof. We writeR forRk. Let us prove the result in three steps. Upper bound onRQk \u2212Q\u2217. The first part of the proof is similar to the proof of (14), so we have\nRQk \u2212Q\u2217 \u2264 \u03b3\u2016Qk \u2212Q\u2217\u2016e. (19)\nLower bound onRQk \u2212Q\u2217. Again, similarly to (16) we have RQk \u2212Q\u2217 \u2265 \u03b3\u03bbP\u03c0k\u2227\u00b5k(I \u2212 \u03b3\u03bbP\u03c0k\u2227\u00b5k)\u22121(T \u03c0kQk \u2212Qk)\n+\u03b3P\u03c0 \u2217 (Qk \u2212Q\u2217)\u2212 \u03b5k\u2016Qk\u2016. (20)\nLower-bound on T \u03c0kQk \u2212 Qk. Since the sequence of policies (\u03c0k) is increasingly greedy w.r.t. (Qk), we have\nT \u03c0k+1Qk+1 \u2212Qk+1 \u2265 T \u03c0kQk+1 \u2212Qk+1 = (1\u2212 \u03b1k)T \u03c0kQk + \u03b1kT \u03c0k(RQk + \u03c9k + \u03c5k)\u2212Qk+1 = (1\u2212 \u03b1k)(T \u03c0kQk \u2212Qk) + \u03b1k [ T \u03c0kRQk \u2212RQk + \u03c9\u2032k + \u03c5\u2032k ] ,(21)\nwhere \u03c9\u2032k := (\u03b3P \u03c0k \u2212 I)\u03c9k and \u03c5\u2032k := (\u03b3P\u03c0k \u2212 I)\u03c5k. It is easy to see that both \u03c9\u2032k and \u03c5\u2032k continue to satisfy the assumptions on \u03c9k, and \u03c5k. Now, from the definition of theR operator, we have T \u03c0kRQk \u2212RQk = r + (\u03b3P\u03c0k \u2212 I)RQk\n= r + (\u03b3P\u03c0k \u2212 I) [ Qk + (I \u2212 \u03b3\u03bbP\u03c0k\u2227\u00b5k)\u22121(T \u03c0kQk \u2212Qk) ] = T \u03c0kQk \u2212Qk + (\u03b3P\u03c0k \u2212 I)(I \u2212 \u03b3\u03bbP\u03c0k\u2227\u00b5k)\u22121(T \u03c0kQk \u2212Qk) = \u03b3(P\u03c0k \u2212 \u03bbP\u03c0k\u2227\u00b5k)(I \u2212 \u03b3\u03bbP\u03c0k\u2227\u00b5k)\u22121(T \u03c0kQk \u2212Qk).\nUsing this equality into (21) and writing \u03bek := T \u03c0kQk \u2212Qk, we have \u03bek+1 \u2265 (1\u2212 \u03b1k)\u03bek + \u03b1k [ Bk\u03bek + \u03c9 \u2032 k + \u03c5 \u2032 k ] , (22)\nwhere Bk := \u03b3(P\u03c0k \u2212 \u03bbP\u03c0k\u2227\u00b5k)(I \u2212 \u03b3\u03bbP\u03c0k\u2227\u00b5k)\u22121. The matrix Bk is non-negative but may not be a contraction mapping (the sum of its components per row may be larger than 1). Thus we cannot directly apply Proposition 4.5 of Bertsekas and Tsitsiklis (1996). However, as we have seen in the proof of Theorem 2, the matrix Ak := \u03b3(I \u2212 \u03b3\u03bbP\u03c0k\u2227\u00b5k)\u22121(P\u03c0k \u2212 \u03bbP\u03c0k\u2227\u00b5k) is a \u03b3-contraction mapping. So now we relate Bk to Ak using our assumption that P\u03c0k and P\u03c0k\u2227\u00b5k commute asymptotically, i.e. \u2016P\u03c0kP\u03c0k\u2227\u00b5k \u2212 P\u03c0k\u2227\u00b5kP\u03c0k\u2016 = \u03b7k with \u03b7k \u2192 0. For any (sub)transition matrices U and V , we have\nU(I \u2212 \u03bb\u03b3V )\u22121 = \u2211 t\u22650 (\u03bb\u03b3)tUV t\n= \u2211 t\u22650 (\u03bb\u03b3)t [ t\u22121\u2211 s=0 V s(UV \u2212 V U)V t\u2212s\u22121 + V tU ] = (I \u2212 \u03bb\u03b3V )\u22121U + \u2211 t\u22650 (\u03bb\u03b3)t t\u22121\u2211 s=0 V s(UV \u2212 V U)V t\u2212s\u22121.\nReplacing U by P\u03c0k and V by P\u03c0k\u2227\u00b5k , we deduce\n\u2016Bk \u2212Ak\u2016 \u2264 \u03b3 \u2211 t\u22650 t(\u03bb\u03b3)t\u03b7k = \u03b3 1 (1\u2212 \u03bb\u03b3)2 \u03b7k.\nThus, from (22), \u03bek+1 \u2265 (1\u2212 \u03b1k)\u03bek + \u03b1k [ Ak\u03bek + \u03c9 \u2032 k + \u03c5 \u2032\u2032 k ] , (23)\nwhere \u03c5\u2032\u2032k := \u03c5 \u2032 k+\u03b3 \u2211 t\u22650 t(\u03bb\u03b3) t\u03b7k\u2016\u03bek\u2016 continues to satisfy the assumptions on \u03c5k (since \u03b7k \u2192 0).\nNow, let us define another sequence \u03be\u2032k as follows: \u03be \u2032 0 = \u03be0 and\n\u03be\u2032k+1 = (1\u2212 \u03b1k)\u03be\u2032k + \u03b1k(Ak\u03be\u2032k + \u03c9\u2032k + \u03c5\u2032\u2032k ).\nWe can now apply Proposition 4.5 of Bertsekas and Tsitsiklis (1996) to the sequence (\u03be\u2032k). The matrices Ak are non-negative, and the sum of their coefficients per row is bounded by \u03b3, see (13), thus Ak are \u03b3-contraction mappings and have the same fixed point which is 0. The noise \u03c9\u2032k is centered and Fk-measurable and satisfies the bounded variance assumption, and \u03c5\u2032\u2032k is bounded above by (1 + \u03b3)\u03b8\u2032k(\u2016Qk\u2016+ 1) for some \u03b8\u2032k \u2192 0. Thus limk \u03be\u2032k = 0 almost surely. Now, it is straightforward to see that \u03bek \u2265 \u03be\u2032k for all k \u2265 0. Indeed by induction, let us assume that \u03bek \u2265 \u03be\u2032k. Then\n\u03bek+1 \u2265 (1\u2212 \u03b1k)\u03bek + \u03b1k(Ak\u03bek + \u03c9\u2032k + \u03c5\u2032\u2032k ) \u2265 (1\u2212 \u03b1k)\u03be\u2032k + \u03b1k(Ak\u03be\u2032k + \u03c9\u2032k + \u03c5\u2032\u2032k ) = \u03be\u2032k+1,\nsince all elements of the matrix Ak are non-negative. Thus we deduce that\nlim inf k\u2192\u221e \u03bek \u2265 lim k\u2192\u221e \u03be\u2032k = 0 (24)\nConclusion. Using (24) in (20) we deduce the lower bound:\nlim inf k\u2192\u221e RQk \u2212Q\u2217 \u2265 lim inf k\u2192\u221e \u03b3P\u03c0 \u2217 (Qk \u2212Q\u2217), (25)\nalmost surely. Now combining with the upper bound (19) we deduce that\n\u2016RQk \u2212Q\u2217\u2016 \u2264 \u03b3\u2016Qk \u2212Q\u2217\u2016+O(\u03b5k\u2016Qk\u2016) +O(\u03bek).\nThe last two terms can be incorporated to the \u03c5k(x, a) and \u03c9k(x, a) terms, respectively; we thus again apply Proposition 4.5 of Bertsekas and Tsitsiklis (1996) to the sequence (Qk) defined by (18) and deduce that Qk \u2192 Q\u2217 almost surely.\nIt remains to rewrite the update (8) in the form of (18), in order to apply Theorem 4.\nLet zks,t denote the accumulating trace (Sutton and Barto, 1998):\nzks,t := t\u2211 j=s \u03b3t\u2212j ( t\u220f i=j+1 ci ) I{(xj , aj) = (xs, as)}.\nLet us write Qok+1(xs, as) to emphasize the online setting. Then (8) can be written as Qok+1(xs, as)\u2190 Qok(xs, as) + \u03b1k(xs, as) \u2211 t\u2265s \u03b4\u03c0kt z k s,t, (26)\n\u03b4\u03c0kt := rt + \u03b3E\u03c0kQok(xt+1, \u00b7)\u2212Qok(xt, at),\nUsing our assumptions on finite trajectories, and ci \u2264 1, we can show that: E [\u2211 t\u2265s zks,t|Fk ] < E [ T 2k |Fk ] <\u221e (27)\nwhere Tk denotes trajectory length. Now, let Dk := Dk(xs, as) := \u2211 t\u2265s P{(xt, at) = (xs, as)}. Then, using (27), we can show that the total update is bounded, and rewrite\nE\u00b5k [\u2211 t\u2265s \u03b4\u03c0kt z k s,t ] = Dk(xs, as) ( RkQk(xs, as)\u2212Q(xs, as) ) .\nFinally, using the above, and writing \u03b1k = \u03b1k(xs, as), (26) can be rewritten in the desired form: Qok+1(xs, as)\u2190 (1\u2212 \u03b1\u0303k)Qok(xs, as) + \u03b1\u0303k ( RkQok(xs, as) + \u03c9k(xs, as) + \u03c5k(xs, as) ) , (28)\n\u03c9k(xs, as) := (Dk) \u22121 \u2211 t\u2265s \u03b4\u03c0kt z k s,t \u2212 E\u00b5k \u2211 t\u2265s \u03b4\u03c0kt z k s,t  , \u03c5k(xs, as) := (\u03b1\u0303k)\n\u22121(Qok+1(xs, as)\u2212Qk+1(xs, as)), \u03b1\u0303k := \u03b1kDk.\nIt can be shown that the variance of the noise term \u03c9k is bounded, using (27) and the fact that the reward function is bounded. It follows from Assumptions 1-3 that the modified stepsize sequence (\u03b1\u0303k) satisfies the conditions of Assumption 1. The second noise term \u03c5k(xs, as) measures the difference between online iterates and the corresponding offline values, and can be shown to satisfy the required assumption analogously to the argument in the proof of Prop. 5.2 in Bertsekas and Tsitsiklis (1996). The proof relies on the eligibility coefficients (27) and rewards being bounded, the trajectories being finite, and the conditions on the stepsizes being satisfied.\nWe can thus apply Theorem 4 to (28), and conclude that the iterates Qok \u2192 Q\u2217 as k \u2192\u221e, w.p. 1."}, {"heading": "E Asymptotic commutativity of P \u03c0k and P \u03c0k\u2227\u00b5k", "text": "Lemma 4. Let (\u03c0k) and (\u00b5k) two sequences of policies. If there exists \u03b1 such that for all x, a,\nmin(\u03c0k(a|x), \u00b5k(a|x)) = \u03b1\u03c0k(a|x) + o(1), (29) then the transition matrices P\u03c0k and P\u03c0k\u2227\u00b5k asymptotically commute: \u2016P\u03c0kP\u03c0k\u2227\u00b5k \u2212 P\u03c0k\u2227\u00b5kP\u03c0k\u2016 = o(1).\nProof. For any Q, we have (P\u03c0kP\u03c0k\u2227\u00b5k)Q(x, a) = \u2211 y p(y|x, a) \u2211 b \u03c0k(b|y) \u2211 z p(z|y, b) \u2211 c (\u03c0k \u2227 \u00b5k)(c|z)Q(z, c)\n= \u03b1 \u2211 y p(y|x, a) \u2211 b \u03c0k(b|y) \u2211 z p(z|y, b) \u2211 c \u03c0k(c|z)Q(z, c) + \u2016Q\u2016o(1)\n= \u2211 y p(y|x, a) \u2211 b (\u03c0k \u2227 \u00b5k)(b|y) \u2211 z p(z|y, b) \u2211 c \u03c0k(c|z)Q(z, c) + \u2016Q\u2016o(1)\n= (P\u03c0k\u2227\u00b5kP\u03c0k)Q(x, a) + \u2016Q\u2016o(1). Lemma 5. Let (\u03c0Qk) a sequence of (deterministic) greedy policies w.r.t. a sequence (Qk). Let (\u03c0k) a sequence of policies that are \u03b5k away from (\u03c0Qk), in the sense that, for all x,\n\u2016\u03c0k(\u00b7|x)\u2212 \u03c0Qk(x)\u20161 := 1\u2212 \u03c0k(\u03c0Qk(x)|x) + \u2211\na 6=\u03c0Qk (x)\n\u03c0k(a|x) \u2264 \u03b5k.\nLet (\u00b5k) a sequence of policies defined by:\n\u00b5k(a|x) = \u03b1\u00b5(a|x)\n1\u2212 \u00b5(\u03c0Qk(x)|x) I{a 6= \u03c0Qk(x)}+ (1\u2212 \u03b1)I{a = \u03c0Qk(x)}, (30)\nfor some arbitrary policy \u00b5 and \u03b1 \u2208 [0, 1]. Assume \u03b5k \u2192 0. Then the transition matrices P\u03c0k and P\u03c0k\u2227\u00b5k asymptotically commute.\nProof. The intuition is that asymptotically \u03c0k gets very close to the deterministic policy \u03c0Qk . In that case, the minimum distribution (\u03c0k \u2227 \u00b5k)(\u00b7|x) puts a mass close to 1\u2212 \u03b1 on the greedy action \u03c0Qk(x), and no mass on other actions, thus (\u03c0k \u2227 \u00b5k) gets very close to (1 \u2212 \u03b1)\u03c0k, and Lemma 4 applies (with multiplicative constant 1\u2212 \u03b1). Indeed, from our assumption that \u03c0k is \u03b5-away from \u03c0Qk we have:\n\u03c0k(\u03c0Qk(x)|x) \u2265 1\u2212 \u03b5k, and \u03c0k(a 6= \u03c0Qk(x)|x) \u2264 \u03b5k.\nWe deduce that\n(\u03c0k \u2227 \u00b5k)(\u03c0Qk(x)|x) = min(\u03c0k(\u03c0Qk(x)|x), 1\u2212 \u03b1) = 1\u2212 \u03b1+O(\u03b5k) = (1\u2212 \u03b1)\u03c0k(\u03c0Qk(x)|x) +O(\u03b5k),\nand\n(\u03c0k \u2227 \u00b5k)(a 6= \u03c0Qk(x)|x) = O(\u03b5k) = (1\u2212 \u03b1)\u03c0k(a|x) +O(\u03b5k).\nThus Lemma 4 applies (with a multiplicative constant 1 \u2212 \u03b1) and P\u03c0k and P\u03c0k\u2227\u00b5k asymptotically commute."}, {"heading": "F Experimental Methods", "text": "Although our experiments\u2019 learning problem closely matches the DQN setting used by Mnih et al. (2015) (i.e. single-thread off-policy learning with large replay memory), we conducted our trials in the multi-threaded, CPU-based framework of Mnih et al. (2016), obtaining ample result data from affordable CPU resources. Key differences from the DQN are as follows. Sixteen threads with private environment instances train simultaneously; each infers with and finds gradients w.r.t. a local copy of the network parameters; gradients then update a \u201cmaster\u201d parameter set and local copies are refreshed. Target network parameters are simply shared globally. Each thread has private replay memory holding 62,500 transitions (1/16th of DQN\u2019s total replay capacity). The optimiser is unchanged from (Mnih et al., 2016): \u201cShared RMSprop\u201d with step size annealing to 0 over 3\u00d7 108 environment frames (summed over threads). Exploration parameter (\u03b5) behaviour differs slightly: every 50,000 frames, threads switch randomly (probability 0.3, 0.4, and 0.3 respectively) between three schedules (anneal \u03b5 from 1 to 0.5, 0.1, or 0.01 over 250,000 frames), starting new schedules at the intermediate positions where they left old ones.1\nOur experiments comprise 60 Atari 2600 games in ALE (Bellemare et al., 2013), with \u201clife\u201d loss treated as episode termination. The control, minibatched (64 transitions/minibatch) one-step Qlearning as in (Mnih et al., 2015), shows performance comparable to DQN in our multi-threaded setup. Retrace, TB, and Q* runs use minibatches of four 16-step sequences (again 64 transitions/minibatch) and the current exploration policy as the target policy \u03c0. All trials clamp rewards into [\u22121, 1]. In the control, Q-function targets are clamped into [\u22121, 1] prior to gradient calculation; analogous quantities in the multi-step algorithms are clamped into [\u22121, 1], then scaled (divided by) the sequence length. Coarse, then fine logarithmic parameter sweeps on the games Asterix, Breakout, Enduro, Freeway, H.E.R.O, Pong, Q*bert, and Seaquest yielded step sizes of 0.0000439 and 0.0000912, and RMSprop regularisation parameters of 0.001 and 0.0000368, for control and multistep algorithms respectively. Reported performance averages over four trials with different random seeds for each experimental configuration.\n1We evaluated a DQN-style single schedule for \u03b5, but our multi-schedule method, similar to the one used by Mnih et al., yielded improved performance in our multi-threaded setting."}], "references": [{"title": "The Arcade Learning Environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, 47:253\u2013279.", "citeRegEx": "Bellemare et al\\.,? 2013", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas and Tsitsiklis,? 1996", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1996}, {"title": "Off-policy learning with eligibility traces: A survey", "author": ["M. Geist", "B. Scherrer"], "venue": "The Journal of Machine Learning Research, 15(1):289\u2013333.", "citeRegEx": "Geist and Scherrer,? 2014", "shortCiteRegEx": "Geist and Scherrer", "year": 2014}, {"title": "Generalized emphatic temporal difference learning: Bias-variance analysis", "author": ["A. Hallak", "A. Tamar", "R. Munos", "S. Mannor"], "venue": "arXiv:1509.05172.", "citeRegEx": "Hallak et al\\.,? 2015", "shortCiteRegEx": "Hallak et al\\.", "year": 2015}, {"title": "Q(\u03bb) with off-policy corrections", "author": ["A. Harutyunyan", "M.G. Bellemare", "T. Stepleton", "R. Munos"], "venue": "arXiv:1602.04951.", "citeRegEx": "Harutyunyan et al\\.,? 2016", "shortCiteRegEx": "Harutyunyan et al\\.", "year": 2016}, {"title": "Bias-variance error bounds for temporal difference updates", "author": ["M.J. Kearns", "S.P. Singh"], "venue": "Conference on Computational Learning Theory, pages 142\u2013147.", "citeRegEx": "Kearns and Singh,? 2000", "shortCiteRegEx": "Kearns and Singh", "year": 2000}, {"title": "Scaling up reinforcement learning for robot control", "author": ["L. Lin"], "venue": "Machine Learning: Proceedings of the Tenth International Conference, pages 182\u2013189.", "citeRegEx": "Lin,? 1993", "shortCiteRegEx": "Lin", "year": 1993}, {"title": "Off-policy learning based on weighted importance sampling with linear computational complexity", "author": ["A.R. Mahmood", "R.S. Sutton"], "venue": "Conference on Uncertainty in Artificial Intelligence.", "citeRegEx": "Mahmood and Sutton,? 2015", "shortCiteRegEx": "Mahmood and Sutton", "year": 2015}, {"title": "Emphatic temporal-difference learning", "author": ["A.R. Mahmood", "H. Yu", "M. White", "R.S. Sutton"], "venue": "arXiv:1507.01569.", "citeRegEx": "Mahmood et al\\.,? 2015", "shortCiteRegEx": "Mahmood et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "arXiv:1602.01783.", "citeRegEx": "Mnih et al\\.,? 2016", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Off-policy temporal-difference learning with function approximation", "author": ["D. Precup", "R.S. Sutton", "S. Dasgupta"], "venue": "International Conference on Machine Laerning, pages 417\u2013424.", "citeRegEx": "Precup et al\\.,? 2001", "shortCiteRegEx": "Precup et al\\.", "year": 2001}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning.", "citeRegEx": "Precup et al\\.,? 2000", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons, Inc., New York, NY, USA, 1st edition.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Schaul et al\\.,? 2016", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Convergence results for singlestep on-policy reinforcement-learning algorithms", "author": ["S. Singh", "T. Jaakkola", "M.L. Littman", "C. Szepesv\u00e1ri"], "venue": "Machine Learning, 38(3):287\u2013308.", "citeRegEx": "Singh et al\\.,? 2000", "shortCiteRegEx": "Singh et al\\.", "year": 2000}, {"title": "Reinforcement learning: An introduction, volume 116", "author": ["R. Sutton", "A. Barto"], "venue": "Cambridge Univ Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine learning, 3(1):9\u201344.", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems 8.", "citeRegEx": "Sutton,? 1996", "shortCiteRegEx": "Sutton", "year": 1996}, {"title": "On the convergence of optimistic policy iteration", "author": ["J.N. Tsitsiklis"], "venue": "Journal of Machine", "citeRegEx": "Tsitsiklis,? 2003", "shortCiteRegEx": "Tsitsiklis", "year": 2003}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "C.J.C. H"], "venue": "Learning Research,", "citeRegEx": "Watkins and H.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and H.", "year": 1989}], "referenceMentions": [{"referenceID": 18, "context": "One fundamental trade-off in reinforcement learning lies in the definition of the update target: should one estimate Monte Carlo returns or bootstrap from an existing Q-function? Return-based methods (where return refers to the sum of discounted rewards \u2211 t \u03b3 rt) offer some advantages over value bootstrap methods: they are better behaved when combined with function approximation, and quickly propagate the fruits of exploration (Sutton, 1996).", "startOffset": 431, "endOffset": 445}, {"referenceID": 12, "context": "Alternatively, the Tree-backup (TB) (\u03bb) algorithm (Precup et al., 2000) tolerates arbitrary target/behavior discrepancies by scaling information (here called traces) from future temporal differences by the product of target policy probabilities.", "startOffset": 50, "endOffset": 71}, {"referenceID": 6, "context": "Retrace(\u03bb) can learn from full returns retrieved from past policy data, as in the context of experience replay (Lin, 1993), which has returned to favour with advances in deep reinforcement learning (Mnih ar X iv :1 60 6.", "startOffset": 111, "endOffset": 122}, {"referenceID": 4, "context": "We start from the recent work of Harutyunyan et al. (2016), who show that naive off-policy policy evaluation, without correcting for the \u201coff-policyness\u201d of a trajectory, still converges to the desired Q value function provided the behavior \u03bc and target \u03c0 policies are not too far apart (the maximum allowed distance depends on the \u03bb parameter).", "startOffset": 33, "endOffset": 59}, {"referenceID": 15, "context": "To the best of our knowledge, this is the first online return-based off-policy control algorithm which does not require the GLIE (Greedy in the Limit with Infinite Exploration) assumption (Singh et al., 2000).", "startOffset": 188, "endOffset": 208}, {"referenceID": 16, "context": "In addition, we provide as a corollary the first proof of convergence of Watkins\u2019 Q(\u03bb) (see, e.g., Watkins, 1989; Sutton and Barto, 1998).", "startOffset": 87, "endOffset": 137}, {"referenceID": 0, "context": "Finally, we illustrate the significance of Retrace(\u03bb) in a deep learning setting by applying it to the suite of Atari 2600 games provided by the Arcade Learning Environment (Bellemare et al., 2013).", "startOffset": 173, "endOffset": 197}, {"referenceID": 13, "context": "(3) Its fixed point is Q\u2217, the unique optimal value function (Puterman, 1994).", "startOffset": 61, "endOffset": 77}, {"referenceID": 17, "context": "Return-based Operators: The \u03bb-return extension (Sutton, 1988) of both (2) and (3) considers exponentially weighted sums of n-step returns: T \u03c0 \u03bb Q := (1\u2212 \u03bb) \u2211", "startOffset": 47, "endOffset": 61}, {"referenceID": 16, "context": "At one extreme (\u03bb = 0) we have the Bellman operator T \u03c0 \u03bb=0Q = T Q, while at the other (\u03bb = 1) we have the policy evaluation operator T \u03c0 \u03bb=1Q = Q which can be estimated using Monte Carlo methods (Sutton and Barto, 1998).", "startOffset": 196, "endOffset": 220}, {"referenceID": 5, "context": "Intermediate values of \u03bb trade off estimation bias with sample variance (Kearns and Singh, 2000).", "startOffset": 72, "endOffset": 96}, {"referenceID": 16, "context": "By extension of the idea of eligibility traces (Sutton and Barto, 1998), we informally call the coefficients (cs) the traces of the operator.", "startOffset": 47, "endOffset": 71}, {"referenceID": 2, "context": "Importance sampling is the simplest way to correct for the discrepancy between \u03bc and \u03c0 when learning from off-policy returns (Precup et al., 2000, 2001; Geist and Scherrer, 2014).", "startOffset": 125, "endOffset": 178}, {"referenceID": 7, "context": "It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u03bc(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u03bc(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015).", "startOffset": 247, "endOffset": 316}, {"referenceID": 8, "context": "It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u03bc(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u03bc(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015).", "startOffset": 247, "endOffset": 316}, {"referenceID": 3, "context": "It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u03bc(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u03bc(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015).", "startOffset": 247, "endOffset": 316}, {"referenceID": 2, "context": ", 2000, 2001; Geist and Scherrer, 2014). The off-policy correction uses the product of the likelihood ratios between \u03c0 and \u03bc. Notice that the RQ operator (4) defined with this choice of (cs) yields Q for any Q. For Q = 0 we recover the basic IS estimate \u2211 t\u22650 \u03b3 t (\u220ft s=1 cs ) rt, thus (4) can be seen as a variance reduction technique (with a baseline Q). It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u03bc(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u03bc(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015). Off-policy Q(\u03bb) and Q\u2217(\u03bb): cs = \u03bb. A recent alternative proposed by Harutyunyan et al. (2016) introduces an off-policy correction based on a Q-baseline (instead of correcting the probability of the sample path like in IS).", "startOffset": 14, "endOffset": 769}, {"referenceID": 2, "context": ", 2000, 2001; Geist and Scherrer, 2014). The off-policy correction uses the product of the likelihood ratios between \u03c0 and \u03bc. Notice that the RQ operator (4) defined with this choice of (cs) yields Q for any Q. For Q = 0 we recover the basic IS estimate \u2211 t\u22650 \u03b3 t (\u220ft s=1 cs ) rt, thus (4) can be seen as a variance reduction technique (with a baseline Q). It is well known that IS estimates can suffer from large \u2013 even possibly infinite \u2013 variance (mainly due to the variance of the product \u03c0(a1|x1) \u03bc(a1|x1) \u00b7 \u00b7 \u00b7 \u03c0(at|xt) \u03bc(at|xt) ), which has motivated further variance reduction techniques such as (Mahmood and Sutton, 2015; Mahmood et al., 2015; Hallak et al., 2015). Off-policy Q(\u03bb) and Q\u2217(\u03bb): cs = \u03bb. A recent alternative proposed by Harutyunyan et al. (2016) introduces an off-policy correction based on a Q-baseline (instead of correcting the probability of the sample path like in IS). This approach, called Q(\u03bb) and Q\u2217(\u03bb) for policy evaluation and control, respectively, corresponds to the choice cs = \u03bb. It offers the advantage of avoiding the blow-up of the variance of the product of ratios encountered with IS. Interestingly, this operator contracts around Q provided that \u03bc and \u03c0 are sufficiently close to each other. Defining \u03b5 := maxx \u2016\u03c0(\u00b7|x)\u2212\u03bc(\u00b7|x)\u20161 the amount of \u201coff-policyness\u201d, the authors prove that the operator defined by (4) with cs = \u03bb is a contraction mapping around Q for \u03bb < 1\u2212\u03b3 \u03b3\u03b5 , and around Q \u2217 for the worst case of \u03bb < 1\u2212\u03b3 2\u03b3 . Unfortunately, Q(\u03bb) requires knowledge of \u03b5, and the condition for Q\u2217(\u03bb) is very conservative. Neither Q(\u03bb), nor Q\u2217(\u03bb) are safe as they do not guarantee convergence for arbitrary \u03c0 and \u03bc. Tree-backup (TB) (\u03bb): cs = \u03bb\u03c0(as|xs). The TB(\u03bb) algorithm of Precup et al. (2000) corrects for the target/behaviour discrepancy by multiplying each term of the sum by the product of target policy probabilities.", "startOffset": 14, "endOffset": 1738}, {"referenceID": 16, "context": "We analyze the algorithms in the every visit form (Sutton and Barto, 1998), which is the more practical generalization of the first-visit form.", "startOffset": 50, "endOffset": 74}, {"referenceID": 1, "context": "The proof extends similar convergence proofs of TD(\u03bb) by Bertsekas and Tsitsiklis (1996) and of optimistic policy iteration by Tsitsiklis (2003), and is provided in the appendix.", "startOffset": 57, "endOffset": 89}, {"referenceID": 1, "context": "The proof extends similar convergence proofs of TD(\u03bb) by Bertsekas and Tsitsiklis (1996) and of optimistic policy iteration by Tsitsiklis (2003), and is provided in the appendix.", "startOffset": 57, "endOffset": 145}, {"referenceID": 16, "context": "\u03b5k = 0), we deduce that Watkins\u2019 Q(\u03bb) (e.g., Watkins, 1989; Sutton and Barto, 1998) converges a.", "startOffset": 38, "endOffset": 83}, {"referenceID": 6, "context": "To validate our theoretical results, we employ Retrace(\u03bb) in an experience replay (Lin, 1993) setting, where sample transitions are stored within a large but bounded replay memory and subsequently replayed as if they were new experience.", "startOffset": 82, "endOffset": 93}, {"referenceID": 6, "context": "To validate our theoretical results, we employ Retrace(\u03bb) in an experience replay (Lin, 1993) setting, where sample transitions are stored within a large but bounded replay memory and subsequently replayed as if they were new experience. Naturally, older data in the memory is usually drawn from a policy which differs from the current policy, offering an excellent point of comparison for the algorithms presented in Section 2. Our agent adapts the DQN architecture of Mnih et al. (2015) to replay short sequences from the memory (details in Appendix F) instead of single transitions.", "startOffset": 83, "endOffset": 489}, {"referenceID": 0, "context": "We compare our algorithms\u2019 performance on 60 different Atari 2600 games in the Arcade Learning Environment (Bellemare et al., 2013) using Bellemare et al.", "startOffset": 107, "endOffset": 131}], "year": 2016, "abstractText": "In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace(\u03bb), with three desired properties: (1) low variance; (2) safety, as it safely uses samples collected from any behaviour policy, whatever its degree of \u201coff-policyness\u201d; and (3) efficiency, as it makes the best use of samples collected from near on-policy behaviour policies. We analyse the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. To our knowledge, this is the first return-based off-policy control algorithm converging a.s. to Q\u2217 without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins\u2019 Q(\u03bb), which was still an open problem. We illustrate the benefits of Retrace(\u03bb) on a standard suite of Atari 2600 games. One fundamental trade-off in reinforcement learning lies in the definition of the update target: should one estimate Monte Carlo returns or bootstrap from an existing Q-function? Return-based methods (where return refers to the sum of discounted rewards \u2211 t \u03b3 rt) offer some advantages over value bootstrap methods: they are better behaved when combined with function approximation, and quickly propagate the fruits of exploration (Sutton, 1996). On the other hand, value bootstrap methods are more readily applied to off-policy data, a common use case. In this paper we show that learning from returns need not be at cross-purposes with off-policy learning. We start from the recent work of Harutyunyan et al. (2016), who show that naive off-policy policy evaluation, without correcting for the \u201coff-policyness\u201d of a trajectory, still converges to the desired Q value function provided the behavior \u03bc and target \u03c0 policies are not too far apart (the maximum allowed distance depends on the \u03bb parameter). Their Q(\u03bb) algorithm learns from trajectories generated by \u03bc simply by summing discounted off-policy corrected rewards at each time step. Unfortunately, the assumption that \u03bc and \u03c0 are close is restrictive, as well as difficult to uphold in the control case, where the target policy is always greedy with respect to the current Q-function. In that sense this algorithm is not safe: it does not handle the case of arbitrary \u201coff-policyness\u201d. Alternatively, the Tree-backup (TB) (\u03bb) algorithm (Precup et al., 2000) tolerates arbitrary target/behavior discrepancies by scaling information (here called traces) from future temporal differences by the product of target policy probabilities. TB(\u03bb) is not efficient in the \u201cnear on-policy\u201d case (similar \u03bc and \u03c0), though, as traces may be cut prematurely, blocking learning from full returns. In this work, we express several off-policy, return-based algorithms in a common form. From this we derive an improved algorithm, Retrace(\u03bb), which is both safe and efficient, enjoying convergence guarantees for off-policy policy evaluation and \u2013 more importantly \u2013 for the control setting. Retrace(\u03bb) can learn from full returns retrieved from past policy data, as in the context of experience replay (Lin, 1993), which has returned to favour with advances in deep reinforcement learning (Mnih ar X iv :1 60 6. 02 64 7v 1 [ cs .L G ] 8 J un 2 01 6 et al., 2015; Schaul et al., 2016). Off-policy learning is also desirable for exploration, since it allows the agent to deviate from the target policy currently under evaluation. To the best of our knowledge, this is the first online return-based off-policy control algorithm which does not require the GLIE (Greedy in the Limit with Infinite Exploration) assumption (Singh et al., 2000). In addition, we provide as a corollary the first proof of convergence of Watkins\u2019 Q(\u03bb) (see, e.g., Watkins, 1989; Sutton and Barto, 1998). Finally, we illustrate the significance of Retrace(\u03bb) in a deep learning setting by applying it to the suite of Atari 2600 games provided by the Arcade Learning Environment (Bellemare et al., 2013). 1 Notation We consider an agent interacting with a Markov Decision Process (X ,A, \u03b3, P, r). X is a finite state space, A the action space, \u03b3 \u2208 [0, 1) the discount factor, P the transition function mapping stateaction pairs (x, a) \u2208 X \u00d7A to distributions over X , and r : X \u00d7A \u2192 [\u2212RMAX, RMAX] is the reward function. For notational simplicity we will consider a finite action space, but the case of infinite \u2013 possibly continuous \u2013 action space can be handled by the Retrace(\u03bb) algorithm as well. A policy \u03c0 is a mapping from X to a distribution over A. A Q-function Q maps each state-action pair (x, a) to a value in R; in particular, the reward r is a Q-function. For a policy \u03c0 we define the operator P: (PQ)(x, a) := \u2211", "creator": "TeX"}}}