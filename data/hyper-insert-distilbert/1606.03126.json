{"id": "1606.03126", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2016", "title": "Key-Value Memory Networks for Directly Reading Documents", "abstract": "delaying directly reading documents - and directly being able learners to answer questions stems from applying them all is a recurring key validation problem. to partly avoid its highly inherent reporting difficulty, question answering ( technical qa ) has been directed loosely towards readers using knowledge delivery bases ( kbs ) instead, which alone has proven effective. unfortunately kbs suffer from limitations often though being too restrictive, as considering the schema cannot fully support many certain types of multiple answers, and unfortunately too sparse, e. g. wikipedia xml contains much more information than freebase. in this transformation work we introduce radically a new coding method, key - value memory networks, that makes reading data documents more widely viable by critically utilizing different metadata encodings in mastering the addressing and output stages of implementing the memory encoding read operation. to critically compare using kbs, information extraction or relating wikipedia referenced documents directly in a centralized single framework system we construct an associated analysis tool, movieqa, a visual qa usable dataset for in learning the media domain geography of animation movies. our method closes the definition gap between all three settings. moreover it consequently also achieves overall state - some of - the - art computing results on confirming the existing wikiqa reference benchmark.", "histories": [["v1", "Thu, 9 Jun 2016 21:33:55 GMT  (110kb,D)", "http://arxiv.org/abs/1606.03126v1", null], ["v2", "Mon, 10 Oct 2016 20:14:10 GMT  (122kb,D)", "http://arxiv.org/abs/1606.03126v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander h miller", "adam fisch", "jesse dodge", "amir-hossein karimi", "antoine bordes", "jason weston"], "accepted": true, "id": "1606.03126"}, "pdf": {"name": "1606.03126.pdf", "metadata": {"source": "CRF", "title": "Key-Value Memory Networks for Directly Reading Documents", "authors": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston"], "emails": ["ahm@fb.com", "afisch@fb.com", "jessedodge@fb.com", "ahkarimi@fb.com", "abordes@fb.com", "jase@fb.com"], "sections": [{"heading": null, "text": "Directly reading documents and being able to answer questions from them is a key problem. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs suffer from often being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, MOVIEQA, a QA dataset in the domain of movies. Our method closes the gap between all three settings. It also achieves state-of-the-art results on the existing WIKIQA benchmark."}, {"heading": "1 Introduction", "text": "Question answering (QA) has been a long standing research problem in natural language processing, with the first systems attempting to answer questions by directly reading documents (Voorhees and Tice, 2000). The development of large-scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014).\nUnfortunately, KBs have intrinsic limitations such as their inevitable incompleteness and fixed schemas that cannot support all varieties of answers. Since information extraction (IE) (Craven et al., 2000), intended to fill in missing information in KBs, is not accurate nor reliable enough, collections of raw textual resources and documents, such as Wikipedia, will always contain more information. As a result, even if KBs can be satisfactory for closed-domain problems, they are unlikely to scale up to answer general questions on any topic. Starting from this observation, in this work we study the problem of answering by directly reading documents.\nRetrieving answers directly from text is harder than from KBs because information is far less structured, is indirectly and ambiguously expressed, and is usually scattered across multiple documents. This explains why using a satisfactory KB \u2013 typically only available in closed domains \u2013 is preferred over raw text. We postulate that before trying to provide answers that are not in KBs, document-based QA systems should first reach KB-based systems\u2019 performance in such closed domains, where clear comparison and evaluation is possible. To this end, this paper introduces MOVIEQA, a new analysis tool that allows for measuring the performance of QA systems when the knowledge source is switched from a KB to unstructured documents. MOVIEQA contains\u223c100k questions in the movie domain, and was designed to be answerable by using either a perfect KB (based on OMDb1), Wikipedia pages or an imperfect KB obtained through running an engineered IE pipeline on those pages.\n1http://www.omdbapi.com\nar X\niv :1\n60 6.\n03 12\n6v 1\n[ cs\n.C L\n] 9\nJ un\n2 01\nTo bridge the gap between using a KB and reading documents directly, we still lack appropriate machine learning algorithms. In this work we propose the Key-Value Memory Network (KV-MemNN), a new neural network architecture that generalizes the original Memory Network (Sukhbaatar et al., 2015) and can work with either knowledge source. The KV-MemNN performs QA by first storing facts in a key-value structured memory before reasoning on them in order to predict an answer. The memory is designed so that the model learns to use keys to address relevant memories with respect to the question, whose corresponding values are subsequently returned. This structure allows the model to encode prior knowledge for the considered task and to leverage possibly complex transforms between keys and values, whilst still being trained using standard backpropagation via stochastic gradient descent.\nOur experiments on MOVIEQA indicate that, thanks to its key-value memory, the KV-MemNN consistently outperforms the original Memory Network, and closes the gap between answering from a human-annotated KB, an automatically extracted KB or from directly reading Wikipedia. We confirm our findings on WIKIQA (Yang et al., 2015), another Wikipedia-based QA benchmark where no KB is available, where we demonstrate that KV-MemNN can reach state-of-the-art results \u2013 surpassing the most recent attention-based neural network models."}, {"heading": "2 Related Work", "text": "Early QA systems were based on information retrieval and were designed to return snippets of text containing an answer (Voorhees and Tice, 2000; Banko et al., 2002), with limitations in terms of question complexity and response coverage. The creation of large-scale KBs (Auer et al., 2007; Bollacker et al., 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014; Yih et al., 2015) that can return precise answers to complicated compositional questions. Due to the sparsity of KB data, however, the main challenge shifts from finding answers to developing efficient information extraction methods to populate KBs automatically (Craven et al., 2000; Carlson et al., 2010) \u2013 which is not an easy problem.\nFor this reason, recent initiatives are returning to\nthe original setting of directly answering from text using datasets like TRECQA (Wang et al., 2007), based on classical TREC resources (Voorhees et al., 1999), and WIKIQA (Yang et al., 2015), extracted from Wikipedia. Both benchmarks are organized around the task of answer sentence selection, where a system must identify the sentence containing the correct answer in a collection of documents, but need not return the actual answer as a KB-based system would do. Unfortunately, these datasets are very small (hundreds of examples) and, because of their answer selection setting, do not offer the option to directly compare answering from a KB against answering from pure text. Using similar resources as the dialog dataset of Dodge et al. (2016), MOVIEQA, the new benchmark presented here, addresses both deficiencies by providing a substantial corpus of questionanswer pairs that can be answered by either using a KB or a corresponding set of documents.\nEven though standard pipeline QA systems like AskMR (Banko et al., 2002) have been recently revisited (Tsai et al., 2015), the best published results on TRECQA and WIKIQA have been obtained by either convolutional neural networks (Santos et al., 2016; Yin and Sch\u00fctze, 2015; Wang et al., 2016) or recurrent neural networks (Miao et al., 2015) \u2013 both usually with attention mechanisms inspired by (Bahdanau et al., 2015). In this work, we introduce KV-MemNNs, a Memory Network model that operates a symbolic memory structured as (key, value) pairs. Such structured memory is not employed in any existing attention-based neural network architecture for QA. As we will show, it gives the model greater flexibility for encoding knowledge sources and helps shrink the gap between directly reading documents and answering from a KB."}, {"heading": "3 Key-Value Memory Networks", "text": "The Key-Value Memory Network model is based on the Memory Network (MemNNs) model (Weston et al., 2015a; Sukhbaatar et al., 2015) which has proven useful for a variety of document reading and question answering tasks: for reading children\u2019s books and answering questions about them (Hill et al., 2015), for complex reasoning over simulated stories (Weston et al., 2015b) and for utilizing KBs to answer questions (Bordes et al., 2015).\nKey-value paired memories are a generalization\nof the way context (e.g. knowledge bases or documents to be read) are stored in memory. The lookup (addressing) stage is based on the key memory and the reading stage (giving the returned result) is the value memory. This gives both (i) greater flexibility for the practitioner to encode prior knowledge about their task; and (ii) more effective power in the model via nontrivial transforms between key and value. The key should be designed with features to help match it to the question, while the value should be designed with features to help match it to the response (answer). An important property of the model is that the entire model can be trained with key-value transforms whilst still using standard backpropagation via stochastic gradient descent."}, {"heading": "3.1 Model Description", "text": "Our model is based on the end-to-end Memory Network architecture of Sukhbaatar et al. (2015). A high-level view of both models is as follows: one defines a memory, which is a possibly very large array of slots which can encode both long-term and shortterm context. At test time one is given a query (e.g. the question in QA tasks), which is used to iteratively address and read from the memory (these iterations are also referred to as \u201chops\u201d) looking for relevant information to answer the question. Finally, after collecting information from the memory, the original query and the retrieved context are combined as features to predict the final response. Figure 1 illustrates the KV-MemNN model architecture.\nIn KV-MemNNs we define the memory slots as pairs of vectors (k1, v1) . . . , (kM , vM ) and denote the question x. The addressing and reading of the\nmemory involves three steps:\n\u2022 Key Hashing: the question can be used to preselect a small subset of the possibly large array. This is done using an inverted index that finds a subset (kh1 , vh1), . . . , (khN , vhN ) of memories of size N where the key shares at least one word with the question with frequency < F = 1000 (to ignore stop words), following Dodge et al. (2016). More sophisticated retrieval schemes could be used here, see e.g. (Manning et al., 2008), but this simple scheme sufficed for our needs in the following experiments.\n\u2022 Key Addressing: during addressing, each candidate memory is assigned a relevance probability by comparing the question to each key:\nphi = Softmax(A\u03a6X(x) \u00b7A\u03a6K(khi))\nwhere \u03a6\u00b7 are feature maps of dimension D, A is a d\u00d7D matrix and Softmax(zi) = ezi/ \u2211 j e\nzj . We discuss choices of feature map in Sec. 3.2.\n\u2022 Value Reading: in the final reading step, the values of the memories are read by taking their weighted sum using the addressing probabilities, and the vector o is returned:\no = \u2211 i phiA\u03a6V (vhi) .\nThe memory access process is conducted by the \u201ccontroller\u201d neural network using q = A\u03a6X(x) as the query. After receiving the result o, the query is updated with q2 = R1(q + o) where R is a d \u00d7 d\nmatrix. The memory access is then repeated (specifically, only the addressing and reading steps, but not the hashing), using a different matrix Ri on each hop. The motivation for this is that new evidence can be combined into the query to focus on and retrieve more pertinent information in subsequent accesses. Finally, after a fixed number H hops, the resulting state of the controller is used to compute a final prediction over the possible outputs:\na\u0302 = argmaxi=1,...,CSoftmax(q > H+1B\u03a6Y (yi))\nwhere yi are the possible candidate outputs, e.g. all the entities in the KB, or all possible candidate answer sentences in the case of a dataset like WIKIQA (see Sec. 5.2). The d\u00d7D matrix B can also be constrained to be identical to A. The whole network is trained end-to-end, and the model learns to perform the iterative accesses to output the desired target a by minimizing a standard cross-entropy loss between a\u0302 and the correct answer a. Backpropagation and stochastic gradient descent are thus used to learn the matrices A,B and R1, . . . RH .\nTo obtain the standard End-To-End Memory Network of Sukhbaatar et al. (2015) one can simply set the key and value to be the same for all memories. Hashing was not used in that paper, but is important for computational efficiency for large memory sizes, as already shown in Dodge et al. (2016). We will now go on to describe specific applications of key-value memories for the task of reading KBs or documents."}, {"heading": "3.2 Key-Value Memories", "text": "There are a variety of ways to employ key-value memories that can have important effects on overall performance. The ability to encode prior knowledge in this way is an important component of KV-MemNNs, and we are free to define \u03a6X ,\u03a6Y ,\u03a6K and \u03a6V for the query, answer, keys and values respectively. We now describe several possible variants of \u03a6K and \u03a6V that we tried in our experiments, for simplicity we kept \u03a6X and \u03a6Y fixed as bag-of-words representations.\nKB Triple Knowledge base entries have a structure of triple \u201csubject relation object\u201d (see Table 1 for examples). The representation we consider is simple: the key is composed of the left-hand side entity (subject) and the relation, and the value is the right-hand side entity (object). We double the KB and consider\nthe reversed relation as well (e.g. we now have two triples \u201cBlade Runner directed_by Ridley Scott\u201d and \u201cRidley Scott !directed_by Blade Runner\u201d where !directed_by is a different entry in the dictionary than directed_by). Having the entry both ways round is important for answering different kinds of questions (\u201cWho directed Blade Runner?\u201d vs. \u201cWhat did Ridley Scott direct?\u201d). For a standard MemNN that does not have key-value pairs the whole triple has to be encoded into the same memory slot.\nSentence Level For representing a document, one can split it up into sentences, with each memory slot encoding one sentence. Both the key and the value encode the entire sentence as a bag-of-words. As, in this case, the key and value are the same this is identical to a standard MemNN and this approach has been used in several papers (Weston et al., 2015b; Dodge et al., 2016).\nWindow Level Documents are split up into windows of W words; in our tasks we only include windows where the center word is an entity. Windows are represented using bag-of-words. Window representations for MemNNs have been shown to work well previously (Hill et al., 2015). However, in Key-Value MemNNs we encode the key as the entire window, and the value as only the center word, which is not possible in the MemNN architecture. This makes sense because the entire window is more likely to be pertinent as a match for the question (as the key), whereas the entity at the center is more pertinent as a match for the answer (as the value). We will compare these approaches in our experiments.\nWindow + Center Encoding Instead of representing the window as a pure bag-of-words, thus mixing the window center with the rest of the window, we can also encode them with different features. Here, we double the size of the dictionary D and encode the center of the window and the value using the second dictionary. This should help the model pick out the relevance of the window center (more related to the answer) as compared to the words either side of it (more related to the question).\nWindow + Title The title of a document is commonly the answer to a question that relates to the text it contains. For example \u201cWhat did Harrison Ford star in?\u201d can be (partially) answered by the\nWikipedia document with the title \u201cBlade Runner\u201d. For this reason, we also consider a representation where the key is the word window as before, but the value is the document title. We also keep all the standard (window, center) key-value pairs from the window-level representation as well, thus doubling the number of memory slots in comparison. To differentiate the two keys with different values we add an extra feature \u201c_window_\u201d or \u201c_title_\u201d to the key, depending on the value. The \u201c_title_\u201d version also includes the actual movie title in the key. This representation can be combined with center encoding."}, {"heading": "4 The MovieQA Benchmark", "text": "The MOVIEQA benchmark consists of questionanswer pairs in the domain of movies. It was built with the following goals in mind: (i) machine learning techniques should have ample training examples for learning; and (ii) one can analyze easily the performance of different representations of knowledge and break down the results by question type. The dataset can be downloaded from http://fb.ai/babi."}, {"heading": "4.1 Knowledge Representations", "text": "We construct three forms of knowledge representation: (i) Doc: raw Wikipedia documents consisting of the pages of the movies mentioned; (ii) KB: a classical graph-based KB consisting of entities and relations created from the Open Movie Database (OMDb) and MovieLens; and (iii) IE: information extraction performed on the Wikipedia pages to build a KB in a similar form as (ii). We take care to construct QA pairs such that they are all potentially answerable from either the KB from (ii) or the original Wikipedia documents from (i) to eliminate data sparsity issues. However, it should be noted that the advantage of working from raw documents in real applications is that data sparsity is less of a concern than for a KB, while on the other hand the KB has the information already parsed in a form amenable to manipulation by machines. This dataset can help analyze what methods we need to close the gap between all three settings, and in particular what are the best methods for reading documents when a KB is not available. A sample of the dataset is shown in Table 1.\nDoc We selected a set of Wikipedia articles about movies by identifying a set of movies from OMDb2 that had an associated article by title match. We keep the title and the first section (before the contents box) for each article. This gives\u223c17k documents (movies) which comprise the set of documents our models will read from in order to answer questions.\nKB Our set of movies were also matched to the MovieLens dataset3. We built a KB using OMDb and MovieLens metadata with entries for each movie and 9 different relation types: director, writer, actor, release year, language, genre, tags, IMDb rating and IMDb votes, with \u223c10k related actors, \u223c6k directors and \u223c43k entities in total. The KB is stored as triples; see Table 1 for examples. IMDb ratings and votes are originally real-valued but are binned and converted to text (\u201cunheard of\u201d, \u201cunknown\u201d, \u201cwell known\u201d, \u201chighly watched\u201d, \u201cfamous\u201d). We finally only retain KB triples where the entities also appear in the Wikipedia articles4 to try to guarantee that all\n2 http://beforethecode.com/projects/omdb/download.aspx 3 http://grouplens.org/datasets/movielens/\n4The dataset also includes the slightly larger version without this constraint.\nQA pairs will be equally answerable by either the KB or Wikipedia document sources.\nIE As an alternative to directly reading documents, we explore leveraging information extraction techniques to transform documents into a KB format. An IE-KB representation has attractive properties such as a reduced, more precise expression of facts and logical key-value pairings based on subject-verb-object groupings. This can come at the cost of lower recall due to malformed or completely missing triplets. For IE we use standard open-source software followed by some task-specific engineering to improve the results. We first employ coreference resolution via the Stanford NLP Toolkit (Manning et al., 2014) to reduce ambiguity by replacing pronominal (\u201che\u201d, \u201cit\u201d) and nominal (\u201cthe film\u201d) references with their representative entities. Next we use the SENNA semantic role labeling tool (Collobert et al., 2011) to uncover the grammatical structure of each sentence and pair verbs with their arguments. Each triplet is cleaned of words that are not recognized entities, and lemmatization is done to collapse different inflections of important task-specific verbs to one form (e.g. stars, starring, star\u2192 starred). Finally, we append the movie title to each triple similar to the \u201cWindow + Title\u201d representation of Sec. 3.2, which improved results."}, {"heading": "4.2 Question-Answer Pairs", "text": "Within the dataset\u2019s more than 100,000 questionanswer pairs, we distinguish 13 classes of question corresponding to different kinds of edges in our KB. They range in scope from specific (actor to movie: \u201cWhat movies did Harrison Ford star in?\u201d, movie to actors: \u201cWho starred in Blade Runner?\u201d), to more general (tag to movie:\u201cWhich films can be described by dystopian?\u201d); see Table 4 for the full list. For each question type there is a set of possible answers. Using SimpleQuestions (Bordes et al., 2015), an existing open-domain question answering dataset based on Freebase, we identified the subset of questions posed by human annotators that covered our question types. We expanded this set to cover all of our KB by substituting the entities in those questions to also apply them to other questions, e.g. if the original question was \u201cWhat movies did Harrison Ford star in?\u201d, we created a pattern \u201cWhat movies did [@actor] star in?\u201d, which we substitute for any other\nactors in our set, and repeat this for all annotations. We split the questions into disjoint training, development and test sets with \u223c96k, 10k and 10k examples, respectively. The same question (even worded differently) cannot appear in both train and test sets. Note that this is much larger than most existing datasets; for example, the WIKIQA dataset (Yang et al., 2015) for which we also conduct experiments in Sec. 5.2 has only \u223c1000 training pairs."}, {"heading": "5 Experiments", "text": "This section describes our experiments on MOVIEQA and WIKIQA."}, {"heading": "5.1 MovieQA", "text": "We conducted experiments on the MOVIEQA dataset described in Sec. 4. Our main goal is to compare the performance of KB, IE and Wikipedia (Doc) sources when trying varying learning methods. We compare four approaches: (i) the QA system of Bordes et al. (2014) that performs well on existing datasets WebQuestions (Berant et al., 2013) and SimpleQuestions (Bordes et al., 2015) that use KBs only; (ii) supervised embeddings that do not make use of a KB at all but learn question to answer embeddings directly and hence act as a sanity check (Dodge et al., 2016); (iii) Memory Networks; and (iv) Key-Value Memory Networks. Performance is reported using the accuracy of the top hit (single answer) over all possible answers (all entities), i.e. the hits@1 metric, measured in percent. In all cases hyperparameters\nare optimized on the development set, including the memory representations of Sec. 3.2 for MemNNs and KV-MemNNs. As MemNNs do not support keyvalue pairs, we concatenate key and value together when they differ instead.\nThe main results are given in Table 2. The QA system of Bordes et al. (2014) outperforms Supervised Embeddings and Memory Networks for KB and IE-based KB representations, but is designed to work with a KB, not with documents (hence the N/A in that column). However, Key-Value Memory Networks outperform all other methods on all three data source types. Reading from Wikipedia documents directly (Doc) outperforms an IE-based KB (IE) which is an encouraging result towards automated machine reading, although a gap to a humanannotated KB still remains (93.9 vs. 76.2). The best memory representation for directly reading documents uses \u201cWindow-level + Center Encoding + Title\u201d (W = 7 and H = 2); see Table 3 for a comparison of results for different representation types. Both center encoding and title features help the windowlevel representation, while sentence-level is inferior.\nQA Breakdown A breakdown by question type comparing the different data sources for KVMemNNs is given in Table 4. IE loses out especially to Doc (and KB) on Writer, Director and Actor to Movie, perhaps because coreference is difficult in these cases \u2013 although it has other losses elsewhere too. Note that only 56% of subject-object pairs in\nIE match the triples in the original KB, so losses are expected. Doc loses out to KB particularly on Tag to Movie, Movie to Tags, Movie to Writer and Movie to Actors. Tag questions are hard because they can reference more or less any word in the entire Wikipedia document; see Table 1. Movie to Writer/Actor are hard because there is likely only one or few references to the answer across all documents, whereas for Writer/Actor to Movie there are more possible answers to find.\nKB vs. Synthetic Document Analysis To further understand the difference between using a KB versus reading documents directly, we conducted a further experiment where we constructed synthetic documents using the KB. For a given movie, we use a simple grammar to construct a synthetic \u201cWikipedia\u201d document based on the KB triples: for each relation type we have a set of template phrases (100 in total) used to generate the fact, e.g. \u201cBlade Runner came out in 1982\u201d for the entry BLADE RUNNER RELEASE_YEAR 1982. We can then parameterize the complexity of our synthetic documents: (i) using one template, or all of them; (ii) using conjunctions to combine facts into single sentences or not; and (iii) using coreference between sentences where we replace the movie name with \u201cit\u201d.5 The purpose of this experiment is to find which aspects are responsible for the gap in performance to a KB. The results are given in Table 5. They indicate that some of the loss (93.9% for KB to 82.9% for One Template Sentence) in performance is due directly to representing in sentence form, making the subject, relation and object harder to extract. Moving to a larger number of templates does not deteriorate performance much (80%). The remaining performance drop seems to\n5This data is also part of the MOVIEQA benchmark.\nbe split roughly equally between conjunctions (74%) and coreference (76%). When combined, which is the hardest synthetic dataset (All Templates + Conj. + Coref.), this is actually harder than using the real Wikipedia documents (72.5% vs. 76.2%). This is possibly because the amount of conjunctions and coreferences we make are artificially too high (50% and 80% of the time, respectively)."}, {"heading": "5.2 WikiQA", "text": "WIKIQA (Yang et al., 2015) is an existing dataset for answer sentence selection using Wikipedia as the knowledge source. The task is, given a question, to select the sentence coming from a Wikipedia document that best answers the question, where performance is measured using mean average precision (MAP) and mean reciprocal rank (MRR) of the ranked set of answers. The dataset uses a pre-built information retrieval step and hence provides a fixed set of candidate sentences per question, so systems do not have to consider ranking all of Wikipedia. In contrast to MOVIEQA, the training set size is small (\u223c1000 examples) whilst the topic is much more broad (all of Wikipedia, rather than just movies) and the questions can only be answered by reading the documents, so no comparison to the use of KBs can be performed. However, a wide range of methods have already been tried on WIKIQA, thus providing a useful benchmark to test if the same results found on MOVIEQA carry across to WIKIQA, in particular the performance of Key-Value Memory Networks.\nDue to the size of the training set, following many other works (Yang et al., 2015; Santos et al., 2016; Miao et al., 2015) we pre-trained the word vectors (matrices A and B which are constrained to be identical) before training KV-MemNNs. We employed Supervised Embeddings (Dodge et al., 2016) for that\ngoal, training on all of Wikipedia while treating the input as a random sentence and the target as the subsequent sentence. We then trained KV-MemNNs with dropout regularization: we sample words from the question, memory representations and the answers, choosing the dropout rate using the development set. Finally, again following other successful methods (Yin and Sch\u00fctze, 2015), we combine our approach with exact matching word features between question and answers. Key hashing was not used as candidates were already pre-selected. To represent the memories, we used the Window-Level representation (the best choice on the dev set was W = 7) as the key and the whole sentence as the value, as the value should match the answer which in this case is a sentence. Additionally, in the representation all numbers in the text and the phrase \u201chow many\u201d in the question were replaced with the feature \u201c_number_\u201d. The best choice of hops was also H = 2 for KV-MemNNs.\nThe results are given in Table 6. Key-Value Memory Networks outperform a large set of other methods, although the L.D.C. method of (Wang et al., 2016) is only slightly worse. Memory Networks, which cannot easily pair windows to sentences, perform much worse, highlighting the importance of key-value memories."}, {"heading": "6 Conclusion", "text": "We studied the problem of directly reading documents in order to answer questions, concentrating our analysis on the gap between such direct methods and using human-annotated or automatically constructed KBs. We presented a new model, Key-Value Memory Networks, which helps bridge this gap, outperforming several other methods across two datasets, MOVIEQA and WIKIQA. However, some gap in performance still remains. MOVIEQA serves as an analysis tool to shed some light on the causes. Future work should try to close this gap further.\nKey-Value Memory Networks are a versatile tool for reading documents or KBs and answering questions about them \u2013 allowing to encode prior knowledge about the task at hand in the key and value memories. These models could be applied to storing and reading memories for other tasks as well, and future work should try them in other domains, such as in a full dialog setting."}], "references": [{"title": "Dbpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives"], "venue": "Springer.", "citeRegEx": "Auer et al\\.,? 2007", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Askmsr: Question answering using the worldwide web", "author": ["M. Banko", "E. Brill", "S. Dumais", "J. Lin"], "venue": "Proceedings of 2002 AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases, pages 7\u20139.", "citeRegEx": "Banko et al\\.,? 2002", "shortCiteRegEx": "Banko et al\\.", "year": 2002}, {"title": "Semantic parsing on freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "EMNLP, pages 1533\u2013 1544.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Question answering with subgraph embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston"], "venue": "Proc. EMNLP.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1506.02075.", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Toward an architecture for never-ending language learning", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R. Hruschka Jr", "T.M. Mitchell"], "venue": "AAAI, volume 5, page 3.", "citeRegEx": "Carlson et al\\.,? 2010", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "J. Mach. Learn. Res., 12, 2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Learning to construct knowledge bases from the world wide web", "author": ["M. Craven", "D. DiPasquo", "D. Freitag", "A. McCallum", "T. Mitchell", "K. Nigam", "S. Slattery"], "venue": "Artificial intelligence, 118(1), 69\u2013113.", "citeRegEx": "Craven et al\\.,? 2000", "shortCiteRegEx": "Craven et al\\.", "year": 2000}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of 20th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD\u201914), New York City, USA.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["F. Hill", "A. Bordes", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1511.02301.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["T. Kwiatkowski", "E. Choi", "Y. Artzi", "L. Zettlemoyer"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP\u201913), Seattle, USA.", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u201360.", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Neural variational inference for text processing", "author": ["Y. Miao", "L. Yu", "P. Blunsom"], "venue": "arXiv preprint arXiv:1511.06038.", "citeRegEx": "Miao et al\\.,? 2015", "shortCiteRegEx": "Miao et al\\.", "year": 2015}, {"title": "Attentive pooling networks", "author": ["Santos", "C. d.", "M. Tan", "B. Xiang", "B. Zhou"], "venue": "arXiv preprint arXiv:1602.03609.", "citeRegEx": "Santos et al\\.,? 2016", "shortCiteRegEx": "Santos et al\\.", "year": 2016}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "Proceedings of NIPS.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Webbased question answering: Revisiting askmsr", "author": ["C. Tsai", "Yih", "W.-t.", "C. Burges"], "venue": "Technical report, Technical Report MSR-TR-201520, Microsoft Research.", "citeRegEx": "Tsai et al\\.,? 2015", "shortCiteRegEx": "Tsai et al\\.", "year": 2015}, {"title": "The trec-8 question answering track report", "author": ["Voorhees", "E. M"], "venue": "Trec, volume 99, pages 77\u201382.", "citeRegEx": "Voorhees and M,? 1999", "shortCiteRegEx": "Voorhees and M", "year": 1999}, {"title": "Building a question answering test collection", "author": ["E.M. Voorhees", "D.M. Tice"], "venue": "Proceedings of the 23rd annual international ACM SIGIR", "citeRegEx": "Voorhees and Tice,? 2000", "shortCiteRegEx": "Voorhees and Tice", "year": 2000}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["M. Wang", "N.A. Smith", "T. Mitamura"], "venue": "EMNLP-CoNLL, volume 7, pages 22\u201332.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Sentence similarity learning by lexical decomposition and composition", "author": ["Z. Wang", "H. Mi", "A. Ittycheriah"], "venue": "arXiv preprint arXiv:1602.07019.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2015a", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["J. Weston", "A. Bordes", "S. Chopra", "T. Mikolov"], "venue": "arXiv preprint arXiv:1502.05698.", "citeRegEx": "Weston et al\\.,? 2015b", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Y. Yang", "Yih", "W.-t.", "C. Meek"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2013\u20132018. Citeseer.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "author": ["Yih", "W.-t.", "Chang", "M.-W.", "X. He", "J. Gao"], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Yih et al\\.,? 2015", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Convolutional neural network for paraphrase identification", "author": ["W. Yin", "H. Sch\u00fctze"], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages", "citeRegEx": "Yin and Sch\u00fctze,? 2015", "shortCiteRegEx": "Yin and Sch\u00fctze", "year": 2015}], "referenceMentions": [{"referenceID": 19, "context": "Question answering (QA) has been a long standing research problem in natural language processing, with the first systems attempting to answer questions by directly reading documents (Voorhees and Tice, 2000).", "startOffset": 182, "endOffset": 207}, {"referenceID": 4, "context": "The development of large-scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al.", "startOffset": 70, "endOffset": 94}, {"referenceID": 3, "context": ", 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014).", "startOffset": 196, "endOffset": 263}, {"referenceID": 12, "context": ", 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014).", "startOffset": 196, "endOffset": 263}, {"referenceID": 10, "context": ", 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014).", "startOffset": 196, "endOffset": 263}, {"referenceID": 9, "context": "Since information extraction (IE) (Craven et al., 2000), in-", "startOffset": 34, "endOffset": 55}, {"referenceID": 16, "context": "In this work we propose the Key-Value Memory Network (KV-MemNN), a new neural network architecture that generalizes the original Memory Network (Sukhbaatar et al., 2015) and can work with either knowledge source.", "startOffset": 144, "endOffset": 169}, {"referenceID": 24, "context": "our findings on WIKIQA (Yang et al., 2015), another Wikipedia-based QA benchmark where no KB is available, where we demonstrate that KV-MemNN can reach state-of-the-art results \u2013 surpassing the most recent attention-based neural network models.", "startOffset": 23, "endOffset": 42}, {"referenceID": 19, "context": "Early QA systems were based on information retrieval and were designed to return snippets of text containing an answer (Voorhees and Tice, 2000; Banko et al., 2002), with limitations in terms of question complexity and response coverage.", "startOffset": 119, "endOffset": 164}, {"referenceID": 2, "context": "Early QA systems were based on information retrieval and were designed to return snippets of text containing an answer (Voorhees and Tice, 2000; Banko et al., 2002), with limitations in terms of question complexity and response coverage.", "startOffset": 119, "endOffset": 164}, {"referenceID": 0, "context": "The creation of large-scale KBs (Auer et al., 2007; Bollacker et al., 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al.", "startOffset": 32, "endOffset": 75}, {"referenceID": 4, "context": "The creation of large-scale KBs (Auer et al., 2007; Bollacker et al., 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al.", "startOffset": 32, "endOffset": 75}, {"referenceID": 3, "context": ", 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014; Yih et al., 2015) that can return precise answers to complicated compositional questions.", "startOffset": 93, "endOffset": 178}, {"referenceID": 12, "context": ", 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014; Yih et al., 2015) that can return precise answers to complicated compositional questions.", "startOffset": 93, "endOffset": 178}, {"referenceID": 10, "context": ", 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014; Yih et al., 2015) that can return precise answers to complicated compositional questions.", "startOffset": 93, "endOffset": 178}, {"referenceID": 25, "context": ", 2008) have lead to the developement of a new class of QA methods based on semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014; Yih et al., 2015) that can return precise answers to complicated compositional questions.", "startOffset": 93, "endOffset": 178}, {"referenceID": 9, "context": "Due to the sparsity of KB data, however, the main challenge shifts from finding answers to developing efficient information extraction methods to populate KBs automatically (Craven et al., 2000; Carlson et al., 2010) \u2013 which is not an easy problem.", "startOffset": 173, "endOffset": 216}, {"referenceID": 7, "context": "Due to the sparsity of KB data, however, the main challenge shifts from finding answers to developing efficient information extraction methods to populate KBs automatically (Craven et al., 2000; Carlson et al., 2010) \u2013 which is not an easy problem.", "startOffset": 173, "endOffset": 216}, {"referenceID": 20, "context": "For this reason, recent initiatives are returning to the original setting of directly answering from text using datasets like TRECQA (Wang et al., 2007), based on classical TREC resources (Voorhees et al.", "startOffset": 133, "endOffset": 152}, {"referenceID": 24, "context": ", 1999), and WIKIQA (Yang et al., 2015), extracted from Wikipedia.", "startOffset": 20, "endOffset": 39}, {"referenceID": 2, "context": "Even though standard pipeline QA systems like AskMR (Banko et al., 2002) have been recently revisited (Tsai et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 17, "context": ", 2002) have been recently revisited (Tsai et al., 2015), the best published results on TRECQA and WIKIQA have been obtained by either convolutional neural networks (Santos et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 14, "context": ", 2016) or recurrent neural networks (Miao et al., 2015) \u2013 both usually with attention mechanisms inspired by (Bahdanau et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 1, "context": ", 2015) \u2013 both usually with attention mechanisms inspired by (Bahdanau et al., 2015).", "startOffset": 61, "endOffset": 84}, {"referenceID": 22, "context": "The Key-Value Memory Network model is based on the Memory Network (MemNNs) model (Weston et al., 2015a; Sukhbaatar et al., 2015) which has proven useful for a variety of document reading and question answering tasks: for reading children\u2019s books and answering questions about them (Hill et al.", "startOffset": 81, "endOffset": 128}, {"referenceID": 16, "context": "The Key-Value Memory Network model is based on the Memory Network (MemNNs) model (Weston et al., 2015a; Sukhbaatar et al., 2015) which has proven useful for a variety of document reading and question answering tasks: for reading children\u2019s books and answering questions about them (Hill et al.", "startOffset": 81, "endOffset": 128}, {"referenceID": 11, "context": ", 2015) which has proven useful for a variety of document reading and question answering tasks: for reading children\u2019s books and answering questions about them (Hill et al., 2015), for complex reasoning over simulated stories (Weston et al.", "startOffset": 160, "endOffset": 179}, {"referenceID": 23, "context": ", 2015), for complex reasoning over simulated stories (Weston et al., 2015b) and for utilizing KBs to answer questions (Bordes et al.", "startOffset": 54, "endOffset": 76}, {"referenceID": 6, "context": ", 2015b) and for utilizing KBs to answer questions (Bordes et al., 2015).", "startOffset": 51, "endOffset": 72}, {"referenceID": 16, "context": "Our model is based on the end-to-end Memory Network architecture of Sukhbaatar et al. (2015). A high-level view of both models is as follows: one defines a memory, which is a possibly very large array of slots which can encode both long-term and shortterm context.", "startOffset": 68, "endOffset": 93}, {"referenceID": 16, "context": "To obtain the standard End-To-End Memory Network of Sukhbaatar et al. (2015) one can simply set the key and value to be the same for all memories.", "startOffset": 52, "endOffset": 77}, {"referenceID": 23, "context": "identical to a standard MemNN and this approach has been used in several papers (Weston et al., 2015b; Dodge et al., 2016).", "startOffset": 80, "endOffset": 122}, {"referenceID": 11, "context": "Window representations for MemNNs have been shown to work well previously (Hill et al., 2015).", "startOffset": 74, "endOffset": 93}, {"referenceID": 13, "context": "We first employ coreference resolution via the Stanford NLP Toolkit (Manning et al., 2014) to reduce ambiguity by replacing pronominal (\u201che\u201d, \u201cit\u201d) and", "startOffset": 68, "endOffset": 90}, {"referenceID": 8, "context": "Next we use the SENNA semantic role labeling tool (Collobert et al., 2011) to uncover the grammatical structure of each sentence and pair verbs with their arguments.", "startOffset": 50, "endOffset": 74}, {"referenceID": 6, "context": "Using SimpleQuestions (Bordes et al., 2015), an existing open-domain question answering dataset based on Freebase, we identified the subset of questions posed by human annotators that covered our question types.", "startOffset": 22, "endOffset": 43}, {"referenceID": 5, "context": "(Bordes et al., 2014) QA system 93.", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "for example, the WIKIQA dataset (Yang et al., 2015) for which we also conduct experiments in Sec.", "startOffset": 32, "endOffset": 51}, {"referenceID": 3, "context": "(2014) that performs well on existing datasets WebQuestions (Berant et al., 2013) and SimpleQuestions (Bordes et al.", "startOffset": 60, "endOffset": 81}, {"referenceID": 6, "context": ", 2013) and SimpleQuestions (Bordes et al., 2015) that use KBs only; (ii) supervised embeddings that do not make use of a KB at all but learn question to answer embeddings directly and hence act as a sanity check (Dodge et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 4, "context": "We compare four approaches: (i) the QA system of Bordes et al. (2014) that performs well on existing datasets WebQuestions (Berant et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 5, "context": "The QA system of Bordes et al. (2014) outperforms Supervised Embeddings and Memory Networks for KB", "startOffset": 17, "endOffset": 38}, {"referenceID": 24, "context": "5132 2-gram CNN (Yang et al., 2015) 0.", "startOffset": 16, "endOffset": 35}, {"referenceID": 15, "context": "6652 AP-CNN (Santos et al., 2016) 0.", "startOffset": 12, "endOffset": 33}, {"referenceID": 14, "context": "6957 Attentive LSTM (Miao et al., 2015) 0.", "startOffset": 20, "endOffset": 39}, {"referenceID": 26, "context": "7069 Attentive CNN (Yin and Sch\u00fctze, 2015) 0.", "startOffset": 19, "endOffset": 42}, {"referenceID": 21, "context": "(Wang et al., 2016) 0.", "startOffset": 0, "endOffset": 19}, {"referenceID": 24, "context": "WIKIQA (Yang et al., 2015) is an existing dataset for answer sentence selection using Wikipedia as the knowledge source.", "startOffset": 7, "endOffset": 26}, {"referenceID": 24, "context": "Due to the size of the training set, following many other works (Yang et al., 2015; Santos et al., 2016; Miao et al., 2015) we pre-trained the word vectors (matrices A and B which are constrained to be identical) before training KV-MemNNs.", "startOffset": 64, "endOffset": 123}, {"referenceID": 15, "context": "Due to the size of the training set, following many other works (Yang et al., 2015; Santos et al., 2016; Miao et al., 2015) we pre-trained the word vectors (matrices A and B which are constrained to be identical) before training KV-MemNNs.", "startOffset": 64, "endOffset": 123}, {"referenceID": 14, "context": "Due to the size of the training set, following many other works (Yang et al., 2015; Santos et al., 2016; Miao et al., 2015) we pre-trained the word vectors (matrices A and B which are constrained to be identical) before training KV-MemNNs.", "startOffset": 64, "endOffset": 123}, {"referenceID": 26, "context": "Finally, again following other successful methods (Yin and Sch\u00fctze, 2015), we combine our approach with exact matching word features between question and answers.", "startOffset": 50, "endOffset": 73}], "year": 2016, "abstractText": "Directly reading documents and being able to answer questions from them is a key problem. To avoid its inherent difficulty, question answering (QA) has been directed towards using Knowledge Bases (KBs) instead, which has proven effective. Unfortunately KBs suffer from often being too restrictive, as the schema cannot support certain types of answers, and too sparse, e.g. Wikipedia contains much more information than Freebase. In this work we introduce a new method, Key-Value Memory Networks, that makes reading documents more viable by utilizing different encodings in the addressing and output stages of the memory read operation. To compare using KBs, information extraction or Wikipedia documents directly in a single framework we construct an analysis tool, MOVIEQA, a QA dataset in the domain of movies. Our method closes the gap between all three settings. It also achieves state-of-the-art results on the existing WIKIQA benchmark.", "creator": "LaTeX with hyperref package"}}}