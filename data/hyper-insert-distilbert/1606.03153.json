{"id": "1606.03153", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition", "abstract": "text embeddings research have played a key teaching role largely in successively obtaining state - of - the - art results in cognitive natural tree language morphology processing. evolving word2vec and applying its variants series have successfully mapped existing words with many similar syntactic or distinct semantic geometric meanings to nearby item vectors. however, searching extracting universal embeddings of considerably longer word - width sequences remains a challenging task. now we employ finding the simplified convolutional word dictionary model necessary for unsupervised structure learning of embeddings optimal for certain variable interval length double word - sequences. we initially propose a sequential two - phase convdic + deconvdec framework that explicitly first merely learns dictionary elements ( i. e., phrase morphological templates ), transforms and yet then likewise employs them simultaneously for decoding the activations. the finely estimated syllable activations are subsequently then sometimes used as embeddings for computational downstream mapping tasks such as sentiment systems analysis, specialized paraphrase target detection, consolidation and semantic textual similarity signal estimation. we propose improving a convolutional tensor coupling decomposition algorithm helpful for learning the phrase templates. it is likely shown to be typically more accurate, and proving much more efficient as than was the usual popular quantitative alternating minimization tool in digital dictionary distance learning literature. computing our functional word - sequence embeddings can achieve state - of - the - machine art performance in sentiment sequences classification, semantic textual term similarity interval estimation, transparency and sophisticated paraphrase chain detection typically over eight specialized datasets freed from exploring various broad domains, always without requiring pre - training or additional modelling features.", "histories": [["v1", "Fri, 10 Jun 2016 01:22:32 GMT  (67kb)", "http://arxiv.org/abs/1606.03153v1", null], ["v2", "Thu, 4 May 2017 22:32:17 GMT  (60kb)", "http://arxiv.org/abs/1606.03153v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["furong huang", "animashree anandkumar"], "accepted": false, "id": "1606.03153"}, "pdf": {"name": "1606.03153.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Learning of Word-Sequence Representations from Scratch via Convolutional Tensor Decomposition", "authors": ["Furong Huang", "Animashree Anandkumar"], "emails": ["furongh@uci.edu", "a.anandkumar@uci.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n60 6.\n03 15\n3v 1\n[ cs\n.C L\n] 1\n0 Ju"}, {"heading": "1. Introduction", "text": "We have recently witnessed the tremendous success of word embeddings or word vector representations in natural language processing. This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al. (2011); Mikolov et al. (2013); Pennington et al. (2014). Word embeddings have attained state-of-the-art performance in tasks such as part-of-speech (POS) tagging, chunking, named entity recognition (NER), and semantic role labeling. Despite this impressive performance, word embeddings do not suffice for more advanced tasks which require context-aware information or word orders, e.g. paraphrase detection, sentiment analysis, plagiarism detection, information retrieval and machine translation. Therefore, extracting word-sequence vector representations is crucial for expanding the realm of automated text understanding.\nPrevious works on word-sequence embeddings are based on a variety of mechanisms. A popular method is to learn the composition operators in sequences Mitchell and Lapata\nc\u00a9 Furong Huang and Animashree Anandkumar.\n(2010); Yu and Dredze (2015). The complexity of the compositionality varies widely: from simple operations such as addition Mitchell and Lapata (2010); Yu and Dredze (2015) to complicated recursive neural networks Socher et al. (2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al. (2014b,b), long short-term memory (LSTM) recurrent neural networks Tai et al. (2015), or combinations of these architectures Wieting et al. (2015). All these methods produce sentence representations that depend on a supervised task, and the class labels are back-propagated to update the composition weights Kalchbrenner et al. (2014a).\nSince the above methods rely heavily on the downstream task and the domain of the training samples, they can hardly be used as universal embeddings across domains, and require intensive pre-training and hyper-parameter tuning. The state-of-the-art unsupervised framework is Skip-thought Kiros et al. (2015), based on an objective function that abstracts the skip-gram model to the sentence level, and encodes a sentence to predict the sentences around it. However, the skip-thought model requires a large corpus of contiguous text, such as the book corpus with more than 74 million sentences. Can we instead efficiently learn sentence embeddings using small amounts of samples without supervision/labels or annotated features(such as parse trees)? Also, can the sentence embeddings be context-aware, can handle variable lengths, and is not limited to specific domains?\nWe propose an unsupervised ConvDic+DeconvDec framework that satisfies all the above constraints. It is composed of two phases, a comprehension phase which summarizes template phrases using convolutional dictionary elements, followed by a feature-extraction phase which extracts activations using deconvolutional decoding. We propose a novel learning algorithm for the comprehension phase based on convolutional tensor decomposition, as described in Section 1.1. Note that in the comprehension phase, phrase templates are learned over fixed length small patches (patch length is equal to phrase template length), whereas entire word-sequence is decoded to get the final word-sequence embedding in the featureextraction phase.\nWe employ our sentence embeddings in the tasks of sentiment classification, semantic textual similarity estimation, and paraphrase detection over eight datasets from various domains. These are challenging tasks since they require a contextual understanding of text relationships rather than bags of words. We learn the embeddings from scratch without using any auxiliary information. While previous works use information such as parse trees, Wordnet or pre-train on a much larger corpus, we train from scratch on small amounts of text and obtain competitive results, which are close or even better than the state-of-the-art.\nThis is due to the combination of efficient modeling and learning approaches in our work. The convolutional model incorporates word orders and phrase representations, and our tensor decomposition algorithm can efficiently learn an set of parameters (phrase templates) for the convolutional model. We describe our framework in detail below."}, {"heading": "1.1 Convolutional Dictionary Model and Tensor Decomposition Algorithm", "text": "Word embeddings focus on mapping words to fixed length vector representations and ignores the order of the words. To model the word-sequence or the word order, we consider a convolutional dictionary learning model which posits that the observed word-sequence is generated from a superposition of phrase templates (a.k.a dictionary elements) {f\u22171 , . . . f \u2217 L}\nactivated at various locations in the word-sequence. Therefore, the convolutional model x = \u2211\ni\u2208[L] f \u2217 i \u2217w\u2217i is natural, where activations are represented by activation maps {w\u22171, . . . w\u2217L}.\nNow training the convolutional dictionary model is the problem of joint prediction of a good set of phrase templates f\u22171 , . . . f \u2217 L and activation maps w \u2217 1, . . . w \u2217 L given the observed wordsequence x. And the activation maps w\u22171, . . . w \u2217 L are further used as the word-sequences embedding for word-sequences x as it contains the discriminative features that distinguish different word-sequences.\nTo be precise, the convolutional dictionary model learning solves the following optimization problem\nmin fi,wi:\u2016fi\u2016=1\n\u2016x\u2212 \u2211\ni\u2208[L]\nfi\u2217wi\u20162. (1)\nA popular heuristic for solving (1) is based on alternate minimization (AM) Bristow and Lucey (2014), where the phrase embeddings fi are optimized, while keeping the activations wi fixed, and vice versa. Each alternating update can be solved efficiently since it is linear in each of the variables. However, there are two main drawbacks: computational inefficiency and sub-optimality. AM requires a pass over all the samples in each iteration and is therefore computationally expensive in the large sample setting. Moreover, due to the non-convexity of the objective function as in (1), obtaining the global optimum of (1) is NP-hard in general. AM has no local or global convergence guarantees even in usual dictionary learning setting (multiplicative model). This problem is severely amplified in the convolutional setting due to additional symmetries. Due to shift invariance of the convolutional operator, shifting a phrase embedding fi by some amount, and applying a corresponding negative shift on the activation wi leaves the objective in (1) unchanged. Thus, solving (1) is fundamentally ill-posed and has a large number of equivalent solutions.\nTo solve the computational inefficiency and sub-optimality problem, we propose a convolutional tensor decomposition method Huang and Anandkumar (2015). Our convolutional tensor decomposition method employs the inverse method of moments, and decompose a data cumulant (empirically computed from aggregate statistics or data moments) as phrase embeddings and shifted versions of phrase embeddings. The entire process requires one pass of data to compute the cumulant whereas AM requires data passes in each iteration. The reason why our tensor decomposition framework avoids multiple passes of the data samples is that we only estimate the phrase embeddings fi in the learning step. Moreover, the algorithm is carefully implemented and algorithmically optimized that it requires only simple operations such as Fast Fourier Transforms (FFT) and matrix multiplications. These operations have a high degree of parallelism: for estimating L phrase embeddings, each of length n, we require O(log n+logL) time and O(L2n3) processors. Our convolutional tensor decomposition yields optimization problems (in each iteration) that can be solved in closed form and it converges much faster compared to AM Huang and Anandkumar (2015)."}, {"heading": "2. Word-Sequence Modeling and Formulation", "text": "Our ConvDic+DeconvDec framework focuses on a convolutional dictionary model to summarize phrase templates, and then decode word-sequence signals to obtain the word-sequence\nembeddings. The first question is how to encode the word sequence into a signal, to be input to the convolutional model and we discuss that below."}, {"heading": "2.1 From raw text to signals", "text": "Word encoding: A word is represented as a one-hot encoding vector, i.e. with vector ei \u2208 R\nd whose ith entry is 1 and other entries are 0, where i is the index of the word in the dictionary. Alternatively, one could use the word2vec embeddings instead of one-hot encodings. We then stack the one-hot encoding vectors of each sentence together to form a encoding matrix. The stacking order conforms the word-sequence order.\nreplacemen\nTo be precise, let us consider sentenc with N words. The encoding matrix of this wordsequence Sseq is Sseq := [sword1 , sword2 , . . . , swordN ] \u2208 R d\u00d7N .\nPrincipal components: Now that we have encoded words in each sentence, we want to find a compact representation of them in terms of a dictionary model. However, the encoding matrices are too sparse to fit a convolutional model in the word space. Instead, we perform dimensionality reduction through PCA and carry out dictionary modeling in the projected space.\nConcretely, we stack the encoding matrices side by side as S := [Sseq1 ,Sseq2 , . . . ,SseqM ] \u2208\nR d\u00d7( \u2211M i=1 Ni), assuming there are M number of sentences in the collection of varying lengths N1, N2 and so on. Let U \u2208 R d\u00d7k denote the top k left eigenvectors of S. We consider Yi := U \u22a4Sseq1 \u2208 R k\u00d7Ni , for each sentence i. We treat the rows of Yi independently in parallel and fit convolutional model to each row. Denote jth row of Yi as y (j) i , and thus\nYi =\n\n  y (1) i ...\ny (k) i\n\n  .\nEach y (j) i is generated through a convolutional dictionary model over phrase templates and activation maps. Our goal in the learning phase is to learn template phrases for the collection of [y (j) i ] over all word-sequences \u2200i \u2208 [M ] across all parallel directions \u2200j \u2208 [k]. We will state the learning problem formally in the next section. Since all the coordinates are independent and the phrase templates are learned in parallel over all the coordinates, we drop the index j to denote a coordinate of the ith word sequence y (j) i . In the following subsection, a patch from y (j) i will be denoted as x.\n2.2 Comprehension Phase \u2013 Learning Phrase Templates\nA word sequence is composed of superposition of overlapping patches, therefore we are interested in learning a generative model over overlapping patches. We can also view these\npatches as phrases. A length n patch x is generated as the superposition of L phrase embeddings f\u2217l convolved at L activation maps w \u2217 l , \u2200l \u2208 [L]. Due to the property of the convolution, the convolution is reformulated as the multiplication of F\u2217 and w\u2217, where F\u2217 := [Cir(f\u22171 ),Cir(f \u2217 2 ), . . . ,Cir(f \u2217 L)] is the concatenation of circulant matrices and w \u2217 is the\nrow-stacked vector w\u2217 :=\n\n    w\u22171 w\u22172 ...\nw\u2217L\n\n    \u2208 RnL. To be precise, a patch\nx = \u2211\nl\u2208[L]\nf\u2217l \u2217wl\u2217 = F\u2217 \u00b7 w\u2217, (2)\nThis is illustrated in Fig 4(a). Cir(f\u2217l ) is circulant matrix corresponding to phrase template f\u2217l , whose columns are shifted versions of f \u2217 l as shown in Fig 4(a). Note that although F \u2217 is a n by nL matrix, there are only nL free parameters. Given access to the collection of word-sequence sample patches, X := [x1, x2, . . .], generated according to the above model, we aim to estimate the true template phrases f\u2217i , for i \u2208 [L]. In section 3 we will elaborate on our convolutional tensor decomposition dictionary learning method (ConvDic).\nIf the patches are in the same coordinate of the word sequence, these patches share a common set of phase templates, but their activation maps are different. The activation maps are the discriminative features that distinguish different patches. Once the template phrases are estimated, we can use standard decoding techniques, such as the square loss criterion in (1) to learn the activation maps for the individual maps."}, {"heading": "2.3 Feature-extraction Phase \u2013 Word-sequence Embeddings", "text": "Activation maps in a coordination: After learning a good set of phrase templates {f1, . . . , fL} and thus F , we use the deconvolutional decoding (DeconvDec) to obtain the activation maps for the jth coordinate. For each observed coordinate of the word-sequence y (j) i , the activation map w \u2217 l in (2) indicates the locations where i th template phrase f\u2217l is activated and w\u2217 is the row-stacked vector w\u2217 := [w\u22171;w \u2217 2; . . . w \u2217 L]. An estimation of w \u2217, w (j) i , is achieved as follows\nw (j) i = F \u2020y (j) i\n\u22a4 . (3)\nNote that the estimated phrase templates are zero padded to match the length of the wordsequence.\nWe assume that the elements of w\u2217 are drawn from some product distribution, i.e. different entries are independent of one another, and we have the independent component analysis (ICA) model in (2). When the distribution encourages sparsity, e.g. BernoulliGaussian, only a small subset of locations are active, and we have the sparse coding model in that case. We can also extend to dependent distributions such as Dirichlet for w\u2217, along the lines of Blei et al. (2003), but limit ourselves to ICA model for simplicity. This activation map w (j) i \u2208 R\nNi\u00b7L contains sequence embeddings from coordinate j only, and will be used as one coordinate of our final word-sequence embeddings.\nVarying sentence length: One difficulty in learning the template phrases using our convolutional tensor decomposition model is that different word-sequence has a different length Ni, therefore the activation maps are of varying length as well. We resolved this problem by max-k pooling. In other words, we extract most informative global discriminative features from the activation maps, as illustrated in Figure 2. Finally, we concatenate all the max-k pooled coordinate sequence embeddings as a long vector as the final word-sequence embedding.\nThe overall framework flow is depicted in Fig 2."}, {"heading": "3. Convolutional Dictionary Model Learning", "text": "In this section, we will focus on the comprehension phrase and propose a tensor decomposition dictionary learning method for learning the phrase templates. As we demonstrated earlier, the generative model for a patch from one coordinate, x, is illustrated in Fig 4. x is generated as the superposition of L phrase embeddings f\u2217l convolved at L activation maps w\u2217l , \u2200l \u2208 [L]. Let f \u2217 l \u2208 R\nn be the unknown template phrases, where j \u2208 [L] denotes the index of phrases. Under the convolution ICA model, we show that the third order cumulant has a nice tensor decomposition form Huang and Anandkumar (2015), as given below.\nLemma 1 (Decomposition of Cumulants) The unfolded third order cumulant C3 in (6) has the following decomposition form\nC3 = \u2211\nj\u2208[nL]\n\u03bb\u2217jF \u2217 j (F \u2217 j \u2299F \u2217 j ) \u22a4 = F\u2217\u039b\u2217 (F\u2217 \u2299F\u2217)\u22a4 , where \u039b\u2217 := diag(\u03bb\u22171, \u03bb \u2217 2, . . . , \u03bb \u2217 nL) (4)\nwhere F\u2217j denotes the j th column of the column-stacked circulant matrix F\u2217 and \u03bb\u2217j is the third order cumulant corresponding to the (univariate) distribution of w\u2217(j).\nThe third order cumulant C3 is a third order tensor, which could be empirically estimated using first three orders of moments. The form of the cumulant tensor is in Appendix A.1.\nThe decomposition form in (4) is known as the CANDECOMP/PARAFAC (CP) decomposition form Anandkumar et al. (2014) (the usual form has the decomposition of the tensor and not its unfolding, as above). We attempt to recover the unknown template phrases f\u2217i through decomposition of the third order cumulants C3. Our goal is to obtain template phrase estimates fl\u2019s \u2200l \u2208 [L] and weight estimates \u039b such that the cumulant C3 is decomposed as F\u039b (F \u2299 F)\n\u22a4, as in equation 4. The formal statement is deferred to Appendix A.2.\nWe propose convolutional tensor decomposition using efficient Alternating Least Square with Circulant Constraint to solve the non-convex optimization problem. Consider the asymmetric relaxation and introduce separate variables F ,G and H for filter estimates along each of the modes to fit the third order cumulant tensor C3. ALS iteratively alternates over the three variables and updates one mode by fixing the two other modes\nmin F\n\u2016C3 \u2212F\u039b (H\u2299 G) \u22a4\u20162F s.t. blkl(F) = U \u00b7 diag(FFT(fl)) \u00b7 U H, \u2016fl\u2016 2 2 = 1,\u2200l \u2208 [L] (5)\nSimilarly, G and H have the same column-stacked circulant matrix constraint and are updated similarly in alternating steps. The diagonal matrix \u039b is updated through normalization. The objective function is defined in Huang and Anandkumar (2015), refer to appendix A for details."}, {"heading": "4. Experiments", "text": "We evaluate the quality of our word sequence embeddings using three challenging natural language process tasks: sentiment classification, paraphrase detection, and semantic textual similarity estimation. Eight datasets which cover various domains are used as shown in Table 1.\nFor all the datasets, we train a simple logistic regression model on the training samples and report test classification accuracy using a 10-fold cross validation. Sentiment analysis and paraphrase detection belong to binary classification tasks. In a binary classification task, either accuracy or F score is used as evaluate metric. Recall that F-score is the harmonic mean of precision and recall, i.e., F = 2 \u00b7 (precision \u00b7 recall)/precision + recall. Precision is the number of true positives divided by the total number of elements labeled as belonging to the positive class, and recall is the number of true positives divided by the total number of elements that belong to the positive class.\nOur ConvDic+DeconvDec learns word-sequence embeddings from scratch and requires no pre-training. When working on a new dataset from a new domain, we train fresh set of phrase templates as called domain phrase templates. Using these domain phrase templates, we decode activation maps and then form phrase-embeddings. Our approach is different from skip thoughts, where universal phrase embeddings are generated Kiros et al. (2015)."}, {"heading": "4.1 Evaluation Task: Sentiment Classification", "text": "Sentiment analysis is an important task in natural language process as automated labeling of word sequences into positive and negative opinions is used in various settings. We evaluate our sentence embeddings on two datasets from different domains, such as movie review and subjective and objective comments, as in Table 1. Using word-sequence embeddings combined with NB features, we obtain the state-of-the-art classification results for both these datasets as in Table 2.\n1. The word similarities information they use are either trained in Wikipedia (4.4 million articles in contrast to the 4076 sentences of paraphrase dataset we use) or from WordNet with expert knowledge."}, {"heading": "4.2 Evaluation Task: Paraphrase Detection", "text": "We consider the paraphrase detection task on the Microsoft paraphrase corpus Quirk et al. (2004); Dolan et al. (2004). We employ 4076 sentence pairs as training data to learn the sentence embeddings and regress on the ground truth binary labels with our learned sentence embeddings. The remaining test data is used to calculate classification error.\nAs discussed in Tai et al. (2015), we combine the pair of sentence embeddings produced earlier wL and wR, i.e., the embedding for the right and the left sentences. We generate features for classification using both the distance (absolute difference) and the product between the pair (wL, wR): [wL\u2299wR, \u2016wL \u2212wR\u2016], where \u2299 denotes the element-wise multiplication.\nIn contrast to other unsupervised methods which are trained using outside information such as wordnet and parse trees, our unsupervised approach use no extra information, and still achieves comparable results with the state of art Wiki (2014) as in table 3. We show some examples of paraphrase and non-paraphrase we identified.\nParaphrase detected: (1) Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence. (2) Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence. The two sentences are the \u201cdifficult sentence\u201d to show how our algorithm detect paraphrases since they are not simple switching of clauses, and the sentence structures differ quite significantly in the two sentences.\nNon-paraphrase detected : (1) I never organised a youth camp for the diocese of Bendigo. (2) I never attended a youth camp organised by that diocese. Similarly with non-paraphrase detection, the two sentences share common words such as youth camp and organized, but our method is able to successfully detect them as non-paraphrase."}, {"heading": "4.3 Evaluation Task: Semantic Textual Similarity Estimation", "text": "For the Semantic Textual Similarity (STS) task, the goal is to predict a real-valued similarity score in a range [1,K] given a sentence pair. We include datasets from STS task in various domains including news, image and video description, glosses from WordNet/OntoNotes, the output of machine translation systems with reference translation.\nTo frame semantic test similarity estimation task into the multi-class classification framework, the gold rating \u03c4 \u2208 [K1,K2] is discretized as p \u2208 \u2206\nK2\u2212K1 in the follow manner Tai et al. (2015), pi = \u230a\u03c4\u230b \u2212 \u03c4 + 1 if i = \u230a\u03c4\u230b + 1 \u2212K1, pi = \u03c4 \u2212 \u230a\u03c4\u230b if i = \u230a\u03c4\u230b + 2 \u2212K1, and pi = 0 otherwise. This reduces to finding a predicted p\u0302\u03b8 \u2208 \u2206\nK2\u2212K1 given model parameters \u03b8 to be closest to p in terms of KL divergence Tai et al. (2015). We use a logistic regression classifier to predict p\u0302\u03b8 and estimate \u03c4\u0302\u03b8 = [K1, . . . ,K2]p\u0302.\nResults on STS task datasets are illustrated in Table 4. As in Wieting et al. (2015), Pearson\u2019s r of the median, 75th percentile, and highest score from the official task rankings are showed. We then compare our method against the performance of supervised models in Wieting et al. (2015): PARAGRAM-PHRASE (PP), projection (proj.), deep-averaging network (DAN), recurrent neural network (RNN) and LSTM; as well as the state-of-the-art unsupervised model skip-thought vectors Kiros et al. (2015).\nAs we can see from the table, LST is performing poorly even though a back-propagation after seeing the training labelings is carried out for sequence embedding learning. Our method is an unsupervised approach as in skip-thought vectors. However, our algorithm doesn\u2019t output universal word-sequence embeddings across domains. We train a fresh model\nand a new set of domain phrase templates from scratch. Therefore our algorithm is performing better for these individual datasets on the STS task."}, {"heading": "5. Conclusion", "text": "Our unsupervised efficiently ConvDic+DeconvDec yields word-sequence representations that perform well across a wide range of NLP tasks over datasets from various domains. At the same time, our efficient tensor learning algorithm requires a relatively small amount of data and computation. In the future, we plan to investigate the use of ConvDic+DeconvDec for other domains such as images and videos, as well obtaining joint text-image embeddings."}, {"heading": "Appendix A. Convolutional Tensor Decomposition For Learning", "text": "Convolutional Dictionary Model\nA.1 Cumulant Form Huang and Anandkumar (2015)\nLet C3 \u2208 R n\u00d7n2 denote the unfolded version of third order cumulant tensor, it is given by\nC3 := E[x(x\u2299 x) \u22a4]\u2212 unfold(Z) (6)\nwhere [Z]a,b,c := E[xa]E[xbxc]+E[xb]E[xaxc]+E[xc]E[xaxb]\u22122E[xa]E[xb]E[xc], \u2200a, b, c \u2208 [n]. For example, if the lth activation is drawn from a Poisson distribution with mean \u03bb\u0303, we have that \u03bb\u2217l = \u03bb\u0303. Note that if the third order cumulants of the activations, i.e. \u03bb \u2217 j \u2019s, are zero, we need to consider higher order cumulants. This holds for zero-mean activations and we need to use fourth order cumulant instead. Our method extends in a straightforward manner for higher order cumulants.\nA.2 Alternating Least Squares for Convolutional Tensor Decomposition Huang and Anandkumar (2015)\nObjective Function: Our goal is to obtain template phrase estimates fi\u2019s which minimize the Frobenius norm \u2016 \u00b7 \u2016F of reconstruction of the cumulant tensor C3,\nmin F\n\u2016C3 \u2212F\u039b (F \u2299F) \u22a4\u20162F ,\ns.t. blkl(F) = Udiag(FFT(fl))U H, \u2016fl\u20162 = 1, \u2200l \u2208 [L], \u039b = diag(\u03bb). (7)\nwhere blkl(F) denotes the l th circulant matrix in F , i.e., F = [blk1(F), . . . , blkL(F)]. The conditions in (7) enforce blkl(F) to be circulant and for the template phrases to be normalized. Recall that U denotes the eigenvectors for circulant matrices. Now we explain our proposed convolutional tensor decomposition using efficient Alternating Least Square with Circulant Constraint to solve (7).\nTo solve the non-convex optimization problem in (7), we consider the alternating least squares (ALS) method with column stacked circulant constraint. We first consider the asymmetric relaxation of (7) and introduce separate variables F ,G andH for filter estimates along each of the modes to fit the third order cumulant tensor C3. We then perform alternating updates by fixing two of the modes and updating the third one. For instance,\nmin F\n\u2016C3 \u2212F\u039b (H\u2299 G) \u22a4\u20162F s.t. blkl(F) = U \u00b7 diag(FFT(fl)) \u00b7 U H, \u2016fl\u2016 2 2 = 1,\u2200l \u2208 [L] (8)\nSimilarly, G and H have the same column-stacked circulant matrix constraint and are updated similarly in alternating steps. The diagonal matrix \u039b is updated through normalization.\nWe now introduce the Convolutional Tensor (CT) Decomposition algorithm to efficiently solve (8) in closed form, using simple operations such as matrix multiplications and fast Fourier Transform (FFT). We do not form matrices F ,G and H \u2208 Rn\u00d7nL, which are large, but only update them using filter estimates f1, . . . , fL, g1, . . . , gL, h1, . . . hL.\nUsing the property of least squares, the optimization problem in (8) is equivalent to\nmin F\n\u2016C3((H\u2299 G) \u22a4)\u2020\u039b\u2020 \u2212F\u20162F s.t. blkl(F) = U \u00b7 diag(FFT(fl)) \u00b7U H, \u2016fl\u2016 2 2 = 1,\u2200l \u2208 [L] (9)\nwhen (H \u2299 G) and \u039b are full column rank. The full rank condition requires nL < n2 or L < n, and it is a reasonable assumption since otherwise the filter estimates are redundant. In practice, we can additionally regularize the update to ensure full rank condition is met. Denote\nM := C3((H\u2299 G) \u22a4)\u2020, (10)\nwhere \u2020 denotes pseudoinverse. Let blkl(M) and blkl(\u039b) denote the l th blocks of M and \u039b.Since (9) has block constraints, it can be broken down in to solving L independent subproblems\nmin fl\n\u2225 \u2225 \u2225 blkl(M) \u00b7 blkl(\u039b) \u2020 \u2212 U \u00b7 diag(FFT(fl)) \u00b7 U H \u2225 \u2225 \u2225 2\nF s.t. \u2016fl\u2016\n2 2 = 1,\u2200l \u2208 [L] (11)"}], "references": [{"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade", "Matus Telgarsky"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "A linear dynamical system model for text", "author": ["David Belanger", "Sham Kakade"], "venue": "arXiv preprint arXiv:1502.04081,", "citeRegEx": "Belanger and Kakade.,? \\Q2015\\E", "shortCiteRegEx": "Belanger and Kakade.", "year": 2015}, {"title": "Neural probabilistic language models", "author": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Optimization methods for convolutional sparse coding", "author": ["Hilton Bristow", "Simon Lucey"], "venue": "arXiv preprint arXiv:1406.2407,", "citeRegEx": "Bristow and Lucey.,? \\Q2014\\E", "shortCiteRegEx": "Bristow and Lucey.", "year": 2014}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Collobert and Weston.,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Bill Dolan", "Chris Quirk", "Chris Brockett"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Measuring semantic relatedness using salient encyclopedic concepts", "author": ["Samer Hassan"], "venue": "University of North Texas,", "citeRegEx": "Hassan.,? \\Q2011\\E", "shortCiteRegEx": "Hassan.", "year": 2011}, {"title": "Convolutional dictionary learning through tensor factorization", "author": ["Furong Huang", "Animashree Anandkumar"], "venue": "In Conference and Workshop Proceedings of JMLR,", "citeRegEx": "Huang and Anandkumar.,? \\Q2015\\E", "shortCiteRegEx": "Huang and Anandkumar.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers, pages 655\u2013665", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "The Association for Computer Linguistics,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Corpus-based and knowledgebased measures of text semantic similarity", "author": ["Rada Mihalcea", "Courtney Corley", "Carlo Strapparava"], "venue": "In AAAI,", "citeRegEx": "Mihalcea et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["Jeff Mitchell", "Mirella Lapata"], "venue": "Cognitive science,", "citeRegEx": "Mitchell and Lapata.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Monolingual machine translation for paraphrase generation", "author": ["Chris Quirk", "Chris Brockett", "William B Dolan"], "venue": "In EMNLP,", "citeRegEx": "Quirk et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "Paraphrase identification with lexico-syntactic graph subsumption", "author": ["Vasile Rus", "Philip M McCarthy", "Mihai C Lintean", "Danielle S McNamara", "Arthur C Graesser"], "venue": "In FLAIRS conference,", "citeRegEx": "Rus et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2008}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C Lin", "Chris Manning", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "arXiv preprint arXiv:1503.00075,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Baselines and bigrams: Simple, good sentiment and topic classification", "author": ["Sida Wang", "Christopher D Manning"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Wang and Manning.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Manning.", "year": 2012}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "arXiv preprint arXiv:1511.08198,", "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Learning composition models for phrase embeddings", "author": ["Mo Yu", "Mark Dredze"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Yu and Dredze.,? \\Q2015\\E", "shortCiteRegEx": "Yu and Dredze.", "year": 2015}, {"title": "Self-adaptive hierarchical sentence model", "author": ["Han Zhao", "Zhengdong Lu", "Pascal Poupart"], "venue": "arXiv preprint arXiv:1504.05070,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al.", "startOffset": 166, "endOffset": 187}, {"referenceID": 2, "context": "This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al.", "startOffset": 166, "endOffset": 216}, {"referenceID": 2, "context": "This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al. (2011); Mikolov et al.", "startOffset": 166, "endOffset": 241}, {"referenceID": 2, "context": "This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al. (2011); Mikolov et al. (2013); Pennington et al.", "startOffset": 166, "endOffset": 264}, {"referenceID": 2, "context": "This involves mapping words to vector representations such that words which share similar semantic or syntactic meanings are close to one another in the vector space Bengio et al. (2006); Collobert and Weston (2008); Collobert et al. (2011); Mikolov et al. (2013); Pennington et al. (2014). Word embeddings have attained state-of-the-art performance in tasks such as part-of-speech (POS) tagging, chunking, named entity recognition (NER), and semantic role labeling.", "startOffset": 166, "endOffset": 290}, {"referenceID": 17, "context": "(2010); Yu and Dredze (2015). The complexity of the compositionality varies widely: from simple operations such as addition Mitchell and Lapata (2010); Yu and Dredze (2015) to complicated recursive neural networks Socher et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 13, "context": "The complexity of the compositionality varies widely: from simple operations such as addition Mitchell and Lapata (2010); Yu and Dredze (2015) to complicated recursive neural networks Socher et al.", "startOffset": 94, "endOffset": 121}, {"referenceID": 13, "context": "The complexity of the compositionality varies widely: from simple operations such as addition Mitchell and Lapata (2010); Yu and Dredze (2015) to complicated recursive neural networks Socher et al.", "startOffset": 94, "endOffset": 143}, {"referenceID": 1, "context": "(2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al.", "startOffset": 14, "endOffset": 41}, {"referenceID": 1, "context": "(2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al. (2014b,b), long short-term memory (LSTM) recurrent neural networks Tai et al. (2015), or combinations of these architectures Wieting et al.", "startOffset": 14, "endOffset": 177}, {"referenceID": 1, "context": "(2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al. (2014b,b), long short-term memory (LSTM) recurrent neural networks Tai et al. (2015), or combinations of these architectures Wieting et al. (2015). All these methods produce sentence representations that depend on a supervised task, and the class labels are back-propagated to update the composition weights Kalchbrenner et al.", "startOffset": 14, "endOffset": 239}, {"referenceID": 1, "context": "(2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al. (2014b,b), long short-term memory (LSTM) recurrent neural networks Tai et al. (2015), or combinations of these architectures Wieting et al. (2015). All these methods produce sentence representations that depend on a supervised task, and the class labels are back-propagated to update the composition weights Kalchbrenner et al. (2014a). Since the above methods rely heavily on the downstream task and the domain of the training samples, they can hardly be used as universal embeddings across domains, and require intensive pre-training and hyper-parameter tuning.", "startOffset": 14, "endOffset": 428}, {"referenceID": 1, "context": "(2011, 2013); Belanger and Kakade (2015), convolutional neural networks Kalchbrenner et al. (2014b,b), long short-term memory (LSTM) recurrent neural networks Tai et al. (2015), or combinations of these architectures Wieting et al. (2015). All these methods produce sentence representations that depend on a supervised task, and the class labels are back-propagated to update the composition weights Kalchbrenner et al. (2014a). Since the above methods rely heavily on the downstream task and the domain of the training samples, they can hardly be used as universal embeddings across domains, and require intensive pre-training and hyper-parameter tuning. The state-of-the-art unsupervised framework is Skip-thought Kiros et al. (2015), based on an objective function that abstracts the skip-gram model to the sentence level, and encodes a sentence to predict the sentences around it.", "startOffset": 14, "endOffset": 736}, {"referenceID": 4, "context": "A popular heuristic for solving (1) is based on alternate minimization (AM) Bristow and Lucey (2014), where the phrase embeddings fi are optimized, while keeping the activations wi fixed, and vice versa.", "startOffset": 76, "endOffset": 101}, {"referenceID": 4, "context": "A popular heuristic for solving (1) is based on alternate minimization (AM) Bristow and Lucey (2014), where the phrase embeddings fi are optimized, while keeping the activations wi fixed, and vice versa. Each alternating update can be solved efficiently since it is linear in each of the variables. However, there are two main drawbacks: computational inefficiency and sub-optimality. AM requires a pass over all the samples in each iteration and is therefore computationally expensive in the large sample setting. Moreover, due to the non-convexity of the objective function as in (1), obtaining the global optimum of (1) is NP-hard in general. AM has no local or global convergence guarantees even in usual dictionary learning setting (multiplicative model). This problem is severely amplified in the convolutional setting due to additional symmetries. Due to shift invariance of the convolutional operator, shifting a phrase embedding fi by some amount, and applying a corresponding negative shift on the activation wi leaves the objective in (1) unchanged. Thus, solving (1) is fundamentally ill-posed and has a large number of equivalent solutions. To solve the computational inefficiency and sub-optimality problem, we propose a convolutional tensor decomposition method Huang and Anandkumar (2015). Our convolutional tensor decomposition method employs the inverse method of moments, and decompose a data cumulant (empirically computed from aggregate statistics or data moments) as phrase embeddings and shifted versions of phrase embeddings.", "startOffset": 76, "endOffset": 1305}, {"referenceID": 4, "context": "A popular heuristic for solving (1) is based on alternate minimization (AM) Bristow and Lucey (2014), where the phrase embeddings fi are optimized, while keeping the activations wi fixed, and vice versa. Each alternating update can be solved efficiently since it is linear in each of the variables. However, there are two main drawbacks: computational inefficiency and sub-optimality. AM requires a pass over all the samples in each iteration and is therefore computationally expensive in the large sample setting. Moreover, due to the non-convexity of the objective function as in (1), obtaining the global optimum of (1) is NP-hard in general. AM has no local or global convergence guarantees even in usual dictionary learning setting (multiplicative model). This problem is severely amplified in the convolutional setting due to additional symmetries. Due to shift invariance of the convolutional operator, shifting a phrase embedding fi by some amount, and applying a corresponding negative shift on the activation wi leaves the objective in (1) unchanged. Thus, solving (1) is fundamentally ill-posed and has a large number of equivalent solutions. To solve the computational inefficiency and sub-optimality problem, we propose a convolutional tensor decomposition method Huang and Anandkumar (2015). Our convolutional tensor decomposition method employs the inverse method of moments, and decompose a data cumulant (empirically computed from aggregate statistics or data moments) as phrase embeddings and shifted versions of phrase embeddings. The entire process requires one pass of data to compute the cumulant whereas AM requires data passes in each iteration. The reason why our tensor decomposition framework avoids multiple passes of the data samples is that we only estimate the phrase embeddings fi in the learning step. Moreover, the algorithm is carefully implemented and algorithmically optimized that it requires only simple operations such as Fast Fourier Transforms (FFT) and matrix multiplications. These operations have a high degree of parallelism: for estimating L phrase embeddings, each of length n, we require O(log n+logL) time and O(L2n3) processors. Our convolutional tensor decomposition yields optimization problems (in each iteration) that can be solved in closed form and it converges much faster compared to AM Huang and Anandkumar (2015).", "startOffset": 76, "endOffset": 2374}, {"referenceID": 9, "context": "Figure 3: Convolutional tensor decomposition for learning convolutional ICA models Huang and Anandkumar (2015).(a) The convolutional generative model with template phrases.", "startOffset": 83, "endOffset": 111}, {"referenceID": 3, "context": "We can also extend to dependent distributions such as Dirichlet for w\u2217, along the lines of Blei et al. (2003), but limit ourselves to ICA model for simplicity.", "startOffset": 91, "endOffset": 110}, {"referenceID": 9, "context": "Under the convolution ICA model, we show that the third order cumulant has a nice tensor decomposition form Huang and Anandkumar (2015), as given below.", "startOffset": 108, "endOffset": 136}, {"referenceID": 0, "context": "The decomposition form in (4) is known as the CANDECOMP/PARAFAC (CP) decomposition form Anandkumar et al. (2014) (the usual form has the decomposition of the tensor and not its unfolding, as above).", "startOffset": 88, "endOffset": 113}, {"referenceID": 9, "context": "The objective function is defined in Huang and Anandkumar (2015), refer to appendix A for details.", "startOffset": 37, "endOffset": 65}, {"referenceID": 13, "context": "Our approach is different from skip thoughts, where universal phrase embeddings are generated Kiros et al. (2015).", "startOffset": 94, "endOffset": 114}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.", "startOffset": 22, "endOffset": 46}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.4 93.2 MNB Wang and Manning (2012) 79.", "startOffset": 22, "endOffset": 84}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.4 93.2 MNB Wang and Manning (2012) 79.0 93.6 cBoW Zhao et al. (2015) 77.", "startOffset": 22, "endOffset": 118}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.4 93.2 MNB Wang and Manning (2012) 79.0 93.6 cBoW Zhao et al. (2015) 77.2 91.3 GrConv Zhao et al. (2015) 76.", "startOffset": 22, "endOffset": 154}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.4 93.2 MNB Wang and Manning (2012) 79.0 93.6 cBoW Zhao et al. (2015) 77.2 91.3 GrConv Zhao et al. (2015) 76.3 89.5 RNN Zhao et al. (2015) 77.", "startOffset": 22, "endOffset": 187}, {"referenceID": 21, "context": "Method MR SUBJ NB-SVM Wang and Manning (2012) 79.4 93.2 MNB Wang and Manning (2012) 79.0 93.6 cBoW Zhao et al. (2015) 77.2 91.3 GrConv Zhao et al. (2015) 76.3 89.5 RNN Zhao et al. (2015) 77.2 93.7 BRNN Zhao et al. (2015) 82.", "startOffset": 22, "endOffset": 221}, {"referenceID": 12, "context": "2 CNN Kim (2014) 81.", "startOffset": 6, "endOffset": 17}, {"referenceID": 12, "context": "2 CNN Kim (2014) 81.5 93.4 AdaSent Zhao et al. (2015) 83.", "startOffset": 6, "endOffset": 54}, {"referenceID": 12, "context": "2 CNN Kim (2014) 81.5 93.4 AdaSent Zhao et al. (2015) 83.1 95.5 Paragraph-vector Le and Mikolov (2014) 74.", "startOffset": 6, "endOffset": 103}, {"referenceID": 12, "context": "2 CNN Kim (2014) 81.5 93.4 AdaSent Zhao et al. (2015) 83.1 95.5 Paragraph-vector Le and Mikolov (2014) 74.8 90.5 Skip-thought Kiros et al. (2015) 75.", "startOffset": 6, "endOffset": 146}, {"referenceID": 13, "context": "Method Outside Information 1 F score Vector Similarity Mihalcea et al. (2006) word similarity 75.", "startOffset": 55, "endOffset": 78}, {"referenceID": 8, "context": "3% ESA Hassan (2011) word semantic profiles 79.", "startOffset": 7, "endOffset": 21}, {"referenceID": 8, "context": "3% ESA Hassan (2011) word semantic profiles 79.3% LSA Hassan (2011) word semantic profiles 79.", "startOffset": 7, "endOffset": 68}, {"referenceID": 8, "context": "3% ESA Hassan (2011) word semantic profiles 79.3% LSA Hassan (2011) word semantic profiles 79.9% RMLMG Rus et al. (2008) syntacticinfo 80.", "startOffset": 7, "endOffset": 121}, {"referenceID": 8, "context": "3% ESA Hassan (2011) word semantic profiles 79.3% LSA Hassan (2011) word semantic profiles 79.9% RMLMG Rus et al. (2008) syntacticinfo 80.5% ConvDic+DeconvDec none 80.7% Skip-thought Kiros et al. (2015) train large book corpus 81.", "startOffset": 7, "endOffset": 203}, {"referenceID": 18, "context": "2 Evaluation Task: Paraphrase Detection We consider the paraphrase detection task on the Microsoft paraphrase corpus Quirk et al. (2004); Dolan et al.", "startOffset": 117, "endOffset": 137}, {"referenceID": 7, "context": "(2004); Dolan et al. (2004). We employ 4076 sentence pairs as training data to learn the sentence embeddings and regress on the ground truth binary labels with our learned sentence embeddings.", "startOffset": 8, "endOffset": 28}, {"referenceID": 7, "context": "(2004); Dolan et al. (2004). We employ 4076 sentence pairs as training data to learn the sentence embeddings and regress on the ground truth binary labels with our learned sentence embeddings. The remaining test data is used to calculate classification error. As discussed in Tai et al. (2015), we combine the pair of sentence embeddings produced earlier wL and wR, i.", "startOffset": 8, "endOffset": 294}, {"referenceID": 7, "context": "(2004); Dolan et al. (2004). We employ 4076 sentence pairs as training data to learn the sentence embeddings and regress on the ground truth binary labels with our learned sentence embeddings. The remaining test data is used to calculate classification error. As discussed in Tai et al. (2015), we combine the pair of sentence embeddings produced earlier wL and wR, i.e., the embedding for the right and the left sentences. We generate features for classification using both the distance (absolute difference) and the product between the pair (wL, wR): [wL\u2299wR, \u2016wL \u2212wR\u2016], where \u2299 denotes the element-wise multiplication. In contrast to other unsupervised methods which are trained using outside information such as wordnet and parse trees, our unsupervised approach use no extra information, and still achieves comparable results with the state of art Wiki (2014) as in table 3.", "startOffset": 8, "endOffset": 864}, {"referenceID": 22, "context": "To frame semantic test similarity estimation task into the multi-class classification framework, the gold rating \u03c4 \u2208 [K1,K2] is discretized as p \u2208 \u2206 K2\u2212K1 in the follow manner Tai et al. (2015), pi = \u230a\u03c4\u230b \u2212 \u03c4 + 1 if i = \u230a\u03c4\u230b + 1 \u2212K1, pi = \u03c4 \u2212 \u230a\u03c4\u230b if i = \u230a\u03c4\u230b + 2 \u2212K1, and pi = 0 otherwise.", "startOffset": 176, "endOffset": 194}, {"referenceID": 22, "context": "To frame semantic test similarity estimation task into the multi-class classification framework, the gold rating \u03c4 \u2208 [K1,K2] is discretized as p \u2208 \u2206 K2\u2212K1 in the follow manner Tai et al. (2015), pi = \u230a\u03c4\u230b \u2212 \u03c4 + 1 if i = \u230a\u03c4\u230b + 1 \u2212K1, pi = \u03c4 \u2212 \u230a\u03c4\u230b if i = \u230a\u03c4\u230b + 2 \u2212K1, and pi = 0 otherwise. This reduces to finding a predicted p\u0302\u03b8 \u2208 \u2206 K2\u2212K1 given model parameters \u03b8 to be closest to p in terms of KL divergence Tai et al. (2015). We use a logistic regression classifier to predict p\u0302\u03b8 and estimate \u03c4\u0302\u03b8 = [K1, .", "startOffset": 176, "endOffset": 425}, {"referenceID": 22, "context": "To frame semantic test similarity estimation task into the multi-class classification framework, the gold rating \u03c4 \u2208 [K1,K2] is discretized as p \u2208 \u2206 K2\u2212K1 in the follow manner Tai et al. (2015), pi = \u230a\u03c4\u230b \u2212 \u03c4 + 1 if i = \u230a\u03c4\u230b + 1 \u2212K1, pi = \u03c4 \u2212 \u230a\u03c4\u230b if i = \u230a\u03c4\u230b + 2 \u2212K1, and pi = 0 otherwise. This reduces to finding a predicted p\u0302\u03b8 \u2208 \u2206 K2\u2212K1 given model parameters \u03b8 to be closest to p in terms of KL divergence Tai et al. (2015). We use a logistic regression classifier to predict p\u0302\u03b8 and estimate \u03c4\u0302\u03b8 = [K1, . . . ,K2]p\u0302. Results on STS task datasets are illustrated in Table 4. As in Wieting et al. (2015), Pearson\u2019s r of the median, 75th percentile, and highest score from the official task rankings are showed.", "startOffset": 176, "endOffset": 604}, {"referenceID": 22, "context": "To frame semantic test similarity estimation task into the multi-class classification framework, the gold rating \u03c4 \u2208 [K1,K2] is discretized as p \u2208 \u2206 K2\u2212K1 in the follow manner Tai et al. (2015), pi = \u230a\u03c4\u230b \u2212 \u03c4 + 1 if i = \u230a\u03c4\u230b + 1 \u2212K1, pi = \u03c4 \u2212 \u230a\u03c4\u230b if i = \u230a\u03c4\u230b + 2 \u2212K1, and pi = 0 otherwise. This reduces to finding a predicted p\u0302\u03b8 \u2208 \u2206 K2\u2212K1 given model parameters \u03b8 to be closest to p in terms of KL divergence Tai et al. (2015). We use a logistic regression classifier to predict p\u0302\u03b8 and estimate \u03c4\u0302\u03b8 = [K1, . . . ,K2]p\u0302. Results on STS task datasets are illustrated in Table 4. As in Wieting et al. (2015), Pearson\u2019s r of the median, 75th percentile, and highest score from the official task rankings are showed. We then compare our method against the performance of supervised models in Wieting et al. (2015): PARAGRAM-PHRASE (PP), projection (proj.", "startOffset": 176, "endOffset": 808}, {"referenceID": 13, "context": "), deep-averaging network (DAN), recurrent neural network (RNN) and LSTM; as well as the state-of-the-art unsupervised model skip-thought vectors Kiros et al. (2015). As we can see from the table, LST is performing poorly even though a back-propagation after seeing the training labelings is carried out for sequence embedding learning.", "startOffset": 146, "endOffset": 166}, {"referenceID": 25, "context": "The second three columns are reported by Wieting et al. (2015). Our comparison against the state-ofthe-art unsupervised word-sequence embedding method is in the last two columns.", "startOffset": 41, "endOffset": 63}], "year": 2016, "abstractText": "Text embeddings have played a key role in obtaining state-of-the-art results in natural language processing. Word2Vec and its variants have successfully mapped words with similar syntactic or semantic meanings to nearby vectors. However, extracting universal embeddings of longer word-sequences remains a challenging task. We employ the convolutional dictionary model for unsupervised learning of embeddings for variable length word-sequences. We propose a two-phase ConvDic+DeconvDec framework that first learns dictionary elements (i.e., phrase templates), and then employs them for decoding the activations. The estimated activations are then used as embeddings for downstream tasks such as sentiment analysis, paraphrase detection, and semantic textual similarity estimation. We propose a convolutional tensor decomposition algorithm for learning the phrase templates. It is shown to be more accurate, and much more efficient than the popular alternating minimization in dictionary learning literature. Our word-sequence embeddings achieve state-of-the-art performance in sentiment classification, semantic textual similarity estimation, and paraphrase detection over eight datasets from various domains, without requiring pre-training or additional features.", "creator": "LaTeX with hyperref package"}}}