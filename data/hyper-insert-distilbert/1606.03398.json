{"id": "1606.03398", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Bootstrapping Distantly Supervised IE Using Joint Learning and Small Well-Structured Corpora", "abstract": "we propose utilizing a framework to perhaps improve performance somewhat of distantly - supervised frame relation extraction, by employing jointly learning criteria to solve with two related tasks : tandem concept - instance comparison extraction and relation matching extraction. we combine this with generating a significantly novel early use of delayed document structure : that in some small, well - documented structured corpora, label sections structures can instantly be identified that correspond rapidly to relation pattern arguments, and relatively distantly - labeled examples return from when such sections nowadays tend to traditionally have good precision. whilst using these as seeds we extract additional stimulus relation examples by unsuccessfully applying label propagation transformations on a graph composed of noisy stimulus examples extracted from a large unstructured testing corpus. essentially combined with the expected soft constraint conditions that concept set examples find should have the same strict type factor as the naive second argument of discovering the conceptual relation, subsequently we get significant improvements in over several state - of - the - art approaches to assist distantly - supervision supervised relation extraction.", "histories": [["v1", "Fri, 10 Jun 2016 17:14:11 GMT  (407kb,D)", "http://arxiv.org/abs/1606.03398v1", "10 pages, 5 figures"], ["v2", "Thu, 11 Aug 2016 01:22:30 GMT  (415kb,D)", "http://arxiv.org/abs/1606.03398v2", "10 pages, 5 figures"]], "COMMENTS": "10 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lidong bing", "bhuwan dhingra", "kathryn mazaitis", "jong hyuk park", "william w cohen"], "accepted": true, "id": "1606.03398"}, "pdf": {"name": "1606.03398.pdf", "metadata": {"source": "CRF", "title": "Bootstrapping Distantly Supervised IE using Joint Learning and Small Well-structured Corpora", "authors": ["Lidong Bing", "Bhuwan Dhingra", "Kathryn Mazaitis", "Jong Hyuk Park", "William Cohen"], "emails": ["wcohen}@cs.cmu.edu,", "katie@rivard.org"], "sections": [{"heading": "1 Introduction", "text": "In distantly-supervised information extraction (IE), a knowledge base (KB) of relation or concept instances is used to train an IE system. For example, a set of facts like sideEffect(meloxicam, stomachBleeding), interactsWith(meloxicam, ibuprofen), etc are matched against a corpus, and the matching sentences are then used to generate training data consisting of labeled relation mentions. Distant supervision is less expensive to obtain than directly supervised labels, but produces noisy training data whenever matching errors occur. Hence distant supervision is often coupled with learning methods that allow for this sort of noise, e.g., by introducing latent variables for each entity mention (Hoffmann\net al., 2011; Riedel et al., 2010; Surdeanu et al., 2012); by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010); or by careful filtering of the KB strings used as seeds (Movshovitz-Attias and Cohen, 2012).\nAnother recently-introduced approach to reducing the noise in distant supervision is to combine distant labeling with label propagation (LP) (Bing et al., 2015; Bing et al., 2016). Label propagation is a family of graph-based semi-supervised learning (SSL) methods in which instances that are \u201cnearby\u201d in the graph are encouraged to have similar labels. Depending on the LP method used, agreement with seed labels can be imposed as a hard constraint (Zhu et al., 2003) or a soft constraint (Lin and Cohen, 2010; Talukdar and Cohen, 2014). When seed-label agreement is a soft constraint, then LP can be viewed as a way of smoothing the seed labels, so that labels for groups of \u201csimilar\u201d instances (i.e., instances nearby in the graph) are upweighted if they agree, and downweighted if they disagree.\nIn combining distant supervision with LP, one must build a graph that connects instances that are likely to have the same label. Previously, systems have constructed graphs which connect mentions appearing in the same coordinate-list structure\u2014 e.g., the underlined noun phrases in \u201cGet medical help if you experience chest pain, weakness, or shortness of breath\u201d (Bing et al., 2015). This approach was shown to improve performance in recognizing instances of certain medical noun-phrase (NP) categories, such as drug names and disease names. An extension of this approach (Bing et al., 2016) learned to classify NP pairs as relations, using a more complex graph structure.\nar X\niv :1\n60 6.\n03 39\n8v 1\n[ cs\n.C L\n] 1\n0 Ju\nn 20\n16\nThis paper presents three new contributions extending this line of work. First, we combine the concept-instance extraction and relation-extraction tasks, in the process greatly simplifying the relationextraction LP step. The combination of the tasks is simple but effective. In (Bing et al., 2016), relation extraction was performed on an \u201centity centric\u201d corpus, where each document is primarily concerned with a particular \u201ctitle entity\u201d, and the first argument of each relation is always the title entity: hence relation extraction can be viewed as classification, where an entity mention is labeled with its slot filling role, i.e., its relation to the title entity. The intuition behind combining concept extraction and relation extraction is that relation arguments are often constrained to be of a particular type; for example, the sideEffect of a drug is necessarily of the type symptom.\nThe second contribution is a novel use of document structure; in particular, we exploit the fact that in some small, well-structured corpora, sections can be identified that correspond fairly accurately to relation arguments. Figure 1 shows a document from such a structured corpus (discussed below) which contains sections labeled \u201cSide Effects\u201d. If \u201cnausea\u201d is distantly labeled as a sideEffect of meloxicam in this well-structured document, it is very likely to be a correct mention for the sideEffect relation. Used naively, extending a corpus with a small well-structured one needs not to lead to improvements, but when combined with LP, we show a consistent and sometimes substantial improvement in performance. We thus illustrate a novel and effective way to make use of a small wellstructured corpus, a commonly available resource that is intermediate in structure between a KB and an ordinary text corpus.\nThe third contribution is experimental. We\nperform extensive experiments comparing this approach to state-of-the-art distant labeling methods based on latent variables, and show substantial improvements in two domains: the relative improvements under F1 measure are from 72% to 110% on one domain, and 22% to 30% on a second domain.\nBelow we present our method, in outline and then in detail; present experimental results; discuss related work; and finally conclude.\n2 DIEJOB: Distant IE by JOint Bootstrapping"}, {"heading": "2.1 Overview", "text": "DIEJOB, our system for distantly-supervised relation extraction, is shown in Figure 2. We consider a common case, in which most information is found in relatively unstructured free text, but some smaller corpora exist that are well-structured. DIEJOB thus assumes at least two corpora exist for the domain of interest: a large target corpus and a smaller structured corpus. Further, it assumes that every document in these two corpora is associated with a particular entity, called title entity or subject entity. Many widely-used corpora have this structure, including Wikipedia and the authoritative consumer-oriented websites we use, DailyMed and WebMD.\nFrom each corpus, DIEJOB produces two types of mention sets: relation mention set R and concept mention set C. For the example of Figure 1, R contains a sideEffect relation mention for \u201cstomach upset\u201d from the first sentence, and C may contain mentions of the Symptom concept, like \u201cstomach upset\u201d and \u201cnausea\u201d from the same sentence. The tail argument values (such as \u201cnausea\u201d in sideEffect(meloxicam, nausea)) of a relation are often from a particular unary con-\ncept. This is especially true in the biomedical domain, where for example, sideEffect takes instances of Symptom as the value range of its second argument. Naively, those concept mentions in C could serve as a source to generate relation examples, but not all concept mentions are relation mentions: e.g., the Symptom mentions of \u201cconfusion\u201d and \u201cmood changes\u201d from \u201cSymptoms of overdose may include: confusion, mood changes ...\u201d are not mentions of the sideEffect relation (or any other relation we currently extract). For the structured corpus, the relation and concept mention sets are referred to as Rs and Cs, and for the target corpus as Rt and Ct. Some special treatments (discussed in Section 2.3) are done while preparing Rs and Cs.\nAfter producing Rs, Rt, Cs and Ct, DIEJOB builds a bipartite graph, following prior work (Lin, 2012), in which the nodes are either mentions in the four sets, or features of these mentions, with edges between a mention and its features. To distill a cleaner set of relation training examples, DIEJOB performs LP on the bipartite graph. Only the mentions from Rs are used as seed relation examples in this LP stage (because they are more accurate, see Section 2.3).\nFinally the distilled relation examples are used to train an ordinary SVM classifier over their extracted features. DIEJOB thus finally learns to classify an unseen mention by the relation which holds between the mention and its corresponding title entity based on features of the mention\u2014a convenient architecture to use for large-scale extraction.\nBelow we will describe the components of DIEJOB and the experiments in more detail."}, {"heading": "2.2 Relations and Corpora", "text": "Even large curated KBs are often incomplete and the situation is worse in the medical domain where the coverage of large KBs like Freebase is fairly limited. We focus on extracting instances of eight relations, defined in Freebase, about drugs and diseases. The drug relations are usedToTreat, conditionsThisMayPrevent, and sideEffect, and the concept types of their second arguments are DiseaseOrMedicalCondition, DiseaseOrMedicalCondition, and Symptom. The disease relations are hasTreatment, hasSymptom, riskFactor, hasCause, and\npreventionFactor, with corresponding concept types as MedicalTreatment, Symptom, RiskFactor, DiseaseCause, and ConditionPreventionFactor.\nWe are primarily concerned with extraction from large, authoritative sources. Our target drug corpus, called DailyMed, is downloaded from dailymed.nlm.nih.gov and contains 28,590 XML documents. Our target disease corpus, called WikiDisease, is extracted from a Wikipedia dump of May 2015 and contains 8,596 disease articles. The structured drug corpus1, called WebMD, contains 2,096 pages collected from www.webmd.com. Each page has the same sections, such as Uses and Side Effects, corresponding to usedToTreat/conditionsThisMayPrevent andsideEffect relations, respectively. The structured disease corpus, called MayoClinic, contains 1,117 pages collected from www.mayoclinic.org. Each page also has regular sections, such as Symptoms, Causes, Risk Factors, Treatments/Drugs, and Prevention, corresponding to hasSymptom, hasCause, riskFactor, hasTreatment, and preventionFactor, respectively. These corpora are all entity centric, i.e., each pages discusses a single entity.\nWe use GDep (Sagae and Tsujii, 2007), a dependency parser trained on GENIA Treebank, to parse the corpora, followed by a simple POS-tag based chunker to extract NPs. We also extract a list (e.g. \u201cstomach upset, nausea, and dizziness\u201d) for each coordinating conjunction that modifies a nominal. For each NP mention, we extract features (described below) from its sentence; and for each coordinate list, we extract the similar features and the NP chunks included in it. A mention not inside a list is regarded as a singleton list that contains only one item."}, {"heading": "2.3 Mention Preparation", "text": "Relation mention sets, i.e. Rs and Rt, are prepared with distant supervision. The extracted NP mentions are distantly labeled using relation seed triples from Freebase (e.g. sideEffect(meloxican,nausea)). Specifically, we require that the title entity matches the first\n1It is not difficult to find such structured pages in different domains, such as scientist (http://famouschemists.org/, having \u201cFamous For\u201d, \u201cAwards\u201d, and \u201cDiscoveries\u201d sections) and movie (http://www.imdb.com/chart/top, having \u201cAwards\u201d, \u201cPlot Summary\u201d, etc.)\nargument value of the relation, and the NP mention matches the second argument value. To improve the quality of Rs, we also require that the section from which the mention was taken is relevant to the relation. E.g., a mention labeled with the sideEffect relation must appear in a section entitled Side Effects. Such constraint limits the number of mentions in Rs. In the next section, we will show how to extend this small but accurate example set to a larger training set of examples, with reasonable quality.\nThe concept mentions are designed to have high recall with respect to possible argument values for a relation. For each relation r, we generate a set of concept mentions which lie in the range of r\u2019s second argument. Following the DIEL system (Bing et al., 2015), we extract concept instances from Freebase as seeds, and extend the seed set using LP in each corpus. The reached coordinate-term lists and singleton lists (NPs) are collected as concept mentions. Thus, we get two concept mention sets: Cs from the structured corpus, and Ct from the target corpus. Note that some mentions in Cs may come from unrelated sections; for instance, Cs for the Symptom concept may contain mentions from the Overdose section, which cannot be examples of the sideEffect relation. Therefore, we filter out the mentions in Cs that are not from the appropriate section for this concept.\nWe emphasize that the section-specific processing is only done on the structured corpus, i.e. for Cs and Rs. Our target corpora have thousands of section titles, most of which are not related in any way to the relations being extracted. Thus the target relation mentions (Rt) and target concept mentions (Ct) are collected without considering section information."}, {"heading": "2.4 Relation Label Propagation", "text": "With the relation mentions and the concept mentions lying in the range of the corresponding relation, we are able to distill a cleaner set of training relation examples to learn extractors. Rs contains more confident relation examples because of constraints by document structure, but it is limited in size. In contrast, the number of Rt mentions is larger, but they are noisier. In general, the degree to which Rt mentions will be useful may be domain- and corpus-specific. Cs and Ct are generated with respect to the type of the men-\ntions, but not their relationship with the title entity: e.g., a mention in Ct corresponding to the NP \u201cdizziness\u201d would not be associated with the triple sideEffect(meloxican,dizziness); and indeed, dizziness might be a condition treated by, not caused by, the title entity \u201cmeloxican\u201d. Therefore, Ct itself cannot be directly used as relation examples, however, it can serve as a resource to distill relation examples. In our experiments, Rs mentions are always used as seed relation examples in LP, but we build bipartite propagation graphs with different combinations of the four sets of mentions and study their performance.\nIn total, we have 7 bipartite graphs, each with a different set of mentions from the following combinations: Rs \u222a Cs \u222a Rt \u222a Ct, Rs \u222a Cs \u222a Rt, Rs \u222a Cs \u222a Ct, Rs \u222a Cs, Rs \u222a Rt \u222a Ct, Rs \u222a Rt, or Rs \u222a Ct. In a bipartite graph, one set of nodes are mentions, and the other set of nodes are features of mentions. An edge is added between each feature and each mention containing that feature. The edges are TFIDF-weighted (treating the features as words and the mentions as documents). Figure 3 shows such a bipartite graph (edge weights are omitted), which has four mentions on the left-hand side, and eight features on the right-hand side.\nWe use an existing multi-class label propagation method, namely, MultiRankWalk (MRW) (Lin and Cohen, 2010), which is a graph-based SSL method related to personalized PageRank (PPR) (Haveliwala et al., 2003) (aka random walk with restart (Tong et al., 2006)). MRW can be viewed simply as computing a personalized PageRank vector for each class, each of which is computed using a personalization vector that is initially uniform over the seeds, and finally assigning to each node the class associ-\nated with its highest-scoring vector. MRW\u2019s final scores depend on the centrality of nodes, as well as their proximity to seeds. The MRW implementation we use is based on ProPPR (Wang et al., 2013)."}, {"heading": "2.5 Classifier Learning", "text": "Given the ranked mention lists of these relation labels from the above LP, we pick the top N to train binary classifiers, which can then be used to classify the entity mentions (singleton lists) and coordinate lists in a new document. We use the same feature generator for both mentions and lists. Shallow features include: tokens in the NPs, and character prefixes/suffixes of these tokens; BOW from the sentence containing the NP; and tokens and bigrams from a window around the NPs. From dependency parsing, we find the verb which is closest ancestor of the head of current NP, all modifiers of this verb, and the path to this verb. For lists, the dependency features are computed relative to the head of the list.\nWe use SVMs (Chang and Lin, 2001) and discard singleton features, as well as the most frequent 5% of features (as a stop-wording variant). Specifically, binary classifiers are trained with examples of one relation as the positives, and examples of the other classes as negatives. We also add N general negative examples, randomly picked from those that are not distantly labeled by any relation. A linear kernel and default values for all other parameters are used 2. A threshold 0.5 is used to cut positive and negative predictions. If a new list or mention is not classified as positive by any classifier, it is predicted as \u201cother\u201d."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Evaluation Dataset", "text": "Our evaluation dataset contains 20 manually labeled pages, 10 pages each from the disease corpus WikiDisease and the drug corpus DailyMed. This data was originally generated in (Bing et al., 2016). The annotated text fragments are manually chunked NPs which are the second argument values of any of the eight relations considered here, with the title drug or disease entity of the corresponding document as the relation subject. The evaluation data contains 436 triples for the disease domain and 320 triples for the drug domain. A system\u2019s task then is\n2https://www.csie.ntu.edu.tw/ cjlin/libsvm/\nto extract all correct values of the second argument of a given relation from a test document."}, {"heading": "3.2 Experimental Comparisons", "text": "The first three baselines are distant supervision (DS) systems. They classify each testing NP mention into one of the interested relation types or \u201cother\u201d, using naive matching to the Freebase seed triples as distant supervision. Each sentence in the corpus is processed with the same preprocessing pipeline to detect NPs. Then, these NPs are labeled with the Freebase seed triples. The features are defined and extracted in the same way as we did for DIEJOB, and binary classifiers are trained with the same method. The first DS baseline, named DS Struct, only uses the section-filtered examples from a structured corpus, i.e. Rs, as training data. The second DS baseline, named DS Target, only uses labeled examples from the target corpus, i.e. Rt. While the third DS baseline, named DS Both, uses examples from both target corpus and structured corpus.\nWe also compare against two latent variable learners. The first is MultiR (Hoffmann et al., 2011) which models each relation mention separately and aggregates their labels using a deterministic OR. The second one is MIML-RE (Surdeanu et al., 2012) which has a similar structure to MultiR, but uses a classifier to aggregate the mention level predictions into an entity pair prediction. We used the publicly available code from the authors3 for our experiments. Since these methods do not distinguish between structured and unstructured corpora, we used the union of these corpora in our experiments, and the feature set used in the bipartite graph. We found that the performance of these methods varies significantly with the number of negative examples used during training, and hence we tuned these and other parameters4 directly on the evaluation data, and report their best performance. Another distantsupervision baseline we compare to is the Mintz++ model from (Surdeanu et al., 2012), which improves on the original model from (Mintz et al., 2009) by training multiple classifiers, and allowing multiple labels per entity pair.\n3http://aiweb.cs.washington.edu/ai/raphaelh/mr/ and http://nlp.stanford.edu/software/mimlre.shtml\n4Parameters include the number of epochs (for both MultiR and MIML-RE) and the number of training folds for MIML-RE.\nWe also compare with DIEBOLDS (Bing et al., 2016), which uses LP on a graph containing entity mention pairs. The graph used by DIEBOLDS is more complex than the mention-feature graph used here, in DIEJOB. One set of vertices correspond to (title-entity, mention-entity) pairs. The other set of vertices are identifiers for coordinate lists: a mention pair is connected with the lists from any document describing the subject, and containing the mention. Additional edges are also introduced based on document structure and BOW context features. DIEBOLDS performs label propagation from the mention pairs distantly labeled with Freebase relation triples."}, {"heading": "3.3 Experimental Settings", "text": "We extracted triples of the eight relations from Freebase as distant labeling seeds. Specifically, if the subject of a triple matches with a drug or disease name in a corpus and its object value also appears in that document, it is extracted. For the disease domain, we get 2022, 2453, 905, 753, and 164 triples for hasTreatment, hasSymptom, riskFactor, hasCause, and preventionFactor, respectively. For the drug domain, we get 3112, 315, and 265 triples for usedToTreat, conditionsThisMayPrevent, and sideEffect, respectively.\nWe have two strategies to pick the top N lists for classifier learning. One strategy picks the top N directly, without distinguishing if they come from the structured corpus or the target corpus. It is referred to as DIEJOB Both. The other strategy picks the top N examples only from the target corpus, and it is referred to as DIEJOB Target. Here our concern is the difference between the feature distributions of the two corpora.\nWe evaluate the performance of different systems from an IR perspective: a title entity (i.e., document name) and a relation together act as a query, and the extracted NPs as retrieval results."}, {"heading": "3.4 Results on Labeled Pages", "text": "The results for precision, recall and F1 measure are given in Table 1. The results for DIEBOLDS are from (Bing et al., 2016). The systems with \u201c*\u201d are directly tuned on the evaluation data and should be considered as upper bounds on true per-\nformance. DIEJOB Target and DIEJOB Both are tuned with a tuning dataset (details in Section 3.5). (Note that for the disease domain, DIEJOB Both and DIEJOB Both* get the same results, because they use the same parameters, although they are tuned with different data.)\nDIEJOB Both outperforms all the other systems. Compared with MultiR, Mintz++, and MIML-RE, the relative improvements under the F1 measure are 22% to 30% in the disease domain, and 72% to 110% in the drug domain. The precision values of DIEJOB Both are much higher than previous work. For recall, DIEBOLDS and DIEJOB Both\u2019s performance are comparable to the latent-variable systems on the disease domain and much better on the drug domain. One reason may be that our method predicts one label for a coordinate-term list (lists are common in the drug domain), which implicitly coordinates the labels of list items, while MultiR, Mintz++, and MIML-RE break a list into individual items which are predicted separately.\nThe precision values of DIEBOLDS are much lower than DIEJOB, especially for the drug domain. Unlike DIEJOB, DIEBOLDS builds an LP graph containing all singleton and coordinate lists of noun phrases in the corpus, which introduces many irrelevant examples. DIEBOLDS achieves the highest recall values, but in practice, it is also likely to predict a testing mention as belonging to one of the eight relations, but not \u201cother\u201d.\nOn these tasks, the simple DS baselines\u2019 performance is competitive with MIML-RE and the other complex models. One exception is DS Struct on the drug domain, where the recall is only 0.072. This is\nperhaps because the total number of examples in Rs for the three drug relations is only 485, which is too small to get good recall. Interestingly, the precision of DS Struct is better than DS Target and DS Both for both domains, presumably, because of the high quality of the examples in Rs.\nFor the disease domain, DIEJOB Both performs better than DIEJOB Target, no matter how they are tuned (i.e. on tuning or evaluation data). This shows that the mentions from Rs and Cs of MayoClinic corpus provide good training examples. For the drug domain, DIEJOB Both and DIEJOB Target achieve similar results. This may be because DIEJOB Both is more sensitive to the difference in feature distributions of structured and target corpora, since it uses examples from the structured corpus to learn classifiers as well. Among the four corpora we use, WebMD, MayoClinic, and WikiDisease are written to be readable by a large audience, while DailyMed articles are more difficult in terms of readability: hence the difference between the structured and unstructured corpora is larger in the drug domain.\nPrecision-recall curves are given in Figure 4. For the drug domain, DIEJOB\u2019s precision is consistently better, at the same recall level, than any of the other methods. For the disease domain, our system\u2019s precision is generally better after the recall level 0.05."}, {"heading": "3.5 Tuning and Variant Comparison", "text": "Here we examine the performance of different variants, and the effect of the parameter N . The performance of all graph variants on a tuning dataset (containing 10 labeled pages) is given in Figure 5. Combined with the strategies for picking top N (i.e. DIEJOB Target and DIEJOB Both), there are 13 variants: shown in Figures 5a and 5b for disease; Figures 5c and 5d for drug. Note that\nDIEJOB Target does not have the variant RsCs, because RsCs does not contain any examples from the target corpus.\nFor the disease domain, the same variant under DIEJOB Both and DIEJOB Target performs similarly, and on average, DIEJOB Both is slightly better than DIEJOB Target. For the drug domain, on average, DIEJOB Target is better than DIEJOB Both. One explanation is that the two corpora in disease domain are similar in the aspect of feature distribution, so in general, mixing the examples from them are beneficial. However, the effect of such a mixture is negative for drug domain, whose structured and target corpora are dissimilar.\nIn Table 1, the reported results of the tuned DIEJOB Both and DIEJOB Target for the disease domain are from the variants RsCs and RsCsRt respectively, while for drug domain, both are from RsRt. One explanation could be: (1) if the structured corpus is similar to the target corpus, it is better to use DIEJOB Both, and including examples of the structured corpus (e.g., RsCs and RsCsRt, both have Cs used) generally performs well with a larger N value; (2) if the structured and target corpora are dissimilar, DIEJOB Target is better and RsRt has an advantage over other variants where the main focus is distilling good training examples from Rt and a smaller number of top N examples is preferred."}, {"heading": "4 Related Work", "text": "To overcome the noise in distantly-labeled examples, (Riedel et al., 2010) introduced an \u201cat least one\u201d heuristic, where instead of taking all mentions for a pair as correct examples only at least one of them is assumed to express that relation. MultiR (Hoffmann et al., 2011) and MIML-RE (Surdeanu et al., 2012) extend this approach to support multi-\nple relations expressed by different sentences in a bag. Unlike these approaches, DIEJOB improves the quality of training data with a bootstrapping step before feeding the noisy examples into a learner, by using the confident examples from a structured corpus as seeds. The benefit of this step is twofold. First, it distills the distantly-labeled examples by propagating labels from good seed examples, and downweights the noisy ones. Second, the propagation will walk to more relation examples in the concept mention set that cannot be distantly labeled with triples from knowledge bases.\nDocument structure was previously explored by (Bing et al., 2016), which used the structure to enrich an LP graph by adding coupling edges between mentions in the same section of particular documents. In this work, we explore the semantic association between section titles and relation arguments. Furthermore, we perform a joint bootstrapping on relation and type mentions to collect training examples with better quality. Technically, the propagation graphs used are different: DIEJOB\u2019s graph has carefully produced mention nodes (from those four sets) and their feature nodes, while DIEBOLDS\u2019s graph has triple nodes (i.e., subject-NP pairs) and all singleton and coordinate lists of noun phrases of the corpora. Accordingly, their propagation seeds are different: DIEJOB uses confident examples as seeds (labeled from particular sections of a structured cor-\npus) to propagate labels to more examples via feature similarity, while DIEBOLDS directly uses Freebase triples as seeds and propagates labels through edges built from coordinate lists and sections.\nIn the classic bootstrap learning scheme (Riloff and Jones, 1999; Agichtein and Gravano, 2000; Bunescu and Mooney, 2007), a small number of seed instances are used to extract new patterns from a large corpus, which are then used to extract more instances. Then in an iterative fashion, new instances are used to extract more patterns. DIEJOB departs from earlier bootstrapping methods in combining label propagation with a standard classification learner, and it can improve the quality of distant examples and collect new examples simultaneously."}, {"heading": "5 Conclusions", "text": "We proposed the DIEJOB framework to generate good examples for distantly-supervised IE. It exploits the document structure of a small wellstructured corpus to collect seed relation examples, and it also collects concept mentions that could be the second argument values of relations. DIEJOB then conducts label propagation to find mentions that can be confidently used as training examples to train classifiers for labeling new entity mentions. The experimental results show that this approach consistently and significantly outperforms state-ofthe-art approaches."}], "references": [{"title": "Snowball: Extracting relations from large plain-text collections", "author": ["Agichtein", "Gravano2000] Eugene Agichtein", "Luis Gravano"], "venue": "In Proceedings of the Fifth ACM International Conference on Digital Libraries", "citeRegEx": "Agichtein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Agichtein et al\\.", "year": 2000}, {"title": "Improving distant supervision for information extraction using label propagation through lists", "author": ["Bing et al.2015] Lidong Bing", "Sneha Chaudhari", "Richard C Wang", "William W. Cohen"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Nat-", "citeRegEx": "Bing et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bing et al\\.", "year": 2015}, {"title": "Distant ie by bootstrapping using lists and document structure. CoRR, abs/1601.00620", "author": ["Bing et al.2016] Lidong Bing", "Mingyang Ling", "Richard Wang", "William W. Cohen"], "venue": null, "citeRegEx": "Bing et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bing et al\\.", "year": 2016}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["Bunescu", "Mooney2007] Razvan C. Bunescu", "Raymond J. Mooney"], "venue": "ACL", "citeRegEx": "Bunescu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2007}, {"title": "LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/ \u0303cjlin/libsvm", "author": ["Chang", "Lin2001] Chih-Chung Chang", "ChihJen Lin"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "An analytical comparison of approaches to personalizing pagerank", "author": ["Sepandar Kamvar", "Ar Kamvar", "Glen Jeh"], "venue": null, "citeRegEx": "Haveliwala et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Haveliwala et al\\.", "year": 2003}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of the 49th Annual Meeting", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "Semi-supervised classification of network data using very few labels", "author": ["Lin", "Cohen2010] Frank Lin", "William W. Cohen"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Scalable methods for graph-based unsupervised and semi-supervised learning", "author": ["Frank Lin"], "venue": "Ph.D. thesis,", "citeRegEx": "Lin.,? \\Q2012\\E", "shortCiteRegEx": "Lin.", "year": 2012}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Bootstrapping biomedical ontologies for scientific text using nell", "author": ["Movshovitz-Attias", "Cohen2012] Dana MovshovitzAttias", "William W. Cohen"], "venue": "In Proceedings of the 2012 Workshop on Biomedical Natural Language Processing,", "citeRegEx": "Movshovitz.Attias et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Movshovitz.Attias et al\\.", "year": 2012}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Learning Dictionaries for Information Extraction by Multi-level Boot-strapping", "author": ["Riloff", "Jones1999] Ellen Riloff", "Rosie Jones"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence,", "citeRegEx": "Riloff et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Riloff et al\\.", "year": 1999}, {"title": "Dependency parsing and domain adaptation with lr models and parser ensembles", "author": ["Sagae", "Tsujii2007] K. Sagae", "J. Tsujii"], "venue": "In Proceedings of the CoNLL 2007 Shared Task in the Joint Conferences on Empirical Methods in Natural Language Process-", "citeRegEx": "Sagae et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sagae et al\\.", "year": 2007}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D. Manning"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Scaling graph-based semi supervised learning to large number of labels using count-min sketch", "author": ["Talukdar", "William W. Cohen"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence", "citeRegEx": "Talukdar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Talukdar et al\\.", "year": 2014}, {"title": "Fast random walk with restart and its applications", "author": ["Tong et al.2006] Hanghang Tong", "Christos Faloutsos", "Jia-Yu Pan"], "venue": "In ICDM,", "citeRegEx": "Tong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tong et al\\.", "year": 2006}, {"title": "Programming with personalized pagerank: a locally groundable first-order probabilistic logic", "author": ["Kathryn Mazaitis", "William W Cohen"], "venue": "In Proceedings of the 22nd ACM international conference on Conference", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Open information extraction using wikipedia", "author": ["Wu", "Weld2010] Fei Wu", "Daniel S Weld"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2010}, {"title": "Semi-supervised learning using Gaussian fields and harmonic functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "In Proceedings of ICML-03,", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 6, "context": ", by introducing latent variables for each entity mention (Hoffmann et al., 2011; Riedel et al., 2010; Surdeanu et al., 2012); by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010); or by careful filtering of the KB strings used as seeds (Movshovitz-Attias and Cohen, 2012).", "startOffset": 58, "endOffset": 125}, {"referenceID": 11, "context": ", by introducing latent variables for each entity mention (Hoffmann et al., 2011; Riedel et al., 2010; Surdeanu et al., 2012); by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010); or by careful filtering of the KB strings used as seeds (Movshovitz-Attias and Cohen, 2012).", "startOffset": 58, "endOffset": 125}, {"referenceID": 14, "context": ", by introducing latent variables for each entity mention (Hoffmann et al., 2011; Riedel et al., 2010; Surdeanu et al., 2012); by carefully selecting the entity mentions from contexts likely to include specific KB facts (Wu and Weld, 2010); or by careful filtering of the KB strings used as seeds (Movshovitz-Attias and Cohen, 2012).", "startOffset": 58, "endOffset": 125}, {"referenceID": 1, "context": "Another recently-introduced approach to reducing the noise in distant supervision is to combine distant labeling with label propagation (LP) (Bing et al., 2015; Bing et al., 2016).", "startOffset": 141, "endOffset": 179}, {"referenceID": 2, "context": "Another recently-introduced approach to reducing the noise in distant supervision is to combine distant labeling with label propagation (LP) (Bing et al., 2015; Bing et al., 2016).", "startOffset": 141, "endOffset": 179}, {"referenceID": 19, "context": "Depending on the LP method used, agreement with seed labels can be imposed as a hard constraint (Zhu et al., 2003) or a soft constraint (Lin and Cohen, 2010; Talukdar and Cohen, 2014).", "startOffset": 96, "endOffset": 114}, {"referenceID": 1, "context": ", the underlined noun phrases in \u201cGet medical help if you experience chest pain, weakness, or shortness of breath\u201d (Bing et al., 2015).", "startOffset": 115, "endOffset": 134}, {"referenceID": 2, "context": "An extension of this approach (Bing et al., 2016) learned to classify NP pairs as relations, using a more complex graph structure.", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "In (Bing et al., 2016), relation extraction was performed on an \u201centity centric\u201d corpus, where each document is primarily concerned with a particular \u201ctitle entity\u201d, and the first argument of each relation is always the title entity: hence relation extraction can be viewed as classification, where an entity mention is labeled with its slot filling role, i.", "startOffset": 3, "endOffset": 22}, {"referenceID": 8, "context": "After producing Rs, Rt, Cs and Ct, DIEJOB builds a bipartite graph, following prior work (Lin, 2012), in which the nodes are either mentions in the four sets, or features of these mentions, with edges between a mention and its features.", "startOffset": 89, "endOffset": 100}, {"referenceID": 1, "context": "Following the DIEL system (Bing et al., 2015), we extract concept instances from Freebase as seeds, and extend the seed set using LP in each corpus.", "startOffset": 26, "endOffset": 45}, {"referenceID": 5, "context": "We use an existing multi-class label propagation method, namely, MultiRankWalk (MRW) (Lin and Cohen, 2010), which is a graph-based SSL method related to personalized PageRank (PPR) (Haveliwala et al., 2003) (aka random walk with restart (Tong et al.", "startOffset": 181, "endOffset": 206}, {"referenceID": 16, "context": ", 2003) (aka random walk with restart (Tong et al., 2006)).", "startOffset": 38, "endOffset": 57}, {"referenceID": 17, "context": "The MRW implementation we use is based on ProPPR (Wang et al., 2013).", "startOffset": 49, "endOffset": 68}, {"referenceID": 2, "context": "This data was originally generated in (Bing et al., 2016).", "startOffset": 38, "endOffset": 57}, {"referenceID": 6, "context": "The first is MultiR (Hoffmann et al., 2011) which models each relation mention separately and aggregates their labels using a deterministic OR.", "startOffset": 20, "endOffset": 43}, {"referenceID": 14, "context": "The second one is MIML-RE (Surdeanu et al., 2012) which has a similar structure to MultiR, but uses a classifier to aggregate the mention level predictions into an entity pair prediction.", "startOffset": 26, "endOffset": 49}, {"referenceID": 14, "context": "Another distantsupervision baseline we compare to is the Mintz++ model from (Surdeanu et al., 2012), which improves on the original model from (Mintz et al.", "startOffset": 76, "endOffset": 99}, {"referenceID": 9, "context": ", 2012), which improves on the original model from (Mintz et al., 2009) by training multiple classifiers, and allowing multiple labels per entity pair.", "startOffset": 51, "endOffset": 71}, {"referenceID": 2, "context": "We also compare with DIEBOLDS (Bing et al., 2016), which uses LP on a graph containing entity mention pairs.", "startOffset": 30, "endOffset": 49}, {"referenceID": 2, "context": "The results for DIEBOLDS are from (Bing et al., 2016).", "startOffset": 34, "endOffset": 53}, {"referenceID": 11, "context": "To overcome the noise in distantly-labeled examples, (Riedel et al., 2010) introduced an \u201cat least one\u201d heuristic, where instead of taking all mentions for a pair as correct examples only at least one of them is assumed to express that relation.", "startOffset": 53, "endOffset": 74}, {"referenceID": 6, "context": "MultiR (Hoffmann et al., 2011) and MIML-RE (Surdeanu et al.", "startOffset": 7, "endOffset": 30}, {"referenceID": 14, "context": ", 2011) and MIML-RE (Surdeanu et al., 2012) extend this approach to support multi-", "startOffset": 20, "endOffset": 43}, {"referenceID": 2, "context": "Document structure was previously explored by (Bing et al., 2016), which used the structure to enrich an LP graph by adding coupling edges between mentions in the same section of particular documents.", "startOffset": 46, "endOffset": 65}], "year": 2017, "abstractText": "We propose a framework to improve performance of distantly-supervised relation extraction, by jointly learning to solve two related tasks: concept-instance extraction and relation extraction. We combine this with a novel use of document structure: in some small, well-structured corpora, sections can be identified that correspond to relation arguments, and distantly-labeled examples from such sections tend to have good precision. Using these as seeds we extract additional relation examples by applying label propagation on a graph composed of noisy examples extracted from a large unstructured testing corpus. Combined with the soft constraint that concept examples should have the same type as the second argument of the relation, we get significant improvements over several state-of-theart approaches to distantly-supervised relation extraction.", "creator": "LaTeX with hyperref package"}}}