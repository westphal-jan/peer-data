{"id": "1606.03432", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2016", "title": "Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on How Much", "abstract": "extended gibbs gibbs sampling is a balanced markov dynamic chain monte carlo fourier sampling technique that iteratively samples variables narrowly from recognizing their characteristic conditional stability distributions. there are two common simple scan orders for analyzing the random variables : symmetric random scan and systematic scan. due indirectly to spreading the continuous benefits of locality in hardware, gibbs systematic scan is commonly used, for even unofficially though, most popular statistical guarantees approaches are only specified for distributed random scan. sadly while it yourself has been conjectured that the mixing pass times of random procedure scan and conditional systematic swept scan don't differ by more than 0 a logarithmic order factor, we show by counterexample above that this is not the case, - and we prove that that the mixing times often don't directly differ just by more than a polynomial factor under mild local conditions. to prove these relative bounds, we together introduce a method approach of augmenting within the final state population space to study systematic scan probability using optical conductance.", "histories": [["v1", "Fri, 10 Jun 2016 19:24:10 GMT  (285kb,D)", "http://arxiv.org/abs/1606.03432v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["bryan d he", "christopher de sa", "ioannis mitliagkas", "christopher r\u00e9"], "accepted": true, "id": "1606.03432"}, "pdf": {"name": "1606.03432.pdf", "metadata": {"source": "META", "title": "Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on How Much", "authors": ["Bryan He", "Christopher De Sa", "Ioannis Mitliagkas"], "emails": ["bryanhe@stanford.edu", "cdesa@stanford.edu", "imit@stanford.edu", "chrismre@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Gibbs sampling, or Glauber dynamics, is a Markov chain Monte Carlo method that draws approximate samples from multivariate distributions that are difficult to sample directly [9; 15, p. 40]. A major use of Gibbs sampling is marginal inference: the estimation of the marginal distributions of some variables of interest [8]. Some applications include various computer vision tasks [9, 23, 24], information extraction [7], and latent Dirichlet allocation for topic modeling [11]. Gibbs sampling is simple to implement and quickly produces accurate samples for many models, so it is widely used and available in popular libraries such as OpenBUGS [16], FACTORIE [17], JAGS [18], and MADlib [14].\nAlgorithm 1 Gibbs sampler input Variables xi for 1 \u2264 i \u2264 n, and target distribution \u03c0\nInitialize x1, . . . , xn loop\nSelect variable index s from {1, . . . , n} Sample xs from the conditional distribution P\u03c0 ( Xs | X{1,...,n}\\{s} ) end loop\nGibbs sampling (Algorithm 1) iteratively selects a single variable and resamples it from its conditional distribution, given the other variables in the model. The method that selects the variable index to sample (s in Algorithm 1) is called the scan order. Two scan orders are commonly used: random scan and systematic scan (also known as deterministic or sequential scan). In random scan, the variable to sample is selected uniformly and independently at random at each iteration. In systematic scan, a fixed permutation is selected, and the variables are repeatedly selected in that order. The existence of these two distinct options raises an obvious question\u2014which scan order produces accurate samples more quickly? This question has two components: hardware efficiency (how long does each iteration take?) and statistical efficiency (how many iterations are needed to produce an accurate sample?).\nFrom the hardware efficiency perspective, systematic scans are clearly superior [21, 22]. Systematic scans have good spatial locality because they access the variables in linear order, which makes their iterations run faster on hardware. As a result, systematic scans are commonly used in practice.\nComparing the two scan orders is much more interesting from the perspective of statistical efficiency, which we focus on for the rest of this paper. Statistical efficiency is measured by the mixing time, which is the number of iterations needed to obtain an accurate sample [15, p. 55]. The mixing times of random scan and systematic scan have been\nar X\niv :1\n60 6.\n03 43\n2v 1\n[ cs\n.L G\n] 1\n0 Ju\nn 20\nstudied, and there is a longstanding conjecture [3; 15, p. 300] that systematic scan (1) never mixes more than a constant factor slower than random scan and (2) never mixes more than a logarithmic factor faster than random scan. This conjecture implies that the choice of scan order does not have a large effect on performance.\nRecently, Roberts and Rosenthal [20] described a model in which systematic scan mixes more slowly than random scan by a polynomial factor; this disproves direction (1) of this conjecture. Independently, we constructed other models for which the scan order has a significant effect on mixing time. This raises the question: what are the true bounds on the difference between these mixing times? In this paper, we address this question and make the following contributions.\n\u2022 In Section 3, we study the effect of the variable permutation chosen for systematic scan on the mixing time. In particular, in Section 3.1, we construct a model for which a systematic scan mixes a polynomial factor faster than random scan, disproving direction (2) of the conjecture, and in Section 3.2, we construct a model for which the systematic scan with the worst-case permutation results in a mixing time that is slower by a polynomial factor than both the best-case systematic scan permutation and random scan.\n\u2022 In Section 4, we empirically verify the mixing times of the models we construct, and we analyze how the mixing time changes as a function of the permutation.\n\u2022 In Section 5, we prove a weaker version of the conjecture described above, providing relative bounds on the mixing times of random and systematic scan. Specifically, under a mild condition, different scan orders can only change the mixing time by a polynomial factor. To obtain these bounds, we introduce a method of augmenting the state space of Gibbs sampling so that the method of conductance can be applied to analyze its dynamics."}, {"heading": "2 Related Work", "text": "Recent work has made progress on analyzing the mixing time of Gibbs sampling, but there are still some major limitations to our understanding. In particular, most known results are only for specific models or are only for random scan. For example, mixing times are known for Mallow\u2019s model [1, 4], and colorings of a graph [5] for both random and systematic scan, but these are not applicable to general models. On the other hand, random scan has been shown to mix in polynomial time for models that satisfy structural conditions \u2013 such as having close-to-modular energy functions [10] or having bounded hierarchy width and factor weights [2] \u2013 but corresponding results for for systematic scan are not known. The major exception to these limitations is Dobrushin\u2019s condition, which guarantees O(n log n) mixing for both random scan and systematic scan [6, 13]. However, there are many models of interest with close-to-modular energy functions or bounded hierarchy width that do not satisfy Dobrushin\u2019s condition.\nA similar choice of scan order appears in stochastic gradient descent (SGD), where the standard SGD algorithm uses random scan, and the incremental gradient method (IGM) uses systematic scan. In contrast to Gibbs sampling, avoiding \u201cbad permutations\u201d in the IGM is known to be important to ensure fast convergence [12, 19]. In this paper, we bring some intuition about the existence of bad permutations from SGD to Gibbs sampling."}, {"heading": "3 Models in Which Scan Order Matters", "text": "Despite a lack of theoretical results regarding the effect of scan order on mixing times, it is generally believed that scan order only has a small effect on mixing time. In this section, we first define relevant terms and state some common conjectures regarding scan order. Afterwards, we give several counterexamples showing that the scan order can have asymptotic effects on the mixing time.\nThe total variation distance between two probability distributions \u00b5 and \u03bd on \u2126 is [15, p. 47]\n\u2016\u00b5\u2212 \u03bd\u2016TV = max A\u2286\u2126 |\u00b5(A)\u2212 \u03bd(A)|.\nThe mixing time is the minimum number of steps needed to guarantee that the total variation distance between the true and estimated distributions is below a given threshold from any starting distribution. Formally, the mixing time of a stochastic process P with transition matrix P (t) after t steps and stationary distribution \u03c0 is [15, p. 55]\ntmix(P, ) = min\n{ t : max\n\u00b5 \u2016P (t)\u00b5\u2212 \u03c0\u2016TV \u2264\n} ,\nwhere the maximum is taken over the distribution \u00b5 of the initial state of the process. When comparing the statistical efficiency of systematic scan and random scan, it would be useful to establish, for any systematic scan process S and random scan process R on the same n-variable model, a relative bound of the form\nF1( , n, tmix(R, )) \u2264 tmix(S, ) \u2264 F2( , n, tmix(R, )) (1)\nfor some functions F1 and F2. Similarly, to bound the effect that the choice of permutation can have on the mixing time, it would be useful to know, for any two systematic scan processes S\u03b1 and S\u03b2 with different permutations on the same model, that for some function F3,\ntmix(S\u03b1, ) \u2264 F3( , n, tmix(S\u03b2 , )). (2)\nDiaconis [3] and Levin et al. [15, p. 300] conjecture that systematic scan is never more than a constant factor slower or a logarithmic factor faster than random scan. This is equivalent to choosing F1( , n, t) = C1( ) \u00b7 t \u00b7 (log n)\u22121 and F2( , n, t) = C2( ) \u00b7 t in the inequality in (1), for some functions C1 and C2. It is also commonly believed that all systematic scans mix at the same asymptotic rate, which is equivalent to choosing F3( , n, t) = C3( ) \u00b7 t in (2).\nThese conjectures imply that using systematic scan instead of random scan will not result in significant consequences, at least asymptotically, and that the particular permutation used for systematic scan is not important. However, we show that neither conjecture is true by constructing models (listed in Table 1) in which the scan order has substantial asymptotic effects on mixing time.\nIn the rest of this section, we go through two models in detail to highlight the diversity of behaviors that different scan orders can have. First, as a warm-up, we construct a model, which we call the sequence of dependencies model, for which a single \u201cgood permutation\u201d of systematic scan mixes faster, by a polynomial factor, than both random scan and systematic scans using most other permutations. This serves as a counterexample to the conjectured lower bounds (i.e. the choice of F1 and F3) on the mixing time of systematic scan. Second, we construct a model, the two islands model, in which there is a small set of \u201cbad permutations\u201d that mix very slowly in comparison to random scan and most other systematic scans. This contradicts the conjectured upper bounds (i.e. the choice of F2 and F3). For completeness, we also discuss the discrete pyramid model introduced by Roberts and Rosenthal [20] (which contradicts the conjectured choice of F2). Table 1 also lists the mixing times of a few additional models we constructed: these models further explore the space of asymptotic comparisons among scan orders, but for brevity we defer them to the appendix."}, {"heading": "3.1 Sequence of Dependencies", "text": "The first model we will describe is the sequence of dependencies model (Figure 1a). The goal of this model is to explore the question of how fast systematic scan can be, by constructing a model for which systematic scan mixes rapidly for one particular good permutation. The sequence of dependencies model achieves this by having the property that, at any time, progress towards mixing is only made if a particular variable is sampled; this variable is always the one that is chosen by the good permutation. As a result, while a systematic scan using a good permutation makes progress at every step, both random scan and other systematic scans often fail to make progress, which leads to a gap between their mixing times. Thus, this model exhibits two surprising behaviors: (1) one systematic scan is polynomially better than random scan and (2) systematic scans using different permutations have polynomial differences in mixing times. We now describe this model in detail.\ns0 s1 \u00b7 \u00b7 \u00b7 si \u00b7 \u00b7 \u00b7 sn x1 x2 xi\u22121 xi xn\n(a) Sequence of Dependencies Model\nVariables There are n binary variables x1, . . . , xn. Independently, each variable has a very strong prior of being true. However, variable xi is never true unless xi\u22121 is also true. The unnormalized probability distribution is the following, where M is a very large constant.\nP (x) \u221d {\n0 if xi is true and xi\u22121 is false for some i \u2208 {2, . . . , n} M |x| otherwise\nState Space There are n + 1 states with non-zero probability: s0, . . . , sn, where si is the state where the first i variables are true and the remaining n\u2212 i variables are false. In the stationary distribution, sn has almost all of the mass due to the strong priors on the variables, so reaching sn is essentially equivalent to mixing. Notice that sampling xi will almost always move the state from si\u22121 to si, very rarely move it from si to si\u22121, and can have no other effect. The worst-case starting state is s0, where the variables must be sampled in the order x1, . . . , xn for this model to mix.\nRandom Scan The number of steps needed to transition from s0 to s1 is distributed as a geometric random variable with mean n (variables are randomly selected, and specifically x1 must be selected). Similarly, the number of steps needed to transition from si\u22121 to si is distributed as a geometric random variable with mean n. In total, there are n transitions, so O(n2) steps are needed to mix.\nBest Systematic Scan The best systematic scan uses the order x1, x2, . . . , xn. For this scan, one sweep will reach sn no matter what the starting state is, so the mixing time is n.\nWorst Systematic Scan The worst systematic scan uses the order xn, xn\u22121, . . . , x1. The first sweep only uses x1, the second sweep only uses x2, and in general, any sweep only makes progress using one transition. Finally, in the n-th sweep, xn is used in the first step. Thus, this process mixes in n(n\u2212 1) + 1 steps, which is O(n2)."}, {"heading": "3.2 Two Islands", "text": "With the sequence of dependencies model, we showed that a single good permutation can mix much faster than other scan orders. Next, we describe the two islands model (Figure 1b), which has the reverse behavior: it has bad permutations that yield much slower mixing times. The two islands model achieves this by having two disjoint blocks of variables such that consecutively sampling two variables from the same block accomplishes very little. As a result, a systematic scan that uses a permutation that frequently consecutively samples from the same block mixes a polynomial factor slower than both random scan and most other systematic scans. We now describe this model in detail.\nVariables There are 2n binary variables grouped into two blocks: x1, . . . , xn and y1, . . . , yn. Conditioned on all other variables being false, each variable is equally likely to be true or false. However, the x variables and the y variables contradict each other. As a result, if any of the x\u2019s are true, then all of the y\u2019s must be false, and if any of the y\u2019s are true, then all of the x\u2019s must be false. The unnormalized probability distribution for this model is the following.\nP (x, y) \u221d {\n0 if \u2203xi true and \u2203yj true 1 otherwise\n(3)\nThis model can be interpreted as a machine learning inference problem in the following way. Each variable represents whether the reasoning in some sentence is sound. The sentences corresponding to x1, . . . , xn and the sentences corresponding to y1, . . . , yn reach contradicting conclusions. If any variable is true, its conclusion is correct, so all of the sentences that reached the opposite conclusion must be not be sound, and their corresponding variables must be false. However, this does not guarantee that all other sentences that reached the same conclusion have sound reasoning, so it is possible for some variables in a block to be true while others are false. Under these assumptions alone, the natural way to model this system is with the two islands distribution in (3).\nState Space We can think of the states as being divided into three groups: states in island x (at least one of the x variables are true), states in island y (at least one of the y variables are true), and a single bridge state b (all variables are false). The islands are well-connected internally, but it is impossible to directly move from one island to the other \u2013 the only way to move from one island to the other is through the bridge. To simplify the analysis, we assume that the bridge state has very low mass. This assumption allows us to assume that islands mix rapidly in comparison to the time required to move onto the bridge and that chains always move off of the bridge when a variable is sampled. The same asymptotic behavior results when the bridge state has the same mass as the other states.\nThe bridge is the only way to move from one island to the other, so it acts as a bottleneck. As a result, the efficiency of bridge usage is critical to the mixing time. We will use bridge efficiency to refer to the probability that the chain moves to the other island when it reaches the bridge. Under the assumption that mixing within the islands is rapid in comparison to the time needed to move onto the bridge, the mixing time will be inversely proportional to the bridge efficiency of the chain.\nRandom Scan In random scan, the variable selected after getting on the bridge is independent of the previous variable. As a result, with probability 1/2, the chain will move onto the other island, and with probability 1/2, the chain will return to the same island, so the bridge efficiency is 1/2.\nBest Systematic Scan Several different systematic scans achieve the fastest mixing time. One such scan is x1, y1, x2, y2, . . . , xn, yn. Since the sampled variables alternate between the blocks, if the chain moves onto the bridge (necessarily by sampling a variable from the island it was previously on), it will always proceed to sample a variable from the other block, which will cause it to move onto the other island. Thus, the bridge efficiency is 1. More generally, any systematic scan that alternates between sampling from x variables and sampling from y variables will have a bridge efficiency of 1.\nWorst Systematic Scan Several different systematic scans achieve the slowest mixing time. One such scan is x1, . . . , xn, y1, . . . , yn. In this case, if the chain moves onto the bridge, it will almost always proceed to sample a variable from the same block, and return to the same island. In fact, the only way for this chain to move across islands is if it moves from island x to the bridge using transition xn and then moves to island y using transition y1, or if it moves from island y to the bridge using transition yn and then moves to island x using transition x1. Thus, only 2 of the 2n transitions will cross the bridge, and the bridge efficiency is 1/n. More generally, any systematic scan that consecutively samples all x variables and then all y variables will have a bridge efficiency of 1/n.\nComparison of Mixing Times The mixing times of the chains are inversely proportional to the bridge efficiency. As a result, random scan takes twice as long to mix as the best systematic scan, and mixes n/2 times faster than the worst systematic scan."}, {"heading": "3.3 Discrete Pyramid", "text": "In the discrete pyramid model (Figure 1c) introduced by Roberts and Rosenthal [20], there are n binary variables xi, and the mass is uniformly distributed over all states where at most one xi is true. In this model, the mixing time of random scan, O(n), is asymptotically better than that of systematic scan for any permutation, which all have the same mixing time, O(n3)."}, {"heading": "4 Experiments", "text": "In this section, we run several experiments to illustrate the effect of scan order on mixing times. First, in Figure 2a, we plot the mixing times of the models from Section 3 as a function of the number of variables. These experiments validate our results about the asymptotic scaling of the mixing time, as well as show that the scan order can have a significant effect on the mixing time for even small models. (Due to the exponential state space of the two islands model, we modify it slightly to make the computation of mixing times feasible: we simplify the model by only considering the states that are adjacent to the bridge, and assume that the states on each individual island mix instantly.)\nIn the following experiments, we consider a modified version of the two islands model, in which the mass of the bridge state is set to 0.1 of the mass of the other states to allow the effect of scan order to be clear even for a small number of variables. Figure 2b illustrates the rate at which different scan orders explore this modified model. Due to symmetry, we know that half of the mass should be on each island in the stationary distribution, so getting half of the mass onto the other island is necessary for mixing. This experiment illustrates that random scan and a good systematic scan move to the other island quickly, while a bad systematic scan requires many more iterations.\nFigure 2c illustrates the effect that the permutation chosen for systematic scan can have on the mixing time. In this experiment, the mixing time for each permutation was found and plotted in sorted order. For the sequence of dependencies model, there are a small number of good permutations which mix very quickly compared to the other permutations and random scan. However, no permutation is bad compared to random scan. In the two islands model, as we would expect based on the analysis in Section 3, there are a small number of bad permutations which mix very slowly compared to the other permutations and random scan. Some permutations are slightly better than random scan, but none of the scan orders are substantially better. In addition, the mixing times for systematic scan approximately discretized due to the fact that mixing time depends so heavily on the bridge efficiency."}, {"heading": "5 Relative Bounds on Mixing Times via Conductance", "text": "In Section 3, we described two models for which a systematic scan can mix a polynomial factor faster or slower than random scan, thus invalidating conventional wisdom that the scan order does not have an asymptotically significant effect on mixing times. This raises a question of how different the mixing times of different scans can be. In this section, we derive the following weaker \u2013 but correct \u2013 version of the conjecture stated by Diaconis [3] and Levin et al. [15].\nOne of the obstacles to proving this result is that the systematic scan chain is not reversible. A standard method of handling non-reversible Markov chains is to study a lazy version of the Markov chain instead [15, p. 9]. In the lazy version of a Markov chain, each step has a probability of 1/2 of staying at the current state, and acts as a normal step otherwise. This is equivalent to stopping at a random time that is distributed as a binomial random variable. Due to the fact that systematic scan is not reversible, our bounds are on the lazy systematic scan, rather than the standard systematic scan.\nTheorem 1. For any random scan Gibbs sampler R and lazy systematic scan sampler S with the same stationary distribution \u03c0, their relative mixing times are bounded as follows.\n(1/2\u2212 )2 tmix(R, ) \u2264 2t2mix(S, ) log ( 1\n\u03c0min ) (1/2\u2212 )2 tmix(S, ) \u2264 8n2\n(minx,i Pi(x, x)) 2 t\n2 mix(R, ) log\n( 1\n\u03c0min\n) ,\nwhere Pi is the transition matrix corresponding to resampling just variable i, and \u03c0min is the probability of the least likely state in \u03c0.\nUnder mild conditions, namely being fixed and the quantities log(\u03c0\u22121min) and (minx,i Pi(x, x)) \u22121 being at most polynomial in n, this theorem implies that the choice of scan order can only affect the mixing time by up to polynomial factors in n and tmix. We now outline the proof of this theorem.\nIn the two islands models, the mixing time of a scan order was determined by its ability to move through a single bridge state that restricted flow. This suggests that a technique with the ability to model the behavior of this bridge state is needed to bound the relative mixing times of different scans. Conductance, also known as the bottleneck ratio, is a topological property of Markov chains used to bound mixing times by considering the flow of mass around the model [15, p. 88]. This ability to model bottlenecks in a Markov chain makes conductance a natural technique both for studying the two islands model and bounding mixing times in general.\nMore formally, consider a Markov chain on state space \u2126 with transition matrix P and stationary distribution \u03c0. The conductance of a set S and of the whole chain are respectively defined as\n\u03a6(S) =\n\u2211 x\u2208S,y/\u2208S \u03c0(x)P (x, y)\n\u03c0(S) \u03a6? = min S:\u03c0(S)\u2264 12 \u03a6(S).\nConductance can be directly applied to analyze random scan. Let Pi be the transition matrix corresponding to sampling variable i. The state space \u2126 is used without modification, and the transition matrix is P = 1n \u2211n i=1 Pi. The stationary distribution is the expected target distribution \u03c0. On the other hand, conductance cannot be directly applied to systematic scan. Systematic scan is not a true Markov chain because it uses a sequence of transition matrices rather than a single transition matrix. One standard method of converting systematic scan into a true Markov chain is to consider each full scan as one step of a Markov chain. However, this makes it difficult to compare with random scan because it completely changes which states are connected by single steps of the transition matrix. To allow systematic and random scan to be compared more easily, we introduce an alternative way of converting systematic scan to a true Markov chain by augmenting the state space. The augmented state space is \u03a8 = \u2126 \u00d7 [n], which represents an ordered pair of the normal state and the index of the variable to be sampled. The transition probability is P ((x, i), (y, j)) = Pi(x, y)s(i, j), where s(i, j) = I[i+ 1 \u2261 j (mod n)] is an indicator that shows if the correct variable will be sampled next.\nAdditionally, augmenting the state space for random scan allows easier comparison with systematic scan in some cases. For augmented random scan, the state space is also \u03a8 = \u2126\u00d7 [n], the same as for systematic scan. The transition\nprobability is P ((x, i), (y, j)) = 1nPi(x, y), which means that the next variable to sample is selected uniformly. The stationary distributions of the augmented random scan and systematic scan chains are both \u03c0 ((x, i)) = n\u22121\u03c0(x). Because the state space and stationary distribution are the same, augmented random scan and augmented systematic scan can be compared directly, which lets us prove the following lemma.\nLemma 1. For any random scan Gibbs sampler and systematic scan sampler with the same stationary distribution \u03c0, let \u03a6RS denote the conductance of the random scan process, let \u03a6RS-A denote the conductance of the augmented random scan process, and let \u03a6SS-A denote the conductance of the augmented systematic scan process. Then,\n1\n2n \u00b7min x,i Pi(x, x) \u00b7 \u03a6RS-A \u2264 \u03a6SS-A \u2264 \u03a6RS.\nIn Lemma 1, the upper bound states that the conductance of systematic scan is no larger than the conductance of random scan. We use this in the next section to show that systematic scan cannot mix too much more quickly than random scan. To prove this lemma, we show that for any set S under random scan, the set S\u0302 containing the corresponding augmented states for systematic scan will have the same conductance under systematic scan as S had under random scan.\nThe lower bound in Lemma 1 states that the conductance of systematic scan is no smaller than a function of the conductance of augmented random scan. This function depends on the number of variables n and minx,i Pi(x, x), which is the minimum holding probability of any state. To prove this lemma, we show that for any set S under augmented systematic scan, we can bound its conductance under augmented random scan.\nThere are well-known bounds on the mixing time of a Markov chain in terms of its conductance, which we state in Theorem 2 [15, pp. 89, 235].\nTheorem 2. For any lazy or reversible Markov chain,\n1/2\u2212 \u03a6? \u2264 tmix( ) \u2264 2 \u03a62? log\n( 1\n\u03c0min\n) .\nIt is straightforward to prove the result of Theorem 1 by combining these bounds with the conductance bounds from the previous section."}, {"heading": "6 Conclusion", "text": "We studied the effect of scan order on mixing times of Gibbs samplers, and found that for particular models, the scan order can have an asymptotic effect on the mixing times. These models invalidate conventional wisdom about scan order and show that we cannot freely change scan orders without considering the resulting changes in mixing times. In addition, we found bounds on the mixing times of different scan orders, which replaces a common conjecture about the mixing times of random scan and systematic scan."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the support of: DARPA FA8750-12-2-0335; NSF IIS-1247701; NSF CCF-1111943; DOE 108845; NSF CCF-1337375; DARPA FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041 and N000141310129; NIH U54EB020405; NSF DGE-114747; DARPA\u2019s SIMPLEX program; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; and Toshiba. The views and conclusions expressed in this material are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, AFRL, NSF, ONR, NIH, or the U.S. Government."}, {"heading": "A Additional Models in Which Scan Order Matters", "text": "Table 2 classifies the models based on the mixing times of the best systematic scan and worst systematic scan relative to random scan. For models in the first column, the best systematic scan mixes asymptotically faster than random scan. For models in the second column, they differ only up to logarithmic factors. For models in the third column, the best systematic scan mixes asymptotically slower than random scan. The rows are classified in the same way based on the worst systematic scan instead of the best systematic scan.\nThe models in Section 3 display possible problems that can occur when changing scan orders. However, two behaviors were not shown by the models in Section 3: a case where random scan is asymptotically worse than all systematic scans, and a case where random scan is asymptotically better than a systematic scan and asymptotically worse than another systematic scan. This section gives two additional models that exhibit these behaviors, along with a more detailed explanation of the discrete pyramid model. These two models are more complicated than the models in Section 3 due to the fact that they require the number of states for each variable to grow with n. Combined with the models in Section 3, these models show that all logically consistent asymptotic behaviors (that is, behaviors where the worst systematic scan is no better than the best systematic scan) are possible.\nA.1 Discrete Pyramid The discrete pyramid model (Figure 1c) was first described by Roberts and Rosenthal [20] and is included for completeness to show that it is possible for random scan to mix asymptotically faster than all systematic scans.\nVariables There are n binary variables x1, . . . , xn. Conditioned on all other variables being false, each variable is equally likely to be true or false. However, the variables are all contradictory, and at most one variable can be true. This model can also be interpreted as an n islands model, where there are n well-connected regions (which consist of a single state) connected by a single bridge state.\nState Space There are n + 1 states s0, s1, . . . , sn with nonzero probability. s0 is the state where all variables are false, and for all other i, si is the state where variable xi is true and all other variables are false.\nRandom Scan The worst-case total variation occurs when the starting state is not x0. Suppose that the starting state is sk. xk will be selected with probability 1/n, and in this case, the state will change to x0 with probability 1/2. Thus, the number of samples needed to leave xk is distributed as a Geometric random variable with a mean of 2n. This means that within O(n) steps, the chain will have left the initial state with high probability. Once the chain has reached x0, each step has a probability of 1/2 of leaving x0 and uniformly going to another state. Thus, O(1) steps are sufficient for the chain to mix after it reaches x0. In total, the chain mixes in O(n) updates.\nAll Systematic Scans Once again, the worst-case total variation occurs when the starting state is not x0. Suppose that the starting state is sk. A systematic scan step will change nothing until xk is reached. Then, with probability 1/2, the state will remain sk, and with probability 1/2 the state will change to x0. If it does move to x0, the scan will continue, and move away from x0 with probability 1/2 at each step. Thus, each time the scan reaches the current state, the state will advance Z steps, where Z is a Geometric random variable with rate 1/2. This corresponds to a weighted random walk on a circle, which is known to have a mixing time of O(n2). Notice that each step of the random walk actually requires one full sweep, so O(n3) steps are needed for any systematic scan to mix.\nA.2 Memorize and Repeat We introduce the memorize and repeat model to show that it is possible for all systematic scans to be asymptotically better than random scan.\nDescription In this example (Figure 3), there are two types of states: states memorizing a permutation, and states requiring the memorized permutation to be repeated. In the stationary distribution, almost all of the mass is on the state where a permutation has been memorized and repeated, and the probability of the states increase exponentially from left to right. There are n variables x1, . . . , xn. In the memorize phase, sampling a variable will allow the state to change as long as the variable has not been sampled before. In the repeat phase, a sampling a variable will only allow the state to change if it is the next variable in the memorized permutation. The repeat phase is similar to the sequence of dependencies example, with the variables rearranged to match the memorized permutation.\nIn the worst-case starting distribution, the wrong permutation can already be memorized, which results in no asymptotic gap between mixing times. To allow the difference in mixing time to appear even in the worst-case analysis, we repeat the memorize and repeat process n times.\nRandom Scan Random scan encounters the coupon collector problem to get through the memorize phase, so O(n log n) steps are needed. Afterwards, the repeat phase is equivalent to a sequence of dependencies, which requires O(n2) steps. Thus, to get through one memorize and repeat chain, O(n2) steps are needed. This mechanism is repeated n times, so O(n3) steps are needed.\nAll Systematic Scans If a systematic scan starts at the beginning of a memorize sequence, 2n steps are sufficient to move through the memorize and repeat chain (n steps to memorize, n steps to repeat). However, when analyzing the worst-case total variation, we must consider the case where the wrong permutation has already been memorized, which requires O(n2) steps to move through. However, this can only happen once, so in the worst case O(n2) steps are needed to move through the one incorrect permutation, and O(n) steps are needed for the remaining n\u2212 1 memorize and repeat chains. Thus, O(n2) steps are needed in total.\nState Space To simplify the explanation of the state space and formulation as a Gibbs sampler, this explanation is for only one memorize and repeat cycle. In the memorize phase, the states are labeled by the part of the permutation that has already been memorized. In the repeat phase, the states are labeled by the remainder of the permutation that still has to be repeated.\nFormulation as a Gibbs Sampler There are n integer-valued variables x1, . . . , xn with values ranging from 0 to n+ 1. In the initial state that does not have anything memorized, all variables have a value of 0. In the memorize phase, any variable that has not yet been memorized has a value of 0, and any variable that has been memorized stores its index within the permutation. This means that for states in the memorize phase with i variables memorized, exactly one variable will have a value of j, for each integer j from 1 to i, and the remaining n\u2212 i variables that have not been\nmemorized have a value of 0. As more variables are memorized, the states become exponentially more likely. Next, in the repeat phase, when a variable is used, its value changes to n+ 1. This means that for states in the repeat phase with i variables repeated, the i variables that have been repeated have a value of n+ 1, and the remaining n\u2212 i variables that still have to be repeated have a unique integer from i+ 1 to n. As more variables are repeated, the states become exponentially more likely. The log probability distribution is the following, where M is a very large constant, Z is the normalizing constant, and \u03c3(a1, . . . , an) denotes the set of all permutations of (a1, \u00b7 \u00b7 \u00b7 , an).\nlogM P (x) = \u2212 logM Z +  max(x1, . . . , xn) if (x1, . . . , xn) \u2208 \u03c3(0, . . . , 0, 1, . . . , i) n+ \u2211n i=1 I[xi = n+ 1] if (x1, . . . , xn) \u2208 \u03c3(i+ 1, . . . , n, n+ 1, . . . , n+ 1)\n\u2212\u221e otherwise\n= \u2212 logM Z +  (Number of Memorized Variables) if valid memorize state n+ (Number of Repeated Variables) if valid repeat state \u2212\u221e otherwise\nFirst, for states in the memorize phase, if a variable that has not yet been memorized is sampled, then its value will almost always change to i + 1, where i is the number of variables already memorized, due to the large value of M . However, sampling a variable that has already been memorized will not change its value because all other values will either result in an invalid state (and have a probability of 0) or be significantly less likely due to the large value of M . Next, for states in the repeat phase, if the correct variable is sampled, then its value will almost always change to n+ 1. However, sampling the wrong variable will not change its value because all other values result in an invalid state or decrease the probability of the state.\nA.3 Soft Dependencies In the soft dependencies model, some systematic scans mix asymptotically faster than random scan, and some systematic scans mix asymptotically slower.\nDescription This example resembles the memorize portion of the previous example. However, no repeat phase is included, and only some permutations are accepted. In particular, a permutation is accepted only if each element of the permutation is followed by an element that is in the next \u221a n unused variables that come after it (mod n).\nMore formally, consider a permutation (a1, . . . , an) of (1, . . . , n). The permutation is accepted only if the following holds for all i \u2208 {1, . . . , n\u2212 1}.\n\u221a n \u2265 |{ j \u2208 {1, . . . , n} | j > i+ 1 and aj \u2212 ai (mod n) < ai+1 \u2212 ai (mod n) }|\nIn this condition, the requirement that j > i + 1 means that aj is an unused element, and the requirement that aj \u2212 ai (mod n) < ai+1 \u2212 ai (mod n) means that starting from ai, aj is reached before ai+1. These two requirements imply that ai+1 is within the next \u221a n unused variables following ai. This condition is equivalent to the following.\n\u221a n \u2265 {\u2211n j=i+2 I[ai < aj < ai+1] if ai < ai+1\u2211n j=i+2 I[ai < aj or aj < ai+1] if ai > ai+1\nRandom Scan First, consider when there are at least \u221a n remaining transitions. The first n\u2212\u221an = O(n) transitions will have this property. During this period, random scan advances to the next state with probability n/ \u221a n = \u221a n. Thus, O(n \u221a n) steps are needed to get until there are fewer than \u221a n remaining transitions. Once there are fewer than \u221a n remaining transitions, random scan needs O(n) steps to make each transition, so O(n \u221a n) more steps are needed. In total, O(n \u221a n) steps are needed.\nBest Systematic Scan The best systematic scan uses the order x1, . . . , xn. This uses n steps to give a valid permutation, and thus mixes in n steps.\nWorst Systematic Scan The worst systematic scan uses the order xn, xn\u22121, . . . , x1. After a transition is taken, nearly a full scan is needed until the next valid transition will be reached, so O(n2) steps are needed.\nState Space The state space has a similar form to the memorize portion of the previous example. The states are labeled by the portion of the permutation that has already been given, but only prefixes to valid permutations are accepted.\nFormulation as a Gibbs Sampler There are n integer-valued variables x1, . . . , xn with values ranging from 0 to n. In the initial state that does not have anything memorized, all variables have a value of 0. Then, as the permutation is memorized, the variables that have not been used yet have a value of 0, and any variable that has been used stores its index within the permutation.\nIn addition, to merge the permutations together into one state, all variables need to be sampled one more time (note that this does not require the variables to be sampled in a particular order). This extra sampling process only adds lower order terms and constant factors to the mixing time. Once each variable is sampled again, its value is changed to n.\nThe log probability distribution is the following, where M is a very large constant, Z is the normalizing constant, and \u03c3(a1, . . . , an) denotes the set of all permutations of (a1, \u00b7 \u00b7 \u00b7 , an).\nlogM P (x) = \u2212 logM Z +  max(x1, . . . , xn) if (x1, . . . , xn) \u2208 \u03c3(0, . . . , 0, 1, . . . , i) and is valid n\u2212 1 +\u2211ni=1 I[xi = n] if at least two of (x1, . . . , xn) have a value of n \u2212\u221e otherwise\n= \u2212 logM Z +  (Number of Memorized Variables) if valid memorize state n\u2212 1 + (Number of Merged Variables) if valid merge state \u2212\u221e otherwise\nFirst, in the memorize phase, sampling a valid unused variable will almost always change its value to i+ 1, where i is the number of variables already memorized, due to the large value of M . However, sampling a variable that has already been memorized or sampling a variable that least to an invalid permutation will not change its value. Then, when the last variable in the permutation is sampled, its value changes to n. Afterwards, sampling any variable will also change its value to n."}, {"heading": "B Priors in the Sequence of Dependencies", "text": "To simplify the analysis of the sequence of dependencies, we assumed that the priors on the variables were strong enough such sampling the correct variable essentially always caused the state to advance. We now analyze how large M (the strength of the priors) needs to be in order to allow the best systematic scan to mix in O(n) time. In this section, we show that if M = \u2126(n), then the best systematic scan mixes in O(n) time, and if M = o(n), then the best systematic scan cannot mix O(n2) time.\nFirst, suppose that M = \u2126(n). This means that M \u2265 cn for some c > 0 and sufficiently large n. The probability of transitioning from si\u22121 to si when variable xi is sampled is\nM 1 +M \u2265 cn 1 + cn = 1\u2212 1 1 + cn > 1\u2212 1 cn .\nThe probability of transitioning from s0 to sn after sampling the sequence x1, . . . , xn is at least (1\u2212 1/cn)n, which limits to e\u22121/c for large n. In other words, if M = \u2126(n), then a single sweep of the best systematic scan will reach sn with a probability that does not approach 0. Thus, a constant number of sweeps is sufficient to reach sn with high probability, which is equivalent to mixing.\nOn the other hand, suppose that M = o(n). Then, for any c > 0, M < cn for sufficiently large n. This means that as n increases, the probability that a single sweep of the best systematic scan will reach sn will go to 0, so no constant number of sweeps will be sufficient to mix."}, {"heading": "C Bridge Efficiency with Normal Mass on Bridge", "text": "In the analysis of the two islands model, we assumed that the bridge has negligible mass. We now analyze the mixing times without the assumption that the bridge has small mass. The same asymptotic behavior still results.\nEven when the bridge has the same mass as the other states, it still acts as a bottleneck to the model. For sufficiently large n, the islands will mix rapidly in comparison reaching the bridge, so the mixing time is still inversely proportional to the bridge efficiency. However, when the bridge has the same mass as the other states, sampling a variable while on the bridge will only have a 1/2 chance of moving off of the bridge.\nRandom Scan In random scan, the variables that are sampled are completely independent, so the bridge efficiency is still 1/2.\nBest Systematic Scan Consider the scan x1, y1, x2, y2, . . .. Suppose the sampling x1 changes the state to the bridge state \u2013 a similar analysis will apply for any other variable. The next variable is y1, which will change the state to the other island with probability 1/2. Afterwards, x2 is sampled, which will change the state to the same island with probability 1/4. Then, y2 is sampled, which will change the state to the other island with probability 1/8. Thus, the probability of moving to the other island is 1/2 + 1/8 + 1/32 + . . . = 2/3.\nWorst Systematic Scan Consider the scan x1, . . . , xn, y1, . . . , yn. Suppose that sampling xn changes the state to the bridge state. For large n, the probability of leaving the bridge state onto island y approaches 1. Next, suppose that sampling xn\u22121 changes the state to the bridge state. There is a probability of 1/2 of moving back to island x via xn, but the chain will move onto island y otherwise. In general, moving onto the bridge via xi or yi will result in moving to the island with probability 2\u2212n+i. The average probability of moving onto the other island is then 2/n."}, {"heading": "D Proofs for Section 5", "text": "In this section, we prove our relative bounds on mixing times (Theorem 1), along with related claims and lemmas.\nClaim 1. The stationary distribution of augmented random and systematic scan is\n\u03c0 ((x, i)) = 1\nn \u03c0(x)\nProof. We prove this claim by showing that applying the transition matrix for augmented random scan or augmented systematic scan does not change this distribution.\nFor augmented random scan,\u2211 x,i \u03c0((x, i))P ((x, i), (y, j)) = \u2211 x,i \u03c0(x) n \u00b7 P ((x, i), (y, j)) = 1 n \u2211 x,i \u03c0(x) \u00b7 1 n Pi(x, y)\n= 1\nn n\u2211 i=1 [\u2211 x\u2208\u2126 (\u03c0(x)Pi(x, y)) \u00b7 1 n ]\n= 1\nn n\u2211 i=1 [ \u03c0(y) \u00b7 1 n ] = 1 n \u03c0(y)\n= \u03c0((y, j))\nFor augmented systematic scan,\u2211 x,i \u03c0((x, i))P ((x, i), (y, j)) = \u2211 x,i \u03c0(x) n \u00b7 P ((x, i), (y, j)) = 1 n \u2211 x,i \u03c0(x) \u00b7 Pi(x, y)s(i, j)\n= 1\nn n\u2211 i=1 [\u2211 x\u2208\u2126 (\u03c0(x)Pi(x, y)) s(i, j) ]\n= 1\nn n\u2211 i=1 [\u03c0(y)s(i, j)] = 1 n \u03c0(y)\n= \u03c0((y, j))\nLemma 1. For any random scan Gibbs sampler and systematic scan sampler with the same stationary distribution \u03c0, let \u03a6RS denote the conductance of the random scan process, let \u03a6RS-A denote the conductance of the augmented random scan process, and let \u03a6SS-A denote the conductance of the augmented systematic scan process. Then,\n1\n2n \u00b7min x,i Pi(x, x) \u00b7 \u03a6RS-A \u2264 \u03a6SS-A \u2264 \u03a6RS.\nProof.\nUpper Bound: The conductance of the whole chain is the smallest conductance of any set with mass no larger than 12 . Then, to prove that this inequality holds, we will show that for any set S \u2208 \u2126 with mass no larger than 12 , there exists a set T \u2208 \u03a8 with mass no larger than 12 such that the conductance of S under random scan is the same as the conductance of T under augmented systematic scan.\nFrom the standpoint of random scan, consider a set S \u2208 \u2126 with mass no larger than 12 . The conductance is\n\u03a6RS(S) =\n\u2211 x\u2208S \u2211 y\u2208Sc \u03c0(x)P (x, y)\n\u03c0(S)\n= 1 n\n\u2211n i=1 \u2211 x\u2208S \u2211 y\u2208Sc \u03c0(x)Pi(x, y)\n\u03c0(S)\nThen, for augmented systematic scan, consider the set T = { (x, i) : x \u2208 S, i \u2208 [n] }. First, notice that \u03c0(T ) = \u03c0(S) \u2264 12 . The conductance is\n\u03a6SS-A(T ) =\n\u2211 (x,i)\u2208T \u2211 (y,j)\u2208T c \u03c0 ((x, i))P ((x, i), (y, j))\n\u03c0(T )\n=\n\u2211n i=1 \u2211n j=1 \u2211 x\u2208S \u2211 y\u2208Sc \u03c0 ((x, i))P ((x, i), (y, j))\n\u03c0(S)\n= 1\nn\n\u2211n i=1 \u2211n j=1 \u2211 x\u2208S \u2211 y\u2208Sc \u03c0(x)Pi(x, y)s(i, j)\n\u03c0(S)\n= 1\nn\n\u2211n i=1 \u2211 x\u2208S \u2211 y\u2208Sc \u03c0(x)Pi(x, y)\n\u03c0(S)\n= \u03a6RS(S)\nThis implies that for any S \u2208 \u2126 with \u03c0(S) \u2264 12 , there exists T \u2208 \u03a8 with \u03c0(T ) \u2264 12 such that \u03a6RS(S) = \u03a6SS(T ). Therefore, \u03a6SS-A \u2264 \u03a6RS.\nLower Bound: In this proof, we will work with the flow between two sets\nQ(A,B) = \u2211\nx\u2208A,y\u2208B \u03c0(x)P (x, y).\nNotice that the conductance of a set can then be defined as\n\u03a6(S) = Q(S, Sc)\n\u03c0(S) .\nTo prove that this inequality holds for the whole chain, we will show that the same inequality holds for any set S \u2208 \u03a8. Consider some arbitrary state x \u2208 \u2126. Flow can leave from the corresponding augmented states in two ways: flowing from some (x, i) \u2208 S to (x, j) \u2208 Sc or flowing from (x, i) \u2208 S to (y, j) \u2208 Sc, where y 6= x (x and y differ in only variable i). Let Sx = { (x, i) \u2208 S }, and let Scx = { (x, i) \u2208 Sc }. These two components can be written as Q(Sx, S c x) and \u2211 y 6=xQ(Sx, S c y).\nNow, we will find upper bounds for the random scan flows and lower bounds for the systematic scan flows. In the following statements, it is implicit that y 6= x, and \u03b3 = minx,i Pi(x, x) will denote the minimum holding probability.\nFirst, we bound the amount of flow from (x, i) \u2208 S to (x, j) \u2208 Sc. For augmented random scan, the following upper bound holds.\nQRS(Sx, S c x) = \u2211 (x,i)\u2208Sx,(x,j)\u2208Scx \u03c0((x, i))P ((x, i), (x, j))\n= \u2211\n(x,i)\u2208Sx,(x,j)\u2208Scx\n1 n \u03c0(x) \u00b7 1 n Pi(x, x)\n\u2264 \u2211\n(x,i)\u2208Sx,(x,j)\u2208Scx\n1 n \u03c0(x) \u00b7 1 n\n= |Sx| n \u03c0(x) n\u2212 |Sx| n\n\u2264 {\n1 4\u03c0(x) if |Sx| 6= 0, n 0 if |Sx| = 0, n\nFor augmented systematic scan, the following lower bound holds.\nQSS(Sx, S c x) = \u2211 (x,i)\u2208Sx,(x,j)\u2208Scx \u03c0((x, i))P ((x, i), (x, j))\n= \u2211\n(x,i)\u2208Sx,(x,j)\u2208Scx\n1 n \u03c0(x) \u00b7 Pi(x, x)s(i, j)\n\u2265 \u2211\n(x,i)\u2208Sx,(x,j)\u2208Scx\n1 n \u03c0(x) \u00b7 \u03b3 \u00b7 s(i, j)\n\u2265 {\n1 n\u03c0(x)\u03b3 if |Sx| 6= 0, n 0 if |Sx| = 0, n\nSimilarly,\nQSS(Sy, S c y) \u2265 { 1 n\u03c0(y)\u03b3 if |Sy| 6= 0, n 0 if |Sy| = 0, n\nNow, we bound the amount of flow from x to y for y 6= x. Note that Pi(x, y) = 0 for all i if x and y differ in more than one variable. As a result, we will assume that x and y differ in only variable i for the next two bounds. For augmented random scan, the following upper bound holds.\nQRS(Sx, S c y) =\n{ 1 n\u03c0(x)Pi(x, y) n\u2212|Sy| n if (x, i) \u2208 S\n0 if (x, i) 6\u2208 S \u2264 {\n1 n\u03c0(x)Pi(x, y) if (x, i) \u2208 S and |Scy| 6= 0 0 if (x, i) 6\u2208 S or |Scy| = 0\nIn the derivation of the next bound, note that if |Scy| = n, then we are guaranteed that (y, i+ 1(mod n)) \u2208 Scy . For augmented systematic scan, the following lower bound holds.\nQSS(Sx, S c y) = { 1 n\u03c0(x)Pi(x, y) if (x, i) \u2208 S and (y, i+ 1) \u2208 Sc 0 otherwise\n\u2265 {\n1 n\u03c0(x)Pi(x, y) if (x, i) \u2208 S and |Scy| = n 0 otherwise\nNow, we can derive several inequalities between the augmented random scan flow and the augmented systematic scan flow as direct consequences of the bounds we just found. First, we bound the relative flows from Sx to Scx.\nQSS(Sx, S c x) \u2265\n4\u03b3\nn QRS(Sx, S\nc x) \u2265\n\u03b3 n QRS(Sx, S c x)\nNext, we bound the relative flows from x to y, where x and y differ in exactly variable i.\nQSS(Sx, S c y) +\n1 n Pi(y, x)QSS(Sy, S c y)\n\u2265  1 n\u03c0(x)Pi(x, y) if (x, i) \u2208 S and |Scy| = n 1 nPi(y, x) \u00b7 1n\u03c0(y)\u03b3 if (x, i) \u2208 S and |Scy| 6= 0, n 0 otherwise\n=  1 n\u03c0(x)Pi(x, y) if (x, i) \u2208 S and |Scy| = n 1 n2\u03c0(x)Pi(x, y)\u03b3 if (x, i) \u2208 S and |Scy| 6= 0, n 0 otherwise\n\u2265 {\n1 n2\u03c0(x)Pi(x, y)\u03b3 if (x, i) \u2208 S and |Scy| 6= 0 0 otherwise\n= \u03b3\nn { 1 n\u03c0(x)Pi(x, y) if (x, i) \u2208 S and |Scy| 6= 0 0 otherwise\n\u2265 \u03b3 n \u03a6RS(Sx, S c y)\nFinally, we bound the relative flows.\nQSS(S, S c) = \u2211 x\u2208\u2126 \u2211 y\u2208\u2126 QSS(Sx, S c y) = \u2211 x\u2208\u2126 QSS(Sx, Scx) +\u2211 y 6=x QSS(Sx, S c y)  \u2265 1\n2 \u2211 x\u2208\u2126 2QSS(Sx, Scx) +\u2211 y 6=x QSS(Sx, S c y)  1\n2 \u2211 x\u2208\u2126 QSS(Sx, Scx) + \u2211 y\u2208\u2126 n\u2211 i=1 1 n Pi(y, x)QSS(Sy, S c y) + \u2211 y 6=x QSS(Sx, S c y)  \u2265 1\n2 \u2211 x\u2208\u2126 QSS(Sx, Scx) +\u2211 y 6=x ( QSS(Sx, S c y) + n\u2211 i=1 1 n Pi(y, x)QSS(Sy, S c y) ) \u2265 1\n2 \u2211 x\u2208\u2126 \u03b3 n QRS(Sx, S c x) + \u03b3 n \u2211 y 6=x QRS(Sx, S c y)  \u2265 \u03b3\n2n \u2211 x\u2208\u2126 QRS(Sx, Scx) +\u2211 y 6=x QRS(Sx, S c y)  = \u03b3\n2n QSS(S, S\nc)\nThe mass of S is the same for augmented random scan and augmented systematic scan, so the same inequality holds for the conductances of the sets. Finally, because this inequality holds for any set S, the inequality also holds for the conductance of the whole chain.\nTheorem 2. For any lazy or reversible Markov chain,\n1/2\u2212 \u03a6? \u2264 tmix( ) \u2264 2 \u03a62? log\n( 1\n\u03c0min\n) .\nProof. The lower bound of this inequality,\n1/2\u2212 \u03a6? \u2264 tmix( )\nis is shown by Theorem 7.3 of [15] and holds for any Markov chain \u2013 that is, it does not actually require the Markov chain to be lazy or reversible.\nThe upper bound of this inequality,\ntmix( ) \u2264 2\n\u03a62? log\n( 1\n\u03c0min\n) ,\nis shown by Theorem 17.10 of [15].\nTheorem 1. For any random scan Gibbs sampler R and lazy systematic scan sampler S with the same stationary distribution \u03c0, their relative mixing times are bounded as follows.\n(1/2\u2212 )2 tmix(R, ) \u2264 2t2mix(S, ) log ( 1\n\u03c0min ) (1/2\u2212 )2 tmix(S, ) \u2264 8n2\n(minx,i Pi(x, x)) 2 t\n2 mix(R, ) log\n( 1\n\u03c0min\n) ,\nwhere Pi is the transition matrix corresponding to resampling just variable i, and \u03c0min is the probability of the least likely state in \u03c0.\nProof.\nUpper Bound for Random Scan: First, we upper bound the mixing time of random scan.\ntmix(R, ) \u2264 2\n\u03a62RS log\n( 1\n\u03c0min ) Next, we lower bound the mixing time for systematic scan.\ntmix(S, ) \u2265 1/2\u2212 \u03a6SS-A \u2265 1/2\u2212 \u03a6RS\nThis theorem results from algebraic manipulation of the previous two inequalities.\nt2mix(S, ) \u2265 (1/2\u2212 )2\n\u03a62RS (1/2\u2212 )2 tmix(R, ) \u2264 2t2mix(S, ) log ( 1\n\u03c0min ) Upper Bound for Systematic Scan: First, we lower bound the mixing time for random scan.\ntmix(R, ) \u2265 1/2\u2212 \u03a6RS-A\ntmix(R, ) 2 \u2265 (1/2\u2212 )\n2\n\u03a62RS-A\nNext, we manipulate the lower bound of Lemma 1.\n\u03a6SS \u2265 1\n2n \u00b7min x,i Pi(x, x) \u00b7 \u03a6RS-A 1\n\u03a62SS \u2264 4n\n2\n(minx,i Pi(x, x)) 2 \u00b7\n1\n\u03a62RS-A\nUsing this result, we upper bound the mixing time for systematic scan.\ntmix(S, ) \u2264 2\n\u03a62SS log\n( 1\n\u03c0min\n) \u2264 8n 2\n(minx,i Pi(x, x)) 2 \u00b7\n1\n\u03a62RS log\n( 1\n\u03c0min ) This theorem results from the previous inequalities.\n(1/2\u2212 )2 tmix(S, ) \u2264 8n2\n(maxx,i Pi(x, x)) 2 tmix(R, )\n2 log\n( 1\n\u03c0min\n)"}], "references": [{"title": "Mixing times of the biased card shuffling and the asymmetric exclusion process", "author": ["Itai Benjamini", "Noam Berger", "Christopher Hoffman", "Elchanan Mossel"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Rapidly mixing gibbs sampling for a class of factor graphs using hierarchy width", "author": ["Christopher De Sa", "Ce Zhang", "Kunle Olukotun", "Christopher R\u00e9"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Some things we\u2019ve learned (about markov chain monte carlo)", "author": ["Persi Diaconis"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Analysis of systematic scan metropolis algorithms using iwahori-hecke algebra techniques", "author": ["Persi Diaconis", "Arun Ram"], "venue": "The Michigan Mathematical Journal,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Systematic scan for sampling colorings", "author": ["Martin Dyer", "Leslie Ann Goldberg", "Mark Jerrum"], "venue": "The Annals of Applied Probability,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Dobrushin conditions and systematic scan", "author": ["Martin Dyer", "Leslie Ann Goldberg", "Mark Jerrum"], "venue": "Combinatorics, Probability and Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Sampling-based approaches to calculating marginal densities", "author": ["Alan E. Gelfand", "Adrian F.M. Smith"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "Stochastic relaxation, gibbs distributions, and the bayesian restoration of images", "author": ["Stuart Geman", "Donald Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1984}, {"title": "Sampling from probabilistic submodular models", "author": ["Alkis Gotovos", "Hamed Hassani", "Andreas Krause"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Finding scientific topics", "author": ["Thomas L. Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Convergence rate of incremental gradient and newton methods", "author": ["Mert G\u00fcrb\u00fczbalaban", "Asu Ozdaglar", "Pablo Parrilo"], "venue": "arXiv preprint arXiv:1510.08562,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "A simple condition implying rapid mixing of single-site dynamics on spin systems", "author": ["Thomas P. Hayes"], "venue": "In 2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS\u201906),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "The madlib analytics library: or mad skills, the sql", "author": ["Joseph M Hellerstein", "Christoper R\u00e9", "Florian Schoppmann", "Daisy Zhe Wang", "Eugene Fratkin", "Aleksander Gorajek", "Kee Siong Ng", "Caleb Welton", "Xixuan Feng", "Kun Li"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Markov chains and mixing times", "author": ["David Asher Levin", "Yuval Peres", "Elizabeth Lee Wilmer"], "venue": "American Mathematical Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "The bugs project: Evolution, critique and future directions", "author": ["David Lunn", "David Spiegelhalter", "Andrew Thomas", "Nicky Best"], "venue": "Statistics in medicine,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Factorie: Probabilistic programming via imperatively defined factor graphs", "author": ["Andrew McCallum", "Karl Schultz", "Sameer Singh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Jags: A program for analysis of bayesian graphical models using gibbs sampling", "author": ["Martyn Plummer"], "venue": "In Proceedings of the 3rd international workshop on distributed statistical computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Beneath the valley of the noncommutative arithmetic-geometric mean inequality: conjectures, case-studies, and consequences", "author": ["Benjamin Recht", "Christopher R\u00e9"], "venue": "In Proceedings of the 25th Annual Conference on Learning Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Surprising convergence properties of some simple gibbs samplers under various scans", "author": ["Gareth O. Roberts", "Jeffrey S. Rosenthal"], "venue": "International Journal of Statistics and Probability,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "An architecture for parallel topic models", "author": ["Alexander Smola", "Shravan Narayanamurthy"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Towards high-throughput gibbs sampling at scale: A study across storage managers", "author": ["Ce Zhang", "Christopher R\u00e9"], "venue": "In Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Segmentation of brain mr images through a hidden markov random field model and the expectation-maximization algorithm", "author": ["Yongyue Zhang", "Michael Brady", "Stephen Smith"], "venue": "IEEE Transactions on Medical Imaging,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}], "referenceMentions": [{"referenceID": 7, "context": "A major use of Gibbs sampling is marginal inference: the estimation of the marginal distributions of some variables of interest [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "Some applications include various computer vision tasks [9, 23, 24], information extraction [7], and latent Dirichlet allocation for topic modeling [11].", "startOffset": 56, "endOffset": 67}, {"referenceID": 22, "context": "Some applications include various computer vision tasks [9, 23, 24], information extraction [7], and latent Dirichlet allocation for topic modeling [11].", "startOffset": 56, "endOffset": 67}, {"referenceID": 6, "context": "Some applications include various computer vision tasks [9, 23, 24], information extraction [7], and latent Dirichlet allocation for topic modeling [11].", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "Some applications include various computer vision tasks [9, 23, 24], information extraction [7], and latent Dirichlet allocation for topic modeling [11].", "startOffset": 148, "endOffset": 152}, {"referenceID": 15, "context": "Gibbs sampling is simple to implement and quickly produces accurate samples for many models, so it is widely used and available in popular libraries such as OpenBUGS [16], FACTORIE [17], JAGS [18], and MADlib [14].", "startOffset": 166, "endOffset": 170}, {"referenceID": 16, "context": "Gibbs sampling is simple to implement and quickly produces accurate samples for many models, so it is widely used and available in popular libraries such as OpenBUGS [16], FACTORIE [17], JAGS [18], and MADlib [14].", "startOffset": 181, "endOffset": 185}, {"referenceID": 17, "context": "Gibbs sampling is simple to implement and quickly produces accurate samples for many models, so it is widely used and available in popular libraries such as OpenBUGS [16], FACTORIE [17], JAGS [18], and MADlib [14].", "startOffset": 192, "endOffset": 196}, {"referenceID": 13, "context": "Gibbs sampling is simple to implement and quickly produces accurate samples for many models, so it is widely used and available in popular libraries such as OpenBUGS [16], FACTORIE [17], JAGS [18], and MADlib [14].", "startOffset": 209, "endOffset": 213}, {"referenceID": 20, "context": "From the hardware efficiency perspective, systematic scans are clearly superior [21, 22].", "startOffset": 80, "endOffset": 88}, {"referenceID": 21, "context": "From the hardware efficiency perspective, systematic scans are clearly superior [21, 22].", "startOffset": 80, "endOffset": 88}, {"referenceID": 19, "context": "Recently, Roberts and Rosenthal [20] described a model in which systematic scan mixes more slowly than random scan by a polynomial factor; this disproves direction (1) of this conjecture.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "For example, mixing times are known for Mallow\u2019s model [1, 4], and colorings of a graph [5] for both random and systematic scan, but these are not applicable to general models.", "startOffset": 55, "endOffset": 61}, {"referenceID": 3, "context": "For example, mixing times are known for Mallow\u2019s model [1, 4], and colorings of a graph [5] for both random and systematic scan, but these are not applicable to general models.", "startOffset": 55, "endOffset": 61}, {"referenceID": 4, "context": "For example, mixing times are known for Mallow\u2019s model [1, 4], and colorings of a graph [5] for both random and systematic scan, but these are not applicable to general models.", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "On the other hand, random scan has been shown to mix in polynomial time for models that satisfy structural conditions \u2013 such as having close-to-modular energy functions [10] or having bounded hierarchy width and factor weights [2] \u2013 but corresponding results for for systematic scan are not known.", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "On the other hand, random scan has been shown to mix in polynomial time for models that satisfy structural conditions \u2013 such as having close-to-modular energy functions [10] or having bounded hierarchy width and factor weights [2] \u2013 but corresponding results for for systematic scan are not known.", "startOffset": 227, "endOffset": 230}, {"referenceID": 5, "context": "The major exception to these limitations is Dobrushin\u2019s condition, which guarantees O(n log n) mixing for both random scan and systematic scan [6, 13].", "startOffset": 143, "endOffset": 150}, {"referenceID": 12, "context": "The major exception to these limitations is Dobrushin\u2019s condition, which guarantees O(n log n) mixing for both random scan and systematic scan [6, 13].", "startOffset": 143, "endOffset": 150}, {"referenceID": 11, "context": "In contrast to Gibbs sampling, avoiding \u201cbad permutations\u201d in the IGM is known to be important to ensure fast convergence [12, 19].", "startOffset": 122, "endOffset": 130}, {"referenceID": 18, "context": "In contrast to Gibbs sampling, avoiding \u201cbad permutations\u201d in the IGM is known to be important to ensure fast convergence [12, 19].", "startOffset": 122, "endOffset": 130}, {"referenceID": 19, "context": "Model tmix(R) min \u03b1 tmix(S\u03b1) max \u03b1 tmix(S\u03b1) Sequence of Dependencies n n n Two Islands 2 2 n2 Discrete Pyramid [20] n n n Memorize and Repeat n n n Soft Dependencies n n n", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "(2) Diaconis [3] and Levin et al.", "startOffset": 13, "endOffset": 16}, {"referenceID": 19, "context": "For completeness, we also discuss the discrete pyramid model introduced by Roberts and Rosenthal [20] (which contradicts the conjectured choice of F2).", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "3 Discrete Pyramid In the discrete pyramid model (Figure 1c) introduced by Roberts and Rosenthal [20], there are n binary variables xi, and the mass is uniformly distributed over all states where at most one xi is true.", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "In this section, we derive the following weaker \u2013 but correct \u2013 version of the conjecture stated by Diaconis [3] and Levin et al.", "startOffset": 109, "endOffset": 112}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "Gibbs sampling is a Markov Chain Monte Carlo sampling technique that iteratively samples variables from their conditional distributions. There are two common scan orders for the variables: random scan and systematic scan. Due to the benefits of locality in hardware, systematic scan is commonly used, even though most statistical guarantees are only for random scan. While it has been conjectured that the mixing times of random scan and systematic scan do not differ by more than a logarithmic factor, we show by counterexample that this is not the case, and we prove that that the mixing times do not differ by more than a polynomial factor under mild conditions. To prove these relative bounds, we introduce a method of augmenting the state space to study systematic scan using conductance.", "creator": "LaTeX with hyperref package"}}}