{"id": "1606.04250", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Experimental and causal view on information integration in autonomous agents", "abstract": "accelerating the amount of digitally available but sometimes heterogeneous information extracted about the world is remarkable, and new access technologies such as smart self - drivers driving commuter cars, premium smart consumer homes or the \" internet registry of things \" will further increase improving it. in this first paper we actively examine certain subjective aspects beginning of the problem of how such more heterogeneous information can suddenly be harnessed by intelligent investigative agents. we first here discuss our potentials and limitations of some existing approaches, followed by constructing two quantitative investigations. concerning the favorite focus problems of trying the flawed first scientific investigation is focus on using the partially novel agent experimentation platform { \\ em malmo } ability to obtain participants a technically better broader understanding of the problem. the strongest focus matter of the second investigation again is on understanding how information about increases the hardware fitness of different agents ( objects such as self - driving cars ), measuring the agents'additional sensory capability data, and physical or causal intelligence information can be utilized. for explaining knowledge transfer behavior between agents and subsequent create more data - efficient reasoning decision than making. finally, we present some thoughts on questioning what a really general data theory for the problem brain could look like, arise and formulate open questions.", "histories": [["v1", "Tue, 14 Jun 2016 08:38:18 GMT  (121kb,D)", "http://arxiv.org/abs/1606.04250v1", null], ["v2", "Fri, 26 Aug 2016 16:37:37 GMT  (122kb,D)", "http://arxiv.org/abs/1606.04250v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["philipp geiger", "katja hofmann", "bernhard sch\\\"olkopf"], "accepted": false, "id": "1606.04250"}, "pdf": {"name": "1606.04250.pdf", "metadata": {"source": "CRF", "title": "Experimental and causal view on information integration in autonomous agents", "authors": ["Philipp Geiger", "Katja Hofmann", "Bernhard Sch\u00f6lkopf"], "emails": ["philipp.geiger@tuebingen.mpg.de", "katja.hofmann@microsoft.com", "bs@tuebingen.mpg.de"], "sections": [{"heading": "1 Introduction", "text": "Vast amounts of heterogeneous information is recorded and connected, e.g., in self-driving cars, smart homes with domestic robots or the \u201cinternet of things\u201d. Intuitively, it makes sense to design intelligent agents in such a way that they automatically integrate all relevant and well-structured information on their environment that is available. Various aspects of the problem of designing such agents have been investigated previously. In this paper we approach the problem from two directions which, to our knowledge, have not been (exhaustively) examined yet: using compex simulated experiments, on a practical level, and causal models, on a more theoretical level. The complexity of the problem allows us only to take small steps though."}, {"heading": "1.1 Main contributions", "text": "The main contribution of this paper consists of two investigations:\n\u2022 In Section 5 we use a simulated experimentation platform to obtain a better understanding of the problem of integrating heterogeneous information. More specifically, we consider a toy scenario that captures important aspects of the general problem but can easily be implemented in that platform, and propose and evaluate a method for it. \u2022 In Section 6 we investigate how detailed information on the hardware of different agents (self-driving cars), their sensory data,\n1 MPI for Intelligent Systems; email: philipp.geiger@tuebingen.mpg.de 2 Microsoft Research; email: katja.hofmann@microsoft.com 3 MPI for Intelligent Systems; email: bs@tuebingen.mpg.de\nand physical or causal information can be utilized for knowledge transfer between them and subsequent more data-efficient decision making.\nThe common structure of both investigations is that we start with a description of a scenario that captures certain core aspects of the general problem, in particular containing a variety of heterogeneous information sources, and then sketch a method to perform information integration and subsequent decision making in these scenarios. After experimentally evaluating the method, or illustrating it based on a toy example, we conclude both investigations with a discussion of the advantages and limitations of the respective methods. Additionally, we briefly discuss potential further experiments that could be carried out and preliminary ideas for a potential general theory for the problem in Section 7.\nA reoccurring theme in our investigations is that we try to treat as much information (including models) as possible explicitly as input to algorithms instead of implicitly encoding it into algorithms. Our hope is that this sheds a better, more explicit light on the problem."}, {"heading": "1.2 Structure of the paper", "text": "The remainder of the paper is structured as follows:\n\u2022 We introduce the experimentation platform and basic concepts in Section 2. \u2022 In Section 3, we formulate the problem. \u2022 In Section 4, investigate potentials and limitations of existing ap-\nproaches for the problem. \u2022 In Sections 5 and 6, we present our two main investigations. \u2022 In Section 7, we propose potential next steps and pose open ques-\ntions. \u2022 We conclude with Section 8."}, {"heading": "2 Preliminaries", "text": "Here we introduce the concepts, models and the experimentation platform we will use in the paper.\nAutonomous agents. By an (autonomous/automatic) agent we mean a mechanism which, at each time point t, takes some input from the environment, in particular its sensory data which we refer to as observation, and outputs some action that influences the environment. Moreover, by an intelligent (autonomous) agent we mean an autonomous agent which is successful in using its inputs and outputs for given tasks, i.e., specific goals w.r.t. the environment, often encoded by a reward or utility function.\nar X\niv :1\n60 6.\n04 25\n0v 1\n[ cs\n.A I]\n1 4\nJu n\n20 16\nNote that in this paper we do not define a clear boundary between agent and environment. Usually, we consider the hardware platform of an agent (e.g., the car) as part of the agent. This particularly has to be kept in mind when we talk of several agents in the \u201csame environment\u201d: the hardware of the agents may still differ. (It is almost a philosophical problem to define what precisely the \u201csame environment\u201d means. Here we simply suggest to interpret this notion as if it were used in an everyday conversation. We pose a related question in Section 7.)\nWhen we consider some agent A w.r.t. which some task is given and for which we want to infer good actions, while information may come from (sensory data of) a collection C of other agents, then we refer to A as target agent and the agents in C as source agents.\nExperimentation platform \u201cMalmo\u201d. For the experiments in Section 5 we will use the software Malmo, a simulated environment for experimentation with intelligent agents, that was introduced recently [4]. Malmo is based on \u201cMinecraft\u201d, which is an open-ended computer game where players can explore, construct, collaborate, and invent their own \u201cgames within the game\u201d or tasks. The Malmo platform provides an abstraction layer on top of the game through which one or more agents observe the current state of the world (observations are customizable) and interact with it through their specific action sets (or actuators). The advantage of Malmo is that it reflects important characteristics of the problem instances we will introduce in Section 3. To illustrate the platform, three sample observations of an agent in three different maps in Malmo will be depicted in Figure 1.\nCausal models. Mathematically, a causal model [8, 11] M over a set V of variables consists of a directed acyclic graph (DAG) G with V as node set, called causal diagram or causal DAG, and a conditional probability density pX|PAX=paX (for all paX in the domain of PAX ) for each X \u2208 V , where PAX are the parents of X in G. Given a causal model M and a tuple of variables Z of M , the post-interventional causal model MdoZ=z is defined as follows: drop the variables in Z and all incoming arrows from the causal diagram, and fix the value of variables in Z to the corresponding entry of z in all remaining conditional densities. Based on this, we define the post-interventional density of Y after setting Z to z, denoted by pY |doZ=z or pY |do z , by the the density of Y in MdoZ=z .\nOn a non-mathematical level, we consider M to be a correct causal model of some part of reality, if it correctly predicts the outcomes of interventions in that part of reality (clearly there are other reasonable definitions of causation). Keep in mind that in this paper, in particular Section 6, will use causal models and causal reasoning in a more intuitive and sometimes less rigorous way, to not be limited by the expressive power of the current formal modeling language.\nNote that we will use expressions like p(x|y) as shorthand for pX|Y (x|y)."}, {"heading": "3 Problem formulation", "text": "Let us describe the problem we consider in this paper in more detail:\n\u2022 Given: a task T w.r.t. some partially unknown environment E, and additional heterogeneous but well-structured information sources H (e.g., in the form of low-level sensory data, or in the form of high-level descriptions). \u2022 Goal: design an agent A that automatically harnesses as much relevant information of H as possible to solve T ; more specifically, it\nshould use H to either improve an explicit model of the effects4 of its actions, which then guides its actions, or let its actions directly be guided by H .\nNote that alternatively, one could also formulate the problem by letting A only be an actuator, and not a complete agent, and include the agent\u2019s sensors into H . This might be a more precise formulation, however, for the sake of an intuitive terminology, we stick to the definition based on A being an agent.5\nTo illustrate the general problem, in what follows, we give three concrete examples of desirable scenarios in which agents automatically integrate heterogeneous information. Ideally, agents would be able to simultaneously integrate information sources from all three examples."}, {"heading": "3.1 Example 1: sharing information between different self-driving cars", "text": "Consider self-driving cars. It is desirable that as much information about the environment can be shared amongst them. By such information we mean up-to-date detailed street maps, traffic information, information on how to avoid accidents etc. For instance, assume that one self-driving car leaves the road at some difficult spot due to some inappropriate action, since, for instance, the spot has not been visited by self-driving cars before (or newly appeared due to say some oil spill or rockfall). If we only consider other self-driving cars of the same hardware, this experience could directly be transferred to them by enforcing them not to perform the very action at the very spot. (I.e., for all cars of the same hardware one could treat the experience as if it was their own and make them \u201clearn\u201d from it in the usual reinforcement learning (RL) way.) However, if we assume that there are self-driving cars of different types, then it is not possible to transfer the experience, and thus avoid further accidents, in this straightforward way. (A related example would be that of two robots with different hardware.)"}, {"heading": "3.2 Example 2: observing another agent", "text": "Consider domestic robots. Such a robot may, with its sensor, observe humans how they handle doors, windows, or kitchen devices. It should be possible that domestic robots learn from such experience. For instance, one could imagine a robot to reason that, if it is able to operate the door knob in a similar way as a human did before, this would also open the door and and thus allow the robot to walk into the other room (to achieve some task)."}, {"heading": "3.3 Example 3: integrating high-level information", "text": "Consider an agent that arrives in a city it has never been to before. The goal is to get to a certain destination, say to the town hall. A resident may be able to explain the way in a simple language, with words such as \u201c... follow this street until you come to a church, then turn right ...\u201d. Or a resident could provide a map and mark directions\n4 In this sense, at least the target of the information integration is clear: modeling the dynamics or causal structure of the agent in the environment. 5 Note that the problem we formulate here does not coincide with developing (\u201cstrong\u201d) artificial intelligence (AI), as defined, e.g., by the Turing test or simply based on human-level intelligence. We restrict to sources of information that are more or less well-structured - either quantitative measurements with a simple and clear relation to the physical world, or information in a language much more restrictive than natural language. Nonetheless, the formulated problem can be seen as one step from say RL into the direction of AI.\non the map. One could imagine that an autonomous agent could combine such a description with a model of the \u201clocal\u201d (or \u201clow-level\u201d) dynamics that is shared by most environments (which is closely related to the laws of physics). (The model of the \u201clocal\u201d dynamics could have been either hard-coded, or inferred based on exploration in other (related) environments.) In principle it should be possible that such a combination allows the agent to successfully navigate to the destination city hall."}, {"heading": "4 Related work: potentials and limitations", "text": "Various research directions exist that address major or minor aspects of the problem formulated in Section 3. Here we discuss the most relevant such directions we are aware of, highlighting their potentials and limitations w.r.t. the problem. Keep in mind that Sections 5.4 and 6.4 contain additional discussions on the advantages of our approaches over these directions."}, {"heading": "4.1 Reinforcement learning", "text": "One of the most powerful approaches to shaping intelligent autonomous agents is reinforcement learning (RL) [13]. Instead of explicitly hard-coding each detail of an agent, for each environment and objective individually, the idea is to take an approach which is more modular and based on learning instead of hard-coding: the supervisor only determines the reward function and then the agent ideally uses exploration of the unknown environment and exploitation of the gained experience (sensory data) to achieve a high cumulative reward.\nRegarding the problem we consider in this paper, RL plays a key role for integration of information in the form of recordings of an agent\u2019s own past, or of an agent with the same hardware. However, as mentioned in Section 3, in contrast to RL, here we consider the problem of integrating information beyond such recordings, such as sensory data from agents with different hardware, or higher level information such as maps."}, {"heading": "4.2 Learning from demonstrations", "text": "According to [1], in learning from demonstrations (LfD), some \u201cteacher\u201d performs a trajectory which is recorded, and the goal is that a \u201clearner\u201d agent, based on this recording, infers and imitates (or utilizes) the teacher\u2019s \u201cpolicy\u201d (or the dynamics of the environment, or both). Central notions that [1] uses to analyze and distinguish various types of LfD problems are the record mapping, i.e., what aspects of the teacher\u2019s demonstration are measured and recorded, and the embodiment mapping, i.e., if the recorded actions can directly be implemented by the \u201clearner\u201d and lead to similar observations as the recoded ones, or if the recordings first have to be transformed \u201cto make sense\u201d for the learner.\nOur problem formulation can be seen as a generalization of LfD. Based on this, while a significant part of the problem we consider can be addressed by LfD methods, others are beyond the scope of these methods: Instead of hand-crafting, e.g., the embodiment mapping for each agent individually, we aim at (semi-)automating the inference of the mapping from recordings of \u201csource\u201d agents to actions of a \u201ctarget\u201d agent. In particular, we propose to do such a (semi-)automation based on additional information sources on the hardware specifications of the agents involved (Section 6).6\n6 Note that there is some work on learning from observations only (not actions) of a \u201cteacher\u201d [10]. However, this approach does not allow to inte-\nGenerally, we aim at integrating information from many different sources simultaneously (e.g., many other self-driving cars in Example 1 and many different forms of information as described in Example 1 through Example 3. In particular, we aim at learning from databases that contain desirable as well as undesirable trajectories (e.g., avoid similar accidents as in Example 1).\nClearly, we do not present methodology that fully tackles the above shortcomings of LfD methods in this paper. Rather, we make first steps towards such methodology in Sections 5 and 6."}, {"heading": "4.3 Multi-agent systems", "text": "In multi-agent systems, collections of agents acting in a shared environment are studied [12]. One important task is collaboration between agents [7]. A common approach is to model the collection of agents again as a single agent, by considering tuples of actions and observations as single actions and observations. Learning-based methods have been extensively studied [5].\nWhile multi-agent systems approaches often allow to share and transfer information between agents, regarding the problem we formulated in Section 3 they have certain limitations: Similar as LfD, they usually do not integrate higher-level information sources (as the map in Example 3) or explicit hardware specifications of the agents (which we do in Section 6). Furthermore, if the mapping from some source agents sensory data to a target agents action is learned via modeling all agents as a single one, then it seems difficult to add agents to an environment, while our preliminary investigation in Section 6 in principle allows for adding agents more easily. Also note that the task of collaboration between agents is external to the problem we consider."}, {"heading": "4.4 Transfer learning for agents", "text": "The problem we consider is related to transfer learning for agents. For instance, [14] consider an example where, in the well-known mountain car example, experience should be transferred although the motor of the car is changed. This comes close to transferring experience between self-driving cars as we suggest in Example 1. However, the scope of methods reviewed in [14] is on transferring observationaction recordings or things such as policies, value functions etc. using an appropriate mapping, while the goal we pursue is to also integrate information which is usually not expressible in these terms (e.g., the map or natural language description in Example 3, or the observation of another agent in Example 2). Furthermore, in this paper we aim at integrating many heterogeneous sources of information, while in transfer learning, even though several sources of information may be considered, they are usually homogeneous."}, {"heading": "4.5 Further related areas", "text": "Other related directions include the following. Recently, the experimentation with intelligent agents in platforms based on computer games has become popular [6]. To our knowledge, the current work is the first one to use such platforms to study the problem of information integration, or related problems such as LfD (Section 4.2).\nThe general integration and transfer of data (not focused on intelligent agents) using causal models has been studied by [9, 3]. The\ngrate information such as the map in Example 3. Note that a difference to our method in Section 5 is that, e.g., an estimate of the complete transition probability is necessary, while our method only requires an idea of the \u201clow-level\u201d dynamics.\nidea of integrating higher-level information (again not for intelligent agents though) has been studied, e.g., by [15]. The relation between intelligent agents and causal models has been studied from a more philosophical perspective, e.g., by [16]."}, {"heading": "5 Investigation 1: integration of \u201cnon-subjective\u201d information, evaluated in a simulated environment", "text": "In this section we aim to shed light on the following aspects of the problem formulated in Section 3:\n\u2022 Generally, what experiments, in particular in simulated environments, can be performed to gain better insights about the problem? \u2022 How can experimentation (exploration) help an agent to \u201ctranslate\u201d experience not recorded by itself into its own \u201ccoordinate system\u201d and use it for (successful) decision making? \u2022 How can partial information on the dynamics, such as a controller that is known to work locally, be merged with \u201chigher-level\u201d information such as hints on the path to some goal position. \u2022 How can we quantify the efficiency gain from additional information sources?\nThe investigation is structured as follows: in Section 5.1 we describe the scenario, in particular the available heterogeneous information sources, in Section 5.2 we sketch an information-integrating agent for that scenario, then, in Section 5.3, we evaluate an adapted version of the agent in a simulated environment, and last, in Section 5.4 we discuss the advantages of our method over those that do not use additional information, and some further aspects."}, {"heading": "5.1 Scenario", "text": "Task. An agent A starts in some unknown landscape and the task is to get to some visually recognizable goal position as quickly as possible.\nAvailable heterogeneous information. We assume the following information sources to be available:\n\u2022 the agent\u2019s own sensory input in the form of images yt and position signal qt (which can be seen an \u201cinteractive information\u201d source since the agent can \u201cquery\u201d this source via its actions), \u2022 the controller ctl , which can be seen as a summary of the agent A\u2019s past subjective experience regarding the invariant local \u201cphysical laws\u201d of a class of environments7, \u2022 a video trajectory y\u22170:L that is a first-person recording of another agent with similar (but not necessarily identical) hardware that runs to the goal in the same environment.\nRelation to the problem formulated in Section 3. On the one hand, this scenario can be seen as a (very) simplistic version of the scenario described in Example 1: A is a self-driving car that is supposed to get to some marked goal in an unknown environment, and the video y\u22170:L comes from other cars that have a similar videorecording device but potentially different hardware (engine etc.).\nOn the other hand, this scenario can be seen as a simplistic version of the scenario described in Example 3: the unknown landscape is\n7 Specifically, we assume that if the distance between position q1 and q2 is small, then ctl successfully steers from q1 to q2. Alternatively, ctl could be a local model of the dynamics which induces such a controller.\nAlgorithm 1 Agent that integrates first-person video of other agent\n1: input: Controller ctl , video y\u22171:L. 2: for i = 1, . . . , L do 3: Use local controller ctl , optimization method opt\nand interaction with the environment to search locally around the current position for a new position qi = arg minq E(dist(y\u2217i , Y )|Q = q).\n4: Use ctl to go to qi. 5: end for\nAlgorithm 2 Proof-of-concept of Algorithm 1 for Malmo\n1: input: Controller ctl , video y\u22171:L. 2: set r0 = current position, once the mission starts 3: for i = 1, . . . , L do 4: use ctl , opt and teleportation to locally search around position\nri\u22121 for a new position ri = arg minr E(dist(y\u2217i , Y )|Q = r)\n5: end for 6: restart the mission 7: set i := 0. 8: while i < L do 9: use ctl to steer to ri\n10: if current position is close to ri then 11: set i := i + 1 12: end if 13: end while\nsome unknown city A arrived in, and instead of a description of the way to the destination in simple natural language, it gets a sequence of photos that describe the path it has to take."}, {"heading": "5.2 Method", "text": "First we sketch a general method, i.e., \u201csoftware\u201d for A, in Algorithm 1, assuming a (stochastic) optimization method opt and an image distance dist as given (for concrete examples, see below). Note that in Algorithm 1 we denote by E(Y |Q = q) the mean image Y observed at position Q = q.\nAlthough Algorithm 1 is in principle applicable to the experimental setup we consider in Section 5.3 below, we will evaluate Algorithm 2 instead, which is a simplified proof-of-concept implementation of it, making use of \u201cteleportation\u201d, allowing the agent to directly jump to other positions without needing to navigate there. For Algorithm 2, as optimization method opt , we use simple grid search. (Note that instead one could use gradient descent or Bayesian optimization techniques.) Furthermore, we define the image distance dist using Gaussian blur as follows:\ndist(u, v) := \u2016N \u2217 (u\u0304\u2212 v\u0304)\u2016,\nwhere u\u0304 is the normalization of u (i.e., subtraction by mean and division by standard deviation over the single pixels), N is a bivariate Gaussian with hand-tuned variance and \u2217 is the convolution in both image dimensions."}, {"heading": "5.3 Empirical evaluation in a simulated environment", "text": "To evaluate Algorithm 2, we consider three simple \u201cParkours\u201d missions in the experimentation platform Malmo, described in Section\n2. These missions consist of simple maps that have a special, visually recognizable, position which is defined as goal. A short description of the three missions is given in column two of Table 1. We generally restrict the possible actions to [\u22121, 1]2, where the first dimension is moving forth and back, and the second is strafing (moving sideways). The task is to get to the goal position within 15 seconds in these maps.\nFor each mission, we record one trajectory performed by a human demonstrator, which solves the task. More specifically, we record positions, which we denote by q\u22170:L, and observations (video frames), denoted by y\u22170:L.\nWe run Algorithm 2 with inputs y\u22170:L and a simple proportional controller [2] (for ctl ), where we tuned the proportional constant manually in previous experiments (but without providing q\u22170:L or the actions the human demonstrator took). Let q\u03020:K denote the trajectory of positions that Algorithm 2 subsequently takes in the map. Furthermore, let U \u2208 {fail, success} denote whether the position tracking while-loop of Algorithm 2 (line 8 and following) gets to the goal within 15 seconds. (We are aware that this is a significantly weaker outcome than Algorithm 2 getting to the goal within 15 seconds. Nonetheless, it still tests if the method works in a \u201creasonable\u201d amount of time.)\nThe outcome of the experiment is given in columns three and four of Table 1."}, {"heading": "5.4 Discussion", "text": ""}, {"heading": "5.4.1 Discussion of the experiment", "text": "Outcome. As shown by Table 1, Algorithm 2 is successful for Missions 1 and 3. It fails in Mission 2 due to the repetitive structure of a wall that fills the complete image that is observed at some point during y\u22171:L. This wall makes the mapping from position to observation (video frame) (locally) non-injective which makes the algorithm fail. Note that this problem could quite easily be overcome by using prior assumptions on the smoothness of q\u22170:L together with considering more than one minimum of dist as the potential true position (using, e.g., Bayesian optimization) or by searching for position sequences longer than 1 that match y\u22171:L.\nLimitation of the experiment. A clear limitation of the experiment is that the human demonstrator that produced y\u22171:L ran the same \u201chardware\u201d as Algorithm 1, while the overall goal of this paper is to integrate heterogeneous information. However, we believe that the experiment can inspire more general methods."}, {"heading": "5.4.2 Theoretical analysis and insight", "text": "Efficiency gain from harnessing y\u22171:L. Assuming there are at most N positions which the demonstrator can reach within one time step, Algorithm 1 takes only about O(L \u00b7N) steps to get to the goal. Note that this theoretical analysis is supported by the empirical evaluation: Algorithm 2\u2019s trajectories - visualized in Figure 2 - are roughly as long as y\u22171:L (note that the visualization does not show the local search of length N ).\nThis has to be contrasted with an agent that does not integrate the information y\u22171:L, and therefore, in the worst case, has to search all positions in the map, a number which is usually is much higher than O(L \u00b7N) (roughly O(L2)).\nComparison to LfD. The task we study is closely related to LfD. However, note that usually in LfD, the target agent (\u201clearner\u201d) has access to the demonstrator\u2019s actions, which is not necessary for our method. Furthermore, in our method, in some sense, the target agent can be seen as translating y\u22171:L into its own \u201ccoordinate system\u201d itself, while this mapping is usually hand-crafted in LfD.\nSome insights during the development of the method. An interesting insight during the development of Algorithm 5.2 was that while the low-pass filter only led to minor improvements, what really helped was to visit each position say three times and then optimize the averaged distance. Furthermore, it was was surprising how well the simple Euclidean distance we used worked.\nLimitations. Note that the method is limited to environments similar to landscapes where some stochasticity and variation may be contained, but not too much. For instance, if the environment varies to strongly in the dependence on the time an agent spends in the environment, the proposed method most likely fails since the tracking of y\u22171:L usually takes longer than the original performance of it."}, {"heading": "6 Investigation 2: integrating sensory data, hardware specifications and causal relations", "text": "In this section we aim to shed light on the following aspects of the problem formulated in Section 3:\n\u2022 How can information on the the hardware specifications of various agents be used for knowledge transfer between them? \u2022 To what extent can causal models help, e.g., for integrating those hardware specifications (i.e., information on the \u201cdata producing mechanisms\u201d)?8 \u2022 How can information from the \u201csubjective perspective\u201d of an agent (i.e., on the relation between its sensory measurements and its actions) be merged with information from an \u201coutside perspective\u201d (i.e., that of an engineer which sees the hardware specifications of an agent).\nThe investigation is structured as follows: in Section 6.1 we describe the scenario, in particular the available heterogeneous information sources, in Section 6.2 we outline an information-integrating agent for that scenario, then, in Section 5.3, we give an intuitive toy example of scenario and method, and last, in Section 5.4 we discuss advantages and limitations of our approach.\nKeep in mind the definition of causal models in Section 2. It needs to be mentioned that at certain points in this section we will allow ourselves some extent of imprecision (in particular in the treatment of the (causal) model M and how it is inferred), since, in particular, we aim at going beyond what current rigorous modeling languages allow."}, {"heading": "6.1 Scenario", "text": "Task. We consider a scenario where a collection C of self-driving cars (one could also think of robots) operates in a shared environment. (For simplicity we assume that the number of cars is small compared to the size of the environment, such that they do not affect each other.) We assume that while some hardware components of the\n8 Another reason why causal reasoning could help is that in the end we are interested in the causal effects of an agent on its environment, and not just correlational information.\nAlgorithm 3 Integration and control algorithm for agent j\n1: input: Time point t, description D, specifications (speck)k\u2208C , experiences (ek1:t)k\u2208C . 2: Initialize a causal model M by the causal diagram implied by the description D over the set of factors in (speck)k\u2208C and (ek1:t)k\u2208C . 3: Update the \u201cbelief\u201d over the mechanisms in M using all values of variables contained in (speck)k\u2208C and the experiences (ek1:t)k\u2208C (possibly based on additional priors). 4: From the updated M , calculate M j , the implication of M for agent j. 5: Find action u(t) that is optimal w.r.t. the given task, under M j . 6: output: u(t)\ncars differ, others are invariant between them. We assume that for each car a task (e.g., to track some trajectory) is given and fixed.\nAvailable heterogeneous information. Note that we could allow C to vary over time, e.g., to account for the fact that new cars get on the road every day, however, for the sake of a simple exposition, we leave it fixed here. We assume the following information sources to be available at time t:\n\u2022 specifications speck of the hardware of each agent k \u2022 past experiences (i.e., actions and observations) ekt of all agents\nk \u2208 C, consisting of observations yk(t) and control outputs uk(t), i.e., ekt = (uk(0), yk(0), . . . , uk(t), yk(t)). \u2022 a description D (e.g., a physical or causal model or collection of such models) consisting of (1) a set of independence statements9\nand (2) a set of dependence statements, possibly including specific information on the shape of the dependence, w.r.t. the factors contained in the specifications speck and the experiences ek, for k \u2208 C. Potentially, the various independence and dependence information pieces could come from different sources. We assume the dependence structure (including the precise shapes of the dependences) to be time-invariant.\nRelation to general problem. This scenario captures certain aspects of Example 3 in that some higher-level information in the form of the description D is available. While here, as a first step, we only consider mathematical models, in the future one could also imagine to include informal but well-structured models and descriptions (in simple natural language), possibly translating them into formal models as an intermediate step (using, e.g., machine learning). Furthermore, the scenario captures important aspects of Example 1, since we consider the integration of information from certain source selfdriving cars for the decision making of a given target car."}, {"heading": "6.2 Sketch of a method", "text": "We sketch a method for the described scenario in Algorithm 3."}, {"heading": "6.3 A toy example", "text": "Let us illustrate how the method proposed in Section 6.2 works in a concrete toy scenario. The core intuition is that while some details\n9 Independence assertions seem central to integration of information: only based on statements of the form \u201cY is more or less independent of all factors that potentially will be included, except for this and this small set\u201d it seems possible to rigorously (automatically) reason about integration.\nof the dynamics of self-driving cars may vary between different cars, they can still share information on say the road conditions (friction, drag, etc.) at certain positions y, or the like.\nSpecific scenario. We consider two simplified self-driving cars, i.e., C = {1, 2}, where we assume the observation yk(t) (or y(t) if we refer to a model for both cars) to be the car\u2019s position. We consider the following concrete instances of the information sources listed in Section 6.1. (Note that, for the sake of simplicitly, we assume all mechanisms to be deterministic, such that we can model them by functions f... instead of conditional distributions p(. . . |pa...), although the latter would be more general, see Section 2.)\n\u2022 The specification for car k is given by its power (e.g., measured in horse powers), i.e., speck = hpk. \u2022 The experience of car k consist of all past position-action pairs of both cars, i.e., ekt = (uk(0), yk(0), . . . , uk(t), yk(t)). \u2022 The description D consists of three elements: \u2013 an engineer contributes the equation F (t) = fF (u(t), hp) for\nthe engine of the cars10, where F (t) is the force produced by the engine (incl. gears), and fF a known function,\n\u2013 a physicist contributes the equation y\u0308(t) = 1 m (F (t)+G(t)) for the acceleration y\u0308(t) of the cars, where G(t) are other forces that affect the cars, such as friction and drag, and m is the known mass (which we assume to be the same for both cars),\n\u2013 the same or another physicist produces a set of additional independence assertions, such that altogether the description D implies the causal diagram H depicted in Figure 3.11\nImplementation of our method. We suggest the following concrete implementation of the crucial part of our method, i.e., line 3 and 4 in Algorithm 3, based on the concrete instances of information available in this toy scenario. Keep in mind that fG, the function that maps a position y to the corresponding force G at that position and thus models the generating mechanism for G(t), is the only unknown part of M after initializing it by D.\n\u2022 Line 3: Use the experience (ek1:t)k\u2208C , to infer the function fG on all positions y visited by either of the cars, based on the equation\nmy\u0308 \u2212 fF (u, hpk) = fG(y)\n10 In particular, the mechanism specification implies non-influence by all other relevant factors. 11 A better way to describe the physical forces causally might be to replace y\u0308(t) in the equations and in D by v(t + \u2206t), v(t), and \u2206t, where v(t) denotes the velocity.\nfor all k \u2208 C, and the fact that the l.h.s. of this equation as well as y are known for all positions (and accelerations) visited by either of the cars. \u2022 Line 4: calculate \u201cp(y\u0308|do(u), y, hpj )\u201d, i.e., the effect of control action u of agent j at position y, for all positions that were visited before by either of the cars."}, {"heading": "6.4 Discussion", "text": "The toy example in Section 6.3 shows how in principle the integration of heterogeneous information could help some \u201ctarget\u201d selfdriving car for better decision making in situations not visited by it but by different \u201csource\u201d self-driving cars. It is important to note that all listed information sources were necessary for this: the hardware specifications are necessary to understand F (t), the experience is necessary to infer fG, and the independence knowledge (hp not affecting G(t)) is necessary to transfer the knowledge about the force G(t) on various positions y between the cars. Note that the above scenario cannot be tackled by standard RL approaches since we transfer knowledge between agents of different hardware. Furthermore, methods like LfD or transfer learning (see Sections 4.2 and 4.4) usually do not automatically harness information on hardware specifications of agents.\nBased on our preliminary investigation above, it seems that causal models are helpful in that they provide a language in which one can express relevant assumptions and reason about them. However, from the perspective of practical methods, it is not clear if the necessary calculations could not be genuinely done e.g. in classical probabilistic models.\nAn important question is whether the method sketched in Section 6.2 can be generalized to dependence statements in more natural - but still well-structured - language than equations and directed acyclic graphs (DAGs)."}, {"heading": "7 Outlook: future directions and open questions", "text": "Here we sketch a potential agenda for future investigations and pose interesting open questions."}, {"heading": "7.1 Potential future directions", "text": "\u201cUniversal representation\u201d. An interesting subject-matter of future research would be a \u201cuniversal representation\u201d of the real world, and in particular the semantics of such a representation, since data alone is nothing - it needs to be related to the world. By a universal representation we mean a rich representation to which each information source could be translated, and which each agent could understand, instead of hand-crafting mappings for each (new) pair of source and agent individually (as is usually done in e.g. LfD, see Section 4.2) Such a \u201cuniversal representation\u201d to and from which all data could be mapped would reduce the number of necessary mappings from n2 to n, when n is the number of agents among which information should be integrated. In some sense, for instance the global positioning system (GPS) could already be seen as a unified automatic representation of position. The important point here is that GPS data by default carries a clear semantics.\nInvestigation and classification of the \u201cintegration mapping\u201d. Another important concept for integration of heterogeneous information could be the mapping that transforms a collection of pieces of well-structured heterogeneous information into a model of the\ncurrent situation or even directly into action recommendations. The study of such a mapping could build on the investigation of related mappings in LfD and transfer learning for agents (see Sections 4.2 and 4.4). Furthermore, parts of such a mapping could be learned (which would be related to Machine-learning based multi-agent systems, see Section 4.3. Generally, it would be interesting to examine the basic conditions under which the integration of heterogeneous information can be beneficial. Note that one way to classify and order various sources of information (and the mappings that are necessary to integrate them) would be from \u201cclosest to the agent A\u201d, i.e., it\u2019s own past observations and actions to \u201cmost distant\u201d, e.g., agentindependent descriptions of the world in simple natural language, as exemplified in Section 3. It remains to be answered how a mathematical treatment of the integration mapping would look like. Potentially, it would be based on information spaces Gi, i = 1, . . . ,K and a function st : \u00d7iGi \u2192 B, where B is either directly the space of possible actions, or some step in between, such as a space of models.\nFurther experiments. Further experiments, with a gradually increasing difficulty (e.g. along the ordering proposed in the previous paragraph), could be performed, e.g. using the platform Malmo (Section 2) to gain a better understanding of integration of information:\n1. Agents can observe other agents from a third-person perspective, enabling Example 2 in Section 3. 2. Higher level observations can be provided in the form of natural language (typed chat or external information in the form of natural language), or through artifacts such as maps, sign-posts, symbolic clues, etc."}, {"heading": "7.2 Open questions", "text": "It would also be interesting to investigate how the following questions could be answered:\n\u2022 One of the main question which guided our investigation in Section 6 can be cast as follows. While the information relevant to an agent is usually in the form of effects of its actions in certain situations, a lot of knowledge is formulated in non-causal form: for instance street maps at various granularities for self-driving cars. How are these two forms of information related? Is there a standard way to translate between them? Stated differently, how can various forms of information be translated into a model of the dynamics of the agent in the world. \u2022 Where is the boundary between additional heterogeneous information and prior knowledge? \u2022 How can the need for information integration be balanced with privacy restrictions? For instance, one may imagine cases where the mapping from a source agent\u2019s experience to a target agent\u2019s action is rather simple in principle, but information collected by the source agent cannot or should not be transmitted to the other, at least not in full. \u2022 How can big databases of information be filtered for useful information, i.e., the information which is correct and relevant for the current environment and task? \u2022 To what extent is the problem of information transfer between two different agents in the \u201csame\u201d environment just a specical case of transfer between different environments? \u2022 How can we reason without having a \u201cglobal\u201d model such as Figure 3? What about interfaces to build global from \u201clocal\u201d models, describing only say the engine?"}, {"heading": "8 Conclusions", "text": "In this paper, we considered the problem of designing agents that autonomously integrate available heterogeneous information about their environment. We investigated how experimentation in complex simulated environments on the one hand, and causal models on the other, can help to address this problem. A next step would be to perform more sophisticated experiments, e.g., with agents of different \u201chardware\u201d."}, {"heading": "9 Acknowledgments", "text": "The authors thank Mathew Monfort, Nicole Beckage, Roberto Calandra, Matthew Johnson, Tim Hutton, David Bignell, Daniel Tarlow, Chris Bishop and Andrew Blake for helpful discussions."}], "references": [{"title": "A survey of robot learning from demonstration", "author": ["Brenna D. Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Feedback systems: an introduction for scientists and engineers", "author": ["Karl Johan Astr\u00f6m", "Richard M Murray"], "venue": "Princeton university press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Causal inference from big data: Theoretical foundations and the data-fusion problem", "author": ["Elias Bareinboim", "Judea Pearl"], "venue": "Technical report, CALIFORNIA UNIV LOS ANGELES DEPT OF COMPUTER SCI- ENCE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "The Malmo platform for artificial intelligence experimentation", "author": ["David Bignell", "Katja Hofmann", "Tim Hutton", "Matthew Johnson"], "venue": "IJ- CAI (to appear),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Multi-agent reinforcement learning: An overview\u2019, in Innovations in Multi-Agent Systems and Applications-1", "author": ["Lucian Bu\u015foniu", "Robert Babu\u0161ka", "Bart De Schutter"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Consensus and cooperation in networked multi-agent systems", "author": ["Reza Olfati-Saber", "Alex Fax", "Richard M Murray"], "venue": "Proceedings of the IEEE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Causality", "author": ["J. Pearl"], "venue": "Cambridge University Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Transportability of causal and statistical relations: A formal approach", "author": ["Judea Pearl", "Elias Bareinboim"], "venue": "Proceedings of the Twenty- Fifth National Conference on Artificial Intelligence. AAAI Press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Accelerating reinforcement learning through implicit imitation", "author": ["Bob Price", "Craig Boutilier"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Causation", "author": ["P. Spirtes", "C. Glymour", "R. Scheines"], "venue": "prediction, and search, MIT, Cambridge, MA, 2nd edn.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Multiagent systems: A survey from a machine learning perspective", "author": ["Peter Stone", "Manuela Veloso"], "venue": "Autonomous Robots,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E Taylor", "Peter Stone"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A new learning paradigm: Learning using privileged information", "author": ["Vladimir Vapnik", "Akshay Vashist"], "venue": "Neural Networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Making things happen: A theory of causal explanation", "author": ["James Woodward"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}], "referenceMentions": [{"referenceID": 3, "context": "For the experiments in Section 5 we will use the software Malmo, a simulated environment for experimentation with intelligent agents, that was introduced recently [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "Mathematically, a causal model [8, 11] M over", "startOffset": 31, "endOffset": 38}, {"referenceID": 10, "context": "Mathematically, a causal model [8, 11] M over", "startOffset": 31, "endOffset": 38}, {"referenceID": 0, "context": "According to [1], in learning from demonstrations (LfD), some \u201cteacher\u201d performs a trajectory which is recorded, and the goal is that a \u201clearner\u201d agent, based on this recording, infers and imitates (or utilizes) the teacher\u2019s \u201cpolicy\u201d (or the dynamics of the environment, or both).", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "Central notions that [1] uses to analyze and distinguish various types of LfD problems are the record mapping, i.", "startOffset": 21, "endOffset": 24}, {"referenceID": 9, "context": "6 Note that there is some work on learning from observations only (not actions) of a \u201cteacher\u201d [10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "In multi-agent systems, collections of agents acting in a shared environment are studied [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "One important task is collaboration between agents [7].", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Learning-based methods have been extensively studied [5].", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "For instance, [14] consider an example where, in the well-known mountain car example, experience should be transferred although the motor of the car is changed.", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "However, the scope of methods reviewed in [14] is on transferring observationaction recordings or things such as policies, value functions etc.", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "Recently, the experimentation with intelligent agents in platforms based on computer games has become popular [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 8, "context": "The general integration and transfer of data (not focused on intelligent agents) using causal models has been studied by [9, 3].", "startOffset": 121, "endOffset": 127}, {"referenceID": 2, "context": "The general integration and transfer of data (not focused on intelligent agents) using causal models has been studied by [9, 3].", "startOffset": 121, "endOffset": 127}, {"referenceID": 13, "context": ", by [15].", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": ", by [16].", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "We run Algorithm 2 with inputs y\u2217 0:L and a simple proportional controller [2] (for ctl ), where we tuned the proportional constant manually in previous experiments (but without providing q\u2217 0:L or the actions the human demonstrator took).", "startOffset": 75, "endOffset": 78}], "year": 2017, "abstractText": "The amount of digitally available but heterogeneous information about the world is remarkable, and new technologies such as self-driving cars, smart homes or the \u201cinternet of things\u201d will further increase it. In this paper we examine certain aspects of the problem of how such heterogeneous information can be harnessed by intelligent agents. We first discuss potentials and limitations of some existing approaches, followed by two investigations. The focus of the first investigation is on using the novel experimentation platform Malmo to obtain a better understanding of the problem. The focus of the second investigation is on understanding how information about the hardware of different agents (such as self-driving cars), the agents\u2019 sensory data, and physical or causal information can be utilized for knowledge transfer between agents and subsequent more data-efficient decision making. Finally, we present some thoughts on what a general theory for the problem could look like, and formulate open questions.", "creator": "LaTeX with hyperref package"}}}