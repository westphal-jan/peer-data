{"id": "1606.04429", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2016", "title": "Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation", "abstract": "the selection of how a suitable document representation systems approach plays a significant crucial role concerned in the performance of constructing a document clustering validation task. being able to pick out specific representative verbal words carried within to a scanned document can lead greatly to considerable substantial functional improvements in document recognition clustering. in the case learning of web documents, extending the dominant html markup hierarchy that defines the entity layout nature of the content language provides then additional structural information that they can be further exploited to identify representative cultural words. in announcing this collaborative paper we introduce a general fuzzy term descriptive weighing approach scheme that makes the most predictable of the characteristic html structure accessible for document clustering. and we set forth and build on the hypothesis that a good representation can potentially take advantage of clearly how humans visually skim through documents aligned to extract the search most influential representative words. since the metadata authors definition of web pages make use of specialised html tags created to convey the presumed most important geographic message of a web item page through clever page capturing elements that attract the tagged readers'' attention, such as typed page titles or emphasized elements. simultaneously we sometimes define a set of criteria aligned to progressively exploit precisely the information which provided uniquely by designing these page sharing elements, and introduce a fuzzy identification combination of all these criteria approaches that we evaluate within even the typical context of a web page clustering task. our standard proposed approach, called descriptive abstract integrated fuzzy combination of criteria ( afcc ), can adapt to datasets. whose features are distributed differently, achieving good results compared to competitors other similar fuzzy fuzzy logic property based testing approaches and tf - idf across different datasets.", "histories": [["v1", "Tue, 14 Jun 2016 15:44:52 GMT  (1854kb,D)", "http://arxiv.org/abs/1606.04429v1", "This is the accepted version of an article accepted for publication in IEEE Transactions on Fuzzy Systems"]], "COMMENTS": "This is the accepted version of an article accepted for publication in IEEE Transactions on Fuzzy Systems", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["alberto p garc\\'ia-plaza", "v\\'ictor fresno", "raquel mart\\'inez", "arkaitz zubiaga"], "accepted": false, "id": "1606.04429"}, "pdf": {"name": "1606.04429.pdf", "metadata": {"source": "CRF", "title": "Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation", "authors": ["Alberto P. Garc\u0131\u0301a-Plaza", "Arkaitz Zubiaga"], "emails": ["alpgarcia@lsi.uned.es,", "vfresno@lsi.uned.es,", "raquel@lsi.uned.es,", "arkaitz@zubiaga.org"], "sections": [{"heading": null, "text": "Index Terms\nFuzzy Systems, Document Representation, Web Page Clustering.\nI. INTRODUCTION\nACCESS to and retrieval of web documents in large collections can be substantially eased whenthe documents are properly clustered into topics. The organization of documents into clusters then facilitates focusing search on the topic or topics of interest, shrinking down the large collection to smaller sets of topically related resources. While a body of research has studied clustering of web documents, little attention has been paid to the improvement of document representation techniques and the definition of robust term weighting functions.\nWe are interested in the study of document representation techniques based on fuzzy logic that can generalize across datasets when the purpose is to group documents by topic in the absence of category information. This can be particularly useful in cases where new categories can emerge, so that the system should be able to accommodate its clustering process to be able to find these new categories with the information extracted from the documents.\nPrior to the clustering process, document representation plays a very important role in web page clustering, and constitutes the central point of research of this work. In the document representation phase we choose the characteristics of the document that we consider useful, and assess how this information could be exploited.\nThe textual content is often used for the representation of web pages, given that it is readily available and is easy to process; however, an unweighted bag-of-words representation of the content does not always lead to optimal results. Interestingly, the content of an HTML document is structured in tags, providing additional clues on how different parts of the content differ from one another, and ultimately affecting its visual presentation [1]. The HTML structure of a web document can be further exploited to identify the\nar X\niv :1\n60 6.\n04 42\n9v 1\n[ cs\n.I R\n] 1\n4 Ju\nn 20\n16\nmost representative words within its content. We pay special attention to document contents, introducing a representation that makes the most of information inherent to the document. Hence, we set out to delve into the study of approaches that garner the additional information that HTML tags provide for improved representation of web documents. Moreover, we also look into the use of additional context information, using anchor texts pointing to web pages, as well as statistics inferred from the whole collection. We assess the suitability of using these additional characteristics for web document representation in a clustering task.\nWe make use of a fuzzy system, as a flexible solution that enables to handle the importance of the different characteristics of web pages. For instance, the titles of web pages can often be deemed rhetorical, where some words are very representative of its content, but other words are solely used to embellish the language. When considering frequency in titles within a linear combination of criteria in order to identify the most important words within a document, these words would get a high importance value, which would not correspond with their real importance to describe the content of the page, since they are only embellishing the language. In these linear combinations, when a word is important with respect to a single criterion, the corresponding component will have a value which will always be added to the importance of the word in the document, regardless of the importance corresponding of the rest of the components. On the contrary, by using fuzzy logic it is possible to define related conditions, e.g., a word should appear in the title and emphasized or within specific parts of the document to be considered important. In the same way, if a word appears in the title but not in other criteria, then we could consider that word less important. Here we delve into the use of fuzzy logic for the purposes of exploiting these characteristics of web pages.\nBuilding on the state-of-the-art unsupervised fuzzy logic approach for HTML document representation [2], known as Fuzzy Combinations of Criteria (FCC), we propose three alternative approaches, namely EFCC, AddFCC, and AFCC. We perform the evaluation of these and additional baseline approaches over three benchmark web page collections through a clustering task using the well-known Cluto library [3]. Our proposed approach AFCC, which more suitably adapts to datasets with different characteristics, consistently outperforms the other approaches on the three datasets under study. AFCC provides a flexible, straightforwardly applicable approach that makes the most of the structure and content of HTML documents for web mining purposes.\nIn what follows, we provide background on the task of web page representation, followed by a summary of previous work in the literature as well as their relevance to our work in Section III. We move on then to the experimentation, describing first the experimental settings in Section V, introducing and evaluating two new approaches, AddFCC and EFCC, to improve the existing FCC approach in Section VI, and further studying the analysis and tuning of membership functions through the so-called AFCC in Section VII. We outline the contributions of our work and conclude the paper in Section VIII."}, {"heading": "II. BACKGROUND", "text": "The document representation process can be split into three stages: (1) selection of feature sources, (2) weighing of those features, and (3) dimensionality reduction. Throughout this paper we delve into these three stages, paying close attention at how to model a term weighing function.\nWithin the selection of feature sources, the information that needs to be represented within each document is picked, e.g., plain textual content, titles, or hyperlinks. There are mainly three different approaches. First, content based, which make use of the textual content of documents. This kind of approaches were initially developed for document retrieval in static collections, but with the popularity of the Internet, they have also been adapted to the Web. Further exploiting the characteristics of the Web, the textual content of the documents has also been enhanced with the information provided by HTML tags about document formatting, page structure, visual aspects, etc. Second, link based, which take advantage of the link structure among the pages in the collections. It considers hyperlinks as citations between pages. When two documents have many incoming links in common, or both documents have outgoing links to\na similar set of documents, then the documents are likely related. Third and last, the hybrid approach, which combines features from the textual content of the document and from the context of the page. Here context can include not only hyperlinks or anchor texts, but also other information sources, such as information inferred from the entire collection, or definitions extracted from external resources such as Wikipedia.\nIn the subsequent step of weighing features, each feature is assigned a weight in each document, the weight being representative of the feature\u2019s importance in the document. There are different elements that can determine the importance of a word within the document. One can then define a set of criteria to make the most of the different elements when it comes to improving the document representation. The initial hypothesis of the present work lies in that a good representation should take advantage of how humans skim through web documents to pick out salient words. For example, some words are explicitly highlighted with specific HTML tags. Then, if one wants to determine the importance of a word in a document, in addition to the rather straightforward frequency of the word in the document, one can also take advantage of these highlighted words as a signal that conveys the remarkable importance of the word.\nIn the final dimensionality reduction step, useless features are removed by keeping the document\u2019s most representative features, which makes it more efficient to be handled computationally."}, {"heading": "III. RELATED WORK", "text": "There have been multiple attempts at exploiting the structure of web pages to maximize understanding of their contents for different purposes. Kwon and Lee [4] aimed to classify web sites by using not only their home pages, but also the content of pages that linked to the home page of each site. Their weighing scheme to establish term importance takes into account different HTML tags such as titles, headlines, and boldfaced texts, to identify the most representative words in a web page. They show that the use of the extended set of pages boosts the performance with respect to the ordinary classifier using only the home pages. Golub and Ard [5] studied how setting the importance of different parts of a web page could have an impact on the outcome of a web page classification task. They classified a set of 1, 003 web pages based on titles, headings, metadata, and text. As a single feature, they found the titles to be the most useful; however, since not all web pages have titles, they found that combining all features leads to the best overall performance. In an earlier work, making use of the link structure among documents, Fisher and Everson [6] analyzed the usefulness of links for web page classification tasks. They conclude that links may be useful, but it depends on link density and quality.\nBesides links and anchor texts, other kinds of information have also been exploited over the years. For instance, Kovacevic et al. [7], Shih and Karger [8], Bohunsky and Gatterbauer [9] and Bartik [10], or more recently Herzog et al. [11], have used the visual appearance of a web page, after rendering its content in a browser, for the purposes of representing web documents. Another work along these lines is that performed by Gasparetti et al. [12], which describes an approach based on the implicit signal that can be captured through web browsing interactions, defining a DOM-based representation of visited pages. While these approaches might be handy for systems that exploit the visual appearance of web pages, our objective instead is to avoid reliance on the visual rendering by solely exploiting the HTML structure.\nInformation from external knowledge bases such as Wikipedia has also been exploited by others such as Hu et al. [13] and Li et al. [14]. The use of these knowledge bases can help enrich the content inherent to the web documents. In these cases, the classification structure of articles within Wikipedia\u2019s taxonomy is leveraged to associate web documents with Wikipedia concepts and categories; this process of linking concepts in documents to Wikipedia articles is also known as wikification [15]. Then, Wikipedia entries or their n-grams are matched with documents to expand the content of each document with related content.\nWhile recent years have seen a growing body of research in the use of fuzzy logic to make the most of the document representation for clustering purposes [16], [17], [18], the exploitation of the characteristics of HTML documents, which are rich in structure, remains relatively unexplored. One of the most recent approaches making use of fuzzy logic representation for semi-structured documents is that introduced by\nEnsan and Biletskiy [19]. The caveat of this approach is the need of a human in the loop for generating templates, which boosts the system\u2019s performance by extracting additional information within a supervised approach. The authors did not however explore an alternative solution for fully automating the process. Our work intends to fill this gap, performing a comprehensive study on the use of the HTML structure and content with fuzzy logic for web document clustering in an unsupervised approach.\nThe works which are closest to ours are by Molinari and Pasi [20], focused on an Information Retrieval task, and by Fresno and Ribeiro [21], who presented an Analytical Combination of Criteria (ACC) to represent web pages in web page classification and clustering tasks. It is based on a linear combination of different heuristic criteria within the Vector Space Model. These criteria were selected taking into account how a human reader skims through a document to identify the most representative words. The criteria used by ACC are title, emphasis, position, and frequency. Based on the same criteria, Fresno [2] proposed an approach called Fuzzy Combination of Criteria (FCC), an alternative way of combining them in a non-linear way. In this case, a fuzzy logic based system is employed to define the expert knowledge about how to combine these criteria. The output is also a single vector within the Vector Space Model, representing the estimated importance of each term in a given document. One of the main advantages of FCC is its flexibility, which can be easily utilized for different purposes within different tasks. In fact, recent works have adapted FCC for different purposes, including Nassem et al. [22] for the detection of near duplicate web pages, and Bartik [10] for web page classification. The use of fuzzy logic for feature selection and web representation is still an active topic of interest, and is used as can be seen in recent research [23], [24]. To the best of our knowledge, however, no alternatives to FCC have been proposed, and therefore FCC represents, at the time of this writing, the state of the art in the fully automated, unsupervised fuzzy model for web page representation based on web page structure.\nIn the present work, we rely on the FCC fuzzy representation as a starting point for our research in order to study the fuzzy combination model in different ways, from analyzing its original definition, to proposing new ways of exploiting the system to perform the combination, as well as to explore the possibility of adapting the system to the input we want to represent. In what follows we further describe the FCC approach, which our work builds on."}, {"heading": "IV. FCC: FUZZY COMBINATIONS OF CRITERIA", "text": "The fuzzy system in FCC is built over the concept of linguistic variable and its fuzzy sets. Each variable describes the membership degree of an object to a particular class and it is defined by human experts. This membership degree is defined by a membership function. For each heuristic criterion (frequency, title, emphasis, and position), an associated linguistic variable is defined, as well as for the system output (importance):\n1) Text Frequency: term frequency in the document. Its input is calculated by normalizing this frequency to the maximum number of occurrences of any term in that document. It is defined in three fuzzy sets: low, medium, and high (see Figure 1a). 2) Title: term frequency within the <title> tag. Its input is calculated by normalizing this frequency to the maximum number of occurrences of any term in the title of that document. It is defined in two fuzzy sets: low and high (see Figure 1b). 3) Emphasis: term frequency in emphasized parts of the text1. Its input is calculated by normalizing this frequency to the maximum number of occurrences of any term in emphasized text segments in that document. It is composed of three fuzzy sets: low, medium and high (see Figure 1c). 4) Position: the global position of a term in the document, defined in two fuzzy sets: standard and preferential (see Figure 1d). It is obtained by means of an Auxiliary Fuzzy System that takes as input all the positions of a term within a document (captured by the other linguistic variable term position) and returns the global position value in terms of two fuzzy sets, standard and preferential.\n1We use a manually created list of HTML tags that add emphasis: <em>, <b>, <u>, <strong>, <big>, <h*>, <cite>, <dfn>, <i>, <blockquote>\n5) Importance: it is the output of the fuzzy system and equates to the estimated importance of a term in the document content. It has five homogeneously distributed fuzzy sets: no, low, medium, high and very high.\nThese membership functions have a trapezoidal shape. All the variables except emphasis are defined by sets of equal size symmetrically distributed along the possible input values. These sets were defined without restricting to specific datasets. However, emphasis is considered separately because when the maximum frequency value for emphasized words in a document is small, the normalization could have high impact on the importance of other emphasized terms. For example, using symmetrical sets and having a maximum of 4 would lead to consider the importance of terms emphasized once as low, when we may want to increase the importance of these terms. For this reason, the sets for emphasis were asymmetrically defined. This way, frequencies that would be strictly low can also be considered as medium, since we can expect small maximum values in emphasis.\nThe other part of the knowledge base is a set of IF-THEN rules. The aim of the rules is to combine one or more input fuzzy sets (antecedents or premises) and to associate them with an output fuzzy set (consequent). Once the consequents of each rule have been calculated, and after an aggregation stage, the final set is obtained representing the word based on its importance within the document content. The complete set of 31 rules defined in the FCC approach can be found in [2, p. 130]. Example 1 shows an example of an IF-THEN rule.\nExample 1: IF Title IS High AND Frequency IS Low AND Emphasis IS Low AND Position is Standard THEN\nImportance IS Low\nThe rule set is complete, so that every possible input has to trigger at least one rule. The inference engine evaluates all the triggered rules on the basis of the Center Of Mass (COM) algorithm, which weighs the output of every triggered rule, taking into account the truth degree of their antecedents. It takes the balance point or centroid of all the scaled membership functions taken together for that variable [25]. The output for each term input to the system is calculated by scaling the membership functions by product and combining them by summation.\nThe rule base presented in [2] relies on the following three considerations: 1) If a word appears in the title or the word is emphasized, it should also appear in one of the other\ncriteria in order to be considered important. This aims to alleviate the problem of rhetoric titles or non-informative highlighting; 2) Words occurring in the beginning or in the end of a document are more likely to be important than the rest of the words, as some documents contain overviews and summaries in order to attract the interest of the reader. When the words in a preferential position do not occur also in the title or emphasized, then we could assume that the document does not adhere to the mentioned structure and we could reduce the importance value of that word;\n3) It might be the case that there are no emphasized words in a document, the document has no title, or the title has no important words. In these cases we have to take care of the penalization it could cause to the combination. If the previous criteria did not pick important words, the word frequencies in the whole document are used. Different from the others, the frequency criterion is always available."}, {"heading": "V. EXPERIMENTAL FRAMEWORK", "text": "In this section, we describe the experimental settings that we use in our research."}, {"heading": "A. Datasets", "text": "To make results comparable to those by Fresno [2], we also use the same two datasets, Banksearch [26] and WebKB [27]. Additionally, we use the Social-ODP-2k9 Dataset [28], which provides the features we need for the extended analysis looking at anchor texts.\n1) Banksearch [26]. A benchmark dataset designed for evaluation of web page clustering. We use the 10 main categories \u2013A to J\u2013,(Commercial Banks, Building Societies, Insurance Agencies, Java, C/C++, Visual Basic, Astronomy, Biology, Soccer, Motor Sports). We removed the other category (K, Sport) for being of a different granularity level and hence not comparable to the rest. This results in 9, 897 documents evenly distributed across categories. 2) WebKB [27]. A dataset that includes web pages from computer science departments of various universities. We use 4, 518 web pages that are categorized into 6 imbalanced categories (Student, Faculty, Staff, Department, Course, Project), after removing the Other miscellanea category that is not comparable to the rest. This dataset is more heterogenous than the others, as web pages on a common subject can be found in different categories, such as Java programming categorized into Student, Course or Department. 3) Social ODP 2k9 [28]. A dataset that consists of HTML documents retrieved from links bookmarked by users on Delicious.com. The classification of these documents was inferred from the taxonomy of the Open Directory Project2. From this dataset, we used 12, 148 documents that passed a valid HTML test. The documents are classified into 17 categories. This dataset is also imbalanced, where the most prominent category accounts for 26% of the documents. In addition to the documents themselves, we collected up to 300 anchor texts per document in the collection. The anchor texts were retrieved by querying Google for links pointing to collection pages."}, {"heading": "B. Baseline", "text": "As a baseline, we compute the weight of each word occurring in a document by using the well-known TF-IDF term weighing function, where the term frequency (TF) in a document is combined with the Inverse Document Frequency (IDF) of that term in the whole collection:\nTF-IDF(ti, dj, D) = TF(ti, dj)\u00d7 log |D|\n|{dj \u2208 D : ti \u2208 dj}| (1)\nwhere ti is a term, j a document, D the whole corpus, |D| is the total number of documents in the corpus and |{dj \u2208 D : ti \u2208 dj}| is the number of documents where the term ti appears.\n2http://www.dmoz.org/"}, {"heading": "C. Dimensionality Reduction", "text": "Dimensionality reduction aims to reduce the number of vector components, consequently attempting to reduce the computational cost while the performance loss is as little as possible. Many different dimensionality reduction approaches have been introduced in the literature, aiming to address the limitations of traditional techniques such as Principal Component Analysis and classical scaling. These approaches range from simpler techniques relying solely on term frequencies, to more complex methods derived from approaches originally defined for text classification. Van der Maaten et al. [29] present a review and comparison of nonlinear dimensionality reduction techniques, which they group into two types: (1) convex techniques (full spectral or sparse spectral), optimizing an objective function that does not contain any local optima; and (2) non-convex techniques (weighted euclidean distances, alignment of local linear models, or neural networks) that optimize objective functions that do contain local optimal.\nOn the other hand, from the perspective of availability and use of labeled data for training, feature selection can be categorized as supervised, semisupervised or unsupervised. When it comes to supervised approaches, He et. al [30] introduce a feature selection algorithm called Laplacian Score, and Kala et al. [31] use Fuzzy C Means clustering to find clusters in the given training data set. Others like [32] and [33] introduced approaches within a semi-supervised learning scenario. For an unsupervised scenario, in the abscense of class information, there are feature selection and dimensionality reduction methods which preserve the local geometrical structure such as Multi-Cluster Feature Selection [34] and L1 Graph Based on Sparse Coding for Feature Selection [35].\nWe introduce a unsupervised reduction method called Most Frequent Terms (MFT), which is based on term importance estimated by a term weighing function. The MFT method works as follows. First, the terms in each document are ranked based on the values of the weighing function. Then, the terms in the first position of the ranked list of each document are sorted according to the number of times they occur in the rankings. In case of a tie, we order them according to the maximum weight between them. We then do the same for terms in the second position of ranking, in the third position, and so on. The process stops when the desired number of terms is reached. Even though the resulting list may be larger than the size sought, the ordered list enables us to get the exact number of terms from the top.\nAs an alternative dimensionality reduction method, we also compare Latent Semantic Indexing (LSI) [36]. LSI projects the initial space of documents and their words into a reduced vector space, where the mapping is performed in such a way that the independence is kept for terms that do not co-occur."}, {"heading": "D. Clustering Algorithm", "text": "We chose Cluto rbr (k-way repeated bisections globally optimized) as the clustering algorithm [3] for our experiments. The number of clusters k is set to the number of categories in each dataset to make the evaluation process more intuitive. Having k set to the actual number of clusters enables to explore differences between representation approaches, leaving aside the effect of the selection of the number of clusters. The rest of the parameters are set to their default values."}, {"heading": "E. Evaluation Measure", "text": "We use the F1 score [37] as the evaluation measure (see Equation 2).\nF1 = 2 \u00b7 precision \u00b7 recall precision+ recall\n(2)\nwhere precision and recall are defined as follows:\nprecision = |{relevant docs} \u2229 {retrieved docs}|\n|{retrieved docs}| (3)\nrecall = |{relevant docs} \u2229 {retrieved docs}|\n|{relevant docs}| (4)\nFrom these, the F1 score for each category can be computed. The overall F1 score is computed as the weighted average of the F1 scores for each category.\nVI. IMPROVING THE COMBINATION OF CRITERIA In this section, we evaluate the performance of the state-of-the-art fuzzy logic approach FCC. We also\npropose and evaluate two novel alternative approaches, EFCC and AddFCC."}, {"heading": "A. Study of FCC and Individual Criteria", "text": "As an initial comparative study over the existing FCC approach, we propose and analyze four variations of this term weighing function, one for each criterion, in such a way that the output of the system will depend only on one criterion at a time. Table I shows an example rule base for a system that solely relies on the emphasis criterion to determine the output.\nWe used the MFT reduction given that it selects the highest weighted features without transforming them. This enables us to perform a fairer comparison of different term weighing approaches.\nTable II and Figure 2 show the results of each individual criterion compared to FCC, where each column shows the performance for different numbers of features ranging from 100 to 5,000, as well as the average.\nFor Banksearch, FCC outperforms all individual alternatives, showing the importance of the combination of criteria. Among the individual criteria, frequency performs best, while position is the worst.\nThe results for WebKB are quite different. On one hand, frequency is not always the best among individual criteria and, on the other hand, FCC does not always outperform individual criteria, specifically title and emphasis obtain equal or higher F1-measure values in some cases when the vector dimensions are reduced to 2, 000 and 5, 000, respectively.\nIn this collection, the frequency distribution of emphasized terms shows a more restricted use of emphasis. It could be due to the limited number of web domains and the similarity among web page contents that only come from Universities. These factors could limit the number of different writing styles, fact that would be reflected in a less scattered distribution of emphasized term frequencies. The same consideration about the restrictions on the creation of WebKB can explain the good results achieved by the title criterion. We can expect that authors use titles in a similar way to emphasis, as both resources are used to highlight important words. In the cases where title and emphasis lead to a better clustering, their combination with frequency and position harms the results. In particular, WebKB documents within categories can be much more heterogeneous than in Banksearch, fact that negatively affects the frequency criterion; the combination should help correct this issue, but it does not. Thus, it suggests that frequency and position are hindering the combination."}, {"heading": "B. AddFCC and EFCC: Modifying the Knowledge Base", "text": "The first step to try to improve the fuzzy combination is to understand the bad performance of FCC in WebKB. In the rules of FCC [2], when frequency is low, the output can be very high (the maximum) depending on position, if title and emphasis are high. As we saw before, frequency contributes to a good clustering much more than position, so the output should reflect that fact. But, in this case, frequency is totally ignored. This occurs again when title is low and frequency medium. Both\ncriteria are important for a good grouping, but the output is very high based on position, the same as the previous case. In these cases we are underestimating the discrimination power of frequency and title. The same happens when frequency is medium, being title and emphasis low: position decides again that importance can be the minimum or not, but frequency should count more than position.\nOn the other hand, the whole set of 31 rules in FCC makes the possible combinations more difficult to understand and evaluate. As the fuzzy system is able to combine the conclusions of the rules, an alternative that we propose is the use of a set of single-input rules for each criterion. Thus, the alternative system calculates the output by combining the different outputs of the fired rules. We refer to this approach AddFCC, whose rule base is shown in Table III, which reduces the number of cases that are set to the minimum to keep the rule set complete.\nSince the reduced expressiveness of AddFCC system may give rise to mistakes due to a bad specification of the heuristic knowledge, we introduce another intermediate approach, Extended Fuzzy Combination of Criteria (EFCC). Its rule base combines some criteria explicitly and for others lets the combination to the fuzzy engine (see Table IV). It has two sets of rules: one for frequency and one for the rest of the criteria. This guarantees having at least one rule of each set fired by the system. This avoids\nunderestimation of frequency while also reducing the discriminative power of position.\nTable V and Figure 3 show the clustering results for AddFCC and EFCC, which are compared to FCC. We observe that EFCC improves FCC clustering results in WebKB in all cases while AddFCC does not, while AddFCC outperforms the other approaches for Banksearch in all cases. Nevertheless, EFCC also achieves good results in Banksearch, particularly with small feature sets. AddFCC has the problem of considering all criteria equally important, and hence overestimating position in the combination, as we observed with FCC too.\nAt this point, we opted for EFCC as an alternative to FCC for our subsequent experiments. We also apply LSI and compare the results of EFCC with TF-IDF and FCC (see Table VI and Figure 4).\nGlobally, EFCC MFT achieves the most stable results among collections, and is generally the best approach, with a few exceptions in Banksearch. If one is thinking of applying the representation to a new collection, EFCC MFT would be the best option. It requires fewer terms to achieve its optimal performance for balanced, homogeneous collections. This posits EFCC MFT as a suitable approach to be applied to new, unseen collections. Furthermore, the additive properties of the fuzzy system make it possible to reduce the number of rules needed to specify the knowledge base of EFCC and therefore, the system is easier to understand.\nOn the other hand, the good behavior of MFT depends on the term weighing function applied before. Because of this, we believe that the use of light dimension reduction techniques is a good alternative, at the price of selecting a proper term weighing function, for the clustering problem to solve.\nC. Incorporating Context: Criteria Beyond the Document Itself Moving away from the sole use of the document\u2019s content itself, now we explore the application of two techniques to improve EFCC with contextual information: (1) Inverse Document Frequency, and (2) anchor texts.\n1) Inverse Document Frequency (IDF): With IDF we incorporate information from the whole collection to the representation, which we do by using the product of both:\nEFCC-IDF(ti, dj, D) = EFCC(ti, dj)\u00d7 IDF(ti, D) (5)\nwhere ti is a term, dj a document, and D the whole corpus. Looking at the Table VII and Figure 5, EFCC-IDF works really well with over 500 features in Banksearch, but much worse with 100. WebKB EFCC IDF results are much worse in all cases. This is due to the penalization that IDF applies to common terms. In a clustering task, instead, we look for terms that are common across documents of the same group. Hence, this suggests that the combination of EFCC and IDF is not suitable for the purposes of a clustering task.\n2) Anchor Texts: There are a number of ways of adding anchor texts to document representation methods. We are interested in elucidating whether anchor texts could help improve web page representation in clustering or not, but at the same time, we want to investigate different alternatives for the combination within a term weighing function.\nTo analyze whether and how anchor texts can contribute to the document representation, we explore two different ways of incorporating them using EFCC: (a) Appended to the document\u2019s content itself, and hence contributing to the frequency criterion; and (b) Appended to the document\u2019s title, and therefore\ncontributing at the same level as the title itself. These approaches considering anchor texts are in line with those described by Wang and Kitsuregawa [38] and Huang et al. [39].\nWe did the experiments in three different settings in each case: (1) Adding anchor texts; (2) Adding anchor texts and removing textual content from outlinks; and (3) Removing words that are frequently used across anchor texts, such as \u2019click\u2019, \u2019link\u2019 or \u2019homepage\u2019.\nWe use the SODP dataset in these experiments, as it is the only dataset that includes anchor texts. As it is a new dataset not explored in previous sections, we also compare results with FCC and AddFCC.\nTable VIII and Figure 6 show the results of different alternatives using anchor texts. Each approach has a letter and a number appended, referring to the way in which anchor texts are exploited, as described above. The first three rows of the table show that EFCC outperforms FCC and AddFCC in all cases. This corroborates the limitations of FCC, and reinforces our motivation looking into an alternative approach where not all the criteria contribute equally to the combination. When it comes to the contribution of anchor texts, no approach improves EFCC clearly in all the cases, as the slight differences suggest when looking at the averages. Anchor texts do have a positive impact when we use vectors of small size, particularly when the terms in the anchor texts are considered as page titles (b alternative). However, as we increase the size of the vectors, anchor texts are not useful any more, leading to worse performance.\nFig. 5: Graphical representation of data in Table VII.\nRep. 100 500 1,000 2,000 5,000 Avg. Banksearch EFCC 0.768 0.778 0.758 0.740 0.759 0.760 EFCC-IDF 0.522 0.773 0.799 0.825 0.827 0.749 WebKB EFCC 0.516 0.546 0.545 0.566 0.484 0.532 EFCC-IDF 0.383 0.346 0.291 0.282 0.451 0.350\nTABLE VII: F1 results for EFCC IDF experiments (all with the MFT reduction method).\nRegarding the use of anchor texts as titles, the best option is to just add anchor texts as title terms (named b-1). The slight improvement achieved with anchor texts might not always pay off, given that the collection of anchor texts is a time consuming process.\nDifferent reasons might explain the unsatisfactory results using anchor texts. The collection may have a link structure that is not sufficiently dense, or anchor texts might not be descriptive enough, hence not enabling to capture the topic of documents. This finding is in line with Eiron and McCurley [40] and Noll and Meinel [41], where authors posited that anchor text terms rather resemble terms used in search queries."}, {"heading": "VII. AFCC: ANALYZING AND TUNING THE MEMBERSHIP FUNCTIONS", "text": "We now set forth a proposal to tune the membership functions, which leads to the definition of a revised and novel approach called Abstract Fuzzy Combination of Criteria (AFCC). We first perform a qualitative analysis of the membership functions that we are utilizing, and then we test AFCC, evaluating and analyzing its performance in comparison with the techniques studied previously."}, {"heading": "A. Analysis of the Membership Functions", "text": "It is worthwhile considering that different datasets will have different frequency distributions for each criterion. Few terms in a collection tend to be in many documents, while many terms are used seldom. The effect of normalizing frequencies with respect to the most frequent term is that low values are compressed, and hence under-represented. This compression effect would exacerbate if the total maximum of the collection was used for the normalization process.\nThe fuzzy sets for FCC and EFCC were symmetrically defined, except for emphasis. Thus, some of the fuzzy sets defined for FCC and EFCC would match the initial state of most of the tuning processes of fuzzy rule-based systems.\nIn fact, what we call high or low are not absolute, but relative values. Therefore, a term is considered important because its normalized frequency is higher than most of the rest, and a certain value being high, medium or low depends on the frequency distribution of the dataset. In an ideal case, all term frequencies would be uniformly distributed between 0 and 1 (see Figure 1a), configuring the basic parameters of the fuzzy set using the original heuristic information. However, the fact that texts tend to follow Zipf\u2019s law, suggests that the uniform distribution is not always the case and more sophisticated approaches are needed. Hence, we believe that each particular dataset should have its own features and tuning of membership functions."}, {"heading": "B. Tuning of the Membership Functions", "text": "Given the limitations of FCC, EFCC and AddFCC to deal with varying term distributions across different datasets, we now delve into alternative considerations that further exploit these characteristics, which ultimately leads to the definition of AFCC. In order to automatically adjust the basic parameters of the membership functions, we assume the two base cases that both the words in the documents as well as the emphasized terms will approximate a Zipfian distribution, as defined by Zipf\u2019s law [42]. For the first base case based on the frequency criterion, we consider we have a distribution tending to a power law when the majority of terms, i.e., more than a half of them (55%) have normalized frequencies below 0.2. Depending on whether this condition is fulfilled or not, we set the membership functions with one of the following two alternatives:\n1) When the precondition is fulfilled, we assume a distribution tending to a power law. As we need 5 intervals to build three sets (low, medium and high and two intersection areas between them, see Figure 1a), our worst case would be to have only one possible value for each interval, that is, a maximum frequency of 5. Thus, to guarantee at least one possible value for the low set in that case, we chose the first interval from 0 to 1/5. The rest of the intervals are selected using equidistant percentiles for term frequencies from 1/5 to 1, because this is suitable for the normalized frequencies that we found in our test data; 2) When our precondition is not fulfilled, then we assume that the distribution tends to be closer to uniform, so that we can establish the fuzzy sets with the original heuristic, that is, all of them will have the same size. We use the corresponding percentiles to fit the distribution slightly better than using exact values (0.2, 0.4, 0.6, 0.8, see Figure 1a). Notice that in case of a uniform distribution, the adjustment for the first case\u2014distribution tending to Zipf\u2019s law\u2014would lead to these exact values too, because as the distribution moves towards a uniform distribution, the percentile 0.2 will approximate to 1/5 and the rest of the parameters belong to equidistant percentiles relative to this initial value in both cases. In those cases, the fuzzy sets would be symmetrical, that is, not only the case of the original sets in FCC and EFCC, but also the initial case used by most of the tuning methods of fuzzy rule-based systems.\nWith regard to the emphasis criterion, we follow the same precondition as with frequency to determine whether or not the distribution tends to a power law, but modifying the fitting rules due to the different meaning of emphasis. Again, we have two alternatives for emphasis:\n1) When the distribution tends to a power law, we set the first interval as in the frequency case, and the rest with decreasing percentiles, each being a half of the previous. The reason is that in the original heuristic-based fuzzy sets, the medium set is the biggest one, and we want to preserve the original heuristic knowledge, but always taking into account the relative difference between the number of elements in each set instead of absolute exact values; 2) If the collection does not fulfill our precondition, we assume that the distribution tends to be more uniform, so that we can establish the basic parameters of the membership functions by using the original heuristic rules but, as in the case of frequency, we use the percentiles instead of the exact values to fit slightly better the distribution (in this case the values were 0.05, 0.15, 0.55, 0.75, see Figure 1c).\nIn the case of titles, we use the lowest value of the distribution to set the first interval, dividing the rest of the space in equidistant percentiles. Finally, it must be noted that we do not adjust the auxiliary system because the positions of words in a page do not depend on anything else than the number of words in the document.\nWe refer to this new approach we came up with after the analysis and tuning of the membership functions as Abstract Fuzzy Combination of Criteria (AFCC), which we test and evaluate next."}, {"heading": "C. Empirical Analysis of AFCC", "text": "As AFCC represents a modification over the fuzzy logic based approaches, we use FCC and EFCC as baselines, as well as TF-IDF. We apply the MFT reduction in all cases to compare the weighing functions in the same conditions.\nTable IX and Figure 7 show F1 scores for these representations. On the one hand, looking at the results, among the fuzzy logic based representations, AFCC outperforms the rest in WebKB in all cases, while in Banksearch got better results than the others with 2 out of 5 vector sizes, having also a higher average F1 score. This varying performance across collections could be due to the fact that frequency distributions in Banksearch rather approximate a power law. In those cases, the least frequent terms are assigned to the low fuzzy set, with few terms remaining for the medium and high sets. This explains the small difference between the EFCC and FCC fixed sets. The same occurs with SODP, where EFCC and AFCC get similar results, although FCC performs worse, probably due to its underestimation of frequency. However, with a rather uniform term frequency distribution, as in WebKB, adjusting the fuzzy sets has a much bigger effect in results, where more terms are assigned to the medium and high fuzzy sets, and small variations of the basic parameters of the membership functions will have a much bigger effect. It is indeed important to adapt to this kind of distributions, as the terms are differently used and structured.\nOn the other hand, TF-IDF obtained surprisingly good results in SODP compared to the results of the same function with Banksearch and WebKB datasets. In general, results with all the representations tend\nto be worse in SODP, due to the special difficulties of this collection. We believe that the use of IDF could help improve the results of TF-IDF because it would alleviate the effect of the bigger categories, whose terms would be penalized giving more representativeness to those belonging to smaller categories. This fact would reduce slightly the bias introduced by the bigger categories, allowing to cluster the smaller ones slightly better. This improvement in the clustering of smaller categories could lead to an improvement in the overall clustering results of TF-IDF.\nIn general, adjusting the membership functions to the dataset seems to be useful not only to add more automatism to the document representation process, but also because this automation allows the system to adapt better to datasets with specific characteristics. The proposed method is able to achieve similar results to EFCC when dealing with exponential distributions. Moreover, when the shape of the distribution changes, the adjustment helps improve clustering results, as is the case of WebKB. Figure 8 shows a summary of the resulting membership functions and the distributions of input values for each dataset and criterion."}, {"heading": "D. Statistical significance", "text": "We analyze in depth the difference between using membership function tuning and the original representation with fixed fuzzy sets. Besides, we also include FCC in the comparison. We are interested in seeing the global improvements of the new proposal, AFCC, with respect to the original baseline. Each dataset was divided in 100 different sub- datasets 50% smaller than the original, where the size of each category is in proportion to the original ones. We performed 100 experiments per vector size corresponding to each sub-dataset, resulting in a total of 4,500 different clustering experiments. We calculated the statistical significance between F1 scores of each pair of representations (AFCC-FCC, EFCC-FCC, AFCC-EFCC) with a paired two-tailed t-test for each vector size.\nIn Table X and Figure 9, for each vector size and representation we show the average F1 scores of the 100 clustering experiments (one per sub-dataset), and in Table X we also show the difference between the corresponding averages, and the p-value resulting from applying the statistical t-test between each pair of representations.\nIn most of the cases AFCC outperforms EFCC, and consequently also FCC. Therefore, the difference between term frequency distributions of the datasets, in combination with all of these results allow us to conclude that membership function tuning helps determine each criterion in a better way, ultimately improving clustering results.\nAdjusting the membership functions to a dataset leads to results as good as or better than FCC in 91.6% of the cases, and as good as or better than EFCC in 86.5% of the cases. EFCC and AFCC outperform FCC in most of the cases, and between them, AFCC allows to improve the results of EFCC in 28.5% of the cases."}, {"heading": "VIII. DISCUSSION", "text": "We have studied the application of fuzzy logic for the representation of web documents in a way that imitates humans skimming through the documents. The use of fuzzy logic enables us to separate the knowledge declaration from the calculation procedure, which also enables us to specify the knowledge by means of a set of rules close to natural language applying non linear combinations of criteria. Building on a state-of-the-art unsupervised document representation, Fuzzy Combinations of Criteria (FCC), we have introduced, evaluated, and analyzed three alternatives that make the most of the HTML structure and content of web documents, namely EFCC, AddFCC, and AFCC. We evaluate and compare the representation approaches in a web page clustering task, using three datasets with very different characteristics.\nWe first defined a set of rules fixed on the basis of expert knowledge. Although there are other options to automatically generate these rules (the rule base could be adjusted by using machine learning techniques that adapt sets of rules to sets of sample data, or by using bio-inspired approaches), both approximations\nFi g.\n8: M\nem be\nrs hi\np fu\nnc tio\nns an\nd di\nst ri\nbu tio\nns of\nin pu\ntv al\nue s\nfo r\nea ch\ncr ite\nri on\nan d\nda ta\nse t.\nC ol\num ns\nre pr\nes en\ntc ri\nte ri\na, w\nhi le\nro w\ns re\npr es\nen t\nda ta\nse ts\n.E ac\nh pa\nir of\nda ta\nse ta\nnd cr\nite ri\non in\ncl ud\nes tw\no ch\nar ts\n.T he\nle ft\n-h an\nd si\nde ,b\nig ge\nr ch\nar t,\nsh ow\ns th\ne fin\nal fu\nzz y\nse ts\nfo r\nth e\nm em\nbe rs\nhi p\nfu nc\ntio ns\naf te\nr th\ne au\nto m\nat ic\nad ju\nst m\nen t.\nT he\nri gh\ntha\nnd si\nde ,s\nm al\nle r\nch ar\nt, sh\now s\nth e\ndi st\nri bu\ntio n\nof te\nrm fr\neq ue\nnc ie\ns no\nrm al\niz ed\nby do\ncu m\nen t\nin th\ne da\nta se\nt fo\nr ea\nch cr\nite ri\non ,w\nhe re\nX ax\nis re\nfe rs\nto th\ne in\npu t\nva lu\ne fo\nr th\ne cr\nite ri\non ,a\nnd Y\nax is\nre fe\nrs to\nth e\nnu m\nbe r\nof te\nrm s\nth at\nbe lo ng to th at bi n.\ncould cause a loss of generality in the learned/generated model in the attempt to fit the system to specific sample data. This could lead to illogical rules. On the other hand, in this automated scenario, we would need to deal with the coherence of the rules, which would require to establish a methodology to measure this coherence among rules. Last but not least, our system evaluates each term within each document using a fuzzy approach, which implies a high computational cost. Therefore, the use of machine learning or bio-inspired algorithms would add a considerable cost to the system. Of course, the manual definition of the rules employed in our work could lead to mistakes in the knowledge definition. However, in the same way, the application of machine learning or bio-inspired techniques would always require an initial knowledge to start the process.\nConsidering these aspects, we analyzed three challenges concerning web page representation for clustering: (1) the selection of feature sources to extract essential information from; (2) the term weighing functions to estimate the weight of each feature; and (3) the dimensionality reduction techniques to select\nthe most representative features and to reduce the computational cost of the clustering. For feature selection, we explored the application of new, mostly unstudied criteria to improve the representation with information from the whole collection as well as from anchor texts. For term weighing we explored the fuzzy combination of criteria performed by FCC [2] aiming to get the most of the fuzzy system and the heuristics in which it is based. We use TF-IDF as the baseline, since it is a standard weighting method employed to represent documents. We presented an improved representation called EFCC, which outperformed the baselines, and another alternative called AddFCC, which did not work as well as expected. Both alternatives attempt to exploit the fuzzy system in a different manner to FCC, taking advantage of its additive properties. For dimensionality reduction, we introduced MFT, a lightweight dimensionality reduction technique, based on the term weighing function, which is able to improve the results of more complex techniques such as LSI when used together with EFCC in our test collections.\nWe also studied whether EFCC could be tuned to fit the specific characteristics of different collections. The aim of this adjustment is not only to improve clustering results in those collections, but also to adapt the representation to different datasets that could have different features. We found the case of the WebKB dataset, which has very different characteristics, particularly when looking at terms that are emphasized within the document contents. This led us to further study the tuning of the fuzzy system in an unsupervised way, for which we proposed AFCC. AFCC adjusts the basic parameters of the membership function on the basis of the term distributions of the collections. We showed that AFCC leveled or even improved the good results of EFCC and FCC in all kinds of datasets, outperforming the results of other approaches. Our results show that AFCC is a competitive approach that outperforms the rest of the techniques, with good performance across datasets of very different characteristics.\nFuture work includes the study of the effect of non-linear scaling factors as a complementary tool to our proposal to adjust the representation to specific datasets, and to study new ways of considering the position criterion. A complementary analysis would include the exploitation of the position of words in documents through visual rendering of web pages. Finally, it would be interesting to study and assess the inclusion of additional criteria in the combination."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper. This work has been part-funded by the Spanish Ministry of Science and Innovation (MED-RECORD Project, TIN2013-46616-C2-2-R) and the PHEME FP7 project (grant No. 611233)."}], "references": [{"title": "Web page classification: Features and algorithms", "author": ["X. Qi", "B.D. Davison"], "venue": "ACM Computing Surveys, vol. 41, no. 2, pp. 1\u201331, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Representacion autocontenida de documentos html: una propuesta basada en combinaciones heuristicas de criterios [selfcontained representation of html documents: an approach based on heuristic combinations of criteria", "author": ["V. Fresno"], "venue": "Ph.D. dissertation, Universidad Rey Juan Carlos, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "CLUTO - a clustering toolkit", "author": ["G. Karypis"], "venue": "Tech. Rep. #02-017, Nov. 2003.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Text categorization based on k-nearest neighbor approach for web site classification", "author": ["O.-W. Kwon", "J.-H. Lee"], "venue": "Information Processing and Management, vol. 39, pp. 25\u201344, January 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Importance of html structural elements and metadata in automated subject classification", "author": ["K. Golub", "A. Ard\u00f6"], "venue": "ECDL. Springer, 2005, pp. 368\u2013378.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "When are links useful? experiments in text classification", "author": ["M. Fisher", "R. Everson"], "venue": "Advances in Information Retrieval, 2003, vol. 2633, pp. 547\u2013547.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Visual adjacency multigraphs - a novel approach for a web page classification", "author": ["M. Kovacevic", "M. Diligenti", "M. Gori", "V. Milutinovic"], "venue": "Proceedings of the Workshop on Statistical Approaches to Web Mining, 2004, pp. 38\u201349.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Using urls and table layout for web classification tasks", "author": ["L.K. Shih", "D.R. Karger"], "venue": "Proceedings of the 13th international conference on World Wide Web, ser. WWW \u201904. New York, NY, USA: ACM, 2004, pp. 193\u2013202.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Visual structure-based web page clustering and retrieval", "author": ["P. Bohunsky", "W. Gatterbauer"], "venue": "Proceedings of the 19th International Conference on World Wide Web, ser. WWW \u201910. New York, NY, USA: ACM, 2010, pp. 1067\u20131068. [Online]. Available: http://doi.acm.org/10.1145/1772690.1772807  IEEE TRANSACTIONS ON FUZZY SYSTEMS  21", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Text-based web page classification with use of visual information", "author": ["V. Bart\u0131\u0301k"], "venue": "International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2010, Odense, Denmark, August 9-11, 2010, 2010, pp. 416\u2013420. [Online]. Available: http://dx.doi.org/10.1109/ASONAM.2010.34", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature-based object identification for web automation", "author": ["C. Herzog", "I. Kordomatis", "W. Holzinger", "R.R. Fayzrakhmanov", "B. Kr\u00fcpl-Sypien"], "venue": "Proceedings of the 28th Annual ACM Symposium on Applied Computing, ser. SAC \u201913. New York, NY, USA: ACM, 2013, pp. 742\u2013749. [Online]. Available: http://doi.acm.org/10.1145/2480362.2480504", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Mining navigation histories for user need recognition", "author": ["F. Gasparetti", "A. Micarelli", "G. Sansonetti"], "venue": "HCI International 2014 - Posters\u2019 Extended Abstracts - International Conference, HCI International 2014, Heraklion, Crete, Greece, June 22-27, 2014. Proceedings, Part I, 2014, pp. 169\u2013173.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting wikipedia as external knowledge for document clustering", "author": ["X. Hu", "X. Zhang", "C. Lu", "E.K. Park", "X. Zhou"], "venue": "KDD, 2009, pp. 389\u2013396.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "An information classification approach based on knowledge network", "author": ["H. Li", "G. Sun", "B. Xu", "L. Li", "J. Huang", "K. Tanno", "W. Wu", "C. Xu"], "venue": "IEEE 8th International Symposium on Embedded Multicore/Manycore SoCs, MCSoC 2014, Aizu-Wakamatsu, Japan, September 23-25, 2014, 2014, pp. 3\u20138. [Online]. Available: http://dx.doi.org/10.1109/MCSoC.2014.10", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis and enhancement of wikification for microblogs with context expansion.", "author": ["T. Cassidy", "H. Ji", "L.-A. Ratinov", "A. Zubiaga", "H. Huang"], "venue": "in COLING,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "A survey of fuzzy web mining", "author": ["C. Lin", "T. Hong"], "venue": "Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery, vol. 3, no. 3, pp. 190\u2013199, 2013. [Online]. Available: http://dx.doi.org/10.1002/widm.1091", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "An improved feature weighted fuzzy clustering algorithm with its application in short-term prediction of wind power", "author": ["X. Wang", "D. Luo", "H. He"], "venue": "Pattern Recognition. Springer, 2014, pp. 575\u2013584.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "A clustering algorithm based on feature weighting fuzzy compactness and separation", "author": ["Y. Zhou", "H.-F. Zuo", "J. Feng"], "venue": "Algorithms, vol. 8, no. 2, pp. 128\u2013143, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Matching semi-structured documents using similarity of regions through fuzzy rule-based system", "author": ["A. Ensan", "Y. Biletskiy"], "venue": "Advances in Data Mining. Applications and Theoretical Aspects - 13th Industrial Conference, ICDM 2013, New York, NY, USA, July 16-21, 2013. Proceedings, 2013, pp. 205\u2013217.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "A fuzzy representation of html documents for information retrieval systems", "author": ["A. Molinari", "G. Pasi"], "venue": "Fuzzy Systems, vol. 1, pp. 107\u2013112, 1996.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1996}, {"title": "An analytical approach to concept extraction in html environments", "author": ["V. Fresno", "A. Ribeiro"], "venue": "J. Intell. Inf. Syst., vol. 22, no. 3, pp. 215\u2013235, 2004.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Near duplicate web page detection with analytic feature weighting", "author": ["R. Naseem", "S. Anees", "K. Muneer", "K.S. Farook"], "venue": "ICACC. IEEE, 2013, pp. 324\u2013327.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Fuzzy information retrieval systems: A historical perspective", "author": ["D.H. Kraft", "E. Colvin", "G. Bordogna", "G. Pasi"], "venue": "Fifty Years of Fuzzy Logic and its Applications. Springer, 2015, pp. 267\u2013296.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic text classification and property extraction: Applications in medicine", "author": ["A. Kolonin"], "venue": "SIBIRCON, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Intelligent Systems for Engineers and Scientists", "author": ["A.A. Hopgood"], "venue": "Taylor & Francis,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "The banksearch web document dataset: investigating unsupervised clustering and category similarity", "author": ["M.P. Sinka", "D.W. Corne"], "venue": "J. Netw. Comput. Appl., vol. 28, pp. 129\u2013146, April 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to construct knowledge bases from the world wide web", "author": ["M. Craven", "D. DiPasquo", "D. Freitag", "A. McCallum", "T. Mitchell", "K. Nigam", "S. Slattery"], "venue": "Artif. Intell., vol. 118, pp. 69\u2013113, April 2000.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Getting the most out of social annotations for web page classification", "author": ["A. Zubiaga", "R. Mart\u0131\u0301nez", "V. Fresno"], "venue": "ACM DocEng, 2009, pp. 74\u201383.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Dimensionality reduction: A comparative review", "author": ["L. van der Maaten", "E.O. Postma", "H.J. van den Herik"], "venue": "2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "Advances in neural information processing systems, 2005, pp. 507\u2013514.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic text classification and property extraction: Applications in medicine", "author": ["R. Kala", "A. ShuLkla", "R. Tiwari"], "venue": "2009 WEE International Advaniee Computing Conference (IACC 2009) Patiala, India, 6-7 March 2009, 2009, pp. 541\u2013545.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Dcpe co-training for classification", "author": ["J. Xu", "H. He", "H. Man"], "venue": "Neurocomput., vol. 86, pp. 75\u201385, Jun. 2012. [Online]. Available: http://dx.doi.org/10.1016/j.neucom.2012.01.006", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Kernel fusion-refinement for semi-supervised nonlinear dimension reduction", "author": ["F. Wanga", "R. Li", "Z. Lei", "X.S. Ni", "X. Huod", "M. Chena"], "venue": "Pattern Recognition Letters, vol. 63, no. 1, pp. 16\u201322, October 2015. [Online]. Available: http: //www.sciencedirect.com/science/article/pii/S0167865515001671", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised feature selection for multi-cluster data", "author": ["D. Cai", "C. Zhang", "X. He"], "venue": "Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201910. New York, NY, USA: ACM, 2010, pp. 333\u2013342.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "An Introduction to Latent Semantic Analysis", "author": ["T.K. Landauer", "P.W. Foltz", "D. Laham"], "venue": "Discourse Processes, no. 25, pp. 259\u2013284, 1998.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "Foundations of evaluation", "author": ["C.J. Van Rijsbergen"], "venue": "Journal of Documentation, vol. 30, pp. 365\u2013373, 1974.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1974}, {"title": "Evaluating contents-link coupled web page clustering for web search results", "author": ["Y. Wang", "M. Kitsuregawa"], "venue": "CIKM, 2002, pp. 499\u2013506.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "Multitype features coselection for web document clustering", "author": ["S. Huang", "Z. Chen", "Y. Yu", "W. Ma"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 18, no. 4, pp. 448 \u2013 459, 2006.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Analysis of anchor text for web search", "author": ["N. Eiron", "K.S. McCurley"], "venue": "SIGIR, 2003, pp. 459\u2013460.  IEEE TRANSACTIONS ON FUZZY SYSTEMS  22", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "The metadata triumvirate: Social annotations, anchor texts and search queries", "author": ["M.G. Noll", "C. Meinel"], "venue": "Proceedings of the WI-IAT, vol. 1, 2008, pp. 640\u2013647.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Human behavior and the principle of least effort.", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1949}], "referenceMentions": [{"referenceID": 0, "context": "Interestingly, the content of an HTML document is structured in tags, providing additional clues on how different parts of the content differ from one another, and ultimately affecting its visual presentation [1].", "startOffset": 209, "endOffset": 212}, {"referenceID": 1, "context": "Building on the state-of-the-art unsupervised fuzzy logic approach for HTML document representation [2], known as Fuzzy Combinations of Criteria (FCC), we propose three alternative approaches, namely EFCC, AddFCC, and AFCC.", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "We perform the evaluation of these and additional baseline approaches over three benchmark web page collections through a clustering task using the well-known Cluto library [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 3, "context": "Kwon and Lee [4] aimed to classify web sites by using not only their home pages, but also the content of pages that linked to the home page of each site.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Golub and Ard [5] studied how setting the importance of different parts of a web page could have an impact on the outcome of a web page classification task.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "In an earlier work, making use of the link structure among documents, Fisher and Everson [6] analyzed the usefulness of links for web page classification tasks.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "[7], Shih and Karger [8], Bohunsky and Gatterbauer [9] and Bartik [10], or more recently Herzog et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7], Shih and Karger [8], Bohunsky and Gatterbauer [9] and Bartik [10], or more recently Herzog et al.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "[7], Shih and Karger [8], Bohunsky and Gatterbauer [9] and Bartik [10], or more recently Herzog et al.", "startOffset": 51, "endOffset": 54}, {"referenceID": 9, "context": "[7], Shih and Karger [8], Bohunsky and Gatterbauer [9] and Bartik [10], or more recently Herzog et al.", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "[11], have used the visual appearance of a web page, after rendering its content in a browser, for the purposes of representing web documents.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12], which describes an approach based on the implicit signal that can be captured through web browsing interactions, defining a DOM-based representation of visited pages.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] and Li et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In these cases, the classification structure of articles within Wikipedia\u2019s taxonomy is leveraged to associate web documents with Wikipedia concepts and categories; this process of linking concepts in documents to Wikipedia articles is also known as wikification [15].", "startOffset": 263, "endOffset": 267}, {"referenceID": 15, "context": "While recent years have seen a growing body of research in the use of fuzzy logic to make the most of the document representation for clustering purposes [16], [17], [18], the exploitation of the characteristics of HTML documents, which are rich in structure, remains relatively unexplored.", "startOffset": 154, "endOffset": 158}, {"referenceID": 16, "context": "While recent years have seen a growing body of research in the use of fuzzy logic to make the most of the document representation for clustering purposes [16], [17], [18], the exploitation of the characteristics of HTML documents, which are rich in structure, remains relatively unexplored.", "startOffset": 160, "endOffset": 164}, {"referenceID": 17, "context": "While recent years have seen a growing body of research in the use of fuzzy logic to make the most of the document representation for clustering purposes [16], [17], [18], the exploitation of the characteristics of HTML documents, which are rich in structure, remains relatively unexplored.", "startOffset": 166, "endOffset": 170}, {"referenceID": 18, "context": "Ensan and Biletskiy [19].", "startOffset": 20, "endOffset": 24}, {"referenceID": 19, "context": "The works which are closest to ours are by Molinari and Pasi [20], focused on an Information Retrieval task, and by Fresno and Ribeiro [21], who presented an Analytical Combination of Criteria (ACC) to represent web pages in web page classification and clustering tasks.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "The works which are closest to ours are by Molinari and Pasi [20], focused on an Information Retrieval task, and by Fresno and Ribeiro [21], who presented an Analytical Combination of Criteria (ACC) to represent web pages in web page classification and clustering tasks.", "startOffset": 135, "endOffset": 139}, {"referenceID": 1, "context": "Based on the same criteria, Fresno [2] proposed an approach called Fuzzy Combination of Criteria (FCC), an alternative way of combining them in a non-linear way.", "startOffset": 35, "endOffset": 38}, {"referenceID": 21, "context": "[22] for the detection of near duplicate web pages, and Bartik [10] for web page classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[22] for the detection of near duplicate web pages, and Bartik [10] for web page classification.", "startOffset": 63, "endOffset": 67}, {"referenceID": 22, "context": "The use of fuzzy logic for feature selection and web representation is still an active topic of interest, and is used as can be seen in recent research [23], [24].", "startOffset": 152, "endOffset": 156}, {"referenceID": 23, "context": "The use of fuzzy logic for feature selection and web representation is still an active topic of interest, and is used as can be seen in recent research [23], [24].", "startOffset": 158, "endOffset": 162}, {"referenceID": 24, "context": "It takes the balance point or centroid of all the scaled membership functions taken together for that variable [25].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "The rule base presented in [2] relies on the following three considerations:", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "To make results comparable to those by Fresno [2], we also use the same two datasets, Banksearch [26] and WebKB [27].", "startOffset": 46, "endOffset": 49}, {"referenceID": 25, "context": "To make results comparable to those by Fresno [2], we also use the same two datasets, Banksearch [26] and WebKB [27].", "startOffset": 97, "endOffset": 101}, {"referenceID": 26, "context": "To make results comparable to those by Fresno [2], we also use the same two datasets, Banksearch [26] and WebKB [27].", "startOffset": 112, "endOffset": 116}, {"referenceID": 27, "context": "Additionally, we use the Social-ODP-2k9 Dataset [28], which provides the features we need for the extended analysis looking at anchor texts.", "startOffset": 48, "endOffset": 52}, {"referenceID": 25, "context": "1) Banksearch [26].", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "2) WebKB [27].", "startOffset": 9, "endOffset": 13}, {"referenceID": 27, "context": "3) Social ODP 2k9 [28].", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "[29] present a review and comparison of nonlinear dimensionality reduction techniques, which they group into two types: (1) convex techniques (full spectral or sparse spectral), optimizing an objective function that does not contain any local optima; and (2) non-convex techniques (weighted euclidean distances, alignment of local linear models, or neural networks) that optimize objective functions that do contain local optimal.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "al [30] introduce a feature selection algorithm called Laplacian Score, and Kala et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "[31] use Fuzzy C Means clustering to find clusters in the given training data set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "Others like [32] and [33] introduced approaches within a semi-supervised learning scenario.", "startOffset": 12, "endOffset": 16}, {"referenceID": 32, "context": "Others like [32] and [33] introduced approaches within a semi-supervised learning scenario.", "startOffset": 21, "endOffset": 25}, {"referenceID": 33, "context": "For an unsupervised scenario, in the abscense of class information, there are feature selection and dimensionality reduction methods which preserve the local geometrical structure such as Multi-Cluster Feature Selection [34] and L1 Graph Based on Sparse Coding for Feature Selection [35].", "startOffset": 220, "endOffset": 224}, {"referenceID": 34, "context": "As an alternative dimensionality reduction method, we also compare Latent Semantic Indexing (LSI) [36].", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "We chose Cluto rbr (k-way repeated bisections globally optimized) as the clustering algorithm [3] for our experiments.", "startOffset": 94, "endOffset": 97}, {"referenceID": 35, "context": "We use the F1 score [37] as the evaluation measure (see Equation 2).", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "In the rules of FCC [2], when frequency is low, the output can be very high (the maximum) depending on position, if title and emphasis are high.", "startOffset": 20, "endOffset": 23}, {"referenceID": 36, "context": "These approaches considering anchor texts are in line with those described by Wang and Kitsuregawa [38] and Huang et al.", "startOffset": 99, "endOffset": 103}, {"referenceID": 37, "context": "[39].", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "This finding is in line with Eiron and McCurley [40] and Noll and Meinel [41], where authors posited that anchor text terms rather resemble terms used in search queries.", "startOffset": 48, "endOffset": 52}, {"referenceID": 39, "context": "This finding is in line with Eiron and McCurley [40] and Noll and Meinel [41], where authors posited that anchor text terms rather resemble terms used in search queries.", "startOffset": 73, "endOffset": 77}, {"referenceID": 40, "context": "In order to automatically adjust the basic parameters of the membership functions, we assume the two base cases that both the words in the documents as well as the emphasized terms will approximate a Zipfian distribution, as defined by Zipf\u2019s law [42].", "startOffset": 247, "endOffset": 251}, {"referenceID": 1, "context": "For term weighing we explored the fuzzy combination of criteria performed by FCC [2] aiming to get the most of the fuzzy system and the heuristics in which it is based.", "startOffset": 81, "endOffset": 84}], "year": 2016, "abstractText": "The selection of a suitable document representation approach plays a crucial role in the performance of a document clustering task. Being able to pick out representative words within a document can lead to substantial improvements in document clustering. In the case of web documents, the HTML markup that defines the layout of the content provides additional structural information that can be further exploited to identify representative words. In this paper we introduce a fuzzy term weighing approach that makes the most of the HTML structure for document clustering. We set forth and build on the hypothesis that a good representation can take advantage of how humans skim through documents to extract the most representative words. The authors of web pages make use of HTML tags to convey the most important message of a web page through page elements that attract the readers\u2019 attention, such as page titles or emphasized elements. We define a set of criteria to exploit the information provided by these page elements, and introduce a fuzzy combination of these criteria that we evaluate within the context of a web page clustering task. Our proposed approach, called Abstract Fuzzy Combination of Criteria (AFCC), can adapt to datasets whose features are distributed differently, achieving good results compared to other similar fuzzy logic based approaches and TF-IDF across different datasets.", "creator": "LaTeX with hyperref package"}}}