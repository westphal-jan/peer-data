{"id": "1606.04597", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora", "abstract": "we introduce it an appropriate agreement - based quantitative approach to learning parallel formal lexicons and noun phrases modeled from non - parallel syntax corpora. the basic idea is to encourage two asymmetric latent - variable translation method models ( i. e., matrix source - target to - target and 2d target - to - reference source ) to agree somewhat on identifying latent phrase positions and word index alignments. effectively the agreement relationship is defined at integrating both complementary word structure and phrase levels. \u2022 we jointly develop a modular viterbi em mapping algorithm paradigm for harmon jointly training teaching the next two dual unidirectional models efficiently. our experiments on the rare chinese - brazilian english dataset sample show concern that agreement - coefficient based learning significantly thus improves both alignment and translation performance.", "histories": [["v1", "Wed, 15 Jun 2016 00:28:51 GMT  (194kb,D)", "http://arxiv.org/abs/1606.04597v1", "Accepted for publication in the Proceedings of ACL 2016"]], "COMMENTS": "Accepted for publication in the Proceedings of ACL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chunyang liu", "yang liu 0005", "maosong sun", "huan-bo luan", "heng yu"], "accepted": true, "id": "1606.04597"}, "pdf": {"name": "1606.04597.pdf", "metadata": {"source": "CRF", "title": "Agreement-based Learning of Parallel Lexicons and Phrases from Non-Parallel Corpora", "authors": ["Chunyang Liu", "Yang Liu", "Huanbo Luan", "Maosong Sun", "Heng Yu"], "emails": ["liuchunyang2012@gmail.com,", "liuyang.china@gmail.com,", "luanhuanbo@gmail.com,", "sms@tsinghua.edu.cn", "h0517.yu@samsung.com"], "sections": [{"heading": "1 Introduction", "text": "Parallel corpora, which are large collections of parallel texts, serve as an important resource for inducing translation correspondences, either at the level of words (Brown et al., 1993; Smadja and McKeown, 1994; Wu and Xia, 1994) or phrases (Kupiec, 1993; Melamed, 1997; Marcu and Wong, 2002; Koehn et al., 2003). However, the availability of large-scale, wide-coverage corpora still remains a challenge even in the era of big data: parallel corpora are usually only existent for resourcerich languages and restricted to limited domains such as government documents and news articles.\nTherefore, intensive attention has been drawn to exploiting non-parallel corpora for acquiring translation correspondences. Most previous efforts have concentrated on learning parallel lexicons from non-parallel corpora, including parallel sentence and lexicon extraction via bootstrapping (Fung and Cheung, 2004), inducing parallel lexicons via canonical correlation analysis (Haghighi\n\u2217Corresponding author: Yang Liu.\net al., 2008), training IBM models on monolingual corpora as decipherment (Ravi and Knight, 2011; Nuhn et al., 2012; Dou et al., 2014), and deriving parallel lexicons from bilingual word embeddings (Vulic\u0301 and Moens, 2013; Mikolov et al., 2013; Vulic\u0301 and Moens, 2015).\nRecently, a number of authors have turned to a more challenging task: learning parallel phrases from non-parallel corpora (Zhang and Zong, 2013; Dong et al., 2015). Zhang and Zong (2013) present a method for retrieving parallel phrases from non-parallel corpora using a seed parallel lexicon. Dong et al. (2015) continue this line of research to further introduce an iterative approach to joint learning of parallel lexicons and phrases. They introduce a corpus-level latentvariable translation model in a non-parallel scenario and develop a training algorithm that alternates between (1) using a parallel lexicon to extract parallel phrases from non-parallel corpora and (2) using the extracted parallel phrases to enlarge the parallel lexicon. They show that starting from a small seed lexicon, their approach is capable of learning both new words and phrases gradually over time.\nHowever, due to the structural divergence between natural languages as well as the presence of noisy data, only using asymmetric translation models might be insufficient to accurately identify parallel lexicons and phrases from non-parallel corpora. Dong et al. (2015) report that the accuracy on Chinese-English dataset is only around 40% after running for 70 iterations. In addition, their approach seems prone to be affected by noisy data in non-parallel corpora as the accuracy drops significantly with the increase of noise.\nSince asymmetric word alignment and phrase alignment models are usually complementary, it is natural to combine them to make more accurate predictions. In this work, we propose to in-\nar X\niv :1\n60 6.\n04 59\n7v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\n16\ntroduce agreement-based learning (Liang et al., 2006; Liang et al., 2008) into extracting parallel lexicons and phrases from non-parallel corpora. Based on the latent-variable model proposed by Dong et al. (2015), we propose two kinds of loss functions to take into account the agreement between both phrase alignment and word alignment in two directions. As the inference is intractable, we resort to a Viterbi EM algorithm to train the two models efficiently. Experiments on the Chinese-English dataset show that agreementbased learning is more robust to noisy data and leads to substantial improvements in phrase alignment and machine translation evaluations."}, {"heading": "2 Background", "text": "Given a monolingual corpus of source language phrases E = {e(s)}Ss=1 and a monolingual corpus of target language phrases F = {f (t)}Tt=1, we assume there exists a parallel corpus D = {\u3008e(s), f (t)\u3009|e(s) \u2194 f (t)}, where e(s) \u2194 f (t) denotes that e(s) and f (t) are translations of each other.\nAs a long sentence in E is usually unlikely to have an translation in F and vise versa, most previous efforts build on the assumption that phrases are more likely to have translational equivalents on the other side (Munteanu and Marcu, 2006; Cettolo et al., 2010; Zhang and Zong, 2013; Dong et al., 2015). Such a set of phrases can be constructed by collecting either constituents of parsed sentences or strings with hyperlinks on webpages (e.g., Wikipedia). Therefore, we assume the two monolingual corpora are readily available and focus on how to extract D from E and F .\nTo address this problem, Dong et al. (2015) introduce a corpus-level latent-variable translation model in a non-parallel scenario:\nP (F |E;\u03b8) = \u2211 m\nP (F,m|E;\u03b8)\ufe38 \ufe37\ufe37 \ufe38 phrase alignment , (1)\nwhere m is phrase alignment and \u03b8 is a set of model parameters. Each target phrase f (t) is restricted to connect to exactly one source phrase: m = (m1, . . . ,mt, . . .mT ), where mt \u2208 {0, 1, . . . , S}. For example, mt = s denotes that f (t) is aligned to e(s). Note that e(0) represents an empty source phrase.\nThey follow IBM Model 1 (Brown et al., 1993)\nto further decompose the model as\nP (F,m|E;\u03b8) = p(T |S) (S + 1)T T\u220f t=1 P (f (t)|e(mt);\u03b8), (2)\nwhere P (f (t)|e(mt);\u03b8) is a phrase translation model that can be further defined as\nP (f (t)|e(mt);\u03b8) = \u03b4(mt, 0) +\n(1\u2212 \u03b4(mt, 0)) \u2211 a\nP (f (t),a|e(mt);\u03b8)\ufe38 \ufe37\ufe37 \ufe38 word alignment . (3)\nDong et al. (2015) distinguish between empty and non-empty phrase translations. If a target phrase f (t) is aligned to the empty source phrase e(0) (i.e., mt = 0), they set the phrase translation probability to a fixed number . Otherwise, conventional word alignment models such as IBM Model 1 can be used for non-empty phrase translation:\nP (f (t),a|e(mt);\u03b8)\n= p(J (t)|I(mt))\n(I(mt) + 1)J (t) J(t)\u220f j=1 p(f (t) j |e (mt) aj ), (4)\nwhere p(J |I) is a length model and p(f |e) is a translation model. We use J (t) to denote the length of f (t).\nTherefore, the latent-variable model involves two kinds of latent structures: (1) phrase alignment m between source and target phrases, (2) word alignment a between source and target words within phrases.\nGiven the two monolingual corpora E and F , the training objective is to maximize the likelihood of the training data:\n\u03b8\u2217 = argmax \u03b8\n{ L(\u03b8) } , (5)\nwhere\nL(\u03b8) = logP (F |E;\u03b8)\u2212\u2211 I \u03bbI (\u2211 J p(J |I)\u2212 1 ) \u2212\n\u2211 e \u03b3e (\u2211 f p(f |e)\u2212 1 ) \u2212\n\u2211 f \u2211 e \u03c3(f, e,d) log \u03c3(f, e,d) p(f |e) .(6)\nNote that d is a small seed parallel lexicon for initializing training 1 and \u03c3(f, e,d) checks whether an entry \u3008f, e\u3009 exists in d.\nGiven the monolingual corpora and the optimized model parameters, the Viterbi phrase alignment is calculated as\nm\u2217 = argmax m\n{ P (F,m|E;\u03b8\u2217) } (7)\n= argmax m { T\u220f t=1 P (f (t)|e(mt);\u03b8\u2217) } .(8)\nFinally, parallel lexicons can be derived from the translation probability table of IBM model 1 \u03b8\u2217 and parallel phrases can be collected from the Viterbi phrase alignment m\u2217. This process iterates and enlarges parallel lexicons and phrases gradually over time.\nAs it is very challenging to extract parallel phrases from non-parallel corpora, unidirectional models might only capture partial aspects of translation modeling on non-parallel corpora. Indeed, Dong et al. (2015) find that the accuracy of phrase alignment is only around 50% on the ChineseEnglish dataset. More importantly, their approach seems to be vulnerable to noise as the accuracy drops significantly with the increase of noise. As source-to-target and target-to-source translation models are usually complementary (Och and Ney, 2003; Koehn et al., 2003; Liang et al., 2006),\n1Due to the difficulty of learning translation correspondences from non-parallel corpora, many authors have assumed that a small seed lexicon is readily available (Gaussier et al., 2004; Zhang and Zong, 2013; Vulic\u0301 and Moens, 2013; Mikolov et al., 2013; Dong et al., 2015).\nit is appealing to combine them to improve alignment accuracy."}, {"heading": "3 Approach", "text": ""}, {"heading": "3.1 Agreement-based Learning", "text": "The basic idea of our work is to encourage the source-to-target and target-to-source translation models to agree on both phrase and word alignments.\nFor example, Figure 1 shows two example Chinese-to-English and English-to-Chinese phrase alignments on the same non-parallel data. As each model only captures partial aspects of translation modeling, our intuition is that the links on which two models agree (highlighted in red) are more likely to be correct.\nMore formally, let P (F |E;\u2212\u2192\u03b8 ) be a sourceto-target translation model and P (E|F ;\u2190\u2212\u03b8 ) be a target-to-source model, where \u2212\u2192 \u03b8 and \u2190\u2212 \u03b8 are corresponding model parameters. We use \u2212\u2192m = (\u2212\u2192m1, . . . ,\u2212\u2192mt, . . . ,\u2212\u2192mT ) to denote sourceto-target phrase alignment. Likewise, the targetto-source phrase alignment is denoted by \u2190\u2212m = (\u2190\u2212m1, . . . ,\u2190\u2212ms, . . . ,\u2190\u2212mS).\nTo ease the comparison between \u2212\u2192m and\u2190\u2212m, we represent them as sets of non-empty links equivalently:\n\u2212\u2192m = { \u3008\u2212\u2192mt, t\u3009|\u2212\u2192mt 6= 0 } (9)\n\u2190\u2212m = { \u3008s,\u2190\u2212ms\u3009|\u2190\u2212ms 6= 0 } . (10)\nFor example, suppose the source-to-target and target-to-source phrase alignments are \u2212\u2192m =\n(2, 3, 0, 0) and \u2190\u2212m = (0, 1, 2). The equivalent link sets are \u2212\u2192m = {\u30082, 1\u3009, \u30083, 2\u3009} and \u2190\u2212m = {\u30082, 1\u3009, \u30083, 2\u3009}. Therefore, \u2212\u2192m is said to be equal to\u2190\u2212m (i.e., \u03b4(\u2212\u2192m,\u2190\u2212m) = 1).\nFollowing Liang et al. (2006), we introduce a new training objective that favors the agreement between two unidirectional models:\nJ (\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 ) = logP (F |E;\u2212\u2192\u03b8 ) + logP (E|F ;\u2190\u2212\u03b8 )\u2212\nlog \u2211 \u2212\u2192m,\u2190\u2212m P (\u2212\u2192m|E,F ;\u2212\u2192\u03b8 )P (\u2190\u2212m|F,E;\u2190\u2212\u03b8 )\n\u00d7\u2206(E,F,\u2212\u2192m,\u2190\u2212m,\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 ), (11)\nwhere the posterior probabilities in two directions are defined as P (\u2212\u2192m|E,F ;\u2212\u2192\u03b8 ) = T\u220f t=1 P (f (t)|e( \u2212\u2192mt);\u2212\u2192\u03b8 )\u2211S s=0 P (f (t)|e(s);\u2212\u2192\u03b8 ) (12)\nP (\u2190\u2212m|F,E;\u2190\u2212\u03b8 ) = S\u220f\ns=1\nP (e(s)|f ( \u2190\u2212ms);\u2190\u2212\u03b8 )\u2211T\nt=0 P (e (s)|f (t);\u2190\u2212\u03b8 )\n. (13)\nThe loss function \u2206(E,F,\u2212\u2192m,\u2190\u2212m,\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 ) measures the disagreement between the two models."}, {"heading": "3.2 Outer Agreement", "text": ""}, {"heading": "3.2.1 Definition", "text": "A straightforward loss function is to force the two models to generate identical phrase alignments:\n\u2206outer(E,F,\u2212\u2192m,\u2190\u2212m, \u2212\u2192 \u03b8 , \u2190\u2212 \u03b8 ) = 1\u2212 \u03b4(\u2212\u2192m,\u2190\u2212m). (14)\nWe refer to Eq. (14) as outer agreement since it only considers phrase alignment and ignores the word alignment within aligned phrases."}, {"heading": "3.2.2 Training Objective", "text": "Since the outer agreement forces two models to generate identical phrase alignments, the training objective can be written as\nJouter( \u2212\u2192 \u03b8 , \u2190\u2212 \u03b8 )\n= logP (F |E;\u2212\u2192\u03b8 ) + logP (E|F ;\u2190\u2212\u03b8 ) + log \u2211 m P (m|E,F ;\u2212\u2192\u03b8 )P (m|F,E;\u2190\u2212\u03b8 ), (15)\nwhere m is a phrase alignment on which two models agree.\nThe partial derivatives of the training objective with respect to source-to-target model parameters\u2212\u2192 \u03b8 are given by\n\u2202Jouter( \u2212\u2192 \u03b8 , \u2190\u2212 \u03b8 )\n\u2202 \u2212\u2192 \u03b8\n= \u2202P (F |E;\u2212\u2192\u03b8 )/\u2202\u2212\u2192\u03b8\nP (F |E;\u2212\u2192\u03b8 ) +\nE m|F,E;\u2190\u2212\u03b8\n[ \u2202P (F |E;\u2212\u2192\u03b8 )/\u2202\u2212\u2192\u03b8 ] \u2211\nm P (m|E,F ; \u2212\u2192 \u03b8 )P (m|F,E;\u2190\u2212\u03b8 )\n. (16)\nThe partial derivatives with respect to \u2190\u2212 \u03b8 are defined likewise."}, {"heading": "3.2.3 Training Algorithm", "text": "As the expectation in Eq. (16) is usually intractable to calculate due to the exponential search space of phrase alignment, we follow Dong et al. (2015) to use a Viterbi EM algorithm instead.\nAs shown in Figure 2, the algorithm takes a set of source phrases E, a set of target phrases F , and a seed parallel lexicon d as input (line 1). After initializing model parameters \u0398 = {\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 } (line 2), the algorithm calls the procedure ALIGN(F,E,\u0398) to compute the Viterbi phrase alignment between E and F on which two models agree. Then, the algorithm updates the two models by normalizing counts collected from the Viterbi phrase alignment. The process iterates for K iterations and returns the final Viterbi phrase alignment and model parameters."}, {"heading": "3.2.4 Computing Viterbi Phrase Alignments", "text": "The procedure ALIGN(F,E,\u0398) computes the Viterbi phrase alignment m\u0302 between E and F on which two models agree as follows:\nm\u0302 = argmax m\n{ P (m|E,F ;\u2212\u2192\u03b8 )P (m|F,E;\u2190\u2212\u03b8 ) } . (17)\nUnfortunately, due to the exponential search space of phrase alignment, computing m\u0302 is also intractable. As a result, we approximate it as the intersection of two unidirectional Viterbi phrase alignments:\nm\u0302 \u2248 \u2212\u2192m\u2217 \u2229\u2190\u2212m\u2217, (18)\nwhere the unidirectional Viterbi phrase alignments are calculated as\n\u2212\u2192m\u2217 = argmax \u2212\u2192m { T\u220f t=1 P (f (t)|e( \u2212\u2192mt);\u2212\u2192\u03b8 ) } (19)\n\u2190\u2212m\u2217 = argmax \u2190\u2212m\n{ S\u220f\ns=1\nP (e(s)|f ( \u2190\u2212ms);\u2190\u2212\u03b8 ) } . (20)\nThe source-to-target Viterbi phrase alignment is calculated as\n\u2212\u2192m\u2217 = argmax \u2212\u2192m\n{ P (\u2212\u2192m|E,F ;\u2212\u2192\u03b8 ) } (21)\n= argmax \u2212\u2192m { T\u220f t=1 P (f (t)|e(\u2212\u2192mt);\u2212\u2192\u03b8 ) } . (22)\nDong et al. (2015) indicate that computing the Viterbi alignment for individual target phrases is independent and only need to focus on finding the most probable source phrase for each target phrase:\n\u2212\u2192m\u2217t = argmax s\u2208{0,1,...,S}\n{ P (f (t)|e(s);\u2212\u2192\u03b8 ) } . (23)\nThis can be cast as a translation retrieval problem (Zhang and Zong, 2013; Dong et al., 2014). Please refer to (Dong et al., 2015) for more details. The target-to-source Viterbi phrase alignment can be calculated similarly."}, {"heading": "3.2.5 Updating Model Parameters", "text": "Following Liang et al. (2006), we collect counts of model parameters only from the agreement term.2\nGiven the agreed Viterbi phrase alignment m\u0302, the count of the source-to-target length model p(J |I) is given by\nc(J |I;E,F ) = \u2211 \u3008s,t\u3009\u2208m\u0302 \u03b4(J (t), J)\u03b4(I(s), I). (24)\nThe new length probabilities can be obtained by\np(J |I) = c(J |I;E,F )\u2211 J \u2032 c(J \u2032|I;E,F ) . (25)\n2We experimented with collecting counts from both the unidirectional and agreement terms but obtained much worse results than counting only from the agreement term.\nThe count of the source-to-target translation model p(f |e) is given by\nc(f |e;E,F ) = \u2211 \u3008s,t\u3009\u2208m\u0302 p(f |e)\u2211I(s) i=0 p(f |e (s) i ) \u00d7\nJ(t)\u2211 j=1 \u03b4(f, f (t) j ) I(s)\u2211 i=0 \u03b4(e, e (s) i )\n+\u03c3(f, e,d). (26)\nThe new translation probabilities can be obtained by\np(f |e) = c(f |e;E,F )\u2211 f \u2032 c(f \u2032|e;E,F ) . (27)\nCounts of target-to-source length and translation models can be calculated in a similar way."}, {"heading": "3.3 Inner Agreement", "text": ""}, {"heading": "3.3.1 Definition", "text": "As the outer agreement only considers the phrase alignment, the inner agreement takes both phrase alignment and word alignment into consideration:\n\u2206inner(E,F, \u2212\u2192m,\u2190\u2212m,\u2212\u2192\u03b8 ,\u2190\u2212\u03b8 )\n= \u2212\u03b4(\u2212\u2192m,\u2190\u2212m)\u00d7\u2211 \u3008s,t\u3009\u2208\u2212\u2192m \u2211 \u2212\u2192a ,\u2190\u2212a P (\u2212\u2192a |e(s), f (t);\u2212\u2192\u03b8 )\u00d7\nP (\u2190\u2212a |f (t), e(s);\u2190\u2212\u03b8 )\u00d7 \u03b4(\u2212\u2192a ,\u2190\u2212a ). (28)\nFor example, Figure 3 shows two examples of Chinese-to-English and English-to-Chinese word alignments. The shared links are highlighted in\nred. Our intuition is that a source phrase and a target phrase are more likely to be translations of each other if the two translation models also agree on word alignment within aligned phrases."}, {"heading": "3.3.2 Training Objective and Algorithm", "text": "The training objective for inner agreement is given by\nJinner( \u2212\u2192 \u03b8 , \u2190\u2212 \u03b8 )\n= logP (F |E;\u2212\u2192\u03b8 ) + logP (E|F ;\u2190\u2212\u03b8 ) + log \u2211 m\nP (m|E,F ;\u2212\u2192\u03b8 )P (m|F,E;\u2190\u2212\u03b8 )\u00d7\u2211 \u3008s,t\u3009\u2208m \u2211 a P (a|e(s), f (t);\u2212\u2192\u03b8 )\u00d7\nP (a|f (t), e(s);\u2190\u2212\u03b8 ). (29)\nWe still use the Viterbi EM algorithm as shown in Figure 2 for training the two models."}, {"heading": "3.3.3 Computing Viterbi Phrase Alignments", "text": "The agreed Viterbi phrase alignment is defined as\nm\u0302 = argmax m\n{ P (m|E,F ;\u2212\u2192\u03b8 )P (m|F,E;\u2190\u2212\u03b8 )\n\u00d7 \u2211 \u3008s,t\u3009\u2208m \u2211 a P (a|e(s), f (t);\u2212\u2192\u03b8 )\n\u00d7P (a|f (t), e(s);\u2190\u2212\u03b8 ) } . (30)\nAs computing m\u0302 is intractable, we still approximate it using the intersection of two unidirectional Viterbi phrase alignments (see Eq. (18)). The source-to-target Viterbi phrase alignment is calculated as\n\u2212\u2192m\u2217 = argmax \u2212\u2192m\n{ P (\u2212\u2192m|E,F ;\u2212\u2192\u03b8 )\u00d7\n\u2211 \u3008s,t\u3009\u2208\u2212\u2192m J(t)\u2211 j=1 I(s)\u2211 i=1 P (\u3008i, j\u3009|e(s), f (t);\u2212\u2192\u03b8 )\u00d7\nP (\u3008i, j\u3009|f (t), e(s);\u2190\u2212\u03b8 ) } , (31)\nwhere P (\u3008i, j\u3009|e(s), f (t);\u2212\u2192\u03b8 ) is source-to-target link posterior probability of the link \u3008i, j\u3009 being present (or absent) in the word alignment according to the source-to-target model, P (\u3008i, j\u3009|f (t), e(s);\u2190\u2212\u03b8 ) is target-to-source link posterior probability. We follow Liang et al. (2006) to use the product of link posteriors to encourage the agreement at the level of word alignment.\nWe use a coarse-to-fine approach (Dong et al., 2015) to compute the Viterbi alignment: first retrieving a coarse set of candidate source phrases using translation probabilities and then selecting the candidate with the highest score according to Eq. (31). The target-to-source Viterbi phrase alignment can be calculated similarly."}, {"heading": "3.3.4 Updating Model Parameters", "text": "Given the agreed Viterbi phrase alignment m\u0302, the count of the source-to-target length model p(J |I) is still given by Eq. (24). The count of the translation model p(f |e) is calculated as\nc(f |e;E,F )\n= \u2211 \u3008s,t\u3009\u2208m\u0302 I(s)\u2211 i=1 J(t)\u2211 j=1 P (\u3008i, j\u3009|e(s), f (t);\u2212\u2192\u03b8 )\u00d7\nP (\u3008i, j\u3009|f (t), e(s);\u2190\u2212\u03b8 )\u00d7 \u03b4(f, f (t))\u03b4(e, e(s))\n+\u03c3(f, e,d). (32)\nCounts of target-to-source length and translation models can be calculated in a similar way."}, {"heading": "4 Experiments", "text": "In this section, we evaluate our approach in two tasks: phrase alignment (Section 4.1) and machine translation (Section 4.2)."}, {"heading": "4.1 Alignment Evaluation", "text": ""}, {"heading": "4.1.1 Evaluation Metrics", "text": "Given two monolingual corpora E and F , we suppose there exists a ground truth parallel corpus G and denote an extracted parallel corpus as D. The quality of an extracted parallel corpus can be measured by F1 = 2|D \u2229G|/(|D|+ |G|)."}, {"heading": "4.1.2 Data Preparation", "text": "Although it is appealing to apply our approach to dealing with real-world non-parallel corpora, it is time-consuming and labor-intensive to manually construct a ground truth parallel corpus. Therefore, we follow Dong et al. (2015) to build synthetic E, F , and G to facilitate the evaluation.\nWe first extract a set of parallel phrases from a sentence-level parallel corpus using the stateof-the-art phrase-based translation system Moses (Koehn et al., 2007) and discard low-probability parallel phrases. Then, E and F can be constructed by corrupting the parallel phrase set by\nadding irrelevant source and target phrases randomly. Note that the parallel phrase set can serve as the ground truth parallel corpus G. We refer to the non-parallel phrases in E and F as noise.\nFrom LDC Chinese-English parallel corpora, we constructed a development set and a test set. The development set contains 20K parallel phrases, 20K noisy Chinese phrases, and 20K noisy English phrases. The test test contains 20K parallel phrases, 180K noisy Chinese phrases, and 180K noisy English phrases. The seed parallel lexicon contains 1K entries."}, {"heading": "4.1.3 Comparison of Agreement Ratios", "text": "We introduce agreement ratio to measure to what extent two unidirectional models agree on phrase alignment:\nratio = 2|\u2212\u2192m\u2217 \u2229\u2190\u2212m\u2217| |\u2212\u2192m\u2217|+ |\u2190\u2212m\u2217| . (33)\nFigure 4 shows the agreement ratios of independent training (\u201cno agreement\u201d), joint training with the outer agreement (\u201couter\u201d), and joint training with the inner agreement (\u201cinner\u201d). We find that independently trained unidirectional models\nhardly agree on phrase alignment, suggesting that each model can only capture partial aspects of translation modeling on non-parallel corpora. In contrast, imposing the agreement term significantly increases the agreement ratios: after 10 iterations, about 40% of phrase alignment links are shared by two models."}, {"heading": "4.1.4 Effect of Seed Lexicon Size", "text": "Table 1 shows the F1 scores of the Chinese-toEnglish model (\u201cC\u2192 E\u201d), the English-to-Chinese model (\u201cE\u2192C\u201d), joint learning based on the outer agreement (\u201couter\u201d), and jointing learning based on the inner agreement (\u201cinner\u201d) over various sizes of seed lexicons on the development set.\nWe find that agreement-based learning obtains substantial improvements over independent learning across all sizes. More importantly, even with a seed lexicon containing only 50 entries, agreement-based learning is able to achieve F1 scores above 60%. The inner agreement performs better than the outer agreement by taking the consensus at the word level into account."}, {"heading": "4.1.5 Effect of Noise", "text": "Table 2 demonstrates the effect of noise on the development set. In row 1, \u201c0+0\u201d denotes there is no noise, which can be seen as an upper bound. Adding noise, either on the Chinese side or on the English side, deteriorates the F1 scores for all methods. Adding noise on the English side makes predicting phrase alignment in the C \u2192 E direction more challenging due to the enlarged search space. The situation is similar in the reverse direction. It is clear that agreement-based learning is more robust to noise: while independent training suffers from a reduction of 40% in terms of F1 for the \u201c20K + 20K\u201d setting, agreement-based learning still achieves F1 scores over 70%."}, {"heading": "4.1.6 Results", "text": "Figure 5 gives the final results on the test set. We find that agreement-based training achieves significant improvements over independent training. By considering the consensus on both phrase and word alignments, the inner agreement significantly outperforms the outer agreement. Notice that Dong et al. (2015) only add noise on one side while we add noisy phrases on both sides, which makes phrase alignment more challenging.\nTable 3 shows example learned parallel words and phrases. The lexicon is built from the translation table by retaining high-probability word pairs. Therefore, our approach is capable of learning both new words and new phrases unseen in the seed lexicon."}, {"heading": "4.2 Translation Evaluation", "text": "Following Zhang and Zong (2013) and Dong et al. (2015), we evaluate our approach on domain\nadaptation for machine translation. The data set consists of two in-domain nonparallel corpora and an out-domain parallel corpus. The in-domain non-parallel corpora consists of 2.65M Chinese phrases and 3.67M English phrases extracted from LDC news articles. We use a small out-domain parallel corpus extracted from financial news of FTChina which contains 10K phrase pairs. The task is to extract a parallel corpus from in-domain non-parallel corpora starting from a small out-domain parallel corpus.\nWe use the state-of-the-art translation system Moses (Koehn et al., 2007) and evaluate the performance on Chinese-English NIST datasets. The development set is NIST 2006 and the test set is NIST 2005. The evaluation metric is caseinsensitive BLEU4 (Papineni et al., 2002). We use the SRILM toolkit (Stolcke, 2002) to train a 4-gram English language model on a monolingual corpus with 399M English words.\nTable 4 shows the results. At iteration 0, only the out-domain corpus is used and the BLEU score is 5.61. All methods iteratively extract parallel phrases from non-parallel corpora and enlarge the extracted parallel corpus. We find that agreementbased learning achieves much higher BLEU scores while obtains a smaller parallel corpus as compared with independent learning. One possible reason is that the agreement-based learning rules out most unlikely phrase pairs by encouraging consensus between two models."}, {"heading": "5 Conclusion", "text": "We have presented agreement-based training for learning parallel lexicons and phrases from nonparallel corpora. By modeling the agreement on both phrase alignment and word alignment, our approach achieves significant improvements in both alignment and translation evaluations.\nIn the future, we plan to apply our approach to real-world non-parallel corpora to further verify its effectiveness. It is also interesting to extend the phrase translation model to more sophisticated models such as IBM models 2-5 (Brown et al., 1993) and HMM (Vogel and Ney, 1996)."}, {"heading": "Acknowledgments", "text": "We sincerely thank the reviewers for their valuable suggestions. We also thank Meng Zhang, Yankai Lin, Shiqi Shen and Meiping Dong for their insightful discussions. Yang Liu is sup-\nported by the National Natural Science Foundation of China (No. 61522204), the 863 Program (2015AA011808), and Samsung R&D Institute of China. Huanbo Luan is supported by the National Natural Science Foundation of China (No. 61303075). Maosong Sun is supported by the Major Project of the National Social Science Foundation of China (13&ZD190)."}], "references": [{"title": "The mathematics of statistical machine translation: Parameter estimation. Computational Linguisitics", "author": ["Brown et al.1993] Peter F. Brown", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Mining parallel fragments from comparable texts", "author": ["Marcello Federico", "Nicola Bertoldi"], "venue": "In Proceedings of IWSLT", "citeRegEx": "Cettolo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2010}, {"title": "Query lattice for translation retrieval", "author": ["Dong et al.2014] Meiping Dong", "Yong Cheng", "Yang Liu", "Jia Xu", "Maosong Sun", "Tatsuya Izuha", "Jie Hao"], "venue": "Proceedings of COLING", "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Iterative learning of parallel lexicons and phrases from non-parallel corpora", "author": ["Dong et al.2015] Meiping Dong", "Yang Liu", "Huanbo Luan", "Maosong Sun", "Tatsuya Izuha", "Dakun Zhang"], "venue": "Proceedings of IJCAI", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Beyond parallel data: Joint word alignment and decipherment improves machine translation", "author": ["Dou et al.2014] Qing Dou", "Ashish Vaswani", "Kevin Knight"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Dou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dou et al\\.", "year": 2014}, {"title": "Mining very-non-parallel corpora: Parallel sentence and lexicon extraction via bootstrapping and em", "author": ["Fung", "Cheung2004] Pascale Fung", "Percy Cheung"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Fung et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fung et al\\.", "year": 2004}, {"title": "A geometric view on bilingual lexicon extraction from comparable corpora", "author": ["J.M. Renders", "I. Matveeva", "C. Goutte", "H. Dejean"], "venue": "Proceedings of ACL", "citeRegEx": "Gaussier et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gaussier et al\\.", "year": 2004}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["Percy Liang", "Taylor Berg-Kirkpatrick", "Dan Klein"], "venue": "In Proceedings of ACL", "citeRegEx": "Haghighi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Haghighi et al\\.", "year": 2008}, {"title": "Statistical phrase-based translation", "author": ["Koehn et al.2003] Philipp Koehn", "Franz J. Och", "Daniel Marcu"], "venue": "In Proceedings of NAACL", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Herbst."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions.", "citeRegEx": "Herbst.,? 2007", "shortCiteRegEx": "Herbst.", "year": 2007}, {"title": "An algorithm for finding noun phrase correspondences in bilingual corpora", "author": ["Julian Kupiec"], "venue": "In Proceedings of ACL", "citeRegEx": "Kupiec.,? \\Q1993\\E", "shortCiteRegEx": "Kupiec.", "year": 1993}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] Percy Liang", "Ben Taskar", "Dan Klein"], "venue": "In Proceedings of NAACL", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Alignment-based learning", "author": ["Liang et al.2008] Percy Liang", "Dan Klein", "I. Jordan", "Michael"], "venue": "In Proceedings of NIPS", "citeRegEx": "Liang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2008}, {"title": "A phrase-based joint probability model for statistical machine translation", "author": ["Marcu", "Wong2002] Daniel Marcu", "Daniel Wong"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Marcu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Marcu et al\\.", "year": 2002}, {"title": "Automatic discovery of non-compositional compounds in parallel data", "author": ["I. Dan Melamed"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Melamed.,? \\Q1997\\E", "shortCiteRegEx": "Melamed.", "year": 1997}, {"title": "Exploiting similarities among languages for machine translation", "author": ["Quoc V Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Extracting parallel subsentential fragments from non-parallel corpora", "author": ["Munteanu", "Daniel Marcu"], "venue": "In Proceedings of ACL", "citeRegEx": "Munteanu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Munteanu et al\\.", "year": 2006}, {"title": "Deciphering foreign language by combining language models and context vectors", "author": ["Nuhn et al.2012] Malte Nuhn", "Arne Mauser", "Hermann Ney"], "venue": "In Proceedings of ACL", "citeRegEx": "Nuhn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nuhn et al\\.", "year": 2012}, {"title": "A systematic comparison of various statistical alignment models. Computational Linguisitics", "author": ["Och", "Ney2003] Franz J. Och", "Hermann Ney"], "venue": null, "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Bleu: a methof for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of ACL", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Deciphering foreign language", "author": ["Ravi", "Knight2011] Sujith Ravi", "Kevin Knight"], "venue": "In Proceedings of ACL", "citeRegEx": "Ravi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2011}, {"title": "Translating collocations for use in bilingual lexicons", "author": ["Smadja", "McKeown1994] Frank Smadja", "Kathleen McKeown"], "venue": "In Proceedings of the ARPA Human Language Technology Workshop", "citeRegEx": "Smadja et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Smadja et al\\.", "year": 1994}, {"title": "Srilm - an extensible language modeling toolkit", "author": ["Andreas Stolcke"], "venue": "In Proceedings of ICSLP", "citeRegEx": "Stolcke.,? \\Q2002\\E", "shortCiteRegEx": "Stolcke.", "year": 2002}, {"title": "Hhm-based word alignment in statistical translation", "author": ["Vogel", "Ney1996] Stephan Vogel", "Hermann Ney"], "venue": "In Proceedings of COLING", "citeRegEx": "Vogel et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 1996}, {"title": "A study on bootstrapping bilingual vector spaces from non-parallel data (and nothing else)", "author": ["Vuli\u0107", "Moens2013] Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2013}, {"title": "Bilingual word embeddings from non-parallel document-aligned data applied to bilingual lexicon induction", "author": ["Vuli\u0107", "Moens2015] Ivan Vuli\u0107", "Marie-Francine Moens"], "venue": "In Proceedings of ACL", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2015}, {"title": "Learning an english-chinese lexicon from a parallel corpus", "author": ["Wu", "Xia1994] Dekai Wu", "Xuanyin Xia"], "venue": "In Proceedings of the ARPA Human Language Technology Workshop", "citeRegEx": "Wu et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Wu et al\\.", "year": 1994}, {"title": "Learning a phrase-based translation model from monolingual data with application to domain adaptation", "author": ["Zhang", "Zong2013] Jiajun Zhang", "Chengqing Zong"], "venue": "In Proceedings of ACL", "citeRegEx": "Zhang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Parallel corpora, which are large collections of parallel texts, serve as an important resource for inducing translation correspondences, either at the level of words (Brown et al., 1993; Smadja and McKeown, 1994; Wu and Xia, 1994) or phrases (Kupiec, 1993; Melamed, 1997; Marcu and Wong, 2002; Koehn et al.", "startOffset": 167, "endOffset": 231}, {"referenceID": 10, "context": ", 1993; Smadja and McKeown, 1994; Wu and Xia, 1994) or phrases (Kupiec, 1993; Melamed, 1997; Marcu and Wong, 2002; Koehn et al., 2003).", "startOffset": 63, "endOffset": 134}, {"referenceID": 14, "context": ", 1993; Smadja and McKeown, 1994; Wu and Xia, 1994) or phrases (Kupiec, 1993; Melamed, 1997; Marcu and Wong, 2002; Koehn et al., 2003).", "startOffset": 63, "endOffset": 134}, {"referenceID": 8, "context": ", 1993; Smadja and McKeown, 1994; Wu and Xia, 1994) or phrases (Kupiec, 1993; Melamed, 1997; Marcu and Wong, 2002; Koehn et al., 2003).", "startOffset": 63, "endOffset": 134}, {"referenceID": 17, "context": ", 2008), training IBM models on monolingual corpora as decipherment (Ravi and Knight, 2011; Nuhn et al., 2012; Dou et al., 2014), and deriving parallel lexicons from bilingual word embeddings (Vuli\u0107 and Moens, 2013; Mikolov et al.", "startOffset": 68, "endOffset": 128}, {"referenceID": 4, "context": ", 2008), training IBM models on monolingual corpora as decipherment (Ravi and Knight, 2011; Nuhn et al., 2012; Dou et al., 2014), and deriving parallel lexicons from bilingual word embeddings (Vuli\u0107 and Moens, 2013; Mikolov et al.", "startOffset": 68, "endOffset": 128}, {"referenceID": 15, "context": ", 2014), and deriving parallel lexicons from bilingual word embeddings (Vuli\u0107 and Moens, 2013; Mikolov et al., 2013; Vuli\u0107 and Moens, 2015).", "startOffset": 71, "endOffset": 139}, {"referenceID": 3, "context": "Recently, a number of authors have turned to a more challenging task: learning parallel phrases from non-parallel corpora (Zhang and Zong, 2013; Dong et al., 2015).", "startOffset": 122, "endOffset": 163}, {"referenceID": 0, "context": "Parallel corpora, which are large collections of parallel texts, serve as an important resource for inducing translation correspondences, either at the level of words (Brown et al., 1993; Smadja and McKeown, 1994; Wu and Xia, 1994) or phrases (Kupiec, 1993; Melamed, 1997; Marcu and Wong, 2002; Koehn et al., 2003). However, the availability of large-scale, wide-coverage corpora still remains a challenge even in the era of big data: parallel corpora are usually only existent for resourcerich languages and restricted to limited domains such as government documents and news articles. Therefore, intensive attention has been drawn to exploiting non-parallel corpora for acquiring translation correspondences. Most previous efforts have concentrated on learning parallel lexicons from non-parallel corpora, including parallel sentence and lexicon extraction via bootstrapping (Fung and Cheung, 2004), inducing parallel lexicons via canonical correlation analysis (Haghighi \u2217Corresponding author: Yang Liu. et al., 2008), training IBM models on monolingual corpora as decipherment (Ravi and Knight, 2011; Nuhn et al., 2012; Dou et al., 2014), and deriving parallel lexicons from bilingual word embeddings (Vuli\u0107 and Moens, 2013; Mikolov et al., 2013; Vuli\u0107 and Moens, 2015). Recently, a number of authors have turned to a more challenging task: learning parallel phrases from non-parallel corpora (Zhang and Zong, 2013; Dong et al., 2015). Zhang and Zong (2013) present a method for retrieving parallel phrases from non-parallel corpora using a seed parallel lexicon.", "startOffset": 168, "endOffset": 1462}, {"referenceID": 0, "context": "Parallel corpora, which are large collections of parallel texts, serve as an important resource for inducing translation correspondences, either at the level of words (Brown et al., 1993; Smadja and McKeown, 1994; Wu and Xia, 1994) or phrases (Kupiec, 1993; Melamed, 1997; Marcu and Wong, 2002; Koehn et al., 2003). However, the availability of large-scale, wide-coverage corpora still remains a challenge even in the era of big data: parallel corpora are usually only existent for resourcerich languages and restricted to limited domains such as government documents and news articles. Therefore, intensive attention has been drawn to exploiting non-parallel corpora for acquiring translation correspondences. Most previous efforts have concentrated on learning parallel lexicons from non-parallel corpora, including parallel sentence and lexicon extraction via bootstrapping (Fung and Cheung, 2004), inducing parallel lexicons via canonical correlation analysis (Haghighi \u2217Corresponding author: Yang Liu. et al., 2008), training IBM models on monolingual corpora as decipherment (Ravi and Knight, 2011; Nuhn et al., 2012; Dou et al., 2014), and deriving parallel lexicons from bilingual word embeddings (Vuli\u0107 and Moens, 2013; Mikolov et al., 2013; Vuli\u0107 and Moens, 2015). Recently, a number of authors have turned to a more challenging task: learning parallel phrases from non-parallel corpora (Zhang and Zong, 2013; Dong et al., 2015). Zhang and Zong (2013) present a method for retrieving parallel phrases from non-parallel corpora using a seed parallel lexicon. Dong et al. (2015) continue this line of research to further introduce an iterative approach to joint learning of parallel lexicons and phrases.", "startOffset": 168, "endOffset": 1587}, {"referenceID": 0, "context": "Parallel corpora, which are large collections of parallel texts, serve as an important resource for inducing translation correspondences, either at the level of words (Brown et al., 1993; Smadja and McKeown, 1994; Wu and Xia, 1994) or phrases (Kupiec, 1993; Melamed, 1997; Marcu and Wong, 2002; Koehn et al., 2003). However, the availability of large-scale, wide-coverage corpora still remains a challenge even in the era of big data: parallel corpora are usually only existent for resourcerich languages and restricted to limited domains such as government documents and news articles. Therefore, intensive attention has been drawn to exploiting non-parallel corpora for acquiring translation correspondences. Most previous efforts have concentrated on learning parallel lexicons from non-parallel corpora, including parallel sentence and lexicon extraction via bootstrapping (Fung and Cheung, 2004), inducing parallel lexicons via canonical correlation analysis (Haghighi \u2217Corresponding author: Yang Liu. et al., 2008), training IBM models on monolingual corpora as decipherment (Ravi and Knight, 2011; Nuhn et al., 2012; Dou et al., 2014), and deriving parallel lexicons from bilingual word embeddings (Vuli\u0107 and Moens, 2013; Mikolov et al., 2013; Vuli\u0107 and Moens, 2015). Recently, a number of authors have turned to a more challenging task: learning parallel phrases from non-parallel corpora (Zhang and Zong, 2013; Dong et al., 2015). Zhang and Zong (2013) present a method for retrieving parallel phrases from non-parallel corpora using a seed parallel lexicon. Dong et al. (2015) continue this line of research to further introduce an iterative approach to joint learning of parallel lexicons and phrases. They introduce a corpus-level latentvariable translation model in a non-parallel scenario and develop a training algorithm that alternates between (1) using a parallel lexicon to extract parallel phrases from non-parallel corpora and (2) using the extracted parallel phrases to enlarge the parallel lexicon. They show that starting from a small seed lexicon, their approach is capable of learning both new words and phrases gradually over time. However, due to the structural divergence between natural languages as well as the presence of noisy data, only using asymmetric translation models might be insufficient to accurately identify parallel lexicons and phrases from non-parallel corpora. Dong et al. (2015) report that the accuracy on Chinese-English dataset is only around 40% after running for 70 iterations.", "startOffset": 168, "endOffset": 2427}, {"referenceID": 11, "context": "troduce agreement-based learning (Liang et al., 2006; Liang et al., 2008) into extracting parallel lexicons and phrases from non-parallel corpora.", "startOffset": 33, "endOffset": 73}, {"referenceID": 12, "context": "troduce agreement-based learning (Liang et al., 2006; Liang et al., 2008) into extracting parallel lexicons and phrases from non-parallel corpora.", "startOffset": 33, "endOffset": 73}, {"referenceID": 2, "context": "Based on the latent-variable model proposed by Dong et al. (2015), we propose two kinds of loss functions to take into account the agreement between both phrase alignment and word alignment in two directions.", "startOffset": 47, "endOffset": 66}, {"referenceID": 1, "context": "As a long sentence in E is usually unlikely to have an translation in F and vise versa, most previous efforts build on the assumption that phrases are more likely to have translational equivalents on the other side (Munteanu and Marcu, 2006; Cettolo et al., 2010; Zhang and Zong, 2013; Dong et al., 2015).", "startOffset": 215, "endOffset": 304}, {"referenceID": 3, "context": "As a long sentence in E is usually unlikely to have an translation in F and vise versa, most previous efforts build on the assumption that phrases are more likely to have translational equivalents on the other side (Munteanu and Marcu, 2006; Cettolo et al., 2010; Zhang and Zong, 2013; Dong et al., 2015).", "startOffset": 215, "endOffset": 304}, {"referenceID": 1, "context": "As a long sentence in E is usually unlikely to have an translation in F and vise versa, most previous efforts build on the assumption that phrases are more likely to have translational equivalents on the other side (Munteanu and Marcu, 2006; Cettolo et al., 2010; Zhang and Zong, 2013; Dong et al., 2015). Such a set of phrases can be constructed by collecting either constituents of parsed sentences or strings with hyperlinks on webpages (e.g., Wikipedia). Therefore, we assume the two monolingual corpora are readily available and focus on how to extract D from E and F . To address this problem, Dong et al. (2015) introduce a corpus-level latent-variable translation model in a non-parallel scenario:", "startOffset": 242, "endOffset": 619}, {"referenceID": 0, "context": "They follow IBM Model 1 (Brown et al., 1993) to further decompose the model as", "startOffset": 24, "endOffset": 44}, {"referenceID": 8, "context": "As source-to-target and target-to-source translation models are usually complementary (Och and Ney, 2003; Koehn et al., 2003; Liang et al., 2006), Due to the difficulty of learning translation correspondences from non-parallel corpora, many authors have assumed that a small seed lexicon is readily available (Gaussier et al.", "startOffset": 86, "endOffset": 145}, {"referenceID": 11, "context": "As source-to-target and target-to-source translation models are usually complementary (Och and Ney, 2003; Koehn et al., 2003; Liang et al., 2006), Due to the difficulty of learning translation correspondences from non-parallel corpora, many authors have assumed that a small seed lexicon is readily available (Gaussier et al.", "startOffset": 86, "endOffset": 145}, {"referenceID": 6, "context": ", 2006), Due to the difficulty of learning translation correspondences from non-parallel corpora, many authors have assumed that a small seed lexicon is readily available (Gaussier et al., 2004; Zhang and Zong, 2013; Vuli\u0107 and Moens, 2013; Mikolov et al., 2013; Dong et al., 2015).", "startOffset": 171, "endOffset": 280}, {"referenceID": 15, "context": ", 2006), Due to the difficulty of learning translation correspondences from non-parallel corpora, many authors have assumed that a small seed lexicon is readily available (Gaussier et al., 2004; Zhang and Zong, 2013; Vuli\u0107 and Moens, 2013; Mikolov et al., 2013; Dong et al., 2015).", "startOffset": 171, "endOffset": 280}, {"referenceID": 3, "context": ", 2006), Due to the difficulty of learning translation correspondences from non-parallel corpora, many authors have assumed that a small seed lexicon is readily available (Gaussier et al., 2004; Zhang and Zong, 2013; Vuli\u0107 and Moens, 2013; Mikolov et al., 2013; Dong et al., 2015).", "startOffset": 171, "endOffset": 280}, {"referenceID": 2, "context": "Indeed, Dong et al. (2015) find that the accuracy of phrase alignment is only around 50% on the ChineseEnglish dataset.", "startOffset": 8, "endOffset": 27}, {"referenceID": 11, "context": "Following Liang et al. (2006), we introduce a new training objective that favors the agreement between two unidirectional models: J (\u2212 \u2192\u03b8 ,\u2190\u2212 \u03b8 ) = logP (F |E;\u2212 \u2192\u03b8 ) + logP (E|F ;\u2190\u2212 \u03b8 )\u2212", "startOffset": 10, "endOffset": 30}, {"referenceID": 2, "context": "(16) is usually intractable to calculate due to the exponential search space of phrase alignment, we follow Dong et al. (2015) to use a Viterbi EM algorithm instead.", "startOffset": 108, "endOffset": 127}, {"referenceID": 2, "context": "This can be cast as a translation retrieval problem (Zhang and Zong, 2013; Dong et al., 2014).", "startOffset": 52, "endOffset": 93}, {"referenceID": 3, "context": "Please refer to (Dong et al., 2015) for more details.", "startOffset": 16, "endOffset": 35}, {"referenceID": 2, "context": "This can be cast as a translation retrieval problem (Zhang and Zong, 2013; Dong et al., 2014). Please refer to (Dong et al., 2015) for more details. The target-to-source Viterbi phrase alignment can be calculated similarly. 3.2.5 Updating Model Parameters Following Liang et al. (2006), we collect counts of model parameters only from the agreement term.", "startOffset": 75, "endOffset": 286}, {"referenceID": 3, "context": "We use a coarse-to-fine approach (Dong et al., 2015) to compute the Viterbi alignment: first retrieving a coarse set of candidate source phrases using translation probabilities and then selecting the candidate with the highest score according to Eq.", "startOffset": 33, "endOffset": 52}, {"referenceID": 9, "context": "We follow Liang et al. (2006) to use the product of link posteriors to encourage the agreement at the level of word alignment.", "startOffset": 10, "endOffset": 30}, {"referenceID": 2, "context": "Therefore, we follow Dong et al. (2015) to build synthetic E, F , and G to facilitate the evaluation.", "startOffset": 21, "endOffset": 40}, {"referenceID": 2, "context": "Notice that Dong et al. (2015) only add noise on one side while we add noisy phrases on both sides, which makes phrase alignment more challenging.", "startOffset": 12, "endOffset": 31}, {"referenceID": 19, "context": "The evaluation metric is caseinsensitive BLEU4 (Papineni et al., 2002).", "startOffset": 47, "endOffset": 70}, {"referenceID": 22, "context": "We use the SRILM toolkit (Stolcke, 2002) to train a 4-gram English language model on a monolingual corpus with 399M English words.", "startOffset": 25, "endOffset": 40}, {"referenceID": 2, "context": "2 Translation Evaluation Following Zhang and Zong (2013) and Dong et al. (2015), we evaluate our approach on domain adaptation for machine translation.", "startOffset": 61, "endOffset": 80}, {"referenceID": 0, "context": "It is also interesting to extend the phrase translation model to more sophisticated models such as IBM models 2-5 (Brown et al., 1993) and HMM (Vogel and Ney, 1996).", "startOffset": 114, "endOffset": 134}], "year": 2016, "abstractText": "We introduce an agreement-based approach to learning parallel lexicons and phrases from non-parallel corpora. The basic idea is to encourage two asymmetric latent-variable translation models (i.e., source-to-target and target-to-source) to agree on identifying latent phrase and word alignments. The agreement is defined at both word and phrase levels. We develop a Viterbi EM algorithm for jointly training the two unidirectional models efficiently. Experiments on the ChineseEnglish dataset show that agreementbased learning significantly improves both alignment and translation performance.", "creator": "LaTeX with hyperref package"}}}