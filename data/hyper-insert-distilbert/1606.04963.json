{"id": "1606.04963", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2016", "title": "The Edit Distance Transducer in Action: The University of Cambridge English-German System at WMT16", "abstract": "accompanying this paper thoroughly presents the university of cambridge submission attempts to solve wmt16. motivated by the inherent complementary nature limitations of syntactical networks machine translation techniques and cellular neural machine translation ( ft nmt ), we exploit the synergies of semantic hiero databases and protein nmt in different generalized combination schemes. increasingly starting out effectively with identifying a typical simple neural lattice hierarchical rescoring approach, we gradually show significantly that the constraint hiero lattices are often necessarily too narrow for nmt ensembles. therefore, - instead of generating a hard restriction of crossing the nmt search space down to the lattice, we propose to first loosely couple together nmt and hiero by nonlinear composition operations with a weakly modified version of computing the sequential edit distance shift transducer. then the loose combination outperforms lattice rescoring, still especially problematic when successfully using the multiple adaptive nmt systems in an nm ensemble.", "histories": [["v1", "Wed, 15 Jun 2016 20:08:01 GMT  (420kb,D)", "http://arxiv.org/abs/1606.04963v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["felix stahlberg", "eva hasler", "bill byrne"], "accepted": false, "id": "1606.04963"}, "pdf": {"name": "1606.04963.pdf", "metadata": {"source": "CRF", "title": "The Edit Distance Transducer in Action: The University of Cambridge English-German System at WMT16", "authors": ["Felix Stahlberg"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Previous work suggests that syntactic machine translation such as Hiero (Chiang, 2007) and Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) are very different and have complementary strengths and weaknesses (Neubig et al., 2015; Stahlberg et al., 2016). Recent attempts to combine syntactic SMT and NMT report large gains over both baselines. Authors in (Neubig et al., 2015) used NMT to rescore n-best lists which were generated with a syntax-based system. They report that even with 1000-best lists, the gains of using the NMT rescorer often do not saturate. Syntactically Guided NMT (Stahlberg et al., 2016, SGNMT) constrains the NMT search space to Hiero translation lattices which contain significantly more hypotheses than n-best lists. In SGNMT, an NMT\nbeam decoder with a relatively small beam can explore spaces much larger than n-best lists, yielding BLEU score improvements with far fewer expensive NMT evaluations.\nHowever, these rescoring approaches enforce an exact match between the NMT and syntactic decoders. In general, this kind of hard restriction is best avoided when combining diverse systems (Liu et al., 2009; Frederking et al., 1994). For example, in speech recognition, ROVER (Fiscus, 1997) is a system combination approach based on a soft voting scheme. In machine translation, minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) can be used to combine multiple systems (de Gispert et al., 2009). MBR also does not enforce exact agreement between systems as it distinguishes between the hypothesis space and the evidence space (Goel and Byrne, 2000; Tromble et al., 2008).\nWe find that Hiero lattices generated by grammars extracted with the usual heuristics (Chiang, 2007) do not provide enough variety to explore the full potential of neural models, especially when using NMT ensembles. Therefore, we present a \u201csoft\u201d lattice-based combination scheme which uses standard operations on finite state transducers such as composition. Our method replaces the hard combination in previous methods with a similarity measure based on the edit distance, and gives the NMT decoder more freedom to diverge from the Hiero translations. We find that this loose coupling scheme is especially useful when using NMT ensembles."}, {"heading": "2 Combining Hiero and NMT via Edit Distance Transducer", "text": "In contrast to the strict coupling in SGNMT, we propose to loosely couple Hiero and NMT via an edit distance transducer and shortest distance\nar X\niv :1\n60 6.\n04 96\n3v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\n16\nsearch. With loose coupling, the NMT decoder is not restricted to the Hiero lattice as in previous work, but runs independently to produce translation lattices on its own, which are then combined with the Hiero lattices. The combination does not require an exact match. Instead, we will describe a procedure for combining NMT and Hiero that captures similarity under the edit distance and both the NMT and Hiero translation system scores. This scheme is implemented efficiently using standard FST operations (Allauzen et al., 2007). First, we introduce the FST composition operation and the edit distance transducer. We will describe the whole pipeline in Sec. 2.3."}, {"heading": "2.1 Composition of Finite State Transducers", "text": "The composition of two weighted transducers T1, T2 (denoted as T1 \u25e6 T2) over a semiring (K,\u2295,\u2297) is defined following (Mohri, 2004)\n[T1 \u25e6T2](x, y) = \u2295 z T1(x, z)\u2297T2(z, y). (1)\nWe will make extensive use of this operation as tool for building complex automata which make use of both the NMT and Hiero translation lattices."}, {"heading": "2.2 The Edit Distance Transducer", "text": "Composition can be used together with a \u201cflower automaton\u201d to calculate the edit distance between two sequences (Mohri, 2003). The edit distance transducer shown in Fig. 1(a) transduces a sequence x to another sequence y over the alphabet {a, b} and accumulates the number of edit operations via the transitions with cost 1. In our case, x corresponds to an NMT hypothesis which is to be combined with a Hiero hypothesis y. In contrast to SGNMT, where we require an exact match between NMT and Hiero (up to UNKs), our editdistance-based scheme allows different hypotheses to be combined. We replaced the standard\ndefinition of the edit distance transducer (Mohri, 2003) by a finer-grained model designed to work well for combining NMT and Hiero. Instead of uniform costs, we lower the cost for UNK substitutions as we want to encourage substituting NMT UNKs by words in the Hiero translation. We distinguish between three types of edit operations.\n\u2022 Type I: Substituting UNK with a word outside the NMT vocabulary is free.\n\u2022 Type II: For substitutions of UNK with a word inside the NMT vocabulary we add the cost \u03bbsub.\n\u2022 Type III: All other edit operations are penalized with cost \u03bbedit (and \u03bbedit > \u03bbsub).\nWe will refer to the modified edit distance transducer as E. Fig. 1(b) shows E over the alphabet {a, b,UNK}, with \u2018a\u2019 being an NMT OOV."}, {"heading": "2.3 Loose Coupling of Hiero and NMT", "text": "Our edit-distance-based scheme combines an NMT translation lattice N with a Hiero translation lattice H . Weights in N and H are scaled by \u03bbnmt and \u03bbhiero, respectively. The similarity measure between NMT and Hiero translations is parametrized with \u03bbins, \u03bbedit, and \u03bbsub. We keep the various costs separated by using transducers with tropical sparse tuple vector semirings (Iglesias et al., 2015). Instead of single real-valued arc weights, this semiring uses vectors which can hold multiple features. The inner product of these vectors with a constant parameter vector determines\nthe final weights on the arcs1. The sparse tuple vector semiring enables us to optimize the \u03bbparameters with LMERT (Macherey et al., 2008) on a development set.\nExamples for H and N are shown in Fig. 2(a) and Fig. 2(b). The shortest path in H containing the string nicht erlaubt sein sollte zu has grammatical and stylistic flaws but is complete, whereas there is a better path in N with an UNK. Our goal is to merge these two hypotheses by using the NMT translation in N with the UNK replaced by a word from the Hiero lattice H .\n1. Adding UNK insertions. We found that often NMT produces an isolated UNK token, even if multiple tokens are required. Therefore, we allow extending a single UNK token to a sequence of up to three UNK tokens. This is realized by replacing UNK arcs in N with the transducer U shown in Fig. 3 using OpenFST\u2019s Replace operation. Fig. 2(c) shows the result of the replace operation when applied to the example lattice N in Fig. 2(b). We denote this operation as follows:\nReplace(N, UNK, U) (2) 1The ucam-smt tutorial contains details to the tropical sparse tuple vector semiring: http://ucamsmt.github.io/tutorial/basictrans.html#lmert veclats tst\n2. Composition with the edit distance transducer. The next step finds the edit distances to the Hiero hypotheses as described in Sec. 2.2.\nC := Replace(N, UNK, U) \u25e6 E \u25e6H (3)\n3. Shortest path. The above operation generates very large lattices, and dumping all of them is not feasible. We could use disambiguation (Iglesias et al., 2015; Mohri and Riley, 2015) on the combined transducerC to find the best alignment for each unique NMT hypothesis. However, we only need the single shortest path in order to generate the combined translation.\nShortestPath(C) (4)\n4. Projection. A complete path in the transducer C has an NMT hypothesis on the input labels (marked green in Fig. 2(d)) and a Hiero hypothesis on the output labels (marked blue in Fig. 2(d)). Therefore, we can generate different translations from the best path in C. If we project the input labels on the output labels with OpenFST\u2019s Project, we obtain a hypothesis t\u0302NMT in the NMT lattice N .\nt\u0302NMT = \u03a01(ShortestPath(C)) (5)\nHowever, t\u0302NMT still contains UNKs. If we project on the input labels, we end up with the aligned Hiero hypothesis without UNKs (blue labels in Fig. 2(d))\nt\u0302Hiero = \u03a02(ShortestPath(C)) (6)\nbut we do not use the NMT translation directly. Therefore, we introduce a new projection function \u03a0UNK which switches between preserving symbols on the input and output tapes: if the input label on an arc is UNK, we write the output label over the input label. Otherwise, we write the input label over the output label. This is equivalent to projecting the output labels to the input labels only if the input label is UNK, and then projecting the input labels to the output labels. As shown in Fig. 2(e), we obtain the NMT hypothesis, but the UNK is replaced by the matching word Grosswahlstadt from the Hiero lattice. Thus, the final combined translation is described by the following term:\nt\u0302comb = \u03a0UNK(ShortestPath(C)) (7)\nIn general, the final hypothesis t\u0302comb is a mix of an NMT and a Hiero hypothesis. We do not search for t\u0302comb directly but for pairs of NMT and Hiero translations which optimize the individual model scores as well as the distance between them. Stated more formally, the shortest path in C yields a pair (t\u0302NMT , t\u0302Hiero) for which holds\nt\u0302NMT , t\u0302Hiero = argmin (tN ,tH)\u2208N\u00d7H\n( dedit(tN , tH)\n+\u03bbnmt \u00b7 SN (tN |s) + \u03bbhiero \u00b7 SH(tH |s) ) (8)\nwhere dedit(tN , tH) is the modified edit distance between tN and tH (according E and U ), and SN (tN |s) and SH(tH |s) are the scores NMT and Hiero assign to the translations given source sentence s. If we interpret these scores as negative log-likelihoods, we arrive at a probabilistic interpretation of Eq. 8.\nt\u0302NMT , t\u0302Hiero = argmax (tN ,tH)\u2208N\u00d7H ( e\u2212dedit(tN ,tH) \u00b7 P (tN , tH |s) ) (9)\nwith (assuming independence)\nP (tN , tH |s) := PN (tN |s)\u03bbnmt \u00b7PH(tH |s)\u03bbhiero .\nEq. 9 suggests that we maximize the product of two quantities \u2013 the similarity between Hiero and NMT hypotheses and their joint probability. The FST operations allow to optimize over the set N \u00d7 H efficiently. Note that the NMT lattice N is rather small in our case (|N | \u2264 20) due to the small beam size used in NMT decoding. This makes it possible to solve Eq. 8 almost always without pruning 2."}, {"heading": "3 Experimental Setup", "text": "The parallel training data includes Europarl v7, Common Crawl, and News Commentary v10. Sentence pairs with sentences longer than 80 words or length ratios exceeding 2.4:1 were deleted, as were Common Crawl sentences from other languages (Shuyo, 2010). We use news-test2014 (the filtered version) as a development set, and keep news-test2015 and news-test2016 as test sets.\nThe NMT systems are built using the Blocks framework (van Merrie\u0308nboer et al., 2015) based on the Theano library (Bastien et al., 2012) with the network architecture and hyper-parameters as in (Bahdanau et al., 2015): the encoder and decoder networks consist of 1000 gated recurrent units (Cho et al., 2014). The decoder uses a single maxout (Goodfellow et al., 2013) output layer with the feed-forward attention model described in (Bahdanau et al., 2015). In our final ensemble, we use 8 independently trained NMT systems with vocabulary sizes between 30,000 and 60,000.\nRules for our En-De Hiero system were extracted as described in (de Gispert et al., 2010). A 5-gram language model for the Hiero system was trained on WMT16 parallel and monolingual data (Heafield et al., 2013).\nWe apply gentle post-processing to the German output for fixing small number and currency formatting issues. The English source sentences in the training corpus are lower-cased. During decoding, we lower case only in-vocabulary words, and pass through OOVs with correct casing. We apply a simple heuristic for recognizing surnames to avoid literal translation of them into German3.\n2We limit the Hiero lattices to a maximum of 100,000 nodes with OpenFST\u2019s Prune to remove the worst outliers.\n3We mark a word as surname if it has occurred after a first name, is on a census list of known surnames, and is written with a capitalized initial letter."}, {"heading": "4 Results", "text": "Tab. 1 reports performance on news-test2014, news-test2015, and news-test20165. Similarly to previous work (Stahlberg et al., 2016), we observe that rescoring Hiero lattices with NMT (SGNMT) outperforms both NMT and Hiero baselines significantly on all test sets. For SGNMT, we see further improvements of between +0.7 BLEU (newstest2014) and +1.1 BLEU (news-test2015) by using NMT ensembles rather than single NMT. However, these gains are rather small considering the improvements from using ensembles for the (pure) NMT baseline (between +1.9 BLEU and +2.2 BLEU). Our combination scheme makes better use of the ensembles. We report 31.3 BLEU on news-test2016, which in the EnglishGerman WMT\u201916 evaluation is among the best systems (within 0.1 BLEU) which do not use back-translation (Sennrich et al., 2016a). Backtranslation is a technique for making use of monolingual data in NMT training, and we expect our system could benefit from back-translation, although we leave this analysis to future work.\n4http://matrix.statmt.org/ 5The code we used for SGNMT and ensembling is avail-\nable at http://ucam-smt.github.io/sgnmt/html/.\nThe combination procedure we propose is nontrivial. It is not immediately clear how the gains arise as the final scores are mixtures between edit distance costs, NMT scores, and Hiero scores. In the remainder we will try to provide some insight. Unless stated otherwise, we report investigations into the Hiero + NMT 8-system ensemble which yields the best results in Tab. 1.\nFirst, we focus on the projection function \u03a0UNK(\u00b7) which switches between preserving the input and output label at the UNK symbol to produce the combined translation t\u0302comb (Eq. 7). As explained in Sec. 2.3, we can use OpenFST\u2019s Project operation to fetch the NMT and Hiero hypotheses t\u0302NMT and t\u0302Hiero which have been used to produce the combined translation (Eq. 5 and 6). Tab. 2 shows that the hypotheses that are aligned in the final transducer are often not the 1- best translations of any of the baseline systems. Remarkably, using the t\u0302Hiero translations results in 30.4 BLEU, which is a very substantial im-\nprovement over the baseline Hiero system (26.0 BLEU). Note that this BLEU score is achieved with hypotheses from the original Hiero lattice H but weighted in combination with the NMT scores and the edit distance. However, these selected paths are often given very low scores by Hiero: in only 8.6% of the sentences is the Hiero hypothesis left unchanged. If we look for t\u0302Hiero in the Hiero n-best list, we find that even very deep 20,000-best lists contain only 63.5% of the Hiero hypotheses which were selected by the combination scheme (Fig. 4). This indicates the benefit in using latticebased approaches over n-best lists.\nNext, we investigate the distance measure between NMT and Hiero translations, which is realized with the UNK insertion transducer U and the modified edit distance transducer E (Sec. 2.3). Tab. 3 shows that UNK insertions are relatively rare compared to the edit operations of types II and III allowed by E (Sec. 2.3). The average edit distance between NMT and Hiero disregarding UNKs on the best path (type III) is 1.74. In 61.7% of the cases the input and output labels differ not only at UNK \u2013 i.e. in only 38.3% of the sentences do we have an exact match between NMT and Hiero. We note that UNK is often replaced with an NMT in-vocabulary word (55.9% of the sentences). It seems that NMT often produces an UNK even if a better word is in the NMT vocabulary. This could be due to the over-representation of UNK in the NMT training corpus.\nTo study the effectiveness of our edit distance transducer based combination scheme in correcting NMT UNKs, we trained individual NMT systems with vocabulary sizes between 10,000 and 60,000. Tab. 4 shows that nearly one in six tokens (16.3%) produced by our pure NMT system with a vocabulary size of 30,000 are UNKs. Increasing the NMT vocabulary to 50k or 60k does improve pure NMT very significantly, but results show that these improvements are already captured by the combination scheme with Hiero. As in the literature, we see large variation in performance over individual NMT systems even with the same vocabulary size (Sennrich et al., 2016b), which could explain the small performance drop when increasing the vocabulary size from 50k to 60k.\nOne important practical issue for system building is the number of systems to be ensembled as training each individual NMT system takes a significant amount of time. Fig. 5 indicates that even\nfor 8-ensembles the gains for pure NMT do not seem to saturate. The combination with Hiero via edit distance transducer also greatly benefits from using ensembles, but most of the gains are gotten with fewer systems."}, {"heading": "5 Conclusion and Future Work", "text": "We have presented a method based on the edit distance that is effective in combining Hiero SMT systems with NMT ensembles. Our approach makes use of standard WFST operations, and we showed the effectiveness of the approach with a successful WMT\u201916 submission for EnglishGerman. In the future, we are planning to add back-translation (Sennrich et al., 2016a) and investigate the use of character- or subword-based NMT (Sennrich et al., 2016b; Chitnis and DeNero, 2015; Ling et al., 2015; Chung et al., 2016; Luong and Manning, 2016) within our combination framework."}, {"heading": "Acknowledgements", "text": "This work was supported by the U.K. Engineering and Physical Sciences Research Council (EPSRC grant EP/L027623/1)."}], "references": [{"title": "OpenFst: A general and efficient weighted finite-state transducer library", "author": ["Cyril Allauzen", "Michael Riley", "Johan Schalkwyk", "Wojciech Skut", "Mehryar Mohri."], "venue": "Implementation and Application of Automata, pages 11\u201323. Springer.", "citeRegEx": "Allauzen et al\\.,? 2007", "shortCiteRegEx": "Allauzen et al\\.", "year": 2007}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio."], "venue": "NIPS.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Hierarchical phrase-based translation", "author": ["David Chiang."], "venue": "Computational Linguistics, 33(2):201\u2013228.", "citeRegEx": "Chiang.,? 2007", "shortCiteRegEx": "Chiang.", "year": 2007}, {"title": "Variablelength word encodings for neural translation models", "author": ["Rohan Chitnis", "John DeNero."], "venue": "EMNLP, pages 2088\u20132093.", "citeRegEx": "Chitnis and DeNero.,? 2015", "shortCiteRegEx": "Chitnis and DeNero.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions", "author": ["Adri\u00e0 de Gispert", "Sami Virpioja", "Mikko Kurimo", "William Byrne."], "venue": "NAACL, pages 73\u201376.", "citeRegEx": "Gispert et al\\.,? 2009", "shortCiteRegEx": "Gispert et al\\.", "year": 2009}, {"title": "Hierarchical phrase-based translation with weighted finite-state transducers and shallow-n grammars", "author": ["Adri\u00e0 de Gispert", "Gonzalo Iglesias", "Graeme Blackwood", "Eduardo R Banga", "William Byrne."], "venue": "Computational Linguistics, 36(3):505\u2013533.", "citeRegEx": "Gispert et al\\.,? 2010", "shortCiteRegEx": "Gispert et al\\.", "year": 2010}, {"title": "A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER)", "author": ["Jonathan G Fiscus."], "venue": "ASRU, pages 347\u2013354.", "citeRegEx": "Fiscus.,? 1997", "shortCiteRegEx": "Fiscus.", "year": 1997}, {"title": "Minimum Bayes-risk automatic speech recognition", "author": ["Vaibhava Goel", "William J Byrne."], "venue": "Computer Speech & Language, 14(2):115\u2013135.", "citeRegEx": "Goel and Byrne.,? 2000", "shortCiteRegEx": "Goel and Byrne.", "year": 2000}, {"title": "Maxout networks", "author": ["Ian Goodfellow", "David Warde-farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio."], "venue": "ICML, pages 1319\u20131327.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "ACL, pages 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Transducer disambiguation with sparse topological features", "author": ["Gonzalo Iglesias", "Adri\u00e0 de Gispert", "William Byrne."], "venue": "EMNLP 2015, pages 2275\u2013 2280.", "citeRegEx": "Iglesias et al\\.,? 2015", "shortCiteRegEx": "Iglesias et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP, page 413.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Minimum Bayes-risk decoding for statistical machine translation", "author": ["Shankar Kumar", "William Byrne."], "venue": "Technical report, DTIC Document.", "citeRegEx": "Kumar and Byrne.,? 2004", "shortCiteRegEx": "Kumar and Byrne.", "year": 2004}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black."], "venue": "arXiv preprint arXiv:1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Joint decoding with multiple translation models", "author": ["Yang Liu", "Haitao Mi", "Yang Feng", "Qun Liu."], "venue": "ACL, pages 576\u2013584.", "citeRegEx": "Liu et al\\.,? 2009", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D Manning."], "venue": "ACL.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Lattice-based minimum error rate training for statistical machine translation", "author": ["Wolfgang Macherey", "Franz Josef Och", "Ignacio Thayer", "Jakob Uszkoreit."], "venue": "EMNLP, pages 725\u2013734.", "citeRegEx": "Macherey et al\\.,? 2008", "shortCiteRegEx": "Macherey et al\\.", "year": 2008}, {"title": "On the disambiguation of weighted automata", "author": ["Mehryar Mohri", "Michael D Riley."], "venue": "Implementation and Application of Automata, pages 263\u2013 278. Springer.", "citeRegEx": "Mohri and Riley.,? 2015", "shortCiteRegEx": "Mohri and Riley.", "year": 2015}, {"title": "Edit-distance of weighted automata: General definitions and algorithms", "author": ["Mehryar Mohri."], "venue": "International Journal of Foundations of Computer Science, 14(06):957\u2013982.", "citeRegEx": "Mohri.,? 2003", "shortCiteRegEx": "Mohri.", "year": 2003}, {"title": "Weighted finite-state transducer algorithms", "author": ["Mehryar Mohri."], "venue": "An overview. In Formal Languages and Applications, pages 551\u2013563. Springer.", "citeRegEx": "Mohri.,? 2004", "shortCiteRegEx": "Mohri.", "year": 2004}, {"title": "Neural reranking improves subjective quality of machine translation: NAIST at WAT2015", "author": ["Graham Neubig", "Makoto Morishita", "Satoshi Nakamura."], "venue": "arXiv preprint arXiv:1510.05203.", "citeRegEx": "Neubig et al\\.,? 2015", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL.", "citeRegEx": "Sennrich et al\\.,? 2016a", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL.", "citeRegEx": "Sennrich et al\\.,? 2016b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Language detection library for Java", "author": ["Nakatani Shuyo."], "venue": "http://code.google.com/ p/language-detection/. [Online; accessed 1-June-2016].", "citeRegEx": "Shuyo.,? 2010", "shortCiteRegEx": "Shuyo.", "year": 2010}, {"title": "Syntactically guided neural machine translation", "author": ["Felix Stahlberg", "Eva Hasler", "Aurelien Waite", "Bill Byrne."], "venue": "ACL.", "citeRegEx": "Stahlberg et al\\.,? 2016", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Lattice minimum Bayes-risk decoding for statistical machine translation", "author": ["Roy W Tromble", "Shankar Kumar", "Franz Och", "Wolfgang Macherey."], "venue": "EMNLP, pages 620\u2013629.", "citeRegEx": "Tromble et al\\.,? 2008", "shortCiteRegEx": "Tromble et al\\.", "year": 2008}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1506.00619.", "citeRegEx": "Merri\u00ebnboer et al\\.,? 2015", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Previous work suggests that syntactic machine translation such as Hiero (Chiang, 2007) and Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al.", "startOffset": 72, "endOffset": 86}, {"referenceID": 14, "context": "Previous work suggests that syntactic machine translation such as Hiero (Chiang, 2007) and Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) are very different and have complementary strengths and weaknesses (Neubig et al.", "startOffset": 124, "endOffset": 221}, {"referenceID": 28, "context": "Previous work suggests that syntactic machine translation such as Hiero (Chiang, 2007) and Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) are very different and have complementary strengths and weaknesses (Neubig et al.", "startOffset": 124, "endOffset": 221}, {"referenceID": 5, "context": "Previous work suggests that syntactic machine translation such as Hiero (Chiang, 2007) and Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) are very different and have complementary strengths and weaknesses (Neubig et al.", "startOffset": 124, "endOffset": 221}, {"referenceID": 1, "context": "Previous work suggests that syntactic machine translation such as Hiero (Chiang, 2007) and Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) are very different and have complementary strengths and weaknesses (Neubig et al.", "startOffset": 124, "endOffset": 221}, {"referenceID": 23, "context": ", 2015) are very different and have complementary strengths and weaknesses (Neubig et al., 2015; Stahlberg et al., 2016).", "startOffset": 75, "endOffset": 120}, {"referenceID": 27, "context": ", 2015) are very different and have complementary strengths and weaknesses (Neubig et al., 2015; Stahlberg et al., 2016).", "startOffset": 75, "endOffset": 120}, {"referenceID": 23, "context": "Authors in (Neubig et al., 2015) used NMT to rescore n-best lists which were generated with a syntax-based system.", "startOffset": 11, "endOffset": 32}, {"referenceID": 17, "context": "In general, this kind of hard restriction is best avoided when combining diverse systems (Liu et al., 2009; Frederking et al., 1994).", "startOffset": 89, "endOffset": 132}, {"referenceID": 9, "context": "For example, in speech recognition, ROVER (Fiscus, 1997) is a system combination approach based on a soft voting scheme.", "startOffset": 42, "endOffset": 56}, {"referenceID": 15, "context": "In machine translation, minimum Bayes-risk (MBR) decoding (Kumar and Byrne, 2004) can be used to combine multiple systems (de Gispert et al.", "startOffset": 58, "endOffset": 81}, {"referenceID": 10, "context": "MBR also does not enforce exact agreement between systems as it distinguishes between the hypothesis space and the evidence space (Goel and Byrne, 2000; Tromble et al., 2008).", "startOffset": 130, "endOffset": 174}, {"referenceID": 29, "context": "MBR also does not enforce exact agreement between systems as it distinguishes between the hypothesis space and the evidence space (Goel and Byrne, 2000; Tromble et al., 2008).", "startOffset": 130, "endOffset": 174}, {"referenceID": 3, "context": "We find that Hiero lattices generated by grammars extracted with the usual heuristics (Chiang, 2007) do not provide enough variety to explore the full potential of neural models, especially when using NMT ensembles.", "startOffset": 86, "endOffset": 100}, {"referenceID": 0, "context": "This scheme is implemented efficiently using standard FST operations (Allauzen et al., 2007).", "startOffset": 69, "endOffset": 92}, {"referenceID": 22, "context": "The composition of two weighted transducers T1, T2 (denoted as T1 \u25e6 T2) over a semiring (K,\u2295,\u2297) is defined following (Mohri, 2004)", "startOffset": 117, "endOffset": 130}, {"referenceID": 21, "context": "Composition can be used together with a \u201cflower automaton\u201d to calculate the edit distance between two sequences (Mohri, 2003).", "startOffset": 112, "endOffset": 125}, {"referenceID": 21, "context": "We replaced the standard definition of the edit distance transducer (Mohri, 2003) by a finer-grained model designed to work well for combining NMT and Hiero.", "startOffset": 68, "endOffset": 81}, {"referenceID": 13, "context": "We keep the various costs separated by using transducers with tropical sparse tuple vector semirings (Iglesias et al., 2015).", "startOffset": 101, "endOffset": 124}, {"referenceID": 19, "context": "The sparse tuple vector semiring enables us to optimize the \u03bbparameters with LMERT (Macherey et al., 2008) on a development set.", "startOffset": 83, "endOffset": 106}, {"referenceID": 13, "context": "We could use disambiguation (Iglesias et al., 2015; Mohri and Riley, 2015) on the combined transducerC to find the best alignment for each unique NMT hypothesis.", "startOffset": 28, "endOffset": 74}, {"referenceID": 20, "context": "We could use disambiguation (Iglesias et al., 2015; Mohri and Riley, 2015) on the combined transducerC to find the best alignment for each unique NMT hypothesis.", "startOffset": 28, "endOffset": 74}, {"referenceID": 26, "context": "4:1 were deleted, as were Common Crawl sentences from other languages (Shuyo, 2010).", "startOffset": 70, "endOffset": 83}, {"referenceID": 2, "context": ", 2015) based on the Theano library (Bastien et al., 2012) with the network architecture and hyper-parameters as in (Bahdanau et al.", "startOffset": 36, "endOffset": 58}, {"referenceID": 1, "context": ", 2012) with the network architecture and hyper-parameters as in (Bahdanau et al., 2015): the encoder and decoder networks consist of 1000 gated recurrent units (Cho et al.", "startOffset": 65, "endOffset": 88}, {"referenceID": 5, "context": ", 2015): the encoder and decoder networks consist of 1000 gated recurrent units (Cho et al., 2014).", "startOffset": 80, "endOffset": 98}, {"referenceID": 11, "context": "The decoder uses a single maxout (Goodfellow et al., 2013) output layer with the feed-forward attention model described in (Bahdanau et al.", "startOffset": 33, "endOffset": 58}, {"referenceID": 1, "context": ", 2013) output layer with the feed-forward attention model described in (Bahdanau et al., 2015).", "startOffset": 72, "endOffset": 95}, {"referenceID": 12, "context": "A 5-gram language model for the Hiero system was trained on WMT16 parallel and monolingual data (Heafield et al., 2013).", "startOffset": 96, "endOffset": 119}, {"referenceID": 27, "context": "Similarly to previous work (Stahlberg et al., 2016), we observe that rescoring Hiero lattices with NMT (SGNMT) outperforms both NMT and Hiero baselines significantly on all test sets.", "startOffset": 27, "endOffset": 51}, {"referenceID": 24, "context": "1 BLEU) which do not use back-translation (Sennrich et al., 2016a).", "startOffset": 42, "endOffset": 66}, {"referenceID": 25, "context": "As in the literature, we see large variation in performance over individual NMT systems even with the same vocabulary size (Sennrich et al., 2016b), which could explain the small performance drop when increasing the vocabulary size from 50k to 60k.", "startOffset": 123, "endOffset": 147}, {"referenceID": 24, "context": "In the future, we are planning to add back-translation (Sennrich et al., 2016a) and investigate the use of character- or subword-based NMT (Sennrich et al.", "startOffset": 55, "endOffset": 79}, {"referenceID": 25, "context": ", 2016a) and investigate the use of character- or subword-based NMT (Sennrich et al., 2016b; Chitnis and DeNero, 2015; Ling et al., 2015; Chung et al., 2016; Luong and Manning, 2016) within our combination framework.", "startOffset": 68, "endOffset": 182}, {"referenceID": 4, "context": ", 2016a) and investigate the use of character- or subword-based NMT (Sennrich et al., 2016b; Chitnis and DeNero, 2015; Ling et al., 2015; Chung et al., 2016; Luong and Manning, 2016) within our combination framework.", "startOffset": 68, "endOffset": 182}, {"referenceID": 16, "context": ", 2016a) and investigate the use of character- or subword-based NMT (Sennrich et al., 2016b; Chitnis and DeNero, 2015; Ling et al., 2015; Chung et al., 2016; Luong and Manning, 2016) within our combination framework.", "startOffset": 68, "endOffset": 182}, {"referenceID": 6, "context": ", 2016a) and investigate the use of character- or subword-based NMT (Sennrich et al., 2016b; Chitnis and DeNero, 2015; Ling et al., 2015; Chung et al., 2016; Luong and Manning, 2016) within our combination framework.", "startOffset": 68, "endOffset": 182}, {"referenceID": 18, "context": ", 2016a) and investigate the use of character- or subword-based NMT (Sennrich et al., 2016b; Chitnis and DeNero, 2015; Ling et al., 2015; Chung et al., 2016; Luong and Manning, 2016) within our combination framework.", "startOffset": 68, "endOffset": 182}], "year": 2016, "abstractText": "This paper presents the University of Cambridge submission to WMT16. Motivated by the complementary nature of syntactical machine translation and neural machine translation (NMT), we exploit the synergies of Hiero and NMT in different combination schemes. Starting out with a simple neural lattice rescoring approach, we show that the Hiero lattices are often too narrow for NMT ensembles. Therefore, instead of a hard restriction of the NMT search space to the lattice, we propose to loosely couple NMT and Hiero by composition with a modified version of the edit distance transducer. The loose combination outperforms lattice rescoring, especially when using multiple NMT systems in an ensemble.", "creator": "TeX"}}}