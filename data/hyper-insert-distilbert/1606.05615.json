{"id": "1606.05615", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2016", "title": "Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains", "abstract": "\\ t textit { submodular continuous functions } utilities are a sophisticated category package of ( particularly generally ) optimization non - convex / non - metric concave functions each with a wide wide spectrum ranging of applications. \u2026 we will characterize broadly these functions and thus demonstrate that technically they can be maximized efficiently together with approximation optimization guarantees. [ specifically, { i ) for monotone submodular column continuous functions albeit with an additional diminishing returns et property, we propose drafting a \\ & texttt { mb frank - wolfe } style search algorithm with $ ( 1 - time 1 / e ) $ - approximation, and ^ sub - estimate linear finite convergence mutation rate ; ii ) code for general non - constraint monotone matrix submodular continuous functions, we propose introducing a \\ texttt { doublegreedy } algorithm with $ 1 / 3 $ - approximation. \" submodular continuous functions therefore naturally both find productive applications in for various real - tree world settings, functions including boundary influence analysis and revenue distributions maximization combined with continuous distribution assignments, sensor energy management, multi - resolution data warehouse summarization, cloud facility relative location, etc. considerable experimental results exist show potential that however the manufacturers proposed such algorithms routinely efficiently generate superior solutions in varying terms of empirical programming objectives worthy compared to classical baseline complexity algorithms.", "histories": [["v1", "Fri, 17 Jun 2016 18:15:52 GMT  (354kb,D)", "https://arxiv.org/abs/1606.05615v1", "20 pages, 9 figures"], ["v2", "Tue, 20 Sep 2016 16:21:08 GMT  (358kb,D)", "http://arxiv.org/abs/1606.05615v2", "26 pages, 9 figures"], ["v3", "Tue, 6 Dec 2016 15:53:40 GMT  (358kb,D)", "http://arxiv.org/abs/1606.05615v3", "26 pages, 9 figures"], ["v4", "Wed, 1 Mar 2017 16:30:15 GMT  (364kb,D)", "http://arxiv.org/abs/1606.05615v4", "To appear in the 20th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017"]], "COMMENTS": "20 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["andrew an bian", "baharan mirzasoleiman", "joachim m buhmann", "reas krause"], "accepted": false, "id": "1606.05615"}, "pdf": {"name": "1606.05615.pdf", "metadata": {"source": "CRF", "title": "Guaranteed Non-convex Optimization: Submodular Maximization over Continuous Domains\u2217", "authors": ["Andrew An Bian", "Baharan Mirzasoleiman", "Joachim M. Buhmann"], "emails": ["bian.andrewa@gmail.com", "baharanm@inf.ethz.ch", "jbuhmann@inf.ethz.ch", "krausea@ethz.ch"], "sections": [{"heading": null, "text": "\u2217Appears in the 20th International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, Fort Lauderdale, Florida, USA.\nar X\niv :1\n60 6.\n05 61\n5v 4\n[ cs\n.L G"}, {"heading": "1. Introduction", "text": "Non-convex optimization delineates the new frontier in machine learning, arising in numerous learning tasks from training deep neural networks to latent variable models (Anandkumar et al., 2014). Understanding, which classes of objectives can be tractably optimized remains a central challenge. In this paper, we investigate a class of generally non-convex and non-concave functions\u2013submodular continuous functions, and derive algorithms for approximately optimizing them with strong approximation guarantees.\nSubmodularity is a structural property usually associated with set functions, with important implications for optimization. Optimizing submodular set functions has found numerous applications in machine learning, including variable selection (Krause and Guestrin, 2005), dictionary learning (Krause and Cevher, 2010; Das and Kempe, 2011), sparsity inducing regularizers (Bach, 2010), summarization (Lin and Bilmes, 2011; Mirzasoleiman et al., 2013) and variational inference (Djolonga and Krause, 2014). Submodular set functions can be efficiently minimized (Iwata et al., 2001), and there are strong guarantees for approximate maximization (Nemhauser et al., 1978; Krause and Golovin, 2012).\nEven though submodularity is most widely considered in the discrete realm, the notion can be generalized to arbitrary lattices (Fujishige, 2005). Recently, Bach (2015) showed how results from submodular set function minimization can be lifted to the continuous domain. In this paper, we further pursue this line of investigation, and demonstrate that results from submodular set function maximization can be generalized as well. Note that the underlying concepts associated with submodular function minimization and maximization are quite distinct, and both require different algorithmic treatment and analysis techniques.\nAs motivation for our inquiry, we firstly give a thorough characterization of the class of submodular and DR-submodular2 functions in Section 3. We propose the weak DR property and prove that it is equivalent to submodularity for general functions. This resolves the question whether there exists a diminishing-return-style characterization that is equivalent to submodularity for all set, integer-lattice and continuous functions. We then present two guaranteed algorithms for maximizing submodular continuous functions in Sections 4 and 5, respectively. The first approach, based on the Frank-Wolfe algorithm (Frank and Wolfe, 1956) and the continuous greedy algorithm (Vondra\u0301k, 2008), applies to monotone DR-submodular functions. It provides a (1 \u2212 1/e) approximation guarantee under general down-closed convex constraints. We also provide a second, coordinate-ascent-style algorithm, which applies to arbitrary submodular continuous function maximization under box constraints, and provides a 1/3 approximation guarantee. This algorithm is based on the double greedy algorithm (Buchbinder et al., 2012) from submodular set function maximization. In Section 6 we illustrate how submodular continuous maximization captures various important applications, ranging from sensor energy management, to influence and revenue maximization, to facility location, and non-convex/non-concave quadratic programming. Lastly, we experimentally demonstrate the effectiveness of our algorithms on several problem instances in Section 7."}, {"heading": "2. Background and related work", "text": "Notions of submodularity. Submodularity is often viewed as a discrete analogue of convexity, and provides computationally effective structure so that many discrete problems with this property are efficiently solvable or approximable. Of particular interest is a (1 \u2212 1/e)-approximation for\n2. A DR-submodular function is a function with the diminishing returns property, which will be formally defined in Section 3.\nmaximizing a monotone submodular set function subject to a cardinality, a matroid, or a knapsack constraint (Nemhauser et al., 1978; Vondra\u0301k, 2008; Sviridenko, 2004). Another result relevant to this work is unconstrained maximization of non-monotone submodular set functions, for which Buchbinder et al. (2012) propose the deterministic double greedy algorithm with 1/3 approximation guarantee, and the randomized double greedy algorithm which achieves the tight 1/2 approximation guarantee.\nAlthough most commonly associated with set functions, in many practical scenarios, it is natural to consider generalizations of submodular set functions. Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1\u2212 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint. For non-monotone submodular functions over the bounded integer lattice, Gottschalk and Peis (2015) provide a 1/3-approximation. Approximation algorithms for maximizing bisubmodular functions and k-submodular functions have also been proposed by Singh et al. (2012); Ward and Zivny (2014).\nWolsey (1982) considers maximizing a special class of submodular continuous functions subject to one knapsack constraint, in the context of solving location problems. That class of functions are additionally required to be monotone, piecewise linear and concave. Ca\u0306linescu et al. (2007) and Vondra\u0301k (2008) discuss a subclass of submodular continuous functions, which is termed smooth submodular functions3, to describe the multilinear extension of a submodular set function. They propose the continuous greedy algorithm, which has a (1\u2212 1/e) approximation guarantee on maximizing a smooth submodular functions under a down-monotone polytope constraint. Recently, Bach (2015) considers the minimization of a submodular continuous function, and proves that efficient techniques from convex optimization may be used for minimization. Very recently, Ene and Nguyen (2016) provide a reduction from a integer-lattice DR-submodular function maximization problem to a submodular set function maximization problem, which suggests a way to optimize submodular continuous functions over simple continuous constriants: Discretize the continuous function and constraint to be an integer-lattice instance, and then optimize it using the reduction. However, for monotone DR-submodular functions maximization, this method can not handle the general continuous constraints discussed in this work, i.e., arbitrary down-closed convex sets. And for general submodular function maximization, this method cannot be applied, since the reduction needs the additional diminishing returns property. Therefore we focus on continuous methods in this work.\nNon-convex optimization. Optimizing non-convex continuous functions has received renewed interest in the last decades. Recently, tensor methods have been used in various non-convex problems, e.g., learning latent variable models (Anandkumar et al., 2014) and training neural networks (Janzamin et al., 2015). A fundamental problem in non-convex optimization is to reach a stationary point assuming the smoothness of the objective (Sra, 2012; Li and Lin, 2015; Reddi et al., 2016; Allen-Zhu and Hazan, 2016). With extra assumptions, certain global convergence results can be obtained. For example, for functions with Lipschitz continuous Hessians, the regularized Newton scheme of Nesterov and Polyak (2006) achieves global convergence results for functions with\n3. A function f : [0, 1]n \u2192 R is smooth submodular if it has second partial derivatives everywhere and all entries of its Hessian matrix are non-positive.\nan additional star-convexity property or with an additional gradient-dominance property (Polyak, 1963). Hazan et al. (2015) introduce the family of \u03c3-nice functions and propose a graduated optimization-based algorithm, that provably converges to a global optimum for this family of (generally) non-convex functions. However, it is typically difficult to verify whether these assumptions hold in real-world problems.\nTo the best of our knowledge, this work is the first to address the general problem of monotone and non-monotone submodular maximization over continuous domains. It is also the first to propose a sufficient and necessary diminishing-return-style characterization of submodularity for general functions. We propose efficient algorithms with strong approximation guarantees. We further show that continuous submodularity is a common property of many well-known objectives and finds various real-world applications.\nNotation. Throughout this work we assume E = {e1, e2, \u00b7 \u00b7 \u00b7 , en} is the ground set of n elements, and \u03c7i \u2208 Rn is the characteristic vector for element ei. We use boldface letters x \u2208 RE and x \u2208 Rn interchanglebly to indicate a n-dimensional vector, where xi is the i-th entry of x. We use a boldface captial letter A \u2208 Rm\u00d7n to denote a matrix. For two vectors x,y \u2208 RE , x \u2264 y means xi \u2264 yi for every element i in E. Finally, x|xi\u2190k is the operation of setting the i-th element of x to k, while keeping all other elements unchanged."}, {"heading": "3. Characterizations of submodular continuous functions", "text": "Submodular continuous functions are defined on subsets of Rn: X = \u220fn i=1Xi, where each Xi is a compact subset of R (Topkis, 1978; Bach, 2015). A function f : X \u2192 R is submodular iff for all (x,y) \u2208 X \u00d7 X ,\nf(x) + f(y) \u2265 f(x \u2228 y) + f(x \u2227 y), (submodularity) (1)\nwhere \u2227 and \u2228 are the coordinate-wise minimum and maximum operations, respectively. Specifically, Xi could be a finite set, such as {0, 1} (in which case f(\u00b7) is called set function), or {0, \u00b7 \u00b7 \u00b7 , ki \u2212 1} (called integer-lattice function), where the notion of continuity is vacuous; Xi can also be an interval, which is referred to as a continuous domain. In this work, we consider the interval by default, but it is worth noting that the properties introduced in this section can be applied to Xi being a general compact subset of R.\nWhen twice-differentiable, f(\u00b7) is submodular iff all off-diagonal entries of its Hessian are nonpositive4 (Bach, 2015),\n\u2200x \u2208 X , \u2202 2f(x)\n\u2202xi\u2202xj \u2264 0, \u2200i 6= j. (2)\nSubmodular\nConcave Convex\nDR-submodular\nFigure 1: Concavity, convexity, submodularity and DRsubmodularity.\nThe class of submodular continuous functions contains a subset of both convex and concave functions, and shares some useful properties with them (illustrated in Figure 1). Examples include submodular and convex functions of the form \u03c6ij(xi \u2212 xj) for \u03c6ij convex; submodular and concave functions of the form x 7\u2192 g( \u2211n i=1 \u03bbixi) for g concave and \u03bbi non-negative (see Section 6 for example applications). Lastly, indefinite quadratic functions of the form f(x) = 12x\n>Hx + h>x with all off-diagonal entries of H non-positive are examples of submodular but non-convex/nonconcave functions. Continuous submodularity is preserved under\n4. Notice that an equilavent definition of (1) is that \u2200x \u2208 X , \u2200i 6= j and ai, aj \u2265 0 s.t. xi + ai \u2208 Xi, xj + aj \u2208 Xj , it holds f(x+ ai\u03c7i) + f(x+ aj\u03c7j) \u2265 f(x) + f(x+ ai\u03c7i + aj\u03c7j). With ai and aj approaching zero, one get (2).\nvarious operations, e.g., the sum of two submodular continuous functions is submodular, a submodular continuous function multiplied by a positive scalar is still submodular. Interestingly, characterizations of submodular continuous functions are in correspondence to those of convex functions, which are summarized in Table 1.\nIn the remainder of this section, we introduce useful properties of submodular continuous functions. First of all, we generalize the DR property (which was introduced when studying set and integer-lattice functions) to general functions defined over X . It will soon be clear that the DR property defines a subclass of submodular functions.\nDefinition 1 (DR property and DR-submodular functions). A function f(\u00b7) defined over X satisfies the diminishing returns (DR) property if \u2200a \u2264 b \u2208 X , \u2200i \u2208 E, \u2200k \u2208 R+ s.t. (k\u03c7i+a) and (k\u03c7i+b) are still in X , it holds,\nf(k\u03c7i + a)\u2212 f(a) \u2265 f(k\u03c7i + b)\u2212 f(b).\nThis function f(\u00b7) is called a DR-submodular5 function.\nOne immediate observation is that for a differentiable DR-submodular continuous function f(\u00b7), we have that \u2200a \u2264 b \u2208 X , \u2207f(a) \u2265 \u2207f(b), i.e., the gradient \u2207f(\u00b7) is an antitone mapping from Rn to Rn. Recently, the DR property is explored by Eghbali and Fazel (2016) to achieve the worst-case competitive ratio for an online concave maximization problem. The DR property is also closely related to a sufficient condition on a concave function g(\u00b7) (Bilmes and Bai, 2017, Section 5.2), to ensure submodularity of the corresponding set function generated by giving g(\u00b7) boolean input vectors.\nIt is well known that for set functions, the DR property is equivalent to submodularity, while for integer-lattice functions, submodularity does not in general imply the DR property (Soma et al., 2014; Soma and Yoshida, 2015a,b). However, it was unclear whether there exists a diminishingreturn-style characterization that is equivalent to submodularity of integer-lattice functions. In this work we give a positive answer to this open problem by proposing the weak diminishing returns (weak DR) property for general functions defined over X , and prove that weak DR gives a sufficient and necessary condition for a general function to be submodular.\nDefinition 2 (weak DR property). A function f(\u00b7) defined over X has the weak diminishing returns property (weak DR) if \u2200a \u2264 b \u2208 X , \u2200i \u2208 E s.t. ai = bi,\u2200k \u2208 R+ s.t. (k\u03c7i + a) and (k\u03c7i + b) are still in X , it holds,\nf(k\u03c7i + a)\u2212 f(a) \u2265 f(k\u03c7i + b)\u2212 f(b). (3)\nThe following proposition shows that for all set functions, as well as integer-lattice and continuous functions, submodularity is equivalent to the weak DR property.\n5. Note that DR property implies submodularity and thus the name \u201cDR-submodular\u201d contains redundant information about submodularity of a function, but we keep this terminology to be consistent with previous literature on submodular integer-lattice functions.\nProposition 1 (submodularity) \u21d4 (weak DR). A function f(\u00b7) defined over X is submodular iff it satisfies the weak DR property.\nAll of the proofs can be found in Appendix A. Given Proposition 1, one can treat weak DR as the first order condition of submodularity: Notice that for a differentiable continuous function f(\u00b7) with the weak DR property, we have that \u2200a \u2264 b \u2208 X , \u2200i \u2208 E s.t. ai = bi, it holds \u2207if(a) \u2265 \u2207if(b), i.e., \u2207f(\u00b7) is a weak antitone mapping. Now we show that the DR property is stronger than the weak DR property, and the class of DR-submodular functions is a proper subset of that of submodular functions, as indicated by Figure 1.\nProposition 2 (submodular/weak DR) + (coordinate-wise concave) \u21d4 (DR). A function f(\u00b7) defined over X satisfies the DR property iff f(\u00b7) is submodular and coordinate-wise concave, where the coordinate-wise concave property is defined as: \u2200x \u2208 X , \u2200i \u2208 E, \u2200k, l \u2208 R+ s.t. (k\u03c7i + x), (l\u03c7i + x), ((k + l)\u03c7i + x) are still in X , it holds,\nf(k\u03c7i + x)\u2212 f(x) \u2265 f((k + l)\u03c7i + x)\u2212 f(l\u03c7i + x),\nor equivalently (if twice differentiable) \u2202 2f(x) \u2202x2i \u2264 0, \u2200i \u2208 E.\nProposition 2 shows that a twice differentiable function f(\u00b7) is DR-submodular iff \u2200x \u2208 X , \u2202 2f(x) \u2202xi\u2202xj\n\u2264 0,\u2200i, j \u2208 E, which does not necessarily imply the concavity of f(\u00b7). Given Proposition 2, we also have the characterizations of DR-submodular continuous functions, which are summarized in Table 2."}, {"heading": "4. Maximizing monotone DR-submodular continuous functions", "text": "In this section, we present an algorithm for maximizing a monotone DR-submodular continuous function subject to a general down-closed convex constraint, i.e., maxx\u2208Pu f(x). A down-closed convex set Pu is a convex set P associated with a lower bound u \u2208 P, such that 1) \u2200y \u2208 P, u \u2264 y; and 2) \u2200y \u2208 P, x \u2208 Rn, u \u2264 x \u2264 y implies x \u2208 P. Without loss of generality, we assume P lies in the postitive orthant and has the lower bound 0, since otherwise we can always define a new set P \u2032 = {x | x = y\u2212u,y \u2208 P} in the positive orthant, and a corresponding monotone DR-submdular function f \u2032(x) := f(x+ u).\nMaximizing a monotone DR-submodular function over a down-closed convex constraint has many real-world applications, e.g., influence maximization with continuous assignments and sensor energy management. In particular, for influence maximization (see Section 6), the constraint is a down-closed polytope in the positive orthant: P = {x \u2208 Rn | 0 \u2264 x \u2264 u\u0304,Ax \u2264 b, u\u0304 \u2208 Rn+,A \u2208 Rm\u00d7n+ , b \u2208 Rm+}. We start with the following hardness result:\nAlgorithm 1: Frank-Wolfe variant for monotone DR-submodular function maximization\nInput: maxx\u2208P f(x), P is a down-closed convex set in the positive orthant with lower bound 0, prespecified stepsize \u03b3 \u2208 (0, 1]\n1 x0 \u2190 0, t\u2190 0, k \u2190 0; //k : iteration index 2 while t < 1 do 3 find vk s.t. \u3008vk,\u2207f(xk)\u3009 \u2265 \u03b1maxv\u2208P\u3008v,\u2207f(xk)\u3009 \u2212 12\u03b4L; //L > 0 is the Lipschitz parameter, \u03b1 \u2208 (0, 1] is the mulplicative error level, \u03b4 \u2208 [0, \u03b4\u0304] is the additive error level 4 find stepsize \u03b3k \u2208 (0, 1], e.g., \u03b3k \u2190 \u03b3; set \u03b3k \u2190 min{\u03b3k, 1\u2212 t}; 5 xk+1 \u2190 xk + \u03b3kvk, t\u2190 t+ \u03b3k, k \u2190 k + 1; 6 Return xK ; //assuming there are K iterations in total\nProposition 3. The problem of maximizing a monotone DR-submodular continuous function subject to a general down-closed polytope constraint is NP-hard. For any > 0, it cannot be approximated in polynomial time within a ratio of (1 \u2212 1/e + ) (up to low-order terms), unless RP = NP.\nDue to the NP-hardness of converging to the global optimum, in the following by \u201cconvergence\u201d we mean converging to a point near the global optimum. The algorithm is a generalization of the continuous greedy algorithm of Vondra\u0301k (2008) for maximizing a smooth submodular function, and related to the convex Frank-Wolfe algorithm (Frank and Wolfe, 1956; Jaggi, 2013) for minimizing a convex function. We summarize the Frank-Wolfe variant in Algorithm 1. In each iteration k, the algorithm uses the linearization of f(\u00b7) as a surrogate, and moves in the direction of the maximizer of this surrogate function, i.e. vk = arg maxv\u2208P\u3008v,\u2207f(xk)\u3009. Intuitively, we search for the direction in which we can maximize the improvement in the function value and still remain feasible. Finding such a direction requires maximizing a linear objective at each iteration. Meanwhile, it eliminates the need for projecting back to the feasible set in each iteration, which is an essential step for methods such as projected gradient ascent. The algorithm uses stepsize \u03b3k to update the solution in each iteration, which can be simply a prespecified value \u03b3. Note that the Frank-Wolfe variant can tolerate both multiplicative error \u03b1 and additive error \u03b4 when solving the subproblem (Step 3 of Algorithm 1). Setting \u03b1 = 1 and \u03b4 = 0, we recover the error-free case.\nNotice that the Frank-Wolfe variant in Algorithm 1 is different from the convex Frank-Wolfe algorithm mainly in the update direction being used: For Algorithm 1, the update direction (in Step 5) is vk, while for convex Frank-Wolfe it is vk \u2212 xk, i.e., xk+1 \u2190 xk + \u03b3k(vk \u2212 xk). The reason of this difference will soon be clear by exploring the property of DR-submodular functions. Specifically, DR-submodular functions are non-convex/non-concave in general, however, there is certain connection between DR-submodularity and concavity.\nProposition 4. A DR-submodular continuous function f(\u00b7) is concave along any non-negative direction, and any non-positive direction.\nProposition 4 implies that the univariate auxiliary function gx,v(\u03be) := f(x+\u03bev), \u03be \u2208 R+,v \u2208 RE+ is concave. As a result, the Frank-Wolfe variant can follow a concave direction at each step, which is the main reason it uses vk as the update direction (notice that vk is a non-negative direction).\nTo derive the approximation guarantee, we need assumptions on the non-linearity of f(\u00b7) over the domain P, which closely corresponds to a Lipschitz assumption on the derivative of gx,v(\u00b7). For\na gx,v(\u00b7) with L-Lipschitz continuous derivative in [0, 1] (L > 0), we have,\n\u2212L 2 \u03be2 \u2264 gx,v(\u03be)\u2212 gx,v(0)\u2212 \u03be\u2207gx,v(0) = f(x+ \u03bev)\u2212 f(x)\u2212 \u3008\u03bev,\u2207f(x)\u3009,\u2200\u03be \u2208 [0, 1]. (4)\nTo prove the approximation guarantee, we first derive the following lemma.\nLemma 1. The output solution xK \u2208 P. Assuming x\u2217 to be the optimal solution, one has,\n\u3008vk,\u2207f(xk)\u3009 \u2265 \u03b1[f(x\u2217)\u2212 f(xk)]\u2212 1 2 \u03b4L, \u2200k = 0, \u00b7 \u00b7 \u00b7 ,K \u2212 1. (5)\nTheorem 1 (Approximation guarantee). For error levels \u03b1 \u2208 (0, 1], \u03b4 \u2208 [0, \u03b4\u0304], with K iterations, Algorithm 1 outputs xK \u2208 P s.t.,\nf(xK) \u2265 (1\u2212 e\u2212\u03b1)f(x\u2217)\u2212 L 2 \u2211K\u22121 k=0 \u03b32k \u2212 L\u03b4 2 + e\u2212\u03b1f(0). (6)\nTheorem 1 gives the approximation guarantee for arbitrary chosen stepsize \u03b3k. By observing that \u2211K\u22121 k=0 \u03b3k = 1 and \u2211K\u22121 k=0 \u03b3 2 k \u2265 K\u22121 (see the proof in Appendix B.5), with constant stepsize, we obtain the following \u201ctightest\u201d approximation bound,\nCorollary 1. For a fixed number of iterations K, and constant stepsize \u03b3k = \u03b3 = K \u22121, Algorithm"}, {"heading": "1 provides the following approximation guarantee:", "text": "f(xK) \u2265 (1\u2212 e\u2212\u03b1)f(x\u2217)\u2212 L 2K \u2212 L\u03b4 2 + e\u2212\u03b1f(0).\nCorollary 1 implies that with a constant stepsize \u03b3, 1) when \u03b3 \u2192 0 (K \u2192\u221e), Algorithm 1 will output the solution with the worst-case guarantee (1\u2212 1/e)f(x\u2217) in the error-free case if f(0) = 0; and 2) The Frank-Wolfe variant has a sub-linear convergence rate for monotone DR-submodular maximization over a down-closed convex constraint. Time complexity. It can be seen that when using a constant stepsize, Algorithm 1 needs O(1 ) iterations to get -close to the worst-case guarantee (1\u2212 e\u22121)f(x\u2217) in the error-free case. When P is a polytope in the positive orthant, one iteration of Algorithm 1 costs approximately the same as solving a positive LP, for which a nearly-linear time solver exists (Allen-Zhu and Orecchia, 2015)."}, {"heading": "5. Maximizing non-monotone submodular continuous functions", "text": "The problem of maximizing a general non-monotone submodular continuous function under box constraints6, i.e., maxx\u2208[u,u\u0304]\u2286X f(x), has various real-world applications, including revenue maximization with continuous assignments, multi-resolution summarization, etc, as discussed in Section 6. The following proposition shows the NP-hardness of the problem.\nProposition 5. The problem of maximizing a generally non-monotone submodular continuous function subject to box constraints is NP-hard. Furthermore, there is no (1/2 + )-approximation \u2200 > 0, unless RP = NP.\nWe now describe our algorithm for maximizing a non-monotone submodular continuous function subject to box constraints. It provides a 1/3-approximation, is inspired by the double greedy algorithm of Buchbinder et al. (2012) and Gottschalk and Peis (2015), and can be viewed as a procedure performing coordinate-ascent on two solutions.\nAlgorithm 2: DoubleGreedy algorithm for maximizing non-monotone submodular continuous functions\nInput: maxx\u2208[u,u\u0304] f(x), f is generally non-monotone, f(u) + f(u\u0304) \u2265 0 1 x0 \u2190 u, y0 \u2190 u\u0304; 2 for k = 1\u2192 n do 3 find u\u0302a s.t. f(x\nk\u22121|xk\u22121ek \u2190u\u0302a) \u2265 maxua\u2208[uek ,u\u0304ek ] f(x k\u22121|xk\u22121ek \u2190ua)\u2212 \u03b4,\n\u03b4a \u2190 f(xk\u22121|xk\u22121ek \u2190u\u0302a)\u2212 f(x k\u22121); //\u03b4 \u2208 [0, \u03b4\u0304] is the additive error level\n4 find u\u0302b s.t. f(y k\u22121|yk\u22121ek \u2190u\u0302b) \u2265 maxub\u2208[uek ,u\u0304ek ] f(y k\u22121|yk\u22121ek \u2190ub)\u2212 \u03b4, \u03b4b \u2190 f(yk\u22121|yk\u22121ek \u2190u\u0302b)\u2212 f(y k\u22121); 5 If \u03b4a \u2265 \u03b4b: xk \u2190 (xk\u22121|xk\u22121ek \u2190u\u0302a), y k \u2190 (yk\u22121|yk\u22121ek \u2190u\u0302a) ; 6 Else: yk \u2190 (yk\u22121|yk\u22121ek \u2190u\u0302b), x k \u2190 (xk\u22121|xk\u22121ek \u2190u\u0302b); 7 Return xn (or yn); //note that xn = yn\nWe view the process as two particles starting from x0 = u and y0 = u\u0304, and following a certain \u201cflow\u201d toward each other. The pseudo-code is given in Algorithm 2. We proceed in n rounds that correspond to some arbitrary order of the coordinates. At iteration k, we consider solving a one-dimensional (1-D) subproblem over coordinate ek for each particle, and moving the particles based on the calculated local gains toward each other. Formally, for a given coordinate ek, we solve a 1-D subproblem to find the value of the first solution x along coordinate ek that maximizes f , i.e., u\u0302a = arg maxua f(x k\u22121|xk\u22121ek \u2190ua) \u2212 f(x k\u22121), and calculate its marginal gain \u03b4a. We then solve another 1-D subproblem to find the value of the second solution y along coordinate ek that maximizes f , i.e., u\u0302b = arg maxub f(y k\u22121|yk\u22121ek \u2190ub) \u2212 f(y k\u22121), and calculate the second marginal gain \u03b4b. We decide by comparing the two marginal gains. If changing xek to be u\u0302a has a larger local benefit, we change both xek and yek to be u\u0302a. Otherwise, we change both of them to be u\u0302b. After n iterations the particles should meet at point xn = yn, which is the final solution. Note that Algorithm 2 can tolerate additive error \u03b4 in solving each 1-D subproblem (Steps 3, 4).\nWe would like to emphasize that the assumptions required by DoubleGreedy are submodularity of f , f(u) + f(u\u0304) \u2265 0 and the (approximate) solvability of the 1-D subproblem. For proving the approximation guarantee, the idea is to bound the loss in the objective value from the assumed optimal objective value between every two consecutive steps, which is then used to bound the maximum loss after n iterations.\nTheorem 2. Assuming the optimal solution to be x\u2217, the output of Algorithm 2 has function value no less than 13f(x \u2217)\u2212 4n3 \u03b4, where \u03b4 \u2208 [0, \u03b4\u0304] is the additive error level for each 1-D subproblem.\nTime complexity. It can be seen that the time complexity of Algorithm 2 is O(n \u2217 cost 1D), where cost 1D is the cost of solving the 1-D subproblem. Solving a 1-D subproblem is usually very cheap. For non-convex/non-concave quadratic programming it has a closed form solution."}, {"heading": "6. Examples of submodular continuous objective functions", "text": "In this part, we discuss several concrete problem instances with their corresponding submodular continuous objective functions.\n6. It is also called \u201cunconstrained\u201d maximization in the combinatorial optimization community, since the domain X itself is also a box. Note that the box can be in the negative orthant here.\nExtensions of submodular set functions. The multilinear extension (Ca\u0306linescu et al., 2007) and softmax extension (Gillenwater et al., 2012) are special cases of DR-submodular functions, that are extensively used for submodular set function maximization. The Lova\u0301sz extension (Lova\u0301sz, 1983) used for submodular set function minimization is both submodular and convex (see Appendix A in Bach (2015)).\nNon-convex/non-concave quadratic programming (NQP). NQP problem of the form f(x) = 1 2x >Hx+h>x+c under linear constraints naturally arises in many applications, including scheduling (Skutella, 2001), inventory theory, and free boundary problems. A special class of NQP is the submodular NQP (the minimization of which was studied in Kim and Kojima (2003)), in which all off-diagonal entries of H are required to be non-positive. In this work, we mainly use submodular NQPs as synthetic functions for both monotone DR-submodular maximization and non-monotone submodular maximization.\nOptimal budget allocation with continuous assignments. Optimal budget allocation is a special case of the influence maximization problem. It can be modeled as a bipartite graph (S, T ;W ), where S and T are collections of advertising channels and customers, respectively. The edge weight, pst \u2208 W , represents the influence probability of channel s to customer t. The goal is to distribute the budget (e.g., time for a TV advertisement, or space of an inline ad) among the source nodes, and to maximize the expected influence on the potential customers (Soma et al., 2014; Hatano et al., 2015). The total influence of customer t from all channels can be modeled by a proper monotone DR-submodular function It(x), e.g., It(x) = 1\u2212 \u220f (s,t)\u2208W (1\u2212 pst) xs where x \u2208 RS+ is the budget assignment among the advertising channels. For a set of k advertisers, let xi \u2208 RS+ to be the budget assignment for advertiser i, and x := [x1, \u00b7 \u00b7 \u00b7 ,xk] denote the assignments for all the advertisers. The overall objective is,\ng(x) = \u2211k\ni=1 \u03b1if(x\ni) with f(xi) := \u2211\nt\u2208T It(x\ni), 0 \u2264 xi \u2264 u\u0304i,\u2200i = 1, \u00b7 \u00b7 \u00b7 , k (7)\nwhich is monotone DR-submodular. A concrete application is for search marketing advertiser bidding, in which vendors bid for the right to appear alongside the results of different search keywords. Here, xis is the volume of advertising space allocated to the advertiser i to show his ad alongside query keyword s. The search engine company needs to distribute the budget (advertising space) to all vendors to maximize their influence on the customers, while respecting various constraints. For example, each vendor has a specified budget limit for advertising, and the ad space associated with each search keyword can not be too large. All such constraints can be formulated as a down-closed polytope P, hence the Frank-Wolfe variant can be used to find an approximate solution for the problem maxx\u2208P g(x). Note that one can flexibly add regularizers in designing It(x\ni) as long as it remains monotone DR-submodular. For example, adding separable regularizers of the form\u2211\ns \u03c6(x i s) does not change the off-diagonal entries of the Hessian, and hence maintains submodu-\nlarity. Alternatively, bounding the second-order derivative of \u03c6(xis) ensures DR-submodularity.\nRevenue maximization with continuous assignments. In viral marketing, sellers choose a small subset of buyers to give them some product for free, to trigger a cascade of further adoptions through \u201cword-of-mouth\u201d effects, in order to maximize the total revenue (Hartline et al., 2008). For some products (e.g., software), the seller usually gives away the product in the form of a trial, to be used for free for a limited time period. In this task, except for deciding whether to choose a user or not, the sellers also need to decide how much the free assignment should be, in which the assignments should be modeled as continuous variables. We call this problem revenue maximization with continuous assignments. Assume there are q products and n buyers/users, let xi \u2208 Rn+ to be\nthe assignments of product i to the n users, let x := [x1, \u00b7 \u00b7 \u00b7 ,xq] denote the assignments for the q products. The revenue can be modelled as g(x) = \u2211q i=1 f(x i) with\nf(xi) := \u03b1i \u2211\ns:xis=0 Rs(x\ni) + \u03b2i \u2211\nt:xit 6=0 \u03c6(xit)+ \u03b3i \u2211 t:xit 6=0 R\u0304t(x i), 0 \u2264 xi \u2264 u\u0304i, (8)\nwhere xit is the assignment of product i to user t for free, e.g., the amount of free trial time or the amount of the product itself. Rs(x\ni) models revenue gain from user s who did not receive the free assignment. It can be some non-negative, non-decreasing submodular function. \u03c6(xit) models revenue gain from user t who received the free assignment, since the more one user tries the product, the more likely he/she will buy it after the trial period. R\u0304t(x\ni) models the revenue loss from user t (in the free trial time period the seller cannot get profits), which can be some non-positive, non-increasing submodular function. With \u03b2=\u03b3=0, we recover the classical model of Hartline et al. (2008). For products with continuous assignments, usually the cost of the product does not increase with its amount, e.g., the product as a software, so we only have the box constraint on each assignment. The objective in Equation 8 is generally non-concave/non-convex, and nonmonotone submodular (see Appendix D for more details), thus can be approximately maximized by the proposed DoubleGreedy algorithm.\nLemma 2. If Rs(x i) is non-decreasing submodular and R\u0304t(x i) is non-increasing submodular, then f(xi) in Equation 8 is submodular.\nSensor energy management. For cost-sensitive outbreak detection in sensor networks (Leskovec et al., 2007), one needs to place sensors in a subset of locations selected from all the possible locations E, to quickly detect a set of contamination events V , while respecting the cost constraints of the sensors. For each location e \u2208 E and each event v \u2208 V , a value t(e, v) is provided as the time it takes for the placed sensor in e to detect event v. Soma and Yoshida (2015a) considered the sensors with discrete energy levels. It is also natural to model the energy levels of sensors to be a continuous variable x \u2208 RE+. For a sensor with energy level xe, the success probability it detects the event is 1\u2212(1\u2212p)xe , which models that by spending one unit of energy one has an extra chance of detecting the event with probability p. In this model, beyond deciding whether to place a sensor or not, one also needs to decide the optimal energy levels. Let t\u221e = maxe\u2208E,v\u2208V t(e, v), let ev be the first sensor that detects event v (ev is a random variable). One can define the objective as the expected detection time that could be saved,\nf(x) := Ev\u2208V Eev [t\u221e \u2212 t(ev, v)], (9)\nwhich is a monotone DR-submodular function. Maximizing f(x) w.r.t. the cost constraints pursues the goal of finding the optimal energy levels of the sensors, to maximize the expected detection time that could be saved.\nMulti-resolution summarization. Suppose we have a collection of items, e.g., images E = {e1, \u00b7 \u00b7 \u00b7 , en}. Our goal is to extract a representative summary, where representativeness is defined w.r.t. a submodular set function F : 2E \u2192 R. However, instead of returning a single set, our goal is to obtain summaries at multiple levels of detail or resolution. One way to achieve this goal is to assign each item ei a nonnegative score xi. Given a user-tunable threshold \u03c4 , the resulting summary S\u03c4 = {ei|xi \u2265 \u03c4} is the set of items with scores exceeding \u03c4 . Thus, instead of solving the discrete problem of selecting a fixed set S, we pursue the goal to optimize over the scores, e.g., to use the following submodular continuous function,\nf(x) = \u2211\ni\u2208E \u2211 j\u2208E \u03c6(xj)si,j \u2212 \u2211 i\u2208E \u2211 j\u2208E xixjsi,j , (10)\nwhere si,j \u2265 0 is the similarity between items i, j, and \u03c6(\u00b7) is a non-decreasing concave function.\nFacility location. The classical discrete facility location problem can be naturally generalized to the continuous case where the scale of a facility is determined by a continuous value in interval [0, u\u0304]. For a set of facilities E, let x \u2208 RE+ be the scale of all facilities. The goal is to decide how large each facility should be in order to optimally serve a set T of customers. For a facility s of scale xs, let pst(xs) be the value of service it can provide to customer t \u2208 T , where pst(xs) is a normalized monotone function (pst(0) = 0). Assuming each customer chooses the facility with highest value, the total service provided to all customers is f(x) = \u2211 t\u2208T maxs\u2208E pst(xs). It can be shown that f is monotone submodular.\nOther applications. Many discrete submodular problems can be naturally generalized to the continuous setting with submodular continuous objectives. The maximum coverage problem and the problem of text summarization with submodular objectives are among the examples (Lin and Bilmes, 2010). We defer further details to Appendix E."}, {"heading": "7. Experimental results", "text": "We compare the performance of our proposed algorithms, the Frank-Wolfe variant and DoubleGreedy, with the following baselines: a) Random: uniformly sample ks solutions from the constraint set using the hit-and-run sampler (Kroese et al., 2013), and select the best one. For the constraint set as a very high-dimensional polytope, this approach is computationally very expensive. To accelerate sampling from a high-dimensional polytope, we also use b) RandomCube: randomly sample ks solutions from the hypercube, and decrease their elements until they are inside the polytope. In addition we consider c) ProjGrad: projected gradient ascent with an empirically tuned step size; and d) SingleGreedy: for non-monotone submodular functions maximization over a box constraint, we greedily increase each coordinate, as long as it remains feasible. This approach is similar to the coordinate ascent method. In all of the experiments, we use random order of coordinates for DoubleGreedy. We use constant step size for the Frank-Wolfe variant since it gives the tightest approximation guarantee (see Corollary 1). The performance of the methods are evaluated for the following tasks."}, {"heading": "7.1 Results for monotone maximization", "text": "Monotone DR-submodular NQP. We randomly generated monotone DR-submodular NQP functions of the form f(x) = 12x\n>Hx+h>x, where H \u2208 Rn\u00d7n is a random matrix with uniformly distributed non-positive entries in [\u2212100, 0], n = 100. We further generated a set of m = 50 linear constraints to construct the positive polytope P = {x \u2208 Rn|Ax \u2264 b, 0 \u2264 x \u2264 u\u0304}, where A has uniformly distributed entries in [0, 1], b = 1, u\u0304 = 1. To make the gradient non-negative, we set h = \u2212H>u\u0304. We empirically tuned step size \u03b1p for ProjGrad and ran all algorithms for 50 iterations. Figure 2a shows the utility obtained by the Frank-Wolfe variant v.s. the iteration index for 4 function instances with different values of b. Figure 2b shows the average utility obtained by different algorithms with increasing values of b. The result is the average of 20 repeated experiments. For ProjGrad, we plotted the curves for three different values of \u03b1p. One can observe that the performance of ProjGrad fluctuates with different step sizes. With the best-tuned step size, ProjGrad performs close to the Frank-Wolfe variant.\nOptimal budget allocation. As our real-world experiments, we used the Yahoo! Search Marketing Advertiser Bidding Data7, which consists of 1,000 search keywords, 10,475 customers and 52,567 edges. We considered the frequency of (keyword, customer) pairs to estimate the influence\n7. https://webscope.sandbox.yahoo.com/catalog.php?datatype=a\nprobabilities, and used the average of the bidding prices to put a limit on the budget of each advertiser. Since the Random sampling was too slow, we compared with the RandomCube method. Figures 2c and 2d show the value of the utility function (influence) when varying the budget on volume of ads and on budget of advertisers, respectively. Again, we observe that the Frank-Wolfe variant outperforms the other baselines, and the performance of ProjGrad highly depends on the choice of the step size."}, {"heading": "7.2 Results for non-monotone maximization", "text": "Non-monotone submodular NQP. We randomly generated non-monotone submodular NQP functions of the form f(x) = 12x\n>Hx+h>x+c, where H \u2208 Rn\u00d7n is a sparse matrix with uniformly distributed non-positive off-diagonal entries in [\u221210, 0], n = 1000. We considered a matrix for which\naround 50% of the eigenvalues are positive and the other 50% are negative. We set h = \u22120.2\u2217H>u\u0304 to make f(x) non-monotone. We then selected a value for c such that f(0)+f(u\u0304) \u2265 0. ProjGrad was executed for n iterations, with empirically tuned step sizes. For the Random method we set ks = 1, 000. Figure 3a shows the utility of the two intermediate solutions maintained by DoubleGreedy. One can observe that they both increase in each iteration. Figure 3b shows the values of the utility function for varying upper bound u\u0304. The result is the average over 20 repeated experiments. We can see that DoubleGreedy has strong approximation performance, while ProjGrad\u2019s results depend on the choice of the step size. With carefully hand-tuned step size, its performance is comparable to DoubleGreedy.\nRevenue maximization. Without loss of generality, we considered maximizing the revenue from selling one product (corresponding to q = 1, see Appendix D for more details on this model). It\ncan be observed that the objective in Equation 8 is generally non-smooth and discontinuous at any point x which contains the element of 0. Since the subdifferential can be empty, we cannot use the subgradient-based method and could not compare with ProjGrad. We performed our experiments on the top 500 largest communities of the YouTube social network8 consisting of 39,841 nodes and 224,235 edges. The edge weights were assigned according to a uniform distribution U(0, 1). See Figure 3c, 3d for an illustration of revenue for varying upper bound (u\u0304) and different combinations of the parameters (\u03b1, \u03b2, \u03b3) in the model (Equation 8). For different values of the upper bound, DoubleGreedy outperforms the other baselines, while SingleGreedy maintaining only one intermediate solution obtained a lower utility than DoubleGreedy."}, {"heading": "8. Conclusion", "text": "In this paper, we characterized submodular continuous functions, and proposed two approximation algorithms to efficiently maximize them. In particular, for maximizing monotone DR-submodular continuous functions subject to general down-closed convex constraints, we proposed a (1 \u2212 1/e)approximation algorithm, and for maximizing non-monotone submodular continuous functions subject to a box constraint, we proposed a 1/3-approximation algorithm. We demonstrate the effectiveness of our algorithms through a set of experiments on real-world applications, including budget allocation, revenue maximization, and non-convex/non-concave quadratic programming, and show that our proposed methods outperform the baselines in all the experiments. This work demonstrates that submodularity can ensure guaranteed optimization in the continuous setting for problems with (generally) non-convex/non-concave objectives."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Martin Jaggi for valuable discussions. This research was partially supported by ERC StG 307036 and the Max Planck ETH Center for Learning Systems.\n8. http://snap.stanford.edu/data/com-Youtube.html"}, {"heading": "Appendix", "text": ""}, {"heading": "A. Proofs of properties of submodular continuous functions", "text": "Since Xi is a compact subset of R, we denote its lower bound and upper bound to be ui and u\u0304i, respectively in this section.\nA.1 Alternative formulation of the weak DR property\nFirst of all, we will prove that weak DR has the following alternative formulation, which will be used to prove Proposition 1.\nLemma 3 (Alternative formulation of weak DR). The weak DR property (Equation 3, denoted as Formulation I) has the following equilvalent formulation (Equation 11, denoted as Formulation II): \u2200a \u2264 b \u2208 X , \u2200i \u2208 {i\u2032|ai\u2032 = bi\u2032 = ui\u2032}, \u2200k\u2032 \u2265 l\u2032 \u2265 0 s.t. (k\u2032\u03c7i + a), (l\u2032\u03c7i + a), (k\u2032\u03c7i + b) and (l\u2032\u03c7i + b) are still in X , the following inequality is satisfied,\nf(k\u2032\u03c7i + a)\u2212 f(l\u2032\u03c7i + a) \u2265 f(k\u2032\u03c7i + b)\u2212 f(l\u2032\u03c7i + b) (Formulation II) (11)\nProof. Let D1 = {i|ai = bi = ui}, D2 = {i|ui < ai = bi < u\u0304i}, and D3 = {i|ai = bi = u\u0304i}. 1) Formulation II \u21d2 Formulation I When i \u2208 D1, set l\u2032 = 0 in Formulation II one can get f(k\u2032\u03c7i+a)\u2212f(a) \u2265 f(k\u2032\u03c7i+b)\u2212f(b). When i \u2208 D2, \u2200k \u2265 0, let l\u2032 = ai \u2212 ui = bi \u2212 ui > 0, k\u2032 = k + l\u2032 = k + (ai \u2212 ui), and let a\u0304 = (a|ai\u2190ui), b\u0304 = (b|bi\u2190ui). It is easy to see that a\u0304 \u2264 b\u0304, and a\u0304i = b\u0304i = ui. Then from Formulation II,\nf(k\u2032\u03c7i + a\u0304)\u2212 f(l\u2032\u03c7i + a\u0304) = f(k\u03c7i + a)\u2212 f(a) \u2265 f(k\u2032\u03c7i + b\u0304)\u2212 f(l\u2032\u03c7i + b\u0304) = f(k\u03c7i + b)\u2212 f(b).\nWhen i \u2208 D3, Equation 3 holds trivially. The above three situations proves the Formulation I. 2) Formulation II \u21d0 Formulation I \u2200a \u2264 b, \u2200i \u2208 D1, one has ai = bi = ui. \u2200k\u2032 \u2265 l\u2032 \u2265 0, let a\u0302 = l\u2032\u03c7i + a, b\u0302 = l\u2032\u03c7i + b, let k = k\u2032 \u2212 l\u2032 \u2265 0, it can be verified that a\u0302 \u2264 b\u0302 and a\u0302i = b\u0302i, from Formulation I,\nf(k\u03c7i + a\u0302)\u2212 f(a\u0302) = f(k\u2032\u03c7i + a)\u2212 f(l\u2032\u03c7i + a) \u2265f(k\u03c7i + b\u0302)\u2212 f(b\u0302) = f(k\u2032\u03c7i + b)\u2212 f(l\u2032\u03c7i + b)\nwhich proves Formulation II."}, {"heading": "A.2 Proof of Proposition 1", "text": "Proof. 1) submodularity \u21d2 weak DR: Let us prove the Formulation II (Equation 11) of weak DR, which is, \u2200a \u2264 b \u2208 X , \u2200i \u2208 {i\u2032|ai\u2032 = bi\u2032 = ui\u2032},\u2200k\u2032 \u2265 l\u2032 \u2265 0, the following inequality holds,\nf(k\u2032\u03c7i + a)\u2212 f(l\u2032\u03c7i + a) \u2265 f(k\u2032\u03c7i + b)\u2212 f(l\u2032\u03c7i + b).\nAnd f is a submodular function iff \u2200x,y \u2208 X , f(x) + f(y) \u2265 f(x \u2228 y) + f(x \u2227 y), so f(y) \u2212 f(x \u2227 y) \u2265 f(x \u2228 y)\u2212 f(x).\nNow \u2200a \u2264 b \u2208 X , one can set x = l\u2032\u03c7i + b and y = k\u2032\u03c7i + a. It can be easily verified that x\u2227y = l\u2032\u03c7i +a and x\u2228y = k\u2032\u03c7i + b. Substituting all the above equalities into f(y)\u2212 f(x\u2227y) \u2265 f(x \u2228 y)\u2212 f(x) one can get f(k\u2032\u03c7i + a)\u2212 f(l\u2032\u03c7i + a) \u2265 f(k\u2032\u03c7i + b)\u2212 f(l\u2032\u03c7i + b).\n2) submodularity \u21d0 weak DR: Let us use Formulation I (Equation 3) of weak DR to prove the submodularity property. \u2200x,y \u2208 X , let D := {e1, \u00b7 \u00b7 \u00b7 , ed} to be the set of elements for which ye > xe, let kei := yei \u2212xei . Now set a0 := x \u2227 y, b0 := x and ai = (ai\u22121|ai\u22121ei \u2190yei) = kei\u03c7i + a i\u22121, bi = (bi\u22121|bi\u22121ei \u2190yei) = kei\u03c7i + b i\u22121, for i = 1, \u00b7 \u00b7 \u00b7 , d. One can verify that ai \u2264 bi, aiei\u2032 = b i ei\u2032\nfor all i\u2032 \u2208 D, i = 0, \u00b7 \u00b7 \u00b7 , d, and that ad = y, bd = x \u2228 y.\nApplying Equation 3 of the weak DR property for i = 1, \u00b7 \u00b7 \u00b7 , d one can get\nf(ke1\u03c7e1 + a 0)\u2212 f(a0) \u2265 f(ke1\u03c7e1 + b0)\u2212 f(b0) f(ke2\u03c7e2 + a 1)\u2212 f(a1) \u2265 f(ke2\u03c7e2 + b1)\u2212 f(b1) \u00b7 \u00b7 \u00b7 f(ked\u03c7ed + a d\u22121)\u2212 f(ad\u22121) \u2265 f(ked\u03c7ed + b d\u22121)\u2212 f(bd\u22121).\nTaking a sum over all the above d inequalities, one can get\nf(ked\u03c7ed + a d\u22121)\u2212 f(a0) \u2265 f(ked\u03c7ed + b d\u22121)\u2212 f(b0)\u21d4 f(y)\u2212 f(x \u2227 y) \u2265 f(x \u2228 y)\u2212 f(x)\u21d4 f(x) + f(y) \u2265 f(x \u2228 y) + f(x \u2227 y),\nwhich proves the submodularity."}, {"heading": "A.3 Proof of Proposition 2", "text": "Proof. 1) submodular + coordinate-wise concave \u21d2 DR: From coordinate-wise concavity we have f(a + k\u03c7i) \u2212 f(a) \u2265 f(a + (bi \u2212 ai + k)\u03c7i) \u2212 f(a + (bi \u2212 ai)\u03c7i). Therefore, to prove DR it suffices to show that\nf(a+ (bi \u2212 ai + k)\u03c7i)\u2212 f(a+ (bi \u2212 ai)\u03c7i) \u2265 f(b+ k\u03c7i)\u2212 f(b). (12)\nLet x := b,y := (a + (bi \u2212 ai + k)\u03c7i), so x \u2227 y = (a + (bi \u2212 ai)\u03c7i),x \u2228 y = (b + k\u03c7i). From submodularity, one can see that inequality 12 holds.\n2) submodular + coordinate-wise concave \u21d0 DR: From DR property, the weak DR (Equation 3) property is implied, which equivalently proves the submodularity property. To prove coordinate-wise concavity, one just need to set b := a+ l\u03c7i, then it reads f(a+k\u03c7i)\u2212 f(a) \u2265 f(a+ (k + l)\u03c7i)\u2212 f(a+ l\u03c7i)."}, {"heading": "B. Proofs for the monotone DR-submodular continuous functions maximization", "text": ""}, {"heading": "B.1 Proof of Proposition 3", "text": "Proof. On a high level, the proof idea follows from the reduction from the problem of maximizing a monotone submodular set function subject to cardinality constraints.\nLet us denote \u03a01 as the problem of maximizing a monotone submodular set function subject to cardinality constraints, and \u03a02 as the problem of maximizing a monotone DR-submodular continuous function under general down-closed polytope constraints. Following Ca\u0306linescu et al. (2011),\nthere exist an algorithm A for \u03a01 that consists of a polynomial time computation in addition to polynomial number of subroutine calls to an algorithm for \u03a02. For details on A see the following.\nFirst of all, the multilinear extension (Ca\u0306linescu et al., 2007) of a monotone submodular set function is a monotone submodular continuous function, and it is coordinate-wise linear, thus falls into a special case of monotone DR-submodular continuous functions.\nSo the algorithm A can be: 1) Maximize the multilinear extension of the submodular set function over the matroid polytope associated with the cardinality constraint, which can be achieved by solving an instance of \u03a02. We call the solution obtained the fractional solution; 2) Round the fractional solution to a feasible integeral solution using polynomial time rounding technique in Ageev and Sviridenko (2004); Ca\u0306linescu et al. (2007) (called the pipage rounding). Thus we prove the reduction from \u03a01 to \u03a02.\nOur reduction algorithm A implies the NP-hardness and inapproximability of problem \u03a02. For the NP-hardness, because \u03a01 is well known to be NP-hard (Ca\u0306linescu et al., 2007; Feige, 1998), so \u03a02 is NP-hard as well. For the inapproximability: Assume there exists a polynomial algorithm B that can solve \u03a02 better than 1\u22121/e, then we can use B as the subroutine algorithm in the reduction, which implies that one can solve \u03a01 better than 1\u22121/e. Now we slightly adapt the proof of inapproximability on max-k-cover in Feige (1998), since max-k-cover is a special case of \u03a01. According to Theorem 5.3 in Feige (1998) and our reduction A , we have a reduction from approximating 3SAT-5 to problem \u03a02. Using the rest proof of Theorem 5.3 in Feige (1998), we reach the result that one cannot solve \u03a02 better than 1\u2212 1/e, unless RP = NP."}, {"heading": "B.2 Proof of Proposition 4", "text": "Proof. Consider a function g(\u03be) := f(x+ \u03bev\u2217), \u03be \u2265 0,v\u2217 \u2265 0. dg(\u03be)d\u03be = \u3008v \u2217,\u2207f(x+ \u03bev\u2217)\u3009.\ng(\u03be) is concave \u21d4\nd2g(\u03be)\nd\u03be2 = (v\u2217)>\u22072f(x+ \u03bev\u2217)v\u2217 = \u2211 i 6=j v\u2217i v \u2217 j\u22072ijf + \u2211 i (v\u2217i ) 2\u22072iif \u2264 0\nThe non-positiveness of \u22072ijf is ensured by submodularity of f(\u00b7), and the non-positiveness of \u22072iif results from the coordinate-wise concavity of f(\u00b7).\nThe proof of concavity along any non-positive direction is similar, which is omitted here."}, {"heading": "B.3 Proof of Lemma 1", "text": "Proof. It is easy to see that xK is a convex linear combination of points in P, so xK \u2208 P. Consider the point v\u2217 := (x\u2217\u2228x)\u2212x = (x\u2217\u2212x)\u22280 \u2265 0. Because v\u2217 \u2264 x\u2217 and P is down-closed, we get v\u2217 \u2208 P. By monotonicity, f(x+ v\u2217) = f(x\u2217 \u2228 x) \u2265 f(x\u2217). Consider the function g(\u03be) := f(x+ \u03bev\u2217), \u03be \u2265 0. dg(\u03be)d\u03be = \u3008v\n\u2217,\u2207f(x+ \u03bev\u2217)\u3009. From Proposition 4, g(\u03be) is concave, hence\ng(1)\u2212 g(0) = f(x+ v\u2217)\u2212 f(x) \u2264 dg(\u03be) d\u03be \u2223\u2223\u2223 \u03be=0 \u00d7 1 = \u3008v\u2217,\u2207f(x)\u3009.\nThen one can get\n\u3008v,\u2207f(x)\u3009 (a) \u2265 \u03b1\u3008v\u2217,\u2207f(x)\u3009 \u2212 1 2 \u03b4L \u2265 \u03b1(f(x+ v\u2217)\u2212 f(x))\u2212 1 2 \u03b4L \u2265 \u03b1(f(x\u2217)\u2212 f(x))\u2212 1 2 \u03b4L\nwhere (a) is from the selection rule in Step 3 of Algorithm 1."}, {"heading": "B.4 Proof of Theorem 1", "text": "Proof. From the Lipschitz continuous derivative assumption of g(\u00b7) (Equation 4): f(xk+1)\u2212 f(xk) = f(xk + \u03b3kvk)\u2212 f(xk)\n= g(\u03b3k)\u2212 g(0) \u2265 \u03b3k\u3008vk,\u2207f(xk)\u3009 \u2212 L\n2 \u03b32k (Lipschitz assumption in Equation 4)\n\u2265 \u03b3k\u03b1[f(x\u2217)\u2212 f(xk)]\u2212 1\n2 \u03b3k\u03b4L\u2212\nL 2 \u03b32k (Lemma 1)\nAfter rearrangement,\nf(xk+1)\u2212 f(x\u2217) \u2265 (1\u2212 \u03b1\u03b3k)[f(xk)\u2212 f(x\u2217)]\u2212 1\n2 \u03b3k\u03b4L\u2212\nL 2 \u03b32k\nTherefore,\nf(xK)\u2212 f(x\u2217) \u2265 K\u22121\u220f k=0 (1\u2212 \u03b1\u03b3k)[f(0)\u2212 f(x\u2217)]\u2212 \u03b4L 2 K\u22121\u2211 k=0 \u03b3k \u2212 L 2 K\u22121\u2211 k=0 \u03b32k .\nOne can observe that \u2211K\u22121\nk=0 \u03b3k = 1, and since 1\u2212 y \u2264 e\u2212y when y \u2265 0,\nf(x\u2217)\u2212 f(xK) \u2264 [f(x\u2217)\u2212 f(0)]e\u2212\u03b1 \u2211K\u22121 k=0 \u03b3k + \u03b4L\n2 + L 2 K\u22121\u2211 k=0 \u03b32k\n= [f(x\u2217)\u2212 f(0)]e\u2212\u03b1 + \u03b4L 2 + L 2 K\u22121\u2211 k=0 \u03b32k .\nAfter rearrangement, we get,\nf(xK) \u2265 (1\u2212 1/e\u03b1)f(x\u2217)\u2212 L 2 K\u22121\u2211 k=0 \u03b32k \u2212 L\u03b4 2 + e\u2212\u03b1f(0)."}, {"heading": "B.5 Proof of Corollary 1", "text": "Proof. Fixing K, to reach the tightest bound in Equation 6 amounts to solving the following problem:\nmin K\u22121\u2211 k=0 \u03b32k\ns.t. K\u22121\u2211 k=0 \u03b3k = 1, \u03b3k \u2265 0.\nUsing Lagrangian method, let \u03bb be the Lagrangian multiplier, then\nL(\u03b30, \u00b7 \u00b7 \u00b7 , \u03b3K\u22121, \u03bb) = K\u22121\u2211 k=0 \u03b32k + \u03bb [ K\u22121\u2211 k=0 \u03b3k \u2212 1 ] .\nIt can be easily verified that when \u03b30 = \u00b7 \u00b7 \u00b7 = \u03b3K\u22121 = K\u22121, \u2211K\u22121 k=0 \u03b3 2 k reaches the minimum (which is K\u22121). Therefore we obtain the tightest worst-case bound in Corollary 1."}, {"heading": "C. Proofs for the non-monotone submodular continuous functions", "text": "maximization"}, {"heading": "C.1 Proof of Proposition 5", "text": "Proof. The main proof follows from the reduction from the problem of maximizing an unconstrained non-monotone submodular set function.\nLet us denote \u03a01 as the problem of maximizing an unconstrained non-monotone submodular set function, and \u03a02 as the problem of maximizing a box constrained non-monotone submodular continuous function. Following the Appendix A of Buchbinder et al. (2012), there exist an algorithm A for \u03a01 that consists of a polynomial time computation in addition to polynomial number of subroutine calls to an algorithm for \u03a02. For details see the following.\nGiven a submodular set function F : 2E \u2192 R+, its multilinear extension (Ca\u0306linescu et al., 2007) is a function f : [0, 1]E \u2192 R+, whose value at a point x \u2208 [0, 1]E is the expected value of F over a random subset R(x) \u2286 E, where R(x) contains each element e \u2208 E independently with probability xe. Formally, f(x) := E[R(x)] = \u2211 S\u2286E F (S) \u220f e\u2208S xe \u220f e\u2032 /\u2208S(1 \u2212 xe\u2032). It can be easily seen that f(x) is a non-monotone submodular continuous function. Then the algorithm A can be: 1) Maximize the multilinear extension f(x) over the box constraint [0, 1]E , which can be achieved by solving an instance of \u03a02. Obtain the fractional solution x\u0302 \u2208 [0, 1]n; 2) Return the random set R(x\u0302). According to the definition of multilinear extension, the expected value of F (R(x\u0302)) is f(x\u0302). Thus proving the reduction from \u03a01 to \u03a02.\nGiven the reduction, the hardness result follows from the hardness of unconstrained nonmonotone submodular set function maximization.\nThe inapproximability result comes from that of the unconstrained non-monotone submodular set function maximization in Feige et al. (2011) and Dobzinski and Vondra\u0301k (2012)."}, {"heading": "C.2 Proof of Theorem 2", "text": "To better illustrate the proof, we reformulate Algorithm 2 into its equivalent form in Algorithm 3, where we split the update into two steps: when \u03b4a \u2265 \u03b4b, update x first while keeping y fixed and then update y first while keeping x fixed (xi \u2190 (xi\u22121|xi\u22121ei \u2190 u\u0302a), y\ni \u2190 yi\u22121; xi+1 \u2190 xi, yi+1 \u2190 (yi|yiei\u2190u\u0302a) ), when \u03b4a < \u03b4b, update y first. This iteration index change is only used to ease the analysis.\nTo prove the theorem, we first prove the following Lemmas. Lemma 4 is used to demonstrate that the objective value of each intermediate solution is non-\ndecreasing,\nLemma 4. \u2200i = 1, 2, \u00b7 \u00b7 \u00b7 , 2n, one has,\nf(xi) \u2265 f(xi\u22121)\u2212 \u03b4, f(yi) \u2265 f(yi\u22121)\u2212 \u03b4. (13)\nProof of Lemma 4. Let j := ei be the coordinate that is going to be changed. From submodularity,\nf(xi\u22121|xi\u22121j \u2190u\u0304j) + f(y i\u22121|yi\u22121j \u2190uj) \u2265 f(x i\u22121) + f(yi\u22121)\nSo one can verify that \u03b4a + \u03b4b \u2265 \u22122\u03b4. Let us consider the following two situations: 1) If \u03b4a \u2265 \u03b4b, x is changed first. We can see that the Lemma holds for the first change (where xi\u22121 \u2192 xi,yi = yi\u22121). For the second change, we are left to prove f(yi+1) \u2265 f(yi)\u2212 \u03b4. From submodularity:\nf(yi\u22121|yi\u22121j \u2190u\u0302a) + f(x i\u22121|xi\u22121j \u2190u\u0304j) \u2265 f(x i\u22121|xi\u22121j \u2190u\u0302a) + f(y i\u22121) (14)\nAlgorithm 3: DoubleGreedy algorithm reformulation (for analysis only)\nInput: max f(x), x \u2208 [u, u\u0304], f is generally non-monotone, f(u) + f(u\u0304) \u2265 0 1 x0 \u2190 u, y0 \u2190 u\u0304; 2 for i = 1, 3, 5, \u00b7 \u00b7 \u00b7 , 2n\u2212 1 do 3 find u\u0302a s.t. f(x\ni\u22121|xi\u22121ei \u2190u\u0302a) \u2265 maxua\u2208[uei ,u\u0304ei ] f(x i\u22121|xi\u22121ei \u2190ua)\u2212 \u03b4,\n\u03b4a \u2190 f(xi\u22121|xi\u22121ei \u2190u\u0302a)\u2212 f(x i\u22121) ; //\u03b4 \u2208 [0, \u03b4\u0304] is the additive error level.\n4 find u\u0302b s.t. f(y i\u22121|yi\u22121ei \u2190u\u0302b) \u2265 maxub\u2208[uei ,u\u0304ei ] f(y i\u22121|yi\u22121ei \u2190ub)\u2212 \u03b4, \u03b4b \u2190 f(yi\u22121|yi\u22121ei \u2190u\u0302b)\u2212 f(y\ni\u22121) ; 5 if \u03b4a \u2265 \u03b4b then 6 xi \u2190 (xi\u22121|xi\u22121ei \u2190u\u0302a), y\ni \u2190 yi\u22121 ; 7 xi+1 \u2190 xi, yi+1 \u2190 (yi|yiei\u2190u\u0302a) ; 8 else 9 yi \u2190 (yi\u22121|yi\u22121ei \u2190u\u0302b), x\ni \u2190 xi\u22121; 10 yi+1 \u2190 yi, xi+1 \u2190 (xi|xiei\u2190u\u0302b);\n11 Return x2n (or y2n) ; //note that x2n = y2n\nTherefore, f(yi+1)\u2212 f(yi) \u2265 f(xi\u22121|xi\u22121j \u2190u\u0302a)\u2212 f(xi\u22121|x i\u22121 j \u2190u\u0304j) \u2265 \u2212\u03b4, the last inequality comes from the selection rule of \u03b4a in the algorithm. 2) Otherwise, \u03b4a < \u03b4b, y is changed first. The Lemma holds for the first change (yi\u22121 \u2192 yi,xi = xi\u22121). For the second change, we are left to prove f(xi+1) \u2265 f(xi)\u2212 \u03b4. From submodularity,\nf(xi\u22121|xi\u22121j \u2190u\u0302b) + f(y i\u22121|yi\u22121j \u2190uj) \u2265 f(y i\u22121|yi\u22121j \u2190u\u0302b) + f(x i\u22121), (15)\nSo f(xi+1) \u2212 f(xi) \u2265 f(yi\u22121|yi\u22121j \u2190u\u0302b) \u2212 f(yi\u22121|y i\u22121 j \u2190uj) \u2265 \u2212\u03b4, the last inequality also comes from the selection rule of \u03b4b.\nLet OPT i := (x\u2217 \u2228 xi) \u2227 yi, it is easy to observe that OPT 0 = x\u2217 and OPT 2n = x2n = y2n.\nLemma 5. \u2200i = 1, 2, \u00b7 \u00b7 \u00b7 , 2n, it holds,\nf(OPT i\u22121)\u2212 f(OPT i) \u2264 f(xi)\u2212 f(xi\u22121) + f(yi)\u2212 f(yi\u22121) + 2\u03b4. (16)\nBefore proving Lemma 5, let us get some intuition about it. We can see that when changing i from 0 to 2n, the objective value changes from the optimal value f(x\u2217) to the value returned by the algorithm: f(x2n). Lemma 5 is then used to bound the objective loss from the assumed optimal objective in each iteration.\nProof. Let j := ei be the coordinate that will be changed. First of all, let us assume x is changed, y is kept unchanged (xi 6= xi\u22121,yi = yi\u22121), this could happen in four situations: 1.1) xij \u2264 x\u2217j and \u03b4a \u2265 \u03b4b; 1.2) xij \u2264 x\u2217j and \u03b4a < \u03b4b; 2.1) xij > x\u2217j and \u03b4a \u2265 \u03b4b; 2.2) xij > x\u2217j and \u03b4a < \u03b4b. Let us prove the four situations one by one.\nIf xij \u2264 x\u2217j , the Lemma holds in the following two situations: 1.1) When \u03b4a \u2265 \u03b4b, it happens in the first change: xij = u\u0302a \u2264 x\u2217j , so OPT i = OPT i\u22121; According to Lemma 4, \u03b4a+\u03b4b \u2265 \u22122\u03b4, so f(xi)\u2212f(xi\u22121)+f(yi)\u2212f(yi\u22121)+2\u03b4 \u2265 0, so the Lemma holds;\n1.2) When \u03b4a < \u03b4b, it happens in the second change: x i j = u\u0302b \u2264 x\u2217j , yij = y i\u22121 j = u\u0302b, and since OPT i\u22121 = (x\u2217 \u2228 xi\u22121) \u2227 yi\u22121, so OPT i\u22121j = u\u0302b and OPT ij = u\u0302b, so one still has OPT i = OPT i\u22121. So it amouts to prove that \u03b4a + \u03b4b \u2265 \u22122\u03b4, which is true according to Lemma 4.\nElse if xij > x \u2217 j , it holds that OPT i j = x i j , all other coordinates of OPT i\u22121 remain unchanged. The Lemma holds in the following two situations:\n2.1) When \u03b4a \u2265 \u03b4b, it happens in the first change. One has OPT ij = xij = u\u0302a, x i\u22121 j = uj , so\nOPT i\u22121j = x \u2217 j . And x i j = u\u0302a > x \u2217 j , y i\u22121 j = u\u0304j . From submodularity,\nf(OPT i) + f(yi\u22121|yi\u22121j \u2190x \u2217 j ) \u2265 f(OPT i\u22121) + f(yi\u22121|yi\u22121j \u2190u\u0302a) (17)\nSuppose by virtue of contradiction that,\nf(OPT i\u22121)\u2212 f(OPT i) > f(xi)\u2212 f(xi\u22121) + 2\u03b4 (18)\nSumming Equation 17 and 18 we get:\n0 > f(xi)\u2212 f(xi\u22121) + \u03b4 + f(yi\u22121|yi\u22121j \u2190u\u0302a)\u2212 f(y i\u22121|yi\u22121j \u2190x \u2217 j ) + \u03b4 (19)\nBecause \u03b4a \u2265 \u03b4b then from the selection rule of \u03b4b,\n\u03b4a = f(x i)\u2212 f(xi\u22121) \u2265 \u03b4b \u2265 f(yi\u22121|yi\u22121j \u2190c)\u2212 f(y i\u22121)\u2212 \u03b4, \u2200uj \u2264 c \u2264 u\u0304j . (20)\nSetting c = x\u2217j and substitite (20) into (19), one can get,\n0 > f(yi\u22121|yi\u22121j \u2190u\u0302a)\u2212 f(y i\u22121) + \u03b4 = f(yi+1)\u2212 f(yi) + \u03b4, (21)\nwhich contradicts with Lemma 4. 2.2) When \u03b4a < \u03b4b, it happens in the second change. y i\u22121 j = u\u0302b, x i j = u\u0302b > x \u2217 j , OPT i j = u\u0302b, OPT i\u22121 j = x \u2217 j . From submodularity,\nf(OPT i) + f(yi\u22121|yi\u22121j \u2190x \u2217 j ) \u2265 f(OPT i\u22121) + f(yi\u22121|yi\u22121j \u2190u\u0302b) (22)\nSuppose by virtue of contradiction that,\nf(OPT i\u22121)\u2212 f(OPT i) > f(xi)\u2212 f(xi\u22121) + 2\u03b4. (23)\nSumming Equations 22 and 23 we get:\n0 > f(xi)\u2212 f(xi\u22121) + \u03b4 + f(yi\u22121|yi\u22121j \u2190u\u0302b)\u2212 f(y i\u22121|yi\u22121j \u2190x \u2217 j ) + \u03b4. (24)\nFrom Lemma 4 we have f(xi)\u2212 f(xi\u22121) + \u03b4 \u2265 0, so 0 > f(yi\u22121|yi\u22121j \u2190u\u0302b)\u2212 f(yi\u22121|y i\u22121 j \u2190x\u2217j ) + \u03b4, which contradicts with the selection rule of \u03b4b. The case when y is changed, x is kept unchanged is similar, the proof of which is omitted here.\nWith Lemma 5 at hand, one can prove Theorem 2: Taking a sum over i from 1 to 2n, one can get,\nf(OPT 0)\u2212 f(OPT 2n) \u2264 f(x2n)\u2212 f(x0) + f(y2n)\u2212 f(y0) + 4n\u03b4 = f(x2n) + f(y2n)\u2212 (f(u) + f(u\u0304)) + 4n\u03b4 \u2264 f(x2n) + f(y2n) + 4n\u03b4\nThen it is easy to see that f(x2n) = f(y2n) \u2265 13f(x \u2217)\u2212 4n3 \u03b4."}, {"heading": "D. Details of revenue maximization with continuous assignments", "text": "D.1 More details about the model\nFrom the discussion in the main text, Rs(x i) should be some non-negative, non-decreasing, submodular function, we set Rs(x i) := \u221a\u2211 t:xit 6=0 xitwst, where wst is the weight of edge connecting users s and t. The first part in R.H.S. of Equation 8 models the revenue from users who have not received free assignments, while the second and third parts model the revenue from users who have gotten the free assignments. We use wtt to denote the \u201cself-activation rate\u201d of user t: Given certain amount of free trail to user t, how probable is it that he/she will buy after the trial. The intuition of modeling the second part in R.H.S. of Equation 8 is: Given the users more free assignments, they are more likely to buy the product after using it. Therefore, we model the expected revenue in this part by \u03c6(xit) = wttx i t; The intuition of modeling the third part in R.H.S. of Equation 8 is: Giving the users more free assignments, the revenue could decrease, since the users use the product for free for a longer period. As a simple example, the decrease in the revenue can be modeled as \u03b3 \u2211\nt:xit 6=0 \u2212xit."}, {"heading": "D.2 Proof of Lemma 2", "text": "Proof. First of all, we prove that g(x) := \u2211\ns:xs=0 Rs(x) is a non-negative submodular function.\nIt is easy to see that g(x) is non-negative. To prove that g(x) is submodular, one just need,\ng(a) + g(b) \u2265 g(a \u2228 b) + g(a \u2227 b), \u2200a, b \u2208 [0, u\u0304]. (25)\nLet A := supp(a), B := supp(b), where supp(x) := {i|xi 6= 0} is the support of the vector x. First of all, because Rs(x) is non-decreasing, and b \u2265 a \u2227 b, a \u2265 a \u2227 b,\u2211\ns\u2208A\\B\nRs(b) + \u2211\ns\u2208B\\A\nRs(a) \u2265 \u2211\ns\u2208A\\B\nRs(a \u2227 b) + \u2211\ns\u2208B\\A\nRs(a \u2227 b). (26)\nBy submodularity of Rs(x), and summing over s \u2208 E\\(A \u222aB),\u2211 s\u2208E\\(A\u222aB) Rs(a) + \u2211 s\u2208E\\(A\u222aB) Rs(b) \u2265 \u2211 s\u2208E\\(A\u222aB) Rs(a \u2228 b) + \u2211 s\u2208E\\(A\u222aB) Rs(a \u2227 b). (27)\nSumming Equations 26 and 27 one can get\u2211 s\u2208E\\A Rs(a) + \u2211 s\u2208E\\B Rs(b) \u2265 \u2211 s\u2208E\\(A\u222aB) Rs(a \u2228 b) + \u2211 s\u2208E\\(A\u2229B) Rs(a \u2227 b)\nwhich is equivalent to Equation 25. Then we prove that h(x) := \u2211 t:xt 6=0 R\u0304t(x) is submodular. Because R\u0304t(x) is non-increasing, and a \u2264 a \u2228 b, b \u2264 a \u2228 b,\u2211 t\u2208A\\B R\u0304t(a) + \u2211 t\u2208B\\A R\u0304t(b) \u2265 \u2211 t\u2208A\\B R\u0304t(a \u2228 b) + \u2211 t\u2208B\\A R\u0304t(a \u2228 b). (28)\nBy submodularity of R\u0304t(x), and summing over t \u2208 A \u2229B,\u2211 t\u2208A\u2229B R\u0304t(a) + \u2211 t\u2208A\u2229B R\u0304t(b) \u2265 \u2211 t\u2208A\u2229B R\u0304t(a \u2228 b) + \u2211 t\u2208A\u2229B R\u0304t(a \u2227 b). (29)\nSumming Equations 28, 29 we get,\u2211 t\u2208A R\u0304t(a) + \u2211 t\u2208B R\u0304t(b) \u2265 \u2211 t\u2208A\u222aB R\u0304t(a \u2228 b) + \u2211 t\u2208A\u2229B R\u0304t(a \u2227 b)\nwhich is equivalent to h(a)+h(b) \u2265 h(a\u2228b)+h(a\u2227b), \u2200a, b \u2208 [0, u\u0304], thus proving the submodularity of h(x).\nFinally, because f(x) is the sum of two submodular functions and one modular function, so it is submodular.\nD.3 Solving the 1-D subproblem when applying the DoubleGreedy algorithm\nSuppose we are varying xj \u2208 [0, u\u0304j ] to maximize f(x), notice that this 1-D subproblem is nonsmooth and discontinuous at point 0. First of all, let us leave xj = 0 out, one can see that f(x) is concave and smooth along \u03c7j when xj \u2208 (0, u\u0304j ],\n\u2202f(x)\n\u2202xj = \u03b1 \u2211 s 6=j:xs=0\nwsj 2 \u221a\u2211 t:xt 6=0 xtwst \u2212 \u03b3 + \u03b2wjj\n\u22022f(x)\n\u2202x2j = \u22121 4 \u03b1 \u2211 s 6=j:xs=0 w2sj(\u221a\u2211 t:xt 6=0 xtwst\n)3 \u2264 0. Let f\u0304(z) be the univariate function when xj \u2208 (0, u\u0304j ], then we extend the domain of f\u0304(z) to be z \u2208 [0, u\u0304j ] as,\nf\u0304(z) = f\u0304(xj) := \u03b1 \u2211\ns 6=j:xs=0 Rs(x) + \u03b2 \u2211 t6=j:xt 6=0 \u03c6(xt) + \u03b3 \u2211 t6=j:xt 6=0 R\u0304t(x) + \u03b2\u03c6(xj) + \u03b3R\u0304j(x).\nOne can see that f\u0304(z) is concave and smooth. Now to solve the 1-D subproblem, we can first of all solve the smooth concave 1-D maximization problem9: z\u2217 := arg maxz\u2208[0,u\u0304j ]f\u0304(z), then compare f\u0304(z\u2217) with the function value at the discontinuous point 0: f(x|xj\u21900), and return the point with the larger function value."}, {"heading": "E. More applications", "text": "Maximum coverage. In the maximum coverage problem, there are n subsets C1, \u00b7 \u00b7 \u00b7 , Cn from the ground set V . One subset Ci can be chosen with \u201cconfidence\u201d level xi \u2208 [0, 1], the set of covered elements when choosing subset Ci with confidence xi can be modeled with the following monotone normalized covering function: pi : R+ \u2192 2V , i = 1, \u00b7 \u00b7 \u00b7 , n. The target is to choose subsets from C1, \u00b7 \u00b7 \u00b7 , Cn with confidence level to maximize the number of covered elements | \u222ani=1 pi(xi)|, at the same time respecting the budget constraint \u2211 i cixi \u2264 b (where ci is the cost of choosing subset Ci). This problem generalizes the classical maximum coverage problem. It is easy to see that the objective function is monotone submodular, and the constraint is a down-closed polytope.\nText summarization. Submodularity-based objective functions for text summarization perform well in practice (Lin and Bilmes, 2010). Let C to be the set of all concepts, and E to be the set of all sentences. As a typical example, the concept-based summarization aims to find a subset S of the sentences to maximize the total credit of concepts covered by S. Soma et al. (2014) discussed\n9. It can be efficienlty solved by various methods, e.g., the bisection method or Newton method.\nextending the submodular text summarization model to the one that incorporates \u201cconfidence\u201d of a sentence, which has discrete value, and modeling the objective to be a monotone submodular lattice function. It is also natural to model the confidence level of sentence i to be a continuous value xi \u2208 [0, 1]. Let us use pi(xi) to denote the set of covered concepts when selecting sentence i with confidence level xi, it can be a monotone covering function pi : R+ \u2192 2C ,\u2200i \u2208 E. Then the objective function of the extended model is f(x) = \u2211 j\u2208\u222aipi(xi) cj , where cj \u2208 R+ is the credit of concept j. It can be verified that this objective is a monotone submodular continuous function."}], "references": [{"title": "sitions for learning latent variable models", "author": ["Francis Bach"], "venue": null, "citeRegEx": "Bach.,? \\Q2014\\E", "shortCiteRegEx": "Bach.", "year": 2014}, {"title": "Deep submodular functions", "author": ["Jeffrey Bilmes", "Wenruo Bai"], "venue": "arXiv preprint arXiv:1701.08939,", "citeRegEx": "2010", "shortCiteRegEx": "2010", "year": 2017}, {"title": "approximation for unconstrained submodular maximization", "author": ["Gruia C\u0103linescu", "Chandra Chekuri", "Martin P\u00e1l", "Jan Vondr\u00e1k"], "venue": "In FOCS,", "citeRegEx": "C\u0103linescu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "C\u0103linescu et al\\.", "year": 2012}, {"title": "Maximizing a monotone submodular", "author": ["Springer", "2007. Gruia C\u0103linescu", "Chandra Chekuri", "Martin P\u00e1l", "Jan Vondr\u00e1k"], "venue": null, "citeRegEx": "Springer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Springer et al\\.", "year": 2007}, {"title": "Designing smoothing functions for improved worst-case competitive ratio", "author": ["Reza Eghbali", "Maryam Fazel"], "venue": "forty-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "Eghbali and Fazel.,? \\Q2012\\E", "shortCiteRegEx": "Eghbali and Fazel.", "year": 2012}, {"title": "A reduction for optimizing lattice submodular functions with diminishing", "author": ["Alina Ene", "Huy L Nguyen"], "venue": null, "citeRegEx": "Ene and Nguyen.,? \\Q2016\\E", "shortCiteRegEx": "Ene and Nguyen.", "year": 2016}, {"title": "Near-optimal map inference for determinantal point", "author": ["Jennifer Gillenwater", "Alex Kulesza", "Ben Taskar"], "venue": "Satoru Fujishige. Submodular functions and optimization,", "citeRegEx": "Gillenwater et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Gillenwater et al\\.", "year": 1956}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "author": ["Daniel Golovin", "Andreas Krause"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Golovin and Krause.,? \\Q2011\\E", "shortCiteRegEx": "Golovin and Krause.", "year": 2011}, {"title": "Submodular function maximization on the bounded integer lattice", "author": ["Corinna Gottschalk", "Britta Peis"], "venue": "In Approximation and Online Algorithms,", "citeRegEx": "Gottschalk and Peis.,? \\Q2015\\E", "shortCiteRegEx": "Gottschalk and Peis.", "year": 2015}, {"title": "Optimal marketing strategies over social networks", "author": ["Jason Hartline", "Vahab Mirrokni", "Mukund Sundararajan"], "venue": "In Proceedings of the 17th international conference on World Wide Web,", "citeRegEx": "Hartline et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hartline et al\\.", "year": 2008}, {"title": "Lagrangian decomposition algorithm for allocating marketing channels", "author": ["Daisuke Hatano", "Takuro Fukunaga", "Takanori Maehara", "Ken-ichi Kawarabayashi"], "venue": "In AAAI,", "citeRegEx": "Hatano et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hatano et al\\.", "year": 2015}, {"title": "On graduated optimization for stochastic non-convex problems", "author": ["Elad Hazan", "Kfir Y Levy", "Shai Shalev-Swartz"], "venue": "arXiv preprint arXiv:1503.03712,", "citeRegEx": "Hazan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2015}, {"title": "A combinatorial strongly polynomial algorithm for minimizing submodular functions", "author": ["Satoru Iwata", "Lisa Fleischer", "Satoru Fujishige"], "venue": "Journal of the ACM,", "citeRegEx": "Iwata et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Iwata et al\\.", "year": 2001}, {"title": "Revisiting frank-wolfe: Projection-free sparse convex optimization", "author": ["Martin Jaggi"], "venue": "ICML", "citeRegEx": "Jaggi.,? \\Q2013\\E", "shortCiteRegEx": "Jaggi.", "year": 2013}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "author": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "CoRR abs/1506.08473,", "citeRegEx": "Janzamin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Janzamin et al\\.", "year": 2015}, {"title": "Exact solutions of some nonconvex quadratic optimization problems via sdp and socp relaxations", "author": ["Sunyoung Kim", "Masakazu Kojima"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Kim and Kojima.,? \\Q2003\\E", "shortCiteRegEx": "Kim and Kojima.", "year": 2003}, {"title": "Submodularity on a tree: Unifying l-convex and bisubmodular functions", "author": ["Vladimir Kolmogorov"], "venue": "In Mathematical Foundations of Computer Science,", "citeRegEx": "Kolmogorov.,? \\Q2011\\E", "shortCiteRegEx": "Kolmogorov.", "year": 2011}, {"title": "Submodular dictionary selection for sparse representation", "author": ["Andreas Krause", "Volkan Cevher"], "venue": "In ICML,", "citeRegEx": "Krause and Cevher.,? \\Q2010\\E", "shortCiteRegEx": "Krause and Cevher.", "year": 2010}, {"title": "Submodular function maximization", "author": ["Andreas Krause", "Daniel Golovin"], "venue": "Tractability: Practical Approaches to Hard Problems,", "citeRegEx": "Krause and Golovin.,? \\Q2012\\E", "shortCiteRegEx": "Krause and Golovin.", "year": 2012}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["Andreas Krause", "Carlos Guestrin"], "venue": "In UAI,", "citeRegEx": "Krause and Guestrin.,? \\Q2005\\E", "shortCiteRegEx": "Krause and Guestrin.", "year": 2005}, {"title": "Handbook of Monte Carlo Methods, volume 706", "author": ["Dirk P Kroese", "Thomas Taimre", "Zdravko I Botev"], "venue": null, "citeRegEx": "Kroese et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kroese et al\\.", "year": 2013}, {"title": "Cost-effective outbreak detection in networks", "author": ["Jure Leskovec", "Andreas Krause", "Carlos Guestrin", "Christos Faloutsos", "Jeanne VanBriesen", "Natalie Glance"], "venue": "In ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Leskovec et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Leskovec et al\\.", "year": 2007}, {"title": "Accelerated proximal gradient methods for nonconvex programming", "author": ["Huan Li", "Zhouchen Lin"], "venue": "In NIPS,", "citeRegEx": "Li and Lin.,? \\Q2015\\E", "shortCiteRegEx": "Li and Lin.", "year": 2015}, {"title": "Multi-document summarization via budgeted maximization of submodular functions", "author": ["Hui Lin", "Jeff Bilmes"], "venue": "In Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Lin and Bilmes.,? \\Q2010\\E", "shortCiteRegEx": "Lin and Bilmes.", "year": 2010}, {"title": "A class of submodular functions for document summarization", "author": ["Hui Lin", "Jeff Bilmes"], "venue": "In HLT,", "citeRegEx": "Lin and Bilmes.,? \\Q2011\\E", "shortCiteRegEx": "Lin and Bilmes.", "year": 2011}, {"title": "Submodular functions and convexity", "author": ["L\u00e1szl\u00f3 Lov\u00e1sz"], "venue": null, "citeRegEx": "Lov\u00e1sz.,? \\Q1983\\E", "shortCiteRegEx": "Lov\u00e1sz.", "year": 1983}, {"title": "Optimal budget allocation", "author": ["Tasuku Soma", "Naonori Kakimura", "Kazuhiro Inaba", "Ken-ichi Kawarabayashi"], "venue": null, "citeRegEx": "Soma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Soma et al\\.", "year": 2015}, {"title": "C\u0103linescu et al. (2007) (called the pipage rounding)", "author": ["Ageev", "Sviridenko"], "venue": null, "citeRegEx": "Ageev and Sviridenko,? \\Q2004\\E", "shortCiteRegEx": "Ageev and Sviridenko", "year": 2004}, {"title": "As a typical example, the concept-based summarization aims to find a subset S of the sentences to maximize the total credit of concepts covered", "author": ["S. Soma"], "venue": null, "citeRegEx": "Soma,? \\Q2014\\E", "shortCiteRegEx": "Soma", "year": 2014}], "referenceMentions": [{"referenceID": 19, "context": "Optimizing submodular set functions has found numerous applications in machine learning, including variable selection (Krause and Guestrin, 2005), dictionary learning (Krause and Cevher, 2010; Das and Kempe, 2011), sparsity inducing regularizers (Bach, 2010), summarization (Lin and Bilmes, 2011; Mirzasoleiman et al.", "startOffset": 118, "endOffset": 145}, {"referenceID": 17, "context": "Optimizing submodular set functions has found numerous applications in machine learning, including variable selection (Krause and Guestrin, 2005), dictionary learning (Krause and Cevher, 2010; Das and Kempe, 2011), sparsity inducing regularizers (Bach, 2010), summarization (Lin and Bilmes, 2011; Mirzasoleiman et al.", "startOffset": 167, "endOffset": 213}, {"referenceID": 24, "context": "Optimizing submodular set functions has found numerous applications in machine learning, including variable selection (Krause and Guestrin, 2005), dictionary learning (Krause and Cevher, 2010; Das and Kempe, 2011), sparsity inducing regularizers (Bach, 2010), summarization (Lin and Bilmes, 2011; Mirzasoleiman et al., 2013) and variational inference (Djolonga and Krause, 2014).", "startOffset": 274, "endOffset": 324}, {"referenceID": 12, "context": "Submodular set functions can be efficiently minimized (Iwata et al., 2001), and there are strong guarantees for approximate maximization (Nemhauser et al.", "startOffset": 54, "endOffset": 74}, {"referenceID": 18, "context": ", 2001), and there are strong guarantees for approximate maximization (Nemhauser et al., 1978; Krause and Golovin, 2012).", "startOffset": 70, "endOffset": 120}, {"referenceID": 0, "context": "Optimizing submodular set functions has found numerous applications in machine learning, including variable selection (Krause and Guestrin, 2005), dictionary learning (Krause and Cevher, 2010; Das and Kempe, 2011), sparsity inducing regularizers (Bach, 2010), summarization (Lin and Bilmes, 2011; Mirzasoleiman et al., 2013) and variational inference (Djolonga and Krause, 2014). Submodular set functions can be efficiently minimized (Iwata et al., 2001), and there are strong guarantees for approximate maximization (Nemhauser et al., 1978; Krause and Golovin, 2012). Even though submodularity is most widely considered in the discrete realm, the notion can be generalized to arbitrary lattices (Fujishige, 2005). Recently, Bach (2015) showed how results from submodular set function minimization can be lifted to the continuous domain.", "startOffset": 247, "endOffset": 737}, {"referenceID": 14, "context": ", 2014) and training neural networks (Janzamin et al., 2015).", "startOffset": 37, "endOffset": 60}, {"referenceID": 22, "context": "A fundamental problem in non-convex optimization is to reach a stationary point assuming the smoothness of the objective (Sra, 2012; Li and Lin, 2015; Reddi et al., 2016; Allen-Zhu and Hazan, 2016).", "startOffset": 121, "endOffset": 197}, {"referenceID": 4, "context": "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies.", "startOffset": 0, "endOffset": 26}, {"referenceID": 4, "context": "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them.", "startOffset": 0, "endOffset": 152}, {"referenceID": 4, "context": "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1\u2212 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint.", "startOffset": 0, "endOffset": 614}, {"referenceID": 4, "context": "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1\u2212 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint. For non-monotone submodular functions over the bounded integer lattice, Gottschalk and Peis (2015) provide a 1/3-approximation.", "startOffset": 0, "endOffset": 849}, {"referenceID": 4, "context": "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1\u2212 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint. For non-monotone submodular functions over the bounded integer lattice, Gottschalk and Peis (2015) provide a 1/3-approximation. Approximation algorithms for maximizing bisubmodular functions and k-submodular functions have also been proposed by Singh et al. (2012); Ward and Zivny (2014).", "startOffset": 0, "endOffset": 1015}, {"referenceID": 4, "context": "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1\u2212 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint. For non-monotone submodular functions over the bounded integer lattice, Gottschalk and Peis (2015) provide a 1/3-approximation. Approximation algorithms for maximizing bisubmodular functions and k-submodular functions have also been proposed by Singh et al. (2012); Ward and Zivny (2014). Wolsey (1982) considers maximizing a special class of submodular continuous functions subject to one knapsack constraint, in the context of solving location problems.", "startOffset": 0, "endOffset": 1038}, {"referenceID": 4, "context": "Golovin and Krause (2011) introduce the notion of adaptive submodularity to generalize submodular set functions to adaptive policies. Kolmogorov (2011) studies tree-submodular functions and presents a polynomial algorithm for minimizing them. For distributive lattices, it is well-known that the combinatorial polynomial-time algorithms for minimizing a submodular set function can be adopted to minimize a submodular function over a bounded integer lattice (Fujishige, 2005). Recently, maximizing a submodular function over integer lattices has attracted considerable attention. In particular, Soma et al. (2014) develop a (1\u2212 1/e)-approximation algorithm for maximizing a monotone DR-submodular integerlattice function under a knapsack constraint. For non-monotone submodular functions over the bounded integer lattice, Gottschalk and Peis (2015) provide a 1/3-approximation. Approximation algorithms for maximizing bisubmodular functions and k-submodular functions have also been proposed by Singh et al. (2012); Ward and Zivny (2014). Wolsey (1982) considers maximizing a special class of submodular continuous functions subject to one knapsack constraint, in the context of solving location problems.", "startOffset": 0, "endOffset": 1053}, {"referenceID": 1, "context": "C\u0103linescu et al. (2007) and Vondr\u00e1k (2008) discuss a subclass of submodular continuous functions, which is termed smooth submodular functions3, to describe the multilinear extension of a submodular set function.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "C\u0103linescu et al. (2007) and Vondr\u00e1k (2008) discuss a subclass of submodular continuous functions, which is termed smooth submodular functions3, to describe the multilinear extension of a submodular set function.", "startOffset": 0, "endOffset": 43}, {"referenceID": 0, "context": "Recently, Bach (2015) considers the minimization of a submodular continuous function, and proves that efficient techniques from convex optimization may be used for minimization.", "startOffset": 10, "endOffset": 22}, {"referenceID": 0, "context": "Recently, Bach (2015) considers the minimization of a submodular continuous function, and proves that efficient techniques from convex optimization may be used for minimization. Very recently, Ene and Nguyen (2016) provide a reduction from a integer-lattice DR-submodular function maximization problem to a submodular set function maximization problem, which suggests a way to optimize submodular continuous functions over simple continuous constriants: Discretize the continuous function and constraint to be an integer-lattice instance, and then optimize it using the reduction.", "startOffset": 10, "endOffset": 215}, {"referenceID": 0, "context": "Recently, Bach (2015) considers the minimization of a submodular continuous function, and proves that efficient techniques from convex optimization may be used for minimization. Very recently, Ene and Nguyen (2016) provide a reduction from a integer-lattice DR-submodular function maximization problem to a submodular set function maximization problem, which suggests a way to optimize submodular continuous functions over simple continuous constriants: Discretize the continuous function and constraint to be an integer-lattice instance, and then optimize it using the reduction. However, for monotone DR-submodular functions maximization, this method can not handle the general continuous constraints discussed in this work, i.e., arbitrary down-closed convex sets. And for general submodular function maximization, this method cannot be applied, since the reduction needs the additional diminishing returns property. Therefore we focus on continuous methods in this work. Non-convex optimization. Optimizing non-convex continuous functions has received renewed interest in the last decades. Recently, tensor methods have been used in various non-convex problems, e.g., learning latent variable models (Anandkumar et al., 2014) and training neural networks (Janzamin et al., 2015). A fundamental problem in non-convex optimization is to reach a stationary point assuming the smoothness of the objective (Sra, 2012; Li and Lin, 2015; Reddi et al., 2016; Allen-Zhu and Hazan, 2016). With extra assumptions, certain global convergence results can be obtained. For example, for functions with Lipschitz continuous Hessians, the regularized Newton scheme of Nesterov and Polyak (2006) achieves global convergence results for functions with 3.", "startOffset": 10, "endOffset": 1682}, {"referenceID": 11, "context": "Hazan et al. (2015) introduce the family of \u03c3-nice functions and propose a graduated optimization-based algorithm, that provably converges to a global optimum for this family of (generally) non-convex functions.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Recently, the DR property is explored by Eghbali and Fazel (2016) to achieve the worst-case competitive ratio for an online concave maximization problem.", "startOffset": 41, "endOffset": 66}, {"referenceID": 13, "context": "The algorithm is a generalization of the continuous greedy algorithm of Vondr\u00e1k (2008) for maximizing a smooth submodular function, and related to the convex Frank-Wolfe algorithm (Frank and Wolfe, 1956; Jaggi, 2013) for minimizing a convex function.", "startOffset": 180, "endOffset": 216}, {"referenceID": 8, "context": "(2012) and Gottschalk and Peis (2015), and can be viewed as a procedure performing coordinate-ascent on two solutions.", "startOffset": 11, "endOffset": 38}, {"referenceID": 25, "context": "The Lov\u00e1sz extension (Lov\u00e1sz, 1983) used for submodular set function minimization is both submodular and convex (see Appendix A in Bach (2015)).", "startOffset": 21, "endOffset": 35}, {"referenceID": 10, "context": ", time for a TV advertisement, or space of an inline ad) among the source nodes, and to maximize the expected influence on the potential customers (Soma et al., 2014; Hatano et al., 2015).", "startOffset": 147, "endOffset": 187}, {"referenceID": 0, "context": "The Lov\u00e1sz extension (Lov\u00e1sz, 1983) used for submodular set function minimization is both submodular and convex (see Appendix A in Bach (2015)).", "startOffset": 131, "endOffset": 143}, {"referenceID": 0, "context": "The Lov\u00e1sz extension (Lov\u00e1sz, 1983) used for submodular set function minimization is both submodular and convex (see Appendix A in Bach (2015)). Non-convex/non-concave quadratic programming (NQP). NQP problem of the form f(x) = 1 2x >Hx+h>x+c under linear constraints naturally arises in many applications, including scheduling (Skutella, 2001), inventory theory, and free boundary problems. A special class of NQP is the submodular NQP (the minimization of which was studied in Kim and Kojima (2003)), in which all off-diagonal entries of H are required to be non-positive.", "startOffset": 131, "endOffset": 501}, {"referenceID": 9, "context": "In viral marketing, sellers choose a small subset of buyers to give them some product for free, to trigger a cascade of further adoptions through \u201cword-of-mouth\u201d effects, in order to maximize the total revenue (Hartline et al., 2008).", "startOffset": 210, "endOffset": 233}, {"referenceID": 9, "context": "With \u03b2=\u03b3=0, we recover the classical model of Hartline et al. (2008). For products with continuous assignments, usually the cost of the product does not increase with its amount, e.", "startOffset": 46, "endOffset": 69}, {"referenceID": 21, "context": "For cost-sensitive outbreak detection in sensor networks (Leskovec et al., 2007), one needs to place sensors in a subset of locations selected from all the possible locations E, to quickly detect a set of contamination events V , while respecting the cost constraints of the sensors.", "startOffset": 57, "endOffset": 80}, {"referenceID": 21, "context": "For cost-sensitive outbreak detection in sensor networks (Leskovec et al., 2007), one needs to place sensors in a subset of locations selected from all the possible locations E, to quickly detect a set of contamination events V , while respecting the cost constraints of the sensors. For each location e \u2208 E and each event v \u2208 V , a value t(e, v) is provided as the time it takes for the placed sensor in e to detect event v. Soma and Yoshida (2015a) considered the sensors with discrete energy levels.", "startOffset": 58, "endOffset": 451}, {"referenceID": 23, "context": "The maximum coverage problem and the problem of text summarization with submodular objectives are among the examples (Lin and Bilmes, 2010).", "startOffset": 117, "endOffset": 139}, {"referenceID": 20, "context": "Experimental results We compare the performance of our proposed algorithms, the Frank-Wolfe variant and DoubleGreedy, with the following baselines: a) Random: uniformly sample ks solutions from the constraint set using the hit-and-run sampler (Kroese et al., 2013), and select the best one.", "startOffset": 243, "endOffset": 264}], "year": 2017, "abstractText": "Submodular continuous functions are a category of (generally) non-convex/non-concave functions with a wide spectrum of applications. We characterize these functions and demonstrate that they can be maximized efficiently with approximation guarantees. Specifically, i) We introduce the weak DR property that gives a unified characterization of submodularity for all set, integer-lattice and continuous functions; ii) for maximizing monotone DR-submodular continuous functions under general down-closed convex constraints, we propose a Frank-Wolfe variant with (1\u22121/e) approximation guarantee, and sub-linear convergence rate; iii) for maximizing general non-monotone submodular continuous functions subject to box constraints, we propose a DoubleGreedy algorithm with 1/3 approximation guarantee. Submodular continuous functions naturally find applications in various real-world settings, including influence and revenue maximization with continuous assignments, sensor energy management, multi-resolution data summarization, facility location, etc. Experimental results show that the proposed algorithms efficiently generate superior solutions compared to baseline algorithms. \u2217Appears in the 20 International Conference on Artificial Intelligence and Statistics (AISTATS) 2017, Fort Lauderdale, Florida, USA. ar X iv :1 60 6. 05 61 5v 4 [ cs .L G ] 1 M ar 2 01 7", "creator": "LaTeX with hyperref package"}}}