{"id": "1606.05804", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2016", "title": "Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema", "abstract": "universal schema predicts the types of entities data and sql relations in a knowledge base ( denoted kb ) by jointly embedding the union of all uniformly available component schema types - - - unless not exhibiting only types from fairly multiple structured databases ( such as freebase or xml wikipedia infoboxes ), explicitly but must also types expressed as the textual constraint patterns from generic raw text. importantly this metadata prediction is typically structurally modeled as including a semantic matrix completion problem, with one type per column, evaluated and requires either one or two entities per type row ( in contrast the low case variants of simple entity similarity types or binary type relation types, respectively ). above factorizing efficiently this sparsely observed structured matrix yields a specifically learned search vector embedding for storing each associated row and each column. finally in this paper we explore the problem mechanisms of making predictions for entities or entity - pairs unseen at minimum training time ( and largely hence without noting a potentially pre - modelled learned missing row embedding ). we propose an approach having no random per - row parameters at substantially all ; suppose rather suddenly we produce a row vector represented on the brains fly using a learned aggregation function of the vectors of the originally observed 42 columns for that only row. we experiment also with several various aggregation functions, successfully including deep neural network attention models. our approach can further be understood independently as a natural language database, in order that questions about kb entities entries are indirectly answered by attending to textual or database evidence. alternatively in experiments falsely predicting a both relations indices and unsigned entity assignment types, ultimately we demonstrate so that once despite having an implicit order of odd magnitude - fewer parameters total than traditional universal array schema, we can match simply the linear accuracy of the theoretical traditional model, generally and more truly importantly, we can, now completely make multiple predictions explaining about unseen rows rows possible with relatively nearly the same accuracy as rows available at training readiness time.", "histories": [["v1", "Sat, 18 Jun 2016 20:38:42 GMT  (285kb,D)", "http://arxiv.org/abs/1606.05804v1", null], ["v2", "Mon, 9 Jan 2017 21:46:47 GMT  (730kb,D)", "http://arxiv.org/abs/1606.05804v2", "EACL 2017. arXiv admin note: text overlap witharXiv:1604.06361"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["patrick verga", "arvind neelakantan", "rew mccallum"], "accepted": false, "id": "1606.05804"}, "pdf": {"name": "1606.05804.pdf", "metadata": {"source": "CRF", "title": "Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema", "authors": ["Patrick Verga", "Arvind Neelakantan", "Andrew McCallum"], "emails": ["mccallum}@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "Automatic knowledge base construction (AKBC) is the task of building a structured knowledge base (KB) of facts using raw text evidence, and often an initial seed KB to be augmented (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008). KBs generally contain entity type facts such as Sundar Pichai IsA Person and relation facts such as CEO Of(Sundar Pichai, Google).\nExtracted facts about entities, and their types and relations are useful for many downstream tasks such as question answering (Bordes et al., 2014) and semantic parsing (Berant et al., 2013; Kwiatkowski et al., 2013).\nAn effective approach to AKBC is universal schema, which predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema types\u2014not only types from multiple structured databases (such as Freebase or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem. In the standard formulation for relation extraction (Riedel et al., 2013), entity pairs and relations occupy the rows and columns of the matrix respectively (Figure 1). Analogously in entity type prediction (Yao et al., 2013), entities and types occupy the rows and columns of the matrix respectively (Figure 2). The row and column entries are represented as learned vectors with compatibility determined by a scoring function.\nIn its original form, universal schema can reason only about row entries and column entries explicitly seen during training. Unseen rows and columns observed at test time do not have a learned embedding. This problem is referred to as the cold-start problem in recommendation systems (Schein et al., 2002).\nRecently Toutanova et al. (2015) and Verga et al. (2016) proposed \u2018column-less\u2019 versions of universal schema models that generalize to unseen column entries. They learn compositional pattern encoders to parameterize the column matrix in place of individual column embeddings. However, these models still do not generalize to unseen row entries.\nIn this work, we present a \u2018row-less\u2019 extension of universal schema that generalizes to unseen entities and entity pairs. Rather than representing each row entry with an explicit dense vector, we encode each entity or entity pair as aggregate functions over their observed column entries. This is beneficial because when new entities are mentioned in text documents and subsequently added to\nar X\niv :1\n60 6.\n05 80\n4v 1\n[ cs\n.C L\n] 1\n8 Ju\nn 20\nthe KB, we can directly reason on the observed text evidence to infer new binary relations and entity types for the new entities. This avoids the cumbersome effort of re-training the whole model from scratch to learn embeddings for the new entities.\nTo construct the row representation, we compare various aggregation functions in our experiments. We consider query independent and dependent aggregation functions. We find that query dependent attentional models that selectively focus on relevant evidence outperform the query independent alternatives. The query dependent attention mechanism also helps in providing a direct connection between the prediction and its provenance. Additionally, our models have a much smaller memory footprint since they do not store explicit row representations.\nIt is important to note that our approach is different from sentence level classifiers that predict KB relations and entity types using a single sentence as evidence. To make a prediction, we pool information from multiple pieces of evidence coming from both text and annotated KB facts. Also, our methods predict a much richer set of labels (KB types and textual) enabling easier downstream processing closer to natural language interaction with the KB.\nThe current methods for KB entity type prediction operate with explicit entity representations (Yao et al., 2013; Neelakantan and Chang, 2015) and hence, cannot generalize to unseen entities. In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dura\u0301n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time. These models learn representations for the entities instead of entity pairs. Hence, these methods still cannot generalize to predict relations between an entity pair if even one of the entities is unseen. Moreover, Toutanova et al. (2015) and Riedel et al. (2013) observe that the entity pair model outperforms entity models in cases where the entity pair was seen at training time. Since universal schema leverages large amounts of unlabeled text we desire the benefits of entity pair modeling, and row-less universal schema facilitates learning entity pair representations without the drawbacks of the traditional one-embedding-per-pair approach.\nIn this paper we investigate universal schema models without explicit row representations on two tasks: entity type prediction and relation extraction. We use entity type and relation facts from Freebase (Bollacker et al., 2008) augmented with textual relations and types from Clueweb text (Orr et al., 2013; Gabrilovich et al., 2013). We explore multiple aggregation functions and find that an attention-based aggregation function outperforms several simpler functions and matches a model using explicit row representations with an or-\nder of magnitude fewer parameters. More importantly, we then demonstrate that our \u2018row-less\u2019 models accurately predict relations on unseen entity pairs and types on unseen entities. The data and code for all our experiments are available at https://github.com/ patverga/torch-relation-extraction."}, {"heading": "2 Background: Universal Schema", "text": "Universal schema (Riedel et al., 2013; Yao et al., 2013) relation extraction and entity type prediction is typically modeled as a matrix completion task. In relation extraction, entity pairs and relations occupy the rows and columns of the matrix (Figure 1), while in entity type prediction, entities and types occupy the rows and columns of the matrix (Figure 2). During training, we observe some positive entries in the matrix and at test time, we predict the missing cells in the matrix. This is achieved by decomposing the observed matrix into two low-rank matrices resulting in embeddings for each column entry and each row entry. Test time prediction is performed using the learned low-rank column and row representations.\nLet T be the training set consisting of examples of the form (r, c), where row r \u2208 U and column c \u2208 V , denote entity pair and relation type in the relation extraction task, while entity and entity type in the entity type prediction task. Let v(r) \u2208 Rd and v(c) \u2208 Rd be the vector representations or embeddings of row r \u2208 U and column c \u2208 V that are learned during training. Given a positive example, (r, c) \u2208 T in training, the probability of observing the fact is given by,\nP (yr,c = 1) = \u03c3(v(r).v(c)) (1)\nwhere yr,c is a binary random variable that is equal to 1 when (r, c) is a fact and 0 otherwise, and \u03c3 is the sigmoid function. The embeddings are learned using Bayesian Personalized Ranking (BPR) (Rendle et al., 2009) in which the probability of the observed triples are ranked above unobserved triples."}, {"heading": "3 Model", "text": "In this section, we describe the model, discuss the different aggregation functions and give details on the training objective."}, {"heading": "3.1 \u2018Row-less\u2019 Universal Schema", "text": "While column-less universal schema addresses reasoning over arbitrary textual patterns, it is still limited to reasoning over row entries seen at training time. Verga et al. (2016) use column-less universal schema for relation extraction. They address the problem of unseen row entries by using universal schema as a sentence classifier \u2013 directly comparing a textual relation to a KB relation to perform relation extraction. However, this approach is\nunsatisfactory for two reasons. The first is that this creates an inconsistency between training and testing. The model is trained to predict compatibility between rows and columns, but at test time it predicts compatibility between relations directly. Second, it considers only a single piece of evidence in making its prediction.\nWe address both of these concerns in our \u2018row-less\u2019 universal schema. Rather than explicitly encoding each row, we encode the row as a learned aggregation over their observed columns (Figure 3). A row contains an entity for type prediction and an entity pair for relation extraction while a column contains a relation type for relation extraction and an entity type for type prediction. A learned row embedding can be seen as a summarization of all columns observed with that particular row. Instead of modeling this summarization as a single embedding, we reconstruct a row representation from an aggregate of its column embeddings, essentially learning a mixture model rather than a single centroid."}, {"heading": "3.2 Aggregation Functions", "text": "In this work we examine four aggregation functions to construct the representations for the row. Let v(.) denote a function that returns the vector representation for rows and columns. To model the probability between row r and column c, we consider the set \u00afV (r) which contains the set of column entries that are observed with row r at training time, i.e.,\n\u2200c\u0304 \u2208 \u00afV (r), (r, c\u0304) \u2208 T\nThe first two aggregation functions create a single representation for each row independent of the query. Mean Pool creates a single centroid for the row by averaging all of its column vectors,\nv(r) = \u2211\nc\u0304\u2208 \u00afV (r) v(c\u0304)\nWhile this formulation intuitively makes sense as an approximation for the explicit row representation, averaging large numbers of embeddings can lead to a noisy representation.\nMax Pool also creates a single representation for the row by taking a dimension-wise max over the observed column vectors:\nv(r)i = maxc\u0304\u2208 \u00afV (r) v(c\u0304)i,\u2200i \u2208 1, 2, . . . , d\nwhere ai denotes the ith dimension of vector a. Both mean pool and max pool are query-independent and form the same representation for the row regardless of the query relation.\nWe also examine two query-specific aggregation functions. These models are more expressive than a single vector forced to to act as a centroid to all possible columns observed with that particular row. For example, the entity pair Bill and Melinda Gates could hold the relation \u2018per:spouse\u2019 or \u2018per:co-worker\u2019. A query-specific aggregation mechanism can produce separate representations for this entity pair dependent on the query.\nThe Max Relation aggregation function represents the row as its most similar column to the query vector of interest. Given a query relation c,\ncmax = argmaxc\u0304\u2208 \u00afV (r)v(c\u0304).v(c)\nv(r) = v(cmax)\npe r:s\npo us e pe r:c o-\nwo rk\ner\nar g1\n\u2018s w\nife ar g2 ar g1 co -fo un de d\nth e\nfo un\nda tio\nn wi\nth ar g2 ar g1 m ar rie d ar g 2 ...\nBill Gates / Melinda Gates 1\n... arg1 \u2018s wife arg2 (Bill Gates /\nMelinda Gates)\n1 arg1 co-founded the foundation with arg2arg1 married arg 2 Aggregation\nFunction\n...\n...\n... ...\n1\nFigure 3: Row-less universal schema for relation extraction encodes an entity pair as an aggregation of its observed relation types.\nA similar strategy has been successfully applied in previous work (Weston et al., 2013; Neelakantan et al., 2014; Neelakantan et al., 2015) for different tasks. This model has the advantage of creating a query-specific entity pair representation, but is more susceptible to noisy training data as a single incorrect piece of evidence could be used to form a prediction.\nFinally, we look at an Attention aggregation function over columns (Figure 4) which is similar to a single-layer memory network (Sukhbaatar et al., 2015). The soft attention mechanism has been used to selectively focus on relevant parts in many different models (Bahdanau et al., 2014; Graves et al., 2014; Neelakantan et al., 2016).\nIn this model the query is scored with an input representation of each column embedding followed by a softmax, giving a weighting over each relation type. This output is then used to get a weighted sum over a set of output representations for each column resulting in a queryspecific vector representation of the row. Given a query relation c,\nscorec\u0304 = v(c).v(c\u0304),\u2200c\u0304 \u2208 \u00afV (r) pc\u0304 =\nexp(scorec\u0304)\u2211 c\u0302\u2208 \u00afV (r) exp(scorec\u0302)\n,\u2200c\u0304 \u2208 \u00afV (r) v(r) = \u2211 c\u0304\u2208 \u00afV (r) pc\u0304 \u00d7 v(c\u0304)\nThe model pools relevant information over the entire set of observed columns and selects the most salient aspects to the query."}, {"heading": "3.3 Training", "text": "The vector representation of the rows and the columns are the parameters of the model. Riedel et al. (2013)\nModel Parameters Entity Embeddings 3.7 e6 Attention 3.1 e5 Mean Pool/Max Pool/Max Relation 1.5 e5\nTable 1: Number of parameters for the different models on the entity type dataset.\nModel Parameters Entity-pair Embeddings 1.2 e8 Attention 1.4 e8 Mean Pool/Max Pool/Max Relation 6.9 e7\nTable 2: Number of parameters for the different relation extraction models on FB15K-237 data\nuse Bayesian Personalized Ranking (BPR) (Rendle et al., 2009) to train their universal schema models. BPR ranks the probability of observed triples above unobserved triples rather than explicitly modeling unobserved edges as negative. Each training example is an entity pair/relation type or entity/entity type pair observed in the training text corpora or KB.\nRather than BPR, Toutanova et al. (2015) use 200 negative samples to approximate the negative log likelihood1. In our experiments, we use the sampled approximate negative log likelihood which outperformed BPR in early experiments.\nEach example in the training procedure consists of a row-column pair observed in the training set. For a positive example (r, c) \u2208 T , we construct the set \u00afV (r) containing all the other column entries apart from c that are observed with row r to obtain the aggregated row representation as discussed above. We randomly sample a column unobserved with row r to act as the negative sample. All models are implemented in Torch2 and are trained using Adam (Kingma and Ba, 2015) with default momentum related hyperparameters."}, {"heading": "4 Related Work", "text": "Relation extraction for KB completion has a long history. Mintz et al. (2009) train per relation linear classifiers using features derived from the sentences in which the entity pair is mentioned. Most of the embedding-based methods learn representations for entities (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) whereas Riedel et al. (2013) use entity pair representations. \u2018Column-less\u2019 versions of Universal Schema have been proposed (Toutanova et al., 2015; Verga et al., 2016). These models can generalize to column entries\n1Many past papers restrict negative samples to be of the same type as the positive example. We simply sample uniformly from the entire set of row entries\n2data and code available at https://github.com/ patverga/torch-relation-extraction\n(Bill Gates/Melinda Gates) Output Encoder\nper:spouse Attention Encoder\nInner Product + Softmax\nWeighted Avg\n- arg1 married arg2- arg1 \u2018s wife arg2- arg1 co-founded the foundation with arg 2 { }\nInput Output\nQuery Encoder\nFigure 4: Example attention model in a row-less universal schema relation extractor. In the attention model, we compute the dot product between the representation of the query relation and the representation of an entity pair\u2019s observed relation type followed by a softmax, giving a weighting over the observed relation types. This output is then used to get a weighted sum over the set of representations of the observed relation types. The result is a queryspecific vector representation of the entity pair. The Max Relation model takes the most similar observed relation\u2019s representation.\nunseen at training by learning compositional pattern encoders to parameterize the column matrix in place of embeddings. Most of these models do not generalize to unseen entity pairs and none of them generalize to unseen entities. Recently, Neelakantan et al. (2015) introduced a multi-hop relation extraction model that is \u2018row-less\u2019 having no explicit parameters for entity pairs and entities. Concurrent to our work, Weissenborn (2016) proposes a row-less method for relation extraction.\nEntity type prediction at the individual sentence level has been studied extensively (Pantel et al., 2012; Ling and Weld, 2012; Shimaoka et al., 2016). More recently, embedding-based methods for knowledge base entity type prediction have been proposed (Yao et al., 2013; Neelakantan and Chang, 2015). These methods have explicit entity representations, hence cannot generalize to unseen entities.\nThe task of generalizing to unseen row and column entries is referred to as the cold-start problem in recommendation systems. Methods proposed to tackle this problem commonly use user and item content and attributes (Schein et al., 2002; Park and Chu, 2009)."}, {"heading": "5 Experimental Results", "text": "In this section, we compare our models that have aggregate row representations with models that have explicit row representations on entity type prediction and relation extraction tasks. Finally, we perform experiments on a column-less universal schema model. Table 1 and Table 2 show that the row-less models require an order of magnitude fewer parameters since they do not explicitly store\nthe row representations."}, {"heading": "5.1 Entity Type Prediction", "text": "We first evaluate our models on an entity type prediction task. We collect all entities along with their types from a dump of Freebase3. We then filter all entities with less than five Freebase types leaving a set of 844780 entity type pairs. Additionally, we collect 712072 text types from Clueweb. They are appositives extracted from sentences mentioning entities and we select the 5000 most common ones. This results in 140513 unique entities, 1120 Freebase relation types, and 5000 free text relation types.\nAll embeddings are 25 dimensions, randomly initialized. We tune learning rates from {.01, .001}, `2 from {1e-8, 0}, batch size {512, 1024, 2048} and negative samples from {2, 200}.\nFor evaluation, we split the Freebase entity type pairs into 60% train, 20% validation, and 20% test. For each positive entity-type pair in our validation or test set, we randomly select 100 negative entity type pairs and filter false negatives. This results in 707 test relation types over which we calculate MAP.\nTable 3 shows the results of this experiment. We can see that the query dependent aggregation performs better than query independent. The performance of models with query dependent aggregation functions which have far fewer parameters match the performance of the model with explicit entity representations.\n3Downloaded March 1, 2015.\nWe additionally evaluate our model\u2019s ability to predict types for entities unseen during training. For this, we randomly select 14000 entities from our freebase entity type pairs. We remove these entities from our training set and use them as positive samples in our evaluation. This set contains 423 relation types which we calculate MAP for.\nTable 3 shows the results of the experiment with unseen entities. There is no performance drop for models trained with query dependent aggregation functions whereas the performance of the model with explicit entity representations is close to random."}, {"heading": "5.1.1 Qualitative Results", "text": "A query specific aggregation function is able to pick out relevant columns to form a prediction. This is particularly important for rows that are not described easily by a single centroid such as an entity with several very different careers or an entity pair with multiple highly varied relations. For example, in the first row in Table 5, for the query /baseball/baseball player the model needs to correctly focus on aspects like /sports/pro athlete and ignore evidence information like /tv/tv actor. A model that creates a single query-independent centroid will be forced to try and merge these disparate pieces of information together."}, {"heading": "5.2 Relation Extraction", "text": "We evaluate our models on the FB15k-237 dataset from Toutanova et al. (2015). The data is composed of a small set of 237 Freebase relations and approximately 4 million textual patterns from Clueweb with entities linked to Freebase (Gabrilovich et al., 2013). In past studies, for each (subject, relation, object) test triple, negative examples are generated by replacing the object with all other entities, filtering out triples that are positive in the data set. The positive triple is then ranked among the negatives. In our experiments we limit the possible generated negatives to those entity pairs that have textual mentions in our training set. This way we can evaluate how well the model classifies textual mentions as Freebase relations. We also filter textual patterns with length greater than 35. Our filtered data set contains 2740237 relation types, 2014429 entity pairs, and 176476 tokens. We report the percentage of positive triples ranked in the top 10 amongst their negatives as well as the MRR scaled by 100.\nModels are tuned to maximize mean reciprocal rank (MRR) on the validation set with early stopping. The entity pair model used a batch size 1024, `2 = 1e-8, = 1e4, and learning rate 0.01. The aggregation models all used batch size 4096, `2 = 0, = 1e-8, and learning rate 0.01. Each use 200 negative samples except for max pool which performed better with two negative samples The column vectors are initialized with the columns learned by the entity pair model. Randomly initializing the query encoders and tying the output and attention encoders performed better and all results use this method. All models are trained with embedding dimension 25.\nOur results are shown in Table 6. We can see that the models with query specific aggregation functions give the same results as models with explicit entity pair representations. The max-model performs competitively with the attention model which is not entirely surprising as it is a simplified version of the attention model. Further, the attention model reduces to the max-relation model for entity pairs with only a single observed relation type. In our data, 64.8% of entity pairs have only a single observed relation type and 80.9% have 1 or 2 observed relation types.\nWe also explore the models\u2019 abilities to predict on unseen entity pairs (Table 7). We remove all training examples that contain a positive entity pair in either our validation or test set. We use the same validation and test set as in Table 6. The entity pair model predicts random relations as it is unable to make predictions on unseen entity pairs. The query-independent aggregation functions, mean pool and max pool, perform better than models with explicit entity pair representations. Again, query specific aggregation functions get the best results, with the attention model performing slightly better than the max-relation model.\nThe two experiments indicate that we can train relation extraction models without explicit entity pair representations that perform as well as models with explicit representations. We also find that models with query specific aggregation functions predict relations for unseen entity pairs without drop in accuracy compared to predicting relations for seen entity pairs."}, {"heading": "5.3 \u2018Column-less\u2019 universal schema", "text": "The original universal schema approach has two main drawbacks: similar textual patterns do not share statistics, and the model is unable to make predictions about entities and textual patterns not explicitly seen at train time.\nRecently, \u2018column-less\u2019 versions of universal schema to address some of these issues (Toutanova et al., 2015; Verga et al., 2016). These models learn compositional pattern encoders to parameterize the column matrix in place of directly embeddings. Compositional universal schema facilitates more compact sharing of statistics by composing similar patterns from the same sequence of word embeddings \u2013 the text patterns \u2018lives in the city\u2019 and \u2018lives in the city of\u2019 no longer exist as distinct atomic units. More importantly, compositional universal schema can thus generalize to all possible textual patterns, facilitating reasoning over arbitrary text at test time.\nThe column-less universal schema model generalizes to all possible input textual relations and the row-less model generalizes to all entities and entity pairs, whether seen at train time or not. We can combine these two approaches together to make an universal schema model that generalizes to unseen rows and columns.\nThe parse path between the two entities in the sentence is encoded with an LSTM model. We use a single layer model with 100 dimensional token embeddings initialized randomly. To prevent exploding gradients, we clip them to norm 10 while all the other hyperparameters are tuned the same way as before. We follow the same evaluation protocol from 5.2.\nThe results of this experiment with observed rows are shown in Table 8. While both the MRR and Hits@10 metrics increase for models with explicit row representations, the row-less models show an improvement only on the Hits@10 metric. The MRR of the query dependent row-less models is still competitive with the model\nwith explicit row representation even though they have far fewer parameters to fit the data.\nTable shows the performance on the test set with unseen entity pairs. Once again, we can see that row-less models predict relations for unseen entity pairs without drop in accuracy."}, {"heading": "6 Conclusion", "text": "In this paper we explore a row-less extension of universal schema that forgoes explicit row representations for an aggregation function over its observed columns. This extension allows prediction between all rows in new textual mentions \u2013 whether seen at train time or not \u2013 and also provides a natural connection to the provenance supporting the prediction. Our models also have a smaller memory footprint.\nIn this work we show that an aggregation function based on query-specific attention over relation types outperforms query independent aggregations. We show that aggregation models are able to predict on par with models with explicit row representations on seen row entries\nwith an order of magnitude fewer parameters. More importantly, aggregation models predict on unseen row entires without loss in accuracy. Finally, we show that in relation extraction, we can train models that generalize to both unseen rows and columns."}], "references": [{"title": "Neural Machine Translation", "author": ["Yoshua Bengio"], "venue": null, "citeRegEx": "Bengio.,? \\Q2014\\E", "shortCiteRegEx": "Bengio.", "year": 2014}, {"title": "Learning entity and relation", "author": ["Liu", "Xuan Zhu"], "venue": null, "citeRegEx": "Liu and Zhu.,? \\Q2015\\E", "shortCiteRegEx": "Liu and Zhu.", "year": 2015}, {"title": "Fine-grained entity recognition", "author": ["Ling", "Weld2012] Xiao Ling", "Daniel S. Weld"], "venue": "In Association for the Advancement of Artificial Intelligence", "citeRegEx": "Ling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2012}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky"], "venue": "In Association for Computational Linguistics and International Joint Conference on Natural Language Processing", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Inferring missing entity type instances for knowledge base completion: New dataset and methods", "author": ["Neelakantan", "Chang2015] Arvind Neelakantan", "MingWei Chang"], "venue": "In North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Efficient non-parametric estimation of multiple embeddings per word in vector space", "author": ["Jeevan Shankar", "Alexandre Passos", "Andrew McCallum"], "venue": "In Empirical Methods in Natural Language Processing", "citeRegEx": "Neelakantan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Compositional vector space models for knowledge base completion", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Neural Programmer: Inducing latent programs with gradient descent", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": "In ICLR", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "HansPeter Kriegel"], "venue": "In International Conference on Machine Learning", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "11 billion clues in 800 million documents: A web research corpus annotated with freebase concepts", "author": ["Orr et al.2013] Dave Orr", "Amarnag Subramanya", "Evgeniy Gabrilovich", "Michael Ringgaard"], "venue": null, "citeRegEx": "Orr et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Orr et al\\.", "year": 2013}, {"title": "Mining entity types from query logs via user intent modeling", "author": ["Thomas Lin", "Michael Gamon"], "venue": "In Association for Computational Linguistics", "citeRegEx": "Pantel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pantel et al\\.", "year": 2012}, {"title": "Pairwise preference regression for cold-start recommendation", "author": ["Park", "Chu2009] Seung-Taek Park", "Wei Chu"], "venue": "In RecSys", "citeRegEx": "Park et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Park et al\\.", "year": 2009}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["Christoph Freudenthaler", "Zeno Gantner", "Lars Schmidt-Thieme"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Rendle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Rendle et al\\.", "year": 2009}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M. Marlin"], "venue": "HLTNAACL", "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Methods and metrics for cold-start recommendations", "author": ["Alexandrin Popescul", "Lyle H Ungar", "David M Pennock"], "venue": "In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Schein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Schein et al\\.", "year": 2002}, {"title": "An attentive neural architecture for fine-grained entity type classification", "author": ["Pontus Stenetorp", "Kentaro Inui", "Sebastian Riedel"], "venue": null, "citeRegEx": "Shimaoka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shimaoka et al\\.", "year": 2016}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Yago: A core of semantic knowledge", "author": ["Gjergji Kasneci", "Gerhard Weikum"], "venue": "In Proceedings of the 16th International Conference on World Wide Web", "citeRegEx": "Suchanek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Suchanek et al\\.", "year": 2007}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Representing text for joint embedding of text and knowledge bases", "author": ["Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Toutanova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2015}, {"title": "Multilingual relation extraction using compositional universal schema", "author": ["Verga et al.2016] Patrick Verga", "David Belanger", "Emma Strubell", "Benjamin Roth", "Andrew McCallum"], "venue": "Annual Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "Verga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Embedding entity pairs through observed relations for knowledge base completion", "author": ["Dirk Weissenborn"], "venue": null, "citeRegEx": "Weissenborn.,? \\Q2016\\E", "shortCiteRegEx": "Weissenborn.", "year": 2016}, {"title": "Nonlinear latent factorization by embedding multiple user interests", "author": ["Weston et al.2013] Jason Weston", "Ron Weiss", "Hector Yee"], "venue": "In ACM International Conference on Recommender Systems", "citeRegEx": "Weston et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2013}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang et al.2015] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "International Conference on Learning Representations", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Universal schema for entity type prediction", "author": ["Yao et al.2013] Limin Yao", "Sebastian Riedel", "Andrew McCallum"], "venue": "In Proceedings of the 2013 workshop on Automated knowledge base construction,", "citeRegEx": "Yao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "Automatic knowledge base construction (AKBC) is the task of building a structured knowledge base (KB) of facts using raw text evidence, and often an initial seed KB to be augmented (Carlson et al., 2010; Suchanek et al., 2007; Bollacker et al., 2008).", "startOffset": 181, "endOffset": 250}, {"referenceID": 13, "context": "In the standard formulation for relation extraction (Riedel et al., 2013), entity pairs and relations occupy the rows and columns of the matrix respectively (Figure 1).", "startOffset": 52, "endOffset": 73}, {"referenceID": 25, "context": "Analogously in entity type prediction (Yao et al., 2013), entities and types occupy the rows and columns of the matrix respectively (Figure 2).", "startOffset": 38, "endOffset": 56}, {"referenceID": 14, "context": "This problem is referred to as the cold-start problem in recommendation systems (Schein et al., 2002).", "startOffset": 80, "endOffset": 101}, {"referenceID": 19, "context": "Recently Toutanova et al. (2015) and Verga et al.", "startOffset": 9, "endOffset": 33}, {"referenceID": 19, "context": "Recently Toutanova et al. (2015) and Verga et al. (2016) proposed \u2018column-less\u2019 versions of universal schema models that generalize to unseen column entries.", "startOffset": 9, "endOffset": 57}, {"referenceID": 25, "context": "The current methods for KB entity type prediction operate with explicit entity representations (Yao et al., 2013; Neelakantan and Chang, 2015) and hence, cannot generalize to unseen entities.", "startOffset": 95, "endOffset": 142}, {"referenceID": 8, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time.", "startOffset": 43, "endOffset": 190}, {"referenceID": 24, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time.", "startOffset": 43, "endOffset": 190}, {"referenceID": 21, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time.", "startOffset": 43, "endOffset": 190}, {"referenceID": 16, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time.", "startOffset": 43, "endOffset": 190}, {"referenceID": 8, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time. These models learn representations for the entities instead of entity pairs. Hence, these methods still cannot generalize to predict relations between an entity pair if even one of the entities is unseen. Moreover, Toutanova et al. (2015) and Riedel et al.", "startOffset": 44, "endOffset": 475}, {"referenceID": 8, "context": "In relation extraction, entitylevel models (Nickel et al., 2011; Garc\u0131\u0301a-Dur\u00e1n et al., 2015; Yang et al., 2015; Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Socher et al., 2013) can handle unseen entity pairs at test time. These models learn representations for the entities instead of entity pairs. Hence, these methods still cannot generalize to predict relations between an entity pair if even one of the entities is unseen. Moreover, Toutanova et al. (2015) and Riedel et al. (2013) observe that the entity pair model outperforms entity models in cases where the entity pair was seen at training time.", "startOffset": 44, "endOffset": 500}, {"referenceID": 9, "context": ", 2008) augmented with textual relations and types from Clueweb text (Orr et al., 2013; Gabrilovich et al., 2013).", "startOffset": 69, "endOffset": 113}, {"referenceID": 13, "context": "Universal schema (Riedel et al., 2013; Yao et al., 2013) relation extraction and entity type prediction is typically modeled as a matrix completion task.", "startOffset": 17, "endOffset": 56}, {"referenceID": 25, "context": "Universal schema (Riedel et al., 2013; Yao et al., 2013) relation extraction and entity type prediction is typically modeled as a matrix completion task.", "startOffset": 17, "endOffset": 56}, {"referenceID": 12, "context": "The embeddings are learned using Bayesian Personalized Ranking (BPR) (Rendle et al., 2009) in which the probability of the observed triples are ranked above unobserved triples.", "startOffset": 69, "endOffset": 90}, {"referenceID": 20, "context": "Verga et al. (2016) use column-less universal schema for relation extraction.", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "A similar strategy has been successfully applied in previous work (Weston et al., 2013; Neelakantan et al., 2014; Neelakantan et al., 2015) for different tasks.", "startOffset": 66, "endOffset": 139}, {"referenceID": 5, "context": "A similar strategy has been successfully applied in previous work (Weston et al., 2013; Neelakantan et al., 2014; Neelakantan et al., 2015) for different tasks.", "startOffset": 66, "endOffset": 139}, {"referenceID": 4, "context": "A similar strategy has been successfully applied in previous work (Weston et al., 2013; Neelakantan et al., 2014; Neelakantan et al., 2015) for different tasks.", "startOffset": 66, "endOffset": 139}, {"referenceID": 18, "context": "Finally, we look at an Attention aggregation function over columns (Figure 4) which is similar to a single-layer memory network (Sukhbaatar et al., 2015).", "startOffset": 128, "endOffset": 153}, {"referenceID": 7, "context": "The soft attention mechanism has been used to selectively focus on relevant parts in many different models (Bahdanau et al., 2014; Graves et al., 2014; Neelakantan et al., 2016).", "startOffset": 107, "endOffset": 177}, {"referenceID": 13, "context": "Riedel et al. (2013) Model Parameters", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "use Bayesian Personalized Ranking (BPR) (Rendle et al., 2009) to train their universal schema models.", "startOffset": 40, "endOffset": 61}, {"referenceID": 12, "context": "use Bayesian Personalized Ranking (BPR) (Rendle et al., 2009) to train their universal schema models. BPR ranks the probability of observed triples above unobserved triples rather than explicitly modeling unobserved edges as negative. Each training example is an entity pair/relation type or entity/entity type pair observed in the training text corpora or KB. Rather than BPR, Toutanova et al. (2015) use 200 negative samples to approximate the negative log likelihood1.", "startOffset": 41, "endOffset": 402}, {"referenceID": 8, "context": "Most of the embedding-based methods learn representations for entities (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) whereas Riedel et al.", "startOffset": 71, "endOffset": 152}, {"referenceID": 16, "context": "Most of the embedding-based methods learn representations for entities (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) whereas Riedel et al.", "startOffset": 71, "endOffset": 152}, {"referenceID": 19, "context": "\u2018Column-less\u2019 versions of Universal Schema have been proposed (Toutanova et al., 2015; Verga et al., 2016).", "startOffset": 62, "endOffset": 106}, {"referenceID": 20, "context": "\u2018Column-less\u2019 versions of Universal Schema have been proposed (Toutanova et al., 2015; Verga et al., 2016).", "startOffset": 62, "endOffset": 106}, {"referenceID": 3, "context": "Mintz et al. (2009) train per relation linear classifiers using features derived from the sentences in which the entity pair is mentioned.", "startOffset": 0, "endOffset": 20}, {"referenceID": 3, "context": "Mintz et al. (2009) train per relation linear classifiers using features derived from the sentences in which the entity pair is mentioned. Most of the embedding-based methods learn representations for entities (Nickel et al., 2011; Lao et al., 2011; Socher et al., 2013; Bordes et al., 2013) whereas Riedel et al. (2013) use entity pair representations.", "startOffset": 0, "endOffset": 321}, {"referenceID": 10, "context": "Entity type prediction at the individual sentence level has been studied extensively (Pantel et al., 2012; Ling and Weld, 2012; Shimaoka et al., 2016).", "startOffset": 85, "endOffset": 150}, {"referenceID": 15, "context": "Entity type prediction at the individual sentence level has been studied extensively (Pantel et al., 2012; Ling and Weld, 2012; Shimaoka et al., 2016).", "startOffset": 85, "endOffset": 150}, {"referenceID": 25, "context": "More recently, embedding-based methods for knowledge base entity type prediction have been proposed (Yao et al., 2013; Neelakantan and Chang, 2015).", "startOffset": 100, "endOffset": 147}, {"referenceID": 14, "context": "Methods proposed to tackle this problem commonly use user and item content and attributes (Schein et al., 2002; Park and Chu, 2009).", "startOffset": 90, "endOffset": 131}, {"referenceID": 4, "context": "Recently, Neelakantan et al. (2015) introduced a multi-hop relation extraction model that is \u2018row-less\u2019 having no explicit parameters for entity pairs and entities.", "startOffset": 10, "endOffset": 36}, {"referenceID": 4, "context": "Recently, Neelakantan et al. (2015) introduced a multi-hop relation extraction model that is \u2018row-less\u2019 having no explicit parameters for entity pairs and entities. Concurrent to our work, Weissenborn (2016) proposes a row-less method for relation extraction.", "startOffset": 10, "endOffset": 208}, {"referenceID": 19, "context": "We evaluate our models on the FB15k-237 dataset from Toutanova et al. (2015). The data is composed of a small set of 237 Freebase relations and approximately 4 million textual patterns from Clueweb with entities linked to Freebase (Gabrilovich et al.", "startOffset": 53, "endOffset": 77}, {"referenceID": 19, "context": "Recently, \u2018column-less\u2019 versions of universal schema to address some of these issues (Toutanova et al., 2015; Verga et al., 2016).", "startOffset": 85, "endOffset": 129}, {"referenceID": 20, "context": "Recently, \u2018column-less\u2019 versions of universal schema to address some of these issues (Toutanova et al., 2015; Verga et al., 2016).", "startOffset": 85, "endOffset": 129}], "year": 2016, "abstractText": "Universal schema predicts the types of entities and relations in a knowledge base (KB) by jointly embedding the union of all available schema types\u2014not only types from multiple structured databases (such as Freebase or Wikipedia infoboxes), but also types expressed as textual patterns from raw text. This prediction is typically modeled as a matrix completion problem, with one type per column, and either one or two entities per row (in the case of entity types or binary relation types, respectively). Factorizing this sparsely observed matrix yields a learned vector embedding for each row and each column. In this paper we explore the problem of making predictions for entities or entity-pairs unseen at training time (and hence without a pre-learned row embedding). We propose an approach having no per-row parameters at all; rather we produce a row vector on the fly using a learned aggregation function of the vectors of the observed columns for that row. We experiment with various aggregation functions, including neural network attention models. Our approach can be understood as a natural language database, in that questions about KB entities are answered by attending to textual or database evidence. In experiments predicting both relations and entity types, we demonstrate that despite having an order of magnitude fewer parameters than traditional universal schema, we can match the accuracy of the traditional model, and more importantly, we can now make predictions about unseen rows with nearly the same accuracy as rows available at training time.", "creator": "TeX"}}}