{"id": "1606.06061", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2016", "title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices", "abstract": "simulated acoustic regression models studied based on long lifetime short - term memory encoding recurrent tandem neural filter networks ( lstm - modelled rnns ) were firstly applied to integrated statistical parametric speech synthesis ( spss ) and showed quite significant cognitive improvements in naturalness and latency significantly over twice those based on large hidden markov matrix models ( hmms ). this fundamental paper explicitly describes further optimizations of tandem lstm - rnn - based engineering spss trial for deployment on many mobile devices ; variable weight quantization, multi - sampled frame inference, and functional robust inference using evaluating an { \\ plus epsilon } - simulated contaminated gaussian loss function. experimental results incorporated in subjective sample listening strategy tests overwhelmingly show hints that these optimizations can make lstm - rnn - r based programming spss comparable to hmm - bean based - spss similar in runtime listening speed while temporarily maintaining naturalness. evaluations between standardized lstm - simulated rnn - based - spss trials and experimental hmm - device driven mixed unit selection optimization speech synthesis paradigm are essentially also constantly presented.", "histories": [["v1", "Mon, 20 Jun 2016 10:54:51 GMT  (1198kb,D)", "https://arxiv.org/abs/1606.06061v1", "13 pages, 3 figures, Interspeech 2016 (accepted)"], ["v2", "Wed, 22 Jun 2016 15:11:30 GMT  (1199kb,D)", "http://arxiv.org/abs/1606.06061v2", "13 pages, 3 figures, Interspeech 2016 (accepted)"]], "COMMENTS": "13 pages, 3 figures, Interspeech 2016 (accepted)", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["heiga zen", "yannis agiomyrgiannakis", "niels egberts", "fergus henderson", "przemys{\\l}aw szczepaniak"], "accepted": false, "id": "1606.06061"}, "pdf": {"name": "1606.06061.pdf", "metadata": {"source": "CRF", "title": "Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric Speech Synthesizers for Mobile Devices", "authors": ["Heiga Zen", "Yannis Agiomyrgiannakis", "Niels Egberts", "Fergus Henderson", "Przemys\u0142aw Szczepaniak"], "emails": ["heigazen@google.com", "agios@google.com", "nielse@google.com", "fergus@google.com", "pszczepaniak@google.com"], "sections": [{"heading": null, "text": "Index Terms: statistical parametric speech synthesis, recurrent neural networks."}, {"heading": "1 Introduction", "text": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320]. ANN-based acoustic models offer an efficient and distributed representation of complex dependencies between linguistic and acoustic features [21, 22] and have shown the potential to produce natural sounding synthesized speech [2, 4, 7\u20139]. Recurrent neural networks (RNNs) [23], especially long short-term memory (LSTM)RNNs [24], provide an elegant way to model speech-like sequential data that embodies short- and long-term correlations. They were successfully applied to acoustic modeling for SPSS [8\u201311]. Zen et al. proposed a streaming speech synthesis architecture using unidirectional LSTM-RNNs with a recurrent output layer [9]. It enabled low-latency speech synthesis, which is essential in some applications. However, it was significantly slower than hidden Markov model (HMM)-based SPSS [25] in terms of real-time ratio [26]. This paper describes further optimizations of LSTM-RNN-based SPSS for deployment on mobile devices. The optimizations conducted here include reducing computation and disk footprint, as well as making it robust to errors in training data.\nThe rest of this paper is organized as follows. Section 2 describes the proposed optimizations. Experiments and subjective evaluation-based findings are presented in\nar X\niv :1\n60 6.\n06 06\n1v 2\n[ cs\n.S D\n] 2\n2 Ju\nAcoustic features (outputs)\nWaveform\nSection 3. Concluding remarks are shown in the final section."}, {"heading": "2 Optimizing LSTM-RNN-based SPSS", "text": "Figure 1 shows the overview of the streaming synthesis architecture using unidirectional LSTM-RNNs [9]. Unlike HMM-based SPSS, which usually requires utterancelevel batch processing [27] or frame lookahead [28], this architecture allows framesynchronous streaming synthesis with no frame lookahead. Therefore this architecture provides much lower latency speech synthesis. However, there are still a few drawbacks;\n\u2022 Disk footprint; Although the total number of parameters in LSTM-RNN-based SPSS can be significantly lower than that of HMM-based SPSS [9], the overall disk footprint of the LSTM-RNN system can be similar or slightly larger because HMM parameters can be quantized using 8-bit integers [29]. Therefore decreasing the LSTM-RNN system disk footprint is essential for deployment on mobile devices.\n\u2022 Computation; With HMM-based SPSS, inference of acoustic parameters involves traversing decision trees at each HMM state and running the speech parameter generation algorithm [27]. On the other hand, inference of acoustic\nparameters with LSTM-RNN-based SPSS involves many matrix-vector multiplications at each frame, which are expensive. This is particularly critical for client-side TTS on mobile devices, which have less powerful CPUs and limited battery capacity.\n\u2022 Robustness; Typical ANN-based SPSS relies on fixed phoneme- or state-level alignments [2], whereas HMMs can be trained without fixed alignments using the Baum-Welch algorithm. Therefore, the ANN-based approach is less robust to alignment errors.\nThis section describes optimizations addressing these drawbacks. Each of them that follow will be evaluated in Section 3."}, {"heading": "2.1 Weight quantization", "text": "ANN weights are typically stored in 32-bit floating-point numbers. However there are significant advantages in memory, disk footprint and processing performance in representing them in lower integer precision. This is commonly approached by quantizing the ANN weights. This paper utilizes 8-bit quantization of ANN weights [30] to reduce the disk footprint of LSTM-RNN-based acoustic and duration models. Although it is possible to run inference in 8-bit integers with quantization-aware training [30], that possibility is not utilized here; instead weights are stored in 8-bit integer on disk then recovered to 32-bit floating-point numbers after loading to memory."}, {"heading": "2.2 Multi-frame bundled inference", "text": "Inference of acoustic frames takes 60\u201370% of total computations in our LSTM-RNNbased SPSS implementation. Therefore, it is desirable to reduce the amount of computations at the inference stage. In typical ANN-based SPSS, input linguistic features other than state- and frame-position features are constant within a phoneme [2]. Furthermore, speech is a rather stationary process at 5-ms frame shift and target acoustic frames change slowly across frames. Based on these characteristics of inputs and targets this paper explores the multi-frame inference approach [31]. Figure 2 illustrates the concept of multi-frame inference. Instead of predicting one acoustic frame, multiple acoustic frames are jointly predicted at the same time instance. This architecture allows significant reduction in computation while maintaining the streaming capability.\nHowever, preliminary experiments showed degradation due to mismatch between training and synthesis; alignments between input/target features can be different at the synthesis stage, e.g., training: x2 \u2192 {y1,y2}, synthesis: x3 \u2192 {y2,y3}. This issue can be addressed by data augmentation. Figure 3 shows the data augmentation with different frame offset. From aligned input/target pairs, multiple data sequences can be generated with different starting frame offset. By using these data sequences for training, acoustic LSTM-RNNs will generalize to different possible alignments between inputs and targets."}, {"heading": "2.3 Robust regression", "text": "It is known that learning a linear regression model with the squared loss function can suffer from the effect of outliers. Although ANNs trained with the squared loss function are not a simple linear regression model, their output layers perform linear regression given activations at the last hidden layer. Therefore, ANNs trained with the squared\nloss function can be affected by outliers. These outliers can come from recordings, transcriptions, forced alignments, and F0 extraction errors.\nUsing robust regression techniques such as linear regression with a heavy-tailed distribution [32] or minimum density power divergence estimator [33] can relax the effect of outliers. In this work a simple robust regression technique assuming that the errors follow a mixture of two Gaussian distributions, in particular, -contaminated Gaussian distribution [34], which is a special case of the Richter distribution [35\u201337], is employed; the majority of observations are from a specified Gaussian distribution, though a small proportion are from a Gaussian distribution with much higher variance, while the two Gaussian distributions share the same mean. The loss function can be defined as\nL(z;x,\u039b) = \u2212 log { (1 \u2212 )N (z; f(x;\u039b),\u03a3) + N (z; f(x;\u039b), c\u03a3) } , (1)\nwhere z and x denote target and input vectors, \u03a3 is a covariance matrix, and c are weight and scale of outliers, \u039b is a set of neural network weights, and f(\u00b7) is a nonlinear function to predict an output vector given the input vector. Typically, < 0.5 and c > 1. Note that if = 0 and \u03a3 = I , the -contaminated Gaussian loss function is equivalent to the squared loss function. Figure 4 illustrates -contaminated Gaussian distribution (\u00b5 = [0], \u03a3 = [1], c = 10 and = 0.1). It can be seen from the figure that the -contaminated Gaussian distribution has heavier tail than the Gaussian distribution. As outliers will be captured by the Gaussian distribution with wider\nvariances, the estimation of means is less affected by these outliers. Here using the - contaminated Gaussian loss function as a criterion to train LSTM-RNNs is investigated for both acoustic and duration LSTM-RNNs. Note that the -contaminated Gaussian distribution is similar to globally tied distribution (GTD) in [38]."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Experimental conditions", "text": "Speech data from a female professional speaker was used to train speaker-dependent unidirectional LSTM-RNNs for each language. The configuration for speech analysis stage and data preparation process were the same as those described in [9] except the use of speech at 22.05 kHz sampling rather than 16 kHz and 7-band aperiodicities rather than 5-band ones.\nBoth the input and target features were normalized to be zero-mean unit-variance in advance. The architecture of the acoustic LSTM-RNNs was 1 \u00d7 128-unit ReLU [39] layer followed by 3 \u00d7 128-cell LSTMP layers [40] with 64 recurrent projection units with a linear recurrent output layer [9]. The duration LSTM-RNN used a single LSTM layer with 64 cells with feed-forward output layer with linear activation. To reduce the training time and impact of having many silence frames, 80% of silence frames were removed from the training data. Durations of the beginning and ending silences were excluded from the training data for the duration LSTM-RNNs. The weights of the LSTM-RNNs were initialized randomly. Then they were updated to minimize the mean squared error between the target and predicted output features. A distributed CPU implementation of mini-batch asynchronous stochastic gradient descent (ASGD)based truncated back propagation through time (BPTT) [41] algorithm was used [40]. The learning rates for the acoustic and duration LSTM-RNNs were 10\u22125 and 10\u22126, respectively. The learning rates were exponentially decreased over time [42]. Training was continued until the loss value over the development set converged. The model architecture and hyper-parameters were used across all languages.\nAt the synthesis stage, durations and acoustic features were predicted from linguistic features using the trained networks. Spectral enhancement based on post-filtering in the cepstral domain [25] was applied to improve the naturalness of the synthesized speech. From the acoustic features, speech waveforms were synthesized using the Vocaine vocoder [43].\nTo subjectively evaluate the performance of the systems, preference tests were also conducted. 100 utterances not included in the training data were used for evaluation. Each pair was evaluated by at least eight native speakers of each language. The subjects who did not use headphones were excluded from the experimental results. After listening to each pair of samples, the subjects were asked to choose their preferred one, or they could choose \u201cno preference\u201d if they did not have any preference. Note that stimuli that achieved a statistically significant preference (p < 0.01) are presented in bold characters in tables displaying experimental results in this section."}, {"heading": "3.2 Experimental results for optimizations", "text": ""}, {"heading": "3.2.1 Weight quantization", "text": "Table 1 shows the preference test result comparing LSTM-RNNs with and without weight quantization. It can be seen from the table that the effect of quantization was negligible. The disk footprint of the acoustic LSTM-RNN for English (NA) was reduced from 1.05 MBytes to 272 KBytes."}, {"heading": "3.2.2 Multi-frame inference", "text": "While training multi-frame LSTM-RNNs, the learning rate needed to be reduced (from 10\u22125 to 2.5\u00d710\u22126) as mentioned in [31]. Table 2 shows the preference test result comparing single and multi-frame inference. Note that weights of the LSTM-RNNs were quantized to 8-bit integers. It can be seen from the table that LSTM-RNN with multiframe inference with data augmentation achieved the same naturalness as that with single-frame one. Compared with 1-frame, 4-frame achieved about 40% reduction of walltime at runtime synthesis."}, {"heading": "3.2.3 -contaminated Gaussian loss function", "text": "Although c, , and \u03a3 could be trained with the network weights, they were fixed to c = 10, = 0.1, and \u03a3 = I for both acoustic and duration LSTM-RNNs.Therefore, the numbers of parameters of the LSTM-RNNs trained with the squared and - contaminated Gaussian loss functions were identical. For training LSTM-RNNs with the -contaminated Gaussian loss function, the learning rate could be increased (from 2.5 \u00d7 10\u22126 to 5 \u00d7 10\u22126 for acoustic LSTM-RNNs, from 10\u22126 to 5 \u00d7 10\u22126 for duration LSTM-RNNs). From a few preliminary experiments, the -contaminated Gaussian loss function with a 2-block structure was selected; 1) mel-cepstrum and aperiodicities, 2) logF0 and voiced/unvoiced binary flag. This is similar to the multi-stream HMM structure [44] used in HMM-based speech synthesis [25]. Table 3 shows the preference test result comparing the squared and -contaminated normal loss function to train LSTM-RNNs. Note that all weights of the LSTM-RNNs were quantized to 8-bit integers and 4-frame bundled inference was used. It can be seen from the table that LSTM-RNN trained with the -contaminated normal loss function achieved the same or better naturalness than those with the squared loss function."}, {"heading": "3.3 Comparison with HMM-based SPSS", "text": "The next experiment compared HMM- and LSTM-RNN-based SPSS with the optimizations described in this paper. Both HMM- and LSTM-RNN-based acoustic and duration models were quantized into 8-bit integers. The same training data and text processing front-end modules were used.\nThe average disk footprints of HMMs and LSTM-RNNs including both acoustic and duration models over 6 languages were 1560 and 454.5 KBytes, respectively. Table 4 shows the average latency (time to get the first chunk of audio) and average total synthesis time (time to get the entire audio) of the HMM and LSTM-RNN-based SPSS systems (North American English) to synthesize a character, word, sentence, and paragraph on a Nexus 6 phone. Note that the execution binary was compiled for modern ARM CPUs having the NEON advanced single instruction, multiple data (SIMD) instruction set [45]. To reduce the latency of the HMM-based SPSS system, the recursive version of the speech parameter generation algorithm [28] with 10-frame lookahead was used. It can be seen from the table that the LSTM-RNN-based system could synthesize speech with lower latency and total synthesis time than the HMM-based system. However, it is worthy noting that the LSTM-RNN-based system was 15\u201322% slower than the HMM-based system in terms of the total synthesis time on old devices having ARM CPUs without the NEON instruction set (latency was still lower). Table 5 shows the preference test result comparing the LSTM-RNN- and HMM-based SPSS systems. It shows that the LSTM-RNN-based system could synthesize more naturally sounding synthesized speech than the HMM-based one."}, {"heading": "3.4 Comparison with concatenative TTS", "text": "The last experiment evaluated the HMM-driven unit selection TTS [46] and LSTMRNN-based SPSS with the optimizations described in this paper except quantization. Both TTS systems used the same training data and text processing front-end modules. Note that additional linguistic features which were only available with the server-side text processing front-end modules were used in both systems. The HMM-driven unit selection TTS systems were built from speech at 16 kHz sampling. Although LSTMRNNs were trained from speech at 22.05 kHz sampling, speech at 16 kHz sampling was synthesized at runtime using a resampling functionality in Vocaine [43]. These LSTMRNNs had the same network architecture as the one described in the previous section. They were trained with the -contaminated Gaussian loss function and utilized 4-frame bundled inference. Table 6 shows the preference test result. It can be seen from the table that the LSTM-RNN-based SPSS systems were preferred to the HMM-driven unit selection TTS systems in 10 of 26 languages, while there was no significant preference between them in 3 languages. Note that the LSTM-RNN-based SPSS systems were 3\u201310% slower but 1,500\u20133,500 times smaller in disk footprint than the hybrid ones."}, {"heading": "4 Conclusions", "text": "This paper investigated three optimizations of LSTM-RNN-based SPSS for deployment on mobile devices; 1) Quantizing LSTM-RNN weights to 8-bit integers reduced disk footprint by 70%, with no significant difference in naturalness; 2) Using multiframe inference reduced CPU use by 40%, again with no significant difference in naturalness; 3) For training, using an -contaminated Gaussian loss function rather than a\nsquared loss function to avoid excessive effects from outliers proved beneficial, allowing for an increased learning rate and improving naturalness. The LSTM-RNN-based SPSS systems with these optimizations surpassed the HMM-based SPSS systems in speed, latency, disk footprint, and naturalness on modern mobile devices. Experimental results also showed that the LSTM-RNN-based SPSS system with the optimizations could match the HMM-driven unit selection TTS systems in naturalness in 13 of 26 languages."}, {"heading": "5 Acknowledgement", "text": "The authors would like to thank Mr. Raziel Alvarez for helpful comments and discussions."}], "references": [{"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A. Black"], "venue": "Speech Commn., vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proc. ICASSP, 2013, pp. 7962\u20137966.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining a vector space representation of linguistic context with a deep neural network for text-to-speech synthesis", "author": ["H. Lu", "S. King", "O. Watts"], "venue": "Proc. ISCA SSW8, 2013, pp. 281\u2013285.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "On the training aspects of deep neural network (DNN) for parametric TTS synthesis", "author": ["Y. Qian", "Y. Fan", "W. Hu", "F. Soong"], "venue": "Proc. ICASSP, 2014, pp. 3857\u2013 3861.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Voice source modelling using deep neural networks for statistical parametric speech synthesis", "author": ["T. Raitio", "H. Lu", "J. Kane", "A. Suni", "M. Vainio", "S. King", "P. Alku"], "venue": "Proc. EUSIPCO, 2014, pp. 2290\u20132294.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling DCT parameterized F0 trajectory at intonation phrase level with DNN or decision tree", "author": ["X. Yin", "M. Lei", "Y. Qian", "F. Soong", "L. He", "Z.-H. Ling", "L.-R. Dai"], "venue": "Proc. Interspeech, 2014, pp. 2273\u20132277.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis", "author": ["H. Zen", "A. Senior"], "venue": "Proc. ICASSP, 2014, pp. 3872\u20133876.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Y. Fan", "Y. Qian", "F. Soong"], "venue": "Proc. Interspeech, 2014, pp. 1964\u20131968.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis", "author": ["H. Zen", "H. Sak"], "venue": "Proc. ICASSP, 2015, pp. 4470\u20134474.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Prosody contour prediction with long short-term memory, bi-directional, deep recurrent neural networks", "author": ["R. Fernandez", "A. Rendel", "B. Ramabhadran", "R. Hoory"], "venue": "Proc. Interspeech, 2014, pp. 2268\u2013272. 8", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Investigating gated recurrent neural networks for speech synthesis", "author": ["Z. Wu", "S. King"], "venue": "Proc. ICASSP, 2016, pp. 5140\u20135144.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Fusion of multiple parameterisations for DNN-based sinusoidal speech synthesis with multi-task learning", "author": ["Q. Hu", "Z. Wu", "K. Richmond", "J. Yamagishi", "Y. Stylianou", "R. Maia"], "venue": "Proc. Interspeech, 2015, pp. 854\u2013858.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Minimum trajectory error training for deep neural networks, combined with stacked bottleneck features", "author": ["Z. Wu", "S. King"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Z. Wu", "C. Valentini-Botinhao", "O. Watts", "S. King"], "venue": "Proc. ICASSP, 2015, pp. 4460\u20134464.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Sentence-level control vectors for deep neural network speech synthesis", "author": ["O. Watts", "Z. Wu", "S. King"], "venue": "Proc. Interspeech, 2015, pp. 2217\u20132221.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-speaker modeling and speaker adaptation for DNN-based TTS synthesis", "author": ["Y. Fan", "Y. Qian", "F. Soong", "L. He"], "venue": "Proc. ICASSP, 2015, pp. 4475\u2013 4479.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence error (SE) minimization training of neural network for voice conversion", "author": ["F.-L. Xie", "Y. Qian", "Y. Fan", "F. Soong", "H. Li"], "venue": "Proc. Interspeech, 2014, pp. 2283\u20132287.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "An investigation of implementation and performance analysis of DNN based speech synthesis system", "author": ["Z. Chen", "K. Yu"], "venue": "Proc. ICSP, 2014, pp. 577\u2013582.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "The effect of neural networks in statistical parametric speech synthesis", "author": ["K. Hashimoto", "K. Oura", "Y. Nankaku", "K. Tokuda"], "venue": "Proc. ICASSP, 2015, pp. 4455\u20134459.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Modelling acoustic feature dependencies with artificial neural networks: Trajectory-RNADE", "author": ["B. Uria", "I. Murray", "S. Renals", "C. Valentini-Botinhao"], "venue": "Proc. ICASSP, 2015, pp. 4465\u20134469.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in speech synthesis", "author": ["H. Zen"], "venue": "http://research.google.com/ pubs/archive/41539.pdf, Invited keynote given at ISCA SSW8 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "From HMMs to DNNs: where do the improvements come from", "author": ["O. Watts", "G. Henter", "T. Merritt", "Z. Wu", "S. King"], "venue": "Proc. ICASSP, 2016, pp. 5505\u2013 5509.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Static and dynamic error propagation networks with application to speech coding", "author": ["A. Robinson", "F. Fallside"], "venue": "Proc. NIPS, 1988, pp. 632\u2013641.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1988}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Simultaneous modeling of phonetic and prosodic parameters, and characteristic conversion for HMM-based text-to-speech systems, Ph.D", "author": ["T. Yoshimura"], "venue": "thesis, Nagoya Institute of Technology,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Acoustic modeling for speech synthesis: from HMM to RNN", "author": ["H. Zen"], "venue": "http: //research.google.com/pubs/pub44630.html, Invited talk given at ASRU 2015. 9", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech parameter generation algorithms for HMM-based speech synthesis", "author": ["K. Tokuda", "T. Yoshimura", "T. Masuko", "T. Kobayashi", "T. Kitamura"], "venue": "Proc. ICASSP, 2000, pp. 1315\u20131318.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Vector quantization of speech spectral parameters using statistics of dynamic features", "author": ["K. Koishida", "K. Tokuda", "T. Masuko", "T. Kobayashi"], "venue": "Proc. ICSP, 1997, pp. 247\u2013252.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Quantized HMMs for low footprint text-to-speech synthesis", "author": ["A. Gutkin", "J. Gonzalvo", "S. Breuer", "P. Taylor"], "venue": "Proc. Interspeech, 2010, pp. 837\u2013840.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "On the efficient representation and execution of deep acoustic models", "author": ["R. Alvarez", "R. Prabhavalkar", "A. Bakhtin"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Multiframe deep neural networks for acoustic modeling", "author": ["V. Vanhoucke", "M. Devin", "G. Heigold"], "venue": "Proc. ICASSP, 2013, pp. 7582\u20137585.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Loss minimization and parameter estimation with heavy tails", "author": ["D. Hsu", "S. Sabato"], "venue": "arXiv:1307.1827, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1827}, {"title": "Robust TTS duration modeling using DNNs", "author": ["G. Henter", "S. Ronanki", "O. Watts", "M. Wester", "Z. Wu", "S. King"], "venue": "Proc. ICASSP, 2016, pp. 5130\u20135134.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "A survey of sampling from contaminated distributions", "author": ["J. Tukey"], "venue": "Contributions to probability and statistics, vol. 2, pp. 448\u2013485, 1960.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1960}, {"title": "Modelling of continuous speech observations", "author": ["A. Richter"], "venue": "Tech. Rep. Advances in Speech Processing Conference, IBM Europe Institute, 1986.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1986}, {"title": "The acoustic modeling problem in automatic speech recognition, Ph.D", "author": ["P. Brown"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1987}, {"title": "Tail distribution modelling using the Richter and power exponential distributions", "author": ["M. Gales", "P. Olsen"], "venue": "Proc. Eurospeech, 1999, pp. 1507\u20131510.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1999}, {"title": "Probablistic modelling of F0 in unvoiced regions in HMM based speech synthesis", "author": ["K. Yu", "T. Toda", "M. G\u0103s\u0131\u0301c", "S. Keizer", "F. Mairesse", "B. Thomson", "S. Young"], "venue": "Proc. ICASSP, 2009, pp. 3773\u20133776.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "On rectified linear units for speech processing", "author": ["M. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.-V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G. Hinton"], "venue": "Proc. ICASSP, 2013, pp. 3517\u20133521.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "Proc. Interspeech, 2014, pp. 338\u2013342.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P. Werbos"], "venue": "Proc. IEEE, vol. 78, no. 10, pp. 1550\u20131560, 1990.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1990}, {"title": "An empirical study of learning rates in deep neural networks for speech recognition", "author": ["A. Senior", "G. Heigold", "M. Ranzato", "K. Yang"], "venue": "Proc. ICASSP, 2013, pp. 6724\u20136728.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2013}, {"title": "Vocaine the vocoder and applications is speech synthesis", "author": ["Y. Agiomyrgiannakis"], "venue": "Proc. ICASSP, 2015, pp. 4230\u20134234. 10", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "The hidden Markov model toolkit (HTK) version 3.4", "author": ["S. Young", "G. Evermann", "M. Gales", "T. Hain", "D. Kershaw", "X.-Y. Liu", "G. Moore", "J. Odell", "D. Ollason", "D. Povey", "V. Valtchev", "P. Woodland"], "venue": "http://htk.eng.cam.ac.uk/, 2006.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "NEON technology introduction", "author": ["V. Reddy"], "venue": "ARM Corporation, 2008.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 2, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 3, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 4, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 5, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 6, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 7, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 8, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 9, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 10, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 11, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 12, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 13, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 14, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 15, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 16, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 17, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 18, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 19, "context": "Statistical parametric speech synthesis (SPSS) [1] based on artificial neural networks (ANN) has became popular in the text-to-speech (TTS) research area in the last few years [2\u201320].", "startOffset": 176, "endOffset": 182}, {"referenceID": 20, "context": "ANN-based acoustic models offer an efficient and distributed representation of complex dependencies between linguistic and acoustic features [21, 22] and have shown the potential to produce natural sounding synthesized speech [2, 4, 7\u20139].", "startOffset": 141, "endOffset": 149}, {"referenceID": 21, "context": "ANN-based acoustic models offer an efficient and distributed representation of complex dependencies between linguistic and acoustic features [21, 22] and have shown the potential to produce natural sounding synthesized speech [2, 4, 7\u20139].", "startOffset": 141, "endOffset": 149}, {"referenceID": 1, "context": "ANN-based acoustic models offer an efficient and distributed representation of complex dependencies between linguistic and acoustic features [21, 22] and have shown the potential to produce natural sounding synthesized speech [2, 4, 7\u20139].", "startOffset": 226, "endOffset": 237}, {"referenceID": 3, "context": "ANN-based acoustic models offer an efficient and distributed representation of complex dependencies between linguistic and acoustic features [21, 22] and have shown the potential to produce natural sounding synthesized speech [2, 4, 7\u20139].", "startOffset": 226, "endOffset": 237}, {"referenceID": 6, "context": "ANN-based acoustic models offer an efficient and distributed representation of complex dependencies between linguistic and acoustic features [21, 22] and have shown the potential to produce natural sounding synthesized speech [2, 4, 7\u20139].", "startOffset": 226, "endOffset": 237}, {"referenceID": 7, "context": "ANN-based acoustic models offer an efficient and distributed representation of complex dependencies between linguistic and acoustic features [21, 22] and have shown the potential to produce natural sounding synthesized speech [2, 4, 7\u20139].", "startOffset": 226, "endOffset": 237}, {"referenceID": 8, "context": "ANN-based acoustic models offer an efficient and distributed representation of complex dependencies between linguistic and acoustic features [21, 22] and have shown the potential to produce natural sounding synthesized speech [2, 4, 7\u20139].", "startOffset": 226, "endOffset": 237}, {"referenceID": 22, "context": "Recurrent neural networks (RNNs) [23], especially long short-term memory (LSTM)RNNs [24], provide an elegant way to model speech-like sequential data that embodies short- and long-term correlations.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "Recurrent neural networks (RNNs) [23], especially long short-term memory (LSTM)RNNs [24], provide an elegant way to model speech-like sequential data that embodies short- and long-term correlations.", "startOffset": 84, "endOffset": 88}, {"referenceID": 7, "context": "They were successfully applied to acoustic modeling for SPSS [8\u201311].", "startOffset": 61, "endOffset": 67}, {"referenceID": 8, "context": "They were successfully applied to acoustic modeling for SPSS [8\u201311].", "startOffset": 61, "endOffset": 67}, {"referenceID": 9, "context": "They were successfully applied to acoustic modeling for SPSS [8\u201311].", "startOffset": 61, "endOffset": 67}, {"referenceID": 10, "context": "They were successfully applied to acoustic modeling for SPSS [8\u201311].", "startOffset": 61, "endOffset": 67}, {"referenceID": 8, "context": "proposed a streaming speech synthesis architecture using unidirectional LSTM-RNNs with a recurrent output layer [9].", "startOffset": 112, "endOffset": 115}, {"referenceID": 24, "context": "However, it was significantly slower than hidden Markov model (HMM)-based SPSS [25] in terms of real-time ratio [26].", "startOffset": 79, "endOffset": 83}, {"referenceID": 25, "context": "However, it was significantly slower than hidden Markov model (HMM)-based SPSS [25] in terms of real-time ratio [26].", "startOffset": 112, "endOffset": 116}, {"referenceID": 8, "context": "Figure 1: Overview of the streaming SPSS architecture using LSTM-RNN-based acoustic and duration models [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "Figure 1 shows the overview of the streaming synthesis architecture using unidirectional LSTM-RNNs [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 26, "context": "Unlike HMM-based SPSS, which usually requires utterancelevel batch processing [27] or frame lookahead [28], this architecture allows framesynchronous streaming synthesis with no frame lookahead.", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "Unlike HMM-based SPSS, which usually requires utterancelevel batch processing [27] or frame lookahead [28], this architecture allows framesynchronous streaming synthesis with no frame lookahead.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "However, there are still a few drawbacks; \u2022 Disk footprint; Although the total number of parameters in LSTM-RNN-based SPSS can be significantly lower than that of HMM-based SPSS [9], the overall disk footprint of the LSTM-RNN system can be similar or slightly larger because HMM parameters can be quantized using 8-bit integers [29].", "startOffset": 178, "endOffset": 181}, {"referenceID": 28, "context": "However, there are still a few drawbacks; \u2022 Disk footprint; Although the total number of parameters in LSTM-RNN-based SPSS can be significantly lower than that of HMM-based SPSS [9], the overall disk footprint of the LSTM-RNN system can be similar or slightly larger because HMM parameters can be quantized using 8-bit integers [29].", "startOffset": 328, "endOffset": 332}, {"referenceID": 26, "context": "\u2022 Computation; With HMM-based SPSS, inference of acoustic parameters involves traversing decision trees at each HMM state and running the speech parameter generation algorithm [27].", "startOffset": 176, "endOffset": 180}, {"referenceID": 1, "context": "\u2022 Robustness; Typical ANN-based SPSS relies on fixed phoneme- or state-level alignments [2], whereas HMMs can be trained without fixed alignments using the Baum-Welch algorithm.", "startOffset": 88, "endOffset": 91}, {"referenceID": 29, "context": "This paper utilizes 8-bit quantization of ANN weights [30] to reduce the disk footprint of LSTM-RNN-based acoustic and duration models.", "startOffset": 54, "endOffset": 58}, {"referenceID": 29, "context": "Although it is possible to run inference in 8-bit integers with quantization-aware training [30], that possibility is not utilized here; instead weights are stored in 8-bit integer on disk then recovered to 32-bit floating-point numbers after loading to memory.", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "In typical ANN-based SPSS, input linguistic features other than state- and frame-position features are constant within a phoneme [2].", "startOffset": 129, "endOffset": 132}, {"referenceID": 30, "context": "Based on these characteristics of inputs and targets this paper explores the multi-frame inference approach [31].", "startOffset": 108, "endOffset": 112}, {"referenceID": 31, "context": "Using robust regression techniques such as linear regression with a heavy-tailed distribution [32] or minimum density power divergence estimator [33] can relax the effect of outliers.", "startOffset": 94, "endOffset": 98}, {"referenceID": 32, "context": "Using robust regression techniques such as linear regression with a heavy-tailed distribution [32] or minimum density power divergence estimator [33] can relax the effect of outliers.", "startOffset": 145, "endOffset": 149}, {"referenceID": 33, "context": "In this work a simple robust regression technique assuming that the errors follow a mixture of two Gaussian distributions, in particular, -contaminated Gaussian distribution [34], which is a special case of the Richter distribution [35\u201337], is employed; the majority of observations are from a specified Gaussian distribution, though a small proportion are from a Gaussian distribution with much higher variance, while the two Gaussian distributions share the same mean.", "startOffset": 174, "endOffset": 178}, {"referenceID": 34, "context": "In this work a simple robust regression technique assuming that the errors follow a mixture of two Gaussian distributions, in particular, -contaminated Gaussian distribution [34], which is a special case of the Richter distribution [35\u201337], is employed; the majority of observations are from a specified Gaussian distribution, though a small proportion are from a Gaussian distribution with much higher variance, while the two Gaussian distributions share the same mean.", "startOffset": 232, "endOffset": 239}, {"referenceID": 35, "context": "In this work a simple robust regression technique assuming that the errors follow a mixture of two Gaussian distributions, in particular, -contaminated Gaussian distribution [34], which is a special case of the Richter distribution [35\u201337], is employed; the majority of observations are from a specified Gaussian distribution, though a small proportion are from a Gaussian distribution with much higher variance, while the two Gaussian distributions share the same mean.", "startOffset": 232, "endOffset": 239}, {"referenceID": 36, "context": "In this work a simple robust regression technique assuming that the errors follow a mixture of two Gaussian distributions, in particular, -contaminated Gaussian distribution [34], which is a special case of the Richter distribution [35\u201337], is employed; the majority of observations are from a specified Gaussian distribution, though a small proportion are from a Gaussian distribution with much higher variance, while the two Gaussian distributions share the same mean.", "startOffset": 232, "endOffset": 239}, {"referenceID": 0, "context": "Figure 4 illustrates -contaminated Gaussian distribution (\u03bc = [0], \u03a3 = [1], c = 10 and = 0.", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "Figure 4: Plot of 1-dimensional -contaminated Gaussian distribution (\u03bc = [0], \u03a3 = [1], = 0.", "startOffset": 82, "endOffset": 85}, {"referenceID": 37, "context": "Note that the -contaminated Gaussian distribution is similar to globally tied distribution (GTD) in [38].", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "The configuration for speech analysis stage and data preparation process were the same as those described in [9] except the use of speech at 22.", "startOffset": 109, "endOffset": 112}, {"referenceID": 38, "context": "The architecture of the acoustic LSTM-RNNs was 1 \u00d7 128-unit ReLU [39] layer followed by 3 \u00d7 128-cell LSTMP layers [40] with 64 recurrent projection units with a linear recurrent output layer [9].", "startOffset": 65, "endOffset": 69}, {"referenceID": 39, "context": "The architecture of the acoustic LSTM-RNNs was 1 \u00d7 128-unit ReLU [39] layer followed by 3 \u00d7 128-cell LSTMP layers [40] with 64 recurrent projection units with a linear recurrent output layer [9].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "The architecture of the acoustic LSTM-RNNs was 1 \u00d7 128-unit ReLU [39] layer followed by 3 \u00d7 128-cell LSTMP layers [40] with 64 recurrent projection units with a linear recurrent output layer [9].", "startOffset": 191, "endOffset": 194}, {"referenceID": 40, "context": "A distributed CPU implementation of mini-batch asynchronous stochastic gradient descent (ASGD)based truncated back propagation through time (BPTT) [41] algorithm was used [40].", "startOffset": 147, "endOffset": 151}, {"referenceID": 39, "context": "A distributed CPU implementation of mini-batch asynchronous stochastic gradient descent (ASGD)based truncated back propagation through time (BPTT) [41] algorithm was used [40].", "startOffset": 171, "endOffset": 175}, {"referenceID": 41, "context": "The learning rates were exponentially decreased over time [42].", "startOffset": 58, "endOffset": 62}, {"referenceID": 24, "context": "Spectral enhancement based on post-filtering in the cepstral domain [25] was applied to improve the naturalness of the synthesized speech.", "startOffset": 68, "endOffset": 72}, {"referenceID": 42, "context": "From the acoustic features, speech waveforms were synthesized using the Vocaine vocoder [43].", "startOffset": 88, "endOffset": 92}, {"referenceID": 30, "context": "5\u00d710\u22126) as mentioned in [31].", "startOffset": 24, "endOffset": 28}, {"referenceID": 43, "context": "This is similar to the multi-stream HMM structure [44] used in HMM-based speech synthesis [25].", "startOffset": 50, "endOffset": 54}, {"referenceID": 24, "context": "This is similar to the multi-stream HMM structure [44] used in HMM-based speech synthesis [25].", "startOffset": 90, "endOffset": 94}, {"referenceID": 44, "context": "Note that the execution binary was compiled for modern ARM CPUs having the NEON advanced single instruction, multiple data (SIMD) instruction set [45].", "startOffset": 146, "endOffset": 150}, {"referenceID": 27, "context": "To reduce the latency of the HMM-based SPSS system, the recursive version of the speech parameter generation algorithm [28] with 10-frame lookahead was used.", "startOffset": 119, "endOffset": 123}, {"referenceID": 42, "context": "05 kHz sampling, speech at 16 kHz sampling was synthesized at runtime using a resampling functionality in Vocaine [43].", "startOffset": 114, "endOffset": 118}], "year": 2016, "abstractText": "Acoustic models based on long short-term memory recurrent neural networks (LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and showed significant improvements in naturalness and latency over those based on hidden Markov models (HMMs). This paper describes further optimizations of LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization, multi-frame inference, and robust inference using an -contaminated Gaussian loss function. Experimental results in subjective listening tests show that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based SPSS in runtime speed while maintaining naturalness. Evaluations between LSTM-RNNbased SPSS and HMM-driven unit selection speech synthesis are also presented.", "creator": "LaTeX with hyperref package"}}}