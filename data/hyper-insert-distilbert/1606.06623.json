{"id": "1606.06623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jun-2016", "title": "An empirical study on large scale text classification with skip-gram embeddings", "abstract": "formerly we indirectly investigate the integration of word byte embeddings as classification features in the initial setting of large scale graphical text classification. manipulating such representations and have progressively been used in a plethora view of tasks, however comparing their application in vector classification software scenarios with 5000 thousands inclusive of large classes has not originally been undertaken extensively researched, partially due to these hardware limitations. participating in this applied work, increasingly we examine efficient composition modelling functions to obtain balanced document - level from word - value level embeddings and... we are subsequently investigate testing their combination with the traditional digital one - hot - rod encoding representations. by presenting empirical operational evidence on eliminating large, multi - page class, multi - label classification problems, we efficiently demonstrate comparing the efficiency and the performance & benefits of pursuing this cognitive combination.", "histories": [["v1", "Tue, 21 Jun 2016 15:39:35 GMT  (59kb,D)", "http://arxiv.org/abs/1606.06623v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["georgios balikas", "massih-reza amini"], "accepted": false, "id": "1606.06623"}, "pdf": {"name": "1606.06623.pdf", "metadata": {"source": "CRF", "title": "An empirical study on large scale text classification with skip-gram embeddings", "authors": ["Georgios Balikas", "Massih-Reza Amini"], "emails": ["Georgios.Balikas@imag.fr", "Massih-Reza.Amini@imag.fr"], "sections": [{"heading": null, "text": "Keywords Distributed Representations;One-hot-encoding Representations; Neural Networks; Text Classification"}, {"heading": "1. INTRODUCTION AND PRELIMINARIES", "text": "With the proliferation of text data available online, text classification has attracted a lot of interest. Traditionally, N -grams are considered as document features and are subsequently fed to a classifier such as Support Vector Machines (SVMs) [4, 7]. One-hot-encoding representations, although prominent in the literature, have two significant drawbacks: (i) they result in a very high dimensional and sparse feature space, and (ii) they do not encode similarity between words. Lately, a lot of research has been devoted to the direction of distributed representations [6]. Distributed representations of words, are continuous, low dimensional, dense vectors that characterize the meaning and the semantic content of words. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. As a result, semantically similar words, such as \u201cstrong\u201d and \u201cpowerful\u201d, will be close in the output vectorial space.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Neu-IR \u201916 SIGIR Workshop on Neural Information Retrieval July 17\u201321, 2016, Pisa, Italy c\u00a9 2016 ACM. ISBN 978-1-4503-2138-9.\nDOI: 10.1145/1235\nIn this work, we present a focused contribution as part of an interesting classification application. We investigate the performance of word embeddings learned using the skipgram model [17] in the context of large scale, multi-label document classification. We report results on real-world classification problems with up to 10,000 classes and we demonstrate a straightforward way to combine document-level embeddings with one-hot-encoding representations. The contributions of our work are twofold: we, first, propose an efficient way to achieve satisfactory classification performance using only embedding representations and, we further improve it by applying a naturally parallelizable fusion mechanism between the distributed representations and one-hotencoding representations.\nSeveral methods have been proposed for obtaining distributed word-level representations. We cite for instance: [18, 17, 3, 15, 24]. Extensions of those methods have been proposed to cope with larger portions of text such as sentences or paragraphs: [30, 14, 25]. However, generalizing from words to larger text spans can be inefficient, since for every unseen span new passes over a neural network are required. Hence, we focus on methods that given a dictionary of word representations apply composition functions [19, 2] to produce document representations.\nAlthough distributed embeddings have been applied in a plethora of tasks from analogies evaluation [16] to extractive summarization [9], their application on large scale text classification has not been investigated. What is more, most of the work in this direction deals with short text spans, such as sentences or tweets, and the size of the investigated problems with regard to the number of classes is small. Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21]. More frequently, word embeddings for text classification are used for initializing architectures such as convolutional and recurrent networks. The works of [10] and [13] for instance, are in this line but, again, they limit their study at sentence-length spans. The latter is, also, due to hardware limitations of the GPUs used: their limited memory capacity in conjunction with the vocabulary size of large scale classification problems require many data transfer operations which results in significant overhead. In this work we place ourselves at the large scale setting (number of classes in the order of 104) where compositional methods based on neural networks are difficult to be applied.\nThe remainder of this paper is organised as follows: in\nar X\niv :1\n60 6.\n06 62\n3v 1\n[ cs\n.C L\n] 2\n1 Ju\nn 20\n16\nSection 2 we propose and evaluate composition functions for obtaining document from word representations, and in Section 3 we discuss their integration with one-hot-encoding schemes. Finally, Section 4 concludes with remarks to our future work."}, {"heading": "2. DISTRIBUTED REPRESENTATIONS AS CLASSIFICATION REPRESENTATIONS", "text": "We explore three functions to compose document representations: min, average and max [3, 25] which have been used as simple yet effective methods for compositionality learning in vector-based semantics [19], to obtain a document\u2019s representation. We use the output of each composition function as document features and we evaluate both their classification performance as well as the performance of their concatenation:\nzconc(doc) = [zavg(doc), zmin(doc), zmax(doc)]\nwhere z(doc) is the representation of a document and zx(doc) is the result of applying the composition x \u2208 {avg,max,min} element-wise, in the distributed vectors of the words of the documents. For instance, assuming we want the representation of \u201charsh winter\u201d, the composition function requires the vector representations of the words \u201charsh\u201d and \u201cwinter\u201d. Then, applying the max function will produce a vector of the same dimensionality with the original representations where each element will have the maximum value of the corresponding elements of the original word representations. In this process, we assume access to a vocabulary of distributed representations, where each word is associated with a D-dimensional vector. This vocabulary, which we assume readily available, may have been generated beforehand.\nTable 1 shows the data we use throughout the paper. They are abstracts of biomedical texts from PubMed released by the BioASQ challenge organisers [28] as well as texts from Wikipedia [22]. To obtain the dictionary of word embeddings we used the skipgram model of word2vec tool [17]. We have kept the tool\u2019s default parameters for skipgram apart from the number of iterations that we have set to 15. For training the representations, we used 10M PubMed abstracts and we added 2.5M Wikipedia documents for better generalization (\u201cTrain Data\u201d of Table 1). In the preprocessing steps we applied lower-casing, we space-padded the punctuation symbols and we filtered the words with less than 5 occurrences. For classification purposes we used the remaining PubMed abstracts from the released data. We created three classification datasets: \u201cPubMedx\u201d with x \u2208 {1, 000, 5, 000, 10, 000} being the x most common classes in the data. We performed a stratified split in train-test (200K-25K instances) parts. In the experiments hereafter, we use SVMs with linear kernel which have been widely used\nfor text classification. The \u03bb value that controls the importance of the regularization term in the optimization problem was selected using 5-fold cross validation on the training data of each experiment and for each representation. The classification problem is multi-label: each instance is associated with several classes as shown in Table 1. We cope with the multi-label problem using a binary relevance approach [29]. For SVMs, tackling the multi-label problem with binary relevance results in predicting for each instance every label with positive distance from the separating hyperplanes of the one-vs-rest binary problems. If the classifier does not return any label for an instance, the most common label of the training data is assigned. For our implementations, we have used Python\u2019s Scikit-Learn [23].\nWe now present results for classification experiments on the PubMed1,000 and PubMed10,000 datasets when the described composition functions are used. Figure 1 shows the scores for the F1 measure obtained on the test data, when varying the size of the training data. From the Figure, first note that the avg function performs better compared to both min and max. The latter two, perform equally but they are not competitive. However, the best results are consistently obtained with the concatenation (conc) of the outputs of the three composition functions. Interestingly, adding the min and max representations creates a richer representation that benefits the performance. To this direction, note the steep increase in the performance of conc representations with the availability of training data: being richer, those representations have bigger discriminative power. Depending on the dataset and the dimension of the representations, the achieved improvements using conc vary from \u223c3-5 F1 points which is important for such problems. We believe that the avg function does not retain enough information for large documents, given that they consist on average of more than 200 words (Table 1). To this end, Table 2 reports the micro F1 measure for the PubMed10,000 dataset, with respect to the document length in words. Note that the best performance is achieved for smaller documents independently of the embedding dimension.\nAnother observation from Figure 1 and Table 2 concerns the effect of the dimension of word representations in the classification performance. Representations of bigger dimensions benefit performance. In fact, increasing the dimension from 50 to 400, improves the F1 measure for conc for PubMed1,000 (resp. PubMed10,000) by \u223c7 (resp. \u223c13) points. Summarizing, we highlight here two of the advantages of the proposed approach: (i) although simple, our\ncomposition functions have yielded significant performance improvements and, (ii) their application on large datasets is naturally parallelizable, hence, they can be easily applied on real, large scale problems."}, {"heading": "3. COMBINATION OF DISTRIBUTED AND ONE-HOT-ENCODING SCHEMES", "text": "In the previous section we have shown that the concatenated representations obtained by the output of the composition functions achieved the best classification performance. We, now, compare them with the traditional onehot-encoding representations. We focus on two ways of representing text: (i) by using tf-idf representations of unigrams, and (ii) by employing a hash function [5, 27]. For the former, to generate the tf-idf representations we used sub-linear term frequency counts multiplied by their respective smoothed inverse document frequency, i.e., for a term w we have (1 + log(tfw)) \u2217 (idfw + 1). For the latter, the hash function, given a text string, transforms it on a numerical value in a pre-specified space, that is used as the index to generate a vector representation. Increasing the output dimension of the hash functions reduces the probability of collisions, i.e. different words mapped on the same vector indices, but also increases the output vector size. We investigate this trade-off in Figure 2: we present the effect of the size of the hash representations with respect to classification performance for PubMed5,000 and PubMed10,000 which have the biggest vocabulary size. We also report exact timings of each scenario, executed on 10 cores of an Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz. From the figure, note that after 70K features the classification performance does not improve when increasing the size of the feature space. On the other hand, increasing the representation\u2019s dimension, the training time also increases. For reference, tf-idf represen-\ntations in the same computational setting need 602 sec. and 1203 sec. for PubMed5,000 and PubMed10,000 respectively. As a result, we set the hash dimension to 70,000 features. Note the significant dimensionality reduction achieved, given the vocabulary sizes of our problems reported in Table 1.\nTable 3 details the scores achieved with regard to the representations used. We first discuss the performance when single representations are used: x conc, tf-idf and hash. Notice that tf-idf performs better than both x conc and hash, with the latter achieving the lowest performance. The performance of the three representations on PubMed1,000 is comparable, but in the bigger classification problems tf-idf and performs considerably better. We, thus, consider the tfidf representations as our baseline model, and we examine how the models with concatenated representations behave compared to it.\nWe investigate now whether the fusion of distributed document representations with the one-hot-encoding representations benefits the classification performance. The last two lines of Table 3 present the performance of the fusion, by concatenation, of the embedding representations with D \u2208 {100, 200, 400} with hash and tf-idf. In the experiments, both hash and tf-idf consistently achieve better performance when combined with the distributed representations. For instance, for PubMed10,000 and D = 400 the tf-idf (resp. hash) representations improve in absolute numbers by 2 (resp. 3.5) F1 points. We have performed two-sided student\u2019s t-tests (p < 0.01) to compare whether the improvements obtained for each classification problem are statistically significant compared to using tf-idf representations. Those results, indicated by (\u2020) in Table 3, reveal that the concatenation of tf-idf with distributed representations improves the classification performance in a statistically significant way. In addition to the important improvements, note than in the fused representations, the effect of\nD diminishes. For instance in PubMed1,000, one can obtain the optimal performance using embedding dimensions with D < 400 and similar observations can be made in the rest of the datasets."}, {"heading": "4. CONCLUSION AND DISCUSSION", "text": "In this work we have restricted ourselves in representations learned in the word level. This is advantageous in terms of speed since the dictionary of representations can be generated offline. Then, applying composition functions is naturally parallelizable and fast for prediction. However, this poses the challenge of having robust composition functions, which if carefully selected, can result in performance gains such as those reported above. Also, similarly to the bag-of-words paradigm, it does not take into account the word order and the words\u2019 grouping in coherent segments like sentences or phrases.\nIn this line, it would be interesting to further investigate how more complex embeddings such as paragraph vectors [14] perform. Even if such approaches are computationally expensive in the document level, previous research has shown their effectiveness on the sentence level. Hence, a direct extension of this work is to test the investigated composition functions with sentence level representations. In terms of applications, those sentence representations can be\ndirectly used to evaluate the effectiveness of the embeddings and the composition functions. Importantly, their can be evaluated simultaneously in different levels of text granularity from sentences to documents in the framework of passage retrieval and document classification from instance.\nAnother interesting line of research concerns the memory efficiency of such dense representations. Recent research efforts [1, 8] have investigated ways of compressing the learned (or composed) representations using either linear (e.g. PCA) or non-linear (e.g. auto-encoders) approaches to decrease the memory requirements or the dimension of the representations. Such approaches, apart from having a positive effect on the memory footprint, also have a positive effect on the required computational requirements for training.\nIn this work we have evaluated different composition functions for obtaining document-level representations using distributed embeddings of words. Summarizing our findings, adding the concatenated vector of word-level skip-gram derived features to tf-idf unigrams performs better than tf-idf features alone. Also, the result seems to be more pronounced as the representations\u2019 cardinality increases and as the output label space increases. Given the obtained improvements, we have also outlined promising future research directions."}, {"heading": "5. REFERENCES", "text": "[1] G. Balikas and M.-R. Amini. Multi-label, multi-class\nclassification using polylingual embeddings. In Advances in Information Retrieval, pages 723\u2013728. Springer, 2016.\n[2] W. Blacoe and M. Lapata. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546\u2013556. Association for Computational Linguistics, 2012.\n[3] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493\u20132537, 2011.\n[4] C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273\u2013297, 1995.\n[5] G. Forman and E. Kirshenbaum. Extremely fast text feature extraction for classification and indexing. In Proceedings of the 17th ACM conference on Information and knowledge management, pages\n1221\u20131230. ACM, 2008.\n[6] Z. S. Harris. Distributional structure. Word, 10(2-3):146\u2013162, 1954.\n[7] T. Joachims. Making large scale svm learning practical. Technical report, Universita\u0308t Dortmund, 1999.\n[8] J. Jurgovsky, M. Granitzer, and C. Seifert. Evaluating memory efficiency and robustness of word embeddings. In Advances in Information Retrieval, pages 200\u2013211. Springer, 2016.\n[9] M. K\u030aageba\u0308ck, O. Mogren, N. Tahmasebi, and D. Dubhashi. Extractive summarization using continuous vector space models. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL, pages 31\u201339, 2014.\n[10] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A convolutional neural network for modelling sentences. arXiv preprint arXiv:1404.2188, 2014.\n[11] T. Kenter and M. de Rijke. Short text similarity with word embeddings. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, pages 1411\u20131420. ACM, 2015.\n[12] J. Kim, F. Rousseau, and M. Vazirgiannis. Convolutional sentence kernel from word embeddings for short text categorization. In Proceedings of the 2015 conference on empirical methods in natural language processing. In EMNLP, volume 15, 2015.\n[13] Y. Kim. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882, 2014.\n[14] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053, 2014.\n[15] O. Levy and Y. Goldberg. Dependency-based word embeddings. In ACL (2), pages 302\u2013308, 2014.\n[16] O. Levy, Y. Goldberg, and I. Ramat-Gan. Linguistic regularities in sparse and explicit word representations. In CoNLL, pages 171\u2013180, 2014.\n[17] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[18] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111\u20133119, 2013.\n[19] J. Mitchell and M. Lapata. Composition in distributional models of semantics. Cognitive science,\n34(8):1388\u20131429, 2010.\n[20] P. Nakov, Z. Kozareva, A. Ritter, S. Rosenthal, V. Stoyanov, and T. Wilson. Semeval-2013 task 2: Sentiment analysis in twitter. 2013.\n[21] B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 115\u2013124. Association for Computational Linguistics, 2005.\n[22] I. Partalas, A. Kosmopoulos, N. Baskiotis, T. Artieres, G. Paliouras, E. Gaussier, I. Androutsopoulos, M.-R. Amini, and P. Galinari. Lshtc: A benchmark for large-scale text classification. arXiv preprint arXiv:1503.08581, 2015.\n[23] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011.\n[24] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP, volume 14, pages 1532\u20131543, 2014.\n[25] R. Socher, E. H. Huang, J. Pennin, C. D. Manning, and A. Y. Ng. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems, pages 801\u2013809, 2011.\n[26] Y. Song and D. Roth. Unsupervised sparse vector densification for short text similarity. Proc. North Am. Chapter Assoc. Computat. Linguistics, pages 1275\u20131280, 2015.\n[27] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 1998.\n[28] G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas, M. Zschunke, M. R. Alvers, D. Weissenborn, A. Krithara, S. Petridis, D. Polychronopoulos, et al. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16(1):1, 2015.\n[29] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. Dept. of Informatics, Aristotle University of Thessaloniki, Greece, 2006.\n[30] H. Zhao, Z. Lu, and P. Poupart. Self-adaptive hierarchical sentence model. arXiv preprint arXiv:1504.05070, 2015."}], "references": [{"title": "Multi-label", "author": ["G. Balikas", "M.-R. Amini"], "venue": "multi-class classification using polylingual embeddings. In Advances in Information Retrieval, pages 723\u2013728. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe", "M. Lapata"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546\u2013556. Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Extremely fast text feature extraction for classification and indexing", "author": ["G. Forman", "E. Kirshenbaum"], "venue": "Proceedings of the 17th ACM conference on Information and knowledge management, pages  1221\u20131230. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, 10(2-3):146\u2013162", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1954}, {"title": "Making large scale svm learning practical", "author": ["T. Joachims"], "venue": "Technical report, Universit\u00e4t Dortmund", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Evaluating memory efficiency and robustness of word embeddings", "author": ["J. Jurgovsky", "M. Granitzer", "C. Seifert"], "venue": "Advances in Information Retrieval, pages 200\u2013211. Springer", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Extractive summarization using continuous vector space models", "author": ["M. K\u030aageb\u00e4ck", "O. Mogren", "N. Tahmasebi", "D. Dubhashi"], "venue": "In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC)@ EACL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Short text similarity with word embeddings", "author": ["T. Kenter", "M. de Rijke"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Convolutional sentence kernel from word embeddings for short text categorization", "author": ["J. Kim", "F. Rousseau", "M. Vazirgiannis"], "venue": "Proceedings of the 2015 conference on empirical methods in natural language processing. In EMNLP, volume 15", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "arXiv preprint arXiv:1405.4053", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Dependency-based word embeddings", "author": ["O. Levy", "Y. Goldberg"], "venue": "ACL (2), pages 302\u2013308", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["O. Levy", "Y. Goldberg", "I. Ramat-Gan"], "venue": "CoNLL, pages 171\u2013180", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive science,  34(8):1388\u20131429", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Semeval-2013 task 2: Sentiment analysis in twitter", "author": ["P. Nakov", "Z. Kozareva", "A. Ritter", "S. Rosenthal", "V. Stoyanov", "T. Wilson"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 115\u2013124. Association for Computational Linguistics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Lshtc: A benchmark for large-scale text classification", "author": ["I. Partalas", "A. Kosmopoulos", "N. Baskiotis", "T. Artieres", "G. Paliouras", "E. Gaussier", "I. Androutsopoulos", "M.-R. Amini", "P. Galinari"], "venue": "arXiv preprint arXiv:1503.08581", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, 12:2825\u20132830", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennin", "C.D. Manning", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, pages 801\u2013809", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Unsupervised sparse vector densification for short text similarity", "author": ["Y. Song", "D. Roth"], "venue": "Proc. North Am. Chapter Assoc. Computat. Linguistics, pages 1275\u20131280", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "et al", "author": ["G. Tsatsaronis", "G. Balikas", "P. Malakasiotis", "I. Partalas", "M. Zschunke", "M.R. Alvers", "D. Weissenborn", "A. Krithara", "S. Petridis", "D. Polychronopoulos"], "venue": "An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16(1):1", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-label classification: An overview", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "Dept. of Informatics, Aristotle University of Thessaloniki, Greece", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Self-adaptive hierarchical sentence model", "author": ["H. Zhao", "Z. Lu", "P. Poupart"], "venue": "arXiv preprint arXiv:1504.05070", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Traditionally, N -grams are considered as document features and are subsequently fed to a classifier such as Support Vector Machines (SVMs) [4, 7].", "startOffset": 140, "endOffset": 146}, {"referenceID": 6, "context": "Traditionally, N -grams are considered as document features and are subsequently fed to a classifier such as Support Vector Machines (SVMs) [4, 7].", "startOffset": 140, "endOffset": 146}, {"referenceID": 5, "context": "Lately, a lot of research has been devoted to the direction of distributed representations [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 16, "context": "We investigate the performance of word embeddings learned using the skipgram model [17] in the context of large scale, multi-label document classification.", "startOffset": 83, "endOffset": 87}, {"referenceID": 17, "context": "We cite for instance: [18, 17, 3, 15, 24].", "startOffset": 22, "endOffset": 41}, {"referenceID": 16, "context": "We cite for instance: [18, 17, 3, 15, 24].", "startOffset": 22, "endOffset": 41}, {"referenceID": 2, "context": "We cite for instance: [18, 17, 3, 15, 24].", "startOffset": 22, "endOffset": 41}, {"referenceID": 14, "context": "We cite for instance: [18, 17, 3, 15, 24].", "startOffset": 22, "endOffset": 41}, {"referenceID": 23, "context": "We cite for instance: [18, 17, 3, 15, 24].", "startOffset": 22, "endOffset": 41}, {"referenceID": 29, "context": "Extensions of those methods have been proposed to cope with larger portions of text such as sentences or paragraphs: [30, 14, 25].", "startOffset": 117, "endOffset": 129}, {"referenceID": 13, "context": "Extensions of those methods have been proposed to cope with larger portions of text such as sentences or paragraphs: [30, 14, 25].", "startOffset": 117, "endOffset": 129}, {"referenceID": 24, "context": "Extensions of those methods have been proposed to cope with larger portions of text such as sentences or paragraphs: [30, 14, 25].", "startOffset": 117, "endOffset": 129}, {"referenceID": 18, "context": "Hence, we focus on methods that given a dictionary of word representations apply composition functions [19, 2] to produce document representations.", "startOffset": 103, "endOffset": 110}, {"referenceID": 1, "context": "Hence, we focus on methods that given a dictionary of word representations apply composition functions [19, 2] to produce document representations.", "startOffset": 103, "endOffset": 110}, {"referenceID": 15, "context": "Although distributed embeddings have been applied in a plethora of tasks from analogies evaluation [16] to extractive summarization [9], their application on large scale text classification has not been investigated.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "Although distributed embeddings have been applied in a plethora of tasks from analogies evaluation [16] to extractive summarization [9], their application on large scale text classification has not been investigated.", "startOffset": 132, "endOffset": 135}, {"referenceID": 10, "context": "Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21].", "startOffset": 24, "endOffset": 36}, {"referenceID": 11, "context": "Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21].", "startOffset": 24, "endOffset": 36}, {"referenceID": 25, "context": "Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21].", "startOffset": 24, "endOffset": 36}, {"referenceID": 19, "context": "Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21].", "startOffset": 188, "endOffset": 192}, {"referenceID": 20, "context": "Recently, for instance, [11, 12, 26] investigated the problem of short text similarity with applications to classification with a limited number of classes, like binary sentiment analysis [20] and ternary sentiment analysis [21].", "startOffset": 224, "endOffset": 228}, {"referenceID": 9, "context": "The works of [10] and [13] for instance, are in this line but, again, they limit their study at sentence-length spans.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "The works of [10] and [13] for instance, are in this line but, again, they limit their study at sentence-length spans.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "We explore three functions to compose document representations: min, average and max [3, 25] which have been used as simple yet effective methods for compositionality learning in vector-based semantics [19], to obtain a document\u2019s representation.", "startOffset": 85, "endOffset": 92}, {"referenceID": 24, "context": "We explore three functions to compose document representations: min, average and max [3, 25] which have been used as simple yet effective methods for compositionality learning in vector-based semantics [19], to obtain a document\u2019s representation.", "startOffset": 85, "endOffset": 92}, {"referenceID": 18, "context": "We explore three functions to compose document representations: min, average and max [3, 25] which have been used as simple yet effective methods for compositionality learning in vector-based semantics [19], to obtain a document\u2019s representation.", "startOffset": 202, "endOffset": 206}, {"referenceID": 27, "context": "They are abstracts of biomedical texts from PubMed released by the BioASQ challenge organisers [28] as well as texts from Wikipedia [22].", "startOffset": 95, "endOffset": 99}, {"referenceID": 21, "context": "They are abstracts of biomedical texts from PubMed released by the BioASQ challenge organisers [28] as well as texts from Wikipedia [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 16, "context": "To obtain the dictionary of word embeddings we used the skipgram model of word2vec tool [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 28, "context": "We cope with the multi-label problem using a binary relevance approach [29].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "For our implementations, we have used Python\u2019s Scikit-Learn [23].", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "We focus on two ways of representing text: (i) by using tf-idf representations of unigrams, and (ii) by employing a hash function [5, 27].", "startOffset": 130, "endOffset": 137}, {"referenceID": 26, "context": "We focus on two ways of representing text: (i) by using tf-idf representations of unigrams, and (ii) by employing a hash function [5, 27].", "startOffset": 130, "endOffset": 137}, {"referenceID": 13, "context": "In this line, it would be interesting to further investigate how more complex embeddings such as paragraph vectors [14] perform.", "startOffset": 115, "endOffset": 119}, {"referenceID": 0, "context": "Recent research efforts [1, 8] have investigated ways of compressing the learned (or composed) representations using either linear (e.", "startOffset": 24, "endOffset": 30}, {"referenceID": 7, "context": "Recent research efforts [1, 8] have investigated ways of compressing the learned (or composed) representations using either linear (e.", "startOffset": 24, "endOffset": 30}], "year": 2016, "abstractText": "We investigate the integration of word embeddings as classification features in the setting of large scale text classification. Such representations have been used in a plethora of tasks, however their application in classification scenarios with thousands of classes has not been extensively researched, partially due to hardware limitations. In this work, we examine efficient composition functions to obtain document-level from word-level embeddings and we subsequently investigate their combination with the traditional one-hot-encoding representations. By presenting empirical evidence on large, multi-class, multi-label classification problems, we demonstrate the efficiency and the performance benefits of this combination.", "creator": "LaTeX with hyperref package"}}}