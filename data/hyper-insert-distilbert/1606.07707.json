{"id": "1606.07707", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Collective Semi-Supervised Learning for User Profiling in Social Media", "abstract": "exploring the enormous abundance efficiency of raw user - generated data in primary social media has incentivized the worldwide development of generic methods to infer onto the latent attributes of users, which unfortunately are crucially useful for personalization, advertising and recommendation. however, the current user profiling approaches might have observed limited model success, undoubtedly due most to showing the lack of a general principled model way used to socially integrate different types of social relationships overall of a user, and highlighting the reliance on having scarcely - socially available labeled data in, building essentially a relational prediction based model. discussed in this paper, we ultimately present a brand novel solution termed collective responsive semi - supervised learning ( csl ), algorithms which provides themselves a principled means to essentially integrate different types consist of social relational relationship and unlabeled data independently under a unified computational analytics framework. performing the common joint learning outputs from multiple relationships combined and thus unlabeled data yields a computationally versatile sound analysis and accurate optimization approach to model local user intrinsic attributes in social media. extensive comparative experiments using twitter dynamics data have demonstrated over the evolving efficacy of our csl approach focused in inferring user attributes such they as desired account type and multiple marital status. we almost also independently show how csl can consistently be socially used to help determine important user features, accurately and ideally to make inference confident on a larger user population.", "histories": [["v1", "Fri, 24 Jun 2016 14:42:17 GMT  (2551kb,D)", "http://arxiv.org/abs/1606.07707v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["richard j oentaryo", "ee-peng lim", "freddy chong tat chua", "jia-wei low", "david lo"], "accepted": false, "id": "1606.07707"}, "pdf": {"name": "1606.07707.pdf", "metadata": {"source": "CRF", "title": "Collective Semi-Supervised Learning for User Profiling in Social Media", "authors": ["Richard J. Oentaryo", "Ee-Peng Lim", "Freddy Chong", "Tat Chua", "Jia-Wei Low", "David Lo"], "emails": ["davidlo}@smu.edu.sg", "freddy.chua@hp.com"], "sections": [{"heading": null, "text": "Index Terms\u2014Convex optimization, collective learning, semi-supervised learning, social media, user profiling.\nF"}, {"heading": "1 INTRODUCTION", "text": "In recent years, we have witnessed a dramatic growth in social interactions taking place in social media such as Twitter and Facebook. These social media sites allow users to share contents (e.g., text, images, videos or web links), and to build social relationships, user communities, and common interest groups. Social media also generate a massive amount of digital data about user behaviors. The availability of such data has sparked the desire to learn more about consumers/users, fueling in turn the emergence of new services for peer interaction, marketing, and content sharing. For these services, there is a need to profile user preferences and attributes so as to support personalization, advertising, and recommendation [24], [28].\nDespite the abundance of user-generated data, meta data about personal attributes that are directly useful for personalized services and recommendations are often not available. In Twitter, for instance, users rarely provide demographic information as gender, age, religion, or marital status. Such information can be used by Twitter or other organizations to perform market segmentation, contextualize search engine, or make better content/friend recommendations. Recent studies [18], [20], [24], [28] have nonetheless shown that it is possible to use statistical means to profile the latent user attributes, based on public data (e.g., users\u2019 contents and social ties) that the users reveal in social media.\nExisting works on profiling latent user attributes in social media generally involve two types of data: content informa-\n\u2022 R. J. Oentaryo, E.-P. Lim, and D. Lo are with the Living Analytics Research Centre, Singapore Management University, Singapore 178902. E-mail: {roentaryo, eplim, davidlo}@smu.edu.sg\n\u2022 F. C. T. Chua is with the Mechanism and Design Lab, HPLabs, Palo Alto, CA 94304, USA. E-mail: freddy.chua@hp.com\n\u2022 J.-W. Low is with the Infocomm Development Authority, 10 Pasir Panjang Road, Singapore 117438. E-mail: davidlowjw@gmail.com\nManuscript received Month XX, 2015; revised Month XX, 2015.\ntion and social connectivity [14], [24], [28]. However, two major issues hinder the widespread use of these approaches. First, most (if not all) existing methods utilize users\u2019 own contents and/or one specific type of user relationship. They are not able to fuse different types of social relationship when one\u2019s own content is unavailable (e.g., a Twitter user who has no tweets), or when one type of relationship is not sufficiently informative of the attribute of interest. Second, the current approaches employ supervised learning methods that cannot generalize well when labeled data are scarce. This necessitates a more robust method that can also exploit a large pool of unlabeled data."}, {"heading": "1.1 Motivating Example", "text": "To illustrate these more clearly, Figure 1 gives an example for the task of inferring if a Twitter user account is personal or organizational. The example consists of six personal and organization accounts whose labels are known (ovals and round boxes), and two accounts with unknown labels (dashed boxes). The upper half of Figure 1 shows two types of relationships: \u201cfollow\u201d and \u201cretweet from\u201d, and the tweet contents (words) of each user are shown in the left table. The bottom half shows the bag-of-words feature representation of the users. Traditionally, one can infer the label using only a user\u2019s own contents (self features). Often, however, the self features alone are not indicative of the label, e.g., for user Andy who never tweets. Augmenting social features derived from Andy\u2019s followees (Bob, Citibank and HSBC) can provide stronger cues for Andy\u2019s label.\nIn this spirit, recent works [20], [21] have tried to incorporate social features, largely derived from a single type of relationship. But due to the sparse nature of users\u2019 connectivity, utilizing a single type of relationship may still be inadequate. For example, building a predictive model for David\u2019s account type can hardly benefit from social ties if we consider only his \u201cfollow\u201d connections, which he\nar X\niv :1\n60 6.\n07 70\n7v 1\n[ cs\n.S I]\n2 4\nJu n\n20 16\n2 Ci#bank \u00a0 Bloomberg \u00a0\nHSBC \u00a0\nAndy \u00a0\nBob \u00a0\nFOLLOW \u00a0GRAPH \u00a0\nBob \u00a0\nRETWEET \u00a0GRAPH \u00a0\nBloomberg \u00a0\nCi#bank \u00a0\nSELF \u00a0FEATURES \u00a0 Andy \u00a0 Bob \u00a0\nDavid \u00a0 Bloomberg \u00a0\nCi#bank \u00a0 HSBC \u00a0 Cindy \u00a0\nStarbucks \u00a0\nFOLLOW \u00a0FEATURES \u00a0 RETWEET \u00a0FEATURES \u00a0\nba nk  \u00a0 ch in a \u00a0 co ffe e \u00a0 fo od  \u00a0 m on ey  \u00a0 m ov ie  \u00a0 ne w s \u00a0 pr om o \u00a0 st oc k \u00a0\nba nk  \u00a0 ch in a \u00a0 co ffe e \u00a0 fo od  \u00a0 m on ey  \u00a0 m ov ie  \u00a0 ne w s \u00a0 pr om o \u00a0 st oc k \u00a0\nLegend: \u00a0 Personal \u00a0 Organiza#on \u00a0\nStarbucks \u00a0\nCindy \u00a0Cindy \u00a0\nDavid \u00a0\nUnknown \u00a0type/label \u00a0\nba nk  \u00a0 ch in a \u00a0 co ffe e \u00a0 fo od  \u00a0 m on ey  \u00a0 m ov ie  \u00a0 ne w s \u00a0 pr om o \u00a0 st oc k \u00a0\nUser \u00a0 Tweet \u00a0words \u00a0\nAndy \u00a0 Bob \u00a0 \u201cfood\u201d, \u00a0 \u00a0\u201cnews\u201d \u00a0 \u00a0 Cindy \u00a0 \u201cfood\u201d, \u00a0 \u00a0\u201cmovie\u201d \u00a0 David \u00a0 \u201cmoney\u201d, \u00a0\u201cmovie\u201d \u00a0 Bloomberg \u00a0 \u201cmoney\u201d, \u00a0\u201cstock\u201d \u00a0 Ci#bank \u00a0 \u201cbank\u201d, \u00a0\u201cmoney\u201d \u00a0 HSBC \u00a0 \u201cbank\u201d, \u00a0\u201cchina\u201d \u00a0 Starbucks \u00a0 \u201ccoffee\u201d,\t\r \u00a0\u201cpromo\u201d\t\r \u00a0 David \u00a0\nAndy \u00a0\nHSBC \u00a0\nStarbucks \u00a0\nFig. 1. Example of user profiling task\nhas none. Hence, we are not able to gain additional word features from his followers. Similarly for Andy, Citibank and HSBC, there is no additional word feature if we use only the \u201cretweet\u201d links, i.e., all of them do not have \u201cretweet\u201d links. Intuitively, integrating social features from multiple, complementary sources can boost the confidence in the label prediction. For instance, modeling Bob\u2019s label can benefit from the co-occurrence of the words that appear in his follow and retweet features. Finally, it is possible to build a more robust model by utilizing unlabeled data. For instance, Bob\u2019s shares a common word \u201cfood\u201d with Cindy\u2019s, but has no common word with Starbucks. Exploiting this, we can create a better classification boundary that makes Bob\u2019s label closer to Cindy\u2019s\u2019 and further away from Starbucks."}, {"heading": "1.2 Proposal and Contributions", "text": "Generalizing the above scenario, we propose a new take on user profiling task by answering several research questions:\n\u2022 How can we exploit multiple types of social relationship and unlabeled data in order to infer/profile the (latent) user attributes better? \u2022 How do we develop an efficient and robust profiling method that can integrate multiple relationship types and unlabeled data in a computationally sound way? \u2022 Can we understand the contributions of different features and relationship types, as well as infer/profile on a larger user population?\nIn light of these questions, we present in this paper a new computational method dubbed collective semi-supervised learning (CSL), for modeling user attributes in social media. To our best knowledge, this work is the first to formulate user profiling as the problem of jointly exploiting multiple relationship types and unlabeled data, and hence CSL provides a unified approach to solve this problem in a computationally principled and efficient manner. In particular, CSL models multiple relationship types by generically constructing multi-relational features (MRF), and then integrates the concept of convex divergence (CD) regularization in order to establish a convex formulation of semi-supervised learning utilizing unlabeled data.\nWe summarize our main contributions as follows:\n\u2022 We develop a simple method for collective learning via MRF, which takes into account\u2014for different types of relationships\u2014both the tie strength between a given user and its neighbors as well as the features of the neighbors. Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information. \u2022 We put forward the concept of CD regularization that offers a convex formulation of semi-supervised learning using unlabeled data instances (i.e., unlabeled users). This leads to a computationally sound learning procedure that warrants a unique, globally optimal solution, which can be readily identified via off-the-shelf numerical optimization methods (e.g., the Quasi-Newton algorithm in [22]). This makes our CSL approach accurate, robust, and scalable. \u2022 We extensively evaluate our CSL approach through two user profiling tasks in Twitter: modeling users\u2019 account type and marital status. The results demonstrate the accuracy and robustness of our approach, and how different relationship types and unlabeled data contribute to its performance. We also show how CSL can be used to unravel important features for different relationship types, and to infer/profile on a larger user population in Twitter."}, {"heading": "1.3 Paper Outline", "text": "The remainder of this paper is organized as follows. In Section 2, we first give an overview of related works. Section 3 elaborates the proposed CSL approach. In Section 4, we describe the user profiling tasks addressed in this work, followed by the corresponding experimental results and analyses in Section 5. Finally, we conclude in Section 6."}, {"heading": "2 RELATED WORK", "text": "We first survey related works on user attribute profiling, semi-supervised learning, and multi-relational learning. We then discuss how our approach differs from these works."}, {"heading": "2.1 User Attribute Profiling", "text": "Several works have recently been developed to infer multiple user attributes in social media. Rao et al. [28] proposed a set of network structure-based features to infer the attributes of Twitter users, including gender, age, geographical origin, and political preference. These features were then fed into a stacked classifier to infer the attributes. Mislove et al. [24] used both global and local community detection methods in order to find communities of users who share common attribute values. Ikeda et al. [14] used social communities to infer the demographic information of the Twitter users. They developed a hybrid method that uses text features as well as network structure features.\n3 Recently, Kosinski et al. [18] showed that public information obtained from Facebook can be used to predict demographic attributes of users. By factorizing a sparse matrix representing which user likes which subject/topic, a low-rank representation of the attributes was obtained and then used as features for regression and classification. Li et al. [20] devised a distant supervised learning method to infer the attributes of Twitter users by augmenting structured auxiliary data from the Facebook and Google+ networks. The unstructured information is matched with the structured ground truth to increase the prediction accuracy.\nIn [21], Li et al. developed a new co-profiling to jointly infer the users\u2019 attributes (partially observed) and relationship type (completely unobserved) within the users\u2019 ego network (observed). The underlying assumption is that social connections are discriminatively correlated with user attributes (e.g., employer, college) via a hidden relationship type (e.g., colleague, classmate). Dong et al. [8] presented a factor graph model to predict the demographic attributes of mobile phone users. The model utilizes three types of factor: attribute factor, dyadic factor, and triadic factor, which represent correlation between the user\u2019s attributes and his/her network characteristics, between the attributes of two users, and among the attributes of user triads, respectively."}, {"heading": "2.2 Semi-Supervised Learning", "text": "The literature on SSL methods is vast, so here we review only methods that are most related to our work. The simplest form of SSL is bootstrapping, whereby a classifier is first trained using labeled data, and then applied to unlabeled data so as to generate more labeled samples for the next rounds of training [1], [12]. Bootstrapping works based on a simplistic assumption is that the classifier\u2019s own (highconfidence) predictions are correct. Co-training is an extension of bootstrapping in which two (or more) classifiers are trained on different, ideally disjoint sets of features, and generate labeled samples to improve each other [5], [25]. This method is less prone to mistakes than bootstrapping, but it requires that natural feature splits exist in the data.\nAnother class of SSL methods uses the low-density assumption [6], encouraging the decision boundary to lie in low-density regions for improving generalization results. The most common way to achieve this is to use a maximum margin algorithm such as transductive support vector machine [17]. However, the corresponding learning problem is nonconvex, which is hard to solve and does not warrant globally optimal solution. Grandvalet and Bengio [11] devised an alternative method based on entropy regularization (ER). This approach encourages the posterior probability to be closer to 1 or 0 through any high-density region, while the decision boundary corresponds to intermediate probability. Again, however, the resulting problem is nonconvex.\nThere are also active research works on graph-based SSL (GSSL) methods, which treat both labeled and unlabeled data as nodes in a graph and build edges between pairs of nodes weighted by their affinities (similarities) [6]. A popular example of GSSL methods is label spreading [36], which iteratively propagates a node\u2019s label distribution to its neighbors according to their affinity. Generalizations have been proposed under the umbrella of manifold regularization [4], [6]. We note, however, that the GSSL methods\nwork well only when the affinity or manifold assumption holds for the data, i.e., nodes that are similar would likely have similar label distribution. As such, the GSSL methods requires the right choice of affinity graph to work well.\nExtending the entropy regularization method [11], several information-theoretic SSL methods have been developed [23], [27]. Mann and McCallum [23] proposed the expectation regularization (XR) to build a simple and robust SSL method. The XR augments the learning procedure with a regularization term that minimizes the Kullback-Leibler (KL) divergence between the label expectations predicted by the model and human-provided label expectation priors. More recently, Niu et al. [27] devised a squared-loss mutual information regularization (SMIR) method, which led to a convex SSL problem formulation guaranteed under a mild condition. The key appeal of this approach is that an analytical (closed-form) solution can be computed to identify unique, globally optimal model parameters [27], [31]."}, {"heading": "2.3 Multi-Relational Learning", "text": "Multi-relational learning (MRL) is applicable when the data are available in multiple structured formats and can be represented as multiple graphs (a.k.a. multigraph). That is, a multigraph can be used in MRL to encode different types of relationship (edge) among entities (nodes). In [35], Xu et al. presented a seminal work on multi-relational Gaussian process (MRGP) that utilizes a generative probabilistic model based on Gaussian process. It combines the covariance and random variables approaches to model multiple relations, which in turn provides support for multiple relational learning tasks with multiple types of entities and relations.\nIn a different task domain, Wang et al. [33] proposed a MRL method for video annotation that integrates multiple graphs into a regularization framework, so as to sufficiently exploit their complementation. This method was shown to be equivalent to first fusing multiple graphs and then conducting graph-based SSL on the fused graph. A similar approach was used in [32] to tackle the task of protein domain ranking in structural biology. In this approach, the intrinsic manifold of protein domain distribution was approximated by combining multiple graphs for regularization.\nAnother branch of relational learning considers the relations among entities as resulting from the latent factors of these entities. These approaches often translate into learning an embedding of the entities, which corresponds to a matrix factorization problem. This can be naturally extended to MRL by stacking the matrices to be factorized and then applying tensor factorization methods [7], [13]. Another natural extension to MRL is to share the common embedding or the entities across relations via collective matrix factorization [26], [29]. This method has shown state-of-the-art performances on relational datasets [26], although the number of relation types is usually modest (less than 100). Extensions have recently been proposed in [9], [15] to handle multirelational data with a large number of relation types."}, {"heading": "2.4 Our Approach", "text": "Our CSL approach differs from the existing works in several important ways, which we elaborate below.\n4 Comparisons with existing user profiling methods. While many of the current profiling methods utilize users\u2019 social information for attribute predictions, they have focused on just one type of relationship (e.g., only the follow relationship in Twitter), lacking a systematic method to incorporate different types of relationships altogether. Second, the existing profiling methods utilize only labeled data, which are often very scarce. A more robust predictive model can be obtained by also exploiting a large pool of unlabeled data. CSL offers these two capabilities in a unified and synergistic manner, which\u2014to the best of our knowledge\u2014is the first of its kind for user profiling applications.\nComparisons with existing SSL methods. In contrast to conventional SSL methods such as bootstrapping [1] and co-training [5], our CSL approach does not rely on the assumption that the model\u2019s own (high-confidence) prediction is correct, or that natural feature splits exist in the data. Compared to the GSSL methods such as label spreading [36]\u2014whose performance is sensitive to the choice of affinity graph\u2014our approach works based on the empirical distribution of unlabeled data, which is simpler and less restrictive. Our approach also provides a convex formulation of SSL that is more robust and computationally elegant than the ER method [11], whereby the learning procedure can be easily trapped to one of the (multiple) local optimal solutions. Finally, the CSL approach is more general than the state-of-the-art information-theoretic SSL methods such as XR [23] and SMIR [27]. These methods have not accounted for multi-relational information in their formulation.\nComparisons with existing MRL methods. Our CSL approach compares favourably to the MRGP method [33] in several ways. First, CSL adopts a discriminative probabilistic model, which should in principle be more accurate than the generative model used in MRGP [30]. Second, MRGP handles only binary relations (graphs), whereas ours can take weighted graphs. Third, MRGP is trained using the expectation maximization (EM) algorithm, which does not warrant a globally optimal solution. CSL is also less restrictive than the MRL methods in [32], [33]. The latter treat multi-relational information (i.e., multigraph) as constraints to the learning process, whereas ours casts multirelational information into multi-relational features that in turn serve as additional information to be augmented into the learning process. Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29]. These methods rely on low-rank assumption for matrix/tensor decomposition, and do not yet cater for explicit (i.e., non-latent) features defined for each entity."}, {"heading": "3 PROPOSED FRAMEWORK", "text": "Our CSL framework operates based on two inputs: 1) partially labeled data, comprising a feature matrix X with known labels YL and missing labels YU , and 2) multigraph, composed of multiple directed graphs Gm that encode different types of social relationship. We first describe our notations: Let G = {G1, . . . ,Gm, . . . ,GM} be a multigraph composed of M graphs, where each graph Gm = (V,Em) comprises nodes V and edges Em. Note that here we have a common set of nodes V, but different sets of edges Em. We denote the feature matrix of nodes V as X \u2208 RN\u00d7J , and the\nweighted adjacency matrix of edges Em as Wm \u2208 RN\u00d7N , whereN and J are the number of nodes and features respectively. We also denote the labels of V as Y, representing the user attributes of interest. Lastly, a node maps exactly to a data instance, so we shall use the two terms interchangeably."}, {"heading": "3.1 Probabilistic Foundation", "text": "We first outline the probabilistic formulation of our CSL approach here. Ultimately, our goal is to maximize the posterior distribution of the model parameters \u0398, given the labels Y, node features X, and multigraph G. Here the posterior can be computed using the Bayes\u2019 rule:\np(\u0398|Y,X,G) = p(\u0398,Y,X,G) p(Y,X,G) = p(Y|X,G,\u0398)p(\u0398) p(Y,X,G)\n\u221d p(Y|X,G,\u0398)p(\u0398) (1)\nIn this work, we focus on partially labeled data, whereby only a few data instances have observed labels YL, while the remaining instances are largely unlabeled, i.e., their labels YU are assumed to be missing at random [11]. Since Y = YL \u222a YU , it follows that p(Y|X,G,\u0398) = p(YL|X,G,\u0398)p(YU |X,G,\u0398) and the posterior becomes:\np(\u0398|Y,X,G) \u221d p(YL|X,G,\u0398)p(YU |X,G,\u0398)p(\u0398) (2)\nwhere YL and YU are treated as conditionally independent. In turn, we can maximize the posterior p(\u0398|Y,X,G) by minimizing its negative logarithm (a.k.a. loss function) L:\nL = \u2212 ln(p(YL|X,G,\u0398))\u2212 ln(p(YU |X,G,\u0398))\u2212 ln(p(\u0398)) (3)\nFor convenience, we break (3) into two parts, respectively:\nLL = \u2212 ln(p(YL|X,G,\u0398))\u2212 ln(p(\u0398)) (4) LU = \u2212 ln(p(YU |X,G,\u0398)) (5)\nRemark. It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28]. All these approaches focus only on a single type of relationship, whereas ours can readily cater for multiple types of relationship (i.e., multigraph G). Moreover, the existing methods do not yet exploit the additional information provided by unlabeled data (i.e., YU ) in guiding their learning processes."}, {"heading": "3.2 Base Model", "text": "The proposed CSL approach can be viewed as a generalization of the contemporary logistic regression model [10]. Traditionally, logistic regression learns in a fully supervised fashion based solely on the labeled data YL (i.e., it does not use YU ), and it does not take into account multi-relational information encoded as multigraph G. That is, by excluding YU and G and by assuming independent and identically distributed (i.i.d) data instances, logistic regression essentially learns to minimize the following loss function:\nLL = \u2212 L\u2211 i=1 ln(p(yi|xi,\u0398))\u2212 J\u2211 j=1 ln(p(\u03b8j)) (6)\nwhere yi \u2208 YL and xi \u2208 X are the actual label and feature vector for data instance i respectively, L = |YL| is the\n5 number of labeled data instances, and \u03b8j \u2208 \u0398 is a model (i.e., weight) parameter that we want to learn for each feature j.\nWithout loss of generality, we consider binary class label1 yi \u2208 {0, 1}. For binary classification, we may take that each sample likelihood p(yi|xi,\u0398) follows a Bernouli distribution (which is analogous to the toss of a coin):\np(yi|xi,\u0398) = \u03c3yii (1\u2212 \u03c3i) (1\u2212yi) (7)\nwhere \u03c3i = \u03c3(f(xi,\u0398)) = 11+exp(\u2212f(xi,\u0398)) refers to the logistic function, and f(xi,\u0398) is the linear model:\nf(xi,\u0398) = J\u2211 j=1 \u03b8jxi,j (8)\nFor the prior p(\u03b8j), we use a Gaussian distribution with zero mean and inverse variance \u03bb:\np(\u03b8j) = 1\nZ exp\n( \u2212\u03bb\n2 \u03b82j\n) (9)\nwhere Z is a normalizing constant and \u03bb > 0. Accordingly, we can write the overall loss LL for logistic regression as:\nLL =\u2212 L\u2211 i=1 [yi ln(\u03c3i) + (1\u2212 yi) ln(1\u2212 \u03c3i)] + \u03bb 2 J\u2211 j=1 \u03b82j\n(10) Note that the regularization term \u03bb2 \u2211J j=1 \u03b8 2 j serves to penalize large values of the model parameters \u03b8j , thereby reducing the risk of data overfitting [10]."}, {"heading": "3.3 Multi-Relational Features", "text": "We now extend the base logistic regression model to incorporate the multi-relational information G through adding multi-relational features (MRF). Specifically, by incorporating G into the parameterization of the base model in (8), we obtain an extended linear model f(xi,G,\u0398):\nf(xi,G,\u0398) = J\u2211 j=1 \u03b8jxi,j + M\u2211 m=1 J\u2211 j=1 \u03b1m,j\u03c0m,i,j (11)\nand correspondingly \u03c3i = \u03c3(f(xi,G,\u0398)), where \u03c0m,i,j is the jth relational feature of data instance (i.e., node) i for graph Gm, and \u03b1m,j is the corresponding jth relational weight for Gm, and \u0398 = {\u03b8j}\u222a{\u03b1m,j} is the set of all model parameters. Under this notation, we call \u03b8j as the self weight corresponding to the self features xi,j of node i.\nThere are numerous ways to define the relational feature \u03c0m,i,j of a node i. In principle, one can derive the relational features through an arbitrary aggregation function summarizing some global or local properties of each graph Gm, and the aggregation function need not be the same for different graphs Gm. For efficiency and interpretability, however, in this work we focus on a simple aggregation function that combines the information from only the immediate neighbors of a node i by taking a weighted average of their features:\n\u03c0m,i,j =\n\u2211 (i,i\u2032)\u2208Em wm,i,i\u2032xi\u2032,j\u2211\n(i,i\u2032)\u2208Em wm,i,i\u2032 (12)\n1. Extension to multi-class task with C > 2 labels is straightforward, which can be done by constructing C binary logistic regression models.\nwhere wm,i,i\u2032 \u2208 Wm represents the tie strength of a node (instance) iwith its neighbor i\u2032 in graph Gm. For instance, in the context of Twitter follow graph, the notion of neighbors refers to the followees of a given user.\nWith the addition of the relational weights \u03b1m,j , the penalized loss LL now becomes:\nLL =\u2212 L\u2211 i=1 [yi ln(\u03c3i) + (1\u2212 yi) ln(1\u2212 \u03c3i)]\n+ \u03bb\n2 J\u2211 j=1 ( \u03b82j + M\u2211 m=1 \u03b12m,j ) (13)\nRemark. The MRF formulation in (11) and (12) provides a simple yet powerful way to incorporate multiple types of social relationship into user attribute prediction. Such formulation has several key appeals:\n\u2022 Unlike previous MRL methods that use multigraph to constrain the learning processes, e.g., [32], [33], or rely only on latent features, e.g., [7], [13], [26], our MRF formulation is more generic and makes less stringent assumption. That is, we treat multigraph as additional source of information, and we can use any aggregation function to summarize this information. \u2022 Our MRF formulation can also readily cater for different types of features, such as numeric features (e.g., tweet count), n-gram representation of text features, or binary vector of categorical features. For ease of interpretation/analysis, though, we shall focus on the n-gram text features in this work. \u2022 By aggregating and augmenting the neighbors\u2019 features on a per-graph basis, we can exploit the dependencies and complementarity among various features, while preserving the semantics of each type of relationship. Especially, the learned relational weights \u03b1m,j can be used to understand the contribution and importance of different types of relationship in modeling latent user attributes. \u2022 From a computational standpoint, the MRF formulation is efficient. First, the aggregation function keeps the problem dimensionality moderate; we only require (M+1)\u00d7J features instead of na\u0131\u0308vely appending all neighbors\u2019 features. Second, the relational features \u03c0m,i,j can be pre-computed once for every instance/node i prior to parameter learning process. Finally, our MRF formulation maintains the linearity of our model (11), which preserves the convexity of the overall loss (3) (see Section 3.5).\nIt is also worth noting that our MRF formulation is different from that of conditional random field (CRF) [30]. The CRF approach usually involves some form of dependencies among the (output) labels yi, whereas our MRF method focuses on the dependencies in the input space xi and assumes that the labels yi are (conditionally) independent. While structured modeling via CRF can potentially improve performance, it comes at the expense of higher computational complexity and degraded model interpretability. As such, we do not pursue the CRF approach in this work.\n6"}, {"heading": "3.4 Convex Divergence Regularization", "text": "After constructing the MRF for all data instances (both labeled and unlabeled), CSL carries out a semi-supervised learning (SSL) using unlabeled data for improving model generalization and robustness. To this end, we put forward the idea of convex divergence (CD) to regularize learning via unlabeled data. The CD regularization stems from the following definition of p(YU |X,G,\u0398) in (2):\np(YU |X,G,\u0398) = 1\nZ exp (\u2212\u03b2Df (\u00b5||\u03c1)) (14)\nwhereZ is a normalizing constant, \u03b2 is a (nonnegative) userspecified regularization parameter, and Df (\u00b5||\u03c1) is the f - divergence [2] between two distributions \u00b5 and \u03c1:\nDf (\u00b5||\u03c1) = \u2211 z f ( \u00b5(z) \u03c1(z) ) \u03c1(z) (15)\nwhich is defined over some space z and f : [0,+\u221e) \u2192 R+ is a continuous convex function, such that f(1) = 0.\nWe note here that Df (\u00b5||\u03c1) does not uniquely define the form of the prior distribution p(YU |X,G,\u0398), but the latter can be constructed through constraints imposed by Df (\u00b5||\u03c1). Also, to ensure convexity in the overall loss L, it is necessary to choose the distributions \u00b5 and \u03c1 such that Df (\u00b5||\u03c1)\u2014or its approximation\u2014is twicedifferentiable, and its second derivative52Df (\u00b5||\u03c1) is nonnegative for all possible values of z.\nIn this work, we focus on an instantiation of Df (\u00b5||\u03c1) that involves computing the Kullback-Leibler (KL) divergence between some class prior p\u0303 and the expected predictions E[\u03c3i] made by the model on unlabeled data:\nDKL(\u00b5||\u03c1) = p\u0303 ln ( p\u0303\nE[\u03c3i] ) = \u2212p\u0303 ln(E[\u03c3i]) + p\u0303 ln (p\u0303)\ufe38 \ufe37\ufe37 \ufe38\n=constant\n\u221d \u2212p\u0303 ln(E[\u03c3i]) (16)\nwhere the function f is defined as f(t) = t ln(t). In this case, DKL(\u00b5||\u03c1) = 0 when \u00b5 and \u03c1 match exactly. Our goal here is to minimize (16), implying that we want to obtain a classification model such that the expectation of its predictions on unlabeled data is similar to the class prior.\nThe class prior can be either (manually) specified based on domain knowledge, or computed based on the class distribution on the labeled data. For simplicity, we choose the latter approach in this work, by defining p\u0303 as p\u0303 = 1L \u2211L i=1 yi, i.e., the proportion of positive instances in the labeled data. Finally, by subtituting (16) into (14) and dropping constant terms, we obtain the CD regularization:\nLU =\u2212 ln (p(YU |X,G,\u0398)) \u221d\u2212 ln (\u2212\u03b2DKL(p\u0303||p\u0302\u0398)) \u221d\u2212 \u03b2 p\u0303 ln (E[\u03c3i]) (17)\nNote that (17) is not convex in its current form. Fortunately, we can use the Jensen inequality [16] in order to derive a convex upper bound of (17). The Jensen inequality states that the expectation of a convex function \u03d5 is equal to or\ngreater than the function of the expectation, i.e., E[\u03d5(x)] \u2265 \u03d5(E[x]). It then follows that the convex upper bound is:\nLU \u221d \u03b2 p\u0303 (\u2212 ln (E[\u03c3i])) \u2264 \u03b2 p\u0303 E [\u2212 ln (\u03c3i)] (18)\nwith \u03d5(x) = \u2212 ln(x). Subsequently, we can approximate the expectation via an empirical average E [\u2212 ln (\u03c3i)] = \u2212 1U \u2211L+U i=L+1 ln (\u03c3i)), where U = |YU | is the total number of unlabeled data instances. This leads to a new convex formulation of LU using unlabeled data:\nLU \u221d\u2212 \u03b2 p\u0303 L+U\u2211 i=L+1 ln (\u03c3i) (19)\nwhereby we absorb the term 1U into \u03b2 for simplicity. Remark. The formulation in (19) is related to the XR approach [23], with some key differences. First, the XR method tries to minimize (16) directly, which is non-convex and may lead to local optima. In contrast, our CD formulation aims at reducing (16) by minimizing its convex upper bound (19), which is simpler and computationally more appealing (due to convexity). Second, our formulation generalizes XR by not only learning from unlabeled data, but also taking into account the different types of relationship among instances via MRF. We will empirically show in Section 5 how multirelational information and unlabeled data can work together to improve user profiling performances. Our formulation is also conceptually superior to that of the SMIR method [27], whose convexity is not guaranteed when the L2 regularization parameter (\u03bb in our notation) is not sufficiently large [27]. We will show in Section 3.5 that our CSL formulation imposes strict convexity for any positive \u03bb."}, {"heading": "3.5 Parameter Learning", "text": "We can now combine (17), (13) and (11) to derive the overall loss function L for the CSL approach, which is given by:\nL =\u2212 L\u2211 i=1 [yi ln(\u03c3i) + (1\u2212 yi) ln(1\u2212 \u03c3i)]\n+ \u03bb\n2 J\u2211 j=1 ( \u03b82j + M\u2211 m=1 \u03b12m,j ) \u2212 \u03b2 p\u0303 L+U\u2211 i=L+1 ln (\u03c3i) (20)\nwith \u03c3i,1 = \u03c3(f(xi,G,\u0398)) and \u03c3i,2 = 1\u2212 \u03c3(f(xi,G,\u0398)). Convexity. Before we proceed with the learning procedure for minimizing L, we first give a proof sketch for the convexity of L. This is done by examining the slope (i.e., first derivative) and curvature (i.e., second derivative) of L. Firstly, the slope with respect to each self weight \u03b8j is:\n\u2202L \u2202\u03b8j =\u2212 L\u2211 i=1 yi \u03c3i \u2202\u03c3i \u2202\u03b8j + \u03bb\u03b8j \u2212 \u03b2 p\u0303 L+U\u2211 i=L+1 1 \u03c3i \u2202\u03c3i \u2202\u03b8j\n(21)\nand it is easy to show that \u2202\u03c3i\u2202\u03b8j = \u03c3i (1\u2212 \u03c3i)xi,j , where \u03c3i = \u03c3(f(xi,G,\u0398)). We can then evaluate the slope as:\n\u2202L \u2202\u03b8j =\u2212 L\u2211 i=1 [( yi \u03c3i \u2212 1\u2212 yi 1\u2212 \u03c3i ) \u2202\u03c3i \u2202\u03b8j ] + \u03bb\u03b8j \u2212 \u03b2 p\u0303 L+U\u2211 i=L+1 1 \u03c3i \u2202\u03c3i \u2202\u03b8j\n= L\u2211 i=1 [(\u03c3i \u2212 yi)xi,j ] + \u03bb\u03b8j \u2212 \u03b2 p\u0303 L+U\u2211 i=L+1 [(1\u2212 \u03c3i)xi,j ]\n(22)\n7\nAlgorithm 1 CSL Learning Procedure Input: Feature matrix X, actual labels YL, multigraph G Output: Model parameters \u0398 = {\u03b8j} \u222a {\u03b1m,j}\n1: Initialize all parameters \u03b8j and \u03b1m,j to zero 2: for each graph m \u2208 {1, . . . ,M} do 3: Construct the relational features \u03c0m,i,j using (12) 4: end for 5: repeat 6: Generate the prediction scores \u03c3i based on (11) 7: Compute the overall loss L using (20) 8: Compute the slopes \u2202L\u2202\u03b8j (as well as \u2202L \u2202\u03b1m,j ) via (22)\n9: Perform an L-BFGS iteration using L and \u2202L\u2202\u03b8j ( \u2202L \u2202\u03b1m,j ) 10: until stopping criterion is met\nFinally, we may differentiate (22) to obtain the curvature:\n\u22022L \u2202\u03b82j = L\u2211 i=1 [ \u03c3i (1\u2212 \u03c3i)x2i,j ] + \u03bb+ \u03b2 p\u0303 L+U\u2211 i=L+1 [ \u03c3i (1\u2212 \u03c3i)x2i,j ] (23)\nIt is clear that the curvature (23) will always be positive for any positive \u03bb (since \u03c3i \u2208 [0, 1]). We can thus conclude that the full loss L is strictly convex for \u03bb > 0. The convexity for the relational weight \u03b1m,j can be proven in the same manner, and thus we omit the details here for brevity.\nAlgorithm. Thanks to the convexity trait, we can use any off-the-shelf gradient-based algorithm to learn the parameters of our CSL model. In this work, we use the limited memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (L-BFGS) algorithm [22], a popular, efficient Quasi-Newton method for solving unconstrained optimization problems2. Algorithm 1 outlines the overall CSL learning procedure, combining the MRF and CD formulations. As for the stopping criterion, we terminate the algorithm when a maximum iteration I (default: 10) is reached, or the maximum projected slope is below a threshold (default: 10\u22125). Notably, the memory/time complexity of the L-BFGS algorithm is linear in the problem size [22], and the convexity of our CSL formulation makes it possible to reach the optimum within a few iterations."}, {"heading": "4 USER PROFILING IN TWITTER", "text": "This section provides an overview of the Twitter dataset and user profiling tasks we consider in this work."}, {"heading": "4.1 Twitter Dataset", "text": "In our study, we use the Twitter data of Singapore users\u2014 hereafter called SGTwitter\u2013collected from the period of 1\u201328 February 2014. Starting from a set of seed Singapore users, we crawled their network based on the follow, retweet, and user mention links. Next, we added to our user base those followers/followees, retweet sources, and mentioned users who declare Singapore as their profile location. Accordingly, we obtained a total of 130,142 public user accounts whose profiles can be accessed/studied. We then focused on active users who tweeted at least twice within 1 month, which gave us the final set of 100,497 active users.\n2. More specifically, we use an implementation of the L-BFGS algorithm provided in the SciPy library: http://goo.gl/q2dfnZ\nTable 1 summarizes the count statistics of our SGTwitter data for different activities, aggregated at the user level. In general, we can see that the activity counts follow a heavytail distribution. As expected, other than celebrity users, a user generally follows more users than being followed. Intuitively, a user could select followees he/she is interested in, but not the followers. Hence, we can expect the followee links to be a better representation of user interests than the follower links. On the other hand, we can see that user mention and retweet activities are much more focused/targeted, resulting in sparser connectivity in the mention and retweet graphs than in the follow graph. We shall focus on the followee, mention and retweet links in our studies later."}, {"heading": "4.2 User Attributes", "text": "In this work, we consider the task of classifying two user attributes (i.e., labels): account type (i.e., personal vs. organization) and marital status (i.e., single vs. married)3. Profiling these attributes is a relatively new problem that has not been well studied before. This could bring about benefits in terms of providing customized services/supports that cater for the different needs of each user type. For example, organization accounts may require a service to standardize the format of their content postings or to track sentiments on their products, whereas personal accounts would likely benefit from personalized friend and content recommendation. Similarly, married users would likely be more interested in familyrelated products or contents than single users.\nTo derive the account type and marital status labels, we first defined several keywords/phrases describing the respective labels. For the account type task, we detected organization accounts by checking if the URLs in their profile description end with \u201ccom.sg\u201d, \u201cedu.sg\u201d and \u201cgov.sg\u201d. We randomly sampled the remaining Twitter users, and then manually labeled and judged if they are personal accounts. For the marital status, examples of relevant keywords/phrases are \u201cwife\u201d, \u201cspouse\u201d, \u201cmy son\u201d for married users, and \u201cgirlfriend\u201d, \u201cin a relationship\u201d, \u201cmy gf\u201d for single (unmarried) users. After identifying accounts with the relevant keywords/phrases in their profile description, we manually verified the label assignment of each account.\n3. While our work currently focuses on two user attributes, we note that CSL is general and readily applicable to any attributes/labels.\n8 Table 2 summarizes the label distribution of our SGTwitter data. Here the minority class labels are \u201corganization\u201d and \u201cmarried\u201d for the account type and marital status tasks, respectively. Our main interest is to correctly predict these minority cases, which are expected to form a small portion in the complete data, and are thus harder to predict."}, {"heading": "4.3 Feature Extraction", "text": "Our primary interest here is to investigate to what extent the contents generated by a user can be used to infer his/her (latent) attributes. As such, this work shall be focused primarily on text features derived from users\u2019 tweets, though we note that our CSL approach is generic and can work on any type of features. We use the term document to refer to a data instance i, which represents the collection of tweets posted by a user i. In this context, our goal is to infer a user\u2019s attribute based on his/her tweet document.\nTo extract the text features, we first converted the raw tweets into a bag-of-words vector, from which we can derive an n-gram representation suitable for our CSL model. We summarize our feature extraction steps as follows:\n\u2022 Tokenization: We broke a tweet document into its constituent word tokens, and then created bags of word tokens, where each bag has the frequency of the tokens appearing in a document. Prior to tokenization, we also converted all letters to lowercase and devised regular expressions to extract and retain special entities such as emoticons, HTML/URL tags, phone numbers, and hashtags. \u2022 Stop-word removal: We then omitted words that appear very frequently and contribute little to discriminating the tweets of a user from those of another user. We used the list of English stop-words in [19]. \u2022 Normalization: To normalize the word frequencies, we applied the term frequency\u2013inverse document frequency (TF-IDF) scheme [3], which puts greater importance on words that appear frequently in a document, and deems words that occur in many documents as less important. Our TF-IDF vectors comprise 1-gram, 2- gram, and 3-gram representations [3]."}, {"heading": "4.4 Multi-Relational Information", "text": "In our study on the SGTwitter data, we consider the multi-relational information G derived from three directed graphs: the follow, mention, and retweet graphs. The follow graph contains binary edge weights. That is, wm,i,i\u2032 = 1 if a user i follows another user i\u2032 and 0 otherwise. On the other hand, the weights of the mention/retweet graph refer to the number of times (count) of a user i mentioning/retweeting user i\u2032. In this case, no edge is constructed for a zero count. For each user, we consider his/her out-edges in order to compute the relational features \u03c0m,i,j for all three graphs."}, {"heading": "5 EXPERIMENTAL RESULTS AND ANALYSIS", "text": "This section presents the results of our study on profiling the account type and marital status of the SGTwitter users. We aim at addressing several research questions (RQs):\n\u2022 RQ1: How does the performance of CSL compare with that of other SSL methods?\n\u2022 RQ2: How do multi-relational features and unlabeled data contribute to the performance? \u2022 RQ3: What are the important features and relationship types for predicting user labels? \u2022 RQ4: How well can the learned CSL model generalize to unseen (unlabeled) data? \u2022 RQ5: What can predictions made by CSL tell about a larger user population?\nProcedure. To address the above RQs, we consider two scenarios: evaluation using labeled data (for RQ1\u2013RQ3), and evaluation using unlabeled data (for RQ4\u2013RQ5). For the first scenario, we adopt a stratified 10-fold CV procedure, i.e., we split the SGTwitter data into 10 sets of training and testing data, each retaining the class label proportion as per the original data. We then report the averaged performance. Also, for all SSL methods considered in this study, their training for each fold involves using labeled instances in the training set, plus all the remaining unlabeled instances from the full original data. For the second scenario, we manually inspect the top k-predicted users for each class label. The goal is to see how well our method predicts on completely unseen data (i.e., not labeled apriori), and compare the predictions with the labeled dataset.\nMetric. To quantify performance, we examine the averaged F1-score, an evaluation metric that is popularly used in text classification and information retrieval [3]. The F1-score measures classification accuracy in terms of a harmonic mean of Precision andRecall, i.e., F1 = 2\u00d7Precision\u00d7RecallPrecision+Recall , where Precision = TPTP+FP , Recall = TP TP+FN , and TP , FP and FN are the true positives, false positives, and false negatives respectively. For these metrics, again the positive class refers to the minority labels, i.e., \u201corganization\u201d for account type and \u201cmarried\u201d for marital status. Lastly, we also look at the averaged training time (in seconds) of a given method, so as to gauge its computational efficiency.\nBaseline. We compare our CSL method with several representative SSL baseline methods. The first baseline is bootstrapping [12], where we first train a logistic regression using labeled data, apply it to predict on unlabeled data, and then add into the labeled dataset those samples that have high prediction scores. We repeat this for 10 iterations, where for each iteration n \u2208 {1, . . . , 10}, we add the top n10 predicted samples into the labeled set. The second baseline is label spreading (LS), a popular graph-based SSL method [36]. We reiterate from Section 2.2 that LS relies on the affinity assumption, and its success depends on the choice of affinity matrix. We explore two renowned variants of kernel functions to define the affinity matrix in LS: k-nearestneighbor (kNN) kernel, and radial basis (RBF) kernel [36].\nAdditionally, we compare our CSL approach with two state-of-the-art information theoretic SSL methods, namely entropy regularization (ER) [11] and expectation regularization (XR) [23]. The overall loss functions L for the ER and XR methods are respectively as follows:\nER : L =\u2212 L\u2211 i=1 [yi ln(\u03c3i) + (1\u2212 yi) ln(1\u2212 \u03c3i)]\n\u2212 \u03b2 L+U\u2211 i=L+1 \u03c3i ln (\u03c3i) (24)\n9\nXR : L =\u2212 L\u2211 i=1 [yi ln(\u03c3i) + (1\u2212 yi) ln(1\u2212 \u03c3i)]\n+ \u03bb\n2 J\u2211 j=1 ( \u03b82j + M\u2211 m=1 \u03b12m,j ) \u2212 \u03b2 ln ( p\u0303 L+U\u2211 i=L+1 \u03c32i ) (25)\nConfiguration. For the ER, XR, and CSL methods, we chose the best SSL regularization parameter \u03b2 from the following candidate list: {10\u22124, 10\u22123, 10\u22122, 10\u22121, 1, 10, 100}. Meanwhile, the L2 regularization parameter for XR and CSL was fixed to \u03bb = 1, which we found to give good results overall. We also note that all experiments presented in this paper were carried out on a computer server with the following virtual machine configuration: 7-core Intel Xeon 2.6 GHz processor with a total of 70 GB memory (RAM).\nSignificance test. To evaluate whether the performance difference between two methods is statistically significant, we perform the Wilcoxon signed-rank test [34] with a critical value of 0.05. The Wilcoxon test provides a non-parametric alternative to the t-test for matched pairs, when the pairs cannot be assumed to be normally distributed. When the test yields a p-value less than 0.05, we deem that the performance difference is significant (and vice versa)."}, {"heading": "5.1 Performance Comparisons (RQ1)", "text": "We first evaluate the 10-fold CV results of our method using the \u201cfull\u201d training set. Specifically, for each fold, we train our CSL method using 90% of the available labeled data (i.e., 90% of 1,308 and 2,313 labeled accounts for account type and marital status tasks respectively; cf. Table 2), plus all the remaining unlabeled data. Tables 3 and 4 show the results for the two tasks respectively, comparing the F1-scores and training time of our CSL method with those of all the other SSL methods. To facilitate comprehensive evaluations, we present the results for different MRF settings: no graph, single graph, and all three graphs. In addition, we show the pvalues of the Wilcoxon test for comparing the F1-scores of the respective MRF settings (e.g., the p-value for bootstrapping with follow graph features involves comparison to CSL with follow graph features as well). As an additional reference, we include the F1-score produced by a random guess4\nFrom Tables 3 and 4, we can see that our CSL method significantly outperforms the bootstrapping and LS methods, in terms of both F1-score and training time. This is true for all graph configurations. The F1-score of our method is also substantially better than that of the random guess baseline. It can be seen here that the \u201chigh-confidence prediction is correct\u201d assumption of the bootstrapping method leads to a suboptimal performance (especially for the marital status task). It is also evident that the affinity assumption of the LS methods is inappropriate for our profiling tasks. In sum, these show that incorrect assumption about the data/task at hand in SSL can lead to mistakes that reinforce themselves.\nAdditionally, the results show that in general our CSL approach compares favourably to the ER and XR methods in terms of F1-score, although there are cases where the performance difference is marginal (i.e., p-value \u2265 0.05). Regardless, we will show later in a further sensitivity study\n4. Random guess refers to the case where TP TP+FN = FP FP+TN .\n(to be presented in Section 5.2) that CSL is significantly more robust than the two methods. As for the training time, Tables 3 and 4 show that CSL is as efficient as the ER and XR methods, but is an order of magnitude faster than the bootstrapping and LS methods. Finally, comparing the different graph configurations, we find that incorporating\n10\nrelational features from the follow graph alone already leads to F1-scores comparable to those using all the three graphs. This implies that the retweet and mention features are not as useful as the follow features. Nevertheless, unlike the LS methods (especially for the marital status task), using all three graphs in CSL does not significantly degrade the F1-scores (compared to using the follow graph alone). This suggests that CSL can make better use of MRF, even when noisy or less relevant relational features are used."}, {"heading": "5.2 Contribution of MRF and Unlabeled Data (RQ2)", "text": "To see the contributions of the MRF and CD formulations, further sensitivity studies were carried out by varying the graph configurations and number of labeled instances in the training data, respectively. We chose the number of labeled instances L from {25, 50, 100, 200, 400, 800, \u201cfull\u201d}, where \u201cfull\u201d refers to 90% of all the labeled instances, as already explained in Section 5.1. Fig. 2 and 3 consolidate the results of our studies for account type and marital status tasks respectively. Note that the results at the right-hand extremes of the figures correspond those in Tables 3 and 4.\nWe can see here that the CD regularization in CSL yields more robust and consistent F1-scores than the ER and XR\nregularization, especially for small labeled data size. For example, even when L = 25, CSL is able to achieve F1-score not far from that obtained with L = \u201cfull\u201d, for both profiling tasks. This is also verified by our Wilcoxon tests, all of which yielded p-values less than 0.05. Moreover, our CSL method performs more consistently than the other methods under different graph configurations. As before, the retweet and mention features contribute less compared to the follow features. Finally, Fig. 4 and 5 present the breakdown of training times of different methods. It is again shown that CSL is on par with ER and XR, but substantially faster than the bootstrapping and LS methods. All in all, these show that CSL exhibits both robustness and efficiency, making it more preferable than other methods for profiling tasks."}, {"heading": "5.3 Feature Importance Analysis (RQ3)", "text": "We can now probe into the parameters of the trained CSL model, and investigate in details which features are the most discriminative for our user profiling tasks. Specifically, we assess the importance of the individual features by looking at the learned self weights \u03b8j and relational weights \u03b1m,j . Fig. 6 and 7 present the top 15 features (i.e., having the largest absolute values of \u03b8j and \u03b1m,j) for the account type\n11\nand marital status profiling tasks respectively. The leftmost bar chart in each figure shows the self weights \u03b8j , while the remaining charts show the relational weights \u03b1m,j for the follow, mention, and retweet graphs respectively. We also note that a positive weight suggests that the respective feature is correlated with the positive label (i.e., \u201corganization\u201d or \u201cmarried\u201d), whereas a negative weight corresponds to the negative label (i.e., \u201cpersonal\u201d or \u201csingle\u201d).\nThe results reveal several interesting insights that conform with our intuition. For example, in the account type profiling task, we can see that informal expressions such as \u201cjust\u201d, \u201clol\u201d or \u201chaha\u201d are correlated with personal accounts, whereas organizations tend to be associated with more neutral words such as \u201csingapore\u201d, \u201cnew\u201d, or \u201ccny\u201d (acronym for \u201cChinese New Year\u201d). Similarly for the marital status task, family-related words such as \u201cgod\u201d, \u201cgreat\u201d or \u201ckids\u201d are often correlated with married people, whereas single people consist of young adults and students, who like to use words such as \u201cschool\u201d, \u201cshit\u201d, etc. In general, we also find that the feature correlation is consistent across different feature groups (e.g., when \u201csingapore\u201d in self features correlates with the organization account, the correlation also holds for \u201csingapore\u201d in mention features).\nFurthermore, it is evident from the overall weight mag-\nnitudes in Fig. 6 and 7 that the self features are the most discriminative, though the other, relational features are still collectively useful. Among the three groups of relational features, we find that those derived from the follow graph are the most relevant. This can be attributed to the fact that the follow graph is more dense than the retweet or mention graph (see Table 1), thus yielding more informative features.\nTo examine the validity of the learned top features, we grouped all (labeled) instances according to the labels and examined the feature distributions for each label. Fig. 8 shows the cumulative distribution function (CDF) of the top positive and negative features for each feature group in the account type task. An early increase in the CDF value implies a more skewed feature distribution, according to which we can then judge whether the learned positive or negative correlation is valid. For instance, the CDF of the word \u201csingapore\u201d is less skewed for organization accounts, suggesting that organizations use the word more often than personal accounts. Comparing Fig. 6 with Fig. 8, we can conclude that, overall, the feature distributions conform with the learned feature correlations. The conclusion also holds for all the remaining features as well as the features for the marital status task. Owing to space constraints, however, we are not able to show the exhaustive results here.\n12"}, {"heading": "5.4 Generalization to Unseen Data (RQ4)", "text": "Further studies have been conducted to evaluate the ability of our learned model to generalize to novel, unlabeled data. To this end, we used our trained CSL model to predict for all unlabeled data, and picked the top K positive instances with the highest prediction scores \u03c3i, as well as the top K negative instances with the highest (1 \u2212 \u03c3i). We then manually inspected all these instances to see how well the CSL predictions match with human judgments. To assess the model robustness, we varied K from 20 to 100. For each class label, we recorded the number of correctly predicted instances (TP ), that of incorrectly predicted instances (FP ), and that of unclassifiable instances (UC). The UC refers to the case whereby we are unable to manually determine the actual label of an instance (i.e., a \u201cdon\u2019t know\u201d answer). We then computed the precision at top K (Prec) by excluding the unclassifiable instances, i.e., Prec = TPK\u2212UC .\nTables 5 and 6 summarize the results for account type and marital status tasks respectively. Overall, we can see that the learned CSL model can predict for both the account type and marital status pretty well, which is evident from the fairly low FP and high Prec results. Meanwhile, the UC numbers are generally low, except for the \u201cmarried\u201d label of the marital status task. The latter suggests that determining whether a person is married is a difficult task, even for humans. Nonetheless, the overall results demonstrate that our method has good generalization abilities."}, {"heading": "5.5 Inference on Larger User Population (RQ5)", "text": "The final part of our empirical studies consists of making inference on a larger population of SGTwitter users. To this end, we carried out quantitative and qualitative analyses on\nthe predictions made for all unlabeled data. Our quantitative study involves comparing the label distributions in the labeled dataset (as per Table 2) with the predicted distributions as inferred by CSL on the unlabeled dataset. Table 7 shows the results. We find that the predicted distributions are much more imbalanced than the distributions of the labeled data. Although we cannot fully verify this observation (due to the need to label all 100K samples), it is reasonable to expect that the larger SGTwitter population would consist of more personal accounts than organization ones, and more single users than married ones. The distribution difference also suggests that the larger population contains new cases that are not previously captured in the labeled data, and our method can generalize to these cases fairly well.\nFurther qualitative analysis on the individual users reveals additional insights about the (larger) SGTwitter population. Fig. 9 shows the screenshot of a top-predicted organization account that is not previously captured in the\n13\nlabeled dataset. In particular, the profile description of the account has an URL with a new suffix \u201c.sg\u201d, which is not part of the suffices used to derive the labeled dataset (i.e., \u201c.com.sg\u201d, \u201c.edu.sg\u201d and \u201c.gov.sg\u201d; see Section 4.2). This shows that our CSL method is able to properly predict for novel instances, based on content (word) features alone. Similarly for the marital status task, we found several interesting insights (not shown here due to space limitation). For instance, some of the top-predicted married users never use the keywords/phrases listed for the labeled data (see again Section 4.2) in their profile descriptions, but their profile pictures clearly show that they have a spouse or children."}, {"heading": "6 CONCLUSION", "text": "In this paper, we put forward a novel CSL approach for modeling/profiling the attributes of users in social media. The proposed approach provides a principled and efficient solution to the novel problem of simultaneously exploiting multiple types of social relationship and large pool of unlabeled data in user profiling tasks. The centerpiece of the proposed CSL approach is to first expand the input space by generically constructing a set of MRF features that capture different types of relationship, and then perform the CD regularization to establish convex semi-supervised learning using unlabeled data. The experimental results on Singapore Twitter users have demonstrated the accuracy, robustness, efficiency, as well as interpretability traits of our approach in profiling the user attributes.\nMoving forward, we wish to extend our methodology to more challenging profiling tasks involving multiple social\nFig. 9. Example of top organization account\nnetworks (e.g., Facebook, Foursquare, etc.). We also plan to develop a multi-task learning framework that can infer multiple user attributes jointly by modeling their dependencies (e.g., correlation between age group and marital status)."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office, Media Development Authority (MDA)."}], "references": [{"title": "Understanding the Yarowsky algorithm", "author": ["S. Abney"], "venue": "Computational Linguistics, vol. 30, no. 3, pp. 365\u2013395, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "A General Class of Coefficients of Divergence of One Distribution from Another", "author": ["S.M. Ali", "S.D. Silvey"], "venue": "Journal of the Royal Statistical Society, vol. 28, no. 1, pp. 131\u2013142, 1966.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1966}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 2399\u2013 2434, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Proceedings of the Annual Conference on Computational Learning Theory, 1998, pp. 92\u2013100.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Schlkopf", "A. Zien"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Probabilistic models for incomplete multi-dimensional arrays", "author": ["W. Chu", "Z. Ghahramani"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics, 2009, pp. 89\u201396.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Inferring user demographics and social strategies in mobile social networks", "author": ["Y. Dong", "Y. Yang", "J. Tang", "Y. Yang", "N.V. Chawla"], "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2014, pp. 15\u201324.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimizing multi-relational factorization models for multiple target relations", "author": ["L.R. Drumond", "E. Diaz-Aviles", "L. Schmidt-Thieme", "W. Nejdl"], "venue": "Proceedings of the ACM International Conference on Information and Knowledge Management, 2014, pp. 191\u2013200.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 1871\u20131874, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1871}, {"title": "Entropy regularization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "Semi- Supervised Learning, O. Chapelle, B. Sch\u00f6lkopf, and A. Zien, Eds., 2006, pp. 151\u2013168.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Analysis of semi-supervised learning with the Yarowsky algorithm", "author": ["G. Haffari", "A. Sarkar"], "venue": "Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2007, pp. 159\u2013166.  14", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "PARAFAC: Parallel factor analysis", "author": ["R.A. Harshman", "M.E. Lundy"], "venue": "Computational Statistics and Data Analysis, vol. 18, no. 1, pp. 39\u201372, 1994.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Twitter user profiling based on text and community mining for market analysis", "author": ["K. Ikeda", "G. Hattori", "C. Ono", "H. Asoh", "T. Higashino"], "venue": "Knowledge-Based Systems, vol. 51, pp. 35\u201347, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G. Obozinski"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 3176\u20133184.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Sur les fonctions convexes et les ingalits entre les valeurs moyennes", "author": ["J.L.W.V. Jensen"], "venue": "Acta Mathematica, vol. 30, no. 1, pp. 175\u2013 193, 1906.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1906}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "Proceedings of the International Conference on Machine Learning, 1999, pp. 200\u2013209.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Private traits and attributes are predictable from digital records of human behavior", "author": ["M. Kosinski", "D. Stillwell", "T. Graepel"], "venue": "Proceedings of the National Academy of Sciences, vol. 110, no. 15, pp. 5802\u20135805, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "RCV1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "Journal of Machine Learning Research, vol. 5, pp. 361\u2013397, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Weakly supervised user profile extraction from Twitter", "author": ["J. Li", "A. Ritter", "E. Hovy"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "User profiling in an ego network: Co-profiling attributes and relationships", "author": ["R. Li", "C. Wang", "K.C.-C. Chang"], "venue": "Proceedings of the International World Wide Web Conference, 2014, pp. 819\u2013830.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming, vol. 45, no. 3, pp. 503\u2013528, 1989.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1989}, {"title": "Simple, robust, scalable semisupervised learning via expectation regularization", "author": ["G.S. Mann", "A. McCallum"], "venue": "Proceedings of the International Conference on Machine Learning, 2007, pp. 593\u2013 600.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "You are who you know: Inferring user profiles in online social networks", "author": ["A. Mislove", "B. Viswanath", "K.P. Gummadi", "P. Druschel"], "venue": "Proceedings of the ACM International Conference on Web Search and Data Mining, 2010, pp. 251\u2013260.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "The role of unlabeled data in supervised learning", "author": ["T. Mitchell"], "venue": "Language, Knowledge, and Representation, J. Larrazabal and L. Miranda, Eds. Springer, 2004, vol. 99, pp. 103\u2013111.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the International World Wide Web Conference, 2012, pp. 271\u2013280.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Squared-loss mutual information regularization: A novel information-theoretic approach to semi-supervised learning", "author": ["G. Niu", "W. Jitkrittum", "B. Dai", "H. Hachiya", "M. Sugiyama"], "venue": "Proceedings of the International Conference on Machine Learning, 2013, pp. 10\u201318.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Classifying latent user attributes in Twitter", "author": ["D. Rao", "D. Yarowsky", "A. Shreevats", "M. Gupta"], "venue": "Proceedings of the International Workshop on Search and Mining User-Generated Contents, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Relational learning via collective matrix factorization", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2008, pp. 650\u2013658.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "Foundations and Trends in Machine Learning, vol. 4, no. 4, pp. 267\u2013373, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Mutual information estimation reveals global associations between stimuli and biological processes", "author": ["T. Suzuki", "M. Sugiyama", "T. Kanamori", "J. Sese"], "venue": "BMC Bioinformatics, vol. 10, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiple graph regularized protein domain ranking", "author": ["J. Wang", "H. Bensmail", "X. Gao"], "venue": "BMC Bioinformatics, vol. 13, no. 1, p. 307, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Unified video annotation via multigraph learning", "author": ["M. Wang", "X.-S. Hua", "R. Hong", "J. Tang", "G.-J. Qi", "Y. Song"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 19, no. 5, pp. 733\u2013 746, 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Individual comparisons by ranking methods", "author": ["F. Wilcoxon"], "venue": "Biometrics Bulletin, vol. 1, no. 6, pp. 80\u201383, 1945.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1945}, {"title": "Multi-relational learning with gaussian processes", "author": ["Z. Xu", "K. Kersting", "V. Tresp"], "venue": "Proceedings of the International Jont Conference on Artifical Intelligence, 2009, pp. 1309\u20131314.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 22, "context": "For these services, there is a need to profile user preferences and attributes so as to support personalization, advertising, and recommendation [24], [28].", "startOffset": 145, "endOffset": 149}, {"referenceID": 26, "context": "For these services, there is a need to profile user preferences and attributes so as to support personalization, advertising, and recommendation [24], [28].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Recent studies [18], [20], [24], [28] have nonetheless shown that it is possible to use statistical means to profile the latent user attributes, based on public data (e.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "Recent studies [18], [20], [24], [28] have nonetheless shown that it is possible to use statistical means to profile the latent user attributes, based on public data (e.", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "Recent studies [18], [20], [24], [28] have nonetheless shown that it is possible to use statistical means to profile the latent user attributes, based on public data (e.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "Recent studies [18], [20], [24], [28] have nonetheless shown that it is possible to use statistical means to profile the latent user attributes, based on public data (e.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "tion and social connectivity [14], [24], [28].", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "tion and social connectivity [14], [24], [28].", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "tion and social connectivity [14], [24], [28].", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "In this spirit, recent works [20], [21] have tried to incorporate social features, largely derived from a single type of relationship.", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "In this spirit, recent works [20], [21] have tried to incorporate social features, largely derived from a single type of relationship.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information.", "startOffset": 144, "endOffset": 148}, {"referenceID": 31, "context": "Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information.", "startOffset": 150, "endOffset": 154}, {"referenceID": 5, "context": "Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information.", "startOffset": 221, "endOffset": 224}, {"referenceID": 11, "context": "Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information.", "startOffset": 226, "endOffset": 230}, {"referenceID": 27, "context": "Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information.", "startOffset": 232, "endOffset": 236}, {"referenceID": 20, "context": ", the Quasi-Newton algorithm in [22]).", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "[28] proposed a set of network structure-based features to infer the attributes of Twitter users, including gender, age, geographical origin, and political preference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] used both global and local community detection methods in order to find communities of users who share common attribute values.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] used social communities to infer the demographic information of the Twitter users.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] showed that public information obtained from Facebook can be used to predict demographic attributes of users.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] devised a distant supervised learning method to infer the attributes of Twitter users by augmenting structured auxiliary data from the Facebook and Google+ networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "In [21], Li et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "[8] presented a factor graph model to predict the demographic attributes of mobile phone users.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The simplest form of SSL is bootstrapping, whereby a classifier is first trained using labeled data, and then applied to unlabeled data so as to generate more labeled samples for the next rounds of training [1], [12].", "startOffset": 207, "endOffset": 210}, {"referenceID": 10, "context": "The simplest form of SSL is bootstrapping, whereby a classifier is first trained using labeled data, and then applied to unlabeled data so as to generate more labeled samples for the next rounds of training [1], [12].", "startOffset": 212, "endOffset": 216}, {"referenceID": 3, "context": "Co-training is an extension of bootstrapping in which two (or more) classifiers are trained on different, ideally disjoint sets of features, and generate labeled samples to improve each other [5], [25].", "startOffset": 192, "endOffset": 195}, {"referenceID": 23, "context": "Co-training is an extension of bootstrapping in which two (or more) classifiers are trained on different, ideally disjoint sets of features, and generate labeled samples to improve each other [5], [25].", "startOffset": 197, "endOffset": 201}, {"referenceID": 4, "context": "Another class of SSL methods uses the low-density assumption [6], encouraging the decision boundary to lie in low-density regions for improving generalization results.", "startOffset": 61, "endOffset": 64}, {"referenceID": 15, "context": "The most common way to achieve this is to use a maximum margin algorithm such as transductive support vector machine [17].", "startOffset": 117, "endOffset": 121}, {"referenceID": 9, "context": "Grandvalet and Bengio [11] devised an alternative method based on entropy regularization (ER).", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": "There are also active research works on graph-based SSL (GSSL) methods, which treat both labeled and unlabeled data as nodes in a graph and build edges between pairs of nodes weighted by their affinities (similarities) [6].", "startOffset": 219, "endOffset": 222}, {"referenceID": 2, "context": "Generalizations have been proposed under the umbrella of manifold regularization [4], [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "Generalizations have been proposed under the umbrella of manifold regularization [4], [6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 9, "context": "Extending the entropy regularization method [11], several information-theoretic SSL methods have been developed [23], [27].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "Extending the entropy regularization method [11], several information-theoretic SSL methods have been developed [23], [27].", "startOffset": 112, "endOffset": 116}, {"referenceID": 25, "context": "Extending the entropy regularization method [11], several information-theoretic SSL methods have been developed [23], [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "Mann and McCallum [23] proposed the expectation regularization (XR) to build a simple and robust SSL method.", "startOffset": 18, "endOffset": 22}, {"referenceID": 25, "context": "[27] devised a squared-loss mutual information regularization (SMIR) method, which led to a convex SSL problem formulation guaranteed under a mild condition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "The key appeal of this approach is that an analytical (closed-form) solution can be computed to identify unique, globally optimal model parameters [27], [31].", "startOffset": 147, "endOffset": 151}, {"referenceID": 29, "context": "The key appeal of this approach is that an analytical (closed-form) solution can be computed to identify unique, globally optimal model parameters [27], [31].", "startOffset": 153, "endOffset": 157}, {"referenceID": 33, "context": "In [35], Xu et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "[33] proposed a MRL method for video annotation that integrates multiple graphs into a regularization framework, so as to sufficiently exploit their complementation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "A similar approach was used in [32] to tackle the task of protein domain ranking in structural biology.", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "This can be naturally extended to MRL by stacking the matrices to be factorized and then applying tensor factorization methods [7], [13].", "startOffset": 127, "endOffset": 130}, {"referenceID": 11, "context": "This can be naturally extended to MRL by stacking the matrices to be factorized and then applying tensor factorization methods [7], [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 24, "context": "Another natural extension to MRL is to share the common embedding or the entities across relations via collective matrix factorization [26], [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 27, "context": "Another natural extension to MRL is to share the common embedding or the entities across relations via collective matrix factorization [26], [29].", "startOffset": 141, "endOffset": 145}, {"referenceID": 24, "context": "This method has shown state-of-the-art performances on relational datasets [26], although the number of relation types is usually modest (less than 100).", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "Extensions have recently been proposed in [9], [15] to handle multirelational data with a large number of relation types.", "startOffset": 42, "endOffset": 45}, {"referenceID": 13, "context": "Extensions have recently been proposed in [9], [15] to handle multirelational data with a large number of relation types.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "In contrast to conventional SSL methods such as bootstrapping [1] and co-training [5], our CSL approach does not rely on the assumption that the model\u2019s own (high-confidence) prediction is correct, or that natural feature splits exist in the data.", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "In contrast to conventional SSL methods such as bootstrapping [1] and co-training [5], our CSL approach does not rely on the assumption that the model\u2019s own (high-confidence) prediction is correct, or that natural feature splits exist in the data.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "Our approach also provides a convex formulation of SSL that is more robust and computationally elegant than the ER method [11], whereby the learning procedure can be easily trapped to one of the (multiple) local optimal solutions.", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "Finally, the CSL approach is more general than the state-of-the-art information-theoretic SSL methods such as XR [23] and SMIR [27].", "startOffset": 113, "endOffset": 117}, {"referenceID": 25, "context": "Finally, the CSL approach is more general than the state-of-the-art information-theoretic SSL methods such as XR [23] and SMIR [27].", "startOffset": 127, "endOffset": 131}, {"referenceID": 31, "context": "Our CSL approach compares favourably to the MRGP method [33] in several ways.", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "First, CSL adopts a discriminative probabilistic model, which should in principle be more accurate than the generative model used in MRGP [30].", "startOffset": 138, "endOffset": 142}, {"referenceID": 30, "context": "CSL is also less restrictive than the MRL methods in [32], [33].", "startOffset": 53, "endOffset": 57}, {"referenceID": 31, "context": "CSL is also less restrictive than the MRL methods in [32], [33].", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 88, "endOffset": 91}, {"referenceID": 11, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": ", their labels YU are assumed to be missing at random [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 120, "endOffset": 123}, {"referenceID": 12, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 19, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 137, "endOffset": 141}, {"referenceID": 22, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 143, "endOffset": 147}, {"referenceID": 26, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 149, "endOffset": 153}, {"referenceID": 8, "context": "The proposed CSL approach can be viewed as a generalization of the contemporary logistic regression model [10].", "startOffset": 106, "endOffset": 110}, {"referenceID": 8, "context": "Note that the regularization term \u03bb2 \u2211J j=1 \u03b8 2 j serves to penalize large values of the model parameters \u03b8j , thereby reducing the risk of data overfitting [10].", "startOffset": 157, "endOffset": 161}, {"referenceID": 30, "context": ", [32], [33], or rely only on latent features, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 31, "context": ", [32], [33], or rely only on latent features, e.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": ", [7], [13], [26], our MRF formulation is more generic and makes less stringent assumption.", "startOffset": 2, "endOffset": 5}, {"referenceID": 11, "context": ", [7], [13], [26], our MRF formulation is more generic and makes less stringent assumption.", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": ", [7], [13], [26], our MRF formulation is more generic and makes less stringent assumption.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "It is also worth noting that our MRF formulation is different from that of conditional random field (CRF) [30].", "startOffset": 106, "endOffset": 110}, {"referenceID": 1, "context": "whereZ is a normalizing constant, \u03b2 is a (nonnegative) userspecified regularization parameter, and Df (\u03bc||\u03c1) is the f divergence [2] between two distributions \u03bc and \u03c1:", "startOffset": 129, "endOffset": 132}, {"referenceID": 14, "context": "Fortunately, we can use the Jensen inequality [16] in order to derive a convex upper bound of (17).", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "The formulation in (19) is related to the XR approach [23], with some key differences.", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "Our formulation is also conceptually superior to that of the SMIR method [27], whose convexity is not guaranteed when the L2 regularization parameter (\u03bb in our notation) is not sufficiently large [27].", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": "Our formulation is also conceptually superior to that of the SMIR method [27], whose convexity is not guaranteed when the L2 regularization parameter (\u03bb in our notation) is not sufficiently large [27].", "startOffset": 196, "endOffset": 200}, {"referenceID": 0, "context": "It is clear that the curvature (23) will always be positive for any positive \u03bb (since \u03c3i \u2208 [0, 1]).", "startOffset": 91, "endOffset": 97}, {"referenceID": 20, "context": "In this work, we use the limited memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (L-BFGS) algorithm [22], a popular, efficient Quasi-Newton method for solving unconstrained optimization problems2.", "startOffset": 92, "endOffset": 96}, {"referenceID": 20, "context": "Notably, the memory/time complexity of the L-BFGS algorithm is linear in the problem size [22], and the convexity of our CSL formulation makes it possible to reach the optimum within a few iterations.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "We used the list of English stop-words in [19].", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "The first baseline is bootstrapping [12], where we first train a logistic regression using labeled data, apply it to predict on unlabeled data, and then add into the labeled dataset those samples that have high prediction scores.", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Additionally, we compare our CSL approach with two state-of-the-art information theoretic SSL methods, namely entropy regularization (ER) [11] and expectation regularization (XR) [23].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "Additionally, we compare our CSL approach with two state-of-the-art information theoretic SSL methods, namely entropy regularization (ER) [11] and expectation regularization (XR) [23].", "startOffset": 179, "endOffset": 183}, {"referenceID": 32, "context": "To evaluate whether the performance difference between two methods is statistically significant, we perform the Wilcoxon signed-rank test [34] with a critical value of 0.", "startOffset": 138, "endOffset": 142}], "year": 2016, "abstractText": "The abundance of user-generated data in social media has incentivized the development of methods to infer the latent attributes of users, which are crucially useful for personalization, advertising and recommendation. However, the current user profiling approaches have limited success, due to the lack of a principled way to integrate different types of social relationships of a user, and the reliance on scarcely-available labeled data in building a prediction model. In this paper, we present a novel solution termed Collective Semi-Supervised Learning (CSL), which provides a principled means to integrate different types of social relationship and unlabeled data under a unified computational framework. The joint learning from multiple relationships and unlabeled data yields a computationally sound and accurate approach to model user attributes in social media. Extensive experiments using Twitter data have demonstrated the efficacy of our CSL approach in inferring user attributes such as account type and marital status. We also show how CSL can be used to determine important user features, and to make inference on a larger user population.", "creator": "LaTeX with hyperref package"}}}