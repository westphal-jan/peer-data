{"id": "1606.08003", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2016", "title": "Functional Distributional Semantics", "abstract": "vector space models have become popular in distributional semantics, despite the challenges they face in capturing various semantic phenomena. we propose a novel experimental probabilistic logical framework \u00ab which draws also on both formal semantics and recent rapid advances in sophisticated machine supervised learning. certainly in another particular, whilst we separate predicates solely from the entities they cannot refer information to, not allowing primitive us methods to truly perform bayesian inference steps based on logical forms. we describe an evolved implementation of incorporating this statistical framework therefore using virtually a combination suite of equally restricted boltzmann machines and feedforward neural networks. \u2022 finally, thereby we demonstrate how the feasibility verification of this conceptual approach alone by training it visually on a custom parsed syntax corpus and evaluating it on firmly established similarity between datasets.", "histories": [["v1", "Sun, 26 Jun 2016 07:44:08 GMT  (96kb,D)", "http://arxiv.org/abs/1606.08003v1", "Published at Representation Learning for NLP workshop at ACL 2016,this https URL"]], "COMMENTS": "Published at Representation Learning for NLP workshop at ACL 2016,this https URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guy emerson", "ann copestake"], "accepted": false, "id": "1606.08003"}, "pdf": {"name": "1606.08003.pdf", "metadata": {"source": "CRF", "title": "Functional Distributional Semantics", "authors": ["Guy Emerson"], "emails": ["gete2@cam.ac.uk", "aac10@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Current approaches to distributional semantics generally involve representing words as points in a high-dimensional vector space. However, vectors do not provide \u2018natural\u2019 composition operations that have clear analogues with operations in formal semantics, which makes it challenging to perform inference, or capture various aspects of meaning studied by semanticists. This is true whether the vectors are constructed using a count approach (e.g. Turney and Pantel, 2010) or an embedding approach (e.g. Mikolov et al., 2013), and indeed Levy and Goldberg (2014b) showed that there are close links between them. Even the tensorial approach described by Coecke et al. (2010) and Baroni et al. (2014), which naturally captures argument structure, does not allow an obvious account of context dependence, or logical inference.\nIn this paper, we build on insights drawn from formal semantics, and seek to learn representa-\ntions which have a more natural logical structure, and which can be more easily integrated with other sources of information.\nOur contributions in this paper are to introduce a novel framework for distributional semantics, and to describe an implementation and training regime in this framework. We present some initial results to demonstrate that training this model is feasible."}, {"heading": "2 Formal Framework of Functional Distributional Semantics", "text": "In this section, we describe our framework, explaining the connections to formal semantics, and defining our probabilistic model. We first motivate representing predicates with functions, and then explain how these functions can be incorporated into a representation for a full utterance."}, {"heading": "2.1 Semantic Functions", "text": "We begin by assuming an extensional model structure, as standard in formal semantics (Kamp and Reyle, 1993; Cann, 1993; Allan, 2001). In the simplest case, a model contains a set of entities, which predicates can be true or false of. Models can be endowed with additional structure, such as for plurals (Link, 2002), although we will not discuss such details here. For now, the important point is that we should separate the representation of a predicate from the representations of the entities it is true of.\nWe generalise this formalisation of predicates by treating truth values as random variables,1\n1The move to replace absolute truth values with probabilities has parallels in much computational work based on formal logic. For example, Garrette et al. (2011) incorporate distributional information in a Markov Logic Network (Richardson and Domingos, 2006). However, while their approach allows probabilistic inference, they rely on existing distributional vectors, and convert similarity scores to weighted logical formulae. Instead, we aim to learn representations which are directly interpretable within in a probabilistic logic.\nar X\niv :1\n60 6.\n08 00\n3v 1\n[ cs\n.C L\n] 2\n6 Ju\nn 20\n16\nwhich enables us to apply Bayesian inference. For any entity, we can ask which predicates are true of it (or \u2018applicable\u2019 to it). More formally, if we take entities to lie in some semantic spaceX (whose dimensions may denote different features), then we can take the meaning of a predicate to be a function from X to values in the interval [0, 1], denoting how likely a speaker is to judge the predicate applicable to the entity. This judgement is variable between speakers (Labov, 1973), and for borderline cases, it is even variable for one speaker at different times (McCloskey and Glucksberg, 1978).\nRepresenting predicates as functions allows us to naturally capture vagueness (a predicate can be equally applicable to multiple points), and using values between 0 and 1 allows us to naturally capture gradedness (a predicate can be more applicable to some points than to others). To use Labov\u2019s example, the predicate for cup is equally applicable to vessels of different shapes and materials, but becomes steadily less applicable to wider vessels.\nWe can also view such a function as a classifier \u2013 for example, the semantic function for the predicate for cat would be a classifier separating cats from non-cats. This ties in with a view of concepts as abilities, as proposed in both philosophy (Dummett, 1978; Kenny, 2010), and cognitive science (Murphy, 2002; Bennett and Hacker, 2008). A similar approach is taken by Larsson (2013), who argues in favour of representing perceptual concepts as classifiers of perceptual input.\nNote that these functions do not directly define probability distributions over entities. Rather, they define binary-valued conditional distribu-\ntions, given an entity. We can write this as P (t|x), where x is an entity, and t is a stochastic truth value. It is only possible to get a corresponding distribution over entities given a truth value, P (x|t), if we have some background distribution P (x). If we do, we can apply Bayes\u2019 Rule to get P (x|t) \u221d P (t|x)P (x). In other words, the truth of an expression depends crucially on our knowledge of the situation. This fits neatly within a verificationist view of truth, as proposed by Dummett (1976), who argues that to understand a sentence is to know how we could verify or falsify it.\nBy using bothP (t|x) and P (x|t), we can distinguish between underspecification and uncertainty as two kinds of \u2018vagueness\u2019. In the first case, we want to state partial information about an entity, but leave other features unspecified; P (t|x) represents which kinds of entity could be described by the predicate, regardless of how likely we think the entities are. In the second case, we have uncertain knowledge about the entity; P (x|t) represents which kinds of entity we think are likely for this predicate, given all our world knowledge.\nFor example, bell peppers come in many colours, most typically green, yellow, orange or red. As all these colours are typical, the semantic function for the predicate for bell pepper would take a high value for each. In contrast, to define a probability distribution over entities, we must split probability mass between different colours,2\n2In fact, colour would be most properly treated as a continuous feature. In this case, P (x) must be a probability density function, not a probability mass function, whose value would further depend on the parametrisation of the space.\nand for a large number of colours, we would only have a small probability for each. As purple and blue are atypical colours for a pepper, a speaker might be less willing to label such a vegetable a pepper, but not completely unwilling to do so \u2013 this linguistic knowledge belongs to the semantic function for the predicate. In contrast, after observing a large number of peppers, we might conclude that blue peppers do not exist, purple peppers are rare, green peppers common, and red peppers more common still \u2013 this world knowledge belongs to the probability distribution over entities. The contrast between these two quantities is depicted in figure 1, for a simple discrete space."}, {"heading": "2.2 Incorporation with Dependency Minimal Recursion Semantics", "text": "Semantic dependency graphs have become popular in NLP. We use Dependency Minimal Recursion Semantics (DMRS) (Copestake et al., 2005; Copestake, 2009), which represents meaning as a directed acyclic graph: nodes represent predicates/entities (relying on a one-to-one correspondence between them) and links (edges) represent argument structure and scopal constraints. Note that we assume a neo-Davidsonian approach (Davidson, 1967; Parsons, 1990), where events are also treated as entities, which allows a better account of adverbials, among other phenomena.\nFor example (simplifying a little), to represent \u201cthe dog barked\u201d, we have three nodes, for the predicates the, dog, and bark, and two links: an ARG1 link from bark to dog, and a RSTR link from the to dog. Unlike syntactic dependencies, DMRS abstracts over semantically equivalent expressions, such as \u201cdogs chase cats\u201d and \u201ccats are chased by dogs\u201d. Furthermore, unlike other types of semantic dependencies, including Abstract Meaning Representations (Banarescu et al., 2012), and Prague Dependencies (Bo\u0308hmova\u0301 et al., 2003), DMRS is interconvertible with MRS, which can be given a direct logical interpretation.\nWe deal here with the extensional fragment of language, and while we can account for different quantifiers in our framework, we do not have space to discuss this here \u2013 for the rest of this paper, we neglect quantifiers, and the reader may assume that all variables are existentially quantified.\nWe can use the structure of a DMRS graph to define a probabilistic graphical model. This gives us a distribution over lexicalisations of the graph \u2013\ngiven an abstract graph structure, where links are labelled but nodes are not, we have a process to generate a predicate for each node. Although this process is different for each graph structure, we can share parameters between them (e.g. according to the labels on links). Furthermore, if we have a distribution over graph structures, we can incorporate that in our generative process, to produce a distribution over lexicalised graphs.\nThe entity nodes can be viewed as together representing a situation, in the sense of Barwise and Perry (1983). We want to be able to represent the entities without reference to the predicates \u2013 intuitively, the world is the same however we choose to describe it. To avoid postulating causal structure amongst the entities (which would be difficult for a large graph), we can model the entity nodes as an undirected graphical model, with edges according to the DMRS links. The edges are undirected in the sense that they don\u2019t impose conditional dependencies. However, this is still compatible with having \u2018directed\u2019 semantic dependencies \u2013 the probability distributions are not symmetric, which maintains the asymmetry of DMRS links.\nEach node takes values in the semantic spaceX , and the network defines a joint distribution over entities, which represents our knowledge about which situations are likely or unlikely. An example is shown in the top row of figure 2, of an entity y along with its two arguments x and z \u2013 these might represent an event, along with the agent and patient involved in the event. The structure of the graph means that we can factorise the joint distribution P (x, y, z) over the entities as being proportional to the product P (x, y)P (y, z).\nFor any entity, we can ask which predicates are true of it. We can therefore introduce a\nnode for every predicate in the vocabulary, where the value of the node is either true (>) or false (\u22a5). Each of these predicate nodes has a single directed link from the entity node, with the probability of the node being true being determined by the predicate\u2019s semantic function, i.e. P (tc, x = >|x) = tc(x). This is shown in the second row of figure 2, where the plate denotes that these nodes are repeated for each predicate c in the vocabulary V . For example, if the situation represented a dog chasing a cat, then nodes like tdog, x, tanimal, x, and tpursue, y would be true (with high probability), while tdemocracy, x or tdog, z would be false (with high probability).\nThe probabilistic model described above closely matches traditional model-theoretic semantics. However, while we could stop our semantic description there, we do not generally observe truth-value judgements for all predicates at once;3 rather, we observe utterances, which have specific predicates. We can therefore define a final node for each entity, which takes values over predicates in the vocabulary, and which is conditionally dependent on the truth values of all predicates. This is shown in the bottom row of figure 3. Including these final nodes means that we can train such a model on observed utterances. The process of choosing a predicate from the true ones may be complex, potentially depending on speaker intention and other pragmatic factors \u2013 but in section 3, we will simply choose a true predicate at random (weighted by frequency).\n3This corresponds to what Copestake and Herbelot (2012) call an ideal distribution. If we have access to such information, we only need the two rows given in figure 2.\nThe separation of entities and predicates allows us to naturally capture context-dependent meanings. Following the terminology of Quine (1960), we can distinguish context-independent standing meaning from context-dependent occasion meaning. Each predicate type has a corresponding semantic function \u2013 this represents its standing meaning. Meanwhile, each predicate token has a corresponding entity, for which there is a posterior distribution over the semantic space, conditioning on the rest of the graph and any pragmatic factors \u2013 this represents its occasion meaning.\nUnlike previous approaches to context dependence, such as Dinu et al. (2012), Erk and Pado\u0301 (2008), and Thater et al. (2011), we represent meanings in and out of context by different kinds of object, reflecting a type/token distinction. Even Herbelot (2015), who explicitly contrasts individuals and kinds, embeds both in the same space.\nAs an example of how this separation of predicates and entities can be helpful, suppose we would like \u201cdogs chase cats\u201d and \u201ccats chase mice\u201d to be true in a model, but \u201cdogs chase mice\u201d and \u201ccats chase cats\u201d to be false. In other words, there is a dependence between the verb\u2019s arguments. If we represent each predicate by a single vector, it is not clear how to capture this. However, by separating predicates from entities, we can have two different entities which chase is true of, where one co-occurs with a dog-entity ARG1 and catentity ARG2, while the other co-occurs with a catentity ARG1 and a mouse-entity ARG2."}, {"heading": "3 Implementation", "text": "In the previous section, we described a general framework for probabilistic semantics. Here we give details of one way that such a framework can be implemented for distributional semantics, keeping the architecture as simple as possible."}, {"heading": "3.1 Network Architecture", "text": "We take the semantic spaceX to be a set of binaryvalued vectors,4 {0, 1}N . A situation s is then composed of entity vectors x(1), \u00b7 \u00b7 \u00b7 , x(K) \u2208 X (where the number of entities K may vary), along with links between the entities. We denote a link from x(n) to x(m) with label l as: x(n) l\u2212\u2192 x(m). We define the background distribution over situations using a Restricted Boltzmann Machine\n4We use the term vector in the computer science sense of a linear array, rather than in the mathematical sense of a point in a vector space.\n(RBM) (Smolensky, 1986; Hinton et al., 2006), but rather than having connections between hidden and visible units, we have connections between components of entities, according to the links.\nThe probability of the network being in the particular configuration s depends on the energy of the configuration, Eb(s), as shown in equations (1)-(2). A high energy denotes an unlikely configuration. The energy depends on the edges of the graphical model, plus bias terms, as shown in (3). Note that we follow the Einstein summation convention, where repeated indices indicate summation; although this notation is not typical in NLP, we find it much clearer than matrixvector notation, particularly for higher-order tensors. Each link label l has a corresponding weight matrix W (l), which determines the strength of association between components of the linked entities. The first term in (3) sums these contributions over all links x(n) l\u2212\u2192 x(m) between entities. We also introduce bias terms, to control how likely an entity vector is, independent of links. The second term in (3) sums the biases over all entities x(n).\nP (s) = 1\nZ exp\n( \u2212Eb(s) ) (1)\nZ = \u2211 s\u2032 exp ( \u2212Eb(s\u2032) ) (2)\n\u2212Eb(s) = \u2211\nx(n) l\u2212\u2192x(m)\nW (l) ij x (n) i x (m) j \u2212 \u2211 x(n) bix (n) i (3)\nFurthermore, since sparse representations have been shown to be beneficial in NLP, both for applications and for interpretability of features (Murphy et al., 2012; Faruqui et al., 2015), we can enforce sparsity in these entity vectors by fixing a specific number of units to be active at any time. Swersky et al. (2012) introduce this RBM variant as the Cardinality RBM, and also give an efficient exact sampling procedure using belief propagation. Since we are using sparse representations, we also assume that all link weights are non-negative.\nNow that we\u2019ve defined the background distribution over situations, we turn to the semantic functions tc, which map entities x to probabilities. We implement these as feedforward networks, as shown in (4)-(5). For simplicity, we do not introduce any hidden layers. Each predicate c has a vector of weights W \u2032(c), which determines the strength of association with each dimension of the semantic space, as well as a bias term b\u2032(c). These\ntogether define the energy Ep(x, c) of an entity x with the predicate, which is passed through a sigmoid function to give a value in the range [0, 1].\ntc(x) = \u03c3(\u2212Ep(x, c)) = 1\n1 + exp (Ep) (4)\n\u2212Ep(x, c) =W \u2032(c)i xi \u2212 b \u2032(c) (5)\nGiven the semantic functions, choosing a predicate for a entity can be hard-coded, for simplicity. The probability of choosing a predicate c for an entity x is weighted by the predicate\u2019s frequency fc and the value of its semantic function tc(x) (how true the predicate is of the entity), as shown in (6)-(7). This is a mean field approximation to the stochastic truth values shown in figure 3.\nP (c|x) = 1 Zx fctc(x) (6)\nZx = \u2211 c\u2032 fc\u2032tc\u2032(x) (7)"}, {"heading": "3.2 Learning Algorithm", "text": "To train this model, we aim to maximise the likelihood of observing the training data \u2013 in Bayesian terminology, this is maximum a posteriori estimation. As described in section 2.2, each data point is a lexicalised DMRS graph, while our model defines distributions over lexicalisations of graphs. In other words, we take as given the observed distribution over abstract graph structures (where links are given, but nodes are unlabelled), and try to optimise how the model generates predicates (via the parameters W (l)ij , bi,W \u2032(c) i , b\n\u2032(c)). For the family of optimisation algorithms based on gradient descent, we need to know the gradient of the likelihood with respect to the model parameters, which is given in (8), where x \u2208 X is a latent entity, and c \u2208 V is an observed predicate (corresponding to the top and bottom rows of figure 3). Note that we extend the definition of energy from situations to entities in the obvious way: half the energy of an entity\u2019s links, plus its bias energy. A full derivation of (8) is given in the appendix.\n\u2202\n\u2202\u03b8 logP (c) = Ex|c\n[ \u2202\n\u2202\u03b8\n( \u2212Eb(x) )] \u2212 Ex [ \u2202\n\u2202\u03b8\n( \u2212Eb(x) )] + Ex|c [ (1\u2212 tc(x)) \u2202\n\u2202\u03b8 (\u2212Ep(x, c)) ] \u2212 Ex|c [ Ec\u2032|x [ (1\u2212 tc\u2032(x)) \u2202\n\u2202\u03b8\n( \u2212Ep(x, c\u2032)\n)]] (8)\nThere are four terms in this gradient: the first two are for the background distribution, and the last two are for the semantic functions. In both cases, one term is positive, and conditioned on the data, while the other term is negative, and represents the predictions of the model.\nCalculating the expectations exactly is infeasible, as this requires summing over all possible configurations. Instead, we can use a Markov Chain Monte Carlo method, as typically done for Latent Dirichlet Allocation (Blei et al., 2003; Griffiths and Steyvers, 2004). Our aim is to sample values of x and c, and use these samples to approximate the expectations: rather than summing over all values, we just consider the samples. For each token in the training data, we introduce a latent entity vector, which we use to approximate the first, third, and fourth terms in (8). Additionally, we introduce a latent predicate for each latent entity, which we use to approximate the fourth term \u2013 this latent predicate is analogous to the negative samples used by Mikolov et al. (2013).\nWhen resampling a latent entity conditioned on the data, the conditional distribution P (x|c) is unknown, and calculating it directly requires summing over the whole semantic space. For this reason, we cannot apply Gibbs sampling (as used in LDA), which relies on knowing the conditional distribution. However, if we compare two entities x and x\u2032, the normalisation constant cancels out in the ratio P (x\u2032|c)/P (x|c), so we can use the Metropolis-Hastings algorithm (Metropolis et al., 1953; Hastings, 1970). Given the current sample x, we can uniformly choose one unit to switch on, and one to switch off, to get a proposal x\u2032. If the ratio of probabilities shown in (9) is above 1, we switch the sample to x\u2032; if it\u2019s below 1, it is the probability of switching to x\u2032.\nP (x\u2032|c) P (x|c) = exp\n( \u2212Eb(x\u2032) ) 1 Zx\u2032 tc(x \u2032)\nexp (\u2212Eb(x)) 1Zx tc(x) (9)\nAlthough Metropolis-Hastings avoids the need to calculate the normalisation constant Z of the background distribution, we still have the normalisation constant Zx of choosing a predicate given an entity. This constant represents the number of predicates true of the entity (weighted by frequency). The intuitive explanation is that we should sample an entity which few predicates are true of, rather than an entity which many predicates are true of. We approximate this constant\nby assuming that we have an independent contribution from each dimension of x. We first average over all predicates (weighted by frequency), to get the average predicate W avg. We then take the exponential of W avg for the dimensions that we are proposing switching off and on \u2013 intuitively, if many predicates have a large weight for a given dimension, then many predicates will be true of an entity where that dimension is active. This is shown in (10), where x and x\u2032 differ in dimensions i and i\u2032 only, and where k is a constant.\nZx Zx\u2032 \u2248 exp\n( k ( W avgi \u2212W avg i\u2032 ))\n(10)\nWe must also resample latent predicates given a latent entity, for the fourth term in (8). This can similarly be done using the Metropolis-Hastings algorithm, according to the ratio shown in (11).\nP (c\u2032|x) P (c|x) = fc\u2032tc\u2032(x) fctc(x) (11)\nFinally, we need to resample entities from the background distribution, for the second term in (8). Rather than recalculating the samples from scratch after each weight update, we used fantasy particles (persistent Markov chains), which Tieleman (2008) found effective for training RBMs. Resampling a particle can be done more straightforwardly than resampling the latent entities \u2013 we can sample each entity conditioned on the other entities in the situation, which can be done exactly using belief propagation (see Yedidia et al. (2003) and references therein), as Swersky et al. (2012) applied to the Cardinality RBM.\nTo make weight updates from the gradients, we used AdaGrad (Duchi et al., 2011), with exponential decay of the sum of squared gradients. We also used L1 and L2 regularisation, which determines our prior over model parameters.\nWe found that using a random initialisation is possible, but seems to lead to a long training time, due to slow convergence. We suspect that this could be because the co-occurrence of predicates is mediated via at least two latent vectors, which leads to mixing of semantic classes in each dimension, particularly in the early stages of training. Such behaviour can happen with complicated topic models \u2013 for example, O\u0301 Se\u0301aghdha (2010) found this for their \u201cDual Topic\u201d model. One method to reduce convergence time is to initialise predicate parameters using pre-trained vectors. The link parameters can then be initialised\nas follows: we consider a situation with just one entity, and for each predicate, we find the meanfield entity vector given the pre-trained predicate parameters; we then fix all entity vectors in our training corpus to be these mean-field vectors, and find the positive pointwise mutual information of each each pair of entity dimensions, for each link label. In particular, we initialised predicate parameters using our sparse SVO Word2Vec vectors, which we describe in section 4.2."}, {"heading": "4 Training and Initial Experiments", "text": "In this section, we report the first experiments carried out within our framework."}, {"heading": "4.1 Training Data", "text": "Training our model requires a corpus of DMRS graphs. In particular, we used WikiWoods, an automatically parsed version of the July 2008 dump of the full English Wikipedia, distributed by DELPH-IN5. This resource was produced by Flickinger et al. (2010), using the English Resource Grammar (ERG; Flickinger, 2000), trained on the manually treebanked subcorpus WeScience (Ytrest\u00f8l et al., 2009), and implemented with the PET parser (Callmeier, 2001; Toutanova et al., 2005). To preprocess the corpus, we used the python packages pydelphin6 (developed by Michael Goodman), and pydmrs7 (Copestake et al., 2016).\nFor simplicity, we restricted attention to subject-verb-object (SVO) triples, although we should stress that this is not an inherent limitation of our model, which could be applied to arbitrary graphs. We searched for all verbs in the WikiWoods treebank, excluding modals, that had either an ARG1 or an ARG2, or both. We kept all instances whose arguments were nominal, excluding pronouns and proper nouns. The ERG does not automatically convert out-of-vocabulary items from their surface form to lemmatised predicates, so we applied WordNet\u2019s morphological processor Morphy (Fellbaum, 1998), as available in NLTK (Bird et al., 2009). Finally, we filtered out situations including rare predicates, so that every predicate appears at least five times in the dataset.\nAs a result of this process, all data was of the form (verb, ARG1, ARG2), where one (but not\n5http://moin.delph-in.net/WikiWoods 6https://github.com/delph-in/pydelphin 7https://github.com/delph-in/pydmrs\nboth) of the arguments may be missing. A summary is given in table 1. In total, the dataset contains 72m tokens, with 88,526 distinct predicates."}, {"heading": "4.2 Evaluation", "text": "As our first attempt at evaluation, we chose to look at two lexical similarity datasets. The aim of this evaluation was simply to verify that the model was learning something reasonable. We did not expect this task to illustrate our model\u2019s strengths, since we need richer tasks to exploit its full expressiveness. Both of our chosen datasets aim to evaluate similarity, rather than thematic relatedness: the first is Hill et al. (2015)\u2019s SimLex-999 dataset, and the second is Finkelstein et al. (2001)\u2019s WordSim353 dataset, which was split by Agirre et al. (2009) into similarity and relatedness subsets. So far, we have not tuned hyperparameters.\nResults are given in table 2. We also trained Mikolov et al. (2013)\u2019s Word2Vec model on the SVO data described in section 4.1, in order to give a direct comparison of models on the same training data. In particular, we used the continuous bag-of-words model with negative sampling, as implemented in R\u030cehu\u030ar\u030cek and Sojka (2010)\u2019s gensim package, with off-the-shelf hyperparameter settings. We also converted these to sparse vectors using Faruqui et al. (2015)\u2019s algorithm, again using off-the-shelf hyperparameter settings. To measure similarity of our semantic functions, we treated each function\u2019s parameters as a vector and used cosine similarity, for simplicity.\nFor comparison, we also include the performance of Word2Vec when trained on raw text. For SimLex-999, we give the results reported by Hill et al. (2015), where the 2-word window model was the best performing model that they tested. For WordSim-353, we trained a model on the full WikiWoods text, after stripping all punctuation and converting to lowercase. We used the gensim implementation with off-the-shelf settings, except for window size (2 or 10) and dimension (200, as recommended by Hill et al.). In fact, our re-trained model performed better on SimLex-999 than Hill\net al. reported (even when we used less preprocessing or a different edition of Wikipedia), although still worse than our sparse SVO Word2Vec model.\nIt is interesting to note that training Word2Vec on verbs and their arguments gives noticeably better results on SimLex-999 than training on full sentences, even though far less data is being used: \u223c72m tokens, rather than \u223c1000m. The better performance suggests that semantic dependencies may provide more informative contexts than simple word windows. This is in line with previous results, such as Levy and Goldberg (2014a)\u2019s work on using syntactic dependencies. Nonetheless, this result deserves further investigation.\nOf all the models we tested, only our semantic function model failed on the relatedness subset of WordSim-353. We take this as a positive result, since it means the model clearly distinguishes relatedness and similarity.\nExamples of thematically related predicates and various kinds of co-hyponym are given in table 3, along with our model\u2019s similarity scores. However, it is not clear that it is possible, or even desirable, to represent these varied relationships on a single scale of similarity. For example, it could be sensible to treat aunt and uncle either as synonyms (they refer to relatives of the same degree of relatedness) or as antonyms (they are \u201copposite\u201d in some sense). Which view is more appropriate will depend on the application, or on the context.\nNouns and verbs are very strongly distinguished, which we would expect given the structure of our model. This can be seen in the similarity scores between flood and water, when flood is considered either as a verb or as a noun.8 SimLex-999 generally assigns low scores to nearantonyms, and to pairs differing in a single feature, which might explain why the performance of our model is not higher on this task. However, the separation of thematically related predicates from co-hyponyms is a promising result."}, {"heading": "5 Related Work", "text": "As mentioned above, Coecke et al. (2010) and Baroni et al. (2014) introduce a tensor-based framework that incorporates argument structure through tensor contraction. However, for logical inference, we need to know how one vector can entail another. Grefenstette (2013) explores one method to do this; however, they do not show that this approach is learnable from distributional information, and furthermore, they prove that quantifiers cannot be expressed with tensors.\nBalk\u0131r (2014), working in the tensorial framework, uses the quantum mechanical notion of a \u201cmixed state\u201d to model uncertainty. However, this doubles the number of tensor indices, so squares the number of dimensions (e.g. vectors become matrices). In the original framework, expressions with several arguments already have a high dimensionality (e.g. whose is represented by a fifth-order tensor), and this problem becomes worse.\nVilnis and McCallum (2015) embed predicates as Gaussian distributions over vectors. By assuming covariances are diagonal, this only doubles the number of dimensions (N dimensions for the mean, and N for the covariances). However, similarly to Mikolov et al. (2013), they simply assume\n8We considered the ERG predicates flood v cause and flood n of, which were the most frequent predicates in WikiWoods for flood, for each part of speech.\nthat nearby words have similar meanings, so the model does not naturally capture compositionality or argument structure.\nIn both Balk\u0131r\u2019s and Vilnis and McCallum\u2019s models, they use the probability of a vector given a word \u2013 in the notation from section 2.1, P (x|t). However, the opposite conditional probability, P (t|x), more easily allows composition. For instance, if we know two predicates are true (t1 and t2), we cannot easily combine P (x|t1) and P (x|t2) to get P (x|t1, t2) \u2013 intuitively, we\u2019re generating x twice. In contrast, for semantic functions, we can writeP (t1, t2|x) = P (t1|x)P (t2|x).\nGa\u0308rdenfors (2004) argues concepts should be modelled as convex subsets of a semantic space. Erk (2009) builds on this idea, but their model requires pre-trained count vectors, while we learn our representations directly. McMahan and Stone (2015) also learn representations directly, considering colour terms, which are grounded in a wellunderstood perceptual space. Instead of considering a single subset, they use a probability distribution over subsets: P (A|t) forA \u2282 X . This is more general than a semantic function P (t|x), since we can write P (t|x) = \u2211 A3v P (A|t). However, this framework may be too general, since it means we cannot determine the truth of a predicate until we know the entire set A. To avoid this issue, they factorise the distribution, by assuming different boundaries of the set are independent. However, this is equivalent to considering P (t|x) directly, along with some constraints on this function. Indeed, for the experiments they describe, it is sufficient to know a semantic function P (t|x). Furthermore, McMahan and Stone find expressions like greenish which are nonconvex in perceptual space, which suggests that representing concepts with convex sets may not be the right way to go.\nOur semantic functions are similar to Cooper et al. (2015)\u2019s probabilistic type judgements, which they introduce within the framework of Type Theory with Records (Cooper, 2005), a rich semantic theory. However, one difference between our models is that they represent situations in terms of situation types, while we are careful to define our semantic space without reference to any predicates. More practically, although they outline how their model might be learned, they assume we have access to type judgements for observed situations. In contrast, we describe how a model can be learned from observed utterances, which was\nnecessary for us to train a model on a corpus. Goodman and Lassiter (2014) propose another linguistically motivated probabilistic model, using the stochastic \u03bb-calculus (more concretely, probabilistic programs written in Church). However, they rely on relatively complex generative processes, specific to individual semantic domains, where each word\u2019s meaning may be represented by a complex expression. For a wide-scale system, such structures would need to be extended to cover all concepts. In contrast, our model assumes a direct mapping between predicates and semantic functions, with a relatively simple generative structure determined by semantic dependencies.\nFinally, our approach should be distinguished from work which takes pre-trained distributional vectors, and uses them within a richer semantic model. For example, Herbelot and Vecchi (2015) construct a mapping from a distributional vector to judgements of which quantifier is most appropriate for a range of properties. Erk (2016) uses distributional similarity to probabilistically infer properties of one concept, given properties of another. Beltagy et al. (2016) use distributional similarity to produce weighted inference rules, which they incorporate in a Markov Logic Network. Unlike these authors, we aim to directly learn interpretable representations, rather than interpret given representations."}, {"heading": "6 Conclusion", "text": "We have introduced a novel framework for distributional semantics, where each predicate is represented as a function, expressing how applicable the predicate is to different entities. We have shown how this approach can capture semantic phenomena which are challenging for standard vector space models. We have explained how our framework can be implemented, and trained on a corpus of DMRS graphs. Finally, our initial evaluation on similarity datasets demonstrates the feasibility of this approach, and shows that thematically related words are not given similar representations. In future work, we plan to use richer tasks which exploit the model\u2019s expressiveness."}, {"heading": "Acknowledgments", "text": "This work was funded by a Schiff Foundation Studentship. We would also like to thank Yarin Gal, who gave useful feedback on the specification of our generative model."}, {"heading": "Appendix: Derivation of Gradients", "text": "In this section, we derive equation (8). As our model generates predicates from entities, to find the probability of observing the predicates, we need to sum over all possible entities. After then applying the chain rule to log, and expanding P (x, c), we obtain the expression below.\n\u2202\n\u2202\u03b8 logP (c) =\n\u2202 \u2202\u03b8 log \u2211 x P (x, c)\n= \u2202 \u2202\u03b8 \u2211 x P (x, c)\u2211\nx\u2032 P (x \u2032, c)\n= \u2202 \u2202\u03b8\n\u2211 x 1 Zx fctc(x) 1 Z exp ( \u2212Eb(x) )\u2211 x\u2032 P (x \u2032, c)\nWhen we now apply the product rule, we will get four terms, but we can make use of the fact that the derivatives of all four terms are multiples of the original term:\n\u2202\n\u2202\u03b8 e\u2212E\nb(x) = e\u2212E b(x) \u2202\n\u2202\u03b8\n( \u2212Eb(x) ) \u2202\n\u2202\u03b8 tc(x) = tc(x) (1\u2212 tc(x))\n\u2202\n\u2202\u03b8 (\u2212Ep(x, c))\n\u2202\n\u2202\u03b8\n1 Zx = \u22121 Z2x \u2202 \u2202\u03b8 Zx\n\u2202\n\u2202\u03b8\n1 Z = \u22121 Z2 \u2202 \u2202\u03b8 Z\nThis allows us to derive:\n= \u2211 x P (x, c)\u2211 x\u2032 P (x \u2032, c) [ \u2202 \u2202\u03b8 ( \u2212Eb(x) ) + (1\u2212 tc(x)) \u2202\n\u2202\u03b8 (\u2212Ep(x, c))\n\u2212 1 Zx \u2202 \u2202\u03b8 Zx ] \u2212 \u2211\nx P (x, c)\u2211 x\u2032 P (x \u2032, c) 1 Z \u2202 \u2202\u03b8 Z\nWe can now simplify using conditional probabilities, and expand the derivatives of the normalisation constants:\n= \u2211 x P (x|c) [ \u2202 \u2202\u03b8 ( \u2212Eb(x) ) + (1\u2212 tc(x)) \u2202\n\u2202\u03b8 (\u2212Ep(x, c))\n\u2212 1 Zx \u2202 \u2202\u03b8 \u2211 c\u2032 fc\u2032tc\u2032(x)\n]\n\u2212 1 Z \u2202 \u2202\u03b8 \u2211 x exp ( \u2212Eb(x) )\n= \u2211 x P (x|c) [ \u2202 \u2202\u03b8 ( \u2212Eb(x) ) + (1\u2212 tc(x)) \u2202\n\u2202\u03b8 (\u2212Ep(x, c))\n\u2212 \u2211 c\u2032 fc\u2032tc\u2032(x) Zx (1\u2212 tc\u2032(x)) \u2202 \u2202\u03b8 ( \u2212Ep(x, c\u2032)\n)]\n\u2212 \u2211 x\nexp ( \u2212Eb(x) ) Z \u2202 \u2202\u03b8 ( \u2212Eb(x) )\n= \u2211 x P (x|c) [ \u2202 \u2202\u03b8 ( \u2212Eb(x) ) + (1\u2212 tc(x)) \u2202\n\u2202\u03b8 (\u2212Ep(x, c))\n\u2212 \u2211 c\u2032 P (c\u2032|x) (1\u2212 tc\u2032(x)) \u2202 \u2202\u03b8 ( \u2212Ep(x, c\u2032)\n)]\n\u2212 \u2211 x P (x) \u2202 \u2202\u03b8 ( \u2212Eb(x) )\nFinally, we write expectations instead of sums of probabilities:\n=Ex|c [ \u2202\n\u2202\u03b8\n( \u2212Eb(x) ) + (1\u2212 tc(x)) \u2202\n\u2202\u03b8 (\u2212Ep(x, c)) \u2212 Ec\u2032|x [ (1\u2212 tc\u2032(x)) \u2202\n\u2202\u03b8\n( \u2212Ep(x, c\u2032) )]] \u2212 Ex [ \u2202\n\u2202\u03b8\n( \u2212Eb(x) )]"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Vector space models have become popular in distributional semantics, despite the challenges they face in capturing various semantic phenomena. We propose a novel probabilistic framework which draws on both formal semantics and recent advances in machine learning. In particular, we separate predicates from the entities they refer to, allowing us to perform Bayesian inference based on logical forms. We describe an implementation of this framework using a combination of Restricted Boltzmann Machines and feedforward neural networks. Finally, we demonstrate the feasibility of this approach by training it on a parsed corpus and evaluating it on established similarity datasets.", "creator": "TeX"}}}